project defect prediction CPDP refers predict defect target project lack defect data prediction model historical defect data project source data however CPDP source target project metric CPDP CM recently heterogeneous defect prediction HDP drawn increase attention predicts defect across project heterogeneous metric however building performance HDP remains challenge owe serious challenge imbalance nonlinear distribution difference source target datasets propose novel kernel spectral embed transfer ensemble KSETE approach HDP KSETE address imbalance source data latent feature source target datasets combine kernel spectral embed transfer ensemble perform public project HDP CPDP CM scenario multiple performance auc mcc experimental KSETE improves performance previous HDP percent auc mcc respectively KSETE improves performance previous CPDP CM percent auc mcc respectively conclude propose KSETE effective HDP scenario CPDP CM scenario introduction software defect prediction sdp active research software engineering drawn increase attention academic industrial community sdp crucial role allocate limited resource reasonably improve efficiency aim predict defect status software module software sdp approach prediction model historical defect dataset project apply model predict defect software module project within project defect prediction WPDP historical defect dataset consists information software metric defect software metric mainly static code metric halstead metric McCabe metric CK metric metric however perform WPDP project project without sufficient historical defect data project defect prediction CPDP propose researcher address issue CPDP aim predict defect project lack historical defect data prediction model project defect data fortunately publicly available defect datasets promise repository public datasets predict defect project lack historical defect data category instance feature instance CPDP reduce distribution difference source target datasets instance target data source data source instance feature CPDP attempt maximize distribution similarity source target data latent feature exist CPDP assume source project dataset target project dataset software metric however source target datasets software metric metric scenario CPDP specially HDP address defect prediction project heterogeneous metric HDP propose researcher propose HDP cca canonical correlation analysis cca technique unified metric representation  latent feature nam kim HDP feature selection  reduce distribution difference source target data propose KCCA HDP combine kernel cca technique HDP CTKCCA combine sensitive kernel cca technique however building performance HDP model serious challenge owe critical imbalance distribution complex nonlinear relationship source target datasets distribution difference source target datasets imbalance distribution factor accounting unsatisfactory prediction performance performance minority sample prediction model however exist HDP cca HDP KS KCCA CTKCCA imbalance although CTKCCA sensitive address appropriate matrix accord nonlinear relationship source target datasets impact prediction performance distribution difference source target datasets scenario HDP building performance defect prediction model serious challenge imbalance distribution nonlinear correlation increase difficulty HDP propose novel kernel spectral embed transfer ensemble HDP HDP balance source dataset series latent kernel feature subspace subspace maximizes similarity project source target datasets preserve intrinsic characteristic source target datasets finally built classifier feature combine predict label target data experimental KSETE significantly performs previous HDP CPDP CM contribution imbalanced data nonlinear correlation distribution difference exist heterogeneous defect prediction novel HDP KSETE imbalance multi kernel transfer ensemble propose evaluate performance propose KSETE extensive perform publicly available datasets community jira promise HDP CPDP CM scenario experimental KSETE effectiveness baseline replication package KSETE github zenodo publicly available organize previous CPDP CM HDP sdp briefly review formulate HDP detail propose KSETE experimental setup described experimental potential threat finally conclusion research related briefly introduce previous project defect prediction metric heterogeneous defect prediction sdp project defect prediction metric CPDP propose previous sdp CPDP attempt training data target data source data propose CPDP  filter NN aka filter training instance source data specifically instance target data source instance euclidean distance training data developed CPDP hybrid instance selection  phase target data instance selection source data instance filter CPDP attempt reduce distribution difference source target data source instance accord similarity target data propose CPDP approach transfer naive bayes TNB instance source data similarity calculate distributional characteristic target data construct classifier naive bayes developed CPDP approach hydra adjust source instance training propose extend adaboost algorithm propose cognitive boost vector machine  svm approach imbalance CPDP specifically similarity source data instance mechanism balance training data defect prediction model CPDP approach aim latent feature minimize distribution difference source target datasets propose CPDP TCA extends transfer component analysis TCA introduce appropriate normalization obtain CPDP performance krishna  introduce baseline  project defect prediction exist CPDP TCA TNB CPDP approach assume metric source target project however metric zero scenario approach cannot achieve prediction performance heterogenous defect prediction recently heterogenous defect prediction propose predict defect across project heterogeneous metric propose HDP cca canonical correlation analysis technique unified metric representation latent feature source target project specifically  metric metric source target data source specific metric target specific metric  transfer cca introduce metric maximize canonical correlation coefficient source target data nam kim propose HDP feature selection source subset metric remove redundant irrelevant metric source target metric metric similarity   source target metric built defect prediction model HDP complex nonlinear relationship software metric software defect propose HDP kernel canonical correlation analysis KCCA combine kernel cca technique argue previous HDP imbalance increase difficulty building performance HDP model therefore propose HDP CTKCCA combine sensitive kernel cca technique experimental CTKCCA performs related CPDP HDP however performance exist HDP satisfactory owe serious challenge imbalance complex nonlinear relationship source target datasets software metric defect distribution without lose intrinsic characteristic datasets transfer assumption traditional machine approach training future data feature distribution traditional machine approach transfer assumption allows transfer domain knowledge domain accord definition transfer source domain DS task TS target domain DT task TT transfer aim improve target predictive function DT knowledge DS DT DS DT TD TT transfer technique apply image classification recognition recognition robot model recommend actually CPDP transfer sdp formulation denote label source project dataset unlabeled dataset target project respectively denote module function file granularity source target project metric ith module source project denotes metric jth module target project corresponds defect information ith source project module denotes non defective defective source target datasets heterogeneous metric module defective defect respect unlabeled target project objective KSETE predict label label source data aim feature source target datasets optimal project define 3Definition source dataset target dataset mapping function RNS rnt denote respectively project source target datasets optimal projection project source dataset BΦ project target dataset BΦ optimization objective  BΦ BΦ BΦ BΦ BΦ sourcewhere denotes difference project dataset project dataset BΦ difference project datasets BΦ BΦ hyper parameter desirable datasets define BΦ BΦ BΦ BΦ BΦ BΦ sourcewhich average difference project source dataset project target dataset BΦ project target dataset project source dataset BΦ propose overall framework propose KSETE HDP detail KSETE aspect preprocessing kernel spectral embed transfer ensemble framework propose KSETE heterogeneous defect prediction preprocessing preprocessing remove duplicate instance remove instance alleviate imbalance perform synthetic minority sample technique SMOTE normalize SMOTE commonly imbalance sdp imbalance demonstrate improve prediction performance magnitude difference metric defect datasets normalization metric standard deviation kernel spectral embed sake computation source target datasets preprocessed instance suppose random sample increase instance dataset target data increase instance remove predict label target instance denote mapping matrix project source target datasets PΦ PΦ respectively difference project dataset dataset distortion function define BΦ BΦ PΦ FL BΦ BΦ PΦ FL BΦ BΦ PΦ FL BΦ BΦ PΦ sourcewhere  frobenius norm optimization objective rewrite  BΦ BTΦ BΦ IG BΦ BΦ PΦ PΦ  BΦ BTΦ BΦ BΦ PΦ BΦ PΦ BΦ PΦ BΦ PΦ sourcewhere BΦ BΦ assume orthogonal hyper parameter adjust desirable project datasets BΦ BΦ lemma BΦ BΦ PΦ PΦ PΦ BTΦ  PΦ BTΦ  source proof relationship frobenius norm matrix trace orthogonal matrix cyclic permutation trace xtx BTΦ BΦ  BTΦ BTΦ  sourcethe deduction obtain BΦ PΦ  BTΦ   PΦ source BΦ PΦ BΦ PΦ BΦ PΦ express fashion optimization rewrite  BΦ BTΦ BΦ IG BΦ BΦ PΦ PΦ source  BΦ BTΦ BΦ  source   PΦ source  PΦ BTΦ  source BTΦ  BTΦ  BTΦ  source derivation respect PΦ PΦ obtain PΦ BTΦ BTΦ PΦ BTΦ BTΦ source accord karush kuhn tucker PΦ PΦ obtain namely PΦ BTΦ  PΦ BTΦ  source BΦ BΦ derive accord theorem theorem minimization maximization  BΦ BTΦ BΦ IG  itr  source BΦ BΦ source source proof optimization rewrite  BΦ BTΦ BΦ IG BΦ BΦ PΦ PΦ    SourceSince   constant optimization  BΦ BTΦ BΦ IG  itr  source symmetric symmetric symmetric therefore optimization BTB theorem fan theorem denote symmetric matrix eigenvalue eigenvectors     sourceand arbitrary orthogonal matrix accord theorem optimal eigenvectors project datasets BΦ BΦ respectively algorithm algorithm lanczos calculate eigenvectors transfer ensemble objective transfer ensemble transfer ensemble classifier HDP perform  multiple target data multiple feature subspace source data framework random RF adopt RF ensemble algorithm decision learner RF characterize random sample training instance replacement random selection feature training data combine voting classification regression specifically training datasets learner dimension feature subspace RF construct series training subset perform bootstrap random selection feature subspace training dataset learner training subset finally voting combine predict label learner instance however difference transfer ensemble RF RF transfer traditional supervise machine task adapt RF transfer learner cannot directly training subset RF training subset target dataset propose  obtain correspond project datasets learner project source dataset algorithm kernel spectral embed input source data target data similarity confidence parameter dimensionality project feature kernel function output project source data BΦ project target data BΦ remove duplicate instance instance perform SMOTE algorithm denote sample source data increase dataset random sample source dataset target dataset instance instance target data increase instance remove project target data obtain construct matrix accord calculate eigenvalue correspond eigenvectors BΦ BΦ respectively BΦ BΦ return BΦ BΦ detail transfer ensemble iteration preprocess source data ensure source data target data instance iteration subset source data instance source data partial feature source data source target datasets calculate project datasets BΦ BΦ  algorithm target data increase instance remove BΦ classifier project source data probability prediction project target dataset assign classifier criterion iteration met combine prediction classifier output predict label target data algorithm detail propose transfer ensemble experimental setup detail research benchmark datasets performance statistical experimental setting algorithm transfer ensemble input source data label target data similarity confidence parameter feature dimensionality project feature kernel function learner learner learner output predict label target data remove duplicate instance instance perform SMOTE algorithm denote sample source dataset increase dataset random sample instance increase instance remove project target data obtain bootstrap sample feature replace  perform  algorithm denote obtain optimal project datasets BΦ  perform algorithm remove instance  accord BΦ training data learner obtain hypothesis apply  denote probabilistic prediction  assign default hypothesis obtain argmaxy  suppose  jth instance  label jth instance predict argmaxy   return predict label target data research investigate RQ KSETE outperform previous HDP motivation objective propose novel HDP therefore important research obtain performance previous HDP approach RQ KSETE HDP approach cca HDP KS CTKCCA hypothesis detail baseline default setting accord correspond aforementioned statistical propose hypothesis null hypothesis KSETE statistically significant difference HDP alternative hypothesis KSETE outperforms HDP statistical significance RQ KSETE outperform previous CPDP CM motivation although CPDP CM cannot HDP scenario HDP CPDP CM scenario investigate outperform exist CPDP CM approach RQ KSETE CPDP CM  filter TNB TCA   TCA transfer learner hypothesis detail baseline default setting accord correspond propose hypothesis null hypothesis null hypothesis KSETE statistically significant difference CPDP CM alternative hypothesis KSETE outperforms CPDP CM statistical significance benchmark datasets publicly available datasets community jira promise datasets community metric datasets community heterogeneous metric nasa datasets previous utilized owe data quality inter release defect prediction multiple version project version detail datasets brief description community overview project overview project dataset consists software metric source code metric CK metric orient metric entropy source code metric churn source code metric entropy metric metric promise prepared jureczko  contains metric CK metric  metric jira repository highly curated defect datasets jira contains software metric code metric metric ownership metric module jira refers java file performance evaluation apply evaluate prediction performance PD PF auc mcc popt ifa accuracy precision performance indicator imbalanced datasets accord previous sdp PD PF widely previous sdp ideal prediction model PD PF auc mcc overall performance PD PF consideration widely previous sdp imbalanced defect datasets popt ifa effort aware jit defect prediction apart PF ifa prediction performance define confusion matrix convention defective module regard positive sample non defective negative confusion matrix PD aka recall positive rate sensitivity PF aka false positive rate define PD tptp FN PF  TN source auc receiver operating characteristic roc curve curve plot PF axis PD axis auc widely rarely affected imbalance auc denotes performance random prediction model harmonic PD PF widely previous sdp indicator performance imbalanced datasets prediction performance define PD PF PD PF source mcc overall performance TP TN FP FN consideration mcc widely previous sdp utilized data unbalanced definition mcc mcc TP TN FP FN TP FP TP FN TN FP TN FN source popt refers percentage detect defective defective percent effort code concept code churn  diagram ifa initial false alarm encounter defect accord sort predict defective ascend sum code delete inspect actual defective ifa obviously ifa defective correctly predict prediction model benchmark datasets metric related loc code module instead compute popt ifa furthermore positive instance misclassified TP negative instance correctly classify FP mcc nan TP FP avoid situation FP TN TN remain TP FN unchanged mcc negative imbalanced datasets benchmark datasets highly imbalanced therefore directly mcc mcc nan statistical statistical wilcoxon rank cliff scott knott difference esd adopt wilcoxon rank clarify KSETE statistically outperform baseline dataset cliff quantify magnitude difference KSETE baseline scott knott esd demonstrate KSETE statistically performs baseline datasets specifically wilcoxon rank significance detect significant difference predictive performance KSETE baseline dataset wilcoxon rank non parametric alternative assumption data normally distribute cliff non parametric suggestion negligible medium cliff widely previous sdp cliff delta compute accord propose scott knott esd hierarchical cluster analysis partition treatment statistically distinct overlap multiple comparison nemenyi hoc  scott knott esd variant scott knott correction assume data normally distribute merges statistically negligible accord cohen delta scott knott scott knott esd previous sdp experimental setting validation datasets community project jira project promise project benchmark WPDP fold validation commonly model validation technique however unsuitable CPDP scenario source data training data target data data project model validation technique HDP scenario dataset source data community target data remind community combination project community combination EQ ant simplest validate model model EQ ant however excessively pessimistic optimistic evaluation performance model ant alleviate potential overcome inherent randomness KSETE SMOTE setting validation previous HDP randomly percent instance source data training data model target data percent instance source dataset ensure training data percent instance source data extent procedure minimum sample satisfy central limit theorem report average target dataset CPDP CM scenario dataset source data community target data community obtain combination validation procedure HDP scenario report average model target data parameter setting parameter algorithm source source denotes metric source dataset respectively SMOTE source data alleviate imbalance source data ratio defective module non defective moreover calculate kernel matrix advantage multiple kernel construct hybrid kernel function multiple gaussian kernel function exp positive dataset gaussian kernel function denotes instance feature kernel matrix calculate   dist XT sum var sourcewhere dist XT return matrix aij denotes euclidean distance ith instance jth instance var return vector jth variance jth feature sum sum function logistic regression LR chosen learner widely previous sdp furthermore LR performs complex model technique sdp unlabeled instance LR probability positive threshold probability positive threshold label instance positive experimental RQ KSETE outperform previous HDP PD PF project PD KSETE varies PF varies average KSETE obtains PD PF respectively although CTKCCA achieves average PD extremely PF HDP KS cca KSETE greatly improves PD acceptable average PF graphically visualize average PD PF baseline overall project  PD PF overall project comparison PD PF propose KSETE exist HDP respectively comparison overall auc mcc effort aware popt ifa KSETE baseline exist HDP project boldface significant difference wilcoxon rank significance marked average report average across overall project median median sig lose report project achieves performance correspond baseline statistical significance accord wilcoxon rank background denotes significant improvement correspond baseline background indicates moderate significant improvement cliff described comparison auc mcc propose KSETE exist HDP comparison popt ifa propose KSETE exist HDP comparison popt ifa propose KSETE exist HDP KSETE achieves performance project auc mcc popt ifa auc KSETE varies varies mcc varies popt ifa KSETE nearly significantly outperforms baseline auc mcc meanwhile KSETE performs significantly baseline popt ifa KSETE CTKCCA popt average KSETE obtains auc mcc popt ifa improvement KSETE baseline average percent auc mcc ifa respectively average popt CTKCCA closely KSETE obtains performance median KSETE obtains auc mcc popt ifa auc mcc improvement KSETE baseline median percent respectively cliff KSETE nearly largely improves performance baseline auc mcc KSETE improvement statistical significance HDP KS cca KSETE loses project CTKCCA project statistical significance popt KSETE largely improve performance baseline ifa boxplots auc mcc popt ifa KSETE baseline across overall project KSETE obviously outperforms comparable baseline auc mcc popt ifa boxplots auc mcc popt ifa across datasets KSETE HDP horizontal denotes median asterisk indicates scott knott esd propose KSETE previous HDP datasets auc  mcc popt ifa respectively axis HDP axis rank corresponds vertical denotes rank datasets dot vertical average rank denotes distinct statistical significance rank performance ifa auc mcc KSETE average rank KSETE without baseline KSETE significantly outperforms baseline popt CTKCCA obtains rank KSETE ifa KSETE obtains average rank baseline scott knott esd auc mcc popt percent ifa across datasets KSETE HDP rank performance ifa conclusion propose KSETE significantly outperforms previous heterogeneous defect prediction improves performance baseline average percent auc mcc ifa respectively KSETE significantly outperforms baseline CTKCCA popt RQ KSETE outperform previous CPDP CM comparison PD PF project PD KSETE varies PF average KSETE achieves PD PF KSETE largely improves PD baseline tolerable performance loss PF graphically visualizes average PD PF plot  average PD PF overall project boxplots auc mcc popt ifa across datasets KSETE CPDP CM horizontal denotes median asterisk indicates comparison PD PF propose KSETE exist CPDP CM respectively overall auc mcc effort aware popt ifa KSETE baseline exist CPDP CM project boldface average report average overall project median median report project achieves performance correspond baseline statistical significance wilcoxon rank comparison auc mcc propose KSETE exist CPDP CM KSETE achieves performance auc mcc popt auc KSETE varies varies mcc varies popt varies ifa varies comparison popt ifa propose KSETE exist CPDP CM average KSETE obtains auc mcc popt ifa KSETE improves auc mcc baseline percent respectively TNB KSETE improves popt baseline percent KSETE improves ifa baseline percent  filter median KSETE achieves auc mcc popt ifa KSETE improves auc mcc baseline percent respectively popt TNB KSETE outperforms baseline percent ifa KSETE improves baseline percent  filter  boxplots auc mcc popt ifa baseline across overall project KSETE outperforms baseline auc mcc average median popt KSETE outperforms baseline TNB average median ifa KSETE performs  filter outperforms baseline average scott knott esd propose KSETE previous HDP datasets auc  mcc popt ifa respectively axis HDP axis rank corresponds vertical denotes rank datasets dot vertical average rank denotes distinct statistical significance rank performance ifa auc mcc KSETE obtains average rank categorize without baseline popt KSETE categorize without baseline obtains average rank ifa  obtains average rank closely KSETE achieves average rank scott knott esd auc mcc popt ifa across datasets KSETE CPDP CM rank performance ifa conclusion propose KSETE suitable project defect prediction metric CPDP CM scenario KSETE significantly outperforms previous CPDP CM improves performance baseline average percent auc mcc across project popt ifa KSETE outperforms baseline TNB  respectively 7Discussion KSETE perspective architecture empirical analysis architecture perspective performance propose KSETE attribute aspect imbalance multiple kernel spectral embed transfer ensemble software defect dataset usually imbalanced non defective majority instance defective minority imbalance accounting predict performance minority instance defective module imbalance widely WPDP scenario imbalance category data random oversampling random sample SMOTE algorithm sensitive ensemble actually scenario HDP however previous HDP consideration CTKCCA sensitive address imbalance CTKCCA SMOTE preprocess imbalanced source data perform SMOTE project source data prediction performance KSETE obviously decrease perform SMOTE directly source data kernel consideration complex nonlinear relationship metric source target datasets complex nonlinear relationship metric defect advantage multiple kernel hybrid kernel function gaussian kernel function construct optimal project feature maximize distribution similarity kernel project source target datasets simultaneously preserve intrinsic characteristic source target datasets maximally however previous HDP apply kernel kernel function moreover previous HDP directly maximize distribution similarity without preserve intrinsic characteristic datasets useful information datasets lose feature feature cannot perform previous HDP transfer latent feature utilize feature transformation datasets however propose KSETE ensemble transfer attempt construct series latent feature multiple kernel subspace datasets combine latent feature challenge distribution sufficiently optimal feature multiple feature feature illustrate viewpoint intuitively empirical perspective KSETE component SMOTE multiple kernel transfer ensemble TE investigate component performance KSETE benchmark baseline typical datasets lucene hbase poi benchmark community module hbase medium lucene poi defective rate DR DR hbase medium DR poi DR lucene obtain combination lucene poi accord construct baseline combination percent instance source data training data model target data average performance combination finally report average KSETE baseline combination auc mcc popt baseline investigate KSETE boxplots auc mcc popt KSETE baseline  TE performance KSETE auc mcc performs KSETE popt indicates SMOTE positive performance KSETE KSETE outperforms  SMOTE TE popt performance auc mcc indicates SMOTE feature transformation SMOTE kernel function obviously outperforms without SE almost performs popt KSETE outperforms KSETE auc popt performance mcc indicates multiple kernel positive performance  outperform  auc mcc popt indicates TE positive performance KSETE boxplots auc mcc popt KSETE baseline horizontal denotes median asterisk indicates performance KSETE default similarity confidence parameter performance propose KSETE prediction combination lucene ant lucene source data ant target data specifically KSETE percent instance source dataset randomly training data prediction performance HDP evaluate target data finally report average PD PF mcc KSETE error PD PF mcc KSETE axis axis denotes performance dot vertical denotes average performance KSETE across prediction combination lucene ant error performance KSETE prediction combination lucene ant error standard deviation attention distribution similarity project source target datasets BΦ BΦ PF mcc attention loss intrinsic characteristic source target datasets BΦ BΦ PD mcc PD therefore naturally balance objective performance KSETE default learner transfer ensemble algorithm performance propose KSETE prediction combination lucene ant lucene source data ant target data specifically performance KSETE error PD PF mcc KSETE axis axis denotes performance dot vertical denotes average performance KSETE across prediction combination lucene ant error KSETE prediction combination lucene ant error standard deviation utilize ensemble strategy PF mcc obviously utilize ensemble strategy extent PF mcc generally increase along increase although PD relatively stable increase extent performance KSETE improve decrease KSETE sensitive PD therefore utilized default threat validity potential threat validity research internal validity threat internal validity mainly relate implementation baseline lack causal analysis model interpretability bias source code baseline CTKCCA  publicly implement remind baseline carefully accord description correspond although checked source code carefully error causal independent dependent variable objective analyze related KSETE achieves prediction performance HDP CPDP CM scenario feature transformation however illustrate meaning feature feature transformation cca CTKCCA TCA suffer threat building prediction model performance interpretability future external validity external validity indicates generalize research situation evaluate performance KSETE perform public project community jira promise KSETE CPDP CM HDP performance wilcoxon rank cliff delta scott knott esd however cannot finding completely suitable defect datasets defect datasets baseline apply reduce threat construct validity threat construct validity mainly refer bias performance evaluation performance PD PF auc mcc popt ifa popt ifa effort aware widely jit defect prediction however balance apply owe limitation utilized imbalanced defect datasets previous meanwhile jit defect prediction benchmark metric loc instead compute popt ifa therefore threat conclusion CPDP CM HDP aim predict defect across project heterogeneous metric propose novel KSETE HDP KSETE SMOTE alleviate imbalance source data KSETE latent feature propose multiple kernel spectral embed transfer maximize distribution similarity source target datasets preserve intrinsic finally ensemble classifier multiple feature transfer ensemble algorithm predict label unlabeled target data perform project community jira promise performance KSETE evaluate PD PF auc mcc popt ifa KSETE exist HDP CPDP CM wilcoxon rank cliff scott knott esd HDP scenario CPDP CM scenario respectively experimental KSETE effective HDP CPDP CM scenario future improve owe powerful feature capability