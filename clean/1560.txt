cyber physical cps application smart traffic smart agriculture smart grid etc commonly distribute compose user application microservices typically connection physical inherent cps brings respond becomes computation loop requirement reflect approach guarantee response service geographically user service developed program elaborate allows predict upper bound response service computer service importantly approach focus minimize impact developer service program model limit usage library etc previous keywords cyber physical guaranteed latency introduction software service commonly distribute compose application user device microservices cyber physical cps data driven application smart traffic agriculture utility application rely data sensor perform computationally intensive task data analytics optimization decision prediction cannot execute constrain device therefore execute however connection physical inherent cps respond whereas primarily built average throughput massive requirement impose bound response execute task significant response due communication latency concept aim reduce latency computation cluster physically closer user device throughout definition  assume computation traditionally centralize data regular network closer user differs fog compute related research workload traditionally decentralize execute user device localize iot gateway load usage compute reduces communication latency alone guarantee bound response becomes computation focus optimize average performance computation guarantee upper bound computation individual request address requirement cps approach reflect requirement CPSs computation loop guarantee request domain program rarely reasonable choice price developer program limited choice library relatively exotic program model periodic non task advocate standard technology microservices package container kubernetes program java scala python development microservices guarantee restrict application requirement guarantee response probabilistic response response application augment reality planning coordination video audio processing etc generally comprises application local loop application computation consequently guarantee pertains quality availability optimality safety importantly microservices continuous workload processing video audio etc closing workload etc explicit operation context article approach guarantee response microservices container environment kubernetes microservices developed program java elaborate allows predict upper bound response confidence microservice computer microservices prediction essential admission schedule deployment container computer combine adaptive deployment deployment component enables microservices probabilistic guarantee response important feature approach aim remove burden specify computational resource developer service guarantee treat microservices apriori knowledge microservices developer instead performs microservices data performance prediction deployment decision approach specifically  non public environment  operator infrastructure deployed microservices contrast public provider cope unknown application unknown workload unknown client etc structure motivation approach evaluation discus limitation approach related concludes motivation motivation application simplify realistic version augment reality   project  focus develop architecture image video processing cps application consists client application mobile device mobile phone service host node client client application capture video via phone camera service analysis augmentation comprises identification lookup database technology kubernetes KS listing deployment descriptor yaml application deployment descriptor capture application architecture microservices define via kubernetes  construct image KB image seamlessly mandatory augmentation information client without significant delay ideal service analysis device service collocate service potentially tenant deliver response focus specify latency requirement approach satisfy address service collocate resource service latency requirement service met image KB image manage latency outline approach perspective developer application overall architecture approach algorithm predict response upper bound microservice colocated microservices finally discus operational boundary prediction algorithm strategy emphasis minimize impact application developer essentially approach application developer creates typical KS artifact code container deployment descriptor extension approach developer specifies requirement per service KS deployment descriptor requirement interpret platform locates microservices node microservices interact performance wise performance interaction threshold violation requirement contrary traditional exist deployment developer selection VM virtual CPUs memory IOPS etc similarly developer specify auto trigger concern internal developer confront anyway typically justified abstraction developer application requirement responsibility ass performance application virtual CPUs memory IOPS etc platform approach deployment happens developer develops application collection microservices creates deployment descriptor specifies requirement developer submits application performs performance assessment consists performance microservices baseline performance performance workload assessment requirement guaranteed deployment parameter virtual CPUs dedicate memory reserve IOPS allocate microservice assessment informs developer potentially price service developer confirms deployment calculates performs deployment data assessment capture requirement latency extend KS deployment descriptor additional attach specification microservice extend deployment descriptor listing technically requirement capture probe deployment descriptor probe function application developer performance measurement requirement specify statistical expression probe assume probe developed probe emulates typical behavior request actual microservice probe input performance encapsulates workload image image recognition service execute performance another important probe execute without affect functionality microservice allows execute probe production environment ass microservice complies requirement requirement probe SLA service agreement developer platform autonomously platform probe requirement feasible contract developer latency sensitive microservice schedule workload thanks probe approach automatically ass service treat understood automate creation SLA novelty approach image KB image motivation application consists client capture video microservice detect recognize plus database microservice lookup database microservice important timing requirement recognizer load data database discussion specification requirement recognizer service developer probe utilizes procedure regular api microservice detect recognize predefined predefined nevertheless probe affect microservice cumulative detect ass microservice deployed probe microservice isolation predefined background workload characterize microservice resource demand along dimension cpu memory IOPS bandwidth allows infrastructure model microservice service admit deploy timing requirement met exclude network characterization assume latency sensitive traffic network magnitude overall capacity network latency sensitive network configure dedicate qos priority regular traffic network network connection latency sensitive microservices virtually unlimited resource overall architecture overall architecture approach motivation depict architecture depict infrastructure layer computer organize application layer microservices computer image KB image overall architecture infrastructure layer architecture consists data etc consist computer microservices timing requirement deployed additionally central data microservices without requirement deployed manage latency adaptation controller addition management interface manages deployment deployment individual data agent client microservices deployed data typically closest depends actual utilization client application microservices target privately assume data largely homogeneous compose computer identical configuration application layer deployment application client motivation recognizer microservice deployed database microservice without timing requirement central client another application contrast motivation microservice ensure timing requirement met adaptive architectural manage latency adaptation controller manages KS consist multiple data manage latency adaptation controller interprets requirement specify extend deployment descriptor decides collocate microservices instructs exist unmodified KS perform deployment interaction manage latency adaptation controller KS realize standard KS api manage latency adaptation controller implement MAPE adaptation loop periodically requirement met knowledge predicts future requirement prediction enables intervention detection requirement violation importantly proactive reaction ahead adaptation loop manage initial deployment deployment microservices deployment nearly identical initial deployment calculation requirement periodically within loop account placement microservices prevent unnecessary location monitoring phase loop controller periodically monitor probe currently deployed microservices resource utilization computer cluster analysis phase controller data microservice model perform analysis prediction algorithm described shortly evaluate deployment alternative microservices currently microservices submit deployment fitting deployment planning phase comprises deployment task alternative execution phase deploy microservices satisfy timing requirement image KB image initial data collection predictor node information microservice obtain assessment phase assign deployed microservice resource perform task within timing constraint resource allocation strictly enforce feature operating containerization technology virtualization platform prevent microservices exceed allocate resource due spike client negative impact execution microservices prototype enforce resource allocation feature docker linux  performance prediction collocate workload analysis deployment alternative introduce novel performance prediction algorithm statistical characterization measurement similarity comparison reveal dependency background workload microservices measurement structure complex data performance prediction scenario relevant prediction data extract linearize data fitting model model constrain reliable statistic estimate performance fidelity explain initial data assume initial workload parameter random variable characterize behavior scenario background workload ensure robustness predictor fix measurement various realize finally random sample random variable scenario measurement workload scenario without background workload various combine denote scenario simplify exposition standard matlab tensor notation multidimensional algebra notation  initial data data structure tensor frontal slice matrix corresponds scenario measurement frontal slice organize scenario correspond workload  frontal slice matrix entry positive integer zero encodes scenario correspond frontal slice scenario background otherwise vector entry positive integer compress encode organization scenario tensor access relevant frontal slice computation frontal slice illustrates data data correspond performance workload encode entry vector frontal slice similarly initial data workload odd odd frontal slice combine slice available initial data prediction approach consists phase data preprocessing evaluation statistical data representation task fitting constraint linear regression positive integer data prediction combination predict dependency phase computation apriori computational later phase phase goal evaluate statistical characteristic scenario capture dependency parameter measurement proceed construct tensor frontal slice corresponds scenario information statistical distribution measurement compute sample median sample percentile standard relative deviation standard error difference sample maximum sample minimum image KB image data workload characteristic median typical behavior sample maximum minimum capture information extreme measurement difference later effective penalization measurement fidelity ensure reliability performance prediction alternatively prior bound confidence interval sample percentile contains various quantity reveal dependency performance workload slowdown parameter correspond difference sample percentile random sample separately background workload scenario absolute relative version similarly influence parameter influence background workload estimate upper bound confidence interval phase initial data statistical characteristic user specify prediction requirement detect precomputed scenario relevant prediction scenario performance prediction already workload performance prediction workload available situation simpler prediction statistical characteristic scenario prediction model extract integer matrix encode scenario slice correspond workload corresponds scenario integer vector meaning analogous workload scenario otherwise data fitting model unknown correlation preselected initial scenario data fitting various reduce underestimate phase ensure entry negative dimensional sparse model measurement recently constraint nonnegative constraint nls advantageous standard approach modification nonnegative algorithm applicable nls euclidean norm norm nonzero entry frontal slice relevant prediction positive importance correspond slice model meaning measurement available prediction situation separately extra measurement available modify remove processing task avoid cycling prediction detect workload resembles compute characteristic similarly initial data characteristic frontal slice correspond similarity sample characteristic workload carefully vector norm difference median deviation relevant random variable proceed replace workload tensor matrix vector extend measurement phase approximation combination workload dependency predict behavior scenario recall matrix encode relevant scenario detect phase importance nonnegative entry extract frontal slice correspond safety penalization coefficient difference sample maximum minimum measurement matrix characteristic scenario predict role played slowdown parameter percentile estimate performance obtain shift percentile linear combination estimate slowdown prediction algorithm summarize schema ensure operational boundary interaction colocated microservices underlie physical resource generally complex non linear physical resource exhaustion consequently prediction accuracy varies combination application resource cannot actionable scenario ensure predictor confidence within adaptation loop critical establish predictor operational boundary ensure manage within boundary boundary express limit utilization cpu memory IO resource characterize microservice workload demonstrate predictor limit along dimension synthetic benchmark execute colocated workload target resource cpu memory IO bandwidth utilization predict actual benchmark quad core intel  linux hyper thread turbo boost management feature disabled obtain stable timing cpu utilization focus interaction colocated cpu bound microservices analyze response demand cpu increase exceeds available cpu capacity evaluate scenario deploy cpu bound microservices container container limited overall cpu capacity core experimental quad core machine simulate microservices benchmark target cpu utilization service utilization benchmark passively utilization performs cpu intensive calculation overall cpu demand axis correspond cpu utilization container axis response boxplots response container cpu utilization percentile response response remain stable container cpu utilization overall increase container cpu utilization overall response grows linearly cpu  percentage container cpu utilization overall response roughly overall cpu capacity sufficient intuition image KB image benchmark response colocated microservices cpu utilization interpretation reference colour legend reader refer web version article contrast predict response percentile increase container cpu utilization overall model specifically account context switch overhead consume variable portion overall cpu capacity limit predictor operational boundary overall cpu utilization IO throughput focus interaction IO bound microservices access disk analyze response demand IO bandwidth increase exceeds available IO capacity define available IO capacity maximal amount data computer per IO utilization percentage capacity microservice deploy microservices simulated benchmark container IO utilization overall IO demand axis correspond IO utilization container axis response container IO utilization overall median quartile response remain stable extreme linearly increase utilization container IO utilization overall response inter quartile becomes wider microservices influence variance response increase container IO utilization overall median response increase grows along extreme linear fashion overall demand IO bandwidth exceeds available IO capacity image KB image benchmark response colocated microservices IO utilization predictor relatively conservative estimate response percentile slightly percentile throughout entire suggests predictor operational situation overall IO demand exceeds available IO capacity possibly workload therefore conservatively limit predictor operational boundary available IO capacity memory utilization focus interaction memory bound microservices analyze response memory utilization increase define memory utilization microservice spends memory bound task allocate reading memory explicitly define available memory utilization capacity assume service spending processing perform memory intensive operation almost saturate memory subsystem similarly previous deploy microservices simulated benchmark container memory utilization overall memory utilization demand axis correspond memory utilization container axis response memory utilization container increase response median percentile increase exponential fashion inter quartile remains stable image KB image concurrent memory utilization predictor useful estimate response percentile container memory utilization overall operational boundary limit predictor however limit synthetic benchmark amount memory intensive perform normal workload therefore externally observable metric IO throughput overall cpu utilization ensure within operational boundary rate cache llc per millisecond strongly correlate linearly memory utilization experimental platform overall memory utilization operational boundary corresponds approximately llc rate evaluation evaluate ability predictor predict performance workload execute workload establish application benchmark evaluate approach opt option emulate application workload combination custom benchmark benchmark exist benchmark suite workload  suite extends  suite scala workload stress suite suite establish  suite memory computationally intensive workload execute java virtual machine stress suite variety benchmark target computational resource granularity benchmark suite allows easily wrap application perform considerable amount response lightweight request implement custom benchmark workload perform operation commonly data structure json processing compression recognition benchmark meaning cpu additional commenced cpu usage ram corresponds cache per millisecond execution additional additional llc disk negligible disk usage disk utilization additional additional disk utilization benchmark    demand   image ray trace  memory benchmark transaction banking application  optimize abc SWC swf file  topic model toolbox  program grid avr microcontrollers   matrix  writes json data disk  image writes pdf file disk  sort writes random disk  random cypher writes disk  remove item avl  remove item  warshall shortest vertex  dynamic program  dynamic program  detection image local directory  archive extraction compress folder file relatively  benchmark reflect workload likely application benchmark categorize resource usage image KB image evaluation prediction algorithm predict triplet interpretation reference colour legend reader refer web version article experimental evaluation model situation scheduler predictor workload admit benchmark workload already execute workload admission gate predict performance workload admit predictor measurement data isolated execution workload already execute admit data combination subset execute workload workload admit summary scheduler information measurement reflect simultaneous execution workload predictor adequate service workload admit benchmark evaluate benchmark combination numerous article therefore combination representative benchmark evaluation benchmark available online therefore limited benchmark cpu intensive application refer  benchmark  suite version bach  utilizes multi thread global illumination render photo realistic image synthesis disk intensive application refer ZB  application extract MB zip file file directory GB uncompressed data disk memory intensive application refer SM matrix benchmark stress suite application repeatedly performs transposition matrix cpu memory intensive application refer  library application detects input image generates amount output image overlaid mask identifies detect evaluate accuracy prediction algorithm scenario correspond scheduler algorithm specifically predict percentile response workload execute combination workload unrealistic prediction algorithm completely accurate inaccuracy prefer predictor conservative estimate predict longer response scheduler response guarantee ensure operational boundary prediction algorithm prediction conservative prevent scheduler  workload probabilistic response guarantee utilize resource efficiently scenario scheduler performance benchmark execute individually combination colocated benchmark scheduler data predict performance benchmark execute specific situation application benchmark admission scenario axis combination workload enclose parenthesis already deployed workload scheduler performance data available axis correspond response benchmark deployed plot percentile benchmark response predict percentile configuration predictor estimate response percentile conservatively within relative average scenario scheduler performance combination colocated benchmark data triplet predict performance fourth benchmark colocated triplet scenario predictor estimate response percentile conservatively within relative average exclude combination instance ZB benchmark configuration outside operational boundary predictor instance ZB benchmark utilizes IO bandwidth similarly exclude combination instance SM benchmark outside operational boundary predictor express cache per millisecond limit instance SM benchmark exhibit llc rate respectively discussion manage qos extremely complex facet sufficient sustain entire research consequently amount exist daunt nearly impossible fitting context usable interface developer therefore advantage context avoid complexity traditionally associate qos management public generic unknown application unknown workload unknown developer unknown client aware operational boundary prediction algorithm combination colocated workload therefore operational boundary predict beyond scope however context assume microservices microservice privately infrastructure somewhat restrict bias towards latency sensitive application therefore chosen approximate static boundary synthetic benchmark prediction operational boundary future restrict microservices microservice combinatorial headroom stag phase resource demand performance interference combination microservices evaluate IO utilization model microservice demand network bandwidth unlimited model purpose rationale decision application likely latency sensitive bandwidth intensive defeat primary purpose reduce communication latency due distance therefore assume network infrastructure configure assign critical network traffic qos priority latency sensitive service response requirement saturate network bulk transfer related compute blessing curse user benefit unprecedented availability elasticity resource benefit attach infrastructure service provider continually balance tension efficient resource utilization determines quality service guarantee demand provider latency sensitive LS application management resource therefore become vast quickly research survey mapping categorize challenge various domain context research review chronologically relevant approach discus specific define frame reference research amount exist research selection admittedly exhaustive qos aware framework transparently adjusts resource allocation mitigate interference resource profile virtual machine VM submit client stag server ass amount resource attain desire qos without interference manages resource allocate deployed vms loop  technique predict performance degradation due shard processor cache placement linear oppose exponential measurement application replace synthetic clone tune mimic application cache pressure interference due colocation predict matrix interference configuration cache clone  fledge scheduler workload placement decision performance resource constraint bubble avoids pairwise colocation profile characterize qos degradation LS application synthetic workload configurable memory subsystem stress bubble  batch application reporter workload sensitivity curve  batch application mapped configuration bubble predict interference inflict batch application LS application bubble flux improves bubble perform online profile LS workload account workload phase identify colocation opportunity   aware scheduler collaborative filter classify incoming application limited profile signal similarity previously schedule application differentiate batch LS application schedule application minimize interference maximize utilization application classify interference tolerance microbenchmarks stress specific resource tunable intensity concurrently application interference application performance performance isolation  improves  performs resource allocation instead resource assignment  extends classification  scenario workload constraint resource allocation api allows express performance constraint regard throughput latency  representative model approach qos aware resource management discrete markov chain model predict performance interference colocated vms  within host application VM related metric runtime metric maintain application specific model capture proportion application resource model predict slowdown due colocation ultimately placement guest VM instance adjust resource available hypervisor  performance aware resource manager controller optimizes allocation cpu resource vms qos target maintains online model relationship allocate resource application performance loop adapt resource allocation progress towards probabilistic performance target express percentile request response within bound  colocation manager linear regression model predict combine contention resource  multiple batch workload LS workload  performs contention characterization batch workload LS workload remove batch workload contentious colocation selects subset batch workload colocate latency sensitive workload combine contention linear regression prediction model contention due multiple batch workload approach aim performance interference aware adaptive essential thereof manages resource allocation assignment environment achieve efficient utilization available resource application qos target selection illustrates variety approach propose fitting context none approach similarly  approach profile application stag utilize proxy workload approach profile application probe resource demand representative workload periodically monitor response probe ensure deployment desire performance guarantee approach exception bubble  important dedicate probe developer processing proxy workload regular user request business logic application dedicate probe guarantee precise semantics guarantee creates suitable contract application developer treat application unlike approach approach performance interference treat resource equally relies statistical characterization similarity reveal dependency background workload aware potential non linear relationship predict model therefore actively limit importantly enforce operational boundary prediction algorithm inherent integration prediction adaptation mechanism creates novelty approach prediction admission admission preserve quality prediction specifically target non public environment allows constrain cater context specific detail instead vms focus container technology probabilistic performance guarantee developer monitoring probe explicitly specify performance objective probe admit application deployment considers objective satisfiable treat application focus container stem mainly overhead flexibility vms allows relocate service flexibly response mobility user another related target service agreement SLAs environment difference approach classic SLAs target non public privately allows additional assumption underlie infrastructure another difference approach estimation response realtime  approach focus estimate throughput finally SLAs interconnect adaptation retroactive proactive performance model microservice measurement therefore react proactively situation review approach classic SLAs similarity approach  babu  SLA aware schedule load balance primary goal load balance service deploy another computer computer evenly load reduce SLA breach approach service response instead relies information application developer amount memory cpu instruction per etc semantic gap concept SLA specification domain specific application requirement formulation SLA  approach statically analyzes service important performance runtime predicts bound response series analysis difference  focus specific approach performance service function probe approach information service rely fix knowledge important addition approach prediction manage deployment deployment service panda  SLA schedule algorithm similarly approach author compose multiple data schedule service deployment data honor SLAs contrary approach author SLAs service execution penalty SLA violation schedule algorithm attempt minimize service execution resource aware scheduler SLA  previous approach considers heterogeneous approach author deployment service environment focus mainly minimization deployment finally monitoring environment partially related orthogonal purpose advocate holistic monitoring service context fog infrastructure author analyze requirement service classification qualitative analysis exist  framework dedicate monitoring service flexible monitoring infrastructure fog compute paradigm monitoring approach monitoring trigger migration micro service node improve latency performance guarantee monitoring operates basis primary purpose avoid undetected monitoring generally focus indicator related health application trigger typically coarse grain corrective action restart application migrate virtual machine etc action remedy transient cascade monitoring alone execution guarantee approach technical aspect monitoring monitoring application health information primarily operates conceptual goal approach probabilistic execution guarantee monitoring data predict application performance probe deployment scenario enact deployment scenario trigger deployment multiple manage application corrective action conclusion summarize approach guarantee response microservices deployed approach allows developer express desire guarantee directly probabilistic requirement response within contrast explicit reservation exist platform CPUs IOPS disconnect developer perspective application performance approach relies statistical predict upper bound microservice response confidence colocated microservices prediction adaptive loop manage deployment deployment microservices proactively maintain performance target performance interference colocated workload non linear resource  negatively impact accuracy response prediction limit operational boundary prediction algorithm algorithm estimate broadly described non  consequently scheduler admission deployment microservices response guarantee ensure manage within operational boundary prediction algorithm important mention non  issue primarily microservices continuous workload processing etc already mention introduction  microservices processing burst longer inactivity burst benefit workload continuous  automatically implies inability timing requirement important novelty approach respect minimal impact microservice developer approach treat microservices framework apart performance requirement express application probe infrastructure automatically profile microservices submit deployment data prediction algorithm consequently impose specific software architecture partition task developer program importantly developer performance model application specify application resource requirement CPUs IOPS disconnect perception application performance specifically focus networking future knowledge network topology network latency loop enable flexible distribution microservices across node