code comment generation summarize semantic information source code generate description developer comprehend program reduce spent software maintenance approach rnn recurrent neural network encoder decoder neural network however generate quality description summarize information code dependency propose novel semantic cnn parser SeCNN code comment generation cnn convolutional neural network alleviate dependency novel component source code cnn ast cnn capture semantic information source code evaluation conduct widely dataset java experimental SeCNN achieves performance execution baseline previous keywords program comprehension code comment generation convolutional neural network memory network introduction software development maintenance developer nearly source code comprehension code comment code intuitive effective developer understand software code quality code comment important role software maintenance reuse unfortunately due tight project schedule software project comment comment outdated due software update significantly reduce readability maintainability program moreover code comment tedious consume task software development effort developer automatic code comment generation technique effective address issue code snippet code comment generation generate target code comment previous code comment generation approach classify category template approach AI artificial intelligence approach template approach usually predefine template content target code although significant progress keywords template selection template approach limitation development AI approach tend apply encoder decoder framework code comment generation framework rnns usually encoder decoder apply code comment generation source code input sequence generates code comment output sequence however strict structural text source code contains structural information important program model address issue approach generate comment abstract syntax ast code via rnns analysis approach dependency code comment summarize information code semantics encode exist ast approach serialize ast sequence token via traversal extract feature rnns however rnns encode sequence cannot capture structural semantics propose novel semantic cnn parser SeCNN generate code comment java functional java program javadoc comment correspond java typically describes functionality java accord javadoc guidance cnns capture feature code effectively slide SeCNN cnn capture feature effectively code alleviate dependency furthermore novel component source code cnn ast cnn improve structure traversal isbt encode semantics source code identifier split via camel alleviate vocabulary SeCNN convolutional neural network cnns encode semantic information source code cnn extract lexical information code token another cnn extract syntactic information ASTs generate code comment SeCNN memory lstm attention mechanism decoder generate code comment evaluate effectiveness SeCNN conduct dataset java experimental SeCNN achieves performance metric approach knowledge contribution summarize propose novel SeCNN generate code comment SeCNN cnns encoder capture semantic information source code lstm attention mechanism decoder generate code comment propose novel ast traversal improve structure traversal isbt encode structure information moreover alleviate vocabulary source code token ast node camel conversion split source code identifier effectively decrease unique token ast node vocabulary evaluate SeCNN dataset java experimental SeCNN effective efficient baseline facilitate replication evaluation future code comment generation technique source code dataset available github repository organize background introduces framework detail SeCNN setup analysis discus threat validity survey related code comment generation novelty finally concludes potential future background introduce background motivation model code comment generation technique inspire technique text generation task nlp model generate code comment code corpus specifically sequence generate input model aim estimate probability output sequence respond sequence previous code comment generation recurrent neural network rnn memory model lstm popularly technique introduce subsection standard recurrent neural network recurrent neural network rnn widely code comment generation approach standard rnn sequence data input perform recursion evolution direction sequence generation direction sequence finally standard rnn connects node cyclic chain framework standard rnn unfolded specifically predicts subsequent calculates hidden accord previous hidden memory model propagation standard rnn model gradient explode disappear dependency input sequence alleviate researcher improve rnn propose memory model lstm lstm consists gate memory gate forget gate input gate output gate respectively forget gate information discard retain input gate update status output gate hidden contains relevant information previously typical memory lstm content vector previous hidden previous generate forget gate decides information discard retain generate input gate decides information information generate output gate decides output content vector convolutional neural network convolutional neural network cnn representative neural network breakthrough image analysis computer vision recent research cnn model effective processing nlp perform classification web task cnn technique usually convolutional layer pool layer convolutional layer cnn calculate dot input data matrix filter filter slide across entire input data dot calculation operation average pool maximum pool commonly pool latter cnn pool layer reduce spatial dimension however reduce depth network pool layer feature sensitive image input average pool layer average feature input recent cnn technique propose technique advantage handle due limitation TextCNN introduce cnn TextCNN cnn handle text related convolutional layer convolution kernel TextCNN convolutional layer vector feature extraction TextCNN vector vector correspond convolution compute formula convolution kernel apply extract feature adjacent vector dimension input vector slide adjacent vector bias non linear function feature extract adjacent vector image KB image convolutional layer TextCNN motivation code comment generation approach developer understand purpose content code exist rnn encode decoder approach cannot generate quality description summarize information code dependency besides another code comment generation developer usually define various identifier identifier compose multiple situation vocabulary explosion negative lexical information extraction convert vector previous split identifier code multiple usually lexical information lack syntactic information limit effectiveness code comment generation approach improve quality generate comment propose novel code comment generation approach SeCNN previous approach SeCNN split identifier code ast multiple cnns extract source code semantic information finally SeCNN lstm attention mechanism generate comment motivation cnn previous cnn capability extract lexical syntactic information source code lstm previous lstm attention mechanism suitable text generation moreover traditional attention mechanism attention mechanism focus feature extract cnn input approach introduce framework SeCNN detail propose approach framework SeCNN significant issue exist code comment generation technique source code introduces vocabulary faster SeCNN employ cnn extract semantic information source code split identifier code ast multiple handle vocabulary explosion challenge framework SeCNN SeCNN consists data preprocessing semantic information extraction code comment generation data preprocessing convert source code code token vector ast vector alleviate vocabulary camel conversion split identifier code token ast node described detail innovation mainly reflect cnns extract semantic information source code finally code comment generation lstm attention mechanism decode semantic information generation comment detail SeCNN introduce subsection data preprocessing input cnn model vector convert input vector data preprocessing source code preprocessing source code consists keywords operator identifier lexical information neural network algorithm construct input sequence neural network algorithm employ widely  convert source code token furthermore address vocabulary explosion split identifier code accord camel conversion code token convert lowercase sequence data code token embed convert vector dimensional vector code token abstract syntax improve structure traversal code token lexical information syntactical information abstract syntax ast abstract representation syntax structure source code ast syntactical information source code  parse source code generate ast approach structure traversal sbt propose aim preserve structural information source code extent syntax information extraction although sbt promising code comment generation task sequence generate sbt contains duplicate content moreover sbt bracket structure conducive cod structural information sbt sequence contains identifier source code contains vocabulary explosion propose novel ast traversal improve structure traversal isbt isbt encode structure information code isbt propose component encode isbt sequence illustrate isbt traverse sbt traverse ast generate sbt sequence traverse ast sbt serial ast via pre traversal replace bracket sbt sequence split sbt sequence serial ast node ast node attribute finally address vocabulary explosion challenge camel conversion split node  node corresponds identifier source code serial node split node  serial node  split layer serial analysis restore sbt sequence unambiguously generate sequence isbt therefore improvement sbt information construct input cnn model propose isbt cnn encode isbt sequence specifically serial sequence node sequence embed convert vector dimensional vector extract feature isbt cnn integrate sequence ast node sequence convolution compute isbt cnn kernel vector extract convolution besides relu rectify linear commonly non linear activation function neural network define image KB image sequence ast sequence isbt replace bracket sbt traversal activation function introduce dynamical network widely subsequent neural network research semantic information extraction SeCNN cnn model extract semantic information detail subsection convolutional neural network model propose approach cnn model capture semantic information source code cnn extract lexical information code token another cnn extract syntactic information ASTs vector code token denote dimensional input vector vector apply series convolutional layer extract feature convolution calculate convolution kernel trainable parameter update training dimension input vector slide adjacent vector non linear function relu function feature vector extract convolution operator convolution matrix correspond convolution slightly differs TextCNN maintain feature vector dimension convolution convolution kernel convolution layer feature vector convolution apply zero pad input vector therefore feature vector extract convolutional layer dimension input vector therefore shortcut connection feasible SeCNN network degradation depth neural network layer increase accuracy rate saturation increase depth accuracy rate decline shortcut layer parallel activate function code token vector isbt information convolution operation extract feature vector finally obtain semantic vector concat function tensorflow semantic vector pool SeCNN pool construct lstm initial encode input decoder lstm initial context vector hidden pool algorithm max pool attention pool max pool reduce feature dimension input data attention pool combine lexical grammatical information construct content vector semantic information SeCNN firstly apply max pool feature extract code SeCNN fix vector code pool compute attention isbt encoder SeCNN feature extract isbt attention pool aim lexical grammatical information similarly feature max pool vector isbt pool apply attention pool isbt pool feature vector finally initial lstm decoder code comment generation SeCNN lstm attention mechanism decode semantic information source code generate code comment attention mechanism SeCNN attention mechanism assign semantic vector extract cnn model determines important vector output target specifically propose approach classical attention propose attention mechanism SeCNN attention mechanism calculate context vector predict target formula semantic vector extract cnn model indicates correspond attention calculate SeCNN firstly calculates alignment model input output decoder calculation formula define alignment model neural network finally softmax function normalize obtain attention formula define sequence output purpose decoder decode semantic vector generate cnn model generate code comment predict formula stochastic output layer estimate probability context vector hidden decoder input decoder eos lstm receives series prior input model training predict decoder component training mode decoder mode previous predict input label text setup research evaluate effectiveness propose approach baseline accuracy efficiency generate java comment specifically focus research RQ SeCNN outperform code comment generation baseline RQ verify code comment generation effectiveness SeCNN RQ conduct series empirical SeCNN code comment generation baseline DeepCom TL  hybrid DeepCom dual model ast attendgru RQ code comment affect performance propose approach SeCNN RQ investigate impact source code comment code comment generation effectiveness propose SeCNN RQ analyze experimental SeCNN generate comment source code RQ difference comment generate SeCNN comment RQ investigate quality code comment generate SeCNN manual manner RQ difference code comment generate SeCNN dataset previous corpus java comment project github dataset split dataset training validation statistic information dataset item indicates dataset code comment average comment token average code token moreover code comment java token comment punctuation comment code code segmentation previous data training validation accord ratio adopt dataset split detail statistic code snippet dataset project file item statistic code comment statistic comment  statistic code  performance metric evaluate effectiveness SeCNN MT machine translation metric chosen evaluation metric widely previous code comment generation widely performance metric text generation task processing nlp evaluation quality generate code comment calculates similarity generate sequence reference sequence closer candidate reference split dataset dataset item training validation define geometric gram accuracy simplicity penalty prevent generate calculate formula geometric average modify gram positive formula brevity penalty compute candidate generate comment indicates reference extract javadoc specifically performance metric widely adopt choice previous code comment generation widely machine translation metric evaluates translation hypothesis align reference translation calculate similarity feature introduction synonym explicitly improve correlation judgment machine translation quality compute penalty coefficient parameterized harmonic compute chunk chunk adjacent reference determines maximum penalty determines functional relation fragmentation penalty respectively compute unigram unigram correlation MT metric performance code comment generation technique positive indicates code comment generation technique statistical analysis hypothesis propose approach exist baseline conduct data code comment adopt wilcoxon rank analyze experimental wilcoxon rank alternative hypothesis approach data cannot assume normally distribute therefore reliable statistical basis effectiveness approach statistical analysis strength relationship variable population sample estimation quantity specifically calculate cliff delta non parametric quantify amount difference leverage cliff delta difference SeCNN baseline metric besides defines cliff delta negligible medium respectively baseline employ code comment generation approach baseline detailed description baseline introduce baseline DeepCom attention seqseq model generate comment java DeepCom ASTs input sbt convert ASTs sequence format performance SeCNN DeepCom implement DeepCom model accord parameter setting correspond baseline TL  model generates code comment capture semantics information source code api knowledge directly experimental correspond perform comparison baseline hybrid DeepCom variant attention seqseq model generate comment java hybrid DeepCom source code token ast structure generate code comment implement hybrid DeepCom model accord parameter setting correspond perform comparison baseline ast attendgru neural model combine source code code structure ast attendgru involves unidirectional gate recurrent gru layer source code ast ast attendgru modifies ast flatten procedure propose implement ast attendgru model accord parameter setting correspond perform comparison baseline dual model dual framework jointly code generation CG code summarization CS model enhance relationship task joint training addition impose constraint probability dual model creatively proposes constraint attention mechanism directly experimental correspond perform comparison experimental setting propose approach parameter accord previous related remain parameter optimize via validation isbt batch parameter optimize replace constant source code  str respectively accord java token isbt sequence exceed token therefore maximum code sequence isbt sequence pad sequence longer sequence clipped java token therefore maximum comment token eos comment decode sequence eos indicates decode sequence vocabulary code isbt comment respectively accord vocabulary tag replace unk model parameter setting described sgd algorithm parameter minimum batch randomly sample training encoder model SeCNN layer cnn convolution kernel convolution parameter optimize validation decoder model SeCNN layer lstm hidden dimension embed dimension parameter optimize validation detail initial rate rate decayed rate exponential decay reduce rate rate decay coefficient parameter optimize validation SeCNN clip gradient norm dropout strategy training dropout SeCNN entropy minimization function python tensorflow framework implement SeCNN conduct linux server intel xeon cpu ghz cpu tesla gpu GB memory analysis RQ SeCNN outperform code comment generation baseline introduce widely code comment generation technique baseline baseline technique TL  dual model implement technique DeepCom hybrid DeepCom ast attendgru evaluate performance SeCNN baseline MT metric gap automatically generate comment manually comment correspond experimental SeCNN outperforms baseline metric specifically SeCNN achieves improvement baseline respectively SeCNN semantic information source code generate quality code comment baseline average performance approach approach DeepCom TL  hybrid DeepCom ast attendgru dual model SeCNN automatic code comment generation improvement acceptable although impressive propose obtains absolute accuracy improvement hybrid DeepCom propose ast model DeepCom increase propose approach improves furthermore conduct wilcoxon rank verify competitiveness propose approach SeCNN hypothesis implement baseline approach perform hypothesis approach hypothesis significant difference SeCNN approach significance statistical rejection null hypothesis imply significant difference propose approach metric performs baseline conclude SeCNN significantly achieve performance baseline approach hypothesis approach DeepCom hybrid DeepCom ast attendgru moreover cliff delta evaluate difference SeCNN baseline metric cliff delta corresponds negligible outperform exist baseline lesser extent performance independent    DeepCom hybrid DeepCom ast attendgru SeCNN cliff delta SeCNN baseline approach DeepCom hybrid DeepCom ast attendgru finding analyze performance comparison baseline hybrid DeepCom performs DeepCom DeepCom learns syntactic information lack lexical information hybrid DeepCom slightly outperform TL  syntactic information api information generate code comment hybrid DeepCom outperforms ast attendgru prof rnn suitable construct decoder model generate comment moreover dual model baseline code generation comment generation task related future research improve performance propose approach relationship alleviate sample bias random shuffle sample dataset independently evaluate performance propose approach independently dataset training validation randomly model implement DeepCom hybrid DeepCom ast attendgru cannot implement TL  dual model experimental DeepCom hybrid DeepCom ast attendgru SeCNN performance overall experimental SeCNN achieves improvement achieves improvement baseline image KB image RQ code comment affect performance propose approach SeCNN RQ analyze prediction accuracy SeCNN source code comment introduce RQ implement baseline DeepCom hybrid DeepCom ast attendgru hybrid DeepCom extend DeepCom performance DeepCom moreover experimental RQ ast attendgru achieve performance DeepCom therefore baseline hybrid DeepCom ast attendgru evaluate performance SeCNN average SeCNN baseline demonstrate performance metric technique code approximate code approximate function define actual code return correspond approximate SeCNN performs baseline code increase SeCNN increase maintain accuracy code SeCNN node SeCNN achieve performance proportion respectively code incomplete performance reasonable demonstrate metric technique comment SeCNN impact comment SeCNN achieve performance subfigure proportion respectively specifically comment increase SeCNN increase maintains accuracy comment increase SeCNN increase decrease comment contains SeCNN achieve besides utilize wilcoxon rank verify competitiveness propose statistical analysis verify polyline significantly specifically hypothesis define significant difference SeCNN approach code comment report approach wilcoxon rank situation accept conclusion SeCNN achieve significantly performance hybrid DeepCom ast attendgru hypothesis approach code  DeepCom ast attendgru comment  DeepCom ast attendgru furthermore cliff delta SeCNN approach code comment metric performance SeCNN significant difference baseline code comment metric image KB image cliff delta code comment approach code  DeepCom ast attendgru comment  DeepCom ast attendgru RQ difference comment generate SeCNN comment analyze difference comment generate code comment generation technique automatic evaluation metric cannot fully reflect actual quality conduct manual evaluation evaluate quality automatically generate code comment RQ implement code comment generation technique DeepCom hybrid DeepCom ast attendgru hybrid DeepCom performance technique employ hybrid DeepCom baseline code comment generation technique perform comparison due manually analyze code comment commonly sample minimum code comment calculate formula code comment confidence error margin calculate obtain evaluation SeCNN hybrid DeepCom dataset respectively recruit volunteer development teacher professional doctoral feedback comparison guideline experimental within volunteer circumstance questionnaire user specifically randomly prediction reference therefore questionnaire consists input source code comment generate SeCNN hybrid DeepCom reference comment questionnaire volunteer volunteer evaluate comment code furthermore ensure fairness comment generate random delete tag ensure volunteer cannot distinguish comment generate hybrid DeepCom SeCNN manual evaluation volunteer relevant information unfamiliar concept internet volunteer evaluate quality generate comment modality naturalness relevance naturalness refers grammatical correctness fluency generate comment text comment easy understand relevance refers correlation generate comment input code understand intention code comment questionnaire volunteer input code reference comment generate comment generate comment naturalness relevance finally calculate average volunteer feedback indicates average relevance hybrid DeepCom average naturalness relevance SeCNN outperform hybrid DeepCom respectively indicates volunteer agreement comment generate SeCNN besides cliff delta quantify difference SeCNN hybrid DeepCom comment naturalness relevance cliff delta naturalness relevance performance SeCNN weak advantage volunteer image KB image manual analysis  DeepCom naturalness relevance discussion discussion vocabulary embed encodes relationship vector representation however due identifier define source code vocabulary challenge neural network code comment generation technique specifically processing text processing usually vocabulary vocabulary load advance define researcher extract data data exist vocabulary vocabulary processing effective handle vocabulary limit vocabulary data processing vocabulary replace token  however due user define identifier source code unique token source code cannot directly source code manipulation moreover identifier usually consist therefore alleviate vocabulary camel conversion split identifier code token ast node unique code token vocabulary ast node vocabulary become specifically decrease unique code token vocabulary decrease unique ast node vocabulary subword information vocabulary effective processing however cannot handle compound code mechanism vocabulary output previous vocabulary input verify identifier segmentation improve performance SeCNN performance SeCNN identifier segmentation SeCNN without identifier segmentation identifier segmentation achieve performance identifier segmentation improvement respectively comparison epoch     DeepCom hybrid DeepCom ast attendgru SeCNN sbt SeCNN average performance SeCNN identifier segmentation SeCNN without identifier segmentation approach without identifier segmentation identifier segmentation discussion training code comment generation technique discus training SeCNN baseline hybrid  ast attendgru code token sbt sequence input SeCNN subsection baseline besides verify effectiveness isbt training SeCNN isbt sequence replace SeCNN sbt sequence training epoch technique training epoch neural network epoch comparison SeCNN isbt training SeCNN sbt isbt reduce training SeCNN prof isbt obviously improve efficiency SeCNN hybrid DeepCom training SeCNN reduce encoder stage hybrid DeepCom layer gru network SeCNN sbt layer cnn network experimental cnn training gru DeepCom training SeCNN reduce experimental cnn training gru besides ast attendgru training predicts rnn decode generate code comment discussion influence hidden SeCNN hidden important parameter neural network significant impact performance neural network model parameter hidden lstm embed vector discus impact parameter performance SeCNN performance parameter ensure comparison parameter hidden parameter increase increase hidden increase hidden effectively improve performance propose SeCNN moreover hidden increase growth rate becomes hidden increase performance model almost converges doubt increase hidden increase model training therefore comprehensively hidden experimental discussion impact training data SeCNN reasonable training data training data neural network model achieve dependable performance inaccurate comment generation data SeCNN discus impact training performance SeCNN performance SeCNN training data ensure comparison parameter training data increase training data performance SeCNN training data easy SeCNN relationship code comment moreover training data performance SeCNN training data SeCNN efficiently discussion impact network depth furthermore conduct empirical investigate impact network depth parameter model performance recent network depth crucial importance performance SeCNN network depth network depth parameter slight impact performance SeCNN image KB image network depth performance SeCNN threat validity discus potential threat internal validity threat internal validity implementation error code alleviate issue carefully perform code inspection software code moreover release source code researcher replicate another threat internal validity bias implementation baseline alleviate threat implement baseline experimental baseline however setting suitable dataset besides threat concern factor internal affect empirical performance hyperparameter configuration hyperparameter setting propose mainly optimization validation baseline hyperparameter construct validity construct validity related metric reduce impact evaluation machine translation metric widely machine translation task construct validity related dataset dataset item item code comment however cannot trace item project due data limitation therefore propose approach remarkably project poorly others construct validity related quality comment mitigate impact code quality comment java javadoc previous code comment generation however explain code comment chosen dataset github heuristic reduce comment comment update code update code comment dataset conclusion threat conclusion threat related dataset splitting split dataset training validation percentage split strategy consistent previous code comment generation moreover alleviate sample bias random shuffle sample conduct independently  randomly conclusion validity related conduct increase generalization experimental conduct fold validation technique exist gpu resource therefore setting consistent exist threat fold validation model validation technique limited related code representation algorithm commonly image processing processing recent researcher employ algorithm software engineering code representation challenge researcher propose approach code representation attention sequence sequence cnn etc attention calculate distribution representation code employ neural network generate code comment sequence sequence model medium vector representation code related query utilized neural network predict related api sequence heap node distribute vector representation utilized synthesize candidate formal specification code generates heap employ custom cnn feature code fragment obtain distribute vector representation restore mapping classification previous approach transfer processing approach code representation dependency traditional processing approach limitation source code token cnn ast cnn capture semantic information source code effectively alleviate dependency model source code recent model source code become fundamental successfully software engineering task fault detection code summarization code clone detection program repair code generation propose gram model source code model software developed framework learns code code gram model java identifier sequence sequence neural network medium vector representation query predict related api sequence yin  establish grammar neural network model automatic code generation propose novel approach SeCNN novel component semantic information source code effective neural network model generate comment java code summarization code summarization novel task software engineering aim generate description source code recent researcher attention propose approach handle approach template code summarization AI code summarization template automatic code comment generation approach researcher define template target code information attempt generate code comment calculate keywords metric TF idf improve content extraction heuristic  modify heuristic mimic developer code  mcmillan propose software usage model swum generate code comment AI code comment generation approach neural network algorithm machine translation model neural network generate code comment developed code NN utilizes rnns distributes annotate directly code token code NN successfully recommend description corresponds source code snippet stack overflow employ reinforcement framework handle code representation  bias code summarization propose novel approach DeepCom generate descriptive comment java DeepCom structure traversal traverse ast employ neural machine translation NMT code comment generation model propose approach TL  generates summary java assistance transfer api knowledge another task api sequence summarization moreover extend propose neural network generate code comment java propose approach hybrid DeepCom combine lexical structure information source code performance technique extract dependency source code related code model seqseq generate code summarization source code dependency combine file context subroutine code subroutine enhance automatic summary subroutine approach CO propose approach model employ dual multi task improve code summarization code retrieval propose approach SeCNN AI code comment generation approach however previous approach SeCNN cnns source code cnn ast cnn alleviate dependency semantic information source code empirical verify effectiveness propose approach conclusion future propose novel neural network technique SeCNN generate code comment java exist research improve quality generate comment however accomplishment propose approach cannot future improve quality automate generate code comment SeCNN cnn alleviate dependency source code manipulation contains novel component capture semantic information source code source code cnn component lexical information ast component syntactical information lstm attention mechanism decoder generate code comment comprehensive conduct widely dataset java SeCNN performs baseline specifically SeCNN achieves performance metric baseline hybrid DeepCom ast attendgru source code ast SeCNN performance efficiency execution apparently baseline future employ program analysis richer information combine approach improve effectiveness propose code comment generation approach