rapid development neural network dnns enable intelligent application however chip training dnns challenge due extensive computation memory bandwidth requirement bottleneck memory compute memory cim approach exploit analog computation along memory array significantly vector matrix multiplication cim architecture target implement inference offline training article propose CIMAT cim architecture training  version transpose SRAM implement directional vector matrix multiplication feedforward FF  BP moreover periphery circuitry mapping strategy data BP update chip training cim improve training performance explore pipeline optimization propose architecture utilize mature advanced CMOS technology CIMAT architecture transpose SRAM array directional parallel explore training performance imagenet resnet achieve efficiency TOPS frame rate fps percent chip baseline architecture conventional SRAM array performance obtain architecture TOPS fps percent chip baseline introduction recently dnns achieve remarkable improvement intelligence application image classification recognition autonomous vehicle dnns convolutional layer fully layer achieve incremental accuracy improvement dnns tend increase depth neural network aggressively amount computational resource memory storage precision accumulate mac operation resnet achieve percent accuracy parameter although graphic processing gpus popular hardware dnn training effort academia application specific integrate circuit ASIC accelerator inference training chip however memory remains conventional CMOS ASIC accelerator tpu parameter activation global buffer computation perform digital mac array despite parallel computation optimize data realize across multiple processing PE intermediate data inefficient chip chip memory access drawback exacerbate dnn training due frequent forth data movement alleviate memory access bottleneck compute memory cim promising dnn hardware acceleration movement eliminate memory compute cim improve parallelism within memory array activate multiple analog readout conduct multiplication summation however propose cim architecture dnn inference effort accelerate training cim resistive random access memory RRAM however relatively latency asymmetry nonlinearity conductance tune RRAM prevent ideal candidate extensive update addition backpropagation BP dnn training cim array perform convolution computation transpose matrix regular memory array wise input wise output parallel analog achieve processing transpose matrix instead sequential BP significantly decrease throughput efficiency network compression promising approach reduce storage effort reduce precision parameter inference bnn xnor net due incremental accumulation stochastic gradient descent sgd optimization precision demand computational complexity training inference recently discrete training technique dorefa net wage propose training inference bitwidth parameter chip training precision float propose transpose SRAM cim architecture multi precision dnn training namely CIMAT explore correspond mapping strategy data pipeline SRAM mature CMOS technology recent silicon prototype chip demonstrate efficacy SRAM cim inference therefore explore architectural SRAM cim training cim architecture contribution novel hardware dnn training chip training transpose SRAM perform direction access addition propose memory hardware perform update transpose crossbar structure implement backpropagation computation novel explore mapping strategy data transpose crossbar structure perform backpropagation memory array cim gradient calculation cim approach matrix matrix gradient calculation propose improve performance cim chip training pipeline optimization cim training SRAM propose pipeline error calculation SRAM respectively improve efficiency processing optimize hardware SRAM pipeline entire training organize introduces dnn training cim approach novel transpose SRAM peripheral  entire dnn training propose cim architecture mapping strategy data dnn training namely inference error calculation gradient calculation update pipeline memory respectively optimize architecture accelerate pipeline obtains significant improvement efficiency throughput specification propose architecture implement resnet training chip evaluation perform  simulator technology node performance efficiency frame rate chip propose transpose SRAM cim array baseline conventional SRAM array concludes extension prior conference advanced transpose perform backward training pas simultaneously pipeline optimization CIMAT explore deeply improve training performance background dnn training convolutional neural network cnn popular dnn model training cnn namely FF BP error calculation gradient ΔWn calculation update loop obtain model iteration diagram dnn training diagram dnn training FF input data calculates error predict output label truth intermediate activation layer buffer later usage inference intermediate activation discard layer FF operation  SourceRight click MathML additional feature neuron activation function relu layer output layer denotes output previous layer activation layer bias BP goal calculate gradient layer stochastic gradient descent sgd calculate gradient layer layer layer chain error calculate convolution error transpose matrix  layer buffer error output BP gradient ΔWn layer layer obtain another convolution error activation obtain FF finally layer update ΔWn modulate rate LR  source ΔWn source  LR source critical challenge implement chip training training hardware transpose matrix another issue massive intermediate data generate entire training compute memory cim memory compute attractive extensive mac operation dnn inference training combine memory access computation cim architecture performs mixed signal computation analog summation along analog digital conversion adc array due increase parallelism reduce data movement cim significantly improve throughput efficiency limited precision ADCs variation approximate computation cim generally slight degradation inference accuracy SRAM mature candidate cim approach modify SRAM periphery enable parallel access expand bitwise xnor VMM parallel fashion input vector activate multiple dot obtain voltage amplifier SA replace adc quantize output multi inference SRAM cim architecture FF cim transpose matrix BP input vector apply perform summation digital adder along cim realize directly therefore imperative cim architecture FF BP calculation hardware transpose transpose SRAM transpose SRAM feasibility transpose SRAM validate silicon chip overhead directional access disturb access regular data storage  innovation additional transistor directional access transpose SRAM mode backward respectively mode RWL enable neuron input  partial sum backward mode exchange role RWL   RWL RWL enable neuron input  partial sum WL writer ADCs analog along   mac partial sum digitize quantize ADCs transpose SRAM operation mode SRAM transpose SRAM despite transpose SRAM perform direction access FF BP calculation readout FF BP cannot perform simultaneously batch mode SRAM cim realize  FF BP  improve processing efficiency propose SRAM structure additional PMOS  gate PMOS transistor  access SRAM mode backward additional NMOS transistor activate RWL activation input  bitline analog partial sum readout backward mode additional PMOS transistor activate BP calculation error fed RWL input  bitline partial sum readout backward mode  wordline bitline access direction perform simultaneously pipeline implement FF error calculation speedup training transpose SRAM operation mode SRAM transistor structure conventional SRAM data storage normal  compact foundry apply without modification additional transistor implement directional access memory computation stage additional transistor logic foundry optimize periphery circuit training transpose SRAM array typical periphery circuit memory array WL writer access WL decoder pre circuit addition flash ADCs multilevel amplifier reference employ quantize partial sum shift adder accumulates digitalize partial sum significant lsb input cycle significant msb input cycle multi input activation directional access periphery circuit besides transpose SRAM SRAM BL  update inspire diagram transpose SRAM sub array periphery  diagram transpose SRAM sub array periphery  propose architecture CIMAT propose compute memory architecture training perform dnn inference implement BP calculation update mapping strategy FF dataflow approach implement error calculation memory array FF cim gradient calculation propose finally introduces perform update periphery circuit FF cim array filter output feature ofm channel input feature ifm filter channel ifm height width ofm height width filter unrolled matrix memory array ifm vector apply parallel ofm vector obtain parallel mapping scheme cim convolution scheme unroll filter filter channel vector PE scheme filter subarray PEs location subarrays filter PE sum partial sum PEs adder although scheme transpose cim architecture backward operation asymmetric output FF directly entire partial sum output BP partial sum balance FF BP mapping scheme transpose cim architecture pre SRAM wordlines  regular mode memory mac operation activation fed SRAM wordlines  multiplication implement nand gate compute mode readout bitline  multiplication sum along namely partial sum mac operation partial sum sum dot kernel filter output feature partial sum kernel sum fully layer treat convolution layer filter mapping scheme apply data FF PE implement filter implement filter PEs backpropagation detail error calculation channel filter backward pas convolution operation convolution spatially flip filter FF input slide filter across channel sum generate output dot PE sum however BP input slide channel across filter sum dot sum essentially transpose version matrix BP FF  transpose BP mapped memory array BP input vector error layer apply parallel output vector error layer obtain parallel transpose architecture FF error calculation perform within PE additional memory access error calculation improvement throughput efficiency mapping BP PE implement filter gradient calculation calculation gradient matrix ΔWn propose cim approach additional non transpose cim SRAM array perform outer dot multiplication error matrix related input activation matrix mapping ΔWn calculation calculate previous cim SRAM array load chip dram chip buffer activation channel mapped matrix height width slide input unrolled activation height matrix fed array cycle cycle thereby perform bitwise multiplication accumulation partial sum multiple cycle generate gradient filter partial sum entire gradient matrix ΔWn normal batch training mode ΔWn image chip dram batch gradient load accumulate chip average ΔWn input update perform periphery circuit array sub mapping cim approach gradient calculation error cim SRAM array activation load input cycle cycle filter mapping cim approach gradient calculation error cim SRAM array activation load input cycle cycle filter update structure weigh update module SRAM disabled FF BP accumulate gradient batch fed shift register realize multiplication rate shift modify scheme ΔWn matrix sub array WLs update activate simultaneously information update module update module adder calculate sum ΔWn memory compute technique obtain cout adder significant cin significant update PE treat update significant pipeline entire update multi lsb msb update structure update module perform modify structure update module perform modify pipeline SRAM pipeline pipeline dataflow FF BP SRAM implement resnet pas BP pas error calculation realize stage pipeline latency layer entire image almost latency convolution layer previous layer therefore layer treat pipeline stage stage respectively implement pipeline layer latency stage layer stage stage fully layer activation function circuit stage intra pipeline inside FF BP training timeline architecture entire training timeline FF batch image fed training stage stage direction FF batch error  stage stage backward direction generate intermediate data FF BP chip gradient calculation obtain activation error batch mode chip typically image batch gradient calculation error obtain batch  activation calculate gradient apply image image gradient gradient calculation perform batch FF BP finally gradient average across batch update average gradient SRAM pipeline described SRAM perform directional simultaneously subarray directional vector matrix mac calculation synchronously optimize pipeline SRAM stage configuration FF BP pipeline however instead FF error calculation pipeline FF increase throughput significantly addition activation error image gradient calculation  gradient calculation cim approach accelerate duplicate cim array latency  stage approximately latency FF BP stage gradient calculation pipeline fashion FF BP avoid chip ΔWn movement stage function cycle FF BP stage perform FF calculation image error calculation image  meanwhile  stage calculate gradient image activation error image obtain cycle cycle respectively pipeline structure SRAM cim training stage function super cycle illustrate pipeline structure SRAM cim training stage function super cycle illustrate intra pipeline  implement inter pipeline training aggressively improves throughput training besides optimize pipeline beneficial due reduce chip memory access standby leakage SRAM overhead training architecture chip buffer pipeline evaluation evaluate CIMAT architecture implement resnet model chip training imagenet setup simulation chip performance modify   circuit macro model flexible cim array option device technology SRAM emerge nonvolatile memory various peripheral circuitry module validate spice simulation actual device data setup recent progress algorithm dnn training sufficient maintain accuracy data activation CIMAT baseline setting enable directional access significance tile tile shift combine obtain partial sum multi activation array lsb msb cycle output accumulate shift batch training image FF BP operation average gradient update entire batch typical matrix PE resnet matrix partition technique limit subarray practical maximum SRAM array access parallel hence subarrays PE PEs correspond filter tile layer resnet tile precision simplify periphery circuit  subarray significance tile tile shift combine obtain partial sum CIMAT architecture convolution layer contains tile shift activation function buffer global buffer tile contains multiple PEs adder buffer FF BP multi input array lsb msb cycle output accumulate shift calculation adder accumulate subarrays PE filter accumulate PEs tile accumulate adder outside PE bitwise mac entire filter significance outside tile shift perform tile obtain eventual output ofm CIMAT architecture convolution layer baseline regular SRAM array memory computation digital adder array accumulate partial sum BP non transpose SRAM obtain transpose filter multiplier filter input accumulate adder sum error gradient calculation FF activation layer fetch chip dram multiplier adder realize bitwise matrix matrix multiplication accumulation activation error gradient chip update finally ΔWn batch input accumulate adder calculate digital circuit SRAM array propose memory computation baseline verify advantage cim approach circuit parameter model CIMAT accord performance HP CMOS library CIMAT architecture modify  framework consideration chip buffer chip dram access circuit parameter SRAM hardware configuration precision circuit module per operation sub array sub array vector matrix multiplication buffer average data estimate chip dram access prior assume 3D bandwidth memory described multi activation array lsb msb cycle output accumulate shift calculate mac lsb adc register mac msb shift register adder information shift operation CIMAT parameter CIMAT parameter intermediate data training massive completely chip SRAM buffer SRAM CIMAT global buffer MB massive generate intermediate data FF activation output layer chip dram reuse gradient calculation calculate error layer batch input chip gradient calculation gradient calculation image ΔWn chip dram update without chip data transfer efficiency TOPS dram access efficiency decrease TOPS improve efficiency throughput propose SRAM CIMAT implement inter intra pipeline circuit  transpose SRAM architecture memory array due overhead additional transistor architecture FF calculation error calculation perform simultaneously output buffer tile increase  directional computation moreover global buffer enlarge inter pipeline FF error calculation gradient calculation global buffer MB MB buffer feasible tpu MB chip buffer hardware chip remains chip totally tile resnet network despite pipeline reduce dram access feedforward error calculation dram access gradient calculation cannot eliminate observation evaluation TOPS achieve TOPS however efficiency limited massive chip memory access TOPS without dram access CIMAT parameter CIMAT parameter benchmark benchmark benchmark evaluation propose CIMAT architecture baseline regular SRAM array described baseline architecture memory computation digital computation memory SRAM array serf storage comparison performance SRAM architecture baseline batch training fix imagenet image per batch efficiency TOPS frame rate fps chip evaluate FF BP update respectively BP performance error calculation gradient calculation accord efficiency CIMAT FF BP improve baseline verifies hypothesis cim architecture minimize memory access convolution computation besides transpose SRAM BP baseline due cim array elimination additional digital circuit error calculation BP hardware gradient calculation CIMAT accelerates gradient calculation duplicate cim array implement inter pipeline training memory computation baseline SRAM CIMAT training architecture achieve overall improvement efficiency chip CIMAT increase efficiency throughput respectively chip performance efficiency CIMAT gpu implementation propose memory computation baseline neural cache hardware accelerate dnn inference SRAM memory compute architecture gpu platform perform pytorch nvidia titan RTX nvidia smi gpu measurement neural cache TOPS average technology report reference evaluation CIMAT achieve average speedup efficiency training gpu approach neural cache CIMAT speedup efficiency training instead inference CIMAT training performance inference structure evaluation training achieve efficiency inference due massive chip intermediate data access training throughput almost inference training training limited inference throughput inter pipeline training performance benchmark throughput comparison efficiency comparison performance benchmark throughput comparison efficiency comparison tpu training efficiency estimate approximately TOPS custom architecture training obtains TOPS reduce  RRAM policy efficiency pipelayer TOPS intermediate data RRAM array employ SRAM technology achieves TOPS achieves TOPS benefit propose CIMAT architecture related training dnns involves BP update complicate prior inference limited technique implement dnn training discus RRAM pipelayer RRAM memory morphable subarrays memory array FF memory array future backward computation morphable array compute storage exploit intra inter layer parallelism implement pipelined training gpu achieve improvement efficiency propose  cim dnn training architecture exploit analog memory without convert data analog domain  float fix precision overcome internal data movement issue author switch enable parallel data transfer evaluation  achieve average speedup efficiency pipelayer propose duality cache develop instruction multiple thread SIMT architecture enable situ float  function improve data parallel acceleration improve performance gpu benchmark OpenACC benchmark respectively conclusion propose SRAM compute memory training architecture namely CIMAT maximize hardware reuse transpose array novel cim error calculation propose hardware overhead focus resnet  propose methodology apply implement dnn model SRAM CIMAT achieve efficiency TOPS frame rate fps percent chip baseline architecture conventional SRAM array advanced optimize pipeline SRAM CIMAT achieves TOPS aggressively throughput fps tolerable overhead CIMAT reveal cim promising implement chip dnn training reduce chip significantly limited chip buffer constraint pipeline implementation deeper neural network replacement SRAM buffer RRAM buffer worthy future exploration another limitation cim approach extreme dnn model fully chip silicon technology cim approach attractive device due improve efficiency algorithm perspective effort develop network AI application besides advanced algorithmic transfer reduce load training hardware perspective MB SRAM cache available foundry recently increase capacity cim network future node