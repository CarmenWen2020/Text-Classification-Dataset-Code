convolutional neural network cnns widely machine task deliver accuracy cnns compute memory intensive SqueezeFlow accelerator architecture exploit sparsity cnn model increase efficiency unlike prior accelerator complexity flexibility SqueezeFlow exploit concise convolution benefit reduction computation memory access acceleration exist dense architecture without intrusive PE modification specifically SqueezeFlow employ PT OS sparse dataflow remove ineffective computation maintain regularity cnn computation layout SqueezeFlow achieves speedup vgg dense architecture overhead percent respectively representative sparse cnns SqueezeFlow improves performance efficiency sparse accelerator introduction convolutional neural network cnns achieve unprecedented accuracy machine application recognition detection scene understand cnns compute memory intensive efficiently handle cnns CPUs gpus hence customize accelerator propose deliver computational throughput however cnns deeper yield accuracy amount synaptic billion remains challenge efficiently network accelerator researcher propose effective technique compress cnn model reduce computation maintain comparably accuracy cnns previous demonstrate input activation prune zero without loss accuracy percent input activation percent convolutional layer  vgg zero percent computation unnecessary due zero operand zero correspond computation contribute prune technique significantly shrink amount computation cnn inference sparsity convolutional layer vgg however improvement regularity cnn computation improve performance efficiency exist accelerator processing regular dense cnns irregularity sparse cnns inevitably introduce conditional cnn computation utilize sparsity although enable conditional trivial cpu hardly applicable accelerator grain data thread parallelism flexible data zero fed accelerator unnecessary computation hence dense accelerator hardly benefit sparsity cnns overcome prevail enable PEs flexibility independently skip unnecessary computation eyeriss exploit sparsity computation zero input activation compress dram cnvlutin cambricon compress encode activation respectively dataflow remove correspond ineffective computation memory access scnn eliminates ineffective computation zero activation simultaneously however significantly increase hardware complexity moreover incur performance degradation due unbalanced sparsity distribution PEs attainable performance merely percent nominal performance unlike prior approach complexity flexibility alternative smartly partition sparse cnns propose concise convolution ccr diminish gap sparse cnns dense cnn accelerator ccr transforms sparse convolution multiple effective ineffective sub convolution ineffective convolution activation zero contribute computation eliminate effective activation dense easily mapped exist dense cnn accelerator sparse cnns easily mapped exist dense cnn accelerator obviate intrusive PE modification ccr advocate novel approach benefit reduction computation memory access acceleration exist dense architecture furthermore SqueezeFlow efficient cnn inference accelerator exploit sparsity cnns architectural implication ccr unlike previous convolutional dataflow depends complex PE remove ineffective computation SqueezeFlow employ PT OS sparse dataflow delivers input activation nonzero PE array ensure effective multiplication fed PE array ineffective computation already remove computation PEs SqueezeFlow dataflow sparse cnns comparably PEs dense cnn accelerator SqueezeFlow employ compress encode activation reduce dram access meanwhile inter PE propagation enable reduce expensive chip memory access evaluate SqueezeFlow concrete layout CMOS technology empirically evaluate SqueezeFlow previous accelerator representative cnn model experimental SqueezeFlow achieves speedup vgg dense cnn accelerator computational resource overhead percent respectively sparse accelerator SqueezeFlow improves performance efficiency respectively furthermore SqueezeFlow considerable merit scalability PE array compatibility sparse dense cnns organize review related  detail ccr sparse cnns introduces architectural implication ccr dataflow detailed architecture SqueezeFlow accelerator describes experimental methodology implement SqueezeFlow dense sparse cnn accelerator discus future concludes related recent accelerate cnns hottest research computer architecture review related cnn accelerator motivation building accelerator sparse cnns dense cnn accelerator ASIC accelerator dense cnns diannao series hardware accelerator dedicate neural network diannao member focus memory bandwidth utilization dadiannao propose efficiently processing neural network sufficient chip memory shidiannao completely eliminate chip memory access embed besides diannao accelerator focus optimize cnns compute perspective memory perspective eyeriss ASIC cnn accelerator couple compute grid noc enable flexibility schedule cnn computation  exploit parallelism scheme  boost PE utilization dna dnn accelerator leverage input output reuse within fabric MAERI recent dnn accelerator mapping arbitrary dataflow arises dnns due topology mapping FPGAs widely cnn accelerator flexible accommodate dataflows reconfigurable substrate focus balance compute resource memory bandwidth  fuse cnn explore cnn architecture focus  optimization FPGAs propose cnn accelerator address limited bandwidth dynamic precision data quantization propose methodology optimal accelerator cnn model implementation sparse cnn accelerator sparsity proven effective approach reduce computation memory access cnns investigate compress cnn model maintain accuracy sparse cod auto encoder decoder compression prune technique propose learns important connection cnns prune unimportant connection shrink amount synaptic negligible accuracy loss meanwhile activation sparsity stem zero pad activation function rectify linear relu sparsity cnns inevitably irregular workload accelerate accelerator dedicate dense regular model cnn accelerator exploit sparsity reduce computation cnvlutin sparse activation compress format skip computation cycle zero activation improve performance efficiency cambricon exploit sparsity compress prune skip computation cycle zero scnn leverage sparsity activation exploit algorithmic dataflow eliminates ineffective computation zero activation simultaneously EIE performs inference compress fully layer accelerates sparse matrix vector multiplication explores granularity sparsity cnns coarse grain sparsity hardware friendly efficient sparse cnn accelerator  exploit repetition improve performance cnn inference enable PEs flexibility independently skip unnecessary computation thereby reap benefit sparsity however significantly increase underlie hardware complexity moreover incur performance degradation due unbalanced sparsity distribution PEs PEs usually tightly couple generate PEs idle  load imbalance severe PE utilization degrade performance confirm experimental scnn percent nominal performance tackle novel accelerator architecture SqueezeFlow exploit sparsity cnn model increase efficiency SqueezeFlow exploit concise convolution benefit reduction computation memory access acceleration exist dense architecture without intrusive PE modification hence SqueezeFlow exploit sparsity prior incur concise convolution concise convolution smartly partition sparse convolution effective ineffective sub convolution introduce formulate representation convolution clarify rationale ccr formulate representation ccr proof formulate representation convolution 2D convolution building cnns 2D convolution image local kernel specifically input imap convolve kernel generates output omap denote 2D convolution mode output valid focus mode mode obtain mode  unless otherwise specify convolution text mode computation 2D convolution define sourcewhere denote height width imaps kernel  respectively coordinate coordinate adopt coordinate formulate ccr triplet format sparse matrix representation delve 2D convolution introduce triplet format matrix representation matrix multiple sub matrix coordinate index sub matrix matrix kij sourcewhere source similarly sub matrix concatenate generate sub matrix overlap others happens overlap accumulate concatenate otherwise sub matrix triple format representation sparse matrix combination nonzero dense zero matrix ccr sparse kernel ccr sparse kernel define ccr    source denotes sub matrix decompose denote coordinate index sub matrix denote height width sub matrix proof ccr proof ccr    sourcewhere accord exists offset coordinate coordinate corresponds coordinate exactly coordinate concatenate partial  therefore SourceRight click MathML additional feature ccr transforms convolution multiple sub convolution imap sub kernel ccr computation memory access convolution sparse kernel reduce decompose sparse kernel nonzero zero sub kernel triplet representation convolution transform effective sub convolution nonzero sub kernel ineffective zero sub kernel ineffective sub convolution contribute computation memory access eliminate demonstrates kernel decompose zero sub kernel contribute compute effective sub convolution generate sub omap concatenate offset calculate source ccr sparse kernel kernel decompose convolve respectively computation volume quantify accumulation mac operation significantly reduce convolution contains MACs mac operation aggregate effective sub convolution achieve computation reduction percent meanwhile memory footprint reduce cnn inference nonzero sub kernel coordinate importantly ccr diminishes gap sparse dense convolution effective sub convolution derive ccr maintains imap reveals easily mapped exist dense accelerator hardware overhead architectural implication introduce ccr sparse input similarly ccr sparse imaps define ccr ifi   source denotes sub imaps decompose denote coordinate index sub imap denote height width kernel proof ccr ccr omit due limitation accord ccr convolution transform multiple sub convolution sub imap kernel computation convolution sparse imaps reduce ccr exemplify imap partition sub imaps zero sub imaps contribute compute effective sub convolution generate concatenate generate output achieves computation reduction percent effective sub convolution maintain kernel indicates limited hardware modification previous accelerator effective sub convolution ccr sparse input input decompose convolve respectively ccr sparse kernel input ccr ccr simplify computation convolution sparse kernel imaps ccr sparse kernel sparse imaps derive ccr   source accord ccr sub imap convolve sub kernel generates sub omap convolution zero sub imaps zero sub kernel eliminate demonstrates nonzero sub imaps convolve nonzero sub kernel generate sub  concatenate generate ccr maximally remove ineffective computation exploit sparsity imaps kernel achieves reduction mac operation percent ccr sparse kernel input input decompose kernel decompose sub imap convolve sub kernel respectively ccr decomposes 2D convolution matrix multiplication mention essential difference ccr prior matrix computation optimization 2D convolution perform matrix multiplication convert input matrix imap toeplitz matrix involves replicate data imap multiple across matrix construct toeplitz matrix convolution implement prior matrix multiplication optimization downside representation explosion building matrix convolution 2D kernel matrix matrix imap increase memory requirement significantly increase memory traffic increase execution therefore representation rarely adopt cnn accelerator usually limited chip local memory instead loop perform 2D convolution scenario 2D convolution contains matrix multiplication exists plenty data reuse opportunity prior matrix computation optimization optimize matrix multiplication ccr optimization matrix multiplication exploit data reuse ccr eliminates data replication input significantly shrink memory footprint limited chip memory budget ccr approach cnn accelerator easy achieve performance efficiency theoretical reduction computation data volume ccr calculate theoretical reduction computation data volume  vgg ccr eliminates computation layer percent average exploit sparsity input kernel ccr ccr remove percent computation respectively meanwhile ccr ccr ccr achieve percent data volume reduction respectively architectural implication introduces architectural implication ccr ccr bridge gap sparse cnns dense cnn accelerator typical dense architecture investigate evolves ccr without intrusive PE modification generic dense architecture generic architecture dense accelerator model comprises buffer KB input activation buffer IB output activation buffer OB processing PU accumulation  scatter crossbar PU 2D mesh processing PEs convolution scalar kernel matrix input fetch respective buffer fed PU computes scalar matrix multiplication matrix scalar deliver  accumulate correspond partial sum fetch OB rout OB crossbar regular address OB output coordinate derive loop index machine architecture described sake investigate evolves ccr unique feature prior optimize data movement enable detail refer sparse architecture ccr architecture input kernel uncompressed format proceed lock scalar kernel zero fed PU unnecessary computation dense architecture cannot benefit reduction computation memory access sparse architecture ccr demo sparse architecture ccr denote SparseArch analysis ccr kernel sparse nonzero coordinate kernel compress format nonzero coordinate KB nonzero fed PU correspond coordinate fetch coordinate computation CCU compute coordinate output accord rectangle output address OB regular coordinate calculate coordinate deliver  indexed coordinate accumulate correspond partial sum fetch OB finally accumulate rout OB crossbar sub convolution derive ccr remain input convolution mapping sub convolution onto architecture exhibit difference mapping dense architecture dense architecture coordinate derive machine dedicate CCU CCU calculate coordinate implement overhead accordingly SparseArch efficiently sparse convolution obviate intrusive PE modification dense architecture architecture ccr SparseArch derive rationale ccr SparseArch architecture complicate overhead SparseArch SparseArch sub convolution derive ccr imap kernel complexity significantly increase mapping hardware feasible imaps kernel partition sub imaps sub kernel convolution imap kernel scalar multiplication scalar imap scalar kernel convolution transform cartesian vector CCU calculate coordinate coordinate randomly distribute hence complexity CCU notably randomness output coordinate contention  crossbar multiple partial sum hash buffer summarizes characteristic architecture clearly SparseArch eliminates ineffective computation however hardware complicate moreover SparseArch compatibility dense model cartesian cannot dense convolution although unable remove ineffective computation SparseArch remarkably efficient economic achieves comparably computation reduction rate overhead SparseArch therefore opt SparseArch architecture efficiently cope dense sparse cnn model sparse architecture specification SqueezeFlow accelerator architecture dataflow accelerator ccr SqueezeFlow PT OS sparse dataflow employ ccr inner core SqueezeFlow accelerator employ dataflow PT OS sparse dataflow cnn dataflow nest loop structure defines loop partition parallelize dense architecture sparse architecture employ specialized dataflows optimize performance efficiency specific dataflow PT OS sparse sparse dataflow enables input data reuse minimize psum accumulation implementation PT OS sparse dataflow inspire ccr decomposes sparse convolution dense sub convolution dense sub convolution mapped PEs dense dataflows extensively prior loop nest PT OS sparse dataflow output input channel dimension already factor output input channel loop tile increase data reuse improves efficiency pseudo code inner loop dataflow brevity omit outer loop spatially temporally across PEs extensively prior focus inner loop demonstrate dataflow leverage ccr exploit sparsity PT OS sparse dataflow employ spatial tile strategy across array PEs PE operates independently activation partition planar tile PT distribute across PEs employ output stationary OS computation psums stationary computation accumulation minimize psum accumulation PT OS sparse dataflow sparse convolution decompose matrix scalar convolution kernel therefore perspective ccr kernel partition multiple sub kernel PT OS sparse dataflow contains loop dimension dimension fetch scalar correspond index compute coordinate activation fetch matrix activation accord coordinate compute matrix scalar parallel   function compute coordinate temporal loop index spatial loop index RLC index idx kernel width height easy decode grouped compress encode compress denote compress dimension feature compress dimension vector contrast input activation uncompressed format access buffer delivers scalar nonzero filter along coordinate similarly input buffer delivers matrix input activation coordinate matrix compute   function multiplier array computes matrix scalar partial sum correspond output loop index unlike prior cartesian dataflows scnn multiplier output PT OS sparse contiguous correspond matrix output feature significantly reduces complexity scatter output psums output buffer output stationary computation output matrix natively accumulate PE array meanwhile loop index coordinate computation ensure output matrix concatenate correctly generate output computation output channel output buffer compress dram overview SqueezeFlow architecture demonstrates SqueezeFlow accelerator architecture PT OS sparse dataflow SqueezeFlow dataflow architecture squeeze dense cnns sparse cnns SqueezeFlow consists accelerator chip chip memory usually dram accelerator chip primarily compose processing PU global buffer GLB decoder encoder CU buffer controller BC coordinate computation CCU processing PPU PU contains PE array via network chip noc compute parallelism perform massive convolution operation PPU responsible apply accumulation  integrate PPU non linear activation relu pool dropout function accelerator storage dram GLB inter PE connection register file PE GLB exploit input data reuse hide dram access latency storage intermediate data encoder decoder reduce dram access data transfer chip GLB chip dram compress format chip memory access accelerator largely reduce SqueezeFlow accelerator architecture accomplish  computation activation dram GLB compress format compress fed KB directly activation however decompress decoder fed IB BC selects activation IB KB respectively load instruction CU transfer activation PU perform sparse convolution PT OS sparse dataflow non linear activation pool dropout function perform PPU execute  compose multiple output feature multiple input feature accelerator continuously performs computation output feature output feature construct describes detail SqueezeFlow processing PU structure PU operates activation define PT OS sparse dataflow psums PU simultaneously input activation fetch BC distribute PEs addition PU contains local storage structure PE enable local propagation input activation PEs reduce data transfer PEs GLB perform multiplication activation PU delivers partial PPU PPU accumulate generate processing PE micro architecture PE executes fix accumulation operation pei PE PU input input signal input reading kernel  KB input reading activation IB OB PEs signal pei output output computation OB IB output propagate locally activation PEs efficiently reuse data execute  PE continuously accommodates output activation switch another output activation compute distribute nonzero kernel PEs PEs compute activation output feature simultaneously inter PE propagation ccr implies matrix scalar data rectangular input rectangular output dense convolution significantly overlap adjacent activation slide characteristic convolution although data available IB OB repeatedly reading buffer PEs bandwidth therefore inter PE propagation efficient data reuse sparse convolution inter PE propagation enable data reuse however overlap randomize due random distribution zero enable PEs locally input activation dense architecture efficient enable PEs locally input activation PE implement network chip PEs architecture data transfer PEs although reuse randomize deterministic overlap adjacent input activation hence CU generates signal accord overlap noc propagates data PEs furthermore unlike shidiannao leverage fifo structure register PE inter PE propagation reduce complexity PEs coordinate computation dense accelerator address output usually generate finite machine FSM regular data access dense convolution however FSM hardly address generation sparse accelerator hence dedicate CCU address generation coordinate computation tightly related compression technique introduce compress activation compression RLC selection specific format orthogonal sparse architecture data compression technique construct SqueezeFlow decode yield nonzero correspond coordinate RLC nonzero zero adjacent nonzero RLC encode nonzero zero adjacent nonzero respectively compress RLC format KB encode sparse kernel encode sparse kernel CCU calculates coordinate nonzero recursive fashion coordinate nonzero compute distance prior nonzero zero dimension KB suppose coordinate denote zero coordinate derive  mod source CCU computes coordinate input activation matrix output stationary dataflow coordinate output activation matrix loop index nonzero coordinate input activation easily calculate accord convolution characteristic coordinate input activation contiguous dimension accord ccr CCU computes coordinate input activation coordinate output activation loop index coordinate input activation generate coordinate dimension sparse cnn mapping subsection detail SqueezeFlow remove ineffective computation sparse  snapshot sparse convolution processing without lose generality PEs pei PE ith jth  kernel nonzero stride sake brevity cycle analyze accelerator cycle cycle demonstrate algorithm hardware mapping convolution operation input activation reuse PEs algorithm hardware mapping convolution operation psum initialize data fetch OB otherwise register output stationary dataflow inter PE propagation cycle PE partial output feature PEs KB nonzero convolution kernel cycle coordinate prior nonzero initialize accord coordinate calculate   mod  coordinate calculate coordinate input activation coordinate output activation xin xout  yin   sourcewhere xout  coordinate output activation matrix coordinate input activation coordinate input activation contiguous dimension coordinate input activation easily calculate fetch PEs PE performs multiplication input activation local register psum register accumulate data psum register psum derive previous activation accumulate locally psum register future accumulation addition PE input activation input reg future inter PE propagation cycle PEs nonzero cycle CCU computes coordinate   mod sourcethen coordinate input activation xin yin PEs respectively input activation already input reg PE PE respectively PE PE enable inter PE propagation reading correspond input activation input regs PEs PE PE input activation IB multiplication perform PE input activation locally correspond register PE convolutional convolutional cycle cycle cycle coordinate nonzero calculate PEs respectively input activation accord index already input reg PE PE enables inter PE propagation reading correspond input activation input regs PEs meanwhile PEs input activation IB PEs perform multiplication input activation locally correspond register cycle cycle PEs fourth nonzero perform operation PE partial output feature convolutional cycle hence cycle partial psum register OB cycle processing convolutional cycle PEs nonzero KB coordinate calculation cycle coordinate input activation xin yin coordinate input activation easily calculate operation omit sake brevity described cycle mapping sparse  SqueezeFlow clarifies fifo structure inter PE propagation FIFOs significantly increase overhead PEs nonzero randomly distribute data access input activation regular dense cnns FIFOs access sparse convolution experimental methodology implementation implement SqueezeFlow synopsys TSMC technology simulate synopsys verilog compile simulator vcs synthesize synopsys compiler DC analyze synopsys primetime PT synopsys IC compiler ICC CACTI estimate dram access resize implement dense accelerator DenseArch rationale described baseline DenseArch representative sparse architecture cambricon cnvlutin scnn developed custom cycle simulator evaluate performance baseline cambricon cnvlutin scnn implement rtl configuration verilog simulator model dataflow memory hierarchy PE configuration simulator evaluates performance compute cycle layer accelerator meanwhile simulator mac operation memory access statistical data model estimate consumption baseline accelerator significant difference dataflow buffer organization implementation choice evaluate architecture cannot precisely prior proposal architectural configuration SqueezeFlow equip PU array PEs IB OB SRAM KB KB SRAM KB KB nonzero additional KB index sparse kernel fix arithmetic effective cnn accelerator comparison resize baseline architecture equip multiplier SqueezeFlow additionally frequency SqueezeFlow baseline ghz nominal dram bandwidth configuration GB dual channel ddr benchmark benchmark performance representative cnns vgg alexnet googlenet googlenet primarily focus  inception module model sparsity suitable adaptability flexibility accelerator experimental SqueezeFlow baseline performance scalability layout characteristic parameter setting layout characteristic baseline DenseArch SqueezeFlow compute resource buffer SqueezeFlow increase percent DenseArch versus across architecture buffer IB KB OB dominate architecture percent DenseArch SqueezeFlow respectively PU remain almost architecture ccr enables sparse convolution almost without intrusive PE modification overhead SqueezeFlow stem KB increase index sparse kernel CP increase mainly CCU dec encoder decoder overall SqueezeFlow incurs slight overhead efficiently sparse cnns layout DenseArch SqueezeFlow parameter setting DenseArch SqueezeFlow comparison DenseArch SqueezeFlow SqueezeFlow percent DenseArch average benchmark consists aspect additional hardware logic exploit sparsity increase CP increase integrate coordinate computation dec chip buffer increase buffer index relatively randomize access buffer percent increment IB OB KB respectively performance summarizes speedup deliver sparse architecture baseline DenseArch SqueezeFlow consistently outperforms baseline scnn vgg achieves average speedup baseline respectively performance improvement SqueezeFlow varies widely across model specifically SqueezeFlow improves performance DenseArch cnvlutin cambricon scnn although scnn theoretically achieve performance model remove ineffective computation stem zero kernel imaps delivers slight performance advantage SqueezeFlow vgg perform alexnet googlenet scnn incurs severe performance degradation unbalanced distribution computation PEs reveal although SqueezeFlow unable remove ineffective computation remarkably efficient across various model speedup baseline dense architecture across model speedup baseline dense architecture across model performance understood performance breakdown SqueezeFlow scnn across  vgg explanation layer SqueezeFlow achieves superior performance scnn sparsity kernel dominates layer advantage SqueezeFlow input sparse percent sparsity sparsity increase layer deeper scnn achieves overwhelm performance SqueezeFlow imaps kernel sparse later layer however computation volume mainly dominate layer input output SqueezeFlow achieves comparable overall performance scnn due superior performance layer speedup baseline dense architecture across model benchmark SqueezeFlow significantly outperforms scnn scnn hardly nominal performance although scnn considers activation sparsity incurs significant performance degradation mainly load unbalance scnn exploit cartesian perform convolution however SqueezeFlow matrix scalar load balance natively address PT OS sparse dataflow stride alexnet layer compute correspond convolution input feature filter stride extract valid data stride generate operation pool layer implementation increase computation volume introduces unnecessary computation easily implement exist dataflow furthermore layer benchmark impact overall performance impact dram bandwidth dram bandwidth affect latency chip memory access performance SqueezeFlow bandwidth configuration simulation performance SqueezeFlow degrade dram bandwidth GB nominal dram bandwidth configuration GB ample bandwidth absorb chip traffic summary SqueezeFlow achieves almost speedup average across model tremendous performance advantage baseline report comparison architecture normalize DenseArch consumption memory access usually dominates consumption average SqueezeFlow improves efficiency baseline respectively strike SqueezeFlow achieves improvement DenseArch vgg improvement efficiency varies widely across model sparsity model specifically improvement scnn alexnet vgg efficiency SqueezeFlow stem improvement performance SqueezeFlow achieves computation reduction rate performance degradation irregular computation distribution hardware overhead architecture SqueezeFlow simpler scnn implement consumption normalize dense architecture breakdown accelerator DenseArch PU consumes DenseArch SqueezeFlow PU consumes percent DenseArch consistent report prior performance SqueezeFlow consumption component SqueezeFlow DenseArch breakdown DenseArch SqueezeFlow scalability compatibility evaluate scalability architecture sensitivity PE array hardware merit performance architecture vgg PE array SqueezeFlow maintains stable performance imap usually PE array SqueezeFlow maintains PE utilization cnvlutin cambricon suffer performance degradation increase PEs scnn incurs severe performance degradation PE array SqueezeFlow surpasses scnn performance vgg highlight SqueezeFlow scalability speedup vgg PE array speedup vgg PE array evaluate compatibility architecture performance dense sparse vgg SqueezeFlow achieves comparable performance DenseArch although dedicate dense model however scnn incurs percent performance degradation DenseArch reveals SqueezeFlow compatibility scnn speedup dense sparse vgg DenseArch  described SqueezeFlow accelerator exploit sparsity increase efficiency however limitation prevent SqueezeFlow optimal efficiency limitation SqueezeFlow remove ineffective computation zero observation sparsity usually activation sparsity accelerator adaptively remove ineffective computation zero activation limitation arises lack efficient various convolution stride SqueezeFlow handle introduce unnecessary computation although widely prior efficiency dedicate various convolution stride future conclusion describes concise convolution smartly decomposes sparse convolution effective ineffective sub convolution computation ineffective sub convolution eliminate effective easily mapped exist dense cnn accelerator without intrusive PE modification ccr propose SqueezeFlow accelerator architecture exploit sparsity cnns SqueezeFlow tremendous performance efficiency advantage prior approach footprint SqueezeFlow achieves speedup vgg comparably provision dense architecture sparse accelerator SqueezeFlow improves performance efficiency respectively