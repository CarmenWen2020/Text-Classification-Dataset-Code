currently amount data device burden network bandwidth service delay requirement delay sensitive application processing data network choice however device smart wearable autonomous vehicle usually limitation computational capacity influence quality service effective efficient strategy offload widely address issue device heterogeneity task complexity increase service quality degradation resource utility decrease due unreasonable task distribution conventional simplex offload strategy limited performance complex environment motivate dynamic regional resource schedule framework effectively index consideration article propose offload framework simulate offload scenario consists server device formulate offload markov decision MDP utilize reinforcement DRL algorithm asynchronous advantage actor critic AC offload decision strategy balance workload server finally reduce overhead comparison local compute DRL algorithm DQN conduct comprehensive benchmark performs adjust overhead reduction introduction explosive growth mobile device due series obvious advantage access wealth data suitable processing locally transmit latency return greatly improve user device processing massive data network significantly relieve burden backbone bandwidth user privacy data network device instead upload feature attractive feasible deploy diverse application scenario extreme delay autonomous remote surgery ignore terminal suffer constrain computational resource limited battery significant challenge improve quality service derive compute era cooperation computation offload envision useful approach overcome challenge compute traditionally task occupy computational resource offload remote centralize amazon EC microsoft azure processing transmit device approach actually enables weak computational capacity resource hungry task however approach severe delay bandwidth stress backbone network moreover transmission increase distance address mobile device offload task cloudlets server distance offload avoid transmission delay offload strategy optimal decision resource allocation situation becomes complicate user uncertainty random production task diverse besides dynamic available resource server wireless channel offload resource management becomes intractable mobile device numerous accord report cisco visual networking index billion mobile device hence novel strategy compute offload urgently reinforcement performance tackle decision dynamic scenario driven awareness capability characteristic trial error delayed reward alpha zero player aspect chinese chess utilize reinforcement dynamic offload definitely combination reinforcement computation offload propose comprehensive strategy reinforcement developed joint consideration resource utility mobility computational complexity exploration inspiration contribute compute task offload scenario algorithm improvement specifically capable action discrete input dimension constrain task diversity obviously contradictory situation despite network DQN dimensional sequential data constrain discrete output convergence important hierarchical massive amount mobile device model practical motivate devise novel framework optimize offload strategy via appropriate algorithm propose offload framework dynamic task randomness device amount specifically ESs offload task offload policy illustrate user equipment UE pico BS chooses offload computation intensive task server ES normal task locally accord criterion UE incur task explosion ES offload task idle ES effectively avoids task resource waste formulate decision procedure MDP action offload decision task queue respectively immediate reward transition probability optimal offload decision generate task distribution obtain convergence accurate decision usually UE massive offload strategy reinforcement algorithm AC AC outperformance framework compute scenario framework compute scenario related resource management important role effective distribution deployment storage network bandwidth compute resource significant improvement latency reduction offload aim improve qos properly distribute computational load scenario complex dynamic offload algorithm computational architecture scalability tough model handle derive scenario model offload strategy capacity introduce offload model discus user deterministic task model stochastic task model subsequently introduce management resource MEC server resource multi user finally management MEC heterogeneous server extension server distribution resource offload user offload happens sole user sole server server deterministic task stochastic task task model focus consumption execute latency ues aim goal optimize transmit rate variable substitution technique apply optimization nonconvex recast convex  extensively multi server optimization complicate scenario task arbitrary dependent sequential task relation graph differentiate parallel parameter sequential parameter independent task sequence execute currently trick comprehensive offload scheme jointly minimize consumption latency stochastic task model complex purpose balance consumption application execution threshold adaptive offload algorithm lyapunov function complexity characterize algorithm consumption execution comparison network scenario WI FI 3G dynamic jointly local cpu resource resource distribute task appropriate cpu network interface algorithm propose mention lyapunov optimization trace driven simulation realize 3G lte wifi architecture apply android platform conduct elucidate advantage architecture delay decrease multi user cooperative management involves resource computational resource centralize distribute offload scheme centralize MEC server management handle computation request distributes resource mobile device resource mobile access technology jointly TDMA adequate computational resource convex optimization formulate offload priority function useful finite capacity derive strategy infinite capacity upper bound algorithm balance consumption delay multi user MEC requirement task buffer stability resource distribution strategy network influence various parameter expound distribute theory decomposition technique useful concentrate multiuser multichannel offload centralize optimization NP theoretic utilized distribute optimization model derive distribute computation offload decision exist nash equilibrium correspond algorithm propose obtain max converge research extend multichannel wireless contention environment computation offload constrains task workload maximum execution delay transmit distribute efficient resource schedule algorithm dynamic voltage frequency technique apply adjust computation local device finally reduce heterogeneous server server selection server cooperation beneficial local internet resource jointly maximize probability task successfully execute within constrain threshold exceed task offload internet propose schedule policy utilized priority queue dimension markov chain delay requirement application priority delay sensitive application local resource geographical geo distribute mobile compute scenario device access resource geographically increase resource utilization resource lease resource service provider resource deficient service provider remote cooperation local cooperation described resource cooperation formulate coalition theory framework strategy simulate cooperation noteworthy device heterogeneity scenario difficulty effective offload classical tends limited performance dynamic offload multiple constrains utilization reinforcement greatly dilemma built offload model server task device although optimal offload decision function device capacity model robust  powerful network apply widely compute offload duc van chen   algorithm DQN optimal offload decision jointly task loss probability user mobility adopt network algorithm  attention task distribution resource model optimization offload allocation reduce computational complexity action selection propose improve efficiency despite feasibility reinforcement offload optimize model unable reveal scenario adaptation capacity interested comparison devise layer architecture guarantee offload UE server server mechanism properly allocate regional resource improves efficiency dynamic task offload avoids resource idleness utilize reinforcement algorithm appropriate role processing uncomplicated algorithm optimize sequential decision DQN strategy strategy computational resource convergence implementation DQN strategy dependent powerful gpu strategy realize device multi core cpu friendly device limited computational resource model introduce model layer model consists various ues ESs structure UE task probability slot probability simulate computation density ues offload strategy user offload computation ES communication pico BS UE maintain task queue task locally ES maintain task queue service FCFS principle influence efficiency task processing surplus computational resource ESs available ES offload task ESs macro BS execute task queue user precisely simulate transmission computation devise appropriate communication model computation model introduce detail convenience reading summarize notation notation communication model introduce communication model traditional wireless cellular network typically deployed homogeneous network macro centric planning flexibility terminal qos requirement configuration motivates heterogeneous network management uplink downlink ues ESs deploy macro pico placement pico coverage issue traffic density roughly transmit brings flexibility pico communication ues ES inside macro powerful location selection strict consideration macro communication server accord compute uplink rate UE offloads computation server formula compute uplink rate server wlog NHS npi  SourceRight click MathML additional feature channel bandwidth transmission UE offload task server denotes channel gain UE offload task server loss shadow attenuation background computational model denotes computation task UE server compute locally offload server computational input data byte program entire compute cycle accomplish task assume execute locally offload denotes upper bound tolerable delay task important guarantee qos heterogeneous network reference criterion decision offload acquire information calculate assume task partition offload partly UE integral task offload server define computational offload decision vector denotes UE offloads task server denotes local compute local compute local compute UE server task mobile device continuity task generation buffer  tun tun task queue UE task queue processing due FCFS principle  denote task pre exist task processing experimentally update  slot remove task task priority uncomplicated function program detailed formulate description unfriendly crucial explain logical structure concisely pre processing  define computational capacity compute cycle per UE ues computational capacity computation task   sourceand processing task  sourcewhere denotes compute cycle per consumption  accord practical combine local compute  λts  λes   λes λts λes sourcewhere λts λes denotes improve flexibility model constrain λts λes requirement UE practically appropriate apply multiple criterion decision multi attribute utility theory offload compute computation offload UE upload task pico wireless access task server accord communication model propose transmit task compute respectively   sourceand  NBS  sourcewhere denotes input data uplink rate wireless channel transmission mention transmit server task computational capability server compute cycle per execution task offload UE denote tps   server receives task ues pico server task queue tsi tsn tsi allocate task FCFS principle location label task queue task task consists remain task queue slot offload task earlier mention  task described tsi server task queue pre exist task processing  calculate    SourceRight click MathML additional feature  processing task task server task queue combine offload compute tos  tps  SourceAccording calculate offload compute task  λts  λes  sourcewhere coefficient λts λes λts λes offload server practically UE connection quality service offload  task server severe task queue task actually offset bonus obtain offload unavoidable UE ES however communication UE ES server constrain queue resource surplus server offload target accord practical average tolerant service delay task beyond threshold offload server offload task ES task queue comparison ES minimum workload target therefore respectively task transmission execution   sourceand   server task server maintain buffer queue  task sort queue task server task queue local task priority local task execute local task task offload server twait   sourcewhere  pre exist task processing combine task offload server  twait sourcethen entire offload twice  λts  λes  SourceRight click MathML additional feature coefficient λts λes λts λes normally data downlink overlook influence downlink transmission application objection detection input data application image graph analyze return description probability besides development communication technology layer server resource downlink rate nearly uplink rate ues therefore ignore downlink delay bottleneck optimization data transmission model introduce devise reinforcement strategy multi user offload decision policy mobile compute computation offload via parallel actor critic reinforcement offload formulation motivation reinforcement offload decision improve flexibility circumstance although ues server procedure task processing fix task emerges randomly ues offload server via policy server task return ues server slot affected slot remain task queue essentially markov procedure formulate MDP generally denote MDP tuple ST ST denotes action stt stt indicates transition probability action  stt immediate reward action target model reward maximize policy stt instructs optimal action action correctly ST ues server obtain execution information task queue besides queue burden threshold server offload target ST ST DT sourcewhere UE queue server queue parameter signal indicates burden server threshold DT denotes offload target server mention UE task queue define task execute locally    combine ues server tsn tsi denotes task currently queue server define action sourcewhere action composite offload decision ues denotes offload decision ues coverage server offload task server execute locally reward function target model minimize processing delay various situation optimal offload decision specifically constrain task assure offload ues offload choice maintain appropriate device utility server overwhelmed ues offloads task idle UE waste computation resource consideration devise reward function action server rts rcs SourceRight click MathML additional feature rts reward function rcs reward function rts calculate rts tos  source define rts reward task successfully without exceed limit task execute locally action reward requirement task encourage execute locally server suffer burden coefficient responsibility influential reward task improvement reward positive task action punishment negative reward reward rcs calculate rcs cli   sourcewhere action obtain execution locally reward positive encourages action however becomes punishment response via negative reward AC offload algorithm scenario action tend nearly impossible obtain accurate action via normal reinforcement combine reinforcement DRL algorithm reinforcement lean network DDPG becomes useful handle action data data chunk randomly update mechanism replay algorithm however replay exists drawback due characteristic replay occupy memory computation resource agent interacts environment besides replay limit algorithm utilize data generate policy update normally terminal relatively sufficient computational resource resource consumption important aspect implementation computational consumption appropriate motivates seek asynchronous advantage actor critic AC AC algorithm actor critic network combine function policy function AC scheme user agent offload action accord policy critic network generates estimation action actor network utilizes optimize policy aim maximize future discount reward specifically critic network temporal difference error estimate action calculate  sti SourceRight click MathML additional feature immediate reward action actually critic network realizes update via bootstrapping effectively improves efficiency actor network actor network update parameter ascend gradient policy obtain critic network gradient accumulation parameter calculate sti  sti SourceRight click MathML additional feature formulation baseline reduce variance baseline function related function policy function influence policy gradient function sti baseline function advantage function described sti sti addition actor critic network asynchronous interaction parallel agent environment improves performance AC without replay AC executes agent synchronously enable obtain parallel parallelism mechanism  agent data stationary sample eligible training interact environment agent update global network parameter agent utilize global improvement individual interaction furthermore exists another advantage algorithm training traditional reinforcement algorithm dependence powerful gpu normal device unable powerful graph processing capacity AC algorithm realize machine standard multi core cpu thread agent update parameter parallel implementation algorithm relatively friendly device specific description AC offload decision algorithm algorithm performance evaluation utilize tensorflow evaluate performance algorithm performance comparison exist algorithm reward service delay algorithm AC offload decision algorithm assume global parameter vector global counter assume thread specific parameter vector initialize thread counter reset gradient  synchronize thread specific parameter tstart stt perform accord policy stt reward stt terminal tstart tmax stt terminal stt tstart accumulate gradient wrt  sti sti accumulate gradient wrt   sti perform asynchronous update  tmax setting metric offload consists server ues pico coverage communication service ES ues setting consideration computational limited scenario density user setting server UE reasonable service pico server UE  experimental workload increase exponentially without obtain extra research consist server UE functionally capable interaction smart device compute scenario experimental convenience ES ignore transmission delay ES transmission model refer parameter channel bandwidth mhz transmission background dbm channel gain  distance UE loss factor computation task detection typical application relies device image locally offload server task computation intensive tolerance requirement compute load input data ratio  KB complexity task compute cycle per task randomly assign ghz simulate task variety compute capacity local UE assign  consideration equipment difference compute capacity assign server  coefficient λes λts λes λts influence model simulation parameter summarize simulation parameter simulation parameter accurately analyze effectiveness efficiency algorithm complex scenario DQN non offload reward improvement convergence obvious crucial delay sensitive application addition influence task density consumption consumption necessity offload mechanism ESs comparison reward strategy subsection evaluate comprehensive performance strategy local compute LC DQN offload DQN AC offload AC AC offload AC chose reward metric strategy performance progress consumption directly increase reward reward performance task density UE average task task tensorflow episode local compute episode linear reward local compute convergence curve strategy plot obtain useful information comparison local compute disappoint accord task local compute capacity task within tolerant threshold analyze task sequence appearance task random task task subsequent task influence violation reward becomes really local compute resource limited UE satisfy quality service complex situation obvious AC strategy reward strategy decision task sequence besides faster convergence DQN specifically AC reward episode DQN episode observation prof strategy nearly optimal decision shorter crucial delay sensitive application reward difference AC AC analysis later subsection analyze influence task density comparison reward strategy comparison reward strategy influence task density subsection analyze influence task density strategy task density respectively task density variation involve workload workload performance comparison task density helpful reveal stability experimental framework important plot convergence curve task density AC strategy reveal brilliant convergence DQN ACs nearly DQN difference AC AC denote complex hierarchy increase processing burden task density however advantage complex hierarchy task density becomes plot average episode task density DQN AC AC average random disturbance algorithm density AC AC DQN obvious difference emerge density becomes latter situation AC exceeds DQN performance AC stable augment due increase task observation demonstrates offload powerful server stable service narrow task density task become offload structure inefficient AC performs AC density situation actually ignore due task randomness stability task density increase DQN performs AC offload structure capable effectively offload diverse situation analysis subsection comparison density comparison task density comparison task density influence task density subsection analyze influence task density consists plot comparison task density density AC AC nearly DQN denote AC strategy task offload ES DQN compute locally consume upload DQN reward density DQN offload task growth obvious correspondingly ACs density ACs AC sharply offload structure obviously responsible local burden local without offload task ES explosion density AC DQN AC perform processing capability growth task local compute compute consists ESs ues random task distribution load device offload structure bridge transform computational resource idle effectively improve resource utility reduce comparison task density comparison task density adjust ability random exploration subsection analyze adjust ability random exploration scheme framework severe fluctuation episode reward decline recover episode reward explore plot curve curve respectively suddenly decline subsequently task offload ES combine curve random exploration scheme framework action offload decision prior decision action policy probability however action useless random exploration enables algorithm action subsequent coin action decision spine curve framework decision direction episode moreover performance AC closer AC reward trigger action choice truly demonstrates adjust ability framework meaning random exploration reward flaw AC flaw AC flaw AC flaw AC conclusion future mainly concentrate schedule limited regional resource scenario effectively utilize idle resource propose offload framework link server alleviate partial load  algorithm reinforcement efficiently appropriate offload task user equipment simulation demonstrate advantage resource offload decision reward curve task density AC algorithm decision brilliant convergence DQN comparison AC DQN highlight improvement delay besides AC AC task density offload framework effectively schedule task distribution reduce partial overhead delay moreover stability adjust ability task density increase decision however exists challenge overcome future explain shed research direction framework DRL decision action UE discrete UE increase action exponentially neuron network node output layer action impossible neuron network action discretely research direction focus approximate discrete action continuous action overcome challenge relation mapping discrete action continuous action important