compute paradigm compute capability pervasive access network user critical research challenge compute efficient offload strategy task offload server limited resource although research effort attempt address challenge centralize practical user rational individual maximize benefit article decentralize algorithm computation offload user independently offload decision theory apply algorithm exist address challenge user refuse expose information network bandwidth preference therefore offload decision without knowledge formulate partially observable markov decision POMDP policy gradient reinforcement DRL approach extensive simulation proposal significantly outperforms exist introduction mobile phone gain enormous popularity mobile application recognition processing augment reality emerge attract attention mobile application typically resource hungry demand intensive computation consumption hardly mobile phone limited computation resource battery overcome limitation novel compute paradigm compute propose promising modest compute server deployed pervasive access network user user offload compute task server latency although computation offload approach significantly augment computation capability user develop comprehensive reliable compute remains challenge server limited hardware resource user offload task simultaneously exceed capacity server task response therefore critical efficient offload strategy task offload server recognize critical challenge compute exist centralize achieve global optimal performance unfortunately practical user accord centralize individual rational choice computation offload theory powerful framework analyze interaction multiple player decentralize mechanism player incentive deviate unilaterally thanks promise theory apply offload algorithm compute recent research effort decentralize computation offload mobile compute  propose selfish decentralize computation offload dense wireless network user offload computation multiple wireless however exist hardly apply weakness discrete action model allows user limited action although model scenario user cannot handle straightforward approach action formulation algorithm complexity exist assumption user information quality network connection preference efficiency offload decision however user unwilling expose personal information due privacy security concern conquer weakness algorithm theory enhance reinforcement DRL specifically user server via multiple access wifi router user arbitrarily task subtasks offload portion server challenge arises partial offload model flexible user action continuous discrete model exist considers offload decision offload task scenario user information network bandwidth preference algorithm achieve nash equilibrium insight algorithm extend scenario without information formulate multi agent partially observable markov decision POMDP address challenge network dynamic continuous decision propose decentralize approach reinforcement DRL policy gradient differential neural computer DNC approach effectively optimal offload policy network dynamic continuous decision directly computation offload without prior knowledge model merit model computation offload strategy model computation offload apply complex unpredictable situation obtain precise model moreover DNC policy gradient DRL capable information infer hidden observation automatically incorporate DNC framework policy optimization accelerate significantly user policy network uncertain contribution summarize task offload compute formulate decentralize computation offload slot account communication computation propose algorithm achieve nash equilibrium offload without information formulate multi agent POMDP algorithm DRL DNC propose challenge simulation demonstrate effectiveness propose scheme remainder organize discus related description algorithm scenario information detailed multi agent reinforcement approach scenario without information finally evaluates performance simulation concludes related compute paradigm attract considerable attention academia nokia introduce compute platform compute platform application server fully integrate    introduce fully virtualized compute platform environment compute application compute apply various scenario exist computation offload perspective user computation offload accord experimental optimization scheme efficient application execution propose assist mobile application platform propose adaptive timeout scheme computation offload improve investigate computation offload multi user propose hybrid minimize service delay compute virtual machine migration transmission iterative algorithm propose perform joint optimization computational resource multi compute budget constraint latency centralize offload framework multi user compute TDMA ofdma aim minimize user consumption comprehensive survey computation offload compute however centralize ignore interaction multiple user independently computation offload strategy recent model user interested player propose decentralize scheme multi user computation offload however mainly focus computation offload relatively static environment network environment due wireless network utility user dynamically nash equilibrium static model propose multi user computation offload variant wireless network user compete computational resource approach propose achieve nash equilibrium dynamic computation offload however user decision discrete model propose approach complexity challenge achieve nash equilibrium stochastic decentralize dynamic environment multi agent nash propose discrete stochastic propose DDPG approach multi agent markov decision environment fully observable propose incentive mechanism federate achieves nash equilibrium reinforcement contrast previous research formally address partial computation offload dynamic environment incomplete information compute non trivial due user obtain partial observation derive optimal decision description user computation intensive delay sensitive task execute multiple wireless user offload computation task nearby server deployed network operator model denote input data user user offload computation task server denote xkn xkn user computation task locally xkn xkn user offloads input server network bandwidth assign user denote  minimum maximum network bandwidth respectively offload model apply application scenario malware detection malware detection explore feature runtime behavior application mobile device involve data application execution application trace scan malware signature file security database offload detection task secure server malware detection reduce computation mobile device scenario user offload trace server multi user computation offload user compute environment server applies algorithm input data amount input data offload user server xkn denote amount computational resource available server normalize processing capability user server allocates computation resource user accord uploaded data amount user obtains portion xkn   denote computational capacity user slot transmission delay processing delay server task execution delay user slot denote tkn tkn max    source transmission user depends transmission bandwidth  transmission data user processing server user utility user define xkn    xkn sourcewhere xkn xkn xkn decision user user coefficient obtain computational resource server model flexibility user specific demand user parameter decision battery user user decision consumption processing input data user computational user local data xkn user decision user decision xkn maximize utility consumption processing  max xkn xkn source theory powerful framework analyze interaction multiple user optimal computation offload scheme user incentive deviate unilaterally objective algorithm achieve nash equilibrium define 3Definition nash equilibrium strategy constitutes nash equilibrium decentralize computation offload slot satisfied xkn source nash equilibrium nice stability user equilibrium achieve mutually satisfactory incentive deviate important decentralize computation offload user algorithm information algorithm mobile user information network bandwidth  proceed introduce important concept response  strategy user user strategy response xkn source accord user offload strategy towards nash equilibrium concept response observation lemma communication  computational optimal computation offload strategy nth user      otherwise SourceRight click MathML additional feature       SourceRight click MathML additional feature proof accord eqn derivative respect xkn xkn     pnbkn SourceRight click MathML additional feature derivative respect xkn xkn   SourceRight click MathML additional feature utility function strictly concave function xkn therefore feasible strategy profile user optimal computation offload strategy user unique exit derivative respect xkn xkn     pnbkn sourceby obtain    SourceRight click MathML additional feature    otherwise lemma theorem user decision nash equilibrium theorem information computation offload strategy user unique nash equilibrium satisfies  νmαm pnbkn   νmαm sourceif  νmαm pnbkn    sourcethe utility user     sourceand computation delay tkn max    source proof accord eqn       eqn pnbkn  sourceand therefore    SourceSince hence unique eqn eqn obtain unique  νmαm  unique eqn unique user  νmαm pnbkn   νmαm SourceTherefore computation offload unique nash equilibrium eqn obtain  νmαm pnbkn   νmαm SourceRight click MathML additional feature completes proof satisfies plug eqn obtain utility computation delay tkn user nash equilibrium remark accord theorem offload strategy user nash equilibrium computation resource server transmission user bandwidth user local computational user user preference delay consumption corollary information battery nth user offload computation server proof mention user battery user consumption computation offload decision accord eqn derive derivative respect pnbkn     νmαm  νmαm pnbkn   νmαm SourceSince derive pnbkn   νmαm  νmαm pnbkn  obtain therefore user battery reduce offload computation server accord analysis propose algorithm achieve nash equilibrium information algorithm slot user information user private information others user decides offload strategy accord optimal computation offload strategy obtain theorem algorithm algorithm information slot user publish private information xkn  alpha information user calculate optimal computation offload strategy accord theorem algorithm without information scenario information user knowledge user information bandwidth decision alpha however unrealistic obtain information user refuse expose parameter due consideration privacy protection furthermore physical parameter user variant challenge user estimate user accurately offload algorithm user unobservable specifically formulate dynamic decentralize computation offload multi agent partially observable markov decision novel dynamic computation offload algorithm DRL user multi agent DRL approach algorithm user approximately optimal computation offload strategy directly without prior information user partially observable markov decision demonstrate multi agent POMDP mathcal langle mathcal mathcal mathcal mathcal mathcal mathcal rangle consists mathcal lbrace mathcal lbrace forall mathbb rbrace rbrace mathcal action mathcal lbrace mathcal rbrace mathcal transition probability function mathcal lbrace mathcal mathcal mathcal rightarrow rbrace mathcal reward mathcal lbrace mathcal rbrace mathcal observation mathcal observation function mathcal lbrace mathcal mathcal rightarrow rbrace mathcal POMDP mathcal partially decision observation mathcal user strategy lbrace boldsymbol boldsymbol ldots boldsymbol rbrace establish POMDP partially observable markov decision observation observation mathcal lbrace mathcal rbrace mathcal mathcal lbrace boldsymbol forall mathbb rbrace boldsymbol boldsymbol ldots boldsymbol boldsymbol randomly generate leq slot observation user consists bandwidth previous slot input data uploaded user previous slot action mathcal lbrace mathcal rbrace mathcal mathcal lbrace forall mathbb rbrace slot action user input data uploaded server observation transition user action observation user transit boldsymbol satisfy boldsymbol sim int mathcal cdot lbrace rbrace mathcal mathrm noteworthy transition stochastic hence observation transition stochastic reward reward mathcal lbrace mathcal rbrace mathcal mathcal lbrace forall mathbb rbrace boldsymbol user action user calculate reward accord utility function multi agent objective computation offload policy user parameterized boldsymbol theta boldsymbol theta define boldsymbol theta mathcal mathcal rightarrow policy optimization user derive align boldsymbol theta arg max boldsymbol theta boldsymbol theta arg max boldsymbol theta mathbb boldsymbol theta boldsymbol rho arg max boldsymbol theta mathbb boldsymbol theta boldsymbol rho boldsymbol theta tag align sourcewhere equation boldsymbol theta boldsymbol mathbb boldsymbol boldsymbol mathcal mathcal qquad tag equation source align boldsymbol theta boldsymbol mathbb boldsymbol boldsymbol mathcal mathcal tag align source equation sum gamma qquad qquad qquad tag equation sourcein equation lbrace boldsymbol theta rbrace mathcal user policy boldsymbol theta function observation boldsymbol theta boldsymbol function observation action rho initial observation probability distribution user discount future reward user slot gamma discount factor therefore formulate dynamic decentralize computation offload multi agent POMDP computation offload strategy user optimize multi agent policy gradient DRL approach non cooperative scenario algorithm overview user module DRL controller determines input data uploaded sever DRL controller illustrate actor network critic network replay buffer utility calculation module memory module function optimizer policy optimizer specifically actor network output action accord observation directly multiple fully neural network critic network observation feature vector feature memory combine feature estimate observation derive memory renew operation replay buffer slot actor critic update buffer dynamic decentralize computation offload DRL actor critic network  propose multi agent reinforcement extend actor critic framework multi agent continuous policy optimization however assume agent observation centralize critic agent user unable observation decentralize actor decentralize critic user denote actor network approximate policy user boldsymbol theta critic network approximate function user boldsymbol omega boldsymbol theta boldsymbol omega parameter actor network critic network respectively precisely actor network boldsymbol theta multi layer fully neural network determines input data uploaded user observation boldsymbol critic network boldsymbol omega complex consists layer DNC DNC recurrent neural network internal memory module capable hidden input DNC observation estimate boldsymbol omega depends entire observation previous recurrent neural network effective address POMDP demonstrate critic network DNC user achieve faster convergence equilibrium policy optimization optimize continuous policy user policy gradient objective function define accord policy gradient theorem proven trust policy optimization theory propose policy gradient calculate align nabla boldsymbol theta mathbb boldsymbol theta rho boldsymbol nabla boldsymbol theta boldsymbol theta boldsymbol boldsymbol theta boldsymbol mathbb boldsymbol theta rho boldsymbol nabla boldsymbol theta boldsymbol theta boldsymbol boldsymbol theta boldsymbol approx mathbb boldsymbol theta rho boldsymbol nabla boldsymbol theta boldsymbol theta boldsymbol boldsymbol theta boldsymbol tag align sourcewhere frac boldsymbol theta boldsymbol boldsymbol theta boldsymbol boldsymbol theta boldsymbol boldsymbol theta boldsymbol boldsymbol theta boldsymbol advantage function observation action boldsymbol theta parameter policy sample rho boldsymbol observation distribution induced POMDP accelerate convergence policy optimization adopt proximal policy optimization ppo propose clip policy gradient equation nabla boldsymbol theta approx mathbb boldsymbol theta rho boldsymbol nabla boldsymbol theta boldsymbol theta boldsymbol boldsymbol tag equation SourceRight click MathML additional feature equation boldsymbol min boldsymbol theta boldsymbol eta boldsymbol theta boldsymbol tag equation source equation eta lbrace array varepsilon varepsilon varepsilon leq leq varepsilon varepsilon varepsilon array tag equation sourceand varepsilon adjustable parameter update actor critic define loss function update critic network user equation boldsymbol omega mathbb boldsymbol sim rho boldsymbol boldsymbol omega boldsymbol mathbb boldsymbol prime boldsymbol omega boldsymbol prime tag equation sourcewhere boldsymbol prime observation boldsymbol user training estimate gradient boldsymbol omega calculate equation nabla boldsymbol omega frac sum boldsymbol omega boldsymbol frac boldsymbol omega boldsymbol boldsymbol omega tag equation sourcewhere equation gamma gamma boldsymbol omega boldsymbol tag equation sourceand mini batch update critic network critic network boldsymbol omega update mini batch stochastic gradient descent equation boldsymbol omega leftarrow boldsymbol omega nabla boldsymbol omega tag equation sourcewhere critic rate user moreover estimate gradient boldsymbol theta calculate equation nabla boldsymbol theta frac sum nabla boldsymbol theta boldsymbol theta boldsymbol boldsymbol tag equation sourcewhere mini batch update actor network update actor network boldsymbol theta mini batch stochastic gradient ascent equation boldsymbol theta leftarrow boldsymbol theta nabla boldsymbol theta tag equation sourcewhere actor rate user algorithm DRL algorithm without information user mathcal initialize gamma boldsymbol theta boldsymbol omega boldsymbol slot ldots user mathcal update observation boldsymbol boldsymbol lbrace boldsymbol boldsymbol rbrace mathcal input boldsymbol actor network boldsymbol theta input data uploaded server calculate reward boldsymbol ldots user mathcal calculate nabla boldsymbol theta nabla boldsymbol omega via update boldsymbol theta boldsymbol omega replay buffer mathcal algorithm detail pseudo code algorithm user initializes observation parameter actor critic network slot user observes bandwidth update observation user observation strategy reward observation replay buffer user observation input actor network upload portion input data sever accord output actor network user upload data user calculates utility reward user update actor critic network slot replay buffer mini batch estimate gradient update actor critic network user optimizes actor critic network mini batch stochastic gradient ascent descent respectively finally replay buffer performance evaluation simulation setting extensive simulation conduct evaluate performance propose algorithm user amount input data  data delete periodically avoid storage overflow user transmission alpha user computation capacity slot computation capacity server default bandwidth satisfies constrain theorem reasonable simulation minimum maximum network bandwidth frac mbps frac mbps satisfies constrain parameter DRL controller tune specifically actor network hidden fully layer contains node node respectively critic network hidden fully layer node node respectively DNC module memory matrix default baseline approach simulation nash equilibrium NE calculate algorithm information moreover scenario user information others offload algorithm without prior information derive NE  modify AC implementation AC multi agent environment   modify ppo implementation ppo competitive multi agent training  greedy heuristic algorithm greedily chooses policy maximum reward replay buffer random user randomly input data offload server compute user  user offload computation server simulation convergence propose DRL algorithm user simulation frac frac frac frac offload strategy user utility converge NE slot offload strategy user nash equilibrium respectively influence network bandwidth user bandwidth frac frac bandwidth user frac frac frac user increase offload data obtains utility bandwidth user offloads percent input data frac utility however bandwidth improve frac utility obtain offload percent input data transmission decrease bandwidth motivates user upload data server user computation delay decrease growth bandwidth delay user decrease percent bandwidth frac frac user uploads data server bandwidth increase data compute locally reduce delay local computation consumption user user consumes upload data server decrease computation delay increase utility user offload data server increase user bandwidth consumption reduce addition user performance degradation bandwidth increase user user data offload decrease percent utility decrease percent computation delay increase percent user competitiveness decline computation resource reduce analysis derive user bandwidth increase user upload data server performance DRL user bandwidth user performance DRL user bandwidth user convergence DRL convergence DRL affect alpha alpha user alpha alpha alpha meanwhile fix bandwidth frac frac frac frac user offloads data server coincides statement corollary user battery offload data server reduce computation delay user input data offload server increase percent computation delay decrease percent performance DRL user user performance DRL user user performance algorithm variant network environment frac frac frac randomly chosen lbrace frac frac frac frac frac rbrace propose algorithm DRL significantly exceeds others average utility convergence instance DRL increase average utility user percent  percent  percent greedy percent random meanwhile proposal slot converge stable almost shock however  slot  slot shock average performance user variant network environment average performance user variant network environment impact user simulation simulation clearly average utility user decrease user server although computation capability server remains unchanged competition user therefore user decrease computation offload server decline average utility DRL maximum average utility  converge nash equilibrium convergence strategy DRL converges stable performance decentralize computation offload user performance decentralize computation offload user  complexity analysis DRL execution observation information input user utilizes actor network boldsymbol theta generate action computational complexity merely fully neural network accord complexity fully neural network multiplication operation sum epsilon cdot epsilon epsilon neural fully layer fully hidden layer actor network boldsymbol theta meanwhile mobile device become afford computational overhead incur actor network task offload scenario data analytics amount analyze volume approximate continuous hence analysis task arbitrarily chosen portion offload server model scenario analysis task task consist amount cpu instruction regard continuous therefore model conclusion dynamic computation offload decision user compute dynamic environment propose computation offload formulation admits unique nash equilibrium decentralize computation offload algorithm DRL combine policy gradient DRL approach DNC achieve optimal offload strategy simulation demonstrate propose algorithm efficient baseline approach