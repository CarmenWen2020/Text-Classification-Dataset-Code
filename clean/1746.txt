semi supervise anomaly detection approach identify anomaly distribution normal data backpropagation neural network BP nns approach recently drawn attention generalization capability typical situation BP NN model iteratively optimize server machine input data device however iterative optimization significant effort distribution normal data concept drift data transfer server impose additional latency consumption address issue propose ONLAD IP core ONLAD core ONLAD highly optimize perform sequential concept drift millisecond ONLAD core realizes device device consumption realizes standalone execution data transfer server ONLAD favorable anomaly detection capability environment simulates concept drift evaluation ONLAD core confirm training latency faster software implementation runtime consumption ONLAD core implement pynq fpga cpu soc platform introduction anomaly detection approach identify rare data instance anomaly distribution majority normal mainly approach anomaly detection supervise anomaly detection semi supervise anomaly detection unsupervised anomaly detection typical strategy supervise anomaly detection binary classification model normal versus anomaly label normal anomaly data model however anomaly instance basically rarer normal imposes imbalanced address issue undersampling majority data oversampling minority data assign misclassified data classifier concentrate minority semi supervise anomaly detection topic assumes training data belong normal typical strategy semi supervise anomaly detection distribution normal data identify data sample distribution anomaly semi supervise approach anomaly model applicable task various approach propose technique cluster approach classification approach unsupervised anomaly detection label training data constraint restrictive semi supervise unsupervised manner unlabeled data model unlabeled data belong normal sometimes unsupervised anomaly detection semi supervise anomaly detection distinguish explicitly focus semi supervise anomaly detection recently neural network approach attention achieve relatively generalization performance traditional approach data image audio data although variant neural network backpropagation neural network BP nns currently widely illustrates typical application BP NN semi supervise anomaly detection model device implement model detect anomaly incoming data device suppose perform inference computation calculate anomaly training computation offload server machine model iteratively server machine amount input data device training loop completes parameter device update optimize however issue approach BP nns iterative optimization approach considerable computation series distribution normal data concept drift data transfer server machine impose device additional latency consumption communication mention distribution normal data feature semi supervise anomaly detection approach however distribution phenomenon refer concept drift concept drift serious frequent surround environment data behavioral data source semi supervise anomaly detection model normal data however BP nns iterative optimization approach introduces considerable delay widens gap distribution normal data model gap identify anomaly gradually typical application BP NN semi supervise anomaly detection model usually device implement machine model specialized prediction computation backpropagation amount computational training computation BP nns typically offload server machine computational data transfer server machine inevitable imposes additional consumption communication potential risk data breach device practical issue device sequential approach illustrate approach incoming input data sequentially device approach allows device sequentially distribution normal data standalone execution data transfer however challenge regard construct sequential algorithm implement device limited resource device sequential approach underlie challenge propose device sequential semi supervise anomaly detector ONLAD IP core ONLAD core algorithm ONLAD perform sequential concept drift millisecond ONLAD core realizes device resource limited device consumption contribution ONLAD leverage OS elm lightweight neural network perform sequential core component theoretically analyze training algorithm OS elm demonstrate computational significantly reduces without degrade training batch propose computationally lightweight forget mechanism OS elm FP elm OS elm variant dynamic forget mechanism feature semi supervise anomaly detection distribution normal data OS elm forget normal data distribution propose function OS elm additional computational propose ONLAD sequential semi supervise anomaly detector combine OS elm autoencoder neural network dimensionality reduction model combination propose technique reduce computational realizes sequential semi supervise anomaly detection public datasets ONLAD comparable generalization capability BP NN model context anomaly detection confirm ONLAD outperforms BP NN model anomaly detection capability environment simulates concept drift implementation ONLAD core evaluation ONLAD core ONLAD core perform training prediction computation approximately millisecond comparison software counterpart training latency ONLAD core faster prediction latency faster average confirm propose forget mechanism faster baseline algorithm FP elm average addition evaluation ONLAD core implement pynq fpga cpu soc platform practical model demonstrate runtime consumption pynq implement ONLAD core software counterpart training computation continuously execute organize brief review technology ONLAD propose ONLAD describes implementation ONLAD core ONLAD evaluate anomaly detection capability ONLAD core evaluate latency fpga resource utilization consumption related described concludes preliminary brief introduction technology ONLAD extreme machine elm online sequential extreme machine OS elm autoencoders elm elm illustrate hidden layer feedforward network SLFN consists input layer hidden layer output layer suppose dimensional input chunk batch dimensional output chunk compute sourcewhere denotes input input layer hidden layer RN output hidden layer output layer RN denotes bias vector hidden layer activation function apply hidden layer output extreme machine SLFN approximate dimensional target chunk zero error implies exists satisfies equation SourceLet hidden layer output optimal output compute sourcewhere pseudo inverse calculate matrix decomposition algorithm singular decomposition svd hth hht non singular calculate efficient hth HT HT hht training simply replace initialize random conversion random projection elm iterative optimization BP nns shot optimization training faster elm compute optimal output faster BP nns categorize batch algorithm wherein training data assume available advance elm retrain dataset training data instance OS elm OS elm elm variant perform sequential instead batch suppose ith training chunk   batch minimizes error sourcewhere define optimal output sequentially compute  HTi  HTi     compute hth  sourcethe initial training sample hidden node hth nonsingular equation OS elm sequentially optimal output training chunk without memory retrain training data unlike elm OS elm optimal faster BP nns autoencoders autoencoder illustrate neural network unsupervised model characterize dimensionality reduce input chunk generally output intermediate layer regard elm OS elm intermediate layer therefore hidden layer output regard basically hidden node constrain input node autoencoders specially refer  autoencoders however sometimes refer  autoencoders although  autoencoders cannot perform dimensionality reduction obtain characterize representation classification apply regularization loss function autoencoder training input data target therefore autoencoder correctly reconstruct input data output data empirically tends become characterize error input data reconstruct output data converges label data training autoencoder categorize unsupervised model autoencoders attract attention semi supervise anomaly detection context autoencoder normal data therefore output tends relatively reconstruction error anomaly anomaly detect threshold error approach categorize semi supervise anomaly detection normal data training data principal component analysis pca another non statistical dimensionality reduction algorithm autoencoders autoencoder model detect subtle anomaly pca fails moreover autoencoders perform nonlinear transformation without costly computation kernel pca ONLAD mention introduction ONLAD leverage OS elm core component theoretical analysis OS elm demonstrate computational training algorithm significantly reduces batch without deterioration training propose computationally lightweight forget mechanism concept drift finally formulate algorithm ONLAD analysis OS elm training algorithm OS elm equation mainly consists matrix matrix inversion suppose computational iteration matrix pqr matrix inversion computational iteration operation equation calculate  sourcewhere  denotes computational iteration matrix  denotes matrix inversion input hidden output node OS elm respectively denotes batch instance computational iteration  HTi calculate compute RN  HTi RN computational iteration calculate respectively computational iteration matrix matrix inversion equation batch accordingly equation derive   source finally obtain inequality training algorithm becomes computationally efficient batch batch insight software implementation computational model account software specific overhead memory allocation function however bare implementation ONLAD core benefit insight overhead moreover computational matrix inversion  HTi equation significantly reduce target matrix  HTi training algorithm derive equation       sourcewhere RN denotes thanks trick OS elm perform training without costly matrix inversion reduce computational hardware resource ONLAD core easy parallelize training algorithm matrix inversion parallelism equation furthermore training OS elm affected batch OS elm output training perform batch batch notable difference BP nns training batch basis discussion batch OS elm ONLAD lightweight forget mechanism OS elm environment distribution normal data ONLAD function adaptively forget normal data additional computational challenge propose computationally lightweight forget mechanism forget parameter extreme machine FP elm OS elm variant dynamic forget mechanism review FP elm brief review FP elm training algorithm FP elm formulate    HTi   compute hth hth  sourcewhere regularization parameter limit become prevent overfitting forget factor significance training chunk suppose training kth training chunk gradually decrease  variable parameter adaptively update accord information input data output error propose forget mechanism FP elm training chunk however cannot remove matrix inversion equation batch target matrix denotes hidden node address issue modify FP elm remove matrix inversion batch equation derive disable regularization trick equation      SourceNext update formula derive woodbury formula     HTi  HTi   training algorithm obtain define  αiPi αiPi HTi αiPi HTi αiPi    compute algorithm equation propose forget mechanism eliminates matrix inversion equation batch target matrix αiPi HTi denotes batch equation becomes training algorithm OS elm αiPi replace propose forget function additional computational training algorithm OS elm however suffer overfitting regularization trick disabled quantitatively evaluate algorithm ONLAD random random hth  exists αiPi  singular matrix encounter  anomaly detect αiPi      algorithm ONLAD leverage OS elm batch conjunction propose forget mechanism equation derive combine equation  αiPi αiPi  αiPi αiPi     built OS elm autoencoder construct semi supervise anomaly detector equation training algorithm ONLAD  αiPi αiPi  αiPi αiPi     compute equation hth  sourceas equation ONLAD performs training forget operation prediction algorithm formulate sourcewhere denotes loss function anomaly stability OS elm training OS elm training stability issue  HTi equation singular matrix training becomes unstable regardless batch context ONLAD occurs αiPi  equation ONLAD training αiPi  denotes positive ONLAD practical ONLAD algorithm intend practical initialize random compute equation initial training sample hidden node hth nonsingular ith training loop inequality αiPi  evaluate skip false anomaly compute equation judged anomaly user define threshold otherwise ONLAD judge normal sample finally sequential perform equation ONLAD core describes implementation ONLAD core IP core ONLAD demonstrate ONLAD core implement device limited resource pynq soc platform fpga integrate display specification develop ONLAD core vivado HLS implement pynq vivado frequency ONLAD core mhz pynq specification pynq overview implementation brief overview implementation diagram processing PS mainly responsible preprocessing input data trigger memory access dma controller dma controller convert preprocessed input data dram axi format packet transfer ONLAD core convert output packet ONLAD core axi memory mapped format data transfer dram programmable logic PL implement ONLAD core ONLAD core performs training prediction computation accord information header input packet detail described later diagram implementation detail ONLAD core illustrates diagram ONLAD core important sub module parameter buffer input buffer module predict module explains sub module diagram ONLAD core diagram ONLAD core parameter buffer parameter buffer manages parameter ONLAD core parameter implement brams hence bram instance consume parameter increase specifically matrix parameter buffer denote  calculate   apply equation ONLAD autoencoder equation utilization bram instance module proportional hidden node proportional input node input buffer input buffer input vector preprocessed PS parameter buffer implement brams matrix input buffer denote  calculate   utilization bram instance module proportional input node module module predict module input vector module module executes training algorithm equation update parameter parameter buffer processing processing sequentially execute accord discussion module interrupt computation implementation output signal indicates inequality satisfied satisfied satisfied matrix operation matrix matrix matrix sub wise multiplies implement arithmetic fix precision DSPs hardware resource matrix operation specific arithmetic regardless input hidden node matrix processing implement brams processing module matrix module denote strain calculate strain  utilization bram instance module proportional hidden node proportional input node  denotes computational iteration processing calculate manner described  sourcethe computational proportional hidden node proportional input node predict module predict module executes prediction algorithm equation output anomaly processing predict module methodology module processing predict module matrix predict module denote  calculate   utilization bram instance module proportional hidden node input node  denotes computational iteration processing  sourcethe computational proportional hidden node input node implementation matrix operation ONLAD core matrix operation matrix matrix matrix sub wise implement dedicate circuit matrix operation synthesize vivado HLS loop unroll loop pipelining directive innermost loop operation parallelization unroll factor parallelize arithmetic instruction ONLAD core ONLAD core execute instruction update params update input update training prediction packet format instruction detailed input packet mode specifies instruction execute ONLAD core data reserve accord instruction output packet embeds output module predict module packet format sub module ONLAD core accord instruction update params instruction update parameter buffer packet format target parameter specify mode input packet index data embeds index target parameter update target parameter update target index  parameter manage flatten array parameter buffer update input instruction update input buffer packet format almost update params instruction mode index  buffer update formula input packet dimensional input vector update instruction update forget factor manage module packet format instruction data embeds update update source training instruction executes training computation module module parameter parameter buffer input vector input buffer executes training algorithm update parameter buffer parameter packet format fourth input packet instruction trigger perform training output packet instruction embeds evaluation denote inequality described satisfied satisfied prediction instruction executes prediction computation predict module predict module output update input vector module predict module executes prediction algorithm output anomaly input vector packet format input packet instruction trigger prediction output packet instruction embeds output anomaly denote compute ONLAD core evaluation anomaly detection capability anomaly detection capability ONLAD evaluate comparison model server machine OS ubuntu cpu intel core ghz gpu nvidia gtx GB dram ddr GB storage ssd GB experimental machine experimental setup ONLAD model FPELM AE NN AE dnn AE FPELM AE FP elm autoencoder model quantitatively evaluate disable regularization trick ONLAD NN AE layer BP NN autoencoder dnn AE BP NN autoencoder consist layer model OS elm autoencoders FPELM AE ONLAD BP NN model ONLAD implement tensorflow comprehensive evaluation testbeds offline testbed online testbed conduct offline testbed simulates environment training data available advance concept drift occurs standard experimental setup evaluate semi supervise anomaly detection model purpose offline testbed generalization capability ONLAD context anomaly detection testbed evaluate propose forget mechanism fix concept drift occurs testbed online testbed simulates environment dataset arrives online testbed assumes concept drift occurs purpose testbed evaluate robustness propose forget mechanism concept drift comparison model public classification datasets construct offline testbed online testbed data sample normalize within min max normalization hyperparameters model explore within detailed datasets hyperparameters experimental describes experimental offline testbed online testbed respectively algorithm offline testbed  xtest average auc xnormal xnormal  num anomaly len xnormal  sample  num anomaly model xnormal model predict concat xnormal  average auc average auc calc auc model reset average auc average  algorithm experimental offline testbed testbed dataset training data  percent data xtest percent respectively suppose dataset consists training data normal data training denote xnormal data normal data denote xnormal data anomaly data denote  sample  limited percent xnormal simulate practical situation anomaly data rarer normal data model xnormal NN AE dnn AE batch epoch training procedure model evaluate xnormal  auc curve calculate auc widely metric evaluate accuracy anomaly detection model independently anomaly threshold auc average output trial auc report average trial fold validation conduct hyperparameter tune auc offline testbed algorithm experimental online testbed testbed dataset initial data  percent data xtest percent validation data  percent  data sample exit  xtest  data sample sequentially xtest auc  hyperparameter tune normal data xnormal percent anomaly data  percent denote index consist integer construct randomly shuffle output indicates normal concept suppose index normal concept ith concept  normal data index anomaly data index anomaly sample per concept limited percent normal sample model initial data normal index init NN AE dnn AE batch epoch model computes anomaly data sample continuously   anomaly compute model data sample model NN AE dnn AE batch sequentially transition normal data sample fed model auc calculate anomaly auc trial auc report average trial hyperparameter tune conduct algorithm trial replace xtest  algorithm algorithm online testbed  init init init xtest xnormal  split xtest index shuffle index  concept index normal num anomaly len index normal concept append sample index anomaly num anomaly  append shuffle concat concept model index init  model predict append model auc calc auc auc online testbed experimental experimental offline testbed hyperparameter setting NN AE dnn AE achieve slightly auc ONLAD approximately almost datasets implies BP NN autoencoders slightly generalization capability OS elm context anomaly detection however NN AE dnn AE iteratively epoch achieve performance epoch contrast ONLAD optimal output epoch ONLAD achieves auc NN AE dnn AE datasets reduce computational hardware resource implement ONLAD core addition difference auc ONLAD FPELM AE within ONLAD favorable generalization performance regularization trick disabled summary ONLAD comparable generalization capability BP NN model training epoch model hyperparameter setting offline testbed experimental online testbed hyperparameter setting another model ONLAD NF ONLAD forget mechanism introduce examine effectiveness propose forget mechanism ONLAD NF ONLAD forget mechanism disabled hyperparameter setting ONLAD NF ONLAD ONLAD NF suffers significantly auc ONLAD obvious ONLAD NF function forget data therefore gradually becomes detect anomaly concept drift happens NN AE dnn AE achieve auc ONLAD NF BP nns catastrophic forget forget mechanism however BP nns numerical parameter analytically progress forget unlike ONLAD ONLAD stably achieves favorable auc additionally ONLAD FPELM AE auc datasets offline testbed propose forget mechanism significantly affected regularization trick datasets summary ONLAD achieves auc NN AE dnn AE approximately datasets achieves comparable auc BP NN model datasets hyperparameter setting online testbed evaluation performance ONLAD core evaluate latency fpga resource utilization consumption comparison software implementation experimental setup ONLAD core evaluate comparison software implementation NN AE cpu dnn AE cpu NN AE gpu dnn AE gpu FPELM AE cpu FPELM AE gpu cpu execute cpu gpu execute gpu cooperation cpu implementation developed tensorflow tensorflow built avx advanced vector EXTENSIONS instruction option accelerate cpu computation built cuda enable gpgpu execution hyperparameter setting implementation detailed omit parameter unrelated evaluation metric latency fpga resource utilization consumption batch NN AE dnn AE fix ONLAD core FPELM AE conduct comparison latency consumption hyperparameter setting latency training prediction latency refer training latency elapse model receives input sample training algorithm compute prediction latency elapse model receives input data sample anomaly calculate training prediction latency implementation versus input hidden node report average trial practical latency exploration input node hidden node basis hyperparameter setting ONLAD comparison training latency comparison prediction latency latency software implementation remain almost constant input node increase outcome execution occupy software overhead invoke training prediction task gpu implementation suffer latency communication gpu cpu addition software overhead contrast ONLAD core overhead consequently ONLAD core achieves speedup average NN AE cpu dnn AE cpu FPELM AE cpu NN AE gpu dnn AE gpu FPELM AE gpu training latency speedup average prediction latency ONLAD core perform sequential prediction concept drift approximately millisecond however ONLAD core become others input node computational predict module proportional input node equation hence ONLAD core difficulty achieve speedup beyond software implementation input node moreover computational module proportional hidden node however contrary expectation latency almost proportional hidden node equation computational module almost proportional hidden node input node hidden node practicality empirically demonstrate hyperparameter setting ONLAD core satisfy hence practical situation computational ONLAD core excessively increase hidden node increase computational propose forget mechanism propose forget mechanism ONLAD core baseline algorithm FP elm computational forget operation ONLAD core FP elm unified training algorithm training latency comparison computational cpu implementation ONLAD core FPELM AE cpu implement library tensorflow experimental exploration input hidden node increase ratio computation model comparison computational consequently forget mechanism faster FPELM AE cpu average computational forget mechanism equation however FP elm matrix matrix inversion FP elm gap computation gradually widens hidden node increase comparison training latency propose forget mechanism FP elm relationship training latency ONLAD core hidden node training latency versus input node hidden node training latency versus input node hidden node prediction latency versus input node hidden node prediction latency versus input node hidden node fpga resource utilization evaluates fpga resource utilization ONLAD core input hidden node exploration input node chosen hidden node basis previous analysis pre synthesis resource utilization report vivado HLS experimental experimental dsp utilization remains almost constant input hidden node increase reasonable outcome dsp slice consume module predict module specific arithmetic regardless input hidden node mention fpga resource utilization ONLAD core pre synthesis however ONLAD core consumes bram instance model increase  denotes matrix entire ONLAD core    strain   utilization bram instance ONLAD core linearly increase input node increase experimental consistent equation bram utilization proportional input node equation utilization bram instance proportional hidden node however bram utilization ratio ONLAD core almost proportional hidden node logic previous explain outcome equation practicality described previous hence practical situation bram utilization ONLAD core suppress excessively increase hidden node increase consequently utilization rate ONLAD core limit relationship bram utilization hidden node relationship bram utilization hidden node consumption evaluates runtime consumption implementation comparison software implementation ordinary watt meter consumption pynq software implementation tui nvidia smi tui source cpu monitoring consumption cpu intel core ghz equip experimental machine nvidia smi gpu monitoring utility nvidia consumption gpu nvidia gtx GB equip experimental machine input hidden node implementation commonly confirm consume amount resource resource utilization report ONLAD core fpga resource utilization ONLAD core synthesis consumption implementation training computation continuously execute implementation consumes others report consumption implementation ONLAD core component dual core cpu hence consumption ONLAD core comparison consumption related device data important role machine although sometimes privacy sensitive device prediction ensure data privacy user data transfer external server machine propose  exist BP NN model reduce memory user device without significantly degrade accuracy leverage locality sensitive hash LSH projection distillation training framework  propose federate framework utilizes user device computational node global model framework user device suppose perform training local data update aggregate global model federate approach device wireless sensor network explore essential building underlie challenge approach device perform training although aim global model locally personalize model target device anomaly detection OS elm sequential approach capable input data online utilized anomaly detection adaptation prediction OS elm exception report anomaly detection OS elm propose OS elm irregular behavior detection electricity customer prevent non technical loss theft illegal connection svm superiority propose OS elm network traffic intrusion detection IDS perform training amount traffic data limited memory propose decentralize anomaly detection wireless sensor network utilize OS elm semi supervise anomaly detection conjunction autoencoder propose combination OS elm variant forget mechanism OS elm variant forget mechanism propose forget mechanism OS elm  elm  elm slide approach training chunk account fix parameter  elm FP elm introduce variable forget factor forget training chunk gradually adaptively update forget factor accord information input data output error approach FP elm modify forget mechanism additional computational algorithm OS elm hardware implementation OS elm hardware implementation elm report however implementation OS elm report theoretical analysis hardware implementation OS elm significantly reduce computational propose efficient fpga implementation OS elm embed propose IP core implement propose OS elm semi supervise anomaly detection approach IP core implement device limited resource consumption neural network hardware implementation anomaly detection NN anomaly detection hardware implementation propose fpga monitoring prediction msec induction motor propose employ supervise anomaly detection approach layer binary classification model anomaly data normal data training propose electrocardiogram anomaly detection approach fpga propose consists feature extraction dimensional reduction classification implement dedicate circuit fpga report prediction latency approximately cycle although approach classification model contrast implementation ONLAD core adopts semi supervise approach normal data training comparison NN hardware implementation anomaly detection propose fpga anomaly detector frequency signal propose IP core realizes semi supervise anomaly detection BP NN autoencoder approach prediction latency  however model parameter ONLAD core fpga platform besides IP core training computation propose belief network DBN IP core training ONLAD core anomaly detection propose efficient training model contrastive divergence algorithm DBN report performance IP core achieves gop however model adopts classification approach ONLAD core training adopts semi supervise anomaly detection approach applicable application hardware implementation neural network pynq pynq library allows developer cpu fpga architecture python code although specialized implement neural network  automate framework convolutional neural network cnn classification model fpga platform framework adopts synchronous dataflow model performance explore account platform specific constraint  generates synthesizable dnn accelerator configuration caffe  compiler tile schedule batch dnn operation maximize data reuse utilize target fpga memory propose framework binarized neural network  arithmetic  bitwise logic operation instead costly float operation computational fpga resource implement accelerator significantly reduce conventional cnns  gui implement  xilinx soc platform designer rtl code script enables software designer develop prototype bnn accelerator without knowledge hardware conclusion summary propose ONLAD realizes sequential semi supervise anomaly detection construct autoencoder OS elm computational OS elm significantly reduce batch fix contributes speedup ONLAD propose computationally lightweight forget mechanism OS elm FP elm enables ONLAD concept drift computational addition propose ONLAD core realize device execution ONLAD resource limited device consumption ONLAD core offload training computation external remote server machine enables standalone execution data transfer server machine experimental public datasets ONLAD comparable generalization capability BP NN model context anomaly detection confirm ONLAD favorable anomaly detection capability environment simulates concept drift evaluation ONLAD core confirm perform training prediction computation faster software implementation BP nns FP elm average  propose forget mechanism faster FP elm average addition evaluation ONLAD core implement pynq practical model demonstrate runtime consumption pynq implement ONLAD core comparison software implementation training computation continuously execute future direction BP nns achieve generalization performance extent stack layer although OS elm algorithm limited hidden layer multi layer online sequential extreme machine ML  propose multi layer framework OS elm accord ML  outperforms OS elm classification datasets accuracy anomaly detection capability ONLAD improve replace OS elm ONLAD ML  multi layer version ONLAD ONLAD core multiple action mode conditioner robot turbine context anomaly detection formulate mixture model consist multiple sub distribution normal data recently mixture model framework utilizes multiple OS elm instance propose apply framework ONLAD ONLAD core