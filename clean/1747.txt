google deployed tpu generation lesson semiconductor technology advance  compiler compatibility trump binary compatibility VLIW domain specific architecture dsa target ownership initial multi tenancy neural network dnn annually dnn advance evolve workload inference task float inference DSAs apps limit latency batch backwards ML compatibility deploy dnns quickly lesson mold TPUvi inference dsa deployed TPUv address harder task training training parallelization harder inference independent cluster server dsa chip training iterates coordinate across parallel resource consistent model computation harder backpropagation derivative computation model precision activation function  multiplication transpose matrix training memory update access intermediate propagation vastly storage requirement temporary storage storage fourth programmable training algorithm model continually dsa restrict algorithm rapidly become obsolete finally integer inference sufficiently capture sum update training normally float arithmetic TPUv diagram transform TPUv via sequence training inference split chip SRAM buffering data sequential fix function TPUv  chip memory training inference optimization training writes merge activation storage accumulator vector memory TPUv programmable vector replace fix function datapath activation pipeline TPUv pool activation bfloat dnns MXU become hardware mac systolic array MXU attach vector matrix processor training goal significant buffer temporary per variable dram vector memory compiler memory hierarchy package HBM dram increase bandwidth ddr TPUv core utilized TPUv fetch VLIW instruction local memory host cpu introduction TPUs commercial domain specific architecture DSAs neural network dnns establish revisits expands fourth generation dsa evolution architecture production inform beyond familiar issue sluggish CPUs diminish moore TPUv diagram TPUv diagram TPUs deployed google datacenters feature review TPUs TPUv google dnn dsa handle inference bandwidth loop data computation  dnn layer activation storage accumulator SRAM buffer computation matrix MXU activation pipeline systolic array MXU integer accumulate mac ddr dram loop bandwidth model parameter TPUv connects host cpu pcie exchange model input output bandwidth host cpu sends TPUv instruction pcie contemporary gpus CPUs performance TDP perf watt production workload isca program feature TPUv TPUv TPUv TPUvi nvidia peak tflops chip int int int deployed GA date dnn target inference training inf training inf inference inference network link gbit chip max chip supercomputer chip rate mhz turbo idle watt chip TDP watt chip transistor chip technology memory chip MB GB MB GB MB GB MB GB MB GB memory GB chip ecc disabled MXU core core chip chip  characteristic DSAs underline prior tpu generation TDP dsa memory plus server host host TDP DSAs per host document unequal improvement logic SRAM dram update horowitz operation systolic float matrix TPUvi versus systolic integer matrix TPUv explain difference performance per TCO per capex HBM TDP TPUvi TPUv headroom application  explain backwards ML compatibility inference float spur TPUvi TPUv backwards ML compatible training tailor dnns TPUvi production inference application DSAs normally multiple dnns concurrently google inference DSAs multi tenancy discus dnn advance production inference workload workload mlp cnn bert rnn succeed lstm document growth production dnns memory computation annually encourages DSAs headroom google TCO TDP dnn DSAs strongly correlate likely due dennard TDP proxy dsa TCO document slo limit inference application typical batch chip SRAM performance explain TPUvi architect chose compiler compatibility binary compatibility VLIW ISA google inference accelerator production march evaluate performance training another enhancement custom chip chip interconnect fabric ici enable TPUv supercomputer chip unlike TPUv TPUv  per chip global chip shrink feature lesson relative delay increase core per chip prevent excessive latency chip core easy compile program efficiently  core numerous wimpy core TPUv   mild redesign TPUv technology MXUs HBM capacity increase rate memory bandwidth ici bandwidth TPUv supercomputer chip TPUv contemporary volta gpu float bfloat however volta training google production workload TPUv faster application chip perfect linear speedup introduces TPUvi inference forge earn lesson building deploy TPUs  insight TPUs differently TPUv lesson TPUvi avoid repetition TPUv TPUvi performance TDP TPUv production apps nvidia mlperf inference describes inference DSAs  lesson none google discussion conclusion highlight contribution TDP TPUv nvidia inference gpu production apps mlperf inference benchmark operation int int bfloat FP FP int int bfloat FP FP KB SRAM SRAM KB SRAM MB SRAM geomean dram ddr HBM GDDR lesson apply dsa gpus CPUs  clarity lesson tag logic SRAM dram improve  horowitz insight operation inspire dsa update average gain uneven SRAM access improve SRAM density SRAM capacity per dense ideal dram access improve due packaging innovation bandwidth memory HBM stack dram DSAs bus per improve delay TPUv core core TPUv logic improves faster SRAM logic relatively HBM efficient GDDR ddr dram HBM per GB bandwidth  per operation circa circa leverage prior compiler optimization fortune architecture bound quality compiler indeed compiler likely   VLIW architecture DSAs rely VLIW TPUs architect compiler developed simulator progress occurs hardware available compiler writer actual code architecture potential quickly easy leverage prior compiler optimization scratch DSAs exception  TPUs rely XLA accelerate linear algebra compiler nvidia gpus cuda compiler gain mlperf training benchmark version cuda compilation improve gpu mature XLA tpu contrast compiler improve generalpurpose code annually compiler critical dsa XLA compiler developed TPUv enhance TPUv TPUv intermediate TPUv per operation memory per access dsa gain per chip mlperf training compiler unverified TPUv mlperf mask cnn transformer lesson important lesson reader unsurprising however architect recent commercial ML accelerator ignore lesson performance per TCO per capex expense capex price item operation expense opex operation electricity consume provision standard accounting amortizes computer capex TCO capex opex google performance TCO production apps perf TCO raw performance horowitz MB SRAM SRAM multiple explains reduction MB SRAM omit geomean ddr dram HBM GDDR performance capex perf capex benchmark TCO variable google optimizes capex influence business decision outside scope CPUs gpus typically aim maximize performance benchmark versus purchase price announcement price depends volume contract negotiation architect performance proxy perf capex ala performance perf TCO increase rate alibaba  ghz counter tune dnn performance memory error correction ecc moreover benchmark inherently backward normally factor growth important perf TCO contrast TPUv headroom enable app improvement publication developer maintain  increase operation mlp cnn dsa aim perf TCO lifetime birth ala TCO confidential plot TCO versus TDP TPUs plus nvidia involves everything rack rack switch  nearly perfectly correlation correlation coefficient CPUs gpus TPUs TDP TCO unavailable without dennard transistor faster processor likely lesson focus dnn DSAs backwards ML compatibility dnns constraint economic timeliness perspective principle backwards ML compatibility goal cpu exactly exception behavior correlate performance training across tpu generation TPUv inference tpu  exception behavior bfloat tpu operation numeric compiler backwards ML compatibility guarantee float addition associative meaning operation prevent operation matrix convolution identical constrain compiler fix operation performance optimization subtle complication implies compiler generate code target compiler sometimes evaluation identical tpu prior TPUs compiler perspective performance correlate deploy immediately developer dnn reduces training inference latency violate dnn slo moreover lesson backwards ML compatible training pre tune dnns hardware inference DSAs global TPUv TPUv TPUv TPUs adjacent rack amortize infrastructure placement restriction training supercomputer already consist adjacent rack moreover downside limit training datacenters widespread deployment unnecessary user inference user latency worldwide footprint strategic datacenters already packed adjacent rack deployment inference DSAs TCO TDP TPUv nearly perfect TDP capacity planning average capacity expensive detail TCO factor directly electricity provision distribution twice electricity computer capex chip capex directly correlation coefficient moore slows inference apps float arithmetic quantization dnns aim retain inference model quality integer training TCO ignores financing lump capacity datacenter infrastructure amortization datacenter amortize inference float quantize arithmetic grant saving reduce quality delayed deployment apps quantize NMT mlperf inference TPUv quantization integer arithmetic datacenter application TPUv development application developer quality acceptable hardware dnn overall quality improve error relatively error relatively extra development restore quality integer expert achieve float training quality imagenet competition improve lesson DSAs quantization unlike TPUv quantization aware training dnns rnn quantize primary benefit halve memory footprint mlperf unet DLRM training quantize accuracy loss versus FP accuracy loss bert problematic software update production inference workload multi tenancy application developer demand switch model cannot met load cpu host pcie bus DSAs local memory chip SRAM load MB mlp external memory bandwidth GB faster inference chip moreover predicts dnns likely multi tenancy suggests dram DSAs SRAM ala dnn dsa designer ignore multi tenancy indeed multi tenancy mention TPUv lucky available ddr dram GB TPUv software multi tenancy mlp mlp cnn cnn rnn rnn bert bert avg max avg  program tenancy MB MB stddev average maximum multi tenancy dnns dsa google inference workload july february inference workload per dnn TCO TPUv mlp  cnn  cnn internal model image classification bert  mlperf likely multi tenancy requirement model annual memory increase annual FLOPS increase cnn mlp cnn mlp quantization error segmentation image identifies outline photo int outline fuzzy around bystander isolate unreliable outline photo incorrectly annual increase production application lesson dnn apps hardware dsa gpu cpu dnns memory compute unlike benchmark programmer continuously improve production application usually increase memory computation requirement average annual increase memory computation production inference apps TPUv annual increase production dnns moore PC software rate suggests architect headroom DSAs remain useful lifetime production inference normally multi tenancy CPUs DSAs multi tenancy reduce latency application model translation dnns dnns handle dialect multi tenancy multiple batch balance throughput latency another simply software engineering feature customer slowly deploy release reduce dnn workload evolve dnn breakthrough dnns google inference workload mlp cnn remain popular although apps switch MLPs bert dnns explains mlp bert already workload improve quality transformer encoder plus lstm decoder rnn rnn rnn replace lstms lesson importance programmability flexibility inference DSAs dnn progress deliver fleet perf TCO afford inference training optimize chip realize chip core chip inference TPUv dual core chip training TPUv chip core version uncore developed unified codebase improve TCO inference chip reduce  communication bandwidth MXU logic layout density lower consumption maximum density enable google deployed core TPUvi inference dual core TPUv chip training google  TPUv mlperf training july faster TPUv performance nvidia  gpu compiler compatibility binary compatibility TPUv TPUv VLIW instruction bundle conventional architecture wisdom TPUvi TPUv maintain backwards binary compatibility chose compiler compatible instead binary compatible argument VLIW enable hardware resource recompiling apps compiler instruction parallelism binary compatibility restricts engineer built  compiler XLA drawback binary compatibility VLIW compiler hardware XLA compiler accepts  pytorch tensorflow TPUs rely compiler versus interface compiler tpu software maintain distribute source code binary code XLA compile task highlevel operation HLO machine independent operation LLO machine dependent optimization HLO apply platform tpu restricts compiler  wider VLIW maintains compiler compatibility nvidia gpu cuda tpu XLA illustrates hardware software commercial increase chip SRAM storage memory cmem concern dsa compiler memory limit memory improve perf capex hurt perf TCO despite aim inference multi tenancy rapid growth dnns superior efficiency HBM TPUvi HBM TPUv nevertheless SRAM efficient dram data structure inference slo limit latency batch latency limit model production apps however recent dsa redefine latency limit batch slo production apps mlperf inference benchmark batch recent TPUs slo clearly datacenter application limit latency batch future DSAs advantage batch production workload mlperf average batch despite stricter latency constraint backwards ML compatibility performance portability training inference google internal model pre tune contrast mlperf inference model gpus tune TPUs production mlperf dnn batch dnn batch dnn batch mlp rnn resnet mlp rnn ssd cnn bert GNMT cnn bert latency limit batch picked TPUvi lesson TPUvi importance leverage prior compiler optimization backwards ML compatibility plus benefit reuse earlier hardware TPUvi TPUv  core per chip systolic MXU array vector per core compiler vector memory compiler dma access HBM avoid repetition insight concentrate difference TPUv interested detail TPUv reconsider strategy building chip optimize training inference resource limited TPUv TPUv concurrently nvidia release training inference chip sequentially pascal gpu volta likely vector memory MB memory cmem TPUvi expand memory hierarchy reduces access slowest efficient memory picked MB knee curve performance reasonable chip amortize chip significant TCO cmem TPUvi aim inference closer TPUv TPUv dimensional tensor dma memory architecture critical dnn accelerator maximize performance workload flexible future model TPUvi contains tensor dma distribute throughout chip uncore mitigate impact interconnect latency challenge tensor dma function coprocessors fully decode execute  dma instruction feedback XLA usability performance TPUv dimensional stride DMAs motivate development dimensional triple stride tensor dma architecture TPUvi compiler compatible binary compatible tpu chip tensor dma arbitrary per stride positive negative stride distance dimension inner vector TPUvi memory native lane vector inherit TPUv facilitates efficient HBM access interconnect described stride parameter independently programmable source destination dma feature offloads  enable memory granular tensor  scatter  transfer architectural memory chip across chip chip host dma similarly operation emulate dimension multiple DMAs thereby reduce ISA encode dma complexity software sensitive dma bandwidth latency secondary concern compiler issue DMAs DMAs flight dma bandwidth independent chosen stride parameter predictable performance goal effective compiler optimization maximize predictable performance simplify hardware software TPUvi unifies dma architecture across local chip remote chip chip host host chip chip host transfer simplify application chip retains essence TPUv relaxed dma model built around explicit software synchronization unrolled dma writes completely unordered within dma across DMAs chip memory concurrently access DMAs load chip HBM access DMAs concurrent overlap address DMAs load instruction explicit core dma synchronization avoid memory TPUvi chip diagram architectural memory HBM memory cmem vector memory  scalar memory SMEM instruction memory  data matrix MXU vector processing VPU lane   sequencer TCS uncore everything chip interconnect OCI ici router  ici link stack lst HBM controller  unified host interface  chip manager mgr TPUvi chip floorplan cmem OCI  floorplan dimension overall layout dominate  cmem SerDes location  cmem arrangement derive TPUv floorplan hazard TPUvi synchronize partial completion progress DMAs  hide dma ramp ramp latency becomes useful future compiler optimization dnns tighter workload latency constraint custom chip interconnect OCI rapidly evolve dnn workload driven tpu uncore towards flexibility generation component TPUs memory bandwidth increase component grows approach becomes expensive significant rout resource choice communication TPUv  access HBM local memory ici access HBM split imposes limit software chip future TPUvi chip interconnect OCI connects component topology component OCI particularly important addition cmem choice allocate transfer data HBM cmem  evolve wider datapaths typical soc native access instead cache HBM bandwidth per core increase TPUv similarly significant increase future handle inspire numa memory advantage spatial locality access minimize latency bisection bandwidth numa boundary core inside core entire memory accessible component bandwidth memory HBM cmem  physically partition optimize HBM access correspond memory OCI minimal overlap effectively non overlap network GB HBM bandwidth network GB locality reduces latency wiring resource simplifies interconnect arbitration split essential wiring resource logic arithmetic improvement another decision arithmetic quantization importance backwards ML compatibility retain bfloat TPUv despite aim inference application quantize TPUv easily TPUvi TPUvi int XLA colleague handle twice MXUs TPUvi TPUv logic improve advanced technology node afford MXUs equally important cmem VLIW instruction extra handle MXUs cmem scratchpad memory easy binary compatibility TPUvi instruction wider TPUv reduce latency systolic array MXU minimize sequentially float multiplication previous partial sum series input adder TPUvi sum multiplication previous partial sum series input adder optimize addition critical systolic array latency baseline approach adopt input sum recognize opportunity optimize component building custom input float adder eliminates normalization logic intermediate although numerically equivalent eliminate increase accuracy summation logic fortunately difference versus input adder affect ML meaningfully moreover input adder relative series input adder reduce overall MXU peak directly impact TDP MXUs dense component chip rate TDP inference reduce TCO rate ghz chip TDP closer TPUv TPUv ici headroom future dnn growth TPUvi ici link chip per access nearby chip memory quickly via model partition TPUv ici link apps software stack matures dnns TPUvi chip ici workload analysis feature building upon lesson TPUv TPUvi extensive trace performance counter hardware feature particularly uncore software stack analyze bottleneck user workload continuous compiler application optimization feature increase worthwhile aim perf TCO perf capex feature enable significant performance improvement boost developer productivity lifetime dnn workload evolve scenario server latency constraint offline batch inference task without slo int resnet ssd NMT nvidia int dnn nvidia mlperf inference code google datacenters explains mlperf inference slows google datacenters TPUs benchmark TPUvi average TPUvi perf TDP although NMT perf TDP DSAs compute float performance per average instead TDP TPUvi NMT resnet ssd geomean ssd depends  suppression involves operation memory intensity gpu coalesce memory likely faster tpu HBM backwards ML compatibility TPUvi google int perf TDP performance performance watt production apps relative TPUv TPUs TPUvi performance analysis performance perf TDP TPUs relative TPUv production inference apps TPUv TPUvi faster TPUv TPUv  TPUv core cooler TPUvi TPUvi perf TCO deployment TPUvi perf TDP TPUv combination FLOPS SRAM capacity MB dram bandwidth GB TDP microarchitectural MXU per core improve utilization technology upgrade improves efficiency transistor density enable FLOPS SRAM perf TDP factor cmem gain contribute others contribute remain accelerator predicts perf TDP across dsa generation increase transistor semiconductor node TPUvi delivers TPUv perf TDP transistor performance perf TDP TPUv TPUvi relative nvidia mlperf inference benchmark discus mlperf datacenter performance performance TDP relative TPUv datacenter int resnet ssd NMT TPUs maintain backwards ML compatibility TPUv mlperf inference omits NMT mlperf inference code code resnet ssd unofficial verify mlperf performance depth cmem benefit cmem TPUvi mlperf inference benchmark average benefit offline server relationship latency latency limited server performance nonlinear utilization understand cmem impact production application performance HBM bandwidth cmem disabled standard HBM bandwidth cmem enable compiler flag disabled cmem TPUv core disabled HBM bandwidth core HBM bandwidth apps cmem gain benefit roofline model explain application compute bound memory bound operational intensity application relative roofline  operational intensity FLOPS per byte memory access memory bound compute bound roofline model normally application compute bound dram bound recursively cmem bound cmem bandwidth GB bandwidth GB unlike HBM simultaneously traditional dram roofline HBM along  cmem cmem production application vertical performance gain per app cmem cmem bert HBM footprint MB cmem however XLA allocates parameter HBM reduce context switch prefetches cmem HBM traffic operational intensity without cmem cmem HBM  speedup bert embed indexed memory access buffer MiB random access HBM performance cmem MiB input cmem random access bandwidth significantly faster yield overall model performance gain bert HBM footprint MB cmem prefetching cmem HBM traffic speedup explain embed MiB account without cmem cmem faster rnn HBM footprint MB cmem nevertheless cmem filter impressive HBM traffic model iteration gru layer intermediate tensor cmem avoid costly HBM traffic filter traffic cmem model cmem bound cmem explores impact cmem MB apps mlperf server benchmark app average performance MB MB MB MB mlperf MB MB MB MB reduce cmem impact dnns perf TCO orientation growth dnns cmem unverified mlperf inference impact cmem cmem performance HBM bandwidth cmem relative cmem standard HBM bandwidth roofline model apps without cmem cmem operational intensity OI operation memory access HBM cmem OI relative HBM cmem increase OI application speedup TPUvi without cmem interpretation benefit cmem memory bandwidth application cmem cheaper easy perf TCO architect perf TCO dsa entire lifetime explain TPUvi perf TDP commercial inference DSAs related entry datacenter inference chip mlperf inference server benchmark inference chip symmetric multiprocessor relatively GDDR dram GB ecc standard rate ghz turbo mode ghz float integer  primary omission prevents backwards ML compatibility TPUv training lack TPUvi chip closest meeting google habana goya moderate inference chip VLIW simd vector core ghz attach relatively ddr memory perf TDP ssd benchmark int omits bfloat backwards ML compatibility training TPUs lesson importance multi tenancy dnns goya choice google datacenters intel  nnp inference chip VLIW vector core int MACs ghz relatively LPDDR memory perf TDP resnet benchmark goya omits bfloat relatively dram memory worry backwards ML compatibility plus multi tenancy dnn growth nnp zebra core xilinx  fpga moderate ghz relatively ddr dram perf TDP resnet deployment concern google goya perf TDP plus zebra exclusively cnns finally alibaba  core chip moderate ghz core tensor pool memory executes CISC instruction perf TDP resnet unnormalized performance faster MB SRAM closest lack bfloat issue  heel  dram whatsoever mention pcie switch gang multiple chip increase capacity chip multi tenancy expensive unclear dnns rapid growth percent MB cmem varies MB apps mlperf inference server code perf TDP TDP core goya  zebra  ghz MiB SRAM dram GDDR ddr LPDDR ddr none DSAs mlperf inference goya   zebra server scenario goya ssd resnet performance relative mlperf inference TDP per chip per latter unavailable GB dram  none TPUv unverified mlperf inference benchmark google memory ecc nvidia mlperf TPUvi unverified mlperf datacenter idle mlperf ecc rate inline ecc memory bandwidth machine idle beforehand TPUvi unchanged ecc reminds activation storage TPUv initial buffer allocation scheme MB eventually integer linear program solver effectively triple memory memory relatively weak initial compiler satisfy apps TPUv demonstrates interplay compiler quality chip transistor watt TDP mlperf inference resnet server faster ssd server faster perf TDP within TPUvi candidate discussion benchmark measurement utilization peak FLOPS versus roofline average peak FLOPS TPUs increase MXUs leverage technology node despite MXUs per core TPUvi MXUs occupy useful metric peak FLOPS roofline limited memory bandwidth FLOPS dnn arithmetic intensity generally increase without cmem TPUvi FLOPS roofline tpu TPUv TPUv TPUv TPUvi MXUs chip MXUs FLOPS utilization HBM roofline util average utilization peak performance roofline production application correlation TCO TDP dnn DSAs TCO versus TDP google datacenters chip CPUs gpus TPUs across generation definition correlation coefficient statistician correlation perfectly linear linear correlation moderate weak percent explains variability dependent variable TDP explains variability TCO DSAs explains TCO variability processor future dsa report perf TDP chip lifetime turbo mode perf capex versus perf TCO warns critical performance turbo mode faster mlperf inference chip  fully idle rate mlperf ghz thereafter chip ghz others variation presumably due program operational intensity mlperf datacenter environment plus ecc optional mlperf inference impact mlperf inference ecc enable performance mlperf inference ecc TPUvi google purposely provision datacenters TPUvi latency constant extra memory ecc saving TCO latency maximize average perf TDP temporarily faster latency TPUs omit technique turbo mode cache surely latency probably latency opex provision electricity already provision improve TCO topic TPUvi software multi tenancy chip SRAM XLA compiler allocates dram prefetches cmem execution reduce context switch multi tenancy allocate cmem reload task software stack reload cmem apps prefetch bert microsecond MB GB load HBM borderline acceptability compiler binary compatibility gpus TPUs CPUs backwards binary compatibility VLIW DSAs TPUs software approach TPUvi nvidia ptx virtual instruction nvidia promise ptx compatibility across gpu generation programmer ptx instead cuda XLA operation LLO closest analogy ptx programmer LLO guarantee LLO code future TPUs tpu developer performance without resort LLO cod benchmarking TPUvi versus nvidia TPUvi instead TPUv hungry expensive chip mismatch inference tpu ASIC package HBM stack TPUv swamp tpu ASIC quantization aware training quantization downside training quantization another approach quantization aware training integer training switch without quantization basically developer backwards ML compatibility integer data challenge motivate quantization aware training developer memory footprint performance DSAs integer arithmetic faster float issue harder training integer versus float application dennard TCO become strongly correlate dissipation google TCO TDP correlation coefficient dnn DSAs collection CPUs gpus TPUs update horowitz influential operation technology model SRAM dram recent logic become denser faster SRAM weakly SRAM denser memory access dominates perf TDP hence contrary ML developer community convention minimize FLOPS memory access intensive dnns datacenter context reduce precision FLOPS relatively comparison memory reference moore diminish dennard hardware software dnn dnn DSAs vault accelerator conclusion google developed deployed generation inference tpu datacenters google creative ML application developer economic reality inference training roadmaps widespread adoption across google explosion ML application developer developer velocity enable automate model multiple per google roadmap lesson DSAs dnns dnn DSAs specifically TPUvi logic improves quickly SRAM TPUvi MXUs per core TPUv TPUv leverage exist compiler optimization TPUvi evolve TPUv instead brand ISA perf TCO instead perf capex TDP cmem HBM backwards ML compatibility enables rapid deployment dnns TPUvi avoids arithmetic TPUv XLA compiler perspective inference DSAs global ghz lower TDP inference apps float arithmetic int quantization optional production inference normally multi tenancy TPUvi HBM capacity multiple tenant dnns annually memory compute dnn growth TPUvi MXUs  chip memory ici link adjacent TPUs dnn workload evolve dnn breakthrough programmability software stack pace dnns inference slo latency batch backwards ML compatible training tailor dnns TPUvi yield batch throughput  application restrict batch acknowledgment author built analyze involve contribution others thanks hardware software TPUs david  jeff dean     levy alex     shao feedback  agrawal        bhatia  oliver   carpenter andrew casper clifford chao  chen  chou william   dong  gan   peter    ben  rus gibbon         nil   benjamin  david haskell blake  matthew      huang michael hsu adam  mike     iyer  jacob             kwon  kim andy koch alan   kumar alice kuo steve   lang      stephen    tao liu kyle    david  seth   mueller david       nai   andrew  alexander nguyen  nguyen          park     ram babu  andy    guru  andrew  paul     russo   amir   sander   chris       singh     dan  jim  qian  tan hua tang   alex  ani      jack   wong chan  jung yang  yang  yuan sara   zhang zheng