neural network become ubiquitous mobile application mobile device generally immediate response maintain user privacy demand device machine technology increase nevertheless mobile device suffer restrict hardware resource whereas neural network involve considerable computation communication therefore implementation neural network specialized hardware accelerator generally neural processing npu gain attention mobile application processor AP however NPUs commercial mobile AP challenge realize simultaneously execution application efficient performance propose flexible efficient npu architecture samsung flagship mobile chip soc implement efficient npu efficient  utilizes input feature sparsity propose configurable mac array enhance flexibility propose npu dynamic internal memory assignment maximize chip memory bandwidth utilization efficient architecture mixed precision arithmetic implement propose npu samsung library silicon measurement demonstrate propose npu achieves fps TOPS execute quantize inception model npu core addition analyze propose zero skip architecture detail finally finding lesson implement commercial mobile npu avenue future index neural processing neural network accelerator sparsity mixed precision configurable introduction application rely neural network recognition  vision task  vision personalization become popular mobile device mobile application isca program generally response feature privacy preservation explosion demand device machine technology however mobile device suffer restrict budget memory bandwidth compute resource whereas application necessitate substantial computation memory communication execute neural network challenge execute neural network efficiently purpose mobile processor optimize neural network moreover worsens neural network become deeper achieve accuracy trend recent research recently dedicate hardware accelerator neural network neural processing npu extensively efficient execution neural network neural network acceleration neural network characteristic dataflow sparsity quantization mainly utilized quantization reduce data width enable hardware accelerator accommodate computational memory resource within chip utilize sparsity npu improve performance efficiency skip ineffectual computation zero addition application prune technique increase portion zero sacrifice acceptable accuracy maximize sparsity gain sacrifice acceptable accuracy npu accelerate application immediate response sensitive accuracy camera filter realtime video communication unfortunately commercial mobile application processor AP execute comprehensive neural network satisfy various user requirement instance application exist mobile device accuracy sensitive throughput sensitive adopt precision arithmetic alone throughput sensitive application throughput requirement stringent minor accuracy loss acceptable optimize however performance quality accelerator precision arithmetic degrade execute application accuracy sensitive application fail constraint addition mobile application various developer application optimization restrict extra effort stage prune critical achieve flexibility efficiency npu architecture commercial mobile AP propose npu aim achieve efficiency flexibility commercial mobile AP architecture propose npu scalable core MACs novel architecture methodology achieve flexibility execute neural network efficiently propose npu summarize effort realize efficiency efficient computation mac inner samsung library efficient inner utilize computation utilization input feature sparsity propose sparsity advantage sparsity input feature induced nonlinear function return zero relu observation channel wise apply efficient sparsity inner effort achieve flexibility configurable mac array apply reconfigurable mac array input output channel inner propose npu maintains utilization various layer suitably configure data mac array dynamic assignment utilization chip memory bandwidth bandwidth input feature partial sum layer apply dynamic assignment completely utilize chip memory bandwidth instance accelerate mobilenet input feature bandwidth performance bottleneck propose dynamic assign input feature relieve bandwidth bottleneck efficient mixed precision propose npu adopts optimize integer efficiency however integer arithmetic consume multiple cycle without additional accumulator addition lesson promising avenue future considerable NPUs commercial mobile AP II background efficient npu approach propose develop efficient NPUs target mobile environment eyeriss classify conventional dataflows dnn processing reuse data stationary compute multiple cycle analyze dataflows stationary dataflow dedicate accelerator architecture improve energyefficiency dnn execution quantization effectively exploit npu architecture quantization sensitivity varies network layer optimal width quantization layer fully utilize advantage quantization various NPUs mixed precision arithmetic propose stripe bitwise compute temporally series accumulation shift operation fusion propose width processing construct arithmetic width spatially another utilize characteristic dnn computation involves skip ineffective operation multiplication zero input feature performance improvement  dependent proportion zero data prune propose increase zero prune necessitate retrain complement accuracy reduction however npu commercial soc neural network model render adoption prune npu architecture inappropriate feature sparsity relu popular nonlinear activation function generates numerous zero input feature processing mobile soc exploit input feature sparsity improve efficiency adequate hardware although efficiency npu enhance exploit data sparsity complicates dnn execution replace fix processing variable zero data distribution application zero skip input feature utilization MACs due complex render npu performance sensitive network characteristic therefore tradeoff relationship data sparsity efficiency explore commercial mobile NPUs architecture requirement dnn processing dnn model achieve remarkable performance various model diverse model parameter application target performance crucial model parameter layer multiplier adder register multiplier adder register mac computation adder  inner depth channel feature data volume connection layer layer resnet introduce residual alleviate vanish gradient backpropagation previous model mobilenet depthwise separable convolution technique effectively reduce model operation despite diversity model principal operation dnn processing inner addition network gain importance transformer model dominates application convolution crucial image processing application inner data reuse scheme context various dnn model efficient manner npu architecture optimize perform inner operation verify exploitation fix integer compute instead float achieve accuracy traditional model image classification alexnet vgg mobilenet dnn model complex exhibit considerable accuracy degradation integer int hence mobile npu aim realize efficiency flexibility  STRATEGIES motivation strategy propose architecture overhead accelerate neural network without sacrifice quality various approach sparsity dynamic voltage frequency DVFS dynamic quantization propose approach optimize neural network execution runtime dynamic additional overhead particularly flip flop increase transistor reduces performance improvement realize additional feature accompany excessive consumption mac computation adder inner multiplier input data component mac adder register multiplier adder dilute due overhead focus achieve considerable performance improvement minimize overhead realize objective propose sparsity aware architecture analyze tradeoff hardware overhead performance propose sparsity aware architecture variant detail vii increase overhead demand reduce accumulator increase illustrates adder inner mac computation multiplier mac accumulator flip flop whereas multiplier  accumulator although independent accumulator assign multiplier render mac flexible additional overhead inner implement efficient inner computation propose additional feature achieve flexibility inner consumption propose inner mac multiplier implement samsung library consumption propose inner reduce mac resource utilization NPUs mobile device execute application various layer efficiently execute neural network inner layer 1D tensor assign input computation computational resource utilization depends combination hardware configuration adder tensor strategy mapping tensor computation realize resource utilization however  suffer utilization due lack flexibility static adder execute various tensor maximize resource utilization propose architecture various tensor propose reconfigurable mac array enables dynamic tensor mapping strategy runtime thereby compromise efficiency flexibility propose architecture described  subtile tile input feature output feature distribution model convolutional layer excellent strategy mapping tensor computation npu suffer utilization degradation bandwidth computation memory insufficient execute neural network computation generates data input feature output feature communicates memory data designate data fetcher chip memory maximum bandwidth data fix sum chip bandwidth data traffic dominant data idle data transfer occurs designate overall memory bandwidth waste completely utilize overall memory bandwidth runtime propose chip memory dynamic data fetcher fetch arbitrary data detailed mixed precision requirement commercial mobile device comprehensive application satisfy various requirement however simultaneously satisfy representative requirement throughput accuracy instance although quantization promising achieve throughput restrict resource accompany accuracy reduction demand mixed precision arithmetic satisfy various requirement observation integer arithmetic sufficient mobile application optimize propose npu integer arithmetic addition cope application accuracy propose npu integer arithmetic multiple cycle without additional accumulator described IV architecture distribution model distribution model npu computes convolutional layer propose npu computes entire layer iteratively perform partial computation overall execution propose npu summarize convolutional layer output feature along channel spatial dimension generate partial computation tile tile assign npu npu similarly computation tile multiple subset  computation subtile partial sum output feature subtile chunk input feature along input channel dimension cst input assign subtile 1D tensor subtile npu generates partial sum tensor partial output feature tensor input feature tensor input feature tensor cst tensor tensor cst tensor compute accumulate generate partial sum tensor architectural parameter cst multiple respectively partial sum tensor compute subtile  compute tile propose npu iterates tile layer generate output feature input feature layer overall architecture computation propose npu core illustrates core core NPUEs NPUE NPUE equip MACs chip scratchpad memory npu controller orchestrates data transfer chip scratchpad memory external memory NPUE comprises data fetcher tensor vector employ stationary dataflow maximize reuse subtile fetcher load associate tensor local buffer buffer cycle ifm fetcher load associate input feature tensor scratchpad memory load input feature tensor sparse data zero skip ineffectual computation associate zero input feature sparsity generates dense tensor maximizes nonzero data zero skip dense tensor transfer mac array  generate partial sum tensor npu core NPUE MB chip scratchpad memory external memory data fetcher fetcher psum fetcher sparsity controller NPUE ifm fetcher npu controller NPUE sparsity controller controller controller MAA ifm reg mac array tensor vector buffer MAA MAA MAA ifm ofm psum MAA output NPUE activation function buffer NPUE NPUE quantization buffer buffer buffer MAA output NPUE ifm psum fetcher architecture propose npu core comprise NPUE NPUE chip scratchpad memory data fetcher tensor vector NPUEs npu core communicate directly tensor output NPUE vector input NPUE implement configurable MAA  input feature tensor input feature tensor cst MAA generates partial sum tensor along output channel dimension partial output feature tensor accumulation partial sum tensor transmit vector performs wise operation nonlinear activation function data fetcher data fetcher static fetchers ifm fetcher fetcher psum fetcher handle input feature partial sum respectively unlike static fetcher handle designate data dynamic fetcher refer ifm psum fetcher load arbitrary data input feature partial sum fetchers buffer sufficiently avoid stall due scratchpad memory delay data load fetcher parallel bandwidth byte ifm fetcher scratchpad memory byte propose npu utilizes input feature sparsity ifm fetcher bandwidth twice data quantity  per cycle alleviate utilization load partial sum variable width psum fetcher adopts  assume input data correspond partial sum cope accumulation ifm fetchers load cycle byte cycle load correspond partial sum byte data psum fetcher load per cycle transfer tensor cycle byte fetchers cycle byte fetcher cycle fetcher  subtile computation subtile psum fetcher  partial sum partial sum tensor computation partial sum tensor overcome load latency partial sum buffering tensor sparsity nonzero input feature correspond partial sum sparsity controller generates nonzero flag information input feature zero  mechanism sparsity generates appropriate dense data transfer  input feature  tensor MAA MAA sparsity broadcast nonzero input feature  reduce data traffic propose npu bypass sparsity execute pool layer propose zero skip mechanism described mac array MAA illustrates architecture propose MAA tensor  comprise mul mul multiplier adder accumulator  achieve throughput reduce bandwidth sparsity input feature across mul arrow annotate ifm lane horizontal MACs input feature define lane cycle MAA load input feature tensor mul col IFMs mul col mul col OFMs ofm ofm ofm adder ifm reg dual adder sparsity accumulator adder mul output acc acc sel flag dual adder accumulator ifm lane architecture propose MAA MAA mul comprise multiplier adder accumulator denote dual adder accumulator mul input feature tensor performs inner generate partial sum tensor propose MAA configurable achieve utilization flexibility zero skip architecture suffer load imbalance due difference effectual computation computation reduce utilization alleviate load imbalance lane mul equips input adder accumulator propose architecture accumulates partial sum output pixel adder acc addition propose architecture mixed precision efficiently MAA optimize arithmetic propose MAA perform operation consume multiple cycle without additional accumulator detailed vector activation function receives output tensor tensor performs vector wise operation wise summation output NPUEs compute nonlinear activation function activation function performs linear algebra wise linear approximation output tensor tensor scalar propose npu activation function relu variant sigmoid auxiliary arithmetic function implement vector activation function assign output pixel computation cycle compute output feature activation function operates multiplexed manner instance activation function output pixel cycle cycle   input lane lane lane lane lane lane lane intra lane nonzero steal conflict inter lane nonzero steal lane priority algorithm hardware friendly zero skip mechanism priority algorithm conflict lane load nonzero input conflict resolution priority algorithm lane lane zero assume conflict occurs lane load nonzero input precedes lane priority priority lane lane lane load nonzero input consume output feature propose activation function generally receives 1D output feature tensor along channel dimension channel wise quantization apply without reshape tensor quantization reduce consumption propose npu bypass activation function quantization partial sum output feature vector buffer partial sum transfer data scratchpad memory  FEATURES sparsity utilization employ intra lane inter lane skip zero input feature  nonzero assign cycle apply intra lane lane perform effectual computation assign nonzero feature consume however mul MAA generate partial sum output feature MAA proceeds accumulation lane MAA consume input performance gain achieve zero skip rely intra lane varies nonzero lane execution MAA straggler lane mitigate apply inter lane additionally expands steal nonzero input feature lane propose inter lane inspire  skip propose unlike skip zero hout hin cout wout input feature output feature tile NPUE tile NPUE hin cout tile NPUE tile NPUE hout wout input feature output feature cout wout hout input feature hin output feature config config config configuration NPUEs tile subtile assign config  tile config independent tile spatial dimension config independent tile channel dimension tensor assign NPUE NPUE NPUEs respectively aim utilize input feature sparsity commercial mobile device environment skip zero beneficial accompany prune however model commercial mobile device developer without prune advantage sparsity practical usage propose utilize sparsity input feature generate nonlinear function relu return zero negative input distribution nonzero feature unpredictable execution sparsity nonzero runtime implement specific hardware nonzero lane within unfortunately overlap perform inter lane render hardware complicate tackle propose priority algorithm enables skip zero input feature hardware friendly manner propose priority algorithm intra inter lane illustrates propose priority algorithm conflict lane intra lane inter lane lane zero zero coordinate coordinate precisely assume lane zero coordinate lane propose sparsity controller nonzero data lane utilizes data lane utilizes data instead zero illustrates propose  algorithm conflict occurs assume lane lane zero zero coordinate nonzero data conflict occurs nonzero data lane lane denote conflict lane priority lane precede conflict lane lane lane steal nonzero assign inter lane lane nonzero assign cycle instead intra lane inter lane extra mux circuit handle overlap overhead increase increase inter lane optimize performance propose sparsity experimental detailed analysis performance various intra inter lane vii configurable mac array improve mac utilization propose inner propose configurable mac array dynamically connection tensor vector implement configurable mac array execution propose npu tensor vector within NPUE across NPUEs inter connection denote NPUE NPUE subtile assign NPUE implement operating mode configuration assign tile subtile multiple NPUEs depicts operation configurable mac array assign tile subtile NPUEs illustrate config NPUEs responsible processing  tile partial sum output assume NPUE accumulates output feature partial sum generate  assign NPUE load NPUE NPUE denote NPUE partial sum sum vector input adder NPUEs simultaneously operating input adder mac utilization degrade input channel multiple mode config NPUEs responsible processing tile along spatial dimension input output feature NPUE generates output feature along spatial dimension output tensor NPUE transfer vector parallel NPUE generates output feature along spatial dimension parallel mac utilization degrade width height multiple NPUEs core config NPUEs responsible processing tile along output channel dimension NPUE performs computation input feature hence NPUE performs execution parallel without communication NPUE generates output feature along output channel dimension NPUE generates output feature mac utilization degrade output channel multiple compilation propose npu selects operating mode layer configuration described enables propose npu achieve mac utilization dynamic assignment propose npu designate data fetchers data input partial sum output chip memory data data idle naively assign chip memory data inefficiency mitigate completely utilize chip memory bandwidth propose dynamic assignment redirects data suffer lack bandwidth idle focus partial sum bandwidth runtime partial sum bandwidth mainly load multiple  exist tile memory accumulation implement dynamic assignment ifm psum fetcher load chip memory MAA LH HL HH vector FH FL WH WL ifm ifm psum accumulate psum load psum concatenate FH FL WH WL FH FL WH WL WL WL FH FL WH FH FL WH LH HL HH simplify operation diagram computation fix arithmetic propose architecture multiplexing without additional computational resource data originally fix arithmetic arbitrary data input feature partial sum depth wise convolutional layer widely utilized mobile application representative benefit propose dynamic assignment depth wise convolution input feature reuse along spatial dimension unlike conventional convolution input activation reuse kernel channel dimension output feature hence execute depth wise convolution MAA distinct input feature unlike conventional convolution input activation broadcast MAA NPUE  distinct input tensor depth wise convolution ifm fetcher propose npu load input tensor cycle without dynamic assignment MAA utilization degrades due lack input feature bandwidth however propose ifm psum fetcher enables load extra input feature tensor cycle thereby fully utilize  mixed precision inference integer input int format improves latency consumption sacrifice acceptable accuracy image classification model however cope requirement precise without compromise accuracy propose npu inference integer input int format handle int input data minimum overhead propose multiplexing int arithmetic data originally int input data suitable  operation data fetcher vector propose  approach handle int input data without addition multiplier accumulator illustrates propose npu operation multiplexing manner int data input feature int format perform int arithmetic propose  byte input data decompose byte data FH FL WH WL multiple  LH HL HH perform rectangle byte  data fetcher load appropriate byte data tensor perform int arithmetic tensor generates partial sum accumulates significant byte byte data previous partial sum  iteration input feature int format iteration byte data input feature FL WL load tensor int multiplication perform byte data partial sum valid onchip memory iteration LH FL WH load tensor appropriate previous partial sum multiplication accumulate previous partial sum byte partial sum generate chip memory iteration perform similarly iteration FH WH load multiplication accumulation perform iteration generates output valid byte previous partial sum load byte data previous partial sum load accumulator sufficient computation byte data accumulate newly compute finally byte data previous partial sum concatenate accumulator output transfer vector VI experimental setup neural network model developed rtl implementation cycle accurate simulator verify advantage propose npu dnns inception resnet mobilenet addition performance silicon dnns mobilenet unet deeplab model imagenet dataset randomly image input measurement apply channel wise quantization feature bias integer prune technique apply simulation built cycle accurate simulator implement model performance propose npu simulator model npu core comprises NPUEs chip scratchpad memory detail architectural parameter simulation II utilized simulator II architectural parameter simulation propose npu npu ifm fetchers fetchers psum fetchers ifm psum fetchers sparsity mac array multiplier adder accumulator architectural exploration various sparsity baseline propose npu fix config described without feature sparsity described dynamic assignment described synthesis fabrication synthesize fabricate propose npu technology node samsung library synopsys compiler SP breakdown hardware component compiler report vii evaluation performance speedup simulator speedup various configuration feature zero skip baseline mac array configuration fix config described application intra lane zero skip along inter lane zero skip improves performance significantly skip inefficient computation accord experimental inception resnet mobilenet combine application intra lane zero skip inter lane zero skip realizes geomean speedup maximum speedup resnet intra lane zero skip inter lane zero skip apply intra lane zero skip alone geomean speedup maximum speedup inception propose configurable mac array improves performance significantly increase mac utilization demonstrates benefit configurable mac array dynamic assignment apply reconfigurable mac array dynamic assignment propose npu achieves geomean speedup maximum speedup resnet inter intra lane zero skip zero skip apply speedup speedup realize baseline mobilenet derives significant advantage configurable mac array dynamic assignment mainly dynamic assignment improves mac utilization depth wise convolution prevalent MobileNetv addition configurable mac array increase intra intra intra intra intra intra intra intra intra intra intra intra intra intra intra intra speedup baseline inter inter inter inter shift inception resnet mobilenet geomean intra intra intra intra intra intra intra intra intra intra intra intra intra intra intra intra speedup baseline inter inter inter inter shift inception resnet mobilenet geomean overall speedup various zero skip without configurable mac array dynamic assignment inter intra denote inter intra lane zero skip respectively shift denotes feature shift algorithm described intra intra intra intra intra intra intra intra intra intra intra intra intra intra intra intra normalize delay inter inter inter inter shift inception resnet mobilenet geomean normalize delay adp baseline performance silicon neural network model frame per fps inception mobilenet mobilenet resnet net deeplab utilization convolution input channel depth narrow discus configurable mac array vii imbalance nonzero feature input channel empirically application intra lane zero skip alone considerable performance improvement input feature shuffle across channel wise direction execution denote shift propose shuffle achieves geomean speedup maximum speedup mobilenet shuffle analyze detail performance silicon depicts performance npu core execute representative neural network model mobile device highly optimize fabricate soc various circuit  IV breakdown ratio npu core NPUE NPUE data fetcher tensor vector etc SRAM etc ratio tensor sparsity buffer multiplier adder accumulator etc  technique addition apply specific compiler optimization technique architecture configuration configurable mac array decompose stage mixed precision optimization technique tile layer fusion experimental silicon establish propose npu execute representative neural network mobile device efficiency normalize delay adp baseline propose npu efficiency critical implement efficient mobile soc adp implement synthesizable rtl model tensor various utilized report synopsys compiler propose npu intra inter lane respectively achieves minimum geomean adp implement silicon intra inter lane minimize adp satisfy bandwidth limitation chip memory realize minimum geomean adp propose npu fabricate samsung technology illustrates layout propose npu comprises npu core npu core npu MB chip scratchpad memory npu core achieves npu core npu core npu core npu soc layout propose npu fabricate samsung technology inception resnet mobilenet geomean mac utilization config config config oracle mac utilization mac array configuration efficiency TOPS IV report detailed breakdown npu core tensor occupies NPUE data fetcher fetch data chip memory dynamic assignment occupies vector occupies diverse operation configurable mac array configurable tile subtile scheme demand processing vector handle vector processing independent output tensor sparsity logic feature zero skip occupies NPUE tensor buffer arithmetic circuit multiplier adder occupy NPUE configurable mac array mac array utilization configuration introduce demonstrate advantage configurable mac array assume stall due lack memory bandwidth depict mac utilization fix config respectively oracle optimal mac array configuration offline compilation configurable mac array achieves geomean utilization maximum utilization resnet mobilenet overall mac utilization model due depth wise convolution   discus experimental suggestion npu  non zero ratio conv reduction reduction reduce nonzero portion input feature extract inception tions compromise performance mobile NPUs sparsity shift scheme propose npu achieves significant performance gain skip ineffectual computation sparsity inter lane zero skip drastically enhances performance enhance computational load balance lane however inter lane zero skip expensive due inclusion multiplexer nonzeros lane dispersion nonzero input channel depicts feature distribution extract inception nonzero ratio random channel sample layer inception depict easily zero ratio channel unpredictably imbalanced therefore instead inter lane shift scheme shuffle input feature across channel dimension shift scheme mitigates load imbalance lane enable efficient skip zero intra lane shift scheme inferior performance configuration applies intra  deserves consideration reduces logic complexity eliminate multiplexer depicts propose shift scheme sparsity utilizes intra lane cycle consume input tensor lane nonzero data shift scheme apply intra lane propose npu cycle consume input tensor assume kernel propose architecture input feature flatten tensor assign lane amount shift kernel coordinate ascend kernel shift amount ascend kernel shift shift additional hardware logic correspond fetcher load align shift input feature shift intra lane shift shift shift shift cycle lane lane lane lane shift scheme tensor lane intra lane zero skip shift scheme cycle whereas cycle without shift scheme although dynamic detection realizes performance static shuffle demonstrate efficient shift scheme inter lane zero skip optimize delay increase intra lane overhead increase inter lane render shift scheme effective performance comparable zero skip silicon intra lane network mac configuration configurability unnecessary hardware logic enable configuration enhance mac utilization mac configuration feature described adder vector output NPUEs config data vector feature activation enable config moreover replicate controller buffer NPUE config IV tensor occupy buffer mac array configuration fix config buffer NPUEs tradeoff mac utilization overhead mention however performance degradation sacrifice configurable mac array negligible whereas significantly reduce buffer effective increase parallelism spatial dimension dynamic chip memory assignment npu cannot memory bandwidth due conflict predict multiple request data access onchip memory simultaneously address additional memory dynamically assign input feature partial sum decision assignment dynamic assignment described maintain mac utilization amount data mac differs layer variety layer convolution layer width height feature reuse mac bandwidth layer width height feature layer considerable storage partial sum additional memory partial sum enables onchip memory bandwidth performance fully layer generally limited memory bandwidth load onchip memory additional memory feature allows memory bandwidth load IX conclusion crucial prerequisite develop npu architecture commercial mobile soc addition possess processing efficiency mobile environment npu flexible cope various application target performance therefore implement flagship mobile npu employ feature adder inner mixed precision zero skip ifm sparsity dynamic assignment configurable mac datapath evaluation silicon npu achieve peak efficiency TOPS fps resnet addition extensive simulation verify optimization boost factor dense baseline implementation evaluation additional analysis perform enhance npu performance finding inter lane static shuffle satisfactory compromise hardware complexity performance due uneven distribution nonzero ifm limit configurability reduce performance slightly however reduce significantly advantageous extra memory flexibility load ifm psum conflict chip scratchpad memory finding contribute improve npu architecture apply future development practical issue dealt lesson beneficial advancement mobile NPUs