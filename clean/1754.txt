prefetching instruction instruction cache fundamental technique performance computer efficient effective prefetcher timeliness coverage accuracy timeliness essential instruction increase risk instruction evict cache request instruction demand coverage important reduce instruction cache accuracy ensure prefetcher pollute cache interacts negatively hardware mechanism entangle prefetcher instruction  instruction maximize timeliness prefetcher instruction trigger prefetch subsequent instruction accounting latency cache prefetcher carefully adjust account coverage accuracy evaluation KB storage entangle increase performance outperform prefetchers index instruction prefetching cache entangle correlation latency introduction software service compute become increasingly popular server application exhibit notoriously instruction instruction cache LI LI rate therefore stall significant performance degradation addition wasteful expenditure underutilization resource processor traditionally scientific desktop application characteristic inefficient  instruction footprint server application accord conduct google warehouse compute  server machine workload service billion user processor useful stall analysis demonstrates stall instruction available european research council european union horizon research innovation programme grant agreement   research contract  execution server application cache memory server processor prime optimization target identifies significant LI bottleneck due instruction footprint server application rate per LI conclusion reinforce recent demonstrate instruction fetch considerable memory stall data access underline importance prefetching data indeed memory latency recognize critical factor performance plethora prefetching technique propose decade however research community focus predominantly data prefetching relatively research instruction prefetching despite increase importance application prefetch mechanism instruction prefetchers prefetchers arbitrary advanced propose prefetchers prediction execution prefetchers idle hardware resource fetch instruction ahead helper thread technique prefetching employ correlation namely building correlation memory reference previous memory reference instruction address exploit temporal spatial temporal prefetchers correlation prefetchers sequence cache predict future replay coverage accuracy predecessor incur impractical storage prefetchers interact hardware structure predictor BTB gain insight program execution ahead however intrusive processor typically prior prefetching adopt ahead mechanism UI OOVBM  PNQVUFS SDIJUFDUVSF acm annual international symposium computer architecture isca doi isca address coverage accuracy ahead prefetchers execution advance prefetch correspond instruction cache etc refer instruction function etc typically refer ahead distance nevertheless employ fix ahead distance prefetchers rigid cannot timely instruction ahead distance instruction unnecessarily pollute cache instruction evict ahead distance prefetch instruction demand processor ahead popular technique distance identify careful tune inspire previous proposal observation instruction fetch latency ahead distance timely useful prefetch illustrates timely prefetches fix ahead distance selection server workload IV ahead distance discontinuity akin previous proposal generate baseline without prefetching tracked LI latency dedicate structure detail LI compute discontinuity advance prefetch issue oracle identify optimal ahead distance percentage distance timely prefetches distance remain LI prefetching distance ahead distance fix statically akin suitable ahead distance dynamically observation perform phase remark ahead distance fix sub optimal distance distance execution proposal distance simultaneously address fix ahead distance across benchmark ahead distance prefetch LI timely manner application another ahead distance considerable cannot neglect effective prefetcher complement emphasizes prefetching pollution prefetchers lack accuracy fix lookahead distance application tolerate increase ahead distance without loose accuracy reduce accuracy distance departure proposal timeliness metric instruction prefetching demonstrate fix ahead distance coverage timely manner ahead distance discontinuity timely ratio timely prefetches respect ahead distance ahead distance discontinuity ratio useless prefetches respect ahead distance accuracy issue prefetchers useful approach performance ideal instruction cache LI propose entangle prefetcher instruction entangle prefetcher contrast predecessor around notion timeliness entangle computes latency cache  cache access trigger prefetch ensure timely arrival request instruction entangle robust effective agnostic application characteristic achieves LI rate approach perfect LI contribution observation significant LI cannot timely prefetched employ fix ahead distance proposes instruction prefetcher core timeliness demonstrate effectiveness selection benchmark proposes entangle mechanism identify prefetch cache fix ahead distance instead lookahead distance central prefetcher aware correlation LI handle LI learns prefetch associate timely performance orient version entangle instruction prefetcher instruction prefetch championship ipc code entangle prefetcher propose available http github com alberto  demonstrate entangle trigger timely prefetches deliver coverage accuracy ideal LI performance LI rate offering performance compression scheme clever encode organization yield entangle compact storage demand allot hardware budget II  prefetcher conceptual contribution entangle  entanglement instruction namely instruction upon execution trigger timely prefetch instruction concise representation define src entangle cache refer source trigger prefetch dst entangle cache destination request arrives timely ensure timeliness compute latency cache request cache enters cache timestamp cache latency cache previous access cache latency cycle access entangle prefetcher LI access cache src entangle cache fitting criterion entangle dst entangle cache cache src entangle access cache trigger timely prefetch dst entangle transform previous entangle cache program considerable storage requirement entangle prefetcher  define consecutive cache consecutive refers program instruction grouped cache therefore non consecutive cache access cache consecutive suffices efficient prefetching contiguous cache reduce entangle entangle prefetcher merges almost consecutive entangle prefetching trigger upon cache access prefetches entire cache  cache entangle mechanism versatile easily adapt multiple execution variation latency src entangle cache multiple dst entangle cache timely dst entangle cache dynamic execution access access access access access access  access access access access access access latency  entangle source destination latency cache BB consists access BB access marked bold illustrates access cycle access execute bracket occurrence access LI latency illustrates prefetch BB latency cycle access instruction execute latency cycle ahead access BB entangle BB access BB access organization cache link multiple src entangle cache timely prefetched regardless execution fluctuation latency entangle prefetcher constantly creates entangle discard longer useful latency fluctuation accommodate default source entanglement instruction precisely latency fetch target instruction effective implementation describes effective implementation entangle prefetcher implementation advanced technique improve accuracy reduce storage overhead finally implementation detail consideration implementation mention entangle prefetcher trigger prefetches access source entangle address prefetching within destination entangle address detail hardware mechanism identify LI timely prefetch source entangle hardware interaction structure depict compute compute dynamic fetch processor constantly cache fetch comparison LI access LI cache entangle prefetches PQ MSHR cache latency buffer update update update entangle destination overview entangle prefetcher hardware extension register circular queue entry information timestamp LI access entangle cache structure entry consists src entangle cache compress array dst entangle cache advanced optimization technique confidence counter associate destination LI PQ MSHR extend information timing timestamp request issue src entangle source entangle access access stem demand access prefetch access address plus indicates access non consecutive cache increase detect entangle core structure proposal information issue prefetches already entangle update maximum already decision increase coverage prefetcher extra false positive finally update register cache reset mechanism populates entangle source entangle building entangle timely prefetching suitable source entangle cache LI compute latency LI LI prefetch identify cache execute latency cycle source address LI destination entangle potential src entangle cache recent timestamp access LI circular queue buffer compute latency demand LI timestamps timestamp demand along entry allocate default status register MSHR additionally MSHR entry access demand pointer access buffer access similarly latency prefetches compute actual latency prefetches already prefetched cache extend prefetch queue PQ prefetch issue along currently allocate prefetch entry access PQ initialize prefetch cache automatically handle regular cache additionally ensure information PQ transfer MSHR entry allocate correspond cache otherwise information PQ entry discard subsequent demand MSHR entry allocate prefetch access unset simply enable access indicates prefetch access LI despite precede prefetch timestamp corresponds cache upon cache access MSHR demand prefetch addition entry valid pointer buffer corresponds entangle  attempt src entangle cache newly cached latency memory access compute cache timestamp MSHR entry LI source access latency cycle identify parse backwards buffer access without pointer buffer src entangle prefetching src entangle cache entangle update correspond src entangle entry newly cached dst entangle src entangle entry entangle destination dst entangle address multiple src entangle entry prefetching cache execution automatically fourth illustrates entangle entangle trigger prefetches entangle checked cache access entire access cache prefetched dst entangle entire dst entangle prefetched entangle parse dst entangle address prefetch MSHR demand access pref conf cache latency entangle conf max prefetch MSHR cache access access access timely pref conf prefetch MSHR cache access cache evict pref conf demand MSHR access cache latency entangle conf max action prefetch cache access eviction entanglement prefetches source buffer confidence counter increase prefetch decrease prefetches advanced technique optimization confidence latency variation latency cache cache fetch llc memory due contention access structure rout packet interconnection network therefore entangle timely timely occurrence adapt variation trigger timely prefetches confidence counter saturate counter entangle entangle entangle confidence maximum timely counter decrease upon prefetches increase upon timely prefetches confidence entangle invalid entangle array destination dst entangle confidence replace update confidence prefetch timely entangle trigger prefetch information access PQ MSHR prefetch LI MSHR LI cache prefetches detect cache access unset MSHR PQ timely prefetches detect cache access unset prefetches detect upon cache eviction access unset unnecessarily access evict depicts scenario information actually src entangle address dst entangle address prefetched hence src entangle address PQ accordingly MSHR LI implement version entangle prefetcher context information increase accuracy source replicate context overload entangle suffers frequent eviction consequently achieves performance moreover compression  dst entangle BLOCKS mode destination  context prediction opportunity report benefit context information markov  merge spatio temporal reduce entangle entangle storage overhead manageable perform merge quasi consecutive address overlap consecutive merge critical prefetcher employ budget entangle merge aim address scenario sequence access cache  cache prefetched another abc evict however substantial entangle  address issue buffer buffer inspect compute merge previous consecutive overlap address previous update become  merge dedicate merge perform cache compress destination entangle mode encode array dst entangle address confidence mode dst entangle address confidence destination encode significant  dst entangle significant differs src entangle significant infer source distance src entangle dst entangle typically destination highly compress mode indicates destination array dst entangle cache associate confidence significant destination destination encode virtual address cache confidence saturate counter detail available mode entry dst entangle array mode hence  entry insert compute maximum mode mode previously destination improve compression upon eviction dst entangle compute mode ensure unnecessarily restrict due destination longer exists finally maximize utilization entangle src entangle destination entry prefetcher src entangle namely cache timestamp earlier destination entry insert evict entry implementation detail execution arise execution pollute cache prefetches trigger training prefetcher entangle compute avoid issue prefetches trigger prefetches instruction retire issue entangle latency compute accounting instruction commit addition cache latency however prefetch retire disregard performance degradation significant LI usually plenty cache evict without increase rate entangle prefetcher commonly  destination recent timely source tolerates LI eviction fix ahead alternative avoid pollute information entangle speculatively compute structure destination instruction commits update entangle timing constraint trigger prefetches entangle maximum extra average IV perform entangle entangle destination entangle indexed xor operation address parallel retrieve prefetching information account latency prefetches timely regardless latency access entangle update critical issue prefetches memory overhead buffer entry circular queue tag timestamp register queue maximum therefore cache memory structure byte timing src entangle information along PQ entry MSHR entry LI cache entry timing information consists request issue access buffer src entangle information source entangle model entangle entangle entry respectively access resolve timing information longer LI cache  information memory timing src entangle information KB byte byte byte entry configuration respectively entangle associative cache source along maximum destination employ enhance fifo replacement policy information entry eviction reallocate another entry entangle entry configuration respectively per tag encode encode format destination confidence encode structure employ prefetcher KB entry KB entry KB entry entangle structure alternative unified entangle likely beneficial storage configuration future physical address recent architecture employ virtual cache consequence efficiently prefetcher virtual address however core employ virtually indexed physically tag cache prefetcher physical address likely alternative otherwise access perform prefetches critical pressure translation ahead buffer tlb although described virtual address perfectly physical address reduce storage requirement physical address prefetcher physical address compression mechanism adapt  address confidence mode dst entangle address confidence mode indicates destination array dst entangle cache associate confidence detailed II additionally buffer cache address instead virtual LI entangle prefetcher KB KB KB version entry respectively II compression  USING physical  mode destination  IV evaluation methodology evaluate entangle prefetcher modify version ChampSim simulator employ instruction prefetching championship ipc ChampSim version ipc model frontend modify version extends model develop ChampSim implement realistic decouple model fetch prefetching target buffer BTB target cache predict target indirect return address stack RAS prefetches issue fetch prefetching demand access hence baseline report prefetch request extend model processor stage pipeline described misprediction penalty stage flush stage misprediction detect processor memory hierarchy parameter aim resemble intel sunny  machine configuration parameter baseline consumption cache hierarchy consideration expenditure tag access writes cache model CACTI technology version ChampSim employ ipc LI prefetchers virtual address however LI prefetchers physical address training LI prefetchers physical address scenario guarantee consecutive virtual memory consecutive physical slightly reduce prefetcher coverage ChampSim simulate execution therefore prefetches issue consequently realistic implementation accuracy prefetchers reduce explain entangle prefetcher avoid pollution prefetchers evaluate benefit model future examine detail implication execution prefetcher secret trace championship prediction  qualcomm datacenter technology trace integer compute int float compute cryptography crypto server srv ipc subset trace evaluate prefetchers baseline configuration processor decouple width instruction fetch queue entry decode queue entry dispatch queue entry target buffer entry target cache entry return address stack entry penalty cycle decode stage predictor hash perceptron processor execute width instruction retire width instruction penalty cycle execute stage buffer entry load queue entry memory hierarchy LI cache KB cycle prefetcher cache KB cycle cache KB cycle spp dev cache MB cycle prefetcher dram GB byte channel MT workload ChampSim format workload MPKI per kilo instruction LI baseline configuration analysis perform benchmark additionally evaluate prefetcher variety benchmark behaviour performance application cloudsuite exhibit MPKI LI workload instruction evaluate prefetchers evaluate detail prefetching strategy pure prefetcher prefetches cache access overhead SNL memory efficient proposal implement vector cache access prefetched correspond prefetching cache useful KB storage mana refinement SNL dir BTB vector consecutive prefetchers previously propose pif performance representative  instruction prefetchers evaluate  configuration described  mana KB entry mana KB geometric ipc entry mana KB RDIP RAS instruction prefetcher return address stack context signature consult upon return operation trigger prefetching evaluate entry trigger prefetchers discontinuity vector consecutive cache storage KB evaluate ranked proposal ipc  refinement RDIP implement accurate context signature dual ahead distance mechanism generate prefetches evaluate KB entry storage KB  mma combine footprint  prefetcher multiple ahead mma prefetcher  enhance prefetcher estimate cache worth prefetch mma selects ahead distance evaluate entry storage KB epi performance orient hardly implementable previous version entangle prefetcher model highly associative structure entry buffer entangle entry storage requirement KB addition evaluate configuration effective entangle prefetcher entangle propose entangle prefetcher model entangle entry perform aggressive merge budget configuration merge distance buffer configuration respectively requirement compute KB KB KB respectively finally increase cache instead budget prefetching mechanism LI KB LI KB increase associativity LI respectively LI access latency cycle ideal instruction prefetcher LI cache return issue prefetches cache model pollution entail LI cache performance evaluate entangle prefetcher prefetchers metric coverage percentage LI prefetching accuracy percentage useful prefetches respect prefetches issue LI ratio instruction per cycle ipc indication performance evaluate proposal application simulation geometric instruction per cycle evaluate scheme focus prefetching technique KB storage prefetcher KB speedup geometric LI KB LI KB nextline SNL mana mana mana RDIP  mma  epi entangle entangle entangle ideal ipc memory requirement performance storage summarizes performance prefetchers geometric ipc obtain  workload normalize respect baseline along storage requirement medium budget configuration budget configuration ipc entangle prefetcher LI configuration ideal prefetcher entangle prefetcher achieves speedup respect baseline configuration KB entangle ideal LI cache speedup interestingly KB overhead entangle performance balance achieve performance improvement par demand proposal  mma KB overhead  KB epi KB budget configuration KB entangle performance improvement entangle outperforms budget configuration mana furthermore budget entangle version outperforms budget version mana ipc ipc normalize configuration without LI prefetcher across  workload normalize IPCs individually configuration medium budget configuration entangle outperforms prefetchers medium budget configuration entangle prefetcher ideal workload ideal prefetcher significant improvement respect proposal importantly entangle prefetcher performance degradation respect prefetcher clearly happens nextline prefetcher technique LI rate LI ratio  workload individually configuration label baseline configuration without dedicate LI prefetcher workload normalize ipc nextline SNL mana mana entangle entangle RDIP ideal ipc normalize baseline configuration workload rate nextline SNL mana mana entangle entangle RDIP rate instruction prefetchers entangle prefetcher significantly outperforms competitor across benchmark reduce drastically rate entangle prefetcher reduces rate entry rate entry evaluate prefetchers report rate entangle prefetcher approach ideal LI cache LI prefetcher coverage coverage ratio become prefetchers across workload individually mimic rate entangle prefetcher coverage prefetchers workload  coverage around entangle coverage workload contrast prefetchers coverage workload LI prefetcher accuracy accuracy ratio useful prefetches across workload individually entangle achieves workload coverage nextline SNL mana mana entangle entangle RDIP coverage instruction prefetchers workload accuracy nextline SNL mana mana entangle entangle RDIP accuracy instruction prefetchers accuracy workload accuracy almost workload RDIP accuracy workload mana workload accuracy entangle indicates efficient prefetcher prefetches issue cache llc memory generates useless traffic consumption accuracy representative indicator expenditure accurate prefetcher generates extra traffic memory respect prefetching non accurate LI prefetcher pollute LI access cache llc cache unnecessary request IV expenditure cache employ configuration entangle prefetcher accuracy prefetchers reduce consumption llc considerably average nextline accounting entry entry entry norm exec BB   ent  breakdown contribution performance extra LI access generate prefetches entangle prefetcher reduces overall consumption memory hierarchy configuration respectively contrast efficient technique RDIP prefetches therefore access however prefetches analyze entangle prefetcher detail average performance obtain isolate propose technique analyze configuration entangle prefetcher BB prefetches access source  extends BB prefetches dst entangle cache  extends BB prefetching  ent  cache cache finally  proposal extends  mechanism merge improvement entangle cache timely manner prefetching dst entangle  contributes extent improvement entangle tracked respect ent merge relevant prefetcher compression essential fitting reduce storage budget analyze compression ratio dst entangle cache format category workload  crypto int srv arithmetic standard deviation workload almost destination highly compress crypto int workload however srv workload nonnegligible destination cannot compress compression rate srv workload majority destination destination compress crypto int srv overall average destination entangle crypto workload srv workload finally compute prefetches issue entangle average cache currently access omit cache crypto int srv format ratio entangle entangle entangle compress format destination insert entangle crypto int srv average destination entangle entangle entangle average entangle destination crypto int srv average entangle entangle entangle average access entangle cache entangle destination omit cache respectively compute average trigger prefetches formula  destination  destination prefetches srv workload workload although dramatically entangle prefetcher accuracy evaluate prefetchers prefetcher benefit prefetch queue entry employ evaluation prefetches discard physical address evaluate prefetchers physical address  workload entangle prefetcher outperforms competitor achieve ipc improvement geometric  baseline IV average  cache NJ geometric  nextline SNL mana mana entangle entangle RDIP average LI average L1D average LC average llc geomean norm crypto int srv average entangle entangle entangle average entangle destination cassandra  normalize ipc nextline SNL mana mana entangle entangle ideal normalize ipc cloudsuite application entry entangle respectively trend virtual address application cloudsuite finally evaluate prefetchers benchmark specifically application behavior previous workload performance improvement entangle prefetcher application cloudsuite MPKI LI entangle prefetcher outperforms prefetchers evaluate related driven impact performance prefetchers evolve prefetchers complex technique described prefetching technique ahead execution advance prefetches correspond instruction cache etc recent competitive ahead prefetchers  mma combine footprint  prefetcher multiple ahead mma prefetcher  enhance prefetcher estimate worth prefetch mma identifies ahead distance combine technique predict nth LI fix ahead distance impact prefetcher accuracy efficiency technique adjust ahead distance dynamically heuristic propose however ahead remains fix cache execution entangle cache entangle prefetcher along correlation prefetchers markov prefetchers probability predict prefetch cache fix ahead distance prefetch predict  LI predicts cache cached accurate timely prefetchers proactive instruction fetch pif prefetcher improves performance capture cache access commit instruction instruction handler OS interrupt pif operates retire instruction instruction fetch sequence compute spatial locality technique rate evaluate benchmark incurs substantial storage overhead beyond limit evaluation capture context function  stack instruction prefetching RDIP return address stack context signature consult upon return operation trigger prefetching RDIP approach performance pif within significantly storage demand entangle significantly outperforms RDIP refine  accurate  signature improves performance predecessor RDIP entail memory overhead performance report prefetcher entangle correlate prefetchers markov ahead prefetchers  pif etc similarity correlate difference consists correlation built distance access etc correlation  timeliness latency express cycle previous correlation prefetchers fix ahead distance motivation cannot timely increase accuracy prefetchers interact prediction mechanism propose instance leverage target buffer BTB simultaneously  BTB instruction decode instruction avoid BTB achieve leverage information prefetcher without BTB storage overhead generally instruction prefetchers BTB BTB prefetches considerably hinder BTB significant processor attempt  BTB suffer BTB rate application instruction footprint server workload usually incur LI shotgun dedicates significant BTB unconditional conditional plus return instruction handle thanks dedicate storage shotgun remains ineffective workload rate due mechanism reactively pre recently propose SNL dis BTB lightweight prefetcher reduces storage demand SNL dis BTB classifies category tailor enhance NL prefetcher detect worthy prefetch dedicate sequential discontinuity prefetcher observation discontinuity introduce extra storage aim remain finally confluence pre BTB avoid BTB proposal competitive storage budget cannot fully leverage storage performance mana version budget brings performance improvement competitive entangle prefetcher overall BTB prefetchers highly sensitive BTB prediction accuracy entangle prefetcher operates target entangle hinder BTB cache correlation issue prefetch building correlation frequent execution entangle sensitive accuracy predictor mechanism execution multiple source destination confidence counter etc approach correlate cache instead focus prediction target address yield entangle robust sensitive prediction ofthe technique buffer additional hardware structure prefetch sequence successive cache target akin prefetcher primarily target coverage accuracy latency entangle bold target timeliness novel approach prof highly effective achieve accuracy predecessor VI CONCLUSIONS entangle prefetcher instruction alternative prefetching direction driven timeliness entangle estimate latency cache operation  instruction trigger prefetch ensure timely arrival request instruction sequence mostly sequential cache source sequence associate destination sequence cache access cache source sequence entangle generates prefetches source sequence associate destination sequence confident source destination cache latency sequence auxiliary structure earlier sequence latency cycle ahead sequence sequence destination sequence sequence cache granularity entangle subsumes prefetching novel compression scheme distance relationship source destination clever encode organization storage bay access prediction structure contention critical structure entail associative implementation entangle prefetcher highly efficient without intrusive processor robust effective agnostic application characteristic achieves LI rate approach perfect LI clearly outperform proposal offering performance