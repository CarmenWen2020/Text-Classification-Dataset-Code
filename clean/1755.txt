server CPUs cache llc critical hardware resource exerts significant influence performance workload manage llc performance isolation qos multi tenancy argue addition cpu core important llc management intel architectural innovation data DDIO directly injects inbound traffic llc instead memory summarize DDIO default DDIO configuration achieve optimal performance DDIO decrease performance non workload llc iat llc management mechanism treat citizen iat monitor analyzes performance core llc DDIO cpu hardware performance counter adaptively adjusts llc DDIO tenant demand llc capacity addition iat dynamically chooses tenant llc resource DDIO minimize performance interference tenant multiple microbenchmarks application demonstrate minimal overhead iat effectively stably reduce performance degradation DDIO index cache partition DDIO performance isolation introduction dominance infrastructure  IaaS data IaaS hide underlie hardware upper tenant allows multiple tenant physical platform virtualization technology virtual machine VM container workload collocation facilitates operation management achieves efficiency hardware utilization however benefit workload collocation multi tenant tenant contend hardware resource incurs severe performance interference hence carefully allocate isolate hardware resource tenant resource cpu cache llc access dram memory limited capacity MB critical handful proposal partition llc cpu core tenant hardware software recently intel resource director technology rdt enables llc partition monitoring commodity hardware cache granularity spur innovation llc management mechanism multi tenancy workload collocation however role impact intel data DDIO technology traditionally inbound data pcie device deliver memory cpu core fetch later however scheme inefficient data access latency memory bandwidth consumption advent device extremely bandwidth 0Gb network device nvme storage device memory cpu inbound traffic buffer overflow packet loss occurs DDIO instead directly steer inbound data llc significantly relief burden memory sec II processing latency throughput core DDIO llc ownership core cachelines meaningful intensive platform typically DDIO completely transparent OS application however sub optimal performance network traffic fluctuates workload tenant device contend core llc resource previously researcher identify leaky dma device buffer exceed llc capacity DDIO data forth llc memory  propose properly buffer  impact performance sec identify another DDIO related inefficiency latent contender sec without DDIO awareness cpu core assign llc DDIO incurs inefficient llc utilization incur performance degradation non workload deficiency pure core orient llc management mechanism necessitate configurability awareness DDIO extreme performance propose iat knowledge aware llc management mechanism iat periodically statistic core llc activity cpu hardware performance counter statistic iat determines finite machine FSM identifies contention core adaptively allocates UI OOVBM  PNQVUFS SDIJUFDUVSF acm annual international symposium computer architecture isca doi isca cpu integrate controller pcie device integrate memory controller llc core cache memory chip interconnect core cache conventional dma DDIO update allocate typical cache organization server cpu conventional dma DDIO device llc core DDIO alleviate impact leaky dma besides iat sort selects memory intensive tenant llc DDIO shuffle llc allocation performance interference core latent contender reduce develop iat user daemon linux evaluate commodity server bandwidth NICs microbenchmarks application workload apply iat scenario restrict performance degradation networking  application without iat degradation facilitate future DDIO related research enhance rdt library  DDIO functionality public http github com  iat  II background manage llc server cpu prior research llc performance interference collocate VM container motivates llc monitoring partition server CPUs xeon generation intel rdt resource management memory hierarchy rdt cache monitoring technology cmt ability monitor llc utilization core cache allocation technology assign llc core tenant programmer leverage technique simply access correspond model specific register MSRs library furthermore dynamic mechanism built atop rdt data technology conventionally memory access dma operation pcie device memory destination transfer device host data memory address designate device driver demonstrate later cpu core inform transfer fetch data memory cache hierarchy future core assign llc core allocate cachelines assign llc load update cachelines llc device OS hypervisor user stack container app virtio container app virtio aggregation PF VF VF kernel container app container app device slice model tenant device interaction processing however due dramatic bandwidth increase device decade drawback dma scheme become salient access memory relatively consume potentially limit performance data processing suppose 0Gb inbound network traffic packet ethernet overhead packet arrival rate mpps component controller core packet packet loss consumes memory bandwidth 0Gb inbound traffic packet memory easily 0Gb 5GB memory bandwidth consumption relieve burden memory intel propose cache access DCA technique device data directly cpu llc intel xeon CPUs implement data technology DDIO transparent software specifically cpu receives data device llc lookup perform cacheline correspond address valid cacheline update inbound data update inbound data allocate llc allocate dirty cachelines evict memory default DDIO perform allocate llc similarly DDIO device directly data llc data device memory allocate llc prior comprehensive persistent DIMM DDIO disabled enable DDIO improve application performance memory access latency reduce memory bandwidth consumption DDIO disabled inbound data cache immediately evict memory performance consideration coherence domain cache operation dependency perform although DDIO intel specific CPUs concept cache stash discussion applicable tenant device interaction virtualized server data adopt popular model organize device multi tenant virtualized server offs difference interact physical device model logically centralize software stack deployed device interaction OS  gbps proc rate mpps proc rate   gbps proc rate mpps proc rate  packet KB packet buffer entry buffer entry  rfc hypervisor user sdn compatible virtual switch vswitch OVS  developed NIC regard ssd  performance scalable user stack demonstrate software stack physical device sends receives packet tenant device via interface virtio traffic model software stack model aggregation model hardware input output virtualization SR IOV technique leveraged SR IOV physical device virtualized multiple virtual function VFs physical function PF host OS hypervisor bind VFs directly tenant host bypass switch functionality offload hardware tenant directly physical device data reception transmission model  hardware resource assigns tenant slice hardware offload multi tenancy intrinsically treat slice model motivation impact llc leaky dma leaky dma multiple default llc DDIO allocate inbound data rate NIC rate rate cpu core data llc likely evict memory newly incoming data later llc core significant packet flight packet packet consume cache packet hence incurs extra memory bandwidth consumption increase processing latency packet eventually performance  author propose reduce buffer however workaround drawback environment vms container couple physical virtualization stack entry buffer maintain default DDIO llc capacity VM container shallow buffer SR IOV setup container assign virtual function overlap DDIO overlap overlap DDIO overlap avg latency MB  MB MB throughput latency DDIO mem performance traffic guarantee buffer accommodate default DDIO cache capacity MB buffer entry shallow buffer severe packet issue bursty traffic ubiquitous service hence statically balance traffic dynamically imbalanced traffic hitter container incur performance demonstrate inefficiency sec VI detail setup dpdk  application core testbed traffic rout header network packet emulate traffic packet rfc maximum throughput zero packet traffic generator machine KB packet packet shrink buffer throughput typical however packet totally situation buffer maximum throughput buffer entry throughput throughput factor packet processing rate rate packet traffic tends cpu core intensively idle polling skew producer consumer imbalance buffer shallow buffer easy overflow packet hence buffer panacea compound dynamically traffic motivates merely buffer tune DDIO llc capacity adaptively latent contender identify DDIO latent contender llc management mechanism unaware allocate llc core unconsciously allocate DDIO llc core llc sensitive workload llc entirely isolated core DDIO actually contend core capacity another demonstrate container bound cpu core llc NIC VF container dpdk  0Gb traffic another container another core transition llc alloc tenant info poll prof data prof stable tenant llc alloc execution iat mem microbenchmark application memory behavior characteristic increment mem MB MB apply random memory access emulate application behavior average latency throughput mem container bound dedicate llc overlap container bound DDIO llc DDIO overlap mem  explicitly llc core DDIO worsen mem throughput average latency tenant llc DDIO previous propose DDIO llc core llc allocation argue sub optimal fundamental motivate dynamically allocate llc DDIO sec DDIO occupy portion llc core llc allocation traffic llc isolate DDIO llc wasteful llc portion efficiently IV iat iat aware llc management mechanism DDIO technology various situation  server iat detects increase amount llc DDIO traffic decides traffic application core decision iat allocates llc core DDIO mitigate core interference iat shuffle tenant llc allocation reduce core contention specifically iat performs achieve objective depict tenant info llc alloc initialization tenant iat obtains tenant information available hardware resource tenant info regard hardware resource allocate core llc tenant regard software tenant workload networking iat performance fluctuation non application execution phase behavior non tenant maintain connection device ssh etc intensive traffic priority tenant improve resource utilization data tend collocate workload priority physical platform cluster management software commonly hint priority iat obtain information directly iat assume priority realworld deployment workload performance critical PC effort although software stack aggregation model virtual switch tenant assign priority tenant information iat allocates llc tenant accordingly llc alloc poll prof data iat poll performance status tenant optimal llc allocation application metric operation per latency etc strategy across tenant instead directly profile statistic hardware hardware counter instruction per cycle ipc ipc commonly metric execution performance program cpu core although sensitive microarchitectural factor misprediction serialize instruction stable timescale detect tenant performance degradation improvement llc reference llc reference reflect memory access characteristic workload derive llc rate another critical metric workload performance DDIO DDIO DDIO transaction apply update meaning target cacheline already llc DDIO reflect DDIO transaction apply allocate indicates victim cacheline evict llc allocation metric reflect intensity traffic pressure llc ipc llc ref per core metric tenant occupy core aggregate tenant DDIO chip metric per cpu cannot distinguish device application data iat previous iteration delta threshold threshold stable iat transition potentially adjust llc allocation otherwise regard status unchanged iteration transition ipc significant llc reference DDIO assume attribute neither cache memory ipc non tenant DDIO overlap correspond llc reference significant DDIO reclaim demand core demand transition diagram iat mainly cpu core demand llc exist mechanism allocate llc tenant ipc non tenant DDIO overlap correspond llc reference DDIO shuffle llc allocation sec IV transition core iat mealy FSM decides data poll prof data iteration transition transition trigger poll prof data otherwise iat remain previous traffic intensive llc contend core llc resource iat DDIO DDIO necessarily DDIO transaction update llc thrash traffic trigger extensive cache llc DDIO minimum DDIO  min already allocate llc DDIO DDIO  max regardless DDIO upper bound DDIO compete core without constraint across entire llc PC tenant priority demand contends core llc resource traffic becomes intensive llc update cannot satisfy demand DDIO transaction allocate DDIO happens frequently amount cacheline eviction core demand contends core llc resource specifically core demand llc memory intensive application core buffer frequently evict llc allocate core decrease DDIO increase DDIO reclaim traffic intensive difference llc DDIO medium potentially wasteful reclaim llc DDIO llc specific tenant motivate reclaim llc core iat initialize DDIO threshold threshold indicates llc DDIO insufficient iat determines examine DDIO llc reference decrease DDIO llc reference implies core increasingly contend llc DDIO entry buffer frequently evict llc core demand otherwise increase DDIO demand DDIO attribute intensive traffic core demand decrease DDIO regard signal balance reclaim increase DDIO DDIO demand core longer competitor neither iat core demand demand amount DDIO allocate DDIO  max DDIO transit significant degradation DDIO assume llc capacity DDIO provision reclaim meanwhile DDIO stable DDIO core contend llc core demand obeys reclaim meaningful increase DDIO DDIO  min llc DDIO otherwise demand allocate llc DDIO amortize pressure intensive traffic decrease DDIO core demand llc alloc transition iat correspond action allocate llc DDIO core iat llc assign DDIO tenant specifically demand iat increase llc DDIO per iteration curve increment  explore core demand iat increase llc tenant per iteration iat llc allocation reclaim iat reclaims llc DDIO core per iteration observes llc llc reference tenant idle pool allocation virtual switch PC tenant tenant unallocated tenant DDIO core demand reclaim aggregation model traffic traffic traffic tenant phase IO demand reclaim unchanged slice model llc allocation iat allows core consecutive llc selection idle consecutive exist allocation otherwise shuffle iat identify workload llc core demand reclaim mechanism depends model tenant device model apply aggregation model buffer allocate manage centralize software stack performance software stack bottleneck performance application attach tenant iat increase decrease llc software stack core however slice model VF buffer manage tenant iat selects tenant llc related tenant sort delta llc rate percentage iteration chooses llc rate increase satisfy llc demand correspond core reduce DDIO iat shuffle llc assign tenant properly tenant minimal llc pressure assign DDIO sec llc DDIO incur performance degradation core core non workload hence reduce interference avoid core llc llc fully allocate intuitively tenant PC workload priority isolated llc DDIO iat tenant DDIO meanwhile tenant contend llc DDIO PC tenant performance correlate DDIO shuffle iat sort tenant llc reference iteration chooses llc DDIO accord footnote shuffle tenant access data previously assign llc evict tenant hence shuffle cache utilization performance degradation application performance become stable llc alloc polling performance counter sec VI hence polling interval iat interval iat simply OS schedule task core iat inform tenant tenant addition removal application phase tenant info llc alloc otherwise conduct iteration poll prof data tangible NIC device illustrate iat aggregation model throughput network traffic fix tenant PC assign llc network traffic tenant llc DDIO traffic virtual switch becomes llc already assign hence iat detects DDIO DDIO core demand llc assign virtual switch iteration balance virtual switch shift llc tenant tenant llc DDIO virtual switch maintain llc iat reclaim reclaims llc virtual switch idle llc remove core llc tenant setup slice model throughput network traffic traffic PC tenant llc DDIO becomes insufficient DDIO iat detects situation transit demand allocate llc DDIO workload tenant enters phase llc consume iat delta llc reference tenant consumes llc llc DDIO reduce performance interference amount incoming network traffic decrease llc capacity DDIO iat reclaim llc DDIO implementation implement iat linux user daemon transparent application OS currently user implementation portable flexible however iat implement kernel instruction msr manipulation   kernel implementation potentially monitoring overhead another possibility implement iat cpu configuration intel xeon cpu core core 3GHz cache KB L1D LI MB MB non inclusive llc split slice memory ddr channel controller iat integrate cpu resource management llc allocation standard functionality allocate llc core leverage apis intel  library isolate application demonstrate influence DDIO llc across tenant deployment explore query llc DDIO DDIO related MSRs via msr kernel module profile monitoring similarly  apis regular profile monitoring llc ipc etc monitoring DDIO uncore performance counter intel CPUs apply non uniform cache access NUCA architecture physically split llc multiple slice reduce monitoring overhead DDIO performance counter cache agent cha controller llc slice intel CPUs llc slice intel CPUs apply hash mechanism llc address data core DDIO distribute llc slice evenly hence access llc slice performance counter infer DDIO traffic slice tenant awareness assigns llc core core affiliation tenant simplicity affiliation text file daemon notify parse file environment iat interface orchestrator scheduler query affiliation information dynamically VI evaluation setup hardware quad socket intel server xeon scalable CPUs 3GHz hyperthreading turbo boost disabled tab cpu configuration 2GB ddr memory intel XL 0GbE NICs attach socket NIC directly another server traffic generator bandwidth NIC difference significant observation conclusion software DDIO remote socket socket reflect multi tenant environment application docker container network connectivity model aggregation physical NICs container via OVS dpdk slice II iat parameter threshold stable threshold DDIO  min max interval baseline iat baseline iat baseline iat cpp ipc baseline ipc iat cpp baseline cpp iat DDIO per DDIO per memory bandwidth consumption OVS ipc cpp ipc mem BW GB DDIO DDIO packet packet packet packet performance packet bind VF physical NIC container SR IOV default entry buffer container tcp IP stack  performance host container ubuntu iat absolute overhead disturb tenant iat daemon dedicate core iat parameter empirical parameter tab II balance stability agility tune various qos requirement hardware parameter sensitivity dCAT microbenchmark isolate sec separately verify iat alleviate microbenchmarks leaky dma apply aggregation model container dpdk pmd program bounce traffic dedicate core dedicate llc OVS dedicate core dedicate llc NICs OVS insert OVS NIC container NIC container container NIC container NIC NICs traffic rate setting llc OVS negligible affect performance packet OVS performance stable packet MTU KB performance baseline default DDIO configuration without iat static core iat fundamental DDIO packet default llc DDIO inbound packet however packet increase encourage collocate iat  application deployment baseline iat cpp ipc baseline ipc iat cpp baseline cpp iat llc per ipc cpp ipc llc OVS performance  MB baseline core iso iat latency baseline core iso   MB baseline core iso iat latency baseline core iso iat packet packet packet packet throughput avg latency throughput avg latency stable performance mem container packet pressure llc default llc become deficient reflect increase DDIO iat detects unstable status transit demand allocate llc DDIO DDIO iat baseline DDIO memory throughput performance iat memory bandwidth consumption reduce due limited capacity llc iat eliminate memory traffic desirable combine iat slightly buffer achieve memory traffic reduction modest throughput loss plot OVS performance packet iat improve OVS ipc reduce cycle per packet cpp meanwhile iat identify core demand llc capacity networking application demonstrate setting difference fix traffic rate core dominant source llc rate traffic gradually increase traffic report performance maintain OVS memory hence static initial llc allocation OVS suffer llc ipc iat detect ipc increase llc rate identify demand llc OVS core allocate llc OVS maintains llc gain ipc baseline ipc cpp inevitably worsen OVS  lookup instead pure faster lookup container comparable performance improvement llc container container container container DDIO MB MB DDIO stable stable stable transient transient llc allocation llc container iat KB packet latent contender slice model demonstrate iat efficiently llc policy core VF NIC bind container marked PC dpdk pmd container dedicate core dedicate llc DDIO overlap NIC generate rate traffic packet besides identical container PC dedicate core dedicate llc DDIO overlap mem random mem container MB increase container MB cache llc furthermore stable manually increase llc DDIO iat dynamically mitigate DDIO overlap interference addition baseline iat core adjust llc allocation without awareness iso core excludes DDIO core llc allocation emulate behavior stateof llc management mechanism comparison report stabilize performance mem container PC container increase dramatically iat allocate llc container DDIO avoid contention core iat shuffle assign llc container selects container workload llc DDIO packet pressure llc DDIO interfere core seriously drag mem throughput core simply allocate idle actually DDIO llc mem performs packet fails maintain trend packet core contention mitigate eliminate iat maintain constantly throughput packet baseline core allows mem llc avoids core contention regard latency core randomly access data mem dedicate llc core llc memory average latency isolate temporarily disable iat functionality llc DDIO disable demand llc shuffle norm exec solo bzip gcc mcf milc zeusmp cactusADM leslied gemsfdtd lbm omnetpp astar sphinx xalancbmk RocksDB RocksDB RocksDB RocksDB RocksDB geomean redis baseline redis iat FastClick baseline FastClick iat normalize execution solo spec RocksDB networking application baseline however iat maintains latency baseline core achieves llc isolation container phase iso achieves performance iat due limited available reduce container PC container DDIO llc container iat detects  increase llc container core reshuffle llc allocation llc isolation core llc DDIO suffers severe performance interference significant packet throughput latency baseline worth packet core performs llc DDIO inbound packet distribute amortizes contention regard iso llc core recall footnote PC container relative priority container container latency throughput degradation anyway iat dynamic performance depict llc allocation iat container llc another independent  packet iat react timely within timescale interval application phase reflect hardware metric application evaluate application non networking workload spec benchmark suite memory sensitive benchmark ref input RocksDB persistent ycsb zipfian distribution performance RocksDB avoid storage operation load KB per RocksDB  memory  network function virtualization NFV service chain representative networking workload involve tremendous network traffic cache sensitive target usage DDIO iat memory  redis popular memory  conduct redis container dedicate core OVS another dedicate core OVS redis container llc DDIO overlap besides PC container spec benchmark RocksDB dedicate core llc container llc dedicate core mem random MB MB summary core assign llc allocation non networking container randomly shuffle DDIO NICs OVS ycsb benchmark traffic generator machine thread pre load KB operation NFV service chain FastClick stateful service chain without iat service chain consists network function nfs classifier firewall  stats network address translator  NICs virtualized VFs vlan tag identical container bound VF container processing vlan traffic service chain dedicate core container llc DDIO overlap non networking workload  generate traffic KB packet  traffic generator machine bandwidth 0Gbps per vlan isolate performance impact DDIO highlight commit tackle temporarily disable iat functionality assign llc tenant tenant shuffle application solely solo purely isolated performance application scenario without iat baseline iat performance degradation application report execution non networking application normalize solo spec cache sensitivity benchmark without DDIO awareness iat performance degradation redis FastClick without DDIO awareness non networking application likely affected networking application norm latency solo  redis baseline redis iat FastClick baseline FastClick iat ycsb workload normalize average latency solo RocksDB networking application    solo baseline iat norm latency solo  avg baseline avg iat baseline iat ycsb workload ycsb workload throughput latency redis performance ycsb workload isolation llc portion happens overlap DDIO baseline maximum significant degradation overlap baseline minimum impact incurs baseline recall llc allocation randomly shuffle however iat effectively stably maintain performance isolation degradation perfectly performance solo partial llc overlap DDIO inevitable iat assigns llc DDIO memory bandwidth consume networking application affect performance non networking application apply intel memory bandwidth allocation mba scope similarly ycsb workload RocksDB cache locality requirement affected networking application various extent respectively iat shuffle llc non networking application isolated DDIO throughput degradation respectively intensive network traffic rate inbound outbound traffic FastClick generally exerts impact performance non networking application redis redis impact severely instance server report latency RocksDB operation ycsb benchmark normalize operation latency calculate average normalize latency data RocksDB evict llc memory inbound DDIO data average latency performance solo redis FastClick iat mitigate unexpected eviction shuffle llc non networking application longer latency respectively discus performance networking application tenant exec core unstable core stable core unstable core stable iat execution tenant depicts ycsb redis baseline DDIO application heavily consumes cache resource mem MB mcf omnetpp xalancbmk spec RocksDB happens llc DDIO non networking application networking application adversely impact specifically throughput degradation longer average latency longer latency ycsb workload workload involve dense operation iat mitigates degradation allocate llc DDIO inject inbound packet llc shuffle llc minimize eliminate overlap DDIO cache hungry application contradictory llc DDIO overlap application actually llc DDIO inbound packet distribute evenly llc amortize pressure llc overlap overall benefit outperforms adverse impact iat minimizes performance degradation regard FastClick packet cpu core bottleneck packet processing meaningful throughput service chain due limitation software packet generator report average latency however maximum latency variance significant difference latency consecutive packet iat baseline packet fetch llc iat FastClick service chain performance stable iat overhead tenant iat daemon execution interval exclude initialization tenant dedicate core tenant dedicate core iat daemon dedicate core report average classify category stable poll prof data unstable poll prof data transition llc alloc depict cpu core tenant core per tenant iat execution spent poll prof data conduct transition llc alloc relatively cheap poll prof data daemon cpu hardware performance counter costly context switch contrast transition mainly numerical comparison llc alloc typically involves couple cpu register writes iat execution increase roughly sub linearly core monitoring monitoring core counter operation dominant execution core tenant correspond shorter overhead context switch alleviate core iat execution exceed  efficiency iat interval iat tenant core negligible overhead  iat mem container core utilization configuration sec impact mem throughput practically impact mem latency vii discussion iat limitation mechanism intel CPUs llc partition granularity iat commodity hardware limitation associativity intel CPUs insufficiency allocate tenant performance degradation mitigate  strict isolation tenant assign llc limitation dynamic llc partition proposal iat handle microsecond traffic workload mainly cache interval fluctuation sample data grain routine characteristic transient interval shorter workload ipc fluctuate dramatically monitor tenant constantly stable without interference access cpu register negligible timescale iat cpu core schedule mechanism maintain strict performance isolation finally iat relies user tenant information characterize workload however argue concern target scenario extreme performance private public private tenant information transparent operator iat hardware iat implement hardware per device DDIO public economical relatively looser resource allocation tenant avoid SLA violation statistic adapt grain DDIO statistic faster rate without worry performance overhead hardware implementation enable DDIO llc runtime DDIO grain per cacheline destination data llc detect react contention congestion faster microsecond DDIO remote socket currently DDIO local socket inbound data inject socket correspond device attach application remote socket overcome constraint multi socket NIC technology inbound data NIC dispatch socket DDIO extend remote socket socket interconnect intel  future DDIO consideration DDIO implementation intel CPUs distinguish device application inbound traffic update allocate various pcie device treat performance interference application DDIO simultaneously batch application hadoop inbound traffic evict data PC application redis nfs DDIO llc performance degradation PC application however batch application performance insensitive memory access latency cannot significant benefit fetch data llc instead memory DDIO future intel CPUs device aware assign llc pcie device queue device cpu core DDIO application aware meaning application DDIO entirely partially instance avoid cache pollution application enable DDIO packet header payload memory iat evolve leverage awareness related cache partition isolation prior research cache partition hardware software technique previous leverage related llc allocation  proposes utilize DDIO feature critical data core local llc slice performance interference applicable per core llc slice unrealistic application iat complementary prior proposal comprehensive robust cache qos  observes NFV performance contention DDIO however neither formulates proposes     conduct performance analysis DDIO multiple scenario propose optimization guideline however inaccurate speculation DDIO speculate data core eviction priority llc bijective another disable DDIO llc completely bypass data llc controller clarify confusion DDIO behavior sec II proposes concrete systematic aware llc management performance partition plenty related partition application tenant queue schedule throttle prioritize classify application software hardware partition isolation device OS application etc none investigate interference cpu llc inevitably application performance intensive scenario iat capability identify alleviate interference partition technique IX conclusion server intel DDIO technology become important factor affect cpu llc performance utilization summarize DDIO propose iat  mechanism llc management allocates llc core iat effectively reduce performance interference DDIO application attract attention aware llc management architecture community