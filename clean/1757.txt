despite research inter gpu communication mechanism extract performance  remains significant challenge inter gpu communication via bulk dma transfer expose data transfer latency gpu critical execution transfer logically interleave compute kernel conversely grain peer peer memory access kernel execution memory stall exceed gpus ability operation via multi thread sub cacheline transfer highly inefficient inter gpu interconnects remedy issue propose proact enable remote memory transfer programmability pipeline advantage peer  achieve interconnect efficiency rival bulk dma transfer combine compile instrumentation grain data readiness within gpu proact enables interconnect friendly data transfer hiding transfer latency via pipelining kernel execution describes hardware software implementation proact demonstrates effectiveness proact software prototype generation gpu hardware interconnects achieve ideal interconnect efficiency proact realizes speedup  performance gpu capture available performance opportunity gpu nvidia DGX demonstrate average speedup gpu performance bulk dma approach index gpgpu multi gpu gpu memory management data movement heterogeneous introduction despite advancement gpu architecture program model achieve peak performance multi gpu remains challenge gpu programmer efficiently parallelize application across multi gpu developer typically distribute application data structure across gpu physical memory program logical phase alternate computation access mostly locally available data data distribution synchronization gpus unfortunately typically inefficient resource interconnects idle computation phase compute idle communication phase obtain interconnect utilization maximize performance multi gpu developer dedicate substantial manual effort performance tune architecting application ensure optimal data  gpu bulk dma expose transfer latency gpu gpu peer peer load expose remote load latency gpu gpu peer peer inefficient interconnect utilization gpu gpu proact peer peer auto optimize interconnect usage proact HW SW producer kernel consumer kernel data transfer multi gpu communication paradigm  inter gpu communication synchronization management detailed knowledge internal gpu architecture gpu interconnects memory access application peak multi gpu performance achievable ninja programmer impediment improve multi gpu performance difficulty efficiently overlap gpu computation communication phase resource utilization historically multi gpu application bulk dma communication gpus dma transfer interconnect utilization transfer phase unfortunately due bulk synchronization gpu computation cannot generally communication phase depicts typical dma transfer gpus dma transfer occurs producer consumer kernel gpus expose data transfer latency recently grain peer peer PP transfer enable gpu compute issue load directly physical memory remote gpus peer  PP transfer enable inherent overlap compute communication PP transfer inefficient gpu interconnects gross utilization UI OOVBM  PNQVUFS SDIJUFDUVSF acm annual international symposium computer architecture isca doi isca inter gpu interconnect depict PP load consumer kernel stall remote load due interconnect latency negatively impact performance PP gpus generate sub cacheline granularity sporadic access although latency hidden due protocol  overhead gpu interconnect efficiently proposes proact joint compile runtime combine flexibility peer peer interconnect efficiency bulk transfer proact expose easy  program model developer leverage profile gpu runtime data generation perform transfer coalesce dynamically issue transfer interconnect achieve utilization proact efficient overlap per gpu computation inter gpu data transfer perform transfer proactively data generation dedicate hardware software expense gpu computation resource evaluate proact approach balance grain transfer intelligent interconnect utilization comprehensive performance comparison multi gpu program paradigm automatic gpu memory management barrier gpu transfer finegrained data access demonstrate performance proact implement software prototype hardware consume gpu resource overhead eliminate future hardware proact perform approach multi gpu program enable performance improvement prototype gpu alternative standard bulk synchronous approach II background cuda program model cuda extends programmer define function kernel execute parallel thread grouped thread recent gpus within grouped warp schedule hardware execute lockstep multiprocessor SM gpu program involves launch kernel pre configure thread dimension gpu parallelize application across gpus kernel launch gpu separately execution across multiple gpus data structure localize gpu replicate partition across multiple gpus gpu along compute memory bandwidth available application application developer prefer perform gpu fully independent algorithm inherently amount communication thread application relative portion interconnect  transfer granularity spent inter gpu communication generally increase along gpu communication eventually becomes performance bottleneck inter gpu communication mechanism regardless amount remote data multi gpu program access execution gpu programmer mechanism perform access mechanism unique performance characteristic described mechanism performance implication nvidia terminology specific concept pedagogically refer kernel data another gpu consumer kernel kernel generates data kernel gpu producer kernel kernel producer data consumer data simultaneously dma bulk transfer bulk dma transfer gpus data transfer typically explicitly invoked host program execute cpu via cudaMemcpy transfer schedule completion producer kernel invocation consumer kernel cudaMemcpy invokes gpu hardware dma transfer data directly gpus memory without transfer reflect cpu memory paradigm saturate gpu interconnects transfer granularity data ensures subsequent access consumer service bandwidth within consumer gpu local physical memory however suitable cache medium KB transfer due initialization synchronization overhead return host program program dma consume microsecond dominates data transfer programmer attempt interleave computation communication dma transfer substantial programmer expertise effort peer peer PP gpu access multi gpu individual gpus capable directly reading physical memory peer gpus without gpu gpu thread memory update gpu thread automatic data readiness HW SW assist  data transfer interconnect optimize transfer gpu local compile profile overview logical proact component runtime intervention host cpu synchronization advantage perform remote access gpu thread initiation overhead program dma access perform unrelated computation parallel within thread without stall thread however load perform remote memory PP load stall thread execution beyond gpus ability hide load latency multi thread due longer latency inter gpu interconnect local memory load stall within gpu memory eventually consume precious gpu resource otherwise execution progress unlike PP eventually stall issue thread gpu PP writes almost stall issue thread implicit dependence within program execution consume substantially chip resource non PP fundamentally efficient perform multi gpu communication PP panacea however straightforward PP typically numerous writes byte issue inter gpu interconnect interconnect utilization proact adopts PP basis program model contribution overcome interconnect efficiency perform PP writes programmatic communication library development burden gpu programmer gpu aware library mpi nvidia NCCL nvidia  universal inter gpu communication synchronization irrespective interconnect technology topology code library selectively bulk dma transfer PP access hood combine library memory management optimize communication gpus although library optimize avoid dma initiation overhead perform transfer none library attempt aggregate grain transfer intelligently improve interconnect efficiency knowledge brief compile profile runtime optimization interconnect utilization substantial performance improvement underlie mechanism library without loss generality proact technique implement commonly library VI discus additional gpu communication library inter gpu interconnect efficiency architect builder interconnect technology choice multi gpu decade pcie dominant interconnect attach peripheral accelerator CPUs recently NVLink nvidia dedicate gpu interconnect target bandwidth improve scalability pcie performance interconnects infiniband amd infinity fabric exist target networking cpu cpu cpu gpu connection despite evolutionary pcie slate NVLink progression protocol  peer access gpus pcie NVLink efficiency perform access percentage  achieve interconnect percentage useful data deliver pcie NVLink interconnects granularity interconnect technology efficiency transfer byte cacheline dramatically transfer interconnect efficiency decrease granularity protocol  overhead dominate effective  transfer efficiency NVLink pcie byte therefore PP writes fundamentally efficient transfer data latency hiding perspective dramatic improvement interconnect efficiency achieve improve overall multi gpu performance proact implementation proact attempt bridge benefit dma bulk peer peer access multi gpu interconnect efficiency transfer non semantics SM initiate peer peer proact improves performance multi gpu balance overlap data transfer gpu computation maximize opportunity coalesce smooth interconnect utilization ensure bandwidth waste increase interconnect efficiency ensure communication occurs sufficiently granularity benefit application data access proact choice application architecture transfer mechanism gpus transfer data memory approach overlap compute communication interconnect utilization efficiency transfer granularity data transfer mechanism efficiency function granularity perform transfer coarser chunk reduce initiation overhead chunk extends computation delay phase transfer data grain chunk interconnect efficiency described II transfer resource perform data transfer employ thread typically interconnect inefficiency compute stall develop application per thread warp specialization subset thread data remote gpus implement  proact automatically identify appropriate amount gpu resource perform transfer hide complexity programmer data generation data transfer perform thread thread data proact develop appropriate mechanism data production synchronize producer transfer mechanism proact primary component coordinate software hardware stack choice maximize throughput overview building  profiler proact parameter achieve optimize transfer efficiency monitor data readiness data transfer mechanism initiate optimize transfer component described detail optimize transfer efficiency via profile improve interconnect efficiency identify extract performance multi gpu interconnect without hamper gpu compute throughput granularity PP interconnect efficiency described earlier II sufficient writes flight interconnect bandwidth underutilized unsurprisingly performance complicate function multiple parameter automatic profile workload throughput gpu transfer thread aggregate transfer sensitivity transfer thread aggregate transfer granularity nvidia kepler microbenchmark described IV performance trend explore detail application throughput obtain transfer granularity MB thread transfer thread sufficient saturate interconnect bandwidth thread improvement interconnect utilization gpu generation interconnect specific gpu thread saturate interconnect bandwidth examine profile proact transfer thread maximize interconnect bandwidth minimize gpu execution lane data transfer facilitate exploration complex proact contains compile software profile suite identifies appropriate balance compete factor affect performance gpu interconnect proact profiler performs parameter sweep analyzes application performance across proact transfer mechanism described later transfer chunk transfer thread identify configuration achieves application runtime application configures application compilation emit code efficient accord profile brute across configuration explain feasible sufficient proact profile pas proact configuration depends gpu architecture interconnect architecture compute interconnect bandwidth requirement application profile  basis gpu PA gpu producer kernel demarcate memory gpu consumer kernel local writes local remote writes atomic counter per transfer granularity interconnect transfer agent gpu PA decouple local data generation decouple transfer local data transfer readiness proact user program model encourages PP however transmit data interconnect granularity interconnect efficiency propose remote gpu transfer decouple thread writes aggregate writes local gpu memory transfer remote gpus proact stag mechanism whereby gpu thread individually local memory programmer  locally data propagate remote gpus asynchronously operating transfer agent decouple transfer data interconnect generation data locally proact optimize transfer timing granularity mechanism chooses employ profile proact allocate deploy explicit data transfer buffer avoid extra operation memory  correspondence maintain local remote memory location local writes proact enable remote gpus nvidia gpu memory model weak writes writes cta gpu scope visible remote gpus later succeed global synchronization barrier proact exploit slack aggregate data local memory writes transfer chunk data interconnect saturate efficiency transfer writes sys scope typically global memory synchronization instruction perform data transfer hence  proact atomic counter data generation existence decouple transfer mechanism implies transfer proact employ atomic counter per chunk within transfer granularity data generation transfer ultra hardware data readiness beneficial unnecessary gpus already atomic counter memory typically implement gpu performance atomic counter initialize compute thread array CTAs issue writes correspond chunk compiler execution counter decrement cta writes chunk trigger transfer agent copying data counter zero proact application issue deterministic identify completion data generation per chunk basis perform transfer simplify application memory laid linearly producer kernel data update atomic counter maintain location memory transfer agent poll counter transfer linear arrangement data memory writes contiguous memory performance reduce amount storage overhead atomic counter proact typically chooses transfer granularity MB later II storage overhead counter significant concern decouple data transfer mechanism decouple data transfer local data generation flexibility inter gpu transfer mechanism transfer granularity maximize bandwidth utilization gpus initiation overhead gpu dma appropriate frequent data movement gpus alternative polling technique configurable gpu warp specialized perform data transfer granularity auto generate manage entirely proact runtime independently launch kernel poll structure data atomic counter update application producer kernel transfer thread chunk local stag array perform remote writes target gpu proact profiler minimize transfer thread maximize interconnect performance ultimately thread polling counter software implementation compete compute kernel execution throughput memory consistency producer thread decouple transfer thread enforce exist gpu memory fence proact software prototype dynamic kernel launch cuda dynamic parallelism CDP another transfer agent mechanism kernel launch kernel contrast polling consumes compute bandwidth polling loop monitor chunk transfer readiness dynamic kernel launch chunk transfer consume compute resource transfer cuda runtime guarantee kernel fully consistent global memory kernel additional synchronization proact implement CDP transfer alternative polling avoid compute bandwidth polling transfer bitmap however CDP kernel launch initiation latency polling dma transfer CDP transfer atomic data counter indicates data chunk transfer proact instrumentation producer kernel launch dynamic kernel pre configure thread perform transfer dynamic kernel transfer data chunk destination gpus tightly packed SM instruction inline evaluate entire option proact software prototype inline version wherein native PP issue remote writes distribute data remote gpus data without defer decouple transfer thread perform inline advantage remote writes producer kernel without overhead decouple transfer smooth interconnect utilization remote writes evenly distribute within kernel execution gpu usually non coalesce adjacent writes remote transfer latency hidden unless queue resource exhaust functionally memory barrier implicitly producer kernel ensures transfer subsequent access consumer kernel locally data generate producer spatial locality coalesce fail inflate interconnect bandwidth transfer application data relative decouple transfer mechanism proact chooses grain transfer mechanism sys scoped release operation proact buffer flush hardware proact implement software prototype proact allows validate concept across gpu generation interconnects impossible simulate reasonable additional hardware envision implementation data readiness counter provision dedicate memory structure initialize proact runtime associate bound proact enable gpu memory local kepler pascal volta volta gpu tesla tesla tesla tesla gpu arch kepler pascal volta volta gpus interconnect pcie NVLink NVLink NVSwitch bidirectional 6GB 0GB 0GB 0GB BW per gpu aggregate aggregate aggregate aggregate core SMs tflops BW GB sec mem cap GB characteristic gpus interconnect topology writes issue tracked proact automatically update correspond counter replace explicit instruction proact instrumentation maintain counter prototype counter decrement zero hardware signal transfer agent initiate transfer correspond chunk transfer agent realize simplify dma descriptor geometry transfer prepared advance memory proact runtime salient aspect hardware transfer trigger automatically without interaction gpu driver host cpu proact prototyped entirely software software overhead quantify later microarchitectural detail proact hardware realization future focus demonstrate efficacy generality proact approach across numerous gpu interconnect topology software prototype IV experimental methodology evaluate proact software prototype perform across gpu platform kepler pascal volta gpu platform volta characteristic described proact code framework proact software prototype demonstrate listing compiler automatically incorporates proact configuration parameter profile insert code enable proact transfer generates inline decouple version code proact allocates meta data structure data generation framework arbitrary mapping thread chunk address mapping proact mapping utility code proact mapping stride stencil user define mapping proact init atomic counter initialize CTAs correspond data chunk execution proact decrement counter associate chunk decrement trigger chunk chunk automatically compile profile empirically balance overhead contention atomic counter initiation bandwidth utilization void proact init proact proact proact  mech  mech profiler proact counter counter num chunk chunk num chunk proact counter chunk chunk val profiler chunk proact  mech polling proact bitmap bool num chunk data bitmap launch polling kernel proact global void user kernel inline proact proact generate ptr tid computation tid temp computation tid proact gpu tid temp proact gpu tid temp proact gpu tid temp proact gpu tid temp global void user kernel decouple proact proact int dest arr ptr proact  global memory ptr tid computation tid code compiler thread cta proact  mech dedicate HW counter chunk proact  mech CDP  counter chunk counter chunk src   dest   dest   dest   kernel   src  dest dest dest proact  mech polling  counter chunk counter chunk bitmap chunk int memory allocation user proact  addr addr addr addr proact proact proact  mapping proact mapping proact contiguous proact addr addr addr addr initialization parameter proact init proact  inline user kernel inline  nthreads proact user kernel decouple  nthreads proact listing proact sample code gpus evaluate alternative demonstrate dynamic benefit proact static multi gpu program approach cudaMemcpy paradigm computation kernel cudaMemcpy duplicate data structure gpus initiation kernel data structure access kernel resident local gpu memory remote access however overlap data transfer compute unified memory UM workload implementation UM replace conventional memory allocation UM allocation remove explicit data transfer inter gpu synchronization UM hint apis expert user avoid fault respective overhead various hint strategy data prefetching replication pre population gpu reduce fault overhead effort attempt optimize application proact inline proact inline remote inject directly source kernel data remote gpus PP user kernel inline listing rely decouple transfer agent proact decouple proact decouple flexibility proact mechanism profile data transfer polling cuda dynamic parallelism transfer parameter platform application transfer thread transfer granularity infinite interconnect BW finally limit performance data transfer instantaneous maximum speedup application attain optimize data movement application enjoy benefit grain memory data transfer overhead grain neglect attainable understand multi gpu program paradigm theoretical maximum performance compute performance bound bulk transfer implementation workload discounting execution spent perform data cudaMemcpy benchmark evaluate proact implement multiple version workload across various scientific domain microbenchmarks demonstrate difference proact transfer mechanism benchmark compile cuda microbenchmarks microbenchmarks consist synthetic compute kernel source gpu generate data entirety destination gpus phase opportunity overlap data transfer kernel execution maximize duration compute kernel transfer tune compute synthetic kernel transfer cudaMemcpy transfer ideal interconnect zero latency infinite bandwidth perform data transfer instantaneously speedup interconnect obtain reasonable compute kernel criterion fix amount data transfer MB tune compute across various gpu generation compute kernel source gpu data production initiate transfer via proact decouple mechanism  ray CT model iterative reconstruction  computational technique highquality image ray dose computational algorithm FDA approve GE  CT  approve clinical rank rank assigns rank web importance benchmark assigns pagerank article wikipedia dataset source shortest shortest algorithm navigate physical location network iteration vertex computes shortest distance source vertex bellman ford algorithm compute HVR dataset alternate ALS ALS widely perform matrix factorization recommender algorithm iterative iteration fix user matrix optimizes item matrix vice versa performs ALS stochastic gradient descent vertex HVR dataset jacobi solver jacobi solver iteratively solves linear equation coefficient matrix constant vector vector solver iteratively computes vector jacobi perform jacobi solver matrix arise widely finite analysis RESULTS analyze proact decouple transfer mechanism polling cuda dynamic parallelism cudaMemcpy microbenchmark understand performance variation across gpu architecture proact inline mechanism performs transfer granularity data generate microbenchmark however transfer mechanism later proact framework decouple inline variant transfer mechanism separately clarity microbenchmarking decouple transfer mechanism performance microbenchmark described IV granularity decouple transfer KB MB source thread generates KB data kepler CDP performance curve exhibit transfer granularity KB per chunk performance initiation bound initiation overhead dominate net slowdown relative cudaMemcpy transfer bandwidth bound KB MB per chunk proactive transfer peak speedup granularity increase beyond MB transfer bound transfer compute kernel completes transfer become significant net slowdown polling substantially underperforms cudaMemcpy CDP kepler due gpu resource waste numerous  poll loop utilize portion gpu compute memory resource polling loop detrimental kepler pascal volta compute memory bandwidth availability pascal CDP trend kepler offering peak speedup bandwidth bound however polling performs attain speedup transfer granularity delay iterate polling bitmap amortize transfer operation CDP slowdown granularity  achieve speedup granularity initiation overhead dynamic kernel volta architecture polling speedup nearly granularity gpus launch dynamic kernel intervention host driver dynamic kernel launch impact performance depends gpu hardware driver substantially across gpu generation furthermore data transfer kernel interferes computation depends warp data transfer kernel varies across gpu platform per gpu platform perform data transfer mechanism varies across transfer granularity transfer mechanism dma grain transfer agent polling cuda dynamic parallelism performance application speedup achievable gpu gpu across gpu generation II depicts correspond proact configuration programmer  hint UM substantially improve performance gpu application however hardware fault migration pascal architecture kepler earlier architecture kepler pascal volta performance proact microbenchmarks decouple transfer paradigm transfer granularity primitive version unified memory performance jacobi UM outperforms cudaMemcpy duplication sporadic access expensive fault migration UM perform poorly pagerank application pascal volta generation perform explicit duplication via cudaMemcpy gpu outperforms gpu application pagerank underperforms gpu volta pascal proact inline outperforms decouple transfer application naturally exhibit spatial locality jacobi ray CT gpu thread schedule kepler pascal volta gpu speedup data transfer hardware configuration data densely increase address excellent coalesce within SM increase interconnect efficiency achieve decouple data transfer insufficient overcome overhead software implementation consume SM resource marginal difference performance artifact prototyping proact decouple software hardware implementation outperform inline variant remain application coalesce due random memory update proact decouple performs grain transfer coalesce ALS volta transaction interconnect proact inline proact decouple due coalesce application architecture proact suitable mechanism perform data transfer abundant parallelism available application apparent linear achieve application kepler pascal volta ray CT CDP jacobi poll pagerank CDP MB poll poll CDP MB poll poll ALS CDP MB poll poll II perform configuration proact profiler configuration transfer scheme proact inline proact decouple transfer granularity MB transfer thread transfer mechanism poll polling CDP cuda dynamic parallelism theoretical upper bound infinite interconnect bandwidth regardless gpu generation geometric theoretical opportunity speedup gpu application proact enables speedup across generation capture opportunity complicate cudaMemcpy significantly variation across gpu generation average speedup gpu UM display variable achieve speedup application platform average underperform gpu configuration conclude programmer hint traditional  UM rely upon achieve peak performance communication paradigm benefit proact inline decouple cudaMemcpy ability overlap compute communication benefit UM avoid expensive fault migration proact decouple performs proact inline interconnect efficiency achieve decouple outweighs overhead compute kernel vice versa decompose proact performance understand performance analyze overhead incur proact proact inline incurs overhead computation instrumentation involve proact decouple performance overhead stem prototyping data interface entirely software overhead runtime instrumentation without data transfer runtime theoretical infinite interconnect BW report overhead reveals overhead average gpu platform variation across application significant negligible overhead pagerank overhead proact decouple achieves substantial performance improvement despite software overhead hardware implementation alleviate overhead compute kernel improve performance motivate inclusion future gpu architecture understand efficient proact overlap data transfer computation investigate data compute slowdown due proact decouple transfer transfer overlap achieve proact transfer overhead overlap computation gpu obtain execute application instrumentation initiation overhead proact elide actually perform data transfer remote gpu memory difference runtime without data transfer operation reveals portion transfer overlap overlap  transfer baseline duplication cudaMemcpy report although variation application platform reveals proact hide transfer overlap nearly communication enable scalability gpu relative cudaMemcpy duplication achieves transfer overlap UM hint achieve overlap compute communication theory substantial programmer effort data generation granularity schedule prefetch operation synchronize computation proact proact inline decouple performance impact gpu generation gpus interconnects scalability proact multi gpu absolute speedup proact kepler pascal volta scalability achieve hardware configuration achieves kepler pascal volta gpus respectively omit unified memory average instead focus performance improvement proact cudaMemcpy duplication gpu performance achieve respective employ gpus performance insensitive transfer across platform surprising spent data transfer insignificant gpus however gpus increase kepler pascal volta performance cudaMemcpy flattens decrease manifest beyond gpus kepler hypothesize pcie kepler inter gpu bandwidth transfer overhead affect performance quickly pascal performance cudaMemcpy generally competitive gpus leveling examine gpu volta prototype cudaMemcpy gpus leveling proact however exhibit nearly linear multi gpu interconnect bandwidth effectively overlap gpu communication computation overall gpu volta proact achieves speedup cudaMemcpy duplication gpu configuration respectively within theoretical application performance limit discussion performance benefit proact ability perform grain data transfer ensure overlap compute communication application satisfy benefit proact performance limited inter gpu communication access application deterministic programmer structure program annotate data structure proact writes compute bound application compute overhead proact software prototype render suitable however application benefit proact hardware implementation application perform sporadic access granularity overhead initiate decouple transfer hurt performance proact decouple profiler proact inline instead VI related accelerate scientific application gpus optimize gpu communication widely legacy context introduction switch LD ST  communication gpu knowledge explore proactive optimize multi gpu communication prior optimize communication gpu thread improve mpi communication gpus across multiple node optimize inter gpu communication via barrier synchronization  overlap computation data transfer gpu global memory MVAPICH gpu integrates cuda data movement transparently mpi intra node mpi communication multi gpu node  automate manage optimize cpu gpu communication explore weak execution reduce host synchronization overhead  asynchronous multi gpu model irregular application optimize grain communication gpus  package integration multiple gpu module proact improve performance efficient local dram partition prior explore various hardware software mechanism improve multi gpu performance perform numa aware optimization improve gpu performance perform hardware peer cache performance heterogeneous stag memory schedule decouples primary task memory controller improve performance cpu gpu improve performance texture cache approximation gpus developer accurately predict execution gpu theoretical analysis multicore scalability unicorn parallel program model hybrid cpu gpu cluster framework automatic multi gpu parallelization  schedule policy propose thread criticality predictor parallel application  attempt overlap cpu gpu pcie transfer data layout transformation gpu technique effective gpu offload topic orthogonal complementary optimize grain multi gpu communication auto tune code generation literature auto tuner automate construction compiler heuristic machine automatic tune heuristic code inlining genetic algorithm approach compiler optimization additionally obtain heuristic schedule algorithm automatically shortest program compute function previously gpu realm multiple explore application specific auto tune gpus methodology multi gpu performance attempt improve load balance bus utilization attempt overlap compute communication  generic auto tuner OpenCL kernel inter device communication explore code generation specific gpu application attempt generic mechanism application vii conclusion propose proact hardware software improves multi gpu performance overcome limitation exist inter gpu communication mechanism proact combine flexibility peer peer transfer efficiency bulk transfer enable interconnect friendly data transfer hiding transfer latency demonstrate across gpu architecture proact average speedup gpu implementation capture theoretical limit proact dramatic scalability improvement generation gpu achieve average speedup gpu implementation capture available opportunity scalable multi gpu program easy task maximize programmer productivity runtime proact enable rapid development cycle leverage generation architectural improvement future gpus