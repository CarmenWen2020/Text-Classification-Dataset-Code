prevalence computational demand artificial intelligence AI workload widespread hardware accelerator execution performance AI accelerator across generation pivotal commercial deployment intrinsic error resilient AI workload unique opportunity performance improvement precision motivate recent algorithmic advance precision inference training rapid core AI accelerator chip spectrum precision namely float fix rapid chip fabricate euv technology delivers peak tflops HFP mode TOPS int mode nominal voltage performance model calibrate within measurement evaluate dnn inference fix representation core rapid chip dnn training float representation tflops AI comprise core rapid chip int inference batch achieves average TOPS FP training mini batch achieves sustain average tflops across application keywords hardware acceleration neural network reduce precision introduction decade witness paradigm shift workload execute compute platform across spectrum mobile iot device datacenters driven availability massive amount data advance neural network dnns AI application service significantly grown prevalence surpass challenge AI task involve image video text however accuracy dnns computational image recognition dnns resnet billion scalar operation classify rapid expands reduce precision dataflow accelerator AI image furthermore training dnn model  compute massive training datasets GB compute storage bandwidth requirement severely stress capability traditional compute platform AI accelerator slowdown CMOS technology associate benefit meeting computational demand AI workload fuel future AI research complex robust model necessitates innovation across hardware software stack approach broadly adopt building specialized AI hardware accelerator AI workload lend hardware acceleration static dataflow graph computation dnns express primitive evidence academic demonstration specialized accelerator dnns commercial AI core google TPUs nvidia tensor core intel nnp etc precision training inference precision performance AI accelerator across generation pivotal commercial deployment beyond traditional performance compute stack viz technology node core heterogeneous architecture others AI workload unique opportunity performance improvement precision advance approximate compute recent UI OOVBM  PNQVUFS SDIJUFDUVSF acm annual international symposium computer architecture isca doi isca successfully demonstrate judiciously intrinsic error resilient AI workload leveraged reduce width data representation computation without loss accuracy research effort consistently precision requirement inference training inference precision driven deployment device width activation precision training significantly challenge due maintain fidelity gradient propagation dynamic representation recently float representation effective dnn training ultra precision capable AI accelerator algorithmic advance imperative generation AI accelerator capable ultra precision execution beyond float training integer inference rapid accelerator architecture spectrum precision float fix fabricate core rapid chip euv technology operating ghz precision advantage favorable integrate within accelerator impact aspect improves compute efficiency reduces memory footprint data bandwidth requirement preserve regularity compute minimizes dataflow complexity overhead however introduces challenge careful architecture highlight feature rapid mixed ultra precision rapid architecture precision float FP flavour float FP fwd FP bwd programmable bias refer hybrid FP HFP fix int  int precision detailed algorithmic context training inference noteworthy although target ultra precision execution critical retain precision ultralow precision apply computation layer etc precision preserve accuracy core rapid AI chip euv technology delivers FL ops peak achieves FL ops FP HFP int precision respectively TOPS TOPS precision tenet rapid improve performance TOPS efficiency TOPS ultra precision catering  deployment scenario implies compute commensurate precision navigate tradeoff precision application coverage convolution GEMMs account significant dnn ops embed within activation pool normalization data shuffle operation execute operation accelerator vital  accelerator host costly function sfu rapid accurate version operation permute data shuffle float FP operation sparsity aware frequency throttle enhance efficiency fuse accumulate FMA rapid zero gate logic bypass trigger multiplicand zero management software rapidly throttle effective frequency context inference leverage feature boost performance sparse prune dnn model specifically offline analysis sparsity exhibit layer estimate zero gate invest layer execution increase effective frequency benefit performance multi core rapid contains memory interface MNI enables core core core memory communication synchronization communication protocol contains primitive concurrent data transfer overlap multi cast  core software effectively utilize available bandwidth summary rapid architecture enables training inference ultra precision foster scalability multiple core functional converge execute AI workload evaluate core rapid chip model inference primary focus fix evaluate distribute chip core rapid chip model training float knowledge rapid effort mixed precision architecture capable training inference organize II discus systolic dataflow architecture akin recent prior baseline core architecture summarizes advance algorithmic approximation foundation enable ultra precision rapid architecture microarchitecture innovation core ultra precision IV rapid chip core along program model discus propose training inference derive rapid chip describes effectiveness rapid inference training ultra precision popular dnn benchmark VI discus related effort finally vii concludes future II background AI workload static dataflow graph computation dnns express primitive academic demonstration commercial AI core exploit systolic dataflow architecture systolic dataflow architecture baseline enhance architecture ultra precision training inference baseline systolic dataflow architecture building baseline systolic dataflow architecture computation comprises 2D systolic array processing PEs float FP computation execute convolution matrix multiplication operation dnns 1D array function SFUs float computation FP FP perform activation function pool gradient reduction normalization operation precision baseline systolic dataflow architecture PE contains simd  accumulate mac operand PE local register file  similarly output mac PE  typical dataflows diagonal operand PEs execute instruction sequence systolic fashion SFP contains simd MACs precision vector tiered memory hierarchy scratchpad data PE array SFUs scratchpad data along direction scratchpad memory memory direction sfu PE array interface external memory maximum flexibility baseline architecture fully decentralize decouple compute dataflow component multiple thread execution access execute paradigm programmable link architecture grain sequence data link orchestrate dataflow programmable address sequence data link upon data programmable determines location SFUs individual program data operand incoming outgo link local register file execution dnn operation therefore orchestrate multiple program broadly classify data sequence program load data scratchpad memory sequence PE SFUs data processing program define computation execute PE SFUs incoming data ensure functionality producer consumer dependency architecture  hardware synchronization programmable data subsequently PE array program data reading synchronize periodically ensure writes precede training beyond FP mixed FP precision training reduce precision dnn training significantly challenge due maintain fidelity gradient propagation recently algorithmic approximation hybrid FP HFP data format training dnns HFP format involves FP exponent mantissa representation representation dynamic activation dynamic error operand pas FP whereas backward pas gradient computation operand FP FP representation addition exponent mantissa combination backward HFP exponent bias configurable enables dnn layer dynamic despite exponent HFP training achieve model accuracy equivalent FP representation training model across spectrum application image classification detection machine translation inference beyond int precision inference demonstrate successfully int int fix representation recently quantization technique viz parameterized clip activation pact activation statistic aware binning  demonstrate int inference negligible loss accuracy int inference minimal accuracy loss pact introduces activation function derive relu clip output beyond thereby reduce insight clip statically fix learnt model training independently layer dnn  quantizes retain distribution pact  impact model training core architecture ultra precision feature precision preserve regularity compute tensor equally precision addition precision execution memory interconnect subsystem capacity requirement data structure amount data transfer component reduce hence baseline systolic dataflow architecture organization precision AI workload computational bandwidth requirement ultra precision training inference baseline architecture enhance overall peak TOPs commensurate precision balance PE array effectively utilize increase peak TOPs bandwidth constraint data PE array data operand effectively across simd PE array output format PE array auxiliary operation perform precision maintain accuracy classification balance computational distribution precision activation sfu array ultra precision convolution matrix operation PE array architecture microarchitecture innovation component fundamental building baseline architecture overcome challenge realize goal MPE array mixed precision PE array precision computation challenge overall peak TOPs commensurate precision discus architecture microarchitecture enhancement mixed precision PE array enable peak TOPs precision overcome challenge bandwidth int FP pipeline training inference ultra precision MPE array comprehensive mixed precision execution format viz FP hybrid FP int int II diagram MPE float fix operation compute increase overhead potential mismatch pipeline depth execution latency separation integer float pipeline solves architectural complexity handle multiple precision circuit implementation opportunity aggressively improve efficiency baseline architecture MPE FPUs  FP HFP int int format respectively HFP training MPE FPU pipeline FP HFP datapath simd FPU MPE FPU receives input operand local register file  input operand propagate link output propagate enhance FPU innovation HFP enable representation input operand achieve peak TOPs relative FP conversion custom FP representation FP training matrix multiplication convolution backward training tensor FP format input however increase hardware complexity float enhance FPU custom exponent mantissa format input operand format convert format HFP training activation memory scratchpad bias format convert format horizontal bus data scratch pad fifo interface data scratch pad via link int complement int diagram mixed precision processing MPE instruction format ISA  input tensor  instruction  FP format MPE program desire format input operand data chosen however within program precision operand remains fix register hardware  width operand another feature HFP datapath exponent bias programmable dynamic computation execution MPE configure appropriate exponent bias sub simd partition enablers peak TOPs HFP mode grain partition simd sub simd within FPU realize performance FP interface bus width HFP mode accumulate instruction  simd MPE realizes multiplication addition additional complexity logic depth HFP multiplicand remains comparable FP due multiplier ISA extension microarchitecture enhancement described MPE HFP FP FPU pipeline maintain accuracy auxiliary operation precision FP HFP training FP HFP mode FP FPU compute FP HFP merge adder finally HFP training chunk accumulation accumulate partial sum hierarchical fashion preserve fidelity intermediate sum SFUs realize chunk accumulation partial sum FP int  int int inference mention augment MPE FPU  pipeline addition int int target dnn inference circuit optimization become feasible reduce pump int int pipeline analysis couple FPU  reveal opportunity int int within  summarize addition int pipeline incurs overhead int pipeline FP pipeline enable int int compute MPE therefore  int int accumulate operand reuse sub simd across simd completes int int  operation cycle datapath width simd mac enhance architecture access register  mac instruction mitigates accumulation multiplier reduce precision simd  integer mac operation integer datapath  efficient data mapping balance bandwidth constraint operand within  modify  data int input operand link across sub simd propagate across MPE reuse overall operation convolution gemm dataflows MPE array dataflow simplify pseudocode convolution matrix multiplication MPE array convolution layer express dimension input output channel feature kernel minibatch dataflow involves define dimension spatially mapped along simd lane local register file  MPE array subsequently data structure along stationary  sequence access convolution dataflow utilized novel stationary dataflow derive constraint narrow dataflow choice achieve utilization batch eliminate batch spatial dimension avoid communication implies cannot chosen spatially minimize residue due  workload dimension multiple hardware dimension choice typically prime therefore input output channel spatial dimension avoid simd reduction hardware along simd along  systolic dataflow data operand fed along reuse vice versa implies dimension spatially mapped along unrelated data structure along therefore input data structure along output data structure along stationary  correspond input output channel MPE location array contains simplify pseudocode loop sequence load  computation interval load maximize utilization therefore innermost loop reuse load loop input output volume scratchpad reuse iteration loop finally loop loop mapped spatially hardware chosen dataflow yield utilization almost convolution layer layer fully layer frequent load batch sfu array spectrum activation function function sfu accurate version spectrum FP non linear activation function precision FP operation activation function relu leaky relu sigmoid tanh backward phase dnn operation normalization function pool operation SFUs addition function sqrt exponent tanh sigmoid reciprocal realize approximation sfu array realize operation shuffle permute transpose update phase training precision peak TOPs MPE array percentage cycle spent auxiliary operation SFUs necessitate sfu array maintain balance compute spent ultra precision convolution matrix operation precision auxiliary operation sparsity aware zero gate frequency throttle achieve TOPs ultra precision AI core architecture mechanism gate FPU pipeline sparsity aware frequency throttle maximize TOPs within limit saving zero gate significant zero input operand opportunity compute zero operand  skip entire FPU pipeline multiplicand zero simply addend sparsity aware frequency throttle across layer dnn significant difference sparsity distribution sparsity static inference rapid exploit hardware software throttle maximize TOPs within limit unlike DVFS modulation involves costly voltage regulation pll loop throttle software modulate within cycle silicon characterization dynamic static function voltage frequency admissible voltage extend characterization derive stall rate throttle operating voltage frequency sparsity model limit operating nominal voltage frequency effective frequency sparsity derive stall rate operating graph compiler analyzes sparsity layer dnn characterization data input throttle individual layer closest envelope compilation overhead critical inference amortize multiple inference dnn rapid chip module via skip throttle rate recommend compiler reduce overall execution operating within thermal limit workload aware throttle ultra precision core  AI core  scratchpad maximize data scratchpad exploit reduction capacity requirement due ultralow precision AI core combine MPE array sfu array scratchpad refer  MB scratchpad scratchpad communicates independent programmable load  bandwidth byte cycle MB capacity scratchpad allows intermediate output chip ultra precision inference similarly bandwidth  balance dataflows across precision modify stationary dataflow int computation MPE consume available bandwidth byte cycle precision int cycle partial sum scratch pad limit available bandwidth AI core equip MPE array sfu array along tiered scratchpad hierarchy feature  accelerator fundamental building comprise multiple core data communication core memory communicate external memory chip multiple core adopt directional interconnection bandwidth byte cycle direction communicate data core memory core programmable memory interface MNI facilitate data communication memory via  riu enable asynchronous domain core frequency overview MNI request data return core memory programmable load MNI LU MNI SU data access dnns static regular data fetch latency effectively hidden buffering data scratchpad overlap computation core compiler tile program loop granularity data fetch balance scratch pad capacity available bandwidth load request assign unique identification tag generate execution primitive MNI SU  respectively load queue MNI multiple outstanding request memory MNI LU allows data return local scratchpad address tracked load queue  SU program stall limit outstanding request exploit directional bandwidth MNI LU data return cycle exploit architecture scratchpad reservation management policy riu avoid conflict scratchpad MNI SU  MNI LU  req   sharer incoming req program buffer   recv   resp data  outgo data resp request aggregation selection clockwise counterclockwise request request generation consume response MNI LU primitive outgo req data address unicast multicast response generation program buffer address data MNI SU primitive incoming data resp request generation consume response outgo req program buffer address MNI LU corec memory memory memory data incoming data resp riu riu riu multi cast memory interface dnn workload exhibit parallelism spatial partition data across multiple core data behavior exploit multi cast communication ISA hardware MNI load multi cast data transfer primitive MNI SU MNI LU respectively generate compiler assign identification tag participate core multi cast data transfer core core consumer individual recv request core identification tag specify participate consumer independently core program multi cast data identification tag consumer enable core highlight MNI SU core hardware request aggregation request participate consumer core MNI SU dynamically construct consumer data scratch pad identification tag consumer request  external memory interface enables MNI LU multiple core request data memory multi cast transfer IV rapid chip training inference SYSTEMS architecture core rapid chip data format FP FP programmable bias FP int int advantage algorithmic approximation training quantize inference core core core core CW  chip cmu  cycle cycle generation management core pll pll   throttle core rapid architecture rapid chip consists core directional clockwise CW counter clockwise  core asynchronous domain core  optimally balance performance compute data movement core communicate memory memory interface interface across asynchronous boundary enable chip management cmu within rapid chip multiple chip manycore finally rapid chip module workload aware throttle via skip fully utilize chip budget maximum application performance core rapid chip euv technology rapid chip fabricate euv technology nominal voltage achieves tflops HFP TOPS int inference training rapid chip architecture core construct multicore multi chip inference training AI training chip core estimate inference training performance across architecture configuration developed detailed performance model rapid chip calibrate within measurement performance int inference core rapid chip model TOPs ghz HFP training model TOPs rapid chip core ghz bandwidth interconnection communicate gradient update phase training software architecture building deploy AI beyond accelerator core AI balance diverse critical requirement deliver sustain performance processing efficiency across workload flexibility cater future workload rapidly evolve integrate seamlessly within exist AI software ecosystem preserve user productivity overview software stack AI chip compile execution extension pluggable exist framework leverage exist capability DL framework enables aggressive accelerator specific performance optimization software stack inference training component software stack graph compiler automatically identifies execute dnn graph AI chip construct program binary execution runtime trigger manages execution compute data transfer operation AI chip compilation systematic exploration perform focus graph optimization scratchpad management assignment core AI chip exploration bandwidth centric analytical performance model AI chip prune identify profitable mapping choice operation AI chip performance model validate hardware measurement therefore AI multiple core chip context training inference RESULTS experimental methodology summarize performance compute efficiency achieve inference training rapid chip architecture experimental methodology performance estimation estimate performance across architecture configuration developed detailed performance model rapid chip calibrate within measurement consume dnn primitive convolution relu etc silicon combine project utilization component MPE array sfu scratchpad others estimate compute efficiency TOPS configuration inference rapid chip core described IV attach external ddr memory gbps bandwidth training distribute  rapid chip IV core MB distribute scratchpad attach bandwidth memory HBM gbps bandwidth chip chip interconnect bandwidth gbps sensitivity performance inference training benchmark dnn benchmark multitude application domain image classification vgg resnet inceptionv inceptionv MobileNetV imagenet dataset detection ssd yolov yolov coco dataset bert sequence WMT dataset layer lstm  PTB dataset layer bidirectional lstm  dataset experimental setup baseline batch inference minibatch training comparison FP implementation rapid identical configuration baseline report relative improvement achieve precision performance report mlperf FP baseline competitive accelerator normalize technology absolute inference latency training throughput input per achieve precision inference limit FP fwd int precision reserve int implementation future model accuracy loss mention II addition benefit sparsity aware frequency throttle context dnns prune FP precision combine prune ultra precision evolve research inference performance efficiency inference latency shade contour achieve core rapid chip across benchmark FP format int precision FP baseline rapid FP int implementation achieve average average improvement toend performance respectively speedup precision primarily limited operation execute FP viz layer activation function normalization pool operation others image classification detection benchmark compute convolution layer achieve improvement mobile network lean convolution significant auxiliary operation benefit sustain compute efficiency TOPS achieve FP int precision shade contour thanks micro architectural circuit rapid TOPS across precision FP implementation achieve average TOPS int achieves average classification per core rapid chip TOPS across benchmark amount improvement FP baseline sustain TOPS core rapid chip batch rapid chip achieves sustain TOPS TOPS inference FP int precision training throughput training throughput input per across benchmark FP  precision training rapid chip IV FP HFP speedup average unlike inference availability mini batch achieve core utilization reduce precision convolution gemm operation however overall speedup training slightly inference due factor training incurs additional chip communication gradient reduction broadcast training memory intensive activation pas retain compute gradient propagation benefit sparsity aware throttle improvement performance achieve sparsity aware zero grate frequency throttle scheme described budget rate frequency throttle apply sparsity core rapid chip derive silicon measurement apply scheme context inference publicly available sparse throughput chip rapid training prune model throttle ascertain compile prune version benchmark prune model FP precision combine prune precision evolve research explore future average sparsity speedup achieve baseline sparsity aware throttle across benchmark average sparsity varies across layer network negligible loss accuracy correspondingly achieve average improvement performance operating frequency performance benefit sparsity aware throttle performance breakdown analysis factor impact performance rapid int inference breakdown compute cycle category viz conv gemm conv gemm overhead quantization auxiliary operation category constitutes conv gemm operation execute MPE array leverage compute capability rapid chip layer execute int precision layer execute FP preserve accuracy category capture overhead conv gemm execution overhead factor dataflow inefficiency due spatio temporal  MPE array workload dimension imbalance assign core  others category additional quantization operation perform convert data FP int throughput MPE array precision overhead becomes non trivial activation category auxiliary operation activation function batch normalization etc execute sfu FP breakdown compute cycle int inference benchmark heterogeneous respect compute cycle expend category dnns inception yolov lstms incur overhead conv gemm operation workload dimension exactly dimension MPE array convolutional network activation incur quantization overhead mobile network MobileNetv yolov auxiliary operation average conv gemm occupy compute cycle conv gemm overhead quantization auxiliary operation amount respectively inference training speedup achieve inference training inference increase core chip speedup int precision training increase chip speedup HFP precision mini batch performance core compute intensive benchmark vgg resnet yolov ssd performance improvement core benchmark auxiliary operation dominate MobileNetv memory stall dominate due TOPs int saturation speedup increase core fix external bandwidth core similarly speedup HFP training increase chip chip chip bandwidth gbps data parallelism hence communicate gradient update phase training HFP reduces communication overhead pas chip concurrently computes update portion communicates update performance inference training VI related improve efficiency AI workload hardware platform vibrant topic research related research effort accelerate AI workload CPUs gpus accelerator commercial AI chip cpu technique accelerate AI workload CPUs optimize linear algebra library technique efficient parallelization multicores efficient data layout batching recent effort propose compiler ISA micro architectural optimization exploit AI workload sparsity gpu technique research effort accelerate AI workload gpus focus data model pipeline parallelization technique memory management locality aware device placement CPUs effort explore exploit sparsity activation hardware accelerator technique specialized hardware  computational AI workload recognize myriad accelerator ASIC  cgra core propose architecture demonstrate impressive peak processing capability dense arithmetic array heterogeneous processing tile precision data representation sometimes dynamic hardware reconfiguration recent effort explore exploit sparsity activation 3D memory technology serial architecture memory computation boost efficiency commercial AI chip immense specialized AI accelerator driven commercial effort google TPUs nvidia tensor core intel nnp others AI accelerator performance across generation pivotal commercial deployment AI workload unique opportunity performance improvement precision exploit knob rapid mixed precision architecture capable training inference vii conclusion core AI chip rapid ultra precision training inference rapid mixed precision execution format viz FP hybrid FP int int rapid workload coverage cnns lstms transformer scalable multiple core chip silicon measurement core rapid chip demonstrates tflops FP mode TOPS int mode performance model calibrate within measurement evaluate FP training TOPs AI comprise rapid chip int inference rapid chip int inference batch yield average TOPS FP training mini batch achieves sustain average tflops across application future int performance rapid sparsity aware throttle ultra precision