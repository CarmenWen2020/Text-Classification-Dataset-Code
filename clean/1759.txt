neural network dnn variety application service evolve dnns optimal hardware datacenter purpose multi core CPUs unique attractive advantage dnn inference datacenter cpu pipeline complexity target towards optimize purpose thread performance  relatively simpler hugely important data parallel dnn inference workload address disparity efficiently enable raw performance overall performance watt improvement multi core cpu dnn inference reduct innovative bypass traditional cpu resource impact dnn inference limit performance fundamentally reduct policy enables consecutive execute reduct enables instruction delivery decode execution instruction execution data ISA extension encode  loop workload behavior enable effective bypass hungry stage OoO cpu pipeline per core performance efficiently distribute lightweight tensor compute cache multi cache hierarchy maximizes cumulative utilization exist architectural bandwidth resource minimizes movement data across dnn model reduct achieves increase convolution performance watt raw performance similarly reduct achieves increase inner performance watt performance reduct performance achieve increase cache capacity bandwidth mere increase crucially reduct operates entirely within cpu program memory model simplify software development achieve performance ofthe domain specific accelerator dsa dnn inference choice AI era introduction data centric paradigm compute machine ML neural network dnn pervasive endeavor hardware dnn execution custom domain specific accelerator dsa programmable FPGAs purpose gpus CPUs goodness hardware platform judged multiple parameter performance throughput latency deployment factor software ecosystem programmability hardware platform innate goodness subset parameter none currently optimal academic research development hardware platform improve offering cpu vendor invest hardware software towards improve dnn inference intel announce compute directly target matrix operation advanced matrix extension  despite dedicate neural accelerator   cpu lightning core extend armv ISA furthermore additional multiple data format int BF cpu  dnn algorithmic research performance library cpu vendor continuously developed tune extract maximum performance cpu compute intel mkl dnn library achieve performance improvement cpu configuration smart algorithm processor centaur cpu frontier CPUs currently constitute significant compute resource employ dnn inference prevalence CPUs already datacenters facebook provision peak load conjunction diurnal load cycle abundant availability cpu compute dnn inference programmable purpose CPUs mature ecosystem program model enable development deployment CPUs latency benefit mlperf inference rely driver offload dnn task enable interaction dnn non dnn task realtime inference dnn task limited parallelism recurrent nns naturally CPUs frequency UI OOVBM  PNQVUFS SDIJUFDUVSF acm annual international symposium computer architecture isca doi isca memory dram HBM core performance compute eciency reuse  spare bandwidth dominate core pipeline memory dram HBM core performance compute eciency roi spare bandwidth primarily core pipeline cache data movement compute  bandwidth  ops byte matrix matrix   ops byte matrix vector   CPUs multi memory hierarchy inference primitive OoO cpu pipeline expose instruction parallelism ILP thread performance significant instruction unrolled hardware everyone hungry pipeline stage compute monolithically atop serially access multi cache hierarchy minimize average load latency load cache restrict primary source bandwidth stark disparity cpu requirement thread performance limited ILP  primitive convolution inner heavily data parallel dnn performance raw compute throughput data throughput bandwidth generational compute CPUs achieve via intra core multi core bandwidth compute however depends primitive compute intensity ops byte bandwidth vice versa dnn usage topology evolve increase heterogeneity ops byte compute bandwidth requirement II monolithic core centric cpu organization sub optimal performance resource utilization execute dnn primitive diverse ops byte requirement matrix matrix primitive convolution illustrate typically ops byte reuse rate cache delivers sufficient bandwidth compute resource however due serialize access hierarchy compute cache bandwidth limit despite cache bandwidth heavily underutilized matrix vector primitive inner ops byte rate bandwidth cache delivers insufficient bandwidth compute resource serialize hierarchy unnecessarily introduces bottleneck despite cache rate heavily underutilized bandwidth cpu execution  instruction instance pipeline stage fetch decode rename rat dispatch RS stage expose ILP extremely hungry rake consume II hardware complexity stage useful highly repetitive data parallel fix iteration dnn kernel reduct employ policy enables consecutive execute reduct enables instruction delivery decode execution instruction execution data reduct proposes reduct extension rSX ISA extension encode structure loop workload behavior allows effective bypass hungry stage OoO cpu pipeline tensor functional TFU core fetch decodes bulk offload decode tensor load compute TFUs instruction delivery decode execution TFUs distribute cache multi cache hierarchy  bind serialization hierarchy allows bypassing inner cache instruction execution data reduct cache approach maximizes cumulative utilization exist architectural bandwidth resource demand increase cache capacity bandwidth crucially reduct operates entirely within cpu program memory model exist virtual memory hardware coherence along hardware memory consistency distribution compute leverage simultaneous multi thread smt cpu capability contribution address disparity requirement thread performance dnn workload reduct distribute lightweight tensor functional cache reduct maximizes efficient utilization exist cumulative bandwidth minimizes data movement reduct minimal additional hardware increase cache capacity bandwidth cpu performance dnn DSAs develop reduct extension ISA condense encode multiple loop fix iteration information allows unnecessary hungry stage cpu pipeline bypass unroll execution perform proximity data drastically improve achieve performance watt knowledge reduct practical implementation memory processing exist hardware software architectural interface exist cache smt hardware context cpu program memory model evaluate across multiple dnn model reduct achieves improvement convolution performance watt raw performance similarly reduct achieves increase inner performance watt performance exist cpu program model increase cache capacity bandwidth mere additional reduct enables unprecedented cpu efficiency gain TCO saving datacenters performance ofthe dnn DSAs II characterization opportunity perform depth performance characterization multiple primitive dnn inference cpu configuration goal derive insight efficient performance performance watt program execution model dnn model specify framework tensorflow caffe pytorch mxnet  etc framework developer easy apis topology model parameter abstract away underlie hardware framework leverage highly optimize platform specific library intel oneDNN mkl dnn amd   compute library nvidia cudnn extract maximum performance underlie hardware inner loop dnn primitive typically implement vectorized highly optimize  manner optimal performance maximum data reuse outer loop primitive compute output parallelize establish thread openmp TBB distribute across compute core target hardware parallel output kernel  subset output load load load load load input mac mac mac mac loop loop output parallel reuse input reuse output reuse  input reuse reuse output  input output peak reuse   output output channel convolution kernel performance analysis evaluate oneDNN primitive focus int data seminal precision sufficient inference accuracy precision reduces dependence expensive chip dram bandwidth enable focus cache bandwidth compute intensity ops byte dnn primitive fundamental role bandwidth requirement hence compute efficiency metric evaluate multiple abstraction algorithm theoretical peak ops byte perform infinite register file RF data convolution reuse across input input channel output similarly input output peak reuse opportunity kernel software implementation extract reuse finite RF compute core RF data implement RF usage determines minimum load execute kernel evaluation methodology configuration workload described detail IV kernel parallel output load compute instruction loop compute output parallel smt core smt core program baseline multi core processor load reuse across input inner loop iteration innermost loop reuse compute output RF hardware cache rate bandwidth delivery core cache data movement kernel execute underlie hardware define data movement overhead introduce hardware ratio cumulative cache data movement eviction load compute core RF kernel analyze summarize metric convolution FC primitive highlight convolution characterization evaluate optimize oneDNN implementation fuse convolution relu non linear function output primitive detail characterization ops byte matrix matrix convolution primitive across convolutional layer resnet insight highlight kernel employ output stationary data maximize output reuse RF bandwidth  input reuse variability across layer subsume within RF fairly steady load mac instr requirement across layer average rate compute efficiency MACs cycle peak eviction cache average overhead data movement conv layer resnet rate data movement overhead performance MACs cycle performance opportunity rate available bandwidth load mac inst mac inst core cycle load cycle core furthermore bandwidth hugely utilized due cache hierarchy  resnet convolution transformer inner characterization resnet convolution transformer  metric avg min max avg min max ops byte algorithm input output memory transaction instr kernel load hardware cache rate hardware data movement overhead hardware performance peak MACs cycle core ops cyc  tensor compute cache enable performance without increase overall capacity bandwidth directly cache reduces data movement cache load mac instr requirement peak bandwidth cache determines peak compute cache inner characterization inner primitive involve matrix vector operation peak ops byte convolution primitive prominent recurrent dnn model sequence sequence model transformer heavily application processing detail  characterization layer transformer primitive rate couple load mac instr bandwidth requirement compute efficiency achieve MACs cycle furthermore  overhead cache data movement performance opportunity rate rate MB MB per core significantly tensor compute directly cache bypassing entirely leverage rate bandwidth compute along eliminate data movement performance increase cache capacity bandwidth pool concat evaluate pool dimensionality reduction concat primitive mainly involve data movement data reuse model densenet pas feature output directly later layer input concat primitive data execution cache reduce data movement analysis contribution various cluster cpu CPUs unroll instance instruction loop stage cpu pipeline instruction fetch decode allocation dispatch register allocation rename rat dispatch RS extremely expensive core compute bound resnet stage contribute bandwidth bound resnet transformer  cpu FE RS rat MACs cache access data movement overhead opportunity  consumption convolution dominate resnet inner dominate transformer II characterization summary dnn primitive primitive observation data movement overhead opportunity convolution bandwidth provision compute utilization bandwidth mostly perform tensor compute cache inner compute provision bandwidth  KB tensor compute cache pool concat data reuse mostly data movement execute outer cache dominate unrolled fetch alloc dispatch exploit structure  kernel encode multiple loop inner primitive transformer contribute cache access data movement another opportunity structure loop fix iteration dnn kernel loop unroll lean scheduler tensor compute macro ISA encodes loop information cpu offload multiple loop decode unrolled lean cache compute effectively bypassing hungry stage legacy cpu pipeline execution summary II concludes summarize observation performance optimally execute primitive cache requirement performance resource utilization benefit furthermore leverage structure workload bypass hungry stage legacy cpu pipeline preferably unroll schedule pre decode instruction execution reduct reduct tensor functional TFU cache depict combination reduct extension rSX ISA goal efficiently leverage exist resource maximize minimize performance benefit crucially reduct retains exist cpu program memory model significantly development deployment effort detail architectural micro architectural program model aspect reduct reduct architectural reduct extension rSX examination oneDNN kernel implement dnn primitive opportunity encode multiple loop information succinctly optimization enable minimize bypassing  stage cpu pipeline unroll dispatch proximal execution reduct multiple meaning reduce memory simplify relevant overview reduct depict meta data information instruction encode loop behavior kernel loop iteration ISA limit maximum loop encode loop sufficient kernel capture load compute operation instruction loop resides within load execute outer loop load address address stride loop resides compute dnn primitive implementation employ structure data layout maximize cache width capacity finally data dependence register hence stride per loop destination register iteration innermost loop reuses load parallel output kernel  subset output       input     loop loop parallel output meta data encode loop  loop ISA limit loop  valid loop instr default loop input load mac loop load outer loop rSX loop encode instr address load address stride load dst reg stride default loop requirement encode loop information instruction rSX enable kernel comment  flush TFU code register core  loop loop iteration calculation baseline ISA execute core  iteration outer loop loop calc loop iteration calculation baseline ISA execute core  iteration inner loop loop calculate  rSX instruction execute TFU cache  disables outer loop previously allocate rSX instr  calculation baseline ISA execute core   previously allocate rSX instr stride calculation baseline ISA execute core  stride loop previously allocate rSX instr calc  rSX instruction execute TFU cache reduct rSX instruction execution semantics outer loop compute output register destination register stride output compute innermost loop illustrates rSX instruction semantics kernel instruction tag rSX denote cache TFU execution decode allocate TFU code register core examination primitive across multiple dnn model register sufficient however kernel instruction loop split kernel within constraint rSX instruction       respective meta data loop information instruction tag rSX meta data information calculate regular ISA baseline kernel currently  instruction flush TFU code register rSX tag instruction  instruction dispatch TFU code register cache TFU unrolled execution conservatively estimate TFU code register information opcode register register address loop iteration valid address register stride entire offload cycle offload bus width amortize cycle unrolled execution TFU tensor functional TFU depicts tensor functional TFU TFU code register lean unroll scheduler populates entry issue queue compute opcodes another load simplifies schedule allows hoist load compute hide load latency maintain strict load within TFU load directly access cache TFU proximal bypassing inner snoop inner cache handle coherency cache translation cache assist memory management schematic tensor functional TFU kernel characterization performance analysis entry TFU data register file per TFU sufficient register rename load mac instr requirement workload cache bandwidth bound peak compute width mac execution TFU evaluate performance implication compute width TFU analysis IV leverage smt server CPUs intel amd smt ibm sparc however compute across smt thread dnn framework disable smt thread per core primitive rely multi core compute instead reduct TFU essentially lean compute directly access cache hierarchy leverage smt bind TFU exclusively logical smt thread physical core therefore TFU fully capable OS visible hardware context dnn framework distribute across TFUs exist thread runtimes enables grain cache TFUs primitive rSX ISA TFU smt usage reduct maintain cpu memory model kernel parallel output load compute rSX instrs loop compute output parallel parallel core distribution core smt thread TFU TFU TFU smt smt  rSX instruction offload TFUs non rSX instruction execute exist core functional rSX rSX rSX reduct program leverage smt bind TFU OS visible thread dispatch rSX TFUs reduct micro architectural virtual memory TFUs virtual memory physical translation cpu cache physically tag oneDNN optimization layout customize compute spatial locality tensor access core tlb rate entry local translation cache TC per TFU recently virtual physical mapping achieve rate TFU snoop local core tlb TFUs cache physically core TC rate performance impact TC tlb snoop coherence reduct operates cache coherent address hardware coherence exist framework minor addition cache controller TFU requirement core valid denote ownership local TFU ability generate request ownership rfo similarly TFU directory entry distinguish owner exist core local TFUs additional core valid per TFU controller generate  generate local TFU snoop due  exist coherence framework oneDNN output stationary kernel TFU operates output therefore negligible increase snoop traffic limited output cacheline distribute cache cache CPUs distribute across multiple core TFU per core slice core cache private per core duplication data across data duplication across slice however reduct duplication TFUs compute respective output traverse interconnect address available locally cripple TFU performance significant extra data movement overhead enhance exist service technology intel partition portion  cache subset local cache attach TFU performance sensitivity reserve local cache capacity TFU memory within TFU strict load maintain tso within thread non TFU operation TFU bulk offload allocate till TFU bulk offload operation bulk offload cycle TFU rSX ISA amortizes performance serialization non TFU load maintain core TFUs smt thread guarantee execution load across TFUs knowledge reduct memory proposal fully hardware memory consistency model cpu context switch exception dnn usage model micro service datacenters context switch exception extremely unlikely however TFUs signal exception exist functional compute physical core TFU context TFU code register TFU data register context restore reduct program model generate rSX code performance library already implement primitive optimize code  kernel instruction tag rSX incorporate ISA enhancement avx avx  already compute various loop variable programmer aware TFU cache rSX instruction eventually execute optimize compiler generate rSX code native without  explore expose reduct capability cache presence TFUs cache correspond compute width expose  interface processor optimal TFU selection primitive characterize summarize II analysis primitive optimal TFUs performance efficiency leverage smt binding TFU logical thread exist openmp apis affinity primitive subset core specifically KMP affinity llvm openmp runtime achieve dnn framework tensorflow caffe etc invoke oneDNN primitive distribution across TFUs cache across multi hierarchy bandwidth correspond TFU compute width compute bound primitive convolution across TFUs proportionate compute strength optimal performance cache rate predictable performance static sufficient workload towards introduce schedule static asymmetric llvm openmp runtime TFUs compute strength ratio static unequal thread completion weak TFU runtime static asymmetric distribution ratio thread optimally minimal software stack reduct software stack reduct dnn framework tensorflow caffe pytorch KMP thread affinity appropriately primitive performance library oneDNN rSX extension tensor load compute instruction thread runtime openmp asymmetric static schedule appropriate convolution summarizes intercept overall software stack reduct reduct operates entirely within cpu program memory model IV evaluation methodology simulation framework modify version sniper cycle accurate multi core simulation model intel core datacenter processor smt parameter IV baseline peak MACs cycle core compute intel DL boost per core cache bandwidth simulation framework thoroughly validate silicon execute evaluate oneDNN primitive verify performance trend cascade ubuntu oneDNN library CACTI McPAT quantify cache overall impact IV simulator parameter core 6GHz smt entry rob MACs cyc core cache private associative lru  MSHR data access cycle tag lookup cycle cache private MB associative lru  MSHR data access cycle tag lookup cycle cache distribute non inclusive MB slice associative   MSHR per slice data access latency cycle tag directory  cycle latency mesh noc XY rout cycle latency max reduct model reduct sniper sweep peak compute TFU cache smt thread TFU prefix reduct traditional monolithic core indicates peak MACs cycle core configuration notation attache explicit distribution compute resource across cache notation reduct configuration MACs cycle core distribution VI breakdown TFU register MACs TC queue byte configuration mac MACs cycle core specifies hardware resource distribution reduct configuration requirement reduct synthesize verilog implementation TFU instance capable MACs cycle TFU configuration synthesis TSMC library target ghz synopsys compiler per TFU detailed  VI additional KB core storage core valid coherence overhead sum coherence storage TFUs core project overhead technology intel xeon server chip built conservatively estimate overall overhead due reduct mere xeon core plot reduct avx already core deliver peak MACs cycle core cumulative compute workload software stack evaluate dnn topology resnet densenet mobilenet resnext transformer  transformer comprises solely inner layer others mostly convolution layer source oneDNN intel compiler ensure evaluate software implementation dram multi core evaluation int inference model cache couple reuse overall impact performance dram workload parallelize openmp multi thread framework static asymmetric schedule performance data movement impact reduct cache compute dnn primitive resnet transformer model detailed performance reduct resnet transformer dnn topology evaluate summarize performance performance watt goodness reduct conclude available mlperf data gpus DSAs aggregate cache bandwidth  data movement overhead achieve MACs cycle core MACs cycle core aggregate  data movement overhead peak  ISA compressibility compressibility rSX ISA reduct reduct reduct reduct achieve MACs cycle core data movement overhead peak  ISA compressibility perf data movement rSX ISA impact reduct resnet convolutional layer transformer inner layer convolution impact reduct resnet convolutional layer reveals multiple observation traditional compute plateau average MACs cycle core onwards bandwidth saturates despite available bandwidth however reduct performance achieve performance baseline cumulative bandwidth utilization peak compute reduct performance achieve peak performance peak bandwidth rate achieve performance contrast configuration peak bandwidth cache hence performance define compressibility reduction dynamic instruction core FE rat usage rSX ISA essentially bulk offload opportunity rSX ISA achieves average compressibility translate saving peak compressibility data parallel loop actually rSX instruction loop meta data reduces compressibility finally reduct reduces data movement overhead mainly interface conv resa resa brancha resa branchc resb brancha resb branchb resb branchc resc brancha resc branchb resc branchc resa resa brancha resa branchb resa branchc resb brancha resb branchb resb branchc resc brancha resc branchb resc branchc  brancha  branchb  branchc resa resa brancha resa branchb resa branchc resb brancha resb branchb resb branchc resc brancha resc branchb resc branchc  brancha  branchb  branchc  brancha  branchb  branchc  brancha  branchb  branchc resa resa brancha resa branchb resa branchc resb brancha resb branchb resb branchc resc brancha resc branchb resc branchc  layer resnet compressibility  ISA  achieve MACs cycle core achieve MACs cycle core achieve MACs cycle core conv resc branchc achieve MACs cycle core achieve MACs cycle core rSX ISA compressibility per resnet convolution layer reduct performance configuration compressibility rSX ISA per layer performance compressibility convolution layer resnet configuration spar layer conv layer resc branchc configuration achieves peak performance across layer initial layer suffer cache rate due model load layer ops byte TFUs rate KB local partition cache traffic increase reserve improves performance layer compressibility increase increase input channel dimension due accumulation per output generally kernel kernel due input reuse performance factor resource inner plot average impact reduct configuration performance data movement transformer inner layer ops byte bandwidth bound primitive reduct configuration schedule thread selectively cache merely execute inner directly MB rate therefore bandwidth delivery reduct achieves performance reduction data movement overhead traffic eliminate execution KB local cache reduces data movement however provision capacity compute increase performance reduct execute primitive improves performance achieve baseline achieve increase cache bandwidth simultaneously reduce data movement overhead finally rSX ISA achieves compression directly translate saving pool concat layer brevity summarize observation evaluation pool concat primitive execute resc resnet pool layer solely reduces data movement overhead similarly concat layer densenet average data movement overhead overhead via execution detailed analysis detailed analysis resnet convolutional layer transformer  layer baseline configuration component relative configuration deconstruct impact cache rSX ISA component reduct proposal separately resnet FE stage cpu pipeline dominate overall twice resource iso cache capability cache compute reduces inter cache data movement increase access  cache therefore cache compute iso baseline rSX ISA proposal achieves average compression translates reduction FE stage pipeline reduct configuration therefore operates baseline translate decrease along increase performance bandwidth bound ops byte inner layer transformer data movement reduction primarily eliminate brings reduction despite data reuse convolution rSX ISA proposal achieves compression bypassing FE pipeline stage reduction rSX ISA rSX ISA reduct rSX ISA rSX ISA rSX ISA reduct rSX ISA resnet transformer  FE RS rat MACs cache access data movement minor  due data movement  rSX ISA eliminates core overhead rSX ISA reduce rSX ISA reduce rSX ISA eliminates core overhead data movement    consumption reduction overall consumption performance performance performance impact reduct  respectively dnn inference topology inner transformer bandwidth bound achieves performance iso reduct  dnn model mostly convolution layer exception densenet topology around performance compute increase around performance compute densenet concat layer nearly runtime reduct reduces data shuffle primitive impact performance slightly performance improvement densenet reduct slightly convolution dnn topology densenet resnext mobilenet resnet  transformer dierence speedup speedup densenet resnext mobilenet resnet  transformer dierence speedup speedup overall perf improvement difference dnn topology reduct configuration relative primary axis plot performance secondary axis plot performance improvement baseline  baseline rSX ISA reduct compute MACs cycle core rSX ISA rSX ISA monolithic rSX ISA resnet overall performance summary performance per watt benefit succinctly summarizes goodness reduct resnet performance configuration normalize baseline performance watt ppw improvement across configuration capture ratio respective coordinate monolithic performance saturates ppw improvement rSX ISA improves ppw equivalent monolithic configuration reduct leverage cache compute rSX ISA enable performance available TDP maintain ppw gain rSX ISA brings inner topology transformer reduct achieves increase inner performance watt improvement performance comparison reduct DSAs gpus reduct platform dnn inference  publicly available throughput resnet mlperf inference query variety accelerator habana lab intel  tpu gpus titan RTX TX cascade intel xeon platinum processor cpu baseline normalize data per node accelerator gpus per socket CPUs platform incredibly reduct achieves comparable somewhat performance DSAs intel  google tpu performance achieve mere additional maximize usage exist cache capacity bandwidth cpu program memory model  goodness contrast discrete DSAs compute cache budget performance watt com nvidia titan RTX habana tpu intel  intel xeon reduct throughput query sec mlperf inference throughput gpus accelerator CPUs throughput reduct project  due absence metric mlperf significant improvement cpu performance watt reduct VI reduct CPUs verify performance benefit reduct across compute width cache bandwidth compute mac cycle core cache bandwidth typical CPUs CPUs additional challenge typically shallower cache hierarchy multiple core cache reduct TFU per core cache compute strength compute strength across TFUs proportional bandwidth core typically smt capability hence core simultaneously schedule TFUs tso memory however relaxed consistency ML workload socs budget latency battery requirement reduce roi specialized dsa hardware  offload reduct excellent cpu performance metric vii related data processing active research topic span dram cache recent emphasis site processing practical approach enable processing memory dram 3D stack HBM HMC SSDs direction compute cache neural cache duality cache recent propose convert cpu cache compute capable serial  operation SRAM sub array gain involve highly optimize SRAM sub array degrade complicate signal integrity density floorplan  propose data centric architecture task cache closest data source architecture limitation  irregular workload data presence across hierarchy task inference workload multiple tensor access regular  proposes program model application rewrite reduct compute cache approach reuse exist micro architectural interface extract improvement performance exist program model perspective reduct  explicit instruction encode repetitive loop bypassing RS rat pipeline stage summary amount data analyze grows exponentially OEMs willing harness available compute tackle computational CPUs dominant platform choice dnn inference datacenters depth analysis reveals significant opportunity efficient cpu resource utilization drastic improvement performance watt ability performance without increase cache capacity bandwidth combination ISA enhancement tensor compute cache cpu reduct cpu dnn inference performance performance watt maintain program memory model reduct fundamental imagination purpose cpu AI era