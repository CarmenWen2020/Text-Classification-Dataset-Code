distribute training enable development complex neural network model datasets sophisticated task distribute stochastic gradient descent intensively invokes reduce operation gradient update dominates communication iterative training epoch identify inefficiency widely allreduce algorithm opportunity algorithm architecture propose MULTITREE reduce algorithm topology resource utilization awareness efficient scalable reduce operation applicable interconnect topology moreover network interface schedule coordinate reduce message contention communication synergy algorithm simplify exploit bulk data transfer gradient exchange evaluate reduce data synthetic demonstrate effectiveness various interconnection network topology addition neural network workload MULTITREE achieves communication speedup training reduction reduce approach respectively index distribute data parallel training reduce interconnection network algorithm architecture codesign introduction onset data era rapid advance accelerator architecture enable application achieve  accuracy complex image recognition processing autonomous dnn model GPT billion parameter trillion compute operation gigabyte storage massive bandwidth recent project magnitude growth dataset model exceed accuracy epoch model data explode dnns evolve deeper crucial scalable fulfill trend compute requirement research NSF grant grid specialized accelerator deployed dnn model parallel distribute manner data parallelism easy model parallel distribute compute widely dnn training stochastic gradient descent sgd typical optimization algorithm improve dnn accuracy iterative training intensively invokes reduce communication dominant component communication reduce stall computation training epoch significantly reduce quickly become bottleneck distribute training communication algorithm propose reduce operation baidu research implement bandwidth optimal reduce algorithm later nvidia collective communication library NCCL popular framework however reduce suffers latency resource utilization network topology instance link utilization rate 2D torus network attempt improve reduce latency reduce algorithmic halve reduces latency recursive distance halve  phase respectively binary  implement NCCL improves latency reduction broadcast algorithm perform reduce medium message however message significant network congestion communication poorly physical network topology therefore crucial physical network topology reduce algorithm message schedule achieve latency medium message contention communication data nvidia NCCL enables binary message disables reduce message threshold tune UI OOVBM  PNQVUFS SDIJUFDUVSF acm annual international symposium computer architecture isca doi isca comparison reduce algorithm algorithm data data apply latency bandwidth contention various topology optimal none  optimal topo oblivious 2D sub optimal none 2D torus mesh  optimal none bigraph MULTITREE optimal none recently dedicate network accelerator pod deployed accelerate emerge application tpu  computation acceleration significantly communication specialization architecture algorithm codesign infancy adopt 2D reduce 2D torus network tpu cluster fully utilize link reduce communication although achieve link utilization 2D increase amount communicate data optimal communicate data network 2D torus network 2D transmits data communicates data recently alibaba propose  training platform algorithm server architecture extends halve algorithm rank mapping  stage fully bigraph topology avoid contention promising potential codesign approach however trivial due connection switch algorithm limited specific topology 2D torus bigraph generalize network topology trend deeper dnn model accelerator grid deployed distribute training therefore scalable synergy various topology practically interconnect node moreover communication acceleration specialization urgently computation throughput addition lack hardware coordination communication schedule potential optimization opportunity improve performance furthermore grain arbitration purpose network inefficient gradient exchange extra performance significant overhead summarizes comparison reduce communication algorithm interconnection architecture efficient scalable reduce operation propose MULTITREE scalable topology aware reduce algorithm applicable various topology MULTITREE couple construction message schedule topology global link utilization awareness fashion leverage insight closer sparse closer leaf denser MULTITREE communication closer communication closer leaf sparse communication balance moreover network interface accord propose communication algorithm facilitate reduce schedule management achieve contention reduce simplify arbitration exploit characteristic gradient reduce operation MULTITREE tackle limitation previous summarize summary contribution identify inefficiency allreduce algorithm reduce algorithm interconnect hardware gradient exchange propose MULTITREE reduce algorithm applicable various interconnect topology couple construction communication schedule topology global link utilization awareness efficiently coordinate concurrent reduction broadcast augment network interface hardware schedule MULTITREE facilitate lockstep communication schedule simplify dedicate gradient reduce evaluation synthetic message ofthe dnns MULTITREE greatly improves scalability prior achieves communication speedup training reduction reduce approach respectively organize II introduces background motivation propose MULTITREE reduce algorithm architecture detailed IV methodology described evaluation VI respectively discussion outline vii related finally conclude IX II background motivation introduce background  neural network training reduce communication distribute stochastic gradient descent synchronization motivate research outline limitation exist reduce algorithm data parallel neural network training dnn training usually stochastic gradient descent training sample propagation gradient calculation backward propagation backward propagation gradient update dnn model minimize loss function training faster mini batch pas update mini batch training sample daunt task dnns amount data distribute training perform multiple compute node compute node equip gpus accelerator creates challenge regard resource usage communication bandwidth provision computation node node node node node node node node reduce scatter node node node node reduce scatter reduce storage parallel strategy enable scalable efficient distribute training data parallelism distribute dnn training non overlap training sample distribute compute node node calculates gradient training gradient aggregate update centralize decentralize approach centralize approach relies parameter server node periodically report compute parameter update parameter server however parameter server efficient bandwidth latency dnns alternative decentralize approach compute node exchange parameter update via reduce operation reduce algorithm important role widely reduce topology achieve contention optimal bandwidth topology typically embed network topology however latency optimal reduce distribute stochastic gradient descent baidu popularize reduce sequence reduce scatter operation reduce scatter operation optimize exploit hierarchical communication bandwidth heterogeneous network architecture assume tensor node node reduce scatter node iteration node node tensor aggregate tensor aggregate iteration iteration node node iteration node node iteration tensor aggregate node similarly node iteration reduce node reduce node respectively iteration reduce scatter sequence reduce scatter operation operation similarly iteration node node node namely similarly iteration node iteration node node subsequently node node iteration iteration node aggregate motivation widely reduce  suitable gradient exchange however link utilization topology torus mesh furthermore suffers latency attempt improve link utilization latency 2D reduce utilizes link reduces communication 2D torus mesh network transmits twice amount data bandwidth optimal algorithm instance 2D communicates data reduce communicates data 2D torus network binary algorithm logical binary reduce latency medium message construct leaf node internal node therefore data node data simultaneously outperform reduce utilizes node bandwidth network topology medium message message significant contention mapped physical network topology severe unfriendly topology torus recently propose  extends halve mapping rank node achieve contention communication datasets communication involves node upper switch node switch exploit  distance node switch fail expedite latency sensitive communication message furthermore algorithm apply network topology achieve performance latency bandwidth typical interconnection network purpose communication grain message however generate packet gradient extra bandwidth arbitration overhead mitigate overhead streamline advantage gradient conventional packet packet consists packet overhead packet payload byte byte flit byte flit byte flit cmp noc chip interconnection network packet flit bandwidth overhead flit flit metadata flit payload flit incurs bandwidth overhead chip network payload packet cache whereas chip network payload varies byte byte flit incur bandwidth overhead distribute dnn training consecutive gradient node packet route consecutive address flit consecutive packet redundant information unnecessary bandwidth overhead simplify exploit distinct characteristic MULTITREE reduce algorithm explain rationale MULTITREE approach illustrate algorithm rationale insight span instead reduce scatter phase reduce node reduction broadcast chunk data reduce node communicates chunk data unidirectional phase node communication structure reduce algorithmic logk ary node propose algorithm  reduces latency construct multiple instead thereby improve reduce scalability topology awareness construct without network topology link utilization performance reduce multiple contend link without careful schedule furthermore closer leaf denser closer reduce leaf reduce scatter phase dense sparse communication contention leaf node MULTITREE exploit insight combine message schedule construction topology link utilization awareness schedule communication sparsify communication leaf addition instead construct construction link allocation reduce scatter schedule reduction schedule broadcast MULTITREE construction link allocation schedule reduce communication mesh network node denote label allocation sequence link label communication node link allocation sequence topology graph link available predecessor link topology graph allocation construction label construct reduce scatter schedule schedule MULTITREE concurrently generate balance global coordination network node finally span communication MULTITREE approach predecessor denser communication balance across construction topology graph allocate link remain node span allocate link remove graph available link remain node topology graph binary broadcast broadcast phase binary algorithm node involve data chunk binary broadcast communication odd illustrate construct schedule mesh network topology graph facilitate construction respectively label indicates global link allocation sequence connects node construction construction sequence node sustain balance sequence node topology graph topology graph creates algorithm newly construct reduce scatter schedule finally adjust generate schedule respectively balance symmetric necessarily structurally symmetric structural symmetry representation node respect remain network applies specific symmetric network moreover network mesh distance source node varies asymmetric height broadcast schedule binary  network respectively MULTITREE longer latency unary span  although logical height MULTITREE physical height deeper connection node hop due mismatch structure physical topology mismatch severe network addition  schedule communication odd node receives sends data simultaneously lengthen completion MULTITREE physical link hop distance demonstration purpose mesh network benefit MULTITREE although network advantage MULTITREE cannot accommodate due limit algorithm MULTITREE reduce algorithm input topology graph output reduce scatter schedule allgather schedule initialization node graph node compute schedule node schedule communication node balance ID ascend node previous node remove schedule message allgather schedule calculate tot compute reduce scatter schedule reverse allgather schedule tot reduce scatter schedule adjust schedule replace tot algorithm formally MULTITREE algorithm understand network extend switch network algorithm description algorithm initializes node network construct schedule phase broadcast instead reduce scatter approach topology graph remove node node predecessor node previous remove topology graph schedule communication alternate ID ascend simplicity symmetric network torus asymmetric irregular network remain height prioritize communication schedule earlier node examine breadth previous predecessor denser dimension dimension torus mesh network structural information asymmetric irregular network future topology graph remain node link allocation procedure allgather schedule schedule construct construct reduce scatter adjust communication reduce scatter direction respect communication algorithm simply revers communication schedule adjust schedule adjust reduce scatter schedule static algorithm dnn workload dynamic node allocate workload complexity analysis expensive algorithm loop schedule construction topology graph core node schedule node algorithm already node pending node graph node indirect network switch network switch node switch indirect network algorithm topology graph adjacency switch switch connection network switch attach node indirect network extend additional node switch switch node connection available node breadth topology component described attach switch node switch multiple node attach switch switch node connection available connection node remove connection node switch connection switch node return available connection switch switch switch node connection available node besides connection remove connection traverse switch switch remove allocate link reduce nil nil nil nil reduce nil nil nil nil reduce nil nil nil nil nil nil nil nil nil accelerator reduce nil nil nil nil reduce nil nil nil nil reduce nil nil nil nil nil nil nil nil nil accelerator reduce nil nil nil nil reduce nil nil nil nil reduce nil nil nil nil nil nil nil nil nil accelerator reduce nil nil nil nil reduce nil nil nil nil reduce nil nil nil nil nil nil nil nil nil accelerator FlowID addr FlowID FlowID FlowID FlowID reduce schedule entry reduce nop FlowID ID reduce schedule addr omit brevity IV architectural  outline communication architecture specialized mechanism MULTITREE reduce operation reduce schedule management network interface NI facilitate MULTITREE reduce schedule algorithm construct data chunk schedule convert schedule per node allreduce schedule entry consists file opcode FlowID ID dependency addition indicates communication initiate addr address gradient message respectively opcodes reduce namely reduce nop reduce operation communication happens leaf internal node reduce communicate accelerator reduce accelerator leaf node reduce accelerator accelerator receives dependent reduce accelerator operation node sends message unless node nop maintain communication lockstep manner link contention without schedule message frequent topology generate imbalanced mesh limit improvement degrade performance destroy schedule therefore mechanism maintain communication lockstep fashion achieve calculate bandwidth ratio network interface network link bandwidth FlowID addr timestep counter nil nil nil nil reduce FlowID addr nil FlowID addr reduce nop dma lockstep counter nop counter reduce idle increment counter dma req dma resp reduce message network data timestep reduce message network reduction logic message network architecture reduce schedule management performance option message passing scheme introduce additional coordination overhead message therefore propose lockstep mechanism implicit coordination exploit static communication reduce message estimate serialization latency assume contention nop insert reduce injection stall estimate although nop link utilized observation happens irregular network leaf fully utilize link prune adjust future exploration addition estimate lockstep mechanism global synchronization across  data minor variation node minimum impact bandwidth bottleneck data serialization latency becomes dominant variance insignificant depicts architecture reduce schedule management injection regulation reduce schedule timestep counter decoder lockstep  conventional NI facility upon reduce operation schedule initialize timestep lockstep counter reset processor configure schedule reduce entry estimate flit num flit  data chunk NI buffer completely otherwise estimate num flit NI buffer translate flit effort utilization link utilized data perfectly divisible aggregate bandwidth inspect timestep counter reduce dependency satisfied operation issue message decode correspond action nop lockstep counter counting estimate reduce addr request dma bulk data transfer data FlowID encapsulate address information data packet communication lockstep counter zero reduce idle timestep counter incremented operation schedule upon reduce message issue reduction logic aggregation aggregation reduce dependency future reduce schedule dependence upcoming message gradient exchange unlike purpose application reduce communication data parallel dnn training relatively fix traffic specific reduce algorithm communication advance training task MULTITREE construct schedule training prior knowledge leveraged simpler arbitration hardware thereby simplify logic improve efficiency MULTITREE algorithm aim coordinate global dynamism interconnection network maintain communication schedule thereby concurrent communication progress rate addition traffic communicate reduce gradient unnecessarily incurs bandwidth overhead massive packet flit optimize aspect revisit traditional technique redesign specifically reduce communication commonly packet switch mechanism gradient message message partition multiple packet packet consists flit flit highlight flit consumes bandwidth incurs extra rout arbitration extra delay consumption adapt message approach reduce overhead instead fix message chunk gradient message convert sub message sub message sub message  sub packet sub packet sub message sub packet behaves gradient message  sub message sub packet gradient message similarly sub packet partition flit unlike conventional packet switch message message message packet packet packet flit flit flit gradient flit gradient message sub message sub message sub message gradient sub packet sub packet sub packet flit flit sub sub packet sub packet sub packet sub packet sub packet sub packet flit sub flit flit flit flit flit gradient message message packet gradient message packet gradient VC packet info header info flit VC unused payload flit packet info route info dest src normal packet information flit packet info route info info  ID sub packet information flit flit format torus network flit flit packet information flit normal packet sub packet sub packet flit sub packet sub flit completion sub packet flit gradient message achieve perfect bandwidth efficiency improve performance efficiency gain benefit circuit switch without setup avoids critical packet physical link II packet flit normal packet flit code sub packet flit code sub flit format flit flit respectively VC indicates allocate virtual channel specifies packet flit II packet info encode differently normal packet reduce sub packet normal packet packet info simply route info dest src distribute rout algorithm reduce sub packet packet info route info info info ID message belongs MULTITREE communicates source rout hop output ejection  flit network interface information pre compute route info directly router specifically source router route interchange  rout computation stage destination identify message dependency schedule purpose MULTITREE reduce schedule communication node flit hop therefore increase possibility risk deadlock wormhole switch seamlessly traffic synchronization traffic virtual channel avoid starvation message methodology model configuration extend sim dnn inference simulator propagation training output stationary dataflow apply configure tpu accelerator processing PEs PE systolic array assume buffering sufficient memory bandwidth bandwidth memory maintain peak compute throughput accelerator aggregation reduce communication booksim interconnect model implement python interface sim booksim accelerator network interact network interface implement reduce schedule extra hardware overhead schedule counter lockstep counter counter entry node reduce scatter allgather entry node entry node node entry entry incurs KB overhead schedule compute initialization load network interface reuse iterative training epoch offload schedule communication hardware protocol software overhead configuration parameter configuration PE mac array dataflow output stationary precision accelerator PEs ghz network accelerator topology 2D torus mesh bigraph virtual router ghz vcs VC buffer depth flit data packet payload byte baseline link latency bandwidth GB software schedule reduce schedule mechanism apply baseline comparison configure buffer credit loop link target bandwidth payload training link bandwidth relax pressure reduce benefit MULTITREE approach demonstrate effectiveness generality MULTITREE topology 2D torus mesh nvidia DGX recent bigraph network node node node conduct scalability torus accelerator 2D torus mesh network google tpu network interface integrate chip assume network interface bandwidth network bandwidth attach router network switch network accelerator NIC connects leaf switch 2D torus dnn benchmark evaluation configuration parameter workload conduct synthetic reduce bandwidth network topology VI scalability evaluation VI reduce data chosen amount communication stress network simulation reasonable reduce bandwidth network topology reduce data KiB MiB scalability allreduce KiB node evaluate dnn model  VI alexnet  FasterRCNN googlenet NCF recommendation NCF resnet transformer mini batch node sample per accelerator evaluate training mini batch node fully utilize compute resource mini batch training model accuracy scope iteration non overlap propagation allreduce computation communication overlap layer wise reduce layer wise reduce layer queue reduce propagation communication overlap computation sgd propagate previous layer VI evaluation evaluate MULTITREE without  message enable respectively propose approach theart reduce algorithm reduce algorithm apply evaluate topology  binary  apply network topology 2D dimensional reduce dedicate 2D torus mesh network  halve rank mapping dedicate bigraph topology  reduce bandwidth applicability MULTITREE various network topology con torus network mesh network node network DGX physical network node ary node node bigraph network apply extend version algorithm described switch bigraph reduce data KiB MiB evaluate bandwidth calculate reduce data simulation MULTITREE  achieve bandwidth others regardless data data MULTITREE reduce data MULTITREE exploit network topology increase link utilization particularly  topology node poorly network severe contention 2D torus mesh MULTITREE  2D bandwidth optimal communicates data MULTITREE due allreduce phase dimension network interestingly 2D mesh network twofold perfect topology dimension mesh network latency slowest farthest node dimension 2D bandwidth suboptimal twice amount data bandwidth optimal algorithm MULTITREE bigraph MULTITREE  outperform data data achieve torus network mesh network network bigraph network reduce bandwidth topology various data torus mesh node DGX node node node bigraph  scalability KiB reduce data normalize node performance node almost performance topology MULTITREE derive MULTITREE node communicate node switch link traversal critical reduce latency offchip interconnection network contrast latency serialize slowest node leaf switch link traversal therefore MULTITREE data  data algorithm fully utilize bandwidth achieve performance  friendly network  achieve latency due suffers contention message network node data shift MULTITREE   bigraph network although  MULTITREE extra link traversal incur communication upper switch offset benefit performance data data  fully utilizes bandwidth  increase payload bandwidth another scalability weak scalability reduce KiB node communication normalize node network performance algorithm linearly node sustain linear factor  although fully utilize network link  2D 2D bandwidth sub optimal communicate nearly twice amount data  fully utilize network link achieves performance summary  achieves speedup 2D respectively scalability variation algorithm contention serialization latency dominant reduce dnn benchmark performance training breakdown torus network normalize non overlap training approach computation communication overlap approach alexnet non overlap training breakdown reduce speedup overlap training breakdown layer wise reduce training breakdown dnn training torus network propagation computation reduce communication breakdown primary reduce speedup secondary normalize non overlap training approach computation computation communication overlap communication breakdown normalize overlap training approach layer wise reduce dnns considerable amount allreduce communication cnns alexnet FasterRCNN googlenet resnet compute intensive compute transpose convolution input gradient propagate previous layer contrast NCF transformer embed attention layer computation requirement communication dominant summary communication baseline compute intensive cnns MULTITREE improves training performance 2D respectively communication intensive dnns MULTITREE improves training performance 2D respectively normalize reduce speedup average MULTITREE achieves speedup 2D respectively apply message reduce performance improve average speedup 2D respectively binary  algorithm 2D torus  topology oblivious algorithm logical node poorly onto physical network node multiple hop network contention furthermore contention link message due model worsen performance message apply algorithm bandwidth flit contribute nearly amount improvement reduce communication understand computation communication overlap reduce reduce communication overhead overlap training approach layer wise reduce training breakdown computation computation communication overlap communication depict MULTITREE achieves performance  performs computation dominant workload cnns alexnet FasterRCNN googlenet resnet computation largely overlap reduce communication mitigate communication bottleneck workload MULTITREE improves training performance 2D perform similarly MULTITREE portion  overlap due longer communication contrary communication dominant dnns NCF transformer computation overlap amount communication workload amount embed attention computation computation requirement communication bottleneck MULTITREE achieve speedup 2D respectively training performance recent dnn computation cycle non cnn layer meaning dnn model data communication dominant therefore MULTITREE promising faster distribute training vii DISCUSSIONS bandwidth versus latency ideal algorithm optimal bandwidth latency theoretically MULTITREE aim multiple ary height logk node butterfly exchange respectively reduce data butterfly achieve latency due however suffers contention data serialization latency important role  multi hop communication butterfly unfriendly topology worsen situation multi phase benefit algorithmic reduction offset communicate data bandwidth data serialization latency  contrast MULTITREE bandwidth optimal latency reduce communication hop switch network broader application although MULTITREE data parallelism hybrid parallel inference training reduce scatter naturally message improve bandwidth efficiency addition MULTITREE data parallel component hybrid approach parallelism strategy dnn workload MULTITREE node involve allreduce communication easily collective recent dnn workload DLRM MULTITREE implement software schedule synchronization offset benefit network heterogeneous link bandwidth topology graph model multigraph bandwidth wider link model multiple proportional link bandwidth MULTITREE applies properly MULTITREE purpose cluster network public network topology probed however achieve performance due interference training opportunity although theoretical logarithmic node algorithmic MULTITREE achieves limited network diameter network topology nonetheless MULTITREE demonstrates effectiveness algorithm architecture communication acceleration exploit network topology message reduce distribute reveals opportunity topology topology  training complex hybrid parallel addition reduce trading bandwidth latency attempt recent explore aspect future additional related collective acceleration dnn training recent research topology information structure improve reduce however linear program complexity network another implementation applies partition optimization algorithm leaf specific network topology backtracking operation exhaustive network therefore neither practical portable various network configuration recently propose blink generates multiple span increase link utilization however span DGX dedicate algorithm contrast MULTITREE generalize various topology generates blink dedicate DGX addition blink usage link MULTITREE finegrained schedule link communication earlier critical blink algorithm creates stem DGX approximate pack minimizes integer linear program ILP rate optimization reduce computation dependency MULTITREE inherently considers computation dependency construction multiple swan bidirectional link attach data distinct reduction broadcast phase link bandwidth utilized MULTITREE node internal leaf node utilize bidirectional link moreover MULTITREE network blink limited expensive ILP recently library probe physical network schedule hierarchical aggregation efficient gradient update address communication overhead dnn training apply network acceleration recently propose network architecture switch reduction accelerate reduce target memory multiprocessor arbitration technique ensure packet source destination addition functionality extend reserve packet ahead data packet arrival allows achieve buffer usage eliminates latency rout arbitration decision motivation propose pseudo circuit exploit communication temporal locality propose token technique improve rout establish bypass avoid rout switch arbitration logic IX CONCLUSIONS identify inefficiency widely reduce algorithm opportunity  propose MULTITREE reduce algorithm construct multiple topology link utilization consideration contention reduce schedule augment network interface coordinate communication enforce schedule lockstep estimation mechanism evaluation message achieve bandwidth improvement furthermore codesign topology achieves communication speedup training reduction  respectively