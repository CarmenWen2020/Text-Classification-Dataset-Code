memory significant limit performance workload application feature complex chain dependent indirect memory access cannot picked advanced microarchitectural prefetchers superscalar processor majority stall purpose architecture exploit fundamental memory parallelism microarchitectural technique automatically improve performance conventional processor remain elusive runahead execution tempt proposition hiding latency program execution however achieve  parallelism standard runahead execution skip ahead cache workload prefetches cache load dependent chain argue fundamental limitation runahead instead stall cache generate dependent chain load regain performance stall insight vector runahead technique prefetches entire load chain speculatively reorder scalar operation multiple loop iteration vector format independent load vectorization runahead instruction increase effective fetch decode bandwidth reduce resource requirement achieve memory parallelism faster rate across variety memory latency bound indirect workload vector runahead achieves performance speedup superscalar significantly improve stateof technique introduction workload poorly outof superscalar core database graph workload hpc code workload feature sparse indirect memory access characterize latency cache unpredictable stride prefetchers workload outof superscalar processor majority stall ample reorder buffer issue queue resource insufficient capture memory parallelism hide dram latency performance gap insurmountable elaborate accelerator significant improve performance workload domain specific program model specialized hardware dedicate task fetch data gap eliminate sophisticated programmer compiler  mechanism ideal pure microarchitecture technique achieve performance memory  workload programmer input binary compatible application stride technique  cache complex indirection inability cache identify complex load chain generate address limit exist technique array indirect pointer chase code achieve instruction visibility calculate address complex access workload conclude ideal technique within core instead within cache runahead execution promising technique date upon memory stall reorder buffer rob execution enters speculative runahead mode prefetch future memory access runahead mode address future memory access calculate memory access speculatively issue load return rob  processor resume normal mode execution memory access nearby cache runahead execution highly effective technique identify independent load future instruction processor stall latency load speculatively issue multiple independent memory access runahead execution significantly increase  parallelism mlp ultimately improve overall application performance runahead execution successfully prefetches independent load future instruction suffers fundamental limitation runahead unsuitable complex indirection consist chain dependent load combine traditional hardware stride prefetcher runahead enable prefetching indirection prefetch load chain dependent load runahead execution limited processor fetch decode rename width rate runahead execution generate mlp instruction independent load future instruction speculation depth runahead limited amount available resource issue queue slot physical register propose vector runahead novel runahead technique overcomes limitation innovation vector runahead alters runahead termination remain runahead mode load dependence chain issue oppose return normal mode load return memory enables vector runahead prefetch entire load chain UI OOVBM  PNQVUFS SDIJUFDUVSF acm annual international symposium computer architecture isca doi isca KŽK WZ KŽK WZ KŽK WZ KŽK WZ KŽK WZ KŽK WZ KŽK WZ KŽK WZ KŽK WZ KŽK WZ              cpi stack OoO core precise runahead execution pre vector runahead VR memory component broken attribute stride load indirect dependent chain load previous runahead cannot prefetch majority indirect memory access unlike vector runahead vector runahead vectorizes runahead instruction reinterpret scalar instruction vector operation generate cache offset subsequently dependent load instruction sequence hiding cache latency complex  vectorization virtually increase effective fetch decode bandwidth runahead mode resource issue queue slot vector operation memory operation vector runahead issue multiple vectorized instruction vector unroll pipelining speculate deeper increase effective runahead fetch decode bandwidth vector runahead vector load speculative prefetches issue parallel evaluate vector runahead detailed simulation variety graph database hpc workload report vector runahead improves performance baseline processor significant improvement precise runahead execution pre technique achieves speedup performance speedup memory parallelism prefetching load within dependent load sequence accurate timely manner vector runahead significantly impact complexity KB baseline II background motivation explain vector runahead detail context around limitation previous runahead technique potential exploit memory stall core OoO core frequently stall  memory access load  cache reorder buffer rob stall commit return meantime pipeline dispatch instruction rob completely stall memory access typically stall core cycle cpi stack benchmark OoO core experimental setup addition cycle spent perform useful component cpi stack cycle processor due issue queue IQ memory access memory access cycle load instruction stall processor stride load instruction stride indirect load instruction directly indirectly stride load instruction indirect load instruction indirect load instruction stall processor execution average HJ performance critical eliminate stall indirect memory access OoO core limitation runahead technique alleviate bottleneck memory access standard runahead execution checkpoint release architectural application  stall enters runahead mode processor speculatively generate memory access memory access return runahead interval terminates pipeline flush architectural restore entry runahead normal execution resume prefetches generate runahead mode future data processor cache reduce upcoming stall normal mode improve performance precise runahead execution pre ofthe runahead execution improves upon standard runahead mechanism pre leverage available issue queue physical register file resource speculatively execute instruction runahead mode thereby eliminate release flush processor exit runahead mode pre speculatively pre executes instruction generate memory access stall pre mechanism quickly recycle resource runahead mode pre performance benefit reduce overhead transition runahead mode normal mode enables ahead runahead interval dispatch load address generate instruction runahead mode dependent memory access dependent load thereby reduce amount backend resource runahead mode pre prefetches substantial memory access upcoming instruction pre reduces processor cycle stall memory access average although pre eliminates rob stall indirect memory access fails prefetch majority indirect memory access access beyond runahead interval stall core pro int hash code memory access array indirection intermediate address computation pointer access pre prefetch array contrast array cannot prefetched runahead mode likewise data cannot prefetched access serially pre runahead mode terminate prefetch array furthermore resource runahead mode limit speculation depth hardware stride prefetching prefetch array enable pre prefetch array runahead mode unfortunately data cannot prefetched runahead mode indexed depends pointer limitation apply vector runahead vectorizes memory access along memory dependence chain runahead mode multiple access parallel parallel access parallel data vector runahead versus pre without stride prefetching illustrative code load highlight trigger stall load highlight stall vector runahead prefetches multiple memory access parallel along memory dependence chain runahead mode  runahead mode consequently processor remains stall execution indirect memory access average HJ overall despite positive performance impact pre leaf potential improve performance inability effectively prefetch indirect memory access vector runahead execution dependent load analyze detail pre fails effectively prefetch indirect memory access illustrative code indirection index array hash index array dependent access data pre initiate upon stall speculatively executes future instruction runahead mode illustrate assume memory access cache pre issue prefetch access array instruction memory access cannot execute hence access array pointer cannot prefetched processor instruction runahead mode access array issue another prefetch unfortunately dependent access data cannot prefetched prefetch issue etc runahead mode load initiate runahead mode return memory access array prefetched array dependent data normal mode processor stall due cache array processor typically feature hardware stride prefetcher prefetch stride access array pre access array cache therefore pre issue prefetch request indirection array illustrate runahead mode pre access array label prefetch request dependent data cannot prefetched depends access access cache successfully prefetched stride prefetcher hence processor compute address access array issue prefetch request label dependent data cannot prefetched access cache processor prefetches access label etc summary stride prefetching enables runahead execution prefetch indirection cannot prefetch beyond pre prefetch future memory access stall initiate memory access improves mlp overall performance unfortunately illustrate pre limitation pre unable prefetch load along chain dependent load data access chain available hardware prefetcher alongside runahead solves rate pre issue speculative prefetches limited indeed runahead service independent load multiple load chain simultaneously independent load issue limit effective mlp achieve instruction per loop iteration complicate hash function significant processor resource runahead mode independent load limit pre speculation depth future vector runahead overcomes fundamental limitation vector runahead runahead mode termination instead return normal mode load return memory vector runahead runahead mode load along dependent load chain issue addition vector runahead vectorizes dynamic instruction runahead mode equivalent ahead faster rate illustrate vector runahead mode initiate multiple access array vectorized memory operation speculatively issue multiple induction variable offset parallel vectorize available vector width instruction array vectorized access array dependent data vectorizing dependent instruction runahead mode runahead mode dependent load issue enables speculatively prefetching entire chain dependent load vectorizing runahead instruction issue memory operation multiple iteration loop simultaneously issue parallel batch dependent memory access etc effective reorder memory access instruction enables vector runahead issue batch access batch access finally batch access dependent data independent memory access apart dynamic instruction vector runahead issue parallel reorder memory access enable vectorization feature benefit substantially increase effective fetch decode bandwidth runahead mode fetch decode multiple loop iteration hardware resource vector instruction vector runahead mode corresponds multiple scalar instruction multiple loop iteration code occupy issue queue slot illustrates vector runahead boost runahead performance issue multiple loop iteration parallel achieve performance generate mlp faster rate vector unroll pipelining vector unroll issue multiple vector runahead return normal mode vector pipelining reorder processor issue multiple vector instruction immediate succession vector runahead loop iteration per enables speculate across loop iteration parallel vector runahead code  verify vectorized instruction preserve behavior sequence vectorizes instruction runahead mode increase performance  bound code cache become eligible prefetching increase load coverage upon return normal execution vector runahead successfully prefetches indirect memory access reduce processor stall indirect memory access average describes mechanic vector runahead detail vector runahead implement vector runahead modification core microarchitecture microarchitecture overview baseline OoO pipeline modify newly hardware structure vector runahead stride detector regular access code induction variable speculative vectorized code vector runahead mode instruction dependent vectorized stride tracked taint vector vectorized address calculate arithmetic operation convert vector operation dependent load vector assume across vectorized mask handle improve memory parallelism vector load vector unroll pipelining technique issue future load simultaneously relationship scalar instruction vector instruction vector register allocation  introduce register allocation register deallocation queue rdq vector runahead mode terminate unrolled iteration iteration executes load dependent vectorized stride load detect stride load detect sequence code generate induction variable vectorize future memory access reference prediction update execution load instruction indexed load PC entry maintains access memory address stride load saturate counter confidence terminator PC dependent load instruction chain stride load vector runahead mode generate instruction generate future indirect memory access stride load computation intermediate memory access standard reference prediction vector runahead allows terminate useful vector runahead mode stride load rdq cache dispatch fetch stride detector issue execute commit register rename rat decode taint vector load structure modify structure exist structure normal mode runahead mode physical register file vectorizer  avx instruction rdq processor pipeline vector runahead execution vector runahead core enters runahead mode satisfied load instruction rob rob instruction issue queue capacity vector runahead checkpoint PC register allocation rat checkpoint per entry rat addition checkpoint recovery misprediction entry runahead mode processor restore checkpoint return normal mode runahead mode processor fetch decode execute future instruction access stride detector load instruction stride load stride load exists vector runahead performs similarly pre without fully associative stall slice vector runahead eliminates avoid harm workload without capture scalar dependency later vector runahead equivalently mode behaves traditional runahead execution active register reclamation efficient checkpointing entry vector runahead mode decode stride load confidence vectorize stride load sequence instruction terminates another dynamic instance stride load detect dependent chain dependent instruction dynamic stride load instance indirect chain taint vector operation transitively newly vectorized stride load instruction taint vector TV feature entry architectural integer register flag previous instruction register vectorized operation vectorize previous instruction register invalid invalid TV empty runahead whenever runahead terminates vectorize destination architectural register discover stride load invalid destination unsupported operation float operation input invalid TV entry propagate vector taint mechanism propagate vectorization instruction input register tag destination register becomes tag input register tag destination register flag unset instruction issue conventional scalar runahead operation treat loop invariant respect vectorized instruction sequence vector runahead mode iteration instruction invalid discard instruction vectorize vectorized vectorizing instruction vectorization perform via  routine generates vectorized version input scalar instruction stride load vectorizer generates vectorized version memory address access stride load stride input vectorizer generates vector load instruction injects vector instruction pipeline generate multiple vector instruction regardless input width scalar operand vector assume vector instruction vector register intel avx source destination reuse microarchitecture physical vector register  implement microarchitecture vector similarly vectorize arithmetic load instruction directly indirectly stride load generate correspond vector version vectorized instruction rename vector register allocation  rename instruction dispatch processor execute speculatively chain load instruction stride load multiple indirect dependent load pointer chase code load instruction dependency chain vectorized operation stride terminator update empty runahead PC dependent load therefore vector runahead generate memory parallelism multiple indirect memory access individual lane within vector generate invalid memory access individual lane marked invalid lane within subsequent vectorized instruction masked execution ignore instruction execute runahead mode useful generate memory access maintain rob therefore rob entry allocate runahead mode instead simpler register deallocation queue rdq handle register availability float instruction rarely calculate address ignore instruction instruction invalid along instruction already vectorized code vectorizing implicit assumption vector lane however execute vector runahead mode divergence lane instruction micro convert scalar predicate mask vector lane vector runahead code lane direction mask lane mask persists terminate iteration vector runahead contrast unrolled iteration within vector runahead interval independent vector unroll pipelining vector runahead suffers drawback illustrate assume processor vector width ahead execution limit timeliness runahead mode limit coverage issue simultaneous load saturate status register MSHRs limit mlp issue multiple vector runahead return normal mode vector unroll issue vector load identify stride sequence vector avx incrementing address stride load issue unroll vector runahead sequence issue iteration scalar loop vector runahead mode vector unroll limited mlp avx load limit parallel memory access stall dependent load vector becomes hence introduce vector pipelining software pipelining style optimization reorder load issue multiple independent memory access parallel instead previous vector runahead issue multiple vectorized instruction simultaneously stride load input lookahead distance mapping scalar vector instruction unroll pipeline depth vector width chosen illustrative purpose vector width simulated configuration vector runahead mlp limited vector instruction outstanding memory access prefetched future memory access memory parallel vector runahead limit performance gain future normal execution vector unroll vector runahead operation sequence maximum mlp significantly future memory access return normal execution improve latter performance gain vector pipelining overlap independent operation multiple unrolled iteration allows handle simultaneously execute parallel mlp vector runahead technique vector unroll vector pipelining improve performance increase runahead wider vector natively instruction architecture allows issue vector load parallel dependent issue parallel finally pipelining allows extract memory parallelism limit service simultaneous cache MSHRs unrolled vector distinct pipeline reuse physical register simultaneously contrast vector pipelining deliberately overlap physical register unrolled iteration increase  default assume unroll pipeline depth allows issue vector scalar load simultaneously code implementation tune dynamically performance accuracy register availability vector rat scalar architectural register input vectorize redirect vector instruction appropriate source vector physical register normally register allocation rat renames architectural scalar register physical scalar register however rename physical vector register instead addition vector pipelining  relationship architectural scalar register rename physical vector register pipeline depth rename architectural scalar register vector physical register data vector register allocation  entry per architectural integer register destination physical vector register assign pipelined instruction register  vectorized instruction entry input enables distinguish input output pipelined iteration within vector pipelining arrangement instruction fetch alias instruction entry integer register typically manage pipeline resource runahead sufficient unused issue queue physical scalar vector register file entry speculatively execute indirect chain indirect load vector runahead vector instruction occupies  entry dispatch execution upon execution issue queue entry freed allocate instruction standard OoO core unlike issue queue physical register file entry cannot release OoO core physical register freed instruction architectural register physical register mapped commit commit instruction runahead mode core stall due unavailability physical register vector runahead physical register register longer address generation via register deallocation queue rdq pre instruction  vector pipelining physical vector register destination instruction architectural register become instruction pipeline vector pipelined instruction generate scalar instruction issue independent vector operation rdq entry  physical vector register rdq operating loop  benchmark assume instruction pipelined stride load load address stride detector vector runahead mode assume scalar physical register currently rdx rdi rbp respectively physical scalar register physical vector register instruction invalid indirect chain indirect load          ŶƐƚ        ŶƐƚ      ŶƐƚ     ŶƐƚ      ŶƐƚ        ŶƐƚ               ŶƐƚ           ŶƐƚ             rename avx register vector runahead register deallocation queue instruction sequence vector runahead mode duplication vectorized instruction via vector pipelining associate rename rdq lookup  reuse destination register longer apart instruction vectorized instruction source destination physical register IDs content rdq consists physical register ID instruction execute rdq entry allocate vectorized instruction rdq entry allocate invalid instruction marked rdq maintains pointer  instruction pointer instruction instruction execute marked instruction rdq execute rdq physical register pointer vector register release reuse instruction executes register release instruction execute pointer instruction terminate runahead vector runahead mode terminates satisfied encounter dynamic instance stride load encounter issue terminator PC identify stride detector dependent load sequence vector lane marked invalid scalar equivalent instruction execute vector runahead mode unexpected code unroll pipeline depth vector runahead mode immediately stride load num  hash hash code          ĞĚǆ  ĞĐǆ   ĞĐǆ ĞĚǆ  ĞĚǆ ĞĐǆ   ĞĚǆ  ĞĐǆ   ĞĐǆ ĞĚǆ  ĞĚǆ ĞĐǆ   ĞĚǆ  ĞĐǆ   ĞĐǆ ĞĚǆ          ĞĚǆ  ĞĐǆ   ĞĐǆ ĞĚǆ  ĞĚǆ ĞĐǆ    ĞĚǆ  ĞĐǆ    ĞĐǆ ĞĚǆ  ĞĚǆ ĞĐǆ   ĞĚǆ  ĞĐǆ   ĞĐǆ ĞĚǆ       assembly loop kangaroo stride load dependent indirect load issue vector issue normal execution resume VI benefit vectorizing entire indirect chain exceeds additional duration core runahead mode vector runahead yield memory parallelism typical execution upon termination restore rat entry runahead mode TV  rdq redirect fetch instruction dispatch instruction rob hardware overhead vector runahead incurs structure vector runahead structure repurpose exist component stride detector address stride distance counter terminator entry byte storage taint vector byte storage register  incurs byte encode mapping pipelined physical vector register scalar architectural register rdq entry pre byte SRAM overhead vector runahead KB pre KB IV representative code contrast vector runahead ofthe precise runahead execution pre representative code kangaroo benchmark loop consists stride load fetch hash generate indirect address indirect load hash generate address indirect load instruction generate indirect address similarly instruction generate address finally loop iteration behavior pre described baseline  stride prefetcher load typically cache enable issue load indirect dram access delay execution load execute furthermore mlp exploit limited load issue queue instruction occupy issue queue slot exploitable mlp limited instruction issue queue runahead mode mlp saturates around pre later quantify evaluation contrast vector runahead generates substantially mlp runahead mode vector runahead initiate vectorization dynamic instance stride load assume vector runahead generates vector instruction vectorization corresponds iteration loop contrast pre vector runahead ability issue independent scalar equivalent load immediate succession allows generate substantially mlp processor remains vector runahead mode vectorized issue maximize coverage load processor resume normal mode addition mlp vector runahead generates mlp faster pace pre whereas pre fundamentally limited processor fetch decode bandwidth operation within issue queue vector runahead generates independent memory access immediate succession vector runahead dramatically increase effective fetch decode bandwidth runahead mode vector runahead generates mlp faster rate experimental setup simulation setup accurate superscalar core model sniper cycle hardware validate simulator adjust faithfully model vector runahead configuration baseline processor intel skylake aggressive  stride prefetcher MSHRs outstanding cache predictor KB tage SC prediction championship skip initialization instruction within workload representative workload variety benchmark feature complex memory compute dependency execution benchmark memory latency bound performance compute hpc core ghz rob queue issue load processor width fetch dispatch rename commit pipeline depth stage predictor KB tage SC functional int cycle int mult cycle int div cycle cycle mult cycle div cycle register file int vector cache KB assoc cycle access cache KB assoc cycle access stride prefetcher private cache KB assoc cycle access cache MB assoc cycle access memory min latency GB bandwidth request contention model baseline configuration core graph database workload evaluate previous programmer compiler manage prefetching mechanism benchmark parameter II benchmark camel hash input workload significant computation indirect memory access graph input graph breadth hash per bucket per bucket database kernel hash calculation perform generate chain address kangaroo array hash workload indirect chain complex address computation conjugate gradient CG integer sort NAS parallel benchmark suite input supercomputing kernel indirect memory access CG indirect memory access likely cache llc  hpc kernel complex indirect address computation benchmark variety complex memory access indirect chain compute requirement compiler flag  vectorize via comparison  alter performance code  despite amenable vector runahead VI evaluation microarchitectural mechanism OoO baseline core hardware stride prefetcher precise runahead execution pre ofthe runahead execution technique propose assume ideal stall slice therefore indirect memory prefetcher imp indirect memory prefetcher propose imp workload ind chain arithmetic insts per load camel graph graph HJ HJ kangaroo NAS CG NAS  II summary workload indirect chain stride load within inner loop complex arithmetic beyond array index calculate address prefetching runahead mode instruction average per load        KŽK DW WZ performance vector runahead execution baseline core vector runahead yield harmonic speedup baseline OoO core pre respectively attach cache detects indirect access stride memory access vector runahead VR mechanism propose representative vector runahead technique unroll pipeline depth unless mention otherwise overall performance comparison report speedup evaluate technique vector runahead achieves harmonic speedup across benchmark baseline OoO architecture achieve speedup camel HJ HJ kangaroo pre achieves harmonic speedup baseline vector runahead achieves speedup relative pre imp cannot detect complex address computation improves speedup relative baseline significant improvement performance achieve vector runahead memory parallelism fetch load within dependent sequence without fetch irrelevant data elaborate subsection discus pre pre improves performance processing future instruction beyond rob stall   improvement pre depends indirect access runahead interval baseline stride prefetcher attach stride stride indirect indirect chain cache runahead execution prefetch indirect memory access somewhat successfully camel CG ineffective workload indirect chain lack access intermediate load address calculation hinders coverage limit amount mlp expose pre standard execution modify pre runahead mode vector runahead prefetch indirect chain extra performance improvement limited average imp improve performance application primarily indirect camel CG however complex chain instruction stride indirect load HJ kangaroo  imp cannot perform computation vector runahead achieves performance mechanism important  reorder load instruction service simultaneously reorder implement scalar micro ops instead vector micro ops sufficient gain average speedup optimization pack vector operation due simd layout increase performance virtue improve compute throughput issue queue slot load issue earlier finally alter termination vector runahead completes entire chain memory access exit allows longer chain multiple memory access achieve rob return increase performance graph performance sensitivity analysis memory parallelism vector runahead achieve performance pipelined vector issue memory hiding serialization dependent load core pre workload others although baseline OoO core feature relatively rob enables achieve mlp simplest workload vector runahead extract significantly mlp unsurprisingly vector runahead achieves speedup core comparatively weak camel HJ HJ kangaroo instruction address compute otherwise execute along load II starve core reorder buffer issue queue resource limit  ability contrast vector runahead rely reorder buffer memory parallelism achieve vector        KŽK WZ memory parallelism MSHR entry utilized per cycle allocate precise runahead improves mlp vectorizing indirect chain generates mlp OoO core workload baseline relatively vector runahead complex limit ability vector runahead application memory access throttle vector issue particularly input frequently  data dependent inner outer loop others CG datasets llc meaning cache service quickly without vector runahead finally workload MSHR constrain within vector runahead mode achieve mlp cpu feasibly achieve average mlp maximum achievable MSHRs vector runahead cannot continuously kick resource unroll pipelining analysis default unroll pipeline depth allows issue scalar equivalent dependent load chain unroll pipelining issue scalar load runahead almost performance improvement register limit mlp load struggle OoO core performance compute away execute runahead barely worth effort issue unroll without increase pipeline depth improves performance slightly increase coverage vector runahead lack memory parallelism load sufficient saturate MSHRs gain amount performance overall llc cache substitute vector runahead workload target feature datasets typical data workload reasonably cache meaning neither baseline performance vector runahead affected significantly cache speedup obtain vector runahead largely invariant MSHRs ipc relative                      performance impact unroll pipelining unroll vectorized scalar instruction pipeline depth vector instruction launch simultaneously           speedup vector runahead function llc performance normalize baseline OoO core respective llc increase llc minor impact performance due indirect memory access workload hence vector runahead speedup largely invariant baseline MSHR setup MSHRs cache previously demonstrate MSHRs insufficient achieve benefit vector runahead contention concurrent MSHRs limit achievable mlp average utilization MSHRs gain improvement workload  mode vector pipelining theoretically issue load simultaneously workload issue parallel vector runahead mode quickly MSHRs vector runahead effectiveness coverage chip memory access issue core vector runahead versus pre relative baseline issue normal mode adequately prefetched runahead mode observation neither technique significantly  generate unnecessary memory access pursuit mlp vector runahead coverage proportion memory access normal mode vector runahead average              impact MSHRs performance vector runahead sufficient MSHRs fully benefit vector runahead WZ WZ WZ WZ WZ WZ WZ WZ WZ WZ          coverage chip memory access pre vector runahead normalize OoO graph memory access normal versus runahead mode vector runahead successfully prefetches dram access convert chip cache program actually access normal mode pre due comprehensive handle indirect chain accuracy excess dram echoed vector runahead rarely brings data runahead mode later normal mode leaf cache hierarchy exception dataset complex stride detector generate vector address later access graph accuracy net performance improvement timeliness despite mlp exploit vector runahead prefetches timely cache evict cache preferable latency chip access vector runahead terminate issue load chain prefetches incomplete normal mode access portion chip memory latency MSHR normal mode overall normal execution vii related vector runahead prefetching summary perform                accuracy                timeliness accuracy prefetched cachelines runahead mode later access normal mode timeliness prefetched cachelines runahead mode data cache normal mode chip cachelines data transfer memory vector runahead rarely brings data runahead mode later normal mode cachelines prefetched runahead mode latency cache normal mode  relevant categorize auto vectorization vector runahead auto vectorization software pipelining improve load service simultaneously limited resource contrast static software technique vector runahead dynamic hardware technique adequately prefetch cache runahead execution runahead execution microarchitecture technique skip ahead latency load runahead execution  processor  load prefetch independent memory access throttle redesign increase efficiency filter runahead backwards dataflow directly target instruction sequence precise runahead execution improves performance efficient checkpointing technique register reclamation filter unnecessary instruction continuous runahead offloads execution accelerator avoid resource core inorder core directly achieve memory parallelism indirect workload vector runahead hypothetically offload processor load slice core freeway slice core slice program offload memory access queue vector runahead increase exploitable mlp slice core improves runahead efficiency runahead execution inability target dependent memory access literature address delta prediction predictor predict  load instead directly perform load vector style vector runahead sequential without memory parallelism calculation memory controller fundamentally vector runahead achieve memory parallelism dependent chain mechanism ability complex memory access implementation considers runahead within thread within core runahead technique equally apply mechanism offload runahead pre execution helper thread generally technique exist literature precompute memory access dependence graph computation execution precompute instruction sequence memory access slice processor similarly extract instruction sequence generate load  load thread kim  compiler generate helper thread data computation thread speculative precomputation allows speculative helper thread spawn speculative helper thread target complex chain dependency vector runahead handle via vectorization offload partner core ultimately helper thread runahead execution decouple access execute load compute slice execute independently desc split entire processor access execute helper thread   emulate microarchitectural prefetchers via helper thread contrast technique vector runahead thread execution neither programmer compiler moreover vector runahead dependent chain unlike pre execution helper thread achieve substantially memory parallelism architecturally visible prefetching workload vector runahead target mostly target prefetchers software  instruction prefetching introduce hardware prefetcher programmer hint specialized configurable prefetchers developed graph workload link structure  programmer hint assist temporal prefetchers array prefetcher generally programmability within memory hierarchy trigger programmable prefetcher extract memory parallelism via extreme thread parallelism contrast data parallelism within core vector runahead core accelerator fetchers memory access directly program semantics instead prefetch hint  walker     software prefetching widely deployed prefetching technique software non load insert program  develops algorithm insert software prefetches compiler  jones develop compiler technique indirect memory access software prefetching building enable fetcher behavior  software prefetching emulate walker generally instruction software pipelined compiler extract memory parallelism  comparison vector runahead microarchitecture binary source code speculate within runahead mode vector runahead freely vectorize sequence instruction software prefetchers fault microarchitectural prefetchers prefetchers deployed date software microarchitecture stride prefetchers sequence address commercial exists improve coverage performance selectivity temporal prefetchers replay cache cannot volume data lack repetition complex data workload content prefetching mechanism fetch data cache pointer regardless program compiler input typically throttle fetch bouquet prefetchers multiple style prefetcher PC predictor imp target stride indirect memory access cache via contrast vector runahead operates within core arbitrary indirection depth complex address calculation workload conclusion vector runahead microarchitectural generate memory parallelism highly complex workload feature chain dependent memory access complex address computation dynamically speculatively vectorizing runahead sequence generate runahead execution brings future dependent memory access hiding memory overlap vector access parallel report average performance speedup baseline architecture minimal amount processor