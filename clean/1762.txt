unlimited vector extension UVE novel instruction architecture extension simd processing compute scenario aim overcome shortcoming scalable vector extension data simultaneously reduce overhead associate loop memory access index memory access latency achieve instruction pre configure loop memory access attain accurate timely data prefetching predictable access multidimensional array indirect memory access configure data associate generalpurpose vector register interface iterate simply achieve reading correspond input output data instantly consume evaluate propose UVE proof concept gem implementation integrate processor model cortex consideration typical speculative execution paradigm highperformance compute processor evaluation representative kernel assess execute instruction impact memory bus overall performance upcoming scalable vector extension sve obtain propose extension attains average performance speedup processor configuration vector index scalable simd vector processing data instruction extension processor microarchitecture highperformance compute introduction instruction multiple data simd instruction extension  exploitation data parallelism DLP significant performance speedup however conventional simd extension focus operating fix register intel  SSE avx etc neon simplifies implementation limit hardware HW requirement non trivial regard vector optimal depends target workload moreover modification register usually adoption newer instruction extension inevitably previously compile code obsolete overcome issue recently emerge sve RISC vector extension  agnostic physical vector register software SW developer compiler runtime hence processor implementation adopt distinct vector code modification highperformance compute hpc processor vector attain throughput processor adopt vector fulfill resource constraint however usually presence predicate vector instruction disable vector outside loop bound eventually increase loop instruction illustrates saxpy kernel implementation sve  sve  instruction overhead shade instruction memory index loop memory access neither directly contribute maximize data processing throughput overhead instruction majority loop code waste processor resource significantly impact performance performance data parallel application constrain memory hierarchy hence performance processor rely SW HW prefetchers improve performance however SW prefetching typically impose additional instruction loop code increase overhead HW prefetchers improve accuracy coverage generation however timeliness dependent mechanism identify memory access predict future access prediction inaccuracy increase consumption pollute cache accordance unlimited vector extension UVE propose distinguishes simd extension feature decouple memory access rely  paradigm input data directly register file effectively decouple memory access computation data load parallel data manipulation implicit prefetching facilitates UI OOVBM  PNQVUFS SDIJUFDUVSF acm annual international symposium computer architecture isca doi isca VL int   sub      saxpy saxpy inst loop preamble VL  vector memory index loop computation mov   lds lds fmla sts   irst lsl lsl lsl loop saxpy loop conig input conig input conig output broadcast const vector elem broadcast compute  loop loop saxpy loop code loop VL RISC  propose UVE input input output inst inst  legend memory VL int   sub      saxpy saxpy inst loop preamble VL  vector memory index loop computation mov   lds lds fmla sts   irst lsl lsl lsl loop saxpy loop conig input conig input conig output broadcast const vector elem broadcast compute  loop loop saxpy loop code loop VL RISC sve propose UVE input input output inst inst  legend memory saxpy kernel implementation sve RISC propose UVE RISC sve code official documentation instruction highlight grey loop overhead acquisition data significantly reduce memory access latency decrease memory dependent instruction remain pipeline reduces potential hazard rename commit stage processor improve performance index loop memory access load loop preamble minimizes instruction dedicate memory index significantly reduce instruction pressure processor pipeline UVE saxpy implementation furthermore rely representation memory access descriptor avoids erroneous transfer data due predict prefetching simplify vectorization loop access exactly described descriptor representation complex multi dimensional stride indirect transparently perform  operation transform non coalesce access linear hence execution pipeline viewpoint data sequentially align memory simplify vectorization illustrates feature computation maximum function across input matrix triangular matrix indirect matrix access operation difference concern UVE code exactly compute maximum regardless implicit load described loop preamble remove index instruction explicit load simply associate active vector register hence reading register associate input output load implicitly trigger iteration additional instruction naturally reading multiple copying register insert specific instruction code temporarily suspend iteration register agnostic sve  UVE code agnostic register however prevent operating bound multiple vector sve  explicitly instruction define vector active contrast UVE generally instruction instruction specification achieve automatically disables vector bound equivalent automate pad multiple vector consequence loop become simpler minimal instruction corresponds instruction understand impact propose UVE extension purpose processor proof  implementation processor architecture simd extension minor modification rename commit stage besides introduce manage operation notwithstanding minor modification exploitation architectural opportunity reduce load latency architectural propose mitigation load conig input conig output load irst data  hmax dim sweep  dim compute max across vector horizontal max loop matrix hmax loop kernel loop hmax UVE implementation input matrix output vector max max max max max max max matrix input matrix output vector max max max max max max max triangular input matrix output vector max indirect max input vector input output UVE pseudo code implementation computation maximum across matrix matrix triangular matrix matrix pointer array latency directly data vector register contrast conventional prefetching structure input data load cache cache buffer data across virtual boundary propose relies accurate representation access safely boundary access multiple data across prediction benefit performance propose implementation speculative execution access exactly described processor speculatively consume data load buffer consume instruction commits hence speculation data immediately reuse correction execution without duplicate memory load finally sve processor configuration vector cumulative contribution average performance representative benchmark II data  define   sequence data transfer memory processor hence adopt model representation fulfill generality loop affine indirect memory address sequence complex multi dimensional multi construct HW feasibility representation adopt encode scheme easily efficiently interpret dedicate hardware parallel data manipulation decouple memory index memory index perform outside computation pipeline without explicit instruction computational loop autonomy infrastructure autonomously data processor memory without processor intervention memory access model propose model aim sequence address comprise access array variable compiler friendly typical structure nest loop convert nest loop index loop data dependent indirect index dynamic dimensional affine function    hence access described sum address dimensional variable   index variable stride multiplication factor corresponds dimension usually bound loop code index variable integer data dimension index offset index variable correspond dimension variable offset associate variable address  model complexity achieve combine multiple function achieve assign address offset function another affine function additionally data obtain sequence address generate affine function perform association description indirect memory access descriptor representation model define hierarchical descriptor representation encodes variable dimension affine function  scheme mechanism combine multiple function representation complex indirect access descriptor uni dimensional access parameter tuple correspond offset stride variable dimension depicts simplest description linear memory access memory offset memory stride encode extent dimensional access multiple descriptor hierarchically combine linearly cascade scheme nest loop descriptor correspond dimension calculate displacement offset descriptor correspond dimension description illustrate matrix access dimension encodes horizontal scan dimension iterates vertically dimension generates linear stride dimension iterates consecutive stride conversely stride parameter dimension skip memory generate scatter static descriptor modifier access loop inner loop generate iteration outer loop outer loop iteration triangular depict parameter inner loop incremented parameter descriptor correspond inner loop update iteration descriptor correspond outer loop behaviour optional static descriptor modifier introduce tuple parameter encodes information target tuple parameter modify behavior modification operator addition sub subtraction displacement constant apply target descriptor parameter accord iteration modification apply hence modifier implicitly subtracts displacement target parameter correspond descriptor iterate association static modifier descriptor correspond dimension realize descriptor corresponds dimension depict descriptor dimensional modifier affect dimension bound dimension indirect descriptor modifier propose descriptor specification extend content modify another indirect indexed  interaction realize optional indirect descriptor modifier tuple descriptor displacement parameter replace pointer origin data iteration dynamic displacement load origin modify target parameter indirection relation origin code descriptor dimension dimension NC dimension  inc linear rectangular rectangular scatter triangular indirection indirect offset descriptor offset stride static modifier target behavior displacement legend indirect modifier target behavior pointer dim descriptor static modifier indirect modifier modeling memory access    memory access  memory access representation hierarchical descriptor dependent hence indirection modifier parameter behavior parameter operator dynamic displacement target sub subtracts displacement target origin target contrast static modifier behaviour operator II indirect modifier perform implicit addition subtraction displacement target parameter instead iteration target parameter addition subtraction displacement parameter solely displacement finally syntax indirect descriptor modifier equivalent static modifier depict PROPOSED UVE extension fully exploit data specification II program interface instruction architecture ISA introduces unlimited vector extension UVE principle RISC style comprehend hardware friendly instruction efficient implementation avoid complex decode instruction correspond processor μop scalable program model fully scalable non constrain vector coherent inherit structural principle ISA RISC source due extensible instruction extension definition propose UVE comprises processor architecture infrastructure architectural besides adoption vector register file propose UVE interface vector instruction predicate register facilitate execution lane furthermore instruction scalar register load mostly inherit RISC scalar register vector register propose UVE vector register satisfy compromise operand encode register file pressure vector limited maximum minimum define corresponds width elementary data byte naturally maximum vector multiple minimum upon acceptance specification UVE available author github along evaluation source code width independently configure vector register interface data implicitly associate specific vector register hence reading register instruction transparently consume correspond predicate register predicate register file compose predicate register however regular memory arithmetic instruction predicate register hardwired valid lane execute eliminate pre configure register non conditional loop remain predicate configuration context balance related analysis compile optimize code benefit mitigate predicate register pressure model implicit propose UVE devise upon principle scalability principle adopt UVE focus runtime variable vector consumption production automatically enforces iteration ensure automatic progression loop iteration moreover reading outside automatically disabled infrastructure false predicate loop perform conditional dimension destructive behavior consumption data mapped register automatically iterates hence data program register memory already data  option eliminates additional instruction loop promote code reduction compiler optimization despite straightforward benchmark kernel adopt experimental evaluation propose extension compiler optimization technique loop unroll software  future implementation compiler toolchain exploit convenient compiler technique identify linear combination loop induction variable calculate address sequence  memory access eliminate memory index loop apply vectorization technique complexity limitation although UVE complex memory access reasonable limit define constrain hardware resource analysis dimensionality moreover dimensional 5D outer loop reconfigure access iteration significant performance difference nonetheless implementation dimension modifier memory model  performance propose extension runahead execution input automatic pre load input data similarly otherwise achieve loop unroll software register prefetching naturally imposes source memory location input cannot modify runtime conventional memory output runtime aggressiveness prefetching generate  hazard however constraint practical situation loop explicit conventional load code hazard generally  amenable scalable vector extension infinite impulse response filter dependency normally dealt handle propose model loop code encodes dependency memory operation ensure computation adopt model assumes processor responsible synchronization input pending instruction precede configuration input delayed due execution latency similarly exit loop processor responsible ensure synchronization output load instruction finally premise RISC specification weak memory model herein adopt synchronization hardware thread achieve explicit fence instruction instruction UVE currently feature integer float memory instruction instruction variation configuration descriptor modifier configuration parameter described II width behaviour etc encode configuration instruction identify prefix configure instruction complex data multiple instruction per dimension modifier specific configuration instruction namely sta append app mod ind mod static modifier ind indirect modifier mod ind additionally configuration specify data byte subset instruction responsible suspend suspend resume resume purpose momentary restore vector register enable context switch concurrent execution multiple without interfere configuration manually load suspend precise predication individual lane simd execution subset predication mask instruction execution dup mul  loop saxpy loop assembly code operation execution dup mul  loop saxpy loop assembly code operation UVE saxpy code emulation fuse operation cannot accepts operand cannot simultaneously mode disable lane purpose UVE instruction configure predicate vector register comparison valid vector register predicate manipulation instruction available offering predicate predicate width configuration wise negation loop assure distinct conditional format predicate specify predicate register dimension dimension dimension format allows iteration access variable bound loop scalar code vector manipulation UVE instruction vector manipulation processing vertical horizontal arithmetic logic shift operation instruction optionally predicate predicate register illustrates vector manipulation data processing instruction duplication dup scalar replicate vector multiplication mul addition processing register vector register instruction automatic load explicit register distinction finally worth UVE completely dependent transfer data memory conventional non vector load instruction ISA load legacy instruction fix address increment explicit memory index linear memory access scalar processing although propose extension target vector processing infrastructure exploit scalar processing specific vector scalar scalar vector instruction impose wise shift vector consumption production advanced UVE specialized instruction explicitly  configure vector  narrower vector emulation vector align processing UVE dimension description configure vector align automatic pad vector register dimension multiple vector cope memory access footprint temporal spatial locality profile data UVE cache access selection configuration cfg  instruction directs correspond cache concurrent UVE program model assumes concurrent independently directly encode dependency across however compromise purpose performance compute code particularly computation dependency described loop code saxpy kernel illustrate depends impose avoid potential hazard IV microarchitecture operation UVE perform within embed architecture pipeline IV besides cpu processing pipeline extend minor structure highlight modification apply traditional processing pipeline pipeline experimental evaluation VI modification summarize decode register file execution convenient decode propose UVE  extension vector register correspond logic integer load lsu alu  fmul   alu  fmul   predictor BPU inst inst inst inst return stack inst inst inst inst instruction cache rename allocate commit reorder buffer dispatch decode queue instruction fetch decoder decoder decoder decoder purpose register file register simd FP register file issue speculation load configure writeback execution memory access data cache legend generic simd modification slightly modify module FIFOs cpu core microarchitecture diagram highlight introduce modification alleviate representation complexity load coherency mechanism arithmetic functional requirement  sve rename besides vector register rename vector extension rename allows speculative configuration others logical execute commit commit squash signal speculation commit related processing namely configuration iteration suspension resume termination operation understand architectural propose mechanism operation architecture described IV configuration complex data multiple instruction resolution data dependency execution configuration instruction although commit buffer reorder prevent speculative configuration impact performance hence alternative herein adopt whenever configuration instruction rename register configuration reorder structure configuration reorder buffer  buffer configuration instruction correspond operand available upon completion configuration immediately processing pre load data input calculate address output commit data speculative execution configuration generates speculative commit dynamically iterate instruction manipulates rename commit stage respectively rename rename mechanism essential speculative configuration configure correspond identification register associate another due speculation pipeline latency avoid pipeline alias identical register alias rat introduce performs mapping logical register physical identifier register currently associate active suspend verify instruction operand corresponds normal register operation iteration iteration logically perform reading input output architecturally achieve rename signal iterate speculative output implies reserve fifo buffer IV data commit signal operation input devise attempt minimize  latency allocate input physical register consequence consume instruction rename operand immediately data pre load physical register achieve perform register rename request load correspond data target physical register termination termination achieve commit explicit termination instruction commit instruction signal completion occurs structure release resource allocate configuration speculation speculation erroneous configuration iteration configuration associate structure release accept configuration contrast speculate iteration correspond instruction squash action perform pipeline reverts physical register previously commit signal revert speculate pointer load circular buffer commit hence impact buffer data load address data access deterministic simply consume accordingly speculatively input remain valid immediately without load cache hierarchy naturally commit writes signal advance correspond commit iterators release load buffer finally termination release structure perform commit hence affected speculation cache access minimize impact cache avoid inclusion additional access input output request merge conventional memory load access although impose additional pressure channel eventual delay concurrent writes scalar pipeline impact option usually loop usually scalar memory operation consequence conventional load memory operation typically perform mutually exclusive fashion occurs benchmark mitigate impact merge previously refer illustrate UVE data load cache memory particularly important cache data temporal locality waste cache capacity imposes consumption hurt performance hardware avoid significant memory hierarchy infrastructure simply achieve issue correspond request non cacheable load cache default non cacheable request cpu likely cache treat normal cacheable load configure memory access issue correspond input request non cacheable avoid cache pollution likely memory load request probably cache concern output implementation issue cache however adopt load simply  policy cache attain cache memory memory coherence coherence ensure distinct mechanism core non operation coherent load core load queue load dependency typical request delay replay squash mechanism hence data conventional pipeline immediately newly configure input data output load conventional load instruction ensures reliable transition sequential code loop cache coherence ensure conventional MOESI cache snoop protocol naturally directory approach equally cpu core memory execution pipeline cache cache load FIFOs configurable request cache bypassing conventional request merge cache access model transparently  core pipeline overview depict embed core connection memory hierarchy finally propose mechanism coherence mechanism FIFOs assume preloaded data already consume core loop unroll register pre fetch detailed IV load FIFOs naturally adopt extend coherence protocol internal buffer however alternative future  kernel amenable scalable vector extension usually impose dependency memory location exception iterative algorithm however reconfiguration algorithm iteration impose synchronization exception handle source UVE specific exception fault handle commit stage active fault access attempt sve nevertheless fault consequence execution speculation terminates earlier fault otherwise inactive predicate false context switch context thread suspend active commit iteration iteration describes scalar access resume execution due context switch upon recover exception simply perform load iteration resume commit naturally pre fetch data internal buffer lose load depends data varies byte byte maximum modifier responsible manage input output depict architecture consists configuration module responsible configuration request information regard configuration multiple dimension information regard speculative commit iteration scheduler responsible selection load descriptor iterate load processing module queue memory request load FIFOs buffering purpose module configuration whenever configuration rename stage processor pipeline register   data dependency satisfied instruction retrieve per cycle validate data configuration processing processing manage scheduler cycle selects descriptor iterator address generator sort queue validation configuration memory request queue arbiter fifo load fifo occupancy scheduler fifo register file writeback load fifo data memory data memory memory access request address generator scheduler descriptor processing module processing module configuration descriptor iter flag load processing module address offset stride descriptor dim accumulation     offset iteration dim memory address configure fifo occupancy filter active sort occupancy descriptor selection descriptor iterator dim dim mod dim mod descriptor descriptor iter iter iter iter iteration iteration processing module logical diagram load iterate address generator processing module depict upon iteration register unless processing multiplex iteration prioritization achieve selection fifo occupancy precedence others ensure consume fifo priority hardware complexity processing module dimension plus respective modifier detailed II domain dimension modifier restart iteration descriptor update accumulate offset descriptor address generator architecture  adder multiplexing logic descriptor iteration modifier per cycle allows generate cache request per cycle additional cycle switch descriptor dimension succeed descriptor iteration cache memory request issue cache data request reuse iterate load generates load request register load fifo memory request queue arbiter request performs virtual  translation tlb access issue memory subsequent response correspond load fifo queue slot fault correspond vector immediately flag exception subsequently handle commit stage avoids request invalid memory speculatively allows prefetching data across boundary iterate simply generates address directly fifo data processor subsequently commit load FIFOs described IV memory coherence integrate internal structure core load FIFOs additional dimension vector register consequently data buffer structure regard memory hierarchy already consume data core data coherence memory hierarchy longer enforce accordingly associate independent fix fifo queue depth constraint hardware resource naturally management attain queue across however impose additional hardware complexity therefore future scheduler policy iterate scheduler relies policy prioritizes fifo queue occupy architectural impact overview besides addition propose UVE imposes minimal impact core microarchitecture modification achieve establish connection vector register file execution rename commit stage register minimize impact operating frequency decode stage extend cpu model configuration PARAMETERS BASED public information cortex PARAMETERS   specific UVE  UVE baseline cpu instruction fetch μop commit 5GHz μop issue dispatch writeback IQ LQ SQ rob entry int RF FP RF vector RF functional int ALUs entry scheduler int vector FP fus entry scheduler load entry scheduler load processing module entry load FIFOs per default load entry scheduler KB lru stride prefetcher depth KB lru  prefetcher  snoop cache coherence protocol dram dual channel ddr UVE instruction simd extension architectural impact core simd extension conversely aside processing logic described IV mostly compose storage buffering structure particularly dimension minimize impact attain significant performance gain detailed VI refer conventional load typically mutually exclusive fashion merge verify benchmark experimental methodology simulation environment propose microarchitecture simulated modify version gem parameter described although feature riscv instruction adopt configuration public information regard cortex furthermore goal evaluate per core performance feature cache memory typically core data multi core processor additionally propose extension feature load processing module entry fifo queue coherence cache ensure snoop MOESI protocol baseline architecture performance evaluation propose core sve feature vector propose apart nonexistent core processor parameter configuration validate propose baseline architecture feature prefetchers stride prefetcher associate  prefetcher associate unified evaluation benchmark representative benchmark suite workload memory access adopt evaluation propose UVE instruction correspond implementation representative benchmark relevant application domain memory linear algebra blas stencil data mining dynamic program physic compilation toolchain refer development compilation toolchain UVE progress future consequence benchmark computational loop cod although adopt straightforward implementation avoid loop unroll software pipelining technique hence gnu compiler extend UVE instruction assembly mode remain code compile flag sve compilation toolchain flag march armv sve  however compilation toolchain fail vectorize benchmark identify namely seidel 2D MAMR variant covariance floyd warshall benchmark inspect guarantee vectorization correctly perform compiler benefit model loop unroll vectorization consistent UVE implementation VI RESULTS performance evaluation UVE performance evaluation comparison core feature neon extension expand upcoming sve sve UVE extension configure vector UVE configure default cache evaluation aspect code reduction performance average stall per cycle rename stage dram memory bus utilization herein limited benchmark memory access ratio memory core operating frequency analyze propose extension significant average performance advantage sve vectorized benchmark significant ups attain without rely specific code optimization loop unroll performance improvement performance ups consequence contribution significant code reduction average commit instruction sve neon unroll factor    DE  neon UVE sve DE MN reduction commit instruction neon reduction commit inst UVE sve def def vectorized sve compiler vectorized sve compiler sve UVE sve UVE UVE UVE rename cycle neon rename cycle UVE sve DE  def vectorized sve compiler sve UVE UVE memory bus utilization   sve   neon neon sve UVE DE  vectorized sve compiler 1D gemm blas covariance data mining floyd warshall dynamic progr MM algebra jacobi 2D stencil mvt algebra  algebra seidel 2D stencil jacobi 1D stencil memory MAMR data mining MAMR diag data mining MAMR ind data mining memcpy memory saxpy blas   stencil 2D 1D 4D 4D 2D 2D 1D 2D 3D 2D 4D 2D 2D 2D 1D 4D static modifier static modifier indirect modifier  algebra 2D static modifier knn data mining 2D indirect modifier kernel corresponds  loop statement exclude nest  kernel max loop nest memory access  neon UVE BENCHMARKS unroll default loop unroll gemm UVE unroll UVE without unroll evaluation propose vector extension UVE default perform cache none UVE benchmark implementation feature loop unroll vectorization exception infrastructure significantly reduce load latency increase effective memory hierarchy utilization considerable improvement memory bus utilization average increase saxpy   MAMR benchmark gemm MM jacobi 2D seidel 2D covariance floyd warshall affect utilization benchmark bound insignificant rate advantage contribute reduction introduce stall pipeline particularly rename stage decrease instruction code significantly alleviate pressure  issue queue reduction load latency allows incoming instruction pipeline earlier decrease pressure physical register file consequence significant decrease rename per cycle sve enable core benchmark vectorized compiler vector physical register UVE sve ref PRs ref PRs gemm jacobi 2D MAMR gemm jacobi 2D MAMR gemm jacobi 2D MAMR gemm jacobi 2D MAMR gemm jacobi 2D MAMR gemm jacobi 2D MAMR vector physical register performance sensitivity physical vector register sensitivity parameter variation vector register complement analysis register file pressure sensitivity evaluation regard physical vector register  perform subset previously refer benchmark increase  significant performance improvement UVE decrease load latency decrease pressure register file contrast sve increase  increase performance sensitivity  load fifo depth evaluation impact fifo buffer depth MM gemm jacobi 2D MAMR benchmark ref increase depth performance sensitivity depth fifo buffer normalize default vector entry per buffer gemm jacobi 2D MAMR benchmark ref dram dram dram dram UVE performance sensitivity cache memory overall performance default obtain subset benchmark minimum attain adequate performance setup slot default previous slightly improve performance performance saturates application sensitive data access latency MAMR sensitivity fifo depth cache evaluation impact cache memory generally performance benefit however specific benefit fetch regard data structure reuse across program processing module finally overall impact processing module evaluate although conduct simulation dimension modifier per cycle significant difference overall performance hardware overhead accord configuration depict compose processing module feature  adder multiplexing logic individual remain module mostly compose storage implement  accommodate concurrent maximum descriptor dimension modifier KB storage memory request queue maintains outstanding request packed within byte entry finally load fifo buffer compose byte structure entry approximately KB storage hence although physical implementation obtain accurate chip conclude structure footprint cache however impact mitigate reduce maximum dimension gain reduce memory footprint KB cache memory resource across future vii related propose UVE significant related processor architecture research scalable vectorization data memory access decouple ahead execution prefetching scalable vectorization decade simd extension establish option exploit DLP hpc workload however rapid evolution vector architecture forth code portability issue across platform overcome issue approach vector agnostic vectorization recently propose sve  instruction vector fix execution code architecture vector without cod compile sve relies loop predication enable vector scalability predicate instruction eliminate loop enable partially execute loop iteration  allows configuration specific vector handle scalar loop explicitly reduce vector contrast propose UVE allows automatically disable vector bound loop simpler minimal instruction corresponds instruction data representation data traditionally associate dedicate accelerator architecture accurate representation data access substantially accelerate data acquisition approach propose rely dedicate ISAs descriptor mechanism complex underlie specification introduce propose UVE unifies approach adopt descriptor representation configuration ISA interface moreover extends feature predication capability data dependent purpose domain continued effort specialization construct introduce memory access specialization processor execution driven prefetching access propose register file extension issue processor introduces semantics subset processor register however limitation UVE capable dimensionality directly expose loop induction variable albeit indirection although specialized application susceptible vectorization none mention approach explores vectorization ISA memory access decouple benefit concept adopt UVE decouple memory access computation achieve configure loop preamble data acquisition parallel data manipulation explore feature decouple access execute architecture  multiple thread decouple memory access execution reduce data access latency tolerance memory access latency complexity microarchitecture another desc explores approach hardware combine core memory access accelerator computation fully decouple processing program ahead execution prefetching HW SW prefetching specifically tailor memory access latency reduce data locality complex indirect memory access datasets cache currently prefetchers achieve accuracy data access prediction mostly timeliness effectiveness procedure mitigate prefetching cache pollution contrast prefetching approach  paradigm propose UVE allows assure data obtain memory eliminate cache pollution issue nevertheless UVE alternative prefetching instead complementary structure processor data acquisition conclusion unlimited vector extension UVE propose overcome performance degradation vector agnostic simd extension execute costly scatter loop operation extension relies data paradigm explicitly decouple memory access computation remove  memory index instruction code linearize memory access attain simpler vectorization dedicate processor pipeline descriptor memory access specification capable encode complex multi dimensional indirect access vectorized register file introduce processor pipeline directly input output data vector register mitigate load latency accord evaluation conduct gem simulator proof concept implementation processor architecture combination feature introduce UVE performance increase sve