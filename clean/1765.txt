rapid influx  data couple stagnation processing compute highlight critical explore performance accelerator increase throughput demand bioinformatics application argues processing memory pim effective enhance performance mer critical bottleneck stage standard bioinformatics pipeline characterize random access computational intensity proposes dram situ mer accelerator optimize optimize throughput strike balance hardware performance dubbed sieve leverage novel data mapping scheme simultaneous comparison dna lightweight circuitry termination mechanism prune unnecessary dram activation reduce latency evaluation sieve workload datasets aggressive average speedup saving multi core cpu gpu baseline mer index processing memory bioinformatics accelerator introduction bioinformatics enable significant advance health contribution precision medicine disease surveillance population genetics critical application  bioinformatics pipeline genome sequence comparison classification involves align query sequence reference sequence goal identify structural similarity divergence traditional sequence alignment algorithm employ computationally intensive dynamic program technique shift performance heuristic approach mer query sequence subsequence scan reference database underlie assumption biologically correlate sequence mer deployed array bioinformatics task limited population genetics cancer diagnosis metagenomics bacterial protein classification mer application domain focus bioinformatics execution breakdown kraken clark     acceleration bulk mer paramount importance mer sits critical genome analysis pipeline execution breakdown important bioinformatics application target variety task metagenomics population genetics clearly kmer dominates execution application sequence technology generate data rate surpass moore metagenomics alone billion amount data analyze metagenomics pipeline project surpass youtube twitter exemplify data explosion processing overhead precision medicine patient sample sequence roughly  TB  dna rna data develop personalize treatment sample raw sequence parallel various metagenomics stage mer critical kraken task critical role combat pandemic treat antibiotic resistant infection billion health however despite significance acceleration mer compute platform remains challenge due inherently memory bound considerably limit downstream genome analysis task realize potential mer algorithm typically characterize random access across memory cache behavior server feature cache cache  mer rapid growth complexity UI OOVBM  PNQVUFS SDIJUFDUVSF acm annual international symposium computer architecture isca doi isca genomic database task bottleneck bioinformatics pipeline exacerbate computation per mer lookup mask data access latency thereby render exist compute centric platform multi core CPUs gpus inadequate genome analysis task memory centric accelerate bioinformatics application variety flavor recent proposal demonstrate data memory processing promising potential improve efficiency genome analysis task owe application increasingly characterize data movement memory processor computation within processor explores performance mer accelerator logic dram basis acceleration aggressive processing memory pim situ compute goal parallel processing sequence data within dram buffer propose sieve novel scalable situ dram accelerator massively parallel mer specifically sieve architecture incrementally extra hardware complexity unlock performance benefit although approach involves modify conventional dram organization propose conventional dram goal leverage dram technology accelerator ultimately accelerator  chip worth manufacturing effort advantage situ compute bandwidth buffer magnitude cpu data access magnitude however situ compute introduces challenge situ acceleration necessarily tight integration processing logic core dram component prohibitively overhead highly efficient situ accelerator dense regular dram however bioinformatics application typically accelerator memory capacity due ability accommodate  dna datasets analyze within budget exist situ approach rely multi activation wise data mapping perform bulk boolean operation data within buffer substantial loss throughput efficiency finally capitalize performance benefit situ compute mer imperative accelerator provision efficient mer index scheme avoids query broadcasting mechanism quickly transfer payload genome taxon contribution distinguish feature sieve placement reference mers vertically along bitlines dram chip subsequently utilize sequential  activation multi activation propose prior query reference mers simultaneously wise placement  allows employ novel termination mechanism ETM interrupt activation upon successful detection mer mismatch thereby considerably alleviate latency overhead due serial activation knowledge introduce showcase effectiveness wise data mapping scheme mer termination substantially advance throughput efficiency exploit individual mers relatively complex conventional pim task graph processing specialized circuit mer goal minimize associate hardware  explore situ pim accelerator custom logic dram hierarchy chip interface subarray detailed analysis performance  offs discussion integration issue deployment model thermal concern sieve mer implementation cpu gpu perform rigorous sensitivity analysis demonstrate effectiveness processing sieve linearly respect storage capacity considerably enhance performance genome analysis pipeline aggressive average speedup average saving conventional multicore cpu gpu baseline II background introduce mer procedure explain bottleneck stage conventional architecture mer bioinformatics dna sequence series nucleotide commonly denote mers subsequence  algorithm attempt assign taxonomic label genetic fragment sequence unknown origin taxonomic label assignment sequence organism specie traditionally align individual query sequence reference sequence prohibitively processing metagenomics file sequence alignment blast algorithm cpu expert predict genomics become prominent data producer decade demand scalable sequence analysis infrastructure recently alignment rely mer emerge aid genome analysis task due properly label mers sufficient infer taxonomic functional information sequence illustrates typical mer  sequence classifier offline stage reference query seq query kmer payload mers query seq kmer kmer query kmer kmer reference mer null retrieve payload payload payload classify query seq payload mer sequence classification mer database built unique mer taxon label mer   bacteria sequence entry   kmer algorithm slide across query sequence mer attempt retrieve associate taxon label database function query kmer repeatedly kmer database query mer exists database mer taxon label payload retrieve otherwise mer query mers query taxon label mers decision originate organism query sequence popular choice counter retrieve taxon label taxon label classify query sequence reference mer implement clark  leverage hash mer taxon label kraken sophisticated data structure hybrid hash sort mers signature hash bucket binary assumption adjacent mers within query sequence likely signature overlap thereby likely indexed bucket theory improves cache locality purely hash sort approach query mer brings bucket cache subsequent query mers optimization cache performance remains memory bottleneck mer realworld mer application expose limited cache locality sequence classifier reference mers hash access hash generates cache due link traversal hash resolve hash collision hash sort hybrid locality mer bucket fetch cache previous mer lookup kraken datasets discover consecutive mers index bucket bucket fetch repeatedly memory query sequence taxon taxon taxon taxon classify query sequence mers mer DB assign taxon ID mer DB mer taxon ID counter taxon taxon taxon taxon illustration clark mer metagenomics taxon label formal scientific identify bacterial  virus specie request mer benefit  memory access mer typically around byte memory access retrieves cache data usually serf request due locality waste bandwidth finally computational intensity mer mask extend data access latency clark retrieve mers database cycle due cache update counter mers trivially inexpensive amplify memory motivation  address challenge situ mer accelerator namely integrate logic dram hardware overhead propose sieve combat issue identify limitation prior situ adapt mer motivate novel data layout mechanism finally introduce termination mechanism ETM optimize sieve exploit characteristic sequence datasets dram overhead concern situ accelerator dramatic performance gain memory intensive application building reasonable overhead amplifier buffer laid manner dram layout carefully optimize storage density fitting additional logic buffer minimally invasive non trivial moreover layer dram substantially logic building complex logic dram incurs significant interconnect overhead core mer operation sieve boolean logic sieve hardware overhead pim architecture kmer mainly accomplish minimal boolean logic offs sieve explore optimal sieve placement custom kmer logic dram hierarchy interface dram chip sieve local buffer subarray sieve       mismatch mismatch mismatch  activation     mer exist situ accelerator triple activation horizontal data layout  buffer activation mer sieve activation vertical data layout subarrays mer recall dram transistor layout highly optimize storage insert extra logic however minimal significant redesign effort illustrate layout intact intrusive however suffers parallelism latency comparison restrict entire sieve increase parallelism efficiency access leverage subarray parallelism salp performance potential complexity hardware overhead novel data layout mechanism wise mer data layout  mechanism combine termination outperforms prior situ accelerator rely multi activation conventional wise mapping majority mer workload perform bulk bitwise xnor operand dram prior ambit  implement xnor operation  along additional logic analysis timing delay operation advantage previous situ pim ambit baseline ambit   inspire situ procedure performance mer ambit performs bulk bitwise reserve dram assume dna encode ncbi standard typical dram width mer mers wise manner query reference ambit reference data  query  target operation  preset triple activation perform    finally another  activation precharge command completion tRAS trp typical dram chip contrast approach ComputeDRAM enables memory computation commodity DRAMs without integrate additional circuitry approach issue constraint violate sequence dram command rapid succession leaf multiple simultaneously logical logical operation perform via essentially hardware approach leveraged perform mer analysis suggests significant gain performance efficiency achieve employ approach propose eliminates multi activation enables synergistic termination mechanism inhibits activation upon specifically sieve query mer reference mers instead query extensive reference shorter tRAS trp progress reference sieve laid wise along bitlines activation transfer matcher embed buffer comparison matcher latch activate batch reference ETM introduce interrupt latch return zero processing hurt sieve performance leverage parallelism across performs comparison vertical data layout greatly expands initial reference mers reference mers termination mechanism ETM quickly eliminates candidate activation besides latency reduction adopt activation sieve reduces activation additional wordline increase activation data mapping strategy apply multi activation approach efficient sieve simply internal data movement internal data movement associate multi activation unavoidable operand designate furthermore arbitrarily activate inside dram prohibitively decoder possibly overhead activate potentially destroy motivation termination activate consecutive highly unfavorable dram access characterize delay due cycle opening mismatch mers happens encode activate reference  input mer ancestor  sequence reference mer  4GB encode scheme mers mers characterization mismatch mers subarray BL amp matcher latch query ref xnor local RB ETM col  WL WL WL BL BL BL BL WL WL BL BL BL BL WL WL WL WL WL WL WL WL WL WL WL BL BL BL BL WL WL BL BL BL BL local RB local RB compute buffer subarray subarray subarray sieve overview dram zoom subarray facilitates inter subarray data compute buffer subarray matcher circuit  matcher reside local buffer matcher data layout subarray subarray partition mer payload offset payload dominates dram consumption identify novel optimization opportunity exploit concept prefix esp describes mismatch location random sequence average dna sequence mismatch sixth eighth esp mers random mers extract metagenomics reference mers mismatch within encode IV sieve architecture describes sieve introduce exploit parallelism due difference detail sieve functional diagram mainly placement logic circuitry subarray data mapping scheme data layout mer encode binary transpose onto bitlines shift RS index latch finder mer otherwise wise placement described previous within subarray however physical modification interleave reference query mers offset address payload reference kmer precisely payload actual payload taxon label data conventional format motivation payload minimize contention achieve parallelism densely packed dedicate subarrays request rout access contention serialize request broken batch query mers replicate transmission delay inside dram chip prevents broadcasting query matcher dram cycle subarray lockstep manner equivalent matcher query dram cycle ddr micron happens reference mers query mers query mers per batch chip prefetch chip prefetch byte writes command chip prefetch batch batch query mers subarray replace batch command replace batch mers compute subarray matcher enhance amplifier buffer matcher matcher xnor gate gate latch xnor gate reference query latch xnor operation reference query exactly latch default gate previous latch xnor gate update latch accordingly capture outcome finally matcher bypass engage toggle enable signal cycle cycle cycle cycle cycle register SR  matcher ETM ETM seg activation mer WL SA pre identify activation subsequent mers WL SA pre finder WL SA pre ETM seg shift seg ETM seg register ETM seg ETM seg pre WL SA WL SA pre WL SA pre ETM seg ETM seg latency ETM timing analysis WL SA pre latency associate wordlines enable amplifier precharging ETM matcher operation overlap opening ETM critical extra cycle identify  shift RS CF operates parallel opening ETM mer query reference amplifier subarray controller  selects query query subarray bus matcher query distribute matcher bus termination module ETM ETM interrupt activation entire latch zero mer latch latch however challenge approach gate latency dram cycle latch propagate gate propose latch propagates partial pipelined fashion register SR insert latch implement pipeline dram cycle previous SR ORS latch output SR although cycle latch zero SR extra cycle flush finder CF unless interrupt ETM activation query checked query previously reference latch buffer finder identifies bitline latch retrieve offset subsequently payload shift latch challenge approach shifter reasonable hardware latency reference mer CF shift entire latch propose pipelined shifter CF illustrates CF circuit purpose mainly ETM ETM mux backup register   SRs maintain update simultaneously ETM operation zero  associate implies another latch reserve RS amount latch gate finder  shift narrow appropriate contains reserve RS shift happens ETM freed mer CF background retrieve shift RS overlap subsequent mer detail activation query mer ETM dram cycle flush pipeline activation issue CF operation stall ETM completes mer dram cycle CF operation dram cycle scenario therefore contention CF consecutive subarray sieve retains ETM data mapping circuit etc differs aspect instead integrate logic subarrays local buffer logic subarray subset adjacent subarrays within subarrays bandwidth link isolation transistor subarray equip compute buffer retains capability mer ETM local buffer without amplifier unlike mer perform locally individual subarray mer inside compute buffer regardless target subarray query  dispatch involves transfer across subarrays compute buffer subarray enable across subarrays leverage lisa albeit adapt fold bitline architecture sieve built upon validate feasibility detailed circuit spice simulation compute buffer local RB local RB RB RB local RB local RB local RB local RB compute buffer compute buffer local RB local RB compute buffer RB RB RB RB tRAS tsa  iso disable iso enable SA enable hop delay compute buffer iso iso iso iso data across subarrays illustrates transfer source subarray compute buffer dram subarray activate data latch onto local amplifier bitlines subarray fully driven link subarray subarray enable due bitlines subarrays local amplifier subarray sens voltage difference bitlines amplifies local amplifier subarrays bitlines voltage finally bitlines subarrays fully driven isolation transistor disconnect local amplifier subarray precharged data compute buffer local amplifier enable validate spice simulation latency activate subsequent amplifier tsa activate source subarray tRAS latency subarray refer hop delay consists enable isolation transistor link activation amplifier mer walkthrough illustrate mer activation query reference local buffer comparison mechanism described ETM propagates register SRs payload associate mer retrieve CF determines shift  index shift latch calculate index subarray controller index payload address offset sieve sieve quintessential situ architecture due lack processing embed buffer however preserve overall data layout ETM addition intrusive implementation sieve batch batch entry batch query reg logic batch skip reg decoder batch decoder xnor query ref matcher SRAM buffer addr col addr matcher array batch reg sieve query mer query register activation issue controller logic address batch index SRAM buffer batch entry query reference matcher array matcher entry SRAM buffer physical layout dram width batch batch retrieve dram burst command batch varies width introduce component SRAM buffer SB SB organize 2D array entry batch entry width batch batch preset update progress capture outcome highlight batch zero indicates mismatch matcher array consists query reference xnor gate update writes  SB output xnor skip register   ETM contains batch batch  preset progress  zero meaning batch skip without  activation batch comparison comparison mismatch  significant latency reduction batch register   reduces processing due ETM skip batch dram cycle per skip batch valid dram cycle waste previous skip  batch quickly batch finder payload retrieval logic skip batch mapping batch skip shifter apply index batch calculate batch index batch index logic offset payload integration dual inline memory module DIMM pcie factor integrate sieve host pcie incurs extra communication overhead due packet generation DIMM suffers limited typical ddr DIMM around watt GB delivery GB bandwidth sufficient satisfy bandwidth requirement pcie lane pcie lane GB sieve illustrate sieve communicates host pcie interconnect unlike communicates host individual mer request packet protocol delivers mer request per pcie packet pcie accelerator maintains pcie input queue pcie queue pcie packet response queue  service mer request cpu scan query sequence generate mers mer byte request contains sequence ID destination subarray ID header information pcie packet contains request assume KB pcie packet payload sieve buffer request fully saturate capacity GB sieve depth pcie queue pcie packet request packet rank rank request sieve remove pcie packet pcie input queue unpacks distributes request target request   batch pcie packet pcie queue sieve sends interrupt cpu packet pcie queue empty slot pcie input queue entire sieve memory mapped host  memory avoid virtual memory translation cache coherence management regardless configuration DIMM pcie program interacts sieve device sieve api transpose conventional database format wise access later load database sieve device mer query api implementation user library associate kernel module driver interface sieve hardware api implementation future mer database relatively stable database load sieve device user database database standard within genomics community reuse amortize database load mer subarray mapping without appropriate mapping scheme query broadcast across accelerator ıve mapping scheme involve index query subarrays scheme quickly index increase exponentially mer efficient scalable index scheme wherein index linearly memory capacity kmer specifically reference mers subarray sort  entry index maintains byte subarray ID along integer mers respective subarray identify index upon request sieve convert query mer integer representation consults index subarray contains exploit parallelism index scheme address index scheme query checked subarray index MB GB capacity reasonable dedicate bioinformatics workstation sieve host input query sequence extract mer mer mer subarray index consult destination subarray mer request described IV mer request subarray grouped batch kmer request per batch query mers query batch buffer pcie device buffer dma pcie bundle batch pcie packet described IV sieve device sieve dispatch batch query mers destination subarray replaces already query mer batch batch individual mer request batch potentially issue oforder subarray becomes available request involve response packet host sequence IDs payload examine processing upon completion mer request sequence accumulate payload fed classification illustrate additional reorder host accumulate payload typically histogram taxon dna sequence workstation configuration cpu model intel xeon core thread frequency ghz KB KB MB memory ddr mhz memory organization 2GB channel rank gpu model pascal nvidia titan II query sequence summary query file sequence seq mer  accuracy HA sequence mers  accuracy sequence mers simba accuracy SA sequence mers  timing HT sequence mers  timing MT sequence mers simba timing ST sequence mers methodology workload kraken clark cpu baseline  gpu baseline  4GB 4GB  8GB 8GB ncbi bacteria genome 4GB query sequence summarize II baseline performance model report workstation configuration gpu baseline idealize latency data transfer host gpu memory assume avoid query multiple baseline dram consumption estimate memory trace associate mer function obtain  DRAMSim configure workstation cpu intel PMC exclude interference component gpu nvidia visual profiler perform characterise multi gpu inference server efficiency exclude spent operation consistent methodology  circuit spice validation sieve component matcher contact amplifier BLs presence matcher circuit load capacitance BL increase spice simulation confirm sieve reliably amplifier matcher circuit implement ptm transistor model relatively input capacitance matcher circuit comparison BL capacitance matcher negligible operation amplifier activation BL voltage matcher validate operation link dram circuit model simulate transfer data local buffer adjacent subarrays simulation initial varied across dram variation matcher link subarrays flip distortion latency model estimate latency overhead sieve component FreePDK  model synthesize SRAM buffer factor  technology node planar dram model propose park estimate overhead model sieve assume pipelined implementation sieve host cpu performs pre processing kmer generation driver invocation pcie transfer postprocessing accumulation response payload genome sequence classification sieve responsible kmer analysis confirms latency pipeline limited mer processing sieve mer sieve comparable pre processing cpu cpu mer request sieve fully utilized model pre processing baseline cpu described treat classification pipeline algorithm differs application independent mer primary focus forgo model effort genome classification mer processing model mer trace driven simulator custom DRAMSim simulator model pcie communication overhead standard pcie parameter micron ddr chip ddr 4GB building sieve dram parameter extract datasheet modify account estimate latency overhead matcher ETM finder finder VI experimental RESULTS latency estimation evaluation summarizes dynamic static sieve component incurs additional consumption dram activation however formula micron technical documentation sieve consumes activation regular dram load extra transistor introduce amplifier bitline driver overhead understand sieve component matcher array ETM dominate consumption capture overhead incur sieve spent finder finder negligible overhead regular dram activation modification buffer intensive latency evaluation latency sieve component assume access sieve component latency analysis component dynamic static latency QR   SRAM buffer ETM finder finder SRAM buffer query register overlap entirely command retrieves batch reference although register critical negligible overhead dram cycle ETM gate timing requirement operation within dram cycle finder compose shifter latency operation within dram cycle evaluation estimate overhead sieve model propose adopt dram amplifier layout described patent micron conventional dram layout amplifier respectively accommodation matcher ETM finder circuit local buffer local amplifier extra amplifier overhead link subarrays overhead compute buffer CB dram chip local amplifier enhance mer logic enable subarray parallelism address latch subarray overhead component strip dram model SRAM buffer kbit circuit increase individually kernel performance improvement comparison situ accelerator simulate ideal baseline mimic prior proposal improve accelerator ComputeDRAM speedup cpu baseline implement sieve without ETM col assumption ComputeDRAM col accelerator latency transfer payload assume sieve architecture configure capacity  parallelism index scheme fourth assume ComputeDRAM shorter triple activation latency due issue memory command rapid succession convention workload axis kernel query kernel kraken clark query file II 4GB 8GB ncbi bacterial reference 4GB observation perform similarly without ETM slightly activate mer data ComputeDRAM writes comparison query mer replicate across ComputeDRAM outperform without ETM approach owe triple activation approach sieve allows benefit ETM strategy additional speedup contrast ComputeDRAM lack opportunity conclude contribution layout therefore enable ETM amortize setup across writes performs slightly without ETM mer mismatch average roughly evaluation sequence datasets typically characterize mer rate around sieve leverage ComputeDRAM mer accelerator entail challenge query query command per query sieve query query brings challenge impact ability efficient index scheme address challenge maintain performance efficiency benefit approach future improvement cpu average speedup saving normalize cpu measurement constrain memory capacity GB compute buffer per midpoint CB performance configuration VI performer concurrently subarrays SA clearly energyefficient limited speedup benchmark workload significant additional performance potential tapped via situ approach however likely outperform cpu gpu memory capacity grows parallelism bandwidth memory capacity proportional performance achieve non pim traditional architecture due memory situ sieve comparison comparison cpu baseline comparison gpu baseline speedup saving respectively cpu baseline comparison speedup reduction cpu baseline clearly showcasing substantial benefit realize exploit finer grain parallelism subarray sieve sensitive characteristic application MT BG benchmark performs ST BG benchmark mer MT BG ST BG benchmark activation increase overall query turnaround furthermore recall IV termination mechanism interrupt activation detect mismatch minimize overall turnaround consumption workload mer improvement gpu speedup saving various sieve GB gpu baseline gpu efficient modestly faster however memory capacity sieve dataset increase likely outperform gpu unless gpu memory capacity reference datasets onto sieve avoid repetitive data transfer host memory gpu dramatically outperforms gpu leverage subarray parallelism speedup saving increase dram bandwidth simply increase bandwidth dram cpu gpu baseline sufficient address performance bottleneck kmer bottleneck bandwidth memory intensive percentage load rob memory bandwidth underutilized SA SA SA SA SA SA SA SA cycle 4GB 8GB 6GB 2GB average cycle spent cpu benchmark MSHR unable multiple load available MSHRs quickly deplete stall subsequent load rob prevent bandwidth fully saturate  broadwell core MSHRs sustain outstanding memory access load concurrently memory latency throughput workstation equip core substantial increase consumption considerable wastage dram bandwidth portion retrieve cache useful  highly optimize suspect gpus constrain bottleneck CPUs although pinpoint microarchitectural structure sensitivity analysis subarrays per analyze impact subarray parallelism performance various configuration memory capacity subarrays per average across benchmark subarrays perform mer simultaneously without increase overhead significantly feasible due delivery constraint however assume issue although sieve mer throughput increase concurrent subarrays speedup plateau subarrays probably access conflict resolve subarrays compute buffer explore performance tradeoff compute buffer reference SA without subarray parallelism compute buffer per observation compute buffer faster margin activation burst batch matcher CB hop across subarrays compute buffer hop delay faster burst latency tCCD equip ETM CB likely data movement average however chain activation amplifier relay compute buffer consumes significant sparse compute buffer efficient compute buffer CB compute buffer generally increase compute buffer per increase efficiency explain previously compute buffer reduces activation amplifier reduces delay consumption overhead compute buffer per finally speedup reduction CB slightly trail SA CB hop per activation however overhead CB enable subarray parallelism ETM simulate adversarial query kmer ETM speedup reduction cpu gpu baseline average across benchmark without ETM faster efficient cpu faster efficient gpu pcie overhead pcie simulation overall pcie communication overhead ideal mer request dispatch destination subarray return host vii related discus previous concern sieve concept pim date proposal integrate logic 2D planar dram effort largely remain inception stage due challenge fabricate logic dram recently 3D stack technology practical approach logic underneath dram  pim research fully exploit benefit 3D stack architecture domain specific accelerator graph processing pointer chase data analytics propose evaluate sieve 3D stack context future non dram situ accelerator nvm SRAM situ accelerator  compute cache propose dram maturity availability quicker development deployment cycle furthermore SRAM generally capacity dram subarrays shorter buffer evaluate nvm sieve future pim genomics accelerator recently pim explore algorithm specific pim architecture genomics GenCache modifies commodity SRAM cache algorithm specific operator achieve reduction speedup dna sequence  medal leverage commodity load reduce dual inline memory module LRDIMM augments data buffer custom logic exploit additional bandwidth parallelism dna radar scalability blast mapping extension onto dense 3D non volatile memory however effort ideal mer GenCache hardwired logic SRAM compute shift ham distance myer levenshtein distance mer medal highly optimize FM index dna relies data structure array accumulative array occurrence array mer associative data structure radar bind extension stage irrelevant mer maximize speedup pim genomics accelerator pim explore algorithm specific architecture genomics GenCache SRAM accelerator dna sequence alignment medal augments data buffer commodity DIMM exploit additional bandwidth parallelism dna radar scalability blast mapping extension onto dense 3D nvm effort rely domain specific knowledge achieve maximal speedup specific algorithm applicable kmer complementary sieve CONCLUSIONS identify mer bottleneck stage genomics pipeline due memory intensive propose sieve dram memory architecture accelerate mer reference mer along bitlines enhance buffer minimal boolean logic mer optimize sieve termination mechanism limited benefit CPUs gpus extensive speedup CPUs modest benefit gpus compelling benefit speedup saving cpu respectively gpu