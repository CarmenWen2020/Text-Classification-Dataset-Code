popular important web service invert index standard data structure adopt text recently custom hardware accelerator invert index emerge demonstrate throughput conventional cpu gpu however attention paid address memory capacity pressure invert index conventional DDRx dram memory significantly increase terabyte memory instead memory pool compose storage memory scm device promising alternative memory capacity however scm pool memory challenge limited bandwidth scm device interconnect host cpu propose boss data processing ndp architecture invert index scm pool memory maintains throughput query processing  environment boss mitigates impact bandwidth scm device employ termination algorithm reduce footprint intermediate data introduce programmable decompression module compression scheme invert index furthermore boss selection module hardware substantially reduce host accelerator bandwidth consumption apache lucene production grade library cpu core boss achieves geomean speedup various complex query reduce average consumption index text invert index data processing storage memory hardware accelerator introduction text popular web service user quality service efficient processing invert index fundamental data structure manage document information decade demand motivate numerous research compression scheme invert index optimization technique target purpose CPUs gpus recently hardware accelerator propose address architectural limitation cpu gpu hence throughput efficiency hardware software technique effectively improve query throughput attention paid address memory capacity issue due sheer volume invert index usually partition multiple shard distribute across multiple node invert index steady flood web document conventional  approach memory capacity deploy node effective dram DIMMs cpu core super linear increase increase memory capacity memory capacity cpu socket increase factor emergence commercially available storage memory scm intel optane DC persistent memory module  opportunity alleviate memory capacity scm tier memory hierarchy target bridge gap memory dram disk scm performance dram latency bandwidth significant advantage dram capacity intel optane  GB per channel maximum dram capacity per channel furthermore scm DIMMs organize memory pool cpu socket via byte addressable cache coherent interconnect compute express link  gen capacity memory pool virtually unlimited via memory disaggregation without additional cpu socket minimize increase although effective capacity  memory pool introduces challenge architecting throughput text specifically bandwidth limitation negative impact query performance bandwidth scm device fold dram device furthermore interconnect bandwidth host cpu dram channel yield bandwidth capacity ratio without address bandwidth memory pool introduce severe bandwidth bottleneck therefore propose boss bandwidth optimize accelerator target storage memory reduce data movement host cpu scm memory pool interconnect bandwidth consumption adopt data processing ndp paradigm boss UI OOVBM  PNQVUFS SDIJUFDUVSF acm annual international symposium computer architecture isca doi isca memory pool filter traffic host cpu interconnect bandwidth consumption boss integrates selection module pipeline document instead entire unsorted transfer cpu outcome addition prevent bandwidth scm device become performance bottleneck boss integrates technique scm device bandwidth hardware skip mechanism examines relevant data minimize volume intermediate data multi query processing programmable decompression module compression scheme invert index summary contribution identify memory capacity invert index proposal host invert index scm pool memory efficiency propose boss specialized ndp architecture efficient invert index accelerates entire processing bandwidth efficient manner detailed evaluation boss  simulator synthesizable rtl chisel boss achieves geomean speedup apache lucene cpu core across various complex query consumption TSMC technology node II background storage memory storage memory scm tier memory hierarchy bridge gap memory dram disk hdd ssd variant scm utilize device technology  stt ram phase memory pcm byte addressable nonvolatile scm performance dram latency bandwidth scm feature asymmetric bandwidth writes latency intel optane DC persistent memory module  dram bandwidth dram respectively despite performance penalty scm spotlight effective alternative dram capacity data environment intel optane  GB per channel maximum dram capacity per channel moreover capacity gain scm per dram scm attractive data workload memory invert index invert index invert index standard data structure effectively manage document information text relates document unique query processing invert index document identifier docIDs sort correspond additional information frequency document document focus comprise docid frequency express tuple docid frequency invert index usually prepared offline query invert mostly data structure compression invert index invert index owe web document compress minimize storage overhead multiple fix delta compute consecutive docIDs compression apply delta instead docIDs storage widely compression scheme pack BP  VB  pfd    detailed explanation scheme available VI text typical query workflow comprise fetch compress memory decompress execute operation retrieve optional rank illustrate google employ multiple distribute node invert index consists node leaf node user query node dissect correspond scatter multiple leaf node typically invert index multiple disjoint partition shard accord interval docIDs leaf node distinct shard operates shard entire query processing fully parallelize across leaf node leaf node compress resides memory relevant fetch decompress user query operation perform accord query instance query union query merge perform union operation docIDs query intersection query docIDs chosen perform intersection operation union merge sort suffices already sort intersection document input return membership docid docid exists greatly versus SVS intersection algorithm efficiency algorithm performs operation hence reduce comparison membership rank sort relevant document multi stage rank algorithm retrieve candidate document ranked stage improve quality satisfy tight constraint response stage usually adopts bag function retrieval candidate adopt  BM rank function production relevance document BM query idf  idf query document query sum query frequency query document  document average document document corpus respectively constant usually chosen idf inverse document frequency query  across document corpus nutshell frequently relatively document rarer throughout document corpus impact overall document stage various sophisticated rank algorithm apply possibly multiple recently neural rank model introduce refine stage boss leaf rank stage software prior stage candidate retrieval stage memory capacity invert index memory channel volume content web demand additional memory capacity data straightforward approach memory capacity  cpu socket dram bandwidth capacity however accompanies superlinear increase cpu core obtain TB memory scm pool memory architecture cpu socket assume cpu socket GBs dram scm memory alleviate increase hardware accommodate capacity cpu socket intel optane  eighth cpu socket ddr dram memory capacity TB cpu socket assume socket memory channel 2GB optane  attach channel scm pool memory scm pool memory effective option memory capacity scm pool memory architecture memory pool consists multiple memory node comprise multiple scm DIMMs memory controller memory node cpu socket via memory semantic cache coherent interconnect  gen memory node memory pool memory capacity performance issue arise memory node increase link host cpu fix bandwidth GB  link reduce memory bandwidth capacity ratio hardware acceleration invert index commodity CPUs gpus ideal platform execute invert index memory hierarchy CPUs originally boost performance exploit data reuse becomes source inefficiency access invert index feature data reuse  acceleration invert index challenge due limited memory capacity GBs gpu incurs significant overhead transfer data host cpu memory gpu memory narrow pcie channel hardware accelerator custom memory hierarchy emerge viable alternative commodity platform related IIU specialized hardware architecture processing essential operation invert index decompression operation intersection union relevant limitation IIU suboptimal performance scm memory effective capacity requirement specifically IIU intersection incurs frequent random memory access binary intersection algorithm union algorithm retrieve data memory without prune mechanism finally IIU specific compression scheme hardware limit applicability boss however carefully address limitation IIU enables efficient hardware accelerate efficient scm memory STRATEGIES bandwidth  boss primary objective boss bandwidth efficiency scm device interconnect host cpu exist accelerator invert index  IIU assume bandwidth  memory perform sub optimally scm pool memory efficient utilization scarce memory bandwidth  boss exist accelerator discus strategy interconnect bandwidth cpu scm device bandwidth implementation strategy described detail IV host interconnect bandwidth consumption data processing boss data processing architecture invert index perform scm DIMMs host cpu boss fully exploit internal bandwidth scm DIMMs memory node contrast accelerator deployed host fetch data bandwidth interconnect regardless aggregate memory node memory pool hardware selection exist accelerator selection operation compute instead perform operation invert index host memory task selection host cpu deployed scm pool memory host accelerator output unsorted document memory transfer host cpu sort selection integrate selection logic compute boss greatly reduces volume intermediate cpu entire invert index pool memory usually user browse enables memory pool without introduce performance bottleneck interconnect scm device bandwidth consumption termination exist accelerator consume amount dram bandwidth query exhaustively document satisfy query however termination ET technique skip portion document satisfy criterion ET document compression ratio scheme synthetic datasets datasets hybrid applies compression scheme entire dataset indicates compression scheme dataset skip evaluation document specifically estimate upper bound query document skip upper bound query document cutoff ET technique effective query redundant computation boss integrates ET hardware pipeline union fetch compress data document former similarity  interval prune estimate upper bound contrast latter wand estimate upper bound query document multi query processing optimization multi query processing efficiently hardware crucial reduce bandwidth usage  na√Øve merge perform multiple operation intersection query multiple effective  merge load constitute query however perform iterative intersection versus SVS algorithm reduces intersection operation subset unlike  IIU adopts SVS algorithm  intersection query however generates unnecessary memory access load intermediate data instead boss implement pipelined intersection performs multiple intersection obviate intermediate data memory effectively reduces waste bandwidth access intermediate data programmable decompression module compression ratio synthetic datasets compression scheme implement BP VB  apply compression scheme datasets  CC news hybrid approach applies compression scheme datasets  outperforms pfd former synthetic data comprise integer uniformly integer sparse dense uniform cluster consist uniformly picked integer randomly chosen cluster outlier overall structure boss integer normal distribution standard deviation outlier distribution zipf zipf compression scheme differs characteristic input achieve compression ratio boss various compression scheme dynamically configure datapath inside decompression module approach hardware efficient reuses hardware primitive execute decompression scheme furthermore decompression scheme express compose primitive IV boss overview overall structure architecture boss boss handle invert index pipeline rank stage handle software boss memory controller memory node consists boss core peripheral query arrives host cpu boss buffer query command queue query scheduler assign query boss core schedule core boss perform pipelined execution fetch decompression operation selection depicts dataflow within boss core boss core access scm memory query processing memory access interface mai handle memory request address translation local tlb remote access boss core operates shard local node finally deliver host via byte addressable interconnect index structure per metadata invert index sort lexical indexed compose tuple docid frequency BM delta gap compute consecutive docIDs delta instead raw docIDs compress compression efficiency hybrid approach minimize overhead variable per accord distribution maintains metadata efficient skip decompression efficient skip uncompressed docid maximum address offset compress decompression contains encode width offset exception index metadata per information various compression scheme boss boss query execution boss query union intersection handle query differently query complex mixed query compose query mixed boss executes query accord priority operation execution query union query union module merge boss core union union multiple boss core iterative loop within core processing union query fetch module fetch skip apply scm bandwidth inspect metadata individual fetch boss core subsequently performs decompression union selection generate outcome intersection query unlike union query boss core intersection intersection query boss core iteratively fetch  performs intersection intersection query boss core computes instruction intersection fed fetch module fetch module fetch compute intersection decompression intersection intersection query execution significantly improves fetch efficiency discus skip mechanism detail IV fetch module mixed query mixed query union intersection boss performs intersection mixed query boss core performs intersection union intermediate decision boss bandwidth storage efficiency intermediate intersection yield input boss core explains detail individual building boss core fetch operation myth missouri  module fetch module decompression module intersection module union module module module fetch module fetch module selectively fetch candidate reduce volume load candidate candidate document satisfy query potentially outcome intersection candidate document exist input union suffices exist intersection query overlap query overlap inspects metadata docid overlap input illustrate overlap operation intersection overlap candidate load intersection myth query pipelined manner intermediate docIDs intersection myth fed fetch module overlap intermediate candidate unlike intersection query query union satisfied document input query cannot reduce volume load union instead boss applies termination ET algorithm inspire   prune inclusion outcome document cutoff become candidate document compute document computation pipeline maximum sum multi query upper bound criterion examine estimation within fetch module calculates upper bound query document sum maximum overlap document information retrieve overlap previous upper bound query cutoff rank document entry bitstream buffer lut reg reg extractor reg reg docid reset input output    reg extractor extractor stage structure decompression module boss safely skip candidate document illustrates union missouri  estimation calculates upper bound query shade  upper bound query document docIDs upper bound query cutoff shade load maximum shade instead document docIDs become candidate document shade correspond missouri load downstream execution decompression module structure decompression module analyze multiple popular compression scheme invert index canonical structure compose stage flexibly multiple compression scheme individual payload extract serialize bitstream extract payload manipulate differently accord compression scheme payload exception handle compression scheme delta encode payload previous obtain compress docid observation datapath nearly compression scheme stage stage programmable specify connection input output primitive array muxes  stage fix datapath configurable parameter reduce hardware program interface configure decompression module elaborate IV configure decompression module intersection module intersection module consists intersection merger augment comparator docIDs intersection multiple intersection loop fetch decompression intersection stage IV iterative fully pipelined neither performance max union module union module union module union module sorter pivot selector document scheduler loader cutoff  docIDs sid pivot cutoff cutoff cutoff structure operation union module sid  docid bold pivot shade  docIDs dot penalty spill intermediate scm memory refill grain pipelining query facilitates operation fetch perform aggressive filter decompress docIDs available information available iteration boss selects input increase efficacy SVS membership union module union module implement wand algorithm hardware wand effectively skip computation document upper bound  cutoff impossible outcome estimate upper bound query document docid evaluate denote docid sid aforementioned upper bound sid calculate sum maximum entire sid evaluate intuition sid sid possibility document sid vice versa wand selects pivot sid upper bound query cutoff document docid pivot guaranteed query cutoff hence safely skip without pivot sid sort increase sid sake illustration docIDs evaluate pivot query  document docIDs pivot dot safely skip without calculate query document pivot actually sid pivot hardware structure union module queue bold equivalent sid  docIDs document queue docid pivot union module operates sorter defines sids loader fetch pre calculate upper bound query sid lookup accord output sorter pre calculation module maximum entire unique combination upperbound query limited union pre calculation perform query pivot selector chooses pivot upperbound query sid cutoff document scheduler sends pivot available module pop document queue sids pivot illustrates sid pivot estimate upper bound cutoff assume document docIDs document discard queue pivot document module boss adopts widely  BM metric reduce runtime computation overhead boss pre computes invariant portion function II index specifically document boss calculates sub expression BM frequency metadata metadata arithmetic operation multiplication addition sufficient runtime obtain pre computation increase per document metadata module utilizes fix divider fix multiplier fix adder compute accumulate module module return document boss utilizes priority queue entry contains docid query priority queue sort descend query docid arrives query module insert hardware priority queue boss adopts shift register implementation document entry shift entry insert broadcast entry queue entry local decision remain shift load incoming entry default boss chip buffer boss core chip buffer intermediate data pipeline buffer individual module fetch module address metadata instance decompression module target compress intersection union module intermediate docIDs previous stage header encode variable extractor extractor extractor extractor  stage  reg reset reset shr input input  reg reg output output valid stage   stage  configuration file  module instance module KB temporarily docIDs buffer KB summary boss core KB SRAM chip buffer issue offload api boss expose intrinsic function offload init void init file  file  init communication pipe host boss memory mapped register load invert index file  disk scm memory pool par configuration file  sends encode configuration boss val  val   addr  addr  val  invoked offload query request boss query specify  argument user define query query bracket logical operator distinguish query operator user query quotation user specify query api par  convert appropriate sequence operation sends boss  specifies compression scheme parameter allows user pre define compression scheme query   argument address  address return  memory reserve    boss query configure decompression module decompression module flexibly configure configuration file configuration  VB compression scheme brevity component index counter exception index comparator omit configuration file correspond stage decompression module IV stage stage parameter suffices stage configuration file specifies connection primitive style structural model hardware description verilog chisel actually visualizes datapath configuration decompression module configuration file address translation init sends physical virtual address mapping information invert index memory address interface mai boss assume memory node pool 2GB DIMMs TB physical address 2GB workload memory footprint boss local duplicate tlb entire physical address entry prevents tlb generate additional memory access host intervention multi query boss core natively query query multiple boss core merger intersection union module boss core flexibly another merger multi operation boss core query hardware text query trec query boss handle query host query subqueries allocates memory intermediate subqueries boss subquery without prune selection intermediate host memory finally host data retrieve output evaluation methodology evaluation model evaluate boss apache lucene IIU apache lucene production grade library popular web service twitter instagram netflix ebay IIU invert index accelerator summarizes configuration scheme core scheme fix estimate performance boss IIU implement cycle simulator integrate DRAMSim assume intel optane  baseline memory module scheme carefully validate behavior report experimental adjust DRAMSim timing parameter IIU ignore selection hardware methodology host  core intel xeon scalable processor 0GHz KB cache KB cache MB private MB unified host memory organization ddr ecc reg channel 4GB intel apache pas memory channel TB bandwidth GB GB per channel GB GB per channel boss configuration boss boss core 0GHz boss core fetch module decompression module intersection module union module module module boss memory organization scm channel bandwidth bandwidth 6GB sequential 6GB random bandwidth 3GB II query  operation estimation implement boss chisel compile verilog synthesize verilog code synopsys compiler TSMC standard library workload conduct web datasets   CC news publicly available dataset contains news article crawl news site   crawl  web crawler public cmu research purpose correspond web datasets compression scheme BP VB  advance boss query randomly query query trec terabyte dataset trec dataset specify query randomly assign query II performance query throughput analysis multiple core query throughput boss IIU computation core datasets normalize thread lucene cpu core query throughput boss superior  IIU boss core achieves average query throughput improvement lucene baseline  CC news datasets respectively contrast average query throughput IIU core lucene respectively IIU decent performance unlike query IIU membership strategy skip unnecessary overlap multi core throughput analysis  normalize lucene core scm multi core throughput analysis CC news normalize lucene core scm bandwidth utilization  bandwidth utilization CC news document boss intersection query however contrary boss sequential IIU binary generates random access latency random access longer sequential access scm IIU performs boss bandwidth utilization boss IIU query  boss substantially bandwidth consumption IIU maintain query throughput IIU  core throughput analysis normalize lucene core scm normalize evaluate document  boss bandwidth efficiency IIU boss maintains throughput superior bandwidth efficiency boss scalable performance IIU increase core due difference bandwidth efficiency IIU maximum performance core boss aggregate bandwidth scm device future boss utilize additional core effectively IIU yield query throughput detailed performance analysis core source bandwidth efficiency boss elimination intermediate data movement termination reduce computational memory access waste normalize core throughput lucene IIU boss evaluate boss multi query module denote boss exhaustive visualize throughput improvement due ET algorithm integrate fetch module union module comparison decompression module boss IIU throughput improvement boss exhaustive decrease increase union query increase likely document overlap upper bound query calculate estimation become looser increase hence degrade effectiveness contrast throughput intersection query improves increase due pipelined intersection overlap intersection iterates docIDs overlap reduce boss skip unnecessary intersection perform hardware pipeline simultaneously intersection cycle hidden boss exhaustive exhibit throughput increase IIU query due normalize memory access performance comparison lucene IIU boss dram scm normalize lucene core scm lack intra query parallelism boss contrast IIU decompression regardless whereas boss decompression normalize evaluate document boss IIU union query boss skip document fetch module boss fetch module union module module skip efficiency  boss module skip document increase skip document decrease fetch module false positive increase overlap selection prevents fetch module effectively skip however union module reduce unnecessary docIDs wand reduce evaluate document decrease memory access normalize memory access boss IIU majority memory access generate intermediate data LD inter ST inter ST filter via multi query optimization module respectively reduce volume memory access ST boss decrease interconnect bandwidth consumption effectively boss hinder memory pool boss significantly reduces memory load LD LD skip mechanism comparison dram depicts performance comparison lucene IIU boss core dram dram evaluation ddr channel 2GB throughput normalize lucene scm core throughput lucene dram almost lucene scm former outperforms latter boss boss component component boss core command queue query scheduler mai tlb boss core fetch module decompression module intersection module union module module module lucene affected choice memory device performance mostly bottleneck computation IIU boss achieve substantial speedup lucene dram lucene dram boss substantially outperforms IIU almost IIU benefit dram boss IIU generates random access query random access substantially faster dram scm finally IIU boss faster dram performance scm however dram practical choice memory node secure capacity analysis breakdown boss boss core occupies occupies boss core module module boss core due fix divider pipelining operation module due shift register maintain remain component boss remain component boss average breakdown boss boss consumes host cpu evaluation average package prof superior efficiency boss purpose cpu invert index boss achieves performance improvement saving normalize consumption lucene boss multiple computation boss consumes lucene combination performance improvement saving yield saving boss efficient alternative purpose CPUs invert index consumption cpu intel soc consumption boss core normalize lucene core scm axis VI related software optimization technique improve query performance various software improve query performance CPUs gpus cpu optimization technique target intersection decompression stage improve compression throughput query performance leverage simd instruction cache structure technique limited coverage operation invert index boss performs operation efficiently hardware exist gpus aim accelerate operation gpu parallelism approach however substantial overhead limited memory capacity gpus frequent cpu gpu communication instead boss data processing eliminate unnecessary data movement host cpu accelerator invert index compression compression scheme propose mitigate storage BP calculates minimum VB utilizes multiple actual payload msb integer pfd majority delta gap compress array seek pack integer within array various combination boss compression scheme reconfigurable decompression module termination technique max skip evaluation document upper bound wand max skip upper bound calculation pivot interval prune  apply finegrained approach boss adopts interval prune technique building hardware pipeline decouple per per docid termination specifically boss longer interval minimize delay adjacent load request vii conclusion boss ndp accelerator architecture invert index target emerge scm memory pool bandwidth constraint apply strategy synergistically overcome bandwidth challenge impose scm pool memory skip mechanism multi query optimization internal scm device bandwidth consumption selection hardware module data processing paradigm external host accelerator bandwidth byte addressable interconnect accord evaluation cycle simulator synthesizable rtl chisel boss achieves geomean speedup various complex query reduce average consumption apache lucene production grade library cpu core