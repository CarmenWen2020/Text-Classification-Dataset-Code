gpu workload complexity increase demand gpu computational emerge workload gpu kernel launch incur overhead exhibit data dependent behavior kernel synchronization gpu utilization task execution model propose issue significant programmer effort application proprietary task program model specify task task dependency address propose BlockMaestro software hardware combine command queue reorder kernel launch static analysis runtime hardware dynamically identify resolve thread data dependency kernel static analysis memory access kernel launch BlockMaestro extract inter kernel thread data dependency BlockMaestro introduces kernel pre launch reduce kernel launch overhead experienced multiple dependent kernel correctness enforce dynamically resolve thread data dependency runtime hardware BlockMaestro achieves average speedup data dependent benchmark minimal hardware overhead index gpgpu simd data dependency thread schedule introduction graphic processing gpus computationally powerful hungry massively parallel device capable processing application thread advantage instruction  SIMT paradigm workload complexity gpus stress accelerator machine framework typically layer encapsulate gpu kernel accelerator future exascale computer scientific compute application iterative structure grid computation exhibit wavefront parallelism multiple gpus specialized interconnects emerge workload significant burden gpus launch kernel application execution kernel launch overhead become significant kernel typically exhibit significant data dependency layer cnns data consume layer stencil computation scientific compute operation perform dependent inter kernel data dependency typically enforce coarse grain manner implicit barrier synchronization kernel launch stall computation already satisfied dependency circumvent issue task execution model runtimes propose framework programmer decompose application task express task dependency proprietary program model amd  cuda graph openmp task etc enforce runtime benefit task execution kernel launch overhead significantly reduce collectively launch kernel launch persistent kernel task queue dependent task execute data dependency met however gain benefit exist gpu application refactored proprietary task program model propose BlockMaestro benefit task execution exist SIMT program model cuda amd hip avoids code modification insight BlockMaestro kernel pre launch  inter kernel data dependency resolution achieves benefit task execution model pre launch dependent kernel mask kernel launch overhead enforce correctness thread TBs pre launch dependent kernel execute thread data dependency resolve inter kernel thread data dependency kernel denote kernel respectively bipartite graph illustrate UI OOVBM  PNQVUFS SDIJUFDUVSF acm annual international symposium computer architecture isca doi isca TB TB TB TB TB TB TB TB TB TB TB TB TB TB TB TB TB TB TB TB TB TB TB TB TB TB TB TB TB TB TB TB TB TB TB TB TB TB TB TB TB TB TB TB data kernel constitutes dependency TBs series bipartite graph entire gpu application series bipartite graph collectively task graph thread task task paradigm leverage SIMT program model grid thread inherently task explicit input output global memory define kernel launch parameter therefore achieve benefit task execution automatically extract enforce bipartite dependency graph pre launch dependent kernel contribution propose kernel pre launch mask kernel launch overhead dependent kernel addition introduce command queue reorder increase opportunity kernel pre launch leverage compiler extract inter kernel data dependency exist gpu application without programmer intervention define structure gpu application SIMT program model allows extract data dependency bipartite dependency graph propose resolve grain data dependency inter kernel thread ensures correctness pre launch kernel enables dependent thread execute data dependency satisfied II background motivate detail BlockMaestro evaluation IV relevant contribution literature conclude VI II background gpu execution api command queue gpu application host series api function interact gpu api kernel launch memory transfer host synchronization etc api command queue cuda hip terminology processing api serialize command queue therefore kernel execute command queue concurrent execution independent kernel kernel issue multiple command queue synchronize kernel across command queue complex synchronization host api default memory operation memory allocation transfer gpu synchronous kernel launch asynchronous non therefore host launch gpu kernel host execute code explicitly synchronize gpu kernel kernel output therefore programmer aware dependency host gpu  raw ensure synchronization compile gpu program assembly gpu program typically cuda OpenCL hip compilation program translate assembly hip compile gcn assembly cuda compile ptx intermediate IR jit compile kernel launch SASS assembly target gpu offline compilation stage hip gcn cuda ptx potentially compilation stage ptx SASS compilation stage enables optimization additional parameter kernel  thread grid optimize kernel launch overhead due complexity launch computation kernel gpu kernel launch overhead negligible prior kernel launch incur overhead gpu application complexity machine framework utilizes gpus compute operation convolution incur kernel ML model workload significant synchronization implement implicitly kernel towards prior explore reduce kernel launch overhead related detail prior another approach reduce kernel launch overhead program SIMT program model task program model task runtimes avoid kernel launch overhead dynamically resolve data dependency task persistent kernel task queue task execution model paradigm task program model programmer specify series operation task dependency exist task program model categorize broadly task kernel task thread paradigm detail paradigm discus strength weakness propose thread task approach task kernel paradigm task task graph mapped kernel amd  cuda graph user define kernel dependency alleviate kernel launch overhead framework aim identify static operation graph consist kernel consolidate kernel launch task graph launch framework overhead kernel launch fail advantage grain data dependency exist kernel thread dependent kernel execute due satisfied dependency thread kernel cannot execution kernel launch therefore handle dependency stall thread finer grain task paradigm warrant task thread finer grain approach  execution execute task task graph thread task graph define programmer variety task program model defines task dependency gpu task runtimes resolve dependency task sends task queue persistent kernel persistent kernel approach kernel launch overhead avoid grain dynamic dependency resolution along persistent kernel reduce amount  task execution however runtime overhead task management significant programmer effort algorithm task program model specifically programmer domain specific knowledge algorithm decompose express task graph thread task goal enable benefit task execution model minimal programmer intervention core task program model ability define task graph execution towards propose thread task paradigm instead programmer define task mapped execute thread extract derive task task graph exist thread SIMT program model grid thread essentially task explicit input output global memory specify kernel launch parameter leverage multi kernel gpu application task graph series bipartite dependency graph essentially decompose task graph alleviate overhead kernel launch overhead propose pre launch kernel dependency met rely dynamic data dependency resolution hardware enforce dependency effectively remove kernel launch overhead critical mask kernel launch towards goal challenge achieve task paradigm identify extract inter kernel thread data dependency derive task graph reduce kernel launch overhead dynamically resolve task data dependency lightweight manner task partition limitation remap application task execution partition task task partition statically algorithmic workload pipeline stage image render dynamically input data cuda graph capture static task graph kernel execute across operation intensive essence profile workload static graph executes repeatedly static task graph workload input dependent task graph structure input sparse matrix sparse solver typically input dependent task graph information partition task due extract task graph exist application propose framework goal demonstrate ability extract static grain task graph cuda graph programmer transparent task execution ability handle  task graph future  overview BlockMaestro enables programmer transparent thread task task paradigm gpus BlockMaestro consists component extract grain  data dependency exist gpu application kernel pre launch mask kernel launch overhead dynamic inter kernel data dependency resolution ensure correctness pre launch kernel combine technique gpu program exist SIMT program model gain benefit task execution without overhead proprietary task program model illustrates operation BlockMaestro illustrative launch kernel along correspond thread label omit vertical kernel launch overhead execution timeline baseline gpu kernel execution serialize baseline scenario inefficiency exist due kernel launch overhead dependency stall thread gpu underutilization already cannot alleviate issue task runtimes programmer express task execution dynamically task specify dependency allows execute whenever dependency satisfied baseline execution model BlockMaestro inter kernel thread data dependency identify extract task kernel pre launch mask kernel launch overhead dependent thread enforce hardware  dynamic inter kernel thread data dependency resolution enables overlap execution kernel achieve benefit task runtimes baseline execution model suffers kernel launch overhead dependency stall resource utilization BlockMaestro insight kernel launch hiding inter kernel data dependency resolution enable benefit task runtimes without programmer burden kernel launch overhead displayed vertical ptx code analysis cuda code inter kernel TB dependency graph gpu gpu memory ptx TB scheduler SMs pre launch kernel command queue   overview BlockMaestro avoid kernel launch overhead persistent kernel task runtimes execute immediately achieve goal BlockMaestro introduces kernel pre launch inter kernel data dependency resolution eliminate kernel launch overhead enable overlap execution thread dependent kernel respectively illustrates kernel pre launch hide overhead kernel launch kernel launch BlockMaestro pre launch kernel enforce correctness resolve data dependency thread scheduler conservatively execution kernel launch overhead eliminate dependency stall underutilization resource exist fully achieve benefit task execution identify thread data dependency exist dependent kernel annotate arrow kernel launch compilation occurs ptx SASS illustrates inter kernel data dependency resolution utilizes thread  data dependency information dependent kernel data dependency enforce thread scheduler dynamically resolve enables thread execution regardless kernel overview BlockMaestro data dependency inter kernel thread acquire analysis kernel launch ptx code compile SASS assembly dependency graph hardware dependency dynamically resolve BlockMaestro eliminates kernel launch overhead enable application  dependent kernel without synchronization data dependency thread dependent kernel met TB eligible issue execution remainder discus BlockMaestro enable kernel pre launch identify enforce inter kernel data dependency identify inter kernel dependency task runtimes task dependency specify directly programmer dependency conveyed task program model cuda graph amd  programmer define graph operation BlockMaestro programmer transparent thread task paradigm identify inter kernel thread data dependency identify kernel kernel dependency data dependency kernel data reside global memory due SIMT program model input output kernel define global memory kernel allocate api cudaMalloc cuda  hip load address identify static analysis kernel ptx SASS code compilation phase kernel launch kernel allocate global memory pointer memory allocation kernel launch api memory apis pointer similarly writes cudaMemcpy  device operation device host operation therefore data dependency kernel api identify within command queue fairly straightforward manner handle arbitrary inter kernel dependency BlockMaestro linear non linear issue kernel launch command serialize command queue application kernel launch multiple kernel kernel linear dependency non linear dependency without kernel completion kernel completion dependency inter kernel dependency dependency correctness enforce kernel completion significantly reduce amount dependency solid due implicit dependency dash execute concurrently shorter therefore ensure correctness dependency information dependent kernel dependency arbitrary kernel clearly scalable arbitrary  dependency simplify amount dependency enforce kernel completion incorrect instead marked dependency kernel prior implicit dash guaranteed satisfied greatly reduces amount dependency solid limit dependency consecutive kernel addition hypothetically assume completes kernel completion implicitly enforce dependency completes execution explicit dependency implicitly satisfied kernel concurrently execute essentially BlockMaestro allows execution kernel enforce completion kernel potential kernel overlap opportunity gain benefit  inter kernel dependency series bipartite graph kernel identify thread dependency kernel data dependency enable kernel  realize potential task runtimes achieve task runtime benefit BlockMaestro avoid dependency stall thread enforce thread data dependency enforce inter kernel data dependency granularity thread overlap execution thread dependent kernel BlockMaestro performs compiler static analysis kernel launch identify raw dependency global memory raw dependency enforce runtime thread scheduler identify raw dependency thread granularity identify array index thread cuda program model programmer already specify mapping thread data calculate index derive index variable threadIdx blockIdx blockDim etc vector kernel thread index array int threadIdx blockDim blockIdx thread array sum array kernel function allocate cudaMalloc application data graph identify load program identify respectively identify thread identify index access load extract index representation function parameter kernel launch threadIdx blockDim blockIdx variable kernel launch along therefore perform analysis identify array index TB access per TB analysis implement perform analysis index load instruction per thread built ptx parser gpgpu sim algorithm algorithm perform compiler framework ptx  algorithm  code ptx static analysis global load instruction kernel instruction src empty previous instruction dst global load non static dependency remove dst src local register immediate src irst instruction analysis  address load LK SK intersect LK SK TB raw dependency perform backward pas cfg representation kernel identify global load origin source operand encounter source operand originates another load indirect memory access terminate conservatively assume entire kernel dependent previous kernel otherwise load source operand derive kernel launch variable kernel launch parameter grid perform analysis identify thread kernel identify dependency intersect kernel identify inter kernel thread data dependency kernel command queue analyze obtain kernel launch analysis overhead perform critical masked propose kernel pre launch technique later kernel earlier kernel intersection raw data dependency exist dependency graph data dependency node thread dependency node disjoint independent belonging kernel dependency graph bipartite graph  jit analysis compile analysis kernel launch compilation ptx SASS parameter kernel launch grid dependent input data similarly blockDim thread thread blockIdx grid kernel launch variable unknown compile cuda ptx therefore analysis cannot perform identify thread data dependency furthermore conduct kernel launch kernel api command queue allows dynamically bipartite dependency graph task graph decompose manner limitation consideration focus static memory analysis analysis memory location runtime device variable address immediate kernel parameter however cannot global access derive another memory pointer chase etc instance runtime runtime analysis scope application unified memory identify analysis unified memory allocate  global memory address monitor raw dependency within cuda kernel memory access occurs manner non unified memory cudaMalloc cudaMemcpy  cudaMalloc cudaMemcpy  cudaMemcpy DH api cudaMalloc cudaMemcpy cudaMalloc cudaMemcpy cudaMemcpy cudaMalloc cudaMemcpy cudaMalloc cudaMemcpy cudaMemcpy default prevent kernel launch hiding due synchronous api arrow depict api dependency cudaMalloc cudaMemcpy cudaMalloc cudaMemcpy cudaMemcpy cudaMalloc cudaMemcpy cudaMalloc cudaMemcpy cudaMemcpy api command enable kernel pre launch api command queue kernel launch hiding enable kernel pre launch discus enable kernel pre launch overlap achieve future kernel launch completion previous kernel accomplish enable multiple kernel simultaneously execute command queue prevent cuda api command queue issue future cuda apis command queue highlight challenge enable kernel launch hiding refer trace cuda api host executes code cuda api sends along data command queue default cuda cuda api allocate memory transfer data gpu global memory host memory launch kernel device synchronize kernel asynchronous non default however cuda memory api synchronous host function return limited opportunity kernel launch hiding handle apis maximize opportunity kernel pre launch multiple kernel command command queue however scenario prevent due behavior api memory operation cudaMalloc cudaMemcpy host kernel execute cudaMalloc parallel cuda command hardware execute parallel host completes host cudaMalloc return issue cudaMemcpy command queue prevent opportunity overlap host completes therefore multiple command queue interchangeably cuda equivalent OpenCL command queue amd hip kernel command command queue maximize kernel pre launch opportunity overcome issue treat operation non BlockMaestro resolve dependency hardware shift burden implicit synchronization hardware api implicit synchronization enforce raw hazard host cudaMemcpy device host explicit synchronization api  bypass incurs raw hazard host data modify host gpu enforce correctness implicit synchronization gpu asynchronous memory apis programmer program cuda target application already utilizes cuda command queue already issue BlockMaestro seamlessly pre launch cuda  application overlap kernel launch within consideration handle  manner  api command within BlockMaestro generalize cuda remainder focus default application kernel launch overhead utilization issue programmer transparent api command reorder command queue cuda apis command queue implicitly ordering limit amount kernel launch hiding launch execute cannot proactively pre launch cudaMalloc cudaMemcpy command potential maximize kernel launch hiding identify data dependency apis command queue reorder command maximize kernel launch hiding achieve kernel launch satisfies data dependency api kernel launch memory allocate otherwise resource becomes available enable multiple kernel execute command queue baseline kernel command command queue kernel command queue therefore modification command queue multiple kernel command feature already available nvidia hyper enables multiple kernel command modification enable execution kernel per command queue sufficient completely overlap kernel launch enforce correctness resolve data dependency kernel command queue rely thread scheduler enforce dependent kernel execute detect kernel execution kernel launch overhead overlap kernel execution kernel independent data dependency exist independent kernel execute away otherwise data dependency enforce hardware later discus detail thread scheduler enforce  data dependency thread kernel execute fashion however BlockMaestro enforces completion TBs TBs kernel enforce inter kernel dependency inter kernel dependency enforce dependency bipartite dependency graph BlockMaestro dependent kernel dependency information hardware enforce inter kernel dependency dependency buffer counter buffer illustrate BlockMaestro structure enforce dependency detail architectural recall dependency consecutive launch kernel illustrate kernel discus generalize multiple pre launch kernel assume gpu execute TBs dependency bipartite dependency graph indexed thread ID kernel contains dependent kernel TB  TB TB pending dependency outstanding kernel TB dependent thread kernel initial launch pre launch kernel dependency device schedule TBs TB remain TB TB dependency TB TB respective counter decrement TB execute available resource SM TBs TBs schedule TB marked designate kernel shift attention pre launch kernel dependency specifies dependency pending dependency TBs execute scheme execute architectural depicts propose hardware TB scheduler device receives kernel host dependency initial counter TB TB TB TB TB TB TB TB TB TB TB active TB dependency ID ID ID initial execute gpu TB TB TB TB TB TB TB TB TB TB TB ID ID ID active TB dependency TBs schedule active TB TB TB TB TB TB TB TB TB TB TB TB ID ID ID dependency TBs resolve dependency respective TBs active TB TB TB TB TB TB TB TB TB TB TB TB ID ID ID dependency replace kernel TBs invalidate active TB TB TB TB TB TB TB TB TB TB TB TB TB ID ID ID dependency launch dependency memory TB decremented TB scheduler active TB TB TB TB TB TB TB TB TB TB TB TB TB ID ID ID dependency TBs resolve dependency TBs TBs execution TB schedule BlockMaestro inter kernel thread dependency maintain dependency counter global memory TB scheduler dependency buffer counter buffer issue TB schedule policy resolution dependency encode policy ID ID TB IDs dependency dependency SMs SM SM SM SM SMs SM SM SM SM TB scheduler architecture global memory pre launch kernel gpu dependency address counter address global memory minimize amount global memory access dependency buffer counter buffer thread scheduler dependency buffer dependency actively execute thread counter buffer thread pending unresolved dependency thread schedule execution thread entry dependency buffer dependency buffer entry identify thread entry already exist counter buffer allocate entry fetch thread counter information dependency counter entry thread execution buffering critical TB completes identify TB ID dependency buffer index counter buffer decrement correspond TB execution deallocate entry dependency buffer TB completes  entry counter buffer TB execution resolve dependency multiple kernel easily scalable resolve dependency multiple pre launch kernel execution simply append thread ID relative kernel IDs resolve dependency kernel append thread ID kernel identifier kernel identifier incremented whenever kernel launch wrap around saturate dependency kernel kernel identifier essentially significant kernel ID schedule policy BlockMaestro schedule policy across kernel TBs default BlockMaestro priority TBs kernel TBs consume kernel schedule kernel TBs schedule priority TBs consume kernel enable opportunity concurrently execute dependent kernel improve utilization essentially TBs ahead TB TB TB TB TB TB TB TB fully TB TB TB TB TB TB TB TB TB TB fully TB TB TB TB TB TB TB TB TB TB TB TB TB TB TB TB TB TB TB TB TB TB TB TB TB TB TB overlap dependency TBs adjacent kernel policy deadlock issue producer kernel synchronization kernel  TBs barrier TBs cannot schedule consume kernel resource starve producer kernel scenario fully starve producer kernel deadlock scenario eventually consumer kernel TBs unmet dependency producer kernel schedule avoid permanent deadlock inter kernel dependency utilize buffer TB scheduler dependency kernel reduce amount storage advantage dependency kernel encode rarely arbitrary code usually globally load data thread thread load therefore analyze graph device encode fashion greatly reduce memory usage hardware overhead dependency thread BLOCKS overhead fully MN without encode fully overlap  independent display additional memory overhead BlockMaestro utilize graph TBs TBs hardware random dependency graph overhead drastically reduce detect specific usually kernel encode reduce requirement fully signal gpu simply prevent consume kernel kernel TBs TBs encode grouped memory TBs refer location TB TB TBs mapped TB hence TB exclusive TBs TB overlap TBs multiple TBs therefore overhead plus maximum TB addition dependency resolution yield benefit execution speedup dependency device ignore grain dependency resolution treat kernel fully II BENCHMARKS USED  dependency  description kernel MM matrix multiplication alexnet alexnet network bicg bicg sub kernel  linear solver  2D 2D finite domain fft fourier transform gaussian gaussian elimination  gram schmidt decomposition HS hotspot lud LU decomposition mvt matrix vector transpose NW   finder IV evaluation methodology benchmark modify version gpgpu sim baseline titan pascal configuration SMs TBs BlockMaestro generalize SIMT architecture greedy  warp schedule policy kernel launch overhead calculation average baseline launch overhead II evaluate various application rodinia polybench   benchmark suite  application domain normalize speedup con priority con priority con priority prod priority kernel pre launch baseline ideal baseline normalize speedup baseline speedup amount speedup achieve BlockMaestro respect baseline reference ideal baseline kernel launch overhead stack kernel pre launch synchronization apis enforces dependency consumer kernel TBs schedule producer kernel TBs addition producer priority  dependency resolution schedule priority producer kernel TBs consumer priority allows concurrently kernel correspond pre launch kernel respectively consumer priority scheme prioritizes consumer kernel TB schedule increase pre launch kernel increase geometric speedup however diminish return pre launch kernel behavior explain TB data dependency exist kernel workload benefit pre launch kernel significant kernel application data dependency alexnet significant fully dependency lud dependency amenable TBs ahead application gaussian  significant speedup kernel  thread dependency resolution workload tend kernel therefore kernel launch overhead bottleneck alleviate benchmark MM bicg  gain benefit grain dependency resolution producer priority schedule workload tend data dependency easy satisfy capture benefit kernel active sometimes kernel independent parallel therefore TBs execute advance application progress utilization increase utilization display increase normalize average TB concurrency respect baseline workload normalize avg TB concurrency baseline BlockMaestro normalize average TB concurrency baseline compute intensive kernel consist thread alexnet tend suffer overhead kernel launch therefore workload benefit kernel pre launch alone increase TB concurrency due grain TB dependency resolution alexnet achieve speedup kernel along simpler dependency opportunity overlap kernel execution utilize resource device efficiently showcase various data dependency TBs kernel kernel extract ptx code dependency complicate becomes advantage fully scenario functionally synchronization barrier kernel therefore opportunity application via execution overlap kernel launch overhead hiding however simpler opportunity execution TB kernel TBs consume kernel become execution efficient utilization device dependency stall distribution display distribution amount dependency stall TB application execution recall dependency stall thread dependent thread dependency satisfied cannot execute due kernel plot border designate quartile distribution median addition normalize TB execution TB TB amount execute gpu BlockMaestro visibly decrease amount dependency stall TBs application however gpu capacity TB execution remain TBs dependent kernel peer increase stall alexnet kernel bicg mvt parallel BlockMaestro hence dramatic stall reduction workload reflective cuda baseline BlockMaestro normalize dependency stall TB execution dependency stall distribution normalize TB execution  benefit independent kernel concurrently execute exists however cuda concurrently execute non independent kernel demonstrate BlockMaestro gain benefit execute independent concurrent kernel across automatically extract benefit complex dependency overhead inter connectivity analysis demonstrate dependency exist  thread kernel speedup microbenchmark  kernel application dependency kernel default workload TBs kernel workload increase TB dependency artificially introduce dependency kernel fully signifies TBs dependent TBs etc dependency speedup dependency per TB TBs TBs TBs TBs TBs interconnectivity analysis BlockMaestro axis TB dependency benefit dependency resolution quickly deteriorate average dependency threshold deg speedup benefit reflect fullyconnected dependency graph addition speedup threshold decrease TBs kernel cease exist workload TBs kernel resource kernel limit opportunity pre launch kernel ahead leverage insight inter connectivity dependency graph minimize hardware overhead overhead BlockMaestro mainly introduces dependency buffer counter buffer dependency buffer actively TBs entry similarly buffer entry dependency buffer entry TBs per entry aggressively narrower entry workload described dependency encode derive TB IDs rarer scenario cannot encode utilize TBs per entry wider entry exist global memory simply split wider entry across multiple entry dependency buffer index dependency buffer counter buffer TB ID kernel identification TB ID dependency buffer kernel identification compute diminish return interconnectivity counter anything conservatively encode fully without loss speedup storage overhead KB addition logic memory request overhead impact BlockMaestro memory request buffering dependent information memory incur request overhead BlockMaestro average memory request overhead memory request overhead memory request overhead BlockMaestro bipartite dependency graph storage overhead display amount storage entire application normalize respect encode storage average storage reduce bicg mvt exclude kernel independent therefore memory storage without encode  storage bipartite dependency GRAPHS entire application storage storage storage MM alexnet bicg  2D fft gaussian  HS lud mvt NW average comparative comparison task execution model dynamic parallelism showcase comparison cuda dynamic parallelism CDP task kernel execution model wireframe task TBs execution model wireframe programmer specify task dependency proprietary api relies hardware dependency resolution essentially wireframe multi kernel workload mega kernel task mapped TB CDP task device kernel launch avoid overhead host kernel launch comparison prior benchmark application wavefront dependency task kernel overlap dependency predecessor TBs gradually grows dependency graph decline normalize speedup BlockMaestro con priority BlockMaestro prod priority baseline wireframe baseline CDP comparison exist task kernel CDP task TBs wireframe task execution model widely CDP kernel launch latency model kepler estimate CDP kernel launch overhead significantly host kernel launch therefore model CDP kernel launch latency remove kernel launch api overhead host kernel launch normalize speedup comparison normalize CDP BlockMaestro producer priority achieves speedup wireframe achieves geomean speedup due ability task ahead dependency enables task utilize gpu evaluate BlockMaestro consumer priority enable task ahead speedup wireframe reliance constrain hardware task management buffer pending update buffer actually limit amount task BlockMaestro dependency resolution update task global memory execution constrain increase gpu utilization slightly memory traffic successfully demonstrates benefit BlockMaestro achieve benefit task execution model without programmer intervention related task dependency cuda dynamic parallelism enables device kernel launch dynamic kernel launch amortizes kernel launch overhead allows task dynamically spawn gpu however significant drawback limited recursion cuda cuda graph allows user define dependency graph kernel perform optimization graph instantiation execute cuda graph reduce kernel launch overhead however address gpu utilization execution dependent kernel amd expression dependency gpu task author asynchronous task paradigm express application acyclic graph dag heterogeneous architecture HSA author queue oversubscription parallel task propose mechanism prioritize critical task graph priority TBs producer kernel potentially consumer kernel TBs TB pool adaptive task aggregation ata propose software reduce overhead irregular application specifically sparse solver grain task schedule task assign compute task hence avoid launch overhead task schedule various gpu task schedule author introduce wireframe hybrid data dependent workload handle TB schedule hardware eliminates kernel synchronization however significant programmer burden kernel application  employ software runtime persistent thread PT kernel gpu workload data dependency trading synchronization schedule dag author propose overlap kernel execution modify host code paradigm obtain memory access information compiler generate profiler kernel gpu reference TB scheduler goal maximize parallelism author seek overcome memory bottleneck gpu application propose reuse aware thread scheduler exploit data reuse kernel producer consumer data dependency majority dependency TBs ID kernel dependency steal minimize load imbalance paver hybrid TB schedule data locality kernel TBs schedule heuristic reduce cache thrash perform task steal reduce load imbalance towards kernel target load imbalance directly however enable manage schedule TBs additional kernel SMs TBs reduce utilization VI conclusion propose BlockMaestro  hide kernel launch overhead manage execution thread grain manner data dependency hardware enforce correctness increase gpu utilization gpu kernel execution incur memory overhead paradigm average speedup various application