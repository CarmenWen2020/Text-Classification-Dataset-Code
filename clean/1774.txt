timing channel extract cryptographic sensitive document trust enclave specifically cache channel reuse code data memory hierarchy exploit attack evict reload recover rsa spectre variant leak speculatively load data TimeCache cache incorporates knowledge prior cache access eliminate cache channel due reuse software code data goal retain benefit cache access entire cache cache occupancy software achieve goal implement per cache visibility benefit cached data another incur correspond penalty achieves overhead novel combination timestamps hardware efficient parallel comparison timestamps cache without limit security domain defends attacker core another hyperthread another core implementation gem simulator demonstrates defend rsa extraction evaluate performance spec parsec overhead TimeCache average delay due access majority overhead security context bookkeeping incur context switch contribute introduction memory resource expose timing channel reveal information presence security isolation enclave separation cache channel leverage memory capable extract cryptographic sensitive document data cryptographically secure enclave cache channel attack defense developed literature focus cache channel reuse software code data memory hierarchy software essential component instance library code important optimization compute memory footprint likewise service access data data across untrusted client request access code data leaf footprint memory hierarchy exploit attack typical cache channel attack software involves evict data code library cache hierarchy access victim execution evict reload flush reload access indicative access location victim library access indexed secret data attacker infer victim secret attack model attack leak cryptographic spectre spectre II  tenant attack leak data  service discover stroke another cache channel attack memory focus attack refer contention conflict channel attack contention attack mitigate randomize cache CEASER CEASER  efficient multiple hash technique  however technique unable prevent reuse attack memory TimeCache conjunction technique holistic defense reuse attack memory precise handy construct sophisticated attack noisy prefer covert channel leak speculatively load data prevent reuse attack memory attacker provider deploy deduplication  fork operation docker style container increase performance reduce utilization deduplication evaluation literature reduce memory factor increase performance exist reuse attack partition cache implement constant algorithm increase latency partition associate overhead due reduction effective cache individual due potential aliasing memory cache UI OOVBM  PNQVUFS SDIJUFDUVSF acm annual international symposium computer architecture isca doi isca partition technique restriction security domain instance DAWG security domain access llc evaluate overhead hardware software defend reuse attack goal retain benefit cache access entire cache cache occupancy software cache without limit security domain TimeCache creates per cache occupancy delay treat access resident cache cache delay access timing isolation data impression data cache access approach fundamental premise reuse attack software reduction performance due delay elemental secure cache avoid potential consumption data partition cache delay incur data evict reload performance steady cache unaffected consequence defense deploy memory deduplication technique reduce memory footprint without avenue cache channel software stack implement defense novel combination timestamps hardware efficient parallel comparison timestamps couple per cache access software context switch software cache context along context switch timestamp context switch hardware implement serial comparison logic allows parallel timestamp comparison context restore update stale cache context restore security performance TimeCache gem simulator demonstrates effectiveness attack microbenchmarks rsa attack defense prevent classic rsa attack demonstrate flush reload attack performance evaluation spec parsec average overhead due delayed access software overhead due security context bookkeeping context switch contribution prevent reuse attack software access entire cache maintain cached software disallow access cache cache cache cache propose timestamp  cache occupancy across context switch prevent reuse attack develop serial timestamp parallel comparison logic timestamps cache simultaneously simulation evaluation demonstrate propose prevents attack analyze potential overhead timing isolation II background cache channel information leak cache utilization collectively refer cache channel mechanism exploit cache channel expose attack rely cache access timing developed information leak software attacker victim software attacker victim attacker cache access victim prime probe style attack commonly refer contention attack presence software attacker access victim evict reload flush reload style attack depicts attack defense prime probe attack cache randomize placement effective evict reload flush reload style attack latter bandwidth efficient attack address style attack reuse contention attack            PHPRU     software attack library commonly subroutine mapped directly user address physical memory mapped virtual address library reduce memory footprint improve memory hierarchy efficiency however potential leak memory access cache channel due access code data channel exist due hardware software improve precision presence software channel software earlier assume affect cryptographic routine advent recent attack handy gadget sophisticated attack spectre capable leak keystroke another leak password environment amazon EC server leak data across virtual machine prior defense around cache channel strict performance requirement cache structure remain challenge defense mitigate contention attack randomize cache  defense reuse attack resort cache partition implement constant algorithm partition defends reuse contention attack reduces effective cache available execution effectively reduce performance constant algorithm likewise incur significant performance penalty defense restrict security domain others cache  directory presence cache detect access untrusted spatially isolated physical core detect presence channel hardware description information hardware specification detect existence channel specify security label processor building channel resistant processor likewise  relational model detect presence channel processor specification threat model threat model consideration attacker victim software stack addition cache simultaneously  core interleave attack conduct cache reuse attack access software stack cache attacker victim attack sequence attacker victim software hardware cache access software dependent indexed victim secret data reuse attack cache  cache  lib database evict data access data secret data data data secret attacker evicts location cache hierarchy victim execution attacker subsequently reloads location determines location access victim cache timing access attack model sufficient demonstrate capable leak rsa GnuPG library  bandwidth building sophisticated attack spectre II   prefer covert channel recent attack leak library access contention covert channel purpose precise hence attack IV per  context TimeCache eliminates reuse cache timing channel implement technique per cache visibility cache unlike partition approach TimeCache isolation TimeCache ensures access cache isolated timing allows access cache without reveal another cache available cache another rely cache partition intel cache allocation technology approach restrict usable cache applicable cache hierarchy llc access access refers access resident cache cache another resident cache access access minus initial cache cache evict later cache access cache later access importance access construct attack attacker access evict data cache hierarchy detect cache attacker infer victim memory access beyond access access cache clue data access another observation TimeCache enables avoids partition enforce access timing isolation baseline cache delay access due capacity conflict coherence due action due critically cache due data another access latter timing channel TimeCache target TimeCache access distinguish access TimeCache observation attack consideration exploit cache benefit due another hence propose identify access resident cache treat access treat access essentially incur delay attacker unable infer another memory access via cache residency cache already access currently execute context per  security cache cache load context hardware context reset cache access context checked  access proceed otherwise access recognize access treat context future access proceed access handle request memory hierarchy cache data data cache recent mechanism implement cache memory hierarchy handle context switch cache context footprint hardware execution context specific execute context context switch carefully manage ensure neither stale information cache context TimeCache ensures timing isolation context switch combination software hardware retain cache behavior across context switch timing isolation software preempt along context switch timestamp software restores schedule execute cache correspond hardware context resume execution hardware ensures stale restore reflect cache execute update reflect cache content cache reset detect maintain cache restore perform trust compute library context switch operating restore specific ability restore update cache context allows TimeCache enjoy access data evict cache leverage locality across context switch timing isolation something cannot achieve simply flush cache context switch mechanism described processor feature trust reuse cache attack concern  implementation overview hardware modification depict cache consist cache access hardware context hardware conventional cache per cache per hardware context security  per cache timestamp become resident serial timestamp parallel comparison logic transpose gate bitline peripheral timestamps efficiently shift register timestamp resume execution due context switch execute subsection implementation detail hardware modification software defense access delay mechanism traditional cache access request data return processor tag lookup succeed otherwise access incurs request memory hierarchy cache cache checked addition tag access addition cache data return processor cache flowchart action TimeCache maintain timing isolation TimeCache hardware depict cache access core hardware context maintain timing isolation per flowchart creation execution preemption resumption context switch software restores  execute hardware context memory context switch additionally software maintains recently preempt newly reset schedule per cache modify action restore memory hardware context schedule resume reset timestamp comparison logic hardware context schedule resume reset cache evict invalidate request hardware context cache reset hardware context request hardware context access resident cache reset cache indicates access cache reset response processor delayed request memory hierarchy response data discard data cache processor ensure future access cache additional traffic treat without additional delay rationale request memory hierarchy data available cache access response latency equivalent variable access latency incur context reset closer processor cache cache due capacity request memory hierarchy ensures request data available cache request service cache response latency data response however discard cache recent data cache evict invalidate reset cache hardware context load hardware context cache remain reset context switch hardware cache load register software context resume reset hardware context resume enforce delayed access restore context switch memory access thereafter proceed additional lookup parallel cache tag lookup access unlike conventional cache response processor data cached per update restore context switch preserve cache benefit across context switch instead reset context switch equivalent flush cache context switch impact performance heavily byte cache memory access restore dependent cache KB cache byte memory access MB cache byte memory access restore cannot stale update cache cache evict preempt correspond memory date update cache evict invalidate reload preempt timestamp indicates date cache load access resume execution restore cache parallel cache reset timestamp comparison trigger context switch prior resume subsequent access comparison information timestamps serially cache context switch consume significant cycle discus comparison mechanism update array constant transpose SRAM array timestamps proportional subsection serial timestamp parallel comparison timestamps regular data access cache parallel cache along tag timestamp hardware context specific access access cache SRAM array parallel fashion implies perform timestamp comparison proportional cache perform parallel comparison cache timestamps per cache timestamps along cache SRAM array transpose fashion propose neural cache computation perform serial parallel timestamp parallel manner transpose interface transpose memory amp driver access data regular transpose mode access SRAM access parallel cache data array timestamp array comparison logic construct multi access SRAM transpose interface regular operation cache timestamps update context reset regular peripheral interface restores parallel timestamp comparison reset specific load SRAM array correspond hardware context update information cache evict preempt restore transpose timestamps serial  comparison linear timestamp impact timestamp rollover VI logic timestamp comparison reset peripheral serial comparison logic serial computation allows simplify comparison logic unsigned integer sequentially msb significant declare differs encounter binary representation codify algorithm scheme iterate msb consideration marked comparison behavior checked perform xor consideration consideration instance msb load shift register iteration timestamp timestamps SRAM array regular  peripheral interface shift register shift comparison logic update comparison latch output ignore comparison cache newer peripheral latch latch output reset peripheral circuit attach SRAM bitline SR latch reset prior initiate timestamp comparison input gate input gate comparison operation fed comparison gate ignore comparison latch latch fed gate iteration latch latch bitline driver latch wordline correspond hardware context enable VI evaluation implement TimeCache gem cycle accurate simulator LI L1D cache KB llc cache MB timestamp per hardware context cache manipulate described IV context request packet cache CR register within simulator CR register trigger timestamp comparison restores specifies simulation parameter evaluation evaluation setup processor core L1D LI llc cache gem simulator core  2GHz L1D LI llc cache subsection analysis evaluation security performance overhead timestamp defense gem simulator security analysis attack depends reload due another attack broken cache due another access exist cache cache attacker remains oblivious data cached beforehand cannot data access another access significance attacker unaltered access beyond access sufficient ensure security significantly compromise performance reuse attack TimeCache prevent attacker evicts location attacker victim execute data cached attacker access data cache due victim cache additional information tracked defense timestamps restore trust software unprivileged access microbenchmark functionality evaluation confirm operation timestamp approach microbenchmark attack consist access memorymapped array cache attacker flush array yield processor victim execution writes repeatedly array wake performs entire array successful attack attacker defense simulation enable gem flush  mem  mem cache  mem attack rsa flush reload technique attack GnuPG version rsa described attack hardware gem simulator linux attacker independent program machine hence cache machine install non strip GnuPG library offset reduce function library encryption algorithm exponentiation performs sequence  reduce processing sequence reduce processing rsa encryption library indexed secret information secret attack attacker flush cache access memory location reduce function loop couple access cache extract information evaluation simplify attack assume cache attacker successful attack calculate cached uncached access experimental machine threshold cache attacker program independent program loop flush memory reading timestamps fence respect memory access avoid speculative load attack independent attacker program access simultaneously victim performs encryption launch attack machine gem simulation mode defense gem disallows cache attacker attacker access precede flush defense allows cache suffer cache access access flush cached data access delayed attacker perceive attack demonstration flush reload attack defense successfully attack introduce additional channel additional introduce additional channel execute hardware context associate hardware context restore context switch access specific data structure software access context switch accessible trust compute restores context switch constant operation therefore leak information performance evaluation access delay evaluate performance overhead access delay mechanism simulate benchmark spec billion instruction gem simulation mode instance spec benchmark core without TimeCache normalize execution execution TimeCache execution without TimeCache benchmark instance benchmark access impact benchmark specific code library cache context switch across instance instance memory benchmark specific code libc routine file operation    addition kernel memory across access kernel subroutine kernel data structure incur access execute privileged mode within context combination benchmark core access limited library kernel memory geometric overhead across workload access per instruction cache access cache retains content access MPKI wrf perlbench due instruction memory footprint observation perlbench wrf access MPKI cache benchmark however effective access cache contention similarly lbm leslied effective access due capacity eviction cache namd gobmk evaluate overhead due access delay cache pthread parsec benchmark thread core emulation mode simulator clone syscall emulate allocate thread another core geometric overhead due TimeCache thread execute core LI L1D core access parsec thread incurs access access across execution context llc code data overhead per instruction MPKI cache II increase execution proportional increase MPKI due additional access due cache behavior incur access increase MPKI explains overhead II spec parsec execution overhead MB llc MPKI workload overhead MPKI llc baseline MPKI llc TimeCache                 gobmk namd lbm milc zeusmp lbm wrf sjeng perl wrf cactus  gobmk astar zeusmp gromacs average fluidanimate raytrace blackscholes swaptions facesim average llc sensitivity analysis analyze sensitivity cache evaluate performance overhead llc benchmark core cache eviction rate workload effectively access additional delay hence performance overhead cache analysis MB MB MB llc average performance overhead respectively increase cache baseline MPKI reduces cache retain memory access context switch defense cache core spec performance normalize execution due TimeCache delayed access execution TimeCache execution without average overhead instance benchmark core delayed access MPKI cache   OEP    zui                       overhead timestamp rollover increase due additional hardware primarily due SRAM array timestamps comparison logic SRAM array additional amp driver component timestamp comparison logic peripheral consist latch gate shift register evaluation timestamps overhead timestamp counter impact frequency timestamp rollover parameter tune chip maker timestamp rollover additional cycle illustrate correctness operation digit precision rollover cycle purpose illustration preempt resume rollover rollover detect resumption newer  cache reset rollover detect resume rollover action date assume rollover resumption resume cache unnecessary reset correctness operation maintain cache timestamp rollover additional retains correctness defense per hardware context cache cache significant llc server processor coherence directory scalability concern core availability information core principle coherence directory apply limited pointer indirection limited pointer directory demonstrate empirically application typically data across processor pointer hardware context limited sharer reduce overhead oppose per cache core thread parsec benchmark normalize execution execution TimeCache execution without per cache MPKI normalize execution average performance overhead due delayed access delayed access MPKI across core  UD        restore overhead resume preemption restore overhead due copying cache entire array cache KB byte cache memory access overhead cache cache transfer  cache MB parallel via regular interface restore context switch restore kernel memory reserve context intel processor operating  MB cache without cache comparable magnitude null context switch typical slice varies overhead extra layer buffering hardware perform parallel execution restore dma transfer calculate latency transfer buffer equivalent simulation restore cache context xeon processor dma channel delay context switch simulation account overhead due vii ATTACKS SHARED software lru attack lru attack exploit cache replacement policy eviction creation software launch software attack proceeds eviction access cache cache eviction lapse victim execution attacker access eviction access replaces access attacker victim access attack contention attack eviction prevent randomize cache coherence attack attack memory due coherence protocol identify literature variant invalidate transfer attacker flush cache cache flush private cache remote cache access latency subsequent load victim another processor access memory variant exploit difference access response exclusive cached prevent attack TimeCache dram response data available remote cache cache access context flush flush attack execution clflush instruction depends data available cache clflush instruction abort data cached memory access victim data cached reuse attacker infer clflush longer attack prevent clflush constant instruction perform dummy data cached evict evict another contention attack lru attack software launch software attacker evicts cache eviction victim finer grain version evict memory attacker flush clflush cache victim execution attack remains noisy practical unless attacker communicates victim trigger specific access related exist cache channel attack exploit code data resort cache partition remove timing information access approach incur significant overhead cache partition partition cache prevent multiple attack performance due reduce cache availability statically partition cache significant performance deterioration cache become unavailable whereas dynamic cache partition achieve overhead reallocate  dynamic partition technique broadly categorizes application confidential public prevents information leakage confidential application public application allows information direction although dynamic cache partition technique performs static counterpart coarse grain security classification another dynamic cache partition technique utilizes allocate secure domain incur significant recoloring DAWG partition cache maximum security domain associate performance overhead due reduce cache availability  defends reuse attack lock sensitivity analysis overhead relative increase cache VL RUPDOL  PE PE PE cache IDs prevent eviction wise cache partition lock available eviction performance degradation cache defense intel partition catalyst  demonstrate intel cache allocation technology achieve cache partition mitigate cache channel performance ability reassign cache application cache flush minimum  service  per application flush across context switch catalyst pin service provider defense mechanism prevent VM attack attack target llc  cache manually tag pin remain secure   prevents reuse data llc another core directory presence defense assumes victim attacker cache manage directory protocol otherwise isolated hardware usage scenario hardware isolation attacker victim identify attacker threat model hence defense mechanism TimeCache  assumption resource isolation attacker victim beyond address attack cache llc identify hardware context potentially  core perform access code data recognize straightforward presence directory absence hyperthreading context switch across protection boundary across context switch cache enable novel serial timestamp parallel comparator recognize access across context switch defense mitigate spectre variant eliminate core reuse cache channel spectre remove constant algorithm ability data access precisely channel exploit ability away untrusted application sufficient prevent attack technique obtain timestamps microsecond granularity alternate timing primitive recovery resolution obfuscate reduce resolution approach mitigate channel memory program transformation constant implementation program transformation impractical overhead due critical access useful library IX discussion software important component compute efficiency consistency eliminates channel leak secret data via monitoring victim access content cache absence content cache victim access behavior monitor information channel accurate prime probe attack prime entire cache infers cache access victim attacker probe propose defense prime probe attack randomize cache defense attack content accurate noisy channel information TimeCache conjunction defense defense approach defend recent attack spectre stall execution speculative instruction invisible succeed load request prevent non speculative cache channel speculative channel attack rely conventional channel leak speculatively load data attacker data eventually leak via conventional channel conventional cache attack prevent speculative channel leak conclusion evaluate timestamp defense timing channel attack rely reuse software cache secret information TimeCache across context switch prevents attack  core smt context cache without cache partition perform timestamp comparison parallel SRAM array allows serial timestamp parallel comparison easy transpose access evaluate defense microbenchmark attack program classic flush reload attack gem simulator spec parsec performance overhead average due delay access copying  context switch defense timing channel software retains benefit utilize entire cache capacity cache allows cache memory pressure reduction data deduplication 