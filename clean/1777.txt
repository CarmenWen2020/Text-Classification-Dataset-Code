 explosion hardware device conventional processor driven research explore generation approach dna architecture vector thread private memory coherence message passing dataflow von neumann execution hybridize architecture expose unique hardware api performance efficiency critically dependent program apis approach implement custom library hardware architecture application domain scalable approach utilize portable compiler infrastructure tailor application domain easy generate efficient code diverse architecture minimal effort propose unified graphit compiler framework UGC exactly graph application UGC achieves portability reasonable effort decouple  algorithm architecture specific schedule backends introduce domain specific intermediate representation GraphIR decouple GraphIR encodes algorithm optimization information hardware specific code generation easy develop backends GraphVMs diverse architecture CPUs gpus generation hardware swarm HammerBlade manycore schedule extension easy expose optimization decision load balance strategy locality data structure choice evaluate UGC algorithm input graph distinct architecture UGC enables implement optimization speedup programmer generate straightforward implementation index compiler novel architecture domain specific graph intermediate representation introduction  moore architectural diversity rapidly explode generic parallel substrate  dataflow highly domain specific machine graph accelerator research commercially deployed architecture programmable combine understood technique vectorization thread explicit data movement novel oftentimes difference  performance dramatic speedup unified graphit compiler framework UGC GraphIR decouples hardware independent compiler hardware dependent GraphVMs grey denote compiler denote code input intermediate library generate hinge correctly combination feature architecture daunt task hardware architecture application domain target purpose advocate novel compiler software stack explosion architectural diversity pursue domain specific approach focus graph analytics enable compiler capture programmer intent optimize implementation compiler toolchain unified graphit compiler framework UGC target diverse architecture easy compose optimization architecture unique feature recent developed domain specific toolchains image processing target CPUs gpus accelerator potential approach graph due irregularity unique challenge hardware software graph processing crucial application domain benefit hardware acceleration graph algorithm application notoriously optimize graph program exhibit irregular memory access UI OOVBM  PNQVUFS SDIJUFDUVSF acm annual international symposium computer architecture isca doi isca saturate memory bandwidth suffer utilization hardware optimize regular memory access diversity graph application input graph combine unique feature architecture program performance portable graph application processing graph CPUs exploit cache hierarchy execution gpus magnitude compute memory bandwidth structure code exploit data parallelism orient memory access task architecture swarm exploit speculation inter task synchronization critical finally manycore architecture HammerBlade purpose processor tile critical efficiently software manage scratchpad memory abstraction intermediate representation critical simplify code generation diverse architecture expose optimization opportunity achieve goal UGC introduces domainspecific intermediate representation graph intermediate representation GraphIR encode hardware independent optimization interface hardware backends UGC built graphit domain specific DSL decouples algorithm performance optimization schedule graph program UGC schedule combine load balance traversal direction active vertex creation active vertex kernel fusion explicit data movement finegrained task splitting optimization depicts overall compilation various analysis lower generate GraphIR GraphIR lower code architecture  graph virtual machine GraphVM performs hardware specific transformation code generation contribution compiler framework novel carefully intermediate representation GraphIR hardware independent hardware specific GraphVMs generate code diverse architecture novel extensible schedule allows programmer explore optimization hardware platform implementation GraphVMs generate efficient code CPUs gpus swarm HammerBlade manycore evaluation code generate GraphVMs speedup user baseline code insight technique building portable compiler target architecture specific application domain II background performance graph processing depends optimization locality within sparse data structure minimize memory access synchronization balance load across parallel thread unfortunately structure graph varies widely across iteration algorithm optimal approach across graph across iteration graph program notoriously optimize architecture architecture employ hardware feature exploit parallelism achieve throughput thread vector warp task task instruction speculation memory consistency model cache coherence variant atomic operation data movement scratchpad combination feature exponentially architectural variation performance characteristic architecture compiler runtime expose feature implementation cognizant feature implication domain specific  graph processing abstract complexity architecture dynamic challenge graph structure  abstract hardware domain machine image processing networking tensor algebra bioinformatics sometimes combine domain ideal DSL graph processing facilitate algorithm expression abstract away architectural detail performance application architecture graphit domain specific DSL graphit domain specific graph application decouples algorithm specification computation schedule enables graphit generate highperformance code optimization tailor diverse graph input portable algorithm specification func  vertex output bool output func  src vertex dst vertex dst src func frontier  var output vertexset vertex frontier    delete frontier algorithm specification breadth bfs graphit concretely benefit graphit approach algorithm specification  bfs graphit code describes computation perform define function filter vertex update hardware feature cpu swarm gpu HammerBlade parallel execution model thread task SIMT simd SPMD processor thread speculation instruction instruction task memory latency hiding OoO execution smt OoO execution smt multithreading non memory ops synchronization atomics coherence enforce atomics barrier atomics addressable memory GB chip memory coherent coherent globally partition llc core local data memory coherent coherent coherence scratchpad SW coherence chip storage per thread MB MB KB summary parallel architecture architecture   operator function specify computation perform algorithm specification specify loop nest iteration specify schedule separation generate implementation suitable algorithm graph input UGC exactly algorithm graphit enable reuse source code various application operator easy UGC target architecture   operator easily mapped architecture specialized traverse parallel extend schedule optimization architecture parallel architecture target parallel architecture architecture built diverse hardware feature expose parallelism latency hiding technique synchronization architecture significantly optimization strategy unique challenge UGC briefly explain architecture challenge compile graph program multicore cpu multicore cpu core optimize thread performance prefetching multi cache hierarchy hide latency core speculative execution simultaneous multithreading cpu software expose explicit parallelism thread multithreaded runtime CPUs perform graph application limited parallelism locality predictable memory access memory capacity CPUs outperform multi terabyte graph gpu gpus massive parallelism SIMT program simd execution model arithmetic vectorized predication handle divergent gpus multithreading hardware thread context hide memory latency gpus suitable graph application massive parallelism exhibit regularity graph structure memory access limited application pagerank sparse graph swarm multicore cpu grain task parallelism task speculation HammerBlade manycore pod core software manage scratchpad independently partition globally addressable memory architectural overview parallel graph processing architecture workload exist linear algebra library exploit massive memory bandwidth coalesce memory access gpus perform poorly application suffer divergence load imbalance sparse memory access finally gpus function graph device global memory swarm swarm augments cpu grain task parallelism swarm achieve magnitude improvement scalability conventional CPUs gpus graph algorithm dedicate hardware task queue speculative execution distribute task across core swarm execution model synchronization primitive swarm program consist task task arbitrary memory spawn task task timestamp spawn timestamp swarm guarantee task atomically timestamp hiding concurrency software hood swarm hardware executes task parallel preserve semantics task execute speculatively coherence protocol extend detect violation upon violation offend task aborted execute swarm task core chip tile distribute task perform asynchronous throughput task dispatch task commit efficiently task instruction task selectively aborted serialize  hint expose unique tradeoff optimization balance task overhead parallelism abort execution execution model priority iterative algorithm task assign timestamp priority iteration swarm speculative execution uncovers parallelism CPUs gpus execute task timestamps parallel swarm programmed compiler challenge appropriately computation task exploit parallelism minimize abort HammerBlade manycore manycore architecture thread parallelism flexibility purpose core target HammerBlade manycore independent core core scalar pipeline latency software manage scratchpad memory integer float atomic instruction core communicate memory mapped mesh network chip core issue non memory request exploit pipeline parallelism hide memory latency addition scalar core network host processor manages execution architectural diagram HammerBlade manycore HammerBlade manycore memory hierarchy software choice efficiently exploit memory parallelism latency capacity memory hierarchy core local scratchpad  scratchpad cache llc highbandwidth memory HBM core local scratchpad remote scratchpad cache network location mapped non intersect core address software explicit data movement scratchpad latency storage explicitly manage software thread core multiple independent HBM channel service pipelined memory request llc cache exclusive memory HBM address consequently HammerBlade manycore expose  memory model coherent construction HammerBlade manycore kernel centric program abstraction cuda kernel code perspective thread execute core multiple core aggregate rectangular execute kernel core communicate explicitly global memory operation remote local scratchpad core execute within synchronize explicit barrier primitive kernel execution schedule manage runtime software tightly couple host processor SPMD execution model manycore architecture graph application parallelism random memory access unlike gpus independent core controlflow divergence independent HBM channel service multiple memory access simultaneously challenge non load hide latency exploit thread memory parallelism balance independent thread execution compiler abstraction intermediate representation critical simplify code generation diverse architecture expose optimization opportunity achieve goal GraphIR novel intermediate representation feature varied architecture identify abstraction expressiveness capture algorithmic detail graph domain instead loop nest GraphIR operator iterate vertex incident vertex operator directly mapped thread hierarchy manycore tile architecture gpus HammerBlade manycore without computation loop nest GraphIR avoids assumption concrete representation data structure allows architecture various implementation vertex available memory bandwidth tradeoff swarm compiler eliminate software queue vertex instead mapping operation vertex hardware task explains GraphIR enables building GraphVMs specialized optimization hardware independent transformation GraphIR dual goal offering flexibility maximum code reuse GraphIR goal specialization optimization unique hardware backend compiler infrastructure analysis transformation target agnostic specifically UGC adapts domain specific transformation graphit DSL compiler dependence analysis insert atomics user define function UDFs liveness analysis frontier memory reuse opportunity transformation UDFs traversal direction parallelism data structure choice hardware independent transformation analysis perform GraphIR GraphVMs code generation access schedule input metadata GraphIR GraphVMs code generation bulk frontend hardware independent compiler reuse GraphVMs implement GraphIR contribution GraphIR intermediate representation decouples algorithm specification hardware independent optimization  optimization llvm IR GraphIR inmemory representation program allows optimization IR IR transformation code generation enables reusable program analysis transformation lower across hardware platform reduce effort backend GraphVM UGC however unlike llvm IR GraphIR domain specific representation facilitates powerful flexible optimization GraphIR compose variable function instruction variable function instruction argument metadata II argument capture information derive algorithm specification correctness generate code metadata capture information related performance optimization hardware backends ignore specific hardware GraphIR metadata manipulate api function  std label val  std label GraphIR node api allows arbitrarily label metadata easily stack without GraphIR definition metadata api primary GraphVMs extend GraphIR node hardware specific optimization perform hardware specific transformation code generation backend implement abstract machine GraphVM optimize GraphIR java VM llvm detail GraphVMs operator data  easy GraphVM developer data structure choice mapping computation various hardware important instruction GraphIR   instruction II  iterates subset graph invokes function argument  specify graph input graph input frontier vertexset input vset output frontier vertexset output vset user define function apply argument derive operator algorithm specification instruction metadata generate optimize implementation input output function  int src int dst vertexset output frontier bool enqueue  atomic dst src enqueue  format sparse output frontier dst function int  char   fusion  frontier  output reuse frontier direction parallel frontier output    frontier output optimize GraphIR generate compiler bfs algorithm schedule enables kernel fusion text representation generate printing GraphIR memory data structure backend developer manipulate GraphIR UGC api frontier representation traversal direction  output frontier generate specialized code representation dense  iterates vertex frontier similarly argument metadata optimization apart instruction GraphIR instruction data structure allocation host device arithmetic reduction program architecture feature metadata attach instruction implement various optimization gpus hierarchy thread implement load balance strategy efficiently vertex CPUs gpus multiple memory enables cache utilization GraphIR bfs algorithm input II explains GraphIR operator  GraphIR node node contains argument graph iterate input output frontier function apply source destination filter operator metadata attach reuse frontier frontier reuse analysis pas analysis gpu swarm HammerBlade manycore GraphVMs  node another GraphIR node metadata representation frontier enqueue code  version memory GraphIR data structure bfs   function udf  applies compiler insert GraphIR description vertex individual vertex graph graph  graph data unweighted coo csr representation vertexset vertex sparse bitmap  representation function function definition function annotate device host  float int associate vertex graph array struct struct array  queue vertex priority   GraphIR instruction instruction argument metadata  vertexset function apply bool  bool parallel   input graph vertexset input vset vertexset output vset function apply bool output apply deduplication bool reuse frontier parallel  direction  output representation  input frontier  queue update  vertexset output vertex output  output format    expr   bool atomic  bool bool fusion variable hoist var  sum vertex update int update  bool atomic   output append bool destroy   vertexset output bool allocation  vertexset    expr   val bool atomic II data IR node GraphIR argument metadata associate IR node metadata attach hardware independent compiler GraphVMs metadata respective arithmetic IR node brevity  dependence analysis metadata atomic operation appropriate synchronization lower backends GraphVMs implement  instruction differently CPUs swap hardware instruction gpus warp shuffle cheap communication thread swarm GraphVM ignores atomic metadata swarm hardware executes task atomically GraphIR facilitates hardware specific optimization gpu GraphVM fuse multiple kernel launch kernel launch explain gpu GraphVM extends metadata  fusion flag  pas schedule prescribed fuse operator inside loop kernel abstraction extend GraphIR metadata GraphIR ideal representation  hardware specific optimization UGC GraphVM graph virtual machine GraphVM abstract machine executes target independent GraphIR backend developer implement GraphVM tailor architecture hardware specific code generation UGC framework diverse optimization apis access GraphIR node schedule attach reusable enable hardware benefit routine aid code generation GraphVMs architecture diverse GraphVM developer implement interpreter directly consumes executes GraphIR combination transformation code generation developer complexity generate code runtime library discus typical GraphVM hardware dependent analysis transformation GraphIR hardware specific schedule information code generation target device host applicable runtime library backend compiler infrastructure execute generate code UGC library analysis transformation GraphVMs reuse specialize development backends discus GraphVMs hardware specific optimization module version cpu gpu swarm HammerBlade frontend algorithm parser ast definition schedule function hardware independent compiler frontier reuse analysis analysis atomic insertion processing lower lower GraphVM processing specialization kernel fusion code generator runtime library code module UGC module reuse code orient program code module GraphVMs GraphVM pas hardware specific optimization simply pas code bold multiple GraphVMs multicore cpu GraphVM cpu GraphVM cpu specific graphit compiler implement optimization specific CPUs vertex traversal representation priority queue data structure cache numa optimization vertex data array struct struct array transformation others code generate cpu GraphVM comparable code generate graphit compiler maintain performance demonstrate graphit gpu GraphVM gpu GraphVM generates performance host device cuda code tune generation gpus implementation gpu GraphVM implement optimization gpu version graphit easily integrate infrastructure GraphIR gpu GraphVM code generation runtime library offload complexity code generation technique load balance runtime library gpu GraphVM implement load balance strategy utilization synchronization efficiency logic assign thread largely independent actual computation perform load balance implementation cleanly template library function simplifies code generation easy load balance technique code generation kernel fusion kernel fusion important optimization graph amortizes kernel launch overhead application iteration kernel fusion optimization implement entirely gpu GraphVM series preliminary pas identifies loop fuse variable loop function pas code generation generates actual global kernel launch gpu code generation pas insert appropriate cuda api host device fuse kernel fix thread code generator generates outer loop simulate thread insert grid sync synchronization finally pas generates appropriate launch gpu kernel instead kernel iteration loop gpu specific pas code demonstrates choice GraphIR GraphVMs significantly reduce effort unique hardware optimization gpu GraphVM implement optimization  fuse  frontier creation swarm GraphVM swarm architecture relies speculative execution task extract parallelism application core task execute aborted memory dependency violate ensure correctness however abort undesirable waste therefore swarm GraphVM focus eliminate false dependency memory access code generate swarm GraphVM bfs algorithm vertex task GraphIR data vertexset active vertex active active vertex memory introduces data dependency reuse memory across update memory pointer variable prevent swarm obtain parallelism speculate across data dependency spurious insertion distinct vertex actually independent pas swarm GraphVM replaces  vertex ID vertexset task spawn task operation perform vertex  timestamp task vertex dequeued swarm execution model guarantee task execute task task execute speculatively parallel without false dependence arise vertexset memory lambda prio action per frontier lambda spawn task execute lambda vertex later timestamps approach generalizes priority algorithm task timestamps priority private application variable update periodically pas BC variable update per output data structure vertex parallel task access variable data dependency update variable prevent speculation across address swarm GraphVM private task update perform functional style passing task spawn avoid update variable multiple parallel task pas eliminates unnecessary dependence unlocks speculative parallelism grain splitting spatial hint dependence violate swarm hardware execute offend speculative task important minimize waste pas swarm GraphVM hardware schedule task reduces abort abort swarm compiler assign spatial hint task memory location access task access disparate memory address GraphVM annotation instruct split subsequent code subtask access memory address dispatch subtasks chip tile accord cache access access cache execute within chip tile hardware selectively serialize task access cache reduce likelihood abort finegrained subtasks cheaper execute aborted reduce abort additionally GraphVM exploit domain knowledge loop iterate constant array strike balance abort spawn additional task generate annotation backend compiler schedule memory access instruction HammerBlade manycore GraphVM HammerBlade manycore GraphVM parallel code target HammerBlade manycore architecture described II code GraphVM sequential host code parallel device code sequential host code handle initialization coordination parallel device code executes graph algorithm HammerBlade manycore GraphVM implement optimization GraphIR transformation target manycore architecture memory hierarchy gpu GraphVM HammerBlade manycore GraphVM extensive host device runtime library simplify code generation atomics gpu atomics HammerBlade frontier prio int int src int  src int dst   pragma task hint dst dst dst src dst code generate swarm GraphVM bfs manycore expensive operation manycore atomic operation global memory implement lock data structure HammerBlade manycore GraphVM leverage atomics pas gpu GraphVM atomics atomic operation within kernel initialization code lock insert host code access optimization HammerBlade manycore GraphVM implement optimization utilizes software manage scratchpad core optimization access aim reschedule latency request memory parallel format item core iterate assign prefetching entire data core scratchpad memory repurposing  cache HammerBlade manycore GraphVM determines safely scratchpad memory without synchronization core processing alignment partition memory parallelism workload partition important achieve performance HammerBlade manycore propose alignment partition aim improve memory performance core vertex align cache llc increase effective memory access bandwidth optimization utilizes partition scheme access core scratchpad memory due observation graph workload load data scratchpad outweighs benefit latency scratchpad access graph vertex split vertex vertex graph multiple cache core reduces active vertex core increase cache rate reduce cache contention extensible schedule feature UGC decouples algorithm input optimization schedule programmer  generate variant algorithm tailor specific graph input simply schedule GraphVMs optimization schedule abstract description  hardware independent abstract schedule function description  parallelization scheme schedule vertex BASED BASED  direction traversal  frontier  bitmap  explicit deduplication perform output frontier  delta bucket  IV description  associate virtual function abstract description  hardware independent abstract hybrid schedule schedule runtime function description  schedule within hybrid schedule  schedule within hybrid schedule description  associate virtual function target schedule essential feature optimization respective target challenge approach  UGC schedule parameter dependence analysis insert atomics UDFs parallelization vertex traversal direction address orient program technique enable hardware independent UGC query information various schedule representation schedule input internally schedule attach program node UGC creates abstract interface virtual function information hardware independent compiler implement schedule GraphVM inherit abstract interface member function configure various schedule option specific optimization GraphVMs implement virtual function hardware independent UGC information IV abstract schedule virtual function query information direction parallelization schedule input bfs algorithm GraphVMs HammerBlade schedule hybrid traversal cache align load balance swarm enables transformation consecutive frontier priority queue update task   sched sched  sched    sched sched  bitmap sched   bitmap  comp input sched sched program  comp gpu schedule bfs  sched sched   sched  hybrid program  sched HammerBlade manycore schedule bfs  sched sched  sched   sched  vertexset TASKS program  sched swarm schedule bfs schedule statement frontier frontier max num    sched   sched host code runtime generate  inherits   hybrid schedule combine   user specifies runtime criterion associate parameter input criterion criterion compiler generate code chooses sched sched input vertex vertex graph generate code  schedule sched sched attach hardware independent compiler GraphVMs aware compiler generates nest statement multiple  combine IV evaluation demonstrate UGC implement optimization critical performance architecture target CPUs gpus swarm HammerBlade manycore performance optimize code generate GraphVMs architecture baseline unoptimized code graph algorithm graph input baseline code generate apply default schedule GraphVM algorithm optimize version tune schedule application graph compile exactly algorithm specification core core tile core tile ghz ISA haswell OoO core thread core cache KB per core split cycle latency cache MB per tile inclusive cycle latency cache MB static NUCA MB tile inclusive cycle latency coherence MESI cache directory noc bidirectional mesh link rout cycle hop cycle memory controller GB cycle minimum latency queue task queue entry core commit queue entry core conflict kbit bloom filter hash function tile cycle bloom filter cycle per timestamp commit queue commit tile update virtual arbiter cycle VI configuration core swarm methodology cpu gpu evaluate gpu GraphVM nvidia tesla gpu GB GDDR memory MB cache KB cache per SM SMs volta generation gpu evaluate cpu GraphVM dual socket intel xeon core CPUs core hardware thread context machine GB ddr memory MB cache per socket transparent THP enable swarm simulation evaluate swarm GraphVM algorithm compile code opensource swarm architectural simulator model core swarm cpu parameter VI prior model core haswell core xeon cpu GraphVM perform cycle simulation swarm detailed core network memory model model task speculation overhead detail HammerBlade manycore simulation model HammerBlade manycore 1GHz core tile parameter vii detailed cycle accurate rtl simulation model RISC core network chip llc rtl manycore validate silicon configuration occupies approximately model HBM memory DRAMSim timing accurate simulator generate host code natively intel xeon cpu host library interface directly simulator environment  dpi datasets input graph evaluation along vertex graph orkut OK twitter TW livejournal LJ  SW hollywood HW  PK  IC distribution  RU  RN  RC bound distribution datasets social graph web graph graph algorithm evaluate GraphVMs algorithm pagerank bfs component CC betweenness centrality BC pagerank core core grid RISC  ISA KB instruction cache KB data scratchpad cache KB capacity independent associative noc bidirectional 2D mesh data addr memory HBM channel GB per channel MB per channel vii HammerBlade manycore configuration graph vertex RN RC RU PK HW LJ OK IC TW SW graph input evaluation undirected twice per direction CC topology driven algorithm traverse iteration application massive parallelism BC bfs datadriven algorithm active vertex priority algorithm vertex priority efficiency UGC compiles source code specification algorithm reuse application code architecture application algorithm graph graph iteration pagerank bfs BC rerun vertex necessitate tune implementation characteristic graph architecture efficiency schedule performance GraphVMs heavily depends schedule specify manually schedule tune implementation algorithm graph graph social graph schedule parameter tune sweep parameter prior technique autotuning performance schedule relatively performance optimize code performance improvement optimization GraphVMs speedup report baseline code generate apply default schedule baseline optimize code parallel generate compile optimization enable backend compiler discus hardware specific optimization GraphVMs speedup cpu gpu baseline schedule CPUs gpus traversal vertex parallelism UGC achieves speedup architecture bfs BC hybrid traversal tune input frontier representation pagerank greatly benefit  numa optimization improve locality random access tile cache CPUs benefit bucket fusion optimization graph consistent speedup graphit compiler finally CC benefit load balance technique  gpus aware heatmap speedup evaluate architecture report speedup optimize code baseline unoptimized version speedup darker correspond algorithm correspond graph input graph HammerBlade manycore due simulation constraint speedup gpu GraphVM framework gunrock  sep graph vertex parallelism CPUs performance gpu GraphVM graph library specifically target gpus gunrock  sep graph speedup consistent gpu code generate graphit UGC consistently outperform sep graph graph sep graph implement asynchronous execution remove barrier successive UGC currently implement optimization algorithm specific cannot generalize HammerBlade manycore due rtl simulation evaluate HammerBlade manycore GraphVM input graph subset iteration application PR simulate iteration remain application simulate representative iteration frontier density execution behavior hybrid traversal baseline code bfs BC decrease simulation speedup report HammerBlade manycore swarm scalability bfs graph across HammerBlade manycore swarm machine graph dram stall bandwidth speedup LJ HW PC IX impact HammerBlade access optimization reduction dram stall improvement memory bandwidth utilization overall speedup apply HammerBlade manycore specific optimization described BC CC bfs benefit alignment partition PR optimization due compute intensive optimization utilize memory hierarchy speedup unoptimized code performance HammerBlade manycore optimize bfs code machine configuration llc capacity constant core indicates HammerBlade manycore GraphVM successfully exploit parallelism highlight bfs due memory access breakdown core swarm compute ratio IX demonstrates performance improvement access optimization apply input graph optimization exploit memory parallelism hide dram access latency exchange load unused data reduce effective bandwidth optimization decrease dram stall increase memory bandwidth utilization improves overall application performance swarm speedup achieve appropriate schedule algorithm graph input swarm GraphVM default schedule swarm compiler already applies optimization uncover parallelism serial code achieves baseline performance however swarm GraphVM improves performance exploit domain knowledge optimization bfs convert  task responsible majority improvement graph optimization avoids synchronization overhead distance task execute speculatively parallel additionally algorithm benefit swarm GraphVM diverse schedule option task granularity spatial hint grain splitting spatial hint allows trading increase task overhead reduce cache ping  abort finally CC pagerank graph feature node benefit schedule shuffle trading locality reduce abort reorder enable swarm GraphVM domain knowledge valid within graph bfs RN RC RU swarm GraphVM speedup cpu GraphVM code core swarm performance optimize code generate cpu swarm GraphVMs swarm superset cpu feature cpu code swarm hardware graph swarm GraphVM consistently outperforms cpu GraphVM swarm speculative parallel execution grain task speedup swarm GraphVM optimize code manually optimize assembly code prior relative swarm GraphVM default baseline code core optimize schedule average across core core execute task commit abort idle due speculation throttle heuristic exhaustion commit queue lack task core spill content  task queue memory across algorithm core execute useful commits demonstrate swarm GraphVM expose finegrained parallelism task spawn utilize core reflect swarm scalability tile swarm increase aggregate cache queue capacity sometimes yield superlinear speedup prior swarm developed tune version bfs swarm GraphVM version competitive manually tune social graph TW SW algorithm memory bound tune version tailor graph vertex tune code performs poorly social graph swarm GraphVM achieves performance selective spawn task possibly node UGC easy algorithm developer graph processing architecture enables easily explore algorithm implementation obvious architecture designer related amount graph processing framework leverage irs application architecture irs diverse architecture  introduces IR parallel program target heterogenous architecture however  IR generic specific domain customize IR specifically graph domain UGC perform optimization otherwise infeasible program furthermore  extensible schedule allows user specify optimization target MLIR another propose IR generic specific domain tensorflow tvm IR apply machine optimization across architecture graph processing framework graph processing memory gpus manycore architecture framework limited optimization achieve consistently performance across algorithm graph portability across architecture abelian galois framework interface memory cpu distribute memory cpu gpu platform however abelian extensible architecture contrast UGC demonstrates theart performance across platform compiler graph application  compiler framework creates intermediate representation specifically graph application gpus  introduces optimization gpus achieve theart gpu performance graphit domain specific expands optimization outperform cpu gpu framework decouple algorithm optimization UGC extends graphit decouple algorithm optimization hardware  enable efficient implementation across platform VI conclusion UGC novel graph processing framework easy compiler backends across diverse architecture introduce IR graph processing GraphIR implement GraphVMs architecture demonstrate UGC algorithmic hardware specific optimization generate performance code architecture optimization speedup programmer generate baseline implementation