DL training platform built interconnect multiple DL accelerator gpu tpu via customize interconnects gigabyte GBs bandwidth however identify bandwidth challenge  balance accelerator compute memory DL computation communication contribution via measurement detailed model understand compute memory bandwidth demand DL compute comms propose novel DL collective communication accelerator accelerator collective ace sits alongside compute networking accelerator endpoint ace endpoint compute memory resource DL compute reduces memory BW average network BW baseline DL workload network ace average increase effective network bandwidth utilization average speedup iteration resnet GNMT DLRM baseline configuration respectively index communication accelerator accelerator fabric training collective communication introduction DL neural network dnn model deployed pervasively across application domain computational requirement dnn model  rate handle  growth data workload requirement advent efficient accelerator capable handle model accelerate training petabyte input data demand faster efficient DL training achieve scalable efficient distribute training accelerator cannot satisfy compute memory requirement dnns distribute training emergence DL training platform google tpu facebook  nvidia DGX DGX intel gpu feature platform customize interconnect various architecture DL training platform google tpu nvidia DGX DGX facebook  intel gpu accelerator fabric hypercube mesh  torus tpu habana switch  DGX prior offload collective contrast endpoint npu node baseline ace propose baseline collective communication task contend npu core SMs gpu memory bandwidth accelerator addition traditional  memory cpu network QPI tcp IP  infiniband datacenter network accessible via pcie infiniband NICs neural processing NPUs accelerator gpus TPUs FPGAs accelerator fabric AF accelerator network AF traditional NIC datacenter network NPUs directly without cpu NIC intervention AF consistent terminology community fabric facebook  terminology google tpu inter core interconnect ici  interconnect memory fabric UI OOVBM  PNQVUFS SDIJUFDUVSF acm annual international symposium computer architecture isca doi isca bandwidth datacenter network GB GB employ  interconnects bind npu chiplets custom bandwidth interconnects link NVLINK package significant architecture community accelerate compute portion DL training via efficient accelerator memory ignore communication distribute DL training fundamentally involves splitting dnn model training data across multiple NPUs scheme refer model data hybrid parallel respectively parallelization strategy dictate communication NPUs communication collective NPUs synchronize input output error activation backward pas gradient backward pas specifically collective operation allreduce heavily distribute DL training operation latency sensitive layer dnn cannot proceed gradient synchronize bandwidth hungry due activation gradient limited network bandwidth hence easily become bottleneck primary technique employ minimize impact communication overlap compute enable via clever fabric topology accompany topology aware collective communication algorithm binary reduce implementation library intel   nvidia NCCL tpu boast contention scalability reduce torus however identify perfect compute comms overlap training platform despite optimize topology collective algorithm implementation prohibitive due architectural bottleneck endpoint compute communication contend resource compute memory bandwidth npu peripheral source inefficiency compute portion compute resource npu perform collective update network reduces compute available training computation GEMMs backward memory bandwidth gradient memory reduce bandwidth available actual training computation highly  dnns recommendation nlp exacerbate recent advance multi chip module mcm packaging technology silicon interposer bandwidth inter package network enormous network BW NPUs AF assume synchronous update guarantee convergence asynchronous update comparison previous  switch offload scheme ace  collective dcn datacenter network AF accelerator fabric scheme app offload protocol topology  network mellanox hpc DL switch infiniband   dcn  DL switch ethernet   dcn spin hpc NIC ethernet   dcn trigger hpc NIC  flexible  dcn inceptionn DL NIC ethernet param server dcn nvidia DL switch memory   AF ace DL endpoint   switch  AF reduce challenge NPUs fully network corroborate aforementioned issue via measurement DGX microbenchmarks recommendation model DLRM  LM workload implication issue utilization available AF bandwidth demonstrate via detailed simulator knowledge identify issue address inefficiency propose accelerator collective ace accelerator DL training collective ace sits alongside accelerator fabric interface afi interface npu AF network handle communication protocol ace compute variety collective algorithm scratchpad cache gradient npu resource memory bandwidth gemm computation difference previous communication offload datacenter network ace visualize knowledge endpoint offload scheme tune distribute training AF ace demonstrate compute available AF bandwidth utilized efficiently decrease overall training contribution identify challenge DL training platform related compute memory bandwidth limit utilization available AF bandwidth future training platform propose novel microarchitecture ace handle collective communication efficiently enhance AF network utilization IV ace endpoint compute memory resource DL compute reduces memory BW average network BW baseline average ace increase effective network bandwidth utilization average overview collective communication operation dnn training network speedup iteration resnet GNMT DLRM baseline configuration respectively finally reduction memory BW requirement enable ace allows perform workload specific optimization specific optimize DLRM organize II background distribute training establishes challenge DL training specifically focus critical bottleneck endpoint inhibit efficient network utilization IV describes ace microarchitecture detailed description evaluation simulation methodology experimental VI related vii finally conclude II background training dnns involves iteratively refining parameter aka network non convex nonlinear optimization minimize loss function background distribute training parallelization parallelization technique DL training data parallelism replicates entire model multiple node advantage input sample node computes partially gradient subset sample input data aka mini batch iteration node exchange partially gradient perform sgd operation update gradient accumulate node update pas iteration model parallelism node datasets minibatch model node node output activation input gradient pas propagation respectively communicate across node enable pas propagation collective communication operation exchange input gradient output activation node parallelism approach collective communication collective communication operation contributor dnn training communication reduce scatter reduce reduce scatter reduces sum data reside node node portion globally reduce data data scatter across node node data reduce reduce scatter node portion data node reduce dominant communication DL training exchange gradient activation various parallelism scheme however  scenario embed exchange recommendation model facebook DLRM topology aware collective algorithm collective efficient implementation algorithm underlie topology library intel  nvidia NCCL implementation collective hierarchical reduce  optimize available bandwidth underlie topology discus topology aware collective evaluate target NCCL compute NCCL NCCL allreduce MB comms replay DLRM workload backpropagation impact compute comms overlap production DLRM workload reduce AR slowdown gemm embed lookup gemm matrix multiplication dimension matrix  ups per sample batch slowdown communication overlap computation platform nvidia gpu NVSwitch offering GB available network BW per gpu motivation AF BW utilization highlight critical challenge achieve network bandwidth utilization DL training workload qualitatively summarizes finding npu compute availability accelerator npu core cuda core concurrently execute kernel pertain DL collective communication majority compute core execute DL computation sensitivity analysis AF network utilization affected amount npu memory BW available DL communication increase communication MB reduce ideal assumes data collective communication  cycle upper bound network performance baseline assumes SMs available communication task ace consume SM within npu sensitivity analysis network utilization affected baseline available SMs DL communication communication MB reduce assume npu memory BW available communication ace rely npu SMs communication hence applicable GEMMs convolution collective operation reduce parallelize across core npu core cannot fully saturate memory bandwidth network bandwidth various bottleneck npu core memory network data core iterates sequence recv operation peer optional computation data locally available data reduction sum optimal npu core function network bandwidth saturate core memory bandwidth available core communication algorithm message memory bandwidth availability collective communication operation access sufficiently memory bandwidth perform local reduction sum operation involve local memory sum operation local remote network RDMA synchronization operation memory bandwidth requirement function actual collective operation proportional network bandwidth measurement analysis highlight resource contention issue communication computation microbenchmark workload training  DLRM model  LM model microbenchmark executes compute operation communication operation compute operation finally communication fundamental compute operation recommendation model matrix multiplication gemm consumes gpu compute core embed lookup  mainly consumes gpu memory bandwidth slowdown NCCL reduce collective nvidia gpus interconnect via NVSwitch GB available network BW slowdown proportional compute operation reduce sensitive MB reduce concurrently dimension gemm warp per multiprocessor SM typical scenario training recommendation model reduce overlap MB reduce  batch GB memory bandwidth slows highlight performance degradation slowdown NCCL reduce operation concurrently compute kernel gemm  pytorch DLRM workload batch reproduction communication without computation param replay benchmark benchmark MB reduce increase without overlap overlap gemm embed lookup kernel degradation backward propagation degradation consistent allreduce operation across epoch pytorch cuda compute NCCL operation actual overlap operation dynamically cuda HW scheduler pending operation negative impact congestion compute memory resource compete kernel network BW utilization variability execution evaluate  LM workload platform communication scenario communication overlap compute communication explicitly issue propagation computation overlap scenario training algorithm communication average overlap communication computation degrades communication performance non overlap scenario compute communication resource contention simulation demonstrate network BW exacerbate network BW increase available network BW GB conduct simulation communication performance affected available memory BW npu core available DL communication task varied highlight assume gpu NPUs consist processor SM described NPUs MB reduce setup described ideal GB BW utilization available GB intra package link silicon interposer become underutilized due imbalance inter package link baseline GB memory BW average ideal network BW average message multiple writes issue data local reduction baseline however ace 8GB ideal BW utilization reduction memory BW requirement performance improvement elaborate VI multiprocessor SMs prevent gpu compute core bottleneck network SMs data memory inject network frequency mhz BW byte cycle memory BW GB per SM information calculate network BW driven increase SMs communication simulated platform SMs GB memory BW percentage core usage library  NCCL takeaway overall argue npu core cuda core execute collective communication increase contention precious compute core memory bandwidth accelerator node complex interplay compute memory network affect collective performance performance compute kernel execute concurrently increase effective compute available overlap communication operation hiding inefficiency challenge exacerbate future training platform compute accelerator hierarchical bandwidth emerge mcm technology harder hide communication compute II offload switch endpoint offload scheme increase available memory BW compute training endpoint congestion reduction switch topology compatibility topology compatibility hybrid topology compatibility various collective algorithm switch partially endpoint IV accelerator  driven compute memory contention issue identify dedicate accelerator DL collective communication ace ace microarchitecture chip SRAM afi TX dma transfer data memory afi SRAM normal operation ace SRAM ace activate alu afi RX dma transfer data afi SRAM normal operation mode ace SRAM ace activate mode memory input output buffer buffer allocate per physical link packet correspond specific link logic data granularity ace execution granularity constraint payload variable training algorithm training algorithm chunk parameter pipelining storage message parameter multiple node topology packet link technology technology flit network buffer microarchitecture  variable link width technology alongside accelerator fabric interface afi module introduce earlier detail overview overview ace integrate afi module component within ace additional compute reduce computation scratchpad buffer cache gradient domain specific accelerator datapath ace specialized target workload namely collective communication algorithm placement switch communication offload fundamentally option offload switch endpoint offload II comparison reduce burden endpoint memory compute handle  task however endpoint flexible various network topology switch switch network topology ptp link benefit offload scheme afs platform solely network topology nvidia DGX google tpu habana addition intra package network almost ptp additionally unlike switch implementation reduce baseline ace reduction broadcast traffic switch endpoint algorithm halve  etc employ ace endpoint offload alongside afi data granularity granularity data ace execution factor default ace ace initiate execution command npu perform specific collective payload payload activation gradient parallelism approach pas command collective address data reside memory ace payload multiple chunk processing schedule chunk individually pipelined manner multiple chunk schedule parallel chunk decomposes multiple message collective algorithm message granularity message multiple node ace perform reduce NPUs chunk message execute reduce serially iteration message comprises packet enters network layer data processing within ace packet bus width SRAM interface packet data movement execution serialize packet width reduce ace action via detailed reduce collective processing perform reduce message algorithm detail reduce IV baseline ace concept advantage ace baseline applicable topology collective later IV logical algorithm across node assume chunk simplicity node message reduce implement reduce scatter allgather  initiate reduce scatter node sends message another message node reduces message local concludes reduce scatter node reduce message node reduce message node message node baseline ace baseline phase message memory afi inject network local reduction baseline load local message message npu cache sub perform reduction sub network sub memory sub algorithm reduces available memory bandwidth compute resource DL GEMMs contrast ace restricts data movement phase reduce congestion increase available memory bandwidth allows DL GEMMs npu compute resource internal ace interaction node ace SRAM partition serf source reduce phase serf source memory message partition ace SRAM TX dma sub message series packet inject designate output buffer inject network sub message reduce local data sub sub ace overlap algorithm perform reduction sub locally sub sub broken clarity message sub message chunk memory RX dma sub multiple message described IV execute serial simplicity assume message packet packet data actual  data afi link within ace multiple resource available sub proceed sub SRAM input available output resource stall resource available multiple chunk execute parallel maximize internal hardware resource link bandwidth discus parallelism achieve network utilization apply parallelism various algorithmic perspective parallelism complex hierarchical topology implement topology aware collective multiple phase assume collective algorithm phase multiple chunk parallel within phase across phase chunk multiple message hence multiple flight chunk multiple flight message belonging chunk parallel chunk multiple packet parallel packet data transfer within network parallelism network ace memory management ensure algorithmic parallelism opportunity SRAM within ace partition accord phase collective algorithm plus RX dma terminal partition IV reduce phase hence partition detail multi phase reduce mention comprises multiple programmable finite machine FSMs FSM programmed specific phase specific collective algorithm queue chunk entry queue context chunk address inside SRAM address phase chunk ace assign machine phase phase chunk insert queue assign machine phase machine compete access resource overlap execution multiple chunk within across phase increase resource utilization network bandwidth available parallelism bound available machine manage dataflow phase workload parallelism DLRM evaluation collective operation reduce exist phase FSMs allocate phase programmed handle collective operation phase phase assign multiple FSMs interface npu afi ace extends exist afi interface expose npu afi ace specific command npu ace ace npu npu afi command interface ucx  standard communication interface collective command ace decides load data availability SRAM finally ace notifies completion chunk collective interrupt npu flexibility scalability collective principle collective algorithm ace remain collective phase SRAM partition partition assign phase multiple FSMs partition terminal partition memory collective implement program dataflow FSM topology sine ace handle collective endpoint orthogonal network topology switch hybrid logical ace perform collective algorithm reduce physical topology network protocol rout algorithm deliver packet accordingly IV synthesis component alu MB SRAM switch interconnect ace ace exploration implementation ace performance SRAM FSMs component overhead swept increase beyond MB SRAM FSMs diminish return chose parameter parameter network pipeline alu capable preform FP FP parallel sufficient ace interconnect SRAM functional bus implement ace verilog synthesize synopsis compiler technology node IV estimate enumerate individual component ace training accelerator report ace overhead addition heuristic SRAM partition partition SRAM available network performance improvement MB SRAM FSMs performance ace FSM SRAM normalize performance parameter MB SRAM FSMs performance average ace performance across target workload described utilization ace simulation ace utilized assign chunk processing bandwidth initial chunk phase terminal partition partition phase partition partition parameter parameter compute accel max tflops FP gpu SMs bus BW GB npu afi GB npu mem ace message KB packet byte intra inter package per link BW GB intra GB inter link latency cycle intra cycle inter link npu intra package link bidirectional inter package bidirectional horizontal bidirectional vertical BW GB intra package GB horizontal GB vertical link efficiency intra inter package evaluation methodology describes methodology establish simulate performance training evaluate benefit communication acceleration simulator parameter ASTRA sim distribute dnn training simulator developed ace ASTRA sim model training loop parallelization collective communication schedule library  interface compute model compute pas gradient input gradient network simulator cycle network behavior compute parameter compute model npu mhz gpu core multiprocessor SMs achieve max tflops FP precision compute accelerator training accelerator phase link bandwidth initial chunk partition phase bandwidth chunk phase chunk belonging collective operation average chunk VI target configuration  baseline overlap communication computation communication explicitly issue propagation phase communication kernel resource available compute communication strategy enhances compute communication critical training loop proceed communication BaselineCommOpt baseline resource allocate communication performance ideal intuition GB memory BW SMs npu allocate communication task computation BaselineCompOpt baseline training computation performance optimize ace assume GB memory BW allocate communication amount memory BW derive  BW SMs communication task compute ace propose npu resource dedicate training algorithm computation moreover GB memory BW ideal memory BW allocate training compute ideal endpoint handle message  within cycle essentially associate latency endpoint collective communication latency upper bound compute memory allocate training algorithm exception DLRM fwd pas training loop performs communication completes compute afi network parameter extend ASTRA sim model afi interaction traffic afi npu memory model transaction schedule npu afi npu mem queue delay subsequent transaction npu mem bandwidth chosen accelerator npu afi bandwidth assume sum intra package inter package link logical choice prevent npu afi bandwidth bottleneck network link inter package link BW assume NVLINK intra package bandwidth bandwidth performance mcm target training platform ace switch topology 3D torus topology model futuristic platform AF comprise multiple NPUs integrate multi chip packaging technology package multiple package interconnect via dedicate fabric notation  NPUs within package intra package local dimension NPUs offset across package 2D torus within vertical dimension horizontal dimension vertical horizontal respectively topology aware collective algorithm hierarchical multi phase collective algorithm 3D torus topology reduce occurs phase reduce scatter local reduce vertical reduce horizontal local dimension implementation bandwidth intra package local link inter package link simulator collective npu simultaneously sends distinct portion data npu phase FSMs programmed execute addition assign reduce phase perform FSM data output link FIFOs destination npu route xyz local dim vertical dim horizontal dim rout packet destination torus topology AF network NVLink communication natively baseline packet multiple hop communication library NCCL responsible data intermediate hop memory data hop waste memory BW intermediate hop ace prevents unnecessary memory overhead SRAM absorbs packet destination FSM responsible correspond chunk target configuration configuration VI investigate flavor baseline comp comm overlap optimize communication BaselineCommOpt computation BaselineCommOpt baseline overlap communication compute  addition ace ideal upper bound maximum compute communication performance target workload evaluate platform workload resnet vision GNMT nlp DLRM recommendation dnns FP precision activation gradient computation communication data parallel parallelism reduce gradient resnet GNMT hybrid parallel data parallel across mlp layer model parallel across embed DLRM DLRM version described representative model simulate training iteration   collective schedule policy priority collective layer propagation mini batch per npu resnet GNMT DLRM respectively runtime breakdown compute comms overlap assume weak NPUs evaluation metric evaluation workload VI VI VI metric computation expose communication expose communication refers communication cannot overlap computation hence training algorithm compute communication data parallel pas layer gradient communication reduce previous iteration update DLRM additionally embed communication operation mlp layer pas propagation update embed summation computation expose communication determines training iteration VI evaluation RESULTS simulation ace ideal baseline mention VI workload analytical investigation memory BW requirement ace baseline justifies simulation memory BW requirement baseline ace accord baseline memory byte byte network reduce scatter phase exclude initial phase memory byte byte network phase byte identical average byte memory byte explains GB memory BW 0GB network ideal however ace cache data reuses multiple amount reuse depends topology algorithm npu XX byte data cached ace phase reduce scatter vertical horizontal reduce phase finally another phase data network GB memory BW GB ideal however reality memory BW due hardware delay queue delay bus etc limit analytical reduce collective due limitation analysis however reduce frequently usually compute communication overlap evaluate  overlap mention VI training iteration resnet GNMT DLRM goal dissect compute communication overlap configuration NPUs AF network VI trend network  configuration resnet analysis npu compute network utilization resnet training BaselineCommOpt focus BaselineCommOpt despite optimize communication network link completely utilized network utilization fluctuates workload computation operation consecutive communication partially completely communication communication task arrives BW intra package link underutilized collective algorithm additionally resnet issue collective sufficient completely available network BW expose communication consists iteration BaselineCompOpt dive flavor baseline optimize compute BaselineCommOpt BaselineCompOpt reduces computation fails significantly reduce iteration expose communication increase due utilization network network utilization BaselineCommOpt BaselineCompOpt due shorter interval communication task BaselineCompOpt reduce computation allows schedule collective overall iteration improve BaselineCommOpt baseline performance summarize later training iteration duration node baseline epoch training imagenet dataset comparable training performance per node report however release resource compute via ace performance ace ace achieve release resource compute efficient cache data perform collective BaselineCompOpt slightly improve compute release compute core compute however handle communication efficiently expose communication iteration ace outperforms BaselineCommOpt BaselineCommOpt iteration respectively overall ace achieve ideal performance BaselineCommOpt BaselineCompOpt respectively GNMT analysis compute communication utilization GNMT GNMT communication per layer BaselineCommOpt network efficiently compute overlap communication computation however BaselineCommOpt suffers ideal network utilization fluctuates compute communication overlap training iteration resnet GNMT DLRM network node 3D torus report average compute network utilization cycle cycle cycle training burst network activity correspond iteration network utilization corresponds link utilized schedule flit cycle irrespective link BW compute GNMT compute sensitive available memory BW ace outperform BaselineCommOpt BaselineCompOpt respectively additionally achieves ideal BaselineCommOpt BaselineCompOpt DLRM analysis corresponds DLRM communication relatively ace iteration shorter BaselineCommOpt BaselineCompOpt respectively BaselineCommOpt BaselineCompOpt ace achieves ideal iteration respectively ace utilization average utilization ace pas resnet GNMT zero communication DLRM communication utilization ace however propagation average utilization resnet GNMT DLRM respectively utilization presence compute communication task highperformance ace idle communication arrives scalability network utilization training loop algorithm latency decompose compute expose communication workload torus network expose communication delay increase network due increase overhead communication baseline overlap  outperforms BaselineCommOpt compute directly translates reduction iteration communication overlap compute impact training however expose communication due network utilization limit training performance unaddressed another  BaselineCommOpt  thanks saving compute BaselineCompOpt workload resnet NPUs combine communication resnet network performance collective individually ace outperforms baseline accord average across workload  BaselineCommOpt BaselineCompOpt ace achieves ideal performance iteration respectively performance improvement ace baseline network accord performance gap ace baseline increase increase evident  BaselineCompOpt configuration vulnerable increase computation expose communication training iteration NPUs AF network increase correspond performance ace  BaselineCommOpt BaselineCompOpt communication overhead compute latency BaselineCommOpt allows tolerate communication overhead moreover workload benefit ace expose communication performance dependent interplay communication interval communication task compute overlap communication average across network baseline configuration ace achieves speedup iteration resnet GNMT DLRM respectively preserve ratio effective network BW utilization GB ace baseline specific network workload configuration inject amount traffic network average across ace improve effective network utilization baseline DLRM optimization extra available memory bandwidth enable ace opportunity perform various workload optimization increase training performance DLRM optimization extra memory BW overlap memory intensive embed lookup update previous iteration computation iteration embed index barely reuse consecutive training iteration embed operation outside critical training loop demonstrate perform allocate SM GB available memory BW embed update lookup previous iteration iteration embed lookup immediately issue communication lookup iteration impact optimization baseline ace computation decrease training loop embed update lookup iteration however BaselineCompOpt benefit optimization due communication performance BaselineCompOpt ace achieve performance improvement default training loop respectively impact optimize training loop baseline ace vii related ace propose dedicate hardware endpoint DL collective within accelerator fabric closest flavor ace collective offload datacenter NICs switch contrast discus collective offload DL switch offload intel  mellanox nvidia memory switch offload propose aggregation switch switch offload explore accelerate reinforcement unfortunately earlier IV restrict switch topology limit applicability  nvidia platform contrast ace switch topology highly popular training platform collective offload hpc hpc torus topology    collective offload network router however approach CPUs communicate via message passing accelerator TPUs gpus communicate via memory fabric CONCLUSIONS identify issue compute memory bandwidth DL accelerator endpoint distribute training platform limit  overlap optimize endpoint novel collective accelerator ace demonstrate ace efficiently hierarchical AF fabric peak bandwidth critical compute memory resource DL computation training average ace endpoint memory BW network BW baseline DL workload network ace average increase effective network bandwidth utilization average speedup iteration resnet GNMT DLRM baseline configuration respectively future research optimize compute  interaction distribute training