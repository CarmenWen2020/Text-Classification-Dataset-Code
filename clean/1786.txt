recent advance neural network dnns active development specialized dnn accelerator feature processing laid spatially multi memory hierarchy flexible interconnect dnn accelerator advantage data reuse achieve peak throughput expose runtime parameter programmer explicitly manage computation schedule spatially temporally schedule choice variation performance efficiency motivate efficient strategy navigate vast schedule address challenge CoSA  approach schedule dnn accelerator oppose exist approach rely designer heuristic iterative navigate CoSA express schedule decision constrain optimization deterministically mathematical optimization technique specifically CoSA leverage regularity dnn operator hardware formulate dnn schedule mixed integer program mip algorithmic architectural constraint automatically generate highly efficient schedule shot demonstrate CoSA generate schedule significantly outperform approach geometric across dnn network improve index schedule accelerator neural network compiler optimization introduction neural network dnns gain recent due robust ability amount data dnn approach apply computer vision machine translation audio synthesis recommendation model autonomous motivate computational requirement dnns development research commercial building specialized dnn accelerator application dnn accelerator typically incorporate array processing boost parallelism multi memory hierarchy flexible  chip noc improve data reuse architectural structure improve performance efficiency dnn execution expose schedule parameter programmer computation data movement mapped onto accelerator spatially temporally schedule dnn layer partition spatially temporally execute specialized accelerator target dnn layer specific hardware architecture billion valid schedule performance efficiency vast dnn layer dimension hardware architecture significant demand generalize framework quickly efficient schedule option accelerator hardware configuration achieve performance spatially distribute architecture factor carefully tile hardware utilization pipelining data movement compute maximize data previous schedule framework attempt reflect consideration formulate analytical model prune schedule hardware constraint exhaustively candidate model however navigate schedule brute fashion easily become intractable dnn layer complex hardware architecture notable effort employ feedback driven approach tune beam machine algorithm iterative sample however scheduler typically massive training datasets simulation performance model infeasible extend hardware accelerator development hence efficient schedule mechanism quickly navigate performant schedule option demonstrate CoSA  approach schedule dnn accelerator contrast prior exhaustive bruteforce expensive feedback driven approach CoSA express dnn accelerator schedule  deterministically UI OOVBM  PNQVUFS SDIJUFDUVSF acm annual international symposium computer architecture isca doi isca execution latency histogram valid schedule choice resnet layer spatial accelerator mathematical optimization library pas CoSA leverage regularity dnn layer spatial hardware accelerator algorithmic hardware parameter clearly define schedule constraint specifically CoSA formulates dnn schedule prime factor allocation determines tile memory relative loop exploit reuse computation execute spatially temporally CoSA construct schedule constraint expose algorithmic behavior layer dimension hardware parameter memory network hierarchy clearly define composable objective function CoSA dnn schedule shot without expensive iterative evaluation demonstrates CoSA generate schedule outperform approach across dnn network layer schedule iterative summary contribution formulate dnn accelerator schedule constrain optimization pas knowledge CoSA constrain optimization approach tackle dnn schedule decision shot communication orient approach CoSA formulation highlight importance data transfer across chip memory expose clearly define objective function demonstrate CoSA quickly generate highperformance schedule outperform approach dnn layer across hardware architecture II background motivation discus complexity dnn schedule scheduler navigate dnn schedule schedule crucial decision compiler effectively assign workload compute resource emergence numerous dnn accelerator diverse architecture performant explainable approach schedule focus scheduler algorithm brute approach timeloop brute random  brute  brute interstellar brute  decouple brute feedback approach AutoTVM ML iteration halide    MCMC gamma genetic algorithm mapping gradient constrain optimization approach   tensor comprehension polyhedral transformation  CoSA mixed integer program mip dnn accelerator scheduler operator schedule aim optimize performance operator dnn layer specific hardware operator schedule typically comprises loop optimization loop tile loop permutation spatial mapping loop tile describes loop mapped memory hierarchy correspond tile loop permutation determines relative loop spatial mapping bind loop dimension spatial hardware resource parallel processing instead mapping temporal sequential execution optimization significant impact performance optimization achieve performance schedule convolution layer resnet input output channel output dimension accelerator memory split individual loop bound prime factor assign memory billion schedule randomly sample schedule loop tiling fail satisfy buffer capacity constraint schedule invalid KB buffer available buffer KB performance distribution valid schedule performance difference valid schedule outperform addition cluster schedule latency reveal structure scheduler schedule dnn layer billion valid schedule schedule exhaustive become intractable recent effort tackle complexity brute approach recent effort combine exhaustive heuristic manually prune schedule dnn schedule formulation CoSA CoSA dnn layer dimension dnn accelerator parameter express schedule constrain optimization performant schedule shot dram global buffer spatial spatial input buffer spatial buffer accumulation buffer spatial register listing schedule loop nest representation dnn layer dimension variable prefix indicates tile dimension exhaustive scheduler category typically lightweight analytical model estimate latency throughput consumption valid mapping layer schedule disadvantage approach fold brute tends exceedingly expensive complex hardware architecture infeasible schedule quickly generate schedule perform optimally analytical model fail communication latency across spatial hardware feedback approach recent effort feedback driven approach along machine statistical improve accuracy model gradient although approach potentially distribution schedule typically amount training data due feedback driven approach mainly applicable silicon hardware perform measurement feasible hardware development constrain optimization approach  objective function maximize minimize constraint demonstrate ability complex largescale reasonable widely architecture research instruction schedule synthesis memory partition algorithm selection program synthesis polyhedral transformation leveraged constrain optimization approach auto vectorization loop tile prior target purpose CPUs gpus finegrained instruction hardware manage cache oppose software manage spatial accelerator target addition exist polyhedral approach lack tile optimization instead tile input apply transformation tile due limitation tile decision cannot optimize loop transformation loop permutation pas sub optimal schedule address drawback exist approach leverage regularity dnn workload accelerator optimization CoSA employ constrain optimization tackle dnn schedule pas CoSA unique domain specific representation dnn schedule capture utilization communication encodes loop transformation tile loop permutation spatial mapping decision formulation unified representation enables optimization pas efficient schedule complex accelerator multi memory hierarchy  framework navigate schedule dnn accelerator develop CoSA constrain optimization dnn scheduler automatically generate performance schedule spatially distribute accelerator CoSA deterministically solves schedule pas without exhaustive iterative sample easily apply network layer hardware architecture discus CoSA framework CoSA formulates dnn schedule mixed integer program mip CoSA overview CoSA optimizes operator schedule mapping dnn layer onto spatial dnn accelerator specifically CoSA formulates schedule constrain optimization performance comparison schedule loop permutation convolution operator layer dimension leftmost schedule  refers relative input channel dimension outermost loop output height dimension innermost loop layer loop permutation emphasize reuse PCK pkc efficient variable schedule constraint dnn dimension hardware parameter objective function goal maximize buffer utilization achieve parallelism target CoSA CoSA specification dnn layer underlie spatial accelerator input constraint generates valid performance schedule objective function pas target workload target dnn operator express nest loop variable loop bound refer convolution kernel width height refer output width height refers input channel refers output channel refers batch illustrate convolution operation computes dot filter input generate output matrix multiplication express scheme target architecture CoSA target spatial architecture array processing PEs via chip network multiple memory hierarchy commonly adopt architecture template dnn accelerator target schedule decision CoSA generate schedule specify dnn layer execute spatial architecture listing schedule loop nest representation explicitly computation convolution layer mapped memory hierarchy highlight aspect schedule loop tile describes loop mapped memory loop bound loop permutation handle relative loop memory hierarchy spatial mapping defines loop mapped parallel spatial resource spatial loop listing factor role efficiency schedule choice highlight implication loop permutation spatial mapping explore loop tile illustrates impact loop permutation performance comparison schedule spatial mapping convolution operator layer dimension factor spatial mapping factor temporal mapping PC mapping factor dimension factor dimension mapped spatial execution PEs factor temporal mapping convolution layer hardware schedule loop tile spatial mapping loop global buffer label axis  input channel dimension outermost loop output height dimension innermost loop outermost loop PCK pkc speedup layer motivate implication loop permutation schedule impact spatial mapping dnn execution gap rightmost leftmost schedule layer consideration fundamental difference communication traffic generate spatial mapping option schedule rightmost schedule PCK PK obtain factor mapped spatial loop cannot achieve simply model data parallelism spatial partition systematic evaluation spatial mapping choice schedule discus CoSA formulates schedule variable constraint objective dnn schedule CoSA variable constant discus variable constant summarize II CoSA formulation CoSA variable CoSA constant index binary matrix schedule layer dimension data tensor mapping memory layer dimension memory data tensor mapping prime factor index mapping choice permutation data tensor II CoSA notation dnn layer prime factor idx perm schedule layer dim prime factor mapping memory register    binary matrix schedule  indicates spatial temporal mapping   indicates rank loop permutation schedule loop tile dimension allocate within  innermost loop assign temporal execution loop tile mapped spatial resource variable representation devise mathematical representation dnn schedule formulate schedule prime factor allocation layer specification factorize loop bound prime actor loop bound prime pad factorize assign prime factor schedule configuration compose combination decision mapped memory permutation spatial mapping prime factor exactly schedule configuration binary matrix prime factor allocation schedule dimension layer dimension variable indexed prime factor loop bound indexed spatial temporal mapping indexed memory permutation indexed prime factor decomposition CoSA encode schedule guarantee optimization solves binary matrix schedule listing CoSA performs tile optimization assign prime factor memory dimension split tile inner tile allocate input buffer outer tile allocate global buffer mapping prime factor spatial execution factor mapped spatial temporal prime factor spatially mapped finally loop permutation rank index  memory prime factor mapped rank ranked factor allocate innermost loop ranked factor allocate outermost loop dimension mapped global buffer temporal mapping factor assign rank global buffer without factor related idx IA OA related idx IA OA register     dram IV constant binary matrix encodes layer dimension associate data tensor encodes data tensor memory hierarchy global buffer factor rank become innermost loop permutation rank permutation reserve slot prime factor memory slot prime factor allocate memory constant parameter addition loop related variable intrinsic relation across component architecture layer specification encode constant parameter CoSA constant binary matrix encode unique relation dnn schedule tabel IV binary constant matrix encodes association layer dimension matrix data tensor matrix input IA output OA tensor matrix indicates layer dimension calculate data transaction multicast reduction traffic accelerator addition introduce another binary matrix memory hierarchy data tensor dnn accelerator typically deploy multi memory hierarchy memory data tensor matrix IV architecture dedicate input buffer input activation respectively global buffer input output activation CoSA constraint discus constraint derive target accelerator architecture satisfied CoSA express CoSA variable constant buffer capacity constraint generate valid schedule software manage memory constraint ensure data buffer exceed buffer capacity hardware memory hierarchy binary constant matrix earlier memory buffer tensor dimension correlation matrix calculate tile tensor relevant prime factor spatial temporal factor buffer utilization prime factor layer dimension utilization buffer express prime    otherwise upper bound buffer utilization capacity buffer MI however utilization constraint involves decision variable nonlinear infeasible standard constraint solver address limitation logarithm constraint obtain linear expression utilization encode statement UI prime   MI encode precision data tensor logarithm datatype  UI spatial resource constraint another CoSA constraint limited spatial resource chip limited PEs PE limited accumulate mac CoSA factor assign spatial mapping configuration satisfy factor mapped spatial temporal execution factor spatial execution exceed resource limit architecture constraint express equation prime  SI SI available spatial resource objective function objective function CoSA objective individually optimize aspect performance utilization compute communication combine others utilization driven objective chip buffer utilization improves data reuse opportunity demonstrate prior communication bound achieve tile optimize buffer utilization cache formulate utilization objective aim maximize buffer utilization tensor overall communication minimize formulation buffer utilization maximize linear utilization function  maximize sum utilization buffer tensor logarithm equivalent maximize geometric buffer utilization user attach buffer data tensor optimize utilization specific memory compute driven objective compute cycle another factor affect quality schedule formulation temporal factor estimate compute cycle PE intuitively objective allows constraint solver exploit parallelism mapping iteration spatial resource temporal iteration objective express linear function logarithm comp prime  traffic driven objective communication latency contribute factor performance spatial architecture CoSA traffic driven objective capture communication specifically communication traffic decompose data per transfer spatial factor multicast unicast traffic temporal iteration factor amount traffic network discus capture factor CoSA representation buffer utilization expression data per transfer compute allocate prime factor matrix dimension tensor correlation matrix equation prime  spatial factor incur multicast unicast reduction dimension tensor correlation matrix sec traffic patter specifically spatial dimension binary matrix related specific tensor consideration constant matrix traffic multicast unicast reduction unicast intrinsic tensor dimension correlation matrix calculate traffic variable dimension mapped spatially AP implies multicast traffic tensor related global buffer PEs traffic  destination PEs dimension mapped spatially AC implies unicast traffic tensor related similarly dimension mapped spatially AC OA implies reduction traffic output tensor OA partially sum reduce across GB dimension mapped spatially AP OA traffic constant matrix constant encodes traffic multicast unicast reduction data tensor global buffer PEs implication output tensor reduction traffic unicast traffic output tensor OA traffic contributes output CoSA formulates relationship equation prime  temporal iteration calculate data transfer noc introduce traffic iteration factor function permutation indicates outer noc loop bound variable ensure variable relevant factor inside loop loop factor compute traffic iteration regardless related data tensor variable reuse optimization mathematically constrain  index permutation valid permutation traffic iteration express prime  linear objective quadratic factor permutation calculate individual combine tensor contributes traffic network logarithmic transformation earlier instead logarithm linear expression traffic equation raf overall objective construct composite objective comprise linear combination  comp raf minimize compute communication latency maximize chip buffer utilization   comp raf user parameter importance objective  optimization traffic cycle memory access brings raf importance comp optimization another formulation overall objective function balance memory access compute cycle minimize difference Dˆ raf comp objective micro benchmark characterize compute memory communication latency target architecture limitation CoSA CoSA leverage regularity architecture assumes dense cnn workload exploit sparsity data target hardware deterministic behavior explicitly manage scratchpad nondeterministic behavior challenge construct optimization objective capture impact behavior however CoSA augment iterative objective function correspond hyperparameters approximate unknown hardware performance model directly prune invalid IV methodology discus evaluation platform experimental setup CoSA evaluation evaluation platform evaluate schedule generate CoSA platform timeloop cycle performance consumption cycle noc simulator overall latency performance latter accurately capture communication overhead concurrent hardware behavior spatial architecture timeloop microarchitecture  model estimate performance dnn accelerator timeloop report performance maximum cycle processing workload perform arithmetic storage network MACs PE register PE dimension input  buffer KB PE router wormhole precision buffer KB PE flit partial sum input buffer KB PE rout precision global buffer KB multicast baseline dnn accelerator architecture memory access assume perfect latency hiding buffering consumption timeloop calculate access hardware component per access sum access infer schedule per access reference timeloop noc simulator augments timeloop analytical compute model PEs synthesizable noc implementation reflect communication communication contribute factor latency noc communication bound schedule noc simulator transaction cycle model chip traffic leverage synthesizable  router  unicast multicast request construct  mesh network implement rout scheme simulator capture computation communication latency concurrently model data transfer noc PE execution chip dram access DRAMSim model impact traffic congestion noc manifest baseline scheduler evaluate CoSA respect schedule scheme random scheduler valid schedule target metric timeloop hybrid mapper timeloop randomly selects tile factorization prune superfluous permutation linearly explores prune subspace mapping proceeds random factorization mapper default termination thread terminates consecutive mapping valid sub optimal mapper thread independently schedule termination met thread terminate timeloop return schedule obtain valid schedule setup mixed integer program mip solver CoSA gurobi purpose optimization solver mip constrain program solver specify CoSA variable constraint objective function invoke solver solver return schedule dnn layer dnn workload performance  schedule dnn workload target dnn task diverse layer dimension resnet resnext deepbench ocr recognition precision benchmark input partial sum pad dimension multiple incurs overhead outweighs benefit schedule option baseline architecture spatial array architecture simba baseline detailed specification hardware construct summarize demonstrate CoSA framework apply architecture parameter deliver highperformance schedule option shot evaluation demonstrate improve performance CoSA baseline scheduler across evaluation platform dnn architecture diverse dnn layer average CoSA baseline scheduler generate schedule layer target dnn workload VI CoSA optimization driven approach advantage timeloop hybrid strategy timeloop hybrid sample schedule per layer evaluate valid runtime random random sample sample valid schedule demonstrate constraint strategy prune invalid directly CoSA shortens generates quality schedule CoSA random timeloop hybrid avg runtime layer avg sample layer avg evaluation layer VI comparison CoSA output valid schedule per layer CoSA runtime shorter random timeloop hybrid respectively speedup schedule relative random baseline noc architecture axis label convention stride workload CoSA achieves geomean speedup across dnn workload random timeloop hybrid improvement network report timeloop model estimation normalize random evaluate baseline noc evaluation timeloop performance model performance random timeloop hybrid mapper CoSA scheduler dnn workload evaluation baseline architecture described timeloop evaluation platform mention IV performance speedup report timeloop schedule scheme relative random demonstrates CoSA generate schedule valid outperform generate random timeloop hybrid geometric speedup CoSA schedule relative random timeloop hybrid respectively across dnns layer timeloop hybrid slightly outperforms CoSA iteration objective function breakdown resnet layer goal minimize objective CoSA achieves objective function layer approach dram timeloop hybrid schedule reduce dram transaction balance pipeline tune objective function improve CoSA generate schedule exhaustive timeloop hybrid valid schedule improvement latency increase runtime valid sample evaluate timeloop hybrid cannot generate schedule efficiency CoSA timeloop model evaluate schedule highly correlate access hardware component traffic objective CoSA schedule optimization target efficiency demonstrates CoSA simulation feedback generate schedule efficient timeloop hybrid PEs buffer speedup relative random report timeloop model hardware architecture CoSA performance generalizes across hardware architecture compute chip storage resource speedup report noc simulator relative random baseline noc architecture CoSA achieves geomean speedup across dnn workload random timeloop hybrid communication sensitive noc simulator valid schedule optimize objective breakdown detailed breakdown CoSA objective function resnet layer overall objective function aim capture optimization heuristic maximize utilization minimize compute traffic sum CoSA achieves objective approach optimizes sub objective simultaneously observation objective aligns empirical CoSA schedule faster generate random timeloop hybrid HW architecture explore performance CoSA dnn architecture parameter PE array SRAM buffer apply evaluation architecture customize objective eqn micro benchmark architecture geomean speedup CoSA across network hardware architecture PE array dimension PEs increase chip communication dram bandwidth correspondingly modification significantly impact compute communication dnn layer execution spatial array arithmetic schedule decision spatial temporal mapping crucial attain performance CoSA achieves speedup random timeloop hybrid respectively across network performance scheduler generalize NoCs PEs tend speedup relative tvm report gpu affected communication SRAM increase local global buffer demonstrate CoSA achieve consistently schedule across architecture local buffer accumulation input buffer global buffer increase modify memory capacity PE global buffer likely impact optimal strategy data noc communication traffic reduction CoSA speedup random speedup timeloop hybrid demonstrate CoSA capability across architecture evaluation noc simulator quality schedule generate schedule scheme evaluate noc simulation platform noc simulation platform accurately capture communication overhead onchip network timeloop model speedup relative random baseline CoSA generate schedule outperform baseline schedule dnn workload performance gain convolutional layer deepbench layer  layer timeloop hybrid scheduler actually performs random internal analytical model accurately capture communication traffic network significant difference performance FC layer schedule FC layer heavily memory bound PE utilization dram access dominates layer schedule respect reuse buffer data overall CoSA achieves geometric average speedup relative random relative timeloop hybrid schedule across network furthermore unlike iterative random timeloop hybrid schedule CoSA schedule consistently performant shot evaluation gpu potential CoSA purpose hardware formulate gpu schedule  CoSA evaluate performance CoSA gpu tvm target gpu target nvidia gpu cuda core MB cache gpu KB memory KB local register maximum thread cuda thread thread program abstraction thread serially parallel cuda maximum dimension thread violate constraint cuda kernel invalid schedule constraint CoSA express hardware constraint gpu thread local memory similarly specify spatial resource buffer capacity constraint thread spatial specific thread enforce memory utilization calculate buffer capacity constraint register utilization calculate thread inner loop register utilization objective function CoSA compute compute objective discounting compute cycle thread gpu reflect performance gain thread parallelism adjust objective micro benchmark tvm xgboost tuner trial per layer baseline CoSA generates valid schedule shot shorter tvm per layer CoSA generate schedule achieve geomean speedup tvm schedule resnet VI conclusion CoSA optimization driven approach dnn schedule harness regularity dnn workload target accelerator formulate schedule constrain optimization directly without incur iterative schedule devise mathematical formulation simultaneously optimization schedule loop tile loop permutation spatial mapping schedule generate stateof approach achieves speedup efficiency shorter 