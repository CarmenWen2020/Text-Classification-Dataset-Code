recently recurrent neural network popular memory lstm network achieve spectrum application domain autonomous processing sentiment analysis epidemiology due complex feature task lstm model become increasingly complicate enhance ability prediction accuracy however depth characterization purpose accelerator lstm training execution grows inefficient storage performance consumption increase model algorithmic architectural analysis identify lstm training inefficiency massive intermediate variable enable highly efficient lstm training model exploit unique  performance improvement opportunity lstm training procedure leverage propose stack training lstm lstm model lstm comprises software  innovation effectively memory footprint upper bound excessive data movement lstm training drastically improve training performance efficiency experimental lstm training benchmark demonstrate lstm reduces memory footprint average brings data movement matrix activation data intermediate variable respectively furthermore outperforms gpu implementation lstm training average performance partially research laboratory  defense advanced research project agency darpa agreement FA darpa src ada research partially facebook faculty award sydney faculty startup funding australia research council arc discovery project DP research partially NSF grant CCF CCF CCF career shed logic utilization future NPUs index machine neural net recurrent neural network accelerator introduction recent machine algorithm artificial neural network unprecedented growth adaptation social impact recurrent neural network popular memory lstm network achieve application domain autonomous processing business management sentiment parse recent task address covid pandemic however due complex feature task lstm model become increasingly enhance ability prediction accuracy previous mainly focus improve execution efficiency lstm inference enable highly efficient training lstm model detailed characterization lstm training analysis inefficiency challenge sec training lstms frequently hardware throughput efficiency identify profile gpu utilization memory subsystem related overhead clue purpose accelerator conduct algorithmic architectural analysis discover lstm training inefficiency massive intermediate variable UI OOVBM  PNQVUFS SDIJUFDUVSF acm annual international symposium computer architecture isca doi isca variable generate FW propagation reuse backpropagation BP gradient calculation due reuse distance typically dram FW release chip resource variable shorter reuse distance analysis intermediate variable lstm training negative impact storage memory footprint performance training latency throughput efficiency recent focus reduce memory footprint induced intermediate variable  execution vDNN  cnns echo transformer however lstm training exhibit unique complex computation precludes previous approach apply sec enable highly efficiency lstm training model exploit unique  performance improvement opportunity lstm training leverage propose stack training lstm pronounce lstm comprises software hardware innovation software lstm primarily focus reduce intermediate variable generate FW propagation via tackle factor intermediate variable lstm variable reduction sec IV BP layer reduction sec  correspond observation lstm training intermediate variable posse limited opportunity compression execution reorder memory footprint training latency effectively reduce BP significant gradient update skip execution insignificant BP reduce memory footprint data movement training latency hardware propose hardware optimization effectively software innovation architecture enhancement performance efficiency exploration discover propose memory optimization hardware utilization perform NN accelerator inefficient execution increase training sec address challenge propose novel PE omni PE sec perform operation lstm training runtime resource allocation scheduler sec dynamically distributes computational resource accord lstm workload request hardware utilization finally detailed overall hardware along optimization lstm sec contribution conduct comprehensive characterization lstm training gpus identify   lstm FW layer lstm BP layer FW FW FW BP BP BP     unroll unroll context context gradient execution lstm layer FW propagation BP phase timestamps training inefficiency exploit unique memory performance improvement opportunity lstm training leverage propose highly efficient cro training lstm model comprises software hardware innovation software introduce observation regard unique lstm training data leveraged enable reduction memory footprint data movement training latency hardware propose novel architecture enable logic utilization customize NPUs software optimization enhance lstm training performance efficiency experimental lstm training scenario pure software memory optimization reduce memory footprint average reduce data movement matrix activation data intermediate variable respectively return memory technique improve stateof gpu nvidia 2GB implementation performance convergence issue negligible accuracy impact pure hardware architecture achieve average gpu combine overall lstm surpasses gpu implementation average performance II background lstm training memory lstm network popular recurrent neural network rnn adopt application supervise network convolution neural network cnns lstm training comprises FW backpropagation BP procedure however lstm network exhibit unique execution cnns cnn network tanh FW matmul FW EW tanh      BP matmul BP EW      tanh  tanh  zoom lstm execution training FW propagation BP illustrate within layer operation mapping input output integrate FW operation mapping output gradient gradient input gradient integrate BP execute recurrently adopt historic output information feature model context dependency within input activation model sequence task model trajectory prediction etc layer input consume sequential execution sequential execution lstm layer unrolled sequence FW BP timestamps unrolled unrolled within layer matrix adjacent FW BP exhibit context dependency sequentially zoom actual computation FW BP lstm tth timestamp FW  FW matmul wise operation FW EW leverage layer input context output previous multiple lstm  forget gate input gate gate output gate computation function  AF   focus lstm unroll analysis layer unrolled timestamp AF activation function sigmoid hyperbolic tangent tanh matrix respectively compute towards gate offset leverage another wise operation  generates context link output layer memory intermediate parameter future usage FW BP gate gradient  gradient previous  generate perform wise operation BP EW parameter feedback output gradient  layer context gradient  timestamp gradient  gate timestamp calculate previously FW  gradient apply generate gradient inner matrix multiplication BP matmul previous layer timestamp  layer previous timestamp    WT UT  gradient gradient outer  BP matmul    CHALLENGES lstm training due complex feature task neural network model exponentially enhance ability prediction accuracy additionally network compression prune technique training NN model non linearity fitting pinpoint reduce model redundancy inevitable lstm model expand hidden determines matrix layer per layer layer indicates model depth previous mainly focus improve execution efficiency lstm inference technique enable highly efficient training lstm model remain unexplored detailed characterization lstm training analysis inefficiency challenge initial characterization lstm training generalpurpose accelerator gpus become increasingly popular purpose accelerator perform neural network training conduct comprehensive characterization theart gpus 6GB nvidia quadro RTX gflops watt tflops hidden RTX throughput throughput RTX efficiency efficiency gflops watt tflops layer RTX throughput throughput RTX efficiency efficiency gflops watt tflops layer RTX throughput throughput RTX efficiency efficiency performance efficiency comparison training lstm gpus nvidia RTX turing architecture volta architecture lstm model increase hidden layer layer turing architecture 2GB nvidia tesla volta architecture evaluate lstm training efficiency increase model pytorch implement lstm training algorithm various model configuration apply nvidia profiler extract gpu runtime information execution latency consumption utilization detailed experimental setup sec VI typical increase lstm model increase hidden increase layer increase layer illustrates comparison throughput efficiency lstm training lstm model increase hidden layer layer observation impact hidden impact hidden lstm training efficiency implement lstm model task model penn treebank PTB dataset fix layer layer dataset hidden increase gpu throughput increase plateau matrix multiplication matmul computation task lstm training throughput increment directly originates matmul optimization gpus specifically model hidden increase model demand thread creates opportunity parallel execution alu utilization however model alu saturate throughput improve however efficiency decline increase model beyond throughput saturation indicates increase consumption due memory activity impact layer demonstrate impact layer lstm training efficiency implement lstm model PTB model fix hidden layer hidden although gpu throughput varies layer correspond efficiency decrease suggests LN LN LN LN LN LN LN ave data movement GB parameter activation intermediate variable data movement parameter activation data intermediate variable layer significantly impact overall throughput extra overhead training due native memory limitation layer lstm model cannot 6G RTX gpu impact layer lstm layer associate dataset cannot easily tune configuration hidden layer illustrate impact increase layer model depth lstm training efficiency implement lstm model datasets fix hidden layer demonstrates increase layer overall throughput tends decrease efficiency suggests longer layer negative impact throughput efficiency summary training lstms frequently hardware throughput efficiency identify profile gpu utilization nvidia profiler load  utilization significantly increase lstm model memory subsystem related overhead hint investigate inefficient lstm training lstm training inefficiency computation phase FW EW operation lstm generates intermediate variable conduct lstm inference intermediate variable abandon shortly computation however lstm training processing LN LN LN LN LN LN LN ave GB normalize footprint parameter activation intermediate variable breakdown gpu memory footprint due chain variable reuse BP computation backpropagation typical BP execution completion FW computation reuse distance variable ofthe lstm implementation commonly dram release chip resource variable reuse distance characterization analysis discover intermediate variable training negative impact lstm training storage performance inefficiency storage inefficiency illustrates breakdown memory footprint aforementioned lstm training scenario portion runtime data matrix activation data intermediate variable correspond lstm training model configuration hidden LN LN correspond lstm training model configuration layer correspond lstm training model configuration layer average memory footprint contribute intermediate variable easily surpass gpu chip memory capacity frequent access training random memory access sub memory access overhead additionally due storage overhead lstm training easily bound native hardware storage limitation instance layer lstm model cannot 6G RTX gpu inefficiency substantially hinders realworld lstm model implementation performance inefficiency gpu throughput decline lstm training layer memory footprint intermediate variable significantly lstm model longer layer memory access variable usually critical training training performance degradation decrease hardware throughput due significant memory access overhead layer associate specific training dataset task become increasingly complex lstm layer somewhat inevitable expand cumulative absolute distribution FW intermediate variable BP EW training epoch capture context input inefficiency data movement chip chip memory access intermediate variable training directly contribute aforementioned efficiency decline instance quantifies amount gpu dram data movement activation data intermediate variable respectively dram access reduce matrix runtime exhibit data benefit compression average additional data movement intermediate variable activation data moreover intermediate variable grows faster activation data ineffectiveness technique focus reduce memory footprint intermediate variable backward propagation execution vDNN cnns  cnns echo transformer core concept trading memory footprint computation carefully balance memory consumption upperbound training performance instance reduce memory footprint echo observation attention layer transformer partial intermediate variable propagation derive intermediate variable backpropagation echo amount partial variable backpropagation computation calculate gradient however II lstm training exhibit unique computation intermediate variable independent cannot derive without essential memory opportunity echo becomes ineffective lstm training extreme minimize memory footprint without intermediate variable entire FW recomputed scratch BP processing infeasible substantially extend propagation latency serious performance overhead lstm training additionally lstm acceleration approach mainly focus compression prune lstm inference FW BP EW dram compression index decode BP EW  reorder matmul computation skip diagram variable reduction execution reorder reduce matmul computation lstm inference workload propose transform matrix circulate format compression however approach cannot reduce intermediate variable lstm training address aforementioned inefficiency IV unique lstm memory saving   enable highly efficiency lstm training model exploit unique  performance improvement opportunity lstm training procedure leverage propose stack training lstm comprises software sec IV hardware sec innovation introduce observation software layer lstm effectively reduce intermediate variable massive data movement FW BP lstm training specifically focus optimize factor intermediate variable variable sec IV within layer layer sec IV reduction intermediate variable attempt reduce intermediate variable specifically focus compress intermediate variable generate FW variable typically cannot immediately consume BP accelerate calculation gradient ofthe implementation comprise memory footprint upper bound entire lstm training investigation computation observation intermediate variable generate FW posse limited opportunity effective compression intuitive quickly consume variable FW important computation outcome instead fully training efficiency issue sec intuition reorder lstm training execution quickly effectively consume FW intermediate variable maintain computation correctness surprisingly discover reorder creates variable exhibit data compression opportunity effectively compress FW BP significant memory footprint latency reduction lstm training detailed discussion variable reduction enhance data compression opportunity via execution reorder explore opportunity variable reduction FW intermediate variable lstm training benchmark evaluate investigate distribution BP EW computation computation stage BP EW performs computation rely FW intermediate variable BP EW output  calculate gradient backpropagation formula calculate input gate gradient  generate FW procedure belongs BP EW computation involve gradient belongs BP  cumulative absolute distribution FW generate intermediate variable calculate BP EW phase training epoch axis absolute FW intermediate variable BP EW phase FW intermediate variable BP EW within formulation sec II FW intermediate variable activation function around FW intermediate variable BP EW phase depends FW intermediate variable computation generates approximately data output data FW intermediate variable data impact training epoch  computation relies FW intermediate variable hardware resource available reorder lstm training BP EW phase FW phase execute concurrently immediately consume FW intermediate variable benefit associate BP  data significantly enhance data compression opportunity reduce lstm training memory footprint upper bound bound FW intermediate variable apply zero prune threshold around memory saving training accuracy loss sec VI discussion reorder BP EW generate compress replace FW intermediate variable request BP processing FW generate intermediate variable longer BP processing memory footprint reduction additionally BP decode format output BP EW FW skip unnecessary computation gradient magnitude timestamp BP implement loss lstm imdb per timestamp loss lstm WMT mlperf BP EW BP matmul zero operand overall execution reorder strategy illustrate hardware sec BP layer reduction variable reduction extend exploration layer additional memory footprint reduction latency improvement opportunity specifically focus analyze demand BP intermediate variable FW reduce BP demand reuse computation FW generate intermediate variable intuition evaluation observation BP significant gradient update therefore propose predict insignificant BP skip execution skip intermediate variable correspond FW perform lstm inference calculate loss reduce memory footprint training latency elaborate observation propose technique observation lstm layer unrolled sequence lstm BP layer generate gradient update matrix explore characterization gradient generate BP layer gradient layer lstm training benchmark imdb WMT mlperf magnitude accumulate absolute scalar data interestingly loss calculate lstm model designer impact BP skip instance identify lstm model loss compute BP BP BP BP BP BP partial partial partial partial partial factor BP BP BP partial partial factor loss lstms per timestamp loss lstm diagram BP skip BP execution BP skip approach loss lstm per timestamp loss lstm respectively loss lstm per timestamp loss lstm former lstm model calculate loss timestamp layer imdb model review attitude classification latter lstms calculate loss per timestamp BP layer lstm model mlperf perform machine translation demonstrates lstm model model exhibit gradient magnitude loss lstms imdb gradient magnitude per layer decrease loss vanishes increase propagation distance per timestamp loss lstms WMT gradient magnitude grows layer loss correspond timestamp loss information accumulate per timestamp loss lstm generates insignificant gradient BP prediction insignificant BP leverage observation propose skip execution insignificant BP effective mechanism identify BP insignificant gradient update futile attempt perform identification BP execution aim identify BP ahead execution via loss information determines magnitude gradient BP furthermore gradient exhibit correlation propagation distance timestamp gradient increase layer layer layer gradient decrease loss lstms increase per timestamp loss lstms timestamp timestamp model information loss layer timestamp input predict gradient magnitude BP loss LN  timestamp loss loss accumulation timestamp timestamp loss lstms loss loss timestamp layer LN layer layer lstm model respectively  timestamp BP location lstm graph FW matmul matmul EW FW EW BP EW BP matmul BP  FW execution BP execution  execution logic utilization idle EW diagram inefficient computational resource allocation implement lstm training architecture static computational resource allocation indicates gradient magnitude trend accord lstm loss lstms  loss lstms finally factor lstm model dataset calculate training epoch predict magnitude threshold importance BP however reduce intermediate variable load without reduce actual memory footprint variable strategy loss information importance BP loss FW execution therefore obtain BP importance prior FW execution propose predict loss training epoch historic loss pred       pred  predict loss nth epoch loss actual loss generate previous epoch epoch perform prediction prediction historic loss previous epoch loss predict importance BP predict FW execution avoid data movement memory footprint insignificant variable convergence aware BP execution skip reduce impact skip BP execution maintain convergence propose offset loss BP lstm layer generate gradient loss potentially offset via amplify gradient significant BP   actor factor predict gradient skip BP lstm architecture discus hardware optimization effectively software innovation enhancement enable highly efficient lstm training exploration discover propose memory optimization induce hardware utilization perform NN accelerator inefficient execution increase training sec address challenge propose customize highly efficient lstm training architecture comprise novel PE cycle adder input adder input adder output sum processing timing processing  accumulator float accumulate sec runtime scheduler sec finally overall lstm architectural illustrate sec hardware challenge induced  optimization irregular workload previous memory optimization computation BP FW reduces computation workload per BP sec II amount matmul EW operation varies across becomes computation phase FW BP additionally reduction computation workload BP relies runtime information amount matmul EW operation across training iteration training epoch datasets workload becomes irregular  optimization hardware ineffectiveness NN accelerator architectural accelerate NN execution NPUs lstm inference accelerator adopt unified processing logic perform essential functionality multiplication accumulation activation logic utilization commonly statically distribute computation resource hardware module matmul EW module hardware module conduct operation logic resource per hardware module amount operation however static allocation approach inefficient execution workload becomes irregular memory optimization  lstm training EW execution BP becomes sparse correspond hardware logic idle logic utilization degrade performance omni processing achieve hardware utilization correspond performance benefit propose dynamically distribute computational resource accord workload request hardware computational resource grain processing PEs enable flexible resource allocation propose input queue input queue controller output queue partial output queue accumulator omni PE novel universal PE omni PE conduct processing operation involve lstm training specific aim reduce PE resource consumption via maximize logic reuse matmul EW operation matmul computation multiplier accumulator input cycle EW computation multiplier adder activation function input activation function implement chip storage logic difference matmul EW difference accumulator adder accumulator adder perform addition operation cycle accumulator sum cycle input accumulate previous cycle however adder implement execution unable accept input cycle processing perform addition adder multiple cycle cycle periodically stall accumulation addition output therefore accumulator adder usually exhibit enable processing cannot directly perform functionality enable accumulation adder rearrange computation instead accumulation previous cycle leverage partial outcome achieve processing illustrate simplify discussion assume latency cycle accumulation computation conduct cycle input cycle addition partial output later cycle partial output adder input adder effectively pipelined accept input cycle processing finally data input cycle partial accumulate sum adder conduct accumulation propose omni PE perform matmul EW operation illustrate omni PE contains multiplier adder multiplexer mux insert multiplier adder  flip per cycle ensure data evenly transmit input adder  input adder PE input multiplier output adder output partial output queue accumulation finally mux employ output queue appropriate output multiplier adder signal generate controller omni PE dynamically configure perform operation lstm training matrix vector multiplication multiplier adder activate adder accumulator output mux partial output queue wise multiplication  multiplier activate output output queue mux skip adder wise addition adder activate input PE mux partial output queue runtime resource allocation explore runtime resource allocation RA scheduler concurrently launch data dependence operation matmul EW intelligently assigns omni PEs operation optimal performance efficiency RA scheduler estimate amount workload operation entire lstm training model configuration estimate workload matmul EW scheduler PEs configure perform matmul EW operation respectively lstm training PEs become idle assign operation due data dependency another operation improve PE utilization idle PEs firstly reassign conduct operation input switch later perform operation originally assign scheduler PEs perform matmul EW operation propagation EW operation matmul PEs assign EW matmul swing PEs return EW adequate input generate matmul exists pipeline stall swing PEs effectively avoid dependency reassign PEs EW matmul output generation efficiently overlap execution kernel PE assignment functionality switch perform controller illustrate overall lstm architecture lstm accelerator omni PEs channel parallelism enhancement channel controller omni PE omni PE omni PE  module sigmoid tanh  dram scratchpad lstm controller dma swing swing overview lstm architecture micro architecture diagram channel architecture sparse compression module WT index queue WT data queue interface RD index queue decoder module RD data queue sparse channel output channel input data request customize dma module complexity reduction expand concept swing individual PE functionality runtime resource allocation swing channel functionality PEs swing channel additionally customize memory access dma module scratchpad lstm controller perform RA scheduler detailed described channel architecture channel compose omni PEs channel controller manipulates PEs controller within channel channel controller responsible perform data communication chip scratchpad operation outer PEs request input data improve data distribution efficiency insert broadcasting queue inside channel controller data PEs additionally apply activation module inside channel calculate activation function sigmoid hyperbolic tangent tanh reduce overhead activation module contains sigmoid hyperbolic tangent omni PEs workload activation operation operation lstm adopt lookup avoid complex logic sigmoid hyperbolic tangent tanh customize dma module enable intermediate variable reduction skip execution insignificant BP customize dma module data compression module decoder module dma module receives data channel identify data dense sparse dense data WT data queue sparse data compression module perform prune accord magnitude compression module lstm training benchmark abbr hidden layer trec QC PTB LM imdb SA waymo AD WMT MT babi QA output important data index WT data queue WT index queue respectively finally data WT data queue WT index queue chip scratchpad chip dram via interface dma module receives data request channel dense data load via interface RD data queue distribute lstm controller sparse data index RD data queue RD index queue respectively sparse intermediate variable unimportant computation opportunity reduce data request involve dense variable load data important computation dense variable introduce decoder module index information sparse operand correspond address operand finally operand RD data queue channel training execution scalability discussion channel architecture SIMT execution channel PEs explicit execution dependency channel eliminate swing channel lstm highly parallel execution channel  achieve linearly increase throughput within constraint thermal additionally strategy enables drastic intermediate data reduction software consumption hardware memory linearly channel lstm exhibit scalability VI evaluation experimental methodology training benchmark employ representative lstm application training benchmark application unique lstm model configuration hidden hidden layer layer layer trec performs segmentation QC classifies category PTB penn treebank widely model LM imdb performs sentiment analysis SA predicts positive negative attitude text waymo commercial lstm model perform autonomous AD WMT representative lstm model mlperf benchmark performs german english machine translation MT babi trec PTB imdb waymo WMT babi ave speedup baseline MS MS combine MS lstm inf static arch dyn arch trec PTB imdb waymo WMT babi ave normalize consumption baseline MS MS combine MS lstm inf static arch dyn arch speedup normalize consumption lstm performs QA automatic text understand environment setup baseline software optimization lstm training implement pytorch popular source machine framework dynamic computation graph stateof gpus conduct float lstm training moderate batch evaluation hardware implement hardware optimization sec verilog synthesize xilinx vertex ultrascale HBM VCU fpga evaluation mhz chip memory access evaluation adopt HBM IP interface external HBM memory execution latency obtain synthesis consumption evaluate vivado analysis route hardware resource difference nvidia gpus fpga VCU 8GB HBM memory 2GB HBM bandwidth evaluation implement channel VCU HBM bandwidth 4GB nearly quarter nvidia computational capability memory resource assemble lstm training accelerator VCU perform lstm training training workload per latency consumption accordingly comparison evaluate effectiveness  scenario baseline gpu accelerate lstm training execution MS variable reduction optimization IV implement stateof gpu MS BP computation reduction mechanism IV implement gpu combine MS combine software memory optimization sec IV implement gpu lstm normalize trec PTB imdb waymo WMT babi ave efficiency baseline lstm inf static arch dyn arch efficiency comparison across hardware scenario matrix DM activation data DM  variable DM trec PTB imdb waymo WMT babi effectiveness memory technique data movement reduction matrix activation data intermediate variable inf lstm inference accelerator static arch architectural lstm training computational resource statically distribute across hardware module conduct operation distribution trec configuration dyn arch architectural lstm dynamic resource allocation without software memory optimization experimental effectiveness performance consumption illustrates normalize performance consumption lstm scenario evaluate effectiveness  optimization variable reduction MS achieves average speedup improvement baseline BP computation reduction MS achieves average speedup improvement baseline MS effective lstm training hidden MS effective lstm training layer lstm hidden opportunity MS trivial intra variable trivial BP identify lstm layer achieve performance overall optimization achieve average speedup improvement baseline evaluate effectiveness architectural lstm inf average decrease performance incurs additional lstm inf suffers resource consume PE significantly throughput resource limitation additionally lstm inf adopts static computational resource allocation scheme trec dataset allocation strategy lstm inf efficient lstms omni PE static arch average performance decrease additional overhead baseline although improve lstm inf suffers static computational resource allocation issue lstm inf dyn arch outperforms baseline performance improvement efficiency baseline lstm inf static arch dyn arch efficiency lstm inf baseline efficiency static arch varies lstm benchmark lstm workload chip resource distribution efficiency  outperform baseline however  outperform baseline achieve average efficiency finally lstm advantage software hardware optimization outperforms baseline speedup improvement effectiveness data movement reduction effectiveness memory optimization data movement reduction MS average reduces data movement matrix intermediate variable respectively MS impact reduce activation data movement MS average reduces data movement matrix activation data intermediate variable respectively MS outperforms MS data movement matrix intermediate variable MS unique opportunity deduct activation data movement overall optimization reduce data movement involve lstm training layer lstm outperforms baseline average data movement reduction matrix activation data intermediate variable respectively effectiveness memory footprint reduction evaluate effectiveness memory optimization memory footprint reduction optimization efficiently reduce memory footprint specifically MS MS achieve average memory footprint reduction respectively overall integrate memory optimization achieve average reduction memory footprint baseline MS MS baseline MS MS baseline MS MS baseline MS MS baseline MS MS baseline MS MS memory footprint matrix activation data   imdb waymo TDD babi effectiveness memory technique memory footprint reduction II accuracy impact trec PTB imdb waymo WMT babi baseline ppl mae bleu combine MS ppl mae bleu ppl perplexity mae absolute error bleu bilingual evaluation  accuracy impact memory optimization perform approximate compute therefore conduct accuracy analysis convergence accuracy difference convergence affected lstm benchmark neural network training exhibit superior ability tolerate importantly adopt convergence aware BP skip technique compensates negative impact convergence illustrate II memory optimization incur accuracy difference baseline analysis adder accumulator adder accumulator xilinx accumulator IP demonstrate lut FF xilinx IP besides reduces consumption xilinx IP xilinx IP target translate float accumulation fix accumulation consume logic resource dynamic overhead finally xilinx IP exhibit latency however target lstm training input accumulation latency overhead conduct accumulation input already latency overhead overall evaluation vii related WORKS neural network acceleration massive focus acceleration NN workload procrustes exploit sparsity cnn training conduct customize accelerator  accelerator sparse cnn training smart workload distribution dynamic variable sparsity however lstm training exhibit significantly execution cnn comparison xilinx accumulator IP adder accumulator utilization dynamic latency lut FF signal logic cycle xilinx IP training previous hardly reduce intermediate variable focus exploit software architectural accelerate lstm inference execution ese  compress matrix propose accelerator improve inference performance massive intermediate variable movement unique feature lstm training processing lstm inference fail address challenge additionally static computational resource allocation employ accelerator inefficient logic utilization memory optimization decrease overall performance memory footprint reduction explore trading memory footprint reduction computation SmartExchange proposes encode matrix format decode load chip however limited opportunity compress encode FW intermediate variable involve lstm training IV echo proposes partial intermediate variable attention layer FW processing perform compute partial variable generate entire variable however lstm training exhibit computation generate intermediate variable independent hence echo hardly apply lstm training conclusion enable highly efficient lstm training model propose stack training lstm lstm model lstm comprises software hardware innovation effectively memory footprint upper bound excessive data movement lstm training drastically improve training performance efficiency experimental lstm training benchmark demonstrate lstm significantly reduces memory footprint data movement lstm training furthermore outperforms theart gpu implementation lstm training performance efficiency