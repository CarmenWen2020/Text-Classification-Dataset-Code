efficient graph processing crucial discipline graph algorithm naturally expose massive parallelism opportunity performance limited memory irregular memory access fpga graph processor  FabGraph address memory issue scratchpad regularly dram waste bandwidth unneeded data classic cache scratchpad fail deliver FPGAs powerful  resort extreme nonblocking cache handle outstanding significantly increase ability memory coalesce multiple accelerator access dram memory request essentially latency primary concern advantage cache adaptable graph accelerator amazon aws implementation account practical aspect challenge involve multidie FPGAs classic algorithm pagerank scc graph achieve geometric speedup fpga accelerator bandwidth efficiency efficiency multicore CPUs graph gpus index graph MOMS nonblocking cache dram fpga introduction graph effective data representation wealth domain social network drug discovery genomics robot navigation efficient processing graph crucial discipline graph usually embarrassingly parallel performance graph algorithm traditional platform limited bandwidth memory node typically access irregularly gpus algorithm irregular memory access preprocessing graph achieve performance problematic dynamic graph scenario graph generate another application ASICs excellent performance customize processing pipeline memory access typical graph processing however fabrication involves  implement advanced technology node evaluate performance simulation FPGAs cannot performance ASICs available data anyone deploy immediately per fpga graph accelerator tightly integrate complex pipeline directly accessibility FPGAs comparable gpus retain hardware flexibility ASICs attractive platform accelerate algorithm divergent irregular memory access challenge irregular access vertex centric approach propose fpga latter choice recent target graph usually node indeed limit irregular memory access entire node chip memory random access external memory entirely eliminate sort source destination node node access sequential however preprocessing expensive super linear access generally irregular traditional cache effective fpga accelerator graph processing mitigate partition node tile interval access node tile fashion partition source destination interval complexity sort however transfer node granularity tile unnecessary data transfer node access iteration addition tile transfer offchip memory quadratic node redundant data transfer node transfer dominate execution node amount chip memory UI OOVBM  PNQVUFS SDIJUFDUVSF acm annual international symposium computer architecture isca doi isca processing traditional cache external memory external memory processing scratchpad external memory processing ideal performance memory access irregular cache belonging tile scratchpad identify shade traditional cache effective reuse distance rarely largescale graph workload statically manage scratchpad strictly access transfer efficiently tile data guarantee access usually transfer data ideal infinite cache request useful cache exactly optimize memory cache direction reasonable optimize handle maximize previous optimize memory MOMS increase bandwidth DRAMs access irregular application latency insensitive MOMSes insight nonblocking cache minimize stall reuse memory response pending maximum outstanding MOMSes maximize opportunity data reuse closer ideal cache without unrealistically cache array insight throughput perspective secondary MSHR equivalent cache without stall extra memory request chip memory MOMSes efficient traditional cache latency insensitive application irregular memory access expose outstanding highly associative structure implement effectively FPGAs available source multi fpga graph accelerator intuition MOMS irregular access typical graph processing insight skewed distribution opportunity request merge node request magnitude others however node traditional cache stall frequently hurt throughput whereas MOMSes tolerate advantage dynamic grain operation MOMSes avoid redundant data transfer typical tile MOMS handle buffer destination node chip memory however statically schedule node transfer linear interval quadratic reduces node transfer overhead graph lower amount chip memory without increase crossbar MSHR buffer  buffer memory interface external memory controller data buffer MSHR buffer  buffer data buffer  PE pek architecture optimize memory MOMS request processing PEs handle consist optional cache MSHRs  handle outstanding complexity preprocessing beyond linear partition graph accelerator adaptable graph algorithm multiple processing PEs handle hardware thread per simultaneous multithreading fashion mask latency memory MOMS multidie FPGAs extra challenge essential address challenge impose modification MOMS extend implement private architecture FPGAs graph accelerator improve MOMS evaluate xilinx ultrascale FPGAs available amazon aws instance apart prior fpga simulation performance unclear dram controller physically constrain achieve speedup fpga FabGraph bandwidth efficiency CPUs ligra graphmat geometric speedup pagerank gpus gunrock II   memory SYSTEMS cache ineffective graph processing rate cache cpu graph traversal nonblocking cache mitigate impact throughput temporarily address request IDs status register MSHRs instead stall data return increase concurrent memory operation achieve memory parallelism grouped cache cache request data return pending therefore MSHRs implement  memory respective cache already request whenever cache return memory identify pending however MSHRs maximum outstanding interval graph partition graph assume partition shard respective source destination node interval usually limited intel sandy bridge haswell outstanding realistic processor benefit memory parallelism beyond limit throughput orient accelerator easily emit outstanding instead benefit dramatically increase MSHRs optimize memory MOMS extreme version  nonblocking cache outstanding MSHRs ordinary ram abundant FPGAs cuckoo hash instead fully associative lookup significantly increase memory parallelism latency contention memory leveraged maximize reuse opportunity flight cache without necessarily longer storage expensive data array cache MOMSes introduce nonblocking cache FPGAs explore implement easily pareto optimal ideally throughput orient accelerator prior MOMSes multiple accelerator however MOMS conflict severely limit throughput tackle bottleneck propose private MOMSes generally outperform MOMSes MOMSes originally propose reduce dram traffic MOMS private MOMSes reduce traffic MOMS reduces contention improves throughput addition previously evaluate MOMSes mid fpga ddr controller IV technique introduce efficiently multidie fpga ddr channel template program model iter iter max iter active srcs false false parallel across multiple PEs transfer dram bram  init  VDRAM const active srcs  local src     VDRAM      active active srcs src interval   transfer bram dram VDRAM apply  active srcs active srcs graph processing model graph consists node respectively graph undirected graph easily handle duplicate graph algorithm associate node iteratively update fix iteration convergence graph partition described detail adopt centric model iterates entire principle access source destination node arbitrary interval partition lightweight preprocessing technique arbitrary locality access node node partition disjoint interval shard shard contains source destination node interval respectively shard respective source destination interval chip memory performance irrespective access MOMS avoid buffering source node chip memory principle partition shard destination interval alone however source node partition avoid processing source interval node update previous iteration apply compression mechanism introduce  described therefore partition shard source destination node interval interval purpose EXAMPLES algorithm specific PARAMETERS template pagerank scc  OD initial VDRAM OD source const VDRAM  VDRAM min min apply local src false active false false program model template program model implement accelerator execution framework configure implement variety graph algorithm customize function init apply initial node VDRAM per node constant vector  global constant scalar const flag local src active model centric sum apply scatter generalizes model  FabGraph graph algorithm implement model purpose init function model enable additional optimization algorithm implement pagerank  instead reading PR outdegree OD irregularly recomputing normalize OD source node constant OD sequentially upon bram initialization normalize dram reduces irregular allows compute normalize per node  negligible overhead sequential memory operation iteration model synchronous asynchronous execution unlike  FabGraph latter synchronous execution VDRAM VDRAM swap iteration meaning node execution update iteration asynchronous execution VDRAM VDRAM array memory function update dram addition  VDRAM format algorithm remains partial node local src whenever source node destination local bram date version available reduce traffic dram scc satisfy requirement pagerank satisfies partial bram underestimate src dst  ptr  ddr VDRAM  pointer   graph layout memory consist node initialization compress format organize shard pointer graph encode memory layout accelerator accepts graph described coordinate format coo tuples src dst optional per preprocessing partition accord shard preprocessing complexity oppose approach sort source node sometimes implicitly graph convert csr format  complexity entire memory layout contains vertex array VDRAM  algorithm allocates memory VDRAM execution synchronous organize shard source destination node implicit shard explicitly offset within respective source destination retrieve dram usually multiple append terminate shard ensure PEs ignore data dram indeed PEs counter purpose return multiple dram channel IV destination node offset source node offset  flag per unweighted graph node compression technique  FabGraph  flag graph source destination shard arbitrary array pointer identify address shard active srcs flag shard shard template data valid queue stage shift register data valid inter circuit signal handshake crossing buffer combinational logic cycle signal propagate token register queue slot buffer ddr channel PE optimize memory burst addr crossbar burst data crossbar addr data crossbar arbiter demux ddr channel arbiter demux PE scheduler pci express burst addr burst data addr data random addr random data architecture PEs scheduler expose per destination interval burst writes node initialization writeback respective memory channel interleave byte irregular retrieve source node handle MOMS IV architecture introduce logic IV multidie aware IV IV discus detail generic multithreaded processing finally IV node reorder technique maximize workload balance PEs cache reuse logic performance FPGAs increasingly consist multiple fpga multiple dram channel physically lock specific multiple becomes crucial fully exploit available resource fpga CAD expose multidie FPGAs device handle crossing particularly scarce intra interconnection presence multiple account minimize inter connection inter connection register combinational component inter connection handshake signal logic architecture architecture target comprise fpga addr PE crossbar ddr channel arbiter demux demux arbiter demux arbiter data PE crossbar data PE crossbar ddr channel arbiter demux ddr channel arbiter demux arbiter demux crossbar data PE data PE arbiter demux addr PE addr PE addr PE addr PE crossbar ddr channel arbiter demux data PE crossbar data PE addr PE multidie aware interconnect architecture burst burst writes analogous request response rout target target resource within external memory channel latter interleave address channel byte global address PEs maximize aggregate bandwidth scheduler contains memory mapped register transfer configuration parameter node address node pointer array interface accelerator notify completion processor execution PEs scheduler arbiter associate node destination consists address  VDRAM node address VDRAM PE node address pointer node index node PE notify completion maximize resource utilization PEs scatter across multiple distinct exist PEs dram controller multidie aware MOMS random retrieve source node dereferencing source index burst writes transfer initial destination node pointer node latter perspective equivalent bus interconnect logic commonly IP vendor platform independent multidie aware interconnect logic multidie aware splitting crossbar address data address data crossbar per route transaction appropriate arbiter per transaction appropriate resource dram controller PEs request response respectively multidie aware MOMS difference private crossbar memory interface dram controller PE pek  private private  crossbar memory interface dram controller PE pek private private  crossbar memory interface dram controller PE pek  overview MOMS architecture addition MOMS propose prior evaluate private MOMSes MOMS firstly connection MOMS crossbar request PEs multiple parallel response direction logic secondly statically allocate dram channel target dram channel extra crossbar dram controller allows assign respective dram controller reduces crossing addition MOMS architecture propose previously architecture MOMSes private PE cascade private MOMS MOMS cache architecture summarize unlike MOMS private MOMS access PE without contention however increase overall traffic dram inter PE request coalesce perform architecture combine approach despite increase circuit complexity decrease parallelism frequency performance scenario PE architecture internal structure PE PE contains dma handle sequential data transfer node initialization pointer retrieval dram PE node writeback PE dram upon acceptance PE initial node destination minimize initialization dram controller expose wider node node per cycle node initialization PE request pointer respective source active PE request source node retrieve dram MOMS local bram local src active source node destination interval source node MOMS bram pipeline output pipeline modify operation destination node whenever stall logic ensure pipeline receives version destination node algorithm active false scc pipeline return update flag whenever destination node update implement template destination node memory dram PE notifies completion scheduler destination update flag exists handle efficiently response data interleave across multiple dram channel response return whenever multiple channel individual channel responds node initialize specific prevent response avoid expensive burst reorder PE issue outstanding burst initial node issue entry dma queue issue burst queue burst shorter shard necessarily multiple burst limit PE outstanding request frequently empty queue however unlike node initial burst correspond source compress format defines source node therefore tag burst request ID unique source ID return data source prefix downstream logic maximize mlp effectiveness MOMS request source node dram MOMS PE outstanding MOMS independently treat thread source node data request MOMS thread destination node offset suspend response return retrieve respective resume thread execution mechanism implement MOMS interface MOMS graph tag request unique ID  ID queue retrieve destination node offset dma ext local addr ida addr data data idd arbiter addr data data addr scheduler MOMS dst interval index src interval index local src demux sequential transfer interconnect src dst oset MOMS interface bram URAM stall logic pipeline dst oset src oset dst oset node oset shift register src local external dst architecture PE obtain fetch dma dereferencing active pointer source node fetch MOMS unless local src template enable MOMS interface associate response available node pipeline destination node update bram logic handle node initialization writeback simply implement connection dma bram init apply respectively queue memory addr data addr data addr ida idd src addr dst addr dst addr addr src addr ida dst addr idd dst addr available MOMS interface responsible retrieve associate MOMS response graph architecture queue available destination node offset memory bram unweighted graph reduces destination node offset comparable unique architecture implement optimize interface destination node offset directly without duplicate information anyway MOMS memory bram per thread  ID queue  MOMS  buffer destination node offset memory unweighted graph reduces destination node offset comparable  target simultaneous thread therefore destination offset unique ID circuit approximately   per thread directly destination offset ID MOMS entire addition maximum thread limited MOMS capacity instead capacity MOMS memory node reorder graph described coordinate format implicitly assign unique integer label node defines address node memory however label principle arbitrary affect algorithm correctness dramatic impact performance node usually cache node tightly memory improves cache rate MOMS opportunity memory response reuse indeed graph benchmark label preserve tight cluster correspond destination interval parallel therefore desirable node destination interval per interval balance respect distribute uniformly burst transfer destination interval chip memory node belonging destination interval contiguous memory  FabGraph statically schedule interval PEs source node chip scratchpad private PE therefore workload balance PEs critical cluster preservation distribution consecutive node interval compute destination interval node skewed workload distribution therefore propose hash relabeling node interval mod instead uniform workload distribution however PE balance critical dynamically schedule PEs magnitude numerous PEs PE whenever idle instead NP PEs generally achieve workload balance without  relabeling NP contrast maximize cache reuse becomes critical hash partition destroy cluster preserve label hurt cache reuse therefore cache hash entire cache destination interval orthogonally cache reorder evaluate technique introduce DBG reorder prior cache hash handle graph initial label preserve tightly community DBG coarsely partition node accord outdegree intuition cluster node cache reuse complexity graph partition cache reorder evaluate benefit technique experimental RESULTS experimental setup fpga specific assignment benchmark analyze impact PEs MOMS architecture pagerank scc analyze impact various preprocessing  impact memory channel bandwidth performance ass contribution cache array throughput performance CPUs gpus FPGAs conclude resource utilization operating frequency experimental setup rtl chisel synthesize vivado code fully parametric PEs memory channel distribution node init apply function MOMS organization dimension evaluation perform amazon aws instance feature virtex ultrascale fpga host PC via pci express GB ddr channel  proprietary ddr channel theoretical bandwidth GB however optimize burst maximum bandwidth GB per channel request  MOMS burst request memory benefit compensate correspond delay increase target fpga span SLRs xilinx terminology resource central slr reserve central slr host memory controller SLRs controller assign MOMS crossbar central slr respective memory channel slr assign PEs central slr respectively balance private MOMS exist assign slr respective PE PE destination node URAM node scc pagerank pagerank PEs  float implement apply function vivado HLS pipeline cycle latency stall handle raw hazard function scc unsigned integer implement chisel fully combinational meaning stall memory ID queue PEs slot implement bram pagerank iteration algorithm convergence benchmark synthetic graph summarize II random integer specify enable hash DBG II BENCHMARKS  benchmark WT wiki DB dbpedia link UK SK MP twitter mpi RV twitter FR com  WB  RMAT RMAT RMAT architecture exploration perform extensive exploration significant target frequency mhz discard mhz cache contains  cache MSHRs  MSHRs implement bram MOMS cache array  buffer efficiently abundant URAM private MOMSes MSHRs  output data width MOMS width dramatically increase inter rout congestion longer critical rout failure private MOMSes associative cache MOMS  architecture increase private cache timing degradation WT DB UK SK RV MP FR WB geomean  WT DB UK SK RV MP FR WB geomean  WT DB UK SK RV MP FR WB geomean  pagerank scc private MOMSes MOMSes traditional cache MOMSes traditional cache throughput pagerank scc architecture architecture label indicates PEs MOMS private cache architecture generally performance balance PE peak throughput amount conflict MOMS memory efficiency rout congestion MOMSes private cache benefit benchmark traditional cache MSHRs  per MSHR per private cache per associative MSHRs maximum frequency performance gain architecture performance geometric request MOMS conflict reuse multiple PE without extra memory request architecture without private cache generally outperform PEs inter PE conflict remain critical confirm performance MOMSes cannot benefit filter private MOMS contrast reduce conflict private MOMSes throughput prior MOMSes private MOMSes alone tend limited excessive amount redundant request however important exception SK lesser extent UK WT perform without MOMSes locality benefit elimination conflict increase memory traffic benefit traditional cache whenever PEs MOMSes frequency scc achieves throughput application pagerank throttle raw stall due cycle pipeline frequent SK UK WB overhead memory ID queue reduces parallelism operating frequency consume twice bandwidth unweighted throughput scc function cache rate cache architecture traditional architecture rate peak performance MOMSes outperform despite rate cache array critical MOMSes validate hypothesis cache array deactivate hence achieve rate traditional cache naturally lose practically performance MOMSes throughput degradation benchmark meaning MSHRs essentially replace cache array latency irrelevant preprocessing impact pagerank performance MOMS preprocessing technique trend application benchmark benefit hash node critical load balance speedup without hash suggests node destination interval rate cache throughput  DB FR MP RV SK UK WB WT private MOMS MOMS traditional MOMS traditional throughput scc versus cache rate architecture architecture completely without cache array MSHRs  traditional cache MOMSes achieve performance zero cache rate meaning cache array remove altogether essentially performance penalty benchmark WT DB UK SK RV MP FR WB avg  hash DBG hash DBG geomean pagerank throughput MOMS architecture preprocessing processing numerous another important uniform addition label preserve graph community FR MP RV  DBG initial cache significant speedup preprocessing benchmark core intel xeon exclude disk openmp parallelize operation preprocessing generally lightweight besides partition optional preprocessing runtime efficiency quickly explore preprocessing maximize performance application memory bandwidth scalability throughput function ddr channel MOMS pagerank FabGraph theoretical model described equation FabGraph estimate performance active compute optimistic estimation preprocessing  partition hash DBG WT DB UK SK RV MP FR WB ideal dram bandwidth GB per channel ignore bandwidth limitation impose aws request implementation difficulty related multidie handle raw conflict float pagerank implementation FabGraph implement pagerank integer initiation interval pipeline instead realistic identify category benchmark compute bound SK UK WB WT memory bound others benchmark category locality channel achieve peak performance limited PE parallelism pagerank raw conflict function cycle latency indeed benchmark benefit private MOMSes traditional cache pagerank compute bound benchmark decrease performance channel frequency due slr crossing SLRs performance  benchmark instead essentially linearly memory bandwidth geometric FabGraph performs memory channel ideally performance becomes limited internal bandwidth cache transfer particularly numerous graph addition simulation analysis account SLRs affect rout congestion impact cache already MOMSes cache rate achieve peak performance cache MOMSes competitive traditional cache detail impact performance cache array dramatically MSHR array scc throughput MOMS traditional cache without MiB MiB private cache respectively traditional cache throughput decrease without cache array performance MOMS geometric meaning MSHRs replace cache array difference WT DB UK SK RV MP FR WB avg  DB UK SK RV MP FR WT DB UK SK RV  WB ddr channel ddr channel ddr channel pagerank  scc   geomean propose FabGraph scalability throughput function ddr channel MOMS architecture pagerank FabGraph channel pagerank operating frequency channel due slr crossing increase congestion scc frequency constant throughput generally linearly available memory bandwidth benchmark become compute bound already memory channel SK UK WB WT GB channel memory FR MP FabGraph optimistic estimation theoretical bandwidth disregard slr related implementation issue raw conflict WT MOMS traditional DB MOMS traditional UK MOMS traditional MOMS traditional SK MOMS traditional RV MOMS traditional MP MOMS traditional FR MOMS traditional WB MOMS traditional MOMS traditional MOMS traditional MOMS traditional geomean MOMS traditional throughput  cache private private cache throughput scc MOMS traditional cache without private cache MOMS contribution cache array MOMS throughput essentially negligible private cache throughput indeed cache MOMS essentially performance traditional cache despite memory cache generally useful private SK posse significant private reuse opportunity benchmark cache MOMS performs MOMS private cache array cache array latency private MOMS accumulate secondary private MOMS handle response slows throughput request compete pipeline private cache array introduces contention data cache  buffer respectively MOMS response output bottleneck exist cache array absent response return however generally shadow bottleneck memory MOMS contention request response pipeline benefit presence private cache array comparison graph application architecture preprocessing combination IV memory bandwidth consumption   gpu    entire platform ext mem bandwidth FabGraph fpga GB gunrock gpu GB ligra graphmat cpu GB geometric throughput  combination performance benchmark scenario representative purpose possibly harden graph processor scenario performance achieve advantage  FPGAs architecture highly optimize specific situation FabGraph gunrock ligra graphmat whenever respective graph application available FabGraph theoretical model apply pagerank ignores raw stall gunrock nvidia tesla GB HBM memory available aws instance evaluate ligra graphmat dual socket ghz intel xeon core logical thread MT ddr channel ligra benefit DBG graph IV summarizes bandwidth consumption fpga report maximum report fpga local image api cpu intel RAPL register cpu meter measurement exclude external memory therefore comparable unfortunately obtain data gpu TDP corresponds absolute maximum consumption entire HBM memory pagerank generic architecture outperform ligra FabGraph gunrock geomean respectively geomean speedup specialized architecture increase respectively graphmat ligra DBG respectively scc architecture remain competitive cpu baseline absolute generic specialized bandwidth efficient generic specialized efficient gunrock achieves excellent performance frontier granularity node oppose source interval however GB memory benchmark amount memory benchmark FR MP resource utilization resource utilization perform LUTs ffs mostly interconnect brams URAMs PEs MOMSes DSPs underutilized float pagerank report average utilization across occupy utilization per slr factor affect  peak LUTs central slr pagerank without significantly affect operating frequency remains mhz mhz VI related CPUs graphchi core graph processing introduce concept shard confine random access cached memory introduce centric  model sort partition framework core processing graphmat galois  hybrid cpu gpu dual socket intel xeon core TDP GB memory bandwidth combine report GTEPS pagerank RMAT galois graphmat  respectively achieve GTEPS dram bandwidth galois  graph csr format WT DB UK SK RV MP FR WB geo speedup ligra DBG WT DB UK SK RV MP FR WB geom speedup graphmat ligra ligra DBG graphmat gunrock gunrock WT DB UK SK RV MP FR WB geo  pagerank scc FabGraph propose gunrock ligra graphmat ligra DBG private ligra DBG comparison cpu gpu fpga absolute competitive contender bandwidth gap platform outline IV  bandwidth efficient CPUs gpus pagerank scc gunrock bandwidth efficient DB UK benchmark lut FF bram URAM dsp scc pagerank lut FF bram URAM dsp relative utilization resource architecture application mostly limited LUTs interconnect network bram sort source node preprocessing  sort vertex approach sort faster linear partition  cache memory efficient framework sort achieves significant speedup ligra graphmat pagerank scc RV FR benchmark II architecture faster efficient gpus memory bandwidth budget magnitude FPGAs however challenge irregular workload memory access typical graph gpu simd execution model adopt  rely offline preprocessing balance workload memory access constitute relevant overhead graph dynamic gunrock shift overhead runtime  lighter offline preprocessing convert irregular graph equivalent regular node graph  achieves speedup pagerank gunrock speedup sport gunrock application ASICs magnitude rate density efficiency FPGAs significantly  fabrication graphicionado achieves GTEPS pagerank GTEPS RV graph memory bandwidth achieve GTEPS respectively GraphDynS achieves GTEPS RMAT HBM bandwidth ddr faster significantly chip memory MB MB MB fpga FabGraph graph processing fpga linear complexity preprocessing extension FabGraph focus optimize pcie transfer fpga dram cannot entire graph tackle  orthogonal efficiently processing graph dedicate dram  model FabGraph multi fpga processing  outperforms RMAT extremely sparse graph WT addition sort destination node  bfs performance TW  complex graph simulation address challenge related multidie partition affect FPGAs vii conclusion graph processing building application domain achieve performance challenge due irregular workload distribution memory access graph FPGAs attractive accessible option tackle optimize memory helpful maximize memory bandwidth utilization graph node billion demonstrate approach pagerank scc achieve geometric speedup stateof FPGAs bandwidth efficiency efficiency multicore CPUs ability graph reference gpu implementation knowledge graph processing multidie FPGAs boundary efficient graph analytics node