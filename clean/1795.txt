personalize recommendation become AI application data challenge processing personalize recommendation inference memory footprint bandwidth requirement embed layer overcome capacity limit bandwidth congestion chip memory memory processing NMP promising recent accelerate personalize recommendation proposes  NMP bandwidth increase memory capacity performance NMP internal bandwidth prior DIMM approach utilizes DIMMs achieve operation throughput however extend DIMMs eventually significant consumption due inefficient propose novel heterogeneous memory architecture efficient performance exploit compute capable 3D stack dram DIMMs personalize recommendation prior propose quantitative analysis user item interaction define locality locality reduction locality operation proportion item highly access user locality define reduction locality reusability item reduction operation locality allocates highly access embed item 3D stack dram achieve maximum bandwidth subsequently exploit reduction locality utilize remain 3D stack dram reuse partial sum thereby minimize wise reduction operation evaluation achieves performance improvement previous DIMM  leverage 3D stack dram DIMMs dram cache NMP configuration achieves average performance improvement keywords recommendation embed layer locality heterogeneous memory memory processing introduction recommendation continuously expand across commerce social networking personalize recommendation inference consume majority resource AI data leverage personalize interaction input individual item recommendation model employ embed layer lookup pool reduction operation per embed unfortunately production data magnitude embeddings performance dominate embed operation sparse stack stack stack DIMM DIMM DIMM DIMM rank rank rank rank relative operation throughput norm TensorDIMM memory capacity GB HMC opt HBM opt TensorDIMM RecNMP DLRM capacity limit bandwidth capacity scalability memory apply NMP embed layer vertical label DLRM embed vector 3D stack DRAMs HMC opt HBM opt limited capacity whereas DIMM DRAMs TensorDIMM RecNMP irregular memory access memory bound embed operation performance bottleneck conventional data platform promising technique accelerate memory intensive workload personalize recommendation  processing NMP architecture propose variety hardware platform program model prior utilize 3D stack dram attractive approach multiple dram bandwidth logic compute functionality however 3D stack DRAMs limited GBs entire embed production recommendation model recently TensorDIMM RecNMP propose specialized DIMM NMP memory GBs however due relatively bandwidth commodity DIMMs DIMM NMP multiple DIMMs achieve performance comparable 3D stack DRAMs illustrates memory capacity throughput embed operation DIMM NMP NMP optimize 3D stack DRAMs respectively 3D stack DRAMs HBM opt HMC opt bandwidth memory capacity DIMM embed increase fault lack physical memory UI OOVBM  PNQVUFS SDIJUFDUVSF acm annual international symposium computer architecture isca doi isca frequently meanwhile TensorDIMM embed DIMMs capacity scalability however throughput increase rate due relatively bandwidth DIMMs TensorDIMM extends multiple DIMMs performance improvement  DIMMs entail inefficient growth capacity overcome performance limit DIMM liu propose RecNMP NMP rank exploit rank parallelism rank additional performance improvement limited DIMM slot however limit due scalability commodity DIMM structure addition DIMM technology utilize dram chip density increase capacity DIMM 2GB however cannot contribute overall performance improvement due bandwidth limit tackle memory challenge propose sparse locality aware architecture utilizes heterogeneous memory advantage throughput 3D stack dram data storage capacity DIMMs perform detailed analysis characterization embed operation data management technique efficiently allocate embed IV evaluation identify locality embed operation locality embed operation perform embed lookup previously interact item operation phenomenon implies proportion access embed item locality generate popular item recommendation reduction locality operation embed reduction operation perform wise summation item vector embed reduction operation frequently combination item user observation define reduction locality due frequent reuse reduce item heterogeneous memory allocation framework reduction locality optimize memory locality allocates frequently access item embeddings bandwidth 3D stack dram majority rarely access embeddings capacity DIMMs propose item allocation technique leverage maximum throughput entire memory minimize 3D stack dram subsequently optimize memory bandwidth reduction locality exploit partial sum reduction multiple item reusability user partial sum access skip multiple item access wise operation faster efficient embeddings although partial sum improve performance additional memory carefully tradeoff propose allocates embed item 3D stack dram locality partial sum remain knowledge leverage processing heterogeneous memory personalize recommendation analysis embed behavior specialized hardware software summary contribution achieve bandwidth capacity heterogeneous memory bandwidth advantage utilize 3D stack dram capacity advantage density DIMMs analyze locality embed reduction operation across category recommendation datasets demonstrate locality embed operation exhibit analysis insight social phenomenon user item interaction propose embed specific data management framework hardware skip reduction operation achieves performance gain NMP 3D stack dram previously propose heterogeneous model adopt  processor II recommendation memory processing describes dnn architecture personalize recommendation model characteristic layer specifically address embed layer dominates execution recommendation model production environment limitation prior DIMM NMP performance consumption characteristic personalize recommendation model emerge personalize recommendation predict item rank personal characteristic previous interaction accurate prediction interaction user item personalize recommendation employ embed layer personalize recommendation production environment facebook recommendation model DLRM research DLRM consists embed layer fully layer FC FC simplify recommendation model item user DLRM receives user characteristic user interaction input vector user characteristic dense input personal information gender etc user interaction sparse input prefer item index dense input handle user characteristic user interaction concatenate embed layer FC layer compute intensive memory intensive FC layer item item item reduction item item item category category architecture recommendation model FC layer sparse input embed layer embed layer extract feature user prefer item embed operation item vector sparse input reduction operation merges item vector wise summation fully layer vector input vector embed layer relative wise operation access extremely irregular addition embed consists item vector vector inference embed layer fully layer operation characteristic behavior execute fully layer highly regular computational advantage leverage cpu gpu chip memory multi core processor embed layer footprint memory orient operation memory challenge conventional server environment embed exceeds chip memory chip memory conflict frequently introduce frequent chip data traffic memory embed layer contribute slowdown recommendation memory processing recommendation efficient memory production embed layer recent propose memory processing execute embed operation memory instead host processor NMP architecture chip memory conflict accelerate embed layer operation due embed memory capacity MBs GBs previous research processing module inside commodity DIMMs previous implement  NMP accelerate embed operation TensorDIMM focus mitigate pci transfer overhead embed TensorDIMM RecNMP processing SRAM cache dram array dram array embed legend embed embed layer embed DLRM DLRM reduction embed operation previous memory processing approach accelerate embed operation discrete gpu sufficient memory capacity embed DIMMs instead limited gpu memory TensorDIMM DIMMs request embed vector NMP module reduction operation FC layer TensorDIMM transfer reduction gpu recommendation model meanwhile RecNMP target cpu server NMP module per rank rank parallelism NMP rank linearly improve performance proportional rank DIMM addition NMP module RecNMP SRAM cache exploit data locality analysis DIMM memory processing accelerate embed operation aforementioned NMP approach comprise multiple DIMMs exploit DIMM rank parallelism FC embed layer exist independently layer execute parallel device memory intensive embed operation perform NMP module whereas  FC perform cpu core however layer combine transfer FC essential reduce device environment perspective DIMMs increase performance aspect bottleneck embed layer performance improvement achieve reduce execution embed operation DIMMs however performance improvement occurs bottleneck FC layer aspect NMP reduces execution delay embed layer consumption proportionally increase DIMMs DIMMs impact execution delay FC layer due memory utilization consumption proportional DIMMs perform DIMMs core cpu DLRM execute core described NMP model exploit parallelism DIMM rank DIMM NMP module neural network configuration execution embed layer various embed DIMMs network configuration  DLRM network configuration batch FC layer FC layer embed pool embed vector embed embed embed delay embed layer FC layer processing embed operation DIMM embed layer embed longer delay FC layer embed layer operation faster FC layer DIMMs emb DIMMs cannot achieve faster execution delay FC layer utilize DIMMs core expensive memory configuration amd epyc server consists core thread DIMMs rank DIMM DIMM DIMM DIMM DIMM DIMM DIMM DIMM DIMM DIMM DIMM DIMM DIMM DIMM DIMM DIMM emb emb emb emb embed execution norm FC FC embed layer execution fully layer although memory configuration DIMMs feasible DIMM imposes additional consumption consume DIMMs NMP operation embed layer cpu operation FC FC layer embed increase amount computation enlarge NMP cpu operation exhibit growth consumption meanwhile increase DIMMs NMP cpu operation DIMMs NMP operation heavily increase amount consumption DIMMs reduces execution delay owe parallel processing NMP operation cpu dramatic increase consumption DIMMs increase due increase DIMMs successfully reduce execution cpu operation increase consumption expand DIMMs accelerate NMP operation significantly increase consumption cpu operation DIMM DIMM DIMM DIMM DIMM DIMM DIMM DIMM DIMM DIMM DIMM DIMM DIMM DIMM DIMM DIMM emb emb emb emb relative consumption DIMMs embed FC FC consumption embed layer FC FC layer sparse EMBEDDING operating heterogeneous memory locality embed operation observation efficient allocation ratio HBM DIMMs reuse technique challenge apply propose technique previously heterogeneous memory locality sparse embed operation evaluate embed reduction operation across various category recommendation datasets exist locality embed operation reduction practical online service user pool user pool user pool user pool pool graph item item item item item item item item item access item sort access frequently item frequency item item item item item item item item user ID item sort access reduction item combination easy understand illustrates pool graph node embed locality item access frequency embed reduction frequency locality item production personalize inference active user access multiple item index embed model complex embed access reduction across user apply graph representation network interaction user item embed operation item index embed express node pool reduction item user creates item node user item index connects node user correspond node google amazon ebooks lastfm movielens animation locality category datasets google amazon ebooks lastfm movielens animation reduction locality category datasets access item index sort descend item frequently access item item become dominant item node exhibit characteristic popularity locality convert pool graph user item graph item combination item access user define combination repeatedly access item reduction locality verify reduction locality across multiple application evaluate item frequency combination production interaction datasets locality embed operation datasets analyze locality distribution datasets distribution implies user interested portion popular item characteristic  phenomenon item occupy access easily encounter online business discover phenomenon occurs embed operation reduction locality user item matrix gradation category imply popular item likely user simultaneously preference item user generally preference reduction locality intuition collaborative filter utilize locality heterogeneous memory propose heterogeneous memory architecture processing capability introduce technique embed locality embed distribution allocation across heterogeneous memory reduce operation utilize partial sum embed item technique heterogeneous memory allocation determines portion embed HBM DIMMs directly affect performance memory processing achieve performance exploit locality allocation relatively highly access item locality analysis utilize HBM item thereby bandwidth whereas item locality capacity relatively utilization rate therefore DIMMs item relative throughput memory request norm HBM ratio item HBM item DIMM DIMM HBM overall memory bandwidth accord heterogeneous memory allocation lastfm memory request successfully embed item HBM sort access rate axis relative throughput request HBM solely throughput HBM increase rapidly highly access item maximum throughput item maximum indicates request HBM locality item DIMMs channel conflict HBM DIMMs additional throughput item locality achieve maximum bandwidth HBM DIMMs item available bandwidth experimental environment item ratio HBM DIMMs entire dataset rate item psums HBM allocation rate psums sort frequency psum psum rate item psums accord partial sum lastfm technique reduction locality partial sum accelerate embed layer partial sum psums wise addition beforehand remove operation wise summation assume reduce item HBM already partial sum item processing module skip operation wise addition partial sum psum ratio item psums capacity psums psums item psum item psum capacity partial sum exceeds embed psums item axis ratio partial sum item psum maximum psum reduction request psum remove reduction replaces access psum access reduces elementwise summation memory access psum addition item serf embed operation combination psum significantly embed accordingly propose psum HBM II experimental locality entire dataset utilize locality item HBM HBM DIMMs item request ratio maximum throughput overall memory bandwidth improve average HBM addition psum item reduce wise summation memory access average II experimental reduction locality locality avg item allocation rate HBM avg throughput improvement reduction avg operation rate psum challenge previous heterogeneous memory previous heterogeneous memory employ combination HBM DIMMs categorize HBM hardware manage cache DIMMs addressable memory approach across HBM DIMMs memory evaluate approach memory processing challenge apply  technique heterogeneous memory architecture configuration heterogeneous memory architecture summarize kds tvm lfm mvl ani stm avg kds tvm lfm mvl ani stm avg kds tvm lfm mvl ani stm avg  pom memory bandwidth HBM demand DIMM demand HBM overhead DIMM overhead memory bandwidth consumption previous heterogeneous architecture apply propose allocation ratio kds tvm lfm mvl ani stm avg kds tvm lfm mvl ani stm avg kds tvm lfm mvl ani stm avg reuse rate  overhead overhead reuse rate bandwidth overhead reuse rate psums accord psum configuration lessen additional software overhead previous heterogeneous memory adopt hardwarebased management migrates data memory execution embed item memory structure internal bandwidth data movement becomes performance bottleneck therefore memory processing due bandwidth congestion due internal data movement bandwidth consumption previously propose heterogeneous memory operation embeddings accord allocation policy HBM dram cache bandwidth efficient model overhead data movement HBM DIMM pom coarse grain data management consumes bandwidth overhead average summary exist architecture suffer excessive bandwidth consumption due dynamic data transfer HBM DIMM verify effectiveness cache psums implement mapped dram cache introduce psums psum reduction operation combination emb emb emb align locality emb HBM emb emb item DIMM emb emb item HBM emb emb psum alloc HBM DIMM emb emb inference FC pool emb  processing memory dram cache extension core cache memory controller cache core cache mapping pool kernel instruction request exist hardware extend hardware extend software operation execution pool operation execution embed allocation legend locality item partial sum locality item offload driver allocation host memory  allocator embed software hardware framework architecture overview psum pool item limit partial sum dram cache bandwidth overhead psums reuse rate psums pool accord locality item generate psums item psums bandwidth overhead occurs operation reuse psums average quarter psums reduces bandwidth overhead average aggravates reuse rate psums average ideal reuse rate average dynamic cache psums relatively reuse rate replacement item psums significantly degrade performance propose static data allocation IV IV architecture introduce  NMP conventional embed layer processing leverage bandwidth heterogeneous memory hardware software software overall data management NMP module modification host processor DIMMs overview manages embed locality embed layer described analysis locality exclusively allocates  item HBM locality item DIMMs apply psum reuse reduction locality psums remain HBM item allocation psum storage manage software perform prior inference perform data allocation beforehand static manage data dynamically introduce additional bandwidth overhead degrade performance processing embed layer framework prior inference profile locality item rearrange embed inference allocator distributes allocates item embed HBM DIMMs distribution ratio proportional bandwidth ratio HBM DIMMs allocate item framework generates locality psums HBM inference software driver delivers mapping information allocate embed processing NMP detailed algorithm item allocation psum storage described IV inference pool kernel embed layer offload memory computation HBM processing detailed mechanism execution pool kernel described IV extend software hardware conventional embed layer processing propose architecture allocator item psums offload driver dispatch NMP kernel software extension enables NMP processing modify memory controller hardware modification extend memory controller decodes kernel memory address item psums convert instruction address translation item psums perform inside memory controller instead host memory management execute instruction processing receives locality item psums HBM locality item DIMMs proceeds operation embed layer preprocessing memory allocation preprocessing  embed accord locality item determines item psums HBM memory allocation framework creates HBM embed DIMM embed  information allocates heterogeneous memory preprocessing consists described algorithm determines embed HBM buddy memory allocation initial HBM embed MB embed configure HBM embeddings fragmentation HBM embeddings splitting later generate align embed algorithm locality aware pre processing pseudo code procedure reserve EMBEDDING HBM embed MB raw emb HBM embed HBM embed procedure procedure align EMBEDDING access raw emb align emb sort access raw emb procedure procedure item item access item item access access HBM bandwidth bandwidth item access access align emb item item procedure procedure psum  HBM embed HBM embed item vector psum psum psum vector remain HBM embed psum psum psum procedure framework rank item embed access per item access training embed layer previous reference calculate item classify item HBM DIMMs item borderline indicates distribution item vector HBM DIMMs access rate item index  HBM bandwidth ratio memory bandwidth psums remain HBM embed calculate information psums HBM embed psum item index psum psum item HBM align embed HBM embed item psum DIMM embed item psum item HBM DIMM embed emb MB emb emb MB emb MB emb MB emb emb emb emb emb emb emb emb emb HBM embeddings 1GB memory buddy allocation allocation framework illustrates generate HBM DIMM embed buddy allocation  HBM embeddings preprocessing memory allocation perform memory allocation framework item psum align embed obtain preprocessing memory allocation framework generates HBM DIMM embed item distribute item align embed HBM embed DIMM embed HBM embed item item remain HBM psums item within psum psums ascend item index psum HBM psums HBM embed psum item item psum item item psum item item finally DIMM embed item beyond item efficiently HBM embed HBM limited capacity employ buddy allocator buddy allocator behavior buddy allocator however difference allocation incoming embeddings buddy allocator replace embed another instead replaces embed psum allocate pre HBM embeddings GB HBM capacity allocate HBM embeddings MB emb emb however HBM additional MB HBM embed emb HBM replace HBM embed MB instead buddy allocator replaces psum HBM embed emb contains MB psum psum replace incoming HBM embed emb incoming HBM embed reduce psum emb HBM embed MB allocation framework item psums hardware memory processing offloads embed kernel memory controller offload driver additional logic memory controller decodes embed kernel translates instruction instruction physical memory address host memory management address translation item psum inside memory controller acquire address information item illustrates encode instruction embed kernel instruction format allocation memory controller address embed DIMM mapping HBM mapping meanwhile item psum item psum describes operation execute embed layer memory controller offload pool kernel item psum itm item index item psum item index psum coalescer item index instruction encoder DIMM addr DIMM cmd generator HBM addr embed ID emb SLS DQ instruction format vector HBM addr register ID opcode vector source destination vector vector register ID opcode vector destination vector immediate extend memory controller instruction format kernel embed ID emb item index embed ID item psum information embed address embed memory controller classifies merges item index accord item psum item psum respectively item index item classify item DIMMs item index within item controller item HBM  item psums HBM psums item index coalescer calculate psum index item index psum item index item index psum vector coalescer translates item index psum index finally instruction encoder convert item index index psum index instruction vector psum item HBM memory controller launch memory operand addition instruction logic item vector index DIMMs memory controller generates memory request receives item item DIMMs memory controller creates immediate operand addition instruction logic HBM convert instruction location item psum vector item psum index address processing logic item vector HBM accumulate item vector internal register file generate instruction otherwise instruction item vector  DIMMs immediate accumulate inside HBM utilizes operand instruction dedicate instruction format instruction instruction physical address HBM source operand instruction immediate vector addition partition vector vector accord recommendation model fix instruction format instruction HBM NMP module embed layer operation discus detailed hardware NMP module NMP module consist vector logic previous experimental methodology datasets personalize recommendation various datasets web service vendor locality embed operation  recommendation  datasets evaluate utility locality accord embed datasets consist various embed item categorize datasets embeddings accord embed user item interaction datasets category dataset item amazon CDS  amazon  kds amazon TV movie tvm google lastfm lfm movielens mvl  ani stm IV recommendation workload combine datasets workload model multi embed recommendation mixed workload load server IV workload configuration workload dataset configuration CDS kds kds kds kds kds tvm tvm tvm tvm tvm lfm lfm lfm lfm lfm mvl mvl mvl mvl mvl ani ani ani ani ani stm stm stm stm stm kds tvm kds tvm stm kds ani stm tvm lfm mvl lfm mvl ani lfm mvl ani stm  evaluate dataset profile technique IV data inference profile ratio conduct sample inference  inference profile configuration implement hardware gem  cycle cpu memory simulator evaluate host operation fully layer cycle accurate model gem  embed operation alone simulation  execute recommendation python implement NMP architecture  embed kernel transfer NMP architecture heterogeneous memory architecture 3D stack memory DIMM memory configure memory simulation configuration HBM 3D stack dram ddr DIMM dram configuration processor core frequency cache OoO core ghz cache MB 3D stack dram memory component channel rank bus frequency device width   nrp HBM stack channel MB per channel rank per channel 1GHz ddr 2GHz cycle DIMM dram memory component channel rank bus frequency device width   nrp ddr channel GB per channel rank per channel ghz ddr ghz cycle model heterogeneous memory architecture performance evaluation HBM DIMMs adopt propose technique heterogeneous memory architecture architecture described dram cache model grain data management mitigates bandwidth overhead data management  addressable model grain data management  bandwidth efficient model addressable heterogeneous memory pom addressable model coarse grain data management fundamental model addressable heterogeneous memory previous heterogeneous memory pom VI evaluation RESULTS analyze performance impact propose hardware software overhead heterogeneous memory assume embed layer HBM furthermore performance consumption prior DIMM NMP approach analysis performance impact performance improvement NMP infinite HBM utilizes allocation technique employ allocation psum technique average performance gain HBM stack meanwhile speedup average HBM configure stack HBM  portion memory bandwidth achieves relatively performance improvement leverage bandwidth DIMMs performance improvement demonstrate bandwidth reduction exploit psum utilize bandwidth DIMMs  kds tvm lfm mvl ani stm avg performance improvement performance improvement stack HBM infinitely stack HBM  kds tvm lfm mvl ani stm avg performance improvement performance improvement stack HBM infinitely stack HBM hardware overhead evaluate overhead NMP processor generate rtl model processor synopsys application specific instruction processor designer  estimate consumption rtl model synopsys compiler CMOS technology mhz frequency calculate consumption experimental RecNMP CACTI consumption SRAM structure technology VI overhead hostside memory host extension logic index classifier coalescer encoder item psum append memory controller extension logic consists integer adder multiplier comparator addition overhead KB SRAM entry embed memory evaluate memory channel lightweight NMP module perform wise summation overhead embed processing HBM channel overhead increase linearly accord channel growth  kds tvm lfm mvl ani stm avg speedup  pom relative performance model HBM dram cache HBM additional memory address  pom  kds tvm lfm mvl ani stm avg speedup TensorDIMM TensorDIMM RecNMP RecNMP HBM HBM relative performance NMP model normalize TensorDIMM DIMM HBM NMP model model configure DIMMs HBM stack respectively denotes consists HBM stack DIMMs TensorDIMM RecNMP HBM TensorDIMM RecNMP HBM TensorDIMM RecNMP HBM TensorDIMM RecNMP HBM TensorDIMM RecNMP HBM TensorDIMM RecNMP HBM TensorDIMM RecNMP HBM TensorDIMM RecNMP HBM TensorDIMM RecNMP HBM TensorDIMM RecNMP HBM TensorDIMM RecNMP HBM TensorDIMM RecNMP HBM TensorDIMM RecNMP HBM TensorDIMM RecNMP HBM TensorDIMM RecNMP HBM  kds tvm lfm mvl ani stm avg consumption norm TensorDIMM HBM DIMM SRAM processing breakdown consumption NMP model VI hardware overhead location hardware component dynamic host extension logic KB SRAM memory NMP per channel software overhead framework pre processing IV software overhead user item interaction intel xeon sever runtime reduction pre processing consumption respectively recommendation daily facebook server pre processing scheme maintain valid pre processing production inference software overhead pre processing acceptable comparison heterogeneous memory evaluate heterogeneous architecture specialized embed prior heterogeneous memory previous technique NMP adopt processing heterogeneous memory architecture baseline  heterogeneous memory model HBM dram cache prior scheme addressable model exploit granularity data management evaluate heterogeneous memory architecture described relative performance heterogeneous memory processing embed layer normalize addressable memory model performance degradation due bandwidth overhead data management described performance pom apply coarse grain data management performance achieves performance improvement item allocation technique additionally improves achieve speedup baseline exploit reduce locality comparison NMP model NMP model various memory configuration evaluate impact DIMMs rank DIMMs DIMMs exploit DIMM parallelism TensorDIMM RecNMP addition  HBM NMP HBM unlimited RecNMP DIMM rank parallelism performance improvement parallelism however adopt DIMM rank parallelism simultaneously memory capacity NMP model apply DIMM parallelism RecNMP TensorDIMM memory capacity capacity confirm performance HBM stack DIMMs relative performance NMP model normalize TensorDIMM execute embed layer RecNMP achieves TensorDIMM DIMMs utilize SRAM cache rank stack HBM HBM achieve speedup TensorDIMM furthermore heterogeneous memory approach improves performance achieve performance HBM stack DIMMs specifically speedup DIMMs speedup stack HBM relative consumption NMP model execute embed layer normalize TensorDIMM RecNMP DIMM accelerate embed layer consumes additional SRAM cache contrast HBM reduces consumption consumes HBM employ DIMMs achieves reduction average reduce execution vii related sparse embed operation unique memory challenge recent aim optimize memory footprint bandwidth personalize recommendation data characterization embed locality although previous user item interaction explore analysis embed access interaction relatively attention locality characterization embed operation analysis interaction dataset prior embed operation memory capacity  implement dram cache non volatile memory nvm mixed dimension embed software compression technique dramatically compress embed without loss accuracy embed processing memory prior propose DIMM memory processing architecture accelerate embed layer optimize processing operational characteristic embed layer  propose reduction technique minimize data access  however limitation   cannot achieve bandwidth capacity scalability conclusion propose bandwidth memory processing personalize recommendation architecture consists 3D stack dram compute capable cache DIMMs perform characterization embed locality reveal locality optimize heterogeneous memory leverage throughput 3D stack dram data storage DIMMs utilizes 3D stack dram compute capable cache locality optimization achieve average performance improvement previous DIMM  3D stack dram DIMM memory