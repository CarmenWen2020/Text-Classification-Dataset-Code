attention mechanism rapidly emerge important primitive neural network nns ability identify relation within input entity attention orient NN model google transformer variant establish stateof processing task attention orient model achieve competitive computer vision recommender unfortunately despite benefit attention mechanism expensive operation increase quadratically input entity account significant portion inference runtime elsa efficient lightweight attention hardware software substantially reduce runtime spent attention mechanism specifically intuition relation devise novel approximation scheme significantly reduces amount computation efficiently filter relation unlikely affect output specialized hardware approximate attention mechanism elsa achieves geomean speedup magnitude improvement efficiency gpu  computation NN model maintain loss accuracy metric index attention hardware accelerator neural network introduction attention mechanism relatively recently introduce neural network primitive emerge influential community mechanism allows neural network nns identify information relevant specific input attend mechanism identify portion information relevant query extensive collection data  image specific attention mechanism attention mechanism attention mechanism identify relation input data introduction seminal attention transformer NN architecture attention mechanism widely breakthrough processing nlp attention orient nlp model AI google bert facebook RoBERTa OpenAI GPT nvidia  microsoft turing nlg establish author contribute equally various nlp task addition processing attention widely computer vision recommendation despite potential attention costly operation operation identifies relation input data amount computation quadratically increase entity involve operation due attention account substantial amount consumption attention orient NN model becomes limit factor deployment exist nlp model google bert limit attention apply token avoid excessive performance overhead input text token input text multiple token attention separately apply unfortunately scheme nlp model unable capture relation token belong hardware software efficient lightweight attention elsa hardware accelerator elsa exploit hardware specialization improve performance efficiency conventional hardware gpu however merely algorithm hardware proposes novel approximate attention scheme specialized hardware architecture intuition irrelevant relation effectively filter compute approximate similarity elsa substantially reduces computational waste  operation unlike conventional hardware gpus fails benefit propose approximation specialized hardware directly translates reduction improve performance efficiency reduce attention enables apply  data uncover relation within data model cannot handle effectively summary contribution novel approximate attention scheme exploit approximate hardware friendly similarity computation substantially reduce amount computation attention operation inference elsa specialized hardware accelerator UI OOVBM  PNQVUFS SDIJUFDUVSF acm annual international symposium computer architecture isca doi isca similarity computation softmax normalization sum matrix KT normalize matrix output matrix matrix query matrix attention mechanism exploit opportunity approximation parallelism attention operation significantly improve performance efficiency evaluate elsa multiple representative  tion orient neural network model demonstrate accelerator achieve substantial performance efficiency gain conventional hardware II background motivation attention mechanism computation attention essentially operation identifies relation within input entity computation input entity dimensional dense vector representation query assume input entity vector dimension grouped query matrix matrix matrix dimension throughout vector matrix query respectively attention similarity computation computes dot similarity query vector vector purpose query matrix transpose matrix  matrix attention matrix sij similarity dot ith query jth vector implementation attention matrix scalar constant softmax normalization attention matrix   computes output operation query vector compute sum matrix utilize correspond normalize attention equivalent matrix     output matrix ith dimensional vector outcome attention operation ith input entity application description input entity text vector representation query entity query representation entity relevant entity purpose dot similarity query representation entity representation entity compute  softmax function differentiable portion runtime spent attention mechanism approximation argmax function effectively entity entity finally representation entry sum utilize softmax normalize attention input entity output layer NN model nlp model operation identify specific semantic relation token attention sub layer layer attend modifier attend attention mechanism explain attention mechanism consists matrix multiplication accumulate mac operation multiplies matrix matrix softmax operation exponent operation matrix multiplication mac operation matrix portion runtime spent attention popular NN model  dataset nlp model bert RoBERTa albert  recommendation model   nvidia gpu detail workload available attention account significant portion runtime across exist attention orient NN model furthermore increase publish model parameter attention account portion model runtime finally recent research nlp model portion attention increase recent research demonstrates extraneous dimension feedforward layer unnecessary remove hardly affect model accuracy significantly reduce runtime feedforward layer transformer style model runtime portion attention model feedforward layer dimension reduce addition recent proposal investigate replace feedforward layer transformer style model attention model accuracy trend attention portion model runtime future opportunity approximation input matrix attention dense mostly consist nonzero visualization random projection SRP however matrix contribute equally output softmax operation attention matrix zero zero effectively sparse matrix zero hence matrix simply perform sparse matrix multiplication matrix multiplication completely mitigate attention matrix multiplication  multiplication fully exploit approximation potential attention identify query attention without perform expensive multiplication intuition achieve perform approximate lightweight similarity computation instead perform multiplication softmax operation identify ith query jth relevant zero approximate similarity compute quickly filter relevant query approximate similarity computation indicates potentially relevant dot similarity compute similarity computation subsequent computation skip scheme eliminate amount computational waste specialized hardware translate reduction performance improvement saving approximate attention overview approximate attention scheme consists sub operation estimate angle vector query minimal computation utilize concise representation hash binary embed query estimate angle utilized compute approximate similarity query intuition dot directly proportional cosine angle vector finally approximate similarity threshold identify specific relevant query binary hash angular distance random projection random projection SRP technique effectively input vector binary hash vector allows angular distance vector efficiently estimate correspond binary hash vector mapping utilized locality sensitive hash scheme focus efficient estimator angular distance random dimensional vector initialize component sample normal distribution input vector hash assign assign otherwise random vector construct binary hash input vector formally hash function define  function otherwise proven ham distance hash vector ham unbiased estimator angular distance intuitively vector random hyperplanes define random vector likely angle random hyperplanes ham distance angular distance equation estimate angle vector ham employ slight variant SRP utilizes orthogonal vector generate modify gram schmidt utilize orthogonal vector prevents random vector direction unnecessary emphasis specific direction proven reduce error angular distance approximation angle correction estimate angle compute ham distance bias error simply utilize estimator without correction estimate angle angle overestimate angle underestimate similarity vector scheme relation query bias  estimator specifically  percentile error estimator bias angle estimator underestimate angle percentile error obtain synthetic dataset standard random normal vector specific  efficient hash computation hash computation obtain hash dimensional vector orthogonal matrix matrix vector vector orthogonal assign hash positive scheme compute hash vector ndk multiplication addition scheme compute hash query multiplication hash computation ndk negligible dot similarity computation matrix computation however neural network limited model minimize amount computation hash computation exploit kronecker technique efficiently compute matrix multiplication orthogonal matrix kronecker intuition approach utilize structure orthogonal matrix hash computation specifically utilize orthogonal matrix compute kronecker matrix kronecker matrix matrix matrix kronecker    kronecker orthogonal matrix orthogonal matrix obtain orthogonal matrix kronecker orthogonal matrix characteristic allows utilize technique efficiently compute hash vector obtain compute reshape reshape efficient computation kronecker visualizes compute matrix computation matrix kronecker matrix similarly equation matrix kronecker matrix reshape operation reshape dimensional vector matrix vector slice stack technique amount multiplication involve operation reduce reshape reshape similarly technique apply obtain orthogonal matrix compute kronecker matrix equation tensor transpose index scheme batch batch multiplication compute twelve matrix multiplication involves multiplication explain efficient computation mechanism matrix approximate attention algorithm illustrates approximate attention algorithm explain sub operation approximate  algorithm detail preprocessing hash compute efficient hash computation scheme norm compute preprocessing multiplication hash computation multiplication computation norm computation compute query hash phase however assume query hash compute query hardware architecture explain approximate similarity computation preprocessing approximate dot similarity query compute relevant query computation perform query hash obtain efficient computation scheme ham distance query hash compute ham distance translate angle  equation  apply fourth cosine function apply approximate angle correspond norm estimate dot normalize query query normalize similarity vector equation illustrate relation sim  max ham  finally compute inspect constant threshold relevant query threshold explain subsection candidate query query approximate similarity computation query involves ham distance computation multiplication subtraction  cosine function another multiplication substantially multiplication compute dot similarity furthermore IV avoid computation hardware lookup candidate selection threshold motivation filter irrelevant query approximate similarity sort however sort query matrix  hash computation hash query matrix  ham distance  distance angle cosine  norm multiplication candidate selection norm  norm computation  query hash computation approximate attention algorithm reshape reshape efficient computation kronecker nlogn complexity efficiently implement hardware focus filter potentially irrelevant approximate query normalize similarity pre define threshold issue layer sub layer utilize attention threshold sub layer exhibit distribution attention however impractical layer specific threshold user define hyperparameters model bert sub layer utilize attention mechanism avoid  user specify hyperparameter approximation scheme automatically sub layer specific threshold correspond user specify approximation normalize matrix matrix min filter filter exclude div kmax update identify layer specific threshold layer specific threshold  threshold scheme target neural network model inference training inspects characteristic layer utilize attention illustrates invocation attention operation sub layer scheme inspects softmax normalize attention query identify softmax normalize attention exceeds user specify hyperparameter input entity hyperparameter approximation user considers entity softmax normalize exceed relevant selection implies aggressive approximation conservative approximation focus minimum  attention normalize attention query norm maximum norm kmax max denote threshold multiple input data training average sub layer actual inference threshold maximum norm kmax approximate similarity matrix relevant query specifically equation specifies computation skip query kmax max ham  IV elsa hardware architecture motivation hardware specialization approach improve performance efficiency specific computation naturally apply  operation account substantial portion execution emerge NN model however emphasize important overlook benefit building specialized hardware expose unique optimization opportunity specific operation cannot exploit  conventional hardware propose approximation algorithm explain elsa approximate attention avoid dimensional dot ham distance computation binary embeddings multiplication cosine function unfortunately conventional gpu operation internal approximation scheme slowdown simply perform dimensional dot faster perform approximate similarity computation various manual automate optimization cuda implementation  trace benefit propose approximation scheme harness specialized hardware approximation algorithm  optimization uncovers unique opportunity pure hardware software optimization fail exploit exists softmax normalize attention simply maximum hash memory query hash buffer buffer norm memory candidate selection module output div computation xor threshold max norm lut arb query query mem mem query query mem mem dot sum acc SQ RT  matrix multiplication multiplier mem preprocessing phase execution phase ID vec norm per query norm computation output mem hash computation attention buffer multiplier queue xor lut queue mem elsa pipeline diagram hardware overview efficient processing attention operation specialized hardware accelerator exploit novel approximation scheme introduce elsa accelerator specialized functional attention mechanism integrate various compute device CPUs gpus NN accelerator host device issue command initiate elsa accelerator pas input query matrix device scratchpad memory gpus NN accelerator matrix input output buffer reference accelerator directly input without another input accelerator preprocessing execution phase writes output matrix output memory notifies host operation overview diagram elsa accelerator pipeline highlevel dataflow elsa accelerator matrix query matrix matrix input attention generate output matrix input preprocessing phase phase computes hash matrix hash computation module hash memory similarly norm vector compute norm computation module norm memory phase execution phase query matrix sequence output output matrix specifically query candidate selection module retrieve hash norm along query hash cycle output candidate IDs IDs module output queue IDs arbitrate attention computation module computes accumulates contribution output query cycle query compute output module performs output query matrix query operation query hardware module module approximate attention computation candidate selection module candidate selection module performs approximate attention mechanism identify potentially relevant matrix candidate output index attention computation module cycle module input hash hash memory norm norm memory hash query query hash buffer module utilizes xor adder compute ham distance hash query hash ham distance index access pre lookup   ham distance integer zero lookup entry retrieve norm compute approximate similarity threshold vector norm matrix max approximate similarity potentially relevant index module output queue multiple candidate selection module parallel output arbitrate attention computation module candidate selection module fully pipelined per cycle def attention computation float float float val vector int candidate  candidate dot parallel temp   temp exponent computation exp  sum parallel output val  def output float output float  reciprocal  parallel output reciprocal pseudocode attention computation output module attention computation module attention computation module compute output matrix along output module module operation pseudocode cycle module input arbiter queue schedule policy computes dot query multiplier adder softmax normalization attention exponent compute lookup explain IV exponentiated accumulate sum exponent register component correspond matrix multiplier accumulate adder module fully pipelined candidate cycle assume candidate query candidate selection module module cycle output vector sum exponentiated output module processing query output module component output vector accumulate exponentiated softmax normalization purpose hardware utilizes reciprocal explain IV compute reciprocal sum exponentiated component output vector multiplier module fully pipelined handle query cycle module operates parallel pipeline candidate selection attention computation module however module processing ith query module processing query module query hash norm computation hash computation module module compute hash query perform series matrix multiplication described specifically assume specific utilize kronecker matrix hash computation vector twelve matrix multiplication assume multiplier carefully matrix multiplication fully utilizes multiplier perform operation hash computation cycle matrix multiplication module contains register register pre define matrix hash computation matrix multiplication component concatenate hash memory preprocessing phase module computes hash cycle query hash extra cycle execution phase model computes hash query pipeline candidate selection attention computation module processing query norm computation module norm compute preprocessing phase addition hash euclidean norm vector obtain compute dot purpose instead multiplier utilizes multiplier adder attention computation module module utilizes IV detail compute norm memory addition module identifies maximum norm compute threshold candidate selection module memory module hash norm memory memory module implement SRAM structure within elsa accelerator structure initialize preprocessing phase utilized candidate selection module execution phase hash SRAM byte storage norm SRAM byte assume representation norm configuration hash SRAM KB norm SRAM byte query output matrix memory matrix input query output attention within elsa accelerator SRAM structure however elsa accelerator utilized conjunction host device gpus neural network accelerator target neural network model utilize scratchpad memory structure device gpu memory matrix matrix KB storage assume representation pipeline candidate selection candidate selection candidate selection candidate selection cycle cycle query cycle cycle query ith query query cycle query cycle query query query hash computation multiplier attention computation output multiplier elsa accelerator pipeline execution phase pipeline configuration pipeline cycle preprocessing pipeline execution phase hardware module latency query explain IV illustrate hardware module potentially bottleneck pipeline max query candidate candidate selection module avoid introduce bottleneck maximize throughput carefully properly balance pipeline specifically ideal configure parameter module attention computation module cycle become pipeline bottleneck aim pipeline achieve speedup cycle query approximation configuration satisfies requirement configuration achieve speedup min speedup effectiveness approximation scheme reduces attention computation module parallel pipeline extend pipeline elsa utilize multiple attention computation module parallel exploit matrix independently extend pipeline utilize attention computation module parallel matrix matrix hash norm chip memory contains hash norm candidate selection module attention computation module within compute partial sum output exponentiated query partial sum output module sum adder extra adder computes output avoid specific stage pipeline specific phase bottleneck pipeline configuration parameter multiplier hash computation module multiplier output module adjust throughput candidate selection module attention computation module increase throughput elsa accelerator memory replicate exploit batch parallelism evaluation utilizes twelve elsa accelerator exploit batch parallelism detail representation query matrix fix integer predefined matrix hash computation fix pipeline utilizes minimal integer bitwidth avoid overflow maintain custom float representation exponent output exponent function computation empirically verify representation negligible impact model evaluation metric loss across various model FP baseline choice maximum input entity attention model nlp micro benchmark glue sufficient longer text  benchmark utilized capture relation token utilized task text summarization text generation evaluation configure hardware workload utilize evaluate model originally elsa accelerator synthesize efficiently model input choice hash approximation estimate angle vector becomes accurate however increase hash computation hash storage candidate selection module choice batch orthogonal vector utilized generate hash evaluate workload hyperparameter tune hyperparameter determines approximation recommend user tune parameter validation dataset model maintains user desirable accuracy improve performance efficiency tune hyperparameter almost monotonously increase accuracy decrease finally user easily version accuracy desire functional exponential computation computes utilize frac frac utilizes entry lookup fractional exponent reciprocal lookup entry obtain reciprocal float taylor expansion orient scheme tabulate utilized evaluation workload evaluate representative attention orient NN model demonstrate effectiveness elsa processing model popular google bert facebook RoBERTa google albert utilize source implementation model  bert RoBERTa  RoBERTa google albert repository albert nlp model stanford dataset squad dataset reading comprehension dataset examination RoBERTa additionally imdb review sentiment analysis dataset addition nlp model evaluate elsa attention orient sequential recommendation model  layer model model accuracy portion candidate across approximation normalize attention throughput normalize attention operation latency various device hatch spent preprocessing  layer model movielens dataset accuracy evaluation methodology extend attention layer NN model approximation scheme model accuracy metric squad raw accuracy imdb NDCG recommendation model validation workload publicly available conciseness simply refer metric accuracy throughout impact approximation impact approximation model accuracy portion candidate across approximation hyperparameter implies conservative approximation relatively accuracy degradation implies aggressive approximation model workload combination achieve sub accuracy loss inspect entity candidate furthermore achieve sub accuracy loss inspect entity average IV user validation approximation reasonable accuracy loss performance evaluation methodology performance evaluation implement custom simulator elsa integrate pytorch tensorflow implementation NN model gpu performance evaluation utilize core intel xeon cpu nvidia gpu 6GB memory workload batch achieve software implementation fix input text token software implementation pad input token exclude padding normalization portion candidate token throughput elsa performance evaluation twelve elsa accelerator 1GHz configure specifically evaluate twelve elsa accelerator peak throughput TOPS accelerator TOPS approximately nvidia gpu tflops peak throughput FP nlp model workload combination accuracy loss bound elsa conservative moderate aggressive respectively recommendation model NDCG metric configuration evaluate elsa configuration approximation finally elsa configuration ideal accelerator sustain peak FP throughput 1GHz frequency multiplier elsa accelerator effectively upper bound performance matrix multiplication accelerator without approximation throughput throughput  across platform elsa accelerator achieve substantially throughput gpu specialized architecture effectively accelerate  operation overall speedup elsa gpu varies across workload model variation across workload mostly attributable actual input entity input data entity maximum entity model gpu implementation pad data perform matrix multiplication however elsa accelerator ideal accelerator avoid computation pad achieve nvidia gpus achieve raw inference throughput utilize FP format accelerate tensor core however iso peak FLOPS throughput actual throughput increase FP inference increase peak throughput FP throughput advantage gpu calculate normalize throughput speedup speedup difference across nlp model dataset mostly due gpu performance difference across model implementation demonstrates conservative moderate aggressive approximation scheme enables elsa achieve geomean speedup gpu respectively elsa accelerator moderate aggressive approximation performance sometimes bound pipeline bottleneck candidate selection module adjust pipeline configuration parameter IV extra speedup expense extra latency average latency perform attention operation various model across elsa accelerator ideal accelerator elsa latency nearly identical ideal accelerator elsa approximation scheme achieves latency reduction ideal accelerator exploit approximation opportunity average geomean normalize latency elsa conservative elsa moderate  ideal accelerator latency workload amount preprocessing reduction preprocessing desire increase multiple hash computation module impact performance throughput latency attention mechanism model throughput latency portion spent attention varies greatly across model sequence input model configuration FFN dimension elsa conservative average speedup elsa accelerator spent attention negligible spent operation elsa conservative accelerator achieve speedup across model default max input utilized speedup input utilized furthermore accelerator utilized accelerate network FC layer speedup elsa accelerator becomes portion spent attention layer becomes evaluation methodology evaluation implement elsa accelerator chisel hardware description perform functional verification synthesize route chisel generate verilog code 1GHz target frequency synopsys compiler TSMC standard library logic synthesis assume pipeline configuration report elsa accelerator characteristic layout elsa accelerator elsa accelerator utilizes external memory module twelve memory output hash computation candidate selection attention computation query memory memory output memory hash memory norm memory layout image elsa accelerator elsa accelerator utilize external memory module nvidia gpu implies integrate elsa accelerator gpu incurs becomes report elsa estimate technology node nvidia gpu technology node another important candidate selection module utilize relatively prof approximation mechanism hardware friendly consumption elsa accelerator consumes consumption external memory module twelve elsa accelerator consume peak substantially nvidia gpu thermal TDP furthermore actual gpu consumption nvidia smi confirm gpu operating peak perform attention operation workload efficiency comparison elsa accelerator gpu combine efficiency speedup elsa accelerator achieves magnitude improvement efficiency geomean gpu attention computation moreover approximation enable configuration increase efficiency improvement conservative moderate aggressive finally consumption breakdown elsa accelerator approximation scheme despite introduction hardware module reduction significantly reduce spent attention computation output module external memory module discussion comparison accelerator recent proposal applies approximation attention however architecture limitation attention approximation scheme expensive preprocessing sort matrix preprocessing relies external hardware gpu incurs significant performance overhead unfortunately multiple attention accelerator parallel preprocessing linearly increase execution linearly decrease preprocessing dominate portion runtime outcome preprocessing memory twice normalize efficiency performance consumption breakdown elsa accelerator elsa conservative moderate aggressive peak CHARACTERISTICS elsa module dynamic  static  module approximate attention hash computation norm computation candidate selection module attention computation attention computation output internal memory module hash memory KB norm memory external chip memory module mem KB query output mem KB elsa accelerator elsa accelerator external memory module elsa accelerator external memory module matrix approximation scheme complex occupy elsa attention computation module parallelism approximation scheme cycle parallelizable significantly limit ability achieve desire accuracy prevents multiple attention computation module parallel evaluation achieves speedup baseline accelerator without approximation bert model  dataset expense accuracy loss elsa conservative moderate configuration achieve speedup elsa without approximation accuracy loss difference baseline configuration elsa approximate configuration achieve raw speedup approximation configuration finally elsa scalable efficient attention computation module multiple buffer comparison google tpu google tensor processing tpu specialized hardware target neural network training inference task effectiveness attention operation albert model natively tpu execution google TPUv experimental  achieves peak FLOPS  throughput attention operation albert  datasets workload elsa moderate achieves speedup respectively reference tpu peak FLOPS normalize throughput gpu throughput workload NN model lightweight attention recent propose nns reduce computational demand attention operation augment architecture attention layer efficiently capture relation entity compatible decompose attention operation sequence sequence multiple conventional attention moreover elsa fundamentally software approach model agnostic approach without retrain expensive computationally model finally software approach fails achieve inference speedup reasonable sequence despite theoretical reduction operation specifically recent sparse attention technique achieve speedup speedup accuracy loss reformer fails achieve speedup sequence due constant complexity concurrent achieve speedup commercial hardware sequence report speedup approximation around elsa achieves approximation VI related hardware attention mechanism hardware accelerator related attention mechanism recently propose closely related   mann dataflow accelerator relevant module potentially utilized accelerate attention mechanism however focus hardware implementation TPUv peak throughput tflops bfloat internal representation assume peak throughput FP tflops compute iso peak FLOPS throughput actual tpu throughput twelve elsa accelerator comparison gpu TOPS peak throughput instead neural network model without fully exploit approximation opportunity google ntm DNC  facebook memory network  NN approximation hardware prior various approximation strategy improve neural network performance efficiency specifically investigate efficient quantization precision operation neural network furthermore propose approximate mac achieve goal closely related focus likely affect output neural network model     reduce rank  hardware accelerator NN various hardware accelerator propose accelerate neural network operation matrix multiplication specifically proposal focus sparsity activation matrix accelerate operation differs unique approximation scheme dynamically  matrix specifically target attention mechanism vii conclusion attention mechanism recently amount attention ability capture relation within input entity emerge primitive neural network model various domain crucial accelerate operation performance efficiency focus approximation opportunity within operation specialized approximation algorithm hardware operation significantly reduce amount computation operation reduction elsa achieves significant improvement performance efficiency conventional hardware gpu