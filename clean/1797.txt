neural network dnn training notoriously consume quantization promising improve training efficiency reduce bandwidth storage requirement computation however quantize algorithm negligible training accuracy loss  statistic quantization amount data neuron precision update cannot effectively deployed exist dnn accelerator address propose customize architecture efficient quantize training negligible accuracy loss cambricon cambricon feature hybrid architecture consist ASIC acceleration core data processing ndp acceleration core mainly target improve efficiency statistic quantization specialized compute statistical analysis maximum data  ndp avoids transfer precision chip memory acceleration core experimental evaluate benchmark cambricon improves efficiency dnn training performance gpu tpu respectively accuracy degradation precision training introduction dnn training notoriously tedious consume distinct recent GPT model OpenAI billion parameter FLOPS training theoretically nvidia tflops peak model increase academy hardware architecture training efficiency quantization promising technique improve training efficiency convert precision floatingpoint float FP bitwidth data fix int potential highly efficient hardware computation data access reduce latency quantize training algorithm already achieve negligible training accuracy loss int data dnn model alexnet resnet ssd however cannot effectively deployed exist dnn accelerator gpu tpu accelerator already integrate int int arithmetic accelerate dnn inference nvidia gpu tensor core performance quantize training algorithm achieves traditional training FP evaluate benchmark another  accelerator integrates various functional int int BF FP int training  significant accuracy degradation VGGNet alone width data int inefficiency quantize training algorithm exist accelerator quantize training typically employ statistic quantization amount data neuron maximum layer quantize synaptic float data FP compute cosine similarity quantize obtain int data propose vector distance maximum quantize neuron int int respectively data quantize width data update memory access computation precision data FP actually nvidia gpu execution statistic quantization precision update computation training VGGNet address bottleneck propose customize architecture cambricon efficient quantize training negligible training accuracy loss cambricon feature hybrid architecture consist efficiency comparison width data NM RESULTS width operation relative float float mul fix fix mul dram access average float float mul fix fix mul dram access average fix fix mul dram access average UI OOVBM  PNQVUFS SDIJUFDUVSF acm annual international symposium computer architecture isca doi isca II  hardware dnn training hardware tpu  sigma width statistical analysis  update ASIC acceleration core data processing ndp accelerate statistic quantization precision update respectively contrast conventional statistic quantization separately statistical analysis maximum  data inevitably incurs extra data access acceleration core integrates specialized compute conduct statistical analysis  consecutively partition slice entire data contrast conventional update transfer precision chip memory onchip computation logic ndp implement integrate configurable optimizer adagrad RMSProp dram update within memory II cambricon representative dnn accelerator hardware operation training conduct network model experimental gpu tpu concretely gpu jetson TX efficiency gain dnn training performance respectively tpu efficiency gain dnn training performance moreover precision training gpu accuracy loss contribution conduct thorough analysis quantize training algorithm bottleneck stem statistic quantization precision update knowledge propose architecture efficient quantize training negligible training accuracy loss propose acceleration core performs  quantization locally slice data instead entire data improves compute efficiency significantly reduces amount data access propose data processing integrates configurable optimizer dram avoid costly data transfer update organize II discus quantize training technique inefficiency exist training hardware introduces propose hardware friendly quantization technique IV describes cambricon architecture describes methodology VI describes physical pas backward pas wnew layer layer compute gradient neuron compute gradient update dnn training pas backward pas input neuron synaptic output neuron gradient neuron ΔW gradient wnew update data layer respectively implementation hardware evaluates performance cambricon baseline discus related sec IX concludes II background motivation quantize training introduces quantization technique apply training stage dnn training backpropagation BP mainstream dnn training algorithm roughly BP pas algorithm pas backward pas pas dnn computes neuron output input sample neuron output layer compute input synaptic layer layer function backward pas consists stage compute gradient neuron compute gradient update compute gradient neuron stage layer computes local gradient neuron gradient loss function error desire network output actual network output gradient layer compute gradient layer gradient neuron layer layer compute function compute gradient stage layer computes synaptic correction regard gradient output neuron gradient layer compute  compute function update stage synapse update accord delta optimization formulate  update function statistic quantization quantization technique data reduce hardware computation storage data transfer formulate precision data quantize data quantization parameter regard offset quantization technique apply inference stage however challenge apply insufficient resolution fix representable insufficient clip data distribution gradient layer training alexnet conventional quantization cannot data dnn model due variety maximum gradient iteration training alexnet training stage training data precision sensitive statistic quantization technique propose address issue achieve width dnn training negligible accuracy loss factor  quantization precision update regard statistic quantization difference previous quantization dnn inference statistical analysis quantization parameter maximum cosine distance vector distance data distribution gradient backward pas varies drastically across layer training epoch static quantization dnn inference introduce quantization error gradient data distribution depict training alexnet maximum absolute gradient variance magnitude layer magnitude epoch apply static quantization gradient representation data easily conv layer alexnet quantize conv layer conv layer layer error due narrower layer overflow width training ALGORITHMS    algorithm data format statistic FP max update FP int max update FP clip int max update FP  int max update FP quantize int int max update FP adaptive precision dnn training without quantization cpu gpu platform error due similarly sample clip estimate statistic drawback introduce non negligible quantization error affect training accuracy significantly confirm statistic max absolute accurate quantize training inaccurate statistic max absolute quantize training algorithm overall accuracy performance vgg densenet inception therefore dynamic quantization perform statistic counting quantization essential retain training accuracy unstable variable data distribution training stage regard precision update instead quantize width data quantization precision data stage update necessity precision data former gradient become zero exponent training latter invalidate update overflow zero layer gradient error recover avoid convergence severe error accumulative numerical inaccuracy update stage perform precision data FP FP FP motivation dnn training quantize width data factor quantization dnn training prevent exist training hardware gpus accelerator fully leverage benefit data quantization training without quantization quantize training interaction cpu slowdown perform quantize training nvidia gpu comparable training accuracy normal training without quantization average representative dnns fold lack hardware gpu accelerator lack efficient hardware gpus lack cpu acc ddr wnew cpu acc ddr cpu acc ddr quantization cpu acc quantization wnew ddr backpropagation training quantize backpropagation training pas backward pas pas backward pas cpu acc ddr cpu acc wnew ddr pas backward pas naive implementation cpu interaction legend compute gradient neuron update  gradient pas backward pas compute neuron quantization statistic width data width data processing dnns cpu acc platform backpropagation training quantize backpropagation training naive implementation quantization acc quantize data  width data hardware statistic quantization statistic analysis hardware quantization exist quantize accelerator usually quantize data offline lack precision precision update therefore host cpu invoked costly training alexnet data transaction cpu memory gpu quantize training normal training critical data movement update moreover precision update precision data ΔW wnew movement memory accelerator become critical quantize training simply data quantize training quantize width backward pas quantize training normal training analytically training alexnet data quantize precision data movement become quantize training normal training naive intuitive quantization acc address issue specialized statistic quantization equip however efficiently processing stateof quantize dnn training statistic quantization lunch introduces extra data access statistic counting quantization specifically pas data access data statistic analysis quantization epoch acc access data training without quantization memory acc limited onchip storage layer dnns iot device particularly acc exchange intermediate memory inevitably incurs data access summary efficient hardware quantize dnn training efficiently statistic quantization precision update reduce extra data access precision data access propose novel hybrid architecture ASIC acceleration core ndp cambricon propose hardware friendly quantization technique hqt perform statistic analysis quantization pas data access detailed statistic quantization squ quantization buffer controller QBC CambriconQ address efficient hardware  quantization issue reduce extra data access cambricon feature ndp address issue hardware precision update reduce precision data access detailed IV hardware friendly quantization technique introduce hardware friendly quantization technique hqt quantize dnn training hqt avoid extra data access quantization local dynamic quantization ldq reduce quantization error distribute data component error estimation quantization multiplexing  local dynamic quantization ldq previous layer wise statistic quantization technique perform global statistic quantization data dependency bottleneck quantize depends statistic depends data available scan scan inevitable obtain moreover available compute scan compute inevitable bottleneck phenomenon forbids hardware leverage data locality data access grain local dynamic quantization ldq bottleneck phenomenon ldq slice data fix statistic perform within constrain data dependency local scope chip buffer hence avoid excessive memory access           error estimation quantization multiplexing error ldq guaranteed error layer wise statistic quantization statistic max absolute max data algorithm trivially max absolute local data exceeds max absolute data hence dynamic quantization forbids occurrence data clip representation narrower error summary ldq error layerwise dynamic quantization validate proposition model training algorithm ldq achieve accuracy average quantization besides compression ratio ldq layer wise dynamic quantization denote DQ suppose data quantize byte statistic byte compression ratio ldq DQ calculate   data compression efficiency loss error estimation quantization multiplexing  research distribution data dnn model exaggerated error fix representation algorithm distribution various technique mainly dynamically switch quantization propose  fix data format encodes data fix additional representable resolution propose data representation  FxP distribution encode namely  data dynamically data format accord estimate quantization error int int distribution propose direction sensitive gradient clip clip minimal precision penalty technique optimizes clip backpropagation cosine similarity  data loss function divergence technique unfriendly efficient hardware observation technique quantization function candidate QN accord estimation quantization error distance data  data  fix data format candidate accord error estimation direction sensitive gradient clip clip setting accord distance define inner unified error estimation quantization multiplexing  technique technique consists performs statistic data quantizes data multiple candidate via quantization function calculate distance  data estimation quantization error candidate accord error estimation  rectilinear distance estimation simulate direction sensitive gradient clip experimental alexnet resnet accuracy difference respectively significantly affected validate  simulate  fix data format resnet accuracy difference significant improvement IV cambricon architecture overview overall architecture cambricon consists ASIC acceleration core practical data processing ndp acceleration core quantize data hqt perform computation quantize dnn training consists PE array matrix vector compute scalar functional sfu scalar operation chip buffer input neuron NBin output neuron NBout synapsis SB respectively besides specially module efficiently hqt statistic quantization squ performs  statistic ldq quantization buffer controller  couple NBin SB manage data quantize parameter offset ndp perform update  memory consists specialized module squ ndp optimizer NDPO update various optimizers  fix data format directly applicable alexnet report resnet QBC QBC dram squ squ NBout KB SB KB NBin KB PE array memory controller sfu IB KB decoder ddr bus width width acceleration core quantization module cambricon architecture overview cambricon cambricon reduces computation storage transfer overhead data arrow quantization module squ QBC  ndp comp core wnew pas backward pas ndp comp core processing quantize backward pas cambricon fuse statistic quantization perform squ perform PE array sfu perform ndp perform pas dnn training cambricon quantizes input neuron synaptic squ inside ndp directly load quantize data NBin SB acceleration core respectively acceleration core manages data quantization parameter NBin SB couple  acceleration core performs computation PE array sfu computation PE array sends sfu activation function NBout memory quantization squ quantize data transfer ndp acceleration core perform backward pas dnn training cambricon principle eliminates width data traffic stage backward pas stage compute gradient neuron cambricon load output neuron layer gradient layer layer NBin NBin SB respectively data squ ndp arbiter quant stat squ buffer KB 7DJ squ buffer  statistic quantization  data quantization parameter quantize data quantization cambricon computes gradient layer PE array writes quantize gradient memory perform  quantization squ compute core stage compute gradient similarly cambricon load quantize data computes PE array slightly previous stage cambricon writes bitwidth correction instead quantization stage update cambricon performs update NDPO NDPO directly instead passing acceleration core eliminates data traffic data transfer ndp acceleration core quantize data eliminate extra data access hqt introduce specialized module statistic quantization introduce instruction CambriconQ controller introduce functional computation statistic quantization overall cambricon address challenge II hqt specialized module squ QBC NDPO hqt squ statistic counting quantization eliminates non trivial extra data access QBC cache buffer controller manages data quantize parameter hardware ndp synaptic update avoid costly data transfer memory chip compute squ squ statistic counting quantization hqt efficiently squ consists KB buffer statistic stat quantization quant arbiter  data squ buffer squ buffer buffering manner improve throughput simultaneously  data statistic computes statistic quantization parameter quantization parameter quantization performs quantization convert width data     ELW HDG 7DJ HDG DWD ULWH 7DJ ULWH DWD URP   URP URP rug  NBin couple QBC width data moreover  technique squ performs multiplexing quantization parameter quantization parameter arbiter quality quantization quantization parameter selects quantize data correspond tag quantization QBC hqt data split independent quantization parameter manage data QBC cache chip buffer controller couple NBin SB architecture chip buffer QBC NBin similarly cache QBC manages NBin buffer data buffer data format width quantization parameter buffer tag data format data NBin data tag perform computation correctly accord data format implementation buffer cambricon QBC maintains data buffer data format data dnns tensor chip buffer data format QBC tag matrix transposition data buffer format maintain integrity buffer QBC performs  data buffer data indexed byte address manner specifically data buffer tag data correspond tag firstly meanwhile max tag update maximum data buffer quantize max tag flush buffer tag correspond buffer max tag manner data buffer maintain quantization parameter tag NBout couple QBC contains precision data    mem address URP  URP DWD XIIHU URP XV  DWD XIIHU memory architecture  NDPO architecture ndp NDPO IV commonly optimizer update adagrad RMSProp adam denotes rate denotes gradient parameter maintain optimizers decay rate optimizers introduce precision parameter due wise function avoid width data traffic propose data processing optimizer NDPO aside memory leverage inplace computation functionality update relatively ndp aside memory controller introduces overhead evaluation entire ndp NDPO architecture memory ndp ndp enhances memory NDPO computation register buffering optimizer parameter NDPO optimizers IV optimizers summarize formula NDPO perform formula adam optimization formula ndp correspond parameter register accord parameter controller acceleration core memory controller sends successive activate signal memory correspond activate array memory controller cambricon sends signal gradient destination address ddr bus specific data buffer namely update compute NDPO optimizer data 7DJ URP LQ PE acc ELW DWD URP LQ ELW DWD URP  FF  URP 7DJ URP  HU   ELW ELW PE PE PE PE PE PE PE PE PE PE PE PE PE PE PE PE acc acc acc acc  PE array consists width PEs width accumulator acc buffer memory controller sends successive precharge signal update optimizer parameter array instruction cambricon adopts tensor instruction operation convolution conv matrix MM vector operation   operation  horizontal vector operation  vector load   stripe load sload sstore efficiently quantize dnn training manage ndp specially instruction particularly    performs data load quantization acceleration core ndp acceleration core  performs ndp configuration  perform update ndp function cambricon consists compute PE array sfu PE array perform parallel tensor operation sfu perform scalar function non linear operation IV commonly USED optimizers dnn training optimizer parameter computation perform maintain sgd adagrad RMSProp adam  PROPOSED ISA cambricon operation ddr constant   imm data vector  dest src stripe sload dest src dest str src str quantize  dest src   dest dest dest src compute matrix MM dest   2D convolution conv dest src vector operation    organization PE array consists PEs accumulator PE capable processing multiplication output generate accumulator accumulator consists adder shift adder  device adder accumulates PEs input PEs generate summation without loss numerical precision shift adder realize computation wider bitwidth input serial manner therefore PE array perform quantization operator   float format later operation width update precisely instead  input ahead computation cambricon perform  accumulate accumulator reduces  logic shifter adder shifter reduces hardware overhead PEs accumulator operator methodology introduce experimental setup benchmark hardware platform configuration comparison benchmark benchmark VI spectrum efficiency optimize NN model cnns recurrent network dynamic quantization algorithm zhu zhang tailor version optimization hqt zhu hqt VI BENCHMARKS model dataset batchsize alexnet imagenet resnet imagenet googlenet imagenet squeezenet imagenet transformer WMT PTB lstm medium  zhang hqt report average execution consumption per minibatch training resource constrain characteristic platform minibatch memory data quantize format whenever hardware configuration training performance hardware commonly architecture gpu tpu cambricon implement cambricon verilog rtl obtain synthesize route rtl code synopsys toolchains TSMC technology CACTI destiny model dram memory chip SRAM buffer due  duration silicon simulation implement cycle accurate performance simulator evaluate execution latency cycle simulator report memory trace ramulator integrate module activity calculate dynamic consumption cambricon PE array ghz peak performance int int memory bandwidth GB cambricon performs algorithm manner parameter performance gpu nvidia jetson TX gpu baseline platform comparable hardware configuration gpu cuda core perform FP FMA operation cycle mhz max frequency peak performance tflops FP memory bandwidth GB deploy benchmark pytorch cuda cudnn execution nvprof   awe analyzer training mixed data format enable performance gpu hardware int mixed precision training apex pytorch extension nvidia tpu implement tpu architecture simulator sim expand sim feature quantize dnn training mixed precision backward statistic function quantization overall architecture organize comparison align hardware configuration cambricon specifically PE array ghz peak performance int KB SRAM NBin KB SRAM SB KB SRAM NBout memory bandwidth limited GB additionally comparison hqt quantize dnn training tpu performance VI experimental RESULTS evaluate overhead cambricon performance vii hardware CHARACTERISTICS acceleration core squ QBC FU NBin SB NBout decode IB MC phy ndp squ NDPO cambricon tpu gpu finally analyze insight cambricon hardware characteristic detailed hardware characteristic vii acceleration core cambricon occupies consume technology cambricon efficiently statistic quantize dnns training hardware extra extra consumption evaluate additional component introduce ndp squ NDPO occupy consume hardware dram performance performance cambricon tpu gpu network model statistic quantization algorithm accuracy training accuracy cambricon quantization algorithm FP  training zhu cnn benchmark cambricon achieves accuracy loss average version however zhang cambricon achieves accuracy benchmark average training accuracy RESULTS model FP zhu hqt zhang hqt alexnet resnet googlenet squeezenet transformer bleu lstm perplexity cnns bleu transformer slightly underperforms lstm perplexity hqt adjust quantization parameter  manner quantization quality speedup performance improvement cambricon gpu tpu baseline overall cambricon outperforms gpu factor tpu average quantization algorithm dnn model cambricon boost performance statistic quantization algorithm precision sensitive data movement critical breakdown training epoch pas FW backward pas compute gradient neuron NG compute gradient WG update WU statistic analysis quantization cambricon achieves speedup alexnet speedup tpu alexnet alexnet therefore costly update cambricon performs operation ndp data transfer memory difference tpu CambriconQ hqt benefit statistic quantization tpu gpu hardly benefit dynamic quantization technique confirm backward pas costly tpu cambricon report comparison tpu gpu cambricon across neural network overall cambricon achieves efficiency tpu gpu respectively clearly efficiency source breakdown component functional module acceleration core acc chip buffer buf memory standby ddr SB memory dynamic ddr  efficiency cambricon elimination memory traffic precision data movement extra data access reduction memory computation width significant vii discussion performance scalability cambricon efficiency cambricon cambricon gpus desktop nvidia server nvidia peak performance cambricon achieves performance efficiency respectively training resnet moreover achieve throughput CambriconQ cambricon cambricon comparable peak performance respectively cambricon cambricon organize tangram cambricon increase PE array cambricon int  performance tflops PE array SB private PE array NBin broadcast neuron memory bandwidth CambriconQ GB GB bandwidth cambricon increase PE array cambricon 2D mesh int peak performance tflops 2D mesh SB neuron NBin batch parallelism memory bandwidth CambriconQ GB GB bandwidth report performance cambricon cambricon cambricon jetson TX respectively minibatch setting hardware CambriconQ cambricon faster resnet lstm correspond gpus performance scalability generalize quantization cambricon demonstrate bunch statistic quantization algorithm data format statistical quantization policy IV flexibility cambricon achieve propose configurable architecture quant squ QBC format int int int int arbiter various statistical max absolute rectilinear distance bias actually cambricon efficiently  quantization algorithm characteristic via hqt statistic depends data error estimation statistic depends  data future statistic quantization algorithm characteristic cambricon efficiently apply moreover cambricon easily non statistic quantization algorithm simply bypassing stat generalize bitwidth PEs cambricon bitwidth data int directly operator PE fold cambricon PE serial computation flexible data precision int int int precision multiple therefore cambricon PE directly model capable precision inference efficiently model mixed precision model cambricon increase performance efficiency switch cambricon PE maintain DP DP DP DP performance comparison breakdown DP DP DP DP DP DP DP DP performance cambricon cambricon cambricon throughput easily regard serial compute serial compute commonly introduce address bitwidth issue multiple PEs deliver performance parallelism data avoid utilization PEs cambricon adopts operator leverage serial various precision cambricon without ndp analyze II data movement update WU critical therefore NDPO achieve efficient training report benefit CambriconQ  ndp model involve WU alexnet transformer CambriconQ without ndp achieve negligible improvement performance model WU marginal proportion googlenet squeezenet cambricon without ndp achieves improvement cambricon ndp ndp critical arbitrary dnn model efficiently achieves non negligible improvement baseline average performance efficiency improvement tpu comparison approximate pim architecture data processing scheme adopt CambriconQ allows acceleration core perform precision computation efficiency unlike previous pim usually allocate precision approximate computation pim architecture efficiency CambriconQ essential precision update memory specifically propose rank ndp precision update ndp configure various training optimizers sgd adagrad RMSProp adam related quantization algorithm quantization technique commonly reduce computation storage communication overhead width data although quantization technique apply inference training precision sensitivity inference training previous empirically demonstrate fix data handle image detection inference task spawn limit border width width bitwidth training stage however challenge apply quantization retain accuracy training data gradient sensitive quantization methodology width damage model accuracy recently statistic quantization technique propose successfully achieve width training data quantization parameter separately neuron activation gradient accord statistic data distribution envisions inefficiency IX recent  training aware  cambricon agrawal lee wang  data format FxP int HFP FP   FP FP training width dynamic quantization threshold extra update residual stochastic resnet accuracy width technology TOPS int FP FP FP introduces additional processing pas data random generation implement propose hardware deploy smart quantization technique exist architecture proposes cambricon architecture reduce overhead quantization  compute optimizers statistic quantization dnn architecture quantization exist dnn accelerator architecture focus leverage quantization accelerate dnn inference stripe apply quantization layer cnn network width inference BISMO BitFusion propose serial multiplication  execute dnn model variable quantize width OLAccel utilized quantization MACs shapeshifter propose loss memory compression technique width adaption propose pim quantize data memory data movement reduction DRQ dynamic quantization data dnn inference  dnn proposes data format quantification  leverage quantize data format increase hardware density PE array BFP data format ensure training accuracy benefit quantization data transmission storage efficiency performance exist architecture focus leverage quantization accelerate inference training stage variability precision sensitivity data distribution challenge efficiently apply quantization technique cambricon innovatively proposes composite ASIC acceleration  optimizers local dynamic quantization algorithm efficiently accelerate dnn training retain accuracy IX cambricon recently propose quantize training aware ASIC accelerator regard training width cambricon fix arithmetic quantize efficient implementation regard dynamic quantization cambricon statistic quantization dynamically efficiently properly data previous accelerator lack architectural factor manually converge regard extra overhead update cambricon propose ndp minimize overhead update HFP format introduces residual training regard accuracy retain cambricon retain accuracy loss FP baseline suffer non negligible accuracy loss cambricon achieves performance efficiency TOPS int mode therefore cambricon architecture efficiently quantize training negligible accuracy loss IX conclusion propose novel hybrid architecture cambricon efficiently quantize dnn training roughly cambricon leverage propose  quantization technique hybrid  acceleration core ndp hardware address  statistic quantization dnn training cambricon reduce extra data access precision data access achieve performance efficiency gpu tpu respectively knowledge CambriconQ architecture perform quantize dnn training negligible accuracy loss