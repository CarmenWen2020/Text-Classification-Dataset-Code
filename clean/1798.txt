accelerate tensor application spatial architecture performance efficiency accurate performance model evaluate various dataflow alternative model relies notation tensor dataflow formulation performance metric recent propose compute centric data centric notation dataflow imperative directive however notation expressive limited optimization opportunity inaccurate performance model propose framework tenet model hardware dataflow tensor application introduce relation centric notation formally describes hardware dataflow tensor computation relation centric notation specifies hardware dataflow PE interconnection data assignment uniform manner relation relation centric notation expressive  data centric notation sophisticated affine transformation another advantage relation centric notation inherently accurate metric estimation data reuse bandwidth latency tenet computes performance metric counting relation integer structure operator overall tenet achieves latency reduction conv gemm kernel data centric notation identify sophisticated hardware dataflows introduction tensor operation increasingly deployed application data analysis machine  simulation recent spatial architecture emerge promising accelerate tensor operation due performance efficiency typical spatial architecture compose processing PE array scratchpad memory PEs via chip interconnect enables efficient data reuse characteristic spatial architecture diverse hardware dataflow alternative tensor operation usually author contribute equally author peking correspond author described loop nest specific tensor operation hardware dataflow describes assignment loop instance PE array execution sequence loop instance PEs hardware dataflow critical achieve throughput latency determines PE utilization data access onchip bandwidth requirement tensor computation prefers hardware dataflows google tensor processing tpu connects PEs systolic dataflow PE responsible  operation  connects PEs via multicast communication network PE performs dot spatial architecture  plasticine integrate PEs interconnect flexible manner hence wider application despite various dataflows practically implement tensor accelerator formal notation strongly desire hardware dataflow ideally notation dataflow systematically facilitate accurate performance model technique hardware dataflow compute centric data centric notation however notation limitation notation expressive subset hardware dataflows notation architect incomplete limited optimization opportunity notation fail hardware dataflows skew loop iteration tensor dataflows affine loop transformation enable sophisticated mapping loop instance spatial architecture notation fail accurate performance analysis compute centric notation directly model data transfer reuse lack detailed performance model data centric notation integrate maestro model output reuse latency dataflow however maestro model metric UI OOVBM  PNQVUFS SDIJUFDUVSF acm annual international symposium computer architecture isca doi isca comparison notation feature computation centric data centric stt relation centric timeloop interstellar maestro tenet express dataflow instance execution sequence loop loop temporal sequence stamp vector multi dim stamp PE workload assignment parallel directive unroll primitive spatial stamp matrix multi dim stamp affine loop transformation spatial architecture performance model PE interconnection precise reuse analysis data assignment analysis bandwidth analysis latency model tensor apps calculate polynomial parameter accurate tensor dimension cannot explicitly specify data centric primitive propose tenet framework model dataflow tensor application spatial architecture component tenet relation centric notation formally describes hardware dataflow tensor computation specifically formally define relation loop instance PEs perform computation loop instance execution sequence PEs PEs correspond assign tensor PEs interconnection network relation loop instance execute relation model tensor access relation describes tensor traverse across PEs systolic array reduction relation uniform manner precisely model tensor computation spatial architecture data assignment PEs data movement PEs relation centric notation allows user complex hardware dataflows relation relation tenet linear transformation loop instance spatial architecture spatially temporally hardware dataflows relation centric notation inherently accurate model various important performance metric metric crucial evaluate hardware dataflow alternative relation mathematically structural representation performance metric easily compute integer operator overall tenet estimate various hardware metric data reuse latency PE communication bandwidth chip memory bandwidth contribution propose relation centric notation model hardware dataflow tensor computation relation centric notation expressive compute centric data centric notation dataflow data assignment interconnection relation uniformly relation centric notation hardware dataflows optimization opportunity introduce performance model accurately calculate various hardware metric naturally structural representation relation centric notation formulate volume overall data reuse data minimum data transfer PEs scratchpad volume derive various hardware metric systematically analyze notation expressiveness performance model tenet achieves latency reduction conv gemm kernel identify sophisticated dataflows  notation source code tenet publically available github http github com pku liang tenet II background spatial architecture spatial architecture architecture feature processing PE interconnection PEs memory hierarchy PE contains arithmetic logic ALUs configure specific instruction PE contains register file data storage interconnection PEs effectively increase data reuse opportunity reduces bandwidth requirement generally spatial architecture memory hierarchy PE register chip scratchpad chip memory simplicity assumption model hardware behavior alu ability perform  mac operation data transfer interconnect adjacent PEs cycle hardware dataflow refer implementation specific tensor application spatial architecture tensor application usually described loop nest specific tensor application dataflow aspect PE loop instance execute execution sequence loop instance PEs dataflow data reuse critical factor achieve performance categorize temporal reuse spatial reuse temporal reuse happens compute centric notation data centric notation parallel distribute dim across PEs distribute dim across PE compute directive assign workload spatial temporal skewed data access rectangle data access actual reuse data centric reuse complex dataflow exist notation inaccurate reuse analysis limitation compute centric data centric notation data reuse cycle spatial reuse happens data reuse PEs notation introduce widely various compiler framework loop analysis tenet tensor application  loop unconditional statement iteration domain loop nest statement iteration domain DS contains loop instance instance DS 1D conv operator iteration domain DS loop instance affine constraint access function loop instance access function return tensor access statement relation access function tensor 1D conv operator access function tensor loop instance access tensor limitation exist dataflow notation notation feature dataflow expression performance model  notation dataflow specify loop transformation directive reorder parallel compute centric notation flexibility computation perform imperative program style recently data centric notation propose specify data mapping directive spatial temporal data movement structural representation enables easy data reuse computation notation user manually directive straightforward complex dataflows transformation stt loop instance onto systolic array however lack accurate performance model analyze various hardware metric non systolic array spatial architecture tensor app gemm dataflow relation user spatial architecture repo data assignment relation interconnection relation ST stamp relation constraint PE utilization performance model bandwidth requirement latency temporal reuse spatial reuse MACs per cycle dataflow exploration hardware exploration relation centric notation tenet automatic timeloop interstellar compute centric notation loop determines loop instance execution sequence however workload assignment extra directive parallelism specify parallel directive data centric notation explicitly allocates data PEs primitive spatial temporal spatial assigns dimension PE array temporal specifies data movement dimension across stamp spatial distribute dimension output data across PEs temporal distributes data involve dimension across stamp within PE compute data centric notation fundamentally limited expressiveness performance model capability notation fail dataflow denote tensor cycle notation dataflows rectangle data access lack complex dataflows skewed data access skewed data access introduction dimension comb tensor dimension affine transformation importantly limitation expressiveness cannot easily remedied extend data centric notation effort manually transform tensor application explicitly specify data distribution manual transformation estimate performance metric contradicts intention data centric notation performance model perspective previous compute centric notation model analyze data reuse opportunity coarse grain manner interstellar calculates data reuse unroll factor data centric notation analyzes hardware performance maestro model however maestro polynomial estimate data reuse precise calculates data movement polynomial actual reuse tensor maestro inaccuracy primitive gemm operation dataflow PE stamp stamp PE PE domain interconnect PE PE PE PE tensor assignment function domain PE PE PE domain PE PE PE PE domain PE PE PE PE domain PE PE PE PE PE PE PE PE PE PE PE PE PE PE PE PE PE interconnect scratchpad without interconnect PE assignment PE assignment analyze dataflow matrix multiplication PE array relation centric notation data movement tensor without model movement tensor tenet overview tenet automatic framework tensor operation hardware specification input tenet automatically generates relation centric notation dataflow data assignment interconnection spacetime stamp relation tenet calculates performance metric dataflow hardware optimization dataflow relation relates loop instance PE generate automatically exploration DSE specify manually user data assignment relation tensor PEs obtain combine access function tensor operation dataflow relation interconnection relation describes PEs derive architecture specific dataflow tenet calculates multiple spacetime stamp relation performance metric estimation tenet repository contains spatial architecture mesh structure systolic array reduction feature PE functionality PE array topology expressivity relation centric notation accelerator designer explore hardware dataflows dataflows constraint moreover accelerator designer rely tenet obtain critical performance metric dataflow data reuse PE utilization latency IV relation centric notation tenet defines relation mapping loop instance onto PE array IV data assignment IV PE interconnection IV mapping spacetime stamp IV relation centric notation exactly loop instance execute spatial architecture tensor access tensor across PEs dataflow relation dataflow relation applies affine transformation define stamp stamp execution tensor application spatial architecture stamp relation describes PE coordinate loop instance execute stamp relation determines execution sequence loop instance perform PE notation definition dataflow statement iteration domain DS iteration vector dataflow define  PE  assigns loop instance spacetime stamp stamp PE stamp  coordinate PE execute stamp decides execution sequence sequence lexicographical stamp simplicity lexicographically respectively multi dimensional PE array dimension multi dimensional PE array iteration domain relation centric notation express various dataflows employ affine transformation dimension spacetime stamp linear transformation multiple loop dimension mapping gemm systolic array demonstrate  notation dataflow  PE loop instance execute PE assign dimensional stamp stamp affine transformation loop iterators specific constraint loop instance constraint stamp execute loop instance constraint loop instance dataflow relation centric notation expressive compute data centric notation comparison assume PE mac PE array dimension besides assume offset parameter data centric notation coefficient affine transformation relation centric notation assumption  notation enlarges loop former maestro primitive freely primitive SpatialMap TemporalMap exactly SpatialMap later dataflows relation centric dataflow corresponds affine transformation transformation matrix gemm maestro contrast tenet loop bound PE array therefore modulus operator affine transformation quasi affine transformation  PE mod mod mod mod PE array index dimension parallel stamp dimensional modulus operator execution sequence data assignment relation data assignment relation specify tensor access specific PE specific timestamp dataflow mention II access function relate loop instance access tensor therefore data assignment relation formulate chain definition data assignment dataflow  data assignment define AD PE interconnect topology data assignment tensor AD FY PE PE calculate output tensor timestamp tensor stationary iteratively reuse stamp computation interconnection relation specify PE interconnection relation determines tensor PEs definition PE interconnection PE array interconnection describes mapping PE another PE  PE PE PE denote coordinate PEs specify topology interconnection relation data movement dataflow depicts interconnection specification 2D systolic data transfer PE reuse input tensor adjacent PEs interconnection otherwise data access scratchpad model widely interconnect topology interconnection PE PE 2D systolic mesh 1D multicast PEs systolic interconnect widely recent gemm conv accelerator tpu mesh NoCs apply  plasticine multicast network PEs input entry multicast network eyeriss vector dot diannao interconnects data reuse multicast occurs cycle detail interconnection relation affect dataflow model spacetime stamp relation compute various performance metric relation spacetime stamp clearly data assignment interconnection relation correlate spacetime stamp access data movement TotalVolume ReuseVolume UniqueVolume tensor iteration domain PE PE PE PE PE PE PE PE PE tensor iteration domain reuse multiple stamp  tensor iteration domain  PE PE PE PE PE PE PE PE PE PE PE PE PE PE PE PE PE PE PE PE PE PE PE PE PE PE PE reuse multiple stamp PE TotalVolume ReuseVolume UniqueVolume   data movement across volume metric definition spacetime dataflow spacetime relation  another spacetime stamp derive dataflow MD PE PE dataflow spacetime link stamp stamp model hardware behavior continuous stamp stamp furthermore identify data spacetime stamp data assignment relation detect data reuse spatially temporally assume distance within PE specify interconnect spacetime stamp PE PE PE PE PE PE examine behavior tensor assignment relation AD FY PE PE tensor reuse PE stamp similarly reuse tensor tensor AD FA AD FB respectively PE PE PE PE tensor traverse across PE array horizontally tensor traverse vertically respectively estimate various performance metric data reuse apply restriction spacetime restrict PE capture temporal data reuse restrict interconnect PEs compute spatial data reuse II detail volume calculation TotalVolume TotalVolume sum AD ReuseVolume ReuseVolume sum AD AD UniqueVolume UniqueVolume TotalVolume ReuseVolume   TotalVolume UniqueVolume performance model relation centric notation compute various performance metric precisely formulation metric naturally model operation relation compute performance metric data transfer data reuse latency data reuse volume model model hardware metric define data volume specific volume calculate sum function denote sum cardinality II formula metric TotalVolume tensor data access entire spacetime stamp TotalVolume sum volume tensor data access across entire iteration domain metric maximum data transfer PE scratchpad TotalVolume tensor calculate stamp data stamp data stamp data stamp data TotalVolume ReuseVolume reuse data across multiple spacetime stamp ReuseVolume sum volume data overlap adjacent spacetime stamp spacetime MD determines adjacent spacetime stamp  restrict interconnection relation stamp constraint equation contribute data reuse stamp constraint define interval within data reuse assume interconnect systolic topology interval stamp stamp ReuseVolume tensor calculate stamp reuse data stamp stamp reuse data stamp stamp reuse data stamp ReuseVolume multicast interconnection relation interval constraint data reuse via UniqueVolume unique tensor data access tensor data expands stamp increase UniqueVolume sum volume data spacetime stamp data assign spacetime stamp unique cannot fetch adjacent spacetime stamp adjacency spacetime constraint MD ReuseVolume metric minimum data transfer PE scratchpad stamp stamp UniqueVolume tensor calculate stamp data stamp data stamp data stamp data UniqueVolume  describes average data reuse fetch scratchpad memory  amount data reuse across multiple stamp spatial reuse originates broadcasting tensor data multiple PEs  sum volume data spatial reuse stamp interconnect PEs  amount data reuse across multiple stamp within PE temporal reuse data reuse across stamp avoid overlap   restrict  refers temporal reuse PE PE clearly ReuseVolume sum   latency bandwidth model latency dataflow consists communication computation delay PE assume buffer network arithmetic pipelined fashion technique buffering hide latency overall latency maximum communication delay computation delay model communication delay calculate volume data transfer chip scratchpad local register PE fetch data PEs fetch data scratchpad avoid longer latency therefore volume data transfer scratchpad register estimate UniqueVolume input tensor volume data output register scratchpad estimate UniqueVolume metric output tensor communication delay estimate  UniqueVolume input bandwidth  UniqueVolume bandwidth bandwidth available scratchpad bandwidth compute delay estimate loop instance average active PEs  sum DS  PE  average PE utilization across timestamps computation delay calculate bandwidth requirement namely interconnection bandwidth ibw scratchpad bandwidth SBW ibw refers data communication PEs estimate ibw   SBW refers data transfer PE array scratchpad SBW requirement estimate normalize UniqueVolume computation latency SBW UniqueVolume  model implementation performance analysis leverage isl library  library perform integer operation specifically operator isl  library calculate metric relation dataflow interconnection data assignment implement isl union structure isl isl union reverse calculates reverse operation  isl union apply composite relation equation isl union isl union  sum operator sum stamp calculate metric TotalVolume UniqueVolume equation VI evaluation setup benchmark evaluate tenet important tensor kernel gemm 2D conv matrix multiplication chain MMC  tensor khatri rao MTTKRP jacobi 2D gemm 2D conv MMC widely scientific engineering computation MTTKRP tensor operation bottleneck operation tensor factorization recommender dataflow notation various  application benchmark dataflow relation centric data centric temporal spatial gemm IJ ijk apply tpu PE KJ ijk PE IK ijk PE PE SpMap TpMap TpMap PE SpMap TpMap TpMap 2D conv KC OY  PE  OY  PE KC  PE OX OY PE SpMap TpMap TpMap RX TpMap RY TpMap RY RY RY TpMap RX RX RX OY OX PE SpMap TpMap TpMap RY TpMap RX TpMap RY RY RY TpMap RX RX RX  OY OX motivate eyeriss PE TpMap TpMap SpMap RY TpMap RX cluster RY TpMap TpMap SpMap SpMap RY  OY OX motivate shi diannao PE TpMap TpMap SpMap RY TpMap TpMap RY RY RY TpMap RX RX RX cluster SpMap RX KC OY OX motivate nvidia PE SpMap TpMap TpMap RY RY RY TpMap RX RX RX TpMap RY TpMap RX cluster SpMap MTTKRP IJ  PE KJ  PE KL  PE jacobi 2D PE IJ PE MMC IJ  PE KJ  PE jacobi 2D dimensional stencil operation image processing 2D conv gemm MTTKRP MMC jacobi 2D evaluate tensor application IV googlenet mobilenet neural network domain gemm 2D conv alternate ALS MTTKRP core operation transformer network architecture translation task feature MMC operation comparison tenet maestro data centric notation comprehensive performance analysis compute centric notation IV tensor application application domain tensor operation data googlenet 2D conv params layer mobilenet 2D conv params layer ALS matrix fabrication MTTKRP transformer nlp MMC expressiveness data centric notation lack performance model clearly tenet maestro gemm 2DCONV benchmark source code maestro configure framework parameter PE bandwidth buffer interconnection network mesh network maestro model hierarchical PE array assumption PE communicate adjacent PEs dataflow comparison popular dataflows evaluate framework dataflows accord maestro  tenet KC  tenet   latency maestro IJ tenet KJ ijk tenet IJ ijk latency 2D conv dataflows gemm dataflows bandwidth cycle bandwidth cycle dataflow  data centric relation centric notation bandwidth requirement                 evaluation application stamp stamp relation centric notation function modulus multi dimensional stamp innermost dimension simplicity OX OY dataflow 2D conv assigns dimension PE array dimension stamp involves OX OY respectively dataflow affine transformation transform dimension IJ ijk dataflow gemm dimension stamp dataflows data centric notation tenet 2D conv dataflows KC OY   OY  cannot data centric notation affine transformation relation centric notation expressive data centric notation  notation opportunity exploration dataflows essential efficient spatial architecture dataflow depict optimal dataflows data centric notation bandwidth cycle latency dataflow data centric dataflow bandwidth decrease dataflow KC OY  achieves latency reduction optimal dataflow data centric gemm dataflows KJ ijk IJ ijk cannot data centric notation bandwidth cycle dataflow IJ ijk achieves latency reduction dataflow data centric overall tenet achieves latency improvement average data centric notation identify sophisticated dataflows 2DCONV gemm respectively latency bandwidth requirement 1D sys 2D sys mesh maestro 1D sys 2D sys mesh maestro 1D sys 2D sys mesh maestro 1D sys 2D sys mesh maestro 1D sys 2D sys mesh maestro 1D sys 2D sys mesh maestro PE PE PE PE PE PE 2D conv gemm runtime comparison tenet maestro optimal maestro dataflow tenet dataflow benchmark IV latency normalize ideal latency theoretical performance calculate multiplier frequency bandwidth estimate normalize UniqueVolume computation latency maestro model cannot ALS transform application due unsupported operator tenet overall tenet latency reduction reduces bandwidth requirement googlenet mobilenet respectively execution tenet  model dataflow conduct PC core 0GHz   cpu 8GB memory average model dataflow maestro tenet difference mainly tenet model dataflow integer linear program considers architectural detail evaluation interconnection data assignment however maestro calculates metric polynomial faster inaccurate estimation VI model increase complex interconnect sensitive PE array exploration dataflow tenet propose prune restrict data movement assignment relation enumerate data movement PE interconnection systolic multicast tensor data movement rectilinear affine transformation enumerate data assignment boundary PEs dataflow 2D conv loop enumerate legal data movement tensor respectively limit data assignment boundary PEs dataflows explore tenet efficient exploration future performance metric evaluation evaluation critical metric temporal spatial data reuse input output tensor maximum PE utilization average PE utilization latency temporal spatial data reuse calculate normalize                                                                 temporal reuse data spatial reuse data                                                                                         gemm 2D conv MTTKRP jacobi 2D    max PE utilization avg PE utilization latency critical metric analysis tensor application instance respectively evaluation systolic interconnection topology apply dataflows evaluate dataflows 2DCONV dataflows variation reuse PE utilization latency dataflows exhibit temporal spatial data reuse however data reuse necessarily latency latency  OY OX dataflow dimension RY cannot PE array therefore PE utilization OY OX PE utilization latency input reuse therefore load input tensor increase overall latency 2DCONV dataflows latency utilize PEs data reuse tensor gemm benchmark dataflows dataflows dimensional stamp IJ ijk KJ ijk IK ijk outperform dimensional stamp dimensional stamp expose data reuse opportunity dataflows PE utilization IK ijk KJ ijk latency input reuse input tensor MTTKRP benchmark dataflows performance PE utilization reuse factor tensor nevertheless IJ  dataflow reuse input tensor hence outperforms others dataflow PE utilization data reuse dataflows performance usually fail aspect demonstrate framework capable capture spatial temporal reuse separately IJ ijk dataflow gemm spatial reuse temporal reuse tensor PE array temporal reuse spatial reuse tensor stationary PE array bandwidth analysis evaluate bandwidth requirement interconnect topology 1D systolic 2D systolic mesh model interconnects assume exist multicast broadcast data PEs cycle 2DCONV dataflows exhibit data reuse overall evaluate topology bandwidth requirement dataflow tensor application typically regular computation data access distance communication across PEs rarely benchmark jacobi 2D computation intensive scratchpad bandwidth others bandwidth requirement tensor dataflow gemm dataflows illustrate phenomenon dataflow IJ ijk maximizes output reuse output stationary PE therefore bandwidth requirement mainly input tensor topology variation  OY OX 2D conv dataflow jacobi 2D dataflow mesh topology interconnection bandwidth scratchpad bandwidth input tensor dataflows exhibit diagonal input data reuse cannot systolic topology dataflows leverage mesh noc increase data reuse scratchpad bandwidth requirement hence reduce   2D conv dataflow 1D systolic interconnect ibw 2D systolic interconnect SBW ibw tensor stationary PE across stamp summary interconnection network connects PEs necessarily reduce scratchpad bandwidth requirement interconnection network data movement account ibw SBW ibw SBW ibw SBW ibw SBW ibw SBW ibw SBW ibw SBW ibw SBW ibw SBW ibw SBW ibw SBW ibw SBW ibw SBW ibw SBW ibw SBW 2D sys mesh 1D sys 2D sys mesh 1D sys 2D sys mesh 1D sys 2D sys mesh 1D sys 2D sys mesh 1D sys    OX  OY OX  RX KC OY OX input input output ibw SBW ibw SBW ibw SBW ibw SBW ibw SBW ibw SBW ibw SBW ibw SBW ibw SBW 2D sys mesh 1D sys 2D sys mesh 1D sys 2D sys mesh 1D sys IJ ijk KJ ijk JK ijk input input output ibw SBW ibw SBW ibw SBW ibw SBW ibw SBW ibw SBW ibw SBW ibw SBW ibw SBW 2D sys mesh 1D sys 2D sys mesh 1D sys 2D sys mesh 1D sys IJ  KJ  KL  input input input output ibw SBW ibw SBW ibw SBW 2D sys mesh 1D sys IJ normalize bandwidth requirement gemm 2D conv MTTKRP input jacobi 2D output bandwidth analysis interconnection topology latency estimation latency PE utilization PE utilization estimation error rate error rate latency estimation latency PE utilization PE utilization estimation error rate error rate   latency utilization maestro maestro tenet error rate maestro maestro tenet latency PE utilization comparison maestro eyeriss alexnet MAERI vgg VGGNet conv conv conv accord MAERI PE utilization dense cnns metric estimation comparison tenet maestro accuracy latency PE utilization reuse estimation propose model improves metric estimation accuracy previous accelerator 2DCONV tensor input tensor filter dimension output channel dimension input channel latency comparison latency calculate maestro stationary dataflow propose eyeriss alexnet dataflow propose MAERI vgg report latency PE utilization eyeriss MAERI golden maestro implementation input file comparison overall improves latency estimation accuracy eyeriss MAERI improvement partially due tenet capability model complex dataflow affine transformation alexnet filter varies increase PE utilization eyeriss distributes dimension PEs relation centric notation feature assign stamp affine transformation PE array however maestro model due limitation data centric notation limitation explains error layer another eyeriss dataflow data centric notation denote maestro notation maestro cluster primitive merge multiple channel dimension PEs utilized accuracy maestro MAERI feature reconfigurable reduction configure enable multiple vector dot reduction dataflow multiplier PEs PEs via multicast interconnection tenet applies affine transformation denote MAERI dataflow assigns multiple dimension 1D PE array implementation maestro manually transform dimension filter input 1D PE array MAERI tenet improves accuracy thanks accurate reuse analysis PE utilization comparison tenet estimate PE utilization accurately another factor affect latency equation average PE utilization rate maestro eyeriss report metric directly approximate truth   latency obtain eyeriss framework predict        mobilenet conv conv conv conv conv     alexnet    vgg            KC   googlenet conv conv conv conv conv  data reuse comparison maestro layer accurate prediction layer unlike previous approach estimate PE utilization via polynomial compose PE array computation tenet model PE utilization stamp PE assign reuse comparison tenet precisely calculate  affect latency data transfer evaluate reuse analysis maestro dnn model alexnet vgg googlenet  dataflows model alexnet stationary dataflow PE array motivate eyeriss vgg output stationary dataflow PE array motivate shidiannao normalize dataflows execute PE array goal accurately capture characteristic dataflows instead conv layer alexnet layer eyeriss RY dimension dimension PE array PEs OY dimension PE array filter array spatial reuse factor OY reuse across PEs horizontally besides filter array stationary innermost dimension stamp OX contributes temporal reuse factor OX therefore reuse factor accurately calculate maestro output array PEs vertical output data spatial reuse factor dimension PE array moreover temporal reuse factor PE filter width RX input channel continuously reuse factor accurately calculate maestro report reuse output array circumstance likely dimension output array OX OY explicitly specify primitive inaccuracy vgg googlenet mobilenet filter reuse inception calculate tenet maestro actual filter reuse depends input image mobilenet convolutional layer namely depthwise convolution conv pointwise convolution conv depthwise convolution input channel directly output without accumulation therefore layer input data reuse pointwise convolution filter reuse input data maestro accurate data reuse estimation formula maestro data dimension explicitly specify primitive hence reuse report output array maestro analyzes data reuse innermost temporal spatial dimension analyze data reuse multiple dimension maestro mapping multiple data dimension PE dimension technique apply eyeriss PE array vii related WORKS notation express dataflow compute centric notation widely adopt directly dataflow directive dataflow  hyperplanes polyhedral dependency graph timeloop describes concise unified loop representation mapping directive mapping directive consist memory constraint PE workload assignment interstellar hardware dataflow halide schedule exploration interstellar extends halide additional directive loop resource allocation specify hardware feature recently propose data centric notation spatial temporal specify dataflow transformation theory widely systolic architecture mainly focus arithmetic array transformation partition coordinate systolic array recent effort attempt apply theory automatic fpga code generation   compile application polyhedral IR systolic array  combine polyhedral hardware optimization primitive generate performance systolic array FPGAs model optimization performance model guideline insight optimize dataflow prior performance model mainly focus dnn application spatial architecture eyeriss analyzes data reuse tensor however lack performance model evaluate performance maestro applies data centric notation analyzes data reuse latency interstellar explores loop operator loop split loop reorder prior aim model spatial architecture application propose constraint centric schedule algorithm determines placement rout reconfigurable architecture dataflow computation graph  approach application mapping recently propose  built parameterized hardware module template relation centric notation dataflow tenet  automatically generate dataflow hardware chisel conclusion propose relation centric notation dataflow data assignment interconnection uniformly relation relation centric notation dataflow tensor benchmark prior notation analytical framework accurately estimate data reuse bandwidth requirement latency evaluate framework tensor benchmark relation centric notation sophisticated dataflows latency data centric notation tenet achieves latency reduction stateof technique