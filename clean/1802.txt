due deployment application safety critical robust secure execution workload imperative adversarial input carefully mislead machine model challenge attack detect defeat dominant approach defend adversarial systematically network architecture sufficiently robust neural architecture NAS heavily facto approach robust neural network model accuracy detect adversarial metric neural network robustness NAS proven effective improve robustness accuracy NAS generate network model noticeably typical dnn accelerator craft network mainly dnn accelerator optimize robust NAS generate model inherent multi NAS generate network unacceptable performance overhead bridge gap robustness performance efficiency application rethink AI accelerator enable efficient execution robust auto generate neural network propose novel hardware architecture NASGuard enables efficient inference robust NAS network NASGuard leverage heuristic multi mapping model improve efficiency underlie compute resource moreover NASGuard address load imbalance computation memory access task multi parallel compute finally propose topology aware performance prediction model data prefetching fully exploit temporal spatial locality robust NAS generate architecture implement NASGuard verilog rtl evaluation NASGuard achieves average speedup baseline dnn accelerator index robust NAS network adversarial dnn accelerator introduction widespread deployment application safety critical attack potentially deter adoption application extensively adversarial carefully craft misguide highly accurate model challenge various systematic defense technique propose generate neural network architecture robustness correspond author rui hou mail   primary optimization function defend adversarial attack systematic approach commonly neural architecture NAS generate robust model NAS network architecture parameter exhaustively improve robustness machine model summarizes recent approach robust NAS network typical robust NAS network model adversarial attack source   PGD deepfool    pixel   DARTS FGSM PGD  ICML  MI FGSM PGD bim  RAS FGSM PGD pixel aisec  FGSM PGD  arxiv rbf cnn MI FGSM ead arxiv  FGSM mim PGD arxiv contrast traditional neural network dnns robust NAS network exhibit significantly network topology dataflows NAS primarily computational contains multiple building network complex highly concurrent dataflows inter communication backward dataflows finally network usually intra inter shortcut dense connection realize layer data communication significant difference network structure challenge exist dnn accelerator dnn accelerator dnn network robust NAS network inefficient inference network inefficiency mainly stem aspect typical industrial dnn accelerator tpu NVDLA execute serially robust NAS network  multi fabric fail efficient parallel execution characteristic network architecture depth wise convolution  nonlinear computation prevent fully utilize chip compute resource memory bandwidth finally dnn accelerator architecture academia UI OOVBM  PNQVUFS SDIJUFDUVSF acm annual international symposium computer architecture isca doi isca diannao eyeriss adopt serial execution cannot fully exploit data locality across multiple fully reuse internal data inference intersection AI algorithm security dnn acceleration goal accelerator robust NAS network fully utilize accelerator computation resource improve data locality knowledge propose designate accelerator architecture robust NAS network architecture NASGuard fully exploit grain parallelism multiple dataflows hardware maintain data communication synchronization requirement enable efficient execution network analysis robust NAS network network compute characteristic accordingly propose robust NAS network inference accelerator manycore architecture parallel computation multiple within contribution propose accelerator architecture robust NAS network discus analyze limitation dnn accelerator robust NAS network commonly mechanism defend adversarial attack analysis accelerator architecture efficiently inference robust NAS network multi mapping model allocation computation storage resource  scheduler hardware mapping generate heuristic multi mapping model allocate multi computation resource NASGuard alleviate data traffic congestion shorten communication distance consequently improve data reuse reduce data operation employ multi granularity dynamic schedule mechanism alleviate load imbalance schedule mechanism granularity propose achieve computational load balance across avoid performance bottleneck network communication implement hardware prefetch operation fully exploit temporal spatial locality robust NAS network monitor propose prediction information generate  prediction model prefetch backward activation previous layer significantly improve performance inference evaluate NASGuard architecture representative robust NAS network workload experimental NASGuard successfully utilizes processing PE memory bandwidth reduce requirement prefetch buffer capacity improve resource utilization enables NASGuard achieve speedup average baseline architecture exhibit scalability organize II summarize feature robust NAS network weakness dnn accelerator IV overview NASGuard compiler respectively VI depict  mapping model multi granularity schedule mechanism respectively vii introduces performance prediction model prefetching mechanism evaluates performance consumption NASGuard IX discussion related introduce XI concludes avg relu conv avg  conv  conv conv  conv conv wise typical structure conv image softmax typical robust NAS typical structure robust NAS network contains compose depth wise convolution layer convolution layer multiple dataflows layer data communication II background motivation feature robust NAS network robust NAS network usually built upon hierarchy contains multiple operation individual convolution convd depth wise convolution relu pool etc interweave dataflows quantitative analysis typical robust NAS network characteristic robust NAS network heavily multiple network structure multiple network structure prevalent typical robust NAS network addition structure multiple account entire network model correspond average importantly execution layer robust NAS network backward dataflows parallel typical dnn network dataflows router PE GLB router PE GLB router PE GLB router router router PE GLB router PE GLB PE GLB router router PE GLB PE GLB PE GLB instruction fetch decoder multi scheduler multi monitor schedule prefetch buffer PE configuration etc processor chip fabric crossbar host cpu dram pcie activation manage instruction pool memory controller pcie controller PE cluster PE cluster architecture NASGuard accelerator multiple belonging convolution kernel inside data communication percentage external communication backward dataflows computational graph multi dataflow structure usually converge feature representation dimensional feature generally concat wise operation latter utilized frequently application vast shortcut dense connection exist intra inter robust NAS network shortcut dense connection introduce address vanish gradient network structure critical develop highly accurate dnns connection pas feature data downward achieve deeper network data dependency prevalent layer data communication across multi input output node irregular dataflows data dependency frequent access chip memory reduces efficiency exist dnn accelerator addition retrain typical dnn network adversarial training resist adversarial attack complex dataflows diverse network structure network architecture various challenge exist AI therefore vital develop unified dnn accelerator architecture characteristic various network robust NAS network objective correspond accelerator architecture maximize data reuse resource utilization multi parallel execution efficiently deployed perform inference prediction objective NASGuard deploy robust NAS network exist dnn accelerator architecture dnn accelerator usually adopt monolithic systolic array architecture execute dnn layer serially usually dimensional PE array hierarchical chip buffer systolic array architecture google tpu cannot efficiently tap parallelism robust NAS network polygonal computation communication ratio ccr various operation operation communication exacerbate dnn accelerator underutilization moreover dnn accelerator tpu execute within serially severe underutilization resource average utilization rate PEs although multi core architecture dadiannao multi parallel compute coarse grain task schedule resource utilization network operation PE utilization bandwidth utilization rate therefore compose multiple network operation burden performance although partial feature layer fusion technique reduce memory traffic improve performance brings computation redundancy increase demand chip buffer capacity NASGuard layer fusion technique robust NAS network limited chip buffer capacity reuse data communication within multi core ineffective multi dataflows communication fails minimize chip memory traffic core architecture eyeriss simultaneous execution robust NAS network multiple finer granularity data locality multiple within fully exploit due lack efficient mapping compute communication resource lack highbandwidth access memory intensive inability fully exploit intra inter dependency inability reuse data previous layer shortcut dense connection conv depth wise conv conv FC pool relu utilization PE utilization bandwidth utilization conv depth wise conv conv FC pool relu utilization PE utilization bandwidth utilization computation communication ratio PEs bandwidth utilization typical robust NAS network aim aforementioned propose optimize computational efficiency robust NAS network computational perspective mapping computational core architecture data locality improve resource allocation apply multi grain dynamic resource schedule hardware resource optimize data prefetch reduce idle PE array architecture  goal NASGuard fully exploit accelerator compute resource improve data locality heuristic multi resource mapping NASGuard multi granularity schedule mechanism layer tile load balance computation task implement hardware prefetcher compiler NASGuard overview overall architecture NASGuard depict contains component compute resource parallel execution multiple computation mapped PE cluster cluster consists multiple PE PG PG equip GLB NASGuard parallel execution dynamically allocates resource workload balance runtime specifically execution significantly shorter execution release resource longer execution execution release resource shorter execution steal data longer execution data steal scheduler status register guarantee datum memory hierarchy NASGuard memory hierarchy private buffer PG input data PE array compute GLB PG input output activation chip memory prefetch buffer prefetch buffer serf memory hierarchy pgs offchip memory fetch activation processor mainly distribute schedule command component multi scheduler mainly responsible allocation computational resource multi mapping dynamical schedule load balance execution data prefetching operation performance prediction model NASGuard efficient parallel compute multiple network within effective data prefetching shortcut dense connection PE microarchitecture illustrates PG microarchitecture depict PG comprises PEs PEs private buffer within PG compute capability convolution inner layer customizable normalization pool activation module nonlinear layer feature user generate concrete accelerator pgs PEs  pgs unidirectional link input data PG index pgi adjacent PG index pgi PE ofmap psum  input buffer buffer mux output buffer PE PE mux mux prefetch buffer  mux  PE output prefetch buffer chip dram pgi psum output pgi psum input GLB private buffer microarchitecture PG private buffer private buffer fetch input data local GLB obtain prefetched activation prefetch buffer split buffer input neuron buffer NBin output neuron buffer NBout intermediate feature data NBin NBout logically swap efficient data transfer synaptic buffer SB across PE within PG  GLB implement central buffer multiple physically distribute local buffer NASGuard distribute concurrent execution multiple pgs local GLB logically sub buffer namely input activation buffer output activation buffer buffer data channel wise wise performs addition calculation correspond data channel multiple node input typical dnn accelerator channel data node chip memory data channel node calculate data previous channel reload source operand memory access robust NAS network compile compiler hardware properly data adder NASGuard designate data channel adder additional adder introduce avoids chip dram access computation correspond data channel node imperative synchronization mechanism concurrent operation however adopt fledge coherence protocol lock synchronization scheme FCFS policy conduct wise operation channel intermediate data unused directly chip memory multi scheduler multi scheduler component schedule schedule load balance schedule etc activation management monitor invoked whenever task dispatch grain schedule perform hardware prefetch operation trigger function allocate resource parallel execution NASGuard initializes schedule hardware mapping information generate heuristic multi mapping model accord hardware configuration information schedule scheduler configures noc rout PE cluster GLB partition implement schedule mechanism hardware implementation schedule corresponds entry schedule corresponds layer within schedule entry contains layer tile schedule information layer tile index layer tile counter layer tile dependency etc grain schedule initialize statically information generate compiler monitor trigger hardware prefetch operation activation management address activation monitor sends execution status node multi scheduler scheduler detects execution node approach prefetching specify compiler correspond prefetching operation trigger prefetch buffer microarchitecture evaluate multi memory architecture environment memory hierarchy memory hierarchy contributes reduction maximum data reuse robust NAS network temporal spatial localization data access importantly utilize chip memory structure prefetch buffer choice cooperate multi scheduler prefetch data timely switch logic SB SB prefetch buffer index pool empty NBin chip dram crossbar crossbar multi prefetch buffer prefetch buffer multi hierarchy unique physical index dedicate access empty information hardware maintain management data prefetch buffer prefetch buffer inactive correspond index recycle improves utilization chip prefetch buffer logical buffer buffer hide latency maintain bandwidth activation buffer reuse lifespan feature tensor multiple layer usually opportunity buffer reuse global liveness analysis apply computation graph dnn model buffer reuse NASGuard contains parallel PE cluster PE cluster access activation entire SRAM logical buffer enable concurrent access parallel PE cluster NASGuard achieves grain reuse activation buffer via flexible buffer management mechanism IV  compiler robust NAS network stable execution behavior memory access deterministic execution predictable characteristic compiler efficient resource schedule hardware software collaboration illustrates overall workflow NASGuard compiler specification robust NAS network input par computational graph heuristic multi mapping model topology aware performance prediction model computational graph network input generate configuration within configuration relevant information robust NAS network hardware mapping information schedule information activation management information loop orient optimization compiler backend executable binary runtime environment host cpu configuration initialize schedule activation management within multi scheduler perform task schedule resource allocation NASGuard accelerator buffer index status activation management management multi scheduler layer info tile info PE info info configuration PE array robust NAS model specification   parser instruction generator buffer index status HM TAPM simulator compiler PE utilization computation communication behavior dram bandwidth requirement etc  dataflow dataflow graph accelerator parameter PE cluster optimization schedule resource allocation executable binary overall workflow NASGuard heuristic multi mapping model HM responsible allocate compute storage resource multi parallel execution topology aware performance prediction model TAPM prediction information node robust NAS network compile coordinate multi scheduler data prefetch operation runtime model alleviate manual effort derive optimal parameter configuration achieve data locality parallelization experimental demonstrate NASGuard utilized peak hardware capability achieve computational efficiency typical robust NAS network multi mapping mechanism allocate resource demand exploit computational parallelism data locality multi mapping upon principle allocate reasonable amount compute resource accord recommend execution onto nearby PE cluster simultaneous execution efficient inter schedule data dependency frequent communication mapped onto PE cluster reduce inter cluster traffic backward dataflows consumer dependency pipeline data reuse reduce potential data congestion heuristic multi mapping model conforms principle propose generate hardware mapping information schedule initialize hardware mapping information generate extract acyclic graph dag compiler analyze relationship dependency various operator addition information compute intensity memory intensity resource utilization execution obtain simulator data agglomerative hierarchical cluster mechanism AHC adopt classify AHC technique classify distinct cluster cluster advantage  difference execution multiple shortest SJF strategy adopt tune cluster hardware mapping dependency aware statically schedule hardware mapping computation storage configuration PE cluster initialize schedule layer layer addition attach contains resource schedule information PE cluster resource merge dynamically reallocate resource within VI multi granularity scheduling mechanism load  inter layer schedule execution behavior parallel execution multiple issue load imbalance improve memory bandwidth resource utilization PE cluster inter schedule algorithm layer algorithm contains policy allocate resource execution computation layer execution detect monitor predict empirical margin purpose configuration unchanged resource allocate intra tile schedule feature usually chip buffer tile technique usually opportunity accelerate tile execution workload steal strategy grain schedule release compute resource execution PE cluster steal input feature tile unprocessed tile another PE cluster resource utilization monitor detects difference execution predict predefined threshold workload steal strategy trigger specially scheduler dispatch instruction unprocessed tile idle PE cluster PE cluster modify destination address tile execution instruction instead implement steal scheduler hardware monitor correspond schedule parameter generate compiler dynamically detect workload imbalance multi execution scheduler configure crossbar properly feature tile fetch slowest PE cluster idle PE cluster vii performance prediction model data prefetch multi scheduler prefetches activation accord schedule activation management meanwhile monitor detect execution status opportunity inference performance predictable robust NAS network compose multiple computational structure network fix inference computation memory layout variant compile computation memory behavior layer highly deterministic predict execution data dependency issue prefetching operation advance verify predictability inference conduct hardware configuration evaluate rtl implementation NVDLA performance model typical dnn network alexnet googlenet resnet deviation mainly due difference theoretical PE bandwidth utilization rate validate modify maestro performance model verilog simulation NASGuard network hardware configuration execution estimate modify maestro average away cycle accurate verilog simulation topology aware performance prediction model TAPM due predictability inference adopt maestro performance analysis model topology aware performance prediction model TAPM evaluate performance memory access accurately estimate latency complex network TAPM predict node execution node execute sequentially within execute parallel within overall execution TAPM computation graph network hardware configuration PG GLB input generates predict execution node within schedule compute intensive priority memory intensive analyze relation layer TAPM data access layer prefetched advance prefetching technique effective improve utilization compute memory bandwidth reduces configuration latency without prefetching prefetching activation performance node predict TAPM compiler prediction information schedule multi scheduler multi scheduler manages execution network layer contains information operation execution dependent relation etc inference monitor dynamically monitor execution status monitor execution threshold multi scheduler issue prefetch operation load correspond activation prefetch NASGuard information schedule predict backward direction perform prefetches hardware counter apply update layer dependency schedule dimensional array layer dependency parameter previous layer dependency NASGuard dependency layer index dependent relation upper layer layer execution hardware counter correspond dependency minus counter dependence becomes perform prefetch operation management prefetch implement prefetch buffer index pool cache multiple status register prefetch buffer processing status request request related data access status update synchronously management activation prefetch NASGuard multi scheduler prefetch activation previous layer advance shortcut dense layer activation prefetching adopts prefetching activation management manage activation data correspond prefetch buffer buffer assign initialize compile however partition dynamically execution evaluation methodology implement NASGuard verilog rtl architectural parameter evaluate platform comparison typical core architecture without multi scheduler prefetch buffer baseline synopsys compiler SP SMIC technology synthesize timing consumption estimate synopsys primetime PX accord simulated dump file ass consumption chip memory access ddr memory model integrate bandwidth GB sec average consumption data access simulation methodology developed cycle accurate simulator maestro source simulator flexible simulation framework dnn accelerator simulates dataflows chip memory hierarchy noc fabric extend PE PG schedule register inside logic multi scheduler processor besides enhance cache prefetch buffer addition TAPM prediction model  sim maestro tune II network evaluation network layer operation MACs   dense  etc   dense  etc   convd shortcut etc   convd shortcut etc  NAS shortcut  etc  NAS 8G shortcut  convd etc densenet 9G dense convd inception 5G inception module convd pool etc resnet 4G shortcut convd etc parameter breakdown architectural baseline NASGuard component PEs per PG mux partial sum register PG IO FIFOs buffer KB KB input output buffer KB KB logic GLB buffer MB KB pgs GLB router  processor scheduler prefetch buffer KB accelerator accuracy verilog implementation adjust accuracy simulator accurate profile function workload II introduces evaluate model typical dnns NAS network NAS robust NAS network  various application image classification segmentation recognition popular network medium mbytes memory footprint moreover structural feature robust NAS network experimental performance speedup overhead overall performance speedup reduction illustrates experimental baseline architecture NASGuard achieves speedup reduction baseline average  typical characteristic robust NAS network speedup another  proportion depthwise convolution PE  benefit NASGuard optimization technique steal strategy performance speedup NASGuard modest  network important NASGuard effectively variety dnn network NASGuard strength monolithic systolic array typical dnn network usually shortcut dense connection inception module feature fully exploit NASGuard architecture improvement baseline speedup reduction NASGuard performance relative baseline NASGuard consumption baseline architecture reduction average factor execution reduce due utilization PE bandwidth static dynamic NASGuard maximizes data reuse perform pipeline computation dependent layer within data channel avoids unnecessary chip memory access effectively reduce dynamic consumption although additional logic NASGuard definitely increase static analysis explains overall reduction overhead breakdown summarizes breakdown microarchitectural component overall overhead NASGuard increase comparison baseline breakdown modular pgs GLB dominate overhead scheduler account within PG chip buffer occupy PE array overall breakdown PG breakdown pgs GLB noc processor scheduler prefetch buffer PE mux partial sum register IO FIFOs buffer input output buffer logic silicon breakdown pgs GLB noc processor scheduler prefetch buffer others breakdown network breakdown illustrates breakdown NASGuard variety robust NAS network dnns breakdown dependent characteristic network  network consumption noc logic prefetch buffer network induces prefetching operation workload imbalance induced parallel execution multiple PE cluster trigger steal operation baseline architecture static consumption NASGuard increase overhead mainly multi scheduler prefetch buffer former overhead increment increment contribute latter related logic performance evaluation evaluation multi mapping model layer cluster parallel mapping lcp accelerate inception network multi structure apply core accelerator architecture verify effectiveness propose heuristic multi mapping model HM lcp conduct comparison NASGuard HM achieves average performance improvement lcp inception achieves performance speedup lcp aim optimize sequential dataflow inception module propose HM optimizes inception module parallel execution reasonably allocates resource moreover pipeline speedup lcp HM comparison mapping strategy speedup prefetch prefetch performance improvement prefetch evaluation dependent robust NAS network reduce chip memory access noc congestion internal data communication lcp maximize reuse input data improve resource utilization cluster operation compute cannot optimize complex network diversified compute operation inside evaluation performance prediction model prefetch mechanism scenario TAPM evaluation prediction timely prediction lazy prediction overall TAPM effectively predicts execution node false prediction essence effectiveness TAPM ability reduce PE array ability closely related policy timely prediction lazy performance enable disable prefetch functionality NASGuard prefetch witness average performance improvement without prefetch non prefetching   performance speedup respectively input feature prefetched account input data structural characteristic network opportunity exploit data locality prefetching technique reduces memory bandwidth improves resource utilization memoryintensive network memory locality  performance improvement enable prefetch limited compute intensive network  multiple  multi scheduler opportunity dispatch prefetching operation evaluation combination prefetch load balance schedule obvious baseline architecture severely suffers issue irregularity memory access load imbalance performance baseline NASGuard employ prefetching multi granularity schedule mechanism NASGuard effectively address workload imbalance achieve utilization computational resource    layer reduce opportunity overlap prefetching activation achieves limited performance improvement robust NAS network achieve performance improvement prefetching multi granularity schedule mechanism speedup        inception resnet geomean sensitivity analysis PE quantity speedup PG PEs PG PEs default PG PEs PG  sensitivity analysis PEs per PG sensitivity analysis scalability sensitivity analysis PE quantity PG PEs PEs baseline performance improvement almost linearly PEs   dominate network achieves linearly performance PEs PE increase computational performance increase linearly PE utilization decrease significantly increase PE  parallelism data locality another performance dependent latency noc communication sensitivity analysis PE per PG core architecture expose parallelism PE parallelism PG grain plenty adjacent output PG parallelism coarse grain usually generates independent output feature evaluates sensitivity PE per PG PEs varied within PG employ PEs choice inter PE fragmentation aggravates limited parallel multiplier reside PG PG PEs indicates opportunity parallelism beneficial utilization improvement however PEs limited performance improvement due algorithmic load network network operation within multiple parallelism layer inherently perform tile chunk parallelize across multiple array exploit coarse grain parallelism yield resource utilization PE carefully accord characteristic load network fortunately architecture flexibly PE cluster granularity achieve requirement multi parallelism IX discussion NASGuard broaden deploy dnns majority dnn network usually structure parallelism monolithic systolic array tpu fully exploit parallelism NASGuard architecture adopts core performance loss important evolution dnns introduction structure multi structure depth wise convolution shortcut dense connection etc PE array cannot effectively exploit parallelism addition depthwise convolution operation reduce PE utilization NASGuard effectively trend core data prefetch capability fully exploit parallelism maintain PE utilization layer network typical recent dnns densenet inception evaluation eyeriss average performance speedup reduce evaluation validate aforementioned hypothesis NASGuard efficiency dnn network difference NASGuard multi neural network optimization technique software optimization technique usually accelerate inference schedule multi neural network apply computation graph splitting fusion technique perform serial execution dnn accelerator however NASGuard adopts multi scheduler manage task schedule efficiently parallel execution multiple moreover software hardware enables NASGuard reuse prefetch data various network structure shortcut dense connection related robust NAS network acceleration architecture improvement propel unique computational characteristic parallelism mechanism robust NAS network moreover NASGuard intersection AI algorithm security dnn acceleration relevant related category dnn accelerator exist dnn accelerator fix systolic array PEs compute bound dnns whereas computation memory bound robust NAS network suffer PE underutilization fail achieve performance efficiency although  adversarial detection effectively robust NAS network multiple serially executes multiple within fully exploit internal data locality workload architecture average performance improvement  reduces data reuse PE utilization execution contrast monolithic systolic array explores core architecture spatial colocation robust NAS network multi parallel execution schedule challenge  proposes quantitative systematic schema explore dnn model leakage risk dnn  hardware architecture robust adversarial demonstrate hardware architecture resist adversarial attack promising research hardware aware NAS expert disciplinary knowledge generally considerable amount compute resource hardware aware NAS obtain network model dedicate hardware platform effort challenged complexity diversity hardware extremely efficient network correspond various hardware architecture moreover exist hardware characteristic NAS network utilization hardware resource NAS network hardware architecture accuracy robustness inference performance objective dnn accelerator hardware designer evaluate comprehensive option tightly couple accelerator balance performance accuracy robustness dnn algorithm XI conclusion security critical therefore accelerate robust NAS network arguably important accelerate dnns however dnn accelerator architecture incapable ineffective robust NAS network proposes concrete dnn accelerator architecture NASGuard important gap robust NAS network acceleration architecture improvement motivate unique computational characteristic parallelism network