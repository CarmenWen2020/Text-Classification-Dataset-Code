neural network NAS project promising direction automate efficient powerful neural network architecture nevertheless NAS technique dynamically generate candidate neural network iteratively evaluate generate network architecture extremely  deployed gpu cluster dramatically hinders adoption NAS recently specialized architecture propose accelerate training inference neural network exist neural network accelerator typically target static neural network architecture suitable accelerate evaluation dynamical neural network candidate evolve NAS cannot deployed onto accelerator via compilation enable rapid efficient NAS compact  propose nasa specialized architecture shot NAS acceleration generate schedule evaluate candidate neural network architecture target machine workload significantly alleviate processing bottleneck shot NAS motivate observation considerable computation opportunity neural network candidate generate shot NAS nasa equip onchip network fusion remove redundant computation network mapping stage addition nasa accelerator partition schedule candidate neural network architecture granularity maximize data reuse improve utilization accelerator array integrate accelerate network evaluation accord multiple shot NAS task nasa achieves performance speedup consumption reduction average cpu gpu introduction technology automate procedure complex training parameterizing pre built model powerful computer however building predictive model effort conventional technique nowadays neural architecture NAS technology intensively automate efficient neural network architecture task NAS eliminates labor intensive neural network exceeds performance manually architecture task image correspond author classification detection semantic segmentation model parameter architecture becomes fully automate entry barrier become accessible user diverse background expertise rapid progress NAS technology enables  automate development machine algorithm increase requirement growth compute machine infrastructure massive typical NAS iteratively sample neural network architecture evaluate performance generate architecture target task instead optimize fix designate architecture however procedure conventional NAS explore enormous model NAS usually gpu evaluate generate network architecture consumes gpu  conduct cifar dataset methodology shot NAS mitigates computational overhead conventional approach reduce spent candidate architecture training shot NAS sample architecture directly inherit parameter supernet evaluate target task essentially shot NAS greatly reduces traditional NAS enables datasets imagenet nevertheless gpu optimal network architecture specific task promote exploration efficiency effort automate machine application specialized architecture throughput efficient NAS processing demand NAS acceleration achievable machine developer necessarily infrastructure distribute gpus customize network architecture although exist neural network accelerator focus network inference training specialized accelerator efficient NAS remains non trivial task firstly efficient NAS processor handle UI OOVBM  PNQVUFS SDIJUFDUVSF acm annual international symposium computer architecture isca doi isca heterogeneous network architecture parallel achieve throughput fortunately mechanism  NAS brings opportunity accelerator although candidate neural network architecture heterogeneous exist sub network candidate due structure feature  model identical sub network structure inside architecture candidate acceleration opportunity worth exploit  computation model data reuse however opportunity ability accelerator partition schedule dynamically generate network simultaneously reuse intermediate parameter network candidate conventional neural network processor NAS processor generate evaluate performance dynamic heterogeneous neural network architecture evolve rapidly hence dynamically neural network architecture architecture NAS processor typical neural network accelerator compiler pre deterministically static graph neural network architecture underlie computation resource sophisticated implement optimize instruction mapping scheme processor compilation processing setup amortize numerous input instance regard NAS rapidly architecture fix validation dataset cannot afford software compilation setup component instruction generation schedule optimization dynamic network specialized processor shot network architecture nasa throughput accelerator evolutionary  accelerator evaluate dynamic generate neural network architecture quickly multiple candidate neural network architecture simultaneously nasa exploit sub network dynamic network candidate eliminate computational redundancy advantage model computation network data reuse specifically contribution implement nasa specialized  accelerator automatic network architecture accelerator architecture shot NAS chip knowledge nasa accelerator target NAS automate neural network identify opportunity model computation model data reuse unique shot NAS algorithm insight efficient NAS architecture introduce optimize workflow shot NAS network fusion algorithm operation controller performance estimator sample architecture performance architecture component typical NAS schedule algorithm exploit model computation model data reuse respectively implement hardware mapping exploit model optimization quickly convert candidate NN architecture accelerator instruction implement evaluate nasa shot NAS target datasets cpu gpu datasets nasa achieves improvement performance improvement efficiency average II background neural architecture illustrates NAS controller defines strategy detail iteratively generate architecture performance estimator evaluates performance generate architecture return performance controller converge optimize architecture controller tune strategy basis performance feedback network typical chain structure network multi network network chain structure network sequence neural operation multi network introduces skip connection enlarges network construct stack multi network consist neural operation architecture strategy efficient strategy implement controller role NAS various strategy random evolutionary algorithm reinforcement gradient investigate architecture estimation performance estimator evaluates candidate neural network architecture originally network validation accuracy training network scratch computationally expensive evaluation network architecture candidate consume entire NAS procedure usually evaluation architecture converges optimize neural architecture thereby NAS extremely computationally expensive overhead prohibitively conv conv max pool sum concat input input input input avg pool output shot supernet shot NAS training candidate evaluation NAS becomes bottleneck hinders adoption NAS address enable practical NAS shot propose supernet comprise optional network operation NAS task candidate generate sample optional supernet candidate training evaluation directly inherit parameter supernet although parameter inherit supernet optimal candidate proxy rank architecture baseline NAS shot NAS reduce significantly due supernet shot effective NAS approach supernet shot compose series choice CB multiple neural operation convolution max pool complex sub network operation CB remain operation bypass input CB output previous CBs choice operation CB  input CB  CBs  hyper parameter shot chain structure network network fundamental supernet shot NAS  supernet model dataset enables model hyper parameter filter channel mixed precision quantization gain popularity  shot NAS usually supernet reuses architecture supernet training amortize effort cannot avoid recent supernet directly consume model retrain tune eliminate supernet promising approach towards automate model typical strategy shot random evolutionary algorithm due shot evolutionary algorithm efficient converge faster typical shot CBs hence evolutionary algorithm encode gene CBs generates NN candidate classic genetic algorithm despite improvement NAS framework compute overhead shot remains significant challenge compute application model datasets instance  around gpu evaluate architecture motivation mainly illustrate challenge accelerate shot NAS exist dnn processor explore unique acceleration opportunity within shot NAS challenge shot NAS dnn accelerator deploy neural network NN model onto dnn accelerator usually compiler compilation parse NN model explore  option accelerator resource usually consume jetson agx xavier NVDLA acceleration core perform mobilenet inference image within compilation mobilenet model processor core cpu ghz NN model usually static compilation instruction specific dnn accelerator reuse compilation amortize numerous potential inference however shot NAS NN candidate evaluate strategically generate evolve rapidly candidate invokes independent compilation stage evaluate target validation dataset compilation candidate dominate entire NAS avoid frequent compilation NN architecture NAS accelerator allows efficient deployment dynamically NN architecture highly demand furthermore ensure  NAS accelerator conduct online schedule optimization numerous heterogeneous NN architecture  network compiler absent opportunity computation although generate NN candidate shot NAS closely related NN candidate supernet II choice  CB usually CBs limited contrast NN candidate shot indicates CBs identical CBs configuration essentially contribute data reuse addition NN candidate evaluate fix validation dataset input data thereby computation CBs reuse input CB configuration computation data reuse typical NN candidate generate shot execute suppose CBs  digit CB refers specific configuration candidate invoke choice CB input CB candidate network perform identical operation CB output consequently network schedule CB execute reuse network deem model computation similarly CB reuse afterward network chooses CB configuration network network computation CB network execute CB separately CB configuration network input due prior computation divergence accordingly computation CBs input eliminate redundant computation fuse NN candidate CBs setup redundant computation fully remove CBs amount computational load reduce reproduce analyze probability computation redundancy practical  NAS NN candidate generate iteration evolutionary controller analysis proportion redundant operation gradually increase proceeds operation redundant average network fuse observation reveals considerable model computation exploit shot NAS opportunity data reuse computation CBs NN candidate operand inherit supernet CBs NN candidate specific CB CB network network CB network network although cannot computation due input feature CBs schedule execution parallel load external memory reuse parallel execution CBs data img img img CB data sample CB CB CB CB CB network fusion network model computation model data reuse redundant rate iteration percent redundant iteration shot fuse network model data reuse NAS implementation executes NN candidate independently model data reuse potentially reduce memory access significantly model data reuse explore conclusively discover phenomenon computation data reuse opportunity NAS processor summary NN candidate frequently shot NAS introduces challenge opportunity efficient processor architecture efficient NAS acceleration IV nasa workflow introduce optimize workflow shot NAS workflow evolutionary algorithm iterative initial population NN candidate gradually improve enable throughput evaluation NN candidate additional network mapping phase exploit model computation data reuse heterogeneous NN candidate network fusion operation schedule network generation goal network generation phase discover diverse NN candidate competitive performance achieve goal employ genome encode operation configuration partial instruction configuration network fusion operation schedule instruction generation network evaluation perf achieve configuration chip compile network mapping supernet network generation performance perf model sample label workflow nasa heterogeneous NN candidate correspond evolutionary operation crossover mutation explore efficiently genome encode nasa gene CB gene CB configuration connection gene input connectivity CB gene attribute choice choice describes operation CB describes input connection CB connection gene attribute src describes source CB ID connection gene metadata net genome CB gene belongs network evolution evolutionary operation network generation crossover mutation crossover aim inherit sub structure superior explore architecture recombine sub structure crossover operation gene NN candidate randomly attribute gene mutation aim increase diversity network architecture akin biological mutation mutation operation randomly attribute gene network mapping iterative network mapping phase essentially population NN candidate generate network generation phase onto dnn accelerator efficient performance evaluation network fusion remove redundant CBs NN candidate exploit model computation operation schedule resolve dependency operation fuse network identifies operation data batch enhance chip locality reduce outstanding memory request instruction generation fetch schedule operation dynamically generates correspond accelerator instruction issue instruction network evaluation stage network fusion exploit model computation eliminate repetitive operation NN candidate population generate NN candidate acyclic graph DAGs vertex CBs connection CBs discover model computation algorithm network fusion input network GN VN EN contains vertex output fuse network vid nid  vid foreach incoming vertex incoming attr attr indegree indegree flag rue foreach incoming exist flag alse flag rue update vertex flag alse VQ foreach incoming sub graph DAGs fuse fuse network vertex attribution attribution indegree indegree incoming exists attribution return attribute CB indegree return incoming thanks vertex img img img net net net net net net net net net OP OP OP OP OP legend  reuse input reuse reuse reuse schedule schedule graph bfs schedule greedy schedule DAGs attribute incoming vertex dag sub graph therefore complexity DAGs wherein vertex respectively however multiple DAGs sub graph DAGs complexity becomes DAGs fortunately vertex network source vertex therefore propose bypass strategy quickly filter non vertex unique vertex bypass mechanism source vertex pre non vertex fuse network directly without comparison bypass strategy network fusion procedure algorithm procedure algorithm vertex network propose bypass strategy filter non vertex potential vertex algorithm vertex network vertex attribute vertex vertex deem redundant fuse vertex fuse network correspond update vertex otherwise vertex comparison queue comparison subsequent vertex vertex vertex deem unique fuse network algorithm proceeds vertex operation schedule vertex CBs network candidate fuse unified network CBs decompose operation convolution  etc fundamental schedule operation schedule resolve dependency operation leverage greedy policy dependency operation fully exploit chip data reuse algorithm operation schedule input schedule graph output schedule vertex vertex vertex append append foreach outgo dependency dependency dependency vertex append max vertex max flag alse foreach outgo vertex flag rue flag alse max greedy schedule policy attempt maximize data reusability dependency operation fully utilize available chip compute resource define available data reusability fuse network output reuse operation output chip input operation input reuse operation data input reuse operation therefore operation schedule subsequent operation compute variation data reuse data reusability schedule graph vertex graph operation vertex execution refers data reuse operation equivalent memory access reduce operation schedule sequence clarity graph obviously schedule data reusability intuitively simply adopt depth dfs breadth bfs cannot reuse optimal schedule schedule execution bfs cannot fully exploit data reuse contrary optimize schedule data reuse utilized optimize suppose terminal vertex schedule graph link operation network schedule actually maximum hamiltonian NP optimal schedule nasa greedy heuristic algorithm schedule quickly efficiently algorithm category schedule graph vertical horizontal horizontal data reuse vertical dependency vertex vertex  source vertex incoming already schedule dependency operation schedule algorithm maintains dependency operation queue operation schedule algorithm update dependency operation queue algorithm selects operation contribute data reuse dependency operation queue attache schedule however greedy strategy ignore reuse opportunity schedule OP net immediately OP net schedule OP net longer OP net OP net dependency cannot schedule OP net hence advance skip operation ruin potential reuse operation schedule instruction generation operation schedule decision nasa generates instruction sort operation unlike traditional compile neural network static computation graph nasa leverage dynamic instruction architecture disa achieve dynamic instruction generation formally define disa instruction tuple conf operand operation conf static parameter stride kernel operand dynamic tensor parameter dimension buffer address tensor operation compilation stage nasa pre generates partial instruction neural operation CBs define correspond NAS generate partial instruction conf operand instruction assign operation gene buffer conn gene buffer perf genome buffer gene queue interconnect  CGF cgi  EU interconnect network fusion operation schedule instruction generator  CGF cgi  EU inst router tile tile tile tile tile HBM HBM HBM HBM genome accuracy mapping evaluation overview nasa architecture mapping evaluation schedule network evaluation network evaluation stage responsible estimate performance generate NN candidate nasa firstly performs NN inference target validation datasets accord instruction generate network mapping phase prediction truth nasa positive prediction obtains network accuracy validation datasets overall prediction accuracy NN candidate network generation phase utilized NN candidate nasa implementation overview nasa hardware architecture essentially implement shot workflow IV accelerator consists evaluation mapping responsible discover diverse NN architecture towards performance evolutionary algorithm evolution EU generate gene crossover mutation operation genome selection fetch genome performance extract gene gene firstly insert gene queue  EU dequeues gene gene gene generate EU sends gene genome genome buffer genome iteration evaluation responsible evaluate performance NN candidate tile accelerator multiple NN communicate mesh network chip noc coarsegrained parallel execution multiple neural operation identify compile NN architecture bottleneck shot hence nasa equip chip network mapping quickly parse network structure genome crossover mutation fetch conn gene buffer  crossover mutation gene buffer gene evolution  conn gene fetch CGF conn gene evolution  conn gene insertion cgi loop loop evolution EU generate instruction mapping exploit potential computation reuse data reuse opportunity heterogeneous NN candidate computation reuse network fusion CB fuse CBs operation data reuse operation schedule dynamically selects issue operation contribute maximum data reuse genome buffer genome buffer genome iteration specifically buffer partition data gene buffer gene connection gene buffer connection gene performance performance genome genome selection genome selection deterministic tournament selection strategy refine performance individual generation random subset genome picked population without replacement genome performance within subset genome split gene insert  evolution stage pipeline evolution gene dequeued  EU applies crossover mutation gene gene buffer accord newly generate gene EU fetch connection gene connection gene buffer apply crossover mutation EU generate connection gene newly generate gene gene available EU randomly generate remain connection gene crossover receives attribute mutation randomly target attribute mapping network fusion microarchitecture network fusion NFU essentially implement algorithm NFU CBs gene non CBs unique gene  CB NFU firstly fetch gene connection gene gene register directly gene code genome buffer mistake subsequent network evolution conn gene buffer net choice net net src flag gene buffer remapping bypass logic bypass  hash gene unique gene index index architecture network fusion decode logic operation configuration reuse estimation logic operation queue counter counter operation ptr indegree operation schedule tensor management update EN tensor ptr architecture operation schedule iteration src connection gene fetch genome buffer encode refer remapping indexed via net addition encode flag indicates source CB bypass logic flag connection gene gene false flag accord CB cannot gene directly output stage otherwise CB CBs  comparison implement  hash hash occurs CB non redundant insert  otherwise CB redundant entry CB operation CB CB  allocate entry remapping computation reuse operation schedule microarchitecture operation schedule  enable grain operation schedule without inter dependency  operation schedule  tensor management tmt register entry  indegree indicates pending input tensor operation whenever operation schedule  decrease indegree successive operation insert operation operation queue indegree become tmt information tensor NN inference operation tensor entry tmt counter operation tensor counter schedule register operation data reuse previously schedule operation reuse schedule operation register initialize zero minimum data reuse amount schedule reuse estimation logic traverse operation queue estimate volume reusable tensor operation previously schedule operation data reuse metric register  update register estimate reusable tensor candidate operation input tensor tensor ruin potential data reuse operation tensor surpasses data reuse register counter counter exists potentially data reuse  skip operation operation checked  sends operation register schedule operation queue instruction generator operation schedule operation queue dispatch instruction generator IG IG aggressively dequeue issue multiple independent operation within constraint issue width compute resource evaluation IG allocates compute tile evaluation issue operation computation overhead operation within operation operation decomposition operation sub operation accelerator tile operation decomposition fetch parameter operand tensor tmt split output channel sub operation pre compile partial instruction operation fetch empty operand assign dimension parameter memory address split tensor dynamic instruction generation evaluation evaluation responsible batch processing NN candidate target dataset performance execution execution NN candidate consume tile accelerator enable parallel NN execution NN tile tile eyeriss NN consists array processing PEs 2D mesh topology chip buffer feature PE contains register file multi function alu perform accumulate mac neural network processing network chip noc architecture independent NoCs implement evaluation chip data communication instruction distribution respectively chip data communication parameter intermediate reuse 2D mesh noc handle bandwidth transmission instruction distribution utilize network route generate instruction mapping specific tile fix latency accuracy recorder network inference completes verify accuracy recorder exemplify image classification task accuracy recorder label validation image pre load reference prediction NN candidate validation sample accordingly correspond prediction counter NN candidate accuracy recorder update entire validation dataset evaluate counter calculate prediction accuracy NN candidate genome buffer NN candidate generation iteration VI experimental RESULTS experimental setup benchmark nasa evaluate shot model    summarizes configuration target shot NAS framework mapped onto nasa accelerator shot model employ network image dataset cifar  imagenet extract image dataset  evaluate accuracy network generate iteration framework evolutionary algorithm perform iteration configuration evolutionary algorithm consistent setup benchmark report prior baseline cpu gpu baseline CPUs core xeon processor ghz gpu nvidia titan peak performance tflops nvidia titan MiB chip memory GiB HBM GB memory bandwidth baseline implement benchmark pytorch recent version cuda cudnn baseline gpu responsible dnn inference evaluation cpu algorithm generates NN candidate construct computation graph gpu execution implement model optimization multi gpu optimization cpu gpu competitive baseline gpu nvidia smi utility cpu via intel RAPL MSRs nasa evaluate nasa implementation accelerator tile accelerator tile contains PE array KiB SRAM buffer buffer MiB incorporate HBM GiB capacity GB bandwidth contains  KiB SRAM dedicate genome buffer  mapping entry KiB SRAM shot NAS benchmark configuration benchmark      chain structure  chain structure CT ict ict    normalize speedup gpu multiple gpu gpu model optimization nasa  mapping nasa chip mapping nasa network fusion nasa network fusion operation schedule speedup nasa baseline cpu gpu implement nasa rtl synthesize TSMC technology synopsys compiler performance implement  architecture simulator verify rtl implementation evaluate chosen benchmark nasa dram access latency estimate obtain DRAMSim simulator accord synthesis report nasa occupies estimate clocked ghz performance benchmark cpu gpu nasa implementation evaluate model optimization multi gpu optimization cpu gpu baseline  optimization gpu brings marginal performance improvement attribute runtime overhead network fusion stage performance bottleneck addition multi gpu speedup accord performance profile explanation shot NAS frequently transfer deploy newly generate model cpu gpus evaluation iteration severely undermines utility gpu resource analyze propose chip mapping strategy detail implement naive nasa accelerator without mapping optimization situation nasa achieves speedup cpu gpu average PE array utility implementation evaluate nasa schedule fusion optimization situation nasa achieves speedup average cpu gpu thanks chip mapping nasa quickly convert NN candidate accelerator instruction explore parallel execution CBs contrast cpu gpu baseline construct NN model software framework cpu gpu execution frequently switch NN model significantly reduces gpu utilization apply network fusion nasa accelerator benchmark baseline network fusion achieves average speedup com CT ict ict    normalize efficiency cpu gpu nasa without model optimization nasa network fusion nasa network fusion operation schedule efficiency nasa baseline cpu gpu optimize network fusion network fusion operation schedule CT CT CT    normalize searcher mapper tile memory noc breakdown nasa optimization par cpu gpu baseline eliminate redundant computation across network network fusion varies benchmark benchmark depends NAS framework impact  computation rate acceleration network fusion significant  becomes reward explanation computation exists network redundant computation chain structure performance impact operation schedule operation schedule marginal performance nasa NN operation benchmark mostly compute intensive convolution memory performance improvement operation schedule hidden computation overhead execution pipeline prefetch compute efficiency image fix amount image metric efficiency metric nasa achieves average improvement cpu gpu source efficiency boost performance speedup besides nasa consumes magnitude  baseline normalize fuse network    effectiveness model optimization network participate network fusion CB CB CB CB CB CB CB CB normalize optimize network fusion network fusion operation schedule effectiveness model optimization CB  analyze impact model optimization consumption breakdown nasa without model optimization nasa network fusion nasa respectively network fusion operation schedule improve average efficiency respectively particularly operation schedule considerable memory enhances chip data reuse reduces access dram network fusion mitigates spent component due removal redundant operation across NN candidate besides nasa accelerator spends evaluation mapping consume NN accelerator tile reduce significantly factor affect network optimization model computation data closely relevant network participate network fusion analyze influence network population nasa generate network iteration evolutionary recommend population generation around evaluate scheme fuse network analysis prof speedup achieve network population network architecture fusion expose computation opportunity performance improvement thanks bypass logic hash negligible network fusion redundant CBs network however shortcoming network population overflow chip unique normalize performance tile hardware scalability normalize performance tile schedule schedule operation schedule effectiveness nasa tile batch gene incur additional network mapping overhead besides fuse network constrain viable population evolutionary algorithm perspective nasa encourages evolutionary algorithm population detailed performance analysis  demonstrate effectiveness model optimization CBs shot model model optimization reduces spent chosen CBs  supernet CB network CB network adopt population fuse network iteration average iteration comparison scheme nasa without model optimization nasa network fusion nasa network fusion useful network operation schedule affect CBs CB choice CB input CB network identical nasa CBs instead CBs population network candidate therefore computation overhead reduce CB situation CB CB layer network computation redundancy exploit thereby model data reuse becomes relatively effective CB network computation disappears completely model data reuse scalability evaluate scalability analyze normalize performance nasa tile batch experimental performance improves almost linearly increase tile batch however batch comparable rate tile performance steadily improve mainly compute resource tile utilized tile cannot assign sufficient input instance partition batch besides fix HBM bandwidth computation significantly reduce due growth compute resource data access overhead cannot completely hidden computation slot stall pipeline situation operation schedule reduces dram access data reuse alleviate issue operation schedule tile batch tile bandwidth bottleneck operation schedule trivial impact performance however tile memory bandwidth becomes bottleneck consequently performance improvement gain operation schedule grows due reduction dram access vii related NAS NAS exceeds performance manually neural architecture task NAS NN training particularly consume datasets hence propose reduce lengthy training strategy NN candidate inherit parameter supernet acceptable NAS datasets imagenet inspire strategy shot supernet training network reduces gpu imagenet dnn accelerator neural network motivate dnn accelerator 1D inner PE array 2D spatial PE array exploit data reuse computation parallelism within neural network various optimize dataflows propose recent fpga ReRAM dnn accelerator achieve throughput inference multiple neural network tile architecture consist array dnn accelerator noc propose dnn accelerator rely compilation exploit computation parallelism data reuse thereby NAS cannot deployed accelerator directly accelerator focus dynamic neural network AI MT propose architecture multi network execution chip load balance scheduler however relies compiler parse NN model cannot handle dynamically model NAS genesys propose neuro evolutionary NE perform automate NN topology generation training contrast genesys focus NE algorithm shallow NN model nasa orient towards NAS framework effective complicate network architecture generation conclusion NAS critical AI technique automate neural architecture evaluate dynamically generate NN candidate extremely  purpose processor address issue propose nasa specialized architecture NAS acceleration generate NN candidate accord specify strategy mapping efficiently parse fuse schedule dynamically generate nns onto evaluation throughput evaluation particularly considerable finegrained compute data reuse opportunity NN candidate observation exploit opportunity leverage remove redundant computation enhance chip data reuse onchip schedule conventional cpu gpu nasa achieves significant performance efficiency boost