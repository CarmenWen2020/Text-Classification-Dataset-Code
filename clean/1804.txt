guarantee data persistence storage workload database typically synchronous protocol network server stack latency critical request processing byte addressable persistent memory PM mitigate storage overhead server stack networking dominant factor latency request processing emerge programmable network device reduce network latency application compute network cache request however update request client stall server commit update persistently introduce network data persistence extends data persistence domain server network pmnet programmable data switch NIC PM persist data network pmnet incoming update request acknowledges client directly without server commit request failure request redo server recover implement pmnet fpga evaluate performance PM workload PM application evaluation pmnet improve throughput update request average percentile latency index network processing programmable network switch NIC data persistent memory latency rpc introduction benefit economy emergence compute increase enterprise workload hyper data estimate annual growth computation hyper data perform computation data host workload  interactive online data intensive  workload  financial analysis batch mapreduce machine training memory footprint workload data typically manage maintain persistent across multiple server client access update data remotely network interconnect switch remote procedure RPCs invocation rpc request client IO stack network intermediate switch server IO stack request handler server latency rpc significantly affected processing stage computation perform workload dominate RPCs update request access remote data consideration deploy workload data RPCs synchronous asynchronous asynchronous RPCs enable client execution update remote server building application challenge typical application span multiple closely interact contrast application synchronous RPCs easy tune debug google strongly prefer synchronous program model therefore aim improve performance specifically latency synchronous RPCs minimize access remote persistent data recently programmable network device become available trend offload application logic device procedure server network stack processing longer handle server accelerate network device newer computation scheme network compute span application query processing data aggregation computational intensive machine task promising network compute mainly mitigates latency computational task request server query data persistence maintain server update request traverse entire network server IO stack update therefore client entire rtt acknowledgement server proceed minimize request processing server data deploy persistent memory PM technology intel optane nvdimm traditional storage device ssd hdd PM byte addressable access persistent data bypassing OS indirection file PM reduces server storage latency thereby enable software database perform faster integration PM significantly reduces request processing individual server network dominant factor processing request data client stall rtt moreover network resource contention bandwidth switch queue link UI OOVBM  PNQVUFS SDIJUFDUVSF acm annual international symposium computer architecture isca doi isca variable delay latency identify fundamental limitation network compute operation stateless unable accelerate stateful persistent operation server persistent memory server improves performance persistent update network server network stack latency critical expose persistent network persist update request network therefore introduce notion network data persistence enables sub rtt latency processing update request expose data persistence domain network update network persistent memory acknowledgement client request enters persistent domain update request server server processing happens critical request persistent server client proceed server acknowledges implement pmnet mechanism persistent programmable network device however pmnet challenge network device request persistently maintain request persist network recover failure pmnet maintain application guarantee network persistence insight persistent pmnet protocol ensure update persistently network device sub rtt latency pmnet mirror incoming update request traverse network device update request PM request already persistent domain network device pmnet immediately sends acknowledgement client therefore scenario latency significantly reduce client longer rtt pmnet invalidates entry upon reception acknowledgement server indicates server request recovery failure happens persistence domain network device server pmnet ensure entry reflect server pmnet  request server redo server recover consistent request delivery pmnet maintains update reflect later server client stale server however oftentimes workload optimize independent pmnet publicly available http pmnet  org client twitter workload redis backend client update tweet follower without maintain pmnet guarantee strict requirement within multiple client workload enforce application synchronization primitive ensure client update critical tpcc workload modification stock price critical lock primitive pmnet treat lock operation critical regular request server therefore enforce server subsequent lock request client fail server subsequent update request client sub rtt latency persist update request network device implement pmnet fpga programmable switch pmnet switch NIC pmnet NIC evaluation significant benefit baseline however latency difference pmnet switch pmnet NIC negligible roundtrip dominate server network stack server processing pmnet integrate additional functionality pmnet switch cache demonstrate mechanism update request coherently prior propose cache request switch pmnet switch replication develop switch replication mechanism upon pmnet protocol summary contribution expose data persistence network improve performance update request implement pmnet programmable data device integrate persistent memory incomplete update request adapt PM workload intel PMDK PM optimize redis database twitter tpcc pmnet evaluation pmnet improves throughput update request percentile latency workload demonstrate pmnet improves cache replication latency respectively traditional baseline II background motivation discus performance bottleneck perform update request persistent application propose synchronous program model data host workload online data intensive  workload  financial analytics data typically manage persistent multiple server access via query categorize request mainly synchronous asynchronous synchronous request guarantee completion server client rtt request server responds comparison asynchronous request client proceed immediately request deliver server however client risk lose data failure network packet loss failure happens application ensure client server remain sync complicate development application therefore programmer usually prefer synchronous program model mitigation synchronous overhead although synchronous model alleviates program burden entire query rtt critical application future request flight request server demonstrates involve query processing client network stack network latency server network stack server request processing dash arrow demonstrate rtt processing request breakdown server latency server network stack kernel request processing user majority overhead average mitigate synchronous overhead promising literature offload task network computational device programmable switch  network portion server processing network stack latency eliminate network compute load balance congestion packet schedule query processing machine acceleration however network compute handle stateless request persistent update request communication processing overhead remains fortunately recent advancement memory technology alternative performance storage persistent memory PM manage persistent data PM server perform update request efficiently reduce server processing latency database  file optimize data management advantage PM performance despite improvement network stack processing remain critical optimize network stack  enables application bypass server network stack server processing overhead evaluate optimize network stack VI      II  HW latency breakdown update request server network stack processing critical client request sub rtt promising persist update request ahead server stack processing dedicate module bypass majority overhead unfortunately dedicate software module introduce additional cpu memory utilization evaluate VI dedicate hardware module redesign cpu architecture deployment challenge trend data dedicate logic NICs switch effective handle network traffic without extra load server trend goal prototype efficient mechanism programmable network device network data persistence data become persistent network extend data persistence domain within server network maintain persistent ongoing request network update request become equivalently persistent server failure server already persist request  network device apply server recovery solid arrow demonstrate client longer entire rtt server request persistent instead proceed request persistence domain network consequently entire server latency request handler processing network stack latency critical therefore persist request network significantly improve performance server critical  pmnet PM integrate programmable network device realizes network data persistence persistent flight request network device update persistent server server fails due outage kernel crash persistent request redo flight request server recovers failure implementation pmnet challenge pmnet network server processing HUYHU HW   ULWH      OLHQW  DWD request recovery pmnet critical request recover request exist client server application maintain pmnet integrate pmnet address challenge persistent integration PM pmnet maintain flight update request PM redo server request persistent server pmnet sends acknowledgement request client persist network device PM server acknowledgement workflow request procedure upon update request pmnet writes integrate PM request PM pmnet destination server request becomes persistent pmnet sends acknowledgement pmnet ack client request persistent server actually request acknowledges server ack pmnet invalidate reclaim entry future client upon acknowledgement pmnet client proceeds without completion request server detail IV pmnet switch NIC refer pmnet switch pmnet NIC respectively recovery challenge recover server failure recovery procedure upon detection server failure heartbeat signal pmnet request device PM  server pmnet ensures server commits resend request extra sequence header detail IV server commit request notifies pmnet invalidate entry besides sequence update request additional information ensure network recovery pmnet encodes information pmnet header detail IV exist network protocol IP  software client server encode minimum source code delivery pmnet guarantee traditional baseline already pmnet maintains within client additional sequence GD  PD      NN   GD  PD      NN   LPH OLHQW OLHQW HW HUYHU           hvs  twitter workload GD   PK   GD   PK   LPH OLHQW OLHQW HW HUYHU    PK  HW   HW  HW         HW    application tpcc workload information header packet client however pmnet ensures multiple client another client  server update pmnet commit server pmnet target majority client server connection independent unsurprising rtt client server dependency synchronization client inflate latency request therefore workload typically mitigate dependency optimize independent client task microservice storage backends without dependency others worker node distribute machine parameter server without synchronize worker client independent database memcached redis commonly service backends demonstrates update request variable  twitter workload constraint client client independently executes  function uid consecutive request however rare enforce multiple client client cannot update server another completes client request actually server progress workload typically synchronization primitive ensure application client update request access synchronization primitive server acquire lock critical client critical enforce client synchronization scheme tpcc workload pmnet directly lock request server client access critical subsequent update request client benefit pmnet lock request server lock acquire release enforce lock request infrequent majority request pmnet workload lock request tpcc workload access lock primitive bypass pmnet         HW  pmnet architecture IV pmnet introduce pmnet extend persistence domain server network flight update query network device PMs redo failure pmnet effectively server stack overhead critical request processing pmnet realizes network data persistence augment programmable network device switch NICs PM architecture overview pmnet fully function client server application hardware implementation introduce aspect introduce pmnet protocol update request benefit network data persistence encodes metadata recovery pmnet header IV request processing procedure pmnet accord pmnet protocol IV pmnet introduce pmnet integrates pmnet replication IV another implement cache pmnet IV finally detail described illustrate recoverability pmnet IV pmnet protocol pmnet protocol packet header delivery guarantee query header format pmnet header application layer network packet pmnet header consists differentiates pmnet request detail IV  session client sends request differentiates connection client  packet session server query furthermore  informs pmnet avoid redo already query recovery IV  crc hash entire header compute sender network stack pmnet hash index packet PM pmnet delivery pmnet protocol built upon udp network compute differentiate network traffic pmnet reserve specific udp encodes pmnet OLHQW HUYHU     HW   OLHQW HUYHU     HW    OLHQW HUYHU    HW  HUYHU  per client packet guarantee reorder packet packet loss failure header udp packet application originally tcp pmnet software library convert tcp packet udp packet maintain reliable delivery guarantee tcp pmnet protocol ensures packet integrity guarantee IV MTU packet pmnet obtains packet udp header udp packet typically maximum transmission MTU query limit pmnet software library transparently query MTU packet sequence  maintain packet pmnet handle MTU packet per packet pmnet ack client packet pmnet client pmnet ack correspond update request completely pmnet pmnet software library pmnet ack guarantee udp guarantee packet pmnet protocol implement server execute query client discus guarantee scenario normal execution demonstrates scenario client sends packet network transmission packet reorder detection packet server pmnet library corrects  packet detection packet loss demonstrates scenario client sends series packet packet lose packet network transmission server pmnet library detects   request retransmission IV describes detail retransmission failure recovery demonstrates scenario server fails pmnet  packet server recovery scenario server reorder packet reception conclusion although network reorder lose packet pmnet protocol guarantee delivery handle reorder network rare datacenter network typically consistent load balance ECMP  SH    HW WR OLHQW  HW  HV      HQ SH    3D     SH    HW   RJ   RJ    LW  HW  data mat pipeline pmnet multi client pmnet enforces multi client application synchronization request directly server described allows server enforce across critical application code code within critical benefit pmnet pmnet request processing management packet handle pmnet action mat pipeline handle pmnet packet distinguish via along non pmnet packet update request client update req pmnet maintain persistent query recover therefore upon reception packet belongs update request pmnet immediately packet destination server meantime packet persistent memory  pmnet header serf index entry packet persist network device PM pmnet sends pmnet ack client packet persistent device PM  packet collides exist entry pmnet directly packet destination server without acknowledge client bypass request client bypass req purpose request synchronization client acknowledgement pmnet client packet bypass req pmnet directly destination without packet persistent server ack another pmnet pmnet ack multiple  pmnet pmnet ack another pmnet pmnet directly packet along destination ack server server ack upon reception server ack pmnet request  packet request pmnet server ack destination pmnet route request retransmission request server  server detects packet loss sends  request client pmnet pmnet request packet pmnet directly sends server  request otherwise pmnet  request directly target client non pmnet packet pmnet serf regular network device handle non pmnet packet directly destination mat pipeline workflow pmnet implement mat pipeline packet processing pipeline contains stage ingres PM access egress ingres pipeline packet pmnet packet udp header non pmnet packet directly destination remain pmnet packet packet header pmnet ack packet destination PM access stage operates request creates upon update req packet remove upon server ack packet upon  packet  index egress stage outgo packet outcome lookup update req packet destination server PM additionally generates sends pmnet ack client  packet entry egress pipeline generates  request packet server otherwise  destination client PM access stage pmnet manages entry PM dma remove entry mat pipeline PM access stage suffer longer PM access latency prevent incoming packet pmnet maintains queue queue writes buffer PM access request pmnet handle incoming packet rate VI pmnet replication replication fault tolerance mechanism maintains multiple data across various storage server distribute server fails recover corrupt data enable fault tolerance update request commit replication server pmnet accelerate fault tolerance replicate data  PMs demonstrates scheme server replicates data accordingly switch series maintain update request client sends update request pmnet switch request sends pmnet ack pmnet switch receives request performs sends pmnet ack client processing pmnet ack server primary server receives update request replication scheme pmnet switch timeline network replication HUYHU HW    HDG LW   OLHQW  DWD RJ   DFKH HDG LW      cache pmnet switch afterward sends update data replication server replication completes primary server sends server ack invalidate pmnet switch network replication switch persist prior acknowledgement latency persist data switch overlap latency benefit overlap replication procedure switch evaluation VI pmnet cache prior programmable switch cache maintain persistent data cache cannot mitigate rtt update request update request cache pmnet persistent cache enable pmnet implement server tor switch prior network cache simplify consistency issue maintains persistent cache update request response request procedure request arrives pmnet cache request cache pmnet sends cache response request server normal response cached pmnet server update request pmnet server processing critical update request cache cached already update cache maintain consistency describes diagram entry pmnet invalid entry empty initial stale entry date flight update persist request entry persist server pending request pmnet persist server pending persist entry cache transition upon update req client pmnet request server diagram pmnet integrate cache intermittent failure scenario persist pending server ack pmnet notify server persist request status becomes persist entry indexed update via update req pmnet directly bypass request pending entry pending receives another update req becomes stale server update stale entry remains stale another update req stale entry receives server ack becomes invalid prior update req persist pmnet failure recovery focus client server architecture data intermittent failure outage software bug hardware failure temporarily become unavailable permanent hardware failure handle via replication detail IV intermittent failure hardware  functionality failure however data outside persistent domain lose ensure data integrity pmnet ensures accepted request safely persistent domain categorize failure pmnet fails request client packet accepted pmnet server client acknowledge therefore client stall flight request simply resend request timeout recovery pmnet fails accept request server receives request discus pmnet ack client failure scenario client assume packet server recovery server poll pmnet request sequence packet receives server applies request client pmnet agnostic packet performance server  maintain request OLHQW HW HUYHU  HW HUYHU         failure recovery pmnet replication pmnet ack client failure incomplete scenario client acknowledgement therefore stall request client resend request pmnet fails server ack pmnet  request server upon reception  request packet server  packet server request   request therefore server request sends server ack invalidate entry pmnet permanent failure failure hardware persistent data pmnet server cannot recover failure handle replicate data persistent domain across multiple device categorize permanent failure pmnet fails permanently accept request server receives request pmnet handle permanent failure update request persist pmnet server accepted IV discus pmnet fails pmnet device accepted request client pmnet ack satisfy replication requirement stall request  request pmnet fails pmnet device accepted request pmnet ack recovery server poll pmnet request pmnet device request survive pmnet retransmit request server server replication fails server ack replication primary server sends server ack commit update replica replication server fails server server ack invalidate request pmnet eventually pmnet entry pmnet newer request directly server however request fail replica client eventually timeout flight request pmnet equivalent replication strength pmnet device replica replication primary server sends  timeout percentile rtt fpga platform pmnet implementation request pmnet retransmit request server sequence typically monitor server status heartbeat therefore client notify replica fails client failure component component outside persistence domain fail client  network device procedure recovery recovery procedure persistence guarantee device remains pmnet implementation hardware implementation express pmnet processing pipeline xilinx ultrascale VCU platform hardware programmable network device maintain request 2GB dram label component dram latency fpga due dma fpga optane PM latency adapt source code NetFPGA  0G ethernet mac ultrascale platform integrate pmnet network interface label component lut bram IO resource fpga fpga chip label component ion battery module device failure choice network PM flight update request evaluation percentile rtt update request conservatively maximum rtt gbps bandwidth bandwidth delay   rtt BW mbit fpga sufficient memory capacity update request PM access latency buffer access network PM pmnet device switch NIC packet rate buffer  calculation delay memory access latency   BW kbit pmnet software interface client software interface pmnet update update req server pmnet bypass bypass req server pmnet session session pmnet session session server software interface pmnet recv request client pmnet ack ack pmnet II configuration server configuration cpu intel cascade 1GHz core dram 2GB ddr MT PM 8GB intel  interleave config app mode ext DAX NIC mellanox ConnectX  client configuration cpu intel haswell 6GHz core dram 6GB ddr MT NIC mellanox ConnectX  software OS ubuntu linux kernel libs gcc PMDK   server conservatively KB SRAM queue writes PM vii discus network bandwidth pmnet software implementation develop easy software interface allows programmer adapt exist workload pmnet pmnet interface interface function programmer overwrite exist function socket interface pmnet version pmnet library operates function encapsulates payload pmnet compatible format addition pmnet library serf purpose accepts incoming packet pmnet mitigate rtt client IV maintains integrity pmnet packet server IV VI evaluation methodology setup evaluate pmnet testbed described II server equip intel DC persistent memory DAX FS mode workload directly manage PM client normal DRAMs machine request server persistent data access server client mellanox NICs network connection client machine client instance xilinx ultrascale FPGAs programmed pmnet NICs switch pmnet switch configuration client machine rack pmnet switch due limited ethernet fpga regular switch  latency client fpga update latency update byte client server pmnet switch pmnet NIC update latency ideal request handler variable request merge traffic pmnet NIC configuration client machine regular switch directly server fpga bump server NIC tor switch recent microsoft  setup evaluate workload evaluate workload intel PMDK library PM optimize version redis intel ycsb client generate update request server evaluate performance pmnet workload twitter workload twitter clone tutorial online transaction processing benchmark tpcc server workload manage persistent data PM directly DAX FS workload modify code redis PMDK workload respectively payload update request byte default unless specify otherwise evaluation skip request precise baseline protocol implement driver program PMDK workload RB hashmap skip udp therefore baseline pmnet workload udp redis twitter tpcc originally tcp although udp faster adapt udp introduces slowdown due conversion overhead tcp communication baseline baseline evaluate perform protocol pmnet configuration comparison pmnet switch pmnet tor switch server rack pmnet NIC pmnet server NIC client server baseline network packet destination evaluation latency microbenchmarks evaluate raw latency bandwidth pmnet practical server processing bottleneck therefore implement microbenchmark ideal request handler server acknowledges client upon reception request without processing hence network latency becomes primary bottleneck evaluate latency benefit pmnet latency pmnet switch  payload       HW bandwidth latency stress             OLHQW HUYHU OLHQW HUYHU RJ      alternative client replication server replication client observation pmnet switch pmnet NIC speedup respectively client server payload however benefit decrease payload payload processing network device pmnet switch pmnet NIC around speedup client server payload difference absolute latency pmnet switch pmnet NIC almost negligible latency benefit server processing network latency critical stress bandwidth ideal server request handler client client instance request server saturate bandwidth display stress pmnet configuration client server trend latency remains bandwidth spike latency bandwidth physical limit gbps bandwidth gbps pmnet switch pmnet NIC consistently latency  server critical pmnet configuration equally effective update request response maximum bandwidth discus pmnet performance switch comparison alternative pmnet alternative request payload maximize performance difference due communication aforementioned microbenchmark VI client locally request client proceeds client request server network stack processing critical implement client dedicate software client persistent cache replication replication update latency server client pmnet pmnet alternative server client   UHH UHH UHH  nls LVW   throughput normalize client server variable update ratio application directly overwrites socket interface client logger server request server upon reception immediately notifies client server processing critical implement server persistent cache latency without replication client faster pmnet client network stack pmnet whereas server pmnet server network latency remains critical replication enable client becomes inefficient communicate client replicate similarly server latency increase significantly due communication comparison pmnet consistently performs replication without replication communication latency replica critical application performance prior update ratio varies normalize throughput application pmnet update ratio observation pmnet average speedup client server update request ratio request increase throughput improvement pmnet decrease trend pmnet focus update request performance benefit pmnet cache pmnet cache cache implementation lookup interface workload workload PMDK redis excludes workload complex query cdf request latency update request twitter tpcc cumulative distribution function cdf latency update request respectively observation average latency pmnet cache client server request update latency pmnet without cache noticeable transition percentile latency become client server afterward happens request update optimize pmnet comparison request update latency latency client server cache integrate pmnet latency benefit percentile serf update request request cache sub rtt latency workload rate redis significantly benefit percentile latency cache conclude integration cache pmnet effectively reduces latency update request pmnet replication replication scheme connects pmnet switch series implement replication network benefit network replication client server performs replication server observation pmnet replication rup   HW   update request latency replication normalize replication client server   HW  HW update throughput optimize network stack performance server replication average overhead due switch replication latency persist overlap mechanism replication introduces overhead pmnet update recover server failure evaluate pmnet recovers server failure mimic failure manually server pmnet resend request restore workload saturate network bandwidth scenario pmnet maximum request average resend request resend pending request entire recovery procedure resend application recovery server boot pmnet optimize network stack latency breakdown II  server latency consists network stack processing optimize network stack network stack overhead significantly reduce  reduce network stack network procedure user avoid expensive context switch kernel evaluate ideal scenario server overhead minimal microbenchmark introduce VI client server optimize network stack update throughput client server pmnet client server  pmnet  without  pmnet throughput apply  server network stack overhead significantly reduce nonetheless integration pmnet throughput although speedup server overhead reduce  benefit pmnet significant pmnet remain server processing critical vii discussion discus network bandwidth PM performance alternative pmnet network bandwidth fundamentally pmnet bandwidth relatively PM access decouple network traffic queue PM access queue increase queue accord bandwidth delay  equation bandwidth network pmnet buffer incomplete access PM gbps network kbit queue buffer suffice pmnet buffer ongoing update request commit server mbit MB PM buffer flight update request gbps network equation PM bandwidth implementation battery dram fpga bandwidth GB per DIMM bandwidth intel optane PM future PM technology enable bandwidth  battery NVDIMMs emerge persistent cache alternative PM medium  ReRAM PM bandwidth pmnet handle update request bandwidth external persistent storage integrate PM network device alternatively storage maintain persistent data switch access network attach PM device ssd instead persistent data however additional network latency persist data network device eventually inflate critical client execution  calculation PM capacity requirement relatively therefore unnecessary external device persist request related discus related PM network optimization network compute persistent memory integration PM improves performance manage persistent data conventional storage device ssd hdd various software advantage PM storage performance PM optimize file exist program manage data efficiently PM distribute PM file client access PM remotely benefiting PM performance PM optimize database application accepts request client application manage data PM without software indirection maintain PM data structure however faster storage backend client network entire rtt update persistent data server comparison integration pmnet improve performance network stack processing critical performance PM aspect correctness another ensure consistent recovery PM error prone recent ensure PM application recover consistent failure adapt network data persistence validate application persist client server verification programmable data cooperation PM guarantee endto correctness network data persistence direction future network compute network compute reduces latency variety task computation server network prior propose programmable switch network function load balance packet schedule addition programmable switch offload application logic network data aggregation machine network compute accelerate storage workload cache however persistence domain workload limited server recent  utilizes storage capability programmable switch however coordination information persist data network maintain data persistence network introduces pmnet PM network device NICs switch future integrate acceleration logic pmnet accelerate wider application IX conclusion propose network data persistence expose persistence domain network persist update request sub rtt latency PM integrate programmable network device pmnet flight update request server network stack processing critical implement pmnet fpga programmable switch NIC evaluate variety workload exist pmnet improve throughput update request average percentile latency