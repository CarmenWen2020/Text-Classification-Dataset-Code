revolution machine unprecedented demand computation resource urge transistor monolithic chip sustainable moore era multichip integration functional chiplets reduce manufacturing improve fabrication yield achieve reuse dnn workload mapping hardware exploration multichip critical stage hierarchical analytical framework dnn mapping multichip accelerator analyze communication overhead framework propose automatic NN baton pre pre aim chiplet granularity exploration performance budget target workload focus workload orchestration computation package chiplet core hierarchy simba NN baton generates mapping strategy computation memory configuration architecture exploration demonstrates decisive factor chiplet granularity mac chiplet constraint chiplet implementation core lane vector mac computation allocation across benchmark contrast optimal memory allocation policy hierarchy typically depends neural network model index accelerator chiplet mcm neural network schedule exploration introduction application across platform boom flourish neural network dnns image recognition detection image caption processing dnn model feature intensive computation storage significant dnn accelerator recently propose academia FPGAs ASIC accelerator socs mobile device datacenter infrastructure dnn hardware tends increasingly compute throughput chip memory capability however moore transistor reduction slows fabrication yield challenge grows consequently monolithic chip  fabrication prolong development timeline decade overcome multi core processor achieve  efficiency improvement moreover conquer memory novel non von neumann architecture promising actively nowadays namely establish fail obtain integration via chip efficiently due decline fabrication yield increase per transistor advanced challenge motivates employ multichip module mcm package functional chiplets CPUs gpus FPGAs dnn accelerator package integration enjoys manufacturing fabrication yield flexibility monolithic chip nevertheless novel platform dnn accelerator brand challenge computer architecture community simba pioneer multichip dnn accelerator chiplets package contribution  chip package bandwidth latency beyond clarify challenge challenge dnn workload multichip accelerator hardware deployment recent concern dnn mapping mostly focus chip package dram multiple accelerator chiplets accord simba prototype multichip accelerator involves complex communication newly communication consumes bandwidth chip interconnection therefore neural network workload chiplets aggregate locally fragment data likely redundant transfer communication congestion multichip hierarchical parallelism processing workload mapping layer challenge explore chiplet granularity development accelerator UI OOVBM  PNQVUFS SDIJUFDUVSF acm annual international symposium computer architecture isca doi isca performance appropriate chiplet resource allocation policy mac  accumulate chip memory hierarchy chiplets simplifies nop network package topology inter chip communication overhead contrary abundant chiplets simba complex networking development simba chiplet automatic exploration appropriate multichip accelerator urgently address challenge outline aim analyze dnn mapping multichip accelerator explore optimal chiplet granularity therefore establish universal concise hardware model package chiplet core available subsequent analysis hierarchy correspond computation memory component target layer wise mapping multichip accelerator employ directional network package interconnect chiplets intricate network chiplets propose output centric hierarchical description spatial temporal primitive spatial primitive partition workload parallel module chiplets core contrast temporal primitive adopt depict loop unroll strategy aim maximize buffer locality data reuse efficiency description quantitative analytical methodology CP critical capacity critical evaluate hardware generate optimal mapping strategy layer finally propose automatic NN baton pre chiplet granularity conduct multichip accelerator implementation computation memory resource allocation parallel package chiplet core contains spatial temporal partition strategy layer accelerator summary contribution universal multichip accelerator consists parallel hierarchy package chiplet core enable efficient workload mapping exploration hierarchical analytical framework framework contains chiplet friendly output centric description temporal spatial primitive CP methodology evaluate communication overhead automatic output centric description CP methodology develop NN baton promising automatic development multichip accelerator dnn workload deployment demonstrate effectiveness NN baton typical model NN baton simba chiplet additionally NN baton  optimal accelerator mac constraint target model knowledge propose framework automatic facilitate dnn workload mapping chiplet granularity exploration multichip accelerator II background aim core concept dnn workload chiplet technique reader understand motivation principle dnn workload dnns compose various component convolution activation function batch normalization pool fully layer convolution layer feature extraction particularly computation intensive overview convolutional computation dimensional loop nest focus batch minimize inference latency deployment scenario datacenter define output cube HO WO CO layer workload consume 3D input cube 4D tensor generally due chip resource limitation dnn accelerator execute numerous iteration workload convolution stride kernel calculate adjacent planar convolution generates input overlap halo potentially redundant memory access reuse data chip buffer dnn accelerator loop tile loop unroll critical optimization goal recent loop tile partition loop aim buffer exploit memory locality loop unroll target loop nest decides dataflow computation array input stationary stationary WS output stationary OS CO CI CI  WI HI CO KY WO input overlap HO halo  input cube output cube CO WO HO CI KY  overview convolution layer chiplet basis revolution data machine unprecedented demand computation resource transistor integrate chip however due slowdown dennard moore becomes challenge inefficient sophisticated soc requirement tesla  TOPS performance target scenario  fabricate technology TOPS performance datacenter consumes manufacturing fabrication yield chiplet technique becomes promising tackle functional chiplet assemble within package mcm technique chiplets via package link silicon interposer organic substrate  monolithic chiplet enjoys manufacturing fabrication yield addition chiplet silicon heterogeneous node improve efficiency however package communication induces considerable gap traditional fully onchip implementation become active research topic recent target bridging gap bandwidth chip interconnection dnn accelerator simba pioneer fabricate chiplet unlike previous 2D parallel array multichip dnn accelerator contains computation memory complicate workload mapping breakdown multichip accelerator tends multiple array chiplet multiple chiplets package overview breakdown multichip accelerator contrast memory introduce cache data array communication become significant portion rank dram access transfer consumes data phys therefore workload chiplets data locality consideration avoid unnecessary inter chip communication data transfer package behaves differently chip  transfer exploit 2D mesh nop IO complexity overhead onchip communication naturally inter chip communication another chiplet granularity multichip mac distribute chiplets brings communication overhead chiplet motivate develop systematic architect explore superior developer dnn workload efficiently baseline architecture multichip hierarchical model analyze workload mapping introduce universal hardware model later analyze inefficiency dataflow simba overhead typical operation multichip hardware  GRS operation relative feature dram access  attach standard bus transfer data chiplet ddr phy communication D2D phys transfer data chip chip access KB SRAM SRAM multicast unicast data access KB SRAM register  frequently access WS dataflow mac utilization hierarchical output centric dataflow typically friendly multichip accelerator hierarchical parallel model core architecture define accelerator core PE array local buffer activation AL output core architecture simba contains lane parallel vector mac stationary dataflow  input channel mapped along dimension PE array buffer generate SRAMs overlap data load computation buffer implement register modify operation cycle besides workload allocation hoc woc output tile assign core hoc woc local memory footprint requirement various buffer chiplet architecture model chiplet architecture NC core activation buffer global output buffer core buffer interconnect via central bus attach interface chip memory remote chiplet communication employ reference signal GRS  D2D interface model consumes technology central bus multicast data buffer core conducive activation workload partition responsible core workload assignment writes dram function AL reuse activation halo adjacent planar tile assign core organize buffer pool reallocate mode core core buffer merge broadcast data PE array otherwise core distribute private buffer PE array access local policy abstraction IO dram dram dram dram chiplet chiplet chiplet chiplet interconnect memory bus package package architecture core architecture vector MACs SRAM SRAM SRAM SRAM SRAM buffer SRAM arbitrate crossbar local remote core core reg reg reg reg reg acc acc acc acc acc load bus lane chiplet architecture core core core core buffer core core core core buffer data bus ddr dma D2D D2D computation storage interconnect package mapping along HO CO dimension NP chiplet parallelism dram crossbar dram bus chiplet chiplet mapping along HO WO CO dimension NC core parallelism data bus multicast unicast core mapping along CI CO dimension  mac parallelism array multiple lane broadcast overview architecture multichip accelerator refrain duplicate chip buffer package architecture integrate NP chiplets via directional network package simplify inter chip communication chiplets package attach NP global DRAMs via crossbar allows chiplets access chip memory directional network introduce rotate transfer data chiplets generally input feature filter various output channel therefore chiplets mapped along output channel dimension input activation distinct chiplet allocate NP input channel correspond computation subsequently chiplet writes buffer activation local buffer adjacent chiplet via interconnection operation NP accumulation input channel likewise chiplets diverse output feature tile transfer activation stationary chip rotate transfer strategy straightforward chiplets spatially data avoids duplication dataflow simba adopts WS centric dataflow performs convolution computation systolic fashion noc nop centric dataflow spatial mapping parallel around dimension CI CO simba split input channel CI along split output channel CO along however centric dataflow fails leverage planar dimension HO WO activation hidden overhead reload halo besides partial sum accumulate across core chip communication overhead derive partial sum wider width input additionally transfer input partial sum unified interface simba employ SerDes IP PE router allocate along output channel input channel dimension allocate along output planar filter dimension activation tile activation tile activation tile activation tile chiplet input chiplet input chiplet input chiplet input chiplet filter input chiplet filter input chiplet filter input chiplet filter input D2D phy chip bus D2D phy D2D phy chip bus D2D phy D2D phy chip bus D2D phy D2D phy chip bus D2D phy YH local data adjacent chiplet rotation fashion buffer YH PH YH YH PH YH YH PH YH chiplet chiplet chiplet chiplet D2D phy dram chip bus D2D phy D2D phy dram chip bus D2D phy D2D phy dram chip bus D2D phy D2D phy dram chip bus D2D phy YH local data adjacent chiplet rotation fashion buffer  PH YH  YH  PH YH  chiplet chiplet chiplet chiplet rotate transfer data activation via directional network package dataflow activation hardware mechanism rotate transfer manner consequence apply OS output centric dataflow package chiplet mapping workload along output dimension CO HO WO core export partial sum accumulate quantize data layer input transfer simplify interface communication overhead  dataflow introduce spatial primitive workload partition package chiplet parallel simba temporal primitive loop unroll strategy simba rotate primitive data chiplets spatial temporal generates CO HO WO output stationary OS variant multiple output channel activation package  WOt cot temporal spatial HO WO CO chiplet hoc woc temporal spatial  WOt cot core WS dataflow rotate lane mapping vector mapping FY FX hoc hoc woc woc output centric spatial temporal dataflow representation package HO  parallel CO parallel CI chiplet HO WO parallel CO parallel CI core WS dataflow lane mapping vector mapping FY FX hoc hoc woc woc centric simba baseline dataflow representation stationary multiple input output channel CI KY CO CO employ OS dataflow propose output centric spatial temporal dataflow representation simba baseline centric dataflow notation refer workload chiplets core excellent convenience workload mapping evaluate memory access overhead introduce IV rotate transfer occurs buffer rotate primitive inside core computation output tile IV hierarchical analytical framework multichip dnn  introduces framework  dataflow description CP evaluation methodology addition analyze prefer choice partition apply spatial primitive framework detailed characterization propose NN baton hierarchical output centric dataflow description loop transformation core dataflow analysis frequently however analyze intricate tile unroll strategy dimensional loop nest intend spatial temporal primitive loop tile loop unroll respectively spatial primitive refers spatial parallelism mapping computation parallel simba spatial partition strategy package chiplet package output cube spatially NP along channel dimension partition likely apply feature layer parallelize dimension feature layer usually workload description workload HO WO CO chiplet workload  WOt cot core workload hoc woc convention workload partition NP along CO chiplet chiplet chiplet chiplet WO cot chiplet workload partition NC along channel  WOt woc hoc core core core core workload partition NP along channel CO WO HO HO chiplet workload partition NC along cot  WOt chiplet workload partition NC along hybrid dimension cot  WOt visualization loop transformation spatial temporal primitive package chiplet cube output cube bold denote spatial partition chiplets core dot denote temporal partition workload chiplet core model chiplets efficiently via network rotate transfer similarly partition adaptive layer intensive activation chiplets additionally limited chip memory capacity chiplet deliver chiplet workload  WOt cot owe chip communication introduce supplemental partition allows partition along dimension simultaneously chiplet spatial primitive flexibility package spatial primitive loop tile combination partition combination layer organization buffer central bus mode data transfer reconfigured temporal primitive refers temporal unroll mapping sequence simba temporal primitive target loop unroll spatial partition macro workload optimal strategy usually depends layer characteristic thanks  dataflow reduce unroll dimensional loop nest dimension channel priority dimension inner loop unroll priority dimension inner loop unroll illustrates alternative choice temporal primitive hierarchy generate possibility channel priority unroll friendly reuse buffer chiplet core workload computation contrary activation reuse potentially gain benefit priority unroll combine spatial partition manner dataflow loop transformation hierarchical framework facilitates analysis IV package  WOt cot temporal spatial HO WO CO  HO WOt WO cot CO chiplet hoc woc temporal spatial  WOt cot hoc  woc WOt  cot loop temporal primitive analysis reuse critical analysis analysis analysis traverse reuse critical outer loop penalty factor buffer critical capacity critical satisfy capacity inadequate capacity  loop diagram CP loop analysis loop unroll priority  inside temporal primitive CP diagram calculate memory access specific temporal combination memory access analysis buffer analytical methodology evaluation dnn workload mapping strategy buffer memory hierarchy principal factor memory access concisely evaluate memory access overhead formulate critical capacity critical CP methodology understand relation buffer workload mapping critical denote loop data reuse buffer critical capacity refers buffer reuse data outer loop analysis temporal loop category relevant irrelevant loop relevant loop critical irrelevant loop critical boundary reuse leverage diagram analyze firstly loop critical correspond capacity reuse data outer loop obtain critical filter filter refers volume core workload encounter access penalty reload chip memory later outer loop perform procedure critical capacity calculate filter minimal capacity without penalty depends boundary loop nest buffer merge multiple core actual buffer layer mode buffer another loop loop reuse loop PE array computation hoc woc workload therefore additionally supplement framework besides analysis critical capacity specific input feature workload kernel stride contribute data reuse buffer buffer locality analysis additional operation sum entire chip workload calculate correspond input feature summary illustrate equation memory access memory tot comprises intrinsic access layer configuration penalty access denotes critical  critical jth   buffer capacity contribute access penalty overhead tot calculate tot define equation denotes reuse kth critical besides LC loop aforementioned CP methodology evaluate overhead memory chiplet architecture  buf cck buf cck feature partition analysis previous analysis combine dimension dataflow description ignore tile tile tile tile extra memory access due feature overlap tile tile partition partition tile tile partition partition conv layer resnet kernel stride output conv layer vgg kernel stride output height width ratio height width ratio redundant memory access partition convolution layer derive input feature overlap workload allocate chiplets core input resolution model partition previous chip accelerator adopt various partition stripe rectangle however spatial partition dimension significantly impact memory access multichip multichip computation input inappropriate unexpected overhead originate halo consequently partition selection consideration workload mapping procedure redundant memory access planar partition convolution layer convolution layer resnet feature kernel stride halo memory access increase enjoys redundant access rectangle stripe gap tends tile convolution convolution vgg extra access variation trend conclusion motivate employ reduce redundant memory access temporal partition temporal primitive generates numerous tile prefer choice scenario package spatial primitive generates NP tile theoretically choice acceptable however multichip bandwidth chiplets DRAMs integrate appropriate data layout indispensable avoid memory access conflict ensure processing efficiency massive central halo central data access chiplets contrast halo data access chiplets rectangle chip spatial partition logic flexible prefer generally depends specific chiplet workload CI WI HI CI WI rectangle chiplet chiplet chiplet chiplet chiplet chiplet chiplet chiplet halo HI visualization halo partition rectangle overlap potentially dram access conflict CP evaluation extract tech lib overhead mapping analysis mem PE phy hardware exploration partition loop transformation runtime mem phy access mac proposal chiplets core lane vec mac KB KB NN model description parse pytorch model mapping strategy layer package spatial temporal priority layer chiplet spatial temporal priority layer specific HW description input NN baton implement python output report pre resource option constraint overview NN baton diagram compose component hardware exploration mapping analysis analysis module propose NN baton automatic description propose NN baton automatic NN baton generate workload mapping strategy chiplet granularity  respectively pre architect chiplet granularity appropriate hardware resource scheme computation memory neural network workload hardware constraint constraint primarily mac budget mapping strategy configure target technology CP evaluation estimate hardware choice runtime estimation mac utilization hardware  parallelism lane improper layer utilization compute resource finally optimal proposal report output CP evaluation sub pre specific hardware configuration detailed mapping strategy deploy model hardware spatial temporal primitive spatial primitive partition dimension partition temporal primitive loop loop report information potentially optimization hardware compiler experimental setup hardware configuration model multichip dnn accelerator description arithmetic reserve width partial sum evaluate overhead standard memory model synthesize computation core UMC technology synopsys compiler technology GRS macro mac respectively mhz frequency appropriate multiplexer width SRAM configuration optimal memory model library demonstrate preliminary evaluation approximately satisfy linear relationship SRAM allows extend exploration memory linear regression average GRS evaluation dram estimate chiplet SRAM RF mac chip phy ignores controller IP module workload dnn model extract representative layer  layer activation intensive layer activation kernel layer pointwise layer layer alexnet vgg resnet darknet input resolution classification task detection task alexnet contains convolution layer diverse kernel model mainly layer resnet darknet model channel feature resnet reduces earlier vgg darknet consequently SRAM KB register file linear relationship memory SRAM register file overhead RF refers modify operation II computation resource memory footprint experimental setup exploration computation resource memory footprint vector mac lane KB core NC KB chiplets NP KB resnet peak memory requirement layer activation NN baton implementation implement NN baton python resource option II establish exploration evaluation buffer volume chiplet workload model description parse pytorch model torch jit function mapping analysis adopts exhaustive evaluate partition height width ratio loop transformation various spatial temporal combination introduce IV extract per per operation CP evaluation model correspond accelerator runtime depends mac computation resource utilization establish simulator obtain runtime specific workload employ pre discus workload mapping exploration multichip accelerator VI STUDIES workload mapping multichip accelerator analysis mapping diverse layer configure hardware model chiplets core lane vector mac KB KB KB hardware employ  explore workload mapping distinct layer spatial partition strategy vgg conv vgg conv resnet conv resnet resa brancha resa branchb activation intensive layer intensive layer kernel layer pointwise layer layer respectively hybrid partition  overall overhead activation intensive kernel layer feature halo tend partition redundant memory access derive halo however intensive wise layer channel dimension become principal bottleneck computation prompt partition preferable resnet resa branchb layer burden activation partition advantageous others comparison demonstrates trend validates stability NN baton input breakdown impact spatial partition manner overhead distinct primary activation intensive layer partition package activation transfer chiplets situation inverse intensive activation intensive intensive kernel wise layer dram output mac input input estimation breakdown spatial partition strategy input resolution layer temporal strategy axis item denote spatial partition package chiplet denote channel hybrid described IV respectively remove option due mismatch output channel SimbaOurs SimbaOurs SimbaOurs SimbaOurs SimbaOurs normalize   kernel pointwise layer input resolution dram mac SimbaOurs SimbaOurs SimbaOurs SimbaOurs SimbaOurs normalize   kernel pointwise layer input resolution normalize breakdown simba baseline dataflow dataflow generate NN baton distinct layer layer consumption evaluation due stationary dataflow core improper loop transformation primarily extra access dram diverse preference spatial primitive motivates apply optimal layer properly therefore accord layer parameter characteristic NN baton distinct mapping strategy layer wise minimize overall comparison simba model chiplet simba prototype configuration memory computation resource dataflow introduce previous comparability multichip accelerator model NN baton configure memory computation resource simba target workload mapping optimization NN baton omit controller RISC overhead simba primarily memory operation couple communication normalize distinct layer input resolution NN baton overall significant advantage NN baton activation intensive kernel layer resolution advantage output centric dataflow spatial partition  beneficial activation buffer dram access layer numerous activation halo consequently output centric dataflow prevent partial sum transfer noc nop avoid excessively fragment dimension contrary layer feature intensive wise layer perform similarly besides simba overhead slightly due massive transfer partial sum package model comparison simba classical model input resolution accord previous analysis simba baseline dataflow weak layer feature halo inferior feature reduces later vgg darknet resnet NN baton vgg darknet benchmark demonstrate outperformance output centric dataflow centric SimbaOurs SimbaOurs SimbaOurs SimbaOurs SimbaOurs SimbaOurs normalize resnet simba vgg darknet resnet vgg darknet comparison simba NN baton classical dnn model input estimation calculates conv FC layer reorganize FC layer wise layer evaluation manner output centric dataflow manages aggregate data locally reduce overhead exploration multichip accelerator analysis chiplet granularity promising exploration multichip accelerator granularity chiplet performance optimal chiplets significantly impact manufacturing mac possibility combination chiplets per package core per chiplet lane per core mac per lane assemble memory hierarchy buffer proportional computation resource optimal hardware implementation chiplets computation dimension deployed typical model without constraint consumption generally chiplets straightforward chip communication generally inter chip communication regard exception vgg resnet chiplet scheme option limit exploration optimum however chiplet constraint implementation constraint chiplet chiplet implementation overall delay EDP  implementation chiplet scheme optimum computation resource configuration constraint chiplet relatively runtime assemble mac per chiplets scatter creates communication overhead multichip accelerator chiplet significant driven distribute employ chiplet sacrifice performance obtains enables reuse constraint input input alexnet runtime constraint runtime constraint runtime constraint runtime constraint vgg runtime constraint runtime constraint resnet invalid runtime constraint runtime constraint darknet constraint runtime hardware implementation mac runtime estimate optimal workload mapping strategy grey implementation without constraint specific model prime implementation chiplet constraint plot refers chiplets implementation tuple axis chiplet core lane vector dot enjoys EDP chiplet NN baton exploration employ NN baton pre implement multichip accelerator exploration model parameter II runtime layer optimal mapping strategy specific hardware implementation performance budget  recommend computation memory allocation besides skip invalid sweep mac quantity exploration valid sweep model chiplet implementation respectively grey trend zone redundant vgg input resnet input darknet input per chiplet  per chiplet  per chiplet  normalize EDP global optimum optimum constraint chiplets per package constraint constraint constraint exploration multichip accelerator mac RISC interface macro model simba apply chiplet constraint plot zoom optimum constraint grey trend refer memory potential implementation appropriate memory allocation layer plot without constraint chiplet upper direction succession observation conclusion chiplets sacrifice EDP surprisingly benchmark optimal implementation constraint configuration chiplet core lane vector optimal resource allocation compute highly depends constraint however memory allocation recommend scheme distinct benchmark input resolution prefer activation buffer KB benchmark KB darknet input KB benchmark KB peak storage darknet MB vgg resnet MB feature  layer chip memory buffer activation without access penalty buf CC conclusion computation resource allocation depends constraint memory allocation sensitive target model vii related WORKS dnn workload mapping dnn mapping procedure deploy specific model accelerator efficiently loop transformation achieve efficient cache locality dimensional loop nest  prior loop till convolution layer later series propose various dataflow PE array computation output stationary stationary input stationary stationary loop transformation analysis reliable framework fpga platform loop transformation strategy express loop nest manner reveal schedule policy besides variant domain specific architecture propose recently  leveraged vault centric partition NN accelerator 3D stack dram tangram employ cascade loop nest layer layer pipeline maestro  framework concisely depict data movement reuse behavior spatial PE array motivate specific architecture correspond dataflow description currently lack description chiplet hardware exploration chiplet hardware exploration DSE hardware architect chiplet granularity distribute resource application  propose framework analysis DSE generate fpga HLS  chip interstellar explore dataflow memory hierarchy dnn accelerator  primarily explore NVDLA computation core propose novel output stationary local stationary dataflow simba pioneer implement dnn accelerator chiplet technology architecture dataflow multichip accelerator simba depth non uniform workload partition noc nop however analytical framework workload mapping guidance chiplet granularity dnn accelerator chiplet dimension granularity chiplet principal designer exist DSE mainly focus traditional platform fpga monolithic chip DSE multichip accelerator promising future architecture community conclusion gap dnn workload mapping DSE multichip accelerator universal concise model hierarchical analytical framework output centric dataflow description analytical CP methodology evaluation propose automatic NN baton conduct orchestration dnn workload chiplet granularity decision evaluate NN baton relative simba reduction classical model input resolution besides impact spatial partition demonstrate necessity employ preferable spatial primitive layer finally explore chiplet granularity decision mac demonstrate constraint decisive factor optimal computation allocation memory resource allocation typically depends NN model