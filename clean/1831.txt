introduce tensor processor tsp architecture functionally slice microarchitecture memory interleave vector matrix functional advantage dataflow locality operation tsp built observation machine workload exhibit abundant data parallelism readily mapped tensor hardware deterministic processor producer consumer program model enables precise hardware component achieve performance efficiency tsp exploit parallelism inherent machine workload instruction memory concurrency data model parallelism guarantee determinism eliminate reactive hardware arbiter cache resnet image classification demonstrate image per IPS batch improvement gpus accelerator ASIC implementation tsp architecture yield computational density  per silicon chip operating nominal frequency mhz tsp demonstrates novel hardware software approach achieve predictable performance machine workload within desire envelope introduction increasingly  algorithm important transportation security beyond workload complexity serious scalability performance usability challenge traditional cpu gpu architecture pace demand provision abundant chip ALUs utilize peak performance throughout program execution unfortunately hardware complexity microarchitectures runtime stall furthermore microarchitectural enhancement cache predictor prefetchers tremendously improve performance bound performance decade data operator instal core  fixture warehouse conventional 2D mesh core reorganize functionally slice arrangement tile computer  increasingly heterogeneous dozen processing core widely function gpus TPUs FPGAs smart IO controller efficient remote memory access effort largely focus accelerate neural network training inference performance workload convolutional neural network cnns recommendation algorithm computer vision CV image classification recurrent neural network rnns processing attention transformer model increase computational requirement model catalyst resurgence architectural innovation demand domain specific architecture widespread adoption compute ecosystem unique opportunity novel microarchitectures target application innovative emerge   integration traditional chip multiprocessor   broadly effort focus deliver compute capability per tile data focus acm annual international symposium computer architecture isca doi isca increase computational density fix envelope approach rethink conventional chip multiprocessor organization architecture around tensor abstraction tensor processor tsp tsp tile microarchitecture allows easily vector underlie tensor tensor computation perform processing model computational spatially function advantage dataflow locality tensor novel approach enables achieve significantly performance initial resnet image classification sample image per IPS batch improvement gpus accelerator remainder discus core architectural tsp apart functional slice understand novelty approach chip organization conventional chip multiprocessor cmp tile independent core interconnect chip network exchange data core instruction execution stage instruction fetch instruction decode ID execution ALUs EX memory access mem writeback WB update GPRs contrast conventional multicore tile heterogeneous collection functional globally homogeneous tsp inverts local functional homogeneity chip global heterogeneity tsp reorganizes homogeneous dimensional mesh core functionally slice microarchitecture approach tile implement specific function stack vertically slice dimension 2D chip mesh  core per respective function instruction dispatch icu memory mem integer int arithmetic float FPU arithmetic network net interface slice label 2D chip mesh contains functional slice organization functional slice independently sequence instruction specific chip role instance mem slice organization dataflow within chip network mul arithmetic functional slice VXM MXM slice slice tile execute instruction simd factor instruction decode dispatch logic tile icu decompose normal instruction execution pipeline instruction fetch decode parcel operand execute writeback approach decouples memory subsystem functional retrieve operand deposit functional slice implement stage vector pipeline span tile slice tile maximum vector organization naturally decomposes instruction vertical dimension data horizontal dimension function processor organization instruction execution tile instruction fetch decode icu operand decode execution writeback tile functional slice vertical dispatch instruction intersects horizontal operand data operating parallel lane data parallelism slice simd execution via program abstraction parallel lane parallel lane correspond data vector abstraction ML framework tensorflow tsp model instruction   functional slice data operand functional slice inter lane data movement within vector chip network SXM slice chip network implement dim mesh dim mesh tile YX dimension rout instruction specifies hop direction memory instruction semantics address dataflow direction rout dimension mem rout dimension SXM   data vertically mem SXM deterministic rout data dimension respectively byte data int int construct respectively multi byte data naturally align data data alignment accomplish compiler instance int align int align quad SG SG forth conventional load architecture purpose register GPRs access alu operand storage alu output sum vector RISC core loop instruction perform wise addition conventional RISC execution contrast producer consumer tsp load load operand GPRs memory tsp architecture functional slice interact data producer consumer fashion consume operand onto possibly assembly operator functional slice conveyor belt conceptually functional slice fix data across processing data slice functional optionally intercept data operand compute processing alu data lane network switch program abstraction conduit data functional slice unlike GPRs functional slice parallel data across chip horizontally operand intercept vertically  instruction perform computation functional slice compiler precisely chip architectural knowledge ensure instruction correctly intercept operand implement hardware chip register file SR architecturally visible transport operand slice software involves reading operand data mem slice subsequently consume downstream arithmetic slice operation onto another memory operation instruction execute mem slice inward int slice perform lastly memory via collection upon simd manner functional slice organization remainder describes microarchitecture  tensor processor tsp contribution introduce functional slice tile microarchitecture program abstraction built upon implementation tsp ASIC technology memory functional  model instruction architecture ISA tradeoff efficient operation batch performance resnet image classification model execute sample image query yield IPS batch inference throughput improvement google tpu habana lab goya chip detailed discussion architecture tradeoff accelerate machine workload generally lesson mapping resnet image classification model tsp hardware II architecture overview tensor processor architecture deliberate tradeoff hardware software interface complexity associate schedule compiler specifically compiler precisely schedule instruction hardware correctly efficiently involve algorithm meta operation realize hardware remove complexity dynamic instruction schedule multi issue execution allows instruction icu relatively accounting compiler access architecturally visible lane program abstraction overlaid tsp diagram tile chip mesh operates lane simd manner refer lane superlane functional slice chip minimum granularity computation superlane architecture minimum vector  likewise vertical composition tile functional slice maximum vector  independent instruction queue  chip issue instruction per cycle compiler explicit program instruction queue logical per lane operand chip    globally SRAM delivers byte per lane bandwidth latency access model parameter mem MXM install array cycle SRAM chip network transit delay designate identifier direction instance designates inward outward chip inward chip bisection outward outward chip cardinal direction   register numbered location functional slice within superlane component superlane organize spatially tsp instruction architecture ISA defines instruction span functional partition global address PGAS memory mem slice memory semantics vector address SRAM load architecturally visible direction dataflow photo ASIC implementation  tsp functional slice intend instruction icu explicit instruction fetch  inter slice synchronization sync notify instruction perform chip barrier synchronization participate functional slice nop instruction allows precise cycle cycle inter instruction delay compiler cycle accurate schedule operation intervene nop cycle opa nop  vector execution module VXM consists mesh ALUs lane wise arithmetic operation matrix execution module MXM consists independent 2D MACC accumulate array int data chip data movement switch execution module SXM intra superlane inter lane switch rearrange vector SXM analogous net interface communicate core mem SXM tandem dimension chip network hemisphere chip memory module mem compose parallel slice SRAM memory concurrency fully utilize direction slice physical address byte memory byte lane  chip SRAM chip chip CC module primitive exchange byte vector chip tsp implementation sixteen link operating gbps chip bandwidth function instruction description icu nop operation delay cycle  fetch instruction local memory sync park instruction dispatch queue await barrier notification notify release pending barrier operation instruction resume config configure mode previous instruction cycle iteration mem load vector address onto register content memory address indirectly address onto scatter indirectly address VXM unary operation wise operation operand mask negate binary operation wise operation operand mul sub conversion convert fix float vice versa relu rectify linear activation function max tanh hyperbolic tangent activation function exp exponentiation  reciprocal MXM LW load LW buffer IW install IW LW buffer array abc activation buffer abc initiate coordinate activation acc accumulate acc int FP MXM SXM shift lane shift lane shift vector permute bijective permute input output distribute rearrange replicate data within superlane lane rotate rotate input data generate output rotation transpose transpose output interchange CC  manage skew across  link byte vector byte vector  memory summary  functional slice 0Gb direction chip pin bandwidth flexibly partition  interconnection network  host interface pcie gen handle module lightweight dma  model onto tsp memory entry bootstrapping model execution mechanism passing interrupt host multi memory error sequence instruction perform functional slice chain complex action without writeback intermediate memory allows efficiently bandwidth latency parallel program model machine algorithm typically vector coefficient specify data int etc interpret vector abstraction underlie data operation simd manner tsp operates vector sometimes organize rank tensor relies graph lower compiler transform rank tensor rank tensor hardware data tsp program model producer consumer model functional slice consumer producer vector memory identifier direction   vector register direction spatially adjacent functional slice coordinate spatial coordinate increase direction vector slice access operand slice similarly slice access register cycle propagate functional slice overwritten slice cycle similarly consume functional coordinate absent overwrite available cycle slice operand steer slice consume constantly across chip slice communicate another graphical depiction interleave functional register combine program model memory model chip memory operand functional slice reading address memory mem slice denote  memory partition hemisphere slice numbered slice mem closest VXM mem SXM mem slice comprises tile vertical stack yield  per slice capacity  slice chip slice memory concurrency operand per lane cycle slice memory partition byte across superlane byte occupy lane input channel output feature byte lane byte lane forth tile portion vector concatenate adjacent tile beneath instruction execute cycle cycle stagger manner across tile slice instruction  span cycle tile slice sake exposition assume ghz operating frequency core register bandwidth export mem interface mem hemisphere capable functional adequately fed data operand saturate peak arithmetic capacity functional register combine capacity TiB operand bandwidth direction byte lane lane TiB SRAM shuttle data register SRAM SRAM bandwidth exceed bandwidth SRAM bandwidth chip memory equation  slice  slice byte cycle TiB chip memory bandwidth TiB SRAM bandwidth hemisphere instruction fetch described consumes maximum SRAM bandwidth TiB maximum instruction fetch bandwidth mem hemisphere export TiB bandwidth TiB SRAM bandwidth satisfy maximum instruction issue rate TiB across functional slice TiB SRAM bandwidth TiB instruction fetch bandwidth leaf TiB SRAM bandwidth service TiB register bandwidth operand stagger instruction execution tsp program model instruction issue functional slice compiler schedule executes simd operation operand vector vector micro architectural simd instruction pipelined across vertical stack tile slice schedule instruction issue tile slice correspond superlane operand vector subsequent cycle instruction propagate tile  slice executes instruction super lane operand vector  cycle traverse tile slice combination vertical instruction pipelining described along operand instruction coincide precise spatial stagger simd operand data depict byte vector  along data successive  lag cycle accommodate pipelined execution MXM instruction issue southern tile depict error handle reliability deployment within warehouse computer hardware error correction resilient transient error error code ecc vector SRAM memory register chip memory highly replicate avoid replicate xor compute ecc across memory instead advantage producer consumer program model generate ecc producer alongside memory ecc ecc scheme implement SECDED  correction error detection tolerate error memory anywhere along data functional slice consume ecc ensure data integrity operating mechanism SRAM error datapath error arise register error upset  operand instruction text automatically status register csr error handler interrogate later transient error automatic correction  proxy identify marginal chip chain functional slice functional slice predefined instruction mul etc define operation furthermore functional slice consume operand complex sequence operation  compose stagger instruction execution dataflow within superlane slice coordinate producer consumer manner output accomplish logically chain multiple slice consume input data slice data later consume slice manner functional slice direction logically around reverse direction vice versa slice cooperative producer consumer model operating data compose elaborate operation chain functional slice equation composite function amalgam functional slice chain mem SXM MXM dataflow composition allows exploit data locality passing data across multiple functional slice optionally data output output functional slice input another slice logical chain operation register scalable vector underlie data tsp hardware vector vector superlane  chip minimum vector  byte  byte array  byte comparatively typical simd extension  vector VL instruction configure tile mode effectively unused superlane mesh reduce consume scalable vector approach allows VL byte lane unused tile yield proportional instruction tsp instruction architecture ISA expose temporal information instruction compiler precise instruction dispatch augment instruction temporal parameter  functional delay instruction cycle output  timing parameter allows compiler output instruction available architecturally visible register  instruction operand skew timing relationship instruction dispatch relative operand  parameter instruction informs compiler schedule operand arrival instruction dispatch properly intersect parameter spatial relationship instruction operand conceptually compiler dimensional schedule instruction data register location chip execution instruction instruction functional delay propagation transit delay register location sri  superlane dataflow  equation execute instruction tile functional slice  functional delay instruction execute cycle output sri register location route consumer  transit delay distance cycle  sri tsp program model relies critical deterministic data hardware expose temporal information instruction execution latency ISA compiler precisely chip expose additional temporal information across static dynamic interface  hardware remainder summary instruction available functional slice discus functional slice assembly instruction icu instruction instruction icu functional slice instruction nop synchronization primitive sync notify independent functional slice synchronize compiler instruction execution cooperative parallelism functional slice chip import  api random tensor  int random tensor  int strm strm strm strm  malloc  listing compiler explicit  temporal separation instruction program nop allows nop 1GHz compiler nop instruction relative timing functional slice data nop implement icu tile functional slice nop allows slice enables nop ing anything longer cycle nop likely instruction programmer visible compiler insert implicitly synchronization functional slice independent however compiler logical program conceptually program counter conventional cpu compiler independent program queue cycle cycle basis logical compiler IQ chip nop instruction coordinate temporal relationship instruction IQ instruction IQs addition  synchronization across functional slice chip program correctness role sync notify instruction barrier synchronization mechanism across independent queue chip IQ designate  issue notify instruction IQs park sync instruction receipt notify broadcast IQs satisfy pending sync processing instruction barrier synchronization chip reset however program preamble instruction configure tile perform sync ensure functional slice align logical chip barrier synchronization accomplish cycle notify issue sync satisfied retire subsequent instruction compulsory barrier synchronization functional slice compute communicate synchronization manner register program correctness timing model chip instruction fetch  instruction operand text instruction program IQ byte byte vector instruction functional slice fetch instruction simultaneously normal instruction execution compiler performs  prefetching program text IQs cycle insert  instruction slice instruction imperative IQs empty precise notion logical maintain across chip memory mem memory mem slice program abstraction partition global address address laid uniformly across slice mem slice contains pseudo dual SRAMs capable service request simultaneously assume target expose compiler manage underlie SRAM efficiently appropriately allows compiler advantage MXM diagram activation load array int FP inside memory concurrency slice operand mem slice indirect address mode address address fully specify instruction indirect address content specify address scatter indirect address physical address layer indirection memory reference vector VXM processor superlane implement mesh vector ALUs capable simd computation vector ALUs per lane alu input operand organize along naturally align quad SG vector ALUs code status flag instruction stateless instead VXM saturate modulo variant mod mul mul mod addition multiplication allows semantics handle arithmetic exception tsp chain vector ALUs within lane multiple alu operation perform without commit intermediate memory subsequent intermediate allows efficient parallel implementation algorithm batch normalization quantization complex activation function leaky relu activation function instance matrix execution module MXM matrix execution module MXM independent accumulate MACC comprise  partial sum cycle pas adjacent tile computation byte install IW  cycle direction allows  MXM simultaneously MXM hemisphere load chip cycle instal cycle MXM generate int dot input activation instal SXM combination shift operation combine feature output MXM accumulate accumulator int output MXM  integer float byte tandem float sum output convert int switch execution module SXM switch execution module SXM contains function transposition permutation shift rotation data collectively operation perform tensor reshape operation ML workload fulfill functionality net slice data movement chip rout data dimension horizontally propagate dimension shuttle SRAM functional within superlane vertically dimension SXM direction SXM lane shifter execute shift instruction lane shifter usually allocate typically shift vector shift shift  data detail addition SXM permute instruction employ programmed bijection remap lane similarly indexed per superlane distributor slice within SXM arbitrarily remap lane within superlane pas distributor remapped bandwidth zero efficient mechanism tensor operation zero pad rearrange filter transpose dimension tensor operation tensor data tsp  transpose organize transpose operation incoming output exchange allows efficiently data atomic byte mem mem slice addressable instance SXM chip hemisphere issue transpose instruction yield maximum simultaneous transpose operation IV resnet  implement resnet popular image classification model tsp hardware hardware software stack critical mapping underlie tensor operation tsp instruction implement compiler responsible memory management tensor activation program text describes model mem compiler globally address  capacity policy compiler reserve mem slice instruction dispatch slice machine cod instruction service  instruction functional slice instruction eventually execute objective model implementation seek maximize functional slice utilization minimize latency advantage operand   MXM matrix operation vector ALUs lane tasked  int output MXM int activation function relu perspective performance chain functional slice MXM input another functional slice VXM eliminate operation intermediate mem plot consumption program executes layer layer spike correspond cycle perform simultaneous convd operation regime saturate tsp arithmetic throughput explicitly manage memory maximize concurrency compiler allocates memory tensor concurrent operand mem slice propagate mem operand mem slice route MXM grain memory management expose various memory concurrency ISA compiler explicitly schedule individual mem slice simultaneously operand slice transpose instruction input output transpose expose concurrency within mem slice advantage pseudo dual SRAM dual access per slice input SRAM concurrency operation transpose rotate etc max pool operation solid operand dot data concurrent memory SXM undergo transposition mem commit SRAM evident operation precede instruction operand commit mem conventional CPUs rely memory hierarchy implicitly data cache service load operation cache hierarchy introduce reactive agent data undesired unpredictability non determinism data illusion sequentially consistent memory transaction within memory hierarchy tsp mem unlike conventional CPUs instead layer memory management identify memory concurrency operation operation basis code memory management transpose operation instruction input creates output malloc function return tensor address allocate across memory slice concurrent slice onto transpose data slice import  tensor random tensor  int layout tensor transpose  malloc layout  listing memory management resource bottleneck maximize chip resource fully utilize expensive resource tsp MXM MACC array mem slice implementation resnet available alu resource balance computationally expensive operation convolution matrix bandwidth VXM perform  relu operation preparation layer operation limited situation VXM alu resource bandwidth due operation perform  depth operation software pipeline throughput delay minimize parallelism across VXM ALUs concurrency available within alu int data optimization revision resnet utilized algorithm distribute operation across chip advantage compute performance MXM VXM resnet Conv2D  relu tensor resnet layer continuously data MXM VXM cycle pipeline functional slice become available computation tensor pipeline memory delay pipeline approach pipeline layer resource utilized operation latency bubble pipeline empty initial memory allocation prevent pipeline previous pipeline empty due memory slice contention adjust memory allocation input output tensor distribute data across multiple slice interleave within memory slice carefully orchestrate previous pipeline output memory previous pipeline optimization reduce overall latency resnet implementation approximately cycle performance IPS quantization initial implementation resnet training layer symmetric int quantization strategy convolution matrix multiplies MXM accepts int input accumulates int respectively  int VXM capacity rate output MXM approach enable precision across operation matrix multiplies convolution improve model overall precision quantization loss quantize operation initial approach leaf improve architecture capacity  asymmetric quantization approach future revision reduce quantization accuracy loss model accuracy MXM capacity matrix channel depth layer resnet input output channel depth convolution dimension misalignment capacity dimension split across multiple utilizes MXM fitting model capacity MXM increase computation without additional latency alternative version resnet increase channel depth advantage MXM capacity additional contribute accuracy model standard resnet accuracy alternative version advantage VL accuracy encourage demonstrates exploit additional model capacity  improve accuracy computational latency deterministic performance tsp hardware eliminates arbiter reactive data performance deterministic precisely predictable execution within resnet model latency layer resnet resnet resnet structure exception additional layer demonstrate performance resnet tsp project performance resnet resnet cycle resnet implementation resnet throughput IPS resnet throughput IPS discussion describes initial proof performance mapping resnet image classification model underlie tensor processor preface discussion author silicon fab july isca deadline brief timespan validate silicon implement resnet architecture compiler assembler chain debug visualization nevertheless initial implementation resnet proof reference model compiler validation perform inference query resnet model yield throughput image per image sample query batch speedup relative google tpu batch inference importantly tsp inference latency image sample nearly reduction latency intel habana goya inference chip batch inference roofline diagram arithmetic throughput ghz core load usage resnet layer operating regime roofline diagram intuitive framework understand operating regime chip limited chip memory bandwidth arithmetic performance delineate slop peak diagram slop indicates tsp becomes memory bandwidth bound load MXM array subsequent CONV2D matmul roofline peak indicates saturation arithmetic operating peak utilization subsequently  limited matrix operation matrix operation workhorse ML workload mem slice memory install MXM array cycle SRAM chip network transit delay mem slice deliver byte operand parallel lane TiB operand bandwidth  label data model laid mem chip mem slice ideally compiler layout tensor mem slice data transit memory slice  MXM minimize chip network typically chip communication rout packet core whereby packet undergo rout arbitration output schedule incur conflict introduce nondeterminism however tick core tsp propagates register hop direction tsp hardware origin destination slice instead simply propagate   chip overwritten functional slice contrast conventional chip network tsp register numbered within mem data dimension superlane SXM data dimension chip lane permutation instruction schedule max pool resnet VI related   core  SRAM storage approximately  capacity model parameter however   bulk synchronous communication performs implicit synchronization contrast program model explicit synchronization producer consumer program coarse grain  architecture CGRAs focus highly regular communication image transforms ML workload embarrassingly data parallel tensor operation underlie hardware primitive memory pmu compute  stanford  supercomputer construct underlie program hierarchy local register file compute cluster access register file communication cluster contrast tsp architecture local register file FIFOs communicate instead rely chip register communicate processing functional slice prior research reduces chip communication leverage processing memory variable width compression locality aware tsp contains amount deterministic memory avoid frequently access chip memory prior proposal explore prune sparsity model domain specific data communication optimization tsp optimization maintain strictly deterministic execution profile vii conclusion introduce novel hardware architecture generation  tensor processor tsp tsp architecture reorganizes conventional 2D mesh core functionally slice tile microarchitecture   exploit dataflow locality within superlane dramatically reduce latency abundant chip memory bandwidth concurrently MXM array MACC accumulate core matmul CONV2D operation workhorse ML application parallel lane access powerful vector processor vector ALUs chip capable fix float operation int native data allows chip quantize inference model model training float ASIC technology yield billion transistor broadly spending transistor fix float ALUs arithmetic storage communicate data available ALUs maximize ALUs operand bandwidth conversion rate architecture extract underlie CMOS technology operation perform raw performance normalize transistor generation  tsp ghz ASIC package pcie  factor yield peak performance  sec transistor chip ops sec transistor comparatively volta capable  mixed precision arithmetic transistor implement ASIC node yield ops sec transistor gpus tsp architecture delivers computational density ops speedup application performance demonstrate nearly speedup batchsize throughput nearly reduction inference latency tpu gpu habana lab goya chip