manycore processor core hardware cache coherence tremendous peak throughput highly parallel program complexity efficient manycore processor combine performance core execute operating legacy code serial heterogeneous cache coherence HCC hardwarebased cache coherence core software centric cache coherence core unfortunately program heterogeneous cache coherent enable collaborative execution challenge dynamic task parallelism seek address challenge combination software hardware technique detailed description implement steal runtime enable dynamic task parallelism heterogeneous cache coherent propose task steal dts technique  interrupt bypass memory improve performance efficiency steal demonstrate execute dynamic task parallel application core  HCC dts achieve speedup core speedup equivalent  hardware cache coherence performance efficiency core hardware cache coherence introduction parallelism specialization currently technique increase transistor moore performance hardware specialization demonstrate strength domain gpus data parallelism accelerator multi thread application multi core processor hardware architect rely parallelism improve performance processor decade trend increase processor core likely manycore approach integrates relatively core increase hardware parallelism manycore processor core intel knight core  tile core  manycore approach demonstrate potential achieve throughput efficiency per multi thread workload hardware designer realize unoptimized hardware cache coherence protocol  MESI variant due directory storage overhead network latency overhead verification complexity performance complexity scalable hardwarebased cache coherence protocol remains active research another approach increase core manycore sidestep hardware cache coherence adopt software centric cache coherence software manage scratchpad memory message passing without memory manycore processor without hardware cache coherence fabricate core   academia core  core  away hardware cache coherence processor achieve exceptional theoretical throughput efficiency due massive core relatively hardware however widely adopt challenge software developer programmer familiar cpu program model dynamic task parallelism intel cilk plus intel thread building TBB openmp program model parallel task generate mapped hardware dynamically software runtime express parallel automatic load balance improve portability legacy code unfortunately manycore processor without hardware cache coherence programmer explicitly manage data coherence private cache memory adopt restrict program model explicit task partition message passing remote program program model arguably primary manycore processor without hardware cache coherence widely accepted furthermore exist manycore processor without hardware cache coherence generally discrete coprocessor living memory purpose processor host processor enable efficient collaborative execution host manycore processor significant effort bridge gap disparate program model hide data offload latency recent heterogeneous cache coherence HCC demonstrate hardware cache coherence protocol seamlessly efficiently integrate software centric cache coherence protocol chip unified memory address HCC solves issue data offload tightly integrate host manycore program model challenge remains address acm annual international symposium computer architecture isca doi isca dir dir MC dir dir dir dir dir dir MCMC  MCMC diagram manycore HCC efficiency core performance core chip interconnect router private cache software centric cache coherence cache hardware cache coherence dir directory cache MC memory controller attempt address software challenge manycore processor HCC offering TBB  program model steal runtime addition propose task steal dts hardware mechanism inter processor user interrupt improve performance efficiency dynamic task parallel application HCC apply approach manycore architecture HCC consists performance core hardware cache coherence core software centric cache coherence approach allows dynamic task parallel application popular program framework intel cilk plus intel TBB collaboratively core core without fundamental II background HCC  runtimes detail implement steal runtime core component dynamic task parallel framework HCC IV task steal technique address inherent overhead HCC explain dts enables important optimization steal runtime improve performance efficiency cycle evaluation methodology demonstrate potential approach technique apply core complexity effective HCC TBB cilk dynamic task parallel application achieve speedup core speedup equivalent core hardware cache coherence performance efficiency optimistic hardware cache coherence contribution knowledge detailed description implement steal runtimes HCC propose task steal technique improve performance efficiency dynamic task parallel application manycore processor HCC detailed cycle evaluation technique II background background cache coherence protocol HCC dynamic task parallelism characterize representative hardware software centric coherence protocol exist HCC lastly brief overview dynamic task parallelism exemplify TBB cilk program model hardware software centric coherence cache coherence protocol hardware software centric hardware protocol data coherence private cache handle completely hardware transparent software purpose processor usually hardware cache coherence protocol easy program contrast software centric protocol software enforce coherence data private cache software centric cache coherence protocol relatively implement hardware effort software programmer taxonomy described previous categorize representative coherence protocol MESI DeNovo gpu WT gpu WB cache coherence protocol described stale invalidation dirty propagation granularity stale invalidation defines stale data private cache invalidate data return date version initiate invalidation stale data writer initiate reader initiate approach writer invalidates exist target data private cache prior data approach hardware coherence protocol approach reader initiate reader invalidates potentially stale data private cache data dirty propagation defines dirty data becomes visible private cache coherence protocol private cache dirty data owner propagate data reader strategy ownership dirty propagation MESI DeNovo strategy propagate dirty data contrast gpu WB gpu WT ownership dirty data instead rely writer dirty data writes immediately cache dirty data later flush instruction lastly granularity defines data writes perform ownership cache manage summarize coherence protocol MESI DeNovo gpu WT gpu WB difference fundamental offs classification cache coherence  protocol initiate invalidation dirty data propagate granularity MESI writer owner DeNovo reader owner gpu WT reader owner gpu WB reader owner hardware software centric coherence protocol hardware protocol writer multiple reader SWMR invariance enforce SWMR hardware coherence protocol transparent software software assume cache however additional hardware complexity communication storage overhead extra invalidation traffic transient coherence directory storage software centric protocol DeNovo gpu WT gpu WB suffer overhead reader initiate stale invalidation strategy protocol reader cache communication traffic interconnection network directory storage neither  gpu WB ownership writes DeNovo MESI gpu WB  ownership dirty propagation MESI potentially improve performance writes atomic memory operation amo reader initiate stale invalidation gpu WB gpu WT reduce invalidation overhead however software centric coherence protocol complexity software software issue invalidation flush request appropriate ensure coherence gpu WT gpu WB suffer amo performance amos handle globally cache private cache ownership heterogeneous cache coherence previous explore integrate multiple coherence protocol heterogeneous cache coherent diversity coherence requirement ibm coherent accelerator processor interface  coherent MESI proxy interface accelerator FPGAs communicate generalpurpose processor cache directory propose directory approach maintain data coherence cpu gpu coarse granularity cache  recent proposal coherence interface implement cache coordinate coherence protocol protocol interact interface translation program model dynamic task parallelism task parallelism style parallel program workload task computation execute parallel dynamic task parallelism task dependency task generate runtime task dynamically assign available thread computation model dynamic task parallelism fork model mit cilk later popularize program framework intel cilk plus intel TBB others fork parallelism refers specify parallel execution program program diverges fork execute parallel necessarily task fork parallel task refer spawn task task task simply spawn task becomes task task completes model basis express complex parallel  conquer parallel loop reduction nest program framework fork model parallel execution realize runtime  algorithm steal runtimes thread associate task queue data structure task available execution task queue queue deque task spawn task enqueues task queue execute thread thread available attempt dequeue task deque  thread deque empty steal task another thread deque  fifo thread steal becomes thief thread task steal becomes victim task thread execute steal thread algorithm automatically balance workload across thread locality establish bound   runtimes heterogeneous cache coherence detailed description approach implement steal runtime HCC baseline runtime hardware cache coherence TBB cilk program model implementation heterogeneous cache coherence conclude qualitative analysis implication HCC steal runtime program application program interface api intel TBB demonstrate program model task described derive task virtual execute programmer override execute define execution task schedule task spawn task function task synchronize function reference  public task public sum int  int sum sum sum void execute sum return   reference task spawn task spawn task sum fib spawn fib int return parallel invoke fib fib return fib parallel invoke void  int int int dst  parallel  dst vector vector parallel task parallel program calculate  apis lowlevel api explicit spawn api generic templated parallel invoke alternative generic templated parallel unfinished task executes execution task stall addition apis spawn programmer templated function various parallel programmer parallel parallel loop parallel invoke conquer baseline steal runtime implementation intel TBB spawn function hardware cache coherence spawn task pointer onto thread task deque thread schedule loop inside schedule loop thread task deque dequeues task  local deque execute none local deque thread becomes thief attempt steal task another thread fifo task execute reference atomically decremented reference zero task exit schedule loop return thread exit schedule loop program thread terminates thread task queue HCC task queue data structure thread processor hardware cache coherence protocol per deque lock implement atomic modify operation sufficient implement synchronization processor heterogeneous cache coherence private cache software centric cache coherence protocol additional coherence operation thread access task queue addition acquire lock mutual exclusion data private cache execute thread invalidate prevent reading stale data thread access task queue addition release lock dirty data flush cache data becomes visible thread implementation spawn HCC protocol invalidate instruction lock acquire flush instruction lock release protocol invalidate flush private cache DeNovo flush ownership  strategy DeNovo flush treat MESI protocol neither flush invalidate treat ops task steal HCC invalidate flush instruction along per deque lock ensures task queue  coherent HCC however user data coherent fortunately TBB program model structure dag consistency model informally dag consistency model data exists task task dag consistency requirement runtime fulfill task task synchronization sibling task sibling task data regard allows correctly implement steal runtimes HCC without user code task spawn task flush enqueue ensures data task visible task steal execute another thread requirement task invalidate data cache steal stale data moreover thread invalidate flush execute steal task respectively void task spawn task tid lock tid enq tid lock void task task tid lock task tid deq tid lock execute amo sub int vid victim vid lock vid steal vid lock execute amo sub hardware cache coherence void task spawn task tid lock cache invalidate tid enq cache flush tid lock void task task amo tid lock cache invalidate task tid deq cache flush tid lock execute amo sub int vid victim vid lock cache invalidate vid steal cache flush vid lock cache invalidate execute cache flush amo sub cache invalidate heterogeneous cache coherence void task spawn task uli disable tid enq uli enable void task task int uli disable task tid deq uli enable execute steal amo sub int vid victim uli req vid steal task tid cache invalidate execute cache flush amo sub steal amo steal cache invalidate void uli handler int thief task tid deq steal steal task thief cache flush uli resp thief task steal steal runtime implementation pseudocode spawn function hardware cache coherence heterogeneous cache coherence task steal task pointer task pointer reference tid worker thread lock acquire lock lock release lock array task queue per worker thread enq enqueue task queue deq dequeue task queue return empty victim random victim selection vid victim steal dequeue task queue amo atomic fetch amo sub atomic fetch sub cache flush flush dirty data cache MESI DeNovo gpu WT cache invalidate invalidate data cache MESI uli disable disable service uli uli enable enable service uli uli req uli request message response uli handler receiver uli resp uli response message steal task steal task per thread mailbox steal task steal task mailbox task execute thread victim thread HCC performance impact qualitatively discus performance impact HCC steal runtimes defer quantitative characterization VI reader initiate invalidation strategy degrades performance software centric cache coherence protocol DeNovo gpu WT gpu WB due cache spawn execute thread invalidate data private cache later cache atomic operation DeNovo ownership dirty propagation strategy amos perform private cache MESI however gpu WT gpu WB amos perform cache increase latency per operation flush inefficient DeNovo gpu WT flush operation gpu WB explicit flush spawn execute steal task entire private cache cache significant amount memory traffic amount data grain task exacerbate performance impact directly related task granularity finer grain application task amos invalidation flush therefore HCC severe performance impact application grain task IV task  propose hardware technique task steal dts improve performance steal runtimes HCC dts leverage steal runtimes parallelism sufficient steal relatively local task enqueues dequeues steal synchronization victim thread thief thread thread mention previous steal incur significant overhead performance memory traffic HCC dts address issue task steal directly user interrupt described indirectly memory implement task steal user interrupt uli  interrupt uli  architecture ISA RISC regular interrupt user interrupt handle asynchronously enable disabled core software core uli enable receives uli execute thread suspend core software specify uli handler difference uli regular interrupt  handle completely user mode handle uli regular context switch minus overhead associate privilege mode implementation steal runtime dts dts uli perform steal thread attempt steal task sends uli victim thread victim uli enable execution victim redirect handler handler victim access task deque retrieves task sends task thief memory victim sends ack message thief uli response dts victim steal task behalf thief victim uli disabled nack message thief discus dts reduce overhead steal runtimes HCC optimization reduce task queue synchronization dts reduces synchronization associate task steal HCC dts task queue longer data structure task queue access owner local access steal uli dts therefore eliminates synchronization lock task queue access task queue mutually exclusive thread disable uli operates task deque steal runtimes HCC without dts deque access thread task deque invalidate flush associate lock acquire release respectively steal runtimes dts access task queue longer incurs cache invalidation flush task queue private dts reduces cache HCC cache invalidation flush improves performance reduces memory traffic optimization reduce synchronization dts opportunity reduce synchronization steal HCC software optimization steal runtimes without dts ass task actually steal steal happens implicitly memory without information runtime conservatively assume task potentially steal runtime ensure synchronization task reference task update amos task invalidates cache return function steal dts enables runtime task steal avoid synchronization entirely task steal particularly important cache coherence protocol flush gpu WB amos gpu WT gpu WB expensive task steal auxiliary variable steal task task steal steal task thief uli handler victim steal steal variable access thread task modify regular load instead amo accord dag consistency model task steal steal false writes visible thread task writes execute thread flush task actually steal instead enqueue operation considerably local enqueues dequeues steal sufficient parallelism dts significantly reduce flush furthermore task steal unnecessary perform invalidation task cannot possibly stale data task execute thread finally task steal update reference amos instead reference treat local variable update perform thread summary dts enables software optimization leverage dag consistency model  runtimes reduce overhead invalidation flush amo steal runtimes HCC evaluation methodology cycle performance model methodology quantitatively evaluate proposal simulated hardware model manycore gem cycle simulator implement HCC protocol described II ruby memory model infrastructure  model chip interconnection network OCN model per core private cache cache cache cache cache integrate cache coherence protocol  cache coherence request cache coherence protocol cache baseline MESI protocol inclusive cache cache HCC protocol inclusive MESI private cache directory embed cache precise sharer MESI private cache additional directory latency implement inter processor uli simulated manycore specify RISC ISA model uli network mesh network virtual channel request response prevent deadlock ruby  assume uli network cycle channel latency cycle router latency model buffering uli network uli message uli core enhance core hardware uli uli dedicate register hardware core buffer request buffer response buffer receiver sends nack sender configure core oforder performance core core core private cache capacity KB capacity core cache core chip mesh network mesh cache dram controller schematic diagram simulated II summarizes parameter configuration MESI core equip MESI hardware cache coherence HCC dnv HCC core MESI core DeNovo  variant similarly HCC gwt II simulator configuration core RISC ISA  issue cycle execute non memory inst cache cycle KB LI KB L1D software centric coherence core RISC ISA  entry lsq physical reg entry rob cache cycle KB LI KB L1D hardware coherence cache KB per per mesh heterogeneous cache coherence OCN mesh topology XY rout per flit cycle channel latency cycle router latency memory dram controller per chip per mesh 6GB bandwidth HCC gwb HCC configuration gpu WT gpu WB core respectively addition implement dts propose IV HCC configuration refer configuration dts HCC dts dnv HCC dts gwt HCC dts gwb comparison configuration traditional multicore architecture core configuration core respectively CACTI model cache core KB cache core KB cache capacity CACTI estimate core configuration core core future manycore processor likely core overcome challenge simulation weak approach simulate envision manycore core memory bandwidth core described II moderate input dataset moderate parallelism attempt representative future manycore core memory bandwidth proportionally input validate proposal subset application kernel evaluate datasets core steal runtime implement variation library steal runtime described performance baseline runtime hardware cache coherence intel cilk plus intel TBB application natively core xeon processor dataset appropriate native execution baseline steal runtime performance intel TBB intel cilk plus simulated application   speedup serial IO speedup MESI HCC HCC dts input GS PM  span para ipt MESI dnv gwt gwb dnv gwt gwb cilk cilk cilk cilk cilk ligra rMat ligra rMat ligra bfs rMat ligra  rMat ligra rMat ligra rMat ligra radius rMat ligra rMat geomean input input dataset GS task granularity PM parallelization parallel recursive spawn sync  dynamic instruction instruction span instruction critical para logical parallelism define span ipt average instruction per task span ipt analyze  HCC heterogeneous cache coherence dnv DeNovo gwt gpu WT gwb gpu WB benchmark dynamic task parallel application cilk ligra steal runtime application varied parallelization application cilk mainly recursive spawn sync parallelization parallel invoke application ligra mainly loop parallelization parallel ligra application exhibit non determinism typically grain synchronization swap code cilk performs parallel mergesort algorithm calculates LU matrix decomposition cilk matrix multiplication cilk matrix transpose cilk backtracking queen  calculates  centrality graph ligra bellman ford algorithm calculate source shortest graph ligra bfs performs graph ligra  vector optimize version ligra computes component graph ligra solves maximum independent ligra radius computes radius graph ligra graph detailed description benchmark previous task granularity task granularity task important task parallel application programmer task granularity grain coarse grain task task granularity fundamental grain task increase logical parallelism incur runtime overhead coarse grain task hybrid simulation native approach task granularity application sweep granularity  analyze speedup logical parallelism ligra core task granularity task logical parallelism evaluate speedup serial code granularity simulated manycore processor core suitable granularity application achieves speedup serial execution instruction per task ipt logical parallelism ligra granularity demonstrates granularity sub optimal performance former due lack parallelism latter due runtime overhead granularity penalizes HCC configuration heavily benefit dts technique pronounce choice task granularity aim optimize performance baseline relative benefit propose dts technique VI RESULTS summarizes speedup simulated configuration illustrates speedup HCC configuration relative MESI rate data cache execution breakdown core memory traffic byte chip network baseline runtime MESI application MESI performance baseline steal runtime enables collaborative execution load balance core MESI cilk performs MESI runtime overhead outweigh parallel speedup overall MESI demonstrate effectiveness unlock parallelism equivalent traditional multi core configuration steal runtime HCC discus steal runtime HCC analyze performance HCC dnv HCC gwt  MESI HCC dnv decrease rate due reader initiate invalidation strategy decrease rate slight increase memory traffic cpu req data resp category impact negative performance modest application cilk cilk significant performance degradation due additional invalidation increase data latency traffic HCC gwt  protocol gpu WT refill cache therefore HCC gwt unable exploit temporal locality writes significantly rate MESI  network traffic HCC gwt significantly others req category regardless update cache latency amos network traffic increase respectively HCC gwt slightly performance significantly network traffic MESI HCC dnv application cilk performs significantly HCC gwb performance HCC gwt amos however policy allows HCC gwb exploit temporal locality application cilk HCC gwb memory traffic rate performance  HCC gwb efficient memory traffic HCC dnv due lack ownership private cache propagate dirty data cache summary baseline steal runtime HCC moderately performance MESI IV cache invalidation flush rate    app dnv gwt gwb gwb dnv gwt gwb cilk cilk cilk cilk cilk ligra ligra ligra bfs ligra  ligra ligra ligra radius ligra comparison HCC dts HCC  decrease cache invalidation  decrease cache flush  increase rate configuration almost application demonstrate HCC effectively reduce hardware complexity performance penalty evaluation HCC dts IV motivate dts synchronization task steal dts avoids cache invalidation flush unless steal actually happens HCC configuration without dts HCC dts HCC dts profile invalidate flush cache configuration summarize reduction invalidation flush IV calculate increase rate HCC dts configuration correspond HCC HCC without dts configuration HCC protocol across benchmark HCC dts significantly cache invalidation ligra ligra  reduction ligra reduction benchmark reduction flush reduce HCC  benchmark reduction ligra ligra  ligra dts achieves flush reduction due relatively steal IV reduction invalidation translates rate increase rate significant HCC dts dnv rate due ownership dirty propagation increase rate reduce network traffic HCC dts reduce network traffic cpu req data resp HCC dts gwb req traffic significantly reduce due reduction flush however dts cannot reduce req traffic HCC dts gwt cache speedup MESI data L1D cache rate aggregate core execution breakdown normalize MESI chip network traffic byte normalize MESI cpu req request data resp response req request data sync req synchronization request sync resp synchronization response dram req request dram dram resp response dram coh req coherence request coh resp coherence response MESI MESI HCC configuration heterogeneous cache coherence dts task steal dnv core DeNovo protocol gwt core gpu WT protocol gwb core gpu WB protocol RESULTS core speedup MESI input  MESI HCC gwb HCC dts gwb cilk ligra rMat ligra bfs rMat ligra rMat ligra rMat datasets core manycore processor processor consists core core configure mesh MB capacity dram controller core core parameter II input input datasets  dynamic instruction relative datasets HCC heterogeneous cache coherence dts task steal gwb gpu WB discus IV dts enables runtime optimization avoid amos reference unless steal HCC dts gwt HCC dts gwb benefit optimization amo latency protocol reduction traffic due amo request reduce execution atomic category HCC protocol HCC dts gwb benefit dts leverage reduction invalidation flush amos HCC gwt HCC dnv benefit dts flush HCC gwt benefit reduction amos req traffic unaffected dts overhead dts simulation application configuration uli network network utilization rate dts infrequent average latency uli network around hop cycle dts incur cache invalidation flush core hardware coherence available cycle interrupt core cycle interrupt core interrupt instruction flight processor pipeline commit jumping handler cycle spent dts execution dts minimal performance overhead evaluate technique application kernel input datasets execute kernel core core core core core memory bandwidth input increase dynamic instruction amount parallelism approximately achieve weak HCC configuration perform HCC protocol gpu WB core cache capacity core KB KB therefore cache capacity core equivalent core demonstrate HCC steal runtime allows dynamic task parallel application achieve performance simpler hardware MESI dts technique significantly improves performance steal runtime HCC relative benefit dts pronounce core core steal overhead without dts core summary overall combine HCC steal runtimes allows dynamic  application TBB cilk program model simpler hardware hardwarebased coherence slightly performance relative performance efficiency HCC protocol characteristic application dts reduces invalidation HCC protocol reduces flush HCC gwb dts gap enables HCC exceed performance fully hardware cache coherent regard geometric apps perform HCC protocol combine dts achieves improvement performance amount network traffic hardware cache coherence vii related optimize hardwarebased cache coherence protocol coarse grain sharer approach reduce storage overhead sharer cache expense unnecessary invalidation broadcast hierarchical directory attack overhead additional indirection directory technique increase directory utilization directory increase hardware complexity prior  propose remove sharer altogether however technique perform cache optimization technique dynamic  token coherence  prediction propose sequential consistency SC focus optimize cache particularly non uniform cache access NUCA static dynamic technique jigsaw address scalability interference cache software data placement collection partition coherence domain restriction improves scalability cache restrict data  leverage static dynamic information reduce data movement cache HCC primarily address scalability private cache complementary NUCA technique cache technique improve hardware cache coherence protocol leverage relaxed consistency model release consistency RC entry consistency scope consistency propose literature important observation prior unlike SC cache coherence enforce synchronization relaxed consistency model prior propose  acquires RC remove sharer   eliminate directory leverage  policy tso CC another invalidation protocol tso consistency model maintain tso consistency however tso CC additional hardware logic timestamps epoch steal runtime leverage dag consistency model heterogeneous coherence software centric coherence protocol prior DeNovo invalidation eliminate sharer addition software data drf DeNovo eliminates transient DeNovo greatly simplifies hardware drf requirement limit software scope address DeNovo DeNovo ND  lock nondeterminism arbitrary synchronization  gpu coherence protocol combine invalidation achieve simplest hardware suitable gpu workload  protocol described literature gpu WB differs gpu WT defer  barrier gpu WB manycore globally task queue prior propose efficient heterogeneous coherence protocol integrate gpu cpu accelerator  explores efficient interface various hardware software centric cache coherence protocol prior heterogeneous coherence protocol described MESI llc integrate cache coherence protocol private cache improve task program model various improve schedule algorithm improve efficiency software task queue reduce overhead memory fence carbon improves performance grain task adm active message improve task schedule carbon adm task model program framework drastically widely cilk TBB prior explore improve steal runtime heterogeneous core architecture implement propose task steal dts mechanism  interrupt simpler active message adm CONCLUSIONS demonstrate careful hardware software enable efficiently exploit dynamic task parallelism heterogeneous cache coherent important towards  era complexity effective architecture combine performance core efficient core highly productive program framework vision future programmer traditional dynamic task parallel program framework improve performance across core seamlessly without effort application significant performance improvement HCC dts gwb simply collaborative execution across core steal