online service commonly structure network software tier communicate datacenter network RPCs ongoing trend towards software decomposition prevalence tier generate RPCs runtimes microsecond software runtimes latency overhead rpc handle significant relative performance impact network bandwidth introduces queue within server memory hierarchy considerably hurt response latency grain RPCs introduce nebula architecture optimize accelerate challenge microsecond RPCs leverage novel mechanism drastically improve server throughput strict latency goal nebula reduces detrimental queue memory controller via hardware efficient llc network buffer management nebula network interface steer incoming RPCs cpu core cache improve rpc startup latency evaluation nebula boost throughput keyvalue exist proposal maintain strict latency goal index client server  network protocol queue theory memory hierarchy introduction online service deployed datacenters decompose multiple software tier communicate datacenter network remote procedure RPCs software trend microservices function service promote decomposition implication RPCs ubiquitous performance critical software tier data perform computation per rpc exhibit  runtimes software tier  computation ratio designate networking performance determinant response trend datacenter networking technology evolve rapidly deliver drastic bandwidth latency improvement datacenter topology ample diversity limit network queue enable roundtrip latency accommodate demand network bandwidth commodity fabric partially fund swiss national foundation project oracle lab accelerate distribute grant rapidly capacity tbps infiniband tbps ethernet roadmap although improvement sustain demand extreme software decomposition shift  bottleneck server confluence shrink network latency bandwidth  RPCs initiate killer microsecond across stack server implication historically negligible overhead tcp IP become bottleneck server source queue significantly affect latency RPCs load imbalance distribution RPCs cpu core directly affect latency load balance  RPCs recently attract considerable attention software hardware improve upon static load distribution NICs RSS source server queue attention network bandwidth gradually approach memory bandwidth memory access incoming network traffic interfere application access queue memory subsystem noticeably degrade latency RPCs bandwidth interference noticeable context latencyoptimized networking technology DDIO infiniband generation fully integrate architecture numa interference degrade server achievable throughput strict latency goal evident avoid interference specialized network traffic management memory hierarchy leverage advancement software architecture networking hardware tackle source detrimental server queue propose network buffer  nebula architecture optimize grain RPCs prior technique exist ameliorate queue source individually exist tackle nebula extends recently propose hardware load balance novel NIC driven buffer management mechanism alleviates detrimental interference incoming network traffic application memory access insight acm annual international symposium computer architecture isca doi isca network buffer provision function remote endpoint server communicates individual server peak achievable rpc service rate consequently network buffer typically footprint MBs GBs shrunk KBs deliver significant saving precious dram resource datacenters importantly nebula leverage observation network buffer SRAM resident via intelligent management absorb adverse bandwidth interference due network traffic nebula feature NIC core rpc steer introduce NIC extension monitor core queue RPCs directly steer queue rpc payload core cache core processing avoids cache pollution network packet placement DDIO enable NICs restrict llc accelerates rpc startup reduce overall response latency combine nebula memory hierarchy management optimization boost achieve throughput latency constraint prior proposal summary contribution architect currently dilemma building memory bandwidth interference NIC cpu core load imbalance core dilemma particularly pronounce combine immense bandwidth future NICs emerge software layer address aforementioned dilemma propose network protocol NIC hardware maximizes network buffer reuse server llc conduct mathematical analysis enables llc buffer management strict application latency goal met maintain shallow queue incoming RPCs queue easily accommodate llc advance network packet placement NIC memory hierarchy network packet cache avoid cache pollution improve response latency RPCs holistically optimize architecture RPCs integrate load balance network aware memory hierarchy management structure II highlight impact server queue latency RPCs arise load imbalance memory bandwidth interference due incoming network traffic quantifies queue model IV introduce nebula implementation respectively methodology VI evaluate nebula vii discus nebula broader applicability prospect datacenter adoption related IX conclude II background CHALLENGES online service latency sensitive RPCs online service deployed datacenters deliver highquality response plethora concurrent user response deliver highly interactive respond query latency datasets replicate across server software decompose multiple tier communicate network synthesize response inter server communication typically occurs RPCs user query trigger sequence rpc fan sub query span server fan strict limit response latency service tier commonly refer service objective slo focus service generate RPCs unique challenge presence already widespread memory data service profile become increasingly omnipresent trend microservice software architecture grain RPCs particularly vulnerable latency degradation otherwise negligible overhead become comparable rpc actual service frequency RPCs datacenter increase importance handle efficiently architecture latency networking microservices imply growth communication  ratio handle efficiently highly optimize network protocol operating component recent datacenter networking advancement address challenge user network stack dpdk hardware assist  microsoft  infiniband RDMA deployment  protocol drastically shrink overhead network stack processing demand latency motivates radical proposal approach fundamental bound networking latency propagation delay via server integration network fabric controller already exist academia numa fame RISC  soc intel omni gen latencyoptimized datacenters response demand latency sensitive online service protocol architectural optimization minimize network component individual rpc address server inefficiency identify source queue primary obstacle server handle RPCs contention rapidly network bandwidth inflict server memory channel load imbalance across server core exist technique address challenge address memory bandwidth interference load imbalance future commodity network fabric tremendous bandwidth tbps infiniband tbps ethernet already roadmap growth directly affect server incoming network traffic becomes non trivial server available memory bandwidth server naively architected network traffic  interfere memory request execute application queue noticeably degrade latency  RPCs therefore imperative future server prevent interference handle network traffic within SRAM cache requisite bandwidth pace performance user network stack complicate task cache network traffic management latency benefit zero synchronization message reception network buffer provision  basis dedicate buffer per connection server message communicate endpoint anytime connection orient provision creates fundamental scalability waste precious dram resource performance implication rpc library built RDMA NICs furthermore  interleave incoming request unpredictable access network buffer effectively eliminate probability buffer server llc increase dram bandwidth usage overlook significantly hurt latency RPCs aim reduce memory capacity waste  buffer provision infiniband queue srq option enable inter endpoint buffer srq reduce buffer footprint implicitly ameliorate memory bandwidth interference due increase llc buffer residency another approach prevent network buffer overflow memory statically reduce buffer available allocation network stack prior  unfortunately srq vulnerable load imbalance cpu core corresponds multi queue client specify queue QP request imply priori request core mapping multi queue limitation inherent RSS inter core load distribution synchronization  approach multicore server similarly multi queue configuration vulnerable load imbalance prior demonstrate distribution incoming RPCs server core crucially impact latency due fundamentally latency queue proposal advocate queue approach network switch operating NIC hardware RPCs overhead software load balance synchronization comparable rpc service NIC rpc arrival memory dirty writebacks demand access  rpc core assignment legend application data buffer llc memory hierarchy associate handle incoming RPCs model queue correspond throughput loss motivates hardware mechanism load balance RPCValet recently propose NIC driven mechanism  queue load balance improves throughput slo multi queue however RPCValet network buffer provision connection orient creates memory bandwidth interference proceed conduct theoretical shortcoming interference imbalance rpc throughput slo theoretical interference imbalance demonstrate performance impact previous  construct queue model capture critical interaction network core memory assume multi core cpu NIC packet directly llc DDIO model component queue interaction rpc handle incoming traffic NIC RPCs memory hierarchy assignment RPCs core demand traffic generate core service RPCs writebacks dirty llc memory load balance implication pertain hardware assist alternative multi queue behavior RSS srq queue behavior RPCValet former optimistically assume uniform load distribution performance RSS srq multi queue policy without penalize implementation specific overhead memory bandwidth interference concern relate network buffer exceed llc packet fetch buffer dram llc core memory request compete memory bandwidth access dirty llc consume additional memory bandwidth evict llc parameter queue reproduce perform server architecture cpu core MB llc ddr memory channel 0Gbps NIC parameter representative server intel xeon qualcomm  model poisson rpc arrival assume RPCs model mica  perform hash index lookup average service additional queue model detail available appendix evaluate queue configuration discrete simulation RSS multi queue uniform assignment incoming RPCs core MB buffer VI detail configuration suffers load imbalance memory bandwidth interference RPCValet queue load balance RPCs core buffer provision RSS although RPCValet solves load imbalance suffers bandwidth interference srq RSS multi queue uniform distribution incoming RPCs core assumes ideal buffer management network buffer reuse llc eliminate bandwidth interference nebula combine trait RPCValet srq queue load balance ideal buffer management srq nebula hypothetical configuration employ demonstrate upper performance bound idealize zero overhead buffer management mechanism latency RPCs memory bandwidth utilization model assume slo  buffer bloat RSS RPCValet saturate load exhaust server memory bandwidth generate 7GB traffic although RPCValet attains sub latency saturation magnitude RSS load thanks improve load balance maximum load memory bandwidth bottleneck srq beyond RSS RPCValet meeting slo load srq outperforms RPCValet despite load imbalance network buffer reuse llc eliminate traffic writebacks dirty network buffer effectively network contention memory bandwidth remove despite srq improve performance leaf server maximum throughput  due inability balance load across core srq lose throughput increase proportionally rpc service variance model considers narrow distribution nebula combine RPCValet srq attain optimal latency load saturate becomes cpu bound maximum load nebula consumes server maximum memory bandwidth conclusion model rpc optimize architecture address load balance memory bandwidth interference although load balance extensively address prior architectural operating perspective model demonstrate memory bandwidth contention primary performance determinant principle discrete simulation impact load imbalance memory bandwidth interference latency RPCs address memory bandwidth interference attain performance nebula configuration IV nebula insight latency critical software tier RPCs address memory bandwidth interference salient characteristic network protocol server baseline proceed addition propose nebula architecture finally demonstrate underpin mathematical insight baseline architecture network stack baseline network stack feature hardware terminate user network protocol chip integrate NIC baseline suitability latency critical software feature characteristic emerge production datacenters II nebula prerequisite underlie prerequisite rpc orient transport recent advocate rpc orient transport conventional  transport datacenters enable improve latency aware rout inter server load balance nebula rpc transport expose abstraction rpc transport layer enables NIC decision pertinent application rpc limited packet mere chunk byte prerequisite NIC driven load balance nebula relies synchronization mechanism load balance RPCs server core improve latency slo statically partition rpc queue presence load imbalance server violate application slo due increase application layer queue earlier bottleneck bandwidth interference analysis remainder adopt RPCValet baseline architecture feature integrate NIC delivers queue load balance without incur software overhead feature llc rpc buffer management rpc network transport abstraction prerequisite NIC expose manage network endpoint client instead endpoint per client reduce buffer incoming rpc request analysis IV stable latency sensitive active buffer conveniently accommodate server llc core contribution nebula leverage opportunity implement llc buffer management mechanism NIC eliminate memory bandwidth implication demonstrate slo aware protocol nebula target latency sensitive online service strict latency slo response violate slo limited leverage qualitative characteristic relax hardware requirement server queue queue newly RPCs inevitably violate slo eagerly  inform client increase load server bound queue depth slo constraint account synergistic goal maximize llc residency rpc buffer nebula protocol extension judicious  inform application slo violation policy synergistic exist tolerant software technique eagerly retry reject request predict delayed nebula fail approach slo violation complement technique replace prediction timely feedback efficient packet reassembly connection rpc orient transport prerequisite packet belonging various multi packet RPCs  properly  destination buffer assumption hardware terminate protocol demultiplexing reassembly handle NIC hardware baseline architecture feature chip integrate NIC exacerbates rpc reassembly challenge chip resource dedicate NIC reassembly hardware limited tight constraint exist interface server memory architecture integrate NICs  numa frequency rpc fragmentation reassembly exceptionally although RPCs datacenter prior circumvent reassembly challenge allocate dedicate buffer message per node buffer bloat implication detailed II nebula drastically reduces buffering requirement buffer endpoint employ protocol hardware efficient packet reassembly futuristic network bandwidth NIC core rpc steer handle RPCs latency component chip interaction involve delivery rpc core directly steer incoming rpc network core cache core memory llc noticeably accelerate rpc startup however action timely accurate avoid adverse challenge NIC core rpc steer NIC generally priori core handle incoming rpc inaccurate steer detrimental performance peril potentially steer RPCs directly incoming RPCs core cache thrash DDIO avoids complication conservatively limit network packet steer llc cache hierarchy available opportunity performance improvement successful NIC core rpc steer rpc handle distinct arrival dispatch goal arrival payload placement llc resident queue mitigate memory bandwidth interference goal dispatch transfer rpc llc queue core cache core processing rpc dispatch decision integral load balance mechanism prerequisite steer focus shift core dispatch therefore extend RPCValet load balance mechanism rpc core assignment payload steer defer implementation detail bound server queue insight motivate shallow SRAM resident queue incoming RPCs slo violate RPCs typically server queue conversely server slo abide RPCs primarily spent core queue therefore sufficient allocate memory RPCs queue latency correspond slo violation buffer SRAM resident reduces memory footprint boost performance lower memory bandwidth demand reduce queue memory subsystem application access core faster access rpc reduce rpc startup constraint latency implies maximum amount rpc queue server bound server rpc response average rpc queue service respectively assume slo literature constrains queue simply respect slo hypothetical server processing operating load distribution RPCs conduct queue analysis generalize observation estimate queue capacity server operating slo abide load model core server  load queue  slo USING synthetic service  distribution max load slo depth slo deterministic exponential bimodal queue assume poisson arrival service distribution interested distribution  queue RPCs load arrival rate request per core service rate stability distribution  eqn erlang formula  although  equation load  closer core cpu extreme load ideally analytically  distribution however expression various percentile  therefore queue simulator statistic  service distribution  deterministic exponential bimodal SË† maximum load meeting slo  load corroborate intuition service dispersion grows bimodal peak load slo queue depth increase additionally deterministic distribution  variability rate core drain request queue analysis extremely load RPCs queue easily inside server exist SRAM resource provision deeper queue effectively useless RPCs queue deeper upper bound demonstrate analysis violate slo anyway provision buffer limited server transport protocol signal failure deliver nack request queue client react nack reception request retry server propose expose delivery failure client principle client application equip handle violation inform immediately client perform proactive load monitoring slo violation transport protocol reject request summary meeting strict  shallow server rpc queue designer core cache llc memory NIC  NIC rpc payload buffer rpc arrival enqueue rpc core available rpc dispatch network baseline NIC rpc payload buffer nebula overview baseline nebula architecture leverage observation provision amount buffering comfortably inside chip cache server eliminate buffer memory footprint latency implication memory bandwidth interference nebula implementation nebula implementation feature outline IV briefly introduce baseline architecture critical feature detail nebula protocol NIC hardware extension baseline architecture numa  baseline architecture  combine lean user  protocol chip NIC integration NIC leverage integration local cpu coherence domain rapid interaction core application schedule  operation RDMA memorymapped queue QP interface RPCValet NIC extension rpc load balance enabler nebula rpc core steer mechanism RPCValet balance incoming RPCs across core multi core server queue  fashion demonstration operation RPCValet   architecture NIC comprises discrete entity frontend FE backend former handle QP interaction collocate per core latter handle network packet data transfer network memory hierarchy chip rpc arrives NIC writes payload llc creates rpc arrival notification contains pointer rpc payload dedicate queue core becomes available rpc correspond NIC FE notifies NIC dequeues entry arrival notification queue writes core completion queue CQ core receives rpc arrival notification polling CQ pointer notification message rpc payload llc greedy load assignment policy corresponds queue behavior payload ptr rpc msg domain rpc response buffer buffer ptr buffer rpc resp buffer ptr buffer target node msg domain pseudocode rpc handle loop highlight nebula extension baseline architecture feature marked detailed nebula llc network buffer management reduce memory pressure feature NIC core rpc steer extends baseline architecture sequence rpc arrival notification action payload dispatch detailed rpc protocol software interface implement rpc layer  operation maintain RPCValet message interface node service message domain buffer data structure define incoming RPCs memory node participate message domain allocates buffer memory payload incoming RPCs RPCValet underlie connection orient buffer management achieve nebula goal handle traffic within server cache nebula buffer allocate initialize software manage NIC focus software interface detail hardware modification later demonstrates function nebula rpc interface expose application within sample rpc handle loop server thread incoming RPCs rpc function nonblocking NIC sends rpc arrival thread via thread CQ associate incoming rpc message domain rpc application invokes buffer function NIC reclaim buffer finally application sends response rpc specify message domain target node local memory location contains outgo message rpc return outgo rpc accepted remote client request  server server return client provision sufficient buffering response outstanding request  acknowledges message transport layer nebula extends mechanism negative acknowledgement  response operation cannot accommodate incoming rpc response nack reception application layer receives error code appropriate reaction error code application specific retry later arbitrarily sophisticated policy  tid recv buf address NIC llc rpc bitvector   tid pkt idx buffer address buf NIC rpc reassembly buffer management NIC extension buffer management nebula NIC manages buffer per establish message domain software register buffer NIC NIC dynamically allocate incoming RPCs incoming packet appropriately buffer manager role reserve buffer rpc ensure core zero access buffer application explicitly permit NIC reclaim rpc service allocator rate simplicity paramount NIC therefore chunk allocation algorithm manages buffer array consecutive slot host memory allocator style perform allocation return available slot buffer manager bitvector built NIC cache slot buffer correspond allocate slot shade upon rpc arrival NIC advance buffer pointer RP slot correspond bitvector arrival rpc trigger pointer advance slot rpc cache packet entry  described application slot buffer message specify buffer address NIC reset bitvector correspond buffer message buffer manager pointer advance reclaim buffer cannot advance slot immediately allocate pointer pointer buffer NIC  RPCs slot freed application naive implementation suffer internal fragmentation excess  rare latency core receives  handle rpc therefore implement scrub policy buffer manager scan bitvector operation trigger pointer perform critical allocation buffer bitvector rely analysis IV provision queue RPCs conveniently depth bimodal distribution factor rpc buffer avg RP per IV assume core server load average rpc KB provision KB buffer choice KB bitvector modest SRAM server chip NIC extension rpc reassembly incoming RPCs exceed network MTU fragment transport layer reassemble application  protocol reassembly perform NIC hardware specific challenge reassembly hardware nebula baseline architecture MTU NIC chip component former implies reassembly throughput requirement latter implies tight budget emerge transport protocol RPCs packet message unique identifier tid assign originate node incoming packet combination tid source node  uniquely identifies message packet belongs reassembly operation described perform  tid incoming packet header SRAM resident database RPCs currently reassemble return assume packet rpc previously NIC packet header contains  buffer address return packet rpc pkt idx NIC writes payload address throughput CAMs hungry due discharge cycle contrary initial expectation deploy cam feasible nebula decision bound queue depth incoming RPCs shrink buffer provision requirement upper limit incoming RPCs reassembly consequently nebula upper limit cam rpc reassembly purpose core configuration cam entry model hardware nebula cam CACTI configure parameter built  HP device projection dynamic optimization constraint meeting 2GHz cycle target futuristic tbps network endpoint packet arrival report dynamic cam reasonably accommodate chip NIC core rpc steer RPCValet mechanism assign RPCs core involves metadata NIC pointer buffer network packet within facebook datacenters KB II PARAMETERS USED cycle accurate simulation core cortex 2GHz OoO tso dispatch retirement entry rob cache KB KB MSHRs cycle latency tag data llc interleave NUCA MB tile cycle latency coherence directory non inclusive MESI memory latency  ddr interconnect 2D mesh link cycle hop rpc payload CQ thread service rpc nebula extends mechanism trigger dispatch rpc payload target core cache payload dispatch completes timely fashion reduces rpc execution accuracy rpc payload prefetching probabilistic  prediction hardware leverage semantic information rpc arrival assignment core  cache hierarchy faster rpc startup nebula NIC core steer mechanism implement sequence additional RPCValet normal operation modify RPCValet rpc arrival notification addition pointer rpc payload notification payload rpc assign core processing NIC FE payload buffer address notification message core cache controller prefetch hint cache controller information prefetch rpc payload core processing rpc discus alternative mechanism prefetch hint NIC core rpc steer IX guarantee timely rpc payload prefetches configure RPCValet RPCs instead queue NIC FE cache controller ample prefetch rpc payload core slight deviation queue rpc balance corresponds  policy propose  preserve latency chip VI methodology organization simulate armv server ubuntu linux cycle detail enhance qemu timing model  simulator II summarizes simulation parameter server implement   architecture llc dram NIC parameter tune memory provision 8GB dram bandwidth 0Gbps NIC MB llc cpu core simulation practical core cpu maintain llc dram bandwidth per core provision 1GB dram bandwidth MB llc  NIC bandwidth indicates percentile latency memory bandwidth latency bandwidth evaluate query mixture provision 0Gbps NIC however bandwidth intensive workload nebula saturate 0Gbps idle cpu cycle therefore increase NIC bandwidth 0Gbps permit nebula cpu core saturation application software mica memory modification networking layer  compatibility simulator optimize mica armv deploy thread mica instance MB dataset comprise default mica hash bucket circular 4GB datasets reduce llc ratio exacerbate memory bandwidth interference nebula alleviates load generator simulator generates client request configurable rate poisson arrival uniform data access popularity query mixture unless explicitly mention query evaluate configuration evaluate dissect nebula benefit RPCValet RPCValet baseline architecture feature NIC driven queue load balance optimize latency RPCValet provision packet buffer static connection orient fashion buffer bloat connection assume cluster server maximum per message outstanding message per node RPCValet buffer consume MB significantly exceed server llc provision connection allocates buffer per endpoint RDMA optimize udp rpc RSS representative RSS mechanism available NICs implementation optimistically request core uniformly RPCValet RSS suffers buffer bloat suffers load imbalance multi queue NEBULAbase configuration retains RPCValet  capability nebula protocol hardware extension efficient buffer management analysis IV NEBULAbase allocates KB buffering incoming RPCs correspond slot  proxy infiniband queue srq mechanism enable buffer endpoint core tackle buffer bloat optimistically assume hardware buffer manager NEBULAbase without software overhead normally involve thread buffer srq unlike NEBULAbase exist srq implementation feature hardware load balance hence  RSS without buffer bloat equivalently NEBULAbase without hardware queue load balance nebula propose feature namely NEBULAbase plus NIC core rpc steer evaluation metric evaluate nebula benefit throughput slo slo latency target average rpc service measurement server rpc latency measurement NIC buffer freed core request nebula nack incoming RPCs load conservatively  latency measurement vii evaluation impact load imbalance bandwidth interference evaluate VI quantify impact load imbalance memory bandwidth interference latency memory bandwidth function load series RSS RPCValet  NEBULAbase effectively identical RSS performs suffers load imbalance bandwidth contention although RPCValet delivers latency RSS MRPS sensitivity analysis query mixture RPCValet MRPS RPCValet BW slo nebula MRPS nebula BW slo GB GB GB GB GB GB due superior load balance saturate beyond MRPS shed source performance gap  suffer buffer bloat RSS RPCValet utilize 3GB memory bandwidth MRPS server maximum 1GB contrast  consumes bandwidth MRPS therefore delivers load RSS RPCValet corroborate memory bandwidth contention negatively impact latency  performance combine difference RPCValet RSS load balance unimportant however demonstrate bandwidth bottleneck remove load balance performance impact NEBULAbase avoids destructive bandwidth contention load imbalance attain throughput slo MRPS sub latency zero load service mica RPCs payload corresponds throughput bound MRPS NEBULAbase within theoretical maximum saturation NEBULAbase minimal KB buffer adequate nack generation concurs theoretical analysis buffer slot suffice server operating strict slo beyond MRPS quickly becomes unstable server queue rapidly  escalates non issue operating server beyond saturation realistic deployment scenario overall NEBULAbase outperforms  throughput slo latency load MRPS due improve load balance sensitivity workload ratio item sensitivity NEBULAbase RPCValet workload behavior query report maximum throughput slo memory bandwidth consumption peak throughput query largely insensitive query mixture RPCValet saturation workload remain bottleneck memory bandwidth contention GETS increase NIC generate bandwidth payload  despite NIC generate bandwidth core aggregate bandwidth  performance RPCValet nebula mica query data message response ultimately memory bandwidth usage per query remains roughly constant contrast NEBULAbase throughput increase service improvement happens data direction core load incoming NIC chip cache llc payload buffer resides NIC writes dram resident mica payload buffer already chip core lsq payload data remote cache contrast GETS fetch mica response buffer GETS core longer evaluate impact mica item maximum throughput slo memory bandwidth RPCValet NEBULAbase item item cache memory bandwidth utilization item item shrink RPCValet throughput slo  increase MRPS item MRPS item item naturally memory bandwidth generate NIC cpu core alleviate memory bandwidth contention item RPCValet becomes bandwidth bound cap throughput slo MRPS NEBULAbase performance improves item core burden copying data mica reduce rpc service item respectively shorter service saturation MRPS item RPCValet bandwidth contention factor NEBULAbase eliminates costly allocate payload llc rpc dispatch core finally emphasize NEBULAbase attains throughput slo RPCValet handle item NIC core steer finally evaluate nebula NIC core steer mechanism NEBULAbase prefetching nebula NIC core steer enable  ref hardware configuration NEBULAbase contains modification rpc handle loop prefetch future RPCs CQ software prefetch overhead critical optimize prefetch logic scan maximum slot cache CQ rpc prefetch overhead code MRPS configuration perform identically core rpc outstanding software overhead scan CQ rpc arrival MRPS software overhead manifest increase latency NEBULAbase MRPS mica service conclude prefetching load hardware eliminate overhead prefetch address software expectation nebula trim cycle rpc runtime hiding latency fetch remotely cached rpc payload access mica hash index improvement average core rpc CQ without rpc prefetch MRPS nebula improves latency reduction rpc service attribute roughly difference expectation increase cache subsystem latency load therefore benefit remove longer rpc payload critical via timely prefetching increase nebula outperforms NEBULAbase MRPS load RPCs dispatch core CQ depth grows nebula service reduction payload reduce core service  ref overhead grows service therefore nebula delivers latency MRPS nebula delivers MRPS throughput benefit payload employ feature nebula improves throughput slo multi queue srq queue RPCValet respectively RPCs nebula saturates maximum cpu throughput significant performance improvement headroom discussion deployment benefitting nebula datacenter service increasingly adopt microservices architecture overall functionality broken modular component user request across server layer comprise service grows importance minimize latency inter layer rpc interaction typical pervasive decomposition across datacenter deployed service broader nebula applicability evaluate comparison nebula prefetching policy mica query  representative software extensively optimize application virtually service exhibit service application benefit nebula characteristic addition strict latency constraint intensive network traffic combine application memory bandwidth usage approach memory bandwidth core service former nebula remove bandwidth interference inflict limit RPCs queue server ensure cache network packet residency latter nebula convert per packet dram access cache evaluation boost throughput slo sub RPCs core server identify application exhibit characteristic important building datacenter service machine replication SMR fault tolerance elect leader server replicate incoming RPCs across follower  SMR leader criterion benefitting nebula service benefit nebula NIC core steer furthermore replicate incoming RPCs fashion network packet buffer spill dram memory bandwidth interference therefore benefit nebula cache buffer management network function virtualization NFV workload execute user define packet processing operation inside software switch  NFV operation exhibit service llc resident datasets therefore NFV benefit nebula NIC core steer finally nebula benefit application deployment latency critical application llc memory bandwidth intensive nebula constrain llc latency critical application packet buffer improve llc ratio network packet indirectly benefitting  workload limit llc interference integration datacenter network nebula feature underlie network stack native protocol rpc orient transport hardware terminate transport lossless link layer latter requirement address recent target datacenter RDMA deployment native rpc orient transport active research direction nebula compliant protocol addition nack message receiver upon encounter server queue rpc layer return sender argue message already exist performance networking RDMA NIC generates local CQ entry remote memory perform IX related leaky dma  encounter leaky dma bandwidth interference  author deploy NFV workload dpdk llc DDIO exceeds default limit memory traffic multiplies   memory traffic statically limit dpdk buffer allocation llc contrast establishes mathematical bound buffer queue theory intuition limit outstanding buffer reduce excessive memory traffic nebula target hardware terminate transport demonstrates mere KBs buffering sufficient handle RPCs nebula maintains inter core load balance factor beyond  scope leaky dma context  runtime scheduler latency sensitive datacenter workload   zero buffer NIC increase llc reuse DDIO contrast nebula maintains zero useful packet buffer accommodate llc operating tight slo RPCs transport herd   propose optimization alleviate scalability issue infiniband transport  solely unconnected datagram transport reduce queue cached NIC  accepts inherent performance loss connection storm rack node generation ConnectX NICs significantly improve scalability hence transaction api prioritize RDMA operation transport target datacenter deployment applies software RPCs  improves buffer scalability multi packet queue descriptor introduce mellanox ConnectX NICs descriptor NIC resident constant node however  server buffering connection allocates memory thread although server memory plentiful RPCs buffer llc resident avoid memory bandwidth interference tight  compute hardware NIC importance  datacenters multiple proposal  hardware  leverage coherent chip NIC integration introduce NIC extension atomic remote access alleviate software overhead  RDMA latency mica performance characteristic propose bespoke cpu  parameter adopt baseline observes  llc buffer management empirically llc simulation minimize rate dram bandwidth contention amount network communicate server due udp achieve throughput magnitude latency attempt hardware terminate network stack brings challenge dedicate per endpoint nebula therefore hardware terminate protocol demonstrates packet buffering slo chip latency implication via RDMA network program model propose NIC decomposition passing message NIC component avoid multi hop coherence interaction insight regard chip data rpc payload  propose NIC role rpc dispatch accurately prefetch payload without cache coherence modification latency optimize software latency designer aggressively limit queue transport rpc protocol stack limit server queue critical RPCs tcp limit request per connection application slo nebula performs buffer management hardware software terminate protocol  improves rpc latency modify DDIO steer header network packet llc tile closest core packet steer packet core cache network data cache avoid due pollution concern nebula address steer rpc 0Gbps packet  dram latency application perform memory access inevitably backpressure NIC packet propose  slab allocator exist cache partition technology guarantee application dataset llc   therefore benefit application llc resident datasets whereas nebula considers  application memory resident datasets NIC core data steer nebula employ prefetch hint mechanism steer payload incoming rpc target core payload prefetches regular cache access trigger demand access initiate cache chooses prefetch hint payload prefetches transform regular initiate operation cache controller coherence protocol unmodified intel DCA mechanism leverage prefetch hint load data llc mechanism injection DDIO allocate update policy curious cache propose initiate operation nebula trivially adapt mechanism cache controller conclusion  datacenters combination network bandwidth software layer shift performance bottleneck server endpoint vast bandwidth future NICs interfere application traffic server memory controller due network buffer bloat non scalable  buffer provision however software tight latency constraint queue theory indicates limited server queue useful therefore network buffer provision shrunk trivially accommodate server llc insight propose nebula network protocol server hardware actively network buffer  mitigate memory bandwidth interference nebula alleviates bottleneck deployment  software layer hasten adoption latency optimize hardware terminate network stack nebula demonstrates nascent rpc orient transport protocol powerful enablers future NIC architecture actively cooperate network intensive application achieve performance goal nebula NIC integral role load balance minimize latency actively trigger prefetches cache hierarchy RPCs processing without cache coherence protocol evaluation capability delivers performance benefit keyvalue application packet directly cache llc overall nebula improves throughput slo RPCs