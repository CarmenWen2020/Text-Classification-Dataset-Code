sort computational kernel data application sort implementation focus specific input width hardware configuration array sorter optimize narrow application domain merge implement FPGAs performance introduce novel merge architecture develop bonsai adaptive sort consideration chip memory bandwidth amount chip resource optimize sort fpga programmability allows leverage bonsai quickly implement optimal merge configuration memory hierarchy bonsai develop sorter specifically target dram sort aws EC instance GB array implementation minimum speedup CPUs FPGAs gpus respectively exhibit bandwidth efficiency previous sort implementation finally demonstrate bonsai tune megabyte terabyte memory hierarchy ddr DRAMs highbandwidth memory  solid disk SSDs index merge sort performance model memory hierarchy fpga introduction fpga accelerator data application data analytics genomic analysis compression machine focus sort importance data application mapreduce mapping stage sort prior fed reduce stage throughput sort procedure limit throughput mapreduce sort relational database sort merge algorithm focus research sort computational kernel data processing hive spark sql reduce merge implement relational function spark mapreduce sort bottleneck relational operation CPUs convenient purpose sort platform however implementation cpu inferior gpus FPGAs  indicates author contribution cpu sorter GB input MB additionally cpu architecture specialized data sort usually performance cpu efficient gathering portion regard gpu sort hybrid radix sort HRS performance array GB report throughput GB GB array gpu sorter perform GB entire array gpu memory GB HRS sort GB data cpu merge subset input data sort gpu nonetheless cpu merge dominates computation array GB array gpu sorter majority compute cpu gpu sorter focus exclusively amenable gpu parallel instruction workload wider restriction unacceptable perform fpga sorter  terabyte sort  sort GB data GB offering fpga accelerate sort performance however  relies cpu sample bucketing limit scalability indeed array GB performance terabyte sort implement merge fpga sort sort TB data however analysis optimization opportunity perform sort task IV fpga sorter focus exclusively array chip memory MB vii algorithm hardware platform none perform across sort outside preset input dash report implies data engineer procure multiple hardware platform familiarize sort approach achieve performance across almost sorter width acm annual international symposium computer architecture isca doi isca sort per GB sorter cpu gpu fpga across bonsai performance distribute sorter server node dash report GB GB GB GB GB GB GB TB TB cpu  cpu distribute gpu HRS gpu distribute fpga  fpga  bonsai overcome bonsai merge adaptive sort tune merge architecture sort array megabyte terabyte cpu fpga server node considers scalability sort kernel respect available computational resource memory memory bandwidth width bonsai analytical performance resource model configure adaptive merge architecture combination available hardware furthermore approach computer architect understand performance benefit future compute memory technology improvement integrate merge sorter explore unique reconfigurability fpga allows bonsai adapt sort kernel sort demand within millisecond width latency throughput offs available memory compute hardware optimize merge configuration merge sort algorithm asymptotically optimal operation predictable sequential memory access ideal memory burst coalesce furthermore due asymptotically optimal complexity merge sort generally regard prefer technique sort amount data within computational node alternatively bucketing algorithm sample radix sort effectively data distribute sorter useful ratio processor relatively comparison implementation exhibit  performance terabyte distribute sort contribution novel adaptive merge amt architecture allows independent optimization merge throughput concurrently merge array II architecture optimally adapt available chip resource chip memory bandwidth introduce feature performance merge develop bonsai adaptive sort comprehensive model tune amt configuration reduce sort explain model optimize memory dram highbandwidth memory HBM flash memory IV bonsai develop sort dram sort aws minimum speedup sorter CPUs FPGAs gpus respectively exhibit bandwidth efficiency previous sort implementation demonstrate versatility approach bonsai ssd sorter project performance node sorter per node latency distribute terabyte sort implementation hardware merger hardware merger building merge merger hardware merger merge sort input rate per cycle merger tuples input output tuple cycle combination merger amt although merger output per cycle merger pipeline  merger  merger fully pipelined network merges  sort array per cycle network exchange operation execute parallel  merger merges latency logic logic merger asymptotically dominate  merger logic utilization merger merger consume chip memory register bonsai amt architecture underlie merger building limited specific merger implementation formulation focus attention optimize sort fpga input megabyte terabyte data nonetheless building distribute sort II bonsai input parameter definition array width byte array parameter definition βDRAM bandwidth chip memory bandwidth bus  chip memory capacity byte  chip memory capacity byte  chip logic batch byte hardware parameter definition merger frequency logic utilization merger logic utilization coupler merger architecture parameter bonsai goal minimize sort optimize choice adaptive merge AMTs available hardware merger architecture input II AMTs construct merger coupler detail II merger coupler component optimize bonsai resource utilization frequency merger coupler treat input parameter model merger coupler various AMTs II explain AMTs configure minimize sort introduce resource utilization model understand amt configuration implementable available hardware finally bonsai optimal amt configuration chip memory synthesize respective configuration fpga IV report performance dram sorter experimentally validate bonsai prediction VI II amt architecture framework adaptive merge AMTs construct merger binary AMTs uniquely coupler data loader architecture amt throughput leaf merger respectively dram fpga illustration amt configuration associate data movement amt fin throughput leaf throughput amt merge output per cycle leaf amt denote sort array amt concurrently merges important determines recursive data amt amt architecture allows combination implement sufficient chip resource implement amt merger amt merger merger etc binary merge array node merger merger amt throughput leaf merger merger merger grandchild merger grandchild output merger  coupler concatenate adjacent tuples tuples suitable input merger merge sort recursively merge array AMTs array load onto dram via bus pci host ssd ethernet another fpga host data amt merges input sort subsequence recursively entire input merge sort array recursive merge stage data sort output via bus stage amt merges unsorted input data dram output sort subsequence onto dram stage sort subsequence load amt merges sort subsequence output stage sort subsequence stage sort subsequence therefore merge stage sort array recognize merge array increase reduces merge stage sort array thereby reduce sort amt throughput increase reduces execution stage amt configuration parameter definition output per cycle merge input array merge λunrl unrolled merge λpipe pipelined merge increase additional limited chip resource bonsai model choice optimal described IV amt architecture configure width without resource utilization overhead performance degradation wider implement serial comparators merger chip memory peak bandwidth writes batch KB chunk data loader implement batch writes thereby abstract chip memory access amt data loader consume considerable amount chip memory pre fetch batch chip nonetheless allows utilize bandwidth offchip memory microarchitecture detail amt architecture extension performance model introduce amt configuration explain configuration impact performance resource utilization bonsai optimizer optimal amt configuration parameter input parameter II bonsai introduce amt configuration amt configuration summarize define specify amt throughput amt leaf amount amt unroll λunrl amount amt pipelining λpipe amt uniquely define throughput leaf denote amt implementation AMTs within configuration λunrl unrolled configuration λunrl AMTs implement independently parallel conversely λpipe pipelined configuration implies λpipe AMTs sequence output amt input amt unroll pipelining replicate  configuration λunrl optimize amt configuration model performance amt II merge stage sort array amount stage depends throughput amt pfr chip memory bandwidth HBM mux fpga data movement unrolled configuration merge denote βDRAM stage min pfr βDRAM sort amount stage latency min pfr βDRAM model prediction experimental increase beneficial increase amt throughput dram bandwidth amt unroll sort improve employ multiple AMTs independently useful chip memory bandwidth increase throughput demand multiple AMTs λunrl AMTs sort sequence partition data λunrl disjoint subset non overlap amt subset independently partition pipelined merge stage impact sort ensure merge amt approximately AMTs within configuration chosen amt sort subset independently sort unrolled configuration amt sort λunrl assume bottleneck importantly chip memory bandwidth available amt longer βDRAM βDRAM λunrl unrolled AMTs available memory bandwidth λunrl unrolled configuration latency λunrl min pfr βDRAM λunrl partition data λunrl non overlap subset interconnect issue λunrl another approach forgo partition amt sort pre define address amt sort address rely merge sort subset AMTs configuration approach prefer λunrl incurs performance penalty merge stage cannot available AMTs amt pipelining assume dram bandwidth βDRAM multitude bandwidth denote data ssd array bus sort kernel throughput comparison non overlap address partition future dram fpga data movement pipelined configuration kernel sort array return ssd bus λunrl unrolled AMTs sort input array parallel bus idle sort procedure bandwidth scarce resource bus idle therefore introduce amt pipelining configures AMTs data bus constant rate pipeline multiple AMTs merge stage sort procedure execute amt amt executes stage input array concretely array bus amt pipeline merge sort subsequence initial stage array via dram amt performs merge stage concurrently array fed amt pipeline stage completes array fed amt array independently merge AMTs pipeline respectively pipelined approach ensures constant throughput sort data bus amt pipelining useful multiple array sort amt pipelining phase ssd sorter input data sort  subsequence IV specifically pipelining λpipe lower execution phase ssd sorter similarly unroll pipelining available dram bandwidth AMTs pipeline λpipe AMTs bandwidth pipeline limited βDRAM λpipe throughput pipeline limited bandwidth throughput AMTs pipeline pfr throughput λpipe pipeline throughput min pfr βDRAM λpipe sort latency λpipe min pfr βDRAM λpipe contrast unroll amount data amt pipeline sort limited factor amt pipeline intermediate output onto dram specifically λpipe pipelined configuration array sort without spill data dram  λpipe λpipe pipelined amt configuration array λpipe merge stage data cannot backwards pipeline maximum amount data pipeline sort λpipe constraint mitigate pre sort subsequence input data initial merge stage summary λpipe pipelined configuration sort min  λpipe λpipe combine pipelining unroll unroll pipelining configuration replicate λpipe pipelined configuration λunrl combine equation sort λpipe pipelined λunrl unrolled configuration latency λpipe min pfr βDRAM  throughput λunrl min pfr βDRAM  resource utilization bonsai amt configuration implement chip develop model logic chip memory utilization discus resource utilization amt AMTs configuration resource utilization configuration exactly amt logic utilization AMTs merger coupler approximate lut utilization amt lut utilization merger coupler amt lut utilization amt lut LUTs coupler merger respectively summand corresponds lut utilization depth model predicts lut utilization AMTs within report vivado synthesis AMTs synthesize AMTs ensure amt synthesize chip lut   LUTs available fpga sort optimal amt configuration chip memory bandwidth sorter cpu  gpu HRS fpga  entire data memory bound GB input chip memory utilization data loader tasked input data dram KB sequential batch batching dram peak bandwidth input leaf amt dram leaf input buffer batch amt synthesizable chip ensure input buffer chip memory  batch  amount chip memory fpga  amount chip bram bonsai amt optimizer performance resource model define bonsai bonsai optimization strategy exhaustively prune amt configuration onchip resource minimal sort latency optimal maximal throughput  specifically bonsai output optimal amt configuration array hardware merger architecture parameter II formally bonsai latency optimization model argmin λunrl λunrl min βDRAM λunrl pfr λunrl lut  λunrl  pipelining latency optimization model improve sort however pipelining optimize sort throughput array sort optimize throughput optimize latency sort array notably optimize throughput phase ssd sorter data sort dram subsequence detail IV optimize throughput bonsai argmax λunrl λpipe λunrl min βDRAM  pfr  lut    min   λpipe bonsai amt configuration optimally utilize chip memory bandwidth predict sort bonsai fpga function available dram bandwidth along previously perform cpu fpga gpu sorter importantly bonsai implementable amt configuration decrease performance therefore optimal impossible synthesize due constraint anticipate model optimal configuration IV optimal configuration hardware optimal amt configuration fpga vastly chip memory ddr dram GB bandwidth GB capacity HBM GB bandwidth GB capacity ssd GB bandwidth GB TB capacity dram sort concrete bonsai construct sorter aws EC xlarge instance fpga GB ddr dram GB concurrent latency optimize configuration setup amt throughput amt exactly GB mhz configuration peak bandwidth dram leaf implement fpga cannot data loader chip memory equation data load host onto dram pci data onto fpga amt merges data writes merge subsequence onto dram throughput merge GB subsequence merge array finally sort data pci dram roughly 9GB dram fpga ssd flash fpga optimal terabyte configuration data fpga implement phase fpga implement phase phase implement fpga via reprogramming experimental dram sorter sort GB array minimum sort cpu fpga gpu sorter respectively detail VI bandwidth memory sort recently intel xilinx announce release highbandwidth memory HBM FPGAs achieve GB bandwidth capacity GB memory bandwidth optimal amt configuration  sort integer HBM dram model decides optimal configuration λunrl amt data onto sort kernel amt independently merges subsequence input data λunrl partition data non overlap interval AMTs instead amt sort predefined address amt sort sort subsequence meaning sufficient input AMTs stage therefore AMTs idle remain AMTs merge stage AMTs idle remain AMTs perform merge stage entire array eventually merge therefore merge stage AMTs utilize memory bandwidth GB experimentally HBM prediction VI ssd sort analysis expand multi tier offchip memory hierarchy applicable beyond capacity dram discus bonsai optimize sort fpga GB dram GB concurrent bandwidth TB ssd GB bandwidth insight hierarchy sort procedure distinct phase phase amt configuration phase data ssd dram data dram aim sort data onto dram data ssd entire input phase ssd dram sort subsequence phase sort subsequence merge stage crucial stage phase minimize merge stage ssd limited throughput relatively ssd bandwidth phase entire input data completes exactly ssd dram minimize execution phase throughput optimal bonsai optimization throughput optimal configuration aws instance sort GB array pipeline contains amt dram memory fpga dram fpga simultaneously peak rate GB amt saturates bandwidth capacity bandwidth GB implies data amt pipeline saturate bandwidth throughput pipeline min λpipe pfr GB amount data sort pipeline GB assume pre sort input data subsequence equation phase ssd sort ssd effectively chip memory stage phase ssd minimize latency directly reconfiguring fpga latencyoptimized amt configuration latency optimize ssd chip memory consists amt amt peak ssd bandwidth relatively GB conversely amt leaf minimize stage merges therefore amt configuration effectively mitigates impact ssd bandwidth reduce amount memory access optimal amt configuration ssd sort ssd aws refer phase  configuration GB array phase reprogram fpga latency optimize configuration phase phase GB output GB sort subsequence phase merges subsequence concurrently merge GB TB data ssd execute GB sort TB organization merger aws EC instance host configure merge kernel data sort pcie dma channel external dram bandwidth communication sort kernel ddr controller axi interface regardless width  extract FIFOs per cycle automatically width user packer concatenate output merge data role FIFOs zero append zero filter detail data GB server node sorter sort TB TB data merge stage amt sort TB data GB fpga significantly previous scalability sort array increase merge stage ssd sorter latency previous server node terabyte sorter VI microarchitecture consideration microarchitecture propose merge diagram component sort aws EC instance specifically important consideration enable merge efficiently data loader amt configuration data specific leaf amt continuous memory address amt architecture amenable chip memory access data loader performs data ensures offchip memory operating peak bandwidth specific leaf batch initiate data loader leaf input buffer implement fifo dram bus batch data loader robin fashion input buffer batch whenever data loader encounter input buffer sufficient performs batch load buffer data loader maintains pointer address load input buffer input buffer becomes empty amt automatically stall data loader buffer data input buffer become empty unless pause data loader ensure amt behaves correctly empty input buffer due batch sequential writes data loader allows chip memory peak bandwidth verify intermediate flush merge stage amt intermediate flush prepared input another distinct input assume sort array amt leaf stage sort subsequence input amt intermediate flush stage alone sort MB data amt flush entire sort procedure stage sort merge therefore crucial flush scheme efficient address issue reserve terminal propagate datapath signify merger input array fully approach improves exactly terminal adjacent input array terminal propagates amt cycle delay flush merger preparation input zero terminal zero append zero filter zero append append zero terminal whenever entire sort subsequence fed input buffer output merge terminal filter zero filter although reserve zero terminal VI experimental validate model resource performance prediction VI demonstrate performance dram sorter VI validate performance projection HBM sorter VI ssd sorter VI finally discus scalability input width VI sort per GB various AMTs aws instance predict performance model sort per GB various AMTs aws instance predict performance model lut utilization various AMTs resource model prediction experimental setup implement amazon aws xlarge instance virtex ultrascale VUP fpga GB ddr dram GB concurrent bandwidth capacity GB xilinx vivado version synthesis implementation developed verilog mhz frequency benchmark sorter integer generate uniformly random  generate benchmark byte byte byte accordance guideline jim sort benchmark hash byte byte index allows byte byte byte amt sorter generate datasets MB GB model validation resource model resource utilization report synthesis within resource utilization prediction AMTs implement aws EC instance AMTs report lut utilization implement aws fpga performance model validate performance model sort various AMTs input MB GB report sort within predict performance model AMTs throughput amt leaf performance similarly AMTs leaf amt throughput  comparison dram sorter bonsai ofthe cpu  gpu HRS fpga  sort per GB ter performance dram bandwidth saturate dram bandwidth saturate increase throughput decrease sort however increase leaf reduces merge stage reduce sort amt throughput saturate dram bandwidth optimal amt configuration throughput exactly saturate dram bandwidth leaf chip resource permit dram sorter dram sorter latency implement latencyoptimized dram sorter IV consists amt limit leaf frequency due fpga rout congestion GB input IV resource utilization breakdown optimal dram sorter aws component lut flip flop bram data loader merge  available utilization bandwidth efficiency GB input comparison bonsai dram sorter cpu  gpu HRS fpga  latency optimize dram sorter performance previous sorter hardware sort 2GB data implementation sort CPUs FPGAs gpus respectively dram sorter perform input beyond dram capacity GB input GB ssd sorter performance resource utilization breakdown latency optimize dram sorter IV  network  data subsequence merge stage reduces stage execution input IV demonstrates fpga additional resource available leverage future improvement dram bandwidth bottleneck dram sorter bandwidth efficiency formally bandwidth efficiency define ratio throughput sorter available bandwidth chip memory dram sorter phase  sort sort throughput GB dram bandwidth GB bandwidth efficiency dram sorter implementation algorithm bottleneck memory bandwidth bandwidth efficiency important scalability concern domain additionally memory access account consume computer bandwidth efficiency directly related consumption  sorter comparison  implementation knowledge implementation bandwidth efficiency specifically bandwidth efficiency  latency throughput report throughput data execution breakdown sort TB data phase percentage phase reprogramming phase latency per GB latency optimize bonsai sorter across GB TB input along increase latency sorter GB dram label bonsai GB dram bandwidth dram bonsai improvement bandwidth efficiency sorter label bonsai HBM sorter xilinx bandwidth memory tile incorporates ddr memory GB bandwidth ddr dram dram GB bandwidth verify HBM performance projection implement saturate dram resource utilization projection experimentally verify synthesize amt aws instance synthesis AMTs perform mhz predict access HBM verify unroll dram dram bandwidth GB AMTs saturate dram bandwidth amt dram AMTs saturate dram bandwidth amt independently dram demonstrates unroll performance resource utilization linearly unroll amount λunrl ssd sorter validate conclusion IV independently verify throughput phase throttle dram throughput ssd flash GB pipeline phase aws pipeline effectively saturates bandwidth GB predict model verify phase throttle dram GB implement amt operates GB validates performance prediction ssd sort experimentally verify reprogramming fpga phase average summary ssd sorter latency sort TB data previous server node terabyte sorter scalability analysis scalability sort latency per GB input data sorter achieve latency per GB input scalability demonstrate sorter exhibit scalability multiple previous scalability input understand scalability latency per GB increase across input latency per GB bonsai sorter input GB TB latency per GB increase distinct marked arrow latency increase label extra stage happens GB dram sorter data amt additional data extra merge stage sort performance penalty latency increase label switch ssd sorter happens GB input data onto dram data ssd performance penalty latency increase label extra stage phase happens TB extra merge stage phase ssd sorter merge data performance penalty fourth performance penalty happens TB increase latency per GB extra merge stage phase ssd sorter scalability width VI amt building exhibit VI lut utilization throughput building lut merger GB merger GB merger GB merger GB merger GB merger GB lut fifo GB coupler GB coupler GB coupler GB coupler GB coupler GB lut merger GB merger GB merger GB merger GB merger GB merger GB lut fifo GB coupler GB coupler GB coupler GB coupler GB coupler GB comparable resource utilization throughput offering somewhat throughput per lut merger throughput merger almost logic utilization width data shuffle within merger specifically merger throughput merger merger swap operation output per cycle versus per cycle merger output formally logic complexity swap grows linearly width swap within merger grows  GB wider resource sort amount GB narrower vii related fpga sort addition fairly comprehensive analysis sort network FPGAs limit discussion sort MB argue heterogeneous implementation chunk sort fpga later merge cpu report performance advantage cpu input author domain specific automatically generate hardware implementation sort network latency throughput unbalanced  merger approach merge array applicable sort author combination fifo sort sort gigabyte data remove global intra node signal construct however lack implementation focus building sort kernel reporting frequency resource utilization due recent innovation hardware merger memory increase fpga lut capacity analysis become limited extends analysis improves performance throughput merge gpu sort model sort performance gpus model allows researcher predict advance hardware impact relative performance various gpu sorter performance limited global memory bandwidth specifically issue gpu sorter cpu implementation gpu memory multiple cpu ram implies global memory access frequent gpus disk flash access cpu implementation focus building bandwidth efficient radix sort gpu sorter replacement strategy mitigates issue relate pcie bandwidth knowledge strategy gpu reporting sort GB data GB integrate cpu gpu heterogeneous sorter cpu merge sort GB roughly nonetheless approach scalable relies execute merge stage cpu specifically GB cpu computation dominates execution heterogeneous sorter conclusion bonsai comprehensive model sorter optimization strategy adapt sorter available hardware bonsai optimize implement aws fpga yield minimum speedup sorter CPUs FPGAs gpus exhibit bandwidth efficiency previous sort implementation