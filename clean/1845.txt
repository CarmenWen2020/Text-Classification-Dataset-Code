spike neural network SNNs future AI portfolio investment government ibm truenorth intel loihi artificial neural network ann architecture stride target SNN hardware efficiency analysis SNN baseline modest spike rate SNN implementation exhibit significantly efficiency accelerator anns primarily SNN dataflows neuron potential tick introduce data structure dimension reuse introduce novel SNN architecture SpinalFlow compress stamp sort sequence input spike adopts computation output network layer compress stamp sort relevant computation neuron perform consecutive eliminate neuron potential storage overhead data reuse advance efficiency SNN accelerator magnitude temporal aspect SNNs prevents exploitation reuse easily exploit anns input resolution input sparsity SpinalFlow reduces average eyeriss baseline improvement network sparsity resolution SpinalFlow consumes version eyeriss sparsity SNN architecture competitive ann architecture latency inference lower barrier practical deployment scenario demand index accelerator cnns SNNs introduction inspire neuroscience researcher explore potential spike neural network SNNs achieve prediction accuracy various image application spike neuron stateful maintains potential previously input binary input spike potential binary output spike potential threshold spike neuron mimic operation biological neuron emulate brain prediction accuracy however silicon implementation SNNs generally lag silicon implementation anns spite SNN advancement important potential benefit specific application google project SNN sparse temporal code achieve accuracy ann precision SNNs effective useful scenario label training available input deviate training continual establish initial neural network engage resource intensive training approach researcher develop training technique exploit information content relative spike SNNs competitive anns circumstance future potential SNNs echoed commercial project SNN hardware ibm truenorth qualcomm zeroth intel loihi smith  keynote remains develop SNN training architecture theory SNNs naturally exhibit sparsity pack information content ann relative timing sparse spike within modest attempt realize potential SNNs overcome temporal dimension unfortunately SNN architecture achieve throughput per neuron ann accelerator primarily temporal aspect SNNs input across multiple tick constraint architecture dataflows data reuse exploit dataflow manage reuse data structure unique acm annual international symposium computer architecture isca doi isca SNNs neuron potential therefore baseline SNN architecture  model canonical ann accelerator eyeriss processing SNNs rate cod temporal cod traditional imposes significant data movement neuron potential tick exacerbate eyeriss stationary dataflow partial sum neuron potential fully accumulate offload global buffer address introduce accelerator SpinalFlow spike compress chronologically sort manner anns  dataflow approach nontrivial sort overhead address adapt dataflow output stationary model propose dataflow access buffer buffer reduces compute density SpinalFlow relative ann baseline workload drawback alleviate resolution input sparse spike inherent temporally cod SNNs architecture naturally exploit sparsity SNN spike relative baseline ann SpinalFlow simpler processing additional hardware merge sort buffer exploit reuse contribution analysis inefficiency baseline SNN representation spike input output compress stamp sort SNN architecture dataflow tailor input output representation increase reuse neuron potential input spike average improvement resolution sparsity version eyeriss average improvement resolution average latency improvement sparsity eyeriss average improvement sparsity scnn baseline magnitude improvement baseline SNN architecture SNN architecture computation inference ann architecture significantly impact platform SNNs potential achieve accuracy anns II background spike neuron project attempt implement biologically plausible neuron model hardware hardware project implement neuron model highly simplify emulate biologically neuron behavior izhikevich neuron popular neuron model                  input  spike neuron neuron potential incremented input spike leak input spike output spike potential threshold         input image convert input spike fed rate cod SNN SNN integrate model neuron stateful retains membrane potential neuron potential reflect input recent input binary spike spike input synaptic input potential tick leak potential neuron potential eventually specify threshold neuron output spike spike neuron potential reset spike neuron potential hardware efficient input output binary spike spike neuron model multiplier input binary synaptic simply potential spike therefore efficient communication computation feature SNNs later SNNs fail exploit advantage neuron respond spike input image input interval tick pixel input image convert spike extends tick minimum SNN tick neuron evaluates input update potential threshold spike across input interval spike fed input layer neuron prior primarily rate code pixel convert spike tick pixel convert spike input interval temporal code convert input pixel spike specific pixel spike tick pixel spike tick code stochasticity rate code poisson distribution inject spike refer rate cod temporally cod SNNs  SNN respectively biological neuron resolution SNNs resolution SNNs input interval SNNs exhibit sparsity neuron spike input interval spike neuron typically biologically plausible STDP spike timing dependent plasticity unsupervised training neuron adjusts local estimate spike relevance increase accuracy recent resort supervise backpropagation training SNNs recent employ approach SNN achieve accuracy ann SNN inherent resolution sparsity ability correlation input accuracy ann  operand efficiency advantage SNNs evident improve dataflow operand reuse SNN accelerator variety digital analog SNN accelerator described literature ibm truenorth processor prominent digital architecture SNNs truenorth compose tile tile implement neuron input tick tile input spike sends output spike neuron layer within tick tile sequentially neuron tile input spike perform update neuron potential truenorth achieves relatively throughput latency tick parallelism within tile enable comparison anns baseline SNN architecture borrow ann accelerator baseline described magnitude throughput latency truenorth ann accelerator worth contrast spike neuron artificial neuron rely dot calculation retain across consecutive input typically supervise propagation stochastic gradient descent ann accelerator propose recent eyeriss baseline capture innovation implement silicon publicly available detail eyeriss hierarchy global buffer scratchpad register scatter across grid processing PEs  dataflow PEs PE computation kernel input feature exploit reuse partial sum feature kernel adjacent PE computation reuse dataflows feature ann accelerator google tpu ann accelerator leverage sparsity activation multiplier adder eyeriss skip operation operand zero architecture scnn exploit ann sparsity reduce execution understanding  SNN inefficiency define ann SNN baseline ann baseline identify source SNN inefficiency eyeriss architecture ann baseline eyeriss optimization dataflow reuse  widely adopt academic commercial accelerator analysis focus version eyeriss SNN model employ tick input interval temporal code sparsity SNN sparsity resolution advantage inherent SNN artificially introduce accuracy efficiency understand relative merit SNNs resolution ann engages accuracy efficiency operates resolution eyeriss input resolution tick SNN eyeriss grid processing PEs fed input partial sum global buffer PE mac scratchpad input feature ifmap filter partial sum psums output feature ofmap component PE resolution PE elide computation input zero within 2D grid PEs  diagonally filter horizontally psums accumulate vertically refer stationary dataflow SNN baseline SNN baseline spike eyeriss closely eyeriss architecture PE summarize multiplier ifmap scratchpad width partial sum width ann baseline potential threshold neuron output neuron output neuron potential global buffer offchip dram neuron potential retain entire input interval PE eyeriss PE spike eyeriss component eyeriss spike eyeriss PE array alu per PE FxP mac FxP cmp filter scratchpad psum scratchpad ifmap scratchpad global memory KB KB core frequency mhz mhz chip memory HBM HBM parameter ann largely eyeriss baseline SNN PE grid SNN efficient ann multiplier ifmap scratchpad input layer shrunk overall memory requirement SNN neuron potential retain architecture agnostic data encode rate temporal described shortly SNN baseline significantly throughput throughput stateof SNN architecture truenorth analysis employ tick batching PEs tick input interval layer layer tick batching reuse distance membrane potential appropriate tile membrane potential primarily access global buffer meanwhile output spike layer likely global buffer chip dram analyze batching processing layer examine tick conclude tick batching overall data movement evaluation parameter evaluate eyeriss spike eyeriss developed performance model capture latency throughput network model layer specification input output access register scratchpad buffer offchip memory statistic average per operation calculate per layer model analytical equation capture dataflow eyeriss resnet convolutional layer FC layer mobilenet PWC  layer STDP net conv layer network SC layer synthetic SC layer synthetic  layer synthetic  layer synthetic PWC layer synthetic PWC layer synthetic FC layer synthetic FC layer synthetic sparsity resolution II workload sparsity resolution SC standard conv  depth wise separable conv PWC wise separable conv SNN network refer STDP net spike eyeriss layer dimension PE array mapping resource contention ultimately dictate PE utilization performance analysis model zero gate technique eyeriss filter scratchpad ALUs gate zero input activation encounter II summarizes evaluate network input image synthetic layer network understand architecture impact network topology fledge network resnet mobilenet  incorporate varied layer previous research visual cortex organize cascade complex structure cnn implement synthesize processing eyeriss spike eyeriss  technology node model behavior PEs verilog synthesize synopsys compiler innovus backend accurate layout metric delay  account model global buffer CACTI output technology reduce variable attempt memory compression simply assume HBM memory interface methodology calculate average per operation component combine operation component generate overall consumption layer detail accelerator component primary metric roughly per inference latency per inference analysis spike eyeriss estimate efficiency gap baseline SNN ann impact rate cod SNN temporal cod SNN sparsity resolution SNN consume SNN SNN respectively spike eyeriss data normalize ann operating resolution typical activation sparsity activation zero consume per inference SNN  normalize eyeriss refers sparsity respectively consume per inference SNN  normalize eyeriss resolution sparsity spike eyeriss consumes magnitude eyeriss repeatedly update neuron potential fetch multiple across input interval sparse input spike eyeriss eyeriss avoid filter partial sum update SNNs encode non zero input spike consume SNNs difference BSP BSP BSP BSP respectively gap shrink spike activity noteworthy crossover SNN baseline eyeriss SNN SNN happens BSP BSP respectively quantifies sparsity resolution SNN overcome inherent disadvantage manage neuron potential across tick spike summary spike eyeriss baseline peak throughput GOPS truenorth truenorth optimize spike sparsity therefore leakage per neuron peak throughput watt comparable spike eyeriss temporal code sparsity spite throughput advancement relative truenorth gap ann SNN baseline performance gap tick input interval temporal cod clearly superior prior SNN focus rate cod algorithmic advance temporal cod inherent efficiency leveraged focus SNN tick batching however SNN baseline sparsity tick interval consumes per inference ann access neuron potential filter devise technique shrink gap IV PROPOSED SNN architecture  analysis drawback baseline SNN iteratively tick input interval access memory hierarchy update neuron potential filter  dataflow eyeriss highly efficient anns devise dataflow caters temporal aspect reuse exhibit SNNs refer architecture SpinalFlow spine conv layer terminology summary description discus operation conv layer layer  dimension kernel convolve entire ifmap ofmaps kernel apply receptive grid ifmap neuron ofmap darker ofmap likewise kernel apply grid input neuron ofmaps generate refer neuron spine discussion assume computation spine ofmaps shift input receptive spine hardware organization array PEs PE accumulator register adder comparator PE responsible neuron ofmap spine PEs responsible spine ofmaps PE neuron ofmap PE neuron ofmap baseline PEs fed global buffer kernel PEs output stationary dataflow PE dedicate neuron ofmap till input cycle PEs spike input receptive input interval input buffer input spike input spike chronologically sort input spike tick input spike tick input spike tick receptive neuron receptive spike input interval temporal code input buffer entry PEs global buffer therefore demand wider bus baseline later factor evaluation                                cycle cycle spine ofmaps spine cycle cycle examine entry input buffer spike input tick PEs neuron potential incremented correspond kernel therefore global buffer correspond entry kernel PE receives neuron potential neuron potential threshold PE exceed threshold tick spike output buffer spike PE idle input interval neuron spike input interval                                cycle receptive entry cycle cycle spike input buffer happens input spike tick correspond input global buffer fed PEs another neuron potential increment perform PEs PE spike idle output buffer append spike tick spike output buffer naturally sort spine ofmaps chronologically sort spike cycle spike input buffer neuron previous layer spike input interval actual processing variable cycle evaluation assume exploitation activation sparsity future output buffer contains chronologically sort output spike correspond spine ofmaps spine entry neuron spike input interval spine global buffer later input convolutional layer PEs responsible compute spine ofmaps PEs reset neuron potential zero shift receptive input buffer sort spike within receptive previous layer sort spine ofmaps sort ifmap spine layer sort receptive ifmap spine global buffer ifmap spine buffer pre SpinalFlow architecture SpinalFlow PE detail pseudocode SpinalFlow dataflow                                     sort entry spine merge sort sort entry input receptive ifmap spine buffer merge sort replace conceptual sort input buffer earlier merge sort simply comparators cycle entry ifmap spine buffer spine entry offset access initiate cycle ifmap spine buffer overhead convolution cycle computation summary spine orient output stationary dataflow propose SpinalFlow architecture longer suffers drawback baseline temporal code sequentially timestamped spike exactly computation ann baseline longer penalize multi tick input interval compact sort stamp spike trivial spike previous layer architecture yield speedup activation sparsity zero performance ann invasive output stationary dataflow neuron mapped PE entire input interval PE initializes neuron potential accumulator zero increment spike spike threshold discard neuron potential neuron eliminate storage data movement neuron potential dataflow focus maximize neuron potential reuse parallelism across spine sequentially tick eyeriss optimizes combination reuse input kernel partial sum hardware detail provision SpinalFlow resource eyeriss sub optimal therefore resource dataflow PEs feature per layer convolutional network multiple overall architecture SpinalFlow detail PE pseudo code dataflow PE simpler PE eyeriss longer processing entire stationary dataflow eyeriss PE scratchpad scratchpad occupy core eyeriss significant SpinalFlow global buffer earlier split filter buffer input buffer eyeriss retains scratchpad SpinalFlow retains filter buffer buffer receptive filter accommodate convolution workload employ KB filter buffer organize width byte byte PEs cycle filter buffer output bus width layer execute assignment feature configure input KB input buffer neuron spike multiple spine component eyeriss SpinalFlow PEs alu PE mac cmp  scratchpad psum  spad ifmap scratchpad global buffer KB KB GLB bus width psum   ifmap spike core frequency mhz mhz dram GB sec GB sec PEs min inp buff GLB FB architecture specification SpinalFlow eyeriss FB filter buffer GLB global buffer bandwidth spine input receptive fed min finder circuit comparators identifies chronological spike neuron filter buffer PE array output marshal output queue eventually chip memory evaluate processing min finder adopt synthesis spice methodology described model input filter SRAM buffer CACTI estimate summarize exotic SNN feature truenorth leak stochasticity various operation mode future PEs account chip SpinalFlow seamlessly handle sparsity important feature SNNs neuron spike consume resource bandwidth network network input spike neuron layer PEs utilized uncommon workload network computation decompose output feature filter buffer accommodate filter convolutional layer filter buffer load reuse completely correspond output feature demand fully network typically input receptive spike receptive chronologically sort beforehand potentially multiple hierarchical  circuit handle entry spine sort reuse output neuron PEs output neuron entire input spike output neuron output neuron accelerator eyeriss tpu fully layer exhibit reuse typically limited memory bandwidth fetch input spike fetch memory fed PEs discard improve reuse PE utilization batching image SNN image batching effective layer retain chip image batch fetch correspond input spike evaluate RESULTS SpinalFlow eyeriss comparison per inference SpinalFlow synthetic conv layer normalize eyeriss analysis assumes resolution activation sparsity eyeriss later resolution version eyeriss along axis SNN sparsity resolution SNN activation resolution sparsity synthetic workload SpinalFlow consumes eyeriss mainly SpinalFlow handle sparsity breakdown component eyeriss SpinalFlow filter buffer scratchpad dominant contributor SpinalFlow eyeriss respectively SpinalFlow access filter buffer contributes whenever zero activation encounter due SpinalFlow sparsity eyeriss access GLB ifmap spad contribute irrespective activation sparsity therefore gap SpinalFlow eyeriss grows sparsity increase SpinalFlow sub optimal handle  layer ofmaps  input PEs buffer fetch severely utilized inference SpinalFlow normalize eyeriss sparsity refers sparsity SNN inference SpinalFlow relative eyeriss workload mobilenet combination  PWC layer SpinalFlow inefficient processing  layer inference resnet resolution sparsity eyeriss SpinalFlow ifmap  psum refers correspond scratch pad eyeriss PE resolution overall efficient eyeriss mobilenet  layer account execution saving generally workload unlike spike eyeriss SpinalFlow efficient eyeriss nearly evaluate sparsity resolution resolution sparsity average workload SpinalFlow consumes eyeriss baseline SNNs naturally exhibit sparsity prior SNNs STDP achieve significantly sparsity input resolution anns anns unlikely exhibit sparsity already prior assume ann baseline anns certainly resolution accuracy VI evaluate SpinalFlow eyeriss resolution per inference SpinalFlow workload normalize BSP eyeriss resolution plot per inference synthetic workload SpinalFlow unlike earlier graph normalize eyeriss baseline data normalize eyeriss baseline resolution SpinalFlow ann sparsity throughout SpinalFlow improvement resolution trend slightly increase whereas clearly decrease primarily flip flop baseline eyeriss PE SRAM filter buffer SpinalFlow  workload resolution sparsity average SpinalFlow consumes eyeriss baseline inference SpinalFlow normalize eyeriss baseline resolution SpinalFlow sparsity eyeriss fix inference SpinalFlow workload normalize eyeriss baseline resolution SpinalFlow latency comparison latency inference SpinalFlow normalize eyeriss resolution activation sparsity model version eyeriss global buffer SpinalFlow another eyeriss SpinalFlow latency sparsity resolution  exception utilization workload SpinalFlow competitive eyeriss sparsity comparable compute utilization sparsity SpinalFlow magnitude faster eyeriss execution function spike sparsity speedup average workload sparse input baseline eyeriss already gate alu jumping computation accelerator scnn sparse input scnn index generation logic crossbar network achieve performance improvement typical sparsity anns baseline scnn SpinalFlow significant speedup latency per inference SpinalFlow respect eyeriss GLB link GLB link SpinalFlow spike eyeriss SpinalFlow per inference normalize spike eyeriss synthetic conv workload per inference SpinalFlow normalize spike eyeriss correspond resolution sparsity architecture execute SNNs computation overhead recall unlike SpinalFlow spike eyeriss spike tick tick incurs significant chip GLB overhead chip SNNs resolution due GLB access chip access respectively spike eyeriss consume average SpinalFlow resolution input resolution decrease overhead chip GLB access reduce significantly hence improvement SpinalFlow spike eyeriss reduces relative efficiency SpinalFlow improves sparsity trend workload resolution sparsity workload SpinalFlow consume roughly spike eyeriss spike eyeriss input  tick hence resolution eyeriss therefore multiple magnitude SpinalFlow SpinalFlow per inference normalize spike eyeriss network workload fully layer fully network batch execution entirely dominate bottleneck fetch dram account SpinalFlow eyeriss assume input batch SpinalFlow magnitude efficient baseline eyeriss baseline eyeriss relatively chip storage capacity multiple dram access activation chosen dataflow however augment eyeriss substantial chip buffer capacity SpinalFlow dataflow maximize reuse bottleneck shift microarchitectural component eyeriss SpinalFlow per inference synthetic fully workload SpinalFlow normalize eyeriss trend convolutional layer SpinalFlow respect eyeriss execute workload FC FC batch sparsity SpinalFlow consumes eyeriss whereas sparsity resolution SpinalFlow consumes consume eyeriss scalability per inference resnet SpinalFlow normalize eyeriss function PEs SpinalFlow analyze scalability SpinalFlow PEs correspondingly buffer varied per inference PEs buffer increase KB MB resnet increase compute storage resource increase efficiency diminish return beyond PEs layer feature buffer increase significantly workload compute density gop SpinalFlow normalize eyeriss PEs varied throughput per workload SpinalFlow PE buffer varied buffer SpinalFlow fare eyeriss throughput per alleviate PEs buffer SpinalFlow scnn sensitivity analysis SpinalFlow scnn ann accelerator exploit sparsity model scnn resolution PEs mac iso alu comparison favorable assumption scnn model computation storage overhead meta data index crossbar connects MACs accumulator buffer model accumulator buffer instead due limitation cactus resnet SpinalFlow activation sparsity consumes scnn whereas activation sparsity consumes scnn assume buffer organization scnn sweep buffer hierarchy reveal efficient scnn VI  SNN VS ann debate remains healthy debate within community merit SNNs anns issue keynote   HPCA  isca   williams  smith summarize debate finding analysis demarcate SNN choice ann SNN ann efficiency couple analyze SNN ann efficiency micro attempt  comparison ann SNN hardware layer ann neuron layer neuron layer layer SNN neuron mnist workload digit recognition architecture model assume dedicate hardware per neuron approach network limited comparison author conclude anns SNNs per neuron overhead recent improves upon prior efficient accurate neuron model conclusion limited network dataflows analysis diverse network architecture data reuse factor baseline dataflows contrary prior SNNs magnitude anns metric execution resolution resolution resolution dataflow input interval  perform SNNs consume SNN ann efficiency almost par input interval network exhibit sparsity SNNs SNN execution improve significantly comparison nuanced anns resolution approach significantly accuracy approach anns SNNs comparable network sparsity determines architecture discussion prediction accuracy improve dataflow improves efficiency quantify relationship efficiency sparsity resolution dataflow impact accuracy accuracy primary consideration SNN ann debate completeness articulate SNNs anns superior label datasets available supervise training gpu tpu cluster scenario anns propagation sgd SNNs borrow achieve accuracy anns primarily SNN neuron emulate behavior ann SNN training attention SNNs generally accuracy summarize IV SNNs efficient SpinalFlow SNN training demand future investment smith SNN operating resolution sparsity accuracy ann advance along SNNs achieve accuracy anns resolution typically significant  anns accuracy SNNs summary impact resolution anns accuracy IV ann reduce accuracy alexnet imagenet continual ability STDP efficiently perform online training allows SNNs react faster input encounter landscape disaster recovery accent processing curated pre label datasets available training perform device rover handle disaster recovery continual emerge limited amount literature anns sgd suffer concept catastrophic forget sequentially datasets sgd global error minimization tends perturb network parameter react dataset STDP label datasets localize training naturally  subset neuron dataset perturb model SNN STDP winner exploit dataflows significantly reduce execution workload ann accuracy SNN accuracy mnist SNN sgd SNN STDP sgd SNN sgd SNN STDP cifar SNN alexnet SNN imagenet vgg SNN imagenet resnet SNN imagenet IV accuracy comparison supervise training label datasets vii CONCLUSIONS baseline SNN architecture spike eyeriss severely penalize access neuron potential filter tick sequentially input interval spike eyeriss consumes baseline eyeriss sparsity resolution resolution eyeriss devise architecture dataflow increase data reuse tailor sparsity future SNNs SpinalFlow improves efficiency eyeriss version eyeriss consumes eyeriss evaluate sparsity resolution effective convolutional layer effective memory constrain fully layer performance SpinalFlow faster eyeriss assume sparsity SpinalFlow access regular buffer yield throughput eyeriss workload architecture greatly improves latency throughput accelerator truenorth simulate brain model neural network reuse management sparsity exploitation SNN ann relative efficiency useful guideline researcher develop SNNs various analysis quantifies scenario resolution sparsity network topology SNNs surpass efficiency anns highlight develop accurate training model  sparsity per inference