personalize recommendation leverage model account majority data AI cycle performance dominate memory bound sparse embed operation unique irregular memory access fundamental challenge accelerate proposes lightweight commodity dram compliant memory processing accelerate personalize recommendation inference depth characterization production grade recommendation model embed operation model operator data parallelism memory bandwidth saturation limit recommendation inference performance propose RecNMP scalable improve throughput sparse embed model RecNMP specifically tailor production environment location operator server hardware software  technique memory cache  packet schedule entry profile memory latency speedup  baseline overall RecNMP throughput improvement memory saving introduction personalize recommendation fundamental building internet service social network online retail content personalize recommendation leverage maximize accuracy deliver user underlie model consume majority datacenter cycle spent AI recent analysis reveals recommendation model collectively contribute AI inference cycle across facebook production datacenters despite computational demand production impact relatively research conduct optimize DL recommendation research effort within architecture community focus accelerate compute intensive highly regular computational washington louis facebook harvard facebook texas austin facebook  facebook compute memory footprint operator sweep batch roofline  FC SLS model RM speedup enable RecNMP fully FC convolution cnn recurrent rnn neural network unlike cnns rnns recommendation model exhibit compute intensity regularity exist acceleration technique apply improvement tend exploit regular reusable dataflow assume spatial locality performance bottleneck recommendation model volume personalize inference rapid growth rate data analogous effort improve performance model substantial impact personalize content individual user recommendation model generally structure advantage continuous dense categorical sparse feature latter capture embed sparse lookup pool operation embed operation dominate recommendation model markedly distinct layer quantitative comparison raw compute memory access requirement sparse embed operation  SLS consist sparse lookup embed reduction embed entry pool unique challenge acm annual international symposium computer architecture isca doi isca sparse lookup comparatively MBs irregular index exhibit predictability render typical prefetching dataflow optimization technique ineffective embed GBs overwhelm onchip memory resource furthermore circular operational intensity SLS magnitude FC layer intensity limit potential custom hardware specialized datapaths chip memory cnn rnn accelerator fundamental memory bottleneck cannot overcome standard cache tile algorithmic input batching hardware acceleration technique proposes RecNMP memory processing accelerate embed operation  recommendation RecNMP lightweight  built exist standard dram technology focus DIMM memory processing instead resort specialized 5D 3D integration HBM DIMM factor commodity ddr device 0GB capacity production recommendation model eliminate chip memory bottleneck expose internal bandwidth RecNMP significant opportunity improve performance efficiency roofline bandwidth constrain enable optimization opportunity feasible exist perform detailed characterization recommendation model source production DLRM benchmark analysis quantifies potential benefit memory processing accelerate recommendation model intuition  NMP hardware algorithmic recommendation specifically highlight opportunity RecNMP architecture bandwidth intensive embed operation perform memory compute intensive FC operator perform cpu potentially accelerator propose RecNMP exploit DIMM  parallelism dram memory RecNMP performs local lookup pool function memory sparse embed inference operator reduce execution contrast purpose NMP architecture judicious choice implement lightweight functional memory cache limit overhead consumption combine lightweight hardware software optimization  packet schedule entry profile previous performance evaluation solely randomly generate embed access characterization experimental methodology model representative production configuration evaluate production embed trace overall RecNMP significant embed access latency reduction improves recommendation inference performance illustrate research contribution depth workload characterization production recommendation model constrain memory bandwidth locality analysis production embed trace reveals distinctive spatial temporal reuse motivates custom NMP approach recommendation acceleration propose RecNMP lightweight ddr compatible memory processing architecture RecNMP accelerates execution recommendation model memory latency speedup memory saving overall RecNMP achieves throughput improvement examine hardware software optimization technique memory cache aware packet schedule entry profile enhance RecNMP performance customize NMP instruction dram command address bandwidth expansion production aware evaluation framework developed account data representative production configuration model location load balance II  learning  recommendation MODELS describes architecture  recommendation model prominent sparse embed feature performance bottleneck conduct thorough characterization  recommendation model DLRM benchmark characterization latency breakdown roofline analysis bandwidth analysis memory locality illustrates unique memory requirement access behavior production recommendation model justifies propose memory accelerator architecture overview personalize recommendation model personalize recommendation task recommend content user preference previous interaction instance video rank netflix youtube video potentially recommend user deliver accurate recommendation timely efficient manner important recommendation model extremely feature capture user behavior preference feature typically dense sparse feature dense feature vector matrix typical dnn layer FC cnn rnn sparse feature index embed model architecture DL recommendation capture specific model parameter mixture dense simplify model architecture reflect production recommendation model parameter representative recommendation model sparse feature broadly observable across alternative recommendation model embed lookup pool operation abstract representation sparse feature training central DL recommendation model embed organize potentially vector generally embed operation exhibit reduce specific wise reduction operation varies model caffe comprises embed operation prefixed   perform reduce embed operation quantize summation SLS operator primitive widely employ production recommendation application youtube fox aim alleviate performance bottleneck improve throughput devise novel NMP offload SLS embed operation recommendation facebook DLRM benchmark demonstrate advantage memory processing personalize recommendation model facebook recommendation model  dense feature  operator sparse input feature embed lookup output operator combine  prediction click rate user item focus performance acceleration strategy recommendation model canonical model RMC RMC recommendation model consume significant machine execution cycle facebook production datacenter RMC RMC parameter configure notable distinguish factor across configuration embed model operator data parallelism production inference latency breakdown across model RMC RMC RMC RMC batch RMC comparatively model embed RMC embed recommendation employ parallelism achieve throughput strict latency constraint model parallelism grows increase concurrent model inference machine operator parallelism parallel thread per model data parallelism increase batch SLS operator performs batch pool operation pool operation performs summation vector input SLS batch embed lookup index vector sparse IDs optionally vector operator bottleneck SLS operator contributor latency recommendation model batch data parallelism increase depicts execution breakdown per operator majority spent execute FC SLS caffe operator batch SLS account model execution RMC RMC respectively whereas model RMC RMC significant portion execution SLS furthermore spent embed operation increase batch RMC RMC respectively execution RMC RMC RMC comprises embed embed increase model roofline analysis apply roofline model recommendation model memory bandwidth constrain roofline multi thread RMC RMC sweep batch darker indicates batch memory bandwidth saturation increase parallel SLS thread batch theoretical roofline performance bound construct roofline theoretical limit described IV intel memory latency checker mlc derive memory bound derive compute bound sweep fuse  FMA processor operating frequency cpu turbo mode enable roofline data model RMC RMC correspond FC SLS operator separately sweep batch darker batch SLS operator compute memory requirement FC portion model compute combine model SLS fix operational intensity across batch performs vector lookup wise summation FC operational intensity increase batch request batch FC increase FC data reuse increase batch FC operator memory bound roofline  model RMC RMC memory bound operational intensity dominate percentage SLS operation reveals increase batch performance SLS RMC RMC approach theoretical performance bound importantly roofline analysis suggests performance recommendation model within theoretical performance bound improvement without increase memory bandwidth perform embed lookup pool operation pin limited memory interface memory processing exploit internal bandwidth memory effectively roofline fundamentally improve memory  performance bound memory bandwidth production configuration execute embed operation saturate memory bandwidth model operator  parallelism depicts memory bandwidth consumption increase parallel SLS intel mlc bandwidth processor thread traverse memory random sequential stride temporal data locality sweep cache capacity MB fix cacheline spatial data locality sweep cacheline fix cache capacity MB thread batch curve horizontal ideal peak bandwidth GB channel ddr curve empirical upper bound intel mlc memory bandwidth easily saturate embed operation batch thread increase memory bandwidth saturation occurs batch SLS thread available bandwidth SLS bandwidth saturation beyond becomes undesirable memory latency increase significantly perform reduce operation memory output pool return cpu embed locality analysis prior assume embed lookup random however trace production traffic exists modest locality mostly due temporal reuse recommendation model limited memory performance generally memory locality cache improve performance evaluate random trace embed lookup trace production workload production recommendation model contains embed multiple model colocated machine mimic cache behavior production simulate cache rate multiple embed machine comb embed machine trace embed interleave embed comb comb comb embed machine approximates model embed lru cache replacement policy associative cache assume embed contiguous logical address randomly mapped physical estimate amount temporal locality sweep cache capacity MB fix cacheline random trace rate locality combine simulation production trace random rate importantly rate increase cache increase optimization RecNMP advantage locality aware packet schedule software locality hint batch profile spatial locality estimate sweep cacheline fix cache capacity MB illustrates sweep comb cacheline increase rate decrease isolate increase conflict fully associative cache trend decrease rate conclude embed lookup operation spatial locality  unique memory bound characteristic sparse irregular access personalize recommendation propose RecNMP practical lightweight memory processing accelerate dominate embed operation maximize dram rank parallelism compute directly locally data fetch concurrently activate rank employ minimalist style hardware architecture embed specialized logic rank cache SLS inference operator instead purpose computation modify hardware limited buffer chip within DIMM without commodity dram device sparse irregular embed lookup exerts demand command address bandwidth address compress instruction format standard memory interface conform standard dram physical pin timing constraint propose NMP employ NMP instruction without address limitation irregular spatial locality memory access hardware software HW SW interface host NMP coordination adopt heterogeneous compute program model OpenCL finally explore HW SW optimization technique memory cache aware schedule entry profile additional performance gain approach leverage observation workload characterization previous hardware architecture overview RecNMP resides buffer chip DIMM buffer chip bridge memory channel interface host standard dram device interface data pin illustrate buffer chip contains RecNMP processing PU DIMM NMP module multiple rank NMP module approach non intrusive scalable memory capacity memory channel multiple RecNMP equip DIMMs multiple ddr channel utilized software coordination host memory controller communicates RecNMP PU customize compress format NMP instruction NMP inst conventional memory channel interface PU return accumulate embed pool DIMM sum host regular ddr compatible data signal ddr ddr DQ decode RecNMP PU NMP insts dram device across parallel rank DIMM logic rank RecNMP issue concurrent request parallel rank utilize SLS operator internal bandwidth memory channel effective bandwidth aggregate across parallel activate rank memory configuration DIMMs rank per DIMM achieve internal bandwidth DIMM NMP module receives NMP inst DIMM interface correspond rank NMP module rank address  decode execute NMP inst perform local computation embed vector concurrently confine SLS operation rank aggregation across rank within PU simplifies memory layout increase bandwidth DIMM NMP performs remain wise accumulation partial sum vector psum parallel rank DIMM sum fashion psums accumulate across multiple RecNMP pus software coordination dive detail DIMM NMP rank NMP module buffer chip logical module easy DIMMs rank DIMM NMP module dispatch NMP inst DIMM interface DIMM NMP module employ ddr phy protocol conventional DIMM buffer chip relay dram DQ signal host memory controller instruction multiplexed correspond rank rank ID DIMM NMP buffer psum vector accumulate rank NMP local register performs summation adder host via standard DIMM interface memory configuration rank within DIMM input adder rank NMP module RecNMP internal bandwidth DIMM increase effective bandwidth embed operation majority logic replicate rank crucial function perform rank NMP module translate NMP inst lowlevel ddr command manage memory cache compute SLS operator locally illustrate NMP inst decode signal register input address bus limitation ddr command SLS vector embed  NMP inst ddr cmd presence absence RD pre vector vsize dram address  ddr command sequence burst fed local command decoder rank  generate standard ddr style RD pre command communicate architecture overview RecNMP architecture DIMM NMP rank NMP NMP instruction format dram device tag runtime host memory controller relative physical address location consecutive embed access  rank NMP lightweight hostside memory controller perform task request reorder arbitration refresh signal generation vector vsize pre buffer command sequence dram device NMP inst pre RD col RD col decode RD pre vsize tag locality analysis II modest temporal locality within embed vector reuse operand SLS operator cache DIMM cpu ineffective incorporate memory cache RankCache rank NMP module exploit embed vector reuse RankCache RecNMP hint  NMP inst embed vector cached bypass detailed generate  hint entry profile explain entry RankCache tag dram address   NMP inst indicates locality memory request bypass RankCache rank  initiate dram embed inference optimization impact correctness datapath rank NMP module SLS operator embed vector return RankCache dram device load input embed vector register sum computation register NMP inst quantize operator SLS operator  parameter scalar bias embed vector fetch memory load scalar bias register scalar bias register execution non non quantize SLS operator  decode NMP inst identify embed vector belonging pool operation multiple  batch embed parallel controller counter vector register sum register  rank NMP module memory mapped easily accessible configurable host cpu bandwidth expansion although theoretical aggregate internal bandwidth RecNMP linearly rank per channel concurrently activate rank limited bandwidth due frequent buffer conflict spatial locality access embed entry memory pre command probability access embed vector spatial locality exists continuous dram data burst embed vector production embed vector spatial locality consecutive buffer narrow fully understand bandwidth limitation analyze scenario embed vector typical timing diagram ideal sequence interleave dram achieve consecutive data burst burst mode command address RD command accompany address TRL dram cycle data DQ DQ data bus burst mode dram cycle burst transmits DQ pin signal memory employ interleave therefore burst cycle dram cycle data access sequential manner ideal interleave data transfer dram cycle ddr command RD pre DIMM interface consumes bandwidth activate concurrently issue ddr command completely exhaust available bandwidth conventional memory interface overcome bandwidth limitation propose customize NMP inst compress format ddr command transmit memory controller RecNMP pus illustrates timing diagram timing diagram ideal dram interleave operation propose RecNMP concurrent rank activation interleave NMP inst DIMMs rank per DIMM memory configuration NMP insts transfer memory controller DIMMs interface dram data burst cycle data rate spatial locality embed vector NMP inst per vector ideal interleave potentially activate parallel rank perform lookup concurrently dram data burst cycle although customize instruction propose directly bandwidth limitation ddr command compression enables bandwidth expansion embed vector spatial locality expansion ratio achieve vector program model execution previous NMP RecNMP adopts heterogeneous compute program model OpenCL application host cpu NMP kernel offload RecNMP pus NMP kernel compile packet NMP insts transmit memory channel DIMM interface RecNMP pus NMP kernel transmit host cpu NMP inst contains distinctive associate parameter embed operation locality hint  pool tag  HW SW interface propose NMP inst format within standard pin DQ interface SLS function execution propose RecNMP program model memory allocate SLS input output data marked host cacheable NMP non cacheable simplify memory coherence host RecNMP variable host visible data array index initialize load host  host cpu cache hierarchy embed emb memory initialize host host  NMP non temporal hint  5D EE RecNMP SLS code NMP packet NMP kernel offload NMP enable memory controller code marked NMP kernel compile packet NMP insts SLS NMP kernel batch embed  split multiple NMP packet packet pool operation NMP insts belonging embed  NMP packet tag  maximum  packet   runtime NMP kernel launch host hardware driver handle NMP packet offload access memory management MMU request memory NMP operation virtual memory  physical address translation offload NMP packet bypass eventually hostside memory controller NMP extension avoid schedule NMP packet FR FCFS policy NMP extension memory controller extra schedule arbitration logic illustrate memory controller NMP extension receives concurrent NMP packet parallel execution multiple host core queue schedule NMP packet decode queue NMP insts physical dram address mapping perform FR FCFS scheduler reorder NMP insts within packet packet instead ddr command RD pre action compress ddr cmd NMP inst host memory controller calculates accumulation counter configure memorymapped register RecNMP PU finally completion counter local computation inside RecNMP PU NMP packet sum NMP packet schedule scheme prioritizes batch rate MB cache without optimization aware packet schedule optimization aware packet schedule entry profile optimization ideal without interference transmit DIMM interface return output cacheable memory visible cpu HW SW optimization locality analysis production recommendation traffic II illustrates intrinsic temporal reuse opportunity embed lookup propose memory cache RankCache inside rank NMP module extract performance memory cache explore additional HW SW optimization technique  optimization memory latency improvement memory access detailed performance benefit preserve intrinsic locality embed lookup reside propose prioritize schedule NMP packet batch request embed aware packet schedule production workload memory controller receives NMP packet parallel SLS thread schedule priority intra embed temporal locality easily retain interference lookup operation multiple embed locality degrade multiple recommendation model therefore illustrate propose optimize aware NMP packet schedule strategy exploit intrinsic temporal locality within batch request packet embed batch embed vector fetch thereby retain temporal locality SLS operator access embed parallel thread mechanic implementation thread memory scheduler propose another optimization technique entry profile built observation subset embed entry exhibit relatively reuse characteristic profile vector index embed lookup NMP kernel entry locality explicitly annotate NMP insts  NMP inst  cached RankCache otherwise request bypass RecNMP experimental methodology RankCache entry profile perform model inference issue SLS request execution profile index incoming batch embed lookup  vector access within batch infrequent vector bypass RankCache directly dram device sweep threshold cache rate simulation entry profile optimization reduces cache contention eviction frequent entry RankCache depicts rate improvement optimization apply comb indicates overall rate model embed gain insight investigate rate embed comb ideal indicates theoretical rate infinitely cache propose  rate closely approach ideal across individual embed trace limited locality illustrate propose technique effectively retain embed vector likelihood reuse RankCache IV experimental methodology experimental setup combine evaluation cycle memory simulation evaluation production recommendation model server CPUs data allows impact accelerate embed operation improve memory performance FC operation model cycle memory simulation evaluate tradeoff dram augment RecNMP summarizes parameter configuration core intel skylake ddr memory dram simulation standard ddr timing micron datasheet evaluation configure  benchmark model parameter trace II workload characterization II perform socket intel skylake server specification cycle memory simulation RecNMP cycle simulation framework  PARAMETERS CONFIGURATIONS configuration processor core ghz LI KB cache MB llc MB dram ddr mhz 8GB GB channel DIMM rank FR FCFS entry RD WR queue policy intel skylake address mapping dram timing parameter tRC tRCD tcl trp tbl tCCD tCCD tRRD tRRD tFAW latency parameter ddr activate ddr RD WR chip IO RankCache RD WR cycle access FP adder cycle FP mult cycle  physical address mapping module packet generator locality aware optimizer cycle accurate model RecNMP PU consist dram device RankCache arithmetic logic ramulator conduct cycle evaluation ddr device ramulator cycle accurate lru cache simulator RankCache model stage pipeline rank NMP module cactus estimate access latency RankCache hardware implementation estimate latency arithmetic logic built synopsys compiler commercial technology library estimate DIMM cactus  dram device cactus IO chip DIMM simulation emulate schedule packet generation software stack memory controller apply standard mapping generate physical address trace embed lookup assume OS randomly selects physical logical frame physical address trace fed ramulator estimate baseline memory latency RecNMP workload packet generator physical address trace packet NMP insts cycle accurate model evaluate HW SW optimization locality aware optimizer performs aware packet schedule entry profile decides sequence NMP insts RecNMP activate memory rank parallel traditional dram  NMP packet performance slowest rank receives heaviest memory request load rank NMP DIMM NMP logic pipelined hide latency memory operation latency RecNMP extra dram cycle initialization configure accumulation counter vector register cycle stage transfer sum host latency dram cycle component RankCache rank NMP logic perform partial sum sum evaluation RESULTS quantitative evaluation RecNMP accelerates personalize rec normalize latency RecNMP baseline dram memory configuration DIMM rank NMP packet distribution rank load imbalance rank  inference latency improvement offload SLS operator baseline analyze optimization placement memory cache  packet schedule entry profile RecNMP NMP TensorDIMM chameleon analyze RecNMP FC operator finally evaluation throughput improvement saving model overhead SLS operator speedup theory RecNMP exploit rank parallelism speedup linearly rank DIMMs therefore memory channel configuration DIMMs rank per DIMM correspond demonstrate implementation RecNMP without RankCache evaluate RecNMP without RankCache RecNMP addition DIMM rank configuration sweep  NMP packet pool DLRM sum embed vector SLS latency indeed linearly increase active rank channel latency decrease pool operation NMP packet variation performance gap actual speedup theoretical speedup rank rank rank uneven distribution embed lookup across rank rank parallel latency SLS operation slowest rank rank embed lookup statistical distribution slowest rank NMP packet NMP insts workload distributes unevenly longer degrades average speedup address load imbalance software allocate entire embed rank software approach perform data layout normalize latency RecNMP cache RecNMP opt schedule entry profile optimization baseline dram cache sweep RecNMP opt  achieve speedup rank rank rank dram baseline specific mechanism implement operating assign fix frame individual embed virtual memory aware dram configuration allocate physical address rank data layout optimization ideal speedup maintain model task parallelism multiple NMP packet SLS operator issue simultaneously available rank RecNMP RankCache optimization  cache rank aware packet schedule entry profile notable feature RecNMP optimization described depicts performance benefit latency reduction enable apply optimization technique RankCache schedule access  hint software configuration rank  per packet latency improvement KB RankCache additional improvement prioritize schedule NMP packet batch combine optimization schedule profile pas cacheability hint profile index batch reduces cache contention allows locality request marked cache bypass RankCache deliver another improvement memory latency speedup achieve offload SLS optimize RecNMP opt sweep RankCache capacity KB MB display cache affect normalize latency cache rate RankCache KB cache rate dram access latency performance optimal KB increase cache marginal improvement rate already compulsory limit trace incurs longer cache access latency degrades overall performance performance comparison RecNMP NMP chameleon TensorDIMM DIMM memory processing TensorDIMM embed operation comparison host baseline RecNMP opt TensorDIMM chameleon random production trace performance linearly parallel DIMMs non SLS operator accelerate gpus TensorDIMM orthogonal memory acceleration technique memory latency speedup RecNMP chameleon directly embed operation estimate performance chameleon simulate temporal spatial multiplexed DQ timing chameleon nda accelerator RecNMP exploit rank parallelism performance DIMMs rank increase whereas chameleon TensorDIMM increase DIMMs evident sweep memory channel configuration increase rank per DIMM RecNMP deliver performance chameleon TensorDIMM worth RecNMP performance advantage configuration rank per DIMM thanks memory cache aware packet schedule entry profile optimization technique neither chameleon TensorDIMM memory cache explicitly advantage available locality memory access hence performance respect memory latency agnostic trace amount data reuse contrast RecNMP extract performance shade production trace fully random trace FC operator speedup although RecNMP accelerate execution SLS operator improve FC performance alleviate cache contention model location data parallelism increase FC cache hierarchy reuse normally cache however model reusable FC data evict cache SLS data performance degradation performance degradation FC operation amount performance degradation experienced FC layer varies FC location pool examine FC performance baseline worsen FC performance FC location pool RecNMP effectively reduces pressure cache contention RecNMP RecNMP opt impact FC performance equally offloads SLS computation beneficial pronounce FCs parameter exceed capacity cache reside mainly inside model location latency  RMC model  RMC model speedup recommendation inference rank rank rank RecNMP model speedup batch host RecNMP opt model  tradeoff llc cache FCs inside cache  RMC  relative improvement comparatively model speedup throughput improvement estimate improvement recommendation inference latency calculate speedup speedup SLS non SLS operator model speedup across representative model configuration surprisingly model spends SLS operator RMC receives speedup performance improvement obtain RecNMP varies batch model speedup increase batch proportion spent accelerate SLS operator grows overall increase colocation presence random production trace cpu baseline propose RecNMP location generally increase throughput degrade latency random trace locality production trace improves performance however locality performance bonus model location increase due cache interference embed multiple model apply RecNMP rank speedup RMC RMC model increase II summary RecNMP overhead RecNMP PU chameleon cgra accelerator RecNMP RankCache RecNMP opt RankCache SLS latency improvement latency throughput enable RecNMP clearly baseline memory saving baseline dram RecNMP memory RecNMP reduce data movement processor memory perform local accumulation dram device leakage reduce latency addition incorporate memory cache apply optimization technique improve RankCache rate RecNMP achieves extra saving reduce dram access overhead estimate RecNMP overhead assume mhz frequency CMOS technology derive synopsys compiler DC arithmetic logic cactus SRAM memory RankCache II summarizes overhead RecNMP processing configuration without cache optimize configuration cache optimization chameleon embeds cgra core per DIMM RecNMP PU consumes RecNMP RecNMP opt RecNMP pus multiple rank DIMM linearly translates linearly embed speedup DIMM consumes typical buffer chip RecNMP incurs overhead easily accommodate without dram device VI related performance characterization recommendation model recent publication importance personalize recommendation model data cnns rnns FCs analysis demonstrates recommendation model unique storage memory bandwidth compute requirement instance illustrates facebook personalize recommendation model dominate embed operation knowledge RecNMP perform locality production model representative embed trace dram memory data acceleration prior explore memory processing 3D 5D stack dram technology HMC HBM due limited memory capacity 2GB ownership scheme suitable largescale deployment recommendation model GBs production environment chameleon introduces practical approach perform memory processing integrate cgra accelerator inside data buffer device commodity LRDIMM unlike chameleon DIMM acceleration RecNMP exploit rank parallelism speedup potential RecNMP employ lightweight NMP tailor sparse embed operator overhead CGRAs error correction ecc important reliable operation RecNMP rank minimal logical device dram chip rank address simultaneously preserve simplicity ecc operation data chip parity chip alternatively error detection correction logic buffer chip compute logic incur hardware overhead ecc tailor memory processing remain optimization memory constrain model sparse embed representation commonly employ augment neural network dnn model external memory memorize previous explore NVMs embed storage although propose technique improvement effective nvm bandwidth 3GB remains typical dram bandwidth 8GB cannot fundamentally address memory bandwidth bottleneck recommendation model  target optimization memory augment neural network proposes dedicate embed cache eliminate cache contention embed inference operation however technique directly apply personalize recommendation consist magnitude embed TensorDIMM proposes custom DIMM module enhance memory processing core embed tensor operation address mapping scheme TensorDIMM interleaf consecutive within embed vector across DIMM module performance DIMM relies inherent spatial locality embed vector unable apply approach vector memory configuration outperform TensorDIMM memory latency speedup extract additional performance gain  parallelism memory cache optimization introduction customize compress NMP instruction RecNMP fundamentally address bandwidth constraint without restriction embed vector impose TensorDIMM vii conclusion propose RecNMP practical scalable  personalize recommendation perform systematic characterization production relevant recommendation model reveal performance bottleneck commodity dram compliant RecNMP maximally exploit rank parallelism temporal locality production embed trace achieve performance improvement sparse embed operation SLS operator offload SLS alleviate cache contention non SLS operator remain cpu latency reduction FC operator overall systemlevel evaluation demonstrates RecNMP throughput improvement memory representative production relevant model configuration