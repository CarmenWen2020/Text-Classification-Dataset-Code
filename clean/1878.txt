image processing become increasingly important domain application workstation datacenter accelerator performance efficiency gpu accelerator image processing suffers memory bandwidth bottleneck tackle bottleneck architecture promising due enormous internal bandwidth memory access however previous lack hardware programmability image processing workload numerous heterogeneous pipeline stage diverse computation memory access enable programmable architecture hardware overhead remains challenge proposes ipim programmable memory image processing accelerator architecture decouple execution architecture lightweight programmability propose simb instruction multiple ISA enable flexible data access compilation halide image processing application simb ISA develop ipim aware compiler optimization register allocation instruction reorder memory enforcement improve performance evaluate representative image processing application ipim demonstrate average ipim obtains acceleration nvidia tesla gpu analysis compiler optimization contribute speedup unoptimized baseline keywords memory image processing accelerator introduction advance image device computational photography image processing become increasingly important domain workstation datacenter platform various application machine biomedical engineering geographic information image processing workload usually involve amount data intensive computation motivate domain specific accelerator performance efficiency peng  xie primary author NSF accelerator image processing gpu achieve however  impedes performance improvement characteristic image processing workload limited bandwidth  architecture image processing application memory bandwidth pipeline stage arithmetic intensity heterogeneous pipeline stage hardly fuse due introduction redundant computation degradation parallelism memory bandwidth compute centric architecture hinder limited chip pin costly data movement validate bottleneck conduct detailed profile representative image processing benchmark nvidia tesla gpu sec apparent bandwidth bound performance bottleneck memory utilization alu utilization overcome bottleneck memory bandwidth  processing memory 3D pim architecture promising architecture embrace memory bandwidth integrate compute logic nearer memory 3D pim  compute logic logic utilize cube internal  tsv bandwidth demonstrates bandwidth advantage gpu unleash bandwidth  propose closely integrates compute logic dram around peak bandwidth improvement previous  directly access local without limited TSVs therefore architecture emerges competitive memory bandwidth challenge compute centric image processing accelerator although architecture potential accelerate image processing application challenge heterogeneous image processing pipeline exhibit various computation memory acm annual international symposium computer architecture isca doi isca  programmable hardware however directly attach core dram introduces overhead challenge lightweight architecture diverse image processing pipeline instruction architecture ISA concise powerful avoid complex hardware enable flexible computation data movement operation compilation accelerator easy program interface enable efficient mapping various image processing pipeline architecture backend optimization fully exploit hardware potential address challenge architecture image processing pipeline programmable image processing accelerator ipim compilation halide efficiently application onto accelerator ipim decouple execution architecture integrate core tight constraint specifically core logic 3D stack lightweight computation buffer attach memory dram execution instruction core broadcast instruction associate TSVs computation conduct parallel execution lockstep instruction multiple simb ISA propose accelerator simb ISA simd computation utilizes width flexible data movement within memory hierarchy instruction enable index calculation synchronization primitive communication develop compilation halide schedule ipim compilation extends frontend halide schedule backend optimization ipim register allocation instruction reorder memory enforcement reduce resource conflict exploit  parallelism optimize dram buffer locality respectively contribution summarize standalone programmable accelerator ipim 3D stack architecture image processing application decouple  architecture ipim programmability overhead per dram propose simb instruction multiple ISA enables flexible computation data access communication various pipeline stage image processing application develop compilation halide novel ipim schedule various ipim backend optimization register allocation instruction reorder memory enforcement evaluation representative image processing benchmark stage heterogeneous multi stage pipeline ipim backend optimization achieve speedup average nvidia tesla gpu backend optimization improve performance ıve baseline II background 3D stack memory 3D pim architecture introduce overall architecture opportunity 3D pim image processing application 3D stack memory cube HBM HMC consists multiple stack dram logic logic logic access memory TSVs silicon vias vertical interconnects 3D layer previous explore compute logic logic harvest cube internal tsv bandwidth constrain maximal bandwidth TSVs currently 7GB cube TSVs TSVs due overhead already 3D layer tackle tsv bottleneck researcher explore architecture 3D pim integrates compute logic adjacent without dram circuitry enormous bandwidth massive parallelism promising accelerate data intensive image processing application however lack programmability due expensive core challenge enable heterogeneous image processing pipeline diverse computation memory tackle programmability challenge propose lightweight decouple  architecture sec IV simb ISA image processing pipeline sec IV image processing halide program image processing contains heterogeneous pipeline bound memory bandwidth compute centric architecture application image processing pipeline stage arithmetic intensity operation per byte massive data parallelism individual pixel elementwise stencil computation pipeline heterogeneous stage local laplacian filter complex data dependency resampling local laplacian filter therefore hardware feature apply pipeline fusion technique boost performance  accelerator gpu image processing performance bound memory bandwidth sec architecture promising although widely adopt program image processing halide optimization compute centric accelerator gpu fpga exist memory centric gpu profile image processing workload II accelerator halide decouples algorithm description algorithm hardware mapping programmer separately algorithm schedule algorithm schedule halide compiler synthesize hardware specific program propose compilation framework image processing application halide architecture novel schedule sec develop compiler backend improve performance sec motivation memory bandwidth performance bottleneck gpu image processing accelerator conduct detailed profile representative benchmark II halide framework divk dataset nvidia tesla gpu dram bandwidth dram utilization alu FP int utilization benchmark exhibit dram bandwidth bound behavior achieve dram utilization 8GB bandwidth alu utilization average memory alu utilization histogram benchmark histogram involves dependent computation halide schedule gpu cannot achieve ideal performance multi stage benchmark optimize halide pipeline fusion performance improvement stage benchmark alu utilization increase dram utilization merely reduce significantly alu utilization conclude halide compiler optimization cannot  behavior image processing application gpu motivate accelerator memory bandwidth index calculation important programmability flexible memory access consumes portion alu utilization image processing workload profile index calculation int data algorithm related computation FP data breakdown alu utilization average index calculation alu utilization index calculation dominates alu utilization benchmark index calculation ratio image processing frequent translation 2D image 1D memory motivates enable architecture index calculation ipim IV architecture introduce microarchitecture overview sec IV ipim decouple  scheme sec IV explain instruction architecture sec IV discus remote memory access mechanism inter vault synchronization sec IV detail functionality ipim hardware component sec IV microarchitecture overview ipim 3D stack architecture hierarchy cube vault illustrate ipim consists multiple cube interconnect  link HMC cube horizontally partition multiple vault usually per cube chip network vault span multiple 3D stack layer  memory pim usually per vault logic inter layer communication realize silicon vias TSVs usually per vault bandwidth vertical interconnects link layer logic logic vault contains ipim core execute ipim program pim vault contains PG consists scratchpad memory pgsm PE employ architecture compute logic lightweight buffer integrate dram PE address register file integer alu efficiently index calculation important image processing microarchitecture ipim decouples happens logic massive  parallel execution happens pim sec IV addition simb ISA various computation memory access image processing efficiently data ipim hierarchy PE PG vault cube sec IV decouple execution architecture ipim novel decouple execution reduce overhead core ipim execution decouple 3D stack microarchitecture 3D stack cube vault PG component inside ipim core logic component inside PE pim ipim instruction multiple simb instruction architecture category instruction description operand computation comp simd computation mode vector vector scalar vector FP int arithmetic mac logical arithmetic shift xor lsb msb comp mode dst drf src drf src drf vec mask simb mask index calculation calc arf memory address calculation int calc arf dst arf src arf src arf simb mask intra vault data movement load data DataRF dram addr drf addr simb mask pgsm load data pgsm pgsm dram addr pgsm addr simb mask pgsm data pgsm DataRF pgsm pgsm addr drf addr simb mask vsm data vsm DataRF vsm vsm addr drf addr simb mask mov drf arf data DataRF  mov drf arf arf addr drf addr simb mask seti vsm immediate vsm location seti vsm vsm addr imm reset reset DataRF entry zero reset drf addr simb mask inter vault data movement req request data remote vault local vault req dst chip dst vault dst dst dst dram addr src vsm addr  conditional  cond crf addr calc crf data calculation int calc crf dst crf src crf src crf seti crf immediate  location seti crf crf addr imm synchronization sync inter vault synchronization sync phase logic allows parallel execution processing pim benefit abundant  bandwidth core principle hardware rely compiler optimization sec realize performance therefore ipim pipelined issue core data hazard eliminate instruction issue hardware complex logic execution simb ISA sec IV exploit massive parallelism program simb mask introduce detailed pipeline execution ipim related instruction program counter instruction fetch instruction cache decode update register file   calc crf seti crf calculate decode instruction checked instruction issue inst queue anti output data dependency instruction stall pipeline bubble insert instruction issue issue inst queue retirement issue instruction broadcast simb controller PE accord simb mask vault execution seti vsm instruction involves remote vault access dispatch network interface controller NIC vault local simb execution PE correspond simb mask proceed execution idle remote vault access request translate packet traverse chip network chip link simb instruction executes lock instruction retires simb mask PE instruction simb controller execution instruction commit pop correspond entry issue inst queue data dependency later instruction conclusion architecture enables lightweight programmability heterogeneous pipeline stage logic parallel execution abundant memory bandwidth data intensive image processing operation pim instruction multiple simb ISA exploit data parallelism image processing propose instruction multiple simb ISA expose parallelism detailed overview ISA resembles RISC simd ISA enables parallel computation efficient memory access detailed computation highlight simb simd execution enable simb simb capable instruction simb mask boolean vector correspond PE execute instruction vault pgs PG PEs simb mask boolean vector enable simd computation data movement instruction operates vector FP int vector chosen local interface per access tsv data transfer width per cycle internal bandwidth fully utilized vault signal data signal physical TSVs multiplexing realize arbiter therefore additional tsv signal PE memory access emphasize data movement memory index enable data movement simb ISA contains instruction realize customize data along memory hierarchy flexible index simb ISA contains index calculation instruction allows communication address register file data register file enable data dependent computation detailed explanation simb ISA computation instruction comp  scalar operation specify mode vec mask indicates vector valid computation defines operation perform index calculation instruction calc arf parallel address calculation PEs PE independent memory access PEs inside vault address simb fashion indirect address dram addr pgsm pgsm addr vsm vsm addr address indirect address mode correspond address index address register file PE fetch address index target memory component satisfy flexible 2D memory access image processing data movement instruction intra vault inter vault classify involves dram access pgsm local vault access req remote vault access data movement along memory hierarchy within vault instruction  related calculation calc crf seti crf enable ipim program dynamic behavior various computation image processing synchronization instruction sync allows vault synchronize computation stage accord phase sec IV contains detailed remote access synchronization ipim data access remote vault implement asynchronous request instruction req local vault remote vault memory address issue req local vault NIC remote vault request dram request queue correspond PE access data temporarily buffer remote vault vsm local vault inter vault link traversal communication interface guarantee delivery acknowledgment ipim realizes synchronization vault lock synchronization instruction sync barrier instruction sync synchronization relies centralize protocol vault designate vault vault coordinate vault synchronization instruction sync execution vault signal vault vault update global synchronization status vector global synchronization vault broadcast proceed phase message vault vault commit sync instruction proceed execution phase hardware component usage introduces important information regard hardware component ipim data address register file DataRF  DataRF  employ multi architecture avoid resource hazard execution DataRF vector interface aligns width accommodate scalar interface  multiplexer addition  location reserve PE     respectively scratchpad memory pgsm pgsm data PEs PG access another PE memory generate pgsm source PE pgsm destination PE pgsm address enable parallel PE access pgsm allocates individual PE employ multi architecture PE pgsm data load pgsm overlap pgsm access pgsm 2D memory abstraction image processing application vault scratchpad memory vsm vsm functionality vsm data PEs vault access another PG memory generate vsm data vsm location vsm data local PE TSVs pgs vsm data TSVs vsm temporarily buffer data remote vault access vsm instruction memory accepts computation offload host dram memory controller ipim integrates lightweight memory controller serf inside PG memory controller contains memory request algorithm func  func    schedule ipim compute ipim tile load pgsm vectorize listing code image blur queue dram command buffer dram command translation issue logic counter dram command issue cycle dram status register address register currently memory controller policy dram schedule policy FCFS FR FCFS schedule dram refresh command accord  trf timing parameter  chip network ipim adopts 2D mesh topology chip chip network router assumes input queue IQ microarchitecture implement rout algorithm channel allocation policy compiler detail compilation halide ipim hardware introduce program interface ipim sec schedule ipim explain compilation sec extension halide compilation customize backend ipim finally detail backend optimization sec generate efficient ipim executable program program interface various image processing application compose heterogeneous pipeline ipim halide program application domain halide eas burden programmer perspective image processing algorithm halide ipim halide decouples algorithm schedule develop customize schedule easy abstraction workload partition data PEs ipim workload partition data optimize automatically compilation accord schedule without programmer involvement develop customize schedule primitive efficiently exploit hardware characteristic ipim hardware extend halide schedule primitive ipim tile load pgsm distribute data utilize scratchpad  customize schedule ipim ipim tile ipim compilation image blur spatial mapping PG schedule PE schedule specifies dimension image data partition distribute across hierarchy ipim schedule ipim tile listing indicates image partition image tile addition partition image tile schedule indicates distribution image tile across PEs distribution image tile hierarchy ipim specifically image tile distribute interleave PEs PG load adjacent image tile loop iteration improve data customize schedule ipim load pgsm indicates usage scratchpad memory PG schedule load pgsm listing indicates data input image compute output along loop load scratchpad memory computation usage pgsm accord specification load pgsm listing PG load data pgsm temporal schedule computation PE load input data vector output pixel schedule data across adjacent image tile PG addition customize schedule data partition ipim leverage exist halide schedule specify fusion pipeline vectorization computation ipim listing compute ensures loop along dimension func outermost loop stage compute  fuse computation code generation compute implies kernel function reading input data output dram besides compute exploit vectorization schedule vectorize halide ipim compilation ipim ISA simd instruction specifically exploit compilation pas vectorization halide frontend align data improve utilization simd ipim compilation develop compilation automatic transformation halide algorithm customize ipim schedule hardware executable program ipim develop frontend code transformation ipim schedule backend instruction optimization improve performance generate program backend optimization unique challenge due novel architecture perspective core register allocation phase prevent data hazard due register contention phase aim span virtual register physical register avoid data hazard instead minimize allocate register typical register allocation phase instruction reorder phase optimize buffer locality exploit instruction parallelism ILP timing characteristic dram virtual dependency enhance buffer locality critical performance program summary compilation advantage customize schedule generate program exploit ipim hardware feature pgsm backend optimization improve performance program backend optimization detail novel instruction optimization developed backend goal backend compilation generate efficient ipim executable program input halide module backend decouples program generation instruction lower translates halide module ipim instruction instruction optimization improve performance generate program detail optimization register allocation instruction reorder memory enforcement effectiveness backend optimization quantitatively analyze sec vii register allocation goal register allocation assign physical register virtual register avoid instruction dependency due conflict physical register avoid conflict physical register algorithm depth register interference algorithm instruction reorder algorithm input dependency graph instruction output sequence instruction init instruction init outgo neighbour node node execution latency node vopt inst priority vopt vopt vopt vopt max vopt vopt graph attempt assign virtual register physical register recently input algorithm register interference graph built upon traditional liveness analysis virtual register building register interference graph convert register allocation graph algorithm avoid conflict physical register solely minimize physical register allocation architecture ipim core avoid hardware overhead traditional register allocation dependency instruction due conflict physical register pipeline stall instruction reorder although program generate register allocation already executable ipim reorder instruction program maximally exploit instruction parallelism instruction issue mechanism core dependency adjacent instruction pipeline stall therefore instruction reorder aim expose instruction parallelism hardware eliminates pipeline stall improves performance graph node instruction node dependency instruction develop instruction reorder algorithm traverse graph topological associate node timestamp estimation issue algorithm multiple instruction available schedule load instruction node instruction schedule update incoming outgo timestamp finally instruction schedule output sequence iterate graph traversal algorithm demonstrate algorithm complexity node instruction dependency graph memory enforcement addition data  instruction reorder image brighten  issue instruction dependency resource conflict prevent pipeline stall due resource contention dram issue dram load instruction consecutively consumes slot instruction queue logic instruction stall memory request queue longer dram access latency consecutive dram instruction occupy instruction queue impede schedule computation instruction dependency instruction queue prevent pipeline stall due throughput memory request queue insert dependency load instruction instruction defer schedule consecutive memory instruction image brighten pipeline dram access latency varies buffer buffer dependency enforce memory access dram input program newly dependency avoid pipeline stall due dram request queue contention improves locality buffer originally data access locality image tile dependency instruction generate instruction dependency graph instruction reorder stage VI integration ipim standalone accelerator address host cpu memory standalone avoid complexity overhead virtual memory cache coherence introduces extra communication traffic host pim accelerator offset benefit pim ipim integrate host cpu standard bus pcie  chip  link HMC vii evaluation experimental setup methodology sec vii performance ipim sec vii sec vii demonstrate advantage ipim effectiveness decouple execution architecture II image processing benchmark setting category benchmark description stage benchmark image brighten gaussian blur blur blur blur blur blur downsample upsample shift histogram RDom width height histogram multi stage benchmark bilateral grid bilateral grid filter smooth image preserve pipeline stage interpolate interpolates pixel pyramid resolution sample pipeline stage local laplacian image enhances local contrast multi pipeline stage stencil chain compose chain stencil computation pipeline stage sec vii instruction breakdown benchmark sec vii benefit ipim compiler optimization conduct series comparative evaluation conclude ipim compiler optimization optimal achieve hardware utilization instruction per cycle ipc experimental setup benchmark dataset selection detailed II stage multi stage benchmark depth comprehensive analysis stage benchmark computation memory important image processing operation elementwise stencil reduction shift data dependent operation isolated depth analysis image processing operation multi stage benchmark widely image processing program heterogeneous pipeline stage programmability divk dataset contains image diverse scene content resolution evaluate benchmark choice resolution dataset reflect application trend workstation datacenter training medical image processing geographical information image quality hardware configuration ipim assumes 3D stack memory configuration previous accelerator without dram core timing detailed hardware configuration latency consumption dram setting important timing parameter limit   AW ipim contains ipim cube tesla gpu HBM stack HBM stack consumes footprint simulation methodology develop cycle accurate simulator extend ramulator integrate ipim hardware configuration PARAMETERS parameter configuration cube vault pgs PEs   simd len CAS width link width   DataRF pgsm vsm byte tck tRCD tCCD  trp tRAS      sub       hop  hop RD WR pre  DataRF access simd int alu access  tsv  dram  policy dram schedule FR FCFS throughput speedup comparison ipim gpu  compute logic buffer dram ipim frequency 1GHz technology node cactus  evaluate inter PE interconnects tsv 3D dram access latency performance address data register file vault scratchpad memory simulated cactus   previous data processing hardware component simd integer ALUs synthesize compiler derive performance evaluate component dram conservatively assume overhead reduce layer dram core logic adopt cortex core evaluate gpu evaluation baseline image processing workload halide manually tune schedule gpu performance nvprof nvidia smi respectively performance efficiency thermal issue performance ipim achieves average speedup gpu hardware speedup mainly attribute ipim ample memory bandwidth architecture comparison sec vii software speedup achieve compiler optimization analysis sec vii explain variation speedup benchmark brighten benchmark consists elementwise operation completely bound memory IV evaluation ipim  dram  dram overhead overhead simd int alu address register file data register file memory controller pgsm bandwidth ipim enormous bandwidth speedup histogram benchmark involves data dependent computation inferior performance halide default schedule gpu schedule ipim convert reduction parallel reduce partial histogram achieves significant performance improvement blur stencil chain benchmark moderate speedup respectively ipim later analysis sec vii sec vii benchmark computation intensity benchmark involve index calculation bound address register file conclusion ipim effectively accelerate image processing application efficiency ipim achieves average gpu mainly reduction expensive data movement gpu ipim compute logic local without chip data access sec vii detailed breakdown overhead data movement ipim benchmark approximately proportional speedup ipim increase bandwidth data access contributes reduction data movement explain difference stage benchmark multi stage benchmark respectively ipim employ compute schedule intermediate data pipeline without fuse comparison halide employ pipeline fusion multi stage benchmark gpu expensive chip memory access reduce due increase chip data reuse ipim slight multi stage benchmark ipim decouple execution architecture efficient overhead execution per dram core logic evaluate execution component pim layer dram overhead normalize dram per dram programmability accord IV evaluate ipim core logic core consumes silicon footprint comparison ipim gpu comparison vsm extra vault logic contrary core  integrate overhead per dram increase decouple execution thermal issue ipim peak per cube dram logic peak density normal operating HBM dram conservatively assume dram operates prior 3D pim thermal analysis active effectively satisfy thermal constraint  active peak density server active peak density previous pim logic concentrate logic sink ipim distributes pim logic evenly dram dissipation addition majority peak induced simultaneously activate precharging dram ipim compiler optimizes buffer locality image processing workload memory intensive workload ideal buffer locality frequency activity relatively architecture analysis comparison ipim ipim  ipim average achieves speedup breakdown ipim program explain  configuration advantage ipim  difference  ipim component logic component access dram TSVs evaluate  benchmark simulator serialize data traffic TSVs logic dram inferior performance  memory access TSVs limited bandwidth ipim peak memory bandwidth overhead  induced expensive cube data movement ipim local access argue impractical  memory bandwidth ipim increase TSVs increase tsv overhead translate overhead per dram breakdown detailed breakdown ipim program dram contains background activation precharge RAS CAS refresh  contains float integer operation simd  DataRF GSM contains leakage others contains data movement core logic breakdown ipim decouple execution architecture spends pim data movement core justified instruction breakdown analysis sec vii consumption inter vault intra vault data movement contribute ipim architecture localize data movement benefit memory hierarchy compiler optimization sensitivity analysis conduct sensitivity register per PE RF scratchpad pgsm impact execution RF sensitivity RF normalize execution RF RF RF RF performance RF respectively performance attribute decrease register sensitivity register scratchpad instruction breakdown ipim program register spill local dram increase register data dependency pgsm sensitivity pgsm KB KB normalize execution pgsm KB pgsm KB RF KB performance pgsm KB respectively reduce scratchpad increase access latency dram RF pgsm KB tradeoff performance overhead IV instruction breakdown instruction breakdown ipim program benchmark combination instruction varied ratio programmability perspective indicates simb ISA efficiently heterogeneous pipeline stage exhibit diverse computation data movement therefore simb ISA flexible image processing application index calculation instruction average instruction image 2D memory abstraction physical memory assumes linear address frequent index calculation 2D image reference location correspond memory address index calculation overhead effectiveness ipim compiler optimization instruction blur shift histogram bilateral grid stencil chain benchmark explanation ipim moderate speedup benchmark histogram another important observation inter vault data movement instruction instruction confirms image processing kernel data parallelism efficiently mapped onto architecture global data movement compiler analysis effectiveness compiler optimization comparative evaluation justify performance benefit ipim compiler optimization summarize optimization choice register allocation policy determines minimum physical register min scatter register avoid dependency instruction max instruction reorder option determines reorder instruction program generate register allocation stage memory enforcement option chooses dependency adjacent memory request dependency graph instruction reorder stage optimize opt adopts max register allocation policy applies instruction reorder memory enforcement ıve baseline baseline assumes min register allocation policy without instruction reorder baseline baseline baseline compiler optimization option opt specifically baseline min register allocation policy baseline apply instruction reorder baseline enforce memory setting baseline remain opt ipim compiler optimization overall speedup opt baseline analysis max register allocation speedup min register allocation opt baseline ipim core expensive register rename mechanism execution max register allocation optimally eliminate output dependency anti dependency prevent issue stall later instruction instruction reorder speedup opt baseline expose instruction ipim component utilization ipc parallelism overlap instruction without dependency enforcement memory speedup opt baseline maximally interleave instruction memory access request improves buffer locality ipc utilization analysis ipc core utilization hardware component pim average ipc achieves intensive compiler optimization implies ipim currently attains optimal performance improvement upper bound assume pipeline stall detailed analysis benchmark hardware component utilization benchmark intensive index calculation blur shift histogram bilateral grid stencil chain realize utilization address register file conclusion ipc hardware utilization compilation optimize image processing application ipim architecture related image processing accelerator previous explore programmable gate array fpga coarse grain reconfigurable array cgra ASIC image processing acceleration accelerator target mobile embed platform efficiency latency primary goal optimization application scenario mostly image application spatially distribute data architecture adopt entire image processing pipeline achieve desire efficiency buffer widely exploit producer consumer data locality fuse pipeline stage intermediate data chip without expensive chip memory access comparison ipim focus data workstation environment complex algorithm pipeline due resolution image memory capacity bandwidth conventional compute centric accelerator suffer memory bandwidth bottleneck ipim architecture abundant bandwidth resource tackle challenge memory pim accelerator ipim previous practical dram technology without invasive modification dram circuitry category research integrate processor core dram suffers overhead ipim solves challenge propose execution decouple approach core logic execution closely integrate category research adopts 3D logic architecture bound tsv bandwidth available logic ipim evaluation speedup approach improve memory bandwidth employ architecture limited fix functionality cannot heterogeneous image processing pipeline diverse computation memory ipim proposes simb ISA compilation programmability challenge addition recent proposes integrate computation logic dram DIMM module enable overhead data processing practical architecture bandwidth improvement CPUs 3D pim exploit pim architecture non volatile memory nvm technology dram endurance nvm critical image processing application intermediate pipeline memory IX conclusion proposes ipim programmable inmemory image processing accelerator architecture ipim decouple execution architecture lightweight programmability contains novel simb instruction multiple ISA enable various computation memory heterogeneous image processing pipeline addition develops compilation extend halide schedule ipim compiler backend contains optimization ipim register allocation instruction reorder memory enforcement evaluation ipim programmability overhead significant speedup gpu analysis demonstrates benefit ipim previous logic architecture effectiveness ipim compiler optimization