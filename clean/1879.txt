data accelerator NDAs integrate memory potential significant performance benefit fully realize benefit available memory capacity host NDAs permit regular memory access application accelerate others nda avoids copying data enables collaborative processing simultaneously performance host nda identify challenge context mitigate locality interference host NDAs reduce  overhead grain interleave host nda request architecting memory layout locality NDAs sophisticated address interleave host performance packetized traditional memory interface demonstrate approach simulated consists multi core cpu  ddr memory module mechanism enable effective efficient concurrent access microbenchmarks demonstrate potential important stochastic variance reduce gradient svrg algorithm introduction processing data memory data accelerator NDAs attractive application temporal locality arithmetic intensity NDAs perform computation data utilize proximity overcome bandwidth bottleneck memory bus despite decade research recent demonstration nda technology challenge remain NDAs practical context memory nda address outstanding issue context nda enable memory focus memory concurrently access nda memory memory powerful capability nda host processor collaboratively data without costly data prior research context limited grain nda operation cacheline granularity however develop technique  nda operation amortize host interaction across processing entire dram nda host memory access memory device directly host DDRx DIMM reduce access latency adoption illustrates exemplary nda architecture challenge address recently research memory NDAs DIMM memory capacity server memory DIMM compose multiple chip dram dice stack logic chip commodity  approach processing PEs memory controller logic PE access memory internally nda memory controller local nda access conflict external access host cpu rank access host cannot nda request bandwidth rank channel NDAs communication PEs host identical recent commercial nda enable memory exhibit overall characteristic surprisingly prior nda enable memory examines architectural challenge simultaneous concurrent access memory device host NDAs address challenge enable performance efficient NDAs memory concurrent access performance host NDAs challenge interleave access hurt memory performance decrease  locality introduce additional turnaround penalty challenge nda kernel consume entire array data operation local PE memory chip therefore enable cooperative processing host physical address mapped memory location channel rank etc achieves host access performance effective complex interleave maintains nda locality across operand kernel challenge exist packetized interface  controller interleaf access NDAs host traditional host memory controller sends explicit memory command challenge manage concurrent access identify reduce buffer locality interleave host request interfere nda performance contrast increase turnaround frequency acm annual international symposium computer architecture isca doi isca 3D chip 3D chip 3D chip DIMM logic 3D chip 3D chip 3D chip DIMM 3D chip 3D chip 3D chip DIMM 3D dram chip 3D dram chip 3D dram chip DIMM cmd addr data FSM PE MC dram buffer exemplary nda architecture  nda writes mainly interfere host context develop partition scheme limit interference memory host NDAs enable  host task task NDAs scheme compatible advanced memory interleave function recent processor partition mitigates interference host NDAs substantially boost performance interference rank opportunistically issue nda memory command rank briefly host curb nda host interference mechanism throttle nda request selectively predict conflict  prediction stochastically challenge nda operand locality enable grain collaboration architecting data layout preserve locality operand within distribute NDAs simultaneously afford parallel access highperformance host layout minor modification memory controller utilizes coarse grain allocation physical frame OS memory allocation combination allows array shuffle across memory device associate NDAs coordinate manner remain align nda crucial coarse grain nda operation achieve performance efficiency cacheline orient grain NDAs additional important challenge exists host maximizes memory performance directly memory device rely packetized interface nda capability local memory controller memory addition host introduces coordination challenge coordinate memory controller ensure consistent timing minimal signal impact performance replicate controller finite machine FSMs nda host memory channel replicate FSM nda access nda operation host controller host memory operation explicit signal NDAs host therefore non packetized NDAs nda operation deterministic access operand arbitrarily grain introduce chopim SW HW holistic enables concurrent host nda access memory address challenge temporal access interleave physically memory device perform detailed evaluation host nda task data collaborate application demonstrate chopim enables nda memory throughput  bandwidth maintain host performance performance scalability prior approach partition rank coarse grain temporal interleave grain nda operation demonstrate potential host nda collaboration machine application logistic regression stochastic variance reduce gradient descent application host NDAs host stochastically update tight inner loop utilizes speculation locality mechanism cpu NDAs concurrently compute correction across entire input data algorithm converge faster collaborative parallel nda host execution application host execution non concurrent host nda execution evaluate impact  accelerate application host task summary contribution identify challenge concurrent access memory host NDAs conflict host access curb nda performance turnaround penalty nda writes host performance reduce conflict partition architecture compatible sophisticated memory interleave decrease turnaround overhead throttle nda writes mechanism rank prediction delay nda writes rank actively cpu stochastic issue throttle nda writes randomly configurable rate develop memory data layout compatible host NDAs enable collaboratively data parallel maintain host performance sophisticated memory address interleave potential collaboratively processing data conduct important ML algorithm leverage cpu training loop BW NDAs summarization entire dataset develop variant executes NDAs cpu parallel increase speedup II background dram memory compose memory channel independently memory channel memory module DIMMs command address data bus DIMM usually compose physical rank chip rank chip rank compose multiple independent access target another already target conflict increase access latency dram protocol specifies timing parameter protocol access dram manage per channel memory controller address mapping memory controller translates  physical address dram address compose index channel rank typically memory controller policy address mapping minimize access latency interleave address across channel granularity beneficial access independently rank interleave coarse granularity switch rank channel incurs penalty addition xor hash mapping function channel rank address maximally exploit parallelism minimizes conflict multiple access access hash function shuffle address accomplish address along channel rank address turnaround interleave dram transaction incurs latency issue transaction issue transaction immediately suffers particularly penalty memory controller issue command load data bus  cycle data transfer tbl cycle dram device command issue  cycle guarantee conflict IO circuit dram penalty stem actual happens transaction whereas happens issue penalty nda data accelerator processing memory overcome physical constraint limit host memory bandwidth host peak memory bandwidth channel peak bandwidth per channel nda access memory channel potentially increase overall bandwidth memory module multiple rank bandwidth module available channel similarly multiple dram bandwidth available dram chip however NDAs BW advantage access data local memory data layout crucial performance naive layout frequent data movement NDAs host baseline nda architecture target NDAs integrate within capacity memory module role memory accelerator balance specifically baseline nda device  within dram chip module DIMM operation description operation description  dot  NRM axpy  gemv xmy nda operation application chopim limited operation  ddr logic DIMMs capacity predictable memory access characteristic DIMM PEs chip PEs within alternatively NDAs utilize highbandwidth device hybrid memory cube HMC bandwidth memory HBM internal bandwidth limited capacity due numerous connection memory controller HMC capacity via network access latency HBM HBM device standalone accelerator memory coherence coherence mechanism host NDAs prior nda chopim therefore focus coherence exist coherence approach explicitly infrequently copying amount data cache bypassing memory fence address translation application NDAs virtual physical address translation prior proposes address translation within NDAs enable independent nda execution without host assist increase nda complexity alternative nda operation constrain access data within physical memory contiguous virtual address hence translation perform host target nda command physical address propose grain nda operation within cache nda operation within virtual memory host translation complexity bound within NDAs protection nda workload focus nda workload host inherently cannot outperform nda exhibit temporal locality arithmetic intensity bottleneck peak memory bandwidth offload operation nda mitigate bandwidth bottleneck leverage internal memory module bandwidth moreover workload typically logic computation integrate logic within dram chip module practical overhead fundamental linear algebra matrix vector operation satisfy criterion dense vector matrix vector operation prevalent machine primitive particularly candidate deterministic regular memory access arithmetic intensity prior load matrix vector operation workload utilize memory BW propose perform elementwise vector reduction operation learningbased recommendation NDAs focus accelerate dense matrix vector operation summarize demonstrate evaluate svrg application IV concrete contribution generalize nda operation nda execution graph processing propose graph processing bottleneck peak memory bandwidth temporal spatial locality graph processing innovate context chopim develop chopim goal enable grain interleave host nda memory request physical memory device mitigate impact contention permit coarse grain nda operation vector instruction kernel simultaneously locality NDAs sophisticated memory address interleave host performance integrate packetized interface traditional host DDRx interface detail summarize approach grain access interleave opportunistic nda issue ideal nda opportunistically issue nda memory request whenever rank idle perspective host packetized interface memory controller schedule access challenge traditional memory interface host nda controller synchronize prior propose dedicate rank NDAs host coarse grain temporal interleave former approach contradicts goal device latter performance overhead cannot effectively utilize rank naturally idle due host access multi core application methodology VI majority idle shorter cycle vast majority cycle grain access interleave therefore rank idle breakdown idleness granularity coarse grain nda vector kernel operation grain access interleave nda command address cache memory grain nda operation indeed prior overhead  approach issue numerous nda command memory transaction occupies command data channel memory issue nda command frequently degrades host performance infrequent issue  NDAs coarse grain nda vector operation multiple cache mitigate contention channel improve overall performance vector width specify nda instruction operand contiguous dram address nda instruction numerous data without occupy channel  nda operation therefore desirable introduce data layout memory contention host nda synchronization challenge chopim solves localize nda operand distribute host access execute nda vector instruction operand nda instruction fully rank PE data rank prior launch nda instruction reuse rate data copying overhead dominate nda execution contention memory channel increase due command chopim data operand localize nda memory allocation challenge however host memory controller complex address interleave function maximally exploit channel rank parallelism arbitrary host access hence array contiguous host physical address contiguous physical memory shuffle across rank challenge illustrate operand nda instruction shuffle differently across rank layout approach array operand shuffle operand remain correctly align NDAs without operation alignment rank corresponds nda partition data layout across rank rely nda runtime OS combination coarse grain memory allocation ensure operand nda instruction interleave across rank local PE runtime allocate memory nda operand align granularity dram MiB ddr TiB address interleave mechanism aware ensures nda operand locally align rank align maintain rank alignment OS rank alignment explain feature intel skylake address mapping concrete representative interleave mapping mapping rank channel address partly frame offset partly physical frame pfn frame offset coarse grain alignment OS allocation pfn rank channel align physical address rank channel reverse engineer chopim runtime indicates request memory OS specifies operand instruction runtime operand minimize alignment baseline corresponds memory  multiple allocate focus address mapping approach linear address mapping described prior coarse grain allocation buddy allocator allocation granularity optimization already exist fragmentation overhead coarse allocation negligible coarse grain nda execution processing vector data layout across dram chip baseline byte strip across multiple chip whereas approach chip NDAs access local memory host NDAs access memory without copying  data prior memory align cache layout visible software data layout impact host memory controller ecc computation chip ecc protects interpret nda access rely dram ecc limited coverage innovate respect future mitigate frequent penalty memory access schedule policy chopim prioritize host memory request aggressively leverage  rank bandwidth issue nda request whenever NDAs incoming host request detect otherwise issue memory request maximize bandwidth utilization performance potential nda request issue cycle delay host request issue cycle otherwise NDAs infrequently issue command pre therefore prioritize host memory command rank rank channel naive data layout propose data layout vector rank rank channel vector rank rank channel vector rank rank channel vector nda access data layout across rank concurrent access operation naive data layout index rank propose mechanism index NDAs access contiguous vector RK BK BG col CH col offset offset physical address dram address pfn MB KB BG baseline skylake RK BK BG col CH BG col offset msb nda offset physical address dram address pfn propose partition baseline propose host address mapping nda command negligible impact nda performance transaction NDAs impact host command nda transaction however impact host performance turnaround penalty frequently host mitigates turnaround overhead buffering operation cache buffer host NDAs interleave transaction access memory parallel nda writes interleave host degrade performance introduce mechanism selectively throttle nda writes mechanism throttle rate nda writes issue predefined probability mechanism stochastic nda issue issue transaction NDAs detect rank idle flip coin issue adjust coin performance host NDAs issue probability frequent turnaround probability throttle nda progress throttle NDAs analysis profile therefore propose approach approach tune empirically rank prediction approach memory controller inhibits nda request host request controller stall nda lieu nda queue packetized interface memory controller schedule host nda request aware potential turnaround traditional memory interface however challenge host controller explicitly signal nda controller inhibit request signal ahead regular host transaction bus delay predictor inhibits nda request rank outstanding host memory request channel rank specifically nda controller examines target rank request host memory controller transaction queue signal NDAs rank stall writes assume information communicate dedicate pin develop signal mechanism piggyback exist host dram command later  memory scheduler host predictor achieves performance comparable tune stochastic issue approach partition host addition turnaround overhead concurrent access degrades performance decrease dram access locality host NDAs interleave access frequent conflict avoid contention propose partition limit interference memory concurrently data NDAs host particularly useful colocation scenario subset host task utilize NDAs however exist partition mechanism incompatible sophisticated dram address interleave scheme partition relies OS assign core thread isolated host OS frame physical address dram address mapping baseline mapping belongs offset prior partition scheme granularity importantly MiB baseline mapping cannot partition overcome limitation propose interface partition host reserve flexible dram address mapping specifically mechanism significant physical address dram address recent hash mapping function without loss generality assume reserve data OS split physical address host memory host occupy address capacity capacity reserve data OS purpose guarantee significant  address host contrast address  OS informs memory controller reserve memory  memory address mapped dram location hardware mapping function expose software OS remap address reserve address host additional logic dram address ID initial mapping reserve dram address dram address mapped reserve  swap  host address ID host IDs ID initial mapping address host cannot access initial mapping aliasing partition decision adjust affected memory global memory controller unlike conventional chopim enables architecture memory controller MCs manage timing rank host directly manage memory memory enhance NDAs coordinate rank information controller MCs memory channel global memory controller information host transaction easily obtain nda MCs monitor incoming transaction update accordingly however host MC cannot nda transaction due command bandwidth limit replicate finite machine FSMs NDAs host nda controller nda instruction launch FSMs synchronize rely  ddr interface FSM synchronization whenever nda memory transaction issue hostside FSM update host MC without communicate NDAs host transaction nda transaction rank transaction visible FSMs replicate FSMs nda buffer occupancy detect buffer drain trigger throttle RK RK rank RK rank nda FSM nda FSM nda FSM nda controller host memory controller update access RK nda FSM host nda sync ddr interface host memory scheduler global MC host NDAs issue memory command replicate FSMs synchronize ddr interface outer loop summarize inner loop sample cpu mem mem mem nda nda nda llc collaboration host NDAs svrg overhead replicate FSMs negligible byte microcode byte register per rank per nda evaluation approach enable ddr nda enable memory rely IV host nda collaboration potential concurrent host nda execution collaboratively processing data partition ML training task host NDAs processor leverage strength training data processing task vast majority data simplify parallelism machine technique logistic regression stochastic variance reduce gradient svrg simplify version svrg opportunity collaboration algorithm consists task within outer loop iteration entire input matrix summarize vector pseudocode vector correction update model task task consists multiple inner loop iteration inner loop iteration model randomly sample vector input matrix correction model update outer loop iteration task excellent NDAs summarization operation exhibit reuse traverse entire input data contrast task tight inner loop host host maximally exploit locality capture cache NDAs leverage bandwidth access entire input data svrg epoch refers inner loop iteration tradeoff svrg summarization frequently quality correction increase consequently per convergence rate increase overhead summarization increase perform frequently offset improve convergence rate therefore epoch hyper parameter determines frequency summarization carefully optimize tradeoff delayed update svrg chopim enables concurrent access host NDAs explore algorithm leverage collaborative parallel processing instead alternate summarization model update task parallel host NDAs whenever NDAs compute correction host NDAs exchange correction date parallel execution parallel execution faster stale epoch tradeoff delayed update svrg per iteration improve overlap execution whereas convergence rate per iteration degrades due staleness tradeoff prior later delayed update svrg converge serialize svrg task avoid delayed update svrg maintain private variable memory fence guarantee completion dram writes data exchange runtime coordinate polling bypass cache access data consume NDAs data exchange infrequently overhead amortize numerous nda computation delayed update host NDAs data without runtime api chopim whenever host nda concurrent access explanation evaluation concrete exemplary interface summarize command address signal pas nda memory controller host rank processing PEs logic access data local nda memory controller usage api compute average gradient summarization task svrg chopim runtime manages memory allocation launch nda operation nda operation default execute asynchronously programmer nda operation operand runtime insert appropriate data envision compiler 3D dram dice logic 3D dram dice logic 3D dram dice logic 3D dram dice logic 3D dram dice logic 3D dram dice logic host cmd info host cmd update nda ctrl host MC cmd data bus overview nda architecture identify intelligently allocate memory minimize implement compiler instead program directly interact runtime implement within simulator NDAs directly dram address perform address translation launch operation runtime OS translates origin operand physical address communicate along bound NDAs nda controller runtime responsible splitting api multiple primitive nda operation nda operation proceed operand regular access implement microcode hardware bound protection optimization load imbalance load imbalance occurs host access rank uniformly axpy operation launch repeatedly within loop non uniform access host load imbalance NDAs operation NDAs launch axpy reduces performance api asynchronous launch cuda openmp parallel  clause asynchronous launch overlap axpy operation multiple loop iteration load imbalance apparent loop load imbalance likely implement asynchronous launch macro nda operation macro operation loop parallel annotation launch nda operation nda operation launch similarly memory reserve access register NDAs nda packet access register launch operation packet compose operation address operand data scalar scalar vector operation host nda controller role accepts acceleration request issue command NDAs rank roundrobin manner notifies software request completes extends host memory controller coordinate action NDAs host memory controller enables concurrent access maintains replicate FSMs knowledge issue nda operation status void memory allocation float alpha lambda nda matrix float nda SHARED nda vector float nda SHARED nda vector float nda SHARED nda vector float nda SHARED nda vector float nda SHARED nda vector float pvt nda private pvt allocates per nda strip allocation across NDAs initialization average gradient nda gemv nda xmy host sigmoid nda xmy nda  target macro operation parallel int alpha nda axpy pvt alpha pvt alpha host reduce pvt nda axpy lambda average gradient code code corresponds summarization svrg IV batch batch batch batch reg reg   data KB buffer KB spm KB PE architecture execution axpy host memory controller execution processing exemplary PE compose float fuse  scalar register operand input temporary KB buffer access memory KB scratchpad memory memory access granularity per chip performance  per chip data access rate PEs optimize precision operation specialized specific explore focus capability chopim nda execution PE execute axpy operation vector partition KB batch dram per chip maximize bandwidth utilization vector buffer PE another byte vector FP register fuse  FMA operation execute buffer execution  operation pipelined buffer PE writes memory scratchpad KB batch batch entire PE microcode axpy operation operation coarse grain similarly microcode inter PE communication NDAs effective memory bandwidth amplify host DIMM chip NDAs target inter PE communication therefore equivalent communicate host communication application nda architecture primarily replicate data localize operand global reduction operation local per PE reduction communication global data layout therefore enable communication host instance macro operation global reduction PE private pvt accumulates data reduce nda operation replication communication data layout nda locality requirement nda operand communicate host expensive coarsegrained nda operation amortize infrequent communication overhead importantly communication normal dram access host memory interface VI methodology II summarizes configuration dram timing parameter component benchmark machine configuration partition reserve per rank NDAs host ramulator baseline dram simulator nda memory controller PEs execute nda operation modify memory controller skylake address mapping partition data layout scheme simulate concurrent host access gem ramulator host application various memory intensity spec spec benchmark suite application combination II extreme memory intensity respectively core simulate provision bandwidth core simulate realistic scenario nda workload dot operation impact extremely intensity average gradient kernel evaluate collaborative execution performance impact nda application dot svrg conjugate gradient CG streamcluster SC host workload simpoint representative program phase simulation instruction slowest instruction nda workload completes simulation  concurrent access configuration processor core OoO core 4GHz fetch issue width lsq rob nda PE per chip 2GHz fully pipelined buffer tlb tlb tlb associativity KB associativity LI L1D lru MSHRs KB associativity lru MSHRs llc MB associativity lru MSHRs stride prefetcher dram ddr 2GHz 8GB channel rank FR FCFS entry RD WR queue policy intel skylake address mapping dram timing parameter tbl    tcl tRCD trp  tRAS tRC    twr   tFAW component activate PE host PE FMA operation PE buffer dynamic access PE buffer leakage scratchpad memory PE buffer benchmark MPKI mcf lbm omnetpp gemsfdtd bwaves milc soplex leslied mcf lbm omnetpp gemsfdtd mcf lbm gemsfdtd soplex lbm omnetpp gemsfdtd soplex omnetpp gemsfdtd soplex milc gemsfdtd soplex milc bwaves soplex milc bwaves leslied milc bwaves astar  leslied leela deepsjeng  nda kernel nda operation svrg detail CG SC machine configuration logistic regression regularization classification rate tune momentum dataset cifar II evaluation parameter occurs throughout simulation instruction simulated instruction per cycle ipc host performance NDAs utilize bandwidth bandwidth utilization idealize NDAs utilize idle rank bandwidth estimate parameter II CACTI dynamic leakage PE buffer sensitivity PE parameter exhibit impact negligible CACTI  estimate 3D stack dram CACTI IO estimate DIMM vii evaluation evaluation various chopim mechanism analyze benefit coarse grain nda operation partition improves nda performance stochastic issue rank prediction mitigate turnaround impact nda workload intensity load imbalance chopim rank partition benefit collaborative parallel impact coarse grain nda operation axis cache access per nda instruction cpu nda processing efficiency rely replicate FSM enable ddr coarse grain nda operation demonstrates overhead launch nda instruction degrade performance host NDAs rank increase prevent factor conflict parallelism load imbalance affect performance BP mechanism NRM operation precisely granularity asynchronous launch memory intensive application host CBs nda instruction contention host transaction nda instruction launch decrease performance improves addition rank grows contention becomes severe nda instruction NDAs data layout enables coarse grain nda operation beneficial concurrent access situation takeaway coarse grain nda operation crucial mitigate contention host memory channel impact partition performance partition host NDAs emphasize impact intensity nda operation extreme dot intensive intensive operation svrg roughly memory access mode idealize assume host access memory without contention NDAs leverage idle rank bandwidth without transaction overhead overall accelerate intensive dot concurrent host access affect host performance significantly aggressive approach however contention access mode significantly degrades nda performance extra conflict interleave host nda transaction accelerate intensive degrades host performance happens phase NDAs nda buffer drain host NDAs issue transaction due turnaround mitigate impact throttle mechanism host performance despite concurrent access memory stochastic issue rank prediction impact core contention llc increase memory performance dominates overall performance takeaway partition increase buffer locality substantially improves nda performance intensive nda operation mitigate nda interference impact mechanism intensive nda operation intensive operation execute NDAs mechanism apply phase nda execution stochastic issue probability clearly host nda performance tradeoff  prediction stochastic issue tradeoff host nda performance NDAs issue probability host performance degrades appropriate issue probability chosen heuristic host memory intensity explore rank prediction mechanism slightly behavior stochastic approach stochastic issue probability host nda performance stochastic issue extends tradeoff signal robust rank prediction approach takeaway throttle nda writes mitigates impact turnaround interference host performance rank prediction robust effective stochastic issue additional signal impact intensity input host nda performance nda operation execute input host application memory intensity rank prediction mechanism addition identify impact input vector KB rank medium KB rank MB rank evaluate asynchronous launch vector evaluate gemv matrix impact nda operation operand rank partition chopim scalability chopim rank partition vector fix overall performance inversely related intensity execution per launch nda performance NRM operation input shortest execution execution NRM highly impact launch overhead load imbalance concurrent host access gemv executes longer operation impact load imbalance launch overhead asynchronous launch optimization impact load imbalance decrease nda bandwidth increase takeaway asynchronous launch mitigates load imbalance duration nda operation scalability comparison chopim performance rank partition RP RP assume rank evenly partition host NDAs intensive nda operation trend application svrg CG SC demonstrate performance extreme memory intensive host workload cluster performance baseline dram intensive nda workload chopim performs rank partition opportunistically exploit idle rank bandwidth option dedicate rank acceleration cluster performance rank rank partition chopim performance scalability nda bandwidth rank partition exactly chopim due increase idle per rank svrg extreme dot takeaway chopim rank partition issue opportunity rank svrg collaboration benefit convergence without nda NDAs convergence without nda nda speedup normalize host impact nda summarization svrg without delayed update HO host acc accelerate NDAs acc acc option memory enable concurrent access data rank prediction mechanism host optimal epoch decrease NDAs overhead summarization decrease relative host furthermore svrg delayed update gain additional performance demonstrate benefit concurrent host nda access portion workload delayed update update correction frequently perform rate acc epoch impact staleness delayed update nda performance grows NDAs additional rank delayed update svrg demonstrates performance scalability performance tune serialize delayed update svrg host NDAs performance training loss converge away optimum NDAs calculate correction faster staleness decrease consequently rate faster convergence takeaway collaborative host nda processing data svrg logistic regression memory estimate dissipation memory concurrent access theoretical maximum memory host access memory memory intensive application execute average maximum NDAs dissipate scratchpad memory maximally average gra  computation dissipate memory maximum host access efficiency NDAs internal memory access chopim minimizes overhead takeaway operating multiple rank concurrent access increase memory significantly related knowledge proposes data acceleration enable concurrent host nda access without data reorganization non packetized dram context packetized dram scalable suffer latency longer ddr protocol load unique previous influence data acceleration conduct relative data access becomes expensive computation computation dram crossbar emerge technology benefit  acceleration bandwidth data transfer benefit becomes computation closer memory however constraint significant restrict complex logic workload alu operation target 3D stack memory device enable complex logic logic exploit internal memory bandwidth recent conduct device accelerate diverse application however proposal memory role memory device gain attention acceleration prior attempt host nda access data data reorganization packetized dram context potential concurrently host NDAs memory however assume idealize memory contention nda host memory request assume ideal contribution chopim precisely mechanism mitigate interference nda chameleon mcn DIMM conventional DIMM device dram practically PEs unlike rank partition coarse grain mode switch prior host PEs rank maximize parallelism partition decrease contention IX conclusion introduce rank enable concurrent access host NDAs instead partition memory coarse grain manner temporally spatially interleave access grain manner leverage  rank bandwidth maximize bandwidth utilization chopim enables coordinate memory controller host NDAs overhead reduce extra conflict partition efficiently nda transaction stochastic issue rank prediction mitigate penalty turnaround data layout allows host NDAs access data realize performance collaborative execution host NDAs performance chopim insight practically enable nda memory request enables effective acceleration eliminate data encourage tighter host nda collaboration