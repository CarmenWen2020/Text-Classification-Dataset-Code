associative processing AP promising pim paradigm overcomes von neumann bottleneck memory virtue radically execution model decompose arbitrary computation sequence primitive memory operation AP execution model concurrent simd computation situ memory array eliminate data movement execution model native flexible data minimal modification exist memory hardware complexity despite advantage execution model AP limitation substantially increase execution operation perform operation operation propose highly performant associative processor  fully address aforementioned limitation core hyper AP enhance execution model reduces operation computation thereby reduce execution execution model generic improves performance CMOS RRAM AP beneficial  AP due substantially reduce operation architecture micro architecture optimization efficiently implement hyper AP reduce program complexity develop compilation framework user program constraint application hyper AP optimization apply compilation exploit unique hyper AP experimental recent imp  achieves efficiency various representative arithmetic operation evaluate benchmark hyper AP achieves speedup reduction average imp evaluation confirms propose execution model beneficial RRAM AP CMOS counterpart introduction associative processing AP promising  memory pim paradigm tackle von neumann bottleneck memory virtue radically execution model comparison von neumann model processor input data memory writes computation afterwards AP execution model performs simd computation situ memory array eliminate data movement concurrently operating data AP natively arbitrary computation parallel serial manner lookup generate computation addition comprises input correspond computation AP performs sequence consecutive operation input parallel correspond computation input operation perform computation determines execution proportional input perform computation memory operation AP largely reuses exist memory peripheral circuit amplifier minimize modification memory exist pim integrate non conventional memory circuit adc DAC memory array increase hardware complexity moreover due  operation AP natively flexible data arbitrary width allows programmer exploit benefit narrow precision data custom data AP widely explore pim capability superior flexibility massive parallelism however AP performance fundamentally limited execution model limitation execution model AP input operation input lookup grows exponentially respect computation complexity data width perform operation complex computation execution model perform operation operation  significant operation limitation substantially increase execution implementation emerge non volatile memory technology RRAM latency moreover perform input limitation parallelism ratio entry parallel entry acm annual international symposium computer architecture isca doi isca therefore cannot fully utilize massive parallelism AP propose hyper AP highly performant AP substantially improve performance traditional AP series innovation abstract machine model execution model architecture micro architecture compilation framework hyper AP abstract machine model enhance execution model fully address aforementioned limitation address limitation leverage  encode technique enhance capability AP execution model simply reuse encode technique extend technique hyper AP multiple input operation  substantially reduces operation operation limitation address limitation abstract machine model enhance execution model accumulate perform operation multiple operation multi reduce operation moreover improves parallelism perform accumulate multiple input architecture microarchitecture implementation hyper AP RRAM technology additional optimization technique apply improve performance hyper AP logical unified physical array apply implement propose abstract machine model monolithic array previous array nearly latency operation reduce execution moreover latency communication interface memory array reduce synchronization detail described IV finally reduce program complexity develop compilation framework hyper AP user program constraint application hyper AP compilation framework automatically generates optimizes lookup data layout input program optimization technique developed reduce operation compilation framework merge lookup eliminate operation intermediate computation moreover immediate operand embed lookup constant propagation reduce input thereby reduce operation detail optimization technique contribution propose hyper AP abstract machine model enhance execution model address limitation traditional AP specifically hyper AP performs simd computation multi multi manner reduce operation  traditional AP architecture micro architecture implementation hyper AP RRAM technology logical unified physical array propose reduce latency latency communication interface reduce synchronization develop compilation framework reduce program complexity user program constraint application hyper AP compilation framework improves computation efficiency merge lookup eliminate operation intermediate embed immediate operand lookup reduce operation evaluate hyper AP synthetic benchmark representative benchmark application comprehensive comparison recent imp pim architecture built dot capability RRAM crossbar array evaluation hyper AP outperforms imp due parallelism flexible data additional optimization technique specifically representative arithmetic operation hyper AP achieves improvement throughput efficiency efficiency respectively imp hyper AP achieves speedup reduction average representative kernel imp evaluation confirms propose execution model improve performance CMOS RRAM AP beneficial  AP reduction operation operation RRAM AP highly asymmetric latency  tsearch CMOS AP  tsearch organize II background information describes abstract machine model execution model hyper AP IV instruction architecture ISA micro architecture hyper AP describes program interface custom compilation framework VI evaluation vii hyper AP prior conclude II background motivation brief concept AP originate recognize promising compute model von neumann model development AP underwent important stage foster laid foundation AP comprehensive overview related theory algorithm application  effort AP prototype    aerospace corporation developed associative program model however excitement  due limitation traditional AP AP parallel machine model inherently traditional von neumann model von neumann model processor input data memory writes computation afterwards data movement occurs bus memory hierarchy impose significant latency penalty conversely AP operates concurrently data eliminate data movement massive parallelism application  parallelism suitable workload AP mask tag tag tag reduction cam tag parallel tag parallel correspond mask abstract machine model traditional AP drawn illustrate operation selectivity enable mask register illustrate reduction drawn simplicity depicts abstract machine model traditional AP building content addressable memory cam array purpose register mask tag reduction specifically cam contains array organize register cam mask register defines active operation enable selectivity tag register comparison logic upon combination mask tag parallel finally reduction comparison perform operation simd slot motivate AP performs computation serial parallel manner illustrate addition vector cin vector sum vector cout vector wise cam array output vector initialize zero lookup generate addition sequence operation perform computation specifically input lookup content correspond parallel tag computation logic correspond sum cout tag traditional AP operation addition complex operation multiplication perform input computation cin sum cin sum cin sum cin sum cout cin cout cin cout     tag tag tag tag cam operation perform operation traditional AP input cin computation sum input cin computation sum input cin computation sum input cin computation sum computation cout input input cin computation cout input cin computation cout operation addition illustrate simd computation perform traditional AP correspond data layout lookup sequence operation drawn limitation traditional AP execution model traditional AP limitation outweigh benefit obtain massive parallelism execution model input operation input lookup exponentially increase respect computation complexity data width etc limited capability operation execution complex computation another limitation perform operation operation  operation limitation parallelism ratio tag perform input tag ternary content addressable memory TCAM subsection briefly describes  TCAM architecture hyper AP depicts structure crossbar TCAM  bidirectional diode RRAM intersection SL ML adjacent  TCAM adjacent SLs operation MLs precharged  become float SLs driven VH VL input mask voltage carefully  VH VL difference  VL diode consequently mismatch discharge ML discharge ML amplifier discharge generate scheme RRAMs diode effectively suppress leakage unselected sneak detail  TCAM architecture chip demonstration mask RRAM diode ML SL  TCAM TCAM resistance mask discharge resistance structure crossbar TCAM conceptual diagram illustrate operation ML ML mismatch conceptual diagram illustrate operation hyper AP execution model propose hyper AP abstract machine model enhance execution model address limitation traditional AP II abstract machine model modification traditional cam array replace ternary content addressable memory TCAM array additional denote input moreover register modify additional input denote input tag finally accumulation tag register perform logic function correspond tag register enable abstract machine model hyper AP enhance execution model fully address limitation traditional execution model specifically enable TCAM ternary register extend  encode technique enhance capability address limitation encode technique propose improve performance TCAM peripheral circuit encode nevertheless TCAM enhance encode technique operation extend encode technique propose execution model multiple input operation multi instance encode XX XX XX XX encode encode translate thereby input operation enhance capability hyper AP operation addition data layout drawn lookup operation traditional AP reduction achieve complex computation addition worth hyper AP without encode cin non encode traditional AP mask reduction TCAM accumulation ternary acc acc acc acc tag tag tag tag acc acc acc acc accumulation disabled input acc acc acc acc accumulation enable input acc acc acc acc input tag tag mask abstract machine model hyper AP modification highlight input input accumulation performs logic function input address limitation enhance execution model performs operation multiple operation multi enable accumulation instance operation sum perform input related sum reduces operation traditional AP reduction achieve complex computation addition moreover operation perform accumulate multiple input propose exe  model achieves parallelism traditional AP improvement propose execution model addition operation traditional execution model mask encode encode encode operation input cin cin input cin cin computation sum input cin cin input cin input cin computation cout hyper AP multiple operation perform multiple operation additional encode cin without encode multiple encode  encode technique extend encode technique hyper AP multiple input operation operation addition IV hyper AP architecture instruction architecture ISA utilized custom compilation framework construct arbitrary arithmetic logic computation flexible data micro architecture implement abstract machine model hyper AP instruction architecture propose ISA contains instruction grouped category compute instruction perform associative operation data manipulate instruction data register instruction mask register synchronize execution functionality instruction described instruction register active define mask register parallel comparison tag register operation accumulation enable acc otherwise accumulation disabled acc moreover encoder encode instruction TCAM address col tag encode instruction sequentially encode generate encoder TCAM col col encode instruction cycle writes TCAM cycle decode address cycle register cycle RRAM cycle writes TCAM cycle decode address cycle register twice cycle RRAM instruction architecture hyper AP category opcode format cycle byte compute acc encode col encode  imm index  dir data  addr variable manipulate writer addr imm variable   broadcast mask cycle variable   instruction mask register accord imm imm mask specifically mask correspond imm mask correspond input instruction performs population operation return tag index index instruction performs priority encode operation return index tag   instruction data register PE data register adjacent PE direction dir specifically data register PE dir   instruction data register PE addr data buffer controller writer writer instruction immediate imm data register PE addr   instruction data register PE tag register PE   instruction tag register PE data register PE broadcast broadcast instruction specify mask mask register controller instruction execution cycle specify cycle instruction compilation framework synchronize execution synchronization realize instruction computation compute category deterministic latency cycle resolve offline compilation inst mem ctrl inst buffer data buffer host subarray subarray inst mem inst mem local PE PE PE data reg data reg data reg subarray mask acc tag encoder reduction TCAM PE instruction global data local data dispatch mask peripheral circuit hyper AP adopts hierarchical architecture comprises subarrays subarray contains PEs PE simd AP implement abstract machine model described microarchitecture PE micro architecture propose hyper AP adopts hierarchical architecture specifically hyper AP comprises subarrays subarray contains multiple PEs PE simd AP micro architecture PE grouped instruction memory dispatch thereby reduce overhead execute instruction simd exploit data parallelism DLP execute instruction exploit  parallelism ILP allows multiple application concurrently exploit task parallelism therefore hyper AP  architecture global data network data data communication data buffer addition adjacent data realize bandwidth  local data communication local controller subarray configures mask register peripheral circuit PEs instruction SA dff dff encoder dff SA WD WD SA dff dff encoder dff SA WD WD adder priority encoder accumulation tag tag logic logic logic SL ML diode RRAM register mask register WD WD CMOS circuit crossbar array crossbar array  logic physical logic WL WL BL BL BL BL DL DL DL decode logic WD PE comprises RRAM crossbar array implement TCAM array crossbar array contains  logic logic crossbar array monolithically 3D stack CMOS circuit without consume depicts micro architecture PE comprises RRAM crossbar array  contains diode RRAM implement TCAM array input register implement purpose mask register output register implement tag register gate multiplexer implement accumulation adder priority encoder implement reduction amplifier sas operation driver wds operation encoders encode logic decoder driver correspondence abstract machine model hyper AP critical circuit AP operation TCAM array architecture differs previous optimize reduce latency specifically previous RRAM crossbar array implement TCAM array TCAM  TCAM sequentially circuit reduce latency RRAM crossbar array implement TCAM array  TCAM array array circuit TCAM TCAM parallel halve latency employ RRAM crossbar array monolithically stack CMOS logic without consume balance performance crossbar array chosen TCAM array crossbar array simd slot operation decode logic SLs data mask register sas activate MLs detail mechanism crossbar array  generate associative operation wds SLs MLs voltage mask tag scheme apply effectively reduce leakage sneak improve performance reliability  SL parallel improve throughput compilation framework integration program interface hyper AP compilation framework optimization technique apply compilation finally briefly integration program interface program interface hyper AP maximally reuse exist development environment minimize effort program across platform reduce development hyper AP user program instruction apply data compilation framework automatically applies instruction onto multiple data realize data alignment simd computation constraint apply user program loop unrolled compilation loop iteration across data pointer chase operation conditional statement execute computation illustrate nevertheless minimize conditional statement divergence issue performance degradation gpgpu explore flexibility AP data unsigned int int bool integer data arbitrary width declare user structure user define custom data program variable unsigned int unsigned int unsigned int unsigned int declare unsigned int variable unsigned int perform addition return computation return instruction data vector vector vector compile apply multiple data data program interface hyper AP user program instruction data compilation framework applies instruction multiple data compilation depicts compilation framework hyper AP comprises dataflow graph DFG generation DFG cluster inverter graph aig generation lookup generation code generation optimization technique apply compilation function overload aig generation encode operation merge operand embed lookup generation DFG generation DFG cluster code generation lookup generation aig generation application rtl library machine code optimization function overload optimization encode operation merge operand embed compilation hyper AP DFG generation par application performs ensure correctness convert DFG develop custom DFG cluster DFG node cluster computation cluster perform simd slot PE illustrate goal minimize cluster thereby minimize data operation simd slot  AP due latency develop custom heuristic algorithm cluster DFG node cluster algorithm widely fpga compilation adapt compilation function equation cluster sum input cluster cluster input  cluster cluster cluster simd slot simd slot cluster cluster cluster data conceptual diagram illustrate DFG cluster operation computation cluster perform simd slot PE inter cluster data simd slot aig generation cluster generate replaces DFG node correspond rtl implementation netlist logic gate rtl library generate aig cluster rtl library contains rtl implementation various operation developed expert optimize performance developed user program OpenCL synthesis custom operation develop custom appropriate rtl implementation DFG node custom function overload capability reduce program complexity program specifically user operator function operand integer unsigned integer custom appropriate rtl implementation operation data precision operand lookup generation aig node cluster generates lookup cluster technology mapping stage fpga compilation generates input lookup LUTs aig nevertheless difference technology mapping stage input cluster limited limited hyper AP optimization goal technology mapping stage minimize critical thereby improve fpga operating frequency minimize entry lookup writes thereby reduce execution develop custom adapts heuristic algorithm fpga technology mapping stage modification apply algorithm limitation input cluster limitation generate cluster marginal performance improvement throughput substantially increase compilation limit input improves robustness operation moreover function equation cluster sum input cluster cluster lookup latency ratio latency operation operation define compilation framework generate optimal compilation AP implementation CMOS AP RRAM AP costi  optimization technique apply encode extend encode technique apply encode lookup entry pairing operation illustrate obtain optimal custom enumerates pairing operation chooses optimal minimum input limited operation merge cluster operation aig graph boundary DFG node merge multiple operation eliminate intermediate illustrate encode AC BD  AC BD  AC BD  AC BD  encode AB CD pairing operation operand embed embeds immediate operand lookup constant propagation optimization reduces operation thereby reduce execution correspond operation unique feature hyper AP operation fix execution compute device gpu previous pim processor imp code generation generates sequence  instruction allocates simd processor execute instruction insert broadcast instruction allocation instruction computation compute category deterministic latency calculate execution simd processor insert instruction synchronize execution simd processor develop custom sequence operation arithmetic computation cannot pas lookup input conditional statement unsigned int unsigned int unsigned int unsigned int unsigned int unsigned int unsigned int return without operation merge operation merge unsigned int unsigned int unsigned int unsigned int immediate operand return without operand embed operand embed merge operation reduces operation embed immediate operand lookup reduces operation integration hyper AP easily integrate hardware accelerator gpu fpga imp instance pcie apply performance interconnection integration host processor load data hyper AP computation instruction hyper AP perform computation VI evaluation evaluate performance  hyper AP synthetic benchmark benchmark rodinia benchmark suite confirm propose execution model beneficial RRAM AP evaluate performance improvement CMOS RRAM AP subsection experimental setup evaluation unsigned int unsigned int unsigned int unsigned int addition return encode additional allocate intermediate compile unsigned int unsigned int unsigned int  expr  expr return compile additional allocate intermediate sequence  expr sequence  expr encode encode sequence addition limitation input lookup actual implementation multiple lookup calculate output illustrate handle conditional statement experimental setup benchmark selection benchmark complexity evaluate propose hyper AP benchmark relatively synthetically generate evaluate computation capability hyper AP specifically benchmark comprise arithmetic operation perform simd slot inter PE communication peak compute performance throughput achieve hyper AP benchmark performance improvement obtain flexible data precision operation merge operand embed benchmark benchmark rodinia suite evaluate computation communication performance hyper AP benchmark recent imp baseline evaluation comparison convert float benchmark fix imp native data benchmark detail benchmark data baseline recent imp baseline comparison pim capability hyper AP perform purpose computation situ RRAM memory array evaluation gpu nvidia titan XP popular commercially available compute device massive parallelism finally traditional AP performance comparison illustrate effectiveness propose optimization technique impact technology CMOS RRAM II comparison gpu imp hyper AP parameter parameter gpu imp hyper AP simd slot frequency ghz mhz 1GHz TDP memory MB 1GB 1GB 2GB dram RRAM RRAM simulation setup hyper AP benchmark mapped compilation framework delay consumption obtain hspice simulation ptm HP model custom physical specifically verilog module simulate behavior RRAM device diode characteristic RRAM device ron  pulse reset operation voltage diode capacitance SL ML extract physical PE hyper AP executes instruction instruction latency deterministic inter PE communication local data fix latency performance accurately calculate compilation performance gpu imp baseline obtain reference gpu imp hyper AP distinct execution model architecture apply evaluation imp ensure performance comparison assumption gpu imp hyper AP integrate standalone accelerator input data preloaded memory gpu memory array imp hyper AP execution SA WD SA WD accumulation tag encoder physical PE technology configuration II summarizes important parameter evaluation hyper AP simd slot imp memory capacity simd slot hyper AP simd slot occupies imp moreover hyper AP parallelism consumption imp hyper AP hungry inefficient adc DAC gpu imp obtain reference hyper AP obtain custom physical physical PE PEs subarray mask register register local controller instead PE moreover register mask register traditional AP combination hyper AP combination  input without increase mask register monolithically stack RRAM crossbar array CMOS circuit PE minimize CMOS technology node imp physical parameter parasitic capacitance resistance circuit simulation obtain operating frequency consumption latency throughput GOPS efficiency GOPS efficiency GOPS mul div sqrt exp mul div sqrt exp mul div sqrt exp mul div sqrt exp memory access latency gpu hyper AP imp arithmetic operation latency latency throughput efficiency efficiency representative arithmetic operation unsigned integer improvement imp highlight report benchmark latency gpu contains chip memory access latency arithmetic operation obtain comparison benchmark latency imp hyper AP latency correspond arithmetic operation imp hyper AP perform computation inside memory array without access external memory effectively hide instruction decode dispatch latency overlap computation gpu throughput optimize effective hide memory access latency due core limited chip memory operation subsection latency throughput efficiency efficiency evaluate synthetic benchmark benchmark subgroup evaluate performance operation multiple exponential integer evaluate performance improvement obtain flexible data precision evaluate effectiveness operation merge operand embed technique evaluation subgroup hyper AP achieves improvement throughput imp hyper AP parallelism simd slot compensates relatively longer execution latency hyper AP due  operation hyper AP improves efficiency addition multiplication imp hyper AP hungry adc DAC device computation hyper AP achieves efficiency improvement complex operation hyper AP efficient shift wise logical operation iterative perform operation imp finally hyper AP improves efficiency imp hyper AP circuit amplifier perform computation imp  adc DAC computation mul div sqrt exp mul div sqrt exp mul div sqrt exp mul div sqrt exp latency throughput GOPS efficiency GOPS efficiency GOPS gpu imp hyper AP latency throughput efficiency efficiency representative arithmetic operation unsigned integer improvement imp highlight hyper AP flexible data precision imp computation integer hyper AP achieve performance improvement reduce data precision addition operation performance improvement achieve hyper AP imp linearly increase reduce precision additional improvement achieve data precision integer complex operation performance improvement achieve hyper AP quadratically increase reduce precision additional improvement iteration execution latency iteration reduce reduce data precision hyper AP additional optimization improve performance multiple operation merge improve performance operation intermediate eliminate merge addition improve  throughput TOPS  TOPS imp operation merge capability execution merge operation nearly operation latency throughput gap hyper AP imp increase reduce however operation merge capability imp realize consumption adc resolution contrary reduce operation hyper AP reduces consumption merge operation therefore hyper AP achieves efficiency imp operation multi mul div multi mul div multi mul div multi mul div latency throughput GOPS efficiency GOPS efficiency GOPS gpu imp hyper AP latency throughput efficiency efficiency consecutive addition unsigned integer multi computation immediate operand mul div improvement imp highlight moreover hyper AP improve performance embed immediate operand lookup reduce operation performance operation immediate operand average hyper AP achieve additional improvement latency throughput efficiency efficiency report exponential unary operation compute compiler immediate operand comparison application subsection rodinia kernel evaluate performance hyper AP achieve speedup average imp improvement fold hyper AP simd slot imp allows improve performance data duplication moreover hyper AP bandwidth latency inter PE communication interface adjacent PEs latency 2GB bandwidth optimize data layout local data substantially reduce communication simd slot  contrary although imp optimizes data layout router network communication simd slot thereby relatively synchronization imp relatively speedup backprop kernel hyper AP imp natively dot multiplication heavily kernel however realize hungry adc DAC device imp consumption average hyper AP kernel speedup normalize consumption gpu imp hyper AP speedup consumption rodinia kernel consumption hyper AP imp normalize gpu latency throughput GOPS efficiency GOPS efficiency GOPS gpu AP hyper AP AP hyper AP gpu AP hyper AP AP hyper AP gpu AP hyper AP AP hyper AP gpu AP hyper AP AP hyper AP accumulation TCAM array additional CMOS AP RRAM AP AP RRAM AP hyper AP RRAM hyper AP AP CMOS AP hyper AP CMOS hyper AP performance improvement propose hyper AP traditional AP RRAM CMOS implementation breakdown throughput improvement mainly additional traditional AP subsection addition representative operation evaluate performance improvement propose hyper AP traditional AP RRAM CMOS implementation propose hyper AP beneficial RRAM implementation  implementation specifically RRAM  improve latency throughput RRAM traditional AP CMOS hyper AP improve latency throughput CMOS traditional AP trend performance metric reduction operation hyper AP asymmetric operation reduce operation reduce ratio latency operation operation RRAM implementation  tsearch CMOS implementation  tsearch performance improvement obtain RRAM implementation although CMOS hyper AP latency RRAM hyper AP  hyper AP throughput TOPS RRAM TCAM CMOS TCAM storage density substantially increase PE CMOS AP implementation reduces simd slot breakdown throughput improvement mainly additional propose vii related associative processing recent advance emerge non volatile memory fundamentally lower barrier implement AP AP variant explore broadly application specific AP cam ternary cam TCAM AP parallel operation data content relies external ALUs processor perform arithmetic computation computation parallelism limited ALUs processor encode technique improve performance latency consumption limited capability  additional propose purpose AP fully implement execution model described II perform simd computation memory array however performance purpose AP fundamentally limited traditional execution model performs computation manner II  abstract machine model enhance execution model fully address limitation pim architecture propose domain specific pim architecture leverage inherent dot multiplication capability  crossbar array pim architecture perform computation analog domain hungry inefficient DAC adc device contrary  purpose pim architecture inefficient DAC adc perform computation imp purpose pim architecture leverage  multiplication capability RRAM crossbar array performs computation analog domain adc DAC efficiency  moreover flexible data integer contrary hyper AP various data achieve performance precision data previous propose efficiently perform bulk bitwise logic operation SRAM dram pim architecture due constraint memory circuit perform logic operation input input therefore pim architecture perform logic operation complex arithmetic operation contrary hyper AP perform bitwise logic operation complex arithmetic operation specialized pim developed important domain application integrate compute memory leverage 3D integration despite radically hardware implementation principle monolithic compute cpu gpu ASIC etc physically closer monolithic memory fully address memory conclusion propose hyper AP parallel pim processor flexible data address limitation traditional AP substantially improve performance associative processing specifically  abstract machine model enhance execution model computation perform multi multi manner dramatically reduce operation architecture microarchitecture implement abstract machine model hyper AP comprises additional optimization technique logical unified physical array latency communication interface improve performance finally compilation framework reduce program complexity user program application hyper AP optimization operation merge operand embed apply compilation reduce operation computation evaluation hyper AP achieve improvement throughput efficiency efficiency respectively imp recent RRAM data parallel processor hyper AP achieves speedup reduction evaluate kernel imp finally confirms hyper AP beneficial RRAM implementation CMOS implementation due substantially reduce operation