reduction machine model translate improvement accuracy important factor increase training neural network dnns amount temporary data propagation algorithm enable training model temporary data offload limited gpu memory cpu memory data movement incurs performance overhead important dnns convolutional neural network cnns spatial correlation temporary propose jpeg   lossy activation offload accelerator training cnns discard redundant spatial information  adapts jpeg algorithm 2D image compression activation compression optimize jpeg algorithm ensure convergence maintain accuracy training jpeg achieves training performance prior offload accelerator prior activation compression efficient hardware implementation allows jpeg consume gpu index gpu hardware acceleration cnn training compression introduction reduction training neural network played important role enable dramatic improvement accuracy accuracy improvement explosion application recent speedup due graphic processor gpus superscalar processor architecture recent propose advance specialized hardware acceleration network inference network hardware acceleration training focus accelerate training convolutional neural network cnns cnns image classification detection semantic label typically training cnn output individual neuron activation compute memory later restore activation update backpropagation activation memory capacity resnet imagenet dataset 0GB storage gist compr offload sparse compr sparse compr reduce precision memcpy compress compr memcpy cnn kernel conv norm relu compr offload lossy transform average compr ratio offload compr vDNN cDMA  error pas offload schedule conv norm relu cnr resnet imagenet compute memcpy arrow correspond activation offloads compression ratio resnet imagenet error indicates compression validation dataset memory available consumer grade gpus 2GB nvidia titan network layer input image dimension  increase memory storage achieve accuracy versus resnet effective activation storage achieve via recomputation gpu memory compression transfer  memory recomputing activation backward pas incurs compute overhead memory compression evaluate gpus activation gist CPUs limited amount gpu memory naively offload activation cpu dram vDNN disaggregated memory limited pcie throughput expensive specialized interconnects NVLINK ibm however effectiveness offload enhance compress data transfer cDMA upon latter approach allows interconnect memory technology jpeg activation compression cpu offload shallow network sparsity however training resnets extremely network proportion dense activation acm annual international symposium computer architecture isca doi isca frequency entropy distribution image  resnet cifar activation shannon entropy discrete cosine transform sparse activation average sparsity performance penalty sparse gist vDNN perform activation sparsity maximum dense compression ratio propose jpeg compress offload accelerator exploit activation sensitivity distribution maximize compression jpeg extends compress offload domain specific lossy compression insight exploit jpeg dense activation image modify frequency distribution cnns error sensitivity perception jpeg adjusts jpeg compression optimize cnns pas jpeg compress data cpu memory via memory access dma backward pas jpeg  data retrieve cpu memory gpu memory jpeg dense sparse activation improves training performance versus accuracy loss contribution propose fix precision reduction SFPR jpeg integer compression pipeline instead float optimize jpeg activation compression cnns account sensitivity information loss cnn training versus perception achieves stock jpeg optimize jpeg compression ratio uncompressed gist accuracy propose evaluate jpeg offload accelerator jpeg SFPR demonstrate performance improvement uncompressed offload gist gpu overview algorithm training cnns activation compression II detail accelerator optimization jpeg parameter IV finally report experimental setup evaluation jpeg VI input image cnr conv norm relu relu generic layer loss training convolutional neural network backprop conv convolution norm batch normalization relu rectify linear II background review neural network training related activation compression jpeg algorithm sgd backprop illustrates training typical contemporary cnn backpropagation algorithm stage propagation backward propagation update propagation perform input image apply sequence layer function layer network loss calculate layer output quantifies error network output desire target output network gradient loss propagate reverse direction calculate gradient loss respect layer input gradient loss respect calculate update accord sgd update iteration rate parameter adjust aggressively update backpropagation activation compute pas avoid recomputing backward pas layer gradient conv norm relu calculate input activation output activation gradient recomputation approximately float operation FLOPs backward pas significantly increase training gradient calculation reformulate modify activation relu layer multiple formulation computation relu backward calculation respectively  input output backward pas alternatively binary mask instead II framework activation examine overall network structure minimize computation discard unused activation activation information network layer hence dynamic cnn framework  basis caffe pytorch  strategy conv input norm input relu output resp choice knowledge computation calculate gradient activation instance conv input gradient calculation expensive recalculate output framework discard another layer gradient focus apply compression activation conv norm relu cnr nearly cnns cnr smooth loss landscape training variety cnns previously network alternate conv relu layer memoizing sparse relu activation introduction norm however requirement dense conv output due relu compression network storage although focus cnr compression flexible sparse dense activation dropout pool summation activation compression compress activation sustain compression rate throughput gpu memory compression classify lossless lossy lossless compression algorithm data perfectly reconstruct whereas lossy compression permit reconstruction approximation data partial reconstruction lossy discard irrelevant portion data greatly increase compression rate compression error compression rate commonly rate distortion detail prior activation compression jpeg algorithm image binary relu compression BRC binary relu compression formulate compress relu activation input relu activation effectively instead eqn BRC relu activation immediately conv layer network involve dropout vgg resnet BRC resnet precision reduction focus inference explore reduce precision activation however examine training knowledge extensive network modification exception dynamic precision reduction DPR float BFP DPR activation cast float pas reduce activation storage however difficulty activation deeper network vgg addition compress sparse csr storage mask uncompressed compress zero compression ZVC sparse activation author decrease activation storage DPR BFP fix factor activation  network multiplication BFP encode relu dropout activation sparsity lending zero compression encode previously investigate activation compression highly sensitive sparsity compress zero dense activation conv cannot compress manner zero compression ZVC randomly zero compress easily zero compression derivative frequent compression ZVC non zero mask non zero packed mask limit maximum compression ratio advantage equally regardless zero distribution author achieve compression ratio relu dropout activation ZVC jpeg jpeg commonly image compression algorithm jpeg frequency spatial information image precision important perception summarize relevant portion jpeg additional detail elsewhere illustrates jpeg algorithm jpeg split image adjacent pixel quantizes frequency domain due limitation matrix pixel integer discrete cosine transform dct convert frequency quantization div apply frequency quantize correspond entry discrete quantization DQT output quantize DQT compression quantization matrix zero remove stage huffman cod RLE RLE lossless remove zero huffman cod convert variable width code output jpeg accelerator jpeg image compression convolution essence image processing kernel activation div RLE huffman cod encode encode DQT dct huffman jpeg encode illustration instead due limitation convolution image resemble image hypothesis analyze shannon information entropy spatial dct frequency dct domain demonstrate spatial correlation persist network convolution layer frequency domain entropy layer network activation storage requirement implies frequency domain compact representation convolution activation trend sparse activation ReLUs jpeg compress offload accelerator cDMA however goal jpeg address issue introduce network offload cDMA vDNN overhead due pcie bandwidth network resnets due sparsity proportion dense activation gist avoids pcie bottleneck compress gpu memory instead remove offload precious computation resource perform activation compression compression rate gist moderate relief activation storage amount costly gpu memory jpeg instead address pcie bottleneck aggressive lossy compression scheme cheaper memory address compute overhead custom hardware implementation avoid compute resource overview jpeg offload accelerator 2D activation onto accelerator implementation jpeg component convolution activation entropy average epoch activation layer resnet cifar overview baseline comprises gpu bandwidth memory HBM dma pcie cpu dram assume multiprocessor SM cache memory controller partition MC dma symmetric link gpu crossbar training vDNN involves offload activation dma usage pas similarly backward pas activation load dma gpu HBM freed overlap compute gpu HBM baseline gpu vDNN gist SM cpu dram pcie MC MC SM crossbar dma HBM dram MC SM buffer dma cache compr cDMA CDU HBM dram MC SM split dma dma compr cDMA jpeg CDU CDU CDU CDU compression decompression CDU location compression software compression CDUs ZVC  cDMA cDMA  jpeg CDU compress offload augment dma compression decompression CDUs collector splitter CDUs dma maximum effective offload rate compression ratio pcie bandwidth multi link multi CDU avoid limited crossbar link bandwidth explore VI compress data gpu HBM however investigate activation 1GB amount HBM compress traffic multiple CDUs aggregate collector transfer cpu uncompressed traffic distribute CDUs splitter transfer gpu dma compression differs cache compression cDMA cache compression overhead due replication CDUs across cache partition gpu architecture volta additionally load balance sequential cache typically distribute across memory partition jpeg compression operates activation hence span cache  communication across memory partition  perform jpeg exclusively  examine parallel portion CDU cache VI comparison implement cDMA dma technique cDMA alignment buffer DQT SFPR gpu crossbar splitter dma collector SH dct BRC  ZVC  brd compr  jpeg CDU SH shift brd binary relu decompression  zero decompression cDMA cDMA identical CDUs CDU location brief overview jpeg CDU operates compression decompression mode fix precision reduction SFPR introduce convert floatingpoint integer quantization error SFPR gpu crossbar alignment buffer jpeg load gpu cache simultaneous processing dct  pipelined compose dct stage jpeg compression pipeline shift quantization SH ZVC  compression mode collector combine output multiple CDUs dma decompression mode compress input splitter output jpeg standard DQTs jpeg jpeg DQT coefficient selection impact training accuracy tune jpeg DQT activation compression optimize compress entropy recover activation error resnet procedure optimize compression DQTs optL optH finally introduce wise DQT stage optLH selection DQTs IV fix precision reduction SFPR propose fix precision reduction SFPR technique cast float activation integer reduce hardware compression error activation generally float however jpeg compression operates integer jpeg image DQTs jpeg jpeg optimize DQTs optL optH wise DQT optLH cifar input resnet simulated jpeg compr  recover error compr entropy DQT optimizer jpeg DQT optimization procedure naive cast integer error activation dissimilar target integer format develop SFPR normalize activation convert integer SFPR involves channel wise max 4D input activation tensor RN clip integer    clip  integer width denotes function clip trim outside within   activation batch channel height width index respectively global factor hyper parameter specify activation clipped integer max  factor dependent maximum channel batch  compute per layer basis training SFPR global factor minimize recover activation error compress SFPR alone combine jpeg DQTs definition activation clipped increase activation error clip truncation magnitude activation clipped integer min max magnitude activation truncate zero SFPR compression sensitivity average increase recover activation error across SFPR combine jpeg SFPR dct truncation error increase due quantization dct minimizes overall error SFPR jpeg div RLE jpeg SH ZVC across network factor landscape resnet cifar epoch conv sum activation compression optL optH jpeg DQTs layer avoid introduce additional hyper parameter training channel wise factor costly calculation maximum channel activation avoid maximum calculate efficiently activation statistic variance already batch normalization alternatively prior integer quantization activation statistic significantly batch sample promising approach factor calculation due performance hardware overhead SFPR pre stage jpeg benefit normalization without normalization compression ratio greatly training across network compression variation across channel reduces network accuracy input jpeg compression activation truncate compression ratio compression error instance activation zero integer cast normalization ensures entire integer utilized activation channel SFPR jpeg SFPR compression jpeg jpeg accelerator identical SFPR processing spe spe handle conversion integer float pas load channel return cache sector gpu crossbar floatingpoint sector split SPES activation stage float multiplier cast integer cast outof saturates  spe concatenate alignment buffer await jpeg compression backward pas inverse factor load inverse factor calculate without significant overhead computation amortize channel due spatial dimension activation integer decompress jpeg load split SPES convert float concatenate gpu crossbar SFPR similarity DPR BFP SFPR reduces hardware versus DPR convert integer instead float channel wise BFP SFPR however SFPR normalization allows utilization integer data alignment buffer alignment buffer structure convert linear address jpeg dct 2D operation available processing spe spe alignment gpu crossbar float int int float buffer SFPR SFPR processing SPES mode grey arrow float cast integer backward mode arrow integer cast float buffer jpeg prevent duplicate cache access activation pad align cache coincides jpeg alignment buffer jpeg cache activation data SFPR compression ratio assume  memory layout batch channel height width activation tensor performance training cnns default framework jpeg height span cache cache jpeg activation hence alignment buffer cache compress per activation jpeg buffer duplicate cache access jpeg CDU align cache boundary access stride depends activation tensor activation tensor nch nch reshaped reshaped cache cache pad nch pad alignment buffer alignment buffer memory layout pad correspond jpeg pad outline cache boundary outline activation tensor boundary activation tensor sequential cache load exactly jpeg cache stride load alignment zero pad input activation width multiple jpeg width pad pad height activation channel instead pad reshaped activation 4D tensor RN reshaped 2D tensor  pad along reshaped dimension nch pad reshape data movement index pad manner framework modification pad increase memory footprint activation performance overhead however increase usually pad perform hardware however introduces additional hardware unaligned access overhead preferable due warp gpus nch pad similarly activation tensor pad datasets network examines resnet imagenet resnet imagenet pad storage overhead pad nch pad resnet overhead activation storage layer network relative pad alignment buffer compression SFPR perform writes dct compression perform jpeg load dct stage proceeds cycle buffer freed decompression role reverse writes decompression pipeline SFPR structure allows maintain buffer discrete cosine transform dct jpeg jpeg implement utilize 1D dct dct  dct due pipelining efficient multiplier  implementation multiplication addition dct multiplier jpeg dct implement  dct 1D dct compute dct along dimension transpose dct along dimension pas cycle transform 2D dct div jpeg SH jpeg  fashion similarly dct brief  combine normalize shift stage stage invert relative dct multiplier become divider etc 1D dct 1D dct alignment buffer transpose SH jpeg 2D dct 1D dct reproduce  dct dct algorithm building sin implementation pas structure pipeline stage div RLE jpeg jpeg hardware implementation jpeg standard quantization cod stage div quantization DQT RLE cod combine  encode huffman cod implement div parallel multiplier  RLE encode  decode hardware duplicate throughput requirement SH ZVC jpeg jpeg standard software compression image developed shift SH ZVC replace standard jpeg algorithm reduce hardware overhead improve compression activation SH quantization remove multiplier div stage jpeg ZVC cod observation activation frequency distribution drastically image SH ZVC combine SFPR dct compose jpeg accelerator SH motivate observation quantization unnecessary switch shift operation associate quantization operation reduce limit DQT compression mode shift operation perform parallel decompression mode shift replace shift SH expense available quantization mode frequency perform activation compression dct  DQT SH ZVC  SH SH compr  shift SH parallel DQT output arrow compression decompression quantization mode sufficient individual frequency reduce IV ZVC compress sparse SH stage dct quantization image zero frequency conversely activation display flatter profile zero randomly distribute across mid frequency ZVC compression RLE frequency domain activation modification SH ZVC decrease hardware VI increase compression VI collector splitter collector splitter convert multiple CDU data pcie dma data collector variable CDUs splitter split pcie calculate byte collector splitter directly pcie dma schedule policy interleave CDUs impact collector splitter hence address collector splitter rate per cycle load rate gpu crossbar per cycle per CDU hence entire jpeg accelerator bottleneck pcie interconnect compression rate crossbar link compression rate CDU processing faster crossbar rate robin schedule CDUs accomplish mux CDUs schedule cycle solves issue splitting deterministically interleave collector operates pas CDU writes collector cycle robin policy ZVC mask sum obtain non zero byte primary structure align non zero input fifo   variable operation indexed dma CDU CDU vals mask sum dma CDU sum pop byte byte vals mask byte   CDU collector splitter aggregate splitting compress pipeline register signal insert byte signal  popped  packet dma pop operation  location byte splitter operates backward pas packet dma onto output fifo  byte mask   mask calculate byte pop  cycle collection deterministic distribution splitter occurs robin policy utilize collector splitter multiple CDUs avoid issue inter cache communication IV  compression jpeg DQTs image jpeg jpeg etc extensively perception however prior indicates cnns frequency sensitivity optimization perform define metric approximate network convergence compression creation objective function significantly activation compression rate error relative jpeg DQT image network convergence currently poorly understood topic however efficient convergence optimize jpeg DQT objective function gauge accuracy network without training therefore maintain accuracy attempt maximize accuracy jpeg compression training understood layer network training reshaped pad activation  iteration backprop compression output activation gradient calculate respectively output activation gradient generic tensor dot iteration jpeg activation compression approximate gradient calculate qij dct    mod mod function  quantize frequency matrix recover activation tensor dot linear operation hence error relative uncompressed express identical convergence uncompressed achieve error approach zero accomplish minimize activation error eqn approximation global objective function compression optimization procedure shannon entropy eqn quantize frequency coefficient minimum per activation combine average error per activation objective function   quantization width probability counting occurrence normalize factor hyper parameter rate distortion minimize respect DQT convolution layer activation generator network frozen resnet cifar epoch activation calculate DQT sgd optimizer DQT gradient calculate finite difference difference DQT parameter activation fix prevent instability batch normalization parameter examine rate distortion SFPR jpeg DQTs efficacy optimization optimize DQTs activation compression error compression SFPR jpeg image DQTs decrease entropy error image DQTs optH jpeg tune DQT desire compression rate error hence compression variant optL optH respectively increase activation error error decrease optH optL optH jpeg SFPR jpeg optL rate distortion SFPR jpeg image DQTs jpeg optimize DQTs resnet cifar epoch activation error entropy jpeg various DQTs cifar optH optL chosen error jpeg jpeg DQTs error approximately decrease accuracy examine compression error entropy training evaluate DQT snapshot network epoch activation error epoch resnet WRN consequence decay however epoch compression remains constant attribute stable activation distribution batch normalization combine normalization SFPR trend error entropy remainder training epoch training critically important convergence cnns address critical epoch propose wise approach DQTs optLH optLH optL DQT epoch training switch optH DQT remainder training avoids error critical methodology compress activation cnn accord layer dimension II BRC relu activation conv layer hence subsequent layer sum refers dense activation addition activation jpeg compression conv sum activation due algorithm jpeg convolution fully layer due activation datasets network variety network cnn application extremely network  examine due memory requirement evaluate jpeg cifar imagenet divk datasets image classification cnns vgg vgg compression rate offs compression ratio bracket bolded lossy baseline cDMA gist SFPR jpeg jpeg jpeg jpeg optL optH optLH cifar val accuracy compression ratio vgg resnet resnet WRN imagenet val accuracy compression ratio resnet resnet divk val psnr compression ratio VDSR average baseline compression ratio model fail converge II compression selection activation SD SFPR dct conv relu relu pool sum conv dropout cDMA none ZVC gist DPR BRC DPR csr SFPR SFPR jpeg SD div RLE BRC SFPR jpeg SD SH ZVC BRC SFPR ZVC nch otherwise SFPR resnet WRN resnet network unmodified source additionally examine jpeg super resolution VDSR modify random batch normalization implement functional simulation  examine compression neural network accuracy implement cuda code extends framework skip lossless compression functional simulation instead calculate compression ratio offline batch performance simulation gpgpu sim configure simulate nvidia titan gpu pcie effective transfer rate 8GB model boost mhz multiprocessor interconnect capable cycle bidirectional bandwidth mhz HBM network performance assess microbenchmark programmed cuda cudnn  cnr sample network batch network prohibitive simulation requirement warmup relu avoid gpu cache source code gist publicly available  performance cuda  functional  cuda simulation DPR BRC optimization sparse storage dense compute compress sparse csr variant percentage accuracy loss relative speedup implement jpeg accelerator rtl synthesize synopsys compiler evaluate timing requirement synthesis target interconnect frequency technology FreePDK library library longer available overhead manner prior VI evaluation overall plot percentage accuracy versus performance improvement jpeg variant optL optLH achieve performance gain accuracy loss versus alternative compression accuracy network compression report validation accuracy peak signal  ratio psnr average network compression ratio imagenet accuracy cpu efficient augmentation procedure report validation instead accuracy cDMA lossless accuracy however compression ratio relu dropout compression ratio network batch normalization dense activation overall compression training gist significant decrease accuracy psnr SFPR predominantly resnets VDSR issue vgg imagenet DPR hypothesize due truncation channel minimum per channel activation network available DPR utilized utilized SFPR avoid expense compression gist SFPR generally integer utilization DPR due normalization activation error accuracy gist compression ratio significantly network dropout vgg WRN versus resnets VDSR csr gist compress DPR extract non zero index optimization storage DPR index per non zero sparsity increase DPR alone resnets imagenet csr choice network without dropout csr advantageous compress index DPR jpeg improve compression cDMA gist SFPR however accuracy SFPR WRN sensitive lossy compression converge jpeg non convergence decrease accuracy training warn compression jpeg WRN average accuracy across remain network although jpeg compression ratio decrease accuracy quality setting jpeg severe warrant modification jpeg significant increase accuracy psnr compression optimization procedure optL reduces activation error obtain accuracy SFPR baseline decrease error likely due cnn training stochastic optH divergence WRN cifar resnet imagenet however wise technique optLH anneal network epoch optL switch optH average compression ratio accuracy gist jpeg optLH increase compression jpeg jpeg quantization DQT ZVC sparse activation quantization sum activation conv activation compression remain mostly unchanged ZVC compress relu dropout activation decrease SFPR relative activation footprint breakdown activation cifar model contribution compression network converge examine diverge activation distribution diverge standard deviation training suspect activation compression affect activation divergence dependent batch normalization parameter decrease compression DQT coefficient relate reduce behavior similarly anneal epoch compression optLH prevents divergence imply rapid training investigation training dynamic cnns error fully understand issue quantization cod modification isolate DQT optimization quantization cod evaluate DQT jpeg conv sum compression ratio jpeg DQT significantly compression optH highlight effectiveness optimization procedure increase compression however compression expense accuracy training optLH compression ratio optH maintain accuracy optL optLH jpeg increase conv sum compression resnet cifar conv sum compression various DQTs jpeg jpeg jpeg optL optH optLH div RLE SH RLE div ZVC SH ZVC relative performance vDNN ZVC RLE increase compression ratio jpeg RLE  information magnitude frequency zero quantization cnn activation however frequency mode quantize non zero RLE performs poorly randomly distribute zero contrast ZVC additionally optimize DQTs flatter quantization profile image DQTs frequency quantization randomizes zero apparent improvement optL ZVC performance performance measurement accomplish microbenchmarking cnr optional dropout pool layer due simulation memory constraint simulate layer network batch algorithm winograd winograd  convolution implicit gemm algo convolution representative software framework  pytorch gist performance strongly influence network structure performance resnet resnet attribute presence bottleneck layer involve convolution decrease channel bottleneck involve channel activation FLOPs similarly kernel non zero scan   conversion longer kernel performance overhead comparison SFPR jpeg display performance network dependent SFPR performance gist despite compression primarily csr SFPR pcie bandwidth limitation nearly eliminate jpeg optLH performance increase gist overhead consistent performance obtain shift bottleneck effective offload rate instead compression throughput compression increase modify jpeg cnns improve performance decrease error baseline jpeg optLH jpeg jpeg remain overhead  performance CDUs resnet cifar fix compression ratio cache dma refers cache SFPR CDUs dma dct SH ZVC CDUs congestion gpu interconnect increase dma traffic despite jpeg obtains performance versus vDNN VDSR offload performance network VDSR dropout pool bottleneck layer however important difference activation channel spatial dimension cudnn launch compute kernel VDSR hypothesize compute density offload performance CDU location effective offload rate available jpeg highly dependent location configuration CDUs gpu memory importantly CDUs affect available bandwidth gpu dma compression increase performance CDU compression offload bottleneck pcie offload rate compression however bottleneck remove performance increase CDUs performance compression increase CDUs CDUs compression CDUs memory partition become bottleneck prevent increase examine impact SFPR portion CDU cache combine cache dma compression configuration pas cache immediately compress IV jpeg synthesis component component SFPR dct  quantize div quantize SH cod RLE  cod ZVC  collector splitter crossbar comparison buffer CDUs crossbar exclude cDMA SFPR jpeg jpeg jpeg optLH compression offload GB SFPR gpu interconnect compress jpeg dma minimum compression rate due mandatory SFPR SFPR per memory partition SFPR CDUs jpeg CDUs configuration overhead due duplication SFPR performance increase CDU dma synthesis individual jpeg component IV dct expensive component jpeg buffer overall visible cDMA jpeg significant increase effective pcie bandwidth maintain nvidia titan gpu relative data gpus modification jpeg cnns reduce overall respectively increase available pcie offload bandwidth vii related favorably primary examine activation storage training vDNN cDMA gist however proposal compress pre neural network reduce inference unlike jpeg decrease activation storage training frequency transforms precision reduction training network reduce precision effective modification cnn framework network architecture training schedule  examine reduce precision gradient storage multi gpu training decrease local memory consumption activation remove entirely remove gradient reversible network involve computational load conventional compression gradient convolutional activation regenerate contrast jpeg restricts available layer network conclusion jpeg novel offload accelerator cnn activation compression fix compression mechanism fix precision reduction SFPR demonstrate jpeg effectively variety datasets benchmark significantly compression ratio stateof jpeg tune cnns improvement compression increase accuracy hardware jpeg incorporate simply cnn architecture framework