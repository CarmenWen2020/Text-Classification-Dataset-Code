memory capacity soar tlb become increasingly significant performance bottleneck coarse grain intel others propose tailor tps mechanism allows default minimum default minimum KB tps entry pte contiguous virtual memory mapped equivalent contiguous physical frame seamless ISA microarchitecture allocation operation tps eliminate approximately memory access tlb across variety spec data memory intensive benchmark introduction virtual memory fundamental  component computer decade virtual memory application private virtual address memory protection improve security due memory isolation ability utilize memory physically available secondary storage addition application explicitly manage address virtual physical address mapping operating hardware virtual address conventional coarse grain fix virtual mapped physical frame via hierarchy KB MB 1GB refer superpages translation lookaside buffer TLBs cache virtual physical translation reduce trend increase computer physical memory capacity client device gigabyte physical memory server terabyte physical memory become commonplace application leverage physical memory capacity suffer costly virtual physical translation penalty due realistic constraint tlb KB typical tlb capacity entry KB physical memory refer limited tlb processor typically multiple TLBs 1GB typical tlb capacity entry span 4GB physical memory intel corporation microsoft corporation generous financial  research TLBs recent intel skylake processor prior demonstrate application execution service tlb capacity entry reduce impact infrequent costly percentage application execution processor spends performance counter data physical hardware transparent active native execution interference native execution simultaneous multithreading smt hardware thread compete tlb resource virtualized execution dimensional native execution overhead generally modest due tlb capacity smt interference virtualized execution significant increase overhead overhead increase upcoming tlb additionally impose performance penalty demonstrates performance improvement perfect tlb perfect tlb baseline perform cycle simulation model detail IV hide tlb overlap latency useful memory access critical execution link data structure traversal frequent tlb appreciable performance penalty limited tlb capacity role translation overhead coarse granularity conventional processor inadequate MB data structure operating identify contiguous memory allocate available data structure tradeoff UI OOVBM  PNQVUFS SDIJUFDUVSF overhead percent execution spent speedup perfect tlb perfect tlb baseline serious choice MB tlb entry already tlb capacity data structure 1GB MB waste physical memory internal fragmentation trend increase gap consecutive conventional worsens tradeoff translation granule consecutive KB MB along continued growth physical memory capacity virtual memory mechanism translation overhead propose tailor tps extension processor architecture reduces translation overhead significantly lower memory reference substantially reduces tlb tlb intensive workload increase physical memory capacity tps introduces KB tps operating software ISA microarchitecture fragmentation insufficient contiguity utilize conventional tps allows OS leverage contiguity performance intermediate tps leverage address contiguity due standard operation OS buddy allocator memory compaction daemon addition application already perform mapping runtime contiguous virtual address OS implementation split mapping request however compose request utilize tps virtual physical mapping appropriately grain memory intensive application tailor pte cached tlb tps sacrifice backwards compatibility model additional flexibility newly available tps OS implementation gradually adopt introduce retain option solely conventional whatever arise summary contribution propose tps ISA extension define microarchitectural facilitate virtual physical address translation newly hardware mechanism straightforward tlb enhancement translation justify via simulation straightforward OS implement leverage newly across variety benchmark tps tlb rate almost eliminate execution spent access TLBs II background background virtual memory translation implementation topic directly apply architecture utilize hierarchical conventional superpages primarily architecture explanation virtual memory hardware virtual memory illusion private address hardware performs address translation runtime operating OS responsible partition virtual address virtual mapping physical frame architecture defines contract hardware software contains mapping virtual physical frame address memory mapped virtual address consists entry PTEs pte contains mapping virtual physical frame bookkeeping protection purpose intel currently implement hierarchical radix memory reference execute virtual address processor core contains hardware walker virtual address traverse eventually pte frame request virtual address  physical translation KB superpages simply refer MB 1GB currently currently available software mechanism linux leverage transparent THP  tlb hardware translate virtual address physical address memory access potentially multiple memory access processor TLBs cache recently PTEs tlb translation cycle tlb perform cycle worth multiple associative tlb non trivial index tlb typically significant virtual however virtual depends unknown perform tlb lookup MMU cache normally memory access hierarchy processor typically memory management MMU cache contains recently PTEs upper hierarchy MMU cache reduce memory access access cache OS software virtual memory operating component responsible virtual memory management OS maintains creates virtual physical mapping upon request briefly relevant component buddy memory allocation buddy allocator physical memory associate specific allocation request mapping occurs request query physical frame mapping available request request iteratively split appropriately split unique buddy unique buddy split remain appropriate later  physical memory allocator buddy merge buddy merge checked buddy merge operation appropriate buddy allocator split merges physical memory appropriate allocation deallocations demand lazy allocation demand operating performs setup upon virtual physical mapping request OS PTEs invalid actually physical frame reference location newly mapped fault occurs notifies operating demand exists frame appropriately initialize although operation buddy allocator prevalence infrequent mapping request easily utilize contiguous frame contiguous virtual allocation contention interleave scatter demand request memory compaction memory compaction daemon primarily responsible reduce external fragmentation memory compaction scatter physical memory migrate adjacent location contiguous memory memory compaction explicitly invoked sufficient contiguous memory cannot OS receives allocation request tailor extension processor architecture tps leverage exist feature virtual memory mechanism reduce translation overhead subsection discus architectural implementation detail tps OS feature tps additional layer consideration architectural consideration pte tps pte structure update overview typical hierarchical implementation KB MB 1GB KB physical memory access access MB bookkeeping pte identifies similarly 1GB pte identifies pte additional option KB MB reserve pte pte reserve pte limited propose alternative reserve frame pfn assume physical address KB offset pfn KB hierarchical offset pfn reserve specifies pte corresponds standard conventional KB tailor intermediate KB tailor pte pfn unused pte otherwise pfn specifies tailor KB KB pte another unused pfn RISC PMP  encoding easily implement hardware priority encoder identify identify pte perform hardware challenge pte grain tailor additional memory access detail extra access alias pte assumes implies address offset subsection virtual address identify specific pte within index index entry tailor multiple index PTEs actually tailor PTEs address update access pte examine indicates subsection virtual actually offset additional memory access perform virtual address actually offset zero pte pte tailor alias PTEs simply access goal tps nearly eliminate tlb additional memory access actually occurs rarely outweigh reduction IV addition spent alias PTEs relatively inconsequential conventional PTEs anyway PTEs numerous additional maintain alias PTEs pte valid approach approach functionally tps extra lookup however tradeoff approach pte update update alias PTEs correspond generally pte update significantly frequent extra lookup induced alias PTEs approach regardless approach tps tlb update tlb tps recent intel skylake processor tlb data access tlb split contains entry KB entry MB entry 1GB tlb contains entry 1GB entry KB MB tps modify tlb entry fully associative commercial tps tlb tps tlb exist entry entry TLBs tps tlb retain entry KB tlb exist productized amd zen entry fully associative  conventional entry tps tlb reasonably timing constraint processor alternative skewed associative tlb newly tlb mask tlb entry mask tlb tlb virtual VPN memory access normally VPN VPN tag within tlb identify tlb incoming VPN masked entry mask VPN tag identify gate delay associative tlb lookup unlikely impact latency tlb hardware exist hardware within dash tlb lookup operating consideration buddy allocation standard approach demand intermediate contiguity mapped physical frame extract potential tailor utilize eager strategy lazily allocate conventional demand access identify appropriate tailor eagerly allocate tailor appropriately frame physical memory buddy allocator allocation request performs mmap however drawback OS strategy application allocation latency adversely affected application entire initialize standard additionally costly swap swap become server memory workload swap disabled prefer entire memory minimize latency iOS swap secondary storage increase physical memory capacity significantly reduces frequency swap improve robustness tps concern significant utilize alternative demand eager demand frame reservation approach reservation strategy freebsd previously propose reserve frame neither transition demand allocation request occurs operating identifies desire optimal tailor allocate entire standard demand instead buddy allocator query memory remove allocator reservation request virtual address memory reserve virtual address within demand request memory access occurs location within conventional demand request allocate standard demand appropriate frame chosen previously reservation buddy allocator subsequent demand request mapped virtual address already mapped grown promotion upgrade location request physical memory location identify reservation upgrade simply update appropriate PTEs newly mapped newly mapped memory appropriately initialize migration previously mapped frame tps  allows frame reservation incrementally OS demand request within reserve unlike freebsd approach tps adjust promotion aggressiveness utilization threshold prevent memory footprint bloat tps configure upgrade constituent utilized conversely tlb performance tps configure upgrade percentage constituent utilized promotion threshold adjust extreme balance tradeoff machine memory load straightforward algorithm allocator OS application utilized virtual address minimal appropriately fragmentation primary drawback fragmentation fragmentation become tps allocate originally conventional addition OS request memory compaction fragmentation opportunity grain tailor allocation merges standard conventional allocation external fragmentation occurs memory allocate memory intersperse prevent contiguous allocation memory exceeds allocation internal fragmentation occurs allocate memory unused external fragmentation minimize external fragmentation tps conservatively upgrade reservation utilization described external fragmentation increase OS unable desire reservation due lack memory contiguity external fragmentation conventional cannot however whatever minimal memory contiguity available leveraged tps intermediate tailor internal fragmentation potential waste due internal fragmentation increase conservative policy completely disallow extra loss due internal fragmentation exclusive reservation exactly span reservation align request aggressive tps request memory allocation approximately waste allocation request KB MB reservation fragmentation tps throttle towards conservative reservation choice tradeoff magnitude internal fragmentation tlb entry translate logical exist OS proposal ingens translation ranger already address issue maximize memory contiguity manage allocation improve virtual memory performance reduce fragmentation exist approach tps synergistically maximize translation benefit inclusion technique tps ability appropriate minimize tlb entry application memory compaction merge tps standard memory compaction daemon operation improve performance footprint benchmark already benefit upfront compaction application launch conventional tps maximizes benefit sort compaction future optimization memory compaction daemon aware physical frame potentially merge frame pte translation merge appropriately align adjacent frame contiguous virtual address identical permission multiple allocation contiguous virtual address unable contiguous frame due intervene allocation whenever memory compaction perform daemon migrate frame account potential merges PTEs migrate update OS performs merge update relevant PTEs invalid appropriate tlb entry frame successfully setup merge consideration pte access dirty processor update access dirty pte load tlb cache identify additional update pte actually concern access dirty data granularity incur additional overhead swap dirty secondary storage already additional overhead introduce intermediate however fragmentation swap frequent pressure due cleaning dirty OS option splitting reduce associate alternative recall intermediate tailor multiple alias PTEs simply pte remain alias PTEs unused vector reference modify tailor constituent conventional vector cached TLBs actually load exhibit sticky behavior update memory guarantee update cached tlb prevent extraneous update vector strictly tlb lookup vector lookup update operation proceed standard tlb lookup parallel subsequent memory access pipeline stage tailor constituent conventional costly tlb additional memory access impose upper bound vector limit significantly reduce grain granularity function pte specify enable disable grain metadata vector update mechanism already exist modify update operation progress tlb shootdowns invlpg instruction invalidate date PTEs processor TLBs operation instruction standard operation appropriate shootdowns remain memory compaction merge adjacent pte tlb portion optimal tlb entry replacement ideal policy update lru information return unnecessary however likelihood extraneous tlb entry increase shootdowns perform merges OS technique enables multiple virtual identical data physical frame maintain pte fault occurs OS frame update mapping opportunity reduce simply likelihood memory identical substantial desire OS simply prioritize simulated processor configuration core issue entry rob ghz rate cache KB KB byte cache cycle latency associative cache MB associative byte cache cycle latency TLBs  1G  1G  upon OS multiple option OS multiple mapping reduces memory utilization alternatively OS entire expensive memory utilization reduces tlb pressure IV RESULTS methodology evaluate performance impact proposal perform evaluation simulation implement propose OS exist OS physical processor propose ISA preferable workload fully understand tlb behavior translation overhead memoryintensive application however cycle simulator infeasible simulation impact tps primarily evaluate tlb rate affected construct pin OS allocator virtual memory simulator trace memory management memory access simulator model relevant microarchitecture operating model realistic tlb hierarchy MMU cache identify access hierarchy additionally model OS allocator application demonstrate importance tlb zsim cycle simulator superscalar processor simulator strongly correlate exist intel processor faithfully model core microarchitectural detail memory hierarchy TLBs simulator model impact tlb evaluation perform core baseline processor configuration benchmark evaluate spec suite graph  xsbench dbx spec suite fully representative tlb intensive workload profile benchmark benchmark tlb pressure tlb per instruction MPKI evaluation chose tlb intensive spec benchmark MPKI performance counter evaluation machine recent intel kaby processor GB ddr memory DTLB MPKI data prior OS linux transparent memory footprint increase benchmark pin simulator simulated MB maximum memory impact MB KB benchmark modest increase memory utilization benchmark important 1GB increase potential memory loss due internal fragmentation remain tps frame reservation strategy utilization merge tailor approach guarantee identical memory usage KB loses opportunity reduce tlb increase memory utilization exclusive MB pin simulation migration compaction perform initial memory assume lightly load report percentage DTLB eliminate benchmark tps enable baseline reservation transparent THP implement evaluate impact colt rmm tlb rate tps eliminates tlb average colt eliminates approximately DTLB benchmark rmm eliminates DTLB rmm introduces tlb hierarchy colt minimal impact benchmark  random access behavior increase tlb entry factor mitigate random memory access gigabyte physical memory contrast tailor memory significantly reduces tlb DTLB eliminate baseline reservation THP reduction memory reference tps rmm identical performance maximal reduction memory reference rmm generally slightly outperforms tps primarily rmm alignment restriction eager reduction policy unacceptable impact allocation latency scenario tps slightly outperforms rmm gcc benchmark limited available tlb entry memory reference eliminate baseline  THP approximate performance impact execution   lbm   ideal execution benchmark assume tlb  lbm execution lose due    execution lose due THP baseline  lbm via zsim simulation performance counter measurement cannot directly calculate  performance counter cycle walker active application progress due walker active eliminate walker cycle translate reduction execution estimate reduction walker cycle actually realize saving execution machine performance counter information configuration configuration transparent disabled execution cycle  walker cycle  transparent enable execution cycle  walker cycle  calculate reduction PWC translate reduction THP disabled THP enable assume trend estimate reduction PWC translate execution saving percentage PWC saving directly translates execution saving benchmark  walker cycle performance counter information calculate  equation  tps active  ratio memory reference eliminate via simulation  lbm ratio DTLB eliminate perform calculation colt rmm speedup native execution core alone tps achieve average performance improvement rmm colt maximal ideal saving eliminate tlb tps speedup native smt speedup native execution hardware smt thread compete core tlb resource speedup native smt tps achieve average performance improvement rmm colt maximal ideal saving tps relative  smt alone absolute cycle spent increase smt application experienced magnitude slowdown core resource overhead percentage execution cycle perform evaluate impact external fragmentation tps examine physical memory heavily load server linux kernel version transparent memory compaction enable memory utilization server benchmark memory memory error experimentally uninteresting proc  proc pid  identify memory contiguity available percentage memory singular KB MB percentage memory allocation KB coverage takeaway heavily load fragment significant intermediate contiguity exist leveraged tps portion memory contiguity exclusive exist conventional memory coverage various dumped memory input pin virtual memory simulator evaluate potential tps fragmentation tps attain significant reduction tlb memory compaction throughout simulation  minimal benefit tps fragmentation due random access memory behavior benchmark intermediate limited benefit memory access almost spatial locality similarly memory footprint benchmark xsbench graph significant reduction tlb occurs benchmark exhibit locality memory reference memory benchmark  perform memory compaction initial allocation incremental memory compaction tps incrementally reduce tlb memory compaction significant memory contiguity non conventional exists tps advantage DTLB eliminate tps increase OS allocator complexity affect application runtime percentage relative execution workload OS allocator memory intensive workload relative execution average percentage unrealistic increase due tps significant application slowdown percentage execution spent investigate tps actual runtime utilization across benchmark application xaxis workload utilizes nearly available benchmark tend relatively conservative promotion policy relatively unique ultimately enables tps eliminate nearly tlb previous tps per benchmark related virtual memory address translation tlb remains active research prior excessive significantly degrade performance application suffer limited tlb tlb tlb non trivial performance degradation redundant memory mapping rmm closely related tps rmm leverage arbitrary contiguous virtual contiguous physical frame alternative translation mechanism operating rmm maintain concurrently standard virtual address physical frame hardware cache entry tlb tlb cache entry alternative mechanism hardware translate virtual address physical address entry rte descriptor limit offset protection information translation tlb tlb tlb parallel translation tlb translation pte subsequently construct instal tlb translation alignment restriction likely rmm amenable external fragmentation rmm introduces software complexity maintain parallel virtual physical address mapping rmm additionally exist grain lock scheme manage concurrently superpages already utilized processor prevalent intel approach limited choice conventional limitation conventional coarsegrained oppose offering tailor evaluate throughout intel  sparc mechanism software tlb structure pin tlb entry improve performance tps largely orthogonal approach eliminates altogether software tlb approach tps accelerate  split address configurable approach limit benefit despite superpages variability available approach evaluates relocation approach merge promote frame reservation eager allocation approach reduces perform extraneous memory addition considers software manage tlb hardware additional hierarchical radix ingens purely operating proposal significantly improves transparent offering cleaner tradeoff memory consumption performance latency hawkeye another OS technique improves upon ingens hawkeye balance fairness allocation across multiple performs asynchronous pre zero duplicate zero performs grain access measurement address translation overhead hardware performance counter tps increase improvement underlie hardware mechanism technique ingens hawkeye cooperatively improve tradeoff tps additionally finegrained metadata information OS offering choice OS research ingens hawkeye along commercial processor segmentation address translation processor segmentation without processor segmentation unlike segmentation approach tps adheres virtual memory paradigm enable benefit tps allows segmentation segmentation approach address translation alternative virtual memory memory application utilize translation entity hardware contiguous virtual address contiguous physical memory remain virtual address mapped physical memory exist virtual memory approach virtual address translate physical address via hardware hardware TLBs standard segmentation utilizes limit offset register within unlike tps mechanism application explicitly creates startup OS reserve contiguous physical memory application memory workload leverage  technique decouples address translation access permission however  explicit application sub TLBs colt cluster TLBs combine virtual physical translation tlb entry approach rely default operating memory allocator assign contiguous cluster physical frame contiguous virtual however approach limited translation per tlb entry hinders generality applicability data limit potential benefit various technique accelerate seek reduce tlb reduce eliminate tlb MMU cache reduce latency cache thereby skip memory access currently available processor cache PTEs data cache hierarchy reduce latency MMU cache pom tlb cache tlb entry memory reduce tps orthogonal approach eliminates altogether mechanism tps accelerate address translation overhead lower reduce tlb rate synergistic TLBs TLBs seek reduce improve tlb prior propose hardware pte prefetchers approach prefetch PTEs tlb translation however memory access predictability limit tlb prefetcher effectiveness application random access behavior tlb prefetching unlikely prior propose speculative translation tlb prefetching mechanism sequential relies address contiguity propose prediction technique allows associative tlb prior propose gap tolerant mechanism conventional  creation retire physical non contiguity available physical memory however tlb entry conventional limit tlb memory intensive application unlike approach tps creates translation tailor application data cache tlb tps approach improve translation latency prior grain memory protection identify similarity contiguity across conventional tps identifies contiguity tailor appropriate however approach exploit contiguity grain protection across tps leverage enhances address contiguity memory allocation compaction facilitate faster address translation greatly improve tlb rate prior virtual cache reduces translation overhead translate cache however  workload suffer tlb virtual cache shift translation cache hierarchy increase complexity synonym translation penalty incur physical address actually tps seek nearly eliminate VI conclusion conventional tlb limitation insufficient deliver scalable performance virtual memory translation tailor tps hardware improvement operating software additional memory significantly reduce tlb memory reference improve tlb