gpus accelerate throughput application magnitude memory bandwidth traditional cpu however capacity bandwidth memory tends relatively buddy compression architecture novel compression utilize buddy memory host disaggregated memory effectively increase memory capacity gpu buddy compression split compress memory entry bandwidth gpu memory buddy memory compressible memory entry access completely gpu memory incompressible entry source data gpu memory buddy compression compressibility expensive movement allocation buddy compression achieves average effective gpu memory expansion representative hpc application training perform within unrealistic memory limit buddy compression attractive performance conscious developer additional gpu memory capacity  gpus widely memory footprint application performance compute hpc DL hpc application  simulation model fluid molecular dynamic grown model DL network model gpus batch gpu possibly utilization accuracy issue despite increase memory requirement compute gpus likely prioritize memory capacity parallel core bandwidth memory HBM limited package chip periphery capacity gpus HBM stack site maximum capacity 2GB meanwhile non HBM graphic ddr memory cannot driven rank per channel channel already practical pin limit accelerator gpus relatively limited maximum capacity CPUs network attach device memory entry compress sufficiently access buddy memory NVLink GB duplex GB NVSwitch cpu memory buddy gpu storage compress memory buddy storage alternative incompressible traffic unused peer gpu memory disaggregated memory target buddy compression NVLink memory buddy storage overall organization nvidia DGX node currently application memory footprint resort unattractive option compensate limited gpu memory gpus capacity purpose alone  utilize resource explicitly orchestrate data movement cpu gpu within device memory limitation algorithmic complexity rely gpu memory access unified memory automatically  device memory limit performance explores memory compression performant gpu memory expansion alternative memory compression CPUs gpu architecture offs cpu compression technique assume compress allocate compressibility allocation prohibitively expensive gpus due immense memory bandwidth additionally domain specific compression propose footprint gpu workload purpose compression capacity remains unexplored buddy compression compress memory allocation gpu device memory acm annual international symposium computer architecture isca doi isca buddy memory bandwidth interconnect cacheline memory entry sufficiently compress source completely device memory source device buddy memory allocation movement compressibility data bandwidth interconnect NVLink   enables ensures overhead access buddy memory data compress gpu device memory remote memory gpu bandwidth interconnect suitable buddy memory maintains compression ratio performance avoid complexity performance concern cpu memory compression approach gpus summarize contribution introduce purpose compression increase memory capacity gpus buddy compression unique additional data movement compressibility data depth analysis memory gpu workload representative data derive insight effective gpu compression buddy compression achieves average hpc DL training compression performs within unconstrained memory capacity gpu finally DL training understand benefit offs buddy compression expand gpu memory capacity II overview  target workload related hpc workload previous hpc domain suggests gpu memory benefit application fluid dynamic prediction however scientist currently virtual address unified memory multiple gpus gpu capacity limit buddy compression performant alternative approach subset  OpenACC cuda version doe benchmark HPGMG lulesh accessible simulatable representative hpc application subset chosen confidence representativeness data benchmark benchmark consult domain expert affirm input data benchmark reasonably representative omit benchmark fail personal communication positive affirmation data lulesh HPGMG  nvidia  benchmark  pgi nvidia spec benchmark commonly prior compression DL workload gpus currently popular choice training neural network network deeper wider data inevitably memory capacity convolutional neural network DL workload alexnet inception  vgg resnet caffe framework imagenet dataset additionally memory network  english model domain specific propose across stack address DL memory capacity challenge DL specific footprint reduction scheme asynchronous offload data training iteration buddy compression orthogonal possibly complementary approach expand effective gpu memory capacity approach allows footprint DL training algorithm buddy compression elide communication compressible data whereas DL offload transfer data iteration reduction bandwidth improve performance likely allows buddy compression significantly overhead addition buddy compression potentially tandem approach instance buddy compression conjunction vDNN layer gpu memory rely vDNN retire prefetch layer host memory background memory compression memory compression various expand effective cpu memory capacity operating compress swap reduce disk numerous proposal accelerate cpu memory compression hardware however explore II approach inefficient inapplicable purpose gpu memory capacity compression due architectural difference frequent compressibility gpu data graphic pipeline gpus domain specific lossy texture memory compression reduce footprint graphic texture knowledge hardware compression currently purpose compute workload gpus buddy compression target future gpu envision buddy compression compose multiple gpu node bandwidth NVLink interconnect source remote memory currently available remote memory memory cpu unused peer gpu memory NVLink disaggregated memory appliance extension technology explore server average compression ratio allocate memory benchmark equally distribute memory snapshot entire benchmark compression ratio calculate relevant gpu technology bandwidth interconnects bandwidth interconnects enabler buddy compression apply gpu bandwidth interconnect host memory recent bandwidth interconnects NVLink  NVLink alleviate communication bottleneck multi gpu NVLink  duplex unidirectional bandwidth per channel compute gpus NVLink channel per gpu offering bidirectional bandwidth 0GBps duplex  pcie  pcie connection nvidia DGX workstation sixteen gpus NVLink switch NVLink channel remote access memory available amd gpus heterogeneous architecture HSA  recently intel gpus compute express link  unified memory UM bandwidth interconnects enable virtual address host processor multiple accelerator nvidia unified memory UM introduce cuda pascal gpus prominent non local UM request remotely access data gpu interconnect transparent data migration placement data variety heuristic UM memory oversubscription  gpu device memory access without programmer explicitly manage data movement however capability widely performance application frequent fault thrash unified memory significant slowdown compressibility gpu workload estimate gain compression compressible footprint gpu workload memory dump workload tesla gpu intercept gpu malloc api variant pin UM manage memory dynamically allocate device memory entire runtime workload channel denote NVLink connection NVLink connection brick link slot memory dump allocate device memory kernel boundary closest compression ratio benchmark compression BPC entire compression ratio optimistic assume available compress memory entry assume pack overhead memory entry individually compress occupy average geometric compression ratio hpc benchmark DL training average compressibility prior report cpu workload attribute percentage homogeneous data allocation uniform datatype prior establish BPC homogeneous data homogeneity prevalent gpu workload compressibility compressibility frequent gpu benchmark  cpu workload seismic zero slowly  compression ratio execution although overall compression ratio DL workload roughly constant compressibility frequently individual memory entry variability DL framework perform asynchronous gpu memory allocation software manage memory pool reuse memory location variety purpose program execution comparison cpu trend data compressibility decrease prior cpu memory approach allocate memory entry  overflow suffer additional data movement memory entry overflow painful gpus CPUs cpu workload  pagerank graph overflow rate per instruction average overflow rate gpus per warp instruction average across workload difference stem throughput gpu memory workload characteristic DL training furthermore previous gpu allocation incurs  gpu driver latency sequential bottleneck otherwise  previous cpu memory compression relies allocation data movement gpus buddy compression address memory entry compress occupy sector entry compress target compression ratio sector access buddy memory challenge avoid movement due memory entry overflow   buddy compression allows programmer DL framework annotate memory allocation device memory allocation instance user 4GB data gpu 2GB memory capacity data allocate target compression data allocate gpu device memory grain compression opportunistically data reduce device resident allocation memory entry compress sufficiently NVLink  buddy memory overflow storage data compressible memory entry source completely gpu device memory incompressible memory entry source device buddy memory buddy compression strip data sector access granularity gpu memory GDDR  GDDR HBM gpus allocation target compression ratio sector per memory entry mapped device memory mapped buddy memory therefore memory entry compress completely device memory otherwise latter entry pre allocate buddy memory location compression algorithm hardware memory compression algorithm compression rate compression algorithm compression BPC attractive buddy compression achieves robust compression ratio across hpc DL workload buddy compression algorithm agnostic hardware compression algorithm compression granularity cpu memory compression strategy operates cache granularity avoid modify rmw overhead buddy compression decision compression granularity gpu cache histogram compress  across workload buddy compression frequently compression aligns nicely access granularity gpu memory buddy memory carve boot driver  contiguous chunk pin buddy memory gpu directly access host cpu eliminate coherence issue address translation buddy memory buddy memory corresponds maximum target compression ratio gpu maximum target compression ratio carve gpu device memory memory entry sector buddy memory gpu carve considerable significantly host memory aggregate gpu memory capacity DGX node aggregate 2GB gpu versus TB cpu memory sierra node 4GB gpu versus 6GB cpu memory summit node 6GB gpu versus 2GB cpu memory  node 4GB gpu versus 4GB cpu memory prior report host memory remains underutilized disaggregated memory pool capacity address translation access compress data additional address translation metadata informs target compression ratio  compress target ratio address buddy memory access memory entry compress target ratio global physical address buddy memory carve global buddy address register  TLBs augment information compress target compression ratio offset buddy global address MB KB gpu maximum target compression ratio 2GB maximum gpu capacity gpus CPUs entry pte already contains potentially unused metadata documentation nvidia pascal gpus PTEs unused dedicate  attribute texture compression repurposed purpose workload buddy compression address translation metadata buddy compression gpu pin cudaMalloc memory cpu UM ATS memory additional address translation mechanism simulation future buddy compression address translation metadata gpu non gpu peer access translation metadata cannot request buddy compress gpu memory aspect memory ripe future compressibility aware migration heuristic ability dynamically compression target migration memory entry metadata compress memory entry metadata metadata dedicate driver allocate device compression algorithm compress BPC granularity average compression ratio algorithm histogram compress across workload BPC compression algorithm compression BPC delta immediate frequent compression bdi FPC zero compression fibonacci compression massively parallel compression mpc gpu tlb  device memory NVLink  memory controller metadata cache metadata cache entry per memory entry compressor decompressor core architectural overview metadata cache rate metadata cache compression metadata handle architecture  global buddy address register memory amount storage overhead chosen compression strictly per compress cache reserve metadata align provision reserve future compression metadata storage overhead buddy compression comparable cpu compression scheme metadata setup translation  offset address overall translation mechanism straightforward implement cache avoid metadata traffic workload locality metadata cache ratio function metadata cache application ratio KB metadata cache split slice per memory controller metadata cache entry split sector thereby prefetch metadata correspond memory entry metadata sector metadata interleave across HBM channel hash mechanism regular physical address interleave benefit buddy compression fault expense immense parallelism gpu increase overall throughput however driver fault handle remote non distribute gpu fault expensive compressibility data memory decrease allocation prior cpu memory compression scheme fault overhead gpus render prior cpu compression scheme untenable buddy compression unique compressibility memory entry affect allocation compressibility movement allocation compress  exceeds device allocation upper sector spill buddy memory compress decrease stale access buddy memory allocation freed translation overhead memory bandwidth occasional bottleneck gpus accordingly fruitful research bandwidth compression gpu memory buddy compression compression amplify bandwidth capacity gpu memory however earlier compression capacity additional metadata access translation compress address emphasize importance reduce metadata translation buddy compression metadata buddy memory carve physically contiguous address offset straightforward reduce buddy compression overhead buddy compression overhead access buddy memory unexpectedly compression profile target compression ratio target compression ratio important aggressive compression ratio memory entry exceed sensitivity compression ratio buddy memory access optimization optimization apply successively zero optimize contains per allocation compression allocate device memory buddy memory access target compression ratio profile pas representative dataset hpc workload profile pas dataset dataset  DL workload brief profile pas batch embed training platform pytorch tensorflow understand compressibility granularity programmer annotates memory important annotation granularity depends spatial compressibility naive buddy compression considers conservative target compression ratio program granularity coarse naive mechanism achieves overall compression ratio hpc workload DL workload access interconnect buddy memory hpc DL investigate appropriate buddy compression annotation granularity spatial plot virtual address workload compressibility sub plot spatial compressibility memory allocate benchmark  signifies compressibility  denotes compressibility plot structure memory entry spatial locality compressibility varies significantly across benchmark hpc benchmark homogeneous compressibility distribution random DL workload FF HPGMG specific compressibility correlate array heterogeneous structs data structure although DL workload homogeneity hpc workload graph contains mostly mostly insight plot propose optimization buddy compression per allocation compression target benchmark homogeneous compressibility boundary overlap gpu malloc boundary allocation api compress allows capture behavior eliminate futile effort compress profile periodically snapshot memory compression ratio per allocation profile target compression ratio per allocation heuristic compression ratio buddy memory access compression ratio chosen conservatively aggressively reduce buddy memory access seismic exhibit average compressibility per allocation heuristic target allocation compression compensate lesser application compressibility evaluate buddy compression static target compression ratio per allocation dynamic target compression ratio reallocate around memory compression management complicate performant DL framework reuse memory allocation purpose compressibility allocation investigate DL memory allocation despite target compression ratio buddy memory access rate resnet squeezenet training interval program exhibit frequent compressibility per memory entry buddy memory access rate fluctuate individual memory entry frequently compressibility relatively unbiased net negligible buddy threshold meta heuristic benchmark allocation highly homogeneous compressibility per allocation target ratio decision straightforward however benchmark alexnet resnet memory allocation mixed compressibility meaning compression ratio frequency buddy memory access per allocation heuristic define buddy threshold meta heuristic limit buddy memory access buddy threshold achieves compression ratio buddy memory access performance sensitivity sweep buddy threshold alongside achievable compression ratio assume buddy threshold constraint buddy memory access remain infrequent hpc benchmark regardless buddy threshold selection due largely homogeneous allocation benchmark DL workload suffer frequent buddy memory access sensitive buddy threshold exception FF HPGMG achieve optimal compression compressibility palm seismic csp  FF HPGMG FF lulesh  alexnet inceptionv  vgg resnet spatial compressibility heatmap compressibility allocate gpu memory memory entry virtual address buddy threshold comparison marker earlier FF HPGMG peculiar strip compressibility array structs capture maximum compression FF HPGMG buddy threshold overall buddy threshold achieves balance compression buddy memory access parameter buddy compression evaluation mostly zero allocation spatial plot memory remain mostly zero across entire benchmark capture capacity expand opportunity allocation aggressive target compression ratio device memory additional encode tlb optimization increase compression ratio benchmark vgg mostly zero optimization impact buddy memory access rate highly compressible data almost device memory optimization hpc benchmark compression ratio DL enable optimization profiler mostly zero remain entire benchmark limited buddy memory access DL training interval significantly buddy memory carve however profiler constrain overall compression ratio evaluate evaluation buddy compression buddy threshold profile heuristic KB metadata cache per memory controller buddy memory carve gpu device memory maximum expansion gpu memory mostly zero allocation profile application dataset profiler report target compression ratio DL framework hpc user annotate gpu malloc api compression ratio buddy memory access achieve memory compression hpc compression DL workload average proportion buddy memory access hpc data trace DL workload  VA  demonstrate buddy compression compression infrequent buddy memory access zero optimize discus performance benefit buddy compression gpu memory capacity expansion buddy compression approach performance unrealistic unconstrained capacity gpu outperform UM oversubscription DL training estimate performance benefit increase gpu memory capacity evaluation methodology workload previously described II evaluate buddy compression effectiveness hpc DL network training workload representative trace benchmark reference dataset compression aware subsetting sensitivity compression ratio buddy memory access buddy threshold parameter methodology prior trace contains billion warp instruction corresponds dominant kernel benchmark execution exhibit average compression ratio entire benchmark execution trace DL workload span training iteration simulation infrastructure dependency driven gpu performance simulator others tab simulator configuration public information nvidia pascal gpu interconnect characteristic recent volta gpus non public microarchitectural detail configure microbenchmark prior SM model processor greedy warp schedule model multi cache hierarchy private cache sectored cache sector cache parallelism saturate dram bandwidth model software cache coherence private cache gpus device memory consists HBM channel split across memory controller MCs gpu buddy memory NVLink channel unless otherwise conservatively model decompression latency dram cycle prior metadata cache KB associative per MC performance impact evaluate bandwidth compression cache device memory compression increase effective memory capacity alter dram bandwidth latency affect performance simulator model latency overhead access buddy memory latency twice local gpu HBM access prior characterization agreement correlation coefficient simulated actual cycle spent gpu across variety benchmark correspond GPGPUSim correlation coefficient widely academic simulator motivation proprietary simulator  magnitude benefit allows simulate realistic workload GPGPUSim simulator correlation slightly configuration evaluation configuration correlate silicon tab performance simulation parameter core ghz greedy warp scheduler per SM max thread warp per SM cache KB private texture cache per SM KB dedicate scratchpad per SM MB slice chip MCs mhz HBM channel gbps NVLink channel gbps duplex buddy KB metadata cache KB slice compression decompression latency cycle interconnect bandwidth swept later evaluation performance relative unconstrained capacity gpu performance buddy compression unrealistic gpu memory limit  gpu idealize memory density currently multiple rank per channel disregard impact signal integrity memory interface memory generally feasible gpus apart increase effective memory capacity buddy compression various performance gpu dram bandwidth compression alone performance amplify memory bandwidth program dram locality hinder performance compression latency  grain random memory access buddy compression potentially performance penalty due relatively bandwidth latency buddy memory access extra dram traffic latency due metadata access interconnect contention simulator cycle silicon cycle simulator GPGPUSim simulation simulator cycle simulator correlate gpu slope drawn magnitude faster GPGPUSim enable simulation longer program linear regression drawn performance compression accounting capacity benefit interconnect bandwidth swept normalize uncompressed gpu 0GBps interconnect memory limit evaluates performance buddy compression alongside bandwidth compression sweep buddy memory interconnect bandwidth 0GBps duplex 0GBps NVLink analyze various performance contributor application specific performance benefit memory capacity DL training later IV bandwidth compression bandwidth compression increase effective memory capacity achieve overall speedup speedup DL training workload memory intensive regular memory access hpc application  slight slowdown bandwidth compression random irregular access memory request program sector bandwidth compression fetch entire compress cache  bandwidth FF lulesh slowdown despite regular memory access due largely compression decompression latency buddy compression performance buddy compression effective memory capacity possibly introduce additional overhead metadata cache buddy memory access interconnect bandwidth 0GBps achieves average speedup buddy compression interconnect bandwidth slowdown relative unconstrained capacity gpu metadata access benchmark palm seismic slowdown due metadata cache rate benchmark metadata cache rate metadata access discernible impact performance bandwidth sensitivity hpc benchmark rare buddy memory access negligible performance loss bandwidth interconnect interconnect bandwidth reduce however access buddy memory access considerable slowdown bandwidth sensitive application seismic FF HPGMG spends significant perform synchronous cpu gpu memory link bandwidth dramatically affect performance buddy compression normalize baseline gbps interconnect DL training relatively rate buddy memory access buddy memory access lack compression locality workload alexnet access buddy memory location slowdown relative unconstrained capacity gpu combine 0GBps duplex interconnect performance degenerate interconnect bandwidth 0GBps duplex connection slowdown recently developed gpu interconnects enable technology buddy compression slowest link evaluate gbps duplex faster recent pcie generation pcie  duplex bandwidth suffers average slowdown relative unconstrained capacity gpu however bandwidth interconnects NVLink 0GBps duplex buddy compression within performance unconstrained capacity gpu hpc benchmark within DL training insignificant factor due buddy memory access frequency program memory intensity buddy 0GBps interconnect bandwidth utilization average across benchmark average hpc DL simulate interference buddy memory regular interconnect traffic interconnect contention factor program performance simulate cpu gpu traffic gpu workload interconnect potentially gpu gpu communication multi gpu program gpu gpu communication slowdown due interconnect contention affect program portion overlap communication computation performance loss proportional bandwidth utilization buddy memory traffic simulate additional latency  ultimately performance apparent 0GBps interconnect net speedup despite increase buddy memory access latency apart performance proportional leakage buddy com palm seismic csp  FF HPGMG FF lulesh  alexnet inception squeezenet vgg resnet dynamic memory access normalize uncompressed bandwidth compression buddy compression dynamic memory access benefit compression  decrease access dram estimate dynamic access  compression buddy compression relative uncompressed gpu memory limit model memory snapshot program assume average buffer locality KB metadata cache rate assume HBM consumes per activation access data bandwidth interconnect consumes roughly conservatively assume toggle rate compress data account increase entropy interconnect ingres egress switch account NVLink header per buddy memory access packet model remote buddy memory metadata access HBM transaction despite generally conservative assumption buddy compression roughly memory uncompressed baseline generally HBM data movement comparison unified memory nvidia gpus faithfully buddy compression unified memory simulation feasible due complex host driver interaction migration policy within UM instead understand UM performance oversubscribed scenario  hardware performance  application oversubscription illustrate limitation UM program ibm tesla gpu via NVLink interconnect topology channel gbps  cpu gpu bandwidth compile  application manage pgi compiler flag oversubscription interposer hog gpu memory application startup application compiler flag pin allocation host memory slowdown dot slowdown varies widely negligible clarity slowdown thrash hamper UM performance severely application relative pin host memory UM primarily intend program omit visibility negligible slowdown pin memory omit csp due fail compilation seismic hang memory hog interposer runtime relative percent workload oversubscription UM pin  UM  pin UM pin palm UM palm pin overhead UM oversubscription cpu via NVLink channel gbps duplex nvidia gpu dot performance allocation pin host memory tune performance memory oversubscription prior observation UM oversubscription slowdown excessive without extensive hardware buddy compression suffers slowdown program  conservative gbps NVLink indicates buddy compression approach manage performance memory oversubscription software UM DL training increase memory capacity performance buddy compression uncompressed baseline memory limit ignores benefit memory capacity amplification hpc benchmark capacity allows benefit important quantify accordingly instead DL training workload quantify performance benefit compression buddy compression increase maximum mini batch training network gpus efficiency per gpu memory footprint DL workload memory footprint network training depends mini batch mini batch dataset device memory along intermediate data activation gradient memory footprint measurement DL training workload mini batch increase mini batch increase maximum titan gpu 2GB device memory difference batch eventually memory footprint grows almost linearly increase mini batch transition depends network parameter minibatch alexnet network parameter consume portion overall memory due fully layer relatively convolutional layer attribute later transition alexnet batch network transition increase memory footprint batch performance impact mini batch batch beneficial allows perform per iteration utilize resource project speedup network increase mini batch graph generate detailed analytical model project performance batch gpus increase memory footprint function batch pytorch titan project speedup function mini batch project speedup buddy compression batch accuracy across mini batch resnet cifar increase DL training mini batch mini batch relative training gpu utilization plateau buddy compression allows mini batch gpu memory relative speedup project analytical model mini batch titan gpu 2GB device memory average speedup  vgg achieve speedup respectively speedup workload without compression network unable mini batch resource utilization average speedup mini batch performance enable buddy compression buddy compression significantly improve performance capacity constrain gpus enable gpu utilization mini batch convergence mini batch apart improve computational throughput resource utilization mini batch affect DL training accuracy investigate resnet cifar dataset epoch titan gpu mini batch validation accuracy mini batch maximum accuracy despite individually tune hyperparameters additionally mini batch maximum accuracy converges slowly mini batch batch normalization jitter accuracy mini batch validation accuracy batch consistent previous prior report increase mini batch beyond detrimental network generalization however indicates tune loss function hyperparameters enable successful training mini batch DL network recent detection network  processing network GPT bert exceed gpu memory capacity input sample per gpu capacity limitation hurdle developer batch normalization batch effective developer resort horizontal mini batch across gpus  performs batch normalization across gpus communication overhead mini batch accuracy faster horizontal alone batch sustainable due inter gpu communication bottleneck simulation infrastructure unable DL training network buddy compression enables modest vertical combine horizontal sustainable  describes buddy compression generalpurpose mechanism increase user visible memory capacity gpus buddy compression enable highbandwidth interconnects remote memory pool overflow storage incompressible memory entry buddy compression achieves gpu capacity expansion across hpc DL workload performance penalty relative  gpu due unique compressibility additional data movement combination performance compression ratio buddy compression attractive performant alternative exist technology unified memory oversubscription