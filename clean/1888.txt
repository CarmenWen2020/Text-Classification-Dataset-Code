effective multi tenant neural network execution become important goal neural network accelerator emerge AI service consist heterogeneous neural network execution provider client AI accelerator improve effectiveness therefore ideal generation neural network accelerator simultaneous multi neural network execution fully utilize hardware resource however exist accelerator optimize neural network execution suffer severe resource underutilization multiple neural network mainly due load imbalance computation memory access task neural network propose AI multitasking AI MT novel accelerator architecture enables effective highperformance multi neural network execution AI MT fully utilize accelerator computation resource memory bandwidth compute memoryintensive task network execute parallel however highly challenge schedule load task neural network runtime without significantly increase chip memory overcome challenge AI MT creates  task compile layer multiple identical sub layer runtime AI MT dynamically applies sub layer schedule memory prefetching compute merge resource load memory eviction minimum chip memory footprint evaluation mlperf benchmark AI MT achieves speedup baseline schedule introduction neural network nns apply application thanks accuracy performance image classification recognition however execute neural network incur extremely operation data movement due increase network model input data therefore purpose processor cpu gpu neural network suffer performance effectiveness due limited amount compute resource consumption address issue researcher effort various neural network accelerator execute neural network effective efficiently operation neural network correspond author matrix vector operation accelerator adopt dimensional processing PE array accelerator aim utilize PEs concurrently minimize data movement various purpose researcher propose various dataflow mechanism neural  PE array mapping mechanism maximum hardware utilization minimum data movement AI service benefit accelerator target neural network execution effective execution multi tenant neural network become increasingly important provider due service consists AI algorithm execute heterogeneous neural network google vision translation service mlp cnn rnn execution respectively emerge complex service processing neural network simultaneously provider minimize operation application server satisfy quality service therefore sever architect dire AI acceleration architecture enable effective performance  network execution exist neural network accelerator however cannot effective multi tenant neural network execution conventional accelerator target neural network execution naively enable multi neural network execution execute neural network sequence layer neural network iteratively however approach creates compute memory intensive execution specific resource severe underutilization furthermore existence inter layer dependency within layer prevents layer execution layer completion resource underutilization per layer transition enable effective multi neural network execution aim conventional AI accelerator multi neural network execution minimum maximize hardware utilization grain dependency task resource intensity execute parallel achieve goal propose AI multitasking AI MT novel accelerator architecture enables effective acm annual international symposium computer architecture isca doi isca performance multi neural network execution AI MT fully utilize accelerator computation resource memory bandwidth grain compute memory intensive task network heterogeneous task execution latency runtime execute parallel AI MT minimizes chip memory capacity requirement evict allocation grain task AI MT layer multiple identical sub layer compile sub layer statically PE array mapping granularity sub layer SRAM requirement identical sub layer execution define phase load chip SRAM memory MB execution phase input processing load compute CB execution dynamically execute MBs CBs resource load runtime AI MT exploit hardwarebased sub layer scheduler scheduler dynamically schedule MBs CBs dependency satisfied fetch dependency MBs memory prefetching fully utilize memory bandwidth available dependency CBs compute merge fully utilize compute resource available lastly schedule evict SRAM capacity critical MBs memory eviction minimize chip memory capacity requirement evaluate AI MT architecture representative neural network workload mainly mlperf benchmark AI MT successfully utilizes processing memory bandwidth minimize SRAM capacity requirement thanks resource utilization AI MT achieves speedup baseline schedule sensitivity analysis workload batching memory capacity AI MT significantly reduces chip memory capacity requirement summary contribution multi neural network acceleration propose  novel AI acceleration architecture enable  multi neural network execution efficient schedule AI MT exploit hardware schedule efficiently schedule dependency sub layer task performance resource utilization  significantly improves hardware utilization significant performance improvement minimum SRAM requirement AI MT significantly reduces chip SRAM capacity requirement critical contribution future scalability knowledge propose accelerator architecture enable effective multi neural network execution conv output pool conv FC classifier input neural network input feature output feature filter conv layer neural network conv layer II background neural network neural network consist various layer neural network consists convolutional conv fully FC pool pool layer network layer output feature layer input feature layer layer perform operation input feature operation conv layer output layer computes dot input feature filter bias filter entire input feature layer generates output feature another FC layer performs matrix multiplication input feature matrix FC layer conv layer computes dot dimension input feature dimension kernel perform operation output lastly pool layer reduces input feature apply min pool layer layer minimum input feature pool maximum average baseline neural network accelerator architecture construct baseline architecture adopt conventional systolic array architecture google tpu architecture purpose core tpu model processing PE array array bfloat  accumulate mac addition tpu core incorporates HBM GB TPUv core onchip SRAM buffer mitigate memory bottleneck inference training PE array target server neural network inference commonly utilizes reduce precision integer core processing PE array chip buffer input output HBM accumulator norm activation pool input fetcher fetcher PE PE PE PE PE PE PE PE PE PE PE PE PE PE PE PE PE PE PE PE PE PE PE PE PE PE PE PE PE PE PE PE baseline neural network accelerator architecture input feature filter PE array PE partial PE mapping convolution layer systolic array architecture width TPUv HBM technology 0GB core TPUv assume accelerator physically decouple buffer input feature output feature filter matrix buffer multiple simultaneously input PE array output buffer neural network execution baseline architecture load HBM unified buffer PE array performs layer operation array intermediate dedicate perform subsequent operation activation normalization pool lastly accelerator output buffer reuse input layer baseline architecture buffering prefetch PE array computation hiding fetch latency systolic array architecture systolic array architecture dimensional PE array maximize data reuse directly adjacent PEs therefore systolic array architecture reduce consumption reduce data movement PE array chip memory systolic array operates conv layer filter mapped PE array conv operation TPUv memory bandwidth officially disclose assume HBM bandwidth per core TPUv GB CBs MBs CBs MBs conv FC layer execution CBs MBs CBs MBs sub layer execution CBs MBs CBs MBs sub layer execution prefetching layer sub layer granularity execution input mapped PE array fed array manner PE multiplies input partial finally PE update partial input input PE array another filter scheme layer iteration memory bandwidth requirement FC layer matrix filter filter filter mapped array filter PE mapping dim dot operation addition PE array dim array determines filter runnable concurrently dim array iteration execute filter therefore systolic array performs dim dim array iteration layer conv layer reuses relatively filter assume PE array mapping PE array partition input feature systolic array dim dim iteration execute layer baseline schedule prefetching granularity schedule granularity define sub layer PE array mapping iteration accelerator configuration determines sub layer layer sub layer layer granularity sub  schedule MB indicates memory bandwidth usage fetch CB indicates PE array usage adopt sub layer granularity schedule grain task adopt prefetching fetch sub layer mapping previous CB execution ratio computation memory prefetching latency sub layer fifo schedule sub layer RR schedule sub layer greedy schedule multi neural network execution motivation multiple neural network execution conventional accelerator target neural network execution creates compute memory intensive execution specific resource severe underutilization portion computation memory prefetching latency layer vgg earlier layer compute intensive memory bandwidth utilization remains significantly later layer highly memory intensive utilization computation resource quickly addition inter layer dependency prevents layer execution layer completion incur resource underutilization per layer transition layer sub layer improve underutilization sub layer resource intensity multiple neural network potential alleviate layer neural network freely schedule without dependency issue achieve goal baseline accelerator optimal multi neural network schedule grain dependency task resource intensity execute parallel maximize resource utilization however highly challenge conventional accelerator effectively schedule effective resource idleness neural network execution analyze resource underutilization multiple neural network reasonable schedule mechanism fifo robin RR greedy algorithm simplicity assume scenario neural network layer multiple sub layer scenario sub layer MB CB execution fifo schedule timeline fifo mechanism performs neural network effectively network wise serial execution network wise serial execution consecutive sub layer layer resource intensity likely incur resource idleness RR schedule timeline RR schedule repeatedly selects sub layer neural network fairness network schedule outperform fifo schedule schedule sub layer resource intensity however static schedule likely severe resource underutilization due mismatch resource intensity enable flexible schedule greedy algorithm dynamically selects MB currently execute CB algorithm likely outperform fifo RR mechanism suffer resource idleness due mismatch resource intensity resource idleness workload neural network benchmark mlperf inference vgg hardware utilization cycle accurate simulator various multi neural network execution neural network distinct resource utilization characteristic instance combine memory intensive network vgg FC layer GNMT multiple compute intensive network combine resnet resnet mobilenet balance distribution CBs MBs iteratively memory intensive workload properly amount CBs compute intensive workload synthesize workload cycle accurate neural network acceleration simulator described performance resource utilization schedule mechanism RR greedy shortest SJF SJF mechanism max MB cycle CB cycle resource utilization obtain sub layer granularity RR mechanism compute memory bandwidth utilization robin schedule algorithm RN resnet RN resnet MN mobilenet speedup schedule mechanism baseline sub layer fifo schedule scenario RR mechanism incurs severe resource underutilization due frequent mismatch MBs CBs resource demand  compute intensive workload GNMT cnns slightly resource utilization RR schedule mechanism significantly suffers limited resource capability relative performance schedule mechanism fifo schedule mechanism SJF suffers resource idleness repeatedly schedule network eventually performs fifo network wise schedule greedy mechanism outperforms fifo mechanism improvement optimal performance due limited capability MB CB limited SRAM capacity II introduce prefetching fetch sub layer mapping previous CB execution prefetching enable easy amortize computation memory bandwidth underutilization schedule compute intensive sub layer memoryintensive sub layer ignores fairness CBs MBs infinite SRAM capacity CBs MBs SRAM capacity limited SRAM capacity impact limited SRAM capacity multi neural network execution performance improvement impact prefetch aware schedule fifo schedule mechanism propose prefetching significantly improves compute  utilization however schedule fundamental limitation due significant SRAM capacity requirement scheme prefetch MBs SRAM CBs therefore prefetched MBs evict CBs execution SRAM becomes quickly disables MB prefetching SRAM capacity prefetch execute layer mlperf workload estimate accumulate latency CBs assume accelerator prefetches MBs later layer layer execution batch layer execution MB SRAM fully utilize memory bandwidth capacity pressure accumulate multiple neural network execution limit default SRAM capacity MB propose reduce capacity requirement prefetch SRAM buffer layer mlperf inference benchmark goal observation goal architect effective multi neural network execution accelerator effective multi neural network execution fully utilize hardware resource effective schedule minimize SRAM capacity requirement future scalability IV AI multitasking architecture propose AI multitasking AI MT simultaneous multi neural network execution processor architecture AI MT fully utilize accelerator computation resource memory bandwidth grain computation memory access task network schedule  task runtime execute parallel compile AI MT layer multiple identical sub layer grain computation memory access task AI MT generates sub layer schedule sub layer metadata grain task management runtime assume google tpu CISC instruction utilize sublayer granularity operation matrix multiplication PE array AI MT naturally layer sub layer compile without sacrifice performance runtime AI MT dynamically schedule grain computation memory access task multiple network  dependency MBs CBs refer sub layer schedule applies load balance schedule mechanism IV load balance schedule mechanism fetch dependency MBs fully utilize accelerator memory bandwidth resource dependency CBs fully utilize compute resource available load balance schedule mechanism AI MT applies MB eviction schedule evict SRAM capacity critical MBs minimize chip memory capacity requirement algorithm latency estimation model  conv MB cycle cyc per array CB cycle array batch  iters dim dim  FC MB cycle cyc per array array CB cycle batch  iters dim array dim overview overview AI MT  task management AI MT feature sublayer schedule candidate queue management sub layer schedule sub layer metadata grain task execution neural network sub layer schedule layer information MB CB cycle sub layer dependency layer AI MT initializes sub layer schedule compile statically information cycle MB CB layer AI MT MB candidate queue CB candidate queue dependency MBs CBs AI MT dependency MBs CBs refer sublayer schedule enters task correspond candidate queue lastly AI MT address sub layer management AI MT prefetches MB execution consumes CB execution AI MT address CB execution sub layer schedule initialization compile AI MT initializes sub layer schedule AI MT estimate cycle MB CB layer cycle sub layer iters algorithm algorithm cyc per array indicates cycle prefetch PE array indicates cycle input inject PE array output generate mention II adopt mechanism conv layer FC layer data reuse conv layer reuses filter assume PE array mapping PE array partition input feature cycle MB cycle prefetch HBM SRAM PE array cycle CB cycle partition input feature partition input feature AI multitasking scheduler candidate queue CBs MBs schedule logic algorithm schedule CB queue network sub layer schedule network MBs CBs layer indegree iters cycle indegree iters cycle addr cycle   network MBs CBs layer indegree iters cycle indegree iters cycle addr cycle   network MBs CBs layer indegree iters cycle indegree iters cycle addr cycle   estimator algorithm layer network core PE array chip buffer input output HBM accumulator norm activation pool input fetcher fetcher AI multitasking PE PE PE PE PE PE PE PE PE PE PE PE PE PE PE PE management overview AI MT otherwise FC layer assume PE array individual FC layer reuses batch relatively reuse amount conv layer cycle MB cycle prefetch HBM SRAM PE array cycle CB cycle input feature individual input feature AI MT update indegree layer dependency layer later AI MT indegree dependency previous layer previous layer layer layer layer multiple consequent layer AI MT layer  layer candidate queue insertion refer indegree iters layer AI MT dependency MBs CBs insert correspond candidate queue AI MT indegree layer dependency previous layer sub layer layer sub layer whenever sub layer MB CB AI MT refers layer sublayer schedule layer resolve dependency layer AI MT decrease indegree layer MB CB insert layer MB CB correspond candidate queue indegree becomes AI MT iters dependency sub layer layer iters indicates remain sub layer MB CB AI MT executes sequentially decrease iters MB iter com indicates CB correspond MB already schedule therefore AI MT dependency previous MB MB MB correspond CB whenever MB CB schedule  decrease correspond iters candidate queue AI MT CB dependency resolve remain iters MBs CBs iters MB CB AI MT CB CB candidate queue decrease iters CBs prefetched management AI MT prefetches MB execution consumes CB execution AI MT address CB execution AI MT address AI MT feature management sub layer schedule empty buffer management index refers lastly prefetched layer execution combine feature AI MT easily address CBs execute AI MT prefetches memory controller allocates refer AI MT refers management index update newly allocate AI MT update newly allocate layer mechanism enables CB address sequentially CB schedule AI MT refers correspond CB update refer management CB load balance schedule schedule scenario apply RR schedule mechanism MB CB incur resource idleness memory bandwidth idleness CB MB PE array idleness MB CB increase resource utilization memory bandwidth PE array propose balance schedule mechanism scheme MB prefetching CB merge MB prefetching reduces memory bandwidth idleness aggressively prefetching MBs later sub layer improves CB resource utilization earlier MB execution resolve dependency correspond CB earlier however MB prefetching resolve CB dependency incurs PE array idleness dependency resolve CB merge scheme improves PE array utilization MB CB pace memory MB prefetching MB prefetching prefetches MBs whenever remain SRAM capacity CBs MBs baseline RR schedule CBs MBs dependency incurs PE array idleness memory MB prefetching CBs MBs avl CBs avl CBs compute CB merge load balance schedule regardless sub layer boundary MB prefetching mechanism MB prefetching prefetches MB MB successfully amortizes resource requirement utilize idle memory bandwidth moreover earlier execution MB resolve correspond CB earlier PE array idleness reduces compute CB merge prefetched MBs become correspond CBs incurs PE array underutilization due dependency PE array idle schedule MB resolve dependency generates CB candidate improve PE array utilization CB merge ensures available CBs schedule MB CB merge schedule MBs CBs whenever execute MB CB merge selects MB schedule multiple CBs cycle schedule CBs become schedule MB mechanism PE array MB execution ensure available CBs CB merge variable avl CB cycle available CBs dependency correspond MB resolve avl CB becomes pre define threshold CB merge extends priority MBs cycle correspond CBs avl CB increase faster schedule MB CB merge becomes avl CB schedule MB schedule scenario CB merge algorithm MB prefetching CB merge whenever previous MB target none MB MB CQ MB cycle RM AV CB threshold MB CB cycle MB cycle target MB target MB target none stall execute CB AV CB decrease execute CBs MB CQ pop target MB target cycle RM target cycle AV CB max AV CB target cycle target CB cycle CB CB CQ CB MB CB CQ pop CB CB CB cycle CB SQ CB whenever previous CB target CB SQ pop irst RM target cycle schedule CB merge selects CB merge selects selects schedule CBs schedule MB CB merge selects originally available CBs correspond CB CB merge successfully extend available CBs MBs CB merge CB schedule CBs already cycle available CBs combine MB prefetching CB merge algorithm AI MT scheduler assigns MBs CBs MB prefetching CB merge algorithm scheduler variable scheduler MB CB cycle schedule MBs CBs whenever previous MB CB scheduler cycle MB CB scheduler avl CB cycle available CBs avl CB increase MB resolve correspond CB decrease whenever MB avl CB decrease cycle schedule MB overlap MB avl CB overlap schedule MB scheduler avl CB zero scheduler fails MB stall execute CB avl CB decrease cycle execute CB scheduler RM indicates MB cycle remain SRAM capacity scheduler RM remain SRAM capacity prefetch MB initialization RM cycle SRAM capacity increase decrease whenever MB allocates CB variable scheduler schedule MB CBs scheduler assigns MB whenever execute MB MB prefetching scheduler iterates MB candidate queue selects MB target algorithm cycle RM avl CB pre define threshold scheduler extends avl CB quickly priority MBs cycle correspond CB none MB candidate queue RM scheduler execute CB recover correspond SRAM capacity scheduler selects MB target CBs overlap MB execution CBs insert CB SQ CB queue earlier schedule CBs SRAM capacity aware schedule SRAM balance schedule mechanism incur memory bandwidth idleness CB execution CB execution AI MT  prefetch multiple MBs fully utilize memory bandwidth SRAM capacity CB correspond MB execution remain SRAM capacity MB FC sublayer MB worsens scheduler MBs conv sub layer MB continuously algorithm alleviate memory bandwidth idleness limited chip SRAM capacity introduce MB eviction schedule evicts SRAM capacity critical MBs minimize chip memory capacity requirement MB CB algorithm simply iterates correspond candidate queue MB eviction priority SRAM capacity critical MBs cycle cycle correspond CBs FC sub layer MB MB occupies SRAM capacity correspond CB relatively recover amount SRAM capacity quickly AI MT schedule CBs SRAM CBs recover SRAM capacity quickly CBs MBs CBs SRAM MBs due SRAM capacity resource idleness SRAM capacity shortage CBs MBs MBs priority CB available SRAM threshold priority mechanism MB eviction CBs MBs halt resume CB CB penalty cycle PE array split mechanism MB eviction SRAM capacity aware schedule correspond SRAM mechanism CB due batch network configuration SRAM capacity becomes CB execution incurs memory bandwidth idleness alleviate issue MB eviction halt CB execution schedule CBs recover SRAM capacity quickly execute CB algorithm halt resume CB later AI MT status target CB recover CB candidate queue  insert target CB candidate queue pop target CB dependency target CB prevents CB earlier execution target CB AI MT execute input address addr remain cycle cycle sub layer schedule scheduler resume target CB later execution refer addr mechanism overhead acceptable improve resource utilization penalty cycle PE array halt CB resume reduce PE array resource utilization however cycle relatively CB execution potential improve resource utilization CB split correspond scheduler resume CB incurs additional SRAM consumption additional consumption acceptable CB usually compute intensive layer conv layer amount across PE array algorithm alternative software implementation AI MT propose hardware implementation  implement mechanism software implement schedule layer software framework tensorflow accelerator software implementation achieve performance improvement met software schedule sub layer execution  sub layer hide software schedule overhead sub layer split occurs infrequently avoid transfer medium pcie execution environment coarse grain sublayer execution cycle infrequent sublayer split software implementation effective hardware implementation aside software implementation potential hardware implementation effective alternative software implementation aggressive multi NN execution scenario faster accelerator aim heterogeneous neural network model execution model benefit finer grain split increase gap software schedule hardwarebased execution SRAM capacity contention transfer medium contention therefore hardware schedule without performance benefit without increase transfer medium bandwidth hardware implementation overhead hardware software collaborative approach promising another research direction evaluation evaluation setup evaluate AI MT extend systolic array cycle accurate simulator enable multi neural network execution baseline hardware parameter recent tpu specification PE array model reduce precision integer TPUv HBM technology GB core TPUv simulation framework implement physically decouple buffer input output feature II  dataflow II sub layer granularity schedule II SRAM buffer prefetching MB MB chip SRAM buffer input output feature multiple concurrent neural network workload architectural parameter summarize hardware architecture parameter parameter processing dimension processing array frequency ghz memory bandwidth GB chip SRAM input output MB chip SRAM MB II neural network workload configuration layer batch FC conv resnet resnet vgg mobilenet GNMT evaluate AI MT multi neural network benchmark synthesize mlperf inference vgg II summarizes configuration characteristic target workload target workload cnns rnn workload representative layer characteristic typical neural network workload report service provider GNMT assume embed lookup remains cpu neural network distinct resource utilization characteristic instance combine memory intensive network vgg FC layer GNMT multiple compute intensive network resnet resnet mobilenet balance distribution CBs MBs iteratively memory intensive workload properly amount CBs compute intensive workload multi NN execution latency speedup neural network workload fifo mechanism batch evaluate performance impact MB prefetching apply RR mechanism introduce AI MT achieves speedup baseline cnns GNMT vgg respectively overall workload AI MT achieves geomean performance improvement vgg memory intensive FC layer compute intensive conv layer earlier memory intensive layer reduce opportunity overlap resource demand sub layer GNMT achieves performance improvement consists memory intensive FC layer overlap resource demand sub layer apply CB merge MB prefetching neural network workload CB merge achieves speedup speedup AI MT network serial multi neural network execution baseline sensitivity batch baseline fully utilize compute resource improves geomean performance improvement workload MB prefetching varies colocated neural network due resource demand requirement lastly performance impact MB eviction AI MT achieves geomean performance improvement baseline batch cycle CBs performance improvement marginal schedule mechanism impact MB eviction CBs sensitivity batch sensitivity batch sensitivity batch multiple cnns GNMT verify impact MB eviction assume SRAM capacity accommodate input output feature execution batch attach MB eviction AI MT improves performance apply MB prefetching CB merge batch RN GNMT workload apply MB prefetching CB merge achieves speedup apply scheme achieves speedup sensitivity SRAM baseline batch increase CBs become limited SRAM capacity creates bottleneck fully utilize memory bandwidth MB eviction quickly recovers SRAM capacity release SRAM  MBs successfully achieves performance improvement batch reduction maximum performance improvement pcie transaction overhead transfer input output feature becomes dominant computation latency sensitivity chip SRAM speedup network serial multi neural network execution various chip SRAM assume input output buffer accommodate feature batch execute workload iteratively impact execution neural network server environment multiple neural network accelerator continuously SRAM capacity apply baseline schedule mechanism MB prefetching apply naive schedule mechanism schedule compute intensive neural network schedule memory intensive neural network prefetch amount CBs achieves ideal performance SRAM capacity  however overhead chip memory memory input output buffer MB buffer MB sub layer schedule KB  SQ byte management byte byte GB achieve ideal performance improvement evaluation environment apply MB prefetching greedy mechanism greedy mechanism SRAM capacity requirement due imbalance MB CB AI MT successfully achieves almost ideal speedup MB SRAM capacity overhead estimate static overhead memory AI MT CACTI technology estimation assume neural network execute concurrently evaluation setup overhead additional AI MT negligible input output buffer overhead AI MT enable network AI MT sub layer schedule per neural network maximum entry candidate queue network overhead per neural network sublayer schedule KB schedule mechanism negligible amount computation overhead CB accelerator performs mac operation per PE array cycle schedule mechanism dozen addition comparison operation MB schedule algorithm operation frequently VI discussion input output buffer capacity focus reduce SRAM capacity requirement minimal execute multiple neural network increase neural network increase batch increase SRAM capacity requirement currently assume SRAM dedicate input output feature becomes SRAM capacity available useful deploy preemption mechanism minimal input output feature mitigate overhead spatial aspect PE array utilization focus exploit temporal aspect PE array improve resource utilization improve resource utilization spatial aspect PE array address dimension neural network mapping strategy resource underutilization spatial aspect become non trivial issue CB dimension fully utilize mac operation PE array perform multiple CBs improve resource utilization within PE array vii related systolic array dnn accelerator systolic array architecture useful compute dnn model dominate matrix computation google tpu comprises dimensional systolic array embrace stationary dataflow  matrix performs accumulate operation reduce partial sum along moreover recent TPUs bandwidth memory HBM dedicate core avoid critical memory bandwidth bottleneck incur earlier version architecture similarly eyeriss utilizes systolic array exploit data reuse characteristic cnns leverage  dataflow maximize data reuse accumulation RF eyeriss handle various layer dimension flexible onchip network xilinx fpga   architecture operational mode  latency optimize mode adopt mapping strategy cnn layer adjust pipeline stage although demonstrate systolic array architecture highly effective dnns simultaneous multi network multi thread technique fully explore smt SA employ simultaneous multi thread technique address underutilization zero input however cannot handle underutilization layer resource usage characteristic schedule optimize dnn dataflows hardware accelerator employ various dataflows optimize data access communication classify neural network accelerator dataflow category data reuse characteristic  output stationary stationary dataflows spatial temporal data mapping PEs differently addition formal taxonomy dataflows employ recent NN accelerator mRNA maestro explore benefit various dataflow strategy hardware configuration performance analysis maestro analysis introduces compiler directive specify prefer dnn dataflow however challenge  multi thread account situation concurrent execution simultaneous multi dnn improve network performance adopt dataflow optimization split dnn layer multiple iteration multi dnn schedule demand architectural multi dnn schedule maximize hardware utilization reduce production  TensorRT concurrent dnn execution user multiple dnns gpus simultaneously addition  prophet address qos utilization multi dnn execution gpus  proposes preemptive schedule algorithm multi dnn explores various preemption mechanism reduce overhead although demonstrate diverse multi dnn schedule optimization effective meeting restrict latency requirement increase hardware utilization fail handle simultaneous execution multiple dnn model cannot obtain optimal performance smt decouple architecture conclusion propose AI MT novel processor architecture enables effective performance  network execution motivate severe underutilization exist accelerator mainly due load imbalance computation memory access task  proposes memory prefetching compute merge resource load minimize chip SRAM capacity requirement runtime AI MT applies  eviction schedule evicts SRAM capacity critical MBs combine AI MT successfully achieves performance improvement minimum SRAM capacity