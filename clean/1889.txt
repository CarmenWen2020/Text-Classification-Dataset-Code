SmartExchange algorithm hardware framework memory storage access computation efficient inference neural network dnns develop novel algorithm enforce specially favorable dnn structure layerwise matrix basis matrix sparse coefficient matrix non zero knowledge algorithm formulation integrates mainstream model compression sparsification prune decomposition quantization unified framework sparse readily quantize dnn enjoys greatly reduce consumption data movement storage dedicate accelerator fully utilize SmartExchange enforce improve efficiency latency performance extensive algorithm SmartExchange outperforms stateof compression technique merely sparsification prune decomposition quantization various ablation model datasets hardware SmartExchange boost efficiency reduce latency dnn accelerator benchmarked dnn model standard dnns compact dnn model segmentation model datasets index neural network compression neural network inference accelerator prune decomposition quantization introduction recently witness performance neural network dnns tremendously demand dnn intelligence resource constrain device limited storage resource however excellent performance dnns parameter external dynamic random access memory dram storage prohibitive consume massive data transfer dram chip memory processing PEs dnn deployment non trivial resource constrain scenario device motivate efficient domain specific accelerator dnn inference task dnn accelerator challenge alleviate data movement dnn inference mainly comprises accumulate mac operation denotes contribution propose SmartExchange representation data dependency achieve processing throughput via parallelism however mac operation incur significant amount data movement due data access consumes considerable sometimes surprisingly significant inference batch diannao inference consume data movement associate dram therefore minimize data movement improve efficiency dnn accelerator address aforementioned challenge propose SmartExchange spirit algorithm hardware strives memory storage access computation largely avoid dominant data movement dnn accelerator novel SmartExchange algorithm aggressively reduce consumption data movement storage associate dnn limit factor deploy dnn accelerator resource constrain device layer wise dnn matrix basis matrix coefficient matrix simultaneously enforce structural coefficient matrix sparse zero readily quantize non zero compact representation multiplication mac operation shift opera UI OOVBM  PNQVUFS SDIJUFDUVSF tions develop efficient SmartExchange algorithm blend training model datasets favorable decompose compact structure achieve propose algorithm fully leverage SmartExchange algorithm potential develop dedicate dnn accelerator advantage reduce storage readily quantize algorithm enhance hardware acceleration performance propose accelerator outperforms dnn accelerator acceleration efficiency latency respectively contribution summarize fold overall innovation algorithm hardware codesign framework harmonize algorithm hardware innovation maximize acceleration performance task accuracy specifically identify opportunity processing hardware reduce dram access advantage structure activation sparsity enforce correspond favorable structure algorithm dedicate effort accelerator architecture aggressively improve acceleration efficiency latency algorithm contribution SmartExchange algorithm hardware awareness unifies prune decomposition quantization highly compact structure boost acceleration efficiency inference accuracy loss equip training effectiveness SmartExchange algorithm benchmarked datasets dnns hardware contribution dedicate accelerator fully utilize SmartExchange algorithm compress quantize dnns minimize inference latency verify optimize accelerator dedicate simulator validate rtl propose accelerator achieves efficiency speedup organize II introduces background motivation describes formulation SmartExchange algorithm IV dedicate accelerator aim amplify algorithmic gain extensive manifest benefit algorithm accelerator SmartExchange VI vii summarize related conclusion respectively II background motivation neural network dnns usually consist cascade multiple convolutional conv pool fully FC layer input progressively conv FC layer described denote input activation output activation bias respectively conv layer input output channel input output feature filter stride respectively FC layer input output neuron respectively denote activation function relu function relu max pool layer reduce dimension feature via average max pool recently emerge compact dnns mobilenet EfficientNet introduce depth wise conv layer squeeze layer express description demand model compression dnn inference parameter dominate memory storage limit efficiency due associate data movement response model compression technique prune sparsification decomposition  prune sparsification prune sparsification increase sparsity dnns zero non significant usually interleave tune phase recover performance intuitive  zero zero magnitude recent establish advanced prune enforce structure sparsity convolutional layer exhibit due encode index overhead vector wise sparsity obtain compression rate accuracy wise unstructured sparsity decomposition another approach compress dnns decomposition rank decomposition compression model redundancy dnns correlation highly structure filter convolutional fully layer rank decomposition express highly structure filter matrix quantization quantization attempt reduce width data dnns shrink model memory saving simplify operation efficient acceleration addition combination rank decomposition prune compression ratio preserve accuracy motivation SmartExchange access  memory storage capacity com per extract commercial technology dram SRAM mac multiplier adder mac multiplication addition computation operation dnns commercial CMOS technology memory access correspond mac computation therefore promising efficient acceleration potentially enforce  structure aggressively  memory access computation motivate SmartExchange decomposition structure dnn decompose matrix enable reduce memory access computation operation shift operation vanilla network addition integration decomposition prune quantization SmartExchange motivate hypothesis potentially sparse structure recently algorithm perspective enforce wise sparsity matrix directly effective correspond decompose matrix factor additive multiplicative SmartExchange algorithm target hardware favorable representation combine activation representation sparse activation maximize efficiency gain summarize overall goal SmartExchange data movement access reconstruction MACs shift operation achieve goal concrete SmartExchange motivate fold seek compactness representation contribute mainly sparsity decomposition discover  sparse structure reduce multiplication workload reconstruction contribute mainly quantization nonzero PROPOSED  algorithm formulate SmartExchange decomposition knowledge SmartExchange algorithm unified formulation conceptually combine methodology compress dnns sparsification prune matrix decomposition quantization develop efficient algorithm SmartExchange algorithm append postprocessing dnn compression acceleration without compromise accuracy loss demonstrate SmartExchange algorithm incorporate dnn training achieve promising offs inference accuracy resource usage model memory access computational formulation previous compress dnns reduce correlation filter conv layer FC layer via decompose matrix seek decompose coefficient matrix basis matrix min  addition suppress reconstruction error define  decompose matrix factor display favorable structure compression acceleration decomposition usually construct matrix whereas vector layer enforce structure simultaneously aggressively boost efficiency highly sparse typical goal prune nonzero exactly representation compact involve multiplication rebuild simplify shift operation instead matrix propose SmartExchange algorithm highly sparse readily quantize therefore propose algorithm greatly reduces overall memory storage memory memory hierarchy minimize data movement SmartExchange rationale arises previous observation compose prune decomposition quantization combine matrix decomposition prune effectively compress model without notable performance loss innovative assumption non zero pre define discrete specifically picked compact representation  multiplication previous dnn compression cluster quantize data SmartExchange decomposition hence constrain optimization arg min    integer cardinality non zero width nonzero SmartExchange algorithm intractable due nonconvex constraint integer  constraint algorithm SmartExchange algorithm sparsify channel wise manner initialize iteration tol iteration tol maximum quantize fitting iteration iteration sparsity vector wise manner quantize introduce efficient heuristic algorithm iterates objective fitting feasible projection outline SmartExchange algorithm described algorithm iterate quantize quantization project nonzero  specifically normalize norm avoid ambiguity non zero define quantization difference fitting arg   arg   fitting fix update simply unconstrained sparsifying pursue compression acceleration simultaneously introduce  vector wise sparsity prune channel correspond factor batch normalization layer threshold manually layer apply channel wise sparsifying training epoch observation prune channel structure zero magnitude vector wise sparsity constraint manually per layer threshold channel vectorwise sparsity zero magnitude implementation convenience combine channel vectorwise sparsity bypass reading input feature correspond prune parameter storage access computation convolution operation meanwhile sparsity reduce encode overhead storage access computation reconstruction  iterate quantization fitting sparsification sufficient iteration conclude iteration quantize nonzero ensure  fitting update apply SmartExchange algorithm dnns SmartExchange algorithm processing knob SmartExchange trading achieve compression rate model accuracy compression rate accuracy loss rank basis matrix matrix otherwise minimize memory storage basis matrix conv kernel discus apply propose algorithm FC conv layer initialize simplicity SmartExchange algorithm FC layer fully layer RM reshape matrix RC apply SmartExchange algorithm specifically zero pad divisible SmartExchange algorithm apply reconstruction error tend due imbalanced dimension alleviate slice matrix along dimension SmartExchange algorithm conv layer convolutional layer reshape filter matrix SmartExchange algorithm apply matrix slice matrix along dimension reshaped treat FC layer procedure easily parallelize along axis output channel acceleration apply SmartExchange algorithm vgg network pre cifar tol maximum iteration decompose SmartExchange algorithm coefficient matrix basis matrix perform algorithm network without training accuracy validation overall compression rate overall compression rate network define ratio coefficient matrix basis matrix encode overhead FP SmartExchange algorithm training dnn SmartExchange algorithm retrain remedy accuracy regularize training desire coefficient matrix empirical approach alternate training dnn epoch apply SmartExchange algorithm ensure structure default iteration cifar imagenet alternate training improves accuracy maintain favorable structure analytic explore future incorporate SmartExchange algorithm regularization http github com  pytorch vgg cifar illustration 3D filter parameter notation rebuild correspond basis coefficient matrix IV PROPOSED  accelerator propose SmartExchange accelerator introduce principle consideration IV fully propose SmartExchange algorithm maximize efficiency minimize latency propose accelerator IV detail principle consideration propose SmartExchange algorithm exhibit potential reduce memory storage access device dnn inference however potential cannot fully exploit exist accelerator due rebuild operation SmartExchange algorithm restore unique opportunity explore coefficient matrix vector wise structure sparsity subsection analyze opportunity SmartExchange algorithm abstract principle consideration develop optimize dedicate SmartExchange accelerator minimize overhead rebuild thanks sparse readily quantize coefficient matrix SmartExchange algorithm memory storage data movement associate matrix greatly reduce II meanwhile fully utilize advantage SmartExchange algorithm overhead rebuild minimize critical ensure location rebuild properly specifically SmartExchange accelerator basis matrix rebuild restores basis matrix correspond coefficient within processing PEs stationary dataflow basis matrix elaborate principle context 3D filter operation SmartExchange algorithm decomposes matrix correspond 3D filter coefficient matrix basis matrix accord basis matrix reuse rebuild reuses coefficient matrix magnitude reuse opportunity basis matrix coefficient matrix dnn model therefore basis matrix illustration vector wise skip correspond activation reduce index overhead thanks enforce vector wise sparsity SmartExchange algorithm PEs res local memory within res minimize associate data movement res PEs minimize data movement rebuilt rebuilt data movement basis matrix reuse frequently dataflow matrix stationary fetch memory PEs correspond rebuilt advantage structure sparsity enforce vector wise sparsity SmartExchange algorithm coefficient matrix benefit vector wise skip memory access computation correspond activation reduce coefficient matrix encode overhead meanwhile opportunity vectorwise sparsity activation improve efficiency promising benefit SmartExchange algorithm enforce vector wise sparsity coefficient matrix possibility vector wise skip memory access computation correspond activation vector wise sparse coefficient matrix correspond vector naturally vector wise sparsity location offering opportunity directly sparse coefficient matrix encode index identify sparsity skip correspond activation memory access computation skip latency saving vector activation fracture conv operation commonly encode sparsity cod RLC index compress storage vgg resnet mbv vgg resnet deeplabv percentage booth encode booth encode imagenet cifar CamVid sparsity activation model datasets KI iij TT illustration propose SmartExchange accelerator architecture diagram processing PE rebuild res accumulate mac CRS sparsity encode index SmartExchange algorithm vector wise sparsity reduce sparsity encode overhead skip overhead latency benefit sparsity ratio hardware constraint memory bandwidth accelerator vector wise sparsity activation improve efficiency reduce latency vector wise sparsity percentage zero activation activation sparsity activation booth encode popular dnns vgg resnet MobileNetV imagenet vgg resnet cifar deeplabv CamVid sparsity precision correspond booth encode compact model MobileNetV vector wise sparsity widely conv layer kernel conv layer MobileNetV resnet memory access computation zero activation skip performance improvement proportional activation sparsity elaborate combine zero efficiency achieve target zero activation instead merely zero activation vector wise sparsity activation activation zero skip fetch correspond vector due slide processing conv layer compact model recently emerge compact model mobilenet  adopt depth wise conv squeeze layer traditional 2D conv layer restrict model reduces data  opportunity depth wise conv layer extreme conv channel reduce input reuse standard conv layer FC layer reuse opportunity squeeze  layer device efficient accelerator feature compact model adoption leverage compact model efficient processing architecture SmartExchange accelerator architecture overview architecture propose SmartExchange accelerator consists 3D PE array DIMM PE slice input index output global buffer input GB index GB output GB GB denotes global buffer associate index selector sparsity index sel controller accelerator communicates chip dram dma memory access aforementioned principle consideration IV propose accelerator feature insert within PE reduce rebuild overhead hybrid dataflow 1D stationary dataflow adopt within PE maximize input reuses PE slice output stationary dataflow maximize output partial sum reuses index selector index sel none zero coefficient activation vector inspire skip computation data movement associate sparse coefficient activation index selector SmartExchange index vector instead scalar sparsity data driven memory partition bandwidth bandwidth input bandwidth output data reduce access SRAMs implement GB adopt centralize GBs input output index respectively distribute SRAMs buffer PE slice coefficient basis matrix serial multiplier mac array PE activation sparsity booth encoder inspire PE slice dataflow PE slice 3D PE slice array 3D PE slide array SmartExchange accelerator enables parallel processing computation associate illustration propose 1D stationary along PE slice fifo  1D conv processing 1D stationary filter PE slice array DIMM PE slice  PE  input channel partial sum accumulate adder PE DIMM consecutive output channel DIMM filter parallel maximize reuse input activation dataflow employ reshape described PE PE array  MACs fifo buffer res restore wise manner operation PE multiple 1D conv operation 1D stationary rebuild temporally along MACs processing input activation 1D conv operation perform shift input activation along array MACs within PE via fifo 1D conv computation remain 1D conv operation 2D conv computation cycle assumption sparsity serial multiplication MACs cycle intermediate partial sum 2D conv operation accumulate locally mac RF register file basis matrix shift rebuild multiplexing mux fetch coefficient matrix basis matrix enables access data perform manner reduce bandwidth requirement advantage fetch data simultaneously specifically basis matrix fetch stationary within associate computation rebuilt coefficient matrix stationary associate computation mux handle dnns layer SmartExchange apply fourth handle compact model handle compact model adjust dataflow PE configuration improve utilization PE slice array mac array within PE specifically depth wise conv layer conv channel  PE longer correspond input channel instead 1D conv operation along dimension height PE squeeze FC layer PE mac array  MACs multiple cluster cluster illustration res PE denote multiplexing mac array cluster handle computation correspond output pixel improve mac array utilization latency performance propose SmartExchange accelerator advantage maintain compact model thanks adjustment adopt 1D stationary dataflow within PE employ serial multiplier possibility heavily quantize coefficient coefficient matrix index encode sparse coefficient commonly index index cod zero non zero coefficient respectively RLC index zero coefficient SmartExchange algorithm enforces channel wise sparsity vector wise sparsity channel wise sparsity zero coefficient mostly cluster within index efficient cluster zero coefficient remove buffer dnns filter vector wise sparsity skip correspond computation memory access buffer correspond dense model due unknown dynamic sparsity discus balance skip convenience increase buffer specifically enable processing sparsity non zero input activation coefficient input GB index GB correspond coefficient index respectively inspire correspond PE processing output output GB input GB ensure utilization PE array vanilla    input activation dense model counterpart fetch dynamic sparsity    increase input GB bandwidth requirement contrast reduction input GB bandwidth    input booth encode non zero activation cycle FIFOs instr controller runtime parser compiler 2D conv depth wise conv FC convd linear pytorch SmartExchange acc software hardware interface pipeline propose SmartExchange accelerator PE implement ping pong manner buffer thanks adopt stationary dataflow PE relief bandwidth requirement input activation reuse cycle serial multiplier cycle wise multiplication index output buffer input GB index buffer bandwidth expand handle activation sparsity expansion thanks observation vectorwise activation sparsity ratio relatively basis matrix fetch fetch coefficient matrix reconstruction computation computation stall basis matrix fetch coefficient fetch computation correspond basis matrix therefore leverage res PE ping pong manner avoid aforementioned computation stall handle output data adopt fifo buffer output PE slice GB cache PE array output GB reduce output GB bandwidth output calculate cycle software hardware interface briefly software hardware interface deploy SmartExchange algorithm dnn model framework pytorch SmartExchange accelerator hardware pre SmartExchange algorithm dnn model pas dnn parser compiler load accelerator specifically dnn parser firstly extract dnn model parameter layer 2D conv depth wise conv FC layer activation dimension dnn compiler dataflow generate sparse index instruction configure PE array memory data arrangement runtime schedule finally instruction compiler load accelerator controller processing RESULTS thorough evaluation SmartExchange algorithm hardware IV framework algorithm SmartExchange unifies mainstream model compression sparsification prune  BN dorefa FP  network slimming network slimming network slimming   accuracy accuracy accuracy model comparison SmartExchange algorithm SE compression technique imagenet cifar datasets differentiate SE baseline technique decomposition quantization framework perform extensive ablation benchmark structure prune quantization theart compression technique standard dnn model datasets validate superiority addition evaluate SmartExchange compact dnn model MobileNetV EfficientNet imagenet dataset segmentation model deeplabv CamVid dataset mlp model mnist hardware goal propose SmartExchange boost hardware acceleration efficiency evaluate SmartExchange algorithm hardware codesign dnn accelerator consumption latency processing representative dnn model benchmark datasets furthermore insight propose SmartExchange perform various ablation visualize validate effectiveness SmartExchange component technique evaluation SmartExchange algorithm setting evaluate algorithm performance SmartExchange conduct dnn model cifar imagenet datasets segmentation model CamVid dataset mlp model mnist dataset performance ofthe compression technique accuracy model structure prune technique network slimming  quantization technique scalable FP  dorefa quantization technique prune quantization technique SmartExchange exist compression technique SmartExchange unifies mainstream prune decomposition quantization evaluate SmartExchange algorithm performance stateof prune alone quantization alone algorithm dnn model datasets SmartExchange decomposition alone algorithm competitive popular II summary propose SmartExchange training vgg resnet imagenet dataset vgg resnet cifar dataset mlp mlp mnist dataset model CR param spar MB MB MB vgg  resnet   vgg   resnet   mlp mlp SE mlp mlp SE baseline model float representation input output activation benchmark achievable accuracy literature propose SmartExchange model fix representation input output activation representation coefficient basis matrix respectively outperforms prune alone quantization alone competitor achievable accuracy model resnet imagenet quantization algorithm dorefa aggressively shrink model unfortunately accuracy prune algorithm  maintains competitive accuracy model comparison SmartExchange combine obtains almost accuracy prune  quantize dorefa model compact dorefa apart aforementioned quantization evaluate SmartExchange algorithm quantization algorithm mlp model precision compression rate SmartExchange achieves comparable accuracy SmartExchange specifically dedicate FC layer quantization addition prune quantize mlp model SmartExchange achieves compression rate comparable accuracy extensive evaluation summarize II maximally achievable gain incur accuracy loss apply SmartExchange uncompressed model II CR compression rate overall parameter evaluation SmartExchange training compact model imagenet dataset model CR param spar MB MB MB mbv  eff eff  param denote model parameter basis matrix coefficient matrix respectively spar denotes ratio prune parameter without SmartExchange compress vgg network negligible accuracy loss resnets SmartExchange achieve solid compression ratio compress resnet SmartExchange incur almost accuracy compress model SmartExchange apply compact model II naturally apply SmartExchange redundant model gain validate propose SmartExchange algorithm remains beneficial adopt compact model MobileNetV mbv EfficientNet eff indicates despite SmartExchange yield promising gain compress mbv CR SmartExchange incurs accuracy accuracy loss impressive highly competitive context report compression quantization MobileNetV accuracy loss extend SmartExchange beyond classification model model compression hence dominantly evaluate classification benchmark demonstrate effectiveness SmartExchange beyond specific task semantic segmentation heavily pursue computer vision task memory latency demand apply propose algorithm specifically deeplabv resnet backbone output stride CamVid dataset standard split deeplabv apply SmartExchange CR marginal intersection union mIoU validation split SmartExchange decomposition evolution decomposition evolution SmartExchange algorithm matrix conv layer resnet network pre cifar SmartExchange algorithm decomposes  evolution reconstruction error sparsity ratio distance initialization identity sparsity ratio increase increase reconstruction iteration sparsity ratio norm error  WF sparsity illustrate evolution SmartExchange algorithm training error SmartExchange algorithm remedy error iteration maintain sparsity gradually become initialization evaluation SmartExchange accelerator subsection evaluate performance SmartExchange accelerator specifically introduce setup methodology SmartExchange accelerator stateof dnn accelerator diverse consideration dnn model standard dnns compact model segmentation model consumption latency benchmark datasets finally perform ablation SmartExchange accelerator quantify discus contribution component technique breakdown effectiveness sparsity dedicate handle compact model aim insight setup methodology baseline configuration benchmark SmartExchange accelerator accelerator diannao scnn cambricon pragmatic representative accelerator demonstrate promising acceleration performance diverse consideration summarize IV specifically diannao classical architecture dnn inference report faster efficient CPUs diannao considers dense model accelerator advantage sparsity dnns ensure comparison assign SmartExchange accelerator baseline computation resource chip SRAM storage diannao scnn cambricon accelerator non serial IV consideration baseline accelerator accelerator consideration diannao dense model cambricon  sparsity scnn  sparsity activation sparsity pragmatic activation sparsity vector wise sparsity vector wise activation sparsity summary computation storage resource SmartExchange baseline accelerator SmartExchange pragmatic DIMM input GB KB  output GB KB  buff slice KB serial mul precision diannao scnn cambricon chip SRAM storage SmartExchange mul precision multiplier SmartExchange pragmatic employ equivalent serial multiplier handle dynamic sparsity SmartExchange accelerator chip input GB bandwidth GB bandwidth PE slice correspond dense model respectively empirically sufficient handle model datasets meanwhile computation resource baseline accelerator bandwidth setting configure accordingly report principle FC layer benchmarking SmartExchange accelerator baseline accelerator comparison scnn baseline conv layer similarly EfficientNet scnn accelerator scnn handle squeeze layer adopt EfficientNet ablation layer model benchmark model datasets precision representative dnns resnet resnet vgg vgg MobileNetV EfficientNet deeplabv benchmark datasets cifar imagenet CamVid regard precision adopt activation baseline  dnns baseline dnns precision basis coefficient matrix SmartExchange dnns technology dependent parameter evaluate performance SmartExchange accelerator implement custom cycle accurate simulator aim model  rtl behavior synthesize circuit verify simulator correspond rtl implementation ensure correctness specifically gate netlist SRAM generate commercial technology synopsys compiler  memory compiler activity factor input memory computation calculate primetime PX meanwhile thanks description baseline accelerator easy representation implement custom cycle accurate simulator baseline evaluate performance baseline QQ QQ HW normalize efficiency diannao achieve SmartExchange accelerator stateof baseline accelerator dnn model datasets accelerator commercial technology frequency 1GHz performance normalize diannao accelerator diannao modify ensure accelerator hardware resource refer dram access per computation SRAM access SmartExchange accelerator efficiency baseline accelerator normalize efficiency SmartExchange baseline accelerator SmartExchange accelerator consumes dnn model datasets achieve efficiency improvement SmartExchange accelerator outstanding efficiency performance SmartExchange  effort effectively  memory storage access computation rebuild basis coefficient matrix costly RF PE fetch dram SmartExchange non trivially outperforms baseline accelerator compact model MobileNetV EfficientNet thanks SmartExchange algorithm compression ratio SmartExchange accelerator dedicate effective IV handle depth wise conv squeeze layer commonly adopt compact model normalize dram access input output activation baseline dram access SmartExchange accelerator resnet vgg model imagenet cifar datasets segmentation model deeplabv CamVid dataset SmartExchange dram access reduction model activation dominate compact dnn model SmartExchange accelerator reduce dram access baseline EfficientNet effectiveness dedicate handle  layer IV speedup baseline accelerator benchmarking SmartExchange accelerator  QQ QQ HW normalize dram access SmartExchange accelerator SmartExchange baseline accelerator dnn model datasets  latency processing image batch baseline accelerator various dnn model datasets SmartExchange accelerator achieves performance dnn model datasets achieve latency improvement validates effectiveness SmartExchange algorithm hardware effort reduce latency fetch activation memory computation resource SmartExchange accelerator advantage vector wise sparsity activation vector wise sparsity speedup baseline sparsity specifically SmartExchange accelerator average latency improvement scnn cambricon unstructured sparsity pragmatic considers sparsity activation respectively contribution SmartExchange component technique aforementioned efficiency latency improvement SmartExchange accelerator algorithm hardware effort SmartExchange algorithm model compression SmartExchange accelerator vectorwise sparsity index sparsity serial multiplier IV quantify contribution SmartExchange component technique baseline accelerator SmartExchange accelerator dense dnn baseline accelerator specifically baseline accelerator non serial multiplier DIMM   ensure hardware resource SmartExchange QQ QQ HW normalize speedup diannao achieve SmartExchange accelerator baseline accelerator dnn model datasets vgg resnet mbv eff vgg resnet deeplabv index selector accumulator PE GB GB output GB output GB input GB input GB dram index dram dram output dram input vgg resnet mbv eff vgg resnet deeplabv index selector accumulator PE GB GB output GB output GB input GB input GB dram index dram dram output dram input imagenet cifar CamVid imagenet cifar CamVid breakdown SmartExchange accelerator conv squeeze layer conv squeeze FC layer layer dnn model datasets accelerator resnet SmartExchange accelerator achieves efficiency baseline accelerator reduce dram access SmartExchange model compression vectorwise sparsity sparsity contribute saving respectively assume sufficient dram bandwidth SmartExchange accelerator achieves speedup baseline accelerator thanks effort leverage sparsity reduce unnecessary data movement computation increase parallel computation resource serial multiplier non serial multiplier computation resource SmartExchange accelerator breakdown SmartExchange accelerator breakdown computation access various memory hierarchy processing conv squeeze layer exclude FC layer various dnn model datasets access dram dominate input output activation model vgg MobileNetV EfficientNet model imagenet dataset resnet cifar dataset deeplabv model CamVid dataset SmartExchange algorithm largely reduce access dram access dram dominant model model vgg model cifar dataset resnet model imagenet dataset index selector account negligible layer trend vgg model FC layer model consume XX breakdown latency SmartExchange accelerator resnet sparsity ratio whereas FC dram access vgg account parameter although SmartExchange compress vgg resnet model cifar II dram access percentage resnet model activation vgg model activation vgg model largely prune thanks model filter wise sparsity enables prune filter correspond activation enable input output activation prune gap percentage dram access model SmartExchange effectiveness exploit sparsity normalize consumption latency latency model SmartExchange accelerator resnet vector wise sparsity ratio correspond model accuracy summarize input activation dram GB access reduce sparsity increase accelerator effectively utilize vector wise sparsity access sparse correspond input latency reduce sparsity increase SmartExchange accelerator indeed utilize vector wise sparsity skip correspond input access computation reduce latency effectiveness SmartExchange compact model perform ablation evaluate SmartExchange accelerator dedicate optimize dataflow PE configuration IV handle compact model normalize layer wise depth wise conv layer MobileNetV without propose dedicate propose effectively reduce meanwhile normalize layer wise latency reduce norm latency layer dedicate dedicate norm layer dedicate dedicate normalize latency depth wise conv layer propose dedicate compact model processing MobileNetV imagenet dataset VI related WORKS compression aware dnn accelerator achieve aggressive performance improvement researcher explore algorithm architecture exist typical algorithm approach decomposition data quantization sparsification exploit hardware demonstrate dnns tensorized decomposition nonvolatile memory nvm device sparsification accelerator propose unstructured sparsity cambricon proposes  sparsity reduce irregularity recent accelerator  quantize data cluster encode stripe  leverage serial processing flexible width balance accuracy loss performance improvement pragmatic utilizes input sparsity improve throughput efficiency tactical combine unstructured sparsity input sparsity knowledge SmartExchange formulation unifies decomposition quantization sparsification vectorwise structure sparsity approach simultaneously shrink memory footprint simplify computation recover matrix runtime vii conclusion propose SmartExchange algorithm hardware codesign framework memory storage access computation boost efficiency dnn inference extensive SmartExchange algorithm outperforms theart compression technique dnn model datasets various setting SmartExchange accelerator outperforms dnn accelerator efficiency latency respectively