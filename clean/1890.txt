personalize recommendation backbone machine ML algorithm important application domain commerce etc service datacenters sparse embed layer crucial building recommendation attention paid properly accelerate important ML algorithm detailed workload characterization personalize recommendation identifies significant performance limiter memory intensive embed layer compute intensive multi layer perceptron mlp layer centaur chiplet hybrid sparse dense accelerator address memory throughput challenge embed layer compute limitation mlp layer implement demonstrate proposal intel HARPv package integrate cpu fpga device performance speedup energyefficiency improvement conventional approach index accelerator processor architecture fpga machine neural network introduction complexity neural network dnn machine ML algorithm rapidly gpus ASIC fpga ML accelerator widely adopt accelerate computationally dense dnn layer convolutional neural network cnns recurrent neural network rnns multi layer perceptrons MLPs amenable hardware acceleration thanks highly regular deterministic dataflow significant stride accelerate compute intensive dnn layer attention paid address challenge memory limited non dnn layer emerge ML workload consequently witness non dnn layer memory intensive gradually become significant performance bottleneck ML algorithm employ sparse embed layer exhibit drastically characteristic conventional dense dnn layer illustrates structure ML application employ embed layer adopt variety application domain social networking service commerce others backbone ML algorithm application personalize recommendation widely deployed ML workload service embed layer account significant inference recommendation consequently topological structure dnn personalize recommendation model sparse embed layer frontend dense dnn layer backend processing  google facebook alibaba baidu pinpoint embed layer severe performance bottleneck production personalize recommendation model focus address challenge deploy dnn recommendation model personalize recommendation consists module frontend sparse embed layer backend dense mlp layer detailed embed layer consume GBs memory capacity recommendation inference requirement beyond physical memory capacity gpus available GBs GB nvidia deploy model utilize capacity optimize cpu memory embeddings utilize cpu exclusively inference cpu landscape conduct workload characterization personalize recommendation model identify challenge deploy personalize recommendation conventional cpu frontend embed layer stage multiple embed vector embed subsequently reduce bandwidth cpu memory aggregate embeddings inference embed KBs MBs GBs embed embed operation extremely sparse spatial temporal locality exhibit cache llc rate unlike throughput optimize acm annual international symposium computer architecture isca doi isca gpus however CPUs primarily optimize latency handful concurrent thread status register MSHRs CPUs fail maximize memory parallelism significantly utilize memory bandwidth sparse embed operation consequently sparse embed layer account significant inference performance bottleneck another significant challenge cpu recommendation compute intensive MLPs execute throughput CPUs significant latency overhead overall identify limited memory throughput utilization cpu memory computational throughput CPUs significant obstacle address bottleneck personalize recommendation centaur chiplet hybrid sparse dense fpga accelerator holistically address challenge personalize recommendation FPGAs recently surge ML acceleration thanks efficient highly programmable however prior fpga accelerate ML primarily target compute intensive dense dnn layer cannot properly address challenge sparse embeddings traditionally commonly employ integration tier cpu fpga pcie bus local physical memory fpga function discrete processor device discrete gpus ML acceleration service cpu via task offload request recent advance chiplet technology however enable tight cpu fpga integration package highbandwidth latency communication cpu fpga chiplets physically memory allows fpga chiplet directly access embed inside cpu DIMMs obviate memory operation host device memory discrete gpus FPGAs furthermore package integration technology matures compute density inter chip communication bandwidth innovation centaur utilization emerge  cpu fpga technology develop heterogeneous accelerator architecture tailor address conflict resource requirement recommendation model concretely centaur synergistically combine module performance recommendation sparse accelerator embeddings package integrate cpu fpga fpga directly physical memory bandwidth latency cpu fpga communication link centaur implement sparse accelerator throughput embed reduction operation directly embeddings functional behavior  caffe conduct embed reduction reduce cpu memory improve centaur effective throughput embed reduction achieve superior memory bandwidth utilization embed layer dense accelerator GEMMs alongside sparse accelerator centaur incorporates module accelerate compute intensive dnn layer dense accelerator handle gemm purpose matrix multiplication operation MLPs feature interaction significant latency reduction baseline cpu relies throughput cpu core gemm operation overall centaur utilizes unique package integrate cpu FPGAs demonstrate merit chiplet hybrid sparse dense accelerator architecture effectively tackle performance bottleneck personalize recommendation specifically  accelerator overcome limited memory bandwidth utility cpu achieves significant throughput improvement sparse embed layer furthermore centaur improves performance mlp layer thanks throughput fpga logic everything centaur speedup efficiency improvement cpu deploy personalize recommendation model II background sparse dense layer personalize recommendation computer community primarily focus accelerate computationally intensive cnns rnns MLPs exhibit dense highly regular computational highly deterministic dataflow dense dnn layer amenable hardware acceleration custom architecture training inference however emerge ML workload employ embed layer exhibit highly irregular sparse dataflow pseudo code  function implement caffe conduct embed lookup aka embed vector reduction widely employ dnn recommendation vector embeddings contiguously inside embed lookup sparse index ID illustration embed reduction operation feature interaction stage assumes embed operation per feature interaction stage conduct batch gemm operation input concatenate reduce embeddings tensor lookup unique embed operation multiple sparse index input necessarily contiguous within embed lookup multiple consequently embed operation exhibit highly sparse random memory access temporal spatial locality embed vector lookup combine vector wise addition multiplication operation hence perform reduction illustrate reduce embed vector feature interaction algorithmically capture complex interaction embed feature implementation exists feature interaction assume dot feature interaction employ facebook  recommendation model DLRM feature interaction stage DLRM implement dot reduce embed vector batch gemm operation output concatenate output vector mlp layer concatenate vector mlp fed sigmoid function calculate probability likelihood facebook user click advertisement banner ML workload embeddings embed projection discrete categorical feature vector continuous context ML workload embeddings dimensional vector representation feature variable recently effective numerous application domain recommendation machine translation automatic recognition cpu fpga integration tier assume discrete fpga communicate cpu pcie bus package integrate cpu fpga inside cpu socket package integration cpu fpga enables memory address cpu fpga bandwidth latency communication cpu fpga hardware utilizes intel HARPv demonstrate merit chiplet cpu fpga recommendation assume intel technology QPI nomenclature nonetheless intuition proposal equally applicable alternative chiplet cpu fpga recommendation instance formulate estimate likelihood  recommendation utilize embeddings account user item feature embed reduction interact feature altogether later backend dnn execution extract probability discrete integrate FPGAs ML acceleration ASICs significant efficiency gain purpose CPUs gpus dense dnn layer flexibly cope evolve ML algorithm research reconfigurable processor architecture FPGAs intermediate efficiency ASICs programmability purpose cpu gpu processor potential flexible acceleration constantly evolve ML application widely employ cpu fpga integration strategy discrete fpga cpu bus pcie equip local physical memory fpga employ style integration extensibility throughput cpu processor device challenge integration tier cpu fpga communication bound narrow pcie bus bandwidth latency benefit fpga acceleration benefit outweigh task offload overhead recent therefore employ tight cpu fpga integration  cpu fpga chiplets communicate recommendation model CONFIGURATIONS model mlp DLRM MB KB DLRM GB KB DLRM MB KB DLRM GB KB DLRM GB KB DLRM MB KB bandwidth latency discrete FPGAs future bandwidth advanced multichip packaging technology another advantage integrate cpu fpga device physical memory allows grain fpga cpu data access vice versa  obviate latency overhead traverse software stack data movement manual dma invoked  across cpu fpga memory address reduce overall memory access latency workload characterization dnn BASED  recommendation SYSTEMS utilize source recommendation model DLRM conduct detailed workload characterization dnn personalize recommendation DLRM production model configuration generate recommendation model recommendation embed operation per memory usage embed mlp layer prior embed dimensional vector default objective characterization performance bottleneck recommendation model motivate hybrid  fpga accelerator assume cpu baseline architecture commonly deployed recommendation detail merit cpu deploy recommendation IV breakdown inference breakdown inference latency normalize execution sweep input batch observation unlike conventional ML application extensively computer community non dnn layer embed layer significant execution personalize recommendation model mlp layer account non trivial portion runtime inference batch although batch increase latency embed mlp layer mlp layer relatively increase execution embed layer DLRM intentionally configure lightweight embed layer breakdown cpu inference latency embed layer emb mlp layer others axis function batch axis inference latency normalize slowest DLRM model batch DLRM axis compute intensive mlp layer detail methodology batch tend increase data reuse mlp across multiple input batch amortize upload chip detailed subsection llc rate mlp layer whereas batch embeddings translate data reuse whatsoever batch simply amount embeddings memory subsystem relative execution embed layer respect layer proportional increase execution conclude  recommendation severely bottleneck embed layer nonetheless mlp layer account significant portion execution batch configuration chip cache efficiency understand compute memory bandwidth demand aforementioned bottleneck layer sparse embed layer mlp layer conduct detailed analysis cpu llc rate MPKI per instruction execute embed mlp layer embed layer llc rate sensitivity input batch increase llc batch increase embed layer llc rate unique embed GBs embed vector within increase proportional user item user register movie  youtube netflix embed operation capacity embed extremely sparse spatial temporal locality aggregate embeddings proportional batch directly translates memory traffic locality batch embed layer therefore severely  llc llc MPKI mlp layer llc rate layer exhibit relatively sensitivity input batch aggregate model mlp layer execute embed emb mlp layer llc rate MPKI function batch  profile statistic workload sufficiently typically MB capture inside MBs cpu chip cache therefore mlp layer recommendation model typically exhibit llc rate MPKI exhibit compute limited behavior effective memory throughput sparse embed layer exhibit llc rate accordingly MPKI mlp layer effective memory bandwidth utilized gathering embed vector extremely summarizes memory throughput gathering embed vector execute embed layer clearly quantify efficiently memory bandwidth utilized embed lookup effective memory throughput embed layer useful byte transfer gathering reduce embeddings embed vector latency incur execute embed layer depict effective memory throughput embed layer maximum GB sec memory bandwidth baseline cpu memory recall embed vector byte byte default dimensional vector KB dram buffer additionally vector load limited spatial locality due sparse irregular memory access unlike throughput optimize gpus execute concurrent thread MSHRs nvidia volta cache implement cache allows unlimited inflight cache maximize data fetch throughput latency optimize CPUs utilize thread handful MSHRs  dram bandwidth utilization intel vtune trend albeit define effective memory throughput subset embeddings cache embed layer effective memory throughput embed reduction function input batch quantify sensitivity embeddings effective throughput configuration DLRM plot sweep embeddings depict effective memory throughput generally grows monotonically batch increase embeddings increase however effective throughput maximum memory bandwidth batch realistic per typically per aggregate embed vector KBs batch MBs batch GBs embed challenge cpu architecture maximize memory parallelism memory bandwidth utilization sparse irregular grain vector operation IV centaur  sparse dense accelerator  recommendation centaur chiplet hybrid sparse dense accelerator holistically address dual challenge memory limited embeddings compute limited MLPs personalize recommendation knowledge centaur accelerator tackle memory compute bottleneck personalize recommendation model motivation package integrate cpu fpga platform ASICs description propose architecture motivation package integrate cpu FPGAs gpus currently dominate processor architecture ML training throughput optimize throughput algorithmic training deployment recommendation service however latency optimize CPUs prefer architecture choice abundance readily available achieve GB sec effective throughput max embed layer batch embed vector dimension sufficiently dimensional vector however batch embed dimension unrealistic assume inference CPUs datacenters appeal compute platform ownership TCO perspective peak portion diurnal cycle CPUs otherwise remain idle user inference service news advertisement commerce recommendation firm SLA service agreement goal render latency optimize CPUs suitable  gpus lastly recall sparse embed layer significantly memory capacity hungry embed GBs memory usage bandwidth optimize 3D stack memory employ gpus ML accelerator google TPUs cannot embed locally inside physical memory prevent deploy recommendation therefore vast majority ML inference service personalize recommendation primarily cpu  landscape package integrate cpu FPGAs become promising holistically address aforementioned challenge detailed package integrate cpu FPGAs minimally intrusive exist server chassis therefore server rack overall datacenter socket compatible exist node furthermore CPUs function host OS perspective unlike gpus TPUs device utilized non ML usage enhance resource utility optimize TCO reconfigurable fpga logic utilized address performance bottleneck reduce inference latency satisfy SLA goal improve qos importantly cpu fpga physical memory memory DIMMs within across cpu socket  DDRx allows fpga accelerator entire embed cpu memory  access directly bandwidth latency cpu fpga communication channel requirement discrete gpus FPGAs cannot fulfill observation centaur architecture utilizes fpga programmable logic implement heterogeneous compute device synergistically combine sparse accelerator embed reduction dense accelerator gemm computation detail microarchitecture sparse dense accelerator subsection discus propose  cpu fpga architecture discus  concept cpu fpga substrate intel HARPv utilize demonstrate proposal propose chiplet cpu fpga architecture illustrates propose chiplet cpu fpga architecture minimally intrusive propose package integrate cpu fpga architecture exist TCO optimize server chassis rack  fpga chiplet communication cpu memory subsystem cache coherent utilizes cpu fpga cache coherent link denote CC  traverse cpu onchip cache hierarchy chip memory via arrow effective memory access data locality alternative cache bypassing via arrow appropriate memory intensive embed layer utilizes memory channel interface completely bypass cpu cache directly route fpga memory request chip memory interface provision cache bypassing route communication throughput commensurate maximum chip memory bandwidth sparse accelerator centaur significantly boost throughput embed layer conduct vector communication channel unfortunately chiplet commercialize cpu fpga stage limited accessibility functionality therefore utilize intel HARPv proof concept substrate demonstrate merit proposal detail subsection HARPv cache coherent cache bypassing route cpu memory access throughput benefit sparse accelerator constrain memory parallelism  cpu fpga cache coherent accordingly cpu cache hierarchy nonetheless conservatively estimate throughput benefit chiplet cpu FPGAs recommendation subsection detail sparse dense accelerator microarchitecture description software interface overall sparse accelerator objective sparse accelerator enable throughput latency embed reduction operation recall package integrate cpu fpga device enable custom fpga logic directly access physical memory grain byte cache granularity via  bandwidth communication link intel HARPv platform assume theoretical EB streamer overview propose centaur architecture proof concept prototype utilize intel HARPv hybrid  accelerator reconfigurable fpga logic synthesize sparse EB streamer throughput latency embed reduction dense throughput gemm computation accelerator maximum uni directional communication bandwidth GB sec cpu fpga pcie link cache coherent  link sparse accelerator utilize communication technology implement embed henceforth refer EB streamer spawn multiple embed vector operation reduction operation throughput manner detail microarchitecture EB streamer contains pointer register  sparse index SRAM array  embed EB GU embed reduction EB RU embed reduction conduct boot cpu utilizes MMIO interface inform fpga cpu memory address sparse index array IDs embed embed mlp dense feature input mlp pointer  utilized sparse dense accelerator embed gemm operation  initialize EB GU utilizes  pointer address sparse index array perform cpu fpga operation populates  sparse index IDs operation EB GU address generator offset dominate logic gate implementation overhead embed address  sparse index IDs  EB GU generate cpu fpga embed operation maximally utilize cpu fpga communication bandwidth EB GU monitor communication bandwidth utility aggressively instantiates embed vector operation pcie  link whenever cpu fpga communication link become available embed vector sparse accelerator immediately rout EB RU vector reduction operation EB RU conduct embed reduction operation whenever embed vector EB RU embeddings reduce EB RU reduce embed vector dense accelerator complex embeddings typically vector embed vector operation equivalent byte load instruction memory address multiple embed vector gathering scatter across memory address consequently brute software data transfer grain irregular data access incur severe latency overhead  api execution byte cpu fpga operation traverse various layer software stack advantage initiate embed operation package integrate cpu fpga channel data fetch retrieval entirely orchestrate hardware significantly reduce memory access latency furthermore embed conduct interfere bottleneck cpu cache hierarchy embed operation inherently sparse extremely locality render conventional cpu cache mechanism ineffective nonetheless baseline cpu traverse multi chip cache embed vector load operation discover embeddings likely cpu memory entire embed gathering orchestrate handful thread cpu embed limited parallelism locality achieve memory bandwidth utility sparse accelerator directly fetch embeddings cpu fpga communication microarchitecture centaur sparse accelerator link centaur achieve significantly memory bandwidth utilization VI fundamentally address memory bandwidth challenge embed layer dense accelerator dense accelerator microarchitecture primary objective dense accelerator execution gemm algorithm mlp layer batch gemm operation feature interaction altera fpga float IP core optimize matrix multiplication matrix FP matrix mult module building construct dense accelerator complex processing PE instance FP matrix mult module configure handle matrix multiplication matrix utilize compose spatial PE array mlp another instance PEs feature interaction centaur aggregate computational throughput gflops operating mhz mlp employ  dataflow tile input matrix compatible PE gemm compute granularity broadcast tile across spatial PE array mlp conduct  input tile PE array generates partial sum temporally accumulate SRAM buffer allocate per PE addition gemm computation dense accelerator complex contains SRAM buffer mlp  dense feature input mlp layer   mlp input  input model parameter execute mlp layer cpu fpga communication link  boot mlp microarchitecture centaur dense accelerator remain persistent throughout entire deployment overhead upload model fpga  negligible amortize future inference request service centaur module dense accelerator complex finalize recommendation  sparse accelerator complex upload mlp  input mlp layer   initialize  model parameter remain persistent whereas   update whenever inference request mlp    execute mlp layer feature interaction sparse accelerator reduce embeddings feature interaction output vector mlp layer concatenate reduce embeddings tensor feature interaction utilizes concatenate tensor initiate batch gemm computation feature interaction  input output feature interaction  input subsequently rout mlp execute mlp layer model parameter inside  mlp layer execution sigmoid calculate probability cpu memory processing entire dense gemm computation orchestrate seamlessly sparse accelerator centaur significantly throughput reduce latency execute dense dnn layer cpu subsection detail software interface enables cpu fpga integration overall software interface package integrate HARPv platform unified virtual memory address cpu output stationary dataflow centaur mlp gemm operation outer input tile generates output tile accumulate intra PE SRAM buffer PE conduct matrix multiplication operation input tile computation tile tile broadcast PEs within correspond bus interconnection network within mlp fpga cpu fpga function processor operating application concerned pointer pointer semantics concretely pointer sparse index array embed dense feature input others fpga MMIO interface address pointer virtual address fpga IOMMU tlb translates physical address embed operation conduct fpga directly access cpu physical memory hardware invoke multiple software invoked dma operation grain hardware data movement reduce average memory access latency centaur achieve superior memory throughput embed pointer address data structure sparse index array embed centaur  MMIO inference entirely orchestrate hood hardware ML framework tensorflow pytorch readily employ propose architectural minimal methodology evaluation platform demonstrate benchmark centaur intel HARPv broadwell xeon altera arria GX intel HARPv platform publicly accessible package integrate II centaur fpga resource utilization ALM blk mem ram blk dsp pll GX max centaur utilization cpu fpga evaluate centaur compute architecture proof concept prototype entire sparse dense accelerator  rtl  prime pro synthesize route II explore recommender baseline cpu HARPv broadwell cpu without fpga activate comparison centaur aside centaur establish additional recommendation inference CPUs prefer deploy recommendation IV nonetheless evaluate performance gpu completeness assume entire embed cpu memory embed vector reduce cpu  cpu pcie gpu gpu mlp computation refer cpu gpu utilize nvidia DGX cpu gpu performance measurement estimate cpu consumption pcm cpu socket estimation consume memory DIMMs gpu consumption nvidia nvprof profile utilized centaur cpu fpga measurement pcm socket cpu fpga consume memory DIMMs evaluate efficiency estimation inference execution performance sufficiently cpu cache hierarchy benchmark source recommendation model DLRM primary benchmark suite DLRM configure pytorch backend library version  access march extract parallelism openmp avx instruction embed mlp layer DLRM reference model architecture across service configuration configuration embed per embed memory requirement embed mlp layer dimension maintain distinctive characteristic default model configuration highlight compute memory access behavior recommendation model detailed summarizes benchmark sparse VS dense fpga resource usage module LC comb LC reg blk mem dsp sparse ptr reg reduction SRAM array dense mlp feat int SRAM array others misc DLRM embed layer artificially embed layer stage relatively longer mlp computation utilize evaluate centaur sensitivity mlp intensive recommendation VI evaluation explores recommender baseline cpu cpu gpu centaur discus fpga resource utility hybrid sparse dense accelerator memory throughput overall performance cpu centaur comprehensive comparison efficiency centaur fpga resource utilization summarizes centaur sparse dense accelerator utilizes various fpga resource role sparse optimize accelerator perform  embed reduction EB streamer incorporate local sparse index array seamlessly invoke multiple embed operation parallel employ SRAM array sparse index IDs embed aggressively launch multiple operation concurrently boost memory parallelism overall memory bandwidth utilization reflect sparse accelerator complex memory sparse index usage ALMS DSPs usage respectively primary computation conduct inside sparse accelerator address generation reduction lightweight fashion dense accelerator complex computational throughput consumes DSPs ALMS achieve computation throughput cpu discus remainder skewed heterogeneous usage fpga resource centaur strike balance effectively tackle bottleneck memory intensive embed compute limited gemm operation effective memory throughput embed layer cpu cannot effectively execute embed layer memory throughput gathering embeddings spending significant  bottleneck centaur effective memory bandwidth utilized embed axis improvement cpu axis function input batch centaur effective memory bandwidth function embeddings embed exhibit rapid improvement effective throughput baseline cpu layer EB streamer significantly improves effective throughput gathering embed vector batch achieve GB sec throughput maximum effective uni directional cpu fpga communication bandwidth around GB sec HARPv EB streamer achieves communication bandwidth highly irregular sparse data access embed EB streamer communication bandwidth utility demonstrates robustness embed batch cpu utilize memory bandwidth gap cpu centaur memory throughput gradually shrink batch increase EB streamer cpu DLRM DLRM batch EB streamer throughput constrain cpu fpga link bandwidth detailed VI performance overhead batch offset throughput centaur dense accelerator delivers effective throughput EB streamer naturally cpu fpga communication link bandwidth increase highbandwidth package signal technology overall centaur average throughput improvement cpu across configuration conservatively chosen HARPv platform effectively tackle memory bandwidth limitation embed layer discus performance improvement centaur delivers sparse dense hybrid accelerator architecture performance centaur significantly improves performance memory limited embed layer thanks EB streamer throughput operation breakdown centaur inference cpu fpga sparse index fetch idx embed reduction emb cpu fpga dense feature fetch dnf mlp execution others axis axis summarizes performance improvement centaur achieves cpu abundant computation dense accelerator complex reduces latency execute GEMMs recommendation model allows centaur substantially reduce toend latency holistically address significant bottleneck recommendation latency breakdown workload performance improvement baseline cpu achieve speedup DLRM model bottleneck embed layer batch throughput optimize EB streamer resolve bottleneck achieve superior performance improvement DLRM achieves modest average speedup model intentionally configure heavyweight mlp layer lightweight embed layer consequently overall performance relatively insensitive improve memory throughput EB streamer brings nonetheless centaur dense accelerator significant latency reduction execute DLRM gemm achieve substantial performance improvement efficiency demonstrate superior memory bandwidth utility performance centaur baseline cpu comparison centaur cpu cpu gpu efficiency IV summarizes consumption evaluate methodology summarize baseline cpu hungry cpu gpu centaur consumes cpu core mostly remain idle fpga sparse dense accelerator orchestrates embed reduction backend mlp computation efficient manner centaur achieves superior efficiency significantly improve inference overall efficiency significantly improve summary performance efficiency improvement centaur brings baseline cpu performs cpu gpu average achieve performance efficiency improvement cpu gpu embed inside cpu memory cpu invoked embed reduction reduce IV consumption cpu cpu gpu centaur watt cpu gpu embeddings gpu mlp acceleration noticeable latency penalty due cpu gpu communication overhead render cpu gpu perform poorly cpu centaur achieves performance speedup efficiency improvement cpu vii discussion limited availability chiplet package integrate cpu fpga device utilized intel HARPv proof concept substrate demonstrate merit centaur recent advance packaging intel   package signal technology nvidia reference signal architect chiplet cpu fpga architecture discus  cpu FPGAs implication accelerator recommendation cpu fpga bandwidth baseline cpu fpga platform GB sec cpu fpga unidirectional communication bandwidth upcoming  signal technology deliver GB sec communication throughput across chiplets limited parallelism throughput gathering embed vector obstacle cpu embed operation inherently collective operation embed vector proceed feature interaction stage significant vector cache however gathering embeddings suffer significant latency overhead due implicit barrier enforce cpu fpga optimize overall architecture throughput locality bandwidth fpga cpu embed vector operation bypass cpu cache  IV maximize available parallelism throughput however guarantee cache coherence consistency carefully cache primitive sparse embed layer explore future fpga fpga dense accelerator tera operation throughput thanks abundant reconfigurable logic available within fpga device dnn TOPS throughput xilinx VUP embarrassingly parallel dnn algorithm effective throughput dense accelerator proportionally fpga technology integrate cpu accompany throughput cpu fpga communication channel centaur performance efficiency improvement cpu cpu gpu normalize cpu gpu exhibit performance efficiency across chiplets proportionally input tensor accelerator deliver aforementioned bandwidth package signal technology related recommendation model backbone ML algorithm variety internet service significant industrial importance  hint compute memory deploy recommendation attention paid computer community address important research research publish computer architecture venue recommendation model recent TensorDIMM earlier architecture community explore research propose hardware software embed layer TensorDIMM employ DIMM memory processing disaggregated gpu memory overcome memory bandwidth bottleneck embed layer tackle identical centaur factor render unique TensorDIMM focus cpu centric commonly adopt inference deployment  unlike TensorDIMM assumes gpu centric inference TensorDIMM pool memory architecture achieve maximum performance benefit impact overall compute density overall datacenter potentially impact TCO centaur carefully minimally intrusive exist server node chiplet cpu fpga  adoption TensorDIMM memory processing paradigm modification gpu ISA software runtime unlike centaur implement exist package integrate cpu fpga technology lastly TensorDIMM relies rank parallelism increase effective throughput embed operation benefit TensorDIMM limited sufficiently embed vector constrain algorithmic recommendation model embed vector hence flexible applicable variety recommendation algorithm overall contribution centaur orthogonal TensorDIMM unique summary comparison centaur closely related IX conclusion utilize emerge package integrate cpu fpga technology demonstrate acceleration personalize recommendation model hybrid sparse dense centaur architecture synergistically combine sparse accelerator embed reduction dense accelerator gemm computation holistically address dual challenge memory bandwidth compute throughput prototype implementation proposal intel HARPv device centaur achieves performance energyefficiency improvement respectively conventional cpu