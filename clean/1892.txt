slowdown chip performance combine demand compute efficiently renew distribute architecture specialized hardware dedicate accelerator critical operation become effective addition processor peripheral network focus operation reduce critical feature neural network training allreduce impossible fully parallelize amortize benefit greatly hardware acceleration propose accelerator centric memory network improves reduce performance network reduction accelerate collective multicast propose switch network computation reduction offs implementation complexity performance additionally propose network endpoint modification improve collective simulation gpu collective acceleration improves reduce operation message message software algorithm faster DL training network transformer demonstrate scalable gpus introduction amount data generate increase stagger rate compute resource data compute become increasingly parallel augment specialized hardware accelerate performance critical task specialized hardware graphic processing gpu programmable gate array FPGAs various machine ML DL accelerator google tensor processing tpu specialized processor increase performance efficiency therefore compute density increase parallelism performance scalability becomes increasingly dependent communication compute task accelerate specialized hardware communication benefit specialization scientific distribute ML DL application collective broadcast processor data processor although collective concerned data distribution reduce reduce operation involve computation reduce operation participate processor data processor data reduce accord arithmetic relational operation reduce operation broadcast participant allreduce operation critical parallel DL training algorithm distribute worker calculate gradient parameter gradient aggregate worker reduce operation per training iteration operation quickly become bottleneck motivate specialized hardware accelerate operation previous offload reduction computation network effective technique however exist limited cpu initiate communication model data movement accelerator coordinate cpu message passing protocol explicit resource reservation highly parallelize environment overhead diminishes performance gain network reduction furthermore distribute memory network packet memory operation numerous reservation protocol prohibitively expensive focus instead accelerator centric communication model accelerator directly attach distribute memory fabric architecture exemplify gpus interconnect nvidia NVLink NVSwitch network amd  network propose network endpoint architecture accelerates various collective communication focus reduce operation approach highly parallel accelerator thread global address contribution summarize novel scalable memory collective architecture massively parallel accelerator network reduction complexity endpoint switch extend dma efficiently network reduction improve resource utilization performance acm annual international symposium computer architecture isca doi isca mechanism magnitude faster software suitable  exist hardware proposal evaluate benefit training widely important neural network remainder background II review software algorithm allreduce operation description architecture IV simulation VI related vii conclusion II background motivation accelerate compute focus accelerator centric network directly interconnect massively parallel accelerator CPUs particularly nvidia DGX reference applies memory DGX gpus NVLink implement network topology twelve  gpus switch evenly within switch gpus via NVLink switch remain switch topology bisection bandwidth NVLink fabric allows gpus access gpu memory ordinary memory operation load atomics exist network reduction proposal message passing communication source message complex reservation protocol gpus memory fabric paradigm multi gpu concurrently execute thread globally address thread execute arbitrary non deterministic network reduction protocol rely injection ejection feasible furthermore packet network individual memory operation numerous therefore essential packet lean format minimal overhead render exist message passing reservation protocol prohibitive gpus implement complex network interface plurality dma linear multi dimensional data transfer offload principle dma multiple request issue memory network dma command command queue host processor data transfer signal command interface although dma source message data guarantee remains non deterministic DL training neural net dnns excellent sometimes task image classification complex network accurate prediction however amount data training extremely compute intensive overview neural net parallel highlight importance allreduce operation methodology analyze communication requirement DL training data parallel training accurate performance model spent layer dnn model comprises detailed description compute memory nvidia volta gpu silicon performance closely calculate compute layer computational graph gradient generate reduce worker reduction implement non overlap reduce iteration shot reduce operation gradient layer latter allows overlap communication computation model approach allows various hypothetical performance improvement future gpus network bandwidth various reduce algorithm model parallelism focus nvidia  model date iteration reduce without reduce volta DGX report spent reduce operation  model parallelism overlap therefore rely measurement reduce data parallelism DP data parallel training approach parallel training stochastic gradient descent sgd mini batch input sample processor processor model replica sample refer  calculates gradient reduce processor ass importance reduce training account nvidia volta gpu theoretical performance  0GB unidirectional NVLink bandwidth twice bandwidth previous pascal architecture compute performance factor DL training mainly enable specialized hardware tensor core hardware faster specialized important application training dnns software improves rapidly mlperf submission hardware mlperf average performance increase maximal improvement resnet DGX consequently instead performance volta future message distribution layer wise overlap various network transformer DP model  MP DGX DL communication analysis message distribution spent reduce DP MP gpu compute performance rapidly bandwidth driven software hardware improvement therefore model configuration achieves compute performance bandwidth 0GB label volta compute performance label volta prior focus ethernet configuration assume ethernet per gpu 0Gbps 0Gbps bandwidth equivalent node ethernet interface per gpu important aspect training data parallel training fundamental limit mini batch processor cannot arbitrarily minibatch comprises sample training converges iteration training consequently increase sub batch becomes maximal mini batch meaning per gpu spent computation communication remains constant solely depends model input sub batch training becomes sensitive bandwidth overlap backward pas gradient calculate layer reduction layer compute gradient layer maximize computation communication overlap however software reduce algorithm gpu resource dedicate communication away gradient calculation furthermore interference memory slows computational kernel shot approach reduce backward pas without overlap achieve performance interconnects reduce algorithm latency message per layer reduction depict graph distribution message network resnet image classification transformer bert model message resnet MB significant MB similarly transformer MB average message per layer reduction amount MB reduction MB efficiently implement overlap latency reduce operation model analysis transformer network DGX gpus spent gradient reduction algorithm  communication volta architecture training spent allreduce project future generation gpus compute capability exhibit graph NVLink bandwidth interconnect gpus communication dominates training ethernet overlap beneficial hence sub batch latency message algorithm kernel launch overhead gpus model parallelism MP another parallelism model parallelism model distribute although various implement split input matrix matrix multiplication gemm processor calculates partial output matrix reduce across processor spent reduce operation backward pas various model parameter billion parameter although allreduce decrease model due computation becomes efficient model therefore training becomes sensitive bandwidth overall model gpus spends reduce operation model arbitrary gpus due partition memory capacity constraint collective communication  collective communication primitive important distribute training algorithm ML DL scientific application brief overview discus reduce specifically detail collective broadcast sends data multicast subset operation data commonly refer  operation scatter operation matrix operation transpose matrix reduce operation reverse multicast sends data data reduce apply arithmetic relational operation reduce scatter combine reduction operation reduce scatter completes operation exist prefix broadcast reduce operation extends reduce broadcast upon completion reduce operation focus discus various implementation approach implement reduce software effectively implement reduce scatter processor reduces transmits along algorithm processor reduction additional distribute reduction around nvidia NCCL baidu reduce implement algorithm although achieves optimal bandwidth latency proportional processor memory processor mailbox data validation data signal flag commonly relaxed memory model memory fence data flag fence scope comprise therefore expensive operation impact local memory negatively impact kernel consequently synchronization increase linearly processor exchange replace reduce algorithm logp efficient algorithm binary processor leaf therefore maximize network utilization NCCL version later algorithm inter node reduction across infiniband algorithm recursive commonly communication library mpi bandwidth optimal software implementation reduce processor amount data twice allreduce message reduce scatter  gpu ST gpu ST ST gpu ST gpu net addr local addr fabric manager switch ST configure register net addr local addr net addr local addr net addr local addr gpu translation gpu translation gpu translation gpu translation switch multicast mcr address translation ST ST vas vas ST vas vas vas virtual address global address mcr mcr ID addr target IDs exp rsp arbiter rout mcr mcr mcr multicast concept gpus switch phase maximal achievable bandwidth limited available network bandwidth therefore network reduction potential accelerate reduce operation approximately bandwidth perspective realize speedup eliminate expensive synchronization reduction IV network  multicast reduce operation distribution reduction gpus network multicast essential accelerate performance beneficial collective multicast broadcast barrier focus memory propose extend global address multicast MCRs gpu register exist memory allocation network fabric manager marker address mapped gpus virtual address vas mapping implement memory mapped MMIO pcie address register mapped gpu vas nvidia GPUDirect fabric manager initializes mcr switch multicast address target IDs associate mcr target switch gpu switch entry per mcr target IDs address within mcr multicast operation mcr address packet arrives switch address belongs mcr target IDs multicast packet replicate destination accord target ID source packet exclude destination multicast target ID lookup rout arbiter hierarchical network topology packet replicate fashion network minimize bandwidth link switch gpu gpu gpu gpu gpu gpu switch switch switch switch request request ID destination response destination concept nvidia DGX available MCRs limited grows accord combination processor however multicast rarely application initialization mcr application inside network fabric multicast request virtual channel vcs unicast request assume switch allows multicast packet progress output output available packet arbiter packet considers remain arbitration ensures progress avoids deadlock improves performance maximize crossbar utilization multicast native vas trigger gpus ordinary memory operation multicast simply replicate network load operation however simply propagate multicast gpus undefined instead implementation route operation memory local gpu embed multicast information address packet allows packet overhead information multicast destination header packet reduction multicast capability benefit exist software reduction algorithm collective opportunity accelerate reduce operation compute capability switch propose perform network reduction complexity implementation gpus switch gpus trigger reduction inject request network illustrate request essentially load operation reduction operator multicast plurality gpus requester network applies reduction operator response return requester reduce operation gpus gpu issue request therefore network multicast completes operation illustrates multi switch gpus gpu gpu simplicity gpu issue request architecture request issue gpus simultaneously implement reduce request arrives input hop switch entry allocate reduction partial reduction return response request allocate entry  switch reduction stall request freed outstanding request response VC request therefore reduction progress eventually release resource stall request backpressure gpus therefore limit injection request reduction allocates entry request network multicast mechanism described IV multi switch request traverse multicast attempt allocate reduction entry hop intermediate switch allocation succeed return response subsequent multicast subtree reduce intermediate reduction response towards request gpus allocation intermediate switch unsuccessful return response bypass intermediate reduction individually rout request gpu opportunistic allocation multi switch avoids complex management protocol reduction resource allocation hop switch response eventually hop reduction upon response gpus return request gpu reduction ability response reduction request allocates entry register response entry information switch mcr marker fabric manager application initialization phase advantage easily handle reduction stall request hop switch resource bypass reduction intermediate switch resource exhaust downside however synchronization reduction ensure gpu operation manipulates data reduce request inject network otherwise request fetch stale data therefore barrier reduce reduction appropriately sustain bandwidth estimate gpu gpu gpu gpu switch switch switch switch gpu gpu gpu gpu gpu gpu request request ID gpu destination destination partial concept nvidia DGX gpu injects request network gpus within multicast respond data assume gpus participate reduction minimum network reduction cmin gpu injection bandwidth bin maximal hop gpus dmax latency gpus latency function per hop latency network  gpu response latency  cmin bin dmax  network latency  bandwidth entry capture return response request therefore negligible consequently bandwidth hop switch consume response request discus VI evaluate mechanism reduction option network reduction request instead request opcodes mcr distinguish reduction multicast request gpu injects data reduction writes packet scatter across network address network fabric packet address rout guarantee packet address rout reduction contrary synchronization beforehand gpus inject data reduction synchronizes gpus return gpus data operation gpus multiple switch data address reduce marked request network hash host host request rout reduction reduction behaves fully associative cache request allocate entry subsequently packet address reduce entry multicast participate gpu reduction packet arrives recently lru policy evict exist entry eviction cannot stall request deadlock crucial handle eviction efficiently inject arbitrary reduction eviction rare evict entry handle multicast partial gpus participate reduction partial gpu partial address multicasts gpus participate reduction strategy improves latency sensitive eviction frequent multicast request generate increase load network approach sensitive increase latency adopt latter approach partial gpu gpu partial reduces multicasts strategy response however packet gpus accounting partial partial gpu multicast discus gpu architectural later IV remainder assume eviction strategy addition address evict another operand reduction arrives later address reduce consequently cannot rely reduction therefore entry timeouts evict estimate apply gpu inject data contrary gpus inject bandwidth consume inject writes cmin bin dmax  solely depends network diameter latency per hop furthermore reduction network whereas guaranteed gpu hop switch switch reduction reduction switch handle computation distribution reduction reduction policy allocate entry reduction described previous internal architecture reduction reduction FP alu data addr data data flit flit timeouts logic FP alu FP alu FP alu RX ingres route crossbar TX logic egress RX TX arbiter alu array RX TX cnt alu cnt cnt concat cnt rout mcr relevant concat init reduction tag addr addr reduction logic flit counter logic compose ALUs perform arithmetic relational operation illustrate reduction packet arrives switch lookup rout mcr arbiter entry reduction network packet rout packet arrives appropriate reduction address header reduction query data load operand register data flit operand register operand alu array meanwhile operand load flit packet address behavior dependent reduction insert flit empty entry bypass reduction logic intermediate switch error hop switch reduction logic flit cycle rate furthermore data align simplify logic avoid complex pack unpack assume packet within packet consecutive byte align packet operand reduce partial calculate ALUs pre threshold directly output reduction logic entry reset initial reduction implement eviction policy prevent deadlock packet cannot insert due logic evict entry timeout per entry consideration switch architecture network collective load switch internal bandwidth baseline switch assume allreduce endpoint switch reduction switch crossbar reduction response reduction output replicate request multicast output output reduction response packet reduce switch therefore internal bandwidth speedup  sustain output bandwidth calculate  compute ratio reduction hpc switch typically internal speedup sustain peak throughput arbitrary traffic configuration cray tile switch architecture speedup comparison approach internal bandwidth sustain architecture complexity network multicast occurs rout switch packet replication variety switch ingres replicate multicast multiple unicast packet buffer inject sequentially crossbar along switch internal speedup ingres replication sufficient sustain peak bandwidth reduce deadlock avoidance network multicast reduction operation additional vcs request response vcs avoid protocol deadlock guarantee multicast traffic rout deadlock switch component within switch guarantee storage packet advance avoids issue associate multicast worm rout deterministic rout multicast unicast packet finally multicast packet conforms unicast packet rout conform  model error handle model assume reliable data transfer layer underneath link transmission LLR mechanism sequence avoid duplicate packet corrupt duplicate packet detect discard sender issue permanent critical error handle software configure network terminate application network additional transmission  congestion mechanism memory network gen NVLink gpu network consideration reduction guarantee deterministic data aggregate non deterministic float deterministic increase memory focus performance assume non deterministic reduction described prohibits concurrent allreduce operation aliasing pointer MCRs unique address endpoint transfer memory initiate processor core dma nvidia gpus issue memory operation processor core initiate  bulk transfer option guarantee packet inject network discus option regulate reduction request injection injection limit synchronization network multicast replicates packet therefore increase congestion inject traffic without regulation stall traffic source quickly backpressure negatively affect traffic inject traffic interference network accept traffic frequent eviction increase congestion therefore reduce network performance interfere traffic regulate injection limit outstanding reduction network reduction capacity ensure data reduce reduction synchronization ensure inject hide synchronization delay outstanding pipeline fashion multiple outstanding hide synchronization latency core initiate reduction although detailed discussion beyond scope highlight software complex model thread implicitly reduction load return multicast model memory network therefore additional synchronization execute instruction significantly software algorithm demand core dedicate communication memory access reduce reduce interference within gpus improves overlap concurrently kernel additional benefit network reduction dma initiate reduction initiate reduction dma allows TX memory core noc dma req dma rsp request generator CRD ack controller RX network addr dma request cnt req  cnt RX goal reset credit counter array noc req gen memory packet data network address local endpoint address ATS address translation service cnt cnt mem architecture controller injection limit processor available application compute task maximize overlap communication computation dma issue request multicast response return network data realize issue dma request request execute sequentially previous request synchronization dma request furthermore dma ability issue load response multicast request ideally dma lock differs handle partial eviction request issue dma response receives completion packet reduction gpus contribute gpu multicast issue request inject network partial memory gpus cache atomics propose mechanism augment dma counter ability withhold credit approach dma receives dma request processor attempt allocate counter controller request generator credit counter allocation succeed controller return credit request controller credit issue memory request local memory network response controller specify credit return request generator counter allocate retry issue eventually error host processor multiple counter multiple outstanding described IV response associate counter lookup address via counter ID packet implementation complexity discussion architecture reduction complexity additional reduce bandwidth speedup SW simulated reduce performance DGX per reduction per speedup graph dot logic switch gpu differs nonetheless majority stem SRAM reduction buffer assume TSMC fft SRAM occupies reduction buffer per switch chip comparison nvidia NVSwitch overhead reduction buffer logic switch already LLR buffer 0Gbps link cable LLR buffer another aspect consumption alu SRAM access additional relatively consumption serializer  multiple magnitude furthermore overall increase memory access alu activity reduce inside gpu data reduce network methodology simulate proposal  successor booksim network simulator reduce algorithm serf baseline implement NCCL deliver bandwidth various algorithm simulated simulation granularity individual memory operation load reduce data fence flag semantic pas chunk data around data consume flag network reduction initiate traffic generator behaving dma initiate IV simulated switch latency bandwidth 5GB per switch sustain uniform random traffic throughput switch internal architecture output queue virtual channel segregate request response traffic output queue handle latency switch addition extend switch proposal IV IV network collective acceleration maximum packet payload remain flit reserve header request acknowledgment flit packet gpu memory static latency cycle comparable volta gpu model cache dynamic additional randomly chosen latency cycle described nvidia DGX described II approach limited gpu implement therefore simulate gpus switch gpus network switch comprises leaf switch gpus per leaf switch directly gpu leaf via link switch switch leaf switch leaf VI evaluation reduce bandwidth evaluate bandwidth  reduction DGX bandwidth calculate message simulation independent node maximal achievable payload bandwidth 0GB reduce performance without synchronization synchronization reduce flag packet per node parameter achieve optimal performance achieves almost optimal performance message across participate gpus synchronization reduces bandwidth message overall achieve optimal performance reduction capacity per whereas per tight synchronization mechanism entire network reduction capacity efficiently explicit synchronization model impact message impact amortize message increase cannot achieve performance message due overhead request acknowledgment packet compete bandwidth payload packet performance software reduce algorithm described benefit network reduction manifest message reduce bandwidth speedup gain due reduce synchronization overhead network reduction software reduce operation speedup converges performance mostly sensitive bandwidth network reduction twice bandwidth algorithm network scalability reduce evaluation gpus performance trend DGX outperform message maximum achieve bandwidth gpus consistent peak bandwidth 0GB gpus peak bandwidth approximately 0GB therefore suspect performance anomaly congestion simulated network capacity congestion mechanism saturation due congestion impact network effective bandwidth theoretical limit performance software algorithm beyond gpus message network reduction magnitude bandwidth due synchronization overhead algorithm however software implementation reduce operation likely combination hierarchy algorithm reduce synchronization overhead optimize software algorithm beyond scope regardless software synchronization improvement network reduction inherently bandwidth advantage software reduction sensitivity sensitivity per differently DGX overall reduction sensitivity synchronization evaluate separately bandwidth sensitive guarantee entry access stall gpu injection resource exhaust regulate however packet return gpus resource exhaust effectively consume bandwidth reduce overall performance furthermore packet inject arbitrary quickly capacity become frequent without injection limit yield bandwidth performance multicasts barely overlap due stall request congest injection channel synchronization depicts synchronization outstanding chose DGX chose per although yield bandwidth adequate parallel comprises outstanding roughly reduction DGX bandwidth achieve although reduction synchronization allows efficient utilization benefit achieve peak performance outstanding however per significantly bandwidth despite synchronization highlight difference efficiently whereas mostly access consequently symmetric switch memory average packet latency indicator qos DGX gpus MB reduce synchronization reduces average latency synchronization injection regulation mechanism calculate accord gpu DGX configuration per access sustain bandwidth confirm simulation applies per DL training II reduce significant impact training dnns model evaluate benefit network reduction application performance speedup NCCL algorithm DGX model shot overlap communication approach calculate speedup kernel launch assume latency network parameter link latency DGX simulation various without synchronization simulation various per transformer resnet bert project speedup SW algorithm various dnns data parallel training described model transformer benefit faster training NVLink speedup ethernet resnet however improvement model MB project future gpus exhibit speedup sensitive bandwidth speedup sub batch decrease mlperf submission transformer gpus sub batch sample sub batch equivalent gpus performance slightly model due bandwidth difference application implementation complexity model remains compelling another observation network reduction layer wise overlap faster shot reduction data transformer however NCCL algorithm overlap beneficial token volta volta increase compute bandwidth ratio reasonable hardware specialization particularly software improvement progress rapidly optimizer bottleneck sub batch model optimizer gpu distribute algorithm significantly decrease optimizer increase bandwidth sensitivity reduce  model model parallelism MB increase model batch reduce increase speedup network reduction reduce improvement volta gpu network reduction benefit future gpus vii related hardware collective communication primitive propose ibm   interconnect facilitate accelerate collective  opportunistic request reservation response reduction approach tailor CPUs message passing accelerator gpus memory fabric recently switch computation gain attention accelerate gradient reduce ML DL application propose programmable switch implement reduction data propose something synchronization implement software without pipelining mellanox introduce  protocol network compute acceleration newer infiniband switch approach aim message passing  endpoint network accelerator explicit reservation resource overhead memory fabric proposal alongside  deploy multiple network domain reduction phase pipeline fashion propose accelerator switch performs reduction however network bandwidth radix switch central accelerator switch quickly become bottleneck  data due wiring limitation packed crossbar chip rack parameter server introduce reduction logic attach switch incorporate others propose network cache none approach combine reduction propose exist proposal optimize reduce software approach orthogonal conclusion network computation benefit reduce operation become increasingly important ML DL training scientific application reduction mechanism accelerator centric memory complexity simpler implementation performance scalability model superior combine tight synchronization mechanism implement network gpus demonstrate improve performance scalability software reduce network reduction yield bandwidth message improves bandwidth message latency becomes bottleneck DL training speedup already deploy interconnects NVLink