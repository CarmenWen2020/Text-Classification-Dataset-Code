quantization effective technique neural network dnn inference acceleration however conventional quantization technique apply network layer fail exploit grain quantization speedup apply kernel without attention feature dynamic NN accuracy propose dynamic quantization namely DRQ precision dnn model dynamically sensitive feature achieve acceleration reserve NN accuracy propose algorithm identify sensitive architecture utilizes variable  convolution array enable algorithm performance efficiency variety network coarse grain quantization accelerator eyeriss DRQ achieve performance gain reduction accuracy loss mixed precision quantization accelerator OLAccel DRQ achieve performance gain reduction prediction accuracy improvement impressive inference introduction increase demand compute dnns quantization acknowledge effective technique reduce inference workload spawn accelerator eyeriss BitFusion OLAccel quantization technique transform float integer data cluster data greatly reduce workload memory bandwidth therefore overall benefit performance silicon accelerator illustrate conventional quantization technique OLAccel quantize NN model static distribution succeed reduce model precision FP int int beforehand simplifies quantization scheme fix kernel execution input feature important impact NN report however exist quantization technique fail dynamically capture sensitivity variability input feature reduce improve accuracy therefore motivate apply quantization sensitivity feature           DRQ OLAccel    comparison DRQ quantization scheme propose novel dynamic quantization DRQ capture feature characteristic input feature quantization dynamic quantization dynamically sensitive input feature likely important feature information inference smartly adjust activation precision computation accord sensitivity characteristic input feature reserve  NN accuracy meanwhile significant amount compute resource non sensitive mainly propose quantization practical efficient conduct input quantization NN accuracy II input highly correlate NN accuracy sensitive addition sensitive tend aggregate observation implies apply  quantization sensitive reserve NN accuracy apply fidelity quantization insensitive boost performance sensitive dynamic quantization propose algorithm identify sensitive algorithm instantiate predictor neatly embed convolution layer predict sensitivity input layer conduct efficient dynamic quantization acm annual international symposium computer architecture isca doi isca DRQ OLAccel BitFusion eyeriss dynamic quantization quantization granularity network wise layer wise wise wise width comparison quantization IV propose accelerator dynamic quantization mainly consist variable systolic convolution array switch execution mode sensitivity identify predictor applies finegrained scheme perform precision computation sensitive precision computation insensitive understand DRQ scheme quantization technique conventional static quantization DRQ dynamically quantizes input sensitive reserve NN accuracy precision computation feature input image accord realtime application dynamic scheme capture per image basis conventional scheme network layer wise quantization DRQ sub layer sub feature quantization freedom grain tune finally DRQ hardware friendly realize slightly modify widely systolic convolution array largely reserve NN accuracy summary DRQ facilitates efficient accelerator tightly couple algorithm hardware architecture image classification workload remain organize II introduces motivation propose describes sensitivity aware convolution algorithm IV elaborates propose microarchitecture VI experimental methodology vii review related finally concludes II  sensitivity input feature  accord distribution input feature convolution neural network cnns majority input feature batch normalization relu activation function exactly zero quantize impact NN accuracy reveal prior naturally arise input feature important NN accuracy affect accuracy extend location input feature spatially distribute FFXUDF FFXUDF resnet accuracy cifar ILSVRC bearing verify input feature affect accuracy sensitive sensitive impact prediction accuracy improper quantization seriously deteriorate overall NN accuracy addition tend aggregate various input feature sensitive sensitive carefully identify quantize grain manner reserve detailed feature characteristic NN insensitive implication advantage sensitivity convolution differentiate quantization feature almost accuracy loss motivates algorithm architecture elaborate IV sensitive existence input sensitive datasets affect NN accuracy widely datasets cifar ILSVRC resnet classify input feature accord magnitude layer classify threshold distribution contains feature contains contains data onto constant factor magnitude adjust intensity finally NN accuracy sensitivity curve curve label     HU   HU     HU    visualize sensitive feature layer curve  plot observation  curve rapidly  fft network behaves approximately curve  tft TTF coincide feature tolerate fft degrade becomes NN accuracy datasets reaction ILSVRC slightly vulnerable cifar classifies image detail observation clearly manifest input impact sensitivity accuracy popular observation across network investigate specific sensitive implies input feature relatively sensitive accuracy retain NN accuracy dataset restrict quantization impact equivalent sensitive feature quantization insensitive relaxed sensitive explore sensitive spatially distribute feature lenet mnist dataset visualize training network mnist randomly raw image conduct inference visualizes input feature layer plot randomly scatter input feature across layer instead tend aggregate sensitive meanwhile dominate feature insensitive ELW    ELW  ELW GG         HG      0D  RU 0D  RU DRQ algorithm overview apply rectangle partition feature individual attribute sensitive insensitive apply dynamic quantization policy observation sensitive account percentage insensitive dominate entire feature sensitivity analysis opportunity improve inference efficiency runtime quantization strategy therefore propose dynamic feature quantization software hardware scheme accelerate inference performance nearly accuracy loss NN scheme consists DRQ algorithm correspond architecture algorithm dynamic BASED quantization II sensitive input feature observation address identify sensitive input feature runtime efficient hardware friendly learnt offline input feature available sensitivity extract efficiently algorithm hardware conduct efficient input sensitivity aware convolution sensitivity introduce precision feature quantization scheme intra layer multi precision quantization grain underlie hardware reduce convolution workload benefit inference performance algorithm overview DRQ algorithm depict algorithm consist prediction algorithm sensitive input feature performs filter target feature activation function generate binary mask distinguish                   mixed precision convolution kernel sensitive insensitive sensitive insensitive input feature detail propose mixed precision convolution inference computation adjust kernel precision runtime sensitivity input feature kernel slide sensitive denote convolution enters  mode finer quantization input feature otherwise convolution  mode insensitive denote pink detail sensitivity prediction input feature dimension quantize input feature FP int typical conventional quantization scheme int sufficient inference workload afterwards split perform dot kernel filter output consequently obtain output output predefined threshold comparison treat activation function correspond sensitive output threshold finally generate binary mask dimension mask indicates sensitive indicates insensitive apply input channel sensitivity prediction mixed precision convolution distinguish sensitive insensitive perform mixed precision convolution without loss generality define convolution int precision int precision sensitivity information mask mixed precision convolution kernel int format dram input feature int int format sensitivity sensitive kernel slide int perform convolution insensitive int format dram kernel slide clip precision kernel int accordingly perform convolution detailed hardware IV however precision switch sensitivity complicate convolution IV exploration obviously threshold determines sensitive consequently sensitive int computation convolution affect NN accuracy sensitivity NN model feature characteristic coarse grain identifier hardware friendly choice NN accuracy alternatively reserve grain characteristic NN accuracy frequent precision switch complicate IV moreover affect efficiency sensitivity predictor IV ensure NN accuracy retrain predefined NN model tune aware sensitivity information feature threshold acquire distribution input feature layer distribution initial threshold empirically retrain model guaranteed accuracy apply precision convolution propagation precision backward propagation update minimize loss NN accuracy training NN converges conduct precision convolution inference evaluate NN accuracy requirement threshold otherwise halve threshold although trial error satisfactory within iteration IV architecture dynamic BASED quantization introduces DRQ algorithm opportunity reduce computation workload sustain NN accuracy identify sensitivity feature introduce accelerator architecture algorithm architecture overview conventional accelerator cannot DRQ algorithm due challenge DRQ algorithm dynamic precision convolution input feature cannot offline architecture capable handle random precision switch precision interleave inside PE array convolution compute memory throughput novel redesign accelerator architecture goal propose DRQ architecture illustrate consists processing PE communicate memory global buffer PE mainly consists buffer  convolution array accumulation output buffer activation pool sensitivity predictor associate logic DRQ architecture briefly described global buffer raw input feature convolution array imcol pack PE feature global buffer transform regulate format pack input buffer load fix array PE convolution array mixed precision convolution elaborate IV partial sum accumulate output buffer variable kernel activation pool function output feature fed another sensitivity predictor identify sensitive DRQ convolution layer sensitivity predictor implement pool finally output global buffer accelerator customize dnns data communication PE schedule kernel filter PE split  eliminate communication buffer input data pack buffer global buffer convolution array convolution PE input feature global buffer input feature transform  convolution packed binary mask buffer input feature fed leftmost PEs convolution array systolic array convolution feature buffer transform imcol function gpus specifically kernel feature align stagger buffer buffer lag cycle data shift partial sum accumulation systolic array cater mixed precision convolution input feature densely packed utilization    XIIHU    XIIHU  XIIHU          XIIHU  WQ WQ WQ WQ WQ WQ       XIIHU   overview DRQ architecture buffer storage insensitive packed register sensitive packed register sensitivity extract associate binary mask accordingly binary mask mask buffer mode PEs convolution array convolution array variable convolution array consists PEs capable mixed precision computation DRQ architecture quantize int format PE array convolution leftmost PEs accept feature shift buffer cycle feature shift partial sum shift PEs cycle multi precision PE PE convolution array feature multi precision capability mac operation PE capable execute precision int int DRQ algorithm prof quantization sufficient NN model without degrade NN accuracy PE previous exhibit int mac united int mac cycle timing multiplexing manner illustrates adapt PE variable systolic array register input respectively register partial default PE int mode int mac multiplication input demand PE switch int mode binary mask generate previous sensitivity prediction specific cycle  XIIHU    QG        DWD     stall stall stall stall     stall stall stall stall stall stall stall stall stall stall stall stall stall stall stall stall stall stall stall stall data arrangement mixed precision convolution execution timing mixed precision convolution ctrl ctrl   ctrl  ctrl  propose multi precision PE extract input respectively shift register cycle extract input respectively another mac accumulate previous register behavior cycle execution mixed precision convolution array built dual mode PEs PEs insensitive behave int sensitive shift convolution array systolic manner variable explain convolution array normally int feature however int convolution enters sensitive feature int correspond feature insensitive MACs int mode cycle computation rate systolic array stall cycle consequently int fully generate cycle cycle int meaning sensitive PEs consume cycle computation cycle situation happens cycle stall cycle PEs leaf sensitive systolic array return int mode consume input cycle convolution array simply receives shift stall signal replicate exactly behavior cycle latency propose mixed precision convolution array  variable convolution meeting requirement sensitive insensitive int mode switch int computation demand insert stall scheme array scheme mixed precision convolution array easy implement systolic manner data brief normally PEs int mode PE int feature identify binary mask switch int mode denote signal plot PEs cycle computation denote signal meanwhile PEs int PEs stall cycle denote signal PEs synchronize systolic execution addition stall signal shift  array cycle latency denote signal array systolic manner input sensitivity signal timing array local PE PE int int mode signal output buffer accumulation output buffer convolution array predictor temporarily partial performs accumulation output feature output feature activation pool sensitivity predictor quantity output feature binary mask traditional  NN extend activation pool prediction convolution dominates execution prediction insert exist NN pipeline negligible performance overhead described IV dual buffering scheme output buffer parallelize convolution accumulation  prediction propose output buffer accumulation variable kernel channel propose convolution array prioritizes kernel NN model configuration aid accumulation kernel split kernel sub kernel launch convolution array obtain partial sum finally accumulate partial sum sub kernel accumulation calculate widely systolic array NN accelerator sensitivity predictor sensitivity prediction enable DRQ differentiates conventional quantization technique predict scheme actually filter propose sensitivity predictor reuse pool without complexity inference feature undergo pool naturally become input convolution layer reuse pool predictor prediction pool illustrate directly average pool prediction instead    XIIHU    XIIHU  HJ  HV HJ pool prediction pool predictor propose architecture      scan activation pool scan activation prediction conduct pool denote pool temporal buffer partial predictor pool prediction partial prediction register pool prediction predefined threshold actually generates binary mask binary mask indicates pool temporal register quantize int int binary mask mask buffer assist feature convolution layer illustrate access activation pool prediction pool prediction therefore pool temporal buffer later computation predictor additional chip storage brief buffer partial prediction pool width feature stripe economic hurt NN accuracy beneficial storage VI storage overhead KB resnet negligible capacity buffer pool configuration network convolution layer average pool operation eyeriss BitFusion OLAccel DRQ PEs bitwidth int int int int int global buffer MB II configuration accelerator int int int MACs TSMC execute prediction pool global buffer average pool adder parallel operation experimental methodology validation NN accuracy valid DRQ algorithm int int quantization implement tensorflow framework specific built compute graph int int computation built api binary mask generate runtime although int sufficient widely cnn network image classification alexnet vgg resnet recent TensorRT quantize int without accuracy loss apply retrain quantization int however DRQ precision limited easily extend int datasets cifar ILSVRC representative  various cnn accelerator image classification NN model alexnet vgg resnet resnet inception mobilenet classic network parameter NN accuracy DRQ eyeriss spatial efficient dataflow architecture coarse grain quantization int throughout network baseline standard NN accuracy BitFusion accelerator feature composable mac employ model typology propose prior algorithm permit quantization layer granularity coarse grain quantization int apply throughout network retain accuracy eyeriss OLAccel accelerator outlier aware precision computation static quantization magnitude comparison alexnet report resnet resnet vgg inception mobilenet available reproduce report model accelerator architecture II configuration accelerator TSMC technology library mac precision int mac almost int mac budget eyeriss int mac BitFusion int MACs OLAccel int mac int mac accordingly DRQ architecture int PEs MACs organize PE PEs PE freely switch int int mode BitFusion int mode compute int mode comparison global buffer capacity memory bandwidth accelerator CACTI estimate latency goal evaluate performance propose DRQ architecture develop cycle accurate simulator simulate mixed precision convolution array buffer output accumulation predictor specific dump input feature binary mask inference tensorflow fed simulator cycle accurate performance mhz PE frequency verify memory bandwidth typical memory bandwidth ddr regulate format input data cached global buffer algorithm sustain non convolution multi precision VI experimental RESULTS accuracy performance consumption NN accuracy network cifar ILSVRC datasets regard resnet DRQ algorithm nearly accuracy loss cifar  int BitFusion int accuracy improvement OLAccel algorithm ILSVRC DRQ algorithm accuracy loss  BitFusion significant accuracy improvement OLAccel DRQ algorithm identify sensitive input feature  computation precision instead OLAccel statically analyzes numerical distribution quantizes accordingly geometric feature implication feature therefore sensitive feature DRQ reserve feature information precision int quantization percentage precision computation network OLAccel DRQ advantage quantization majority computation int mode however OLAccel performs static quantization configuration NN pre fix across layer datasets however DRQ quantization dynamically accord feature predictor activation demonstrate date inception DRQ HULVV LW XVLRQ FFHO HULVV LW XVLRQ FFHO HULVV LW XVLRQ FFHO HULVV LW XVLRQ FFHO HULVV LW XVLRQ FFHO HULVV LW XVLRQ FFHO HULVV LW XVLRQ FFHO HULVV LW XVLRQ FFHO HULVV LW XVLRQ FFHO HULVV LW XVLRQ FFHO HULVV LW XVLRQ FFHO HULVV LW XVLRQ FFHO FFXUDF     FFXUDF HW HW HVHW HVHW   comparison NN accuracy percentage network HULVV LW XVLRQ FFHO HULVV LW XVLRQ FFHO HULVV LW XVLRQ FFHO HULVV LW XVLRQ FFHO HULVV LW XVLRQ FFHO HULVV LW XVLRQ FFHO HW HW HVHW HVHW       HULVV LW XVLRQ FFHO HULVV LW XVLRQ FFHO HULVV LW XVLRQ FFHO HULVV LW XVLRQ FFHO HULVV LW XVLRQ FFHO HULVV LW XVLRQ FFHO HW HW HVHW HVHW   RUH  XIIHU performance breakdown network loss preserve accuracy OLAccel loss percentage compact mobilenet parameter shift quantization input feature quantization DRQ robust preserve NN accuracy loss OLAccel loss percentage execution cycle accelerator network normalize eyeriss resnet eyeriss DRQ achieves nearly performance improvement DRQ mostly int MACs insensitive limited int MACs sensitive int mac budget int MACs eyeriss BitFusion DRQ achieves performance improvement BitFusion mainly utilizes int computation comparison OLAccel applies dedicate int MACs layer int MACs layer DRQ achieve performance improvement thanks PEs budget scheme speedup OLAccel NN accuracy loss benchmark ILSVRC consumption accelerator network decompose dram global buffer processing core core specifically resnet eyeriss BitFusion OLAccel DRQ consumes respectively reduction eyeriss BitFusion mainly due reduce precision PEs resultant narrower width data transfer dram global buffer OLAccel consumption  component DRQ consumes dram HULVV LW XVLRQ FFHO HULVV LW XVLRQ FFHO FFXUDF      FFXUDF  HULVV LW XVLRQ FFHO rup  rup  average accuracy loss average normalize performance int dram DRQ int OLAccel stationary scheme avoid frequently reading int global buffer systolic array consequently DRQ consumes OLAccel global buffer reduce core DRQ mainly due systolic array processing convolution data shift PEs however OLAccel viable architecture towards gpu processing style PE fetch activation local register file cycle amount data access overall DRQ consumes OLAccel core computation report average accuracy loss execution cycle across network plot accuracy loss average cifar DRQ incurs accuracy loss  BitFusion OLAccel accuracy loss instead ILSVRC DRQ incurs accuracy loss  BitFusion OLAccel accuracy loss unacceptable FFXUDF      FFXUDF  threshold application performance DRQ improves performance average eyeriss BitFusion OLAccel accuracy improvement impressive inference meanwhile DRQ reduces consumption average scheme DRQ capability tune threshold sustain accuracy  BitFusion gain notable performance conclusion properly predict sensitive feature apply sensitivity aware quantization algorithm dynamic precision convolution array reserve feature characteristic deliver faithful NN accuracy comparable precision model bandwidth compute dominant insensitive exploration threshold affect performance propose architecture NN accuracy appropriate threshold balance relationship percentage stall ratio systolic array NN accuracy threshold suitable tradeoff percentage storage overhead NN accuracy impact threshold impact threshold resnet sweep generally threshold sensitive allows int quantization degrade NN accuracy moreover sensitive stall ratio stall ratio probability bubble realtime execution plot threshold DRQ achieves optimal resnet impact impact resnet percentage storage overhead buffering predictor NN accuracy feature width plot impact sensitivity easily affected FFXUDF      FFXUDF  sensitivity feature NN model average threshold integer alexnet vgg resnet resnet inception mobilenet average threshold pixel degrade accuracy therefore int quantization recover accuracy impact feature feature characteristic unnecessarily marked sensitive incur redundant int quantization furthermore IV affect storage overhead prediction normalize storage overhead stripe beneficial storage resnet specifically plot suggests choice balance factor optimal threshold network network actual threshold investigate network threshold integer layer average across layer suitable setting offline architecture correspond parameter mention layer feature shrink due pool operation accordingly convolution layer sensitivity reduce fix threshold remains layer become layer activation aggregate towards zero performance breakdown understand DRQ detailed performance breakdown resnet execution cycle algorithm resnet normalize execution cycle resnet contains multiple layer plot occupy proportion RUPDOL HG  HVHW      utilization breakdown execution computation cycle dominate data load cycle due ratio sensitive int int execution cycle across worth resnet sensitive int operation ratio execution cycle DRQ idle switch another load feature load account relatively longer around execution cycle load overhead negligible vii background related WORKS neural network accelerator various NN accelerator propose inference representative technique diannao series chip memory diannao series introduce multiple NN accelerator dadiannao integrates chip eDRAM avoid frequent chip memory access shidiannao accelerates NN parameter NN chip SRAM accelerator cambricon domain specific instruction architecture multiple NN model reduce offchip memory access important performance DRQ architecture benefit chip memory eyeriss dataflow analysis nns impossible chip memory researcher propose dataflow analysis eyeriss explores input stationary stationary WS output stationary OS  NLR propose stationary RS dataflow improve data reuse nowadays dataflow analysis becomes essential NN accelerator DRQ architecture WS OS RS applies WS priority storage overhead input tpu systolic array systolic array tpu efficient convolution reuse data DRQ proposal closely interact algorithm leverage systolic array efficient infrastructure mixed precision accelerator NN prune quantization NN prune quantization widely inference specific prune NN sparse quantization reduces NN precision reduce memory bandwidth EIE cnvlutin exploit sparse NN model data format additional hardware overhead BISMO BitFusion propose execute quantize NN model OLAccel date accelerator utilizes quantization MACs layer OLAccel applies mac layer applies mac mainly cater NN prune quantization consequently architecture struggle peak performance perfect quantization algorithm quantization algorithm quantization algorithm compress NN model reduce input feature representative algorithm cluster applies cluster reduce quantizes applies huffman cod quantize compression rate however cluster data index consume unfriendly hardware binarization compression rate propose quantize NN binarized ternary input feature  input feature  however achieve compression rate notable accuracy loss intra layer quantization importance NN accuracy precision majority data apply precision data however input feature obtain quantize input feature cannot accelerator due mismatch hardware summary interaction algorithm hardware trend future NN accelerator propose DRQ trial closely link respect NN accuracy enable accelerator performance conclusion contemporary quantization algorithm quantize activation integer focus magnitude statistic fail capture inherent geometric characteristic input feature sensitive feature affect NN accuracy reduce computation complexity propose DRQ algorithm dynamically quantize convolution sensitivity input feature moreover propose DRQ architecture efficiently implement propose DRQ algorithm  convolution array evaluation propose DRQ scheme outperforms scheme performance accuracy