graph processing increasingly bottleneck memory access chip cache irregular structure graph seemingly random memory reference however graph significant potential locality predict ahead graph relatively vertex vertex graph processing enjoy significant data reuse hence graph traversal schedule largely determines locality explores online traversal schedule strategy exploit community structure graph improve locality software graph processing framework locality oblivious schedule purpose core benefit locality aware schedule outweigh overhead software framework rely offline preprocessing improve locality unfortunately preprocessing expensive negate benefit improve locality recent graph processing accelerator inherit insight opportunity hardware acceleration allows sophisticated online locality aware schedule realize software significantly improve locality without preprocessing exploit insight bound depth schedule BDFS online locality aware schedule strategy BDFS restricts core explore graph improve locality graph community structure HATS  traversal scheduler purpose core evaluate BDFS HATS algorithm graph simulated core BDFS reduces memory access average however BDFS expensive software degrades performance average HATS eliminates overhead BDFS improve performance average locality oblivious software implementation average specialized prefetchers index graph analytics multicore cache locality schedule prefetching introduction graph analytics increasingly important workload domain graph algorithm diverse characteristic dominate expensive memory access factor  graph algorithm memory bound algorithm compute communication ratio execute instruction usually vertex suffer temporal locality irregular structure graph seemingly random access predict ahead suffer spatial locality perform sparse access byte conventional wisdom graph algorithm essentially random access misconception partially stem limited evaluation synthetic randomly generate graph however detailed analysis reveals graph abundant structure specifically community structure correspond community exist meaningful graph  skewed distribution subset vertex popular hence access frequently others graph algorithm significant potential locality irregular predict locality exploit traversal schedule vertex graph software graph processing framework cannot exploit insight runtime online traversal schedule simply expensive instruction execute per trivial traversal schedule prohibitive overhead instead software framework vertex laid memory strategy vertex schedule vertex schedule sensible generalpurpose core  significant locality recover locality framework offline preprocessing graph layout improve locality subsequent vertex traversal unfortunately preprocessing expensive graph infrequently ruling important application hardware acceleration enables sophisticated online traversal schedule improve locality without expensive preprocessing propose HATS introduces specialized schedule core ahead chooses traverse prior graph accelerator FPGAs ASICs specialized schedule logic implement locality oblivious vertex schedule HATS exploit hardware acceleration improve traversal schedule annual acm international symposium microarchitecture doi micro VO BDFS memory access BDFS reduces memory access pagerank delta VO BDFS   speedup VO VO HATS  improve performance pagerank delta specifically propose bound depth schedule BDFS BDFS core explores graph  fashion maximum depth restricts core explore graph improve temporal locality graph community structure prior dfs technique exploit locality exploit offline graph preprocessing BDFS exploit dfs online locality aware schedule HATS implement BDFS hardware previous indirect prefetchers unlike prefetchers hide latency BDFS traversal improve locality reduces latency bandwidth illustrates benefit BDFS pagerank delta algorithm web graph BDFS reduces memory access vertex schedule VO prior prefetchers graph accelerator reduce memory access  schedule software framework execution pagerank delta software BDFS improve performance overhead outweigh locality benefit hardware acceleration revers situation HATS improves VO performance VO HATS due accurate prefetching hide memory latency prefetching saturates memory bandwidth improve performance reduce memory access BDFS achieves  outperforms VO HATS VO prototyped BDFS HATS rtl evaluate detailed microarchitectural simulation configuration HATS implement hardware another chip reconfigurable logic xilinx zynq soc performance core BDFS HATS easy implement fpga LUTs translates overhead purpose core evaluate HATS important graph algorithm processing graph chip cache capacity core BDFS HATS reduces memory access average improves performance average HATS practical improve locality graph processing II background motivation graph processing framework software graph processing framework interface application programmer specify algorithm specific logic perform operation graph vertex runtime responsible schedule perform operation runtime vertex active iteration performs algorithm specific operation active vertex termination iteration assume bulk synchronous parallel BSP model update algorithm specific data visible iteration graph algorithm unordered runtime freedom schedule processing active iteration schedule affect correctness algorithm impact locality analyze locality tradeoff schedule framework detail graph format graph processing framework compress sparse csr format variation simplicity efficiency csr array offset graph structure vertex offset array array hence vertex offset offset array vertex entry graph graph array algorithm specific data vertex data array pagerank vertex data vertex graph algorithm perform traversal traversal csr format encodes incoming vertex vertex destination vertex update source traversal csr format encodes outgo vertex vertex source update destination compress sparse csr format offset array vertex location vertex array vertex data array algorithm specific data vertex schedule graph processing framework vertex schedule VO technique achieves spatial locality access suffers temporal spatial locality access vertex vertex schedule simply active vertex vertex vertex consecutively specify graph layout processing usually involves access vertex data vertex listing pseudocode iteration pagerank vertex schedule version destination vertex update def pagerank graph dst  src dst vertex data dst  vertex data src  vertex data src listing pagerank vertex schedule memory graph layout offset array correlate graph community structure vertex schedule suffers temporal locality graph vertex schedule ignores graph structure alternate community graph  weakly maximize temporal locality vertex vertex interleave graph layout vertex schedule alternate yield temporal locality preprocessing improves locality expensive prior propose graph preprocessing technique improve locality technique vertex closely community memory improves temporal spatial locality vertex schedule improves temporal locality vertex access vertex nearby improves spatial locality related vertex cache graph preprocessing analyze graph structure identify modify layout vertex vertex schedule modify layout closely community structure fully processing although preprocessing improves locality expensive rewrite graph preprocessing longer graph algorithm impractical important preprocessing amortize graph reuse application graph another algorithm illustrates iteration pagerank algorithm graph vertex VO schedule slice relatively cheap preprocessing technique ignores graph structure GOrder expensive preprocessing technique heavily exploit graph structure GOrder simulate preprocessing overhead intel xeon haswell processor ghz MB llc VO slice GOrder memory access memory access VO slice GOrder execution preprocessing overhead execution memory access execution pagerank iteration various preprocessing scheme although slice GOrder reduce memory access significantly improve pagerank runtime  schedule incur significant preprocessing preprocessing technique beneficial algorithm converges iteration respectively preprocessing technique exploit locality benefit depth dfs breadth bfs traversal dfs partition graph variant dfs seek vertex  partition graph perform local breadth traversal limit partition  vertex partition dfs  distribute framework  partition schedule improve convergence rate asynchronous graph algorithm leverage graph partition pas improves temporal locality vertex data graph layout unlike technique BDFS improves temporal locality without preprocessing however BDFS graph layout improve spatial locality online heuristic improve locality motivate preprocessing prior explore alternative cheaper runtime technique improve locality propagation translate irregular indirect memory reference batch efficient sequential dram access technique spatial locality optimization benefit algorithm vertex contrast BDFS exploit graph community structure improve temporal locality benefit algorithm vertex technique effective unstructured random graph forgo significant temporal locality graph community structure prior hardware technique accelerate graph processing indirect prefetching conventional stride prefetchers capture indirect memory access graph algorithm imp hardware prefetcher dynamically identifies prefetches indirect memory access without application specific information similarly  jones propose specialized prefetcher information application data structure prefetch indirect memory access prefetchers assume vertex schedule improve performance hiding memory access latency easily saturate memory bandwidth become  contrast BDFS traversal schedule reduce bandwidth demand outperform perfect prefetching vertex schedule HATS fetch graph data ahead core graph accelerator non speculatively fetch data indirect prefetchers imp predict access outside core issue speculative prefetches graph accelerator recent propose specialized graph processing accelerator FPGAs ASICs accelerator introduce specialized schedule logic implement  schedule software algorithm likewise rely expensive preprocessing improve locality premise opportunity specialization enables online locality aware schedule achieve benefit preprocessing without overhead beyond schedule accelerator compute memory specialization achieve performance efficiency gain complement prior locality aware schedule limited chip cache capacity although technique context purpose beneficial accelerator bottleneck memory access decouple access execute HATS inspiration decouple access execute DAE architecture access core performs memory operation execute core performs compute operation access execute core communicate queue access core ahead aspect HATS access core decouple core queue ahead expose abundant memory parallelism however HATS specialized graph traversal cheaper faster programmable access core unlike DAE core performs memory access instead communicate HATS HATS focus handle traversal graph unlike conventional DAE communication HATS core HATS ahead core avoid performance bottleneck DAE communication loss decouple  locality BDFS goal achieve benefit preprocessing avoid overhead improve temporal locality schedule runtime graph community structure without modify graph layout describes technique BDFS describes hardware implementation technique HATS BDFS traverse graph perform series bound depth vertex bound depth context iterative deepen optimization technique moreover described sec II preprocessing algorithm leverage dfs improve locality however knowledge BDFS online locality aware schedule graph traversal sequential implementation BDFS analyze locality discus parallel implementation BDFS algorithm listing pseudocode pagerank recursive implementation BDFS BDFS active bitvector vertex listing version pagerank vertex active iteration bitvector initialize BDFS processing vertex thereafter chooses vertex vertex ignore inactive vertex exploration proceeds depth fashion within maxdepth away vertex exploration vertex BDFS scan active bitvector unvisited vertex vertex efficient version vertex iteration later evaluate pagerank delta performs optimization def pagerank graph iterator BDFS iterator  src dst iterator vertex data dst  vertex data src  vertex data src def BDFS active bitvector  active   active active false BDFS explore def BDFS explore int dst int  src dst yield src dst  maxdepth active src active src false BDFS explore src  listing pagerank implementation BDFS yield return caller resume callee invoked BDFS improves temporal locality processing vertex within community BDFS vertex graph maxdepth traversal BDFS traverse incoming vertex unlike vertex schedule BDFS tends knit BDFS improves temporal locality access vertex data vertex likely processing naturally exploit community structure realworld graph processing involves access vertex data currently vertex processing vertex access already cached data schedule overhead schedule structure BDFS  stack active bitvector BDFS stack zero memory access although bitvector irregular access vertex data pagerank bitvector vertex data per vertex overhead BDFS extra memory access schedule logic listing vertex although BDFS linear complexity vertex graph algorithm execute instruction per relatively expensive software BDFS executes instruction VO extra instruction data dependent limit instruction parallelism overhead outweigh locality improvement BDFS motivate HATS analysis access memory access  VO schedule listing BDFS listing access vertex data array VO vertex sequentially spatial locality offset memory access vertex schedule BDFS array vertex data access currently vertex array data structure graph cache typically amortizes fetch VO BDFS memory access offset vertex data vertex data breakdown memory access data structure pagerank graph however VO suffers temporal spatial locality vertex data access vertex access dominate illustrates breakdown memory access data structure pagerank memory access vertex data BDFS improves temporal locality vertex data access processing community however reduces spatial locality offset array access BDFS access vertex slice array fortunately access remain enjoy spatial locality VO tradeoff vertex data almost offset increase BDFS reduces memory access BDFS tune maximum depth alternative strategy alternative BDFS bound breadth schedule  BDFS outperforms  cheaper implement dfs locality bfs dfs stack bfs fifo queue entire graph illustrates pagerank memory access BDFS  fringe BDFS stack  queue grows memory access normalize vertex schedule BDFS outperforms  fringe achieves peak performance fringe whereas  fringe effective graph evaluate trend BDFS insensitive stack depth BDFS performance stack depth fringe traverse community fringe norm memory access BDFS  fringe norm memory access BDFS  memory access pagerank graph BDFS bound bfs  fringe BDFS reduces memory access storage  cache average vertex depth traverse vertex whereas depth traverse vertex however converse deeper stack traversal vertex cache stem dfs  conquer suppose graph stack depth yield community cache suppose instead depth node equivalent perform depth traversal sequence cache induction BDFS overwhelm cache deeper stack observation yield nice BDFS tune analyze graph stack depth simply fix depth hardware yield traversal BDFS yield locality cache regardless parallel BDFS parallelize BDFS evenly active bitvector across thread thread independent BDFS traversal chunk vertex listing operation active bitvector atomically avoid finally steal avoid load imbalance thread chunk steal another thread remain vertex sophisticated parallelization strategy seek thread explore within community graph graph synchronization overhead without benefit steal approach IV HATS hardware  traversal scheduling BDFS effectively reduces cache implement software overhead negate benefit locality address  traversal schedule HATS HATS specialized performs traversal schedule HATS enables sophisticated schedule strategy BDFS architecture architecture core augment HATS configures perform traversal passing address csr structure HATS ahead core communicates core fifo buffer effectively offloads traversal schedule portion graph algorithm HATS core exclusively vertex processing listing core executes per operation inside pagerank function HATS executes everything propose evaluate HATS purpose processor HATS implement fix function architecture core HATS traverse graph sends core hardware chip reconfigurable logic focus purpose processor traversal schedule graph algorithm specialize algorithm specific vertex processing programmable core specialized traversal scheduler impose negligible overhead prior indirect prefetchers unlike prefetchers HATS reduces memory latency bandwidth sec II HATS dose specialization performance boost important application domain without sacrifice programmability entry purpose processor HATS apply architecture replace purpose core  accelerator generality HATS traversal active non active algorithm HATS accelerate vast majority graph processing algorithm spectrum framework ligra HATS assumes csr graph format commonly addition HATS csr variant  moreover reconfigurable logic implementation HATS graph format overhead remainder explain HATS interface operation detail HATS variant HATS implementation VO BDFS traversal overhead ASIC fpga implementation HATS HATS interface operation HATS accelerates traversal schedule leaf responsibility core initialization vertex processing load balance operation regardless traversal schedule strategy implement HATS VO BDFS traversal iteration pagerank proceeds initialization software initializes data structure graph data active bitvector specifies vertex bitvector depends graph algorithm traversal schedule VO HATS sec IV active bitvector algorithm vertex active iteration bfs BDFS HATS sec IV active bitvector avoid processing vertex multiple HATS configuration thread configures HATS convey information address graph data structure offset vertex data array active bitvector traversal chunk vertex HATS responsible vertex configuration data memory mapped register dma configure core writes configuration HATS traversal processing traversal HATS offset array active bitvector BDFS HATS performs update active bitvector finally prefetches vertex data HATS unvisited active vertex fifo buffer source destination vertex core core fetch instruction fetch buffer instruction fetch return source destination register software extra instruction translate vertex data address HATS traverse assign chunk vertex fetch return fifo empty fetch stall core HATS traversal stall HATS transparent application expose functionality software graph processing framework api consist configure performs configuration fetch translates fetch instruction graph algorithm api code algorithm highly optimize ligra graph processing framework sec framework modify HATS application code unchanged parallelism load balance parallel operation software BDFS implementation sec vertex chunk thread HATS responsible scan chunk perform  steal HATS chunk thread interrupt randomly chosen thread  remain chunk HATS termination algorithm cilk handle preemption HATS behalf thread architecturally visible preemption OS  thread  HATS architectural remainder chunk address data structure thread reschedule core HATS configure thread  exception FPU kernel rare virtual memory finally HATS operates virtual address prior indirect prefetchers HATS leverage core address translation machinery unlike indirect prefetchers HATS monitor core cache access HATS core tlb HATS fault handle prior indirect prefetchers core interrupt OS fault handler invoked HATS stall fault handler completes fault handler core HATS resume normal execution VO HATS implementation HATS vertex schedule operation assume traversal sec IV discus traversal microarchitecture VO HATS VO HATS pipelined implementation illustrate pipeline stage corresponds fetch graph data scan vertex HATS chunk active algorithm scan simply output vertex cycle increment algorithm active scan load active bitvector output active vertex fetch offset vertex input output offset load offset array fetch offset vertex input output load array vertex queue fifo buffer prefetch issue prefetches vertex vertex data memory parallelism stage decouple FIFOs scan fetch stage request cache parallel suffices fifo BDFS HATS implementation implementation BDFS HATS VO HATS additional logic perform data dependent traversal explain operation optimization achieve performance operation component BDFS scheduler fix depth stack stack information vertex vertex offset cache worth microarchitecture BDFS HATS vertex stack provision maximum depth BDFS exploration implementation sec tune BDFS depth maximum depth unlike VO BDFS active bitvector active algorithm HATS bitvector restrict exploration active vertex update traverse graph clearing vertex decides explore traversal empty stack VO scan stage traverse active bitvector active vertex vertex immediately marked inactive vertex serf bound depth exploration vertex stack offset fetch fetch cache vertex checked active bitvector active marked inactive along vertex offset stack entry traversal depth fashion topmost stack stack depth traversal proceeds newly fetch queue fifo buffer stack fetch traverse traverse exploration previous explore fully explore scan stage vertex BDFS HATS finite machine FSM implement procedure cycle FSM decides data fetch stack exploit intra traversal parallelism unlike VO BDFS traversal data dependence serialization additional optimization exploit parallelism within BDFS traversal obtain memory parallelism saturate core active bitvector operation critical perform parallel constitute substantial BDFS VO HATS BDFS HATS implementation ASIC fpga zynq HATS ASIC ASIC fpga core TDP lut fpga VO BDFS instead vertex eagerly vertex issue pending bitvector active inactive response stack expand active parallel instead expand stack additional entry topmost active vertex completely explore data vertex already available switch data vertex fetch greatly reduces critical additional storage optimization BDFS HATS achieves throughput avoid stall core extend HATS traversal described traversal variant HATS easily extend perform traversal difference active bitvector traversal active bitvector checked filter vertex explore VO HATS scan stage contrast traversal fetch vertex graph bitvector filter inactive adapt VO HATS simply perform bitvector fetch stage instead scan stage BDFS HATS prototype  traversal hardware ASIC implementation verilog rtl VO HATS BDFS HATS synthesize commercial ghz target frequency consumption typical operating negligible core intel core manufacture BDFS HATS core core TDP VO HATS cheaper prior indirect prefetchers comparison internal storage requirement proxy VO HATS kbit storage internal fifo buffer BDFS HATS kbit stack addition kbit output fifo buffer comparison imp kbit storage HATS fpga implementation synthesize HATS fpga platform VO HATS BDFS HATS II configuration simulated core core ISA ghz haswell cache KB per core associative split cycle latency cache KB private per core associative cycle latency cache MB hash associative inclusive cycle latency lru replacement global noc mesh flit link rout cycle pipelined router cycle link coherence MESI cache directory memory controller FR FCFS ddr GB per controller LUTs respectively lut modest xilinx zynq soc FPGAs LUTs mhz target frequency ASIC implementation ensure HATS core throughput frequency parallelism within however replicate entire HATS pipeline achieve active bitvector filter stage become bottleneck operating frequency replicate bitvector logic perform multiple bitvector operation parallel sec mhz core evaluation methodology evaluation methodology simulated graph algorithm datasets simulation infrastructure perform microarchitectural execution driven simulation zsim implement detailed cycle driven model propose VO BDFS HATS simulate core parameter II core model validate intel haswell core core private cache core MB cache memory controller  McPAT derive chip component micron  datasheets compute memory algorithm graph algorithm widely ligra framework active non active algorithm algorithm cache pagerank computes relative importance vertex graph originally rank webpage pagerank delta variant pagerank vertex active iteration accumulate pagerank component graph vertex disjoint subset component vertex belonging subset radius estimation estimate radius vertex graph algorithm vertex  active pagerank PR pagerank delta prd conn component CC radius estimation max  MIS perform multiple parallel bfs sample vertex maximal independent maximal subset vertex vertex subset obtain source code algorithm ligra adapt schedule code HATS program model without modify per algorithm code incorporate optimization schedule code careful loop unroll yield significant speedup implementation outperform ligra approach optimize software baseline important affect qualitative tradeoff optimize implementation memory bound saturate bandwidth effectively IV graph datasets graph vertex description arb arabic twi twitter follower web  datasets web social graph detailed IV graph diverse harmonic diameter average cluster coefficient vertex data footprint cache graph memory compress sparse csr format graph algorithm generally execute iteration convergence avoid simulation iteration sample perform detailed simulation iteration iteration skip initialization yield accurate execution characteristic algorithm slowly consecutive iteration iteration sample perform detailed simulation billion instruction graph ASIC HATS evaluation memory access memory access VO BDFS thread pagerank breakdown access data structure due demand access prefetches BDFS benefit stem reduce vertex data explain sec benefit across graph BDFS reduces memory access significantly average indicates access BDFS data structure mainly active bitvector negligible bitvector cache web BDFS reduces graph twi due twi weak community structure twi BDFS improve temporal locality vertex data access offset arb web twi average memory access offset vertex data vertex data breakdown memory access data structure VO BDFS singlethreaded pagerank PR prd CC MIS memory access arb web twi average memory access BDFS algorithm thread normalize VO access PR prd CC MIS speedup VO slowdown software BDFS VO thread imp VO HATS BDFS HATS arb  gmean speedup VO PR arb web twi gmean prd arb web twi gmean CC arb web twi gmean arb web twi gmean MIS speedup software VO thread VO HATS BDFS HATS significantly improve performance software VO hardware prefetchers imp preprocessing technique benefit twi exclude twi BDFS reduces memory access average pagerank twi weak community structure outlier twi cluster coefficient whereas realworld graph therefore BDFS beneficial sec adaptive HATS detect graph weak community structure switch VO schedule memory access BDFS thread algorithm BDFS reduces memory access significantly algorithm average PR prd CC MIS respectively non active algorithm prd CC slightly reduction algorithm subset vertex active iteration active vertex data likely cache slight increase BDFS memory access thread pagerank thread MB llc available traversal whereas thread llc concurrent traversal interference increase graph sensitive per thread cache capacity performance average slowdown software BDFS VO software BDFS VO algorithm happens despite reduction memory access BDFS bookkeeping overhead implement software graph algorithm execute instruction per overhead relatively speedup scheme software VO imp indirect memory prefetcher  VO VO HATS BDFS BDFS HATS ensure imp issue accurate prefetches configure explicit information graph structure imp improves performance non active algorithm memory latency bound prd CC MIS imp saturate bandwidth prd CC  achieves gain offload traversal schedule HATS imp already saturates bandwidth PR MIS VO HATS improve performance overall VO HATS improves performance VO average prd CC MIS respectively however neither imp VO HATS reduce memory traffic performance gain limited memory bandwidth noticeable PR software VO already saturates memory bandwidth VO HATS imp barely improve performance contrast BDFS reduce memory access translate improve performance BDFS HATS PR speedup VO arb graph average BDFS HATS improves performance PR VO VO HATS BDFS HATS achieves slightly gain VO HATS non active algorithm BDFS HATS improves average performance VO HATS prd CC MIS respectively VO breakdown various scheme software VO core memory core depends compute bound algorithm highly memory bound application pagerank memory contributes VH BH PR VH BH prd VH BH CC VH BH VH BH MIS normalize memory cache HATS core static breakdown normalize VO VO imp VH VO HATS BH BDFS HATS HATS reduces core offloads traversal schedule specialized hardware reduce instruction purpose core non active algorithm significant instruction activeness VO HATS completely eliminates instruction average reduces core prd CC MIS respectively BDFS reduction memory access proportional reduction memory overall BDFS HATS reduces average VO algorithm overall reduction graph processing accelerator reduce core memory bottleneck imp barely reduces neither reduces instruction memory access HATS chip reconfigurable fabric assume ASIC implementation HATS VO HATS BDFS HATS reconfigurable logic implementation model chip fpga fabric issue access cache xilinx zynq soc performance core unlike zynq fpga fabric core assume per core fpga fabric private later explore HATS memory hierarchy account integrate fpga fabric harp difference ASIC implementation frequency mhz HATS parallelism replication pipeline explain sec IV sufficient core performance HATS version without ASIC fpga VO HATS BDFS HATS average VH BH PR VH BH prd VH BH CC VH BH VH BH MIS speedup VO ASIC default fpga HATS performance fpga fabric VH BH PR VH BH prd VH BH CC VH BH VH BH MIS speedup VO hardware default software fifo buffer model variant HATS core communicate fifo buffer memory avoids dedicate fifo channel HATS core reconfigurable fabric avoids ISA fetch instruction although buffer management operation increase core instruction PR workload memory bandwidth bound negligible impact performance VO HATS insensitive BDFS HATS performance loss MIS adaptive HATS explore adaptive version HATS switch VO BDFS dynamically adaptive HATS beneficial graph community structure BDFS increase memory access VO lower performance twi graph across algorithm graph community structure later phase BDFS exploration usually locality simpler VO schedule phase improves performance due schedule overhead VO HATS BDFS HATS adaptive HATS arb web twi speedup VO prd graph PR prd CC MIS speedup VO gmean across graph adaptive HATS outperforms BDFS HATS avoid BDFS HATS performance pathology adaptive HATS extension BDFS HATS switch VO BDFS maximum depth exploration VO BDFS cycle HATS switch alternative mode exploration cycle  mode cycle performance VO HATS BDFS HATS  prd graph web twi benefit adaptive HATS prd benefit algorithm gmean performance across graph adaptive HATS outperforms BDFS HATS PR prd CC MIS BDFS HATS versus locality optimization BDFS HATS online offline technique improve locality propagation PB online heuristic improve spatial locality active algorithm pagerank PB access graph sequentially update partition update bin bin update cache fitting slice vertex BDFS HATS PB arb web twi avg memory access normalize VO memory access arb web twi avg speedup VO performance propagation reduces memory traffic significantly limited performance gain update memory PB update memory bin bin finally applies phase generate sequential access memory optimization implementation obtain PB author modify simulator model non temporal crucial reduce PB memory traffic moreover deterministic PB generates per update vertex reuses across iteration bin MB memory access BDFS HATS PB normalize VO average PB achieves slightly memory access BDFS HATS unstructured graph twi however PB software technique non trivial compute achieve memory access reduction hence performance gain PB limited PB achieves speedup hurt performance twi average PB faster VO whereas BDFS HATS faster moreover PB limitation PB extend non active algorithm per update vertex cannot reuse across iteration PB algorithm update commutative memory access BDFS HATS GOrder preprocessing expensive heuristic heavily exploit graph structure GOrder achieves memory access BDFS HATS memory traffic reduction translate improve performance GOrder HATS combine GOrder VO HATS improves performance significantly non active algorithm prd CC MIS BDFS HATS GOrder GOrder HATS PR prd CC MIS memory access normalize VO memory access PR prd CC MIS speedup VO performance BDFS HATS versus GOrder preprocessing sensitivity impact prefetching HATS accurately prefetch vertex data accelerate processing core prefetching effective VO BDFS HATS variant without vertex data prefetching prefetching account speedup achieve BDFS HATS VO prefetches irregular conventional prefetcher perform PR prd CC MIS speedup VO VO HATS pref BDFS HATS pref imp BDFS HATS VO HATS impact prefetching performance HATS prefetches timely limited HATS buffer entry constrains ahead HATS prefetched data KB avoid prefetches HATS buffer avoid prefetches prefetches partially overlap demand access prefetches access latency average PR prd CC MIS speedup VO default llc sensitivity  chip location HATS location location HATS benefit BDFS HATS VO performance slightly HATS however HATS llc fpga fabric noticeable performance  active algorithm configuration HATS prefetch vertex data llc core cycle latency access vertex data memory bandwidth speedup VO HATS BDFS HATS VO memory controller grows peak memory bandwidth PR prd CC MIS speedup VO VO HATS BDFS HATS default speedup VO HATS shade BDFS HATS VO memory  PR prd CC MIS speedup VO inorder  haswell speedup  core VO haswell core grows GB VO HATS BDFS HATS beneficial bandwidth improvement BDFS HATS  increase bandwidth memory controller BDFS HATS outperforms VO HATS memory controller speedup reduce purpose core speedup BDFS HATS core speedup normalize VO haswell core compute bound  active algorithm prd CC sensitive  retains benefit lean  core memory  moreover HATS efficient core faster software VO hungry core cache performance VO HATS BDFS HATS various cache speedup relative software VO fix cache MB speedup across cache comparable PR MIS BDFS HATS MB cache outperforms VO HATS VO MB cache prd BDFS HATS MB outperforms VO MB cache closely  MB cache VH BH PR VH BH prd VH BH CC VH BH VH BH MIS speedup VO MB MB default MB sensitivity llc speedup relative software VO MB PR prd CC MIS speedup VO lru default DRRIP BDFS HATS llc replacement policy llc replacement policy finally llc replacement policy affect benefit BDFS evaluate BDFS HATS lru DRRIP highperformance replacement policy BDFS HATS achieves slightly gain DRRIP happens DRRIP scan thrash resistant reduces cache pollution access temporal locality leaf cache capacity data temporal locality BDFS exploit BDFS HATS highperformance replacement policy complementary  benefit specialize replacement policy graph data structure VI additional related memory specialization graph processing recent graph processing accelerator propose various tune memory hierarchy graph algorithm scratchpad vertex data chip eDRAM scratchpad graph SRAM prior propose data processing ndp execute graph processing operation logic memory reduce memory access ndp memory bandwidth attractive processing unstructured graph without locality graph community structure cache effectively moreover ndp development important optimize conventional chip memory HATS complement prior locality aware schedule limited chip storage preprocessing locality optimization sec II preprocessing algorithm rcm GOrder rabbit improve locality VO expensive technique reorder graph vertex assign related vertex graph adjacency matrix narrow matrix nonzeros cluster around diagonal however reorder magnitude expensive runtime traversal similarly  schedule hilbert outperforms VO balance locality source destination vertex expensive sort vii conclusion graph processing algorithm increasingly bottleneck memory access runtime schedule strategy exploit community structure graph significantly improve locality propose bound depth schedule BDFS highly effective schedule technique improve locality graph community structure HATS hardware accelerate traversal scheduler BDFS HATS inexpensive hardware reduces memory access significantly simulated core BDFS reduces memory access BDFS HATS improves performance locality oblivious software implementation specialized prefetchers