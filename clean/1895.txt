neural network become dominant algorithm rapidly achieve performance application image recognition recognition processing however neural network towards deeper architecture challenge amount data computation although sparsity emerge effective reduce intensity computation memory access directly irregularity sparsity sparse synapsis neuron prevents accelerator completely leverage benefit introduces costly index module accelerator propose cooperative software hardware approach address irregularity sparse neural network efficiently local convergence namely tend cluster training observation propose software coarse grain prune technique reduce irregularity sparse synapsis drastically coarse grain prune technique local quantization significantly reduces index improves network compression ratio hardware accelerator cambricon address remain irregularity sparse synapsis neuron efficiently novel accelerator feature selector module filter unnecessary synapsis neuron sparse neural network accelerator accelerator performance efficiency respectively introduction neural network NN algorithm gain increase attention academia widely achieve theart performance application image recognition recognition synthesis processing important dominance amount data computation neural network due tighten constraint performance requirement customize accelerator emerge performance efficient alternative  chen  ict correspond author traditional CPUs gpus recently accelerator propose address efficiency issue meanwhile neural network advance towards deeper architecture involve sophisticated processing task broader scope scenario neural network intensify computation memory access increase amount data synapsis neuron alexnet neuron increase synapsis billion billion respectively despite computation amount data challenge offchip chip memory bandwidth accelerator architecture recently researcher challenge reduce computation data amount sparsity emerge effective various technique prune synapsis neuron network data reduction negligible accuracy loss encode via quantization entropy cod investigate shrink alexnet technique circuit shorter width operator approximate compute however irregularity sparsity synapsis neuron prevents accelerator fully leverage computation data reduction diannao dadiannao shidiannao cannot benefit sparsity cambricon benefit synapse sparsity cnvlutin benefit neuron sparsity although EIE exploit neuron sparsity synapse sparsity aim fully layer propose cooperative software hardware approach address irregularity sparse neural network efficiently observation local annual acm international symposium microarchitecture doi micro matrix local convergence illustrate fully layer absolute plot pixel convergence tend cluster training illustrate hence propose software coarse grain prune technique reduce irregularity sparse synapsis propose measurement irregularity reduces average coarse grain prune technique local quantization significantly reduces index improves network compression ratio obtain reduction index compression ratio alexnet negligible accuracy loss reduce irregularity sparse synapsis hardware accelerator cambricon address remain irregularity sparse synapsis neuron efficiently novel accelerator feature neuron selector module NSM additional synapse selector module SSMs filter unnecessary neuron synapsis sparse neural network accelerator accelerator performance efficiency respectively contribution observation local convergence leveraged reduce irregularity sparse neural network challenge efficient acceleration observation local convergence propose coarsegrained prune technique reduce irregularity sparse synapsis drastically thoroughly analyze propose coarse grain sparsity additional dynamic neuron sparsity principle although propose prune technique reduces irregularity sparse synapsis significantly hardware accelerator address remain irregularity selector module II background motivation primer neural network neural network neural network consist multiple layer input layer classify recognize layer neuron receives multiple input processing sends output layer via connection connection neuron synapsis usually dependent independent network image comparison   SNS dns  cnvlutin EIE FC layer ese lstm scnn extra coordinate coarse grain sparsity dense neural network static sparsity synapse static sparsity neuron dynamic sparsity application convolutional neural network cnns neural network dnns convolutional pool normalization fully layer usually recurrent neural network rnns widely application memory lstm popular rnns sparsity neural network although neural network dominant application  architecture burden computation memory capacity memory access processing various effort challenge algorithm dropout sparsity architecture width operator inexact compute physical dynamic voltage technique revisit sparsity technique effective approach researcher proven effectiveness sparsity resolve overfitting issue neural network researcher propose training technique sparse cod auto encoder decoder belief network DBN prune redundant synapsis neuron without loss accuracy recently  propose prune achieves sparsity cnns alexnet vgg report detailed prune classify sparsity category static sparsity dynamic sparsity static sparsity synapsis neuron permanently remove network dynamic sparsity occurs neuron output contribute neuron prune neuron output input data sparsity dynamically static sparsity consists neuron sparsity SNS synapsis sparsity dynamic sparsity consists neuron sparsity dns encode compress data researcher apply image compression technique quantization entropy cod remain recent compression apply grain prune quantization huffman cod obtain compression ratio alexnet another effective compression scheme  tackle issue frequency domain finally obtain compression ratio thereby cnns compression ratio knowledge encode procedure consists quantization entropy cod cluster algorithm cluster denote sparsity ratio remain neuron synapsis neuron synapsis cluster quantization entropy encode centroid    encode encode via quantization entropy cod cluster scatter cluster cluster centroid minimal distance within cluster therefore cluster centroid unique index per codebook unique occurrence probability codebook unbalanced entropy cod huffman encode employ reduce encodes variable codewords motivation irregularity leverage benefit reduce computation memory sparse network sparsity efficiently however processing platform fail sparsity efficiently due  irregularity CPUs gpus performance processing sparse network processing dense version customize accelerator without sparsity benefit sparsity accelerator sparsity suffer irregularity notable sparsity processing fail fully sparsity particularly cambricon achieves speedup diannao sparsity feature accelerator index module IM selects transfer neuron PEs focus synapse sparsity cnvlutin accelerator neuron sparsity improves performance overhead however fails leverage benefit synapse sparsity scnn exploit synapse sparsity neuron sparsity introduces extra compute coordinate hinder efficiency scnn achieves performance consumes processing dense network gain improvement performance efficiency sparsity EIE static sparsity dynamic neuron sparsity achieves throughput efficiency efficiency respectively dadiannao however target fully layer dnns ratio computation cnns ese implement fpga focus sparse lstm model aim lstm layer suitable convolutional layer dominates computation cdf distribution  training iteratively coarse grain prune  local quantization entropy encode network highly compress network compression cnns exist architecture cannot irregularity inefficiency observation previous prune approach synapsis independent ignore potential relationship synapsis fully analyze distribution neural network phenomenon matrix fully layer contains correspond input neuron corresponds output neuron local tend cluster local convergence exploit local convergence slide across matrix define absolute classify label representative layer alexnet vgg mlp  lstm conv alexnet slide former layer conv layer additionally plot cumulative distribution classification layer randomly initialize layer comparison distribute initialize layer layer distribute likely training  neural NETWORKS entire propose compression algorithm coarse grain prune quantization entropy cod coarse grain prune instead prune synapsis independently propose coarse grain prune prune synapsis synapsis firstly synapsis permanently remove network topology specific criterion employ tune approach retain network accuracy apply coarsegrained prune iteratively training achieve sparsity avoid accuracy loss nin  bin bout sin sout coarse grain prune fully layer prune methodology clearly explain coarsegrained prune technique fully convolutional layer fully layer output neuron  input neuron nin via synapsis independent 2D matrix nin  prune slide bin bout stride sin sout along dimension respectively synapsis prune simultaneously iterative prune slide prune synapsis prune index prune metric apply max prune average prune max prune prune maximum absolute predefined threshold wth average prune prune average absolute predefined threshold regard convolutional layer output neuron output feature neuron input feature via synapsis convolutional layer express dimensional tensor    input feature  output feature convolution kernel prune slide   stride   along dimension respectively max prune average prune apply prune characteristic optimal balance compression ratio accuracy crucial prune overly inability grain connectivity lose accuracy prune overly fully exploit local convergence fail achieve sparsity dimension coarse grain prune wise grain prune prune network carefully explore layer neural network II sparsity compression ratio alexnet USING   convolutional layer layer overall compression ratio MB index MB fin       coarse grain prune convolutional layer layer instead layer extreme training efficiency hardware focus layer network convolutional layer sensitive fullyconnected layer employ coarse grain prune dimension convolutional layer alexnet illustrate procedure clearly maintain accuracy error II denote prune convolutional layer fully layer respectively II convolutional layer fully layer quantize respectively encode huffman encode varies compression ration increase rapidly rapidly compression ratio increase grows memory index compression ratio grows sparsity increase rapidly maintain accuracy hinder compression ratio compression ratio accuracy convolutional layer respectively importantly coarse grain prune index KB reduction MB grain prune neuron sparsity coarse grain prune neuron directly prune prune synapsis however dynamic sparsity neuron zero ratio network static sparsity dynamic sparsity comparison average entire input dataset neural network alexnet vgg resnet static neuron sparsity convolutional layer however dynamic neuron sparsity promising alexnet vgg opportunity performance gain avg max prune average prune max prune straightforward coarse grain prune strategy characteristic prune strategy sparsity nns lenet mlp cifar alexnet vgg resnet SNS dns SNS dns max prune avg prune cifar model max prune indicates inside dominate importance average prune indicates inside contribute importance accuracy achieve training cifar model sparsity average prune achieves accuracy max prune average prune measurement irregularity coarse grain prune propose effective reduce irregularity compute irr   irr indicates reduce irregularity index sparse neural network grain prune coarse grain prune respectively  indicates lossless image compression standard joint bilevel image  regular data binary matrix contains redundant information data therefore synapsis index binary image compress  data compression irregularity reduce irregularity ratio compress index coarse grain prune grain prune local quantization network quantization proven efficient reduce leverage local convergence propose local quantization employ local matrix instead matrix illustrate local quantization matrix sub matrix perform cluster  sub matrix encode codebook per per global quantization global quantization local quantization exploit local convergence shrink obtain compression ratio overhead local quantization minor layer alexnet global quantization achieves per codebook MB local quantization achieves per KB codebook submatrices MB codebook local quantization entropy encode entropy encode lossless data compression scheme creates assigns unique prefix code unique codeword approximately proportional negative logarithm probability encode entropy encode technique huffman cod arithmetic cod compression detailed compression algorithm representative neural network IV lenet mlp layer mlp hidden neuron cifar model alexnet vgg resnet lstm exist neural network compression compression grain prune  mention II focus lstm layer lstm model propose algorithm achieves promising compression ratio compression  negligible accuracy loss sparsity compression neural network alexnet vgg compression ratio almost compression  residual network algorithm compression obtain compression ratio traditional dnns hypothesize novel feature shortcut connection batch normalization IV sparsity compression ratio  irregularity  algorithm   local quantization  local quantization encoding lstm layer convolutional layer fully layer compression ratio   compression ratio   local quantization overall compression ratio irr  irregularity model sparsity irr alexnet vgg lenet mlp cifar resnet lstm sparsity compression ratio accuracy comparison sparsity compression ratio model ref cmp  alexnet vgg lenet mlp cifar resnet lstm reference accuracy coarse grain prune layer topology correspond connection greatly reduce overfitting limited compression ratio residual network accuracy algorithm compression  reference accuracy obtain non prune network accuracy loss due coarse grain prune negligible average reference accuracy compression  IV report reduce irregularity coarsegrained prune notable coarse grain prune reduce irregularity average consistent intuition reduce irregularity network lenet mlp cifar reduce irregularity average contrarily network alexnet vgg lstm reduce irregularity average additionally resnet prune irregularity reduction validate coarse grain prune significantly reduce irregularity IV  analyze coarse grain prune network accelerator principle fully layer index output neuron connection input neuron output neuron prune connection input neuron output neuron output neuron connection topology index representation synapse index input neuron multiple output neuron without loss generality prune bin bout fully layer bout adjacent output neuron input neuron observation consistent convolutional layer adjacent output neuron  dynamic sparsity crucial efficiency input neuron zero contribute output neuron ignore dynamic sparsity regard actual operation regard static sparsity multiplication addition input data multiplication addition input data additional dynamic sparsity comparison dense layer multiplication addition input data operation data static sparsity static dynamic sparsity respectively crucial leverage dynamic sparsity efficiency almost improvement dynamic sparsity output neuron index input neuron fourth computation balance output neuron input neuron output neuron multiplication addition computation avoid load imbalance hinder performance therefore principle accelerator  utilize efficiency accelerator compute exist non zero neuron synapsis maximize usage computation operator leverage index storage leverage dynamic sparsity efficiency improvement leverage load balance adjacent output neuron accelerator architecture detailed architecture accelerator efficiently address remain irregularity coarse grain prune sparse network overview depict propose accelerator architecture principle neuron selector module NSM component accelerator static sparsity index neural functional NFU multiple processing PEs compute output neuron parallel PE contains local synapse selector module ssm dynamic sparsity storage module consists neural buffer NBin NBout dma NBin NBout sib NSM PE PE NFU  IB CP accelerator architecture synapse index buffer sib module consists processor CP instruction buffer IB CP decodes various instruction IB efficiently detailed signal module define VLIW style instruction accelerator processing sparsity accelerator exploit static sparsity dynamic sparsity compression performance gain reduction accelerator sparsity NSM NFU NSM receives input neuron NBin synapse index sib filter neuron static sparsity index broadcast PEs NFU PE filter synapsis locally dynamic sparsity avoid useless computation data transfer index elaborate NSM NFU explain clearly index sparse data accelerator retain index coarsegrained prune employ efficient index particularly synaptic compact exist synapsis non prune correspond index connection input output neuron synapse index neuron dense network neuron neuron exist due prune relu activation function neuron input neuron usually storage synapsis sparse neural network processing accelerator neuron synapsis neuron dynamic sparsity synapse index static sparsity NSM NSM module static sparsity input neuron efficiently index input coarse grain sparsity central NSM multiple PEs exploit regularity elevate sparsity neuron zero NSM target neuron generate index later synapsis selection specifically neuron index compute firstly index indicates correspond neuron operation apply neuron index synapse index neuron flag neuron flag accumulate accumulate neuron flag operation apply target additionally target cmp mux neuron synapse index neuron index mux target neuron index neuron flag accumulate synapse index accumulate neuron flag neuron selector module NSM accumulate synapse index index synapsis NFU neuron index multiple output neuron broadcast PEs NFU NFU NFU operation neural network efficiently dynamic sparsity homogeneous processing PEs PE consists local synapse buffer SB decoder module wdm synapse selector module ssm neural function  PE load compress synapsis local SB synapsis independent output neuron hence separately PEs wdm lookup lut SB extract actual compress local quantization ssm index NSM synapsis wdm synapsis   consists multiplier adder nonlinear function module neural network onto PEs PE output neuron ideally cycle compute output neuron multiplication  multiplication cycle NFU assembles output PEs later computation ssm selects synapsis dynamic sparsity exists generality synaptic compactly regard static sparsity output neuron fourth synapsis sti sti synapsis computation ssm selects synapsis index NSM contains location synapsis enforce mux operation synapsis index finally synapsis PE output neuron synapsis implement ssm SB locally inside PE avoid bandwidth latency index differs  aspect accelerator contains index module NSM leverage improve regularity sparse synapsis PEs index synapsis due coarse grain prune module neuron NSM PEs reduce index module overhead vii bandwidth requirement NSM PEs accelerator local index module ssm inside PE leverage neuron sparsity efficiently regularity SB wdm ssm  neuron index PE output neuron synapsis architecture PE sparse synapsis improve coarse grain prune exists irregularity sparse neuron relu introduce zero output neuron synapse selector module minimize irregularity index module leverage neuron synapse sparsity IM exploit synapse sparsity wdm introduce accelerator exploit local quantization reduce storage data accelerator behavior split storage input buffer NBin output buffer NBout synapse index buffer sib synapse buffer SBs leverage overlap computation dma memory access implement buffer ping pong manner NBin NBout width neuron NSM efficiency PEs input neuron PEs input neuron avoid inefficiency stall computation neuron fed NSM NSM filter input PEs sparsity nevertheless exist network sparsity neuron synapse sparsity convolutional layer proportion execution efficiency affected SB PE width synaptic consideration dynamic sparsity SB compress prune encode via local quantization width synaptic varies across layer alexnet convolutional layer mlp fully layer alexnet fullyconnected layer leverage varied width avoid drastic hardware overhead decode wdm compress aliasing SB data respectively data load wdm decodes wdm width despite complex data address saving implementation wdm sib width index format indicates correspond synapse exists sib index STn STn STn STn mux STn STn synapsis synapsis neuron synapsis ssm  index NSM neuron correspond input neuron buffer decisive overall performance consumption although previous propose offering buffer synapsis neural network moderate avoid costly chip memory access incurs considerable delay penalty  emerge deeper neural network employ buffer along data replacement strategy scalability performance consumption explore buffer deploy KB KB KB KB NBin NBout SBs sib respectively neuron NBin synapsis SB reuse NBout memory access load neuron load synapsis neuron respectively CP accelerator execution decodes instruction inner IB efficiently execution coordination memory access data organization CP monitor module correspond register highly efficient VLIW style instruction generate library compiler VI experimental methodology introduce experimental methodology implement accelerator rtl synthesize route synopsys toolchain  library CACTI estimate consumption dram access evaluate performance primetime PX vcd waveform file obtain backend simulation typical baseline baseline cpu gpu hardware accelerator cpu caffe popular framework evaluate benchmark cpu core 1GHz technology denote cpu caffe adapt sparse neural network implement evaluate benchmark widely sparse library  cpu sparse gpu caffe evaluate benchmark gpu nvidia KM GB GDDR tflops peak technology gpu caffe furthermore natively cuBLAS implement benchmark comparison gpu cuBLAS sparse version implement csr index  library gpu  speedup accelerator cpu gpu diannao cambricon hardware accelerator accelerator network accelerator diannao cambricon diannao aim accelerate dnns cnns within footprint throughput cambricon accelerator aim exploit sparsity sparse neural network performance gain reduction  baseline accelerator achieves maximum utilization sparsity generality regard utilization sparsity averagely dense version  achieves speedup cnvlutin scnn respectively regard generality EIE capable FC layer implement diannao  multiplier adder TSMC technology assume accelerator plug memory model bandwidth GB benchmark representative neural network IV coarse grain sparse representation dense representation implementation entropy encode vii experimental RESULTS hardware characteristic implementation compatible prune layout characteristic accelerator VI accelerator KB SRAM achieves gop frequency 1GHz accelerator cambricon diannao respectively cambricon diannao VI hardware CHARACTERISTICS accelerator NBin NBout sib NSM CP NFU SB ssm wdm  speedup accelerator cpu gpu diannao cambricon convolutional layer speedup accelerator cpu gpu diannao cambricon fully layer additionally sparsity component improve performance efficiency cambricon additional module synapsis ssm dynamic neuron sparsity module index NSM ssm achieve reduction IM module cambricon NSM module achieve functionality IM module index neuron wdm extract local quantization performance performance accelerator cpu gpu diannao cambricon neural network IV sparse representation dense representation cpu gpu implement evaluate network dense library dense representation  gpu caffe gpu cuBLAS sparse library sparse representation cpu sparse gpu  comparison evaluate performance accelerator dense representation acc dense normalize performance accelerator sparse representation regard dense representation accelerator achieves speedup cpu caffe gpu caffe gpu cuBLAS respectively regard sparse representation accelerator achieves speedup cpu sparse gpu  respectively theart accelerator diannao cambricon accelerator achieves speedup performance accelerator accelerator efficiency accelerator gpu diannao cambricon efficiently sparse network dense network accelerator achieves speedup acc  aspect NSM fully exploit synapse sparsity average reduce mac operation achieve speedup SSMs exploit neuron sparsity average achieve speedup reduce synapsis synapse sparsity local quantization reduction synapse greatly reduce memory access synapsis obtain additional speedup explore performance efficiency performance comparison convolutional layer fullyconnected layer convolutional layer accelerator achieves speedup cpu sparse gpu  diannao respectively fully layer accelerator achieves speedup  gpu  diannao respectively notable accelerator achieves speedup cambricon convolutional layer fullyconnected layer respectively speedup convolutional layer benefit SSMs exploit dynamic neuron sparsity convolutional layer computation intensive dynamic neuron sparsity greatly reduce mac operation conv layer alexnet dynamic neuron sparsity greatly reduce execution speedup fully layer benefit reduce memory access synapsis local quantization index index fully layer memoryintensive wdm fully quantization reduces storage capacity synapsis obtain speedup index coarse grain prune reduce storage capacity index achieve additional speedup report  gpu diannao cambricon accelerator across neural network chip memory access accelerator achieves efficiency gpu diannao cambricon respectively moreover chip memory accelerator without local quantization reduction average reduce data amount local quantization regard accelerator without chip memory access accelerator achieves breakdown accelerator chip memory access breakdown accelerator without chip memory access efficiency respectively demonstrate efficiency accelerator breakdown accelerator evaluate network memory access consume lstm mlp network memory access consume neural network demonstrates network memory intensive informs sparsity quantization chip memory access drastically reduce lstm mlp moreover chip memory access reduce due sparsity reduction dense network breakdown accelerator without chip memory access chip memory access SB NBin NBout consume onchip memory access dominates chip consumption lstm mlp chip memory access consume chip synapsis reuse network convolutional network vgg resnet ratio chip memory access due complicate loop tile data reuse strategy convolutional layer notable chip SB reduces reduction synapse average cambricon discussion entropy cod investigate accelerator entropy cod implementation entropy decode module TSMC report throughput datum per cycle variable cod entropy cod decode module decode sequentially data data apply entropy decode parallel introduce tremendous accelerator SB data cycle PE entropy decode module avoid performance loss entropy decode module additional additional implementation accelerator respectively without entropy cod however performance gain convolutional layer performance gain fully layer limited additional entropy cod accelerator sparsity sensitivity investigate sparsity sensitivity accelerator neuron sparsity coarse grain synapse sparsity separately observation maximum speedup accelerator convolutional layer approach ideal speedup compatible neural network NSM neuron achieve speedup accelerator achieves nearly ideal speedup hiding dma memory access computation ping pong buffering notable accelerator outperforms cambricon synapse sparsity convolutional layer accelerator fully exploit data reuse load balance coarse grain sparsity accelerator easily achieve performance gain sparsity fully layer performance greatly improve decrease sparsity sparsity compress synapsis greatly reduce memory access greatly reduce execution fully layer dense synapsis sparsity accelerator achieves speedup exploit neuron sparsity approach ideal speedup neuron sparsity  convolutional layer ssm synapsis neuron sparsity speedup sufficient neuron sparsity performance gain fully layer memory access synapsis dominates execution neuron sparsity reduce memory access observation validate accelerator exploit neuron sparsity synapse sparsity neural network reduce irregularity reduce irregularity benefit accelerator aspect reduce irregularity simplify accelerator instead multiple distribute  NSM sufficient consumption similarly sib instead independent  reduces KB SRAM reduce irregularity reduces memory access synapse index greatly reduction synapse index achieve speedup efficiency input sensitivity investigate input sensitivity across datasets prune network partially speedup sparse version dense version NS neural sparsity SS synapse sparsity sensitive input datasets  prune vgg faster rcnn pascal vol dataset   however  layer faster rcnn recovers  without loss generality  layer dataset network suffer accuracy loss observation consistent resnet  faster rcnn  dataset vgg   LFW dataset coarse grain prune training procedure periodically scenario input sensitivity critical retrain data acceptable tune obtain optimal compression ratio explore parameter threshold quantization etc network exploration DSE tune training tune perform periodically moreover without loss generality local quantization sufficient convolutional layer reasonable tradeoff compression ratio accuracy coarse grain sparsity recent researcher coarse grain sparsity simd aware prune synapse vector elimination channel reduction filter reduction although technique benefit accelerator reduce irregularity usually notable accuracy loss explore structure sparsity grain sparsity vector sparsity kernel sparsity filter sparsity evaluate model regularity accuracy propose prune approach limit prune generalize version previous specify parameter coarse grain prune achieve sparsity despite knowledge accelerator accelerator leverage coarse grain sparsity vii performance comparison EIE microsecond layer alexnet vgg geomean EIE acc speedup accelerator EIE leverage sparsity fullyconnected layer neural network csc sparse representation scheme SRAM synapsis fully layer SRAM eliminate chip memory access fully layer alexnet EIE consumes accelerator instead fix quantization EIE implement wdm accelerator allows define width data representation nns accuracy comparison performance EIE assume accelerator synapsis fully layer focus computation vii accelerator average achieves speedup EIE fully layer scnn improves performance efficiency respectively dense accelerator accelerator achieves performance efficiency respectively dense version performance efficiency accelerator related neural network typically parameterized neuron synapsis severely hinder efficient NN processing effective algorithm developed tackle challenge utilizes sparse decomposition reduce redundancy computational complexity cnns propose  propose network architecture hash trick matrix reduce rank decomposition approach directly prune neuron specific criterion static neuron sparsity however usually noticeable accuracy synapse prune sparsity therefore achieve sparsity network trim achieves sparsity ratio vgg lenet data parameter prune achieves sparsity ratio alexnet lenet diversity network detailed sparsity evaluate network dataset  cifar optimal brain damage optimal brain surgeon prune network reduce connection static synapse sparsity hessian loss function compression applies prune static sparsity quantization huffman encode obtain compression ratio alexnet  prune synapsis frequency domain dct obtain compression ratio thereby cnns compression ratio compression fully exploit local convergence neural network obtain compression ratio ASIC implement accelerator neural network diannao accelerates various cnns dnns throughput dadiannao sufficient chip memory efficiently processing neural network shidiannao fully exploit neuron reuse synapse reuse convolutional layer completely eliminate chip memory access propose systolic architecture  architecture systolic coprocessor handle 2D convolution efficiently although aforementioned accelerator achieve throughput cannot exploit sparsity irregularity compress neural network cambricon exploit synapse sparsity IM transfer neuron fails exploit dynamic neuron sparsity cnvlutin aim exploit dynamic neuron sparsity cannot exploit synapse sparsity leverage static sparsity dynamic sparsity fail benefit EIE leverage neuron sparsity synapse sparsity aim fullyconnected layer recent accelerator scnn exploit synapse sparsity neuron sparsity limited improvement overall accelerator performance efficiency efficient IX CONCLUSIONS propose generalize coarse grain prune technique exploit local convergence neural network reduce irregularity drastically average coarse grain prune encode obtains compression ratio alexnet vgg hardware accelerator cambricon leverage benefit compression exploit dynamic neuron sparsity synapse sparsity feature accelerator NSM SSMs neuron synapsis benefit coarse grain prune neuron PEs reduce inner bandwidth footprint accelerator achieve peak performance  cambricon accelerator achieves performance efficiency respectively