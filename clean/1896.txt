finite machine FSM embarrassingly sequential depends input enumerative FSM data dependency input processing parallel unknown calculate transition enumeration software hardware implementation suffer drawback amount computation overhead enumeration optimization restrict correctly perform achieve limited improvement proposes cse  enumeration parallel FSM unlike prior approach cse novel computation primitive without specific mapping mapped mapped compute encode hardware implementation convergence ensures cse partition CS CS CSN convergence csi converge successfully compute enumeration csi otherwise execute stage outcome previous stage csi cse realize technique convergence prediction generates convergence random input profile maximizes probability csi converge global execution algorithm ensures correctness execute non converge stage input essentially cse  enumeration  singleton evaluate cse benchmark achieve average maximum speedup lookback enumeration lbe parallel automaton processor pap respectively introduction finite machine FSM important mathematical concept automaton theory FSMs widely computation model important application data analytics data mining bioinformatics network security computational finance software engineering application FSMs essential computational model amount input data therefore performance FSM crucial   foreach input   sequential FSM FSM deterministic finite automaton dfa nondeterministic finite automaton NFA considers dfa NFA convert equivalent dfa FSM input transition transition execution FSM input sequentially due transition dependency specifically FSM input transition transition besides sequential bottleneck transition lookup irregular memory access motivates recent hardware FSM accelerator parallelize sequential computation enumerative FSM propose input parallel unknown therefore computation calculate transition enumeration enumeration FSM challenge due computation fortunately FSM convergence enumeration compute non increase due convergence transition become foundation various hardware software implementation enumerative FSM review detail II essentially annual acm international symposium microarchitecture doi micro compute transition non increase enumeration hardware implementation enumerative FSM NFA typically encode vector active mask active encode combinational logic transition matrix selects input activate detail normal active mask restriction combinational logic hardware enables novel computation primitive another without specific mapping computes enumerative compute mapped importantly convergence enumerative FSM ensures unlikely FSM converge highly subset converge insight proposes cse  enumerative FSM building compute multiple enumeration parallel speculate converge speculation cse achieves significant speedup otherwise stage  concrete output previous stage increase likelihood speculation partition disjoint convergence csi csi csi  quality partition factor performance cse technique convergence prediction generates csi random input profile profile input convergence partition profile input partition maximum frequency MFP predict partition FSM improve prediction accuracy apply partition refinement algorithm merge distinct partition refine partition partition generate profile input prediction accuracy execution ratio input propose global execution algorithm hardware implementation minimally trigger execution stage ensure correctness essence cse  enumeration singleton evaluate cse automaton simulator  benchmark regex ANMLZoo cse lookback enumeration parallel automaton processor optimization implement directly sec cse achieve average maximum speedup lookback enumeration parallel automaton processor respectively remainder organize II review FSM enumerative FSMs introduces analyzes unique IV proposes cse focus convergence generation execution algorithm discus evaluation methodology VI evaluation vii discus related concludes II background  explains concept enumerative FSMs review recent approach improve efficiency enumerative FSMs outline drawback exist motivate approach FSM FSM deterministic finite automaton dfa nondeterministic finite automaton NFA NFA convert equivalent dfa focus dfa remainder FSM dfa interchangeably dfa tuple finite finite input alphabet transition function input another initial reporting accept sequence input dfa sequence computation sequential depends enumerative FSM sequential bottleneck challenge FSM computation enumerative FSM effective technique  sequential FSM data dependence input partition sequence parallel clearly concrete initial input unknown therefore perform enumerative execution compute transition enumeration execution generates concrete output others generate mapping stage chain output compute mapping illustrates enumerative FSM conceptual unknown compute transition curve enumeration sequence transition input finally eventual stage   HFXWLRQ    WV                                 WR   enumerative FSM enumeration computes mapping logical concept computes enumeration perform enumerative execution FSM indicates RT stage compute multiple enumerative component implement  enumerative FSM convergence implies non increase intuitively transition afterwards concept illustrate output infeasible marked empty dot another concept deem enumeration terminates data parallel FSM  enumerative FSM implementation simd instruction cpu enable parallel computation multiple performs convergence execution thereby reduce dynamically approach illustrate compute marked compute resource simd SMs gpgpu devote computation correspond shade principle RT faster execution accelerate processing guidance reduce computation enumeration RT besides convergence elimination deactivation another dynamic optimization reduce RT recent perform static optimization reduce discus terminology lookback enumeration     lookback execution lookback enumeration lbe illustrate lookback reduce initial previous input lbe phase lookback performs enumerative execution prefix compute enumeration predict predict actual previous speculation successful otherwise execute sequentially concrete without prediction lbe essentially enumerative FSM longer input recent probabilistic analysis profile reduce execution multiple worth exist probabilistic software implementation stochastic speculation scheme feasibility probability runtime computation extremely inefficient hardware update float parallel automaton processor parallel automaton processor pap NFA enumeration automaton processor AP pap compute hardware component multiplexing target NFA pap compute mapping encode multiple active difference NFA dfa monotonically decrease sequence decrease important effort pap reduce pap propose static optimization illustrate optimization input partition reduce input frequent feasible optimization component analysis feasible stage partition component CCs                             ZR     EH       parallel automaton processor optimization CCs merge destination disjoint allows compute transition parallel partition CCs reduction active optimization NFAs usually active due loop assign optimization reduce intuition boundary earlier instead drawback recent achieve speedup assumption compute enumeration pap although transition compute parallel II mapping memory computation correspond resume multiplexing amount computation enumeration challenge although pap statically parallelize reduce optimization ensure correctly compute achieve limited improvement explore enumerative FSM inspire  pap primitive unique opportunity enable encode architecture computation primitive discus insight definition application novel computation primitive efficient implementation enumerative FSM motivation memory centric automaton processor AP accelerates finite automaton processing implement NFA transition memory AP architecture accepts input performs transition encode multiple active       computation primitive transition NFA computation phase input transition generates active mask transition perform input active active mask encode active mask independently cycle transition matrix active mask input ste ste ste ste automaton processor architecture implement NFA dfa NFA multiple active mask multiple transition perform parallel dfa active mask transition perform scenario multiple active mask clearly hardware implementation however longer compute transition individual dfa input transition active mask update active mask information transition specific mapping transition define computation primitive transition another concept curve enumeration sequence dot curve enumeration however cannot sequence mapped active transition information mapped lose useful application enumerative FSM application compute lookback lbe refer exist lbe compute enumeration previous     cse approach scenario perfectly goal directly reduce overhead compute enumeration importantly compute enumerative converge enumerative compute parallel moreover achieve hardware convergence enumerative FSM ensures unlikely FSM converge highly subset converge insight IV cse approach cse convergence enumeration explain insight focus convergence generation execution algorithm insight cse building construct enumerative FSM increase probability converge partition convergence CS CS speculate CS converges computes CS enumeration parallel otherwise stage execute illustrates cse approach partition convergence CS CS CS directly reduce CS converges successfully RT mention earlier RT faster execution directly reduce bound RT achieve optimization pap conservative partition instead partition CSs partition CSs CS converges successfully CS CS converge RT reduce bound significantly scenario CS converge successfully compute mapping CS illustrates incorrect partition suggests  execute concrete previous stage execution avoid concrete output previous stage belongs CS CS execution unnecessary detail execution algorithm described IV discussion execute stage cse stage composite stage distinguish false enumerative execution stage pap simply selects concrete previous stage computation available cse similarly convergence granularity correspond CS concrete CS converges concrete output report CS diverges perform execution concrete therefore cse exactly output report perform sequential execution however terminal transition individual cse distinguish transition within convergence recover information another sequential execution service FSMs network intrusion detection compute terminal latency sensitive transition cse accelerate FSM processing latency critical task convergence prediction quality convergence generate factor performance convergence successful execution faster enumerate however likely trigger execution incurs extra latency conservative generate convergence perform redundant computation enumerative FSMs transition propose profile prediction partition refinement algorithm merge partition carefully merge strategy convergence prediction concise converge accurate execution rate partition FSM application profile profile intuitive synthetic input sequence discover convergence behavior profile input randomly generate characteristic input input application split independent describes input benchmark evaluation obtain execution CS arrives reporting accept CS diverge execution distinguish report maximum frequency partition MFP FSM specification FSM benchmark accept visible ASCII code profile sample subset ASCII generate profile input emulate execution input convergence partition frequency appearance distinct partition frequent partition likely input converge convergence partition evaluation profile input profile FSM benchmark PC profile input profile frequency distribution unnoticeable across benchmark although profile randomly generate profile consistent across benchmark convergence partition FSM simply maximum frequency partition MFP minimize probability execution convergence partition merge partition generate profile MFP achieve sufficiently frequency frequency MFP profile frequency MFP clamav benchmark  accurate convergence generate diverge around execution frequent execution performance degradation merge instead simply MFP accuracy propose refine partition merge multiple partition partition refine subset classic algorithm partition refinement partition input output merge partition refinement commutative operation affect output belonging convergence convergence contains subset contains subset intersection difference split return partition refinement algorithm convergence implies input converge converge frequency sum partition essence increase convergence benefit increase frequency partition profile merge partition subset merge algorithm merges partition merge partition profile objective refine partition frequency without increase subset significantly simplicity propose effective heuristic merge strategy merge partition another compatible merge increase subset merge partition frequency merge partition frequency explore frequency merge partition profile benchmark merge increase subset drastically however  MFP generate partition subset prohibitive subset frequency evaluate performance MFP detailed discussion merge strategy VI correctness guarantee execution formalization uniformly specify transition define ST convergence CS assume convergence execute parallel correspond input transition function ST ST define CS CS converges CS otherwise transition function convergence addition transition function depends input CS converge output convergence definition converge CS otherwise concrete trivially modify function treatment performs normal execution essentially compute composition function outcome execution otherwise execute identify execution organize procedure discus function composition compose function compose transition function consecutive FSM linear structure input input output CS definition CS CS output union convergence transition function input output multiple convergence input convert convergence union apply specifically convergence CS belongs CS CS mixed convergence procedure already treat convergence execution algorithm earlier output execution easy satisfied previous converge formalization naturally capture behavior transition function however identify execution approach execute concrete output generate concrete input obviously output previous execute concrete output approach execution sequentially perform execution guarantee output concrete concrete optimization improve approach realize input concrete output perform execution concrete output chain multiple concrete  logic  logic  logic  logic  logic conv conv conv conv conv execution output exec CS transition CS transition vector cse hardware implementation input dynamically execution ensure output concrete perform backward chain concrete execution perform sequentially concrete optimization opportunistic transition function evaluation execute instead directly execute successor sequentially evaluate transition function sequence input function already compute parallel evaluate function faster actual execution depends input benefit function evaluation concrete output execution situation faster execute multiple converge function evaluation identify concrete later essentially skip execution hardware implementation advanced hardware implementation hardware implementation execution algorithm opportunistic transition function evaluation input buffer input execution input buffer execution module determines execute timing execution cycle predictable cycle operation perform easily therefore logic complex interface  module transition essentially specifies transition function convergence compose CS transition vector corresponds convergence vector FSM vector simply active mask convergence input logic directly vector CSs operation generate CSs implement logic module logic module generates output logic module output signal conv converges therefore chain logic module output output global signal execution module conv signal concrete backward conv signal execution warrant conv execution signal vector execute backward evaluation perform logic module execute successor logic chain sequence shorter instead execution logic evaluation methodology evaluate enumerative FSM across FSM benchmark ANMLZoo regex suite benchmark multiple FSMs input sequence environment setup detail validate pap FSM benchmark regex benchmark consists synthetic regular expression  simplest dotstar contains regular expression wildcard rulesets randomly average frequency per regular expression tcp regular expression filter network packet header packet inspection ANMLZoo diverse benchmark automaton processing brill brill tag update brill tag application clamav source repository virus signature intend email scan mail gateway subset signature database dotstar synthetic automaton generate probability regular expression rulesets  synthetic regular expression benchmark suite developed ibm snort opensource network intrusion detection NIDS software monitor network package analyzes active mask convergence restore context switch pap BENCHMARKS benchmark FSM core per MFP dotstar dotstar dotstar  tcp  dotstar  snort clamav brill define community user  convert protein  regular expression regex ANMLZoo representation FSMs NFA regular expression pap focus NFA regular expression convert DFAs opensource regular expression library compile dfa however regex ANMLZoo benchmark ANMLZoo dfa NFA representation evaluate characteristic convert FSMs FSM input regex benchmark generate input trace generator  probability activates subsequent ANMLZoo benchmark input trace application pap input file input however application ANMLZoo input easily split delimiter input provably dependency brill across boundary split input file snort input file contains packet processing independent parallel evaluation input file pap split accord performance average input profile convergence dataset mention random input environment setup  automaton simulator utilized pap implement pap optimization described II  widely source library NFA emulation capability multi thread therefore partition input simultaneously input execute context switch utilize optimization mention pap input II parallel enumerative   FSM FSM static optimization dynamic optimization baseline FSM NA NA lbe FSM NA lookback pap FSM optimization convergence cse FSM convergence prediction deactivation partition deactivation dynamic convergence consideration overhead decode varies benchmark implement baseline sequential FSM enumerative FSMs comparison II hardware building static dynamic optimization apply lbe FSM perform lookback lbe lbe without prediction probabilistic suitable hardware implementation assume enumerative FSM micron automaton processor AP AP core parallelization AP generation rank rank contains core rank core benchmark assign core accord pap benchmark densely AP compiler FSMs multiple AP core resource constraint lbe cse core assign estimate sequential FSM AP per cycle context switch cycle assume cycle dynamic convergence summary adopt evaluation methodology pap split input differently accord verify implement static dynamic optimization correctly RT report pap VI experimental RESULTS speedup enumerative FSM analyze source speedup enumeration overhead RT explain explore parameter lbe cse achieve performance finally investigate predictive profile convergence prediction execution rate partition coverage frequency performance demonstrates propose cse performance baseline lbe pap ideal speedup baseline throughput per cycle ideal throughput per cycle speedup baseline lbe cse parameter lookback merge strategy lbe pap cse baseline ideal speedup propose cse lbe pap application average cse speedup lbe pap cse achieves ideal speedup application  pap speedup  cse outperforms pap margin average however speedup pap consistent snort clamav pap speedup respectively ideal speedup pap faster baseline consistent speedup respectively initial application evaluate recall intuitive indication initial enumeration overhead cse application  reduce dynamically within cycle becomes RT execution enumeration overhead multiplexing doubt application ideal performance  RT become stable enumeration overhead cse  ideal speedup mention pap performs application static optimization pap reduce application  pap enumeration overhead application lbe pap cse intricate cse outperforms pap application input partition optimization pap II input feasible clever technique greatly reduces however boundary input evenly longer cycle execute execution cycle cse rely technique reduce account marginal performance benefit unfortunately application pap lbe cse snort clamav pap initial respectively lbe cse pap author argue rely dynamic optimization reduce active quickly observation input however evaluation methodology input file split independent parallel dependent input sequence rarely exceeds spent initial enumeration become overhead impact overall performance capability pap dynamic convergence limited static optimization perform discus RT demonstrates RT due dynamic optimization convergence deactivation RT RT enumeration computation RT hint FSM performance lbe pap cse RT cse RT becomes around application almost enumeration overhead computation lbe RT average active lbe cycle execute RT lbe confirms lbe performance ideal throughput RT cse lbe weak convergence IV however pap application RT RT tcp dotstar  clamav snort active RT clearly pap extremely dynamic optimization effectively enumeration overhead reveal inefficient dynamic optimization component analysis optimization pap optimization merge component intuitively appeal reduces II detail however merge becomes merge dynamically component component component component pap merge static optimization decrease dynamic convergence merge converge converge component merge converge lbe lookback previous overall lbe performance limited RT around ideal however tune lookback parameter performance lbe explore lookback simplicity speedup configuration lbe speedup lookback application brill lookback extremely beneficial speedup clamav lookback perform sequential baseline introduce enumeration overhead benchmark lookback prefix longer diminish benefit cannot reduce lookback cycle become  overhead cse convergence generation IV convergence generation essential cse performance profile alone MFP frequency partition merge evaluate effectiveness merge strategy convergence MFP cse determines initial enumeration overhead almost benchmark  merge increase significantly  clamav merge partition profile increase possibly incur considerable overhead partition merge average merge increase respectively merge strategy application average overhead introduce merge handle dynamic optimization cycle meanwhile partition prediction becomes reliable avoids  situation speedup respect merge strategy  clamav brill merge desirable partition merge technique prof effective across benchmark boost performance baseline average predictive execution apart RT execution rate another important factor cse performance quality predict convergence merge MFP frequency partition execute possibility random input baseline cse merge strategy baseline cse speedup merge strategy convergence predict convergence behavior input execution rate proxy illustrates execution rate application evaluate representative input MFP generate profile suffers execution rate tcp  brill respectively merge optimization execution significantly execution rate deviate expectation average execution rate unnoticeable affect cse performance convergence generate profile random input indeed predict convergence behavior input vii related cse parallelize FSM convergence enumeration review related FSM acceleration software hardware automaton processor FSM software parallelization FSM parallelization classical propose dfa parallelization parallel prefix sum optimize algorithm reduce complexity  processor baseline cse execution rate merge strategy inspire recent  performs enumeration multicore architecture simd prefix sum leverage convergence reduce computation overhead refer enumerative FSM summarize II proposes speculation concept principled speculation speculate introduce hardware lbe leverage encode hardware feature AP cse proposes enumeration software parallelization literature FSM hardware accelerator hawk hardware accelerator hoc query memory hare target regular expression inmemory optimizes memory bandwidth unified automaton processor  architecture efficient automaton processing unstructured data processor udp processor data transformation automaton processing cache automation proposes novel cache automaton  acceleration pap distinguish hardware accelerator data parallelization dependency automaton processor automaton processor architecture emerge research topic application algorithm AP particle physic bioinformatics recognition machine processing related pap NFA parallelization AP although evaluation AP simulation convergence enumeration specific AP architecture applies hardware accelerator simd implementation conclusion proposes cse  enumerative FSM unlike prior approach cse computation primitive without specific mapping mapped cse essential building construct enumerative FSM essentially cse  enumeration singleton evaluate cse benchmark achieve average maximum speedup lookback enumeration lbe parallel automaton processor pap respectively