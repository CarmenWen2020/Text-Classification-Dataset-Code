traditional von neumann GPGPUs thread communicate memory basis model producer thread writes intermediate memory consumer thread barrier synchronization alleviate memory bandwidth impose communication GPGPUs scratchpad memory prevents intermediate overload dram bandwidth introduce inter thread communication massively multithreaded CGRAs intermediate communicate directly compute fabric  basis avoids memory eliminates dedicate scratchpad avoids workgroup global barrier introduce propose extension program model cuda execution model hardware primitive facilitate communication simulation rodinia benchmark inter thread communication average speedup max reduces average max equivalent nvidia gpgpu index cgra dataflow gpgpu simd mpi reconfigurable architecture non von neumann architecture  communication introduction conventional von neumann GPGPUs employ data parallel instruction multiple thread SIMT model pure data parallelism majority data parallel workload inter thread communication gpgpu program model cuda OpenCL thread cooperative thread array CTAs workgroups enable thread cta communicate memory model limitation communication mediate memory typically implement hardware scratchpad communication bandwidth costly limitation synchronization model schedule thread within cta unknown synchronization barrier invoked consumer thread memory respective producer thread alternative von neumann GPGPUs   recently introduce coarse grain  research israel foundation  gpgpu architecture cgra couple dataflow execution model propose architecture refer massively multithreaded cgra MT cgra compute graph cuda kernel cgra dynamic dataflow execution model multiple cuda thread connectivity cgra fabric leveraged eliminate multiple von neumann bottleneck register file instruction nevertheless MT cgra model bound memory synchronization barrier  communication incurs performance overhead dmt cgra extension MT cgra inter thread communication cgra fabric extend program model execution model underlie hardware architecture  memory scratchpad global synchronization operation dmt cgra extends cuda primitive enable programmer express inter thread dependency primitive programmer thread generate thread arbitrary thread index scalar compiler program primitive temporal link kernel dataflow graph temporal link express dependency concurrently execute instance dataflow graph thread finally functional redirect dataflow token graph instance thread dataflow preserve architecture improves performance consumption leverage distinct reduce memory bandwidth replace memory cgra token rout reduces memory bandwidth associate consumption inter thread data reuse lightweight inter thread communication allows thread data load memory cgra rout alleviates memory traffic software manage cache code compaction replace memory inter thread communication eliminates code associate address computation boundary reduces dynamic operation annual acm international symposium microarchitecture doi micro overall dmt cgra outperforms nvidia GPGPUs average consume remainder organize II describes motivation inter thread communication MT cgra explains rationale propose dmt cgra execution model propose program model extension IV dmt cgra architecture evaluation discus related VI vii concludes II inter thread communication multithreaded cgra massively multithreaded processor namely GPGPUs employ von neumann processing deliver massive concurrency memory scratchpad primary inter thread communication imposes limitation frequency inter thread communication  memory intermediate bandwidth dedicate scratchpad dramatically increase consumption inherently asynchronous memory decouples communication synchronization programmer explicit synchronization primitive barrier impede concurrency thread others synchronization flexible inter thread communication dataflow compute model synchronizes computation couple communication intermediate functional dataflow propose MT cgra dmt cgra architecture extends massively multithreaded cgra MT cgra employ dataflow model  communication whenever instruction thread sends data token instruction thread latter execute data token thread dataflow ensures thread thread dmt cgra model avoids costly communication memory scratchpad exist internal buffer cgra utilized medium inter thread communication communicate propagate directly destination token cannot buffer fabric spill memory moreover couple communication synchronization  extension SIMT program model  implicitly synchronizes data delivery without costly barrier remainder argues couple communication synchronization discus typical program satisfied internal cgra buffering dataflow message passing demonstrate dmt cgra message passing extension separable convolution thread code thread tid margin margin tid margin tid tid  tid kernel  tid kernel  tid kernel eft margin margin tid tid  tid kernel  tid kernel margin margin tid tid  tid kernel  tid kernel spatial convolution global memory thread code 1D cuda style tid threadIdx load image memory  tid  tid pad margin zero margin tid pad margin  tid thread load phase barrier cuda uti reload  tid  tid kernel  tid kernel  tid kernel spatial convolution gpgpu memory thread code 1D cuda style tid threadIdx load global memory mem elem  tid tag variable variable    mem elem token elem fromThreadOrConst mem elem tid elem fromThreadOrConst mem elem tid execute convolution tid elem kernel mem elem kernel kernel spatial convolution MT cgra thread cooperation implementation separable convolution various inter thread data model brevity focus 1D convolution iterative component algorithm nvidia software development kit sdk convolution applies kernel image apply 1D convolution image dimension brevity focus discussion 1D convolution kernel depict pseudo code separable convolution implement global memory memory message passing program model trivial parallel implementation global memory entire kernel within image margin matrix simply correspond convolution kernel thread tid tid tid outside margin zero although naive implementation easy code multiple memory access image translate consumption performance GPGPUs memory overcome matrix load memory  array code image margin pad zero barrier synchronization ensure thread load barrier actual convolution compute nevertheless although computation phase access global memory lack inter thread communication redundant access pre load memory image kernel load multiple thread dataflow architecture seamlessly incorporate message passing framework inter thread communication demonstrates separable convolution implement dmt cgra message passing primitive allows thread request thread variable underlie instruction  SIMT model thread homogeneous execute code diverge thread load matrix register oppose memory load thread thread programmer tag version variable variable rewrite available remote thread  thread remote fromThreadOrConst api argument remote variable thread ID default thread ID invalid negative thread ID cuda OpenCL thread IDs mapped multi dimensional coordinate threadIdx cuda thread IDs encode constant delta source thread ID execute thread ID communication therefore translate compiler code dataflow graph dependency instance graph thread data transfer underlie dataflow facilitate compile translation argument template parameter strength model implicit embed communication dataflow graph directly thread dmt cgra processor eliminate memory mediation addition embed allows thread computation phase respective independently thread barrier implicit dataflow synchronization impede parallelism IDs thread mapped 2D cuda thread code 2D cuda style threadIdx threadIdx load memory   thread load phase barrier cuda compute  dot     dot global memory  matrix multiplication gpgpu memory thread code mapping 2D cuda style threadIdx threadIdx compute memory access predicate compute dot loop statically unrolled compute  compile pragma unroll fromThreadOrMem fromThreadOrMem dense matrix multiplication dmt cgra architecture inter thread communication multiplication dense matrix memory gpgpu inter thread communication MT cgra matrix dimension memory thread multiple concurrent thread load multiple address stress memory redundant load synergy cgra compute fabric  communication eliminates enable dmt cgra load memory cgra fabric illustrates matrix multiplication depicts implementation dense matrix multiplication gpgpu dmt cgra implementation thread computes matrix demonstrates classic gpgpu implementation stress memory implementation concurrently data global memory memory executes synchronization barrier impedes parallelism thread computes matrix consequently source matrix dimension respectively access thread compute target coordinate correspond data dmt cgra  multiplication physical cgra configure dataflow graph layer functional cgra multiplex operation instance thread graph load thread introduce memory thread communication primitive eliminate redundant memory access primitive compile predicate determines load memory load another thread dmt cgra toolchain operation cgra described IV predicate configures cgra route depicts implementation dense matrix multiplication propose primitive thread computes destination matrix program model thread spatial coordinate cuda OpenCL regular memory access code fromThreadOrMem primitive argument predicate determines memory address load primitive parameter dimensional coordinate indicates thread data obtain coordinate encode multi dimensional difference source thread execute thread coordinate finally illustrates data thread matrix multiplication dataflow graph thread remind reader underlie dmt cgra configure dataflow graph executes multiple thread token graph dynamic dataflow token thread computes target matrix thread compute load matrix memory thread compute load matrix thread load memory thread thread load matrix thread sends thread combination multithreaded cgra  communication greatly alleviates load memory plague massively parallel processor elaborate program model dmt cgra execution model underlie architecture execution programming model describes dmt cgra execution model program model extension data movement thread MT cgra execution model MT cgra execution model combine static dynamic dataflow model execute instruction multiple thread SIMT program performance characteristic von neumann GPGPUs model convert SIMT kernel dataflow graph cgra fabric functional multiplex operation token instance dataflow graph thread MT cgra core comprises host interconnect functional arithmetic logical float load architecture described IV interconnect configure program dataflow graph statically token functional execution instruction graph instance thread static dataflow model addition functional cgra employ dynamic tag token dataflow dynamically schedule thread instruction prevents memory stall thread thread thereby maximize utilization functional prior execute kernel functional interconnect configure execute dataflow graph consists replica kernel dataflow graph replicate kernel dataflow graph allows utilization MT cgra grid configuration lightweight negligible impact performance configure thread dataflow core inject thread identifier cuda OpenCL coordinate threadIdx cuda array deliver operand successor functional initiate thread computation dataflow thread inject computational fabric cycle inter thread communication MT cgra described MT cgra execution model return tag token tid token tag elevator node tid source tid within boundary tid valid source tid token token token tid return token tid invalid source tid constant return tid functionality elevator node tid shift fallback constant dynamic tag token dataflow token couple tag multithreaded model  token tag allows functional thread input token crux inter thread communication reduce token tag tid implement token tag elevator node cgra elevator shift elevator node shift token  elevator node input output node configure parameter ΔTID constant functionality node described pseudocode effectively implementation fromThreadOrConst function described downstream tid node generates tag token consist obtain input token tid tid valid tid thread downstream token consists preconfigured constant elevator node communicates token thread  extract compile fromThreadOrConst fromThreadOrMem function inter thread communication function mapped compiler elevator node dataflow graph counterpart cgra elevator node token buffer buffer serf entry output queue target tid ΔTID elevator node limited token buffer ΔT IDs node token buffer elevator node cascade chain whenever compiler identifies ΔTID elevator node token buffer inter thread communication operation sequence cascade elevator node extreme ΔTID multiple cascade node dmt cgra spill communicate memory cascade elevator node IV nonetheless experimental inter thread communication exhibit locality across tid typically communicate thread adjacent tid euclidean distance 2D 3D tid cumulative distribution probability transmission distance cumulative distribution function cdf delta across various benchmark code evaluate communicates across ΔTID communication locality function cdf ΔT IDs exhibit benchmark benchmark methodology described commonly delta token buffer benchmark cascade elevator node however transmission distance gpu shuffle permute operation data execution lane lane gpu SM thread without producer functional inter thread communication enhance load eLSU eLSU extends regular lsu predicate bypass return memory another thread elevator eLSU couple elevator multiple thereof implement fromThreadOrMem primitive program model extension enable inter thread communication extend cuda OpenCL api api allows thread communicate thread thread component api communicate intermediate fromThreadOrConst  function enable thread communicate intermediate  manner function mapped elevator node tag token downstream sender thread token behavior consumer thread producer thread sends token fromThreadOrConst function variant variant template parameter variable thread ΔTID communicate thread multi dimensional constant tid invalid outside transmission transmission define span  communication variant fromThreadOrConst function allows programmer bound template parameter define transmission fromThreadOrConst function encodes monotonic communication thread thread tid thread tid function description token fromThreadOrConst variable  constant variable another thread constant thread exist token fromThreadOrConst variable  constant limit communication thread void  variable tag variable another thread token fromThreadOrMem  address predicate load address predicate another thread token fromThreadOrMem  address predicate limit communication thread api inter thread communication static constant template parameter function ΔTID version 1D 2D 3D tid static dmt cgra mapping execute prefix sum scan thread code mapping thread 1D cuda style tid threadIdx LD global memory mem val  tid load sum sum fromThreadOrConst sum mem val  sum rti sum memory  tid sum prefix sum implementation inter thread communication thread ID thread ID thread ID dynamic execution prefix sum  function thread tid forth transmission define maximum difference  participate communication thread partition consecutive thread thread tid   tid communication tid tid confine thread  thread  default constant thread  bound transmission useful thread sub benchmark useful compute reduction bound transmission enables mapping distinct communicate thread  function tag specific version variable fromThreadOrConst  fromThreadOrConst prefix sum depict nvidia cuda sdk prefix sum array array sum array code  compute prefix sum depends previous thread sends subsequent thread illustrates per thread dataflow graph illustrates inter thread communication across multiple thread graph instance demonstrates decouple  fromThreadOrConst allows compiler schedule instruction parallel inter thread communication thereby expose instruction parallelism ILP memory fromThreadOrMem function allows thread load memory address load operation function ΔTID template parameter address predicate evaluate parameter function variant allows programmer bound transmission predicate function dynamically thread issue actual load instruction thread piggyback load typical fromThreadOrMem function matrix multiplication function allows thread load matrix remain thread load thread memory functionality reduces memory access program complexity parallel program trivial propose model exception nevertheless model  microarchitectural constraint programmer understand parallel algorithm contrast inter thread communication contemporary GPGPUs programmer memory microarchitecture alongside parallel algorithm quantitative comparison model research trajectory however focus model performance benefit qualitative comparison program model scope CU  CU CU   compute compute load load            CU    split  MT cgra core tid  crossbar switch typical typical  MT cgra core overview IV dmt cgra architecture describes dmt cgra architecture focus extension baseline MT cgra facilitate inter thread communication illustrates structure MT cgra architecture MT cgra core grid functional interconnect statically rout network chip noc core configuration mapping instruction functional noc rout compile MT cgra kernel load execution token various functional accord static mapping noc grid compose heterogeneous functional instruction mapped manner memory operation mapped load computational operation mapped float ALUs compute operation bitwise operation comparison mapped CU split operation preserve intra thread memory mapped split  execution parallel task MT cgra core thread reside grid simultaneously information tag token compose data associate tid serf tag tag grid node operand belong thread illustrates structure functionality tag token logic thread interleave dynamic dataflow specifically tag token noc insert token buffer operand specific  available logic access memory  compute alu FPU logic completes operation tag token grid crossbar switch   XN            nsi    elevator node flight token token buffer controller manipulates  output token introduce grid elevator node enhance load eLSU exist manipulate token modify tag preserve association token thread facilitate inter thread communication modify tag exist token depict elevator node eLSU node respectively introduce node grid convert exist node elevator node   conversion combinatorial logic exist grid already internal opcode register token buffer logic elevation operation comprises register delta constant multiplexer  logic overall overhead overhead consumption dramatically reduce comparison MT cgra token via noc instead rerouted token buffer spent noc elevator node elevator node depict implement fromThreadOrConst function communicates intermediate thread mapping fromThreadOrConst node configure ΔTID default constant elevator node receives token tag tid tag tid accord preconfigured sends tag token downstream node receives input token thread sends  token another thread thread data producer token consumer thread consumer token another producer thread alternatively thread tid producer target thread ID tid invalid outside transmission    XN      nsi      IIW   eLSU node consist lsu additional adder manipulate tid comparator outside margin input predicate determines introduce output loop token  data load previous thread correspondingly thread ID tid outside transmission elevator node injects preconfigured constant tag token downstream thread consume token controller input token receiver modify tag tid tid tag token tid entry token buffer addition input tid acknowledge thread token buffer alternatively thread tid simply predefined constant token controller tag token comprise constant tid token buffer acknowledge extra token buffer ability enhance load eLSU eLSU implement fromThreadOrMem function enables thread reuse memory load another thread without issue redundant memory access eLSU lsu enhance logic determines token memory another thread slot token buffer eLSU operates enable input thread access memory load data otherwise thread tid token buffer another thread token controller token data fetch memory token buffer latter scenario thread dataflow graph eLSU output token token duplicate internally parse node logic token downstream MT cgra tid duplicate token tid transmission      cascade elevator node manage ΔTID token buffer LD JS   fromThreadOrMem procedure token buffer function mapped cascade predicate elevator node cycle cascade elevator node tag token token buffer otherwise duplicate token discard consumer outside transmission scheme load memory reuse  significantly reduce memory bandwidth transmission distance dmt cgra architecture token buffer elevator eLSU node implement inter thread communication compilation compiler examines distance thread thread ΔTID fromThreadOrConst fromThreadOrMem function distance token buffer fromThreadOrConst fromThreadOrMem mapped elevator node  respectively ΔTID token buffer compiler cascade multiple node transmission distance distance fromThreadOrConst rare instance fromThreadOrConst function communicate fromThreadOrConst function communicate transmission distance token buffer compiler cascade multiple elevator node effectively chain token buffer communication distance scenario transmission distance token buffer entry compiler handle longer distance mapping operation cascade elevator node compiler configures ΔTID node token buffer desire cumulative transmission distance parameter dmt cgra core interconnect compute  computational ALUs FPUs compute load  split elevator frequency ghz core interconnect dram KB KB GDDR dram channel II dmt cgra configuration transmission token buffer cascade   buf extreme ΔTID elevator node available cgra communicate spill cache compiler manage cache  architecture approach spill technique GPGPUs distance fromThreadOrMem procedure default fromThreadOrMem mapped eLSU node unlike elevator node eLSU cannot simply cascade local buffer flight memory access matrix load thread transmit distance thread ΔTID thread load data eLSU load transmit later token buffer entry token buffer external buffering additional external buffer construct mapping operation loop cascade elevator node depict loop enclose node  reuse memory thread output terminate mux input mux scenario compiler load instruction predicate load predicate fromThreadOrMem selector  predicate evaluates false memory loop mux elevator node cascade originate tid cascade  target thread ID tid sum elevator node ΔTID therefore account communication distance nevertheless typical ΔTID inside eLSU token buffer evaluation evaluation dmt cgra architecture discus impact dmt cgra memory bandwidth code complexity implication overall performance efficiency methodology simulation framework gpgpu sim simulator  model performance monitor estimate execution application description memory reuse mem BW code scan prefix sum  matrix multiplication conv convolution filter reduce parallel reduction lud matrix decomposition srad speckle reduce anisotropic diffusion BPNN neural network training hotspot thermal simulation pathfinder shortest grid benchmark benefit dmt cgra architecture memory reuse kernel inter thread communication eliminate redundant memory access mem BW kernel benefit inter thread communication indirect memory communication insts kernel code simplify execute instruction inter thread communication primitive evaluate performance dmt cgra MT cgra gpu architecture model nvidia gtx nvidia fermi extend gpgpu sim simulate MT cgra core dmt cgra core per operation estimate obtain rtl route component extend model  MT cgra dmt cgra baseline dmt cgra nvidia fermi  architecture MT cgra architecture without inter thread communication although fermi nvidia architecture validate model configuration II replace fermi SM dmt cgra core retain non core component consistency amount logic comprise dmt cgra core amount nvidia SM   MT cgra core amount SRAM MT cgra dmt cgra nvidia SM gpu RF replace memory structure consume SRAM KB token buffer KB  KB  compiler compile cuda kernel llvm extract ssa code configure dmt cgra grid interconnect benchmark benchmark rodinia benchmark memory dynamic delta scheme propose beneficial approximately benchmark evaluate dmt cgra diverse kernel characteristic rodinia benchmark suite nvidia sdk kernel highlight benchmark benefit dmt cgra due memory reuse memory bandwidth execute memory traffic global scan matmul conv reduce lud srad BPNN hotspot pathfinder geomean gpu MT cgra dmt cgra gpu MT cgra dmt cgra gpu MT cgra dmt cgra gpu MT cgra dmt cgra gpu MT cgra dmt cgra gpu MT cgra dmt cgra gpu MT cgra dmt cgra gpu MT cgra dmt cgra gpu MT cgra dmt cgra gpu MT cgra dmt cgra memory global normalize communicate global memory fermi baseline scan matmul conv reduce lud srad BPNN hotspot pathfinder geomean execute ops MT cgra dmt cgra comparison dynamic operation execute dmt cgra baseline architecture code evaluate kernel memory express static delta reduce memory bandwidth depicts amount data global memory memory serf bandwidth nvidia GPGPUs furthermore  memory address mapped global memory data bandwidth contrast dmt cgra program model extension confine almost data transfer inside execution fabric dramatically reduce costly transfer data global memory demonstrates dmt cgra dramatically reduce memory bandwidth eliminate memory intermediary inter thread communication scan convolution addition inter thread communication allows thread reuse memory already load others  lud discus impact reduce memory bandwidth overall performance consumption later reduce operation inter thread communication eliminates redundant address calculation memory communication redundant boundary memory reduction operation dynamic operation execute architecture fermi baseline benchmark dramatically reduce pronounce scan inter thread communication eliminates substantial code performs data reduction contrast  benefit memory scan matmul conv reduce lud srad BPNN hotspot pathfinder geomean speedup MT cgra dmt cgra speedup obtain dmt cgra baseline architecture reuse reduction negligible simply replaces memory load instruction communication primitive overall dmt cgra reduces execute operation average fermi gpgpu performance analysis performance processor depends instruction executes utilization functional dmt cgra reduces dynamic operation execute benchmark furthermore memory reuse across thread  reduces memory latency affect utilization finally spatial architecture bound instruction fetch width register file bandwidth thereby functional grid spatial architecture compose functional theoretically deliver ipc equivalent gpgpu demonstrates performance speedup obtain dmt cgra baseline architecture performance correlate  operation addition benchmark exhibit memory reuse  benefit increase performance reduction global memory bandwidth conversely benchmark exhibit ILP tlp scan performance benefit despite reduction operation overall dmt cgra outperforms nvidia fermi average heterogeneous composition MT cgra functional  utilization workload instruction pathfinder kernel utilizes available alu perform float computation FP idle although fix composition functional nvidia fermi comparison customize functional composition dmt cgra potentially deliver performance gain efficiency analysis efficiency evaluate architecture architecture instruction architecture ISAs define efficiency execute benchmark scan matmul conv reduce lud srad BPNN hotspot pathfinder geomean efficiency MT cgra dmt cgra efficiency dmt cgra core  MT cgra core fermi SM gpu MT cgra dmt cgra reduce lud srad BPNN hotspot pathfinder average breakdown others fds dram comp noc RF  others fds dram gpu MT cgra dmt cgra gpu MT cgra dmt cgra gpu MT cgra dmt cgra gpu MT cgra dmt cgra gpu MT cgra dmt cgra gpu MT cgra dmt cgra consumption across core component overall efficiency dmt cgra MT cgra fermi demonstrates efficiency dmt cgra average efficient fermi MT cgra reduction obtain scan kernel implementation inter thread communication benchmark benefit performance improvement due ILP tlp efficiency improve almost thanks reduction memory access overhead operation attribute elimination complex reduction algorithm examine average consumption core component breakdown benchmark architecture brevity focus representative benchmark specifically breakdown  benefit data reuse breakdown convolution benefit reduction memory bandwidth operation breakdown scan primarily benefit reduction operation pie normalize consume fermi task reduce marked  portion pie execute fermi benchmark waste von neumann artifact namely pipeline  schedule fds memory  register file RF data movement fds inefficient fermi spends functional comp MT cgra architecture variant gpu MT cgra dmt cgra  RF noc comp dram fds others  matrix multiplication breakdown gpu MT cgra dmt cgra convolution breakdown gpu MT cgra dmt cgra scan prefix sum breakdown breakdown representative benchmark eliminate von neumann artifact however regular MT cgra reliance memory communication medium  convolution  memory dram furthermore excess memory traffic additional traffic noc increase noc consumption finally dmt cgra efficient competitor inter thread communication primitive eliminate consume excessive data transfer memory dram noc eliminate von neumann artifact furthermore computational consume thanks reduction operation ultimately dmt cgra incurs overhead management computation fermi MT cgra functional consume average conclude evaluation demonstrates performance benefit dmt cgra architecture von neumann gpgpu nvidia fermi MT cgra without inter thread communication VI related dataflow architecture CGRAs potential dataflow CGRAs  mad extend von neumann processor dataflow efficiently execute code dataflow manner  cgra component core accelerate loop   tartan portion code hyperblocks schedule accord dependency dataflow gpu static dataflow architecture pipeline instance task reconfigurable execution architecture mainly leverage execution model accelerate singlethreaded performance however   enable multi thread  schedule thread tile grid  pipelining instance hyperblocks originate thread nevertheless none mention architecture simultaneous dynamic dataflow execution thread grid   simultaneous dynamic multithreaded execution grid inter thread communication message passing inter core communication inter thread communication vital implement efficient parallel software algorithm mpi program model scalable popular message passing program model implement hardware grain communication across core mit  machine mit raw adm caf  RC architecture integrate hardware multi core communication synchronization core whereas  hardware mechanism transfer loop dependency across core prior explore hardware assist technique communication core apply principle massively multithreaded environment implement communication thread inter thread communication enable decouple software pipelining sequential algorithm  synchronization buffer communication thread nvidia amd HSA ISAs inter thread communication within wavefront warp shuffle permute instruction described relevant program however communication limited data transfer within wavefront cannot synchronize thread thread within wavefront execute lockstep nevertheless addition instruction SIMT program model limited scope demonstrates inter thread communication GPGPUs vii CONCLUSIONS redundant memory access  throughput processor access attribute memory inter thread communication multiple thread access memory address introduce inter thread communication previously propose multithreaded coarse grain reconfigurable array MT cgra propose  architecture eliminates redundant memory access thread directly communicate cgra fabric inter thread communication eliminates memory communication medium allows thread directly memory invoke redundant memory load dmt cgra obtains average speedup MT cgra nvidia GPGPUs respectively dmt cgra reduces consumption average MT cgra nvidia GPGPUs