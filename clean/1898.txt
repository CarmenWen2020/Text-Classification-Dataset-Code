propose  architectural framework building application specific parallel accelerator manual effort framework introduces  computation model explicit continuation passing dynamic parallelism addition static parallelism contrast framework accelerator focus static data thread parallelism identify schedule realize computation model develop accelerator architecture efficiently handle dynamic task generation schedule load balance steal architecture dynamic parallel construct fork data dependent task spawn arbitrary nest recursion task static parallel introduce methodology architectural template allows easily parallel accelerator description propose framework fpga prototype detailed simulation evaluation framework generate highperformance accelerator target FPGAs parallel algorithm achieve average speedup core processor core efficient introduction technology slows compute rely increasingly hardware accelerator improve performance efficiency  gate array FPGAs deployed purpose acceleration platform improve performance efficiency application FPGAs become widely available increasingly integrate purpose core inter socket interconnect intel harp ibm  directly chip xilinx zynq socs intel  socs trend application traditionally purpose processor GPPS potentially benefit fpga acceleration achieve performance GPPS FPGAs application exploit parallelism dynamic parallelism generate statically compile inherent application algorithm widely parallel software GPPS hierarchical data structure graph adaptive grid data dependent execution behavior computation perform recursive algorithm conquer algorithm dynamic parallelism recursion algorithm adaptively explore optimization data physic simulation generate dynamically unfortunately framework fpga accelerator adequate dynamic generation dynamic schedule synthesis HLS OpenCL mostly exploit static data thread parallelism schedule compile mapped fix pipeline domainspecific  abstraction static parallel recent explore dynamically extract parallelism irregular application FPGAs limited pipeline parallelism efficient schedule dynamically generate multiple processing  rtl flexibility implement arbitrary feature cycle significant manual effort unattractive target diverse application realize potential fpga acceleration application framework capable exploit static dynamic parallelism performance accelerator manual effort propose  architectural framework accelerate static dynamic parallel algorithm reconfigurable hardware  description parallel algorithm output rtl accelerator mapped fpga standard framework aim enable accelerate dynamic parallel algorithm FPGAs without manually rtl efficiently parallel unified framework achieve goal address technical challenge framework parallel computation model suitable hardware architecture efficiently realizes computation model hardware productive methodology automatically generate rtl parallel computation model propose adopt tasked program model explicit continuation passing task parallel program become increasingly popular parallel software development annual acm international symposium microarchitecture doi micro intel cilk plus intel thread building TBB openmp task framework diverse parallelism express unified task abstraction arbitrary computation dynamic generation task generate task communication task enable efficient hardware implementation explicit continuation passing encode inter task synchronization propose novel architecture execute arbitrary computation described explicit continuation passing model architecture configurable template platform dynamically schedule task generality efficiency irregular workload architecture adaptively schedule independent task pool processing steal finegrained load balance computation exhibit static parallel data parallel architecture static schedule efficiency architecture logical parallelism computation physical parallelism hardware enables programmer express computation task without worry detail task mapped underlie hardware minimize manual effort accelerator designer propose methodology combine HLS propose computation model architecture template methodology HLS generate application specific worker description parameterized rtl implementation architecture template generate accelerator rtl desire architecture feature configuration designer rtl framework implement parallel accelerator  prototyped xilinx zynq fpga demonstrate framework indeed handle application performance improvement FPGAs evaluate framework context future soc multiple purpose core integrate reconfigurable fabric cache coherent memory detailed simulation  generate scalable fpga parallel accelerator achieve significant speedup optimize parallel software implementation intel cilk plus contribution accelerator architecture explicit continuation passing model capable dynamic generation dynamic schedule methodology accelerator leverage benefit HLS worker implementation rtl template implementation prototype architecture generation fpga detailed simulation evaluation performance accelerator built framework future fpga platform II computation model dynamic parallelism introduce computation model  model explicit continuation passing inspire task parallel program mit cilk allows diverse parallelism express schedule framework primitive task computation input argument continuation formally task tuple args function args argument continuation another task task intuitively task analogous function software args function pointer argument function caller receives function return execution task spawn task execute spawn task task spawn task function task concurrently spawn task eventually subsequent computation potentially depends output task proceed software framework command sophisticated runtime perform synchronization implement hardware instead model explicit continuation passing simpler hardware architecture task pending task argument execute task pending argument task pending task associate counter argument task return pending task continuation upon counter pending task decremented counter zero pending task becomes continuation passing parallel program framework software runtime manage synchronization task approach challenge implement hardware software perform transfer user code runtime function   easily restore program stack frame capability hardware accelerator runtime logic complex incur overhead implement hardware address challenge explicit continuation passing continuation passing style cps style program explicitly continuation procedure generates framework continuation passing express computation dynamic task graph   return continuation passing sequential composition task  downward arrow spawn task horizontal arrow successor task dot arrow return argument vector vector fibonacci dynamic program task graph construct continuation passing vector vector node label index sub vector fibonacci numbered node task fib node label successor sum task dynamic program solid arrow spawn along continuation explicit dependence continuation passing serf foundation construct abstraction data parallel loop fork model continuation task pending task precisely pending task argument task return simplest continuation passing implement sequential composition task suppose execute task sequentially return continuation continuation passing invoke task continuation spawn task passing continuation return illustrates operation continuation implement fork dynamically generate parallel task perform synchronization suppose parallel task combine task creates pending task successor spawn task continuation completes fork task becomes receives completes illustrates fork operation task graph continuation passing combine task spawn static dynamic parallel algorithm express model algorithm executes task graph dynamically unfolds task graph algorithm computes sum vector suppose vector chunk task graph computation data parallel task graph regular addition task perform actual successor task synchronization source vector efficient recursive decomposition vector recursively vector multiple intermediate task rely task perform decomposition another calculates nth fibonacci recursively apply formula calculate concurrently typical fork calculate fib graph illustrates challenge computation dynamic task parallelism hardware accelerator task dynamically execution enumerate statically computation involves recursion hardware task imbalanced technique partition statically inefficient algorithm matrix depends dynamic program algorithm task graph matrix task parallel express continuation passing cannot easily express framework fork nest composability apparent computation model naturally nest parallelism spawn task recursively spawn parallel sub task nest important achieve parallelization algorithm fibonacci parallelism task without nest task parallelize yield maximum speedup nest non overlap subtrees task parallel significantly increase amount parallelism another feature computation model composability computation express combination data parallel fork task parallel program ultimately transform continuation passing primitive schedule computation model described enables express concurrency computation however scheduler schedule task onto processing parallel execution framework dynamic schedule steal dynamic computation static task distribution data parallel computation briefly steal model model processing datapath task local task queue task pending task storage pending task processing operates task queue  manner operating queue processing idle dequeue execute task local task queue task spawn pending task becomes append task queue task queue empty processing thief steal randomly another processing victim steal task victim task queue task queue continuation task refers pending task another processing task return pending task become newly task transfer processing execute implement greedy schedule processing argument pending task execution successor task greedy schedule important bound fully strict computation task sends successor task schedule policy described behavior cilk scheduler provably efficient specifically task execution processing bound SP SP serial execution processing bound important limit task queue function traditional HLS insufficient function deeply nest recursive surprising execute recursive function usually stack hardware accelerator handle function inlined computation model naturally recursive function reuse task spawn continuation mechanism classic continuation passing style program handle function accelerator architecture accelerator architecture implement computation model described II specifically architecture fulfill goal implement task spawn explicit continuation passing hardware schedule computation highlevel architecture accelerator shade accelerator consists multiple tile tile compose configurable processing PEs unique ID tile serf building architecture fully functional task processing accelerator consist comparison tile  FlexArch LiteArch data parallel fork task parallel task schedule steal static distribution tile without functionality  architecture enables accelerator designer easily tile PEs reduces effort component reuse multiple tile chip network tile cache PEs tile architecture context integrate cpu fpga soc purpose core fpga fabric address cache cache coherent interconnect integrate soc platform become increasingly popular attractive purpose acceleration allows grain data cpu fpga application grain propose architecture adapt discrete FPGAs memory hierarchy variant architecture FlexArch LiteArch flexibility overhead FlexArch continuation passing model allows programmer implement algorithm parallel data parallel fork nest flexible steal task schedule comparison LiteArch data parallel static task distribution task schedule LiteArch intend lightweight alternative application static data parallel sufficient summarizes feature architecture FlexArch tile PE architecture architecture FlexArch tile tile contains multiple processing pending task storage argument task router network interface net component via intra tile bus PE consists worker task management  worker performs task specific computation accelerator designer application architecture worker factor functionality task management module reuse worker interface communicate module message task task task spawn task arg return cont req cont resp successor task memory controller cache argument task network cpu steal network PE PE tile PE PE tile worker  net task cont req cont resp cache interface task steal req steal resp queue arg arg task router steal network arg task network arg task net PE steal arg task cont PE steal arg task cont arg task task mem req mem resp worker arg cache interface task  queue arg task network mem req mem resp PE task arg task arg net task arg PE task arg accelerator architecture architecture accelerator shade FlexArch tile LiteArch tile continuation worker memory architecture stipulate worker implement interface protocol implement HLS rtl architecture currently homogeneous worker task computation graph task identify task message corresponds computation model extend architecture heterogeneous worker worker subset task allows coarse grain resource tile hardware worker within tile dedicate PE contrast homogeneous worker HLS perform grain resource logic task worker task management  responsible worker task internally task queue task task queue implement  queue operation worker enqueues dequeues task queue  important task locality fifo traverse task graph depth manner task queue becomes empty  initiate steal linear feedback shift register LFSR random PE victim sends steal request victim network  victim PE receives request dot arrow dequeues task queue sends steal PE steal important efficiency enables steal chunk request task closer task spawn pending task argument execution function analogous reservation outof processor straightforward implement centralize structure entire accelerator pending task however severe contention PEs address challenge propose distribute architecture tile local access tile network locality processing task graph pending task tile likely consume within tile access local without incur network traffic consists counter array metadata array argument array available entry PE request pending task entry allocate continuation ID return counter array argument pending task argument data argument array specify continuation counter decremented counter zero task become PE argument entry deallocated argument task router steer argument task message local remote tile worker return argument refer pending task remote tile receives argument remote PE output task task remote PE implement greedy schedule critical guarantee asymptotic bound LiteArch tile PE architecture architecture LiteArch tile FlexArch tile argument task router pending task rout argument task tile architecture accelerator steal network within PE  simplify remove steal capability worker architecture data parallel host cpu splitting   task execution PEs network argument steal network logical network architecture specify physical implementation network compatible network interface protocol implement topology combine physical network implementation network crossbar memory hierarchy architecture accelerator integrate purpose memory hierarchy via cache address purpose core investigate address cache memory cache reduce program effort remove manually orchestrate data transfer significant portion effort hardware accelerator cache address enable grain data cpu fpga pointer data structure broadens application mapped architecture accelerator cache per tile architecture cache coherence cache coherent cache application grain data accelerator cache implement RAMs FPGAs future FPGAs harden cache worker local memory structure scratchpad cache coherent memory accelerator rely massive internal memory bandwidth local memory achieve performance task steal data movement perform transparently memory coherent cache propose framework non coherent cache  accelerator grain data designer willing explicitly data transfer PE initiate cache flush dma transfer input output data task integrate accelerator purpose memory hierarchy challenge traditional HLS traditional HLS assume fix latency memory access generate monolithic struggle variable memory latency purpose memory delay memory response entire stall accelerator architecture overcomes PEs independent stall PE affect others dynamic schedule balance load PEs imbalance arise due memory latency cpu accelerator interface accelerator contains interface serf interface cpu accelerator implement memory mapped interface cpu task accelerator memory mapped access receives task pas PEs processing FlexArch leverage steal purpose PE steal task architectural template HW generation accelerator rtl parameter architecture PEs etc HLS algorithm desc worker rtl designer framework accelerator  via steal network LiteArch task via argument task network PEs static assignment task execute IV methodology framework discus methodology software framework developed accelerator manual effort overall  framework accelerator designer algorithm worker description format framework synthesizes worker rtl HLS framework combine worker rtl architectural template generates rtl accelerator architectural template implement propose accelerator architecture architecture template  python hardware generation template parameterized designer configure architecture FlexArch LiteArch tile PEs entry task queue cache algorithm description format accelerator architecture specify task processing logic worker implement synthesis usually prefer productivity rtl HLS approach define worker description  format  code fibonacci algorithm described II worker define function argument function worker function header standard worker function task define designer function defines fibonacci algorithm recursively split sub dynamically spawn task merge obtain algorithm challenge express accelerator methodology involves dynamically bound parallel recursion trivial express framework data parallel framework helper function parallel intel TBB wrap detail implement dynamic spawn task easy interface  void    task   task  cont req  cont resp  arg const  task task continuation task task task fib int task arg argument arg successor task successor sum cont req cont resp spawn task spawn  fib task spawn  fib task task sum int sum task task arg argument sum arg worker description fibonacci concept allows splitting linear configurable accelerator rtl generation framework generates accelerator rtl combine synthesize worker rtl architecture template accord parameter specify designer choice architecture PEs task queue entry cache etc framework elaborates template perform hardware generation output rtl accelerator exploration easily parameter framework without rewrite code evaluation evaluation propose accelerator framework hardware prototype accelerator fpga platform perform detailed architecture avoid limitation fpga platform simulation context future integrate cpu fpga soc benchmark benchmark algorithm variety application domain linear algebra graph sort combinatorial optimization image processing bioinformatics benchmark developed others adapt benchmark suite cilk apps unbalanced  cod parallel implementation II summary BENCHMARKS PA parallelization approach PF parallel  fork CP continuation passing recursive  parallelism DP data dependent parallelism MP memory access MI memory intensity PA DP MP MI CP regular medium quicksort  regular medium  cilk apps  regular medium queen cilk apps  regular knapsack cilk apps  regular   regular   PF regular medium   PF irregular spmvcrs  PF irregular   PF regular algorithm propose algorithm description format task granularity depends application characteristic chosen strike balance parallelization overhead load balance II summarizes benchmark characteristic benchmark recursive nest data dependent parallelism challenge express exist accelerator framework framework allows algorithm brief description benchmark algorithm approach parallelize implement   algorithm dynamic program algorithm aligns dna sequence algorithm dimensional matrix depends  parallelize matrix continuation passing construct task graph quicksort implement classic quicksort algorithm conquer algorithm recursively partition array array sort hoare partition scheme implementation fork parallelize across conquer  parallel merge sort algorithm described recursively array array sort performs merge parallel sub array quicksort sort sub array partition sort sub array insertion sort sub array becomes sufficiently fork parallelize across  conquer queen solves classic queen fork parallelize knapsack solves knapsack implementation bound algorithm parallelize fork benchmark dynamically construct unbalanced unbalanced stress load balance capability architecture fork parallelize across subtrees  matrix multiplication kernel achieve memory locality parallelize loop nest nest parallel  breadth algorithm queue frontier node parallelize across frontier parallel loop spmvcrs sparse matrix vector multiplication algorithm compress storage format parallelism across matrix parallel  performs stencil computation 2D image image parallel parallelize across worker generate HLS apply standard HLS optimization technique loop pipelining unroll application specific local memory structure scratchpad buffer achieve internal memory bandwidth PE architecture optimize accelerator HLS without additional parallelization benchmark fork continuation passing implement version parallel target LiteArch multiple processing task graph parallel construct task homogeneous benchmark cannot parallelize rewrite algorithm approach mapping parallel implement parallel version quicksort queen knapsack  due complexity irregularity dynamic task graph cod parallel software implementation algorithm intel cilk plus compile optimization auto vectorization target neon simd extension hardware prototype fpga demonstrate propose framework implement prototype xilinx zynq fpga soc zedboard soc cortex core integrate fpga fabric equivalent  implement FlexArch template fpga generate accelerator described IV zynq platform limitation future integrate cpu fpga platform envision fpga fabric cache interface implement coherent cache fpga implement buffer instead cache PEs cache benchmark rely grain cache access implement bandwidth fpga cache limited  cpu bandwidth memory bandwidth becomes bottleneck PEs performance accelerator optimize parallel cilk plus implementation benchmark core soc performance accelerator performance parallel software zedboard platform configuration technology cpu ISA core issue entry IQ entry rob 1GHz cpu LI L1D KB cycle cache latency prefetcher accel logic fpga fabric mhz accel KB mhz cycle cache latency prefetcher cache MB 1GHz cycle latency inclusive core accelerator coherence MOESI snoop protocol dram ddr 8GB peak bandwidth obtain program execution initialization data transfer performance fpga accelerator PEs PEs normalize parallel software implementation PE accelerator achieve speedup parallel software geomean PE accelerator achieve speedup geomean reveal limitation zynq platform accelerator slowdown spmvcrs memory bound benchmark fpga memory bandwidth cache core similarly performance improvement spmvcrs  increase PEs due limited memory bandwidth simulation methodology limitation fpga platform evaluate propose architecture context future integrate cpu fpga platform cache coherent accelerator memory bandwidth simulation allows explore perform detailed evaluation model future integrate cpu fpga soc cpu fpga cache coherent memory parameter platform gem model integrate cpu fpga soc simulate accelerator modify gem integrate rtl simulator verilator gem   cycle gem cpu model adapter perform synchronization gem component memory cycle accelerator rtl simulator perform detailed rtl simulation accelerator retain flexibility configure component core cache interconnect dram estimate fpga resource utilization synthesize rtl vivado target xilinx series fpga obtain lut FF dsp slice RAMs estimate resource utilization accelerator cache xilinx cache IP estimate accelerator vivado estimation synthesize netlist signal activity factor rtl simulation model core McPAT statistic gem simulation performance scalability scalability propose accelerator architecture parallel speedup speedup PE implementation PE implementation configure tile PEs simulate tile PEs FlexArch LiteArch comparison scalability  baseline core PE core PEs core budget memory tile core configuration cache IV scalability software accelerator accelerator achieve speedup core PEs  task parallel program framework runtime addition accelerator speedup PEs benchmark propose accelerator architecture effective harness parallelism application accelerator architecture LiteArch accelerator scalability FlexArch accelerator algorithm naturally data parallel   spmvcrs  however benchmark dynamic data dependent parallelism irregular parallelize fork explicit continuation passing FlexArch accelerator generally achieves scalability knapsack knapsack implementation LiteArch algorithm sacrifice algorithmic efficiency parallel scalability later absolute performance actually LiteArch adequate regular data parallel algorithm FlexArch parallel algorithm although algorithm rewrite LiteArch dynamic irregular implementation efficient due effective load balance reduce algorithmic efficiency benchmark scalability others sort algorithm  quicksort speedup core PEs however core PEs increase  performance achieve speedup PEs FlexArch performance quicksort quickly taper algorithm amount dynamic parallelism quicksort significant non parallelizable portion specifically partition perform serially achievable speedup limited amdahl contrast  parallel merge sort generates parallel task execution hence achieves scalability FlexArch architecture achieves load balance hardware steal mechanism unbalanced particularly load balance frequent steal operation  achieves speedup core comparison FlexArch accelerator achieves speedup PEs performance PEs hardware implementation steal efficient software incurs overhead steal operation instruction software cycle accelerator normalize performance performance fpga accelerator normalize oforder core horizontal performance parallel software  core obtain program execution accelerator outperform core software implementation benchmark PEs FlexArch accelerator geomean faster core geomean faster core application accelerator outperform core processor frequency PE perform operation cycle processor core standard HLS optimization loop pipelining unroll increase internal parallelism PE addition PEs exploit application specific parallelism queen PE multiple candidate location chessboard parallel optimization hardware customization implement processor accelerator cannot significantly outperform core processor quicksort spmvcrs earlier quicksort significant serial portion processor frequency faster spmvcrs limited memory bandwidth implementation eventually performance LiteArch accelerator achieve performance FlexArch accelerator data parallel benchmark benchmark FlexArch significantly outperforms IV benchmark scalability  speedup core PE implementation core PE implementation benchmark cpu flex accelerator lite accelerator PE PE PE PE PE PE PE PE PE PE PE PE quicksort  queen knapsack   spmvcrs  geomean normalize accelerator performance axis worker PEs axis performance normalize core horizontal indicates performance core  implementation LiteArch PEs performance difference knapsack algorithmic inefficiency earlier demonstrate fpga accelerator exploit parallelism performance benefit parallel software traditional HLS sequential code input generate accelerator roughly performance PE parallel software framework enables easily mapping diverse parallel algorithm fpga achieve compelling performance later advantage parallel software resource utilization per PE per tile resource utilization accelerator tile consists PEs cache dsp mainly implement multiplier brams local scratchpad buffer task storage cache LiteArch accelerator generally resource FlexArch accelerator reduction apparent regular data parallel benchmark   spmvcrs  task graph statically resource reduction LiteArch benchmark significant task graph construct dynamically architecture resource utilization context PEs mapped typical fpga device fpga device  fpga   zedboard mainstream fpga   fpga average tile PEs FlexArch tile PEs LiteArch mainstream fpga tile PEs benchmark  FlexArch LiteArch  resource utilization tile consists  cache   dsp  brams  ram ram  ram benchmark flex PE flex tile incl cache lite PE lite tile incl cache lut FF dsp ram lut FF dsp ram lut FF dsp ram lut FF dsp ram quicksort  queen knapsack   spmvcrs  normalize performance efficiency efficiency inverse consumption performance efficiency normalize  implementation core vertical performance horizontal efficiency diagonal iso diagonal benchmark link efficiency performance efficiency accelerator PE configuration normalize  implementation core propose accelerator efficient benchmark benchmark gain efficiency accelerator architecture exists trend performance efficiency profile FlexArch usually achieves performance LiteArch efficiency average FlexArch achieves normalize efficiency core LiteArch achieves cache customization accelerator cache tile cache built brams fpga fabric cache customize accord application characteristic benchmark memory intensive locality cache reduce bram usage without significantly degrade performance performance FlexArch accelerator PE configuration cache benchmark irregular memory access  spmvcrs performance loss  performance loss reduce temporal reuse cache benchmark perform relatively cache  quicksort  locality relatively memory intensity performance accelerator cache VI related task parallel program task parallel program propose recently gain popularity introduction framework cilk intel TBB task program programmer performance load balance carbon implement hardware task queue processor access instruction inspire task program task framework hardware acceleration steal developed along task program extensively provable bound parallel execution serial execution implement steal hardware efficiently distribute balance load parallel accelerator methodology parallel accelerator generate parallel accelerator description explore implement framework OpenCL adopt generate accelerator data parallelism  domain specific generate accelerator collection parallel extends java accelerator pipeline parallelism  subset posix thread  extends generate accelerator thread channel exist framework parallelism specify compile statically schedule dynamic irregular algorithm framework propose framework dynamic parallelism dynamic generation dynamic schedule prior explore dynamic parallelism hardware propose extract parallelism irregular application dynamic data dependency focus pipeline parallelism within thread execution target parallelism data parallel fork task parallelism focus efficient schedule dynamically generate task onto multiple processing parallel multithreaded execution propose hardware steal mechanism achieve load balance explore implement software steal runtimes FPGAs OpenCL atomic operation incurs performance resource overhead contrast propose hardware architecture implement native steal efficient scalable resource vii conclusion introduce  architectural framework performance parallel accelerator manual effort technique inspire task parallel program propose accelerator architecture implement task computation model explicit continuation passing handle task distribution load balance efficiently steal architecture dynamic nest parallelism addition static parallelism execute irregular regular application efficiently introduce methodology allows task parallel accelerator description evaluation approach generate performance efficient accelerator target FPGAs manual effort focus FPGAs methodology potential adopt generate ASIC accelerator