convolutional neural network cnn implementation computational image task exhibit spatially correlate exploit correlation reduce amount computation communication storage execute cnns introduce diffy hardware accelerator performs differential convolution diffy communicates bulk activation delta cnn model HD resolution input diffy boost average performance baseline agnostic accelerator accelerator effectual content raw activation diffy respectively efficient chip however diffy chip storage chip bandwidth raw profile per layer precision dynamic per precision diffy storage chip memory bandwidth importantly diffy performance achieve processing HD resolution image practical configuration finally diffy robust cnn accelerator improves performance image classification model index neural network differential convolution computational image accelerator introduction neural network dnn highlevel classification application image recognition segmentation recognition however dnns recently achieve output quality computational image CI computer vision task traditionally dominate analytical task image denoising demosaicking sharpen deblurring super resolution essential task virtually incorporate image sensor mobile device digital camera medical device automation embed device typically factor constrain accordingly goal investigate dnn computational image deployed device emphasis device limited dnn computational image benefit scientific application telescope image input image billion pixel automation application manufacturing pipeline server application acceptable quality due computational data demand dnns dnn accelerator propose boost performance efficiency commodity graphic processing gpus processor accelerator advantage computation structure data reuse static dynamic ineffectual content precision requirement dnns demonstrate identify additional runtime behavior dnns invaluable inform innovation accelerator accordingly contribution dnns computational image exhibit spatial correlation runtime calculate tangible practical application diffy practical hardware accelerator exploit spatial correlation transparently reduce network chip computation perform combine reduction increase performance efficiency benefit date cnn acceleration effort focus primarily image classification cnns image classification cnns extract feature identify correlation computational image dnns CI dnns perform per pixel prediction input pixel model predicts correspond output pixel structure behavior dnn model generally variety layer per pixel prediction model fully convolutional specialized convolutional neural network cnns CI dnn model exhibit significantly spatial correlation runtime calculate input activation calculation output tend image model preserve throughout layer model naturally input resolution whereas classification annual acm international symposium microarchitecture doi micro model resolution specific advantage spatial correlation introduce differential convolution operates difference delta activation absolute approach greatly reduces amount execute CI dnn demonstrate differential convolution practically implement propose diffy CI dnn accelerator translates reduce precision reduce effectual content delta improve performance reduce chip storage communication ultimately improve efficiency diffy target CI dnns benefit model image classification image segmentation albeit lesser extend diffy robust CI dnn specific summary contribution finding emerge cnn model performs per pixel prediction exhibit spatial correlation differential convolution DC exploit precede CI dnns reduce compute convolution propose diffy practical DC architecture boost performance efficiency CI dnns convolutional neural network cnns propose delta onchip reduce amount storage communication equivalently boost effective capacity chip storage communication link CI dnns diffy configuration compute equivalent accumulate operation per cycle boost performance baseline agnostic accelerator VAA aware accelerator pra respectively diffy configuration HD frame fps frame sec target application comparison VAA achieves fps pra fps dynamic per precision raw diffy reduces chip storage chip traffic diffy scnn execute CI dnns various assumption sparsity diffy consistently outperforms scnn sparsity layout diffy efficient VAA pra diffy efficiently enables processing HD frame considerably resource performance practical chip memory hierarchy delta compression scheme greatly reduce chip storage footprint traffic diffy benefit image classification cnns improve performance average DnCNN FFDNet IRCNN JointNet VDSR entropy information content entropy raw activation conditional entropy text entropy delta VAA pra respectively benefit earlier layer network diffy prof faster pra II motivation ideally avoid redundant computation communication essential information data evidence raw delta adjacent compactly convey essential information content CI dnns per network entropy activation runtime II conditional entropy activation adjacent along axis activation finally entropy activation delta along axis measurement input datasets detailed II average amount information within activation amount information already redundant information remove replace activation delta measurement considerable redundancy information activation potential compress encode information factor IRCNN VDSR network compress information others however average model potential compress underlie information nearly identical respectively review operation convolutional layer II explain delta principle reduce computation communication data footprint II finally motivate diffy reporting spatial locality CI dnns II potential delta encode reduce computation II communication data storage II background convolutional layer convolutional layer input feature imap 3D array activation channel height width applies 3D filter fmaps HF WF slide fashion stride output omap 3D array activation HO WO output activation inner filter sub array imap filter assume respectively imap omap filter output activation compute inner HF WF input grid stride omap dimension respectively HO HF WO WF discussion assume without loss generality CI dnns however concept apply regardless reveal redundant information abundant reuse convolutional layer beneficial transform input activation raw operation perform apply seamlessly representation compress communication computation transformation delta encode adjacent activation difference delta distributive associative multiplication addition operation convolution raw correlate delta encode compress communication efficient representation multiplication account bulk computational CI dnns spatial correlation imaps opportunity reduce amount understand multiplication activation multiplication amount multiplier shift multiplicand yield effectual modify booth encode reduce effectual addition subtraction convolution layer imap overlap activation processing adjacent along dimension adjacent activation calculate directly instead calculate relatively adjacent activation calculate scratch  almost multiplication however difference relatively typically effectual already calculate approach reduce amount overall imap delta reduce footprint amount information communicate compute delta shorter datatype imap spatial correlation CI dnns imaps heatmap raw imap conv convolutional layer DnCNN denoising  image intermediate layer image discernible relevant discussion heatmap difference delta adjacent along axis activation heatmap reveals correlation around image delta peak reduction effectual calculate omap differentially scheme calculate along raw activation subsequent calculate differentially detailed specific imap average per activation delta respectively processing differentially potential reduce amount saving homogeneous however delta yield raw rapid delta raw activation fortunately typical image former dominant cumulative distribution effectual per activation delta distribution CI dnn model image II significant potential reduction amount computation delta instead raw imap delta considerably effectual per sparsity raw imap zero delta processing delta improves potential performance benefit technique exploit activation sparsity computation reduction potential perform computation approach baseline  approach  aware approach effectual imap ΔE aware approach effectual imap delta report reduction speedup normalize activation activation delta delta along axis delta reduction effectual per imap activation imap CI dnns spatially correlate processing delta instead raw reduces  image input cumulative frequency effectual raw delta zero cumulative distribution effectual per activation delta CI dnns datasets average sparsity raw activation DnCNN FFDNet IRCNN JointNet VDSR geom potential speedup raw raw delta potential speedup processing effectual imaps  delta ΔE speedup report processing imap average ΔE CI dnns benefit ΔE saving FFDNet VDSR VDSR exhibit imap sparsity model around typical explains potential  ΔE ideally reduce average potential reduction  VDSR maximum DnCNN suggests diffy approach potential boost performance efficiency architecture target effectual imaps chip footprint communication reduction potential IV imaps CI dnns occupy fmaps latter tend KB whereas imaps proportionally input image resolution dominate chip bandwidth normalize amount chip storage imaps layer approach  imap RLEz imap encode non zero distance non zero RLE imap encode distance profile imap profile derive precision per layer RawD imap dynamically detect precision per activation deltad imap dynamically detect precision delta RLEz capture activation sparsity benefit VDSR RLE performs slightly exploit repetitive profile reduce offchip footprint  RawD compress deltad reduce chip footprint account metadata scheme moreover communicate depends dataflow tile approach defer traffic measurement IV diffy baseline agnostic accelerator resembles dadiannao understood appropriate baseline optimize data parallel agnostic widely reference enable rough comparison DnCNN FFDNet IRCNN JointNet VDSR normalize footprint RLEz RLE profile RawD RawD deltad deltad chip footprint compression approach normalize fix precision storage scheme filter IP filter  buffer ABout IP tile baseline agnostic accelerator VAA plethora accelerator emerge diffy modifies pragmatic accelerator pra execution proportional effectual imap diffy target processing delta effectual raw activation pra processing approach adapt translate phenomenon performance improvement however pra raw activation modify enable delta processing diffy review pra tile implement additional functionality diffy modest investment extra hardware advantage hardware proposal propose technique incorporate serf motivation followup investigation demonstrate specific implementation essential sufficient baseline agnostic accelerator tile data parallel agnostic accelerator VAA model dadiannao VAA tile comprises inner IPs operating parallel activation partial output activation per cycle cycle IP per input activation calculates reduces via adder accumulates output register per tile memory WM activation memory respectively fmaps imaps  WM sip filter sip filter neg neg neg neg   AB buffer offset gen offset gen offset gen offset gen tile pra partition across activation per cycle respectively broadcast tile slice operates per cycle tile activation slice layer imaps  WM activation buffer hide latency output activation buffer prior activation chip memory filter tile per filter precision etc parameter adjust clarity discussion assume data per layer WM however evaluation considers limited chip storage external memory bandwidth aware accelerator diffy implementation upon pragmatic accelerator pra pra activation serially effectual offset generator convert activation effectual apply modify booth encode pra multiplies  cycle shifter  determines shift pra exceeds throughput equivalent VAA processing concurrently activation reuse sufficient pra tile comprise grid serial IP SIPs sip corresponds cycle tile broadcast activation  sip activation  per cycle pra restricts distance concurrently  balance performance accordingly  valid sip input adder instead multiplier shifter shift input  SIPs along convolution differential convolution filter filter differential convolution inner propagate VAA activation per cycle pra activation  serially dataflow VAA brick pra terminology concurrently mod pra  pra terminology concurrently multiple cycle differential convolution formally output activation compute directly per exploit distributive associative multiplication addition compute differentially wise delta imap correspond stride imap apply along dimension sequence imap appropriate ratio output calculation calculate directly per differentially per convenient dataflow differential convolution oppose convolution applies filter consecutive activation convolution raw activation differential convolution raw activation delta subsequent along compute concurrently reconstruct output differential convolution previous cascade fashion latter phase overlap differential processing additional delta dataflow evaluate calculate leftmost per output directly remain output along differentially compatible buffer imap chip dataflow strategy reduces chip bandwidth imap chip timing wise diffy calculates output phase pipelined phase diffy calculates leftmost per output parallel calculate remain output per phase leftmost output diffy propagates component cascade fashion addition per output bulk processing leftmost inner adder sufficient compute bandwidth phase phase balance adder buffer diffy architecture diffy modifies pra architecture introduce respectively differential reconstruction DR per sip delta output calculation  per tile per sip DR reconstruct output SIPs calculate output activation delta raw reconstruction proceeds cascade fashion across tile per processing imap fed delta fed raw SIPs assign differentially SIPs normally SIPs compute output brick pas along ABout SIPs SIPs update differential output normal SIPs along subsequent per differentially assign previous pas output brick robin fashion processing typically cycle plenty reconstruct output activation pas activation function multiplexer per DR allows calculate raw imap unmodified ABout allows intermediate ABout dataflow processing revert normal convolution layer performance negatively affected differential convolution investigate scheme calculate delta imaps raw calculates delta scheme recomputes delta anytime buffer offset gen sip  curr ABout curr prev ABout prev sip sip sip curr ABout prev sip sip offset gen previous output output DR DR DR DR DR DR offset gen  offset gen offset gen  offset gen diffy tile reconstruct differential convolution output sip  AB buffer sip sip sip DR DR DR DR mux   ABout  diffy  compute delta layer advantage delta reduce chip storage communication instead scheme delta calculate output layer per omap architecture  diffy output brick layer delta format  computes delta brick reuse hardware assume layer stride snext compute delta brick  reading output brick stride snext  correspond ABout passing activation function brick buffer wrap around output brick correspond previous  stride snext reading brick  correspond ABout passing activation function compute delta brick wise  multiplexer ABout multiplexer implement across  snext compute delta brick  output brick belongs previous  output brick selection   snext mod  ABout output brick correspond consecutive output  diffy handle stride beyond model memory per pixel model imap omap storage traffic dominates model naturally input image resolution typically image classification model publicly available imagenet model image frame roughly pixel layer maintain input resolution intermediate layer increase channel fmaps comparatively increase resolution dilate filter expand zero accordingly reasonable assume imaps  chip chip bandwidth becomes concern similarly reasonable assume fmaps chip model effective dataflow utilizes fmap imap omap reuse essential diffy opts chip strategy input activation per layer writes output activation per layer purpose accommodate input plus output diffy chip buffering output activation load activation chip memory simultaneously previous output activation chip memory buffering another output activation fmaps WM fmaps concurrently depends fmaps per tile tile completely hide load fmaps buffer load fmaps layer layer IV WM memory reasonable technology demonstrates delta encode reduce WM desire chip bandwidth increase algorithm efficient dataflows adapt purpose reduce chip traffic diffy encodes activation delta dynamically detect precision per activation instead raw dynamic precision detection similarly dynamic stripe activation contains header activation per chip compression activation virtual  virtual contains precision per IV evaluation detail CI dnns DnCNN FFDNet IRCNN image denoising dnn model rival output quality non local similarity BM3D  JointNet performs demosaicking denoising VDSR layer dnn model delivers quality image super resolution model CI dnns DnCNN FFDNet IRCNN JointNet VDSR application denoising demosaicking super resolution denoising conv layer relu layer max filter KB max filter per layer KB II input datasets dataset sample resolution description  berkeley data   dataset modify  kodak kodak dataset  noisy image camera jpeg compression widely evaluate super resolution algorithm image super resolution algorithm HD HD frame depict texture scene profile derive per layer activation precision network profile derive per layer precision DnCNN FFDNet IRCNN JointNet VDSR VAA pra diffy tile WM tile KB filter tile  tile KB filter chip memory 4GB LPDDR tech node frequency 1GHz diffy tile KB KB VAA pra tile KB KB IV VAA pra diffy configuration retrain perform image task DnCNN network architecture perform image super resolution jpeg  IRCNN perform painting deblurring methodology minimum per layer precision output quality remains within relative float representation quality metric evaluate dnn model signal ratio SNR structural similarity ssim report per layer precision custom cycle accurate simulator model performance architecture IV report default configuration consumption implement verilog synthesize synopsys compiler layout perform cadence innovus TSMC technology estimation mentor graphic  capture circuit activity input innovus CACTI model consumption chip SRAM memory buffer SRAM compiler available accelerator target frequency 1GHz CACTI estimate buffer VAA pra diffy VAA pra diffy VAA pra diffy VAA pra diffy VAA pra diffy pra diffy DnCNN FFDNet IRCNN JointNet VDSR geom speedup VAA  profile delta delta ideal speedup pra diffy VAA chip memory technology node node ddr HBM relative performance investigate performance HD image ddr chip memory interface relative performance performance pra diffy normalize VAA account chip compression scheme compression  encode per layer profile derive precision profile per dynamic precision activation deltad activation delta infinite offchip bandwidth ideal performance RLEz RLE effective scheme IV chip memory bottleneck VAA performance unaffected compression efficiency compression however enables pra diffy deliver performance benefit specifically pra ideally improve performance VAA however model pra profile deltad compression avoid stall noticeably due chip memory transfer deltad scheme JointNet VDSR scheme pra achieves almost ideal speedup VAA diffy outperforms VAA pra respectively average pra deltad compression avoid stall noticeably chip memory JointNet chip memory stall remain noticeable benefit pra diffy nearly VDSR model VDSR exhibit activation sparsity intermediate layer shorter precision model benefit proportional potential II potential achieve underutilization due filter available per layer lane synchronization due imbalance effectual per activation latter significant IV report per layer breakdown lane utilization diffy category useful cycle idle cycle due lane synchronization due filter underutilization stall due chip delay utilization varies considerably per layer per network offchip delay noticeably layer FFDNet JointNet FFDNet layer account percentage overall execution hence impact overall performance JointNet utilization VDSR due lane synchronization VDSR activation sparsity non zero activation dominate execution layer network incurs relatively utilization input image channel dataflow available activation lane typically idle FFDNet exception input layer channel feature input image pre split tile stack along channel dimension extra channel standard deviation rgb channel layer network exhibit utilization layer channel output filter dataflow filter lane tile activation locally enable diffy partition input activation across tile improve utilization moreover idle cycle reduce however exploration future per layer relative speedup diffy pra due limited fairly uniform standard deviation diffy underperforms pra  layer JointNet VDSR diffy variant profile apply differential convolution selectively per layer beneficial eliminate per layer slowdown pra overall improvement negligible absolute performance frame rate HD resolution processing report detailed measurement resolution fps VAA pra diffy chip compression scheme diffy robustly boost fps pra VAA achievable fps varies image content variance minimum chip memory capacity encode scheme mem baseline profile RawD deltad SRAM KB KB KB KB WM SRAM KB average fps pra diffy respectively diffy faster alternative JointNet fps fps rate performance processing tile IV explores configuration diffy appropriate  application photography smartphone compression chip memory delta encode chip storage chip traffic report chip storage fmaps imaps  memory network KB KB KB per tile tile configuration deltad scheme target activation report activation memory storage scheme mirror compression scheme deltad reduce chip activation memory boost effective capacity without compression KB profile reduces storage KB whereas RawD reduces KB capacity MB however remains RawD effective capacity enable processing model resolution finally propose deltad reduces KB reduction profile RawD respectively KB regardless scheme deltad compression considerably reduces chip capacity evaluation capacity report chip traffic normalize  metadata account benefit RLEz RLE scheme significant VDSR due activation sparsity scheme ineffective  effective classification model profile reduces chip traffic dynamic per precision reduces chip traffic RawD RawD RawD overhead due metadata increase decrease activation delta per precision deltad reduces chip traffic uncompressed traffic improvement RawD chip access magnitude expensive chip access reduction chip traffic greatly improve overall efficiency deltad reduces traffic considerably deltad metadata execution breakdown network useful cycle idle cycle memory stall VAA pra diffy VAA pra diffy VAA pra diffy VAA pra diffy VAA pra diffy DnCNN FFDNet IRCNN JointNet VDSR HD frame per fps  profile delta delta ideal HD frame per VAA pra diffy compression scheme RLEz RLE profile RawD RawD RawD deltad deltad deltad RLEz RLE profile RawD RawD RawD deltad deltad deltad RLEz RLE profile RawD RawD RawD deltad deltad deltad RLEz RLE profile RawD RawD RawD deltad deltad deltad RLEz RLE profile RawD RawD RawD deltad deltad deltad RLEz RLE profile RawD RawD RawD deltad deltad deltad DnCNN FFDNet IRCNN JointNet VDSR avg normalize traffic RLEz RLE profile RawD deltad metadata compression scheme chip traffic normalize compression overhead prevents reduction deltad evaluation restrict attention deltad chip chip encode imaps  finally chip memory overall performance memory technology LPDDR HBM demonstrate chip compression scheme essential sustain performance gain realistic chip memory performance diffy normalize VAA compression scheme stack without compression model HBM memory incur slowdown JointNet VDSR sensitive profile  sustain maximum performance respectively network perform within peak DnCNN FFDNet IRCNN JointNet VDSR DnCNN FFDNet IRCNN JointNet VDSR DnCNN FFDNet IRCNN JointNet VDSR DnCNN FFDNet IRCNN JointNet VDSR DnCNN FFDNet IRCNN JointNet VDSR DnCNN FFDNet IRCNN JointNet VDSR ddr  ddr   HBM speedup VAA  profile delta deltad ideal performance diffy chip dram technology activation compression scheme capable memory node performance slowdown noticeable deltad allows network nearly maximum memory node LPDDR JointNet incurs slowdown  node performance deltad within maximum network JointNet within maximum channel  memory diffy sustains maximum performance JointNet VDSR compression profile channel  sufficient preserve performance respectively finally deltad dual channel  memory VDSR incurs slowdown JointNet performs within maximum efficiency VI report breakdown architecture pra diffy consume speedup increase efficient VAA diffy uncompressed chip MB efficient baseline VAA moreover measurement ignore chip traffic reduction achieve diffy chip access magnitude expensive chip access computation overall efficiency diffy compute core diffy pra due additional DR VI consumption breakdown diffy pra VAA diffy pra VAA compute WM  ABout dispatcher offset gen  normalize efficiency vii breakdown diffy pra VAA diffy pra VAA compute WM  ABout dispatcher offset gen  normalize vii report breakdown architecture diffy deltad overall overhead VAA pra furthermore overhead performance advantage diffy pra VAA efficient VAA preferable VAA performance linearly VAA suffer underutilization wider  sensitivity tile configuration report performance tile configuration activation concurrently per filter VAA diffy configure per filter average speedup diffy VAA increase multiple concurrently execution activation effectual induces idle cycle activation lane effectual configuration eliminates stall DnCNN FFDNet IRCNN JointNet VDSR geom speedup VAA performance sensitivity concurrently per filter frame per fps resolution MP DnCNN FFDNet IRCNN JointNet VDSR frame per diffy function image resolution HD frame rate plot tile DnCNN FFDNet IRCNN JointNet VDSR tile diffy sustain HD processing along memory compression scheme deltad profile  gap potential actual speedup model VDSR due activation sparsity absolute performance resolution image application processing resolution image sufficient completeness report absolute performance frame per resolution dnn model diffy subset datasets II exclude HD resolution image diffy achieve processing model DnCNN resolution mega pixel MP DnCNN diffy fps MP frame diffy configuration application processing resolution frame sufficient HD processing achieve fps processing HD frame diffy report model compression scheme report minimum configuration tile axis external memory axis axis report offchip memory configuration LPDDR version transfer rate channel DnCNN demand tile HBM deltad profile HBM otherwise VDSR tile however dual channel  speedup VAA pra diffy pra diffy speedup classification dnns sufficient deltad compression due activation sparsity model FFDNet JointNet tile dual channel ddr IRCNN dual channel  deltad compression classification dnn model diffy target CI dnns execute cnn classification remains important workload imagenet classification model VAA pra diffy fcn seg per pixel prediction model perform semantic segmentation another classification pixel grouped yolo segnet detection model detect classify multiple image frame report performance diffy speedup VAA improvement pra modest differential convolution degrade benefit performance model accordingly diffy cnn accelerator performance comparison scnn speedup diffy scnn various assumption sparsity  scnn scnn scnn refer scnn unmodified randomly sparsified version model respectively average diffy consistently outperforms scnn sparse model specifically diffy faster scnn sparsity assumption respectively sparsity model optimistic analytical model CI dnns mimic pixel depends neighborhood pixel moreover activation sparsity model classification model scnn related due limitation limit attention relevant already pra scnn another accelerator potentially benefit differential convolution dynamic stripe performance varies precision activation delta DnCNN FFDNet IRCNN JointNet VDSR geom speedup scnn  scnn SC scnn scnn scnn scnn  speedup diffy scnn activation precision requirement dynamic stripe perform pra simpler  algorithm reduces inference cnns processing video frame compute convolution activation across frame  target temporally across frame diffy target spatially within frame  limited frame frame video processing whereas diffy computational image task unlike  additional storage previous frame diffy potentially reduce storage bandwidth delta compression moreover  software implementation graphic processor however concept potentially combine similarly  exploit vector generate frame temporal pixel data accelerate cnn semantic task later along vision pipeline instead perform expensive cnn inference frame   detection application vector naturally generate stage pipeline approach complementary operates within frame moreover target computational image task ideal accelerate BM3D image denoising algorithm enable processing HD frame ideal performance dadiannao JointNet diffy boost performance dadiannao specific dnn model VI summary emerge network computational image task rival conventional analytical demonstrate exhibit spatial correlation propose accelerator convert communication storage computation benefit boost performance efficiency accelerator practical deploy network enable processing diffy enables differential processing motivates approach computation perform delta approach inference image model future investigate application model architecture training