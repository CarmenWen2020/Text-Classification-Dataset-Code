model datasets DL model architect challenge memory capacity bottleneck limited physical memory inside accelerator device constrains algorithm propose memory centric transparently expand memory capacity available accelerator inter device communication parallel training proposal aggregate pool memory module locally within  interconnect decouple host interface function vehicle transparent memory capacity expansion conventional proposal achieves average speedup DL application increase memory capacity TBs index architecture hpc machine introduction DL model training datasets phenomenal rate progress DL primarily limited neural network dnn model evaluate memory utilize training DL practitioner therefore seek efficient parallel training increasingly adopt dense node pcie attach coprocessor device increase node throughput performance contingent upon DL algorithm parallelize across device effectively communicate vendor employ custom device interconnection network utilizes proprietary bandwidth signal nvidia NVLINK GB sec bandwidth communication synchronization device centric architecture DC DLA become mainstream DL increase hpc employ standalone device interconnection network efficient DL parallelization researcher seek deploy deeper dnn topology however user upon memory capacity limited device physical memory package 3D stack memory nvidia google TPUv intel  crest accelerator constrains algorithm increase capacity package stack dram however challenge due  silicon  chip  dram stack technology limit dram stack vertically integrate consequently recent propose device gpu tpu memory application cache respect host cpu memory effectively virtualizing dnn memory usage across host device memory via pcie II effectiveness prior however sensitive host device communication bandwidth determines latency incur migrate dnn data memory axis execution ofthe convolutional neural network cnns successive version DL accelerator reduce factor signal circuitry overall communication bandwidth pcie infiniband improve factor pcie gen IB fdr IB hdr respectively steadily increase performance overhead host device memory virtualization via pcie axis performance gap device compute host device pcie communication bandwidth aggravates performance virtualizing dnn memory multi gpu tpu incurs performance overhead effective host device communication bandwidth allocate per device proportionally reduce intra node device therefore overall significant performance slowdown due additional latency incur host device memory overall trend urgent architectural satisfies dual requirement inter device communication parallel training performance memory virtualization memory pool enable memory hungry dnns trainable accelerator device memory centric architecture MC DLA aggregate pool capacity optimize memory module within  interconnect transparent memory capacity expansion proposal reminiscent prior disaggregated memory proposal cpu centric memory disaggregation suffer performance bottleneck DC DLA reliance pcie proposal pool memory module henceforth refer memory node completely decouple legacy host device interface pcie annual acm international symposium microarchitecture doi micro device centric architecture memory centric architecture locally within device interconnect propose interconnect accelerator memory node bandwidth latency signal link NVLINK utilize memory node backing accelerator allows memory node function vehicle transparent memory capacity expansion researcher DL algorithm deeper complex accelerator access memory node via bandwidth link performance overhead virtualizing memory substantially reduce MC DLA connects accelerator memory node manner maximizes inter device communication bandwidth DC DLA efficiency conduct collective communication operation maintain overall contribution knowledge highlight importance device interconnects training DL algorithm quantitative analysis parallel training context hpc multiple accelerator gpu tpu device identifies performance bottleneck DC DLA motivates architecture balance communication user productivity training dnn algorithm propose evaluate architecture MC DLA transparent memory capacity expansion enable inter device communication DC DLA proposal achieves average performance improvement expand memory capacity expose accelerator TBs II background motivation DL training versus inference dnns training inference training involves optimal dnn backpropagation algorithm serialize layer wise computation propagation input layer output layer serialize fashion arrow layer applies mathematical operation convolution activation recurrence etc input feature derives output feature execution cnn model across recent generation DL accelerator device axis performance overhead incur due memory virtualization axis IV evaluation methodology layer input feature propagation prediction input truth define loss function quantifies magnitude error prediction truth encapsulate gradient loss function respect layer output backpropagation perform direction propagation arrow layer wise manner incoming gradient derive output gradient previous layer input gradient layer derives gradient adjust layer loss incrementally reduce improve performance dnn model virtualizing memory chain backpropagation algorithm layer input feature derive gradient layer consequently overall memory allocation dnn training proportionally network depth memory dnn layer user therefore carefully tune network topology layer dnn inter layer connection training batch overall memory requirement within physical memory capacity severely limit user productivity recent research trend DL practitioner seek deploy deeper network algorithm memory allocation training easily exceed GBs tackle memory capacity bottleneck minimize performance overhead becomes vital enable researcher DL algorithm prior virtualizing memory usage dnns propose utilize host device memory concurrently allocate data structure dnn training leverage user dnn topology graph extract compile data dependency information encapsulate acyclic graph dag data structure memory hungry data structure feature dnn virtual memory leverage data parallel model parallel training arrow inside device layer computation backward propagation respectively arrow boundary device per layer inter device communication synchronization operation model parallel training incurs frequent synchronization data parallel training data dependency information derive dnn data reuse distance schedule performance aware data operation via memory overlay across host device memory via pcie exist DL framework therefore opt leverage dag schedule software dma initiate data transfer host device overlap dnn backward propagation maximally utilize pcie communication bandwidth minimize performance overhead data migration dnn data inside device memory memory allocation training network layer reduce enhance DL practitioner ability algorithm parallelization DL algorithm dnn algorithm complex deeper distribute multi node multiple accelerator device significantly increase compute  DL practitioner consequently efficient parallelization DL algorithm communication device become vital maximally exploit hpc dense multi device node scope develop efficient intra node architecture conventional assume inter node communication handle mpi via ethernet infiniband parallel DL training popular parallel training strategy employ DL framework data parallel model parallel training data parallel training parallelization scheme allocates network model across worker worker assign batch overall training dataset  training however worker identical batch training dataset fix batch allocate portion network model parallel task distribute across worker periodically synchronize consistent dnn model within worker prevent data parallel model parallel training achieve perfect model parallel training generally incurs frequent synchronization data parallel approach input feature gradient aggregate across layer boundary due collective communication primitive parallel training parallelization algorithm data parallel training accumulation backpropagation therefore assume amenable achieve linear speedup however network layer data parallelize easily dnns model model data parallel training important quantify robustness interconnect hpc communication imply previous discussion minimize communication overhead highperformance parallel training consequently maximally utilize communication bandwidth provision across accelerator within across compute node crucial collective communication primitive parallel training reduce broadcast prior demonstrate algorithm collective communication optimal link bandwidth utilization aforementioned collective operation vendor therefore employ topology aware algorithm collective communication library nvidia NCCL ibm  ddl baidu allreduce library cast underlie interconnect multiple network orchestrate DL communication operation algorithm maximize bandwidth utilization minimize latency device interconnects DL efficient communication synchronization across accelerator device recent hpc DL employ proprietary highbandwidth device interconnection network GB sec inter device communication bandwidth intel  crest accelerator employ highbandwidth signal link pcie tightly couple DL accelerator device nvidia DGX equip volta gpus highbandwidth  directional bandwidth GB sec per link aggregate channel bandwidth GB sec per gpu cube mesh topology across cast cube mesh topology interconnects gpus communicate bandwidth network NCCL library achieve optimal bandwidth utilization minimize latency device centric architecture DC DLA advantage inter device synchronization communicate hostside cpu legacy pcie link cube mesh device interconnect employ nvidia DGX dot arrow network collective communication operation severe performance bottleneck device utilize host cpu memory memory virtualization architecture optimize spectrum bandwidth link partially access cpu memory achieve communication decent communication bandwidth singular duo network performance virtual memory bandwidth cpu memory host centric DL architecture HC DLA however aspect HC DLA simply feasible option vast majority hpc proprietary bandwidth signal link device interconnects incompatible CPUs HC DLA enable bandwidth cpu gpu communication ibm nvidia gpus challenge allocate subset highbandwidth link host cpu leaf inter device bandwidth available device communication potentially slowdown effective node throughput algorithm sensitive inter device communication importantly however highbandwidth link per accelerator device directly host cpu memory bandwidth available cpu highly unbalanced detail virtualizing memory usage DL algorithm dma fully utilize communication link bandwidth effectively hide latency copying data device memory singular bandwidth link GB sec per device amount GB sec host memory bandwidth consumption accounting pcie attach device cpu socket reference maximum memory bandwidth available intel xeon cpu ibm GB sec GB sec per socket respectively due cpu  throughput orient gpus google TPUs bandwidth latency quantitatively HC DLA consume average host memory bandwidth workload memory bandwidth available host cpu explore argue unbalanced architecture practical future DL severely lack flexibility amount throughput designer provision host device memory virtualization limited maximum memory bandwidth available per cpu socket regardless device bandwidth link available severe bottleneck future algorithm deeper complex memory centric hpc architecture learning propose architectural future hpc optimize goal develop DL architecture enables inter device communication parallel training provision bandwidth communication channel pool capacity optimize memory module refer memory node performance virtual memory argue hpc architecture DL training memory centric manner memory capacity challenge training algorithm prior disaggregated memory similarly expand pool memory expose memory blade access pcie NIC DC DLA however performance gap gpu tpu device compute host device communication render cpu centric pcie memory disaggregation impractical training latency access memory pool become bottleneck pcie consequently memory node  DL architecture MC DLA locally inside device interconnection network eliminate host pcie interface detail memory node architecture application propose MC DLA leverage  building achieve aforementioned goal scope intra node architecture refer pcie attach accelerator device gpus TPUs device node memory node device node function node inside device interconnect MC DLA applicable gpus TPUs proposal concern efficient architecture DL accelerator device node explanation assume device node gpus terminology define nvidia cuda hardware software interface remainder memory node architecture objective memory node unlock bandwidth communication channel  interconnect performance virtual memory illustrates memory node architecture contains bandwidth link communicate device interconnection network link logically partition link exclusively designate  dnn memory virtualization protocol memory node architecture compatible device interconnect maximum bandwidth GB sec per link device node assign link utilize dma data memory DIMMs GB sec throughput dma device node data transfer request memory controller array commodity memory DIMMs manages ASIC handle encryption compression optionally memory node assumes memory DIMMs capacity density optimize commodity memory GB ddr  register DIMMs GB  load reduce DIMMs narrow explore assumes housing memory node equivalent pcie accelerator compatible exist device interconnects minimize server chassis enclosure memory node built  equivalent volta ddr DIMMs maximum GB sec PC GB sec PC memory bandwidth overall memory capacity expansion GB TB per memory node architecture highlight importance device interconnects training DL algorithm architect proposal exploration beyond scope interconnect incorporate memory node discus offs link bandwidth utilization overall performance narrow option assume device node memory node identical device node memory node bandwidth communication link interface node network link GB sec uni directional communication bandwidth configuration nvidia DGX highbandwidth link per device link GB sec communication bandwidth intuition MC DLA interconnect objective MC DLA device interconnect balance communication memory virtualization overall complexity straightforward intuitive interconnect utilize memory node backing device node MC DLA interconnect device node memory node derivative interconnect cube mesh topology device node communicate memory node topology memory node fold inward propose  interconnect node interconnect cod arrow link communication link constitute network singular network construct directional arrow rearrange construct network memory node device node device node ability access designate  bandwidth link GB sec communication bandwidth significantly reduce latency migrate data backing significant limitation however inter device collective communication construct highly unbalanced fashion construct maximum hop remain incurs maximum hop render overall communication latency bottleneck dot directional link neither utilized communication memory virtualization fail maximally utilize available communication resource alternative memory node twice traverse  network dot arrow  without device node device node already fully utilize link parallel DL training message communicate across device generate device node inside gpu memory inside memory node improve performance communication memory virtualization MC DLA interconnect optimize packaging complexity inter node link physical MC DLA via enclosure  balance network performance device node construct maximum hop respectively similarly suffers aforementioned limitation unbalanced underutilization communication resource holistically address challenge MC DLA interconnect maintains competitive collective communication performance DC DLA significantly improve maximize communication bandwidth available memory virtualization illustrates propose MC DLA interconnect overall device node bandwidth link memory node advantage MC DLA architecture twofold device node utilize memory node logical maximally utilize link virtualizing memory bandwidth allows MC DLA achieve link bandwidth node GB sec communication bandwidth significant improvement legacy pcie communication bandwidth memory node linearly proportional signal technology implement bandwidth link oppose pcie DC DLA HC DLA maximal communication bandwidth capped maximum cpu socket memory bandwidth instance DC DLA HC DLA regardless host device interface NVLINK generation pcie latency incur perform collective communication primitive function node inside network normalize node link GB sec directional bandwidth node communicate message KB target synchronization MB maximum per cpu socket memory bandwidth approximately GB sec GB sec highend intel xeon ibm CPUs respectively accelerator device MC DLA GB sec per device device GB sec communication bandwidth memory node proportionally function link bandwidth GB sec illustration MC DLA optimize packaging adoption hpc concerned latency incur collective communication increase MC DLA additional memory node effectively node inside reasonably message MC DLA node incur negligible latency overhead broadcast reduce communication MC DLA incur latency DC DLA scenario communication latency performance limiter amdahl demonstrate impact latency overhead negligible performance software interface MC DLA upon  dnn memory virtualization assume DL framework analyzes neural network dag structure  derives data dependency memory hungry dnn data information utilized runtime memory manager schedule performance aware  memory overlay operation dma initiate  across host device memory expand memory available training MC DLA introduces another tier memory addition host device memory capacity optimize memory inside memory node refer deviceremote memory propose utilize deviceremote memory supplant role host memory stash dnn data reuse distance memory virtualization implement local device  memory deviceremote memory without cpu memory involve runtime memory manager allocate data structure inside software api extension MC DLA api argument semantics  src malloc byte deviceremote memory return ptr src  src memory allocate deviceremote memory  src dst byte src dst direction direction   deviceremote memory initiate dma data transfer memory introduce extension cuda runtime apis  deviceremote memory allocation memory apis exist DL framework seamlessly exploit additional pool memory inside memory node software MC DLA device driver allocate memory deviceremote memory address  program memory node logically partition resource within dma memory controller memory DIMMs exclusively assign  service request resource device node device driver manages client device node memory node physical memory device memory address consequently  physical memory device memory address deviceremote physical memory concatenate mapped address driver perspective device node augment memory node pcie device memory capacity maxwell GB versus volta GB hence exist software apis mmap  enlarge device memory address user MC DLA TB TB additional physical memory fitting address capability gpus virtual address TB physical memory address TB memory capacity device node inform device driver boot driver consideration allocate memory allocate  deviceremote memory exist device walker allocation placement policy bandwidth aware BW aware manner maximally exploit bandwidth communication channel memory node  byte memory allocation request driver entire byte data allocate memory node refer local allocation policy proposal split request malloc chunk align local allocation policy local numa zone allocation policy  linux intend imply allocation inside  memory  local BW aware allocation policy employ MC DLA BW aware allows device node data memory node concurrently reduce overall latency local granularity within chunk memory node memory address robin fashion allows device node utilize bandwidth link data memory node maximally utilize GB sec memory bandwidth memory virtualization IV methodology device memory node developed simulator evaluate MC DLA architecture DL accelerator resembles eyeriss dadiannao device architecture employ spatial array processing PEs contains multitude mac operator handle vector operation local SRAM buffer  overlap computation data fetch leverage data locality bandwidth package memory HBM local  allocation baseline device node configure summarize II evaluate MC DLA sensitivity alternative configuration model optimize generic gemm matrix multiplication operation handle convolutional layer recurrent layer fully layer activation layer etc analysis output stationary dataflow output feature locally chip balance mac utilization efficiency across layer evaluate hence device accelerator employ output stationary dataflow stationary dataflow worth scope hpc architecture DL training development efficiency accelerator device therefore proposal equally effective alternative DL accelerator dnn dataflows II device memory node configuration parameter device node PEs MACs per PE PE operating frequency ghz local SRAM buffer per PE KB memory bandwidth GB sec memory access latency cycle bandwidth link communication bandwidth per link GB sec memory node memory bandwidth GB sec memory access latency cycle bandwidth link communication bandwidth per link GB sec iteration training millisecond gpu perform simulation tractable amount crucial therefore model device node HBM memory memory DIMMs inside memory node fix memory bandwidth latency resort cycle dram simulator methodology accurate estimation without lose fidelity due dnn computation memory access data locality highly deterministic dataflow exist primarily employ lightweight FSM microcontrollers orchestrate chip data movement coarse granular data inter node host   deviceremote data operation conduct coarsegrained bulk data transfer DMAs II data locality render performance sensitive underlie behavior dram microarchitecture conflict architecture assume device node configuration baseline DC DLA architecture model nvidia DGX ibm  ddl hpc employ cube mesh device interconnect flatten multiple network evaluation link per device node maximally utilize inter node link bandwidth minimize latency incur conduct inter device communication DC DLA pcie gen communicate host memory memory virtualization HC DLA architecture model  summit assumes bandwidth link available  HC DLA allocates cpu memory writes trading memory virtualization communication balance manner device node cpu socket device socket maximum cpu socket memory bandwidth fully service aggregate cpu memory bandwidth usage  cpu socket consequently hypothetical cpu HC DLA GB sec per socket cpu memory bandwidth  allows evaluate benchmark network application layer alexnet image recognition googlenet image recognition vgg image recognition resnet image recognition network application timesteps rnn gemv recognition rnn lstm machine translation rnn lstm model rnn gru recognition bandwidth link cpu memory GB sec HC DLA consume provision cpu memory bandwidth discus maximum average cpu memory bandwidth usage cpu memory bandwidth usage potentially incur destructive interference cpu role overall DL training DL framework software interact backing storage hdd ssd fetch training data etc overall training conservative evaluation assume HC DLA cpu memory bandwidth usage performance omit HC DLA partition bandwidth link asymmetric manner robust balance HC DLA oracular version DC DLA establish infinitely package  memory available inside device node obviate  data migration explore  evaluate effectiveness MC DLA memory node MC DLA configure ddr DIMMs maximum GB sec memory bandwidth device node benchmark diverse dnn application encompasses convolutional neural network cnns recurrent neural network rnns cnn topology stateof performance imagenet namely alexnet googlenet vgg resnet rnn application chosen baidu deepbench application suite gemv vanilla rnn topology lstm gru rnns batch evaluation data parallel model parallel training partition DL algorithm across device node model parallel training employ model parallelization strategy employ memory overlay dnn virtual memory implement runtime memory management policy described leverage network dag analyze inter layer data dependency schedule  operation virtual memory implementation device memory utilized  cache respect host memory concretely runtime memory manager layer feature backing reuse  breakdown latency incur data parallel training model parallel training normalize stack sum latency category directly translate performance DL framework overlap computation synchronization memory virtualization tion prefetches local device memory backpropagation local remote data migration operation dnns within memory capacity limit prior employ memory management policy maximally stress interconnect dnn application microbenchmarks stress interconnect evaluate robustness performant virtual memory without compromise communication performance evaluation evaluates baseline DC DLA hypothetical HC DLA IV topology MC DLA MC DLA MC DLA local BW aware allocation policy denote MC DLA MC DLA respectively oracular DC DLA infinite memory DC DLA average harmonic identify bottleneck convolutional layer generally compute limited slide dataflow manifest data locality feature dominate memory allocation training conversely fully layer recurrent layer memory bandwidth limited employ exception layer computation activation layer pool layer memory manager chooses compute feature backpropagation migrate data backing optimization minimizes memory overlay operation currently employ mxnet adopt optimization conservative evaluation performance unnecessarily degrade memory allocation feature consequently data parallel training cnns generally insensitive underlie ability inter device communication synchronization data gradient relatively feature memory virtualization therefore become performance bottleneck data parallel training cnns rnns however relatively synchronization hence communication bandwidth memory virtualization data parallel rnn training model parallel training II incurs frequent synchronization operation data parallel counterpart bandwidth device interconnect crucial scalable DL training context clearly illustrate performance bottleneck derive latency incur perform computation backward propagation inter device synchronization memory overlay memory virtualization stack altogether overall DC DLA spends amount synchronization thanks bandwidth  interconnection network memory virtualization however significant performance bottleneck DC DLA training pcie link bandwidth link service thanks bandwidth link allocate access cpu memory HC DLA significantly reduce latency incur memory virtualization average reduction however  communication bandwidth increase synchronization average increase cpu memory bandwidth usage DLA multiple device HC DLA utilize cpu memory virtual memory consume significant cpu memory bandwidth DL training CPUs role DL framework software training datasets fed accelerator device reading batching input datasets hdd ssd storage device preprocessing input batch upload preprocessed input batch cpu memory host device interaction destructive interference performance slowdown HC DLA conservative analysis behavior account evaluate overall performance MC DLA achieve DC DLA HC DLA interconnect successfully reduces latency incur virtualizing memory incur noticeable overhead inter device communication additionally memory virtualization deviceremote memory cpu memory bandwidth consumption whatsoever enable independently host interface performance summarizes performance MC DLA DC DLA HC DLA oracular DC DLA HC DLA average speedup DC DLA data parallel model parallel training respectively due HC DLA ability balance communication memory virtualization DC DLA fails achieve due asymmetric partition communication bandwidth difference bandwidth provision inter device communication memory virtualization HC DLA however leverage bandwidth link communication virtual memory fail maximally benefit device interconnect propose MC DLA fully unlocks bandwidth link communication memory virtualization average speedup DC DLA data parallel model parallel training respectively average moreover MC DLA performance  oracular DC DLA average MC DLA DC DLA HC DLA suboptimal utilization bandwidth link leaf significant performance maximum average performance loss MC DLA worth performance improvement MC DLA  training model parallel training MC DLA performance sensitivity input batch relatively sub optimal simpler MC DLA achieves performance MC DLA although MC DLA provision memory virtualization bandwidth MC DLA highbandwidth communication channel synchronization equally thanks interconnect MC DLA MC DLA benefit application MC DLA robust scalable option maximally utilize interconnect bandwidth reasonable sensitivity MC DLA performance sensitivity input batch demonstrate MC DLA robustness average speedup DC DLA across batch DC DLA generation pcie gen pcie link bandwidth improves DC DLA memory virtualization performance improves DC DLA performance narrow performance gap DC DLA MC DLA oppose significant cpu memory bandwidth consumption proportional increase pcie link bandwidth IV memory node consumption ddr DIMM memory node ddr module TDP TDP GB GB  GB  GB LRDIMM GB LRDIMM GB LRDIMM faster device node configuration TPUv node configuration DGX  TB device interconnect bandwidth explore MC DLA average speedup DC DLA respectively propose leverage cnn activation sparsity compress reduce local device communication traffic alleviate pcie bottleneck technique average reduction pcie traffic narrow performance gap DC DLA MC DLA cnn application overall MC DLA exhibit robustness across various sensitivity conduct chip configuration input batch etc guarantee performance memory virtualization inter device communication efficiency MC DLA utilizes exist accelerator overhead memory node network nvidia DGX DC DLA TDP gpus consume IV summarizes estimation memory node consumption publicly available measurement ddr DIMMs micron ddr calculator limited environment  GB  appropriate incurs additional consumption increase DGX consumer focus capacity expansion GB LRDIMM  TB memory GB consumption increase option drastically increase pool memory TBs microsoft custom built  server chassis feature pascal gpus TDP overhead MC DLA reasonable unique proposition overall MC DLA achieves increase performance per watt substantially enhance pool memory expose device node scalability although image classification gradually gain traction DL algorithm community research parallelize distribute cnn provision compute communication bandwidth baseline benefit MC DLA pronounce DC DLA becomes completely bottleneck memory virtualization training gpus TPUs reduce training achieve performance scalability recent advance domain research employ data parallel training extremely batch reduce intra inter node communication overhead achieve perfect performance memory usage exist cnn algorithm optimize within physical gpu memory constraint training without cpu gpu data migration involve simulation infrastructure perfect performance scalability DC DLA memory virtualization disabled reduction training cnn application data parallelize across gpus however memory virtualization enable feature migrate local remote memory performance improvement achieve gpus DC DLA  communication bottleneck performance scalability regain MC DLA thanks ability perfectly hide data migration overhead user productivity cnn algorithm image classification super performance DL research community shift towards challenge task video understand video classification caption video input video goal capture context scene activity express relate video understand algorithm commonly implement mixture cnns lstms memory network training algorithm hpc becomes practically impossible memory capacity bottleneck DL practitioner therefore compromise algorithm freeze subset algorithm without endto training reduce input video frame recurrent timesteps per training iteration video frame overall memory footprint within physical gpu memory advent video training datasets youtube sufficient amount memory enhances user productivity become vital aside dnns deeper MC DLA wider complex algorithm training aforementioned video text algorithm employ cnns lstms currently impossible due memory capacity limit propel continued innovation active research although host device data migration cnn workload arguably unnecessary prior exist workload performance scalability publicly available dnn algorithm exceed memory capacity limit cannot dnn algorithm unless memory requirement within physical memory limit chicken datacenter device interconnect assumes node node device memory node bandwidth link device switch allows node communicate node inside node enable cast interconnection topology MC DLA interconnect VI future research direction knowledge literature highlight significance  interconnects training DL algorithm across multiple device due limitation focus intra node architectural issue assume inter node communication handle mpi ethernet infiniband nvidia recently announce NVSwitch technology NVLINK compatible switch enable vendor opportunity device interconnection network instance incorporate gpus within node tightly integrate gpus across node microsoft brainwave introduction device switch technology accelerator enable device interconnects emphasizes importance device interconnection network research opportunity explore memory node architecture device interconnects  memory node across distribute network future vii related memory disaggregation expands cpu memory hierarchy remote memory blade pcie increase pool cpu accessible memory propose interconnect multiple CPUs gpus leverage packet rout capability  effectively compose memory network flexible processor bandwidth utilization scope significantly focus importantly proposal memory technology whereas assumes 3D stack memory rout capability embed inside logic layer slowdown moore  driven researcher pursue chiplet processor soc decompose multiple yield chiplets assemble package envision combine concept chiplet gpus notion memory network tightly integrate gpus  within package memory capacity expansion however maximum gpus  integrate inside package bound various technology constraint load distribution programmability recent mcm gpu assumes gpus integrate within package focus MC DLA efficient parallelization workload partition context oppose prior chiplet context focus package integration prior explore accelerator device architecture inference increase leverage dnn sparsity efficiency improvement propose acceleration platform training machine algorithm fpga node distribute prior orthogonal MC DLA proposal adopt additional enhancement conclusion model datasets DL model vendor employ custom device interconnection network communication synchronization across accelerator device significance device interconnects training DL algorithm highlight importance balance inter device communication memory virtualization memory centric DL scalable programmable efficient hpc platform DL training average speedup DC DLA drastically expand pool memory accessible accelerator TBs