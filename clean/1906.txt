intelligent personal assistant  capability processing nlp increasingly popular mobile device recurrent neural network rnns memory network lstms become core machine technique apply nlp  continuously improve performance mobile gpus local processing become promising data transmission privacy issue induced centric computation  however lstms exhibit inefficient memory access execute mobile gpus due redundant data movement limited chip bandwidth aim explore memory friendly lstm mobile gpus hierarchically reduce chip memory access address redundant data movement propose inter optimization intelligently parallelize originally sequentially execute lstm rnns correspond neuron cnns improve data locality across negligible accuracy loss relax pressure limited offchip memory bandwidth propose intra optimization dynamically skip load computation matrix trivial contribution output introduce module gpus architecture runtime skip matrix moreover technique equip threshold unique  performance accuracy offs directly user preference experimental optimization achieves substantial improvement performance user imperceptible accuracy loss optimization exhibit scalability increase input data user delivers excellent user index approximate compute gpgpu gpu mobile embed architecture research NSF grant CCF CCF career CCF research partially national foundation china grant beijing  introduction intelligent personal assistant  capability processing nlp siri google assistant amazon alexa already heavily mobile user become workload future mobile device core technique nlp ipa application artificial neural network convolution neural network cnns recurrent neural network rnns cnns ability achieve accuracy image recognition increase requirement  nlp  understand complicate request perform feature capture relationship input sample largely ignore cnns rnns memory network lstms promise explore affiliation serial sample detail II lstms become technique nlp  google microsoft amazon baidu leverage advantage lstms processing context information nlp ipa application semantics classification automatic recognition traditionally computation NN ipa application perform data transmission privacy issue continuously improve performance mobile device localize application becomes promising attract attention mobile gpus improve significantly parallel compute neural network become candidate host computation NN  lstms annual acm international symposium microarchitecture doi micro however lstms exhibit inefficient memory access serious memory bottleneck execute mobile gpus redundant data movement feature lstms matrix rnns correspond neuron cnns lstm layer layer due context link data dependence adjacent limited mobile gpus chip storage sequential execution redundant load chip memory matrix across limited chip bandwidth relatively per lstm severe pressure chip memory bandwidth bottleneck significantly extend lstm execution consumption mobile gpus unfortunately previous propose technology cnns cannot effectively address challenge lstms completely computation cnns detailed comparison lstms cnns II aim explore memory friendly lstms mobile gpus achieve substantial improvement performance propose optimization inter intra address challenge respectively hierarchically reduce offchip memory access inter feature lstm layer context link adjacent weak propose lstm layer multiple independent sub layer weak link weak link lose sub layer predict link apply sub layer recover accuracy loss parallelize sub layer enhances data locality across sub layer substantially reduces redundant data movement intra unlike cnns matrix trivial contribution output propose dynamic skip DRS technique skip load computation trivial lstm negligible impact output accuracy introduce module gpus architecture reorganizes cooperative thread array CTAs dynamically disable thread assign trivial DRS reduces matrix lstm hence effectively relax pressure chip memory bandwidth knowledge address memory bottleneck lstms execute mobile gpus propose technique gain substantial performance benefit without sacrifice user perceptible output accuracy moreover technique equip threshold unique   offs directly user preference user contribution memory bottleneck lstms context link schematic rnn layer unrolled model operation mapping input output unrolled model timestamp conv conv FC FC FC conv layer FC layer schematic cnn convolution conv layer cnn fully FC layer mobile gpus mainly frequent data load across sequentially lstm matrix per weak context link adjacent leverage feature explore  optimization intelligently parallelize processing lstm hence reduce data load user imperceptible accuracy loss propose intra optimization dynamically skip load computation trivial matrix negligible contribution output introduce module gpus architecture runtime skip matrix experimental propose technique achieve average upto performance improvement entire accuracy loss generally user imperceptible lstm execution mobile gpus optimization exhibit scalability increase input data user delivers excellent user II background recurrent neural network rnn layer usually contains integrates operation mapping input output output periodically activation layer historic output context link highlight feature model context dependency within input activation sequence model task model task simplify analysis rnn layer unrolled sequence output gate input gate forget gate lstm schematic timestamps clarify focus rnn unrolled layer layer unrolled timestamp correspondingly refer previous layer unrolled previous timestamp rnn unrolled layer cnn neural network layer convolution conv layer fully FC layer completely computation demonstrates schematic conv layer FC layer cnns comparison firstly input format layer totally conv layer multiple matrix FC layer bunch activation input rnn unrolled layer activation matrix processing activation vector furthermore conv layer FC layer output activation layer input layer conv FC operation layer parallelize however operation rnn unrolled layer involve dimension besides layer input output context link adjacent therefore instead concurrently processing layer input rnn layer iteratively partial input activation timestamp vector input activation matrix processing vector processing previous vector memory network lstms mainly rnns rnns vanilla rnns memory network lstms gate recurrent network grus rnns hardly useful information input interval lstms grus introduce address gate inside rnn filter information input historical output useful information unrolled enable memory lstm gate gru increase computation complexity ensures accuracy focus analysis optimization lstms execution mobile gpus propose apply grus adjustment algorithm lstm execution mobile gpus layer lstm kernel sgemm layer kernel sgemv kernel lstm zoom lstm tth timestamp gate lstm input gate forget gate output gate modify context information arbitrary interval lstm input layer input historic output previous vector output output activation vector equation computation within lstms     tanh     tanh generates forget gate apply previous input gate merge update output output gate filter output information lstm execution mobile gpus gpus backend library cudnn computation within lstm function input integrate matrix vector multiplication kernel sgemv matrix concatenate united matrix similarly computation combine united matrix vector multiplication kernel sgemv gpus tesla layer execute parallel data dependence jth layer timestamp parallelize layer tth timestamp however layer parallelism memory matrix multiple layer hardly implement mobile gpus tegra limited chip storage lstm layer sequentially mobile gpus layer imdb MR babi SNLI PTB MT average contribution pipeline stall memory access lack resource inst fetch synchronization contribution factor pipeline stall cycle execute sgemv sgemm sgemv lstm layer chip memory matrix matrix sketch kernel execution lstm layer input lstm layer execution gain matrix multiplication efficiency originally independent matrix vector multiplication per transform matrix matrix multiplication kernel sgemm per layer finally remain operation kernel lstm wise consists activation function individual algorithm summarizes theart lstm execution mobile gpus backend library cudnn memory bottleneck although optimization gpu backend library lstm execution mobile gpus inefficient implement theart lstm execution typical mobile gpu jetson TX kernel sgemv dominates overall lstm execution investigate gpu pipeline stall sgemv execution factor pipeline stall chip memory access barrier synchronization plot contribution factor overall pipeline stall cycle execute sgemv kernel benchmark detail VI chip memory access contributor besides previous chip memory access expensive mobile gpus perspective memory challenge inter lstm intra lstm performance bottleneck efficient lstm execution mobile gpus inter memory bottleneck redundant data movement illustrate algorithm sgemv kernel launch per execute lstm layer united matrix repeatedly request sgemv kernel across timestamps layer unfortunately matrix exhibit data locality gpus chip storage redundant data movement intensive chip memory imdb MR babi SNLI PTB MT utilization chip  chip bandwidth utilization chip chip memory execute sgemv access described mainly unique lstm execution sgemv kernel input data dependent previous layer prevents sgemv kernel across integrate matrix matrix multiplication kernel load matrix per layer sgemv kernel access matrix separately limited chip storage fails matrix frequent load eviction useful data actually load data upto data indicates efficient data load moreover redundant data movement become  increase layer additional load united matrix efficiently minimize redundant data load improve data locality across propose inter optimization scheme lstm layer reorganization lstm layer multiple parallel sub layer sub layer become independent combine enable reuse matrix detail described IV intra memory bottleneck limited chip bandwidth sgemv kernel inside load united matrix numerous however limited chip memory bandwidth mobile gpus fails fulfill demand plot chip onchip bandwidth utilization sgemv kernel execution chip bandwidth almost fully utilized chip bandwidth lightly consume release chip bandwidth limitation propose effectively shrink input data sgemv kernel explore dynamic skip scheme leverage unique computation feature lstm dynamically skip data load united matrix trivial contribution output detail IV inter  focus inter optimization enhance data locality across lstm layer  lstm sigmoid function hyperbolic tangent function tanh activation function lstm computation sigmoid function input within sigmoid  sensitive sensitive sigmoid function tanh function insensitive insensitive insensitive insensitive sigmoid activation function tanh activation function output within interestingly input output nearly linear input refer sensitive output insensitive input within refer insensitive tanh function neural network framework sigmoid function model sigmoid function accelerate computation boundary partition sensitive insensitive sigmoid sigmoid function accord previous output output sigmoid function output tanh within input data matrix multiplication output derive moreover sigmoid function input derive layer processing input easily output feature sigmoid function output irrelevant derivation apply output irrelevant summarize within   derive insensitive previous output irrelevant impact computation context link lstm layer observation context link uniform throughout lstm layer propose lstm layer scheme context link weak link lstm layer multiple independent sub layer sublayer parallelization reduce data reloads explore IV weak context link lose sub layer affect output accuracy predict context link apply sub layer sublayer recover accuracy algorithm relevance acquisition input hidden layer dim matrix output vector matrix multiplication     offset vector output relevant initial relevance sum dim min max min min min max return breakpoints theoretically context link irrelevant impact output accuracy however consecutive layer completely irrelevant weak link becomes target lstm layer scheme quantitatively justify relevance towards weak context link breakpoints introduce relevance describes impact precedent output implies weaker link totally irrelevant algorithm calculates link precedent compute output vector matrix multiplication compute offset vector output vector matrix multiplication calculate input activation function sensitive overlap multiple activation function lstm overlap activation function combine calculate input finally input vector sum derive overall lstm layer relevance compute algorithm timestamp relevance threshold  weak context link lstm layer threshold weakly link breakpoint accuracy recovery pre vector predict context link lose breakpoints context link vector although predict vector accurate apply breakpoints recover application output accuracy weak context link matrix chip memory lstm layer independent sub layer weak context link weak context link context link weak context link accuracy recovery accuracy recovery predict context link tissue tissue tissue exceed maximum tissue tissue formation tissue formation tissue alignment tissue alignment tissue tissue tissue matrix chip memory reorganize lstm layer breakpoints overview inter optimization relatively impact application output insensitive prediction error predict weak context link analyze distribution context link execute lstms offline training datasets distribution context link weak context link distribution context link unnecessary particularly focus weak context link relevance threshold context link vector distribution expectation weak context link achieve equation ρij expectation jth context link ρij possibility distribution jth context link expectation compose vector predict context link breakpoints lstm layer reorganization tissue formation independent lstm sub layer parallelize via fuse sub layer tissue per sub layer tissue lstm layer sub layer respectively combine tissue sub layer sub layer sub layer contains combine another tissue lstm layer transform sequence tissue inside tissue execute concurrently data dependency across sub layer maintains treat data dependency across tissue define per tissue tissue ideally sub layer fuse tissue tissue layer load matrix performance improve however increase tissue hurt performance demonstrates normalize performance lstm layer tissue increase execute investigate benchmark baseline introduce VI performance increase increase tissue tissue layer tissue memory bandwidth imdb MR babi SNLI PTB MT imdb MR babi SNLI PTB MT performance normalize performance lstm layer memory bandwidth utilization tissue increase exceeds babi benchmark others define maximum tissue MTS performance limited chip bandwidth memory bandwidth mobile gpus plot utilization memory bandwidth bandwidth utilization increase increase tissue approach MTS increase tissue kernel configuration compilation ensure chip bandwidth utilization configuration reduces chip bandwidth requirement per thread increase thread amount kernel execution per tissue significantly increase compensate reduce matrix load overall performance droop MTS gpu configuration framework dynamically implement lstm layer reorganization scheme various lstm layer configuration mobile gpus tissue alignment tissue formation mechanism simply combine multiple tissue ignores MTS generate tissue tissue MTS tissue MTS utilized memory bandwidth tissue tissue unable effectively reuse matrix affect performance boost maximize performance explore tissue alignment mechanism balance tissue tissue tissue tissue tissue tissue tissue respectively tissue alignment context link ensures tissue MTS gpu config lstm config MTS  sgemm breakpoints  sgemm tissue alignment input batching  lstm user accuracy preference layer predict context link lstm layer lstm threshold upper limit statistic output accuracy offline  lstm runtime operation threshold update accuracy recovery per app processing per execution processing per layer processing tissue formation implementation inter optimization implementation offline operation execute lstms target gpus platform various tissue MTS ideally tissue MTS tissue layer tissue layer minimize maximal performance therefore minimal tissue conduct nmin  MTS  lstm layer execute lstms equip optimization obtain relevance threshold  nmin tissue upper limit   initialize upper limit aim performance accuracy loss considerable gain performance  adjust per execution application accuracy difference user prefer accuracy application output accuracy optimal performance accuracy offs user perspective furthermore predict context link lstm configuration operation gpu lstm configuration per application runtime operation lstm runtime perform per layer multiplication kernel sgemm breakpoints accuracy recovery trigger layer sub layer transform tissue balance tissue tissue concurrently batching input vector united input matrix input vector batch matrix correspondingly originally per matrix vector multiplication sgemv kernel combine per tissue matrix matrix multiplication sgemm kernel matrix effectively within tissue load frequency reduces per per tissue intra  illustrate besides redundant data movement across chip memory bandwidth performance limitation lstm matrix input data matrix output output gate irrelevant matrix trivial impact output important shrink hence address bottleneck inside lstm focus intra optimization effectively reduce data load per generally mechanism shrink matrix target erase  lstm totally irrelevant leverage unique feature propose compression technique dynamic skip compact matrix without affect output accuracy dynamic skip observation matrix trivial contribution output vector strongly affected output gate zero correspond output become zero correspond furthermore calculate matrix correspond matrix treat irrelevant output propose dynamic skip DRS scheme dynamically skip irrelevant matrix computation trivial contribution output vector skip load computation ignore performance optimization DRS affect vector correspond skip approximate zero however impact limited overall accuracy forget gate algorithm lstm computation DRS layer lstm kernel sgemm layer kernel sgemv kernel lstm kernel DRS  kernel sgemv kernel lstm grid management gmu pending kernel pool software tid queue  mask decode prefix sum hardware tid queue accumulation shifter cta schedule hardware queue CTAs reorganization module LD  offset architecture CTAs reorganization module filter vector previous unlike traditional prune perform offline DRS conduct runtime lstm latent information skip across lstm implementation DRS skip latent vector zero available correspond matrix identify skip execution modification computation lstm generate processing matrix split sgemv kernel kernel multiplies matrix input vector multiplies matrix algorithm illustrates reorganize computation DRS sgemv kernel launch lstm kernel compute latent vector zero threshold  DRS  kernel obtain trivial ID sgemv kernel launch perform matrix multiplication trivial disabled finally remain computation lstm zero threshold adjust user prefer accuracy requirement deliver user satisfied performance accuracy offs hardware DRS implement pure software assign operation trivial non trivial divergence gpu execution decrease warp efficiency skip breakpoints information threshold output lstms pytorch baidu deepbench performance jetson TX gate simulation evaluation diagram optimization platform specification hardware specification tegra soc cpu cortex cortex memory 4GB LPDDR 6GB gpu maxwell core mhz performance propose hardware DRS introduces CTAs reorganization module crm grid management gmu illustrate crm identify thread assign trivial organize CTAs skip kernel launch information kernel argument acquire initialization kernel additional argument implies trivial assign crm CTAs organization kernel enters crm load module LD load trivial IDs trivial buffer  disabled thread IDs  decode trivial IDs grid configuration thread IDs gpu kernel execution software thread ID  within kernel launch hardware thread ID  indicates hardware thread slot assign software thread thread disabled kernel execution offset   thread thread  kernel filter  prefix sum offset sort shift  acquire   acquisition conduct thread warp usually divisible cta partition stage dash pipelined crm finally organize CTAs hardware queue issue VI evaluation experimental setup leverage software hardware cooperate evaluate optimization lstm execution software employ pytorch popular source machine framework dynamic computation graph baidu deepbench benchmark operation hardware platform hardware gpu architecture simulator gpgpu sim cannot backend library II nlp application investigate abbr hidden layer imdb SC MR SC babi QA SNLI ET PTB LM MT MT machine cudnn cuBLAS employ jetson tegra develop kit representative mobile gpu development configuration illustrates evaluation diagram  intra optimization data approximation affect output accuracy computation affect performance data approximation implement pytorch obtain accuracy computation hardly implement pytorch gpu machine backend library cudnn release pre compile binary pytorch deepbench gpu cooperate evaluate technique performance pytorch breakpoints information inter optimization trivial intra optimization information deepbench simulate lstm execution optimization jetson tegra obtain performance obtain describes consumption overall cpu gpu etc performance overhead hardware model via gate simulation overhead benchmark employ nlp apps II lstm benchmark app unique lstm configuration hidden matrix indicates per lstm layer imdb MR perform sentiment classification SC predict positive negative attitude text babi performs QA automatic text understand SNLI collection english manually label balance classification label entailment ET PTB model LM MT performs english french translation MT accuracy loss imperceptible user fix user prefer accuracy requirement evaluate performance improvement gain technique conduct user  accuracy requirement per individual user effectiveness overall plot performance saving obtain inter optimization intra optimization overall combine inter intra combine imdb MR babi SNLI PTB MT average speedup achieve apply inter optimization intra optimization overall combine optimization optimization normalize baseline execute lstms mobile gpus inter optimization average  optimization achieve baseline technique capability improve performance consumption lstm lstm layer increase PTB layer investigate benchmark achieves performance enhancement implies technique longer lstm layer investigate effectiveness inter optimization lstm layer technique perform earlier layer latter layer layer context information closer text input context link distinct earlier layer sub layer performance gain intra optimization average intra optimization achieve baseline technique gain performance improvement matrix PTB matrix investigate benchmark achieves performance technique exhibit scalability increase input data trend nlp ipa application intra technique popular matrix compression scheme zero prune pure software DRS zero prune scheme reduces data movement degrades performance baseline zero prune scheme prune zero matrix without divergence execute lstms gpus  layer layer application lstm layer per layer apply inter optimization compression ratio zero prune pure software DRS DRS hardware matrix compression ratio apply compression scheme DRS scheme achieves compression ratio average zero prune scheme pure softwarebased DRS induces divergence achieve performance gain  hardware enable CTAs organization  optimization maintain warp efficiency achieve additional pure software average combine intra inter optimization outperforms baseline upto performance upto improvement gain overall sum improvement obtain technique overlap data movement reduction technique performance accuracy offs explore performance accuracy offs conduct sensitivity analysis  threshold apply technique   proportional performance boost mainly analyze  offs threshold explore increase baseline without accuracy loss maximal aggressive maximal performance boost obtain threshold   respectively threshold threshold demonstrates normalize speedup accuracy threshold apply investigate application denote AO accuracy orient threshold correspond optimization user imperceptible accuracy loss threshold performance gain denote BPA normalize accuracy loss normalize accuracy loss performance accuracy offs lstms babi hidden input configuration hidden input imdb MR babi SNLI PTB MT user satisfaction baseline AO BPA UO user satisfaction scheme performance accuracy threshold speedup accuracy impact model capacity model capacity defines input format lstms affect computation evaluate impact model capacity technique conduct sensitivity analysis performance accuracy offs lstms model capacity hidden input apply technique representative benchmark babi due limit accuracy requirement technique achieve speedup hidden input increase accuracy loss speedup achieve technique varies slightly across hidden input model capacity trivial impact technique nlp task usually accuracy requirement user technique software hardware simulation impossible evaluate impact user replay program user pre output output accuracy response delay performance accord threshold scheme baseline scheme apply AO threshold scheme apply BPA threshold finally UO user orient scheme dynamically adjusts threshold individual user preference user input randomly recruit participant college campus multiple replay nlp application rate satisfaction unsatisfied satisfied output response delay participant rate replay application scheme accuracy threshold accuracy AO BPA imdb accuracy threshold accuracy AO BPA MR accuracy threshold accuracy AO BPA babi accuracy threshold accuracy AO BPA SNLI accuracy threshold accuracy AO BPA PTB accuracy threshold accuracy BPA AO MT performance accuracy offs threshold across application replay scheme random average user satisfaction scheme AO achieves user satisfaction baseline application response reduce user accuracy loss however BPA achieve user satisfaction user willing accuracy loss aggressive performance improvement finally UO scheme achieves user satisfaction scheme user preference consideration dynamically tune threshold excellent user overhead analysis inter optimization introduce lightweight computation performance overhead average intra optimization modify lstm computation introduce extra computation software performance overhead average hardware CTAs reorganization module mainly compose logic gate performance overhead gate simulation vii related WORKS multiple cnns optimization target cnns optimization mobile gpus others ASIC accelerator performance neural  conduct compression cnns via erase trivial execution efficiency aware matrix compression cnns proposes  compress cnns matrix eliminate explores node prune cnns execution lstms cnns applicable lstms rnns computation optimization propose scheme eliminate memory bandwidth pressure upload recurrent chip however optimization hardly implement mobile device limited chip storage mobile gpus eliminate redundant data access besides explore accelerator performance rnns execution focus mobile gpus flexible various application lstms configuration exploit optimization computation leverage computation characteristic application explore parallelism however none directly apply layer processing lstms explore parallelism inside lstm layer via analyze unique mathematical characteristic lstm computation conclusion nlp  become increasingly popular mobile device core machine technique rnns lstms continuously improve performance mobile gpus local processing become promising data transmission privacy issue induced centric computation  however lstms exhibit inefficient memory access execute mobile gpus due redundant data movement limited chip bandwidth propose optimization hierarchically explore memory friendly lstm mobile gpus achieve substantial improvement performance inter propose lstm layer reorganization technique greatly improve data locality across intra propose dynamic skip DRS technique conduct dynamic matrix compression propose technique achieve average upto performance improvement entire accuracy loss generally user imperceptible lstm execution mobile gpus optimization scalability increase input data user delivers excellent user