training neural network dnns eon without leverage distribute distribute training  spent communicate gradient network distribute training algorithm hierarchy worker aggregator node aggregator repeatedly gradient update allocate worker update reduce significant communication embed data compression accelerator network interface NICs maximize benefit network acceleration propose inceptionn network compute exchange training information neural network uniquely combine hardware algorithmic innovation exploit observation gradient significantly tolerant precision loss lend aggressive compression without complex mechanism avert loss exist training algorithm communicate gradient communication reduces opportunity network acceleration compression aggregator become bottleneck compression compress decompress multiple allocate worker propose lightweight  lossy compression algorithm float gradient exploit unique characteristic compression enables significantly reduce gradient communication practically loss accuracy complexity implementation hardware NIC maximize opportunity compression avoid bottleneck aggregator propose aggregator training algorithm exchange gradient communication worker collectively perform aggregation distribute manner without mathematics training algorithm leverage associative aggregation operator enables network accelerator apply compression communication prevent aggregator node become bottleneck demonstrate inceptionn reduces communication speedup conventional training achieve accuracy introduction distribute training driver constant advance neural network dnns significantly reduce training although distribute training  compute inter node communication proportional dnn worker node aggregator node hierarchical distribute training inceptionn distribute training algorithm conventional hierarchy hierarchical inceptionn distribute algorithm alexnet resnet consist MB MB moreover accelerator computation communication pronounce illustrate ofthe distribute training structure hierarchy worker aggregator node iteration aggregator node gradient update  communicate cumulative gradient upwards update downwards gradient dnns mega byte MB vgg impose significant communication load network reduce communication embed data compression accelerator network interface NICs simply purpose compression technique develop network accelerator compression limited gain due substantial hardware complexity latency overhead instead propose  dubbed inceptionn novel gradient compression technique network accelerator architecture gradient centric distribute training algorithm maximize benefit network acceleration develop inceptionn exploit observation gradient significantly amenable precision loss therefore lend aggressive compression without complicate mechanism alleviate loss exist training algorithm communicate gradient communication reduces opportunity compression network acceleration inceptionn network compute exchange training information neural network annual acm international symposium microarchitecture doi micro compression aggregator bottleneck compress decompress multiple data correspond  building upon observation inceptionn lightweight hardware friendly lossy compression algorithm float gradient compression exploit unique characteristic gradient mostly distribution peak tightly around zero variance observation focus compression float minimizes precision loss offering compression ratio moreover compression algorithm developed conscious implementation complexity enable hardware realization NIC seamless integration network accelerator exist networking software stack inceptionn apis interface accelerator traditional tcp IP network stack mpi framework although compress gradient effective benefit cannot fully utilized conventional distribute training algorithm pas gradient communication moreover aggregator burden compress decompress multiple tackle challenge inceptionn gradient centric aggregator training algorithm leverage algorithmic insight communicate gradient aggregation operator typically sum associative gradient aggregate gradually worker intuition pas partial aggregate worker circular manner contribution partial aggregate algorithm eliminates designate aggregator node conventional algorithm enables distribute node communicate gradient equally load aggregation opportunity compress gradient improve load balance node visually illustrates algorithm replaces leaf conventional worker aggregator hierarchy depicts algorithm replaces hierarchy node worker building distribute training algorithm depict organization combination lossy compression algorithm gradient NIC integrate compression accelerator gradient centric aggregator training algorithm construct stack inceptionn significantly alleviates communication bottleneck without affect mathematics dnn training demonstrate efficacy synergistically integrate aforementioned component dnn model alexnet vgg resnet inceptionn reduces communication speedup comparison conventional worker aggregator achieve accuracy II  training neural NETWORKS mathematics distribute training dnn training involves predictor input data yield prediction supervise training minimize loss function truth output prediction input data data groundtruth available training dataset iteratively epoch commonly optimization neural network gradient descent update direction loss function gradient denotes loss accumulate across sample hence update capture gradient descent training denote update gradient respectively parameter rate however contemporary datasets memory computer gpus popular datasets imagenet 0GB tackle challenge stochastic gradient descent emerge popular technique specifically randomly sample subset refer minibatch instead evaluate gradient entire dataset approximate sample assume parallelize training cluster partial datasets assign correspond worker node worker minibatch calculate local gradient aggregator node update aggregator node update worker node mathematical formulation avoids training data communicates gradient although gradient training data mega byte communicate communication distribute training building mathematical research development effort distribute dnn training distribute training algorithm hierarchical worker aggregator approach illustrate algorithm worker aggregator node construct leaf worker node compute local gradient non leaf node aggregator node calculate local gradient update update data worker data worker data worker data worker gradient aggregator aggregator aggregator worker aggregator approach distribute training model MB alexnet resnet vgg communication alexnet resnet vgg gradient percentage spent exchange training conventional worker aggregator approach worker node hierarchical reduction aggregator node effectively  networking aggregation workload distribute node significantly reduces data exchange perform intermediate aggregation however hierarchical approach aggregator node communicate worker node aggregate local gradient becomes communication computation bottleneck report exchange gradient communication training ofthe dnn model node cluster 0Gb ethernet connection instance per iteration alexnet MB data exchange gradient due data exchange training alexnet communication recent dnns resnet MB alexnet evaluation sec nonetheless complexity task recognition dnns complexity communication computation ratio becomes specialized accelerator deliver performance reduces computation node training  compression reduce communication overhead inceptionn aim develop compression accelerator NICs utilize conventional compression algorithm acceleration suboptimal complexity algorithm impose significant hardware latency overhead compression algorithm leverage algorithmic gradient significantly amenity aggressive compression accuracy trunc accuracy alexnet HDC impact float truncation gradient training accuracy alexnet handwritten digit classification HDC float truncation lsb mantissa exponent FP format truncation lsbs gradient mostly distribution peak tightly around zero variance characteristic motivate lossy compression gradient robustness training loss gradient gradient distribute training normally float whereas fix inference phase widely float compressible lossless compression algorithm instance google lossless compression algorithm snappy compression ratio increase overall spent training phase factor due compute overhead compression employ aggressive lossy compression exploit tolerance dnn training imprecise algorithm lossy compression compression ratio performance benefit lossless compression affect prediction inference accuracy dnns investigate perform lossy compression technique truncate significant lsbs lossy compression prediction accuracy alexnet handwritten digit classification HDC net truncation affect predictor accuracy significantly aggressive truncation  affect accuracy complex dnns alexnet phenomenon intuitive precision loss accumulate iteration gradient frequency iter gradient frequency iter gradient frequency iter distribution alexnet gradient training stage worker data sum worker data sum data worker sum data worker sum gradient worker organization worker worker worker worker partition transmit reduce TR blk blk blk blk gradient blk blk blk blk TR blk blk blk blk TR blk blk blk blk reduce SR blk blk blk blk SR blk blk blk blk SR distribute gradient exchange inceptionn gradient centric distribute training algorithm worker tightness dynamic gradient lossy compression algorithm leverage inherent numerical characteristic gradient mostly distribution peak tightly around zero variance demonstrate analyze distribution gradient phase training alexnet plot gradient throughout training phase distribution dnn model observation focus compression float algorithm minimizes precision loss lossy compression algorithm built upon gradient exclusively aim gradient however gradient communicate direction conventional distribute training update around direction therefore delve detail compression technique hardware discus training algorithm communicates gradient direction hence algorithm maximize benefit inceptionn network acceleration gradient IV gradient centric  training depicts worker organization inceptionn training algorithm algorithm designate aggregator node worker instead worker node maintains model model replica exchange aggregate subset gradient node iteration illustrates procedure algorithm worker node inceptionn evenly partition gradient vector blk blk blk blk worker node training iteration node load computes mini batch data generates local exchange subsequently inceptionn exchange aggregate phase node node cluster initialize model rate iteration load mini batch training data pas compute loss backward pas compute local gradient compress local gradient compress gradient exchange partition evenly node    node node   node gradient exchange decompress aggregate gradient decompress update algorithm inceptionn gradient centric distribute training algorithm worker node aggregation gradient worker sends blk node worker blk worker performs sum reduction blk blk worker concurrently happens across worker worker worker worker worker fully aggregate blk blk blk blk worker respectively propagation aggregate gradient worker sends blk worker worker blk blk concurrently happens across worker worker fully aggregate worker fully aggregate worker algorithm formally describes inceptionn training algorithm generalize arbitrary worker denotes sum reduction summary inceptionn training algorithm utilizes input precision FP output compress vector tag compression mechanism exponent mantissa compress error bound compress error bound shi shi  concat shi error bound error bound concat shi  compress concat shi  compress algorithm lossy compression algorithm precision float gradient network bandwidth worker evenly unlike  approach communication bottleneck furthermore algorithm performs computation aggregate gradient across worker decentralize manner avoid computation bottleneck node lastly inceptionn algorithm efficiently implement popular distribute compute algorithm allreduce   compression algorithm elaborates procedure compress float gradient compress vector tag compression mechanism algorithm described standard float representation split exponent mantissa algorithm chooses compression mechanism compress compress error bound compress gradient error bound aggressive approach preserve precision simplest approach truncate lsb mantissa however approach limit maximum obtainable compression ratio msb exponent affect precision significantly truncate mantissa increase instead approach exponent compress vector normalize essentially input therefore multiplicand decompress encode information concatenate msb shift truncate lsb shift vector msb input compress vector tag compression mechanism output precision FP compress compress compress shi loc rom msb concat shi compress shi loc rom msb concat shi shi concat algorithm decompression algorithm consequently compression algorithm compress vector tag compression mechanism decompression algorithm describes decompression algorithm compress vector tag compress compress decompress output simply zero respectively compress compress reconstruct float obtain distance msb multiplicand exponent compression distance calculate distance obtain shift distance pad lsbs zero truncate compression concatenate float return decompression output VI network acceleration gradient compression apply compression algorithm significantly reduce amount data exchange node inceptionn goal reduce training although researcher machine impact software lossless snappy lossy  compression algorithm training alexnet HDC denotes baseline without compression truncation lsbs tcp IP stack dma NIC driver network application pci express gen packet dma SC CS compression decompression virtual fifo WR RD WR RD 0G ethernet IPs 0G ethernet IPs gth transceiver gth transceiver network host cpu fpga chip fpga overview NIC architecture integrate compressor decompressor community propose compression algorithm report training evaluate compression ratio impact compression training accuracy directly compression algorithm software reduce communication burden computation resource seriously increase computation specifically compression algorithm CPUs gpus cannot efficient manipulation pack float CPUs prior gpus throughput compression ratio snappy training increase factor lossless snappy lossy  compression algorithm lossy truncation operation significantly increase computation simply pack unpack significantly burden CPUs considerably negates benefit reduce communication slightly decrease training therefore reduce communication computation hardware compression inceptionn accelerator architecture integration NIC NIC architecture evaluate implement accelerator xilinx VC evaluation 0Gbps network connectivity along programmable logic insert accelerator within NIC reference illustrates integration compression decompression output traffic reference packet dma network data host pcie link packet compression compress data virtual FIFOs 0G ethernet MACs MACs ethernet phys data network input traffic ethernet MACs data phys virtual FIFOs packet FIFOs decompression processing passing packet dma transfer cpu standard axi bus interact module although hardware acceleration compression decompression algorithm straightforward integration within NIC challenge algorithm devise float NIC tcp IP packet hence accelerator customize transparently tcp IP packet furthermore compression lossy NIC abstraction enables software activate deactivate lossy compression per packet basis discus hardware integration VI elaborates software abstraction compression interfere regular packet compress compression identify packet intend lossy compression extract payload compress  packet compression packet burst axi interface deliver cycle packet burst granularity avoid  processing bandwidth NIC software api packet compressible service tos header tos load burst compression performs sequence burst identifies compressible packet tos compression bypass compression compress header compression burst payload arrives depicts architecture compression hardware payload burst compression equip compression CBs performs compression described algorithm CB CB CB concat concat CB CB concat CB CB concat concat CB CB concat concat compression concat concat fifo alignment fifo mask fifo burst compressor architecture tag decoder burst buffer concat decompression DB DB DB DB DB DB DB DB fifo fifo burst decompressor architecture variable output align vector binary shifter align vector tag CBs simply concatenate vector finally align vector tag vector concatenate output compression burst compression variable vector therefore align vector transfer burst via axi interface alignment accumulates series compress vector output burst decompression compression decompression packet burst granularity identifies packet compress sequence tos burst packet identify incompressible burst header decompression bypass payload burst compressible packet fed decompression hardware architecture delineate compress burst contains FP overlap consecutive burst decompression reading burst insufficient proceed decompression therefore decompression burst buffer maintains burst burst buffer obtains burst tag tag decoder calculate compress vector compress vector obtain buffer compress vector variable compress vector compress vector tag vector fed decompression DBs decompression executes decompression algorithm described algorithm decompression simply concatenates output DBs transfer via axi interface cycle burst buffer shift away consume burst burst dnn training application user OpenMPI application api networking api kernel NIC hardware application packet  comm comp packet tos  compression  comm packet network subsystem dataflow across software stack NIC hardware consume burst apis lossy compression gradient mention previously identify context tcp IP packet utilize tos IP header tos header tcp IP packet prioritize tcp IP tag packet compress decompress reserve tos socket connection  function tos update demonstrates tag tcp IP packet compress decompress OpenMPI framework scenario dnn training application networking application server properly tag tcp IP packet compression decompression introduce mpi collective communication comp specialized mpi collective communication api implement inceptionn algorithm described without compression default mpi collective communication apis mpi collective communication comp propagates variable OpenMPI networking apis tos option correspond tcp socket communication modify linux kernel network stack packet tos NIC regular tcp packet inside NIC comparator tos incoming packet tos packet compression otherwise perform compression outgo packet receiver node NIC comparator incoming packet tos perform decompression packet otherwise packet regular ethernet packet directly processor reception vii methodology dnn model enumerates evaluate dnn model hyper parameter training alexnet alexnet cnn model image classification consists convolutional layer fully layer rectify linear relu activation function fully layer dropout layer apply model alexnet hyperparameters benchmark MB training imagenet dataset handwritten digit classification HDC HDC dnn model compose fully layer performs handwritten digit recognition dimension hidden layer model MB dataset mnist contains training image digit resnet resnet dnn model image classification task variant layer popular variant resnet contains convolution layer fully layer network resnet model MB imagenet dataset vgg vgg another cnn model image classification consists convolutional layer fully layer vgg imagenet dataset model MB distribute dnn training framework develop custom distribute training framework nvidia cuda intel math kernel library mkl OpenMPI inceptionn implement publicly release dnn training framework tensorflow however custom distribute execution framework amenable integration software hardware implementation lossy compression algorithm custom training framework computation dnn training backward propagation perform gpu cpu compatible communication handle via OpenMPI apis besides framework implement diverse distribute training architecture communication algorithm various OpenMPI apis exchange gradient measurement hardware setup cluster node equip nvidia titan XP gpu intel xeon cpu 6GHz 2GB ddr xilinx VC implement 0Gb ethernet reference along compression decompression accelerator employ additional node aggregator conventional worker aggregator approach extend cluster node evaluate inceptionn scalability perform node cluster due limited resource node   0Gb ethernet switch network architecture II detailed breakdown training benchmark worker aggregator node cluster measurement iteration training datacenter internet google facebook 0Gbps network connection within rack 0Gbps connection oversubscribed link rack switch server training application rack switch 0Gbps network connection furthermore compression decompression accelerator affect operating frequency mhz bandwidth successfully demonstrate functionality modify NIC driver OpenMPI apis distribute training framework concurrently node cluster performance evaluation discover 0Gb ethernet reference implement xilinx VC achieve due inefficiency driver intel XT 0Gb ethernet NICs training communication deploy hardware compression intel XT NIC baseline measurement communication deploy hardware compression breakdown communication driver NIC hardware TX RX link NICs xilinx VC intel XT 0Gb ethernet NICs TX RX link intel NIC compression ratio correspond iteration calculate communication accounting compression decompression evaluation performance improvement inceptionn implement conventional worker aggregator training algorithm cluster worker aggregator reference II detailed breakdown training alexnet HDC resnet vgg cluster report absolute normalize iteration training irrespective dnn model II spent local computation backward propagation update communication clearly indicates communication bottleneck training reference WA inceptionn inc iteration epoch without apply compression training training norm alexnet HDC resnet vgg WA WA inc inc WA WA inc inc WA WA inc inc WA WA inc inc comparison training worker aggregator approach wax inceptionn  without hardware compression NICs WA denotes  baseline without compression WA denotes WA integrate compression gradient communication error bound inc denotes inceptionn baseline without compression inc denotes compression error bound training cluster worker  aggregator wax measurement training iteration breakdown computation communication cluster without compression inceptionn training algorithm shorter training worker aggregator algorithm alexnet HDC resnet vgg respectively due reduction communication comparison reference intuitively inceptionn  remove bottleneck link enables concurrent utilization link node besides balance gradient exchange contributes reduction computation gradient summation node distribute manner whereas worker aggregator algorithm burden designate aggregator node perform aggregation gradient  furthermore training reference inceptionn equip gradient compression WA inc conventional worker aggregator approach benefit compression reduction communication baseline WA although direction communication applicable compression  inceptionn algorithm maximize compression opportunity inceptionn hardware compression inc communication conventional worker aggregator baseline WA inceptionn baseline inc respectively therefore inceptionn inc demonstrates training speedup conventional approach WA model epoch inceptionn compression accuracy accuracy loss gradient due lossy compression affect accuracy prolong training necessity epoch converge speedup WA alexnet HDC resnet vgg speedup inceptionn conventional approach achieve accuracy notation lossless baseline accuracy understand lossy compression accuracy prolong training conventional worker aggregator WA inceptionn inc model converge accuracy epoch speedup inceptionn conventional training achieve accuracy modest epoch accuracy inceptionn speedup vgg alexnet convention approach performance sec furthermore extra training epoch essential without accuracy incur evaluation inceptionn compression algorithm compression ratio various lossy compression scheme impact compression relative prediction accuracy dnns training algorithm epoch specifically evaluate truncation lsbs average compression ratio alexnet HDC resnet vgg inc inc inc relative accuracy alexnet HDC resnet vgg comparison compression ratio impact prediction accuracy dnns inceptionn training algorithm various lossy compression scheme accuracy epoch training without extra epoch model denotes baseline without compression denotes absolute prediction accuracy truncation lsbs inc inceptionn lossy compression error bound bitwidth distribution compress gradient compress gradient compose indication tag compress data gradient inceptionn compression absolute error bound ıve truncation float constant compression ratio suffer substantial accuracy loss degradation prediction accuracy resnet due compression error introduce ıve truncation uncontrolled moreover potential truncation limited mantissa perturb exponent significant loss accuracy dnns truncation suitable simpler dnns HDC suitable complex dnns alexnet vgg resnet contrast lossy compression compression ratio preserve training quality truncation complex dnns error induced compression algorithm average compression ratio boost relaxation error bound relaxed error bound almost benchmark demonstrate compression ratio accuracy dnns degrade slightly absolute accuracy slight accuracy incurs dnns epoch lossless baseline easily compensate negligible extra epoch training sec understand significant gain compression algorithm analyze bitwidth distribution compress gradient report statistic error bound evaluate model algorithm compress gradient vector error bound gradient compress vector leverage unique gradient lossy compression algorithm achieves significantly compression ratio purpose compression algorithm lastly compression ratio gradient necessarily proportional reduction communication compression error bound compress communication scalability inceptionn training algorithm inc conventional worker aggregator algorithm WA worker node gradient exchange consists gradient communication gradient summation normalize node WA factor reduce packet network stack overhead network packet header remains consequently relaxed error bound marginally additional reduction overall communication scalability evaluation inceptionn training algorithm evaluate scalability inceptionn training algorithm extend cluster worker node gpus available disposal gradient exchange scalability gradient exchange consists gradient communication gradient summation metric scalability evaluation communication summation overhead node consume dnn training pas backward pas update constant due local computation gradient exchange inceptionn baseline inc worker aggregator baseline approach WA without compression across worker node gradient exchange increase almost linearly worker node WA cluster however remains almost constant inceptionn cluster training model alexnet vgg resnet network bandwidth bottleneck phenomenon intuitive WA cluster communication summation load congest aggregator node inceptionn approach balance load distribute evenly worker node analytically adopt communication model gradient exchange WA cluster worker network link latency model byte byte transfer byte sum reduction distribute dnn training negligible due model limited network bandwidth equation explains clearly conventional WA approach scalable increase node gradient exchange linear cluster contrast communication balance inceptionn gradient exchange cluster cancel inceptionn scalable IX related acceleration ML leverage specialized accelerator machine concentrate inference phase inceptionn specifically aim accelerate training phase google proposes tpu accelerator systolic array architecture inference neural network microsoft unveiled brainwave multiple FPGAs dnn inference eyeriss accelerator cnn inference compute spatial array reconfigurable multicast chip network cnns maximize data reuse inference phase target ML acceleration community recently acceleration training phase   ASIC fpga accelerator training phase offering performance efficiency gpus widely purpose processor ML training google tpu  tpu capable accelerate training computation google distribute machine framework tensorflow  distribute accelerate ML training multiple fpga ASIC accelerator others focus acceleration neural net training approximate arithmetic fpga ML training accelerator node accelerator deployed centralize training worker aggregator approach inceptionn decentralize gradient training efficient network gradient compression accelerator distribute training algorithm propose worker aggregator framework distribute training net approach reduce communication compute node specifically explore approach exchange nonzero leverage sparsity matrix secondly adopt cache approach reduce transmit cache repeatedly compute node deployed approach randomly selectively skip approach assume indeed sparse framework update maintains centralize node contrast gradient centric framework exchange gradient compute node update exploit observation gradient tolerant aggressive lossy compression consequently framework efficiently dense without notably compromise accuracy training procedure recently divert  architecture developed reduction approach specifically instead worker directly communicate aggregator gradient reduce employ topology despite topology central remains vector importantly communication paradigm transmit gradient central broadcast aforementioned algorithm hogwild  ssp worker aggregator approach perform asynchronous update gradient training reduce synchronization overhead stale gradient update significantly rely centralize aggregator gradient reduction technique series proposes technique gradient reduction quantization technique gradient algorithmic reduce gradient precision preserve training capability gradient compression complementary approach reduces amount communication skip communication gradient iteration communicate gradient locally accumulate gradient exceeds threshold worker aggregator distribute training propose network acceleration compression conclusion communication significant bottleneck distribute training community address challenge offering algorithmic innovation employ networking fabric however lack  considers aspect interconnection infrastructure tailor distribute training inceptionn initial direction hardware algorithm network accelerator lossy compression gradient maximize benefit introduce  distribute training demonstrate inceptionn reduces communication speedup conventional training achieve accuracy