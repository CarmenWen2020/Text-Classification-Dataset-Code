neural network dnn emerge important popular artificial intelligent AI technique growth model efficiency challenge underlie compute platform model compression becomes crucial however approach limited various drawback specifically network sparsification approach suffers irregularity heuristic index overhead recent structure matrix approach CIRCNN limited relatively complex arithmetic computation fft flexible compression ratio inability fully utilize input sparsity address drawback proposes PERMDNN novel approach generate execute hardware friendly structure sparse dnn model permute diagonal matrix unstructured sparsification approach PERMDNN eliminates drawback index overhead  compression consume retrain circulant structure impose approach PERMDNN enjoys benefit reduction computational complexity flexible compression ratio arithmetic computation utilization input sparsity propose PERMDNN architecture multi processing PE fullyconnected FC layer target compute entire architecture highly scalable flexible hence application model configuration implement PE CMOS technology EIE PERMDNN achieves throughout efficiency efficiency workload CIRCNN PERMDNN achieves throughput efficiency index model compression VLSI introduction resurgence hinton seminal neural network emerge important powerful artificial intelligence AI technique thanks availability  abundant training data significant advance computer processing neural network dnns deliver accuracy task  deng  liao contribute equally  deng  liao xie yuan  demand intelligence recognition recognition processing etc extraordinary performance dnns respect accuracy mainly attribute model theoretical analysis empirical simulation model improve overall representation capability dnn model classification predication accuracy model motivate encourage finding dnns purpose tackle complicate task accuracy perspective hardware continuous growth dnn model efficiency challenge underlie compute platform chip SRAM usually limited dnn model chip dram SRAM  inevitable choice consequently incur frequent access dram efficient deployment dnn challenge improve efficiency efficient model compression emerge active topic AI research community dnn compression technique approach promising network sparsification popular ofthe strategy balance compression ratio accuracy date propose perform efficient sparsification dnn model echo importance popularity approach sparse model orient hardware architecture propose however network sparsification suffer inherent drawback irregularity heuristic index overhead consequently despite encourage compression ratio unstructured sparse dnn model cannot achieve optimal performance compute platform dnn hardware accelerator essentially inefficient execution hardware implementation annual acm international symposium microarchitecture doi micro unstructured sparse matrix permute diagonal matrix representation conventional unstructured sparse matrix permute diagonal matrix non hardware friendly sparse model overcome drawback irregularity alternative approach directly network structure matrix notable recent CIRCNN circulant matrix partition matrix circulant sub matrix wij due circulant structure instead sub matrix calculation  perform ifft fft wij fft denotes wise multiplication wij wij utilize fft multiplication simultaneously reduces computational storage accuracy loss due hardware friendly model representation CIRCNN demonstrate efficient hardware implementation performance however limited relatively complex arithmetic computation fft flexible compression ratio inability fully utilize input sparsity propose PERMDNN novel approach generates executes hardware friendly structure sparse dnn model permute diagonal matrix illustrate permute diagonal matrix structure sparse matrix non zero entry diagonal permute diagonal matrix dnns format multiple permute diagonal matrix permute diagonal matrix inherent structure sparsity benefit practical deployment specifically eliminates index overhead brings non heuristic compression enables training model generation impose structure construction matrix PERMDNN advantage CIRCNN simpler arithmetic computation unlike CIRCNN inherently complex operation complex multiplication addition fft PERMDNN purely arithmetic thereby hardware compression ratio significantly flexibility PERMDNN hardware freely permute diagonal matrix CIRCNN hardware limited circulant matrix fft hardware PERMDNN flexible choice compression ratio utilization input sparsity PERMDNN fully utilize dynamic sparsity input vector CIRCNN cannot CIRCNN frequency domain input vector lose important domain sparsity allows PERMDNN achieve additional improvement throughput efficiency CIRCNN PERMDNN develop training scheme generate accuracy permute diagonal matrix dnn model scratch develop correspond complexity inference scheme executes efficiently structure sparse dnn model experimental datasets application task enforce structure permute diagonal matrix dnns achieve sparsity ratio negligible accuracy loss accelerate inference propose PERMDNN architecture performance permute diagonal matrix inference target fully FC layer dnn model PERMDNN architecture fully reap benefit permute diagonal matrix structure unlike EIE dnn architecture target FC layer PERMDNN architecture incur index overhead load imbalance due irregularity conventional unstructured sparse dnn model thereby significant improvement hardware performance array processing PEs PERMDNN architecture elastic scalable architecture adapt dnn model component permute diagonal matrix PEs allows architecture deployed various application scenario requirement throughput demonstrate advantage PERMDNN architecture implement PE CMOS technology operating 2GHz frequency PERMDNN implementation consumes meanwhile equip multiplier PE processing PE PERMDNN achieves  compress dnn model approximately corresponds TOPS uncompressed network EIE PERMDNN achieves throughout efficiency efficiency workload CIRCNN PERMDNN achieves throughput efficiency II motivation importance FC layer target architecture EIE PERMDNN customize dnn architecture target FC layer specifically PERMDNN accelerates sparse matrix vector multiplication kernel computation FC layer justify importance FC layer argument FC layer spectrum application scenario summarize besides convolutional neural network cnn computer vision task FC layer dominant layer dnns recurrent neural network rnn multi layer perceptrons mlp recognition processing task therefore optimize FC layer universally benefit spectrum AI task industrial efficient FC layer target dnn architecture reveal google seminal tpu workload google datacenters FC layer dnns rnns MLPs therefore research effort FC layer currently active effort convolutional conv layer finally optimize FC layer target architecture improve overall performance conv layer model cnns analyze conv  architecture execute FC layer noticeable performance degradation realize efficient architecture cnns conv FC layer specialized optimizes FC layer preferable strategy maximize hardware performance dnn model practical AI application dnn model component layer computer vision cnn conv layer FC layer recognition rnn mlp FC layer processing rnn mlp FC layer drawback unstructured dnn sparsification due memory requirement inherent redundancy FC layer usually compress network sparsification heuristic prune conduct regularization downside approach unstructured sparse model friendly implement dnn hardware accelerator structure generate sparse dnns usually highly irregular exhibit weak regularity incurs significant computation overhead due index irregular sparse matrix instance EIE virtual tag actual additional relative entire matrix therefore overall storage actually instead significantly limit achievable performance compression ratio dnn sparsification typically heuristic inherent neuron prune regularization sparsification usually uncontrollable unpredictable consequently FC rnn specifically component matrix rnn scenario compression pre define precisely compression ratio satisfy specification becomes challenge moreover exist dnn sparsification approach introduce training overhead operating pre dense model neuron prune retrain perform iteratively ensure accuracy dense model achieve sparse model clearly consume sometimes longer sparse model dense model instance report compression rate pre alexnet model iteration prune respectively training pre model iteration drawback circulant matrix compression alternative approach achieve network compression directly network structure matrix CIRCNN utilizes mathematical circulant matrix compress dnn model efficiently implement hardware avoid irregularity CIRCNN suffers drawback arithmetic operation training inference algorithm CIRCNN fft computation inherently involve complex multiplication addition unfortunately arithmetic operation complex incur counterpart instance complex multiplication multiplication addition moreover lack flexibility compression ratio compression ratio CIRCNN component circulant matrix however determines fft facilitate fft hardware mostly fft compression ratio layer CIRCNN obviously restriction severely limit potential application CIRCNN finally loses opportunity utilize input sparsity utilize dynamic input sparsity important technique reduce consumption computational dnn hardware however CIRCNN input vector frequency domain abundant sparsity domain completely lose prevents CIRCNN utilize important input sparsity performance improvement PERMDNN ALGORITHMS  PERMDNN representation address drawback sparsification structure matrix propose PERMDNN structure sparse dnn representation specifically enforce matrix non fft complex hardware fft dnn model consist multiple permute diagonal submatrices non zero entry permute diagonal matrix diagonal permute diagonal matrix layer PERMDNN contains permute diagonal sub matrix later permute diagonal matrix exhibit spatial structure brings significant reduction computational inference training scheme PERMDNN representation PERMDNN develop computation procedure backward propagation inference training assume matrix fullyconnected layer PERMDNN permute diagonal matrix consists multiple permute diagonal sub matrix sub matrix permutation parameter index sub matrix denote permutation parameter permute diagonal sub matrix arbitrary entry wij wij  mod otherwise mod mod  vector contains non zero entry propagation inference recall propagation inference phase FC layer perform activation function addition input output vector FC layer respectively accordingly permute diagonal matrix entry eqn calculation computation propagation simplify  mod storage vector contains non zero entry inference phase calculation significantly simplify computation  backward propagation training FC layer impose permute diagonal structure ensure FC layer PERMDNN exhibit permute diagonal structure iteration training phase recall propagation FC layer perform gradient calculation backward propagation perform wij loss function neural network accord zero pad extra overhead pad zero involve computation storage bias combine simplicity principle backpropagation layer backward propagate previous layer therefore equation update FC layer PERMDNN derive wij wij wij wij mod rate eqn calculation aid computation eqn layer backward propagate previous layer update scheme described  theoretically guarantee sparse network exhibit permute diagonal structure hence attractive training extension convolutional layer propagation inference conv layer impose permute diagonal structure matrix FC layer generalize apply tensor conv layer recall tensor conv layer 4D tensor macro matrix entry filter kernel therefore illustrate permute diagonal structure impose input channel output channel dimension tensor FC layer propagation inference conv layer permute diagonal matrix described input output tensor convolutional layer respectively width height input kernel output tensor respectively input channel output channel addition mod permute diagonal tensor conv layer backward propagation training conv layer FC layer ensure convolutional layer PERMDNN exhibit permute diagonal structure training phase correspond update procedure derive training procedure FC layer update propagation conv layer described mod eqn calculation aid computation eqn layer backward propagate previous layer update scheme described  theoretically guarantee sparse network exhibit permute diagonal structure accuracy compression ratio leverage backward propagation scheme  PERMDNN model scratch II task performance compression ratio PERMDNN model datasets FC conv layer permute diagonal PD matrix tensor compression ratio layer detail experimental described alexnet permute diagonal matrix FC layer FC FC FC FC layer stanford neural machine translation NMT stack lstm model lstms FC matrix lstm FC layer resnet conv layer without filter kernel conv layer filter kernel resnet widen parameter model conv layer without filter kernel conv layer filter kernel selection permutation via index random index simulation difference task performance II index instance permute diagonal matrix II impose permute diagonal structure dnn model enables significant reduction storage requirement FC conv layer meanwhile correspond task performance accuracy computer vision bleu translation retain exhibit negligible degradation PERMDNN model achieve compression ratio network spatial network structure simultaneously preserve task performance correspond  architecture IV attractive II alexnet imagenet PD permute diagonal alexnet PD matrix FC FC FC acc compression overall FC layer float MB float PD MB fix PD MB stanford NMT FC layer lstms  english  translation stanford NMT FC layer lstms PD matrix FC layer bleu compression overall FC layer float MB float PD MB fix PD MB IV resnet cifar resnet PD tensor conv layer acc compression overall conv layer float MB float PD layer MB fix PD layer MB resnet cifar resnet PD tensor conv layer acc compression overall conv layer float MB float PD layer MB fix PD layer MB outline theoretical proof universal approximation CIRCNN universal approximation  matrix neural network theoretically effectiveness circulant matrix discover PERMDNN exhibit universal approximation thereby rigorous foundation propose permute diagonal  detail proof individual technical report subsection brief outline proof connectedness PERMDNN thanks unique permute diagonal structure identical permute diagonal matrix sparse connection adjacent permute diagonal layer away information neuron previous layer function achieve permute FC lstm component matrix diagonal network dense finally   theorem exists permute diagonal neural network closely approximate target continuous function define compact approximation error thereby universal approximation permute diagonal network besides derive error bound approximation error model parameter consequently existence universal approximation PERMDNN theoretically guarantee effectiveness dnn application applicability pre model besides training scratch permute diagonal matrix network model obtain pre dense model illustrates correspond procedure consists permute diagonal approximation training tune dense matrix tensor convert permute diagonal matrix tensor via permute diagonal approximation mechanism permute diagonal approximation convert non permute diagonal matrix tensor permute diagonal format entry desire permute diagonal mathematically approximation optimal approximation norm measurement approximation error convert model already exhibit permute diagonal structure tune  finally obtain accuracy permute diagonal network model generate approach apply pre model accuracy instance pre dense lenet model mnist dataset conv layer FC layer finally convert permute diagonal network training achieves accuracy overall compression ratio without quantization PERMDNN pre dense model PERMDNN unstructured sparse dnn exist network sparsification approach propose PERMDNN enjoys attractive advantage PERMDNN hardware friendly model illustrate due inherent regular structure permute diagonal matrix non zero entry calculate modulo operation modulo circuit lsb thereby completely eliminate index entry perspective hardware elimination permute diagonal matrix PERMDNN completely avoids computation overhead incur complicate index address sparse dnn accelerator EIE hence achieves significant reduction requirement computational dnn implementation PERMDNN controllable adjustable compression acceleration scheme reduction model arithmetic operation longer heuristic unpredictable instead precisely deterministically adjust benefit explore explore tradeoff hardware performance accuracy finally PERMDNN enables training preserve accuracy structure sparse matrix PERMDNN pre model designer initialization stage training training algorithm preserve fix structure entire structure sparse network scratch completely avoid increase complexity incur extra iterative prune training conventional unstructured sparse dnn training scheme storage requirement comparison PERMDNN  PERMDNN  structure matrix PERMDNN suffer drawback  VI importantly PERMDNN simpler arithmetic computation complex computation  computation PERMDNN purely thereby significantly arithmetic  workload compression ratio PERMDNN allows compression ratio flexibly adjust  computation PERMDNN restriction component permute diagonal matrix therefore hardware architecture PERMDNN compression ratio PERMDNN attractive application finally fundamental distinction PERMDNN fully utilize input sparsity computation PERMDNN domain important input sparsity leveraged PERMDNN reduce computational consumption sparsity input vector widely exists numerous application benefit greatly expands advantage PERMDNN  VI advantage PERMDNN CIRCNN CIRCNN PERMDNN arithmetic operation complex flexible compression utilize input sparsity IV PERMDNN architecture develop PERMDNN architecture propose structure algorithm exist dnn accelerator PERMDNN architecture inference task data mapping processing scheme propose compute scalable array processing PEs PE performs propagation matrix PEs perform independent parallel computation maximize processing throughput illustrates partition scheme permute diagonal matrix permute diagonal  matrix belonging PE arrangement non zero entry sub matrix PE associate SRAM therefore entire matrix FC layer distribute manner across multiple SRAM enable parallel processing multiple PEs besides fully utilize potential dynamic sparsity input vector activation vector output previous layer entire PERMDNN adopts  processing style specifically illustrate cycle PEs perform multiplication non zero entry correspond vector matrix accumulate update intermediate correspond entry output vector non zero entry PEs perform multiplication non zero entry calculate entry output vector available simultaneously obviously adopt  processing scheme computation involve zero entry completely skip thereby significant consumption exists non negligible sparsity PE multiple permute diagonal matrix along direction overall architecture data mapping processing scheme overall architecture PERMDNN compute entire consists array NP PEs perform kernel operation nonlinear activation operation entry output activation vector FC layer calculate PEs activation SRAM operation perform manner entire activation SRAM partition  SRAM NACC NP PEs cycle PEs belong SRAM output  activation correspond SRAM  width activation SRAM width respectively  simultaneously activation SRAM cycle activation rout network ensure correctly target activation SRAM reading phase activation SRAM described IV non zero activation fetch broadcast PEs achieve signal controller activation selector multiple activation SRAM examination  non zero activation fifo broadcast PE array purpose activation fifo backlog non zero thereby ensure PEs computation processing inner architecture PE PE equip SRAM non zero entry permute diagonal matrix EIE lookup lut PE strategy SRAM PE virtual tag cluster actual decode lut NMUL actual input accumulator cycle PE equip NACC accumulator corresponds NACC matrix PE NMUL accumulation selector ensure rout target accumulator specifically elaborate detail later permutation parameter correspond permute diagonal sub matrix enable selection selector associate accumulator accumulator contains NACC NMUL accumulator target accumulator input PEs processing matrix calculate accumulator activation  wise processing procedure input zero skip scheme overall architecture PERMDNN hardware activation reconfigured rectify linear relu  function tanh application inner architecture PE SRAM reduce consumption SRAM SRAM PE multiple SRAM sub adopt strategy cycle SRAM sub others disabled accordingly achieve utilization multiplier width SRAM sub ensure SRAM sub contains NMUL entry besides accommodate wise processing scheme permute diagonal structure matrix non zero specially SRAM PE illustrates data allocation SRAM sub permute diagonal matrix SRAM sub non zero entry matrix adopt transpose data layout access SRAM sub fetch data wise data processing permute diagonal sub matrix non zero entry PE receives non zero thereby completely eliminate risk load imbalance prior suffer data allocation SRAM accumulation selector accumulator NACC typically multiplier NMUL PE accumulation selector route output multiplier target accumulator accumulation selector consists index calculator array reconfigurable comparators multiplexer index calculator calculate index non zero entry permute diagonal sub matrix PE specifically due unique structure permute diagonal matrix calculation essentially modulo operation sum permutation index permute diagonal matrix hence easily implement circuit consist width adder  comparator ceil ceil ceiling function index calculate accumulator target accumulator identify array comparators multiplexer update output multiplier inner architecture accumulation selector permutation SRAM permutation  accumulation selector permutation SRAM SRAM activation SRAM PE permutation SRAM consists instead multiple partition addition permutation SRAM contains multiple permutation computation accumulation selector width permutation typically width permutation SRAM flexibility network model PE PERMDNN flexibility flexible PE involves consideration factor NACC accumulator PE NMUL multiplier PE nrow NP PE computation scheme PE nrow pNMUL NACC nrow NACC nrow PE sufficient register calculate meanwhile cycle PE NMUL permute diagonal sub matrix identifies NMUL non zero entry pNMUL therefore pNMUL comparators PE activate identify index non zero entry accumulation selector NACC NMUL comparators correspond multiplexer cycle therefore denote nrow pNMUL nrow mod pNMUL cycle cycle comparators multiplexer activate identify target register update comparators selector accumulator activate sequentially nrow pNMUL NACC nrow PEs hardware resource continuously matrix cycle fully instance assume PEs NMUL NACC matrix cycle wise processing continuous nrow pNMUL NACC nrow nrow pNMUL exist integer satisfies pNMUL NACC pNMUL therefore cycle sequential activation described cycle inactivate accumulation selector accumulator sufficient future processing accordingly illustrate overall computation scheme partially calculate accumulator allocate previously calculate release perform unfinished computation procedure nrow pNMUL rare PEs properly ensure multiplier permute diagonal sub matrix occurs sparse model PEs become redundant processing scheme PEs multiple simultaneously consequently strategy benefit increase overall throughput equivalently improve nrow processing thereby transform aforementioned evaluation experimental methodology simulation CAD developed cycle accurate accurate simulator model functional behavior PERMDNN architecture simulator serf golden reference correctness verilog implementation verify rtl model via validate output output simulator synthesize synopsis compiler CMOS library switch activity extract simulation annotate toggle rate gate netlist route synopsis IC compiler generate layout consumption estimate prime PX consumption SRAM SRAM activation SRAM permutation SRAM estimate report cactus benchmark evaluation PERMDNN model FC layer described specifically EIE evaluate FC layer compress sparse model individually layer computation scheme computation scheme computation scheme PE PERMDNN NMUL NACC matrix layout PE CMOS technology workload accordingly information benchmark layer constant sparsity ratio statistically calculate activation sparsity ratio vii configuration hardware performance configuration configuration parameter PE overall PERMDNN sparsity ratio sparsity matrix NMT model vii information evaluate FC layer layer activation description alex FC cnn model image classification alex FC alex FC NMT rnn model translation NMT NMT compute PE equip multiplier accumulator addition PE KB SRAM KB permutation SRAM SRAM strategy ensure architecture application instance strategy PE PERMDNN compute compress layer parameter compress vgg FC layer similarly activation SRAM capacity facilitate execution model illustrate activation SRAM KB corresponds vector usually sufficient FC layer practical model hardware performance IX breakdown PE overall PERMDNN compute PE occupies consumes overall PE compute occupies consumes PE array resource consume regard throughput equip PEs configuration parameter PE parameter multiplier amount NMUL width accumulator amount NACC width  SRAM sub amount width depth permutation SRAM width depth PERMDNN compute parameter amount PEs NP quantization scheme strategy pipeline stage activation SRAM amount  width  depth activation fifo width depth 2GHz PERMDNN compute achieve  compress dnn model equivalent processing dense model varies compression ratio adopt pessimistic conversion scheme assume sparsity activation sparsity consequently equivalent application throughput PERMDNN achieves TOPS conversion scheme conservative instance EIE adopts conversion scheme assume sparsity input sparsity optimistic IX breakdown component PE breakdown memory register combinational network filler PERMDNN compute breakdown PEs others comparison EIE CIRCNN subsection PERMDNN relevant compress dnn orient architecture EIE CIRCNN reference PERMDNN EIE FC layer target dnn architecture category PERMDNN CIRCNN imposes structure matrix dnn gain improve hardware performance comparison EIE summarizes hardware parameter performance EIE PERMDNN comparison adopts strategy quantization scheme EIE configuration EIE technology node adopt strategy EIE project EIE technology comparison EIE PERMDNN EIE PERMDNN PEs CMOS tech report project clk freq mhz memory SRAM SRAM quantization hardware performance EIE PERMDNN execute FC layer alexnet speedup efficiency efficiency project EIE PERMDNN achieves throughout efficiency efficiency benchmark layer analyze improvement mainly due elimination unnecessary storage index inconvenient address calculation load imbalance comparison CIRCNN XI summarizes hardware performance CIRCNN PERMDNN comparison project CIRCNN technology CIRCNN report synthesis XI performance characteristic PERMDNN synthesis report meanwhile CIRCNN throughput information PERMDNN CIRCNN overall throughput TOPS efficiency TOPS XI PERMDNN achieves throughput efficiency CIRCNN PERMDNN enjoys benefit utilize input sparsity arithmetic computation architecture PERMDNN CIRCNN implementation individual configuration compression ratio SRAM multiplier etc contribution advantage overall performance improvement analyze roughly utilization dynamic input sparsity linear reduction computational factor affect throughput efficiency compression ratio frequency multiplier PERMDNN CIRCNN input sparsity PERMDNN enable increase throughput efficiency simplicity arithmetic computation recall projection linear frequency quadratic constant accuracy benchmark layer EIE PERMDNN evaluate complex multiplication consists multiplication addition perform inference compress matrix permute diagonal matrix approach multiplication circulant matrix complex multiplication wise multiplication phase plus  constant complex multiplication fft ifft phase therefore compression ratio arithmetic computation roughly reduction computational translate significant improvement hardware performance scalability matrix FC layer grows easy PERMDNN PEs thanks permute diagonal matrix unique characteristic distribution non zero entry along direction PERMDNN enjoys scalability load imbalance challenge sparse dnn accelerator exist PERMDNN speedup PERMDNN architecture PEs achieves scalability benchmark XI comparison CIRCNN PERMDNN synthesis report CIRCNN PERMDNN PEs CMOS tech report project clk freq mhz quantization throughput equivalent TOPS efficiency equivalent TOPS VI related besides EIE CIRCNN various report explore performance dnn diannao google tpu propose series dnn processor correspond instruction sparse dnn accelerator investigate importance  highperformance dnn hardware extensively utilization memory dnn research effort data dnn propose optimize data flexibility reduce memory access investigate highperformance dnn hardware serial input besides various fpga dnn implement report beyond research digital accelerator inference task dnn hardware analog mixed signal circuit potential efficiency deploy training consume architecture approach propose improve efficiency training dnn model compression active research topic machine community various approach prune rank decomposition quantization structure matrix cluster propose prior survey compression technique vii conclusion proposes PERMDNN novel approach generate execute hardware friendly structure sparse dnn model permute diagonal matrix prior model compression approach PERMDNN enjoys benefit regularity flexibility arithmetic computation propose PERMDNN architecture scalable FC layer target compute implementation PERMDNN achieves throughout efficiency efficiency EIE achieves throughput efficiency CIRCNN