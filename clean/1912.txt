synthesis HLS generate accelerator software program task building hardware unfortunately HLS limited concurrency impact speedup achievable generate accelerator approach target fix static pipeline data parallel kernel constraint ability software programmer express concurrency moreover generate accelerator loses benefit parallel hardware dynamic asynchrony potential hide latency cache developed TAPAS HLS toolchain generate parallel accelerator program dynamic parallelism TAPAS built tapir embeds fork parallelism compiler intermediate representation TAPAS leverage compiler IR identify parallelism synthesizes hardware logic TAPAS architecture spawn coordinate synchronize task accelerator execution demonstrate TAPAS generate accelerator concurrent program heterogeneous nest recursive parallelism evaluation intel altera DE soc arria demonstrates TAPAS generate accelerator achieve efficiency intel xeon maintain comparable performance TAPAS enables lightweight task spawn cycle enables accelerator exploit available grain parallelism TAPAS HLS toolchain synthesize parallel program accelerator source index synthesis llvm chisel HLS cilk TAPAS hardware accelerator efficiency dynamic parallelism fpga introduction academia realize hardware customization performance semiconductor taper amazon EC huawei FPGAs available public microsoft exploit FPGAs accelerate datacenter service address challenge develop application domain specific hardware  HLS introduce HLS translates program rtl circuit specification HLS flexibility permit software engineer performance hardware task parallel accelerator limitation HLS approach concurrency accelerator attains performance instantiate multiple execution effectively coarse grain grain concurrency relative software unfortunately HLS effectively concurrent HLS extensive annotation generate parallel architecture concurrency synthesis HLS interface typically analyze loop employ technique unroll pipelining intel xilinx target HLS data parallelism HLS aware challenge introduce concurrency sought exploit parallelism  subset openmp pthread apis seek benefit thread parallelism ibm kernel parallelism toolchains limited static concurrency parallelism structure hardware generation structure cannot execution recent standard HLS adopt fix hardware template target specific concurrency template data parallelism loop parallelism loop pipelining application programmer annotate modify application template  HLS adopts construct approach concurrency operation schedule statically hardware generation unfortunately concurrent program parallelism evolves program due nondeterminism HLS built sequential compiler compiler intermediate representation restrict sequential program dependence graph hence prior largely focus program static parallelism express template pragma pipeline library OpenCL focus program irregular grain parallelism express implicitly within program built parallel compiler release demonstrate program dynamic concurrency FPGAs achieve performance watt multicore annual acm international symposium microarchitecture doi micro spawn pipe stage chunk chunk chunk chunk null exit spawn pipe stage chunk dup dedup chunk chunk dup spawn pipe stage compress chunk spawn pipe stage buffer chunk task graph execution chunk chunk chunk task queue stage task dataflow chunk chunk chunk chunk null exit generate pipeline accelerator stage chunk dup  chunk chunk dup compress chunk stage spawn sync ram cache axi bus dram  chunk stage parallel accelerator generate TAPAS dynamic pipeline parallelization parsec dedup cilk modify enhance clarity stage pipeline stage stage conditional parallel stage data approach focus synthesis hardware accelerator parallel program dynamic parallelism TAPAS HLS framework leverage parallel IR generate rtl parallel task accelerator accelerator architecture spawn synchronize homogeneous heterogeneous task TAPAS leverage parallelism marker embed tapir generate rtl stage stage analyzes parallel IR infer task dependency synchronization generates architecture granularity task stage generates dataflow execution logic task permit arbitrary loop memory operation microarchitecture generate TAPAS specify parameterized chisel permit designer tile dedicate per task resource per task queue depth register scratchpad memory capacity illustrate dynamic task accelerator flexibility realize nest heterogeneous recursive irregular regular concurrency briefly discus TAPAS handle challenge generate hardware dynamically pipelined program dedup parsec comment pseudo code HLS code sample challenge cannot generate optimal microarchitecture stage pipeline input task graph iteration stage entirely skip stage stage constraint exhibit nest parallelism stage embarrassingly parallel stage enforces across sequence finally pipeline termination evaluate runtime cannot statically bound loop handle dynamic parallel TAPAS generates hierarchical microarchitecture generic task accelerator microarchitecture consists collection unique atomic task heterogeneous task task internally manages dataflow logic execute task generate architecture benefit dynamic task spawn enables program skip stage entirely pipeline communication hierarchical task logic organization permit concurrent task nest TAPAS permit dedup stage internally parallelize task stage architecture eliminates dedicate communication allocates local ram communicate data task permit dedup stage directly pas data stage stage bypass conditionally finally architecture manage task dependency TAPAS derives concurrency compiler IR embeds within task pipeline exit function chunk dataflow embed within stage developed TAPAS source HLS generates parallel hardware accelerator dynamic task parallelism TAPAS framework parallel compiler intermediate representation arbitrarily nest parallelism irregular task parallelism agnostic cilk cilk openmp developed library hardware component spawn synchronize task buffering task inter task communication demonstrate TAPAS HLS compose component generate performance parallel accelerator evaluate performance flexibility TAPAS intel altera DE soc arria fpga TAPAS achieves performance watt intel xeon quad core performance comparable multicore processor II background scope gap quality hardware generate HLS partly due inability HLS comprehend exploit parallelism available software partly due underlie abstraction available hardware architecture resource abundant FPGAs becomes imperative HLS dynamic parallelism user specifies task parallel instead parallel task mapped execution dynamic task parallelism TAPAS naturally arises generate parallel architecture specify parallelism HLS consensus toolchains primarily standard framework concurrent execution CPUs framework openmp intel TBB implement thread however unclear requirement thread precise register context memory  stack non cpu architecture overhead recent thread openmp loop alternate vision task abstraction notion dynamic task discus notion static task explore prior intuitively task analogous encapsulate computation software unlike function argument completion task described tuple args sync function scoped subset program dependence graph implement functionality args argument function sync task synchronize feature task task dynamically spawn task extensive static task parallelism FPGAs prior statically schedule task underlie execution rely task abstraction understand static concurrency TAPAS hardware synchronize task dynamically accelerator execution TAPAS adopts dynamic software task model previously explore context vector architecture gpus  standard HLS len node valid spawn node sync dynamic parallelism TAPAS static parallelism HLS task len task len node valid node pragma parallel unroll node valid len node valid node iteration node valid len node odd iteration generate parallel loop HLS static schedule TAPAS dynamic schedule dynamic parallelism static parallelism dynamic parallelism refers enable task task spawn task spawn prior HLS adopt static parallelism concurrent thread task schedule commonly parallel loop illustrate difference dynamic static parallelism loop exhibit dynamic parallelism loop bound parameter len execution varies parallel loop iteration function invoked node valid TAPAS handle TAPAS creates task loop spawn task node valid dynamic maximum len HLS unroll loop exploit static parallelism loop unrolled HLS multiple hardware execution onto successive loop iteration statically schedule hardware construction unroll factor therefore allocate resource iteration regardless actually execute handle len unroll static dynamic schedule another limitation standard HLS lack dynamic schedule ability dataflow handle variance instruction latency cache HLS thread static schedule instruction memory instruction deterministic schedule prior HLS primarily memory model data load scratchpad ahead invocation combination static schedule static concurrency memory model efficiency limit workload HLS target pre requisite dynamic task parallelism memory cache consequentially TAPAS handle non deterministic latency memory operation concurrent  fpga investigate benefit dynamic schedule instruction however exploit static parallelism loop sequential program TAPAS focus dynamic parallelism parallel program TAPAS task abstraction compiler feasible target parallel cilk TAPAS dynamic schedule however focus TAPAS prior feature HLS HLS HLS kernel TAPAS scope seq kernel parallel prog target loop thread task hint pragma kernel spawn sync dynamic loop unroll pipeline parallelize  limited nest parallel limited lightweight multithreading multicore multiple execution summarizes feature HLS HLS primarily target sequential program unroll loop exploit instruction parallelism parallel architecture realize HLS compiler synthesize hardware core typically expert manually instantiate multiple instance core hardware description avoid xilinx vivado intel HLS unroll pipeline loop convert loop parallelism instruction parallelism achieve efficient hardware software developer identify feasible exploit loop parallelism additional hardware orient pragmas HLS anticipate target parallelism recent subset OpenCL openmp pthreads primarily target data parallel kernel HLS schedule concurrent operation statically dynamic spawn asynchronous behavior nest parallelism furthermore HLS statically schedule memory operation code annotation identify fifo access function finally promising avenue research HLS domain specific hardware expert parameterized template target parallelism pipeline software developer modifies application ensure program structure unfortunately risk become obsolete TAPAS target parallel program programmer identify concurrent task built parallel compiler leverage information automatically synthesize parallel hardware arbitrary task graph novelty TAPAS task dynamically spawn sync task enables TAPAS handle variety program nest recursive heterogeneous parallelism finally TAPAS dynamic schedule operation handle non determinism enable cache memory model TAPAS synthesize dynamic parallel accelerator TAPAS hierarchical HLS toolchain generate rtl parallel application specific accelerator TAPAS agnostic relies tapir llvm parse parallel program generate compiler IR additional marker parallelism input TAPAS parallel program marker task parallel loop currently infrastructure cilk openmp cilk stage TAPAS rtl generation program nest parallel loop TAPAS consists stage stage TAPAS analyzes compiler IR extract task dependency generates rtl task declare memory stage program graph task analyze rtl generate dataflow task finally stage configure hardware parameter execution core specific deployment LUTs available fpga generate fpga bitstream TAPAS generate accelerator dynamic parallelism dynamic schedule cache restrict communication accelerator memory currently TAPAS accelerator fpga soc fpga KB cache synthesize cache accelerator coherent axi TAPAS generates binary program function cannot offload due TAPAS rely logic fpga synthesizes logic parallelism enables flexible execution model independent processor enables TAPAS target fpga concurrent program tapir task extract llvm IR program graph concurrency opt generate task arch chisel rtl task opt generate TXU mem parallel accelerator chisel rtl parameterize chisel verilog cilk cilk stage stage stage spawn spawn sync task task task stage stage program dependence graph dynamic task graph stage task spawn task sync queue queue task spawn sync queue args ram stack ram args ram ID NT asks NR NT asks TXU tile NT  input spawn output sync sync input spawn output axi interface cpu dram fpga soc sync overview TAPAS compilation generate output stage stage task parallel architecture TAPAS relies tapir comprehend semantics task accelerator architecture tapir instruction llvm IR detach  sync express fork parallel program instruction tapir dynamic task spawn concurrent task sync synchronize task compiler pas detail focus hardware generation task dependency generate accelerator consists multiple task task unique task illustrates rtl interface parameter associate interface TAPAS multiplexing equivalent simultaneous multithreading multiple task execution dynamic tile assignment task runtime execution equivalent multicore task execution task serf building architecture accelerator consist task interact task graph component within atomic task task queue manages spawn task task interface spawn synchronization task interface spawn synchronization task execution TXU pipelined dataflow execution execution functionality component task implementation nest loop task graph illustrates task outer loop spawner instance inner loop inner loop spawner instance finally performs actual reading  implicit parameter extends module cache val  module cache dram axi interface val dram module  dram  axi initialize task val task module task   DF val task module task   DF val task module task   DF task task task task detach task spawn task sync task task detach task spawn task sync task cache task TAPAS generate microarchitecture chisel application dynamic instance task iteration outer loop dynamic instance instance instance task queue spawn allocate TXU exe TXU allocate task execution synchronize sync synchronization task task queue metadata consists counter  args ram argument ram illustrates spawn operation initiate task correspond inner loop iteration spawn tuple args   consists spawn active args spawn TXU args TXU task status spawn spawn args spawn spawn args spawn args args spawn TXU spawn sync args spawn args spawn args sync sync sync args spawn TXU args spawn args sync sync execution nest loop accelerator generate TAPAS sid  sid refers task instance  corresponds task queue entry allocate instance task index corresponds dynamic task spawn instance loop  metadata available spawn allocate spawn task synchronization dynamic instance correspond inner loop creates instance inner task queue entry corresponds task dynamic task instance correspond loop iteration task concurrently instance inner loop iteration queue available task asynchronously assigns task execution task instance synchronize task spawn task synchronize task instance correspond task completion dynamic instance loop entail  counter queue entry index correspond purpose sid  task spawn sid permit composability allows heterogeneous task communicate dynamically spawn task sid serf network task route  serf index queue within task correspond sid finally spawn proceeds sync status  task queue interface decouple task creation task execution spawn sync asynchronous employ valid signal asynchronous permit resource parameter per task without reschedule task latency stage generate task exe TXU output  load load   int  idle exe machine output pipeline handshaking def sink src DFG node args data load operation load val  module  ID val  module  ID operation val  module  ID int operation val  module alu ID sadd   sink source          task task execution TXU  load load GEP GEP GEP dynamic ID  load load GEP GEP GEP  load load GEP GEP GEP  load load GEP GEP GEP multiple task simultaneously outstanding TXU TXU representation execution within task TXU fully pipeline execution permit multiple dynamic instance task execute simultaneously  communicate task boundary inter TXU communication marshal scratchpad cache TAPAS generates logic  per task sub program dependence graph  compiler TXU dataflow enables grain instruction parallelism TAPAS HLS dynamically schedule operation TXU automatic pipelining introduces latency insensitive valid interface operation dataflow dataflow graph mapped TXU node multi cycle latency float operation non deterministic latency memory operation approach contrast strength HLS schedule timing operation statically concurrent FPGAs analyze potential dynamic schedule function file TAPAS function communicates load load via decouple handshaking signal valid signal addition data handshaking interface machine dataflow permit multiple concurrent task outstanding execution task pipelining illustrate dynamic task correspond queue index allocate pipeline TXU dataflow task issue load stall pipelined dataflow throttle eventually stall however simpler implementation dynamic dataflow stage parameterized accelerator TAPAS parameterized hardware generator seek permit stage parameter binding hardware complexity modularity becomes asynchrony latency insensitivity permit task parameterized independently task mechanism passing parameter prior hardware elaboration bitstream generation tile multiple parameter width args ram primarily parameter stage toolchain task queue  task execution  permit user parameter per task basis latency individual task task dependency   involve depends processing rate task active task potentially hide memory latency task memory interface memory model heterogeneous soc processing core accelerator integrate chip accelerator cache cache processor axi bus memory model accelerator TAPAS permit arbitrary task graph convert accelerator flexible cache interface req resp arbiter network params load network burst burst allocator params entry addr req arbiter data resp demux cache scratchpad demux network resp arbiter req resp cache scratchpad load TXU dataflow addr data align data interface memory operation logic transfer operation cache scratchpad TAPAS task cache conceivable model suitable handle program model familiar software programmer generate optimal cache hierarchy beyond scope primarily focus route cache  data connects memory operation TXU memory interface currently cache scratchpad evaluate cache memory model logic multiple memory operation alignment data minimize resource requirement architecture data data consists parameterized microarchitecture component arbiter network arbitrates amongst request memory interface demux network route response memory operation TXU dataflow stag buffer actual logic reading byte cache axi memory interface granularity access request response network statically rout compiler task IR TAPAS agnostic relies parallel IR introduce tapir tapir binding translates cilk openmp program llvm IR tapir instruction llvm IR detach spawn  sync spawn  delineate task detach instruction terminates contains spawn task target execution parallel continuation  terminates task spawn precede detach instruction tapir assumes generic  execution model leaf marker pdg program dependency graph leverage marker perform reachability analysis extract explicit task graph architecture blueprint parallel accelerator nest loop irregular analyze stage task graph task relation constitute task explicitly specify perform variable analysis extract requisite argument task parameterize spawn args ram task IV TAPAS generate accelerator standard benchmark suite target dynamic parallelism II brief summary accelerator benchmark characteristic application emphasis implement accelerator workload without additional effort programmer application software HLS challenge error nest parallel conditional loop related benchmark matrix addition stencil image saxpy stencil multiple workload employ nest  task  task sub pdg BB   empty    insert successor succ succ succ succ spawn task succ   insert    spawn  succ  task   pop   tst   tst insert succ regular tst insert succ dfs  tst input pop TAPAS pas extract task tapir analysis reachable nest spawn sync II benchmark HLS challenge memory  inst mem matrix nest loop regular image sca nest loop regular saxpy dynamic exit loop regular stencil nest parallel serial regular dedup task pipeline irregular merge sort recursive parallel regular fibonacci recursive parallel regular loop however variation parallelism loop nest loop depth conditional loop entry exit briefly discus stencil stencil iterative kernel update array loop loop embarrassingly parallel loop bound variable introduces dynamic parallelism HLS parallelize flatten inner loop HLS parallelize innermost loop parallel execute serially contrast TAPAS decomposes nest loop multiple task task asynchronous independently configure tile exploit available parallelism TAPAS arbitrary nest loop serial parallel loop nest depth resource permit task expose asynchronous spawn sync interface TAPAS loop independently loop execute parallel serial fashion program semantics without outer loop pipeline parallelism dedup code hardware accelerator outline dedup irregular pipeline challenge dedup task pipeline HLS limited task pipeline primarily target loop void stencil parallel loop cilk nrows  serial loop  serial loop  int   int col   nrows col  task inner loop serial cilk inner loop serial stencil accelerator pipelining dedup parallelize task non trivial entry exit logic challenge convert loop conditional stage emerge research sought functional pipeline fifo queue program rewrite unfortunately dedup conditional pipeline stage fifo queue cannot fifo queue fix producer consumer stage cannot handle conditional pipeline intra stage parallelism fifo  lose parallelism stage permit chunk processing task dependency pipeline finally pipeline termination dynamically exit function chunk HLS dynamic exit statically schedule operation TAPAS suffer limitation dynamic spawn sync task execution assign runtime furthermore task communicate memory parallelism limited extraneous hardware structure fifo recursive parallelism TAPAS effectively generate accelerator recursively parallel program HLS traditionally recursion due lack program stack precludes addition stack HLS compilation framework illustrates TAPAS recursively parallel mergesort mergesort primary function employ conquer strategy partition array  parallel function merges sort implement recursion invocation function exist data implicitly via stack TAPAS achieves TAPAS precisely capture recursive task llvm IR implicitly manages stack frame scratchpad task controller dynamic schedule asynchronous queue permit task spawn without logic loop task controller dynamic instance implicit synchronization finally return recursion cache void mergesort int mid spawn sort cilk spawn mergesort mid spawn sort cilk spawn mergesort mid cilk sync merge mid mergesort tile task controller spawn sync mergesort dynamic task accelerator recursive mergesort evaluation challenge baseline dynamic parallelism exist HLS toolchains benchmark fpga entail algorithm memory model program baseline cpu challenge cilk tapir currently multicore cache hierarchy deeper FPGAs challenge understand impact dynamic parallelism independent memory TAPAS grain task grain task performance improvement attributable task spawn latency individual task dataflow execution baseline performance intel quad core consumption performance watt benefit intel unmodified cilk program static parallelism prior HLS dynamic parallelism TAPAS parallel task overhead overhead spawn task fpga significantly software enables grain task void int int int cilk return code parallel task tile scalability code performance tile microbenchmark synthesize task spawn generate architecture incrementally varied amount operation loop performance plot increase worker tile arria mhz target device achieve maximum spawn rate spawn grain task instruction performance monotonically addition parallel worker tile software plot refers spawn task increment program intel  MB core granularity software runtime cilk zero benefit due task spawn overhead TAPAS exploit overhead task spawn fpga enables grain parallelism expose software resource utilization arria fpga parallel task integer operation per instruction per tile resource utilization accelerator code synthesize intel soc FPGAs cyclone arria primary source overhead TAPAS task controller logic memory arbitration cyclone soc tile integer operation chip mhz arria accelerator achieve mhz occupy chip MK ram consume queue spawn task task controller logic fpga utilization mhz tile ALM reg bram chip cyclone  arria ALM utilization sub relative amount ALM resource aka LUTs register sub extreme operation task logic non compute overhead operation task overhead execution tile increase overhead logic amortize tile overhead reduce memory network memory access overall chip resource network primarily dynamic schedule rout forth cache internal node task execution scalability performance TAPAS generate accelerator exploit available parallelism expose application increase hardware resource improve accelerator performance intel multicore cache hierarchy plot performance execution tile per task performance increase baseline exception dedup dedup baseline tile heterogeneous task organize pipeline execution tile per task improvement increase tile task feasible pipeline stage unbalanced saxpy matrix addition improve addition tile benchmark quickly saturate cache bandwidth inner loop dominate memory writes contrast stencil benchmark computationally intense consequently tile beyond execution accelerator intel quad core  MB 1GB dram identical cilk benchmark TAPAS concurrency identical core tile TAPAS accelerator generate cyclone soc fpga arria soc fpga cyclone accelerator perform approximately multicore achieve speedup arria generate accelerator perform par frequency mhz mhz cyclone dedup accelerator achieve speedup accelerator implement pipeline efficiently software mergesort accelerator perform poorly comparison intel completely memory bound limited memory fpga performance scalability understand impact memory non cilk sequential program cpu soc memory hierarchy fpga performance difference accelerator fpga mhz  sophisticated cache hierarchy consumption TAPAS generate accelerator exceed efficiency multicore IV report absolute consumption resource utilization generate accelerator obtain intel   estimate static dynamic signal activity derive gate simulation tabulate data varied parallel stencil nest loop mergesort recursive effectively exploit available resource cyclone fpga chip mergesort roughly available chip resource consume approximately performance watt accelerator multicore concurrency multicore directly RAPL interface TAPAS accelerator achieve performance watt multicore IV fpga resource cyclone bench tile mhz ALMS regs bram saxpy stencil matrix image dedup fibonacci mergesort intel HLS TAPAS comparison prior HLS challenge HLS static parallelism feasible convert application static parallelism recursive mergesort others dedup conversion algorithm entirely HLS deploy memory model statically schedule instruction latency TAPAS employ cache memory pre requisite dynamic parallelism attempt quantitative comparison benchmark saxpy image benchmark amenable static parallelism intel HLS compiler employ dram interface dram latency intel HLS TAPAS mhz fpga concurrency HLS loop unrolled TAPAS configure tile TAPAS competitive feasible optimize HLS implementation optimize TAPAS notable difference RAMs utilized intel HLS generate buffer load interface contrast TAPAS cache task  RAMs task queue ddr cpp intel HLS fpga faster performance TAPAS intel fpga performance watt TAPAS intel intel HLS TAPAS cyclone bench mhz ALMS reg  image intel HLS TAPAS saxpy intel HLS TAPAS VI future direction demonstrate FPGAs hardware accelerator potential address outstanding challenge concurrency effective dynamic grain parallelism facet address improve performance cache hierarchy compete multicore processor improve overall cache hierarchy bandwidth latency cache macro release toolchain borrow RISC core limited multiple outstanding cache axi implementation sub optimal exploit burst option available protocol task controller task controller queue logic latency critical workload bound matrix multiplication exist loop statically parallelize TAPAS benefit statically schedule loop eliminate task controller challenge identify loop optimization feasible opportunity dynamic parallelism TAPAS relies compiler tapir capture parallelism intent workload TAPAS currently capable generate accelerator widely fork parallelism workload apart initialization function offload accelerator unfortunately compiler explicitly capture data driven parallelism inter stage queue pipeline channel golang TAPAS achieves data driven synchronization cache memory future data driven parallelism explicit hardware structure hardware fifo queue improve overall efficiency vii summary TAPAS primary goal intuitive HLS toolchain software programmer generate parallel accelerator decouple concurrency parallelism task program framework convey parallel generate architecture dynamically explore available parallelism effective framework community exchange parallel accelerator release TAPAS source github link redact  compiler translates parallel compiler IR parallel accelerator architecture chisel framework convey concurrency task parallelism TAPAS chisel library implement task spawn sync  operation fpga sample parallel accelerator pipeline nest loop heterogeneous