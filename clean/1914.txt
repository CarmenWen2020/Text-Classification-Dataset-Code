recent potential TLBs multiple core tackle memory translation performance challenge data workload stumble hinder effectiveness however access methodology reduce access realize performance scalable TLBs benefit replace monolithic TLBs distribute tlb slice approach reduce tlb lookup latency increase interconnect delay access remote slice therefore devise lightweight  interconnect tlb slice tailor switch unique communication characteristic memory translation request response approach dub NOCSTAR  scalable tlb architecture combine rate TLBs access private TLBs enable significant performance benefit index virtual memory tlb network chip cache introduction memory intensive workload performance challenge computer important challenge achieve efficient  physical address translation efficient translation lookaside buffer TLBs central achieve address translation tlb performance depends attribute access rate penalty recent improve tlb rate technique hardware hardware software approach sub coalesce cluster memory optimization superpages translation others prefetching speculative technique illusion effective tlb capacity similarly synergistic TLBs evict translation per core TLBs improve rate focus reduce tlb penalty finally  TLBs propose improve overall rate avoid replication translation multi thread program multi programmed workload library OS structure joint author contribute equally nvidia unfortunately approach attribute tlb access tlb organization processor vendor implement TLBs private core however recent academic replace equivalently core tlb eliminates however structure physically core longer access latency recall address translation latency critical cache access consequently tlb attractive actually becomes memory demand increase improve tlb without significantly increase access research challenge goal translate rate benefit TLBs overall runtime speedup conceptual rethink architect scalable tlb hierarchy challenge multi monolithic tlb structure suffers latency alternative distribute tlb akin NUCA LLCs distribute tlb slice access latency unfortunately tlb access non uniform location slice translation cached core haswell distribute tlb consequently degrades performance despite average private TLBs tlb access latency critical data cache access propose NOCSTAR  scalable tlb architecture methodology architect scalable latency TLBs NOCSTAR relies latency characteristic chip bandwidth characteristic address translation request realize lightweight specialized interconnect cycle access remote tlb slice however chip consequently NOCSTAR rate benefit TLBs access latency private TLBs via feature capacity NOCSTAR rate private TLBs eliminate replication improve utilization lookup latency NOCSTAR achieves lookup latency replace monolithic tlb structure annual acm international symposium microarchitecture doi micro tlb slice distribute across core network latency NOCSTAR employ interconnect core distribute tlb slice interconnect cycle latency source remote tlb reduce network traversal latency confluence feature enables NOCSTAR within performance ideal  latency tlb equivalent configuration NOCSTAR outperforms private TLBs core haswell average across suite workload II background motivation NOCSTAR applicable instruction data TLBs focus latter focus driven data tlb pressure prevalence data workload summarizes tlb architecture conventional private TLBs tlb alternative propose prior tlb practical involve banking monolithic structure evaluate ultimately distribute tlb slice across core noc choice throughout tlb access latency refer tlb SRAM lookup latency interconnect latency limitation private TLBs promise TLBs private TLBs staple server chip intel skylake amd ryzen processor intel skylake chip entry TLBs entry associative TLBs per core unfortunately private TLBs suffer classic pitfall private cache structure replication utilization replication multi thread application multi core naturally replicate  physical translation across private TLBs virtual address surprisingly multiprogrammed combination thread program exhibit replication library OS structure private TLBs suffer utilization chip tlb resource partition statically usually equally situation runtime private tlb core thrash counterpart another core traffic recent evaluate potential TLBs TLBs TLBs eliminate redundancy private TLBs seamlessly tlb resource core runtime demand overcome utilization TLBs conservatively reduce tlb account interconnect ensure equivalence baseline per core TLBs approach tlb implicit prefetching benefit thread core demand hence prefetch translation eventually thread core TLBs eliminate suffer private TLBs tlb rate quantifies benefit TLBs intel haswell described IV TLBs eliminate majority tlb suffer private TLBs workload entire private tlb entry translation waste furthermore prior private tlb rate naturally rate harmful tlb latency generally core effectively tlb eliminates private tlb situation core core private TLBs entry core replace private TLBs tlb entry core realize entry tlb instead therefore eliminate replication utilization private TLBs effectively core workload notably locality access canneal  xsbench particularly aid TLBs core tlb access rate improvement improve performance overall however tlb performance influence rate SRAM array lookup TLBs typically implement SRAM array unfortunately SRAM array ensure access challenge model SRAMs TSMC technology node memory compiler quantifies access latency function entry array synthesis entry tlb private TLBs intel skylake cycle entry cycle access replace private TLBs  tlb structure grows entry structure core entry  structure core entry increase lookup factor ultimately access latency worsens TLBs core  benefit rate interconnect traversal TLBs focus monolithic entire structure chip naturally exacerbate access due additional interconnect delay access tlb location counteract benefit core core worsen delay tlb sll slice sll slice sll slice sll slice sll slice sll slice sll slice sll slice sll slice tlb organization private tlb monolithic tlb tlb distribute across core percentage private tlb eliminate replace tlb core cycle sll private tlb access latency SRAM tlb entry tlb synthesis TSMC  instance core tile chip hop direction access tlb bandwidth tlb proposal access multiple core suffer contention structure access likelihood concurrent tlb access relatively decrease performance versus private tlb scenario core access private tlb without interference core tlb performance quantifies attribute counteract rate overall performance monolithic TLBs profile performance core haswell monolithic TLBs versus private TLBs SRAM array memory compiler TSMC private TLBs cycle lookup consistent reference haswell tlb lookup intel manual private tlb lookup latency cycle tlb speedup multi TLBs private TLBs tlb access latency varies cycle access latency cycle  ideal SRAM array access private TLBs interconnect zero latency cycle reasonable estimate SRAM array plus interconnect latency tlb plot perform banking configuration workload IV describes configuration assume linux transparent superpages memory footprint workload implement superpages detail despite rate monolithic tlb perform poorly cycle access latency performance dip versus private TLBs  ideal network zero interconnect latency latency arises contention SRAM array latency corresponds scenario tlb access cycle tlb speedup private TLBs understand tlb access aspect tlb access overcome access latency tlb contention across application capture information contention tlb tlb access plot core outstanding tlb access tlb access isolation outstanding tlb access roughly another tlb access concurrently access access etc core haswell tlb access concurrently access access etc average across workload tlb access tlb slice occurs concurrently access slice access slice etc distribute tlb tlb slice core tlb access outstanding tlb lookup tlb contention tlb tlb tlb access impact tlb tlb contention baseline average access distribution distribution private TLBs per core halve increase TLBs tlb lookup consequently access access portion increase significantly imply contention however trend towards TLBs reflect direction processor vendor increase tlb contention access dominate roughly tlb access tlb contention core finally impact core tlb contention baseline core haswell core haswell baseline tlb core assume core haswell tlb contention increase core however contention increase core marginally increase core access access contribution increase roughly respectively approach core beyond contention visibly increase however perform replace monolithic tlb distribute tlb tlb slice core graph showcase quantify contention average per tlb slice core core roughly access tlb slice suffer contention concurrent access takeaway TLBs access quickly performance concurrent access rare configuration remain future core later validate observation tlb storm microbenchmark deliberately tlb situation conceptual underpin motivates specialized interconnect optimize latency bandwidth accelerate tlb access latency interconnects chip delay technology transistor become faster generation relative logic prompt research NUCA cache however  delay cycle remains fairly constant across generation chip repeater regular interval perform cycle traversal across chip technology node recent chip demonstrate noc traversal delay network latency message  denote hop destination router delay delay capture contention router serialization delay incur packet narrow link latency directly proportional challenge latency  usually  optimize latency bandwidth bus traversal broadcast mesh popular due simplicity scalability rely grid link router  however average hop therefore latency increase radix noc topology  distance link router reduce however naturally link bandwidth penalty multi router crossbar narrower datapath reduce bandwidth reduce tlb interconnect choice noc latency bandwidth bus mesh   narrow smart NOCSTAR mesh serialization delay latency optimization smart extreme enable packet dynamically construct bypass mesh reduce effective however guaranteed expensive circuitry setup arbitrate false positive negative moreover buffer router mesh  smart overhead NOCSTAR proposes interconnect NOCSTAR approach NOCSTAR organizes sll tlb distribute array tlb slice reduce lookup latency configurable cycle network fabric reduce interconnect latency tlb organization distribute tlb slice NOCSTAR logically tlb distribute across tile core mirror NUCA LLCs slice private TLBs thereby meeting budget tlb entry entry slice valid translation context ID associate translation index although optimize index mechanism adopt performance index mechanism virtual address tlb interconnect develop dedicate noc communicate TLBs tlb slice II directly adopt  data cache desirable tlb interconnect instead develop  circuit switch interconnect cycle connectivity arbitrary source destination datapath  switch datapath NOCSTAR leverage transmit signal within ghz II enable cycle traversal packet NOCSTAR  switch tlb slice switch simply collection muxes muxes pre message arrives request direction traverse switch directly rout direction multiplexer without latch message latch destination switch  target tlb tlb slice tlb request within cycle tlb slice highlight mux repeater entire traversal conventional bandwidth datapath naturally bandwidth mesh  buffer internally within noc moreover unlike  link cannot multiple simultaneous transmission unless completely link however demonstrate earlier II tlb infrequent access access bandwidth noc sufficient purpose scalability traversal network  chip frequency multiple cycle pipeline latch discus grain circuit switch involve message setup traversal interconnect data link acquire message ensure packet destination cycle link acquire cycle data link associate arbiter allocate link request core core request link arbiter grant link arbiter traverse requester fails acquire link desire contention cycle ensures packet traverse partial avoids complexity acquire message traverse datapath fanout switch core setup slice width arbiter depends rout policy adopt tlb XY policy core arbiter associate link core request core num core num num link arbiter network link associate arbiter reside switch arbiter request core tlb request response packet link arbiter selects request core grant link cycle output mux appropriate input grant requester tlb slice link arbiter local buffer tlb slice link arbiter switch tlb tlb slice core arbiter cycle traversal arbiter arbiter source destination tlb hierarchy core NOCSTAR source destination request request micro architecture switch enables cycle traversal network core request arbiter link arbiter destination req  virtual address enable traversal link selector tlb slice ID source link arbiter core sends request link arbiter grant SRAM tlb switch link arbiter per core switch arbiter SRAM tlb rout NOCSTAR tile TSMC tlb SRAM switch link arbiter highlight switch link arbiter slice comparison SRAM tlb slice target  link arbiter physical location chip rout policy arbiter request suppose XY rout arbiter link requester arbiter link requester arbitration priority arbitration link decentralize possibility livelock request acquire partial link arbitration avoid arbiter static priority requester allot link requester priority guaranteed request link avoid starvation static priority robin fashion cycle implementation implement NOCSTAR interconnect TSMC 2GHz rout core remote tlb slice tlb insert setup tlb tlb slice access setup timeline virtual address translation tlb remote tlb access NOCSTAR critical critical interconnect datapath multi hop traversal intermediate switch perform within cycle recall tlb interconnect timing met desire frequency pipelined latch maximum hop per cycle  boundary increase network traversal delay affect operation moreover core increase tile become maximum hop per cycle actually critical consists setup request furthest link arbiter link arbitration grant traversal core route arbiter reduce average timing synthesis consume NOCSTAR switch arbiter contrast tlb SRAM tile consume switch arbiter tile tlb SRAM link arbiter due  tight timing hungry component overhead reduce overhead restrict rout algorithm correspondingly  earlier timeline tlb access NOCSTAR timeline address translation tlb tlb tlb trigger circuit switch setup setup perform speculatively tlb access request setup remote tlb slice translation mapped identify index setup request arbiter link grant request  grant setup retry grant request request traversal tlb request switch tlb slice header rout information append already setup request cycle intermediate switch latch remote tlb slice enqueued request queue tlb slice access remote tlb slice receives request service request translation exist tlb response response contains physical associate virtual address request tlb response setup circuit switch response request response setup speculatively tlb lookup response requester regardless access response traversal response traverse tlb interconnect within cycle tlb insertion request translation insert request tlb tlb access latency quantify NOCSTAR latency benefit versus monolithic distribute TLBs latency message traverse hop tlb interconnect tlb request translation indexed slice request core virtual address index sll slice local node translation return tlb latency incur lookup latency tlb slice distribute NOCSTAR identical private tlb latency request translation index remote slice translation request remote node slice dedicate network destination node virtual address index sll slice translation request slice upon translation response request core translation tlb latency lookup latency network latency NOCSTAR latency advantage monolithic distribute maximum hop per cycle  NOCSTAR faster distribute consume message traverse hop tlb interconnect understand tlb saving distribute NOCSTAR access SRAM structure monolithic sll tlb datapath circuit switch consume intermediate switch NOCSTAR switch traditional distribute network  hop however NOCSTAR expensive multiple request grant span link arbiter simultaneous arbitration instance traverse hop within cycle NOCSTAR link arbitrate simultaneously slightly distribute however latency gain approach overall saving discus insertion replacement policy recent tlb architecture assume TLBs virtual desire modulo index lru replacement furthermore recent TLBs assume TLBs mostly inclusive multi cache mostly inclusive multi TLBs invalidation message handle suppose core suffers tlb tlb suppose determines tlb slice housing desire translation remote node lookup remote node tlb slice ultimately option perform option remote slice message requester node perform option remote node perform approach pro con handle remote node attractive eliminates message relayed remote requester node however handle remote node increase potential walker congestion multiple core request remote slice queue tlb shootdowns issue involves NOCSTAR responds virtual memory operation perform OS situation entry modify OS core happens OS kernel usually launch inter processor interrupt IPIs pause core interrupt handler shoot cycle hop access latency network latency monolithic multi cycle interconnect distribute multi cycle interconnect NOCSTAR  NOCSTAR  NOCSTAR  message NOCSTAR contention latency cycle injection rate contention delay multi hop mesh NOCSTAR latency mdn mdn mdn mdn mdn mdn mdn MD hop link switch SRAM latency message tlb interconnect various configuration consume message tlb interconnect various configuration    hop average latency message respect increase injection rate NOCSTAR interconnect multi hop interconnect invalidates stale translation tlb operation NOCSTAR specifically multiple core simultaneously relay translation invalidation signal tlb slice stale translation quickly congest cascade tlb invalidation lookup tlb slice sidestep designate node invalidation leader core IPIs core invalidates private tlb specific core permit relay invalidation signal tlb core invalidation leader core receives ipi relay message core core relay message relevant tlb slice invalidate stale translation actual tlb invalidation NOCSTAR mirror private tlb private tlb invalidation access translation private tlb similarly invalidation translation access translation within slice slice permit approach ideal scenario leader core message become congest leader core IV methodology simulation framework evaluate benefit NOCSTAR cycle accurate simulator  model intel haswell ubuntu linux transparent superpages standard configuration model intel haswell core KB instruction data cache cycle access KB cache cycle access llc MB per core cycle access parameter chosen haswell specification parameter intel manual memory TB workload input workload actually memory capacity core maintain private TLBs entry associative TLBs KB entry TLBs MB entry TLBs 1GB per haswell specification TLBs cycle access parallel cache standard virtually indexed physically tag configuration TLBs tlb tlb lookup baseline assumes intel haswell configuration private entry associative TLBs concurrently KB MB baseline cycle synthesis SRAM generate data intel manual tlb organization latency furthermore assume private tlb per tlb slice simulator model TLBs access pipelined request service cycle finally combine simulation framework McPAT target configuration II detail tlb configuration evaluate approach evaluate standard monolithic approach tlb evaluate banking configuration monolithic core configuration core evaluate regular mesh cycle smart noc approach distribute approach tlb array tlb slice core noc  distribute TLBs mesh multi hop involves traditional cycle router couple cycle link latency compete cycle  NOCSTAR buffer link prevent link contention network contention degrade performance workload traditional mesh network NOCSTAR cycle traversal contention otherwise another cycle explain rout XY NOCSTAR evaluation assume core maintains entry entry tlb slice conservative normalize analysis interconnect consumes tlb slice benchmark benchmark parsec cloudsuite furthermore performance multi programmed workload combination application application multi II configuration tlb simulated tlb entry associative physical org interconnect private tlb per core monolithic  monolithic mesh multi hop smart distribute  slice per core mesh multi hop NOCSTAR  slice per core NOCSTAR speedup monolithic distribute NOCSTAR ideal zero interconnect latency tlb assume core haswell KB complementary linux transparent superpages KB MB programmed workload thread execute TB memory experimental EVALUATIONS performance performance core haswell configuration assume KB plot speedup versus baseline private TLBs axis monolithic data corresponds monolithic tlb access latency  IV distribute configuration ideal tlb access zero interconnect latency ideal imply infinite tlb NOCSTAR achieves average max performance private TLBs importantly configuration monolithic degrades performance versus private TLBs  access latency speedup core linux transparent MB  percent address translation versus private TLBs distribute partly NOCSTAR achieves additional performance within ideal performance linux native transparent MB superpages linux allocate workload memory footprint superpages superpages reduce tlb reduce gain NOCSTAR however performance NOCSTAR presence superpages workload memoryintensive TB superpages tlb access frequent however superpages reduce tlb meaning tlb access become contributor overall performance explains workload xsbench  achieve speedup NOCSTAR outperforms monolithic distribute margin simply KB scalability graph quantifies speedup core linux transparent MB superpages along KB average minimum maximum speedup monolithic rate overshadow access particularly worsen performance core employ distribute approach NOCSTAR consistently outperforms approach recent address translation constitute overall processor spent access hardware cache magnitude expensive spent tlb access tlb address translation eliminate plot percent versus baseline private TLBs monolithic approach eliminates roughly address translation however NOCSTAR eliminates core identify saving source NOCSTAR dramatically reduces runtime thereby reduce static contribution another important source saving NOCSTAR reduces tlb ensue cache lookup memory reference lookup eliminate prior speedup baseline configuration private TLBs monolithic approach traditional multi hop mesh smart ideal NOCSTAR contention interconnect ideal tlb slice zero interconnect latency memory reference service llc baseline without NOCSTAR workload evaluate prompt llc memory lookup desire entry NOCSTAR eliminates bulk average llc memory reference thereby lookup saving outweigh overhead dedicate NOCSTAR network interconnect tease apart performance contribution distribute tlb slice versus faster interconnect speedup versus private TLBs core haswell configuration version monolithic approach traditional multi hop mesh implement smart monolithic approach average approach suffer performance degradation interconnect smart monolithic approach SRAM array latency  instead distribute tlb slice per core distribute achieve average performance improvement however NOCSTAR performs ideally message NOCSTAR cycle traverse noc however increase contention message average latency cycle workload xsbench  suffer latency beyond cycle overall NOCSTAR achieves performance idealize interconnect zero contention NOCSTAR ideal finally achievable performance ideal scenario interconnect zero latency NOCSTAR achieves within performance idealize interconnect mechanism adopt NOCSTAR inject random synthetic traffic core average network latency message ideally message NOCSTAR cycle setup another cycle traverse network injection rate message speedup core versus private TLBs acquire acquire speedup tlb invalidation policy cycle per core tlb traffic average latency message NOCSTAR interconnect remains within cycle percentage message delay acquire setup option mode link reservation acquire link acquire access remote slice mode link selection perform request response acquire link acquire  message message selects link traversal graph acquire link separately message delivers performance acquire link tlb invalidation investigate invalidate request tlb slice shootdown flush core various invalidate message across tlb interconnect straightforward invalidate core tlb slice policy congestion interconnect core invalidate slice invalidate message central location manage invalidation slice split manager slice graph speedup workload invalidate message core invalidate message policy policy perform remote core core slice virtual address performs sends translation response request core insert slice request core tlb slice message request core request core performs sends insert message remote slice speedup policy perform remote node involves message interconnect  local cache request remote request remote request remote core core core speedup average canneal graph  xsbench request remote core speedup core haswell impact prefetching hyperthreading latency speedup achieve NOCSTAR tlb configuration versus private TLBs speedup average across workload minimum maximum pref smt ptw lat min avg max monolithic variable distribute NOCSTAR monolithic variable distribute NOCSTAR monolithic variable distribute NOCSTAR monolithic variable distribute NOCSTAR monolithic variable distribute NOCSTAR monolithic variable distribute NOCSTAR monolithic fix distribute NOCSTAR monolithic fix distribute NOCSTAR monolithic fix distribute NOCSTAR monolithic fix distribute NOCSTAR remote core degrade performance perform request core delivers slightly remote core sensitivity quantify NOCSTAR configuration quantifies average min max speedup workload core haswell scenario prefetching pref label hyperthreading smt latency ptw lat scenario tlb prefetching enable tlb impact prefetching translation virtual adjacent virtual prompt tlb monolithic distribute NOCSTAR configuration NOCSTAR benefit consistently enjoy presence prefetching tlb prefetching translation virtual away effective aggressive prefetching pollute tlb however scenario tlb implies pollution versus private TLBs additionally NOCSTAR reduce access latency versus monolithic distribute approach accurate prefetching yield performance quantifies impact multiple   per core tlb pressure TLBs rate benefit private TLBs combine NOCSTAR superior access latency performance exceeds distribute monolithic finally quantifies NOCSTAR performance function latency classify latency variable correspond realistic simulation environment latency depends upon cache desire translation reside fix fix latency cycle latency unrealistically cycle monolithic distribute TLBs severely harm performance configuration suffer access latency rate useful impact tlb minor nevertheless situation NOCSTAR outperforms private TLBs realistic scenario latency cycle typically NOCSTAR performance notably exceeds option scenario cycle benefit become pronounce NOCSTAR outperform distribute TLBs average multiprogrammed combination sequential workload target platform core haswell workload consist combination workload combination overall workload executes thread utilize core sort overall ipc improvement NOCSTAR particularly effective  utilization benefit TLBs without penalize application access latency improves aggregate ipc approach contrast monolithic degrades performance workload access latency issue distribute degrades workload graph speedup perform application monolithic distribute almost  overall throughput core combination workload speedup perform sequential application private TLBs tions application suffers performance loss due access latency sometimes degradation severe contrast workload NOCSTAR degrade performance relatively rare extent performance loss relatively benign versus private TLBs reminiscent interference issue LLCs likely alleviate llc qos fairness mechanism future pathological workload workload tend generate significant congestion stress NOCSTAR devise microbenchmarks tlb storm microbenchmark microbenchmark trigger frequent context switch  storm tlb invalidation access congest network workload profile concurrently execute custom microbenchmark modify linux scheduler context switch workload microbenchmark normally linux permit context switch granularity unrealistically aggressive context switch onwards purpose stress NOCSTAR custom microbenchmark allocate KB promote MB superpages KB confluence modify linux scheduler microbenchmark massive tlb invalidation context switch haswell tlb content flush storm tlb lookup data furthermore microbenchmark promotes KB MB  invalidates distinct tlb entry quantifies slowdown workload tlb activity average across workload core focus generates maximum network congestion context switch microbenchmark generates tlb access per kilo instruction tlb  prior tlb pressure impose microbenchmark naturally degrades performance versus scenario benchmark standalone alone microbenchmark suffer performance degradation however NOCSTAR vastly outperforms average speedup workload versus private tlb configuration core alone workload alone already data data workload concurrently tlb storm microbenchmark approach monolithic TLBs degrade performance versus private TLBs presence contention NOCSTAR achieve performance improvement average certainly performance improvement achievable without congestion promising furthermore improvement achieve NOCSTAR improve context switch granularity unreasonably aggressive tlb slice microbenchmark craft microbenchmark happens immense congestion tlb slice microbenchmark thread core machine thread continuously access tlb slice assign nth core naturally approach degrades performance severely however NOCSTAR private TLBs furthermore NOCSTAR conservative scenario tlb approach monolithic distribute approach consequently NOCSTAR alternative tlb configuration VI CONCLUSIONS proposes NOCSTAR tlb noc achieves rate TLBs without compromise access rate deliver TLBs overshadow latency tlb structure network involve traverse moreover traditional distribute architecture deliver potential performance gain network latency distribute TLBs smart interconnect NOCSTAR improves multi thread multiprogrammed workload performance vii  gabriel  jan  feedback improve draft google VMware  kwon script synthesis route SRAMs   partly darpa  project