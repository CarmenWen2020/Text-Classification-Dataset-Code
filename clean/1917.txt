chip bandwidth scarce resource processor become limited per core basis era throughput massively parallel computation promising approach overcome limited bandwidth chip link compression unfortunately previously propose latency driven compression scheme latency tolerant manycore capacity accommodate concurrent thread cable novel cache link encoder enables  link compression coherent cache purpose data already cache massive scalable data compression applicability cable apply critical chip link memory link interface chip memory  link processor multi chip implement cable pipeline hardware verilog OpenPiton framework feasibility evaluate spec cable increase effective chip bandwidth throughput average CPACK respectively index cache memory data compression parallel processing introduction throughput orient processor integrate thread within chip multi socket chip bandwidth become dominant limitation achieve performance socket bottleneck chip memory dram interface thread compete  memory bandwidth multi chip limited chip chip cache coherence bandwidth QPI infinity fabric limit amount  coherence request limited chip bandwidth challenge technology improve core chip bandwidth monetary HBM interposer layer significant consumption serial link  complex recovery scheme equalization 0GHz  unsolved bandwidth effectively halt progress throughput compute   HPRU  DFKH   HPRU    HPRU  DFKH HPRU  bandwidth starve chip link memory link processor memory coherent link processor HT  HT       cache request response uncompressed request data compress pointer plus diff promising approach compression chip link increase effective bandwidth link compression literature zero encoders sophisticated LZ gzip unfortunately primary shortcoming prior context manycore processor prior tends optimize thread performance non algorithm minimize compression latency throughput orient manycore processor gpus explicitly tolerate memory latency pollution increase concurrent thread grows nullify benefit address issue proposes cable novel cache link encode framework exploit data already cache massive scalable cable compression apply coherent cache memory link llc chip coherence link LLCs multi chip cable mechanism depict content otherwise annual acm international symposium microarchitecture doi micro  memory address already exists remote cache chip llc prior request instead raw data cache chip llc another chip shorter diff pointer remote cache reconstruct content knowledge cable cache data data compression similarity across compression ratio chip link significantly increase evaluate spec demonstrate cable emerge manycore architecture increase effective chip bandwidth throughput average CPACK respectively amount chip cache cable compression performance enhance multiprogram workload whereas gzip suffers due pollution lastly efficiency focus cable reduce memory subsystem average incur thread performance loss complex compression scheme cable consciously compression latency offchip bandwidth argument  already sacrifice thread performance throughput efficiency commonality compression increase effective bandwidth chip bandwidth thread consolidated onto processor thereby increase computational throughput contribution architectural framework exploit data commonality cache massive scalable compression methodology exploit  data identify data similarity data signature applicability cable apply cable memory link coherence link critical chip link implement cable pipeline verilog OpenPiton framework implementation feasibility evaluate cable across spec demonstrate cable compression scalable CPACK consequently throughput increase average II cable motivation challenge project observation exists significant data similarity across cache prior cache compression cache deduplication significant data redundancy compression ratio ideal algorithm increase without pointer overhead cache building upon intuition compression ratio increase cache plot ignore pointer overhead ideal  compression ratio increase challenge pointer overhead however pointer overhead critical issue typically data compress replace pointer pointer become increase accounting pointer ideal pointer improvement compression ratio mirror prior optimal around modest byte cable tackle pointer overhead challenge mechanism amortize pointer multiple data byte cache instead unlike prior reduction pointer novel challenge similarity another challenge identify similarity brute approach simply infeasible instead software compression index hardware signature hash unlike purpose compression tailor signature mechanism exploit sparsity data cache challenge synchronization lastly synchronize across cache instead restrict cache eviction policy complexity exist cache coherence protocol assume cache chip inclusive remote cache chip llc aid identify cache additionally cache exclusive modify reference data modify silently incorrect decompression cable compression framework compression algorithm function cache actual compression operation delegate exist compression algorithm CPACK lbe CPACK modify configurable minus overhead profile non trivial benchmark spec confuse pointer program data compression pointer refers reference data index buffer offset cable pointer cache glossary cable specific  cache cache service compress memory request remote cache cache receives  data reference exist remote cache diff compress representation cache reference signature shorten unique representation cache  index cache cache  index cache remote cache hash mapping hash signature  tag translate     DWD  PDS      DWD    DWD    DFKH  DFKH  datapath cable component cache communicate remote cache LZ gzip framework compose distinct task definition similarity signature compress reference transmit compress diff reference pointer synchronize valid signature hash cache compression sequence cache cache remote cache cache response cache request cache cable analyzes extract signature cable signature cache reference signature index hash obtain cache index  cache cable  reference candidate data array rank data similarity selects diff concurrently minimize pointer overhead cable reduces cache tag  WMT sends diff link decompress diff remote cache  reference  diff reconstructs data instal cache structurally data structure hash WMT source signature hash cache signature     default signature offset potential signature extract checked  representative signature cable skip trivial WMT compress cache tag  structure content addressable memory CAMs standard SRAMs WMT exists cache assumption limitation simplify assumption characteristic architecture assume inclusive cache hierarchy practically memory link compression chip contains chip llc entirely coherence link compression chip module mcm package implies remote cache inclusive local cache assume inclusivity simplifies data synchronization discus non inclusive extension IV cable dirty reference assume chip link simplify correctness discussion link IV  eviction cache coherence assume reduce synchronization complexity memory layout eviction IV cable underlie coherence protocol modifies reduces data payload transmit cable decouple replacement policy cache eviction precisely although requirement remote cache cache replacement info cache request architecture  architecture discus cache request hash commonality local cache WMT reduce reference pointer generate signature signature rank data signature extraction signature succinct unique representation cache however requirement signature ibm centaur intel haswell eDRAM cache       DWD DFKH         hash hash entry contains LineIDs cache signature perfect hash collision similarity representation conflict minimize storage unique similarity accuracy flexible ignore slight edits shift permutation cable explore possibility sample signature offset cache compute hash signature empirically signature content cache benchmark zero abundant non zero distinct sequence tend array typically data layout minimal modification byte shift avoid hash zero signature hash offset data trivial define zero signature offset offset trivial data unlike purpose compression gzip cable shift offset byte instead observation shift byte improve compression performance significantly presumably data program align boundary hash hash standard data structure signature  reference candidate signature request data building index signature per cache extract insert hash non trivial signature generate request data reference signature invalidate cache desynchronize evict cable computes signature remove  invalidate correspond hash entry entry LineIDs default hash inherently inexact simply hash collision signature hash entry source inaccuracy demonstrates hash dissimilar cache entry false positive hash collision signature insert synchronization data signature extract hash reference ranked II comparison memory   operation CPACK compression cache access MB slice chip IO link dram access summarizes cable assume byte cache signature cable extract signature due zero potentially non unique signature request cache signature index hash LineIDs assume bucket LineIDs cable pre rank selects duplicate LineIDs reference request data signature LineIDs duplicate entry output hash entry prioritize likely similarity others cable rank reference candidate data similarity reference rank mechanism coverage vector  compute candidate request cache cable greedily  maximize coverage request data instance suppose  combine  coverage  cable selects combine  coverage baseline parameter cache access data array rank partly empirical inexpensive access data array directly without tag latency wise SRAMs usually cycle access eDRAMs wise advantageous generous link typically consume II    DFKH   rup  EB  DFKH       access WMT indicates cache exists remote cache HI HI HI  DWD   DWD   compression decompression cable temporary reference diff reconstruct data diff receiver cache tag reference pointer   index locally remotely reduce transmission overhead LineIDs reduce pointer overhead versus tag WMT data structure cache cache remote cache structure mirror layout remote cache tag WMT indicates index remote cache entry however  uniquely cache cache remote cache furthermore reduce WMT  normalize alias alias cache index minus remote index instead index WMT reduces cache tag  cable computes normalize  append alias remote cache index access WMT cable  guaranteed exist remote cache WMT entry indicates remote  compression transmission reference rank compression temporary reference compress request data data compression concurrently described pipeline compress request data without compression ratio threshold diff compress data without reference payload overhead minimal flag denote data compress uncompressed specify reference  variable diff diff decompress data fix evaluation bus assume limit max compression width link pcie potential bandwidth cable sub portion data across link data across link cache LineIDs fetch reference temporary decompress diff synchronization cable update hash  cache cache invalidation cable compress dirty request cache incorporate hash cache future compression correspondingly cache insert remote cache hash future compression simultaneously replacement info WMT cache request  cache remote cache cable cache extract signature invalidates entry hash hash WMT invalidation coherency invalidate cache remote cache snoop invalidation cache eviction cache upgrade request dirty worth reiterate cable synchronization precisely WMT operation decouple cache replacement policy compression compression remote cache sequence minor difference unlike cache remote cache WMT simply sends LineIDs cache  split remote index retrieve  cache WMT remote cache update hash data cache IV discussion important cable cache selects reference concurrently evict remote cache cable cannot decompress response evict reference implement eviction buffer remote cache temporarily  eviction link transport intel QPI instance eviction assign sequence  insert eviction buffer tag embed unrelated memory request cache acknowledges embed  response entry remove eviction baseline cable assumes notify non eviction strict requirement memory configuration cache remote cache eviction mapping occurs cache exclusively remote cache llc dram channel architecture sparc eviction message implicitly embed cache request request  info request data embed replacement info allows cache evict data without explicit eviction linear address interleave remote cache across multiple cache occurs address interleave linear cache instance llc linearly interleave across dram channel cache exclusively dram channel mapping replacement info implement implicit eviction without incur dram cache non inclusive cache extension cable assumes inclusive cache adapt non inclusive cache hierarchy intel haswell EP multi chip numa architecture cache agent memory address remote cache cache agent private  cache inclusive remote cache non inclusiveness fundamentally cache directory cache coherence purpose actual data cable opportunistically remote cache compress traffic LLCs llc issue non inclusiveness compression inclusive cache remote node implicitly cache guaranteed exist node assumption longer non inclusive cache disable compression compress writebacks non algorithm latency overhead cable primarily throughput technique memory latency important parameter characterize useful validate feasibility implement pipeline OpenPiton source manycore framework latency stage hash signature access hash reading data cache building coverage vector rank coverage rank processing signature independent highly parallelizable nevertheless throughput limited hash SRAM assume hash signature checked concurrently cycle data array cycle latency signature cycle signature throughput signature per cycle latency cycle latency request data highly redundant zero signature checked reduce latency cycle latency implementation dependent conservatively model cable latency implementation gzip LZ highly complex implement hardware cable relatively built entire pipeline cache OpenPiton source manycore framework compute signature implement performance hash function OpenPiton data array access latency cycle per without aggressive pipelining achieve nominal operating frequency OpenPiton ibm  standard library compression latency compression decompression reference compress decompress diff assume compression rate cycle cache cycle cycle latency latency cycle summarize IV efficiency data transfer across chip link consume onchip operation II reduction link outweighs compression cache access consumption reference candidate plus data array access cache cache compression decompression reference temporary reference construct coverage vector rank conservative estimation compression operation consumption per request chip link transfer overhead addition compressor estimate cable additional SRAM structure hash impose SRAM overhead implement synthesize OpenPiton cable pipeline logic overhead slice interestingly pre rank complex logic implementation accept sort LineIDs per cycle overhead data array access without tag typically around eDRAMs cable  chip multi chip buffer chip cache cache hash  width logic overhead cable per per tile combinational buffer  report percentage data cache assume KB summarize chip link compression assume MB llc MB dram buffer WMT entry alias associativity hence storage overhead cache cache  similarly multi chip WMT overhead processor  coherence compression multiple processor elect WMT per link configuration WMT information pool competitively super WMT hash manage cache decrease storage overhead improve scalability hash independent cache gracefully entry downward entry retain signature recent upward alleviates hash conflict define hash entry cache cache generally hash data cache MB cache  byte architecture overhead  cable bandwidth deprive chip link cable alleviate discus cable chip memory link alleviate bandwidth architecture implement memory buffer dram chip ibm incorporates MB chip eDRAM buffer dram channel intel skylake configure MB eDRAM memory cache furthermore interposer 3D stack memory HBM HMC SRAM cache dram logic layer cable apply compress request traffic cpu chip cache data llc compress fetch data memory compression benefit increase bandwidth throughput skylake cable obviates package link reduces buffer package cable reduce link frequency maintain effective bandwidth saving multi chip cache coherent link cache coherent multi chip chip multiprocessor cmp prevalent enterprise setting manufacturer multi socket gpu architecture coherent across node nvidia cache coherent NVLINK nvidia mcm multiple coherent chip interposer layer advantage chip variant ability application memory footprint interleave memory across chip memory channel communication latency chip limit performance scalability however propose compress coherence link apply cable chip chip ptp QPI NVLINK link assume numa memory interleave across node node cache data chip cache inclusive intel haswell EP nvidia mcm memory link compression cable compress data data request remote node chip instance fully chip ptp link directly chip ptp link cable pipeline VI RESULTS methodology modify prime simulator spec benchmark evaluate cable simpoint trace instruction available publicly online instruction trace phase profile instruction thread faster representative multi program instead trace program instruction instruction sustain load program methodology prior increase fidelity remove phase consist mostly load zero artificially boost overall compression ratio model performance configure core frequency memory subsystem detailed IV latency IV cable finding zero dominant phase phase identify criterion CPACK achieves chip link compression KB llc KB llc chip link compression normalize CPACK IV default configuration core 0GHz cpi non memory instruction KB per core private cycle associative KB per core private associative cycle llc MB per core associative cycle inclusive within chip chip link 6GHz 2GB thread channel throughput quad channel setup latency dram buffer MB per core associative cycle numa node robin allocation dram link 6GHz 8GB dram MCs per chip buffer FCFS controller ddr mhz sub timing compression latency comp decomp CPACK cycle gzip  cycle cable cycle simulation PARAMETERS static dynamic llc dram buffer cable lbe comp cable lbe decomp latency cycle per transfer chip link parameter model intel QPI amd hypertransport thread thread MB llc calculation MB cache model throughput account statistical multiplexing bandwidth purely static bandwidth partition model capture split thread bandwidth competitively within evaluate memory quad channel 8GB cache contention due additional cache access similarity simulated eDRAM chip chip per channel llc chip compression compression ratio chip link compression raw compression ratio uncompressed compress memory link compression cable configure hash buffer hash chip coherence link compression hash quarter  unless otherwise data access VI sixteen parameter VI model static dynamic cache eDRAM buffer dram CACTI dram access consumption micron ddr calculator assume rank chip per rank utilization link estimate per byte prior estimate dram access II summarizes model evaluate cable algorithm non CPACK bdi CPACK lbe gzip compression algorithm modify CPACK lbe byte byte respectively fifo replacement policy chosen performance gzip evaluate KB max configurable timing parameter IV latency gzip estimation ibm ASIC LZ program compression chip memory link compression relative raw compression ratio  compression ratio chip cmp non trivial workload average across  scheme apply chip memory link gzip cable fare favorably despite simpler cable loses gzip benchmark achieve compression workload dealii tonto zeusmp gobmk lbe CPACK simpler scheme cable consistently bandwidth overall cable achieves compression ratio average increase effective bandwidth capacity CPACK compression ratio relative already CPACK apply cable compression benchmark mcf easy compress others chip traffic dominate zero benchmark workload cable scheme consistently compression ratio consistent prior link memory compression node mention cable  data compress without multi chip coherence link compression compression performance link compression apply coherent link processor chip cmp mention spec thread benchmark gauge performance memory load balance interleave across node trend memory link compression previous although compression ratio slightly due dirty transfer harder compress average cable lbe achieves average compression ratio CPACK throughput improvement chip bandwidth cable significantly increase throughput despite compression latency thread throughput increase average almost workload memory intensive workload mcf lbm benefit  effective compression ratio physical link width byte payload multi thread simulation replicate workload configure compression across program multiprogram VI thread thread throughput speedup link compression compression ratio program alone replicate multi intensive workload povray gobmk generally benefit despite achieve compression ratio average throughput speedup thread thread bandwidth subscription significant cable marginally CPACK gzip thread really benefit cable thread rapidly emerge intel knight thread core   processor core per processor computer  RISC processor core besides gpus thread chip coherent chip link multiprogram compression conduct multiprogram simulation cooperative workload data structure compression unrelated destructive workload pollute compression examine chip memory link gzip cable gain lose compress dissimilar data remove zero dominant workload cooperative multiprogram simulate cooperative multiprogram environment program VI multiprogram randomly chosen href soplex hmmer bzip gcc gobmk gcc soplex bzip lbm gobmk perlbench gcc bzip tonto cactusADM perlbench wrf gobmk gcc omnetpp bzip bzip gobmk gcc tonto cactusADM gcc wrf gcc bzip compression improvement thread compression program VI concurrently input  style simulation approximate throughput orient workload multiple program slightly input input configuration architectural simulation cable benefit cooperative multiprogram gzip cable data similarity plot compression obvious namd cable gzip lose gcc gzip loses performance cable explanation identical program rate thread desynchronize execute dissimilar program phase destructive multiprogram pollution program VI compression ratio program separately normalize thread VI trend pollution severe gzip fix KB gzip perform contention multiple data imply gzip algorithm scalable perform manycore meanwhile cable data structure hash WMT scalable increase cache thread cable maintains compression ratio thread compression improve latency compression latency thread performance focus cable useful understand increase compression latency performance overhead scheme due compression latency IV overhead proportional compression decompression latency cycle cable average multi chip coherence compression degradation curve thread performance degradation link compression normalize memory subsystem breakdown uncompressed baseline cable lbe mitigate deficiency compression scheme sample compression effective bandwidth usage scheme thread performance degradation cable effectively nullified decrease throughput average comparison data cod technique cod serial link  pci express scramble transmit data guarantee density error correction recovery data scramble data transmission correlate data transaction normalize chip link component cable SRAM label indicates static leakage dynamic component static resource llc cable compression broken portion compression eDRAM compression SRAM overall cable link significantly compute intensive benchmark link account roughly memory subsystem compression link overall saving promising future efficient compute link toggle reduction memory link scramble data toggle rate useful metric toggle consumption reliability apply link cable reduces toggle average CPACK across llc across llc ratio llc fix MB memory link compression across cache compression performance cable sensitivity exclude zero dominant benchmark cache varies llc allocation program thread KB MB ratio llc compression ratio mostly static across cache gzip cable compression ratio slightly increase cache compress data spill memory ratio llc ratio llc constant compression ratio individual workload fluctuate due average within across configuration amount data accessible cable depends cache llc constant numa coherence link compression numa node processor compression ratio largely unaffected compression although cable performance lbe cable performance CPACK gzip oracle generally lbe gzip CPACK inspect closely insight pointer overhead significant difference lbe CPACK lbe align data overhead improve demonstrate cable oracle cable oracle reference cache scheme compress data compression degradation decrease hash hash compression data access relative access byte shift unaligned duplicate significantly compression ratio hash varies hash denotes  baseline graceful degradation performance extreme benchmark advantageous performance performance loss data access stage cable selects reference pre rank access data array varies threshold plot compression ratio relative access access resilient access within access access performance reference duplicate request  effective filter hash collide LineIDs link width generally link compression effective width interconnects link width effective bandwidth degrades waste payload narrow interconnects intel QPI amd zen infinity  pci lane physical link  logical channel HBM bus channel alternatively transport protocol pack multiple transaction transfer specify byte compress data packed vii related besides approach cable gzip non compression algorithm evaluate cache compression compression link width workload memory link component noc register evaluate performance representative algorithm bdi CPACK lbe contrast compression performs due significant redundancy cache phenomenon cache compression apply memory link compression cache inter similarity encode explore dram interface gzip LZ standard software compression algorithm evaluate cable comparable performance encourage gzip domain hpc compression ratio evaluate lzma configure 4GB storage performance  due inefficient output flush cache llc compression promising direction increase effective cache capacity decrease rate memory link compression cache compression harder issue cache fragmentation compression ratio cache compression typically capped due limited tag otherwise cache link compression orthogonal tandem maximize chip bandwidth data deduplication context storage replication network closely related data deduplication apply  cache cable utilizes hash detect duplicate difference cable compress memory link memory cache cable detects similarity sub granularity signature whereas mention scheme  cache  cache exploit data approximation  cache cache sequence hash function average proximity cache depends programmer annotate memory approximate nevertheless approximation employ increase efficacy link compression recent retrospective review compression methodology simulate memory trace capture compression behavior workload assertion simulate publicly available  simpoint trace avoid reporting unrepresentative context compute cable bandwidth beyond cache bandwidth throttle provider tenant alike exploit optimize profit conclusion evaluate spec benchmark suite cable markedly improves compression ratio prior link compression increase effective memory bandwidth throughput average thread demonstrate cable resiliency pollution framework flexibility across chip link chip bandwidth become scarce cache link compression excellent extract bandwidth