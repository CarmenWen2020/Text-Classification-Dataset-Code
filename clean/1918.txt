memory become bandwidth constrain data compression technique increase effective bandwidth however data compression access metadata incurs additional bandwidth overhead metadata cache bandwidth overhead metadata reduce benefit compression proposes attache framework reduces overhead metadata access attache framework consists component component blend metadata BLEM enables data metadata access BLEM incurs additional metadata access remove almost metadata bandwidth overhead component compression predictor COPR predicts memory compress COPR predictor grain predictor coarsegrained predictor global indicator enables attache predict compressibility memory memory request implement attache memory sub rank average attache achieves speedup ideal consumption ideal baseline employ data compression attache completely hardware KB SRAM index data compression metadata bandwidth  memory introduction increase memory bandwidth instrumental increase performance processor chip accelerator designer traditionally improve bandwidth increase frequency pin memory interface technique tend furthermore interface memory standard propose overcome challenge data compression compression enables memory transfer data pin memory chip thereby unlock bandwidth however identify data compress additional metadata additional memory access metadata offset benefit compression mitigate memory access overhead obtain metadata memory tend comprise dynamic random access memory dram memory request commodity memory module transmit byte data cacheline module typically contains dram chip author contribute equally towards currently affiliate  national korea email   chip contributes byte byte memory compress byte byte enable dram chip request thereby effective bandwidth technique  data compression sub rank improve memory bandwidth unfortunately metadata overhead involve data compression offset bandwidth benefit cacheline memory memory unique metadata identify compressibility capacity memory impractical metadata memory controller instance 6GB memory cacheline metadata memory controller MB storage impractical implement MB SRAM memory controller due latency overhead therefore metadata tends location within memory tends issue additional memory request reduce bandwidth overhead metadata employ metadata cache within memory controller unfortunately additional metadata cache eviction install request reduce performance benefit data compression proportion compress memory byte additional bandwidth overhead metadata access analysis reasonably MB metadata cache inside memory controller plot spec gap workload ideally reduce additional memory request access metadata goal reduce almost metadata access proposes attache framework mitigate metadata access ideal speedup memory access overhead metadata access spec gap benchmark metadata increase memory traffic goal mitigate overhead obtain ideal speedup annual acm international symposium microarchitecture doi micro attache tackle metadata bandwidth overhead metadata cacheline memory within memory attache data metadata access therefore attache avoids almost additional access memory attache consists component component blend metadata BLEM data metadata component compression predictor COPR predicts metadata memory access predictor enables memory controller proactively issue memory request predict sub rank blend metadata BLEM data metadata memory writes memory easy cacheline compressible compression creates additional metadata however cacheline compress additional metadata overcome irrespective data compress BLEM interprets cacheline  metadata header consists compress ID cid exclusive ID xid compress ID cid identifies compression status cacheline cid chosen randomly  memory controller instance cid memory controller appends cid compress cacheline sub rank byte compress data cid target compression cacheline byte byte cacheline cannot compress byte memory controller append cid additional simply writes uncompressed data sub rank byte sub rank compress cid therefore cid identify compress memory unfortunately reading uncompressed byte memory cid false positive cid uncompressed memory cid identify really compress simply false positive refer cid collision BLEM identify cid collision metadata header exclusive ID xid identify cid collision xid checked data cid xid indicates cid collision memory compressible byte xid reset append cid however memory compressible instead unmodified memory memory controller memory cid collision cid collision memory controller memory randomize data perform data scramble due probability cid false positive uncompressed data memory proactively writes xid therefore alter data uncompressed memory data replace xid memory replacement cid collision memory controller entire byte memory fetch data replacement BLEM incurs additional memory cid collision cid collision rare BLEM incurs negligible bandwidth overhead BLEM minimizes memory bandwidth overhead metadata allows access metadata data however identify sub rank enable memory compress memory controller access metadata access data enable metadata lookup access data prior propose metadata cache metadata cache recently access metadata unfortunately metadata cache bandwidth overhead cache additional eviction replacement memory request proposes replace metadata cache predictor predictor compression predictor COPR predicts compression status memory COPR consists predictor predictor LiPR prediction cachelines within dram predictor PaPR prediction compressibility dram global indicator GI counter compressibility recent memory access byte memory memory controller metadata prediction interpret metadata BLEM prediction incorrect memory controller corrective action reading remain byte memory BLEM ensures metadata memory COPR additional metadata access overall contribution blending data metadata BLEM technique encode metadata within data irrespective compressibility data thereby eliminate almost bandwidth overhead metadata access compression predictor confidence predictor enables BLEM compressibility memory attache speedup reduction efficiently compress data  attache software furthermore attache KB SRAM predictor register cid II background motivation brief background memory organization data compression furthermore highlight bandwidth overhead involve manage metadata comparison baseline sub rank sub rank compression baseline employ sub rank compression sub rank unlock bandwidth increase latency sub rank compression reduce latency overhead sub rank bandwidth compressible request memory organization commodity memory module consist dram chip dram chip consists multiple contains dram operates independently within channel memory request memory controller activates dram across chip activate dram buffer memory module typically consists dram chip buffer split across chip instance KB buffer chip data buffer memory request fetch appropriate data buffer chip chip memory request access byte data memory bandwidth latency tradeoff memory module consists dram chip improve bandwidth individual memory request service independently dram chip however dram chip data increase memory access latency reduce latency commodity memory module tend dram chip lockstep reduces memory access latency tolerable bandwidth ideally bandwidth additional latency overhead bandwidth without latency unlock bandwidth without incur additional latency sub rank compression sub rank unlock bandwidth improve bandwidth enable memory request access dram chip within module dram chip sub rank unfortunately simply sub rank memory module increase latency instance memory module sub rank memory request cater dram chip instead dram chip chip data increase access latency pseudo channel HBM without additional latency overhead compress data reduce latency mitigate additional access latency sub rank compress data instance byte memory compress byte sub rank compress data access latency latency baseline therefore  compression practical latency enable bandwidth memory memory data compression compress data memory latency algorithm implement memory controller efficient data compression algorithm data tend within cacheline delta immediate bdi algorithm insight compress  data similarly frequent  FPC algorithm frequently data data zero tend frequently FPC FPC lookup compression decompression data compression perform compression decompression memory controller  typically compression algorithm compression activate data compress sub rank decompression activate data decompress sub rank core cache compression decompression  memory drawn decompression writes compression metric compression decompression latency fortunately bdi arithmetic operation FPC lookup compression algorithm compress decompress within cycle data compression potential understand effectiveness data compression compressibility cachelines byte memory memory intensive spec gap workload percentage cachelines compress byte percentage cachelines compressible byte average cachelines byte memory compressible byte within sub rank without additional latency overhead average cachelines compressible byte compressible potentially obtain bandwidth sub rank remain disable sub rank maintain bandwidth therefore average ideally obtain bandwidth employ data compression  however memory employ compression access additional metadata lower bandwidth benefit compression data compression metadata overhead memory employ compression maintains additional metadata information memory instance decompression per memory identify compression status capacity overhead byte memory metadata memory contains memory therefore overall capacity overhead metadata tends instance 6GB memory MB metadata due impractical metadata processor chip therefore metadata typically within memory capacity overhead bandwidth overhead metadata memory bandwidth overhead memory request additional access metadata additional access tend benefit data compression pitfall metadata cache metadata cache reduce memory bandwidth overhead metadata metadata within metadata cache cache additional impact metadata cache rate performance average MB metadata cache rate obtain speedup memory access however metadata absent within metadata cache cache additional memory request eviction install increase rate metadata cache increase unfortunately impractical metadata cache within memory controller furthermore metadata cache incur significant lookup latency increase chip considerably average rate metadata cache average impractically MB metadata cache obtain speedup attache overview overview attache 6GB memory broadly attache consists component component attache blend metadata BLEM embeds metadata data compress uncompressed cachelines BLEM interprets data metadata header consists compression ID cid exclusive ID xid cachelines uncompressed cid collides BLEM replace data xid BLEM replacement RA replace xid component attache compression predictor COPR predict cacheline compressible COPR replacement metadata cache avoids bandwidth overhead manage metadata cache attache compression decompression compress decompress memory compressibility information memory BLEM COPR overview attache drawn attach consists blend metadata BLEM compression predictor COPR compression decompression compress decompress memory replacement memory replace xid IV attache framework describes attache framework attache framework broadly consists blend metadata BLEM compression predictor COPR blend metadata BLEM blend metadata BLEM aim metadata data cachelines irrespective compressibility however traditional memory challenge access metadata pitfall conventional metadata storage prior memory compression data metadata buffer memory controller fetch data metadata issue consecutive request buffer reduces latency fetch metadata implementation typically dram buffer KB cachelines metadata memory request access byte metadata therefore data cacheline metadata metadata memory request prefetch metadata data cachelines dram conventional technique metadata metadata buffer data therefore metadata data access consecutively memory request buffer metadata manner drawback metadata access access data memory controller metadata sub rank sub rank data uncompressed enable increase data access latency traffic memory workload cachelines tend access within buffer therefore fetch metadata cachelines buffer wasteful thirdly newly fetch metadata cacheline eviction another cacheline  increase traffic memory somehow fetch metadata access data eliminate drawback metadata memory insight intelligently prepend metadata data prepend metadata data compressible cachelines compress cacheline creates additional within cacheline metadata however cachelines compress additional metadata attache metadata interpret cacheline metadata header metadata header memory controller attache framework performs comparison irrespective compressibility cacheline metadata header component compression ID exclusive ID identify compression compression ID cid attache compression ID cid identify cacheline compress writes cid  compress cachelines cachelines compress additional cid therefore memory controller simply writes memory controller insight cid uncompressed however memory uncompressed cid refer scenario false positive cid collision probability cid collision cid probability cid collision depends cid instance cid cid collide memory request probability cid reduce probability cid collision probability cid collision access uncompressed access uncompressed cid collides access probability cid collision versus access uncompressed cid cid collision access extend cid information currently cid attache identify cacheline compressible however cacheline dynamically compress bdi FPC algorithm decompress cacheline metadata information simply reduce cid identify compression algorithm cid easily extend additional information cid information probability collision extend cid additional information cid additional probability information collision analysis BLEM writes writes BLEM proactively insert xid uncompressed cid collision BLEM information identify collision insight cid extend cater multiple compression algorithm unfortunately cid collide cid collision identify memory controller wrongly interpret uncompressed compress data corruption therefore cid collision detect detect cid collision exclusive ID xid exclusive ID xid identify cid collision writes cid collision memory controller proactively replaces data xid replace data replacement RA collision memory controller modify uncompressed therefore memory uncompressed replacement incur access memory compress memory controller simply append xid cid replacement cid data collision handle data replace xid cater cachelines encounter cid collision cacheline memory replacement index mapped manner overhead replacement capacity memory replacement invisible OS visible memory controller BLEM implementation reliability security memory controller scramble memory scramble  scramble data pseudo random interpret metadata header BLEM data scramble ensure cid collision probability BLEM writes BLEM compression information compression decompression disable compression decompression uncompressed request data cannot compress BLEM writes data however cid collision BLEM proactively insert scramble  tend hash memory address input ensures data memory data pseudo random scramble   scramble data compressibility xid data thereafter data replacement compress BLEM simply  cid xid data request cid BLEM simply data data deem compress however cid xid collision BLEM fetch data replacement decompress data compress BLEM simply cid xid sends decompress compression predictor COPR BLEM reduces metadata bandwidth overhead delivers metadata data however identify sub rank enable memory controller access metadata access data therefore prior propose metadata cache pitfall metadata cache unlike regular cache metadata cache tend reside within memory controller goal metadata cache recently access metadata furthermore metadata cache amenable lookup latency memory controller important memory controller probe metadata cache memory request lookup latency slowdown therefore prior academia assume  typically optimistically assume MB metadata cache impractical within memory controller analysis MB metadata cache achieves rate unfortunately additional memory access involve request metadata cache metadata cache memory controller issue request metadata dram fetch metadata metadata fetch metadata cache evict metadata install metadata evict dirty metadata cache memory controller issue another request compression predictor mitigate overhead drawback metadata cache manage metadata therefore metadata cache ensure metadata update memory incurs bandwidth overhead however attache BLEM responsible manage metadata due attache framework metadata cache issue install request memory due attache framework replace metadata cache predictor compression predictor COPR COPR simply predicts compression status cacheline reading cacheline passing BLEM memory controller COPR predictor thereafter memory controller update COPR prediction COPR obtain  prediction accuracy rate metadata cache COPR mitigate additional memory access stem metadata cache COPR enable multi granularity prediction memory compressible cachelines sparsely distribute entire evenly throughout instance cachelines compressible cachelines compressible predict instance COPR employ multi granularity predictor multi granularity predictor compose component global indicator GI GI overall information data compressibility application GI compose saturate counter compressibility memory boot counter initialize zero memory request counter correspond request memory incremented cacheline compressible otherwise  zero GI accurate indicator predict compressibility within memory abundant similarity compressibility predictor LiPR predictor PaPR global indicator memory compressibility GI zero counter threshold counter counter counter max increase counter access cacheline compressible incompressible cacheline compressible cacheline address compression predictor COPR COPR contains component global indicator GI predictor PaPR predictor LiPR predictor PaPR exploit similarity compressibility cachelines within OS PaPR compression prediction granularity PaPR associative cache structure indexed entry PaPR saturate counter memory access access cacheline compressible PaPR entry indexed incremented cacheline incompressible entry decremented entry cachelines correspond predict compressible otherwise cachelines predict incompressible entry access PaPR allocates entry initial GI correspond counter GI threshold entry maximum otherwise reset zero PaPR KB predictor LiPR goal LiPR compression prediction byte cacheline memory granularity LiPR predict compressibility cachelines similarity compressibility cachelines LiPR associative cache structure indexed LiPR entry compressibility contiguous cachelines cacheline index LiPR entry LiPR update entry prediction LiPR proactively update entry entry LiPR PaPR cachelines compressibility LiPR update entry deem compressibility across cachelines compressible incompressible cachelines LiPR KB prediction accuracy COPR prediction accuracy COPR average COPR prediction accuracy rate MB metadata cache COPR achieves insight compressibility tend compressible furthermore benchmark prediction accuracy BLEM metadata access prediction accuracy compression predictor COPR average COPR accuracy overhead COPR KB KB estimate McPAT chip baseline compressibility software attache available compression therefore capacity physical compress memory compressibility data data overflow memory capacity attache attache completely hardware approach memory provision capacity reserve expose OS memory module simply reduce capacity OS boot therefore attache software interaction virtual physical memory management memory controller alone handle compression decompression metadata management attache sub rank highlight benefit attache evaluates attache memory sub rank implementation attache framework limited sub rank attache framework easily apply proposal memory sub rank unlock bandwidth cachelines compress byte byte metadata header disable  uncompressed cachelines maintain latency baseline simplicity implementation compress cachelines odd numbered sub rank cachelines numbered sub rank memory controller receives request  attache COPR predict cacheline compress COPR cacheline compress fetch cacheline entirely sub rank latency baseline reading BLEM interpret metadata prediction accurate prediction inaccurate attache simply fetch cacheline enable sub rank COPR predicts cacheline compress sub rank enable entire byte cacheline fetch request numbered however byte uncompressed data flip therefore uncompressed cachelines memory controller fetch byte sub rank important metadata header byte operation attache update COPR metadata experimental methodology evaluate performance benefit attache sst simulation framework model processor core  core component sst  core model dynamically generates memory instruction application  extend  core model detailed oforder OoO execution II baseline configuration core OoO processor ghz issue width cache MB byte llc access latency cycle memory bus mhz memory channel rank per channel per per memory byte per dram access timing  trp tcas dram refresh timing   model memory   cycle accurate memory component sst  enforces strict timing model JEDEC ddr protocol specification rank memory module dram chip sub rank enable provision chip signal dram chip configure  prioritize request request memory controller buffer drain writes memory watermark enable compression compression compress memory bdi FPC selects compression ratio per prior assume compression decompression data occurs within cycle metadata cache COPR assume access latency cycle cache  model overhead DRAMSim style calculator baseline employ compression sub rank parameter baseline II evaluation chose memory intensive benchmark per instruction cache spec gap suite cache memory billion instruction execute billion instruction execute benchmark rate mode core execute benchmark evaluate thread mixed workload classify workload category highly compressible incompressible randomly benchmark category mixed workload VI RESULTS discus performance benefit attache sensitivity attach parameter performance performance gain attache metadata cache average attache speedup across benchmark KB COPR almost ideal data scramble irrespective benchmark data content memory access replacement average instance baseline data data scramble data memory tends random byte negligible bandwidth overhead replacement tends  performance simulation performance attache baseline employ compression average attache speedup ideal metadata cache synthetic rand benchmark highlight robustness attache regular irregular data consumption attache memory baseline employ compression average across MPKI spec benchmark attache reduces consumption ideal metadata cache synthetic rand benchmark highlight benefit attache irrespective data MB metadata cache speedup attache remains robust synthetic benchmark consistently speedup irrespective access metadata cache useful rand benchmark therefore slowdown performance gain attache bandwidth improvement memory bandwidth usage average memory latency average attache enables bandwidth average memory latency memory bandwidth usage average memory latency memory bandwidth improvement latency reduction due attache performance metadata cache correlate metadata cache rate instance kron slowdown rate metadata cache benchmark libquantum compressible application performance gain additionally metadata cache additional operation attache benchmark due rand benchmark attache additional bandwidth overhead metadata reduction benefit attache metadata cache average attache across benchmark ideal metadata cache saving saving attache sustain synthetic benchmark metadata cache useful rand benchmark therefore increase consumption overhead metadata cache correlate metadata cache rate instance kron rate metadata cache benchmark attache consumption incur additional metadata access benchmark libquantum compressible however application saving attache consistently baseline benchmark additional traffic due metadata cache additional memory traffic due metadata cache average metadata cache increase memory traffic cache eviction installs metadata memory furthermore additional request normalize memory request metadata cache average MB metadata cache incurs additional memory access due eviction installs request installs compressibility lifetime therefore metadata associate compressibility cachelines tends remain metadata cachelines tend predominantly sensitivity replacement algorithm rate MB metadata cache replacement policy DRRIP cache rate metadata cache tend rate therefore policy DRRIP increase rate metadata cache rate MB metadata cache replacement policy baseline lru policy rate policy increase rate sensitivity predictor component component COPR contribute performance benefit PaPR predictor alone speedup however combine GI global indicator speedup LiPR predictor useful mixed workload performance component COPR average mixed benchmark PaPR  predictor GI global indicator prediction accuracy vii related prior closely related data compression memory  compress data improve bandwidth memory  employ static sub rank metadata cache attache easily apply  framework reduce metadata overhead prior lee hong  compression algorithm latency decompression compression decompression latency improves performance reduce overhead metadata cache  propose efficient technique memory compression however compression technique focus improve memory capacity similarly  propose alternative technique implement compression memory primarily improve memory capacity technique compression focus primarily mitigate bandwidth overhead metadata non volatile memory compression reduce improve performance  describes challenge maintain metadata recommend ecc storage metadata predictor technique useful memory module ecc commodity memory module ecc chip attache applicable non ecc furthermore predictor  COPR COPR employ multi granularity predictor enables COPR compression status granularity compression useful increase effective memory capacity prior ibm mxt   balloon driver allocate unused memory data becomes incompressible virtual machine exceed capacity threshold similarly kim propose dual memory compression improve memory capacity attache bandwidth optimization technique improve memory capacity nevertheless attache efficient metadata storage data compression saving introduce compression technique transformation achieve   ratio exploit compression ratio bandwidth  address inefficiency memory compression technique consequent increase bus consumption compression naturally increase toggle efficient improve memory interface performance consumption unfortunately technique address metadata bandwidth overhead therefore attache easily apply technique data compression cache prior focus compression improve effective cache capacity prior focus similarity data content cache employ compression spirit compression dram cache improve capacity bandwidth dynamically improve bandwidth dram cache incur metadata overhead tag storage cache avenue metadata fetch data attache memory tend bandwidth constrain cache compression scheme compressibility attache easily scheme obtain metadata along data  technique  scc decouple compress cache easily attache fetch data metadata compression  improve bandwidth network chip compress cachelines compression primarily employ llc memory controller additional lane metadata interface relatively simpler noc processor vendor therefore customize however ddr interface standard therefore challenge transmit metadata memory interface without additional access nevertheless attache easily extend memory hierarchy data compression gpus memory bandwidth gpus propose lossy lossless compression gpu content attache orthogonal compression technology characteristic therefore attache easily apply gpus additional bandwidth saving relevant  cachelines compressibility compressibility varies metadata cache perform poorer COPR due compressibility traffic metadata cache dominate  propose compress data throughout memory hierarchy approach reduces overhead compression decompression hierarchy attache orthogonal approach approach improve bandwidth efficiency recent han  compression neural network significantly improve performance reduce primarily focus improve capacity however improve bandwidth framework attache network metadata bandwidth neural network technology stack memory non volatile memory tend ecc security primitive endurance optimization increase lifetime attache apply independently scheme broadly applicable attache optimize cache policy memory request schedule policy summary memory bandwidth constrain data compression practical technique improve bandwidth sub rank unlock bandwidth cannot simply employ data compression utilize bandwidth data compression relies metadata identify compressibility cacheline therefore access additional access metadata resolve issue completely metadata chip buffer processor scalable instance 6GB memory cacheline metadata buffer MB therefore prior propose cache metadata chip metadata cache observes metadata cache rate metadata cache request bandwidth overhead proposes attache technique  bandwidth overhead metadata access attache scalable technique implement without software 6GB memory hardware overhead attache KB furthermore attach replacement memory data cachelines incur cid collision overall attache performance ideal ideal reduce metadata bandwidth compression