historically improvement gpu performance tightly couple transistor moore slows performance gpus ultimately plateau gpu performance multiple gpus interconnects however limited inter gpu interconnect bandwidth 4GB hurt  performance frequent remote gpu memory access traditional gpus rely migration service memory access local memory instead migration fails simultaneously multiple gpus recent proposal enhance software runtime replicate local memory unfortunately fails frequent remote memory access address recent proposal cache remote data gpu cache llc unfortunately remote data cache fails data exceeds available gpu llc conduct combine performance analysis stateof software hardware mechanism improve numa performance multi gpu evaluation node multi gpu reveal combination schedule placement migration replication cache remote data incurs slowdown relative ideal numa gpu memory footprint tends significantly gpu llc replicate software footprint exist numa aware software hardware address numa bandwidth bottleneck propose cache remote data video memory carve hardware mechanism recently access remote data dedicate gpu memory carve outperforms theart numa mechanism within performance ideal numa gpu analysis cache coherence investigate overall dedicate gpu memory eliminates numa bandwidth bottleneck incur negligible performance overhead due reduce gpu memory capacity index gpu multi gpu memory numa HBM dram cache coherence migration replication introduction gpu acceleration improve performance hpc application historically transistor improve gpu application performance increase multiprocessor SM gpu generation however moore necessitates alternative mechanism multi gpus gpu performance independent technology node multi gpu inter gpu link bandwidth creates nonuniform memory access numa bottleneck multi gpu application performance offering computational resource  illustrates commercially available  DGX consist gpus dedicate bandwidth local gpu memory TB interconnect inter gpu link NVLink however due physical constraint inter gpu link likely bandwidth local gpu memory bandwidth GB TB consequently  potential substantially compute resource bandwidth discrepancy local memory bandwidth inter gpu bandwidth contributes non uniform memory access numa behavior bottleneck performance numa bandwidth bottleneck local memory unable satisfy majority memory request address runtime migrate local memory unfortunately fails concurrently access multiple node situation recent proposal enhance runtime replicate local memory node however unbounded replication significantly increase gpu memory capacity pressure average gpus limited memory capacity due memory technology constraint desire alternate reduce numa bottleneck recent investigates software hardware enhancement reduce numa performance bottleneck specifically annual acm international symposium microarchitecture doi micro relative performance numa gpu numa gpu replicate   numa gpu replicate  ideal performance numa gpu relative ideal mechanism replicates significant performance gap due limited inter gpu link bandwidth author propose schedule mapping cache gpu cache hierarchy refer baseline numa gpu performance numa gpu numa gpu enhance replicate ideal upperbound numa gpu replicates axis workload axis illustrates performance relative ideal numa gpu replicates across workload workload negligible numa performance bottleneck replicate remove numa performance bottleneck workload remainder workload baseline  slowdown eliminate replicate manifest due normal program semantics due false replicate collapse writes ensure data coherence unfortunately software overhead collapse occasional writes prohibitively consequently develop efficient software technique reduce numa performance bottleneck associate  hardware mechanism cache gpu cache hierarchy reduce numa performance bottleneck however conventional gpu llc unable capture application emerge workload address cache capacity recent propose dram cache  cpu investigate feasibility architecting dram cache multi node gpu investigates limitation combine ofthe software hardware technique address numa performance bottleneck multi gpu overall contribution software mechanism commonly gpus fail reduce multi gpu numa performance bottleneck furthermore cache remote data gpu cache hierarchy limited benefit due data footprint gpus augment  rely MB ensure tlb coverage reduce granularity minimize false severe tlb performance bottleneck cache reduce numa overhead knowledge combine theart software hardware technique improve numa performance bottleneck numa platform multi gpu platform propose augment numa gpu practical dram cache architecture increase gpu cache capacity cache remote data video memory carve carve dedicates gpu memory content remote memory carve transforms gpu memory hybrid structure simultaneously configure OS visible memory cache carve cache remote data dram cache latency bandwidth benefit cache local data perform detailed analysis implication dram cache coherence multi gpu specifically conventional software coherence mechanism gpus giga dram cache software coherence frequently destroys data locality benefit dram cache instead gpus extend hardware coherence mechanism reap dram cache benefit finally loss gpu memory capacity due carve compensate allocate cpu memory situation unified memory UM enables frequently access migrate memory gpu memory performance impact lose gpu memory capacity tolerable situation carve limited performance benefit overall important foundation reduce numa bottleneck emerge multi gpu platform perform detailed performance evaluation theart schedule software hardware cache carve multi gpu consist gpus migration replication carve incur respective slowdown relative ideal numa gpu satisfies memory request locally carve imperative numa gpu performance carve benefit incur loss gpu memory capacity finally loss gpu memory capacity incurs negligible performance overhead CHARACTERISTICS recent nvidia gpus fermi kepler maxwell pascal volta SMs BW GB transistor tech node chip II background motivation gpu performance gpu performance decade due increase transistor density gpu memory bandwidth recently announce volta gpu consists multiprocessor SMs chip consists billion transistor unfortunately impend moore limitation  manufacturing threaten continued gpu performance without denser gpu architect investigate alternative technique gpu performance recent advance packaging signal interconnection technology enable opportunity gpu performance recent propose interconnect multiple gpus package bandwidth interconnection technology GRS NVLink  programmer illusion gpu however non uniform memory access numa asymmetric bandwidth local remote gpu memory software hardware technique propose reduce numa bottleneck discus technique identify limitation numa aware multi gpus illustrates numa gpu performance directly dependent available remote memory bandwidth traditionally application programmer manually manage placement compute data ensure numa performance significantly increase programmer burden necessarily alternate numa reduce programmer burden recent numa gpu proposal software hardware technique reduce numa bottleneck without programmer intervention numa gpu enhances gpu runtime improve cooperative thread array cta schedule placement policy adjacent CTAs tend significant spatial temporal locality numa gpu exploit inter cta data locality schedule batch contiguous CTAs gpu furthermore numa gpu increase likelihood service majority memory request local gpu memory FT placement policy FT policy allocates memory gpu access private FT mapping ensures memory access service locally already mapped remote gpu memory data numa gpu cache remote data gpu organization bandwidth numa aware multi gpu distribute cta schedule memory mapping improve data locality cache hierarchy enable future reference service bandwidth ensure correctness numa gpu extends exist software coherence mechanism gpu llc illustrates software hardware mechanism numa gpu reduce numa bottleneck numa gpu performance bottleneck numa significant performance bottleneck application frequently access remote memory recent remote memory access numa false improve tlb coverage address false runtime OS replicate locally avoid remote memory access unfortunately replication increase application memory requirement average replication memory available future gpu workload entirely gpu memory capacity replication benefit gpu memory utilized besides gpu memory capacity pressure replication limited software overhead collapse occasional writes extremely expensive replication cannot reduce remote memory access remote memory access illustrate baseline numa gpu distribution gpu memory access private memory access private cacheline OS MB distribution memory access private cache granularity memory MB aggregate llc capacity memory capacity across gpus application numa gpu gpu memory access service consequently replication reduce numa bottleneck frequently access address limitation replicate numa performance improve cache data chip cache llc quantify llc requirement memory footprint workload memory footprint unique remote fetch gpus clearly cache remote data gpu llc unlikely benefit application memory footprint significantly exceeds aggregate llc capacity straightforward mechanism fully memory footprint increase llc capacity however llc capacity drastically increase memory footprint workload unfortunately semiconductor LLCs incorporate highperformance processor chip consequently alternative overhead mechanism increase remote data cache capacity gpus investigate increase gpu cache capacity cache remote data video memory carve hardware mechanism statically  local gpu memory replicate remote data carve reduces granularity data replication OS granularity MB finer granularity cacheline ensure program execution however carve efficient mechanism maintain coherence multiple gpus thankfully coherence bandwidth grain replication granularity distribution memory access data emphasize false investigate increase gpu cache capacity discus experimental methodology methodology proprietary trace driven performance simulator simulate multi gpu gpus interconnect bandwidth link gpu processor memory hierarchy nvidia pascal gpu model SMs per gpu warp warp scheduler selects warp instruction II workload CHARACTERISTICS suite benchmark abbr mem footprint hpc   GB HPGMG  HPGMG GB HPGMG  proxy HPGMG  GB lulesh  mesh lulesh MB lulesh lulesh GB comd xyz warp comd MB  particle  MB   GB   GB xsbench grid xsbench GB  euler MB MB bfs usa bfs MB ML alexnet convnet alexnet MB googlenet cudnn  googlenet GB  cudnn   MB bitcoin crypto bitcoin GB optix   MB triad triad GB random memory access  GB cycle gpu memory consists multi cache tlb hierarchy cache tlb hierarchy private SM cache tlb SMs assume software cache coherence across private cache commonly gpus simulation parameter baseline assume multi gpu NVLink technology 4GB bandwidth  assume gpu interconnect cpu NVLink 2GB infrastructure detailed dram 2GB memory capacity per gpu TB local gpu memory bandwidth gpu memory controller entry queue per channel policy minimalist address mapping policy FR FCFS schedule policy prioritize writes writes issue batch queue evaluate cuda benchmark application hpc application fluid dynamic graph machine neural network medical image memory footprint workload II simulate workload billion warp instruction baseline multi gpu gpus SMs max warp per SM gpu frequency 1GHz OS MB data cache KB per SM cache MB inter gpu interconnect GB per link uni directional cpu gpu interconnect GB per gpu dram bandwidth TB dram capacity 8GB architecting gpu memory remote data cache RDC enables cache remote data IV  gpu cache capacity numa bottleneck occurs memory access service remote memory bandwidth interconnection link numa conventional memory technology emerge bandwidth memory technology enables novel tackle numa investigate hardware mechanism dedicate local memory content remote memory enables remote memory data service local memory bandwidth context gpus approach effectively increase cache capacity gpu cache remote data video memory propose cache remote data video memory carve hardware mechanism statically reserve portion gpus package local memory remote data latency bandwidth advantage duplicate local memory data carve remote data carve refer memory remote data cache RDC avoid software overhead RDC entirely hardware manage invisible programmer gpu runtime amount gpu memory dedicate RDC statically boot kernel launch kernel execute gpu carve transforms local gpu memory hybrid structure simultaneously configure software visible memory hardware manage cache carve minimal exist gpu gpu llc gpu memory controller determines memory address service local gpu memory remote gpu memory request local gpu memory data fetch directly local memory llc otherwise memory controller data available RDC RDC data service locally without incur bandwidth latency penalty remote gpu memory access however RDC data retrieve remote alloy cache tag data enable latency access tag alongside ecc simplify data alignment cache controller gpu memory return llc data insert RDC enable future remote data cache RDC RDC architected granularity minimize false model RDC grain alloy cache organizes cache mapped structure tag data alloy access cache retrieves tag data tag cache data service request otherwise request memory implement alloy tag spare ecc commercial implementation RDC limited alloy architected alternate dram cache architecture gpu memory configure RDC carve register gpu memory controller specify location physical address RDC carve combinational logic identify physical gpu memory location RDC finally RDC architected local gpu memory RDC interleave across gpu memory channel ensure bandwidth concurrent RDC access remote data cache evaluation evaluate benefit carve 2GB RDC amount baseline 2GB gpu memory dedicate RDC RDC data memory however additional handle cpu gpu coherence assume gpu memory HBM technology HBM ecc data assume ecc byte granularity bus transfer SECDED data leaf spare ecc tag metadata RDC tag remote access numa gpu carve remote memory access carve reduces average remote memory access numa gpu relative performance numa gpu numa gpu replicate   carve coherence numa gpu replicate  ideal carve performance zero overhead cache coherence carve achieve ideal numa gpu speedup remain expose operating OS manage memory 0GB per gpu RDC numa traffic remote memory access numa gpu  enhance carve workload baseline numa gpu suffer remote memory access xsbench lulesh memory footprint workload gpu llc carve significantly reduces remote memory access average numa gpu remote memory access carve remote memory access carve efficiently utilizes local unused HBM bandwidth consequently carve significantly reduce inter gpu bandwidth bottleneck workload RDC performance analysis upper bound evaluate performance potential carve assume  cache coherence   coherence enables quantify benefit service remote gpu memory access local gpu memory investigate coherence subsection performance numa gpu  software replication carve coherence ideal numa gpu replicates axis workload axis performance relative ideal numa gpu carve enables workload lulesh euler HPGMG significant slowdown  numa gpu enhance replication approach performance ideal  workload reduction remote memory access extend remote data cache capacity local gpu memory remote data cache RDC significantly reduces numa bandwidth bottleneck carve performance improvement without rely software replication carve improves performance significantly across workload carve sometimes degrade performance  performance degradation due frequent RDC additional latency penalty access RDC access remote memory degrade performance  workload however overhead cache predictor mitigate performance outlier overall carve significantly bridge performance gap numa gpu ideal numa gpu average baseline numa gpu numa gpu enhance replication performance gap relative ideal numa gpu carve  performance gap relative ideal numa gpu replicates significant performance opportunity carve efficiently ensure data coherence  coherence implication RDC gpus carve potential significantly improve numa gpu performance reduce remote memory access however remote data cache RDC remote gpu memory efficient mechanism coherent fortunately gpu program model programmer api explicitly insert synchronization ensure coherent data access software coherence already conventional gpu investigate carve easily extend software coherence software coherence conventional gpu maintain software coherence kernel boundary enforce requirement kernel modify data memory flush dirty data kernel invocation gpu core update memory conventional gpus enforce requirement implement cache invalidate kernel boundary memory cache implicitly coherent IV kernel launch delay software coherence cache MB RDC 2GB cache invalidate MB cycle 2GB 4GB local flush dirty MB 4GB 2GB 4GB remote epoch counter invalidation eliminates penalty explicitly invalidate remote data cache numa gpus cache remote data gpu llc extend software coherence gpu llc invalidate llc kernel boundary similarly RDC coherence ensure extend software coherence mechanism RDC kernel boundary memory controller invalidate RDC entry dirty data remote gpu memory IV software coherence overhead RDC onchip LLCs invalidate flush dirty data chip LLCs tends microsecond tolerate within kernel launch latency overhead invalidate RDC millisecond incur millisecond latency kernel boundary significantly impact application performance consequently desire architecture efficiently invalidate RDC efficient RDC invalidation RDC tag valid dirty gpu memory invalidate RDC reading gpu memory RDC RDC invalidation latency bandwidth intensive alternatively physically invalidate simply RDC data stale previous kernel fetch date kernel epoch RDC cacheline instal easily RDC data stale epoch counter invalidation scheme invalidate RDC instantly RDC invalidation mechanism register maintain epoch counter  kernel gpu RDC insertion  kernel RDC cacheline ecc along RDC tag RDC tag  within RDC cacheline  associate kernel efficient RDC invalidation kernel boundary simply increment  appropriate kernel RDC logic ensures newly launch kernel consume previous kernel stale data rare  increment memory controller physically reset RDC cachelines valid  zero RDC architected dense dram technology enables counter per RDC otherwise impractical mechanism apply chip cache efficient RDC dirty data flush desire efficient mechanism flush dirty remote gpu memory kernel boundary writeback RDC RDC flush RDC invalidation mechanism longer physically reading RDC alternate mechanism dirty RDC dirty RDC kernel boundary dirty RDC flush remote gpu memory avoid chip storage overhead dirty propose RDC instead ensures dirty data immediately propagate remote gpu memory bandwidth remote gpu memory RDC identical baseline numa gpu configuration RDC evaluation writeback dirty  RDC RDC performs nearly within performance RDC vast majority remote data cached granularity RDC tends heavily bias consequently remainder assume RDC enable zero latency dirty data flush avoid complexity maintain RDC software coherence evaluate performance extend software coherence RDC refer carve software coherence carve SWC performance carve SWC carve coherence overhead carve coherence carve SWC enables xsbench perform carve coherence however despite eliminate overhead maintain software coherence cache carve SWC remove performance benefit RDC remain workload difference carve coherence carve SWC flush RDC kernel significant inter kernel data locality exploit  coherence motivates research improve carve SWC efficiently prefetching remote data kernel boundary scope instead investigate hardware coherence mechanism flush RDC kernel boundary hardware coherence carve SWC reveal important retain RDC data across kernel boundary software coherence potentially relaxed avoid flush RDC additional complexity maintain software directory stale data cached remotely invalidate explicitly functionality available conventional hardware coherence mechanism investigate extend RDC hardware coherence instead relative performance numa gpu carve SWC carve HWC carve coherence performance carve software coherence hardware coherence software coherence eliminates benefit carve hardware coherence enables carve reduce numa performance bottleneck desire hardware coherence mechanism minimal complexity augment RDC gpu VI cache coherence protocol gpu VI directory protocol implement cache broadcast invalidates remote cache request unfortunately invalidate request significantly increase network traffic invalidates cachelines avoid private cachelines  traffic reduce identify cachelines dynamically identify  cachelines memory tracker IMST memory location cacheline granularity propose IMST spare ecc cacheline node tracked IMST uncached private transition cacheline transition perform gpu memory controller writes local remote gpus remote transition similarly remote transition IMST global behavior cacheline cacheline  avoid probabilistically transition private local gpu writes broadcasting invalidates IMST differs conventional cache coherence MESI MOESI instantaneous cacheline residency cache IMST monitor global behavior cacheline beyond cache residency private   probabilistic update dynamic cacheline behavior uncached memory tracker IMST identify cachelines reduces invalidate traffic gpu VI IMST memory cacheline granularity spare ecc node refer RDC extend gpu VI coherence IMST carve hardware coherence carve HWC carve HWC leverage IMST avoid invalidates private cachelines gpu memory controller node receives request consults IMST cacheline avoid bandwidth latency reading IMST entry IMST entry along cacheline gpu cache hierarchy IMST entry identifies cacheline private invalidate broadcast avoid however cacheline detect invalidate broadcast generate carve HWC introduces negligible invalidate traffic grain RDC enables majority memory access reference private data data evident hardware coherence  restores RDC benefit lose carve SWC lulesh euler HPGMG carve HWC  ideal performance replicates carve summary carve significantly improves numa gpu performance extend cache capacity gpu cache hierarchy however carve efficient performance cache coherence mechanism ensure data execution investigation conventional software coherence mechanism increase gpu cache capacity flush cache hierarchy kernel boundary remove inter kernel data locality benefit capacity cache hierarchy consequently hardware coherence reap performance benefit capacity gpu cache hierarchy performance evaluation corroborate previous gpu cache conclude implement hardware coherence worthwhile endeavor future numa gpu RESULTS analysis carve significantly improve numa gpu performance however performance numa gpu enhance carve depends remote data cache numa bandwidth differential perf relative gpu numa gpu numa gpu replicate   carve HWC numa gpu replicate  ideal performance comparison carve software replicate carve outperforms replication  performance ideal numa gpu replicates locally local remote node potential impact application data memory remote data cache analyzes offs associate issue explores performance sensitivity proposal factor remainder carve HWC performs summary performance numa gpu  enhance replication numa gpu enhance carve ideal numa gpu replicates xaxis workload axis performance speedup relative gpu overall baseline numa gpu achieves speedup relative gpu  enhance software replicate achieves speedup however  enhance carve outperforms speedup carve service content remote local gpu memory numa gpu enhance carve  performance ideal numa gpu speedup carve significantly improves numa gpu performance without rely software replication sensitivity remote data cache cache proposal appropriate cache particularly proposal carve cache capacity consume gpu memory shed illustrate performance sensitivity carve across variety remote data cache RDC numa multi gpu relative gpu investigate RDC per gpu 5GB 1GB 2GB 4GB average carve minimal sensitivity RDC dedicate gpu memory capacity remote data cache 2GB 8GB capacity improve performance additional exist numa gpu however workload xsbench  HPGMG additional speedup aggregate 8GB RDC suggests runtime mechanism appropriate RDC important factor multi gpu performance optimization impact gpu memory capacity loss numa gpus enable unmodified gpu application across multiple gpus without programmer involvement application footprint remains despite across multiple gpus aggregate abundance available package gpu memory capacity consequently carve local gpu memory impact application ability inside memory numa gpu however application designer optimize workload numa gpu increase workload footprint maximize available gpu memory capacity longer generalize carve proposal  application optimize application footprint memory placement carve application footprint spill memory gpu memory subscription handle software manage runtime nvidia unified memory quantify phenomenon geometric slowdown across workload application memory footprint memory unified memory policy average gpu memory carve minimal performance degradation increase RDC substantially hurt workload throughput performance software memory gpu memory active research performance improve rapidly software memory gpu memory focus application data footprint carve focus hottest portion performance sensitivity RDC aggregate numa slowdown memory due carve gpu carve numa gpu carve 5GB carve 1GB carve 2GB carve 4GB 2GB 4GB 8GB 6GB speedup baseline replicate  carve replicate ideal default config performance sensitivity numa gpu inter gpu link bandwidth normalize gpu illustrate baseline numa gpu numa gpu enhance replicate carve ideal replicates technique remain largely orthogonal clearly intersection implementation offs account conclude carve likely worthwhile numa multi gpu carve potential reduce numa traffic carve degrade performance situation entire gpu memory application dynamic cache understand rapidly dynamic balance oppose balance optimal multi gpu memory sensitivity inter gpu link bandwidth inter gpu link bandwidth performance critical factor future multi gpu gpu gpu connection improve ultimately link bandwidth gpus trail local gpu memory bandwidth understand importance remote data cache ratio inter gpu local memory bandwidth examine performance gain carve variety inter gpu bandwidth axis varies direction inter gpu link bandwidth axis illustrates geometric speedup across workload evaluate baseline numa gpu performance depends directly available inter gpu link bandwidth exist numa gpu despite cache remote data gpu llc unable entirely capture workload conversely carve largely insensitive numa gpu bandwidth perform  gpu infinite numa bandwidth carve dedicates local memory remote data hence convert nearly remote access local access carve eliminates numa bottleneck minor decrease gpu memory capacity multi gpu numa bandwidth available unlikely rate local memory bandwidth despite improvement signal technology simply package perimeter available enable linear numa bandwidth quadruple closely couple gpus carve improve performance ratio numa local memory bandwidth grows worth carve relative performance actually increase numa link relative local memory bandwidth 4GB 2GB link scalability carve numa gpu exacerbate node multi gpu increase situation carve arbitrary node cache remote data locally RDC addition gpu node increase gpu memory capacity memory capacity loss carve minimal however increase node efficient hardware coherence mechanism directory hardware coherence mechanism incur significant network traffic overhead multi node frequent scenario directory hardware coherence mechanism efficient workload behavior suggests continued research efficient hardware coherence mechanism  carve enable gpu VI related conclusion exist multi gpus non uniform memory access giga dram cache architecture multi gpus optimization multi gpus already gpu performance workload integration multi node gpu employ context performance compute data application however program  explicit programmer involvement software apis peer peer access combination mpi cuda explores transparent multi gpu approach enables multi gpu utilized gpu enables unmodified gpu code hardware driver maintain performance technique reduce numa exist investigate cache remote data local cache reduce numa traffic CC numa  reactive numa mechanism granularity cache remote memory chip cache CC numa remote data granularity chip cache coma remote data granularity memory software numa switch granularity granularity goal prior numa proposal target reduce remote traffic remote memory content locally however  conventional gpu cache unfortunately semiconductor chip cache incorporate performance processor chip additionally desire reduce programmer effort software replication insight dedicate portion exist gpu memory content remote memory carve implement minimal additional hardware maintain programmer software transparency software replication reduce numa replicate remote node  proposes numa software replication migration access heavily bias replication service request data memory imbalance migration maintain bandwidth memory software technique rely software protection protection incur latency collapse replicate finally replication significantly reduce gpu memory capacity tends limited costly dram cache architecture emerge bandwidth memory technology enable dram cache important research topic typically target heterogeneous memory bandwidth memory technology HBM cache bandwidth denser memory technology ddr pcm  target gpu consists memory technology HBM GDDR specifically transform conventional gpu memory hybrid structure simultaneously configure OS visible memory data frequently access remote data dram cache application bandwidth cache controller  amd vega  intel knight KNL amd  video memory configure cache memory KNL bandwidth memory technology  allows user statically configure  cache memory combination purpose   reduce access heterogeneous memory hierarchy ddr pcm carve similarity  carve reduce inter gpu remote memory access inter gpu coherence ensure execution multi gpu application carve maintain coherence exist approach candy C3D propose coherent dram cache multi socket cpu proposal target maintain coherence HBM cache traditional DIMM memory candy proposes coherence directory dram cache cache coherence entry coherence directory C3D proposes dram cache simplify coherence reduce latency coherent memory access carve extends exist coherence proposal multi gpu proposes architecture extend software coherence vii CONCLUSIONS gpu performance decade due increase transistor density however slowdown moore significant challenge continuous gpu performance consequently multi gpu propose  demand gpu throughput commonly hpc data workstation machine workload  execution leap faith gpu application multi gpu communication profile optimize application avoid numa performance bottleneck advent transparent  technology application across multiple gpus execution numa gpus impact gpu workload explicitly optimize numa gpu numa performance bottleneck primarily due frequent remote memory access bandwidth interconnection network significant research investigate software hardware technique avoid remote memory access evaluation multi gpu reveal combination migration replication cache remote data incurs significant slowdown relative ideal numa gpu memory footprint tends gpu llc replicate software footprint gpus augment cache improve numa performance investigate hardware mechanism increase gpu cache capacity recently access remote data dedicate gpu memory mechanism significantly outperform theart software hardware mechanism incur minor gpu memory capacity investigate implication maintain gpu cache coherence increase gpu cache capacity interestingly conventional gpu software coherence gpu cache hardware coherence reap benefit increase gpu cache capacity overall cache remote data dedicate gpu memory improve multi gpu performance within ideal multi gpu incur negligible performance impact due loss gpu memory capacity