abstract recent commercial hardware demonstrate irregular gpu workload bottleneck virtual physical address translation gpu  thread SIMT execution generate concurrent memory access address translation access unfortunately address translation request tlb generate concurrent investigate reduce address translation overhead application concurrent request irregular perspective gpu wavefront virtual address address mapping typically byte cache cache granularity memory access walker implicitly address mapping entry PTEs virtual address VA however conventional hardware mapping associate request simply discard propose mechanism coalesce address translation pending neighborhood address mapping cache almost walker ptw already cache address mapping neighborhood scheme reduce access inmemory average gpu workload average index computer architecture gpu virtual address introduction gpus emerge compute platform massive data parallelism gpus leveraged highly structure parallel task matrix multiplication however gpus recently across broader application domain graph analytics data analytics computer aid computational finance memory access emerge application demonstrate irregularity access structure data dependent consequently spatial locality recent commercial hardware demonstrate irregular memory access gpu workload due address translation overhead alone evaluation simulator corroborates negative impact access perform author internship amd research irregularity address translation data access memory access cache tlb trigger sequential memory access prevalent architecture typically hardware walker ptw access memory desire translation gpu memory bandwidth hardware designer provision multiple independent  however tlb irregular gpu workload correspond request burst consequently significant queue delay critical execution aim reduce address translation overhead gpu workload relies observation concurrent request address translation virtual belonging neighborhood define request concurrent pending service overlap execution define virtual address neighborhood address mapping correspond entry PTEs cache pte byte therefore typical byte cache contains PTEs virtual address memory access ptw access memory granularity cache byte memory however byte pte translate request virtual address instead propose service concurrent request virtual address neighborhood immediately PTEs  cache PTEs cache consequently obviates later memory access cache otherwise request virtual neighborhood service ultimately reduction memory access performance reduce latency translation lookaside buffer tlb cache address translation entry tlb trigger latency desire translation memory annual acm international symposium microarchitecture doi micro extend mechanism exploit observation whenever ptw receives byte cache request pte scan pending concurrent request address neighborhood request service immediately disregard ptw effectively technique coalesces access memory concurrent request neighborhood mechanism typical hierarchical organization access upper non leaf similarly benefit coalesce access entry neighborhood structure radix translation leaf upper traverse hardware walker leaf node importantly leaf entry upper packed cache entry upper byte upper entry byte cache consequently access upper entry neighborhood walker leverage reduce memory access upper empirical evaluation neighborhood aware service request reduce memory access perform walker average evaluation demonstrates coalesce access leaf upper almost equally important propose enhancement irregular gpu workload average II gpu virtual memory subsystem impact irregular APPLICATIONS briefly discus baseline gpu architecture virtual memory subsystem quantitatively discus impact irregularity memory access gpu address translation mechanism execution hierarchy gpu gpus massive data parallel processing operates data concurrently gpu hardware resource typically organize hierarchy manage massive concurrency effectively depicts architecture typical gpu compute CUs computational gpu typically CUs gpu CU multiple instruction multiple data simd multiple lane execution gpu thread schedule simd bundle wavefront warp typically thread  wavefront execute instruction gpu tlb  tlb cpu IOMMU cache memory controller memory  tlb   tlb core cache tlb ptw core cache tlb ptw buffer  gpu simd reg simd reg simd reg simd reg cache scratchpad global data cache CU baseline heterogeneous architecture data simd instruction  SIMT model mapped simd execution mask gpu thread wavefront execution due gpu memory resource hierarchy CU private data cache scratchpad across simd within CU data access simd instruction reside cache hardware coalescer combine request cache access gain efficiency finally cache across CUs gpu virtual memory gpus gpus adopt programmability enhance feature mature become compute feature virtual memory svm across cpu gpu compliance promote standard heterogeneous architecture HSA gpus svm typically enable virtual memory commercial gpus cpu gpu via IO memory management IOMMU hardware detailed selective mirror cpu gpu OS driver focus former without loss generality depicts hardware component svm commercial gpu conceptually enabler svm gpu ability cpu via IOMMU hardware ppn ppn ppn null ppn ppn ppn ppn ppn ppn ppn ppn ppn ppn ppn ppn index index index index offset pointer ppn VA VA VA memory virtual virtual address translate virtual address gpu virtual physical address mapping cpu consequently virtual memory address translation IOMMU illustrates hardware walker IOMMU translates virtual address memory processor structure ary radix memory access virtual address broken virtual offset virtual translate physical frame concatenate offset generate desire physical address KB node node pointer node refer virtual address index index node pointer appropriate node node contains pointer node refer VA index index node appropriate node similarly index index node node leaf refer node contains physical frame correspond KB virtual address VA index index node desire pointer null correspond virtual address unmapped physical frame henceforth refer upper virtual address VA VA entry upper shade similarly virtual address VA entry upper observation exploit hardware cache PWCs PWCs recently entry upper PWCs reduce memory access upon upper entire upper memory access access leaf node contrast memory access PWCs memory access IOMMU typically multiple independent walker concurrently service request multiple walker important gpus demand memory bandwidth consequently concurrent request IOMMU TLBs cache recently address translation relatively primarily device TLBs network interface controller request typically queue IOMMU request buffer IOMMU buffer walker becomes service service request IOMMU buffer arrival gpu tlb hierarchy gpus typically sophisticated tlb hierarchy reduce tlb cache recently address translation entry avoid memory access multiple data access simd instruction reside virtual physical address translation hardware coalescer exploit locality tlb hierarchy access CU private tlb tlb tlb across CUs gpu portion translation request IOMMU gpu address translation request address translation request generate execute simd memory instruction load coalescer merges multiple request KB generate simd memory instruction coalesce request gpu tlb gpu tlb tlb gpu tlb request IOMMU IOMMU request IOMMU TLBs request queue request IOMMU buffer IOMMU walker becomes selects pending request IOMMU buffer walker performs PWC lookup completes generate memory access desire translation return IOMMU gpu TLBs mvt atx NW bic GEV speedup baseline ideal address translation address translation overhead irregular application irregular gpu workload data dependent memory access spatial locality pointer chase algorithm graph manipulation irregularity memory access irregular memory access memory access divergence gpu execution model although item within wavefront execute instruction access data distinct render hardware coalescer ineffective consequently concurrent tlb access generate execution simd load instruction furthermore request entry physical address VA index node writable user accessible  transparent PCD cache disabled access dirty MB global NE execute node tlb owe access locality irregular workload eventually address translation request queue IOMMU buffer service walker significant queue latency walker slows workload recent commercial gpu hardware demonstrate divergent access irregular gpu workload due address translation overhead simulation detailed broadly corroborate finding speedup achievable representative workload ideal address translation typical baseline gpu svm ideal address translation mechanism unrealistic translation happens cycle height performance lose due address translation overhead various gpu workload due address translation overhead aim reduce address translation overhead irregular gpu workload neighborhood aware address translation utilize observation multiple entry within cache entry byte therefore typical byte cache contains PTEs PTEs contiguous KB virtual memory define KB align virtual memory PTEs cache neighborhood leaf focus architecture format observation widely applicable walker access memory granularity cache cpu walker entry cache PTEs entire neighborhood however desire pte mvt atx NW bic GEV access neighborhood concurrent request leaf neighborhood typically walker calculate physical address cache discard however concurrent generate execution irregular gpu workload within neighborhood PTEs leaf ppn ppn virtual address VA VA respectively cache therefore address VA VA neighborhood virtual address leaf concurrent access address neighborhood uncommon CUs concurrently execute independent workgroups code execution consequently data access correspond request neighborhood virtual address smarter walker exploit neighborhood information reduce memory access access PTEs concurrent request neighborhood coalesce coalesce avoid later access cache service request another virtual address neighborhood empirically potential saving memory access coalesce neighborhood memory access leaf virtual address neighborhood concurrent pending service walker execution around average outlier mvt workload nearly random memory access concurrent request neighborhood virtual memory multiple upper entry within cache upper entry byte therefore typical byte cache contains upper entry mvt atx NW bic GEV access upper neighborhood concurrent request neighborhood upper entry virtual address VA cache virtual address VA VA however neighborhood entry MB align virtual address entry corresponds MB virtual address entry cache similarly neighborhood 8GB TB align virtual address respectively memory access upper neighborhood respective concurrent graph coalesce opportunity  average measurement capture potential reduce memory access upper exploit neighborhood knowledge presence cache PWCs summary typically entry cache walker access memory granularity cache empirically concurrent request entry cache neighborhood observation leveraged enhance walker significantly reduce perform service request IV implementation neighborhood aware  goal mechanism opportunistically coalesces memory access neighborhood virtual address demonstrate coalesce significantly reduce memory access perform walker modify IOMMU hardware walker important achieve goal whenever walker completes memory access pending request IOMMU buffer coalesce opportunity IOMMU IOMMU buffer ptw service gpu tlb  tlb cpu IOMMU cache memory controller  tlb   tlb core cache tlb ptw core cache tlb ptw buffer  virtual address PA PL ptw ptw ptw ptw virtual address PL req req req req req req ptw ptw ptw ptw CL enhance IOMMU performs neighborhood aware coalesce access hardware shade request coalesce opportunity additional IOMMU hardware depicts propose additional hardware IOMMU extend entry IOMMU buffer PL contains request coalesce another null PA physical address node identify PL request coalesce upper access null request coalesce finally flag CL correspond request coalesce access information request service walker explain later structure service  entry walker IOMMU entry  contains virtual address request currently service correspond walker currently access information utilized opportunity coalesce around KB additional structure described II IOMMU buffer request operation coalesce access opportunistically coalesce access leverage neighborhood information whenever memory access completes coalesce logic IOMMU correspond entry  structure virtual address access memory access leaf entire translate physical frame return tlb baseline coalesce logic scan IOMMU buffer pending request virtual address neighborhood KB align VA virtual address desire physical frame address request already available byte cache walker immediately return memory access upper neighborhood MB 8GB TB align virtual memory respectively explain coalesce logic scan IOMMU buffer request neighborhood access request physical frame available byte cache walker PA correspond entry IOMMU buffer update address PL update correspond access upper coalesce partially coalesce modify logic pending request service walker typically IOMMU selects request buffer coalesce logic however slightly modifies avoid service request coalesce access another walker specifically pending request IOMMU buffer entry coalesce CL coalesce indicates access service request entirely partially otherwise valid  entry scan compute neighborhood access respective virtual address access entry IOMMU buffer virtual address within neighborhood CL correspond entry immediately false request buffer basis PA PL IOMMU buffer entry request unset otherwise request partially coalesce VA VA IOMMU  IOMMU buffer ptw PA PL virtual address ptw virtual address PL req req req ptw ptw PA PA VA PA VA PA VA PA VA PA baseline IOMMU buffer ptw service req ptw  ptw completes  CL IOMMU  IOMMU buffer ptw PA PL virtual address ptw virtual address PL req req req ptw ptw CL IOMMU  IOMMU buffer ptw PA PL virtual address ptw virtual address PL req req req ptw ptw CL scan IOMMU  IOMMU buffer ptw PA PL virtual address ptw virtual address PL req req req ptw ptw CL coalesce access virtual address previous walker PL physical address node access available PA finally correspond  entry information request IOMMU buffer entry chosen request delete operation access coalesce neighborhood information initial request service IOMMU buffer address virtual address VA VA VA depict II depicts access entry cache neighborhood demonstrate observation leveraged coalesce access explanation assume independent walker IOMMU request req virtual address service walker ptw although walker ptw req req instead coalesce logic opportunity coalesce request req PL CL correspond IOMMU buffer entry status IOMMU ptw access buffer entry correspond req req update physical address correspond node respective address node req access cache ptw req opportunity req coalesce CL unset remove CL eligible service walker ptw however req CL remains coalesce req till leaf ptw access leaf address translation req req translation pte cache ptw req req req coalesce entirely access req req coalesce till req therefore ptw req access node evaluation methodology execution driven gem simulator model heterogeneous cpu integrate gpu modify simulator incorporates detailed address translation model gpu  gpu tlb hierarchy IOMMU inside IOMMU module model tlb hierarchy multiple independent parallel walker cache mirror hardware closely implement propose logic coalesce inside IOMMU module simulator unmodified workload OpenCL HC relevant parameter gpu memory address translation mechanism baseline sensitivity parameter II workload description workload respective  baseline configuration gpu 2GHz CUs per simd  per CU thread per data cache KB data cache MB tlb entry fully associative tlb entry associative IOMMU buffer entry walker entry IOMMU tlb FCFS schedule dram ddr mhz channel per rank rank per channel II gpu benchmark benchmark  description memory footprint irregular application mvt mvt matrix vector transpose MB  atx matrix transpose vector multiplication MB NW NW optimization algorithm MB dna sequence alignment bicg bic sub kernel  linear solver MB  GEV scalar vector matrix multiplication MB regular application ssp shortest algorithm MB lud lud upper decomposition MB clr graph algorithm MB prop  machine algorithm MB hotspot processor thermal simulation algorithm MB  footprint workload various benchmark suite polybench mvt  bicg  rodinia NW propagation lud hotspot  focus emerge gpu workload irregular memory access workload demonstrate memory access divergence bottleneck gpu address translation mechanism however workload demonstrates irregularity suffers significantly address translation overhead workload mvt atx NW bic GEV demonstrate irregular memory access remain ssp lud clr  fairly regular memory access workload regular memory access translation overhead scope improvement evaluation therefore focus workload category however regular workload demonstrate propose technique harm analysis evaluation quantitatively coalesce access workload factor speedup slowdown micro architectural parameter impact speedup slowdown performance analysis speedup neighborhood aware coalesce baseline workload irregular irregular workload regular workload mvt atx NW bic GEV ssp clr lud  speedup baseline coalesce coalesce ideal speedup neighborhood aware coalesce mvt atx NW bic GEV normalize access memory coalesce coalesce normalize access regular workload workload coalesce speedup access leaf coalesce speedup access upper coalesce coalesce speedup achievable ideal address translation cycle observation coalesce irregular workload around average workload GEV significant coalesce access leaf upper almost equally important workload benefit coalesce leaf node atx bic workload NW GEV benefit significantly coalesce access  however performance irregular workload ideal address translation scope exploration reduce translation overhead workload regular memory access however suffer overhead due address translation workload speedup ideal address translation however propose coalesce mechanism hurt performance workload therefore focus irregular workload remainder evaluation analyze source improvement contribute speedup neighborhood aware coalesce access reduces memory access mvt atx NW bic GEV normalize latency coalesce coalesce normalize average latency coalesce perform walker normalize memory access coalesce irregular workload correspond coalesce access normalization memory access baseline coalesce significantly reduce access memory average coalesce reduction access coalesce access leaf upper important normalize average latency workload metric measurement leverage neighborhood information reduce access previous workload latency correlate decrease access necessarily individual multiple concurrent walker baseline throughput increase due utilization walker coalesce without necessarily faster sensitivity analysis quantify sensitivity architectural parameter towards coalesce performance mvt atx NW bic GEV speedup walker walker walker walker speedup walker mvt atx NW bic GEV speedup baseline IOMMU buffer buffer buffer buffer speedup IOMMU buffer varied independent walker IOMMU walker typically increase address translation bandwidth increase congestion memory controller due concurrent impact walker thirty workload coalesce access walker height normalize baseline coalesce walker atx walker speedup atx coalesce baseline walker speedup remain significant independent walker around average walker however typically improvement due coalesce decrease increase walker independent walker reduces address translation overhead baseline increase bandwidth translation mvt exception trend deeper investigation reveal intrigue interaction walker tlb coalesce access increase independent walker rate completion consequently rate allocation entry tlb sometimes thrash tlb mvt headroom workload performance independent walker increase speedup coalesce mvt atx NW bic GEV speedup baseline KB KB KB coal speedup comparison cache pte access IOMMU buffer buffer allows coalesce access across request consequently consistently speedup increase buffer capacity summary opportunistic coalesce access neighborhood analysis irregular gpu workload average crucial coalesce access coalesce access reduce memory access perform walker around propose coalesce mechanism perform micro architecture configuration walker buffer VI discussion coalesce cache PTEs alternative coalesce data cache IOMMU recently access cache PTEs coalesce cache avoid memory access cache available cache speedup achieve cache baseline workload correspond cache propose coalesce scheme coalesce significantly speedup alternative employ data cache PTEs coalesce proposal KB additional advantage coalesce hardware walker ptw service multiple request fully partially however cache pte available newly cache ptw perform unavailability  significant queue delay service request coalesce reduces queue delay involve  perform something cache cannot achieve overall additional KB cam analysis  consumes per respectively consumption dram operation overall coalesce dynamic replaces costly access dram resident cheaper access cam structure saving due coalesce accounting additional consume hardware coalesce reduce average benchmark mvt atx NW bic GEV vii related efficient virtual physical address translation gpus critical requirement lowe explore gpu MMU lowe demonstrate coalescer tlb multiple independent walker essential component efficient gpu MMU baseline configuration importance wavefront warp scheduler tlb aware multiple entry upper reside cache exploit schedule exploit coalesce opportunity gpu cache coalesce IOMMU typically address translation bottleneck consequently improves cache significantly reduce memory access coalesce access neighborhood demonstrate gpu translation latency longer cpu gpu workload memory access divergence bottleneck due address translation overhead hardware propose tlb hierarchy baseline additionally propose cpu walker gpus however access cpu walker gpu feasible hardware due latency propose software manage virtual memory illusion memory partition gpu program physical memory address translation overhead presence multiple concurrent application gpu selectively bypass TLBs avoid thrash prioritize address translation data access reduce overhead propose virtual cache hierarchy gpu defer address translation till entire gpu cache hierarchy approach remove address translation critical harder execute gpu kernel multiple address another recent propose leverage identity mapping virtual physical memory accelerator avoid overhead hierarchical approach however define software hardware interface  physical address mapping leverage layout concurrent entry approach software OS modification orthogonal technique improves efficacy TLBs already baseline mostly orthogonal recently introduce scheduler achieve progress gpu application reorder address translation request IOMMU unlike coalesce reduces memory access service address translation request schedule orthogonal apply coalesce cpu virtual memory overhead technique employ reduce exploit address locality thread propose  cooperative tlb prefetchers propose exploit naturally contiguity extend effective TLBs enable tlb entry multiple  later propose PWCs efficient increase PWC propose TLBs structure propose  speculatively predicts virtual physical memory mapping hide tlb latency others propose leverage selectively bypass TLBs avoid tlb technique extend gpus OS software orthogonal neighborhood aware address translation propose conclusion hardware walker inmemory granularity cache cache typically contains entry therefore entry virtual address cache leverage observation effectively coalesce access extend mechanism coalesce access upper hierarchical multiple upper entry walker cache propose enhancement irregular gpu workload average