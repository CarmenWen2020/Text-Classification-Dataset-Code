dynamic neural network enable representation flexibility network fix architecture extensively deployed input induced network structure processing optimization training network persistency recurrent chip dynamic net possibly inhomogeneous computation graph input prevents cache recurrent gpu register therefore exist suffer excessive recur chip memory load compound kernel launch overhead underutilization gpu SMs software enables persistency matrix training dynamic neural network gpu training approach virtual persistent processor specialization vpps specializes backward propagation kernel contains register cache operation routine vpps virtualizes persistent kernel CTAs CISC vector processor execute instruction vpps greatly reduces overall amount chip load cache matrix chip simultaneously maximum portability assumption computation graph hence fulfil dynamic net requirement implement dynet abstract away complexity function user volta micro architecture unlike competitive vpps excellent performance batch delivers speedup training dynamic net index gpu neural network dynamic neural network persistent specialization register introduction availability data recent  neural network various machine computer vision recognition processing training neural network data compute intensive consume typically gpus accelerate training massive compute parallelism training neural net usually iteration input network extract gradient backpropagation model parameter author georgia tech converge desire network architecture commonly computation graph node operation node data tensor important neural network dynamic neural net oppose traditional static network computation graph input due input memory lstm due variable network structure input structure lstm network dynamic neural network machine researcher model specification freedom computational perspective however dynamic neural network complicate inhibit apply gpu optimization critical improve performance static network optimization onchip persistence recurrent persistent rnn exploit cache matrix recurrent neural net rnn chip inside gpu register file eliminate recur chip load via persistent thread however persistent rnn assumes static structure computation pre placement input computation graph node input tensor parameter network across input persistent rnn cannot apply contrast dynamic net input computation graph moreover operation predictable persistent rnn specifically craft expert applicable rnn variation gru user specifies custom rnn architecture technique readily applicable enable register parameter cache persistence dynamic neural network propose software virtual persistent processor virtualization vpps prior training jit compiles backward kernel specialized cache model parameter backward pas multiple possibly dissimilar computation graph vpps annual acm international symposium microarchitecture doi micro generates script execution cooperative thread array CTAs within generate kernel parameter model chip throughout backward propagation input batch eliminates fetch recur matrix chip dram construct kernel launch backward pas utilize information model parameter distribute across register file SMs specialize matrix operation matrix multiplication vector jit compilation kernel happens training loop due literal index register within kernel binary computation graph potentially input kernel persistent cta treat virtual CISC vector processor capable execute instruction accord script vpps encodes operation within batch input graph host cpu package kernel via host device memory virtual processor receives exclusively assign instruction interprets executes operation thread accordingly scheme maximum portability assumption structure computation graph input batch eliminates overhead associate launch numerous kernel enable execution operation backward pas update parameter gradient cuda kernel invocation operation batching multiple dissimilar computation graph fed framework training ability virtual processor task parallelism exploit implement dynet automate library user essentially benefit outside training loop pas information model parameter jit compilation cuda kernel nvidia runtime compiler  scene within training loop perform batch computation graph implementation autonomously script generation transfer kernel launch oppose approach delivers excellent performance batch contribution propose virtual persistent processor virtualization vpps software technique enable register cache model recur matrix dynamic neural net thereby greatly reduce overall amount dram load request discus efficient implementation vpps mechanic parameter distribution across gpu SMs kernel specialization virtualizing persistent CTAs CISC vector processor script generation execution optimization enhance concurrency automate vpps software library integrate dynet abstract away inner detail proposal comparison vpps performance boost training throughput organize II background dynamic neural net persistency recurrent model parameter motivates elaborates vpps experimental evaluation IV discus related VI concludes II background motivation training neural net iterative procedure training multiple epoch batch training fed network correspond prediction prediction contrast respective loss function negative softmax likelihood instance loss indicates quality prediction backpropagated network gradient update model parameter respective gradient enables model gradually enhance accuracy prediction systematically perform training specify neural network usually transform acyclic computation graph node denote operation denote array typically float usage content generate source node input destination node dynamic neural network dynamic net neural network architecture network computation graph input hence training input another parameter model reuse computation graph across input instance operation tensor depends upon input specify user network structure memory lstm sentiment analyzer application input unrolled input vector fed lstm output vector lstm instance mixed parse depict lstms parameter clearly input parse therefore network architecture consequently induce computation graph emergence dynamic neural net popularity framework  pytorch dynet construct computation graph  input contrast approach framework tensorflow caffe relies static definition network architecture  LV  ILFR ILFR ILFR ILFR ILFR  LW    ILFR ILFR ILFR ILFR ILFR ILFR ILFR ILFR ILFR input network architecture structure lstm sentiment analyzer application aim classify connotation positive negative chip parameter persistency within computation graph induced input batch input model parameter matrix usually input vector intermediate tensor vector framework dynet pytorch upon multiplication node matrix fetch gpu chip dram operation chip load matrix however account majority global memory load training recurrent operation hint opportunity cache model parameter chip avoid excessive memory access boost performance specially neural net workload mostly memory bound utilize gpu register file cache recurrent matrix chip training model vanilla rnns register access bandwidth access latency competitive chip resource memory register file memory SM persistent rnn persistent thread program style CTAs occupy SMs task decomposition handle within kernel persistent rnn disallows register file content invalidation kernel  persistent rnn matrix fix fetch distribute across register file gpu SMs kernel utilizes cached perform recurrent multiplication matrix vector persistent rnn kernel fusion cognizant model parameter reuse challenge mention register file content invalidate upon kernel termination operation matrix execute within kernel persistent rnn pre computation procedure computation graph observation nicely exist trend collective gpu register file pascal chip GP SMs currently volta chip GV SMs MB chip storage capacity overall register gpus multi chip module gpus                    average distribution chip dram load training dynamic neural net application dynet training setting benchmark IV ahead specific contrast dynamic net computation graph operation necessarily graph across input clearly cannot afford compile cuda kernel encounter computation graph runtime due relatively compilation addition cache SM register file explicit architected register index address within kernel essentially compiler realize specific register kernel binary specific matrix therefore declaration register array suppose parameter kernel compile reference content array literal index met cuda compiler defines array thread private local memory instead living inside physical register resides inside offchip dram cached neural net recurrent assume  computation expert developer persistent kernel focus research community optimize gpu performance dynamic net revolve around enable mini batching static network multiple input simply packed tensor increase data parallelism trivially achievable dynamic net exist framework pytorch minibatching manual default dynamic neural net online therefore invoke kernel per operation node input tensor node kernel extremely underutilization available resource SMs unoccupied plus kernel preparation overhead cpu kernel launch overhead gpu comparable kernel duration overhead overall training duration proportional frequency node degrade performance overcome issue tensorflow fold  operation batching implement dynet enable dynamic batching operation concurrent potentially dissimilar computation graph although mini batching reduce multiple kernel launch overhead underutilization SMs                    HFXWLRQ        UDSK                          overview virtual persistent processor specialization vpps persistency recur model parameter lack feature waste portion computation latency excessive chip memory load batch saturate gpu resource convergence instability poorer generalization accuracy discussion motivates enables register parameter persistency dynamic training environment specification computation graph batch input happens runtime processing graph account training  avoid overhead cancel exceed benefit virtual persistent processor specialization virtual persistent processor specialization vpps allows cache model parameter onto register training input batch dynamic neural network tandem construct source  propagation gpu kernel model parameter jit compiles generates script input batch transfer device dram executes kernel accordingly model execute input batch possibly dissimilar computation graph overview elaborate mechanic separately backward kernel specialization initiate training loop framework construct backward gpu kernel training batch kernel solely responsible propagation backward propagation parameter update invocation parameter model cached within register file kernel specialized compile parameter dynamically enable static register index assembles cuda source kernel function nvidia            RZ RZ RZ RZ RZ RZ RZ RZ RZ RZ RZ RZ RZ RZ RZ RZ RZ RZ RZ RZ RZ RZ RZ RZ RZ RZ RZ RZ RZ RZ RZ RZ         visualize robin assignment matrix register partition CTAs assumes CTAs width CTAs partition thread register per partition runtime compiler  generate kernel binary kernel source construct distinct directly parameter routine generator model parameter independent model specification matrix distribution matrix model parameter assign matrix register belonging thread CTAs avoid subscribe subscribe CTAs consume register virtually partition register available cta thread multiple partition distribute model matrix partition robin fashion visualizes strategy matrix assign inter cta register cache utilization imbalance minimize  creates matrix reduce overall kernel binary instruction cache persistent rnn matrix register specific warp matrix distribute across register warp cta allows coalesce load matrix gpu dram register eliminate synchronization warp inside outside cta perform matrix vector multiplication distribute matrix partition multiple consecutive matrix warp reduce remote atomic perform transpose matrix vector multiplication addition width cta avoid excessive internal usage register thread thread address byte architected register recent gpu micro architecture thread resident SM partition across thread dynet matrix default implementation performs transpose matrix vector multiplication without transpose matrix register utilization register file KB cta width fuse backward propagation propagation kernel avoid recur accumulation matrix gradient chip dram register partition gradient matrix fashion matrix gradient matrix register partition robin schedule depict clearly initialization operation routine generate matrix gradient discus detail discus offs involve cache gradient chip introduce automate decision optimization hyper parameter suitable partition CTAs per SM CTAs kernel occupancy expose parallelism warp resident SM however thread SM register available cache parameter CTAs width per SM aggregate matrix cached automate decision framework thread per SM impractical heavily limit partition hyper parameter partition cta width thread register per partition enforce warp maximum model matrix  partition formulate       thread fix  warp  minimum warp essentially transform decision  decision  assign warp reuse input vector operation matrix vector multiplication data locality execute transpose  multiplication nonetheless granularity assign task thread increase inter cta load imbalance profile load granularity determination advantage transformation  limited valid integer option zero instance model  cta per SM maximum  framework enable profile approach compiles argue cta width replace CTAs width however benefit application due majority tensor calculation kernel conservatively aside register per thread interpretation routine additional register cache vector matrix operation leaf register per thread cache matrix cta SM  DFKH               BRS                  DFKH                 RS SH  summarize kernel source structure highlight specialized runtime model matrix distribution register CTAs static arrow hierarchy multiple kernel valid option  training kernel  average computation multiple training batch incrementally kernel  performs measurement framework observes performance degradation arrives kernel  training framework kernel perform profile stage profile portion training average across training input epoch approach adopt tensor comprehension evolutionary auto tune training gradually converges perform compile kernel routine generation partition previous framework embed register partition dimension source code generate appropriate template function parameter function matrix operation overview structure source jit compiler register matrix operation device templated function source jit compilation operation matrix vector multiplication pas transpose matrix vector multiplication outer vector extract matrix gradient backward pas function argument partition index matrix per warp iteration warp template argument compile register array index addition matrix routine execute kernel routine parameter load dram register register gradient matrix initialization zero application gradient onto parameter static kernel kernel source without across model specification code typical operation neural network backward device function activation routine script interpretation script interpretation routine inner kernel loop script command interprets executes specification script detail script interpretation execution cuda kernel source construct framework assembles within  construct kernel binary worth approach resembles code specialization interpreter specialization dynamically load code gpu script generation execution vpps persistent cta virtual vector processor virtual processor mechanism execute operation accord placement node computation graph discus procedure mechanism execution script batch computation graph script contains exclusive instruction virtual processor processor utilize thread vector instruction script essentially persistent processor tensor content global memory execute operation synchronize strategy portable assumption computation graph therefore dynamic neural net model operation schedule script generation generate script batch computation graph framework sort node maximum depth calculate leaf node node incoming creates execution node parallelism node within exploit due independence guaranteed sort illustrate approach resembles depth batching dynamic net however unlike necessitate operation node regardless schedule possibly concurrent execution creates significant advantage approach task parallelism perspective batch sort framework traverse computation graph leaf node zero upon node operation associate node framework encodes complex    depth sort node   YY  YY  YY    YY  YY  YY    extract execution node      YSWU YSWU     YSWU YSWU   YSWU YSWU YSWU   YSWU YSWU YSWU   YSWU YSWU   YSWU YSWU FRS  YSWU YSWU FRS   YSWU     YSWU YSWU   YSWU YSWU YSWU generate script instruction virtual CISC vector processor content within bracket byte     distribute script instruction virtual persistent processor vpps vpps assume assume parameter cached across vpps parameter cached vpps signal instruction respectively inspire computation graph visualize  operation schedule script generation vpps schedule propagation prop schedule perform similarly instruction eventually decode execute thread within virtual CISC vector processor procedure instruction byte preamble encodes input tensor operation operation generate instruction node byte byte typically address node input output tensor content inside global memory tanh operation framework generates byte instruction byte operation input tensor byte output tensor address byte input tensor address address offset address globally memory pool inside dram byte tensor address instruction allocate memory pool application exceed GB naturally application consume memory assume float necessitate offset virtual processor tensor another reasonable assumption dynet neural net training framework tensorflow custom memory allocator tensor allocator typically request portion gpu dram upfront circumvent recur overhead cuda runtime memory allocation deallocation continuous virtual memory processor consumes enforce execution data visibility virtual processor utilize instruction signal instruction byte specify generate instruction node within virtual processor participate execute instruction signal instruction barrier index instruction node virtual processor execute instruction instruction barrier execution virtual processor barrier signal dependency node satisfied visible execute virtual processor initiate operation essentially enforce consumer producer relationship virtual processor involve execute node consecutive barrier thread inside thread  alongside  ensure correctness designate global memory location traversal computation graph batch collectively acyclic graph framework traverse reverse propagation instruction within backward traversal enable load distribution across virtual processor maximize parallelism framework accumulate assign load instruction dynamically target virtual processor minimum load empirically load metric operation proportional aggregate tensor associate relatively load operation related cached matrix computational intensity respect operation script kernel execution generate script execute within backward kernel device maximize throughput concatenate script virtual processor  pin host memory command initiate transfer script buffer precede prefix sum script virtual processor quickly index instruction upon invoke kernel virtual processor execute script interpretation routine embed kernel source thread inside virtual processor collaborate fetch assign script SM chip memory loop script instruction sequentially decode instruction execute associate operation fragment loop content decoder function switch statement instruction route appropriate operation script virtual processor exceed allocate memory handle fetch decode execute consecutive script multiple another outer  VFULSW xii VFULSW  SH        RS SH   VFULSW xii VFULSW  VFULSW xii VFULSW     HL       VFULSW    VFULSW xii VFULSW  VFULSW xii VFULSW   VFULSW xii VFULSW     HL        VFULSW  fragment content virtual processor script interpretation loop arrow execution thread virtual processor fragment resides within backward propagation kernel loop within kernel CISC RISC operation kernel operating tensor dram memory address source operand destination operand within virtualize CTAs RISC processor instead CISC explicitly intermediate resource schedule component wise specify chip memory location input operand operation fetch specify operation resource management introduces runtime overhead host training negatively impact training throughput contrary CISC abstraction CTAs offloads resource management nvcc handle kernel  plus overall neural network operation kernel limited incentive RISC abstraction additional optimization introduces optimization decision additional performance enhancement kernel execution asynchrony assemble execution script batch input independent previous input concurrently gpu execute script previous batch detail gpu execute  kernel cpu batch creates graph user expression input sort node traverse graph backward direction generate script virtual processor synchronizes device reuse pin host memory buffer  device script transfer approach essentially enhances training throughput enable concurrent execution cpu gpu gradient accumulation strategy cache gradient alongside matrix amount register gpu sufficient register avoid allocate gradient matrix kernel perform outer operation constitute gradient matrix instead pre allocate location gpu dram memory pool concatenate tensor participate outer matrix dense  multiplication primitive  library gradient matrix efficiently matrix automation implement vpps library dynet allows user header access feature modification exist implementation dynet function vpps prior training loop model parameter define user vpps handle  model jit compiles backward kernel scene parameter dynet model within training loop computation graph construct dynet internal mechanism input batch float   model  library execute backward propagation kernel computation graph pas model vpps query specification decay argument reference loss expression node propagation replaces dynet propagation backward propagation parameter update mention gpu kernel execution asynchronous respect host therefore vpps return calculate loss previous explicit synchronization gpu backward propagation request function gpu currently kernel return loss float   sync loss occasionally user intend ensure training epoch completely essentially abstract away complexity user finally implementation enable concurrent training multiple computation graph induced multiple input loss node computation graph regardless graph accumulate summation operation aggregate loss creates super graph loss node aggregate loss aggregate loss propagate gradient node super graph IV experimental EVALUATIONS perform evaluation nvidia titan gpu volta architecture CC SMs KB register file reference rate host intel xeon ghz via pcie centos linux release gcc cuda instal experimental structure lstm sentiment analyzer lstm analyze detail discus performance application lstm irregularly benchmark application structure neural network input parse input associate stanford sentiment treebank benchmark IV performance improvement performance  dynet  batching implement variant variant node construct supergraph maximum depth leaf node depth batching dynet DB active execute node agenda batching dynet AB  DWFKL HW HW  vpps training throughput lstm across batch variant dynet batching  batching dynet DB agenda batching dynet AB tensorflow fold TF fold hidden layer embed fix framework training throughput vpps dynet batch vpps performs dynet specially batch vpps excellent performance mini batch significant advantage machine researcher typically minibatches due convergence model update rate contrary dynet batch training rate vpps batch matrix vector multiplication dynet convert matrix matrix multiplication incurs overall memory load addition batch independent operation merge remedy SM underutilization vpps performance supremacy perform dynet variant batch batch average across batch vpps performs faster dynet training rate tensorflow fold reference implementation lstm generally vpps average dynet average chip memory load analysis batch vpps dynet AB    training  batch  USING vpps dynet AB analyze performance vpps dynet batching matrix load gpu memory incur training input vpps dynet AB batch vpps chip memory access batch grows dram load reduction dynet AB vpps due transform matrix vector multiplication matrix matrix multiplication analyze sensitivity parameter vpps training throughput dynet hidden layer increase hidden layer throughput decrease mainly computational load training vpps average training rate reduces transition hidden layer training rate hidden layer instead explain examine kernel occupancy hidden layer due additional register pressure vpps kernel occupancy cta per SM instead CTAs per SM reduce parallelism exploitation opportunity contributes training rate decline hidden layer therefore matrix slight performance decline batch anymore gpu computation load disallows cpu bottleneck training discus detail fourth hidden layer dynet matrix vpps training throughput execution breakdown analyze performance phase execution training duration cpu gpu activity batch plot normalize duration  DWFKL HW HW hidden layer  DWFKL HW HW hidden layer  DWFKL HW HW hidden layer  DWFKL HW HW hidden layer vpps training throughput lstm hidden layer across batch variant dynet  batching embed fix    DWFKL DWFKL UDSK      RS  HFXWLRQ lstm execution breakdown vpps batch embed hidden layer lstm device operation asynchronous respect host cpu gpu execution duration plot respect batch retrieve per input average actual vpps execution cpu gpu concurrently plot cpu gpu batch gpu kernel execution average longer preparation host gpu kernel execution bottleneck performance however batch per input average kernel duration shortens due exposure task parallelism cpu execution specifically contribution schedule backward schedule slightly increase largely due data structure cache cpu performance bottleneck batch explains decline vpps performance application throughput boost vpps dynamic neural network model exhibit architecture dynamicity BiLSTM  TD rnn TD lstm RvNN lstm simplify unrolled architecture dynamic neural network sample input induce BiLSTM directional lstm entity tagger directional lstm predict tag input  directional lstm tagger optional feature BiLSTM difference frequency corpus another directional lstm embed TD rnn delay neural network inspire adjacent embeddings iteratively recurrent embeddings proposition reuse composition function outcome input multi layer perception predict connotation TD lstm TD rnn transformation vanilla rnns replace lstms RvNN recursive neural net model TD rnn sparser binary construct input parse inspire  leaf node internal node transformation unrolled network architecture model computation graph BiLSTM TD rnn TD lstm due application parse RvNN  frequency corpus  computation graph plot training throughput application vpps dynet lstm  DWFKL HW HW BiLSTM  DWFKL HW HW   DWFKL HW HW TD rnn  DWFKL HW HW TD lstm  DWFKL HW HW RvNN vpps training throughput variant dynet  batching RvNN TD rnn hidden layer embed fix application mlp vector BiLSTM  embed  input model  english corpus training BiLSTM  stanford sentiment treebank training benchmark demonstrate effectiveness approach majority batch specially batch dynet fails matrix utilize SM resource approach superior performance eliminate recur load matrix BiLSTM batch throughput boost vpps dynet perform variant model computation graph comprise limited operation node TD rnn RvNN easy dynet batch operation vpps performance batch application jit compilation overhead BiLSTM  TD rnn TD lstm RvNN lstm prog compilation module load II jit compilation duration  backward propagation kernel vpps finally II jit compilation duration backward kernel benchmark setting described IV IV overall compilation aggregation program compilation cuda ptx module load ptx binary overhead suggests vpps later stage neural network development specification model parameter infrequently user jit compilation price training session database compile kernel non volatile memory disk ssd imaginable although knowledge serialization kernel binary  currently intermediate ptx related addition article throughput distinct resource virtualization vpps virtualizes cta persistent vector processor resource virtualization strategy propose vDNN virtualizes available dram memory utilizes host memory scene seamlessly illusion global memory neural network gpu virtualize register file curse static physical register assignment gpus RegMutex utilizes multi stage allocation approach address issue zorua virtualizes chip memory balance resource allocation difference virtualization vpps layer abstraction enables execution dynamic scenario wireframe virtualizes task node task graph enforces  relationship task hardware gpu maestro another micro architecture technique enable dynamic resource management partition profile gpus propose  abstraction flexible gpu task assignment propose spatial multi task gpus multiple application gpu resource simultaneously SMK warp slicer resource within SM application finally  virtualizes gpu interconnect decouple SMs onchip network router maximize communication throughput chip traffic fusion thread persistency register cache mention persistent rnn resembles kernel fusion model parameter explicitly cached vpps extends dynamic scenario enables benefit kernel fusion dynamic net XLA extension tensorflow recognizes jit compiles inter operation static neural network kernel utilized persistent thread PT eliminate load imbalance irregular gpu workload wald PT active thread compaction chen shen propose launch replacement dynamic parallelism gpus launch deploys PT reuse thread nest task vpps employ PT maintain active register inhibit invalidation PT broadly identify collective register file growth recent gpu micro architecture researcher utilize cache data reuse purpose various application binary multiplication similarity sort exploit memory another software chip resource cache task hardware software specialization although vpps exist hardware microarchitectural gpu hardware specialization insightful efficient tpu utilizes MiB chip memory indicates importance data locality application eyeriss dadiannao shidiannao emphasize chip storage reduce dnn memory footprint  accelerate neural network inference phase 3D memory caterpillar MAERI dnn accelerator configurable building specialized network architecture EIE scnn cambricon propose hardware accelerator specialized sparse compress neural net cnvlutin discus eliminate ineffectual operation neural network  specializes prune network respect underlie hardware architecture  synapse vector elimination  data fission  focus numerical precision variability augment dadiannao propose compute stack machine model onto specialized accelerator software taco specializes cuda kernel operation focus efficient sparse dense tensor finally   specialize warp kernel task VI conclusion virtual persistent processor specialization vpps recur parameter dynamic neural network persist chip throughout training input batch demonstrate approach unlike exist counterpart batch efficient utilization gpu resource addition described automate meaning machine researcher naturally express computation dynamic net without internal mechanic vpps enables training throughput boost