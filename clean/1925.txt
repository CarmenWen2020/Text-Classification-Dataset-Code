SSDs become storage component memory hierarchy ssd research demand explore future simulation integrate ssd subsystem environment however challenge exist model SSDs simulation SSDs compose upon architecture employ hardware CPUs dram interconnect network employ hardware component SSDs multiple device controller internal cache software module respect spectrum storage interface protocol ssd hardware software  storage subsystem environment parallel host introduce ssd simulation framework  namely amber model embed cpu core DRAMs various flash technology within ssd simulation environment enable data transfer emulation amber firmware stack dram cache logic flash firmware FTL hil obey diverse standard protocol revise host dma bus popular simulator functional timing cpu model gem propose simulator capture detail dynamic performance embed core DRAMs firmware flash execution various OS hardware platform amber characterize challenge simulate  mobile device purpose computer comprehensive analysis passive storage active storage architecture index simulator solid nonvolatile memory memory storage flash memory introduction non volatile memory NVMs flash storage widely compute domain role storage subsystem memory storage solid SSDs already replace conventional spin disk  compute device server performance computer leverage SSDs cache burst buffer hiding latency impose underlie disk data satisfy quality service qos service agreement SLA constraint simulation explore account amber project  source simulation framework available http  org ssd technology non trivial ssd subsystem simulation environment SSDs storage memory subsystem contrast conventional memory technology  independent complex computer architecture organization SSDs consist storage backend multiple flash package internal bus computation complex employ embed cpu core dram module memory controller hardware component parallel host simulated evaluation easily actual performance SSDs request communicate host storage interface serial ata SATA  flash storage nvm express nvme channel ssd OCSSD SSDs  host OS decision  subsystem simulation without storage interface software stack  overly simplify evaluation lastly SSDs employ firmware module optimize reduce performance disparity host interface internal storage complex impact user behavior workload execution environment unfortunately ssd simulation infrastructure available software stack simulation emulation data transfer lack ssd internal hardware software resource model insufficient environment widely ssd simulation academia capture functionality specific flash firmware flash translation layer FTL without model hardware resource ssd similarly recent simulator model computation complex therefore detailed timing simulation firmware execution knowledge exists simulation model integrate environment implement actual storage interface data transfer emulation mimic pointer multi queue protocol storage without model bus annual acm international symposium microarchitecture doi micro data movement cannot integrate emulate OS relevant software hardware component lastly none exist ssd simulator attach storage interface respect functional timing cpu model infrastructure enable simulation functional cpu overly simplifies host memory subsystem cpu execution timing motivate lack simulation accommodate SSDs functional timing parameter firmware introduce ssd simulation framework  namely amber accommodates ssd resource environment emulates software stack employ functional timing cpu model propose simulation framework modifies host bus implement device controller dma emulate data transfer spectrum storage interface protocol SATA UFS nvme OCSSD addition amber implement diverse firmware module detailed SSDs computation storage complex model applies flash optimization parallelism aware  partial data update scheme allows easily mimic performance characteristic exist ssd simulator evaluate performance underlie storage complex replay trace evaluate validate amber actually execute microbenchmarks user application OS enable knowledge amber ssd simulation framework incorporates computation storage complex within ssd diverse storage interface protocol data transfer emulation storage stack contribution summarize hardware software simulation storage ssd computation complex model integrate embed cpu core dram module controller bus capture detailed latency throughput execution flash firmware component armv ISA amber storage complex implement reconfigurable interconnection network simulate spectrum flash technology detailed flash transaction timing model integrate firmware stack computation storage complex thereby capture diverse ssd functionality cache garbage collection leveling address translation simulation framework realistic storage latency throughput incorporate ssd resource amber explore various dynamic ssd critical monitoring usage storage component application execution scenario enable SSDs diverse domain attach ssd location platform user demand enable controller hub hardware driven storage SATA UFS memory controller hub software driven storage nvme OCSSD implement mandatory command data transfer mechanism integrate hardware software simulation storage model gem revise gem tight integration data transfer emulation amber modifies gem model host dma data OS memory underlie storage protocol specific implementation host device controller memory reference pointer queue arbitration logic amber gem cpu timing functional cpu model execution OS version holistic analysis storage subsystem amber emulate data transfer execute storage stack host software device firmware implement diverse ssd subsystem amber analyze challenge specifically characterize performance impact employ operating mobile challenge regard UFS nvme passive active storage architecture OS ill tune schedule queue management scheme significantly degrade overall performance user application mobile compute revise performance ssd specifically nvme attach core exhibit performance UFS application cannot advantage nvme due default operation mode evaluation reveal cpu memory utilization problematic issue passive storage architecture employ host FTL OCSSD address future active storage nvme controller firmware stack ssd consumes host cpu passive storage consumes host resource II background architecture overview SSDs attach memory controller hub mch controller hub  via pci express pcie serial respectively mch directly cpu via bus usually manages component dram gpgpu nvme ssd device contrast  mch handle relatively device spin disk conventional SSDs due purpose connection mechanism mch  SSDs interface classify storage subsystem hardware driven storage software driven storage storage serial ata SATA universal flash storage UFS SSDs whereas nvm express SSDs nvme channel SSDs OCSSD storage overall storage efficient storage management efficiency host resource utilization latency bandwidth somewhat limited prevent advantage underlie flash medium due frequent hardware intervention queue interrupt management complexity overview architecture ssd internal architecture storage storage employ host controller device controller data communication SATA UFS interface protocol host cpu device driver issue request underlie storage manage interrupt service routine isr host controller host controller resides  unpacks pack multiple command payload data buffer host controller sends retrieves data device controller exists within ssd physical layer phy behalf host OS driver architectural challenge model storage data transfer host underlie storage multiple data cpu configures register issue request specifically data communication driver cpu construct pointer entry target memory dram transfer content host storage therefore host controller traverse memory pointer memory buffer phase host controller issue request communicate target device controller phys host controller manages data movement request hardware queue interrupt mechanism comparably limited addition queue management interrupt handle host serialize storage architecture performance bottleneck compute domain storage storage host controller component host manage software specifically storage device controller expose specific internal dram host designate memory via pcie refer address software module host storage stack host nvme OCSSD driver directly configure memory mapped memory mapped parallel visible device controller ssd directly data host memory addition interrupt storage manage request packet refer message signal interrupt msi msi device controller completes service directly writes interrupt packet another memory mapped host dram msi msi vector OS driver device controller manage request submission completion  msi vector non trivial queue consistent coherent host storage address challenge  storage per queue pointer doorbell register OS driver expose device controller pointer mechanism host device driver device controller synchronize enqueue dequeue status submission completion data communication allows storage employ queue queue entry refer queue host eliminate queue isr serialization issue however software module involve data transfer storage host resource storage ssd internals hardware architecture performance flash medium bandwidth capacity host storage interface SSDs multi channel multi architecture improve internal parallelism goal reduce bandwidth gap host interface flash multiple flash package multiple interconnection bus refer channel package across channel simultaneously flash firmware host request multiple offset address exist across channel flash package multiple physical span internal channel refer super super respectively multi channel multi architecture storage complex flash medium interconnection network computation complex consists embed cpu core internal dram device controller multiple core allocate flash firmware service address translation within ssd operating frequency domain host storage completely data underlie flash host driver controller writes buffer internal dram target device flash medium storage complex allows overwrite due erase  characteristic requirement specifically flash writes per whereas era FlashSim SSDSim MQSim ssd ext device bandwidth MB depth sequential FlashSim SSDSim MQSim ssd ext device bandwidth MB depth random FlashSim SSDSim MQSim ssd ext device bandwidth MB depth sequential FlashSim SSDSim MQSim ssd ext device bandwidth MB depth random bandwidth comparison device exist ssd simulator latency depth SSDSim FlashSim device ssd ext MQSim sequential latency depth FlashSim SSDSim device ssd ext MQSim random latency depth SSDSim FlashSim intel ssd ext MQSim sequential latency depth SSDSim FlashSim device ssd ext MQSim random latency comparison device exist ssd simulator nand flash timing tPROG  storage channel package internal dram channel rank 1GB chip bus width hardware configuration device per flash contains latency erase operation longer addition writes avoid interference disturbance multi mlc triple TLC flash technology software organization due aforementioned  characteristic exist flash storage employ firmware compatible conventional storage hide complexity flash management flash translation layer FTL component flash firmware prepares erase advance reserve FTL writes data reserve mapping input address logical address LBA physical address ppa ppn reserve runtime FTL migrates valid physical era remaps address migrate thereby secure available incoming request garbage collection GC era per limited due flash issue FTL performs GC task erase flash evenly distribute manner refer leveling addition flash firmware implement request parse protocol management schedule data cache specifically device controller exists storage storage manages data communication interface protocol define host host interface layer hil transfer data performs schedule atop flash firmware module data buffer internal dram ssd flash firmware cache data leverage dram performance hide latency impose underlie flash medium  hardware software CO simulation challenge capture realistic ssd performance ssd bandwidth latency device intel exist ssd simulator namely MQSim SSDSim ssd extension   FlashSim perform analysis disassemble intel device reverse engineer channel flash dram flash extract flash latency erase operation datasheets configure simulator device parameter refer  hardware configuration information extract KB trace flexible tester synthetic benchmark fio replay trace simulator mention depth none exist simulator suitable execute fio user storage stack environment replay generate trace within simulation framework evaluate performance device configuration bandwidth trend exist simulator bandwidth performance device specifically intel utilize bandwidth performance saturates queue entry random queue entry increase exist simulator exhibit performance curve completely device specifically exist simulator bandwidth linearly increase MQSim SSDSim exhibit constant trend ssd extension FlashSim curve saturate SSDSim contrast bandwidth trend device sublinear queue entry increase addition trend difference simulator exhibit significant performance disparity model device queue entry steady error MQSim writes intel simulator error serious specifically error SSDSim ssd extension FlashSim writes respectively latency trend latency  aforementioned bandwidth trend depth increase latency exhibit device due queue delay random sequential latency exist simulator increase latency queue entry increase latency exhibit performance curve device exist simulator depth exhibit  latency trend MQSim SSDSim ssd extension internal architecture gem  dma amber constant latency trend ssd extension linear trend curve unrealistic gradient FlashSim sequential writes MQSim latency behavior device latency sequential exhibit sublinear behavior increase depth ssd extension sublinear latency curve random writes latency variance modulate queue entry execution workload latency error incur MQSim SSDSim ssd extension FlashSim writes respectively exist simulator exhibit significantly performance trend device depth lack computation complex incomplete firmware stack omit storage interface protocol management absence host initiator host driver controller contrast amber simulate computation storage complex execute fio user employ hardware software component  environment overall amber exhibit performance trend device depth user latency bandwidth device average respectively detailed analysis later amber internal architecture depicts internal architecture amber model ssd amber model embed cpu core armv instruction architecture ISA decompose instruction module function granular manner allocate firmware component cpu core function procedure ssd firmware stack workload OS decision software emulation environment amber dynamic procedure instruction execution arithmetic instruction load etc monitoring  execution considers latency overlap flash operation thereby capture detailed per request  timing synchronous asynchronous service addition modify integrate multicore model embed cpu core integration enables amber estimate dynamic firmware execution specific environment execute runtime model internal dram memory controller cpu core ssd internal dram memory controller capture detailed dram timing parameter precharge trp address address delay tRCD CAS latency tcl memory reference various firmware component specifically memory model contains cached data metadata mapping information ssd simulation dynamically update flash firmware internal consumption ssd incorporate dram model dram module controller integrate dram model considers ddr memory refresh account memory controller policy interleave strategy storage complex modify integrate multi channel multi architecture capture detailed timing program tPROG flash memory access latency data transfer TDMA flash command operation integrate allows amber accommodate highly reconfigurable flash memory controller model accurately mimic diverse flash technology multi mlc triple TLC etc physical flash model built model modify flash measurement storage complex flash package model capture dynamic consumption data movement internal DRAMs package buffer flash register addition dynamically actual flash access consume load data buffer flash firmware stack amber firmware stack firmware stack hil schedule request queue protocol host storage interface defines hil storage performs schedule fifo however schedule request arbitration mechanism storage namely robin RR robin WRR hil fetch host request device queue communicate device controller manages storage physical layer phy data movement hil split request multiple internal request amber cache entry super request cached dram model computation complex underlie internal cache layer ICL ICL buffer cache data flash host controller driver data evict due replacement flush command host OS ICL retrieves correspond data internal dram composes  request address indicates super align LBA issue underlie FTL FTL translates request LBA super basis ppn ppn available reserve allocate ppn amber FTL performs GC leveling valid migrate greedy access benefit FTL submits super request underlie flash interface layer  schedule flash transaction parallelizes access across multiple channel parallelism user defines optimize ICL FTL aware flash internal parallelism enhance overall performance explain later IV software module amber firmware stack highly reconfigurable simulation model  diverse storage device  simulation environment ICL configure fully associative associative cache cache entry replacement policy lru random etc reconfigured similarly FTL realize mapping algorithm mapping various hybrid mapping algorithm pure mapping request scheduler  flash controller capture parallelism combination account internal ssd resource channel data transfer emulation contrast exist ssd simulator capture latency throughput timing calculator amber handle actual content request host storage data transfer emulation execute OS user application environment model dma integrate gem transfer data host memory internal DRAMs amber ssd architecture model host driver controller storage storage composes pointer entry indicates memory dma implement gem par pointer actual structure varies interface protocol storage defines detailed discussion later IV dma performs data transfer host dram underlie storage challenge dma implementation environment cpu model gem memory access timing service procedure software module storage stack functional cpu model  actual data service per request execution correspond data communication activity functional cpu employ simplify dram model specific timing OS execution however timing cpu model pipelined execution detail memory access timing software emulation therefore whenever host device controller  storage access memory bus dma involve handle memory access define multiple dma handler service queue mapped memory access queue entry reference command composition pointer traverse activity register gem timing simulation content transmission timing cpu model involve refer memory access transfer host storage perform completion msi msi functional cpu model aggregate data transfer activity request task pointer contains memory transfer timing cpu model dma emulates finer granular data transfer per request arbitrate memory reference device host controller OS driver entry pointer IV DETAILS storage storage illustrate ssd implementation environment SATA UFS respectively SATA host functional timing cpu model processor controller hub pch integrates mch  via bus whereas cpu model directly communicate host controller UFS SATA model pch attache underlie SATA pci endpoint pci configuration pci endpoint host adapter  generates command pointer queue correspond data payload implement  SATA host device controller gem advanced host controller interface  specification  communicates OS device driver ata  library expose underlie ssd simulation model  OS component actual storage volume specifically  expose register mapped memory command another communication information command contains entry related SATA native command queue  management communication information handle data transfer related packet refer frame information structure fis command command pointer contains memory address SATA pointer physical descriptor   memory mapped register OS driver manage request correspond fis payload data  fetch command  register issue request SATA phy implementation SATA phy exists simulator storage simulator device controller par issue SATA SATA UFS storage nvme OCSSD storage request pas hil phase dma gem retrieves target address memory  emulates data transfer completion interrupt host cpu manage device controller  UFS model UFS employ storage protocol management aforementioned SATA UFS datapath host cpu host controller slightly SATA UFS handheld compute host controller resides cpu soc directly memory bus implement host controller UFS transport protocol UTP refer UTP UTP bus advanced extensible interface axi instead pcie host cpu communicate UTP UFS host controller interface  UTP  functionally equivalent SATA  pci endpoint respectively physical layer UFS define phy exists soc storage address issue frequency domain UTP underlie device controller implement fifo queue within controller queue data transfer SATA   SATA UTP expose register via memory mapped address host cpu OS driver OS UFS host controller driver issue command refer UTP transfer request descriptor  command queue fifo queue entry SATA  queue entry contains  UFS protocol information  request response request issue UTP device controller par command data payload communication information expose hil dma emulates data transfer correspond completion interrupt manage device controller UTP storage storage detail amber implement environment nvme OCSSD employ pcie physical interface oppose  interface simplify host datapath remove host controller pch multiple pcie lane pcie endpoint exists ssd model endpoint xilinx fpga gen integrate pcie endpoint employ inbound outbound KB fifo queue convert pcie phy ssd chip interconnect bus advanced microcontroller bus architecture  model axi interconnect core multiple  nvme model nvme controller par pcie nvme packet device controller expose correspond information hil hil handle request collaborate firmware module ICL FTL  leverage protocol queue management module significantly modify gem  dma implement device controller mandatory nvme command optional feature nvme namespace management scatter  diverse demand host software communication strategy nvme controller OS driver nvme driver functionally storage queue mechanism nvme OCSSD storage nvme remove host controller instead  storage per controller queue OS queue entry queue logic couple submission queue SQ completion queue CQ synchronize OS driver nvme controller specifically OS driver issue service compose byte nvme request request SQ entry byte request code nvme command metadata namespace pointer nvme pointer implement physical prp memory KB implementation conventional scatter  version OS driver  service completion deliver nvme controller OS driver byte CQ request queue nvme driver controller synchronize msi register doorbell implement dma emulates transfer relevant data prp  nvme packet OCSSD model OCSSD override nvme interface protocol expose physical flash address host datapath hardware architecture pcie endpoint nvme difference nvme OCSSD OCSSD  ssd storage complex expose flash address host software whereas nvme flash management ssd active device specifically OCSSD allows OS user software tune underlie ssd subsystem PC platform mobile platform cpu intel nvidia jetson TX ISA core frequency 4GHz 2GHz L1D cache private KB private KB LI cache private KB private KB cache private KB MB cache MB memory ddr channel LPDDR channel II gem configuration avg KB avg KB ratio random random authentication server HR sql server HRS msn storage metadata CFS msn storage FS  display payload DAP workload characteristic employ FTL ICL host considers underlie ssd passive device enable OCSSD simulation environment implement OCSSD interface enable   module gem linux kernel host FTL OCSSD driver respectively  tightly couple exist OS nvme driver handle nvme service handle overridden OCSSD command feature management implement OCSSD subsystem ssd leverage nvme device controller hil feature OCSSD specification chunk physical information hostside OS memory latency data erase information  manages flash address underneath file OCSSD subsystem ssd simulation disables FTL ICL datapath firmware stack flash firmware optimization internal cache ICL cache associativities replacement scheme performance simulated ssd device writes immediately service internal DRAMs service  flash therefore performance directly impact physical data layout various internal parallelism address ICL performs parallelism aware  load multiple super exist across physical resource conflict flash advance  activate multiple incoming request exhibit data locality ICL multiple offset request cached dram increase frequency counter incoming request sequentially access address previous cache frequency counter becomes threshold reconfigurable ICL performs  FTL mapping super mapping algorithm increase internal parallelism thereby achieve bandwidth random writes significantly degrade overall performance writes introduce modify operation burden storage complex specifically target cache ICL evict flush super ICL access physical associate target super super storage complex introduces intermixed workload thereby resource conflict challenge optimization ICL writes eviction actual address target update FTL due  characteristic FTL address physical entry across channel FTL selectively remaps target exists channel within correspond super employ super hashmap evaluation methodology simulation configuration configure SSDs flash package program latency latency latency variation fundamentally exists mlc flash due incremental  program ISPP ssd interconnection storage complex computation complex implement via axi mhz  mhz interface link underlie flash  mhz armv cpu core execute ssd firmware default mapping FTL ICL configure fully associative cache model  internal dram ssd interface SATA nvme UFS OCSSD default configuration storage complex channel package etc device explore simulator evaluation apply storage complex configuration testbeds II  host configuration timing parameter gem workload important characteristic correspond description workload workload exhibit request KB average request HRS  KB KB respectively evaluation perform actual storage application user stack instead replay generate trace within storage simulator validation device addition device intel ssd device performance simulation samsung pro samsung ssd prototype samsung dct ssd prototype evaluate nvme SATA device performance significantly fluctuate untenable away therefore carefully device comparison target testbeds pro compose multiple mlc flash   pro bandwidth MB depth intel pro solid amber dash ssd ssd dct sequential intel bandwidth MB depth pro solid amber dash ssd ssd dct random bandwidth MB depth intel pro solid amber dash ssd ssd dct sequential bandwidth MB depth intel pro solid amber dash ssd ssd dct random bandwidth trend accuracy comparison device amber simulation latency depth intel pro solid amber dash ssd ssd dct sequential latency depth intel pro solid amber dash ssd ssd dct random latency depth intel pro solid amber dash ssd ssd dct sequential latency depth intel pro solid amber dash ssd ssd dct random latency trend accuracy comparison device amber simulation multi feature newly nvme ssd nvme interface protocol backend medium replace flash latency respectively configuration amber evaluate validation available simulation source code bandwidth comparison user performance trend SSDs simulated amber solid device dash average accuracy simulation target device plot micro benchmark evaluation writes sequential random amber bandwidth curve exhibit trend device queue depth increase specifically average accuracy amber simulated bandwidth intel sequential random writes exhibit accuracy closely bandwidth curve device device exhibit curve bandwidth accuracy amber latency comparison latency trend accuracy simulated target device queue depth user latency curve simulated amber almost overlap device micro benchmark depth increase accuracy simulation device difference simulation device latency intel pro relatively notable queue depth queue depth intel pro optimization specific optimization technique cache data battery dram dump underlie flash mlc publish public domain  performance device specific report various publicly available article validation bandwidth device amber evaluation increase KB KB error rate device correspond minimum error maximum error rate evaluation summary error calculate per  per fsim per  per  per fsim device performance amber simulation respectively performance curve simulation performance curve device amber simulation trend device performance error rate user host reasonable request across SSDs sensitivity intel error rate device random average internal optimization mention latency comparison provision validation perform fully target storage sequential writes steady perform evaluation steady performance ratio  OP evaluate performance KB KB reduce OP ratio default intel OP rate plot scenario stress randomly data entire steady SSDs amount data actual target volume simulation OP rate significant performance respectively mainly invocation frequent migration data flash package thereby introduce latency frequent resource conflict operating impact performance workload execute user employ version OS interestingly performance kernel kernel average respectively difference version linux kernel viewpoint solid amber dash ssd bandwidth MB KiB intel pro ssd dct pro dct ssd error ssd intel sequential solid amber dash ssd bandwidth MB KiB intel pro ssd dct pro dct ssd error ssd random solid amber dash ssd bandwidth MB KiB intel pro ssd dct pro dct ssd error ssd intel sequential solid amber dash ssd bandwidth MB KiB intel pro ssd dct pro dct ssd error ssd intel random performance validation KB KB normalize bandwidth KiB OP nvme SATA bandwidth MB HR HRS DAP CFS  kernel kernel performance impact OS nvme UFS nvme UFS nvme UFS nvme UFS nvme UFS  CFS DAP HRS bandwidth MB HR performance UFS nvme interface nand dram cpu intel UFS nvme instruction interface load FP arithmetic instruction compute handheld compute storage stack disk scheduler newer version kernel employ refine budget queue  assigns budget slice schedule request sector request refine   employ per queue optimize SSDs bfs unified mechanism merge incoming request improve performance sequential access contrast version kernel disk scheduler completely queue  remove anticipatory schedule mechanism allows kernel scheduler dispatch request thereby improve throughput newer version kernel reflect feature nvme SATA   generate request saturate ssd performance consumes cpu cycle schedule dig deeper OS bound issue prevent request increase amber host cpu frequency ghz ghz employ ssd ssd thanks flash technology performance device GB however kernel execution  protocol management interface ghz cpu degrades performance  performance kernel frequency 8GHz user performance enhance kernel 8GHz operation slash device performance future effort tune kernel scheduler aware storage stack reduce performance loss amber effort research vehicle functionality holistic simulation handheld compute user performance mobile personal compute device UFS nvme protocol respectively nvme performance UFS performance nvme exceeds UFS HR CFS  workload difference compute mobile described previous mobile core cannot generate request enjoy potential benefit nvme performance handheld device tablet compute future nvme storage access critical however nvme device significant technology enhancement specifically breakdown instruction ssd execution respectively cpu exists ssd hungry component budget handheld compute significantly optimization underlie ssd usage load dominant instruction account execution execute nvme execute instruction UFS within core handle nvme queue involve  doorbell amber report nvme internal cpu flash backend nand dram nvme device intel simulated UFS around majority consume internal cpu therefore reduce hardware automation mobile storage passive active storage performance nvme ssd active approach OCSSD passive approach passive approach service request active approach access KB random sequential specifically OCSSD throughput nvme ssd KB request mainly OCSSD utilize host buffer cache information directly request host underlie storage however access KB nvme exhibit average throughput due limited buffer 2GHz 4GHz 6GHz 8GHz bandwidth MB device user interface performance cpu frequency rnd rnd  seq seq  bandwidth MB KiB nvme OCSSD overall performance kernel cpu utilization sec nvme OCSSD  initialization kernel cpu utilization dram usage MB sec nvme OCSSD  initialization dram usage overall performance utilization active passive ssd mode cpu host interface cplx cplx cache FTL dynamic sup SA FS atomic timing minor HPI  SATA UFS nvme OCSSD cpu dram  SP SB ISPP con RA hybrid cpu dram nand exec queue amber  MQSim SSDSim ssd extension FlashSim standalone perf computation complex storage complex transaction schedule super incremental pulse program configurable cache  fully associative mapping dynamic firmware execution IV feature comparison across various simulator amber FlashSim SSDSim MQSim ssd ext FS simulation standalone gem amber execution capacity kernel driver contrast user memory kernel memory freely directly allocates buffer physical memory address instead virtual memory plot cpu utilization memory requirement OCSSD nvme device respectively processing fio consumes cpu cycle OCSSD nvme ssd due initialization however initialization OCSSD consumes core whereas nvme ssd cpu OCSSD   driver perform address translation memory cache physical flash medium management   driver dram memory nvme ssd kernel driver OCSSD allocates memory initialization phase reuses entire execution phase MB fio nvme protocol management basically MB memory driver memory requirement pressure ignore VI simulator  related literature simulator exist performance ssd model ssd extension  ssd extension popular simulator extends  ssd model simulator simulates mapping FTL built upon simplify flash model FlashSim extend mapping algorithm  associativities flash model queue storage complex contrast ssd extension SSDSim capture detail internal parallelism flash channel resource extract fpga platform however SSDSim model storage interface correspond queue mechanism MQSim model storage complex enhances ssd simulation comparison dram flash protocol management latency model partially latency computation complexity however cannot capture internal embed core latency capability ssd emulation load actual data content data movement omit model MQSim cannot storage stack host emulation mode gem capture timing without file storage stack execute ssd enable software hardware component host storage cannot explore checked repository simulation framework verify unavailability simulation ssd simulation built qemu ssd simulator communicate host emulation unfortunately  remove component storage stack achieve reasonable simulation  ide storage contrast  attach gem ide emulate data transfer extension employ nvme queue protocol handle pointer communication host storage simulation however simulator cannot accommodate diverse storage interface protocol atop cpu timing detailed memory model response software hardware module granular manner addition none aforementioned simulator capability capture ssd computation complex dynamic firmware execution measurement IV summarizes difference exist ssd simulator propose amber amber capture embed cpu core performance cpi firmware execution load arithmetic operation none exist simulator perform addition amber report dynamic consumption firmware stack execution account embed core internal DRAMs flash device modify  implement dma enables ssd emulation amber functional cpu timing cpu model  environment model host controller SATA UFS driver nvme OCSSD enable storage storage diverse compute mobile compute simulation diverse standalone simulator pure gem amber simulation amber slightly MQSim capture  characteristic reasonable simulation