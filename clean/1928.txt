atomic durability emerge persistent memory data consistency across potential failure atomic durability non volatile memory traditional ahead wal technique employ guarantee persistency actual data update wal mechanism recent propose  technique undo redo undo redo principle HW manager allows overlap transaction execution atomicity invariant satisfied although efficiency data writes optimize prior exhibit offs performance various access undo approach performance degradation due synchronous inplace data update contains undo redo approach synchronous data update however increase amount writes prior redo approach demand extra nvm bandwidth indirectly update data overcome limitation previous approach proposes novel redo redu performs asynchronous data update nvm redu exploit dram cache remove nvm writes critical experimental propose mechanism performance variety performance previous undo redo undo redo approach respectively introduction non volatile memory nvm technology pcm stt  ReRAM emerge persistency storage density dram nvm access latency closer dram prior generation flash technology latency density persistency NVMs storage memory scm persistent storage memory interface however persistent storage medium scm atomic durability transactional execution potential failure atomic durability failure atomicity data update persistent memory transaction persistent atomicity failure traditional mechanism atomic durability ahead wal technique update data nvm persistent memory interface instruction clwb delineate completion persistent memory interface application data update entry persistent data nvm however critical disadvantage SW technique performance overhead additional instruction within transaction increase execution cycle coarsegrained unlike SW technique recent investigate potential benefit HW mechanism reduce overhead hiding critical transaction execution HW overlap instruction execution transaction modify data nvm HW technique prior approach adopt classical undo redo undo redo principle content principle categorizes HW mechanism efficiency data update efficiency critical aspect reduce amount writes request identifies technique efficiency coalesce combine multiple entry contiguous address reduce entry pack assembles multiple request burst request reduce request aspect HW data update mechanism wal commit data eventually reflect nvm data update data update conduct synchronously asynchronously transaction commit synchronous update cache flush operation critical annual acm international symposium microarchitecture doi micro update data nvm commit asynchronous update postpone data update commit transaction shorter transaction commit latency data nvm directly cache hierarchy update indirectly indirect update nvm update bandwidth efficient indirect update extra nvm reading update nvm data aspect advocate asynchronous update nvm reduce critical latency transaction  unnecessary nvm although data update efficiency factor improve none prior efficiency undo approach limited synchronous update model data undo redo approach allows asynchronous update increase amount writes redo approach asynchronous update previous redo approach suffers performance degradation due indirect update overcome limitation propose redo update redu pursues asynchronous data update nvm combine efficient coalesce pack transaction commits redu flush update cachelines cache synchronously dram cache reserve dram exploit cache nvm data dram cache asynchronously flush nvm dram cache decouples nvm data update critical transaction eliminate nvm data update prior redo approach experimental redu performance variety previous undo redo undo redo approach sequential redu throughput redo undo redo respectively random performance undo undo redo respectively prior approach performance scenario exhibit performance offs scenario redu outperforms prior approach contribution explores hardware assist aspect writes data update identify optimization writes desire data update investigates previous approach efficient data update prior approach exhibit performance offs clwb sfence data clwb data sfence data SW HW code SW HW persistent memory proposes redo performs data update directly dram cache performance benefit data update asynchronous update nvm organize II discus hardware mechanism persistent memory discus  offs previous approach detail redu architecture IV performance evaluation summarize related VI conclusion II background atomic durability nvm persistent memory challenge consistency memory data presence failure memory data structure consistently update recover program data hardware software crash crash partial data nvm leaf inconsistent avoid inconsistency atomic durability transactional update data structure persistent memory atomic update persistent memory mechanism ahead wal propose prior principle ahead perform phase data update writes serialize enforce softwarebased scheme cacheline flush  clwb fence sfence instruction programmer responsible manually instruction creation data update illustrates source code unfortunately software approach incur burden programmer significant performance degradation due coarsegrained throughput degradation SW version benchmark HW assist elaborate advantage HW assist focus improve HW assist mechanism txid addr txid addr txid addr undo entry redo entry undo redo entry entry persistency model assumption previous assume atomic durability persistent memory durable transaction within durable transaction commit manner assume fully hardware creates writes entry persists data programmer annotate transaction boundary   interface explicitly entry cache flush instruction code  approach transaction guarantee atomic durability data update assume isolation transaction parallel transaction conflict access data software mechanism grain lock addition nest transaction orthogonal atomic durability future hardware   overcome overhead software recent propose HW assist technique hardware assist uncacheable nvm addition hardware component dependency data writes enables finegrained without memory fence instruction significantly performance explore data writes hardware assist respectively discus advantage disadvantage previous approach taxonomy generalize various direction writes aspect content optimization entry categorize undo redo undo redo entry undo approach preserve data failure reverts transaction execute redo mechanism creates entry version data recovers crash discard uncompleted commit version nvm data lastly undo redo technique flexible others version DA mda mda address data non coalesce entry granularity data entry non coalesce entry cacheline granularity entry coalesce entry entry entry reduce coalesce entry  align data data data data data data memory writes mem writes pack pack writes generate request without pack meta data address shade waste due unnecessary writes granularity optimization addition granularity entry impact performance granularity optimal data update transaction suffers inefficiency entry efficient address meta data cacheline granularity potentially waste nvm bandwidth unmodified data portion cacheline modify therefore fix granularity optimal performance instead fix granularity optimization coalesce variable granularity efficiency application behavior optimization coalesces entry contains data contiguous address transaction ID entry reduces relative portion meta data address entry entry data fix granularity duplicate address unnecessary writes unmodified data portion cacheline coalesce entry contains data update address portion minimize optimization realize combine buffer processor entry coalesce cacheline optimization reduce nvm writes entry entry burst memory bus nvm writes entry instead KB rand normalize cycle per transaction vector KB rand swap KB rand hashmap KB rand KB rand RB opt coalesce coalesce pack cycle per transaction cpt reduction optimize writes normalize coalesce pack issue individual entry pack optimization locates multiple entry reduces nvm writes describes concept pack optimization consist multiple entry entry entry arrives nvm controller sends data immediately nvm meta data address buffer address buffer entry transaction commits nvm controller flush meta data buffer burst observation undo redo approach undo redo approach data increase critical transaction execution overhead undo redo drawback approach although advantage data update optimization coalesce pack equally applicable approach difference transaction throughput approach without optimization significant breakdown throughput undo redo without optimization label rand indicates transaction modifies random item refer detail evaluation setup sequential random coalesce pack substantial reduction execution cycle undo redo mechanism exhibit performance improvement optimization taxonomy data update writes data update nvm modify data transaction nvm classify approach aspect synchronous asynchronous indirect SW HW initiate cacheline flush synchronous asynchronous related update modify data nvm synchronous update writes modify data transaction commit asynchronous update defer cpu data cache buffer memory controller cpu data nvm update indirect update buffer memory controller cache processor comparison indirect data update hardware assist data transaction commit undo approach allows synchronous update model undo entry contains data failure data loss modify data nvm transaction commit without synchronous update guarantee version data persist nvm synchronous asynchronous update synchronous update cache flush operation critical transaction entire flush operation longer critical transaction execution contrast asynchronous update allows transaction without update data inplace asynchronous update nvm bandwidth parallelize data update commit transaction writes transaction improve transaction throughput asynchronous update opportunity utilize nvm bandwidth reduce critical however asynchronous update increase complexity cache modification cache previous transaction indirect determines component update data update nvm illustrates indirect data update update version data directly cache nvm flush modify cachelines transaction discus indirect update update data nvm modification cache hierarchy information update data entry however indirect update fundamental limitation data fetch indirect update additional nvm operation therefore consumes nvm bandwidth update waste bandwidth reading redo undo redo combine indirect data update version SW flush HW flush update data cache flush operation flush cache nvm flush cachelines aid software cache flush previous data update limitation granularity optimization flush undo cacheline pack synchronous nvm SW flush critical  undo none synchronous nvm SW flush critical wrap redo none indirect asynchronous nvm extra nvm fwb undo redo coalesce asynchronous nvm HW flush demand writes redo coalesce pack synchronous dram asynchronous nvm HW flush consumes dram capacity summary comparison previous approach hardware assist instruction clwb transaction another option hardware cachelines modify transaction flush transaction commits software aid cache flush imposes responsibility modify address flush instruction software coarse grain sfence however hardware assist flush hardware cachelines update transaction remove unnecessary stall memory barrier instruction observation narrow undo approach data update synchronous however synchronous update transaction update data nvm commit redo undo redo approach combination synchronous asynchronous HW initiate cache flush achieves performance transaction throughput nevertheless none prior undo redo scheme however shortcoming discus advantage disadvantage previous approach prior approach revisit approach previous discus offs choice summarizes prior characteristic undo approach prior hardware undo approach commonly fix granularity cacheline data fix granularity incur inefficiency writes unmodified data generates entry undo approach data update directly synchronously prior  employ synchronous update addition approach flush programmer insert  cache flush instruction another optimization undo approach explore hardware initiate cache flush however undo approach inherent limitation longer critical due synchronous update model redo approach option previous redo approach performs data update update data nvm background reading entry version data retire although nvm controller operates retire background non  consumes additional nvm bandwidth fetch entry nvm bandwidth consumption retires report bottleneck perform  transaction indirect commit potentially limit throughput transaction execution undo redo approach recent undo redo approach hardware assist novelty undo redo resides perform data update directly asynchronously status cacheline update transaction machine periodically scan flush dirty cachelines nvm fwb addition undo redo approach flexibility approach previous undo redo approach excellent option data update flexibility limitation version data increase amount writes nvm IV architecture scheme fulfill goal optimization writes coalesce pack enable efficient variable granularity data update efficient data update nvm data update nvm perform asynchronously directly goal cache flush operation initiate hardware instead cache flush instruction memory barrier propose redo approach pursues asynchronous update nvm redu summarize approach enable technique realize approach exploit dram cache nvm writes cachelines modify transaction flush cache hierarchy dram cache transaction commits data update dram cache unlike prior redo data dram cache enables update redo principle allows asynchronous data update nvm enable hardware initiate cacheline flush hardware cacheline private cache flush modify cachelines achieve variable granularity scheme employ optimization coalesce pack architecture redu shade component redu dram cache dram buffer propose address  prior redo approach  occurs redo data undo undo redo cachelines contains safely evict nvm directly update data persist data cache eviction regardless transaction commit prior redo data update evict cachelines directly update nvm data without buffering evict cachelines exist critical overhead update subsequent redo exist mitigate overhead evict cachelines temporarily dram buffer correspond entry retires subsequent cache dram buffer instead however volatile buffer propose previous limitation discard data buffer version data fetch data update extend volatile victim buffer cache update inplace nvm data directly dram buffer improvement eliminates extra nvm entry retire limitation prior redo approach nvm latency version reside victim buffer cache lookup dram buffer access nvm increase latency indirection incurs dram dram buffer nvm access however actual update dram cache exploit probability dram buffer propose filter approach mitigate overhead update dram cache asynchronous data update non trivial hardware maintain  commit currently progress transaction otherwise cache cache core cache ctrl buffer core processor nvm dram dram cache HW filter redu architecture offset addr addr data hash offset data overwrite valid txid data overwrites exist address offset address evict cacheline whenever cachelines evict address entry marked remove however impose significant modification exist hardware non negligible performance overhead previous redo approach avoids challenge perform indirect data update indirect redo scan identify permit update data entry addition previous undo redo approach address challenge substantial hardware extension cache hierarchy instead inefficient indirect update complex hardware extension transaction commits redu synchronously flush modify cachelines dram cache data update nvm asynchronously dram cache explain IV architecture redu synchronous update dram cache efficiently reduce overhead impose asynchronous update dram replaces nvm writes dram writes transaction transaction cacheline private cache cacheline transaction evict cache flush dram cache update dram cache affect correctness transaction commits cache controller scan cachelines private cache flush transaction dram cache organization transaction management dram cache cachelines flush cache hierarchy dram cache accepts data cachelines explicitly flush transaction commits nvm arbitrary already become persistent cachelines evict replacement policy transaction commits cachelines flush nvm correspond transaction commits distinguish cachelines dram cache associate transaction semantic transaction commit manage incoming outgo data dram cache nvm maintain offset transaction offset translates physical address incoming data relative offset within dram cache entry offset consists physical address transaction ID valid dram cache offset fix offset entry cacheline flush cache hash function return index offset bucket address bucket contains address incoming address meaning exists dram cache incoming cacheline overwrites exist data address entry entry allocate replaces exist victim entry nvm replacement policy explain IV transaction TT maintains information related transaction TT entry transaction ID txid address indicates cachelines associate transaction ID dram cache  entry txid global address cacheline arrives dram cache TT entry transaction ID incremented flush dram cache decremented transaction commit notify  offset TT entry offset transaction transaction completely retire dram cache update data fully reflect nvm TT entry commit retire transaction offset commit zero indicates cachelines belonging transaction already update nvm dram cache dram reserve dram cache dedicate DIMM stack memory dram cache configurable booting described fourth data dram cache MB MB dram capacity offset transaction evaluation data dram cache MB overflow outstanding transaction exceeds cpu cache dram cache transaction abort software programmer worthy overflow extremely rare transaction memory persistent data structure KBs footprint data update data nvm update dram cache policy determines flush data nvm cachelines dram cache nvm anytime correspond transaction commit policy configure dram cache buffer data temporal spatial byte threshold cache linux kernel investigates police eager  lru policy eager policy flush cachelines nvm eager policy minimizes dram cache reduce filter mechanism lru policy update data dram cache evict lru policy dram cache lru policy merge update dram cache data request operation directly dram cache nvm latency nvm request dram cache version data dram cache nvm request dram cache without access nvm dram offset another data response delayed dram cache lookup latency mitigate overhead dram lookup latency redu hardware filter avoid unnecessary dram nvm access HW filter policy eager policy flush cachelines immediately dram cache infrequent filter suitable therefore HW filter eager policy implement counting bloom filter consists entry entry hardware SRAM data dram cache counting filter accurately report dram cache however false positive non cached data reporting status erroneously filter return dram cache nvm request dram cache otherwise skip dram lookup fetch data directly nvm uncommon false positive dram cache dram cache lookup subsequent nvm access initiate sequentially insertion eviction dram cache correspond bloom filter entry incremented decremented lru policy dram cache eager policy implement transaction txid discard grows offset initial transaction txid discard grows offset transaction remove transaction txid discard grows offset transaction remove removal ing counting filter capacity overhead instead counting bloom filter redu non counting bloom filter false positive negative bloom filter return membership dram cache request access dram cache dram cache data nvm access sequentially filter report request dram cache access nvm simultaneously simultaneous access waste dram nvm bandwidth consume non counting bloom filter lru policy consists entry entry hardware KB SRAM non counting bloom filter reset false positive rate exceeds threshold discussion management assume global nvm accessible memory controller register manage entry sequentially append global address offset indicates address valid remove address register entry removable correspond cachelines dram cache flush nvm however non trivial scan remove entry entry transaction interleave instead propose remove sequentially allocate processor 2GHz cache private KB cache private KB cache MB dram ddr mhz 4GB channel tRCD trp tRAS twr   tRRD  nvm channel II simulation configuration illustrates transaction already transaction progress omit transaction simplicity suppose cachelines belonging transaction flush although entry transaction removable delete immediately remove entry transaction transaction become removable pointer address entry  transaction remains corrupt recovery recovery recovery mechanism failure prior redo entry uncommitted transaction discard commit transaction data identify transaction commits transaction commit register predefined address nvm recovery software reading address recovery software scan discard entry transaction commit flexibility traditional flexibility update data undo redo scheme flexible scheme however effectively mitigate limitation inflexibility synchronous writes dram cache asynchronous update nvm uncommitted data already evict dram cache overwrite exist data allows mimic steal undo undo redo scheme overwrite uncommitted data dram cache evaluation methodology implement evaluate redu SE mode gem simulator configure simulator model multi core processor dram nvm memory detailed configuration II model nvm performance nvm latency latency previous research nvm simulator evaluate nvm latency KB KB KB KB normalize cycle per transaction vector KB KB KB KB swap KB KB KB KB hashmap KB KB KB KB KB KB KB KB RB ycsb echo tpcc geomean sequential workload undo redo undo redo redu cycle per transaction cpt redu sequential writes CPTs normalize undo rand rand rand rand normalize cycle per transaction vector rand rand rand rand swap rand rand rand rand hashmap rand rand rand rand rand rand rand rand RB ycsb echo geomean random workload cycle per transaction cpt redu random writes CPTs normalize redo transaction performance redu  vector insert update entry vector swap swap random entry vector nvml hashmap insert update entry hash insert update node RB insert update node  ycsb insert update tpcc transaction echo insert update persistent hash benchmark evaluation benchmark evaluate vector benchmark appends persistent array swap benchmark swap random entry array macro benchmark implementation hashmap RB nvml library addition ycsb tpcc echo whisper benchmark modify   interface instead mmap interface evaluate benchmark various data exhibit access within transaction data classify random data KB KB random modify multiple rand indicates workload access update indexed tpcc benchmark transaction workload transaction throughput workload operation allocates writes nvm evaluate workload benchmark evaluate micro benchmark nvm macro benchmark comparison equally optimization coalesce pack undo undo implementation previous source optimization originally propose previous implementation highly optimize undo approach synchronous data update redo redo implementation previous performs data update fetch entry nvm representative redo indirect asynchronous update undo redo undo redo implementation propose previous trigger cycle undo redo asynchronous update redu propose scheme asynchronous data update redo eager policy eager unless otherwise mention transaction performance transaction throughput throughput cycle per transaction throughput transaction tpcc exhibit sequential update eager policy evaluation durability KB KB KB normalize cycle per transaction vector KB KB KB swap KB KB KB hashmap KB KB KB KB KB KB RB ycsb echo tpcc redo redo undo undo   normalize cycle per transaction cpt hardware technique sequential writes rand rand rand normalize cycle per transaction vector rand rand rand swap rand rand rand hashmap rand rand rand rand rand rand RB ycsb echo normalize cycle per transaction cpt hardware technique random writes normalize cycle per transaction cpt hardware technique  without benefit cache dram cache redu gain performance benefit cache dram cache previous cannot evaluate cache lru policy redu outperforms redo undo redo sequential update performance undo performance redo retire demand nvm bandwidth data otherwise undo redo performance significantly degrade twice amount others eager policy redu undo throughput sequential writes perform amount nvm writes interestingly redu throughput undo echo benchmark frequently update data merge dram cache writebacks data update dram cache nvm performance random writes redu redo exhibit throughput overhead indirect update redo hidden workload however redu outperform redo undo redo benchmark ycsb echo benchmark  temporal locality update merge flush nvm undo commit synchronously performance random workload plot geomean CPTs sequential workload redu undo comparable performance redo undo redo approach exhibit performance degradation redu respectively opt coalesce coalesce pack writes random writes IV reduction writes due optimization normalize without optimization random redu outperforms undo redo undo redo respectively undo suffers synchronous update redo performance degradation retire lastly undo redo twice amount however redu limitation performance overall redu improvement transaction throughput prior undo redo undo redo approach intensively optimize optimization transaction throughput substantially affected optimization redo redo redo implementation coalesce coalesce pack optimization respectively label approach similarly cycle per transaction benchmark sequential workload performance random workload CPTs normalize redo omit without optimization comparable plot accord optimization critical scheme achieve performance therefore comparison mislead optimization KB KB KB KB normalize cycle per transaction vector KB KB KB KB hashmap KB KB KB KB KB KB KB KB RB ycsb echo undo redu BF redu BF normalize cycle per transaction redu workload equally attribute redo significantly outperforms unoptimized undo undo redo approach overhead retire amortize coalesce pack optimization reduce bandwidth consumption indirect update similarly undo performance others without optimization IV normalize request benchmark optimization apply coalesce optimization reduces entry addition nvm writes reduce pack optimization addition previous scheme undo redo performs workload sequential undo series performance approach undo throughput workload redo series performance degradation due retire undo redo approach twice others random redo series perform however redu redo asynchronous update scheme various workload workload overhead increase nvm latency due dram cache lookup transaction throughput redu without HW filter filter unnecessary dram access HW filter denote bloom filter BF redu eager writeback policy throughput undo quantify overhead approach redu without HW filter suffers performance degradation ycsb benchmark fortunately HW filter successfully reduces unnecessary access dram cache transaction throughput undo redu performance without filter echo benchmark echo benchmark buffer allocate nvm return buffer client therefore echo benchmark performs nvm writes operation nvm latency transaction throughput normalize undo undo redo  redu nvm latency comparison transaction throughput scheme nvm latency MB MB MB MB normalize cycle per transaction vector MB MB MB MB hashmap MB MB MB MB MB MB MB MB RB MB MB MB MB ycsb MB MB MB MB echo eager lru BF lru BF comparison cycle per transaction eager lru policy eager lru BF access vector eager lru BF hashmap eager lru BF eager lru BF RB eager lru BF ycsb eager lru BF echo nvm nvm writes   ratio dram cache access nvm lru policy dram cache sensitivity nvm latency illustrates transaction throughput various nvm latency cycle per transaction micro benchmark random workload nvm latency fix latency redo outperforms undo latency increase undo suffer synchronous nvm writes data redo performs data update background undo redo throughput scheme incurs nvm writes finally redu throughput configuration nvm latency increase redo performs closely redu overhead update due nvm become significant lru policy evaluate lru policy dram cache lru policy without HW filter eager policy cycle per transaction reduce MB MB benchmark performs insert update operation access generate ycsb suite zipfian distribution zipfian theta cycle per transaction benchmark lru policy outperforms eager policy benchmark without HW filter ycsb benchmark dram cache MB MB however HW filter lru policy efficiently eliminates overhead dram cache lookup latency performance gain lru policy hashmap gain becomes dram cache increase decomposes dram cache nvm writes decomposition dram cache MB rate dram cache skewed access dram cache cachelines evict cache evict llc fetch nvm cached average nvm cached dram cache lru policy allows respond without nvm merge update reduces nvm writes VI related research propose atomic update persistent memory atomic durability  implement software mnemosyne supplement software transactional memory stm atomic update persistent memory  durability therefore guarantee constraint data software employ cpu instruction clwb sfence resides critical program execution overcome drawback persist critical bulk commit decouple however insert instruction program DudeTM hardware redo propose wrap DudeTM commits volatile memory wrap completes nvm meanwhile nvm update data refer retire wrap reproduce DudeTM propose  commits data  redu merge duplicate update nvm volatile buffer however  applicable  multiple version data exist versioned redu update addition  undo redo redu redo mention previous propose atomic durability software approach focus hardware addition research persistent data structure persistent memory propose NV propose persistent structure mitigates nvm overhead disorder leaf node however applicability limited data structure NV heap copying  versioning insert memory fence flush instruction addition  multi versioning  enables software propose concept memory persistency model persist constraint author analyze persistent memory define intel cpu instruction decouple volatile persist propose hardware modification delegate persist dct author dependency satisfied transaction persistency model research reduces inefficiency derive dependency across transaction kiln eliminates version data version non volatile cache  update redu cache eviction cache flush commit kiln however redu differs kiln incorporate conventional dram HW redo instead non volatile cache vii conclusion recent propose hardware assist technique overcome performance overhead SW approach identify hardware assist previous posse limitation propose hardware assist redu update data directly cache hierarchy asynchronously transaction commit dram cache nvm writes successfully decouples data update nvm critical addition enables choice hardware assist optimization writes asynchronous data update performance improvement average prior approach