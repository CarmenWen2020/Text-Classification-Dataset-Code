compute application demand memory bandwidth demand alleviate stack memory typically traditional dram heterogeneous memory dram cache hardware OS manage memory pom cache adapt rapidly application typically performance reduce OS visible memory capacity pom architecture increase OS visible memory capacity exhibit additional overhead due swap data memory propose chameleon hybrid architecture bridge gap cache pom architecture application memory chameleon memory pom maximize available application application footprint physical memory capacity chameleon opportunistically hardware manage cache chameleon hardware software OS notifies hardware allocate freed hardware decides switch memory pom cache mode dynamically evaluation multi programmed workload 4GB memory 0GB memory chameleon improves average performance pom latency optimize cache introduction client mobile data application demand memory bandwidth capacity increase popularity data intensive application resolution graphic machine memory increasingly become performance bottleneck due significant disparity CPUs memory improvement memory latency bandwidth substantial performance improvement memory bound application advent stack memory HBM HMC instrumental bridging performance gap unfortunately bandwidth memory cannot sole memory component due limited stack impact capacity interposer challenge observation heterogeneous memory combine memory component stack dram heterogeneous memory refers memory bandwidth memory refer bandwidth latency typically relatively memory component ddr ddr prior proposal memory heterogeneous memory cache cache performance advantage due spatial temporal locality memory reference access memory however cache duplicate data reduce overall operating OS visible memory capacity degrades performance memory footprint application memory constitutes overall memory capacity enable performance improvement memory without lose capacity cache memory pom architecture expose stack dram OS enhance overall OS visible capacity propose datacenter environment expose additional capacity stack dram OS beneficial enables datacenter scheduler schedule reduce overall thereby improve datacenter throughput reduces fault pathological scenario foreseen administrator user specify constraint submit reduce reduce overall amount chip dram 4GB 6GB stack chip dram replace 4GB 2GB stack chip dram 6GB OS visible memory pom architecture hardware manage OS manage numa  memory overhead mechanism achieve performance react quickly memory demand application hardware manage pom adapt quickly memory demand therefore outperform OS manage heterogeneous memory expense manage hardware address indirection swap memory due necessity swap memory pom degrade performance swap interfere swap refers swap stack chip DRAMs swap chip dram secondary storage device ssd disk imply fault annual acm international symposium microarchitecture doi micro demand memory access propose evaluate novel architecture chameleon attempt achieve cache pom architecture hardware software chameleon hardware manage pom baseline achieve capacity advantage however rely OS inform hardware allocate freed instruction ISA alloc ISA information OS chameleon opportunistically freed hardware manage cache therefore avoid expensive swap operation chameleon switch cache pom mode dynamically available specifically contribution workload exhibit memory footprint propose novel chameleon chameleon opt  chameleon leverage stack dram cache optimize chameleon opt remaps stack dram chip proactively stack dram cache propose chameleon pom memory capacity cache performance propose instruction architecture chameleon instruction OS inform hardware freed allocate chameleon cache simulation chameleon performs static pom cache architecture outperform pom baseline latency optimize cache II background related numa dichotomy multi socket homogeneous memory traditional multi socket socket consists chip core cache llc local chip dram depict socket interconnect amd  task socket access data local dram socket incurs latency access data remote memory socket extra latency incur remote access due additional interconnect traversal latency local remote memory bandwidth variation access latency local remote memory non uniform memory access widely refer numa numa dichotomy computation data placement significant impact performance socket heterogeneous memory advent bandwidth stack DRAMs integrate silicon  socket numa access chip stack dram faster chip memory diagram socket heterogeneous stack chip DRAMs intel QPI amd hypertransport chip memory node socket chip core llc chip memory node socket chip core llc bandwidth chip dram 0GB socket chip core llc bandwidth stack dram 4GB socket homogeneous memory socket heterogeneous memory numa aware OS optimization numa aware memory allocator traditional  linux VMware ESXi default allocate task data socket task maximize local memory access widely refer local memory allocation policy linux allocation policy improve performance minimize remote memory access linux automatic numa balance AutoNUMA linux advanced automatic numa balance mechanism enhance locality execute task correspond memory multi socket AutoNUMA local remote memory access poison invalidate correspond entry processor load correspond fault epoch refer numa balance scan AutoNUMA linux calculates remote local fault ratio numa balance scan epoch  local fault ratio exceeds threshold refer numa threshold misplace remote fault migrate remote socket memory local socket memory  local fault ratio numa balance scan update dynamically misplace migrate quickly local socket important issue AutoNUMA memory migrate remote local socket available local socket memory migration fails  error remote local fault ratio increase AutoNUMA longer migrate memory local socket migrates task socket socket socket minimize remote memory access hardware manage heterogeneous memory context socket heterogeneous memory hardware manage technique stack dram primarily category cache pom statically reconfigurable hybrid memory stack dram cache recent utilize stack dram another cache cache llc memory dram cache performance software transparency appropriately organize cache structure data tag architectural implication stack dram rate stack dram rate  percent thresh  percent thresh  percent thresh numa aware allocator AutoNUMA rate cloverleaf AutoNUMA timeline threshold dram cache prior proposal mapped associative fully associative cache stack dram memory pom recent usage stack dram OS visible extension chip memory propose software hardware approach warrant OS detect access information identify request  majority algorithm  algorithm originally propose database predict migrate stack dram lock sub architecture swap stack chip DRAMs maximize overall bandwidth however proposal explore hardware redirection via hardware remapping ensure stack dram rate optimize meta data overhead spatial locality frequently access KB stack dram  employ byte trading bandwidth meta data overhead spatial locality detail proposal agnostic OS visible manage heterogeneous memory statically reconfigurable heterogeneous memory KNL various mode stack dram refer MC dram operation mode cache OS visible memory static  apart mode KNL static hybrid mode configuration configuration stack dram operates cache mode remain operates memory mode configuration stack dram operates cache mode remain operates memory mode mode static reboot switch hybrid configuration another CHALLENGES architecting performance  heterogeneous memory OS numa aware understand exist OS numa aware fare socket heterogeneous memory gem simulator setup detail experimental setup VI linux OS kernel kernel compile config numa config numa  option emulate numa setup however exist kernel emulate asymmetric capacity numa node linux kernel modify capacity patch configure numa fake linux kernel emulates numa node 4GB capacity another 0GB capacity physical node model numa due bandwidth variation stack chip DRAMs modify memory controller module gem simulate heterogeneous memory bandwidth numa aware memory allocator linux  allocator II allocate faster stack dram increase stack dram rate stack dram rate memory footprint workload 4GB stack dram 0GB offchip dram average stack dram rate footprint workload stack dram rate due factor  capacity stack dram chip dram limit amount data stack dram accommodate 4GB memory OS lack adequate prediction mechanism employ allocation strategy allocate chip dram reduce stack dram rate demonstrate numa aware memory allocator policy optimal heterogeneous memory severe utilization rate stack dram linux AutoNUMA shortcoming mention successfully handle linux AutoNUMA stack dram rate 4GB stack 0GB chip dram numa threshold numa threshold yield stack dram rate average misplace chip dram migrate rapidly stack dram average rate AutoNUMA numa aware allocator rate  specifically cloverleaf cumulative rate threshold timeline graph rate cloverleaf workload primary axis migrate per epoch secondary axis stack dram rate axis timeline epoch processor cycle misplace migrate chip already pre allocate memory stack dram expose OS AutoNUMA memory MBS stamp HH MM SS 2GB 4GB 6GB inter workload memory footprint variation elapse dram stack dram increase stack dram rate increase maximum around epoch however epoch stack dram rate reduces gradually migrate chip memory stack dram stack dram capacity becomes available accommodate misplace stack dram migrate hence rate finally stack dram rate AutoNUMA due socket heterogeneous  stack dram capacity unlike multi socket local memory capacity remote memory hence AutoNUMA memory available multi socket socket stack dram AutoNUMA optimize  evict local memory accommodate misplace migrate remote memory available multi socket migrate AutoNUMA increase local memory access migrate task remote socket however feasible socket memory available numa balance scan millisecond migration happens coarse cpu cycle granularity identify memory memory 4GB overall dram capacity conduct intel xeon cpu detail machine configuration workload sequentially span employ samsung pro 6GB ssd secondary storage hence  service latency ssd workload contains application execute rate mode application chosen spec NAS  suite application workload axis memory information  linux periodically min workload exhibit demand memory varies  MBs  GBs memory allocation allocation varies rapidly phase infrequently phase thereby capture execution behaviour workload impact memory capacity performance workload affected differently OS visible memory capacity ramification limit overall capacity workload overall OS visible capacity varied   2GB intel xeon cpu machine application agnostic variation entire memory footprint memory capacity however workload sensitive overall capacity performance improvement denote imp workload report normalize 6GB capacity calculate  exec ime 6GB exec ime XGB exec ime 6GB capacity increase 8GB 4GB average execution improvement across workload improves saturate 6GB 8GB capacity understand variation performance memory capacity primary secondary average fault encounter workload average cpu utilization task respectively correspond memory capacity increase capacity average fault decrease average cpu utilization increase capacity spent swap dram secondary storage cpu utilization task  linux swap capacity increase fault reduce thereby enable task cpu utilization conclude insufficient memory capacity severe performance degradation memory demand workload dependent stack dram cache cache adapts quickly workload behavior dedicate memory cache significantly degrade performance 6GB 4GB capacity cache severe performance degradation workload operating overall OS visible capacity 8GB workload depict equation geometric execution application workload calculate performance improvement execution improvement 8GB 0GB 2GB 4GB 6GB 8GB impact capacity overall performance normalize 6GB overall capacity 6GB 8GB 0GB 2GB 4GB 6GB 8GB 6GB 8GB 0GB 2GB 4GB 6GB 8GB 6GB 8GB 0GB 2GB 4GB 6GB 8GB 6GB 8GB 0GB 2GB 4GB 6GB 8GB 6GB 8GB 0GB 2GB 4GB 6GB 8GB 6GB 8GB 0GB 2GB 4GB 6GB 8GB 6GB 8GB 0GB 2GB 4GB 6GB 8GB 6GB 8GB 0GB 2GB 4GB 6GB 8GB 6GB 8GB 0GB 2GB 4GB 6GB 8GB 6GB 8GB 0GB 2GB 4GB 6GB 8GB 6GB 8GB 0GB 2GB 4GB 6GB 8GB 6GB 8GB 0GB 2GB 4GB 6GB 8GB bwaves leslied gemsfdtd lbm mcf hpccg NAS SP cloverleaf comd minife cactusADM cpu utilization fault avg fault cpu utilization impact capacity fault cpu utilization memory footprint workload memory footprint fault 4GB stack dram cache limit overall capacity 0GB degrade workload operating marked 2GB stack dram cache degrade workload hence capacity stack dram decision stack dram cache adverse performance workload stack dram pom avoid memory capacity loss incur cache stack dram OS visible memory pom pom architecture enable throughput datacenters datacenter scheduler schedule due increase overall memory capacity reduce fault enable CPUs remain reduce overall memory pom architecture beneficial pom proposal optimal substantially increase demand chip chip memory bandwidth stack dram capacity 6GB 4GB workload operating swap stack chip dram stack dram pom proposal cognizant unallocated OS address OS invalid data invalid data unallocated address swap operation thereby waste memory bandwidth 2GB 4GB stack DRAMs swap waste bandwidth increase hence static decision stack dram pom agnostic swap severe performance degradation demonstrate VI ideal heterogeneous memory ideal heterogeneous memory dynamically maximum performance reduce fault capacity limited workload dynamically operating memory pom mode optimize overhead swap OS visible operating cache mode optimize meta data remapping overhead propose novel hardware software dynamically  heterogeneous memory overall propose opportunistically convert OS visible hardware manage cache switch memory mode capacity limited workload available memory pom mode operating cache mode propose incarnation chameleon chameleon opt algorithm OS memory allocator routine struct alloc gfp gfp mask unsigned int struct     rom  gfp mask null alloc  gfp mask null  pfn  gfp mask contains GF  GF   HP pmd         ISA alloc  return static inline void ISA alloc volatile void asm volatile  volatile unsigned int IV chameleon software communicate allocate unallocated physical address hardware propose processor ISA instruction ISA alloc ISA instruction OS apart traditional KB OS employ transparent  giant reduce virtual memory overhead workload  reclamation  memory footprint hence chameleon granularity granularity allocate unallocated OS memory allocator reclamation routine multiple correspond ISA alloc ISA invocation chameleon algorithm OS memory allocator routine ISA alloc ISA invocation algorithm gfp mask flag linux contains identify correspond granularity allocation gfp  gfp  flag THP allocation request linux detect granularity allocation ISA invocation algorithm algorithm OS reclamation routine static inline void ree struct unsigned pfn struct zone zone unsigned int int   pfn lru zone ree ree  zone ree ree null  pfn  HP pmd  HP pmd         ISA ree  return static inline void ISA ree volatile void asm volatile  volatile unsigned int  algorithm refers employ chameleon various granularity hardware chameleon easily detect OS boot  allocation granularity ISA alloc ISA invocation MB THP allocation reclamation KB employ pom ISA alloc ISA invoked however byte cache employ  ISA alloc ISA invoked chameleon hardware chameleon relies ISA alloc ISA OS dynamically reconfigure heterogeneous memory memory pom cache mode chameleon hardware useful explain hardware exist pom proposal baseline pom proposal employ meta data hardware structure refer remapping SRT location  remap stack chip DRAMs stack chip DRAMs grouped congruence tag SRT  unique seg seg seg SegGrp chip memory 0GB  seg seg seg seg seg seg SegGrp  seg seg seg stack dram 4GB restrict remapping pom architecture SegGrp mode dirty pom counter alloc vector ABV restrict remapping SRRT entry chameleon aid detect mapped stack chip DRAMs reduce meta data overhead involve remapped stack dram swap another chip memory depict hence pom proposal employ restrict remapping SRRT hardware remapped pom proposal employ byte cacheline employ KB swap stack chip DRAMs SRRT counter aid swap frequently chip correspond stack dram KB reduces amount meta data SRRT improves spatial locality thereby stack dram rate hence chameleon pom architecture baseline chameleon augment SRRT additional hardware information augment data structure apart remapping tag counter SRRT entry contains alloc vector ABV mode dirty alloc vector ABV alloc vector ABV indicates correspond currently allocate OS allocate correspond ABV otherwise entry ABV per hence function capacity ratio stack chip memory boot ABV OS ISA alloc reset OS ISA mode mode indicates operating mode operating memory pom mode operating cache mode discus transition pom cache mode later dirty cache mode dirty SRRT indicates currently reside stack dram dirty dirty SegGrp SRRT entry ISA alloc SegGrp cache mode cache anything switch SegGrp pom mode update ABV belongs stack dram tag SegGrp cache mode cache chip writeback dirty allocate correspond ABV alloc chip previous mode chameleon ISA alloc transition flowchart grp  ABV mode dirty  ABV mode dirty grp ISA alloc ISA alloc chameleon ISA alloc transition indicates chip memory eviction stack dram operating pom mode dirty ignore swap irrespective status dirty chameleon cache pom mode ISA alloc instruction transition cache mode pom mode ISA instruction transition pom mode cache mode however  ISA instruction physical address trigger transition currently allocate unallocated governs ISA alloc ISA instruction trigger transition scenario trigger transition pom cache mode explain notation SRRT SRRT actual physical SegGrp 4GB stack dram 0GB chip dram ratio implies stack dram physical address chip belong tag signify correspond physical address remapped cached variation propose architecture flowchart relevant capacity ratio simplicity chameleon chameleon architecture leverage  stack dram cache chip dram transition pom cache mode vice versa trigger seg grp SRRT entry ISA SegGrp pom mode switch SegGrp cache mode update ABV belongs stack dram tag SegGrp pom mode remapped chip swap tag correspond ABV chip previous mode chameleon ISA transition flowchart CB grp  ABV mode dirty  ABV mode dirty grp ISA ISA chameleon ISA transition ISA alloc ISA physical address belonging stack dram address ISA alloc transition flowchart ISA alloc transition chameleon ISA alloc chip dram physical address previous mode however ISA alloc stack dram physical address operating cache mode stack dram cache chip scenario ISA alloc encounter none offchip cached stack dram tag scenario ISA alloc allocate ABV grp operating cache mode mode ISA alloc ABV transition pom mode scenario chip cached stack dram tag dirty correspond otherwise tag simply finally ABV stack dram allocate ISA transition flowchart ISA transition chameleon ISA chip physical address transition mode correspond ABV ISA stack dram physical address operating pom mode prior scenario ISA freed remapped tag freed remapped correspond ABV transition cache mode tag cached stack dram freed currently stack seg grp SRRT entry ISA alloc tag update ABV remapped something allocate destination tag ABV  cache mode switch SegGrp pom mode stack dram ABV  tag tag update ABV chameleon opt ISA alloc transition flowchart dram stack dram remapped chip dram remapped  stack dram chip dram access frequently swap implement swap transition ensures frequently access resides stack dram access frequently program phase swap ISA happens stack dram proactively swap stack dram freed ensure stack dram available cache finally swap ABV transition cache mode pom mode optimize chameleon chameleon opt chameleon leverage OS visible stack dram cache available chip memory consequently chameleon optimally leverage available increase cache capacity optimize chameleon opt proactively remap stack dram chip memory convert available stack chip DRAMs cache chameleon opt outperforms chameleon demonstrate VI ISA alloc transition flowchart ISA alloc instruction chameleon opt ABV crucial role chameleon opt signify switch mode another remains cache mode ABV switch pom mode ABV hence unlike chameleon chameleon opt cache mode correspond stack dram address allocate OS similarly switch cache mode chip physical address unallocated stack dram remains allocate BA grp  ABV mode dirty  ABV mode dirty grp ISA alloc ISA alloc pro actively mapped chameleon opt ISA alloc transition seg grp SRRT entry ISA tag pro actively stack dram switch cache mode dirty stack dram seg grp cache mode ABV switch cache mode dirty cache mode seg grp cache mode ABV cache mode trace remapped stack dram ABV ABV chameleon opt ISA transition flowchart chameleon opt proactively remaps stack dram cache chameleon opt unlike chameleon encounter ISA alloc operating cache mode encounter ISA alloc indicates presence action trigger ISA alloc currently reside stack dram ISA alloc stack dram physical address operates cache mode cached stack dram tag allocate therefore ISA alloc directly  ABV transition pom mode confirm OS visible scenario prior execute ISA alloc instruction flowchart scenario allocate allocate ABV operating cache mode mode stack dram tag currently cached stack dram chameleon ISA alloc allocate stack dram ABV transition pom mode however chameleon opt proactively remapped chip dram hence tag tag stack dram allocate stack dram cached future chameleon opt operates cache mode unlike chameleon proactive remapping explain remapped chip memory stack dram cache chip cache mode ISA alloc allocate stack dram transition pom mode unallocated cache mode occurs  chip address remapped allocate chip address transition pom mode unallocated cache mode ISA transition ISA transition chameleon opt apart available stack dram chameleon opt proactively convert available chip dram visible stack dram leveraged cache proactive creation chameleon opt scenario already cache mode cache mode correspond ABV scenario ISA encounter stack dram address neither cached remapped chip respectively ISA transition cache mode previously operating pom mode correspond freed remapped additional discussion transit access chameleon chameleon opt local buffer propose pom  stack chip DRAMs without OS involvement swap access load transit perform local buffer respective memory hence incur longer latency security concern chameleon chameleon opt potential security concern information leakage data cache access pom mode alternatively pom mode cache access spy transition cache mode avoid issue ISA ISA alloc instruction trigger hardware transition cache pom impact buffer cache linux manages memory buffer secondary storage device disk ssd depict amount allocate buffer cache impact visible OS ISA alloc ISA core 6GHz alpha ISA KB associative cacheline cache KB private associative cacheline cache MB associative MESI cacheline stack dram bus frequency 6GHz ddr 2GHz 4GB bus width channel capacity 4GB channel rank channel rank tcas tRCD trp tRAS tRFC  chip dram bus frequency mhz ddr 6GHz 0GB bus width channel capacity 0GB channel rank channel rank tcas tRCD trp tRAS tRFC  OS linux kernel fault latency cpu cycle micro ssd simulated baseline configuration allocate unallocated OS buffer cache honor chameleon hardware allocation allocation request chameleon chameleon opt away buffer cache hardware manage cache chameleon chameleon opt impact buffer cache hence degrade disk performance pipeline execution detail ISA alloc ISA ISA alloc ISA instruction retire transition trigger instruction ISA alloc ISA instruction necessitate movement instruction retire ABV SRRT update clearing  ISA retire access mode however ISA alloc ISA involve instruction commit fetch local buffer correspond memory controller ensure access flight local buffer destination memory controller buffer drain opportunistically evaluation overhead average VI VI evaluation experimental setup briefly gem  simulator execute linux kernel stack chip DRAMs model memory controller gem ISA alloc ISA invoked OS memory allocator reclamation code pseudo instruction gem summarizes simulated configuration simulated application various suite characteristic II workload cache simulate instruction per application application workload simulated minimum billion instruction assume fault latency cpu cycle service SSDs report 4GB stack 0GB chip dram unless otherwise report VI performance report geometric instruction commit per cycle ipc benchmark workload normalize correspond baseline suite WL llc MF suite WL llc MF MPKI GBs MPKI GBs spec bwaves  cloverleaf lbm comd cactusADM miniAMR leslied hpccg mcf minife gemsfdtd miniGhost NAS SP II workload characteristic MF memory footprint WL workload MPKI per kilo instruction bwaves cactusADM cloverleaf comd gemsfdtd hpccg lbm leslied mcf miniAMR minife miniGhost SP average stack dram rate alloy cache pom chameleon chameleon opt stack dram rate breakdown percentage operating cache mode pom mode chameleon chameleon opt distribution ideally allocation  request workload however workload application allocate  execution encounter ISA alloc ISA simulated snippet average cache mode chameleon chameleon opt chameleon opt leverage available chip dram cache stack dram rate latency optimize alloy cache pom chameleon alloy cache employ latencyoptimized mapped cache stack dram average rate pom KB average rate comparison chameleon chameleon opt average rate respectively rate chameleon chameleon opt pom due operating cache mode pom employ threshold signifies minimum access chip dram swap stack dram chameleon employ threshold operating cache mode stack dram rate pom cache mode chameleon opt rate chameleon quantifies swap incur pom chameleon chameleon opt report normalize swap incur pom due operating cache mode overall swap reduce average chameleon  respectively pom cache mode evict modify dirty bwaves cactusADM cloverleaf comd gemsfdtd hpccg lbm leslied mcf miniAMR minife miniGhost SP average bwaves cactusADM cloverleaf comd gemsfdtd hpccg lbm leslied mcf miniAMR minife miniGhost SP average chameleon chameleon opt cache mode pom mode pom mode cache mode pom cache mode distribution bwaves cactusADM cloverleaf comd gemsfdtd hpccg lbm leslied mcf miniAMR minife miniGhost SP average normalize swap pom  chameleon opt swap stack chip DRAMs bwaves cactusADM cloverleaf comd gemsfdtd hpccg lbm leslied mcf miniAMR minife miniGhost SP geo normalize ipc baseline 0GB ddr stack dram baseline 4GB ddr stack dram alloy cache pom chameleon chameleon opt normalize ipc stack dram writeback chip dram stack dram offchip effectively swap writeback modify consumes stack chip memory bandwidth hence chameleon report scenario swap normalize ipc various alloy cache pom chameleon chameleon opt variant baseline without stack dram capacity chip dram baseline 0GB overall capacity 4GB overall capacity 4GB baseline incur fault unlike 0GB capacity 4GB baseline improves geometric ipc 0GB capacity baseline mention VI alloy cache pom chameleon  configuration 4GB stack 0GB offchip dram thereby capacity 4GB alloy cache fault workload memory footprint baseline sacrifice capacity cache improves performance baseline performance alternative workload pom improves performance geometric ipc 0GB 4GB capacity baseline respectively chameleon improves geometric ipc workload 0GB 4GB baseline respectively pom alloy cache respectively  improves performance 0GB 4GB baseline respectively pom alloy cache respectively improvement baseline mainly chameleon manages cater memory footprint avert fault utilize bandwidth stack dram efficiently avert chip dram access chameleon chameleon opt bwaves cactusADM cloverleaf comd gemsfdtd hpccg lbm leslied mcf miniAMR minife miniGhost SP average memory access geomean latency cpu cycle pom chameleon chameleon opt average memory access latency bwaves cactusADM cloverleaf comd gemsfdtd hpccg lbm leslied mcf miniAMR minife miniGhost SP geo normalize ipc baseline 0GB ddr stack dram baseline 4GB ddr stack dram  4GB 0GB  percent  percent  percent chameleon chameleon opt normalize ipc comparison bwaves cactusADM cloverleaf comd gemsfdtd hpccg lbm leslied mcf miniAMR minife miniGhost SP average bwaves cactusADM cloverleaf comd gemsfdtd hpccg lbm leslied mcf miniAMR minife miniGhost SP average ratio 6GB 8GB ratio 3GB 1GB cache mode pom mode sensitivity distribution chameleon opt pom due increase stack dram rate reduce swap stack chip DRAMs reduces average memory access latency chameleon opt incurs minimal swap hence average memory latency stack dram chameleon thereby outperform comparison OS  memory allocator AutoNUMA utilize stack dram stack dram rate therefore OS leverage potential bandwidth stack dram chameleon average improvement numa aware memory allocator AutoNUMA respectively chameleon opt improves performance benchmark minife miniGhost comd SP benefit chameleon chameleon opt memory intensity denote llc MPKI II application benefit opportunistically convert cache application hpccg bwaves cloverleaf highly memory intensive benefit significantly chameleon chameleon opt comparison polymorphic memory patent chung propose hybrid architecture leverage memory available stack dram cache propose architecture leverage OS visible stack dram offchip dram cache unlike chameleon opt proposal achieves amount chameleon chameleon outperforms proposal polymorphic memory proposal swap frequently chip dram stack dram OS allocate unlike pom thereby utilize stack dram chameleon chameleon opt improve performance polymorphic memory respectively bwaves cactusADM cloverleaf comd gemsfdtd hpccg lbm leslied mcf miniAMR minife miniGhost SP geo normalize ipc baseline 0GB ddr stack dram baseline 4GB ddr stack dram polymorphic memory chameleon chameleon opt polymorphic memory comparison sensitivity cache pom mode distribution ratio stack chip dram capacity ratio stack dram contributes capacity 6GB chip dram contributes 8GB ratio stack dram contributes 3GB chip contributes 1GB ratio stack chip dram increase average cache mode increase configuration per increase probability increase chameleon opt thereby increase cache mode normalize performance correspond ratio chameleon opt consistently performs across ratio ratio chameleon  improve performance respectively pom ratio improvement respectively pom ISA alloc ISA overhead analysis ISA alloc ISA initiate additional swap per ISA alloc ISA stack chip DRAMs due hardware remapping employ ISA alloc ISA performance overhead due warrant swap estimate overhead introduce  ISA perform overhead analysis conservative assumption workload execution assume KB chameleon swap approach propose encounter swap  ISA thereby encounter swap execution assume swap operation memory latency cpu cycle per byte cacheline pom spent 5GHz average 2GHz 5GHz max turbo intel xeon swap KB stack chip DRAMs overhead encounter chameleon due ISA alloc ISA account execution additional hardware transition warrant chameleon significant performance overhead limitation future chameleon chameleon opt architecture opportunistically convert heterogeneous memory bwaves cactusADM cloverleaf comd gemsfdtd hpccg lbm leslied mcf miniAMR minife miniGhost SP geomean bwaves cactusADM cloverleaf comd gemsfdtd hpccg lbm leslied mcf miniAMR minife miniGhost SP geomean ratio 6GB 8GB ratio 3GB 1GB normalize ipc baseline  baseline  4GB pom chameleon chameleon opt sensitivity ipc chameleon opt cache baseline pom architecture employ restrict remapping briefly reduce meta data overhead chameleon chameleon opt employ restrict remapping limit ability maximize cache capacity uneven another none cannot cache mode alleviate limitation expose management information OS OS maintain additional data structure ABV per  update ISA alloc ISA execution explore future another chameleon  opt limitation due benefit workload advantage spatial locality proposal  granularity thereby reduce data movement swap overhead benefit workload limited spatial locality however  incurs remapping overhead due organization vii conclusion propose chameleon novel architecture dynamically configures heterogeneous memory available instruction ISA alloc ISA OS informs hardware allocate freed OS communication chameleon adapts dynamically operating memory mode memory footprint workload opportunistically convert hardware manage cache chameleon ensures performance memory cache proposal