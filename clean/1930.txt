memory capacity memory bandwidth performance efficiency important client datacenter application hardware memory compression promising direction achieve without increase unfortunately memory compression significant challenge memory compress additional memory access sometimes critical performance overhead operating advantage increase capacity handle incompressible data delay deployment propose compresso hardware memory compression architecture minimizes memory overhead due compression OS identify data movement offs propose optimization reduce additional memory movement improve efficiency propose holistic evaluation compress compresso achieves compression memory average speedup competitive hardware compress core multi core competitive compress compresso reduces performance overhead compression increase performance gain memory capacity introduction memory compression improve performance reduce memory demand machine graph analytics database autonomous compresso compress memory architecture explicitly optimizes offs compression mechanism additional data movement implementation without modification application operating compress data memory increase effective capacity access secondary storage thereby boost performance access improve latency decrease partition task across node reduce access additionally transfer compress cache memory byte thereby reduce memory bandwidth usage byte prefetch data directly increase effective bandwidth hardware memory compression ibm mxt lcp RMC  DMC  propose enable capacity bandwidth overhead unfortunately hardware memory compression challenge prevent adoption compress memory management additional data movement performance overhead widely  previous advantage memory capacity hardware memory compression technique operating dynamically memory capacity address scenario memory data incompressible limit  potential compression implement without OS hardware innovative incompressible data additional data movement compress due metadata access additional translation compressibility data compression across cache boundary memory demonstrate analyze magnitude data movement challenge identify novel offs allocation data pack prediction optimization alleviate challenge optimization propose efficient compress memory compresso compression ratio overhead maximize performance gain addition compresso OS transparent standard OS linux OS software modification unlike prior approach sacrifice OS transparency situation promise memory compresso utilizes exist feature operating reclaim memory without OS compression aware propose novel methodology evaluate performance impact increase effective memory capacity due compression contribution identify demonstrate analyze data  overhead memory compression offs memory compression architecture propose compresso optimization reduce compress data movement hardware compress memory maintain compression ratio repacking data compresso exhibit compress data movement access enhance lcp competitive baseline propose approach OS completely agnostic memory compression hardware annual acm international symposium microarchitecture doi micro compress address OS transparent allocate compress cache overflow overview data organization compress memory limited memory controller devise novel methodology holistic evaluation account capacity benefit compression addition overhead evaluate compresso uncompressed memory baseline memory constrain compresso increase effective memory capacity achieves speedup oppose speedup achieve prior memory unconstrained compresso performance uncompressed prior degrades performance overall compresso outperforms prior II overview assume memory compress data cache uncompressed data cache compression orthogonal memory compression discus important parameter compress memory architecture compresso choice compression algorithm granularity aim memory compression increase memory capacity bandwidth demand sufficiently compression ratio observable difference however core uncompressed data decompression latency critical compression algorithm propose domain frequent compression FPC lempel ziv LZ pack delta immediate bdi  compression BPC others although LZ compression approach overhead chose BPC due average compression ratio algorithm BPC context compressor transforms data delta  xor transform increase data compressibility encodes data BPC context memory bandwidth optimization gpu adapt cpu memory capacity compression decrease compression granularity byte byte cache CPUs apply BPC transform suboptimal module compress data without transform parallel chooses option optimization average memory baseline BPC compresso modify BPC compression algorithm achieve average compression application compression granularity compression granularity compress data allows compression ratio latency data movement requirement due access granularity core memory compresso compression granularity address translation boundary data compress cache variable compress memory memory available OS allocate application actual instal memory compress memory translation virtual address VA OS physical address OSPA traditional virtual physical address translation access cache avoid aliasing OSPA machine physical address MPA access memory fix cache VA translate variable MPA OS transparent compression translates cache OSPA MPA layer memory controller compression aware OS variable OSPA memory controller translates address variable cache MPA bus transaction OSPA OS aware per compressibility OSPA bus transaction architectural specification hence hardware peripheral device associate driver modify accommodate compresso OS transparent fix VA OSPA KB OS MB 1GB etc broken KB building MPA pack compress cache compress cachelines variable scheme placement within tradeoff compression ratio calculate offset cacheline pack cachelines LinePack encode compression ratio BPC bdi lcp pack LinePack cacheline within calculate offset sum compress cache thereby metadata per cacheline lcp pack linearly compress lcp propose smart reduce translation overhead compress cache within target exception cache compress target uncompressed handle explicit pointer metadata recent exception frequent benchmark motivation lcp pack LinePack simpler offset calculation however calculation access memory evaluation circuit computes cacheline offset LinePack effectively additional memory cycle lcp pack allows speculative memory request parallel metadata request exception extra memory access additionally assumes tlb aware target per feasible OS transparent lcp pack away compression simpler translation pack flexibility LinePack compression data across cache underperforms data variable due lcp pack simpler compression algorithm lcp loses compression ratio LinePack bdi reduces compression ratio aggressive BPC compresso LinePack cache pack allocation allocate variable variable chunk drawback fragmentation sophisticated management incremental allocation fix chunk trivial manage pointer metadata allows maximum compression balance metadata compression scheme minimum MPA allocation non zero OSPA however chunk pack MPA variable chunk fragmentation additionally compress data movement later IV hence evaluate scheme variable chunk variable chunk KB KB KB fix chunk compresso incremental allocation chunk thereby KB KB compresso metadata OS aware OS transparent compress memory access llc writeback OSPA MPA translation translation metadata identify offset within corresponds OSPA cacheline OS transparent translation OSPA MPA happens metadata perform translation compresso maintains metadata OSPA dedicate MPA OSPA fix memory capacity compresso advertises OS boot compresso per OSPA metadata lcp amount storage overhead memory capacity metadata memory expose OS entry per OSPA metadata address computation bitwise shift metadata entry compresso valid zero  flag valid OSPA mapped MPA zero contains zero compress uncompressed compresso dynamically identify opportunity repacking compression metadata entry machine frame   chunk comprise compress OSPA cacheline compress compresso encode cache KB inflation writebacks cacheline memory overflow compress grows naive overflow multiple metadata entry organization additional compression related data movement memory traffic relative uncompressed baseline memory access instead compresso allows inflate cachelines uncompressed inflation MPA exception lcp entirely reduce compression related data movement specific pack scheme cacheline sequence correspond inflate along inflate per metadata cache align metadata optimal reduce extra data access leftover byte inflation pointer counter inflate cachelines NA data OV EM address manage compress data memory significant data movement overhead average additional access uncompressed memory remains  previous additional memory access due compression source split access cache cache variable compress memory across regular cache boundary memory thereby multiple memory access critical compressibility overflow cache llc memory compressibility decrease meaning longer allocate cache overflow movement cache underneath happens average per instruction moreover incompressible data cache eventually overflow allocation KB overflow KB overflow occurs instruction relocation additional memory writes metadata access earlier memory access OSPA MPA translation despite cache relevant metadata additional memory access critical additional memory access compression competitive baseline lcp relative memory access benchmark additional access amount average maximum data movement related offs contribution compresso identify data movement related offs cache exists compression ratio frequency compress data movement cache indicates permissible compress compress cache prior maximize compression ratio compression however bin likely overflow trigger data movement respect cache pack compress cache average compression ratio respectively couple however cache overflow cache cache bin metadata due encode cache complicates offset calculation circuit compresso cache bin tradeoff exists bin achieve average compression ratio upto resize access compresso incremental fix chunk choice enables important data movement optimization discus later however without data movement optimization choice cache alignment split access cache cache boundary memory extra memory access average previous avoid split access align loss compression loss average BPC complex calculation cache offset alignment friendly cache discus later inflation overflow predictor dynamic inflation expansion illustration data movement optimization data movement optimization optimization compresso alleviate compress data movement optimization reduce data movement without optimization choice incremental allocation chunk outperform variable allocation however optimization compresso significant data movement configuration reduce average relative extra access competitive compress memory analyze mechanism alignment friendly cache previous cache optimization maximize compression ratio cache choice average cache split across cache boundary access cache extra memory access critical redo maintain compression ratio decrease alignment cache decrease compression split access cache decrease split access cache intuitive cache boundary cache pack algorithm introduces avoid split access cache completely avoid extra access misalignment however complexity pack algorithm offset calculation hence skip optimization alignment friendly cache extra access overflow prediction contributor compression data movement frequent cache overflow occurs scenario incompressible data application initialize variable zero incompressible zero maximum compressibility incompressible data cache overflow overflow multiple predictor uncompressed avoid data movement due compression repacking mechanism later restores overall compression ratio associate saturate counter entry metadata cache counter incremented writeback associate cache overflow decremented upon cache underflows data compressible another global predictor overflow speculatively increase maximum KB local global predictor hence uncompressed receives multiple cache overflow phase overall overflow false negative predictor lose opportunity data movement false positive  compression without reduction data movement incur false negative false positive average optimization reduces remain extra access dynamic inflation expansion earlier inflation cache  initial compress however frequently cache overflow cannot inflation insufficient MPA despite available inflation pointer metadata option prior  involve cache writes instead optimize option allocate additional chunk expand inflation cache due metadata constraint till allocation chunk inflate reduce remain extra access dynamic repacking previous evaluate repacking  memory widely assume grows allocation till freed zero storage OS aware compression however reduction additional compression related data movement memory traffic optimization apply OS aware compress repacking important application loss compression ratio repacking perform storage benefit  average focus decrease compressibility increase compressibility underflows however cache within become compressible  additionally propose data movement optimization compressible uncompressed data movement pack  potentially compressibility repacking data movement potentially cache within  writeback improves compression ratio significant data movement compression  optimization mostly affect entry metadata cache target incompressible data evict llc locality insight metadata cache eviction entry trigger repacking compresso dynamically potential within metadata entry novel compresso important overhead repacking repacking trigger available positively impact actual effective memory deallocate chunk dynamic repacking reduces compression  amount extra access memory access overhead repacking attribute reasonable choice trigger repacking metadata cache optimization metadata cache  tlb loss compression ratio repacking challenge OS aware OS transparent translation OSPA MPA data movement due metadata access overhead knowledge OSPA  memory OS tab OS aware OS transparent compression tlb application capture metadata however benchmark forestfire omnetpp rate multi core scenario application particularly problematic identify optimization expands metadata cache coverage exploit cache uncompressed OSPA exactly cache metadata cache entry additional metadata entry amount extra logic tag optimization allows effective cache incompressible data optimization exhibit crucial increase rate omnetpp forestfire pagerank graph despite regular cache optimization brings remain extra access apply data movement optimization extra access average due split access cache due compression due metadata cache optimization orthogonal apply separately exception dynamic inflation expansion combination fix chunk allocation TR par NC challenge compress memory specific OS transparent tab memory compress important memory highly compress avoid memory compression aware OS zero byte partially OS aware ibm mxt OS zero  OS transparent handle memory upon OS transparency performance overhead none previous actively  compression compresso memory compress aggressively compress explain IV memory compress OS promise address physically available data memory compressible assume memory scenario MPA happens prior compresso exception OS thereby OS transparency OS modification compresso maintains OS transparency leverage exist OS feature developed virtual machine challenge commitment memory virtualized environment memory virtual machine exceeds memory capacity host OS virtual machine linux MacOS memory   safely  memory virtualized environment without substantial performance degradation due host memory pressure increase virtual machine VM hypervisor  driver within guest reclaim memory another guest VM VM allocate memory hog virtual machine VM reclaim  driver  demand guest OS memory balloon guest regular mechanism possibly freed claimed balloon driver communicate hypervisor  physical memory propose OS facility memory pressure poorly compress data compresso driver inflate inform hardware freed hardware invalid metadata storage MPA linux compresso driver apis around core function alloc recently discover OS transparency patent detail evaluation evaluate performance overhead  linux memory lcp compresso unconstrained core core core core core core performance relative uncompressed constrain memory baseline benchmark execution tab II speedup memory capacity impact evaluation constrain memory baseline 2GB ram 1GB reclaim without fragmentation highly fragment VI methodology novel dual simulation methodology evaluate compress holistic fashion imperative understand aspect overhead due compression latency additional data movement bandwidth impact compression due inherent prefetching benefit zero cache impact memory capacity increase application aspect compress evaluate previous cycle simulation however memory capacity impact evaluation without compress evaluation incomplete evaluation performance evaluation mechanism cycle simulation evaluate latency overhead bandwidth benefit compression memory capacity impact evaluate performance benefit reduce OS effective capacity available compression memory capacity impact evaluation memory capacity impact evaluation benchmark compression ratio generally exhibit recur phase behavior simulator highly  benchmark HW intel xeon cpu OS linux kernel memory available benchmark dynamically accord compressibility profile stage application pause instruction dump memory allocate physical address vector compression ratio instruction interval benchmark  fix core hardware counter calculate retire instruction core instruction vector compression ratio accordingly memory budget  feature linux budget memory statically dynamically static budgeting memory replicates regular dynamically memory available compressibility data memory allows emulate compress OS swap limited memory evaluate impact compression memory constrain footprint benchmark workload tab II becomes memory constrain benefit compression generally increase footprint benchmark stall frequent illustrate constrain memory memory capacity constrain cycle evaluation alone capture performance impact compresso  representative cycle simulation representative benchmark simulation extrapolate benchmark simpoints vector execution characterize interval benchmark correlate pipeline cache behavior however evaluate compress representativeness data crucial  instead  extends vector memory compression metric compression ratio rate overflow underflows memory usage within interval representativeness compression ratio gemsfdtd exhibit remarkably compressibility simpoint  performance simulation  instruction accuracy virtual address translation operating zero initialize frame therefore virtual address benchmark memory snapshot compression ratio  realistic compression ratio approach linux proc filesystem  physical address  file resident memory simulator benchmark  graph forestfire pagerank centrality benchmark cycle simulation extend zsim estimation McPAT CACTI BPC estimate TSMC standard evaluation parameter detailed tab cycle latency compression decompression difference compressibility representativeness simpoint  gemsfdtd astar core 3GHz core issue width rob entry KB L1D KB MB core MB core cachelines tlb overhead dram 8GB ddr mhz capacity varied memory capacity evaluation BL tcl tRCD trp decompression compression overhead metadata cache latency compression related access regular RD WR queue compress compress compress compresso tab performance simulation parameter BPC model propose BPC GPGPUs extend CPUs cycle buffering cache ddr cycle cycle concatenation multi core simulation benchmark core speedup metadata cache rate sensitivity limited memory benchmark representation tab IV scenario compression overhead multi core  instruction methodology described  feature zsim multicore simulation execution core till  simulates contention memory capacity impact evaluation workload consists benchmark rerun contention till benchmark completes execution unconstrained memory workload constrain memory percentage progress benchmark elapse runtime unconstrained memory average progress across individual benchmark workload chosen metric comparison evaluate baseline competitive baseline compresso optimize version described lcp compress inflation metadata cache compresso OS aware lcp optimize BPC compression algorithm compression benefit zero access prefetch simulated competitive baseline prior lack compresso data movement optimization another related DMC report compression ratio achieve opportunistically cycle memory capacity impact evaluation overall performance combine cycle memory capacity impact evaluation mcf gemsfdtd lbm exclude stall due excessive memory constrain discussion text performance core granularity compression involves substantial additional data movement  another compress memory optimize database lcp therefore lcp OS aware behavior competitive baseline compresso addition impact alignment friendly cache evaluate lcp align version lcp evaluate compresso lcp pack instead LinePack inferior across benchmark average slowdown report overall performance calculate multiplication performance cycle simulation memory capacity impact evaluation speedup mutually independent overall performance metric effort estimate performance combine memory capacity impact cycle simulation unfortunately metric account application phase complex interaction capacity latency uniform impact performance unconstrained memory upper bound performance gain memory capacity VA  impact data movement optimization additional memory access performance simulation overhead core performance cycle simulation relative performance respect uncompressed  lcp lcp alignment friendly cache compresso slowdown due compression overhead improvement due bandwidth benefit impact increase memory capacity application gcc graph cactusADM libquantum leslied soplex gain performance baseline uncompressed happens writebacks zero cache memory access handle access cached compression metadata alone benchmark leslied soplex exhibit zero access respectively soplex bandwidth requirement exhibit overall relative performance access compress memory return multiple compress cache prefetch libquantum benchmark exhibit bandwidth demand memory access compress prefetching compression increase buffer locality libquantum spatial locality benchmark overall speedup libquantum compresso baseline lcp baseline OS aware fault upon overflow OS transparent compresso cheaper overflow handle mechanism addition cycle memory capacity impact evaluation overall performance performance multi core overflow overall data movement optimization implement incremental speedup benefit compresso lcp benchmark gcc cactusADM libquantum astar soplex directly explain instance data movement optimization extra memory access gcc libquantum compresso decrease maximum slowdown due compression benchmark mcf omnetpp forestfire pagerank lcp align performs slightly compresso due parallel metadata memory access lcp allows speculative access memory assumption cache access exception benefit application metadata rate optimization performance breakdown average performance increase alignment friendly cache predict incompressibility dynamic inflation expansion metadata cache optimization memory capacity impact evaluation impact memory compression apply constrain memory relative uncompressed baseline href bzip sensitivity limited memory uncompressed constrain achieves maximum performance unconstrained memory mcf gemsfdtd lbm highly sensitive memory capacity stall due frequent constrain memory environment incompressible compression however workload benchmark compressible execution vii benchmark almost linearly sensitive amount memory threshold memory perform graph forestfire namd due compression benefit lcp exhibit average relative performance compresso upper bound performance approximate unconstrained memory mcf gemsfdtd libquantum soplex milc astar tonto forestfire lbm leslied hmmer sjeng omnetpp gcc namd xalancbmk cactusADM calculix sphinx perlbench bzip gromacs gobmk bwaves povray href pagerank mcf bwaves graph perlbench forestfire povray hmmer forestfire pagerank graph cactusADM tab IV workload multi core evaluation achieves relative speedup combine evaluation average relative overall speedup lcp lcp align compresso compresso therefore outperforms lcp core performance evaluation cycle simulation performance overhead compression core performance compresso despite mcf performer core pronounce bandwidth reduction benefit due soplex libquantum exhibit performance compresso lcp due data movement optimization overflow handle overhead pressure metadata cache core scenario impact contains omnetpp sjeng performer isolation due metadata rate compression bandwidth benefit gcc namd balance performance contains forestfire pagerank graph metadata rate compression speedup slowdown compresso performs lcp across lcp align performs compresso respectively due benefit gain lcp parallel speculative memory access evaluate KB metadata cache target warehouse compute metadata cache increase performance evaluation compresso benchmark omnetpp forestfire pagerank average relative speedup compresso lcp lcp align optimization performance breakdown average performance increase alignment friendly cache predict incompressibility dynamic IR expansion metadata cache optimization core memory capacity impact evaluation performance benefit compression apply constrain memory benchmark maximum footprint allows slack memory phase benchmark workload sensitive memory availability despite xalancbmk pagerank povray sensitive limited memory isolation benchmark workload compress incompressible benchmark workload memory workload performs additional memory difference behavior exhibit performance compresso compresso lcp gain performance compresso lcp forestfire pagerank graph gain significant memory capacity LinePack lcp average compresso achieves relative performance constrain memory achieve lcp upper bound unconstrained memory performance overall performance multi core scenario combine cycle memory capacity impact evaluation relative overall speedup compresso lcp lcp align relative constrain memory overall core setup compresso achieves performance lcp overhead compresso core usage due slowdown speedup dram due data movement memory controller due BPC compressor decompressor related  hardware compression granularity cacheline pack data movement optimization ibm mxt partially llc MC KB RMC BST MC LinePack lcp TLBs MC lcp  partially MC lcp DMC partially MC KB lcp compresso MC LinePack tab related summary metadata cache upon synthesize BPC TSMC standard mhz active utilization dram channel active 2GB ddr channel access KB metadata cache dram access compress benchmark zeusmp cactusADM exhibit dram benefit due access zero cache metadata cache mcf sjeng forestfire pagerank omnetpp exhibit dram due extra memory access metadata cache astar dram due compression related data movement comparison lcp compresso achieves saving lcp align uncompressed compresso reduces dram core usage previous compression increase bus toggle detailed evaluation bus model beyond scope demonstrate dram channel switch comprises dram subsystem conclude bdi compression consume internal dram switch baseline memory optimize encode reduce internal dram switch average across benchmark compresso reduces compress data movement average compression ratio decrease overall saving positive overhead BPC compressor optimization synthesize TSMC standard mhz overall  correspond roughly nand gate KB metadata cache roughly  although overhead negligible benefit compresso justify cache offset calculation compresso cache within compress metadata cache calculate offset custom latency arithmetic bin shift reduce width input adder nand gate nand gate delay reduce gate delay optimization input account ddr mhz allows gate delay cycle offset calculation partially parallelize metadata cache lookup thereby overhead cycle related software memory compression implement commercial decade linux linux android iOS background application compress access compress generates compress fault prompt OS decompress physical frame hardware involve uncompressed thereby limit compression benefit prior compression cache compression bandwidth compression summarize prior memory compression tab ibm mxt mostly non intrusive OS OS inform memory hardware watermark compressibility furthermore OS zero avoid repacking data hardware compression granularity KB memory KB worth data decompress transfer address MB cache KB introduce feature significantly hinder performance efficiency mxt robust memory compression OS aware mechanism propose improve performance mxt RMC translation metadata data cached metadata chip RMC compress  hysteresis  inflate cache linearly compress OS aware address challenge cache offset calculation compress compression remains exception cachelines achieve compression hence metadata access memory access  target lightweight OS manage memory allocation deallocation hardware however OS adapt scenario incompressible data mxt OS visible shadow address OSPA lcp approach handle compute cache offset within compress DMC propose compression LZ KB granularity lcp bdi decides compression mechanism OS transparent fashion KB boundary potentially increase data movement additionally lcp apply KB instead KB compression  propose compression increase capacity hybrid memory cube HMC compress uncompressed per vault float boundary compression allocation granularity cache thereby movement however maintain strictly compress uncompressed amplifies cache data movement scheme explicit defragmentation cycle IX conclusion identify important tradeoff compression aggressiveness unwanted data movement overhead compression optimize tradeoff enables compresso  compression benefit reduce performance overhead data movement memory compression architecture unmodified operating propose detailed holistic evaluation accuracy novel methodology memory capacity impact evaluation overall reduce data movement overhead compression memory compression pragmatic adoption holistic evaluation