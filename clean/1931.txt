slowdown technology mandate rethink conventional cpu architecture quest performance capability direction chip  cache LLCs server processor argues alternative LLCs limitation chip constraint limit storage capacity planar interconnect span increase access latency contention cache capacity hurt performance workload colocation overcome limitation propose stack private llc organization silo combine conventional chip private optionally cache per core private llc stack dram stack llc slice directly core silo avoids planar span private cache inherently avoids inter core cache contention engineering dram latency affords access delay MB capacity per core technology evaluation silo outperforms conventional cache architecture traditional workload deliver performance isolation colocation index cache private cache dram  server workload introduction datacenters underpin digital society storage retrieval processing capability increasingly complex information centric task server inside datacenters core performance server processor maximize overall throughput latency latency critical service volume data consume machine actor essential increase per server performance increase demand  loom traditional technology challenge extract processor performance constraint largely flatten improvement thread performance decade meanwhile slowdown moore combine  manufacturing technology node approach growth core trend motivate beyond traditional chip multiprocessor cmp architecture performance efficiency direction rethink cache hierarchy server server processor tend employ llc capacity attempt capture massive data instruction server workload instance recent core intel core ibm processor feature MB MB LLCs respectively consume estimate  measurement massive footprint proportionately reduces available core attempt shift balance recent argue llc capacity server processor however llc incompatible workload VM consolidation staple feature datacenters instance google already multiple workload per machine sometimes dozen task consolidated server cache capacity indeed useful server exist configuration ideal constraint limit llc capacity afford within yield effective secondly capacity hence access llc slice due multi hop chip network topology llc significant challenge isolate workload evidence recent explore issue around llc contention multi core chip llc effective facilitate latency inter thread data capability useful server workload engineer scalability minimal  data argue private LLCs stack dram prefer alternative traditional chip llc architecture stack naturally overcomes limitation planar silicon offering multiple layer densely integrate memory unfortunately exist stack cache around commodity dram technology capacity latency exist dram cache architecture access latency par memory render unsuitable replace chip cache access latency exist dram cache significant interconnect delay incur cpu rout dram cache interface dram access target capacity orient dram architecture efficiency access latency insight neither fundamental readily address architecture annual acm international symposium microarchitecture doi micro address delay per core private dram cache dram cache partition vertical slice vault exist hybrid memory cube HMC vault memory controller completely independent vault data storage access unlike HMC discrete memory chip vault propose stack cpu vault sits directly core organization naturally avoids chip delay inherent llc architecture latency core private dram cache slice address dram latency engineering vault latency access expense capacity provision subarrays tile stack private llc organization silo combine conventional chip per core private optionally private llc slice stack dram dram optimize latency expense capacity reduce access llc vault cache coherent conventional directory protocol dram metadata rate private dram cache  dram metadata directory access detrimental performance meanwhile usage private cache naturally eliminates inter core llc contention facilitate workload isolation core cycle accurate simulation contribution corroborate prior server workload benefit llc capacity highly sensitive llc access latency inter core data minimal workload introduce stack private llc organization silo chip architecture private cache silo avoids interconnect latency overcomes chip constraint stack dram llc private slice directly core private cache coherent conventional coherence protocol directory metadata embed stack llc demonstrate latency optimize dram cache architecture lower latency  technology node silo propose affords access latency core private dram llc slice MB capacity cloudsuite workload core silo deployment improves performance stateof server baseline combine llc conventional dram cache silo performance isolation preserve application performance colocation llc capacity normalize performance MB MB MB MB MB MB MB MB web data web frontend mapreduce solver sensitivity llc capacity fix latency II motivation examine representative server workload detail sec VI investigate requirement llc perspective goal characterize performance sensitivity workload llc capacity access latency inter thread data sensitivity capacity understand sensitivity workload llc capacity sweep capacity fix access latency core setup detail available sec VI baseline MB llc configure per processor specialized server processor architecture target workload llc capacity access latency unchanged baseline plot workload performance increase llc capacity data normalize MB workload marginal performance gain MB MB attribute although increase capacity secondary capture fully limit performance benefit processor advocate llc minimize access latency footprint beyond MB however performance benefit secondary fitting llc data web frontend solver performance gain MB baseline MB MB web differs somewhat benefit increase capacity MB gain performance MB 1GB secondary sensitivity latency analyze performance sensitivity workload llc access latency llc plot minimize clutter llc capacity MB beyond identify previous deliver benefit capacity sweep access latency baseline MB llc twice baseline latency increase access latency normalize performance MB MB MB MB MB performance sensitivity llc latency capacity normalize MB baseline  geomean workload geomean performance workload normalize baseline llc capacity LLCs translate performance latency gain capacity rapidly diminish increase latency MB 1GB llc latency baseline performs MB llc baseline latency MB llc latency approach twice baseline configuration approach performance MB llc corroborates processor LLCs sub optimal server workload highly sensitive llc access latency memory  expose latency issue core llc access latency detrimental workload accompany llc capacity sensitivity exist server processor deploy LLCs naturally accommodate inter thread data arise producer consumer data exchange synchronization LLCs facilitate capture dirty eviction writer private cache subsequent request core without indirection access server workload characterize extent RW benefit deliver llc accommodate MB llc parameter described sec VI llc access category writes non core writes  writes core writer writes  generally limited RW across  workload finding mapreduce solver negligible RW web data exhibit RW respectively due parallel garbage collector potentially collector thread remote web data web frontend mapreduce solver llc access   breakdown access llc normalize performance web data web frontend mapreduce solver performance impact increase latency RW latency increase multiple baseline core inter thread communication application thread evaluate impact RW data performance quantify performance impact artificially increase access latency  factor increase access latency RW performance degradation latency RW performance data web frontend lose performance conclusion incidence RW workload largely insensitive llc access latency RW data implies deliver llc accommodate access data workload domain summary overall workload benefit llc capacity capture vast however performance benefit diminish capacity accompany increase access latency workload inter core data insensitive latency data LLCs server processor fail accommodate workload characteristic chip constraint limit llc capacity afford planar interconnect delay incur access latency remote cache llc organization facilitate latency inter thread data exchange however impact workload core core llc core core conventional llc core core core core L1D LI L1D LI L1D dram layer cpu layer core private vault llc llc llc llc vault controller stack private llc conventional propose cache architecture core cpu simplicity silo overview silo directly accommodates characteristic server workload optimization directly address deficiency llc silo completely  llc private cache hierarchy per core private cache overcome latency bottleneck cache limit interconnect traverse access whereas conventional llc employ nonuniform cache access NUCA organization request rout silicon remote cache private cache core naturally minimizes interconnect delay another advantage private cache naturally immune cache contention significant concern LLCs deployment scenario involve multiple workload unfortunately simply convert conventional llc private effectively constrain core capacity MBs instance generation intel mainstream server processor broadwell feature llc MB capacity per core configuration aggregate llc capacity multi core broadwell cpu sufficient capture meaningful portion server workload instruction data however private configuration capacity  inadequate overcome limitation planar silicon silo deploys optimization stack dram avoid interconnect delay maintain latency benefit private cache silo organizes dram vault sits processor core vault introduce hybrid memory cube multi stack dram dedicate vault controller logic layer stack silo dram controller vault cpu core directly beneath vault optimization aim reduce access latency vault engineering dram stack access latency explain involves shorter shorter bitlines wordlines optimization reduce storage capacity per vault traditional capacity optimize dram afford ultra access latency chip peripheral decoder wordline driver etc subarray subarray subarray peripheral amplifier subarray subarray tile wordline bitline tile wordline bitline global wordline peripheral dram internal MBs capacity per core technology summarize silo overcomes delay constraint LLCs private stack dram cache additional benefit private cache immunity cache contention plague llc finally silo reduces access latency dram cache engineering dram latency expense capacity latency optimize dram cache detailed discussion aspect silo architecture cache coherence IV architecting dram cache dram technology dram chip comprise dram peripheral circuitry organize hierarchical structure dram chip peripheral decoder width refer subarrays bitline amplifier subarray consists tile global wordlines tile local wordlines driver grey peripheral vertical wordline driver horizontal amplifier tile dimension bitlines local wordlines subarrays tile bitline local wordline electrical load proportional transmission delay longer longer bitlines wordlines peripheral circuitry amplifier wordline driver longer efficiency define dram chip latency conversely shorter reduce electrical load peripheral circuitry choice optimization target commodity dram commodity dram minimize  target affect dram hierarchy firstly dram manufacturer limit normalize normalize latency tile dimension latency dram tile dimension access latency peripheral circuitry chip dram secondly minimize decoder employ per chip ddr subarrays minimize occupy subarray amplifier reduce footprint amplifier important density optimize amplifier dram lastly subarray comprises tile reduce hence footprint local wordline driver subarrays tile improve dram efficiency longer bitlines wordlines naturally increase latency tile dimension access latency dram core tile dimension dram latency quantify tile dimension dram latency model 1GB dram detail sec VI reduce tile dimension combination dram parameter namely per bitline  per wordline  tile dimension correspond shorter bitlines wordlines plot access latency function tile dimension normalize baseline micron ddr tile dimension dram reduce tile dimension baseline decrease access latency increase beyond mere access latency achieve tile  increase conclude reduce tile dimension increase tile subarrays fix effective trading capacity latency valuable optimization however beyond latency gain  justified optimize stack dram cache latency silo described sec silo vault arrangement inspire HMC treat vault per core private cache silo avoids planar interconnect traversal vault capacity access latency latencyoptimized  MB MB MB MB MB MB MB scatter plot vault capacity access latency function dram parameter detrimental access latency reduce latency within vault silo sacrifice efficiency latency optimize custom dram core discussion sec IV silo introduces additional peripheral circuitry effectively reduce delay achieve optimization per vault increase parallelism minimize queue memory interface shorter reduce dram hence global wordline subarrays per reduce bitline within subarray tile per subarray reduce wordline stack thermal feasibility maximize dram cache capacity silo employ stack height stack cache silo limited thermal constraint additional layer dram increase chip celsius negligible thermal distribution industrial specification feasibility dram stack layer logic underneath layer stack widely available mapping model timing stack dram vault silo perform technology analysis CACTI methodology detail sec VI conservatively model dram stack assume per vault core beneath constraint perform dram parameter sweep vault budget plot capacity latency capacity easily budget maintain access latency MB MB capacity increase latency increase MB MB capacity latency increase another capacity MB increase access latency parameter latency optimize capacity optimize efficiency tile access latency comparison latency VS capacity  vault    latency  per vault capacity MB access latency sweet latency optimize traditional dram cache capacity latency MB per vault justified interconnect delay cpu chip chip interface nanosecond dram access latency additional latency difference latency MB MB amount modest overall delay MB sweet chip dram highlight difference technology SRAM dram memory technology gradual slowdown feature scalability exploit vertical stack overcome constraint traditional cache hierarchy attractive option explore dram stack project layer ultimately determines capacity stack cache limited primary factor thermal dictate maximum height stack manufacturing technology integration future improvement  integration technology integrate  stack viable capacity indeed  roadmap project thickness shrink stack layer silo DETAILS detail aspect silo organization dram cache tag data placement cache coherence performance optimization dram cache organization silo stack dram cache data associate tag directory metadata layout data metadata cache focus data tag placement discus cache coherence directory organization maximize available capacity silo cache organization observation tag data significant latency increase due serialize access silo leverage previously propose technique integrate data correspond tag unified fetch tad access dram tad tad tad dir dir dir dir data tad tag dir entry tag logical vault cache directory dram dram organization cache tad avoid delay tag serialization silo inclusive chip private cache organize mapped structure mapped organization avoids latency overhead associative compensate capacity dram cache meanwhile inclusion simplifies coherence easily afford capacity dram cache directory cache coherence silo conventional directory MOESI protocol maintain coherence private cache core private cache hierarchy consists chip cache stack dram vault trigger directory access logically directory sits dram llc logically closer memory physically directory distribute address interleave fashion directory metadata dram cache explain directory organization llc private inclusive mapped implication llc private inclusive tag directly proportional llc capacity assume vault unique respect vault tag accommodate tag across vault secondly associativity directory dictate core  cache llc mapped inclusive cache observation duplicate tag directory organization without vector logically directory organize associative tag core directory entry tag coherence directory entry indicates core cache associate instance tag directory indicates core cache sharer reading tag logical directory coherence update modify directory entry tag however directory entry update core transition exclusive coherence protocol processor conventional onchip llc llc serf coherence writeback core involves simply update llc however private cache coherence memory writeback incurs latency bandwidth overhead memory access avoid expensive writeback dirty eviction core silo MOESI protocol maintain coherence indicates valid dirty cache respond coherence request MESI protocol primary advantage MOESI modify directly core without memory performance optimization silo local dram cache vault access directory node directory metadata resides dram cache fetch dram cache access request another node another dram cache access incur dram cache lookup access chip penalty private dram cache performance optimization silo architecture local vault predictor tad organization dram cache discover dram access completes predictor  avoid dram access avoid associate latency directory cache core private cache hierarchy trigger dram access request directory node fetch directory metadata directory cache eliminate dram access directory metadata chip SRAM optimization apply separately concert option sec vii discussion silo non commodity dram achieve latency maximize performance gain traditionally dram resist however boom datacenter presence  player google amazon facebook tilt dynamic dram customization accommodate specific datacenter customer trend customize processor deploy custom accelerator datacenters already underway gain  specialize dram offset non commodity dram chip LLCs occupy around chip server processor associate estate expensive server CPUs generally built technology silo completely avoids chip llc  associate estate afford reduction improve yield addition core within baseline processor llc another benefit eliminate chip llc reduces demand chip interconnect rate private stack dram reduce chip instruction data traffic afford eliminate chip llc reduce delay feature costly wise faster  evaluation conservative advantage noc optimization afford silo VI methodology evaluate model core cmp OoO core ghz II detail parameter extract llc dram cache latency CACTI detail sec VI dram cache memory assume policy outperform server workload assume fairly aggressive memory access latency combination modest core frequency memory access latency  silo llc relatively cheap faster core memory amplify penalty llc benefit silo llc rate conventional llc organization evaluation focus cache hierarchy superior workload sec vii evaluates cache hierarchy baseline baseline processor processor modestly llc cache hierarchy MB llc split cycle access latency average llc noc cycle baseline dram baseline augment 8GB conventional dram cache dram cache hardware manage arrangement server conventional dram cache dram technology memory access latency indeed package dram cache intel xeon phi knight slightly memory optimistically assume access latency conventional dram cache faster memory assume perfect prediction infinite bandwidth silo fully private cache hierarchy  dram vault llc custom latency optimize vault MB capacity per vault sec IV vault access latency cycle interface serialization cycle processor core 2GHz OoO rob ISA  KB cycle private stride data prefetcher interconnect 2D mesh cycle hop baseline chip llc MB NUCA cycle non inclusive MESI lru silo stack dram llc private mapped inclusive MOESI silo MB vault core cycle silo CO MB vault core cycle trad dram cache 8GB mapped memory access latency II microarchitectural PARAMETERS simulated SYSTEMS baseline chip SRAM llc per static access dynamic silo stack dram llc per vault static access dynamic memory static access dynamic memory subsystem PARAMETERS tad cycle vault controller delay cache access latency cycle silo CO silo capacity optimize vault MB access latency cycle cache access latency vault controller serialization delay cycle vault stack llc organization latencyoptimized vault evaluates dram latency optimization without private organization latency optimize vault stack directly core silo aggregate vault capacity 4GB core NUCA address interleave manner average vault access noc traversal cycle dram SRAM technology model CACTI  model dram SRAM access latency model dram SRAM technology SRAM llc account advanced latency reduction technique standby capacity constraint impose individual highlight text appropriate consume memory subsystem appropriate SRAM llc dram cache memory hybrid model framework technology specific parameter cycle accurate simulation statistic CACTI  extract parameter SRAM stack dram technology estimate memory dram parameter commercial ddr device specification summarizes obtain evaluation web apache  lucene client GB index GB data data apache cassandra client operation per web frontend apache http server connection  worker thread model mapreduce hadoop mapreduce apache mahout bayesian classification algorithm solver parallel symbolic execution klee solver enterprise tpcc ibm DB ese database server client warehouse 0GB 2GB buffer pool oracle oracle enterprise database server warehouse 0GB 4GB  zeus zeus web server connection  IV server WORKLOADS USED evaluation simulation infrastructure  multiprocessor simulator   model sparc ISA extends  OoO core memory hierarchy chip interconnect noc reduce simulation  integrates SMARTS methodology sample execution sample warmup architectural microarchitectural cycleaccurate simulation performance evaluate performance application instruction execute per cycle spent execute operating code metric reflect throughput workload evaluate various cache architecture workload workload web data mapreduce solver workload cloudsuite web frontend workload  latter replaces  web frontend workload cloudsuite exhibit scalability core medium workload cloudsuite beyond thread investigate utility silo traditional enterprise application detail workload IV investigate utility silo traditional enterprise application IV simulation sample drawn billion instruction billion per core workload sample cycle accurate simulation checkpoint architectural partial microarchitectural cache prediction structure cycle achieve steady cycle per sample multi programmed batch workload deployment representative public description sjeng calculix mcf omnetpp lbm namd gromacs mcf zeusmp calculix lbm tonto bzip namd mcf povray gcc cactusADM gobmk perlbench milc astar xalancbmk sjeng cactusADM bwaves calculix leslied astar gcc gromacs gobmk astar omnetpp zeusmp soplex povray spec  USED evaluation generate randomly drawn consist workload spec workload drawn without replacement sample billion instruction billion per core core setup cycle accurate simulation cycle measurement cycle vii evaluation discussion evaluate silo traditional cache architecture workload primary target ass enterprise batch application examine performance isolation cache hierarchy performance workload plot performance evaluate workload normalize baseline sec VI description evaluate silo consistently performance baseline silo llc capacity latency baseline silo improves performance geomean performance improvement across workload performance gain mapreduce solver respectively web silo achieves speedup sec II identify aggregate llc capacity MB beneficial performance web silo aggregate llc capacity 4GB MB per vault delivers performance workload baseline capacity optimize silo silo CO delivers geomean performance improvement slightly speedup latency optimize silo despite twice per vault capacity silo CO vault access latency sec IV consistent sensitivity sec II capacity beneficial accompany access latency similarly vault vault delivers geomean performance improvement despite employ latency optimize vault noc traversal overall access latency vault diminish performance benefit capacity vault normalize performance baseline baseline dram silo   web data web frontend mapreduce solver geomean performance workload llc access breakdown baseline silo baseline silo baseline silo baseline silo baseline silo web data web frontend mapreduce solver local remote offchip normalize llc silo baseline baseline NUCA llc local finally baseline baseline dram performance identify access latency conventional dram cache limited benefit baseline dram sec II benefit cache capacity disappear access latency conventional dram cache beneficial alleviate bandwidth bottleneck however server CPUs bandwidth limited workload recent corroborate google conventional dram cache benefit analysis characterize llc effectiveness silo baseline explore usefulness silo optimization llc rate plot normalize llc baseline silo silo consistently reduces chip baseline across workload rate reduction reduction solver mapreduce surprisingly workload performance improvement sec vii web data web frontend comparatively reduction consistent performance improvement majority silo local vault important performance local faster remote incur directory lookup multi hop noc traversal normalize performance web data web frontend mapreduce solver      silo optimization cloudsuite assumes ideal vault predictor ideal directory cache nonetheless remote silo beneficial faster memory access silo performance optimization sec identify optimization reduce latency incur dram access local vault dram directory local vault evaluate usefulness optimization limit configuration  silo optimization  silo predictor local vault access predictor perfect accuracy  silo directory cache directory cache perfect accuracy   silo local vault predictor ideal directory cache plot performance marginal performance improvement optimize data observes benefit speedup local vault predictor directory cache consistent RW characterization sec II data sensitivity llc access latency RW data additionally data significant amount remote vault   optimization reduce latency remote vault improve performance web frontend exhibit sensitivity latency RW data remote vault data however overall speedup achieve silo modest web frontend hence benefit performance optimization conclude benefit optimization outweigh extra complexity efficiency examine silo architecture memory subsystem dissipation parameter sec VI illustrates memory subsystem dynamic silo normalize baseline baseline silo reduces dynamic across evaluate workload rate silo significantly reduces chip traffic thereby norm dynamic access llc memory baseline silo baseline silo baseline silo baseline silo baseline silo web data web frontend mapreduce solver dynamic memory subsystem normalize performance baseline baseline dram silo   tpcc oracle zeus geomean performance enterprise workload reduce dynamic memory explains silo efficiency advantage silo  llc baseline due combination static dram dynamic per llc access access per due ipc llc consumption silo exceed across evaluate workload budget core server processor performance workload silo alternative enterprise batch workload enterprise workload plot performance enterprise workload normalize baseline sec VI description evaluate baseline silo geomean performance improvement silo CO gain vault vault delivers slowdown performance slowdown attribute access latency llc due combination dram access noc traversal unlike workload baseline dram performance gain across enterprise workload achieve baseline workload datasets application cloudsuite capacity dram cache baseline dram capture datasets frequent cache conventional dram cache access latency memory dram cache contribute performance improvement normalize performance baseline silo core spec multi programmed batch workload plot performance core spec baseline silo overall silo delivers significant performance gain average due massive core private cache capacity capacity advantage latency llc contention manner issue explore silo performs performance gain memory intensive application mcf lbm milc astar benefit capacity therefore exhibit performance improvement performance isolation heterogeneous application colocated physical server contend available llc  contention compromise performance concern application strict latency target private cache hierarchy silo premise remove llc contention guarantee performance isolation evaluate performance isolation silo performance web processor alone mcf memory intensive spec benchmark web core mcf core core setup llc configuration traditional llc silo VI performance web normalize alone web setup llc trend silo improves performance web alone secondly performance silo unaffected colocation mcf contrast web suffers performance degradation llc colocation conclude silo delivers significant performance improvement llc baseline performance isolation colocation cache hierarchy evaluation focus latency optimize cache hierarchy processor evaluate conventional cache llc silo web alone web mcf VI performance web  normalize performance    web data web frontend mapreduce solver performance workload hierarchy hierarchy KB private SRAM cache configuration baseline intel feature MB SRAM NUCA llc refer SRAM MB eDRAM NUCA llc refer  CACTI access latency SRAM cycle optimistically assume access latency eDRAM focus latency optimize silo variant title silo trend hierarchy SRAM llc silo improves performance workload average max consistent earlier motivation gain register mapreduce solver workload respectively improvement web frontend generally sensitivity cache optimization evaluate workload eDRAM modest performance improvement SRAM counter inferior silo overall trend consistent sec II llc capacity latency beneficial workload related maximize performance planar LLCs reduce average access distribute LLCs prior propose non uniform cache architecture NUCA static NUCA NUCA address interleave data across cache distribute chip implement multi hop noc traversal access latency remote cache dynamic NUCA NUCA adaptive data placement reduce average access latency combination data placement replication migration data available cache request core fundamentally scheme limited capacity nearby planar silo circumvents capacity latency planar cache core private stack dram vault MBs capacity stack dram cache stack dram technology identify suitable  cache technology extends density commodity dram bandwidth efficiency target application bandwidth intensive gpus core hpc processor indeed nvidia amd gpus intel knight feature stack dram due significant delay involve rout request request core desire dram access latency comparable memory intel knight access latency dram cache memory improve access latency serialize tag data lookup research proposal argue tag placement SRAM mapped dram tag policy reduce tag lookup underlie dram technology processor organization unchanged  introduce reconfigurable cache hierarchy compose SRAM  dram tile improve access locality  core application cache allocate application  cache fundamentally across core silo differs private cache hierarchy custom dram technology dram latency optimization various custom dram technology introduce commercial latency per commodity dram technical detail generally scarce tend advertise subarrays commodity dram vault silo however due latencyoptimized package discrete dram chip actual latency saving due  chip chip interconnect delay contrast silo minimizes interconnect delay dram stack directly processor treat dram vault core private cache research prior mitigate overhead additional peripheral circuitry latency reduction bitlines dram additional circuitry selective partition dram independent technique apply custom dram technology silo increase vault capacity without compromise access latency technique target reduce dram latency overlap access subarrays improve buffer locality exploit access technique overlap access latency request reduce actual access latency IX conclusion traditional technology  processor architecture embrace emerge technology architectural paradigm quest performance direction traditional LLCs limited improve performance future server processor unable satisfy requirement cache capacity access latency demand workload response introduce silo stack private llc organization combine chip private cache per core llc slice stack dram silo resolve latency capacity conundrum private llc organization latency optimize stack dram evaluation silo improves performance ofthe server processor workload performance isolation