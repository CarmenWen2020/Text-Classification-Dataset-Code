application access vast datasets latency emerge memory technology enable faster access significantly volume data however memory technology significant caveat random access latency cannot effectively hidden hardware software  technique namely microsecond killer microsecond goal critical exist hardware software cannot hide microsecond latency drastic exist platform utilize microsecond latency device effectively fpga microsecond latency device emulator carefully craft microbenchmark source  application exist indeed incapable effectively hiding latency however uncover exist sufficient  device replace  memory access prefetch request user mode context switch increase access parallelism enlarge hardware queue flight access accommodate parallel access conventional architecture effectively hide microsecond latency approach performance dram implementation application successful usage microsecond device predicate drastically hardware software architecture index killer microsecond emerge storage  application fpga introduction access vast datasets compute application data intensive workload web advertising machine translation data driven financial analytics unprecedented demand access vast amount data drastically storage landscape compute quickly access datasets effective fashion innovation across compute stack physic architecture storage hardware software library application traditional challenge access data increase latency data grows trend virtually storage data reside onchip storage memory disk remote server therefore architect variety mechanism hide data access latency amortize across bulk transfer cache data storage overlap data transfer independent computation data access latency hiding mechanism storage interface nanosecond finegrained access chip cache dram millisecond bulk access device spin disk hardware technique multi chip cache prefetching superscalar execution hardware multi thread memory schedule effective hiding nanosecond latency encounter memory hierarchy millisecond latency disk network hidden OS context switch device management technique effective conventional memory storage device compute landscape data warehouse computer introduce device gap memory storage latency microsecond variety technology deployed flash memory latency microsecond infiniband ethernet network  microsecond device 3D xpoint memory intel micron nanosecond introduce unfortunately exist micro architectural technique handle grain access nanosecond latency cannot hide microsecond delay presence pointer serial dependence chain commonly server workload OS mechanism incur overhead microsecond overhead acceptable bulk transfer traditional storage fundamentally defeat benefit emerge latency device dubbed killer microsecond although clearly understood intuitive completely lack quantification examination technical literature context microsecond latency storage target aspect hardware software platform responsible inability hide microsecond latency effective usage device precondition fundamental hardware software exist simply straightforward modification undertake quantitative discus II adoption disruptive microsecond latency storage device data intensive application incumbent integration memory capable grain cache data access latency hidden application developer hardware software platform investigate annual acm international symposium microarchitecture doi micro limit integration developed flexible hardware software platform expose bottleneck prevent usage microsecond latency device hardware developed fpga storage device emulator controllable microsecond latency hardware allows  mechanism commonly software synthetic microbenchmark configure exhibit memory parallelism mlp  memory instruction ratio factor performance data intensive application microbenchmark fpga platform bottleneck xeon server platform hiding microsecond latency verify identify bottleneck relevant workload source data intensive application platform performance microbenchmark trend suffer bottleneck although cannot effectively integrate microsecond latency device without hardware software modification drastic software modification demand device access prefetch instruction combine overhead  context switch hardware properly hardware structure pending memory access prefetch queue effectively hiding microsecond latency software manage queue interface nvme RDMA device scalable microsecond latency storage instead device memory mapped ideally latency interface QPI link ddr bus access load prefetch instruction although shelf processor readily storage device modification overall contribution propose hardware software mechanism hide  latency surprisingly contradiction intuitive understand killer microsecond novel mechanism strictly simulation representativeness establish careful experimentation  hardware software II background trend latency data access storage landscape rapidly data warehouse computer data intensive workload access datasets density per volatile dram non volatile spin disk flash hardware software latency hiding mechanism paradigm HW mechanism SW mechanism cache chip cache OS cache prefetch buffer bulk transfer cache multi KB transfer disk network overlap super scalar execution kernel mode context switch execution user mode context switch speculation prefetching hardware multithreading storage technology already stagnate future trend  latency shard data across memory server interconnect network memory data memory processing framework memory ram trend although storage medium dram access latency nanosecond remote access microsecond extra latency commonly attribute software stack networking OS schedule network device interface queue user kernel fortunately non volatile memory nvm networking technology situation  density nvm device 3D xpoint amount data server without expensive hungry dram furthermore multiple server capacity availability quality service RDMA capable network infiniband converge ethernet remote data within microsecond however successfully leverage device effective mechanism hide microsecond latency hardware software interface avoid overhead OS device management latency hiding mechanism exist latency hiding technique paradigm identify cache useful application substantial temporal locality bulk transfer useful significant spatial locality execution overlap helpful independent overlap access overhead discover dispatch independent access latency paradigm implement mechanism layer compute stack storage technology optimize grain access cache bulk access determinant decision random access latency device effectiveness hardware software mechanism hiding latency bulk access effective substantial spatial locality amortize access across useful data spatial locality careful attention data layout access notoriously task therefore argue grain access preferable bulk transfer particularly context server application exhibit spatial locality presence grain access limited locality latency hiding primarily onto execution overlap technique exist hardware mechanism overhead amount useful overlap data access super scalar execution limited instruction hardware multithreading expand amount available independent factor overall technique ahead effective hiding access notably benefit technique leverage memory parallelism mlp overlap multiple independent memory operation logic arithmetic operation alone limited potential hiding dram latency software technique however independent across thread potentially hide latency significantly overhead overhead  context switch microsecond obviously overhead mechanism effective hiding microsecond latency argue microsecond latency device disruptive impact data intensive workload data latency accommodate grain cache access therefore exist latency hiding technique conjunction exist  mechanism effective microsecond latency device grain memory device access  enumerate exist mechanism interfacing microsecond latency device interaction latency hiding mechanism previous mechanism clearly demonstrates applicability grain access mechanism omit evaluation quantitatively analyze performance uncover limitation mechanism described implementation device access mechanism corresponds factor durability storage density persistence latency ability hide historically important queue request response perform data access processor core writes message request queue completion data access device writes message response queue existence queue sometimes non obvious hidden hardware abstraction nevertheless behavior queue interaction various software hardware component dictate performance characteristic device interface elaborate mechanism software manage queue kernel manage software queue approach device access kernel manage software queue kernel allocates structure host memory request response queue perform access application performs kernel manages queue behalf application upon kernel request queue performs doorbell operation usually memory mapped MMIO inform device presence request application thread schedule OS queue trigger doorbell device request queue memory performs access host memory update response queue request completion finally interrupt signal host response queue kernel manage queue perform millisecond device bulk access although request overhead doorbell context switch device queue device queue interrupt handler context switch microsecond acceptable  bulk access device microsecond latency device however overhead  access latency kernel manage queue ineffective therefore omit access mechanism consideration application manage software queue faster device infiniband NICs excessive overhead  queue inspire alternative kernel permit application directly manipulate queue content doorbell mechanism although technique remove kernel overhead loses important kernel functionality kernel cannot schedule thread access outstanding technique avoids interrupt polling completion queue overcome limitation application manage queue implementation rely cooperative user scheduler processor core request scheduler invoked request  user thread response queue identify request wake  thread originally request newly access although approach avoids kernel context switch interrupt handle operates costly MMIO doorbell  request doorbell overhead reduce flag host memory indicates doorbell request device request queue pre define limit request queue empty doorbell request flag host flag doorbell request flag additionally reading request host memory unnecessarily serializes request processing avoid serialization device perform burst request queue potentially  available request queue minimizes latency overhead reading queue situation multiple request typically application manage software queue doorbell request flag burst request software manage queue  device mechanism lack optimization strictly inferior maximum achievable performance therefore highly optimize mechanism evaluate performance achievable software manage queue nevertheless evaluation overhead software queue management manifest bottleneck microsecond latency device hardware manage queue performance storage networking interface traditionally rely software manage queue however emergence storage memory scm architect memory interface storage device memory interface built around  queue incur overhead software queue moreover device storage access ordinary load instruction dram mitigate software development challenge demand access traditional memorymapped IO MMIO device status MMIO behaves dram allows software access device byte addressable memory software insert request hardware request queue simply perform load operation correspond address response handle perform automatically hardware wake dependent instruction load instruction retire without software overhead explicit polling processing response queue hardware queue automatic management entail advantage disadvantage request handle upside processor latency hiding mechanism automatically leveraged MMIO marked cacheable advantage locality listing prefetch device access function int dev access uint addr asm volatile  addr  yield return addr core issue multiple parallel access device overlap independent downside reorder buffer inherently limit amount overlap request dram access load  device rapidly reorder buffer stall instruction dispatch latency access completes load instruction retires whereas software manage approach quickly request belonging multiple software thread request queue perform parallel processor core limited discover access within software thread severely restrict extent access parallelism exploit CPUs implement simultaneous multi thread smt partition core resource multiple hardware context execute independent software thread smt additional benefit demand access core progress context another context latency access smt extract performance processor core device access encounter however hardware context smt limited context per core available majority commodity server hardware limit utility mechanism software prefetching software hardware manage queue described data access mechanism accent dichotomy storage memory storage device software manage queue overhead queue management benefiting ability switch thread processor request outstanding memory device hardware manage queue avoid queue management overhead limited  technique processor instruction parallelism within thread approach combine performance hardware manage queue flexibility user thread hardware queue management overhead request queue completion notification ultra lightweight user thread overlap amount independent device access enable thread perform device access parallel fortunately processor already building approach software prefetching associate hardware queue listing device access function prefetch access mechanism non binding  instruction enqueues memory address hardware request queue prefetch request processor pipeline software perform user context switch another thread without limited reorder buffer thread performs device access issue another non binding prefetch context switch another thread hiding access latency prefetch context switch independent thread previously thread abstract machine mit  context switch cache user thread execute hardware responsible automatically handle access request queue device access issue MMIO marked cacheable access microsecond latency device reading cache data surround request address access completes hardware responsible request instal cache cache request core eventually user thread issue prefetch perform context switch encounter synchronization operation prevents progress user scheduler switch thread perform prefetch request performs regular load ideally execution proceeds thread hide entire latency device access regular load operation automatically identify presence request address hardware queue MSHR status register await return meantime core issue subsequent independent instruction reorder buffer prefetch request completes hardware automatically software overhead wake instruction dependent latency access allows load instruction retire mechanism effective hiding  latency thread processor core switch thread inexpensive hardware queue accommodate outstanding request thread core requirement relatively easy hardware queue multiple memory hierarchy prevent sufficiently flight device access hide microsecond latency therefore increase queue remedy killer microsecond without drastic architectural processor discus IV evaluation methodology evaluate device access mechanism pcie enable fpga emulate configurable microsecond latency device xeon server host fpga correspond software library hardware software platform access mechanism demand memory mapped access prefetch memorymapped access application manage software queue instead commercially available nvm storage device bulk byte access   chose  storage emulator crucial interface option feature queue management detail software queue device latency finally importantly ensure internal performance device become artificial bottleneck limit flight access mask bottleneck explain fpga employ nontrivial trick challenge emulator QPI interconnect intel harp platform harp overly restrictive due inherent assumption architecture pcie interface flexibility interface demand prefetch mechanism fpga expose host cache addressable memory accessible standard memory instruction  implementation performs user context switch immediately trigger device access software prefetch instruction  queue fpga interacts inmemory software manage descriptor queue request response software lightweight user mode context switch mechanism perform prefetch rely hardware manage request software manage memory descriptor structure communicate request device software request completion microsecond latency device emulator host intel xeon  server cpu remove avoid socket communication hyperthreading disabled hardware prefetching disabled avoid interference software prefetch mechanism altera DE net fpga via pcie gen link host fpga receives request host responds request cache configurable delay configure response delay account pcie latency data application  pointer data dependent access emulate device faithfully host request memory content fpga dram amount data unfortunately fpga dram interface ddr mhz latency bandwidth impossible emulate hardware microsecond latency device emulator request fetchers software manage queue memory mapped access request dispatcher performance microsecond latency device perform  access dram overcome developed access replay mechanism fpga device twice application data access sequence address correspond data load sequence fpga dram dma prerecord sequence continuously bulk onboard dram access advance request host fpga precisely response latency emulate access performance report ensure memory access sequence remains deterministic across core enables reuse access sequence apply address offset handle request multiple core multi core significantly reduce bandwidth capacity requirement dram complexity detailed ensure internal device logic become limit factor increase parallel device request crucial performance bottleneck host otherwise fpga induced limit prevent manifestation host bottleneck memory mapped hardware emulator internal architecture demand prefetch mechanism fpga expose  memory pcie address register host modify host processor  memory register cacheable enable regular load software prefetch instruction perform device access processor cache hierarchy fpga expose interface perform dma transfer access sequence dram replay module maintain synchronization prerecord access sequence dram host request pcie transaction originate processor core ID subdivide expose memory assign core address enable steer core request correspond replay module host request replay module response enqueued delay module sends response host via pcie configurable delay ensure precise response timing incoming request timestamped dispatch replay module enable delay module response scheme vast majority host access however cpu occasionally cache performs speculative access replay module reorder spurious access pre sequence Ä±ve replay module implementation quickly lock due mismatch replay request sequence overcome deviation replay module slide replay sequence performs associative lookup request cpu cache replay entry host request skip immediately entry instead temporarily skip access ensure access reorder spurious request challenge although initiate instruction nevertheless critical respond correctly response cached cpu chip hierarchy respond incorrect data later execution replay module cannot host request within lookup request demand module data dataset dram ratio spurious request regular access therefore dram channel data access lightly load response delay deadline nearly access software manage queue interface software memory access descriptor memory request queue device update correspond descriptor memory completion queue descriptor contains address target address response data device ensures writes completion queue perform writes response address protocol emulator architecture differs memory mapped hardware interface expose per core doorbell register associate request fetcher module request request queue host software trigger request fetcher perform MMIO correspond doorbell trigger request fetcher continuously performs dma request queue host memory request correspond core replay module unlike memory mapped delay module sends completion packet request delay module performs transaction transfer response data another completion queue device access explicitly generate software spurious access demand module unused interface amortize pcie overhead descriptor request fetcher retrieves descriptor burst recently non empty location request queue reading descriptor retrieve burst continually fetch descriptor eliminates host software perform costly doorbell operation  request instead descriptor retrieve burst request fetchers update memory flag host software doorbell restart fetcher access software developed parallel performance software framework interact microsecond latency device framework kernel driver expose device via MMIO software manage queue described IV user thread library api perform finegrained device access built evaluate platform ubuntu kernel gnu pth library user thread extend device access mechanism optimize heavily hardware reduce interference unrelated thread linux  kernel option prevent schedule core perform measurement library api minimize application source code traditional software data memory avoid application code expose api application standard posix thread replace pointer dereferences dev access uint device access function synchronous thread return device access library transparently handle detail perform access switch user thread overlap access latency execution thread application source code minimal api function prefetch model implement sequence prefetch scheduler load load prefetched listing scheduler simply switch thread robin fashion software manage queue model sophisticated software integration scheduler  code scheduler poll completion queue thread remain thread manage fifo ensure deterministic access sequence replay finally advantage device microsecond latency extremely optimize context switch scheduler significant optimization reduce context switch overhead microsecond pth library nanosecond completion queue benchmark carefully craft microbenchmark precise memory access performs validate microbenchmark behavior relate scenario source application framework microbenchmark application data structure emulate microsecond storage application code data structure stack global variable etc memory dram microbenchmark microbenchmark enables analysis performance characteristic various access mechanism loop device access instruction device access mimic application access avoid perturb memory hierarchy comprises arithmetic instruction construct sufficiently internal dependency limit ipc machine microbenchmark instruction perform per device access refer quantity throughout thread microbenchmark dram baseline demand access mechanism multi thread overlap microsecond latency access prefetch software manage queue mechanism thread repeatedly performs device access instruction microbenchmark access cache ensure interpret temporal spatial locality across access report average iteration microbenchmark loop baseline replace device access function pointer dereference data structure dram report microbenchmark performance normalize ipc define ipc average instruction retire per cycle compute normalize ipc ipc microbenchmark device access ipc thread dram baseline application addition microbenchmark source software platform sacrifice functionality ability deliver pending signal individual thread important microsecond device data structure importantly code minimum maintain standard posix thread model demand access program model code bfs breadth benchmark graph traversal benchmark graph bfs source vertex iteratively explores destination vertex graph traversal central component data analytics recommendation social medium model route optimization bloom filter benchmark performance implementation lookup pre dataset bloom filter efficient probabilistic data structure likely bloom filter routinely employ largescale computation genome analysis memcached benchmark performs lookup operation memcached memory widely dynamic content generation application cache expensive sub operation database query interested behavior application regard perform core data structure access without bias memory behavior data access differs implementation relies additional auxiliary data structure microsecond latency device therefore modify application remove code associate access core data structure replace benign loop microbenchmark application report normalize performance obtain execution device access version execution singlethreaded baseline version data dram RESULTS analysis identify performance bottleneck microsecond latency device  storage grain access goal understand integration option limitation impose widely available likely host device demand access unmodified software simply microsecond device normal memory demand memory mapped access microbenchmark performs memory load operation access data instruction ipc request service latency device byte cacheable response load device performance baseline data dram within latency latency latency normalize performance demand access microsecond latency device normalize thread demand dram baseline reasonable instruction per memory load performance  likely precludes microsecond device  amount per device access instruction performance impact device access partially  however infrequent device access usefulness microsecond device questionable implication impractical microsecond latency device dram replacement unmodified software hardware instruction processor instruction execution cannot independent independent device access hide device latency therefore software assist hardware discover independent prefetch access technique software prefetch instruction access device perform context switch another thread listing hardware queue prefetch access performance device access latency normalize thread demand dram baseline thread effectively hide portion microsecond latency improve performance relative dram baseline longer device latency shallower slope device latency overlap robin user mode context switch minimal performance impact thread device latency performance application data dram notably  device marginally outperforms dram dram baseline suffers dram latency fully overlap useful comparison  overlap device access device access thread thereby reduce perceive device latency perform per device access normalize performance thread prefetch access various latency normalize thread demand dram baseline normalize performance thread prefetch access various normalize thread demand dram baseline thread hide device latency performance dram baseline reveals limitation  technique hardware request issue software prefetch instruction outstanding device access manage hardware queue buffer  intel processor essence  status register MSHRs pending cache memory access knowledge xeon server processor  per core severely limit flight prefetches thread additional thread improve performance particularly noticeable device thread hide device latency multicore effectiveness understand multicore scalability prefetch access behavior microbenchmark concurrently across multiple core cpu plot performance function thread per core normalize performance core dram baseline maintain consistency across thread per core multi core performance linearly baseline core  outstanding device access aggregate performance latency multi core limited simultaneous access exceed performance normalize performance thread core core core core core core multicore prefetch access various latency normalize thread demand dram baseline core capped thread LFB limit unfortunately another hardware cap emerges limit technique effective multi core although  across core independent chip interconnect core pcie controller another hardware queue core queue becomes another bottleneck limit performance improvement thread increase sufficient visibility chip location queue however experimentally verify maximum occupancy queue verify queue fundamental chip interconnect confirm simultaneous dram access outstanding multiple core simultaneous access outstanding dram regardless bottleneck existence limit trick leverage  multiple core increase simultaneous device access obtain performance impact mlp microbenchmark straightforward interpret necessarily representative application application likely perform multiple independent data access amount latency data access overlap memcached retrieval span multiple cache independent memory access overlap therefore variant benchmark memory parallelism mlp modify code perform context switch issue multiple prefetches dram baseline scheduler multiple independent access instruction issue memory parallel  device normalize dram baseline mlp mlp variant label variant microbenchmark previous graph similarly variant gain performance normalize performance thread prefetch access various mlp normalize correspond dram baseline thread effective however perform multiple access per thread prior context switch consumes  rapidly significantly reduces thread leveraged thread  thread peak thread summary LFB limit problematic application inherent mlp severely limit performance dram baseline brevity omit multi core latency mlp identical trend microbenchmark aggregate access implication combination user mode thread prefetch device access potential hide microsecond latency obstacle realize potential limited hardware queue prefetch request encounter device per core LFB limit thread latency device performance dram moreover chip  limit increase prefetch mechanism effectively multicore envelope calculation queue memory hierarchy microsecond latency effectively hidden flight device access per core therefore per core queue LFB provision approximately device latency microsecond parallel access chip queue entry queue device  microsecond core per chip sufficient access parallelism chip processor vendor calculation queue encounter prefetch instruction device accordingly ensure balance hardware queue dram access pcie therefore integrate microsecond latency device memory interconnect conjunction per core LFB queue direction emphasize hardware sufficient alone software prefetching normalize performance thread prefetch prefetch application queue application queue comparison application manage queue prefetch access device latency normalize performance thread core core core core core core multicore comparison software manage queue device latency context switch  parallelism approach program model remains unchanged application minimize device access complexity encapsulate thread library software hardware queue properly foresee prevent conventional processor effectively hiding microsecond latency approach performance dram data intensive application application manage software queue hardware queue processor limit prefetch access performance microsecond latency device therefore rely hardware queue instead queue memory descriptor doorbell interface interact queue host memory device effectiveness application manage queue comparison prefetch access application manage queue evident latency prefetch access encounter LFB limit application manage queue gain performance increase thread although thread bound hardware queue application manage queue incur significant queue management overhead fundamentally limit peak performance relative prefetch access thread thread device latency effectively overlap peak performance achieve however queue management overhead incur normalize performance thread core normalize performance thread core impact mlp software manage queue core normalize correspond dram baseline access limit peak performance  queue dram baseline multicore effectiveness scalability application manage queue multiple core unlike prefetch mechanism limited request across core application manage queue limitation achieve linear performance improvement core increase unfortunately core encounter request rate bottleneck pcie interface limit performance improvement pcie protocol significant per request overhead beyond payload response data cache byte byte pcie packet header transaction overhead addition device pcie access descriptor writes actual response data host memory writes request completion indication significantly increase pcie transaction device access request prefetch mechanism although significant minimize overhead burst request amortize pcie transaction sheer request waste available pcie bandwidth 4GB theoretical peak pcie interface core 2GB transfer useful data impact mlp performance application manage queue presence mlp prefetch relative performance dram baseline mlp noticeably without mlp queue management overhead software increase device access access batch context switch peak performance application manage queue workload mlp relative dram baseline mlp impact lower normalize peak performance correspond dram baseline behavior workload MLPs core increase amount data transfer per strain pcie bandwidth peak performance core instead core mlp quickly thread mlp impact mlp  queue severe  mlp queue management overhead pcie bandwidth constraint limit  performance relative dram baseline latency brevity analogous achieve identical peak proportionally thread implication prefetch access limited hardware queue application manage queue scalable killer microsecond however significantly software overhead primarily arises queue management performance parallel request limited queue management overhead interconnect bandwidth trend increase bandwidth generation pcie interconnects gigabyte per bandwidth likely bottleneck highend device queue management overhead however easily remedied software submit request completion notification device additionally programmability prefetching advantage  queue hardware cache coherence microsecond latency storage treat memory request multiple processor core prefetching device data hardware cache coherent across core  queue request device dram location response data hardware coherence moot writes software manage queue preclude across core complex software cache coherence neither  software developer seek easy performance memory interface normalize performance thread prefetch access core normalize performance thread bfs  memcached microbenchmark application manage queue core normalize performance thread prefetch access core normalize performance thread application manage queue core core performance latency application benchmark alongside microbenchmark comparison application confirm microbenchmark broadly applicable practical application extract data access code source data intensive application IV performance trend microbenchmark baseline dram implementation application data access perform demand dram without software modification device implementation modify application batch device request application permit batch memcached  limit bfs due inherent data dependency relative performance application prefetching application manage queue device latency normalize respective dram baseline immediately evident dram baseline allows core mlp issue memory access parallel manual batching effort application behavior microbenchmark behavior presence mlp effectiveness access mechanism prefetch queue although core performance prefetch access limited  achieve adequate performance dram baseline LFB limit due software overhead application manage queue baseline performance core reveals scalability access mechanism core hardware queue limitation fundamentally prevent prefetch access achieve adequate application performance scalability application manage queue inherently limited aggressive interconnect bandwidth queue management significant toll peak performance achieve performance core peak dram baseline performance core implication data intensive application exhibit memory parallelism exacerbate flight request microsecond device bottleneck identify previous application scalability effectively hiding microsecond device latency thread however exist hardware  device achieve modest performance baseline dram core multiple core extremely poorly multi core dram baseline VI related  coin killer microsecond refer microsecond latency appropriate latency hiding mechanism knowledge extent exist server platform inspire context switch mechanism hide latency  propose latency context cache mechanism hardware mechanism complimentary user thread efficient requirement described fpga proprietary ddr emulator emulation spirit pcie interface sufficient flexibility hardware software manage queue mechanism platform proposal architecting nvm device something replacement  disk MMIO access mechanism evaluate apply proposal expose storage memory device application load instruction grain access although focus access significant proposal nvm device motivate related endurance latency persistence consistency issue proposal target efficient transactional semantics persistent update improve performance presence various atomicity consistency requirement proposal motivate leveling requirement limited endurance flash nvm device latency technique absorb writes faster storage typically dram cache respect nvm device finally processor instruction handle cached persistent data vii conclusion although emerge technology accommodate vast amount latency storage exist mechanism interact storage device inadequate hide microsecond latency microsecond latency device emulator carefully craft microbenchmark data intensive application uncover exist sufficient microsecond latency device replace memory access prefetches user mode context switch enlarge hardware queue flight access conventional architecture effectively hide microsecond latency approach performance dram implementation application successful usage microsecond device predicate employ drastically hardware software architecture worth mention investigate performance impact microsecond latency device writes fortunately writes return critical prevent context switch reorder buffer latency easily hidden later instruction thread without prefetch instruction operation significant programmability implication investigate future