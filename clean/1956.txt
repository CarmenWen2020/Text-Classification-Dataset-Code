automaton processor AP accelerates application domain machine genomics however spatial architecture unable handle automaton program without reconfiguration  achieve throughput proposes architectural AP efficiently execute application exist non deterministic finite automaton NFA application enable configure AP chip underutilization careful characterization profile mechanism predict enable hence configure AP furthermore develop  execution mode AP efficiently handle predict NFA detailed simulation across application various domain newly propose execution model AP obtain geometric speedup baseline AP execution introduction application domain genomics malware detection machine data analytics exhibit parallelism accelerate spatial architecture exploit parallelism CPUs significantly reduce data movement spatial architecture usually consist interconnect processing expose parallelism programmable gate array FPGAs classic systolic array matrix google tensor processing spatial architecture fundamental challenge spatial architecture program concern fix available spatial program completely execution otherwise execution impossible multiple reconfiguration execution incur significant performance penalty traditional von neumann architecture issue typically handle traditional mechanism context switch virtualization however spatial program technique transfer directly issue affect traditional architecture graphic processing gpus massive parallelism amount prohibitively efficient multitasking focus architectural execute task spatial architecture automaton processor APs architecture accelerate processing non deterministic finite automaton NFA widely representation finite machine FSMs FSMs foundational application domain dna sequence network intrusion detection machine although exist approach accelerate NFA processing CPUs gpus none completely data movement irregular access due NFA transition lookup comparison AP executes NFAs natively achieves significant performance speedup primarily AP massive parallelism NFA mapped dram activate independently simultaneously cycle AP memory processing capability handle NFA transition without data movement processor memory AP core processing AP however future  application NFAs per application NFA aspect era data application likely mining database clamav anti virus application variant regular expression specify virus signature enlarge database NFA construct signature regular expression consequently AP chip longer exist newly propose technique enhance throughput FSM processing increase exist AP duplicate NFAs multiple input parallel newly propose parallel automaton processor duplicate NFAs parallel enumeration multi stride NFAs transformation increase transition processing multiple AP chip execute application NFAs independent batch NFAs execute batch entire input reconfiguring AP batch address performance inefficiency execution propose hardware software NFA application currently AP chip mechanism observation NFA enable execution TU OOVBM   portion NFA enable configure AP underutilization hence configure AP specifically unnecessarily AP chip transition refer enable remain enable quantitatively observation across diverse application sort increase percentage across NFAs application average application  observation explain revisit NFAs input NFA behavior highly input dependent attempt input enable enable predecessor input exception enable indicates input plausibly valid prefix regular recognize NFA input definitely however AP input enable NFA enable enable indeed NFAs AP insight develop softwarebased mechanism predict hence configure AP propose AP hardware efficiently execute predict knowledge proposes architectural efficiently execute NFA application AP summary contribution demonstrate NFA execution configure AP severe underutilization develop prediction mechanism classify NFA predict predict NFA execution develop effective partition scheme topological profile information develop efficient hardware mechanism execute predict sparse execution mode AP  detailed evaluation achieve geometric speedup baseline AP execution across application II background terminology brief background NFAs processing AP NFA NFA tuple alphabet input transition function accept reporting transition FSM non deterministic NFAs APs homogeneous homogeneous NFA accepts regular expression hexagon reporting NFAs visualize graph node transition NFA accepted multiple successor NFA enable enable prior execution driven input cycle enable input activate successor enable cycle reporting activate generates report relevant input NFA regular expression enable  input activates successor enable cycle activates activate accept successor enable input consume reporting activate input report generate successful baseline automaton processor AP schematic baseline AP chip AP dram spatial architecture NFA memory dram namely transition ste ste accept correspond input maximum alphabet width address decoder AP architecture therefore homogeneous NFAs incoming transition accept input treat homogeneous NFA synonymous NFA computational ability complexity rout matrix vector decoder input illustrates execution cycle AP configure NFA enable input arrives activates enables cycle downward arrow enable signal fed rout matrix cycle upward arrow enable successor cycle physical connection STEs rout matrix directional dash arrow AP chip consists core transition cannot across core due limitation interconnect transition compile reconfigurable interconnect network namely rout matrix entire input sequentially rate per cycle cycle input fed address decoder selects dram orange shade ste ste enable namely STEs combine vector information available previous cycle operation perform shade vector vector determines activate activation information rout matrix update vector enable processing entire input understand AP illustrate execution previously NFA via previously accepts accordingly correspond ASCII ste others remain therefore activate broadcast enable signal successor via rout matrix upward arrow motivation analysis analyze percentage likely avoid configure AP improve performance topological normalize depth predict enable NFAs clearly enable input away however input subsequent transition homogeneous NFA input homogeneous NFAs transition intuitively away likely enable additional increase mismatch verify intuition NFAs application execute AP respect depth NFAs simplicity exposition NFAs acyclic graph DAGs depth simply topological maximum topological topological cannot DAGs cycle NFA graph layer layer topological layer topological reachable layer layer reachable layer however NFAs DAGs later layer earlier layer cycle NFA contains cycle topological sort cannot perform graph therefore pre NFA identify strongly component scc marked component scc belonging scc marked construct graph graph NFA treat scc node scc node scc scc node  dag topological sort identify  topological sort topological belong scc assign topological scc illustration topological normalize depth absolute topological depth uninformative NFAs layer within application therefore normalize depth maximum depth NFA belongs normalize depth maximum topological normalize depth   function return topological normalize depth closer indicates NFA relatively closer indicates closer relatively shallow analysis normalize depth enable NFA normalize depth distribution enable evaluate application application comprise NFAs majority application normalize depth closer NFAs furthermore application normalize depth distribution enable majority application normalize depth deeper NFAs confirm conclusion significant negative correlation average correlation coefficient normalize depth percentage application ER enable enable distribution normalize depth NFA presentation purpose normalize depth classify shallow medium conclude highly correlate normalize depth overall shallow likely likely analysis performance benefit analyze ideal performance benefit completely eliminate configure AP potential benefit performance model assume oracular knowledge configure AP performance model baseline AP execution application across NFAs AP core capacity cap without loss generality discus AP core AP cap configure entire application AP configure AP multiple configuration NFAs collectively AP suppose NFA application AP therefore configuration AP  cap assumption individual NFAs split granularity AP architecture batch partition usually NFAs configuration maintain semantics configuration batch input batch NFAs execute input cycle spent input  input  batch perfect scenario identify  accuracy reduce  configure AP define resource  therefore speedup baseline cap cap sufficiently speedup proportional proportion correctly identify eliminate speedup baseline execution scenario illustrative illustrate benefit configure AP scenario baseline AP execution AP executes execution considers application baseline scenario AP capacity execution batch compiler partition application batch batch individually AP hence input execute twice sequential manner however oracular knowledge compiler generate perfect partition application perfect partition AP execute consume input significant saving execution cycle summary significant speedup achieve configure AP propose effective profile mechanism identify realistic scenario leverage profile information efficiently partition NFAs IV implementation NFA partitioning realistic implementation eliminates NFAs partition NFAs configures AP challenge although predict execution execution execution cycle batch batch batch baseline compile runtime input input input perfect partition batch application illustrative configure AP onto AP reduce execution input hence accuracy develop overhead technique improve accuracy prediction prediction transition configure AP mechanism safety net handle transition AP AP minimize prediction transition unidirectional avoid execution input AP propose partition scheme systematically address challenge profile scheme identify topological layer partition layer NFA application propose scheme handle transition AP intermediate reporting piggyback exist AP reporting hardware finally ensure unidirectional transition partition NFA specific topological proceeds topological partition direction profile prediction portion input application profile input basically compile profile input NFAs application assume profile information actual execution hence predict sub evaluate effectiveness profile prediction profile input application evaluate MB input MB input KB KB input profile input KB input profile input KB portion essentially entire input methodology evaluate effectiveness profile evaluation treat positive negative therefore positive TP profile input input similarly false positive FP profile effectiveness profile prediction percentage entire input accuracy recall precision input actually input negative TN false negative FN define similarly define accuracy TP TN overall profile prediction recall TP TP FN prediction predict precision TP TP FP prediction realize resource scope effectiveness profile average accuracy recall precision profile input evaluate application fermi spm specifically prefix KB entire input achieve recall input profile input consistent across application recall varies addition prediction accuracy precision conclude profile input identify actual execution therefore entire input profile remain actual evaluation vii partition AP architecture application split NFA granularity batch contrast partition NFAs topological granularity topological partition granularity previous analysis correlation normalize depth percentage partition topological granularity guarantee unidirectional transition predict subsection obtain partition layer NFA application partition NFA topological granularity IV fermi spm entire input actual execution enable data  configuration partition layer compile functionally simulate NFAs application profile input predict simulation NFA max  NFA profile input define predict  accordingly predict  predict NFA batch AP configure batch sequentially optimization optimization batch AP completely assign additional predict predict achieve incrementing subsequent partition layer NFA terminates capacity AP met batch partition sub demonstrate partition NFA partition layer calculate description IV handle transition partition imperfect brevity partition scheme NFA separately apply NFA application illustrates NFA partition partition layer dash however prediction perfect predict enable predict AP transition AP predict predict translation intermediate reporting partition NFA partition layer handle NFA introduce intermediate reporting exactly input reporting execution AP contains intermediate reporting along predict therefore enable AP predict activates correspond intermediate reporting instead consequently intermediate report generate notifies handler handler enable correspond predict constrain configure AP due constraint topological partition scheme consequently AP resource underutilized application topological partition predict intermediate reporting activate enable correspond translation predict discussion scc topological partition imposes constraint predict specifically scc scc predict topological partition layer predict reduce AP resource saving extent underutilization evaluate application topological perfect partition constrains average predict reality enable perfect partition NFAs arbitrary exception LV ER  prevent effective partition summary significant opportunity resource saving accurately identify partition layer NFA hardware intermediate report   NFA processing discus efficiently handle intermediate report generate execution predict propose enable intermediate reporting directs cycle input intermediate report generate although perform cpu incurs significant performance slowdown vii therefore propose execution mode AP analysis execution mode AP aforementioned propose augment AP mode BaseAP mode  SpAP mode BaseAP mode execution baseline AP execution however AP mode configure predict execution cycle via input execution BaseAP mode predict intermediate report input remain execution SpAP mode predict realistic partition cycle via perfect partition input remain predict perfect partition realistic partition illustration performance benefit realistic partition operation portion input execute SpAP mode execution execution BaseAP mode generate intermediate report handle SpAP mode SpAP mode AP configure predict AP mode consumes input driven intermediate report context develop operation SpAP mode enable enable operation allows intermediate report enable appropriate predict operation skip input handle intermediate report exists predict predict IV forth switch BaseAP SpAP mode intermediate report intermediate report tuple input ID sid denote intermediate report generate input cycle BaseAP mode execution enable sid algorithm pseudo code SpAP mode execution cycle enable performs operation input input intermediate report generate enable operation perform due scenario input input intermediate report input operation remain functionality SpAP mode BaseAP mode operation handle realistic partition scenario illustrative illustrative earlier performance benefit perfect partition realistic partition inaccurate prediction intermediate report handle illustrative demonstrate benefit execute AP BaseAP SpAP mode execution BaseAP mode configure predict execution intermediate report generate input input respectively input consume SpAP mode driven input algorithm functionality SpAP mode input intermediate report contains sid input report generate enable input input input output report index input input index input enable operation enable sid enable operation accept input activate reporting append successor intermediate report enable SpAP mode input intermediate report generate input intermediate report directly execution enable input SpAP input intermediate report therefore SpAP portion input execute shade implementation detail hardware implementation SpAP mode implement enable operation AP architecture implementation SpAP operation estimate execution overhead operation finally demonstrate storage requirement intermediate report operation operation modifies register input specifically ste enable operation update register input intermediate report configure SpAP enable enable cycle activate cycle therefore rout matrix route enable signal activate assume rout matrix flag ste enable enable operation intermediate report ID information enable correspond ste STEs rout matrix rout matrix hierarchical STEs utilize hierarchy perform enable operation rout matrix consists per core STEs ID enable ste hierarchical manner ste within decoder ste respectively specifically decoder decoder selects finally decoder enables ste enable operation parallel processing input SpAP mode enable operation execution overhead overlap enable operation intermediate report processing input SpAP mode multiple intermediate report generate input BaseAP mode input processing stall simultaneous intermediate report enable SpAP mode input intermediate report input input similarly input intermediate report input comparison pause processing input enable simultaneous intermediate report input processing resume cycle spent enable simultaneous intermediate report overhead overall SpAP mode execution account evaluation methodology intermediate report storage overhead intermediate report chip device memory portion report load chip memory consume SpAP mode queue entry load intermediate report intermediate report input ID tuple byte per intermediate report byte input byte ID overall storage intermediate report queue byte VI evaluation methodology application evaluate mechanism application ANMLZoo benchmark suite regex benchmark suite II application approximately baseline AP core evaluate application generate multiple application source clamav ham snort   convert regular expression cvd clamav virus database  format virus database input clamav ANMLZoo ham generate ham automaton approach ANMLZoo benchmark suite consistent ham ANMLZoo automaton  bound mismatch identification automaton format workload ham NFAs namely HM HM HM workload generate distance ham ANMLZoo generate input randomly snort snort application community register snort network intrusion detector convert regular expression  format network traffic input snort application ANMLZoo application resource requirement contains application capacity AP chip medium resource requirement medium contains application capacity AP  application grouped resource requirement experimental setup mechanism source virtual automaton simulator  mention evaluate AP cpu BaseAP SpAP execution AP cpu execution execute SpAP mode instead execute cpu summary evaluate scenario model timing mechanism AP cpu BaseAP SpAP simulator detailed timing AP cpu execution amount cpu spends handle intermediate report std  library therefore calculate speedup AP cpu execution machine intel xeon cpu cycle per BaseAP execution II evaluate application  reporting  maximum topological across NFAs grp resource requirement medium application abbr grp NFAs   clamav  ham HM ham HM snort snort ham HM spm spm dotstar DS  ER randomforest RF snort snort clamav cav brill brill  pro fermi fermi  pen randomforest RF tcp tcp dotstar DS  EM dotstar DS dotstar DS ham HM levenshtein LV bro bro summary execution scenario software hardware execution entire NFAs execution predict execution predict AP partition NFA granularity BaseAP mode AP cpu partition BaseAP mode cpu BaseAP SpAP partition BaseAP mode SpAP mode cycle BaseAP SpAP execution BaseAP SpAP execution execution cycle via simulator cycle BaseAP SpAP execution sum cycle spent BaseAP mode SpAP mode therefore  SpAP cycle AP baseline execution cycle BaseAP mode cycle SpAP mode performance per ste define metric performance per ste throughput ste average specifically performance per ste throughput cap throughput input cycle allows APs capacity technique improve performance solely increase AP ste AP occupies metric proxy performance overhead focus reduce  overhead performance bottleneck AP SpAP mode incurs stall cycle due simultaneous intermediate report stall cycle generic overhead related output reconfiguration evaluation output overhead rely exist proposes hardware software technique address reconfiguration overhead reconfiguring AP amortize AP execution executes input vii experimental RESULTS performance benefit scheme evaluate speedup application medium mechanism throughput AP application category application baseline AP STEs performance proposal observation AP cpu execution significant geometric slowdown profile input respectively however application application  HM HM DS snort achieve geometric speedup hardware modification BaseAP SpAP execution speedup majority evaluate application achieve geometric speedup input profile input respectively BaseAP SpAP execution AP application pen application generate simultaneous intermediate report lengthy enable stall SpAP mode IV fourth application  prevent efficient partition ER scheme configures BaseAP mode execution execution performance per ste evaluate efficiency scheme across wider configuration performance per ste observation although AP chip execute application performance application AP chip AP core AP chip performance ste STEs AP utilized application underutilization performance ste average scheme increase performance ste scenario AP core profile input consistently achieves performance ste AP predict eliminate configure increase AP utilization cycle SpAP mode prediction handle execution batch hence increase throughput resource saving speedup resource saving observation generally application resource saving speedup pen slowdown although resource saving SpAP mode execution enable stall due amount simultaneous intermediate report IV although resource saving profile input speedup snort predict due optimization IV batch extend predict capacity AP consequently resource saving speedup AP cpu BaseAP SpAP execution profile input capacity resource saving portion configure BaseAP mode speedup resource saving AP performance per ste various AP BaseAP SpAP execution profile input IV runtime statistic AP BaseAP SpAP profile input execution AP BaseAP mode SpAP mode respectively  stall enable operation handle simultaneous intermediate report  define proportion cycle skip SpAP mode baseline execution BaseAP SpAP execution BaseAP SpAP runtime statistic app AP BaseAP mode SpAP mode intermediate report    HM HM snort HM spm DS ER RF snort cav brill pro fermi pen RF however profile input recall IV speedup conclude speedup generally related resource saving explain speedup depends factor quality prediction enable stall intermediate reporting addition intermediate reporting increase increase configuration execution HM IV reporting BaseAP mode normalize baseline BaseAP SpAP mode reporting reporting intermediate reporting stack observation reporting BaseAP mode comparison reporting IM intermediate reporting reporting BaseAP mode profile baseline AP execution contains reporting reporting ER increase predict predict reporting decrease snort snort BaseAP mode execution reporting although scheme increase reporting aware effective softwarebased reporting compression technique apply scheme operation IV although application HM brill execution BaseAP SpAP baseline obtain speedup SpAP mode execution reduce cycle due operation operation define  proportion cycle skip SpAP mode formally  cycle  mode batch SpAP mode input  indicates operation  IV application SpAP mode conclude majority application execute percent input operation speedup AP capacity speedup AP chip capacity sensitivity capacity AP chip sensitivity speedup capacity AP application resource requirement capacity AP core speedup achieve scheme capacity AP observation specifically BaseAP SpAP achieves speedup profile input respectively addition demonstrate another sensitivity AP STEs application BaseAP SpAP execution achieves speedup profile input application related knowledge efficient architectural NFA application AP spatial architecture multitasking spatial architecture usually multiple context consume extra memory contrast BaseAP SpAP proposal relies ability eliminate dynamically unused NFAs improve AP utilization rely mechanism transfer spatially distinct partition accommodate device NFAs implement multiple context recently gate removal propose eliminate unused logic gate purpose processor IPs customize processor specific application approach eliminate NFA program hardware alternative implementation AP cache automaton purpose cache automaton processing technique complementary propose hardware software mechanism automaton processing efficient dfa NFA acceleration deterministic finite automaton dfa characterize previously respect implement machine parallelization parallel execution NFAs AP processor propose trading AP resource throughput however characterization dynamic execution NFAs specific AP execution model knowledge elimination dynamically unused AP resource complement parallel execution FSM decomposition FSM decomposition reduce complexity placement rout rout matrix simplify layout cascade decomposition closest static deterministic machine mostly dynamic behavior predict predict contrast propose approach  technique sequential machine theory focus increase AP throughput predict configure AP approach complementary apply bottleneck AP execution pipeline FSM decomposition reconfiguration efficient technique accelerate NFA execution AP reduce execution input IX CONCLUSIONS automaton processor AP efficient execute non deterministic finite automaton NFAs however spatial architecture AP challenge execution model efficiently execute task inherent NFAs avoid compute resource execution software hardware coordinate approach consequently execution model APs enables efficient performance processing task helpful towards wider adoption APs research direction enable efficient NFA processing