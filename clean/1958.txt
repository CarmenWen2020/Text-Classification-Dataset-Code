explosion convolutional neural network cnns accelerator cnn inference practical architecture community lion effort target cnn inference image recognition closely related video recognition attention accelerator target surprising video recognition computationally intensive image recognition video traffic predict majority internet traffic gap algorithmic hardware advance video recognition exploration flexible architecture accelerate 3D convolutional neural network 3D cnns core kernel video understand 2D cnns image recognition efficiently accelerate 3D cnns significant engineering challenge due variable memory footprint dimensionality address challenge novel accelerator morph adaptively spatial temporal tile strategy layer target 3D cnn codesign software infrastructure alongside morph hardware parameter hardware evaluate 3D cnns morph achieves average reduction consumption improves performance watt average baseline 3D cnn accelerator overhead morph achieves average reduction 3D cnns eyeriss index 3D convolutional neural network hardware software codesign video recognition dataflow hardware acceleration introduction convolutional neural network cnns marked tremendous progress image recognition advance task handwritten digit complex recognition core cnns compute intensive parallel dot operation combine importance computation style cnns target hardware ASIC acceleration stride direction recent progress image recognition stride related video recognition image recognition video understand significant attention partially NSF award CCF darpa SDH contract author contribute equally computer vision community algorithm numerous datasets developed domain achieve dimensional 3D cnns generalize 2D cnns image recognition account dimension thereby model capture spatio temporal feature 3D cnns style computation parallel dot slide data access 2D cnns likewise extremely compute intensive surprising 3D cnns attention target acceleration ASICs video processing important workload video traffic predict account internet traffic additionally application video understand surveillance drone etc mandate video understand hardware acceleration 3D cnns bridge gap hardware acceleration 3D cnn inference ASICs performance constrain environment challenge accelerate 3D cnn inference important 3D cnns generalization 2D cnns 2D cnn accelerator eyeriss efficiently evaluate 3D cnn temporal dimension 3D cnns expose data reuse opportunity cannot capture significantly exacerbates issue 2D cnn accelerator contemporary 2D cnn accelerator exploit spatial width height data reuse effectively however 3D cnns feature data reuse opportunity spatial dimension within frame temporal dimension across frame without mechanism advantage temporal data reuse arithmetic 2D accelerator evaluate 3D cnn frame frame incurs significant memory overhead important observation collectively issue 2D cnns significantly pronounce 3D cnns observation exceeds chip memory byte input activation filter TU OOVBM   memory footprint comparison layer representative 2D 3D cnns assumes input frame channel frame convolve filter channel temporal depth C3D alexnet structure remark recent 2D cnns resnet comparable input footprint average data reuse 2D 3D cnns input activation byte popular 2D cnns alexnet inception resnet 3D cnns C3D  I3D data cnns 2D 3D cnns partial sum data evident 3D cnn memory footprint data exceeds typical chip memory provision energyefficient accelerator 2D accelerator typically pin specific data input statically chip memory data sub optimal data filter chip memory strategy pin filter observation chip memory requirement dramatically input filter memory requirement significantly across layer 3D cnns 2D cnn hardware resource typically provision layer input memory philosophy cannot 3D cnns memory fragmentation overhead stem provision exacerbate observation chip pronounce relative chip ratio  operation  memory footprint sum input filter storage 2D 3D cnns data reuse computation per byte data 3D cnns 3D cnns significantly compute bound reduces ratio spent access chip memory overall 2D cnns whereas chip access consume majority 2D cnn acceleration increase reuse 3D cnns factor chip efficiently manage buffer reuse become prominent proposes morph novel accelerator accelerate 3D cnn inference observation decision architecting morph maximize configuration flexibility allows morph hardware adapt layer 3D cnn target observation morph hardware tile 3D cnn data chip storage tile regardless morph tile schedule processing resource dataflow input data footprint later layer prudent pin input chip memory vice versa layer finally observation morph endows flexibility multiple chip buffering maximize efficiency chip chip summarize contribution implement morph flexible 3D cnn hardware accelerator knowledge morph ASIC accelerator target 3D cnns codesign software framework configuration parameter tile loop loop parallelism layer 3D cnn morph accelerator evaluate morph across multiple 3D 2D cnns proposal flexible version accelerator eyeriss popular accelerator 2D cnns evaluate 3D cnns morph achieves average reduction consumption average improvement performance watt flexible baseline 3D convolution operation input video morph achieves average reduction 3D cnns eyeriss lastly implement morph rtl synthesize flexibility similarly provision inflexible accelerator II background video understand broadly category video understand craft handcraft evolve successful counterpart image recognition  sift generate histogram descriptor spatio temporal volume generate feature subsequent craft approach improve dense trajectory boost accuracy 3D convolution breakthrough image recognition convolutional neural network cnns cnns learningbased become approach processing video apply 2D convolution temporal pool video explore frame spatial optical temporal 2D convolution propose propose 3D convolution model spatiotemporal feature inspire significant 3D convolution derivative video understand consensus video understand 3D cnns currently recent datasets focus accelerate 3D convolution 3D convolution filter spatially temporally perform dot spatial temporal input video input spatial resolution frame temporal channel filter spatial temporal channel 3D convolve input output spatial channel frame graphically depict algorithm remark 2D convolution image 3D convolution hardware 3D convolution 2D convolution algorithm 3D convolution operation procedure  return compute requirement 3D convolution image recognition application 2D cnns 2D convolution kernel dominates computation analysis compute requirement 3D cnns 3D convolution compute bound relative 2D convolution inference C3D 3D convolution compute remain video preprocessing fully layer relu activation pool layer hence important accelerate optimize 3D convolution efficient video understand remark 3D cnn model grows similarly provision 2D cnn compute requirement increase onchip role relative dram 2D cnns tile accelerator access data hierarchy memory expensive chip memory relatively cheaper chip buffer input filter 3D cnn likely accelerator chip buffer efficient perform 3D convolution tile data tile chip improve tile effectiveness chip buffer organize hierarchy register compute tile memory hierarchy tile 3D convolution tile tile parameter  denote tile sub tile etc input chip broken tile  filter chip memory broken tile filter filter  filter filter tile convolve input tile computation tile input filter tile sub tile chip buffer  ktt tile dimension remain variable generally tile 3D convolution tile perform dimension halo dimension stride hence tile minimum tile input filter  convolution involves slide adjacent input tile overlap convolution stride tile dimension refer data halo depict 2D convolution halo dimension 3D convolution halo  dimension loop algorithm 3D convolution remains irrespective loop dot operation commutative loop entail chip memory requirement data movement compute memory resource utilization halo overhead interchange loop widely improve performance significant impact 3D convolution efficiency loop variable bound significantly 3D cnns II tile memory hierarchy similarly tile loop determines dimension tile fetch loop XY dimension outermost innermost dimension loop respectively minor traversal respectively  tile access combination spatial channel temporal dimension inside  tile loop whckf spatial dimension etc data transfer loop specifies data transfer buffer loop dimension data transfer buffer filter tile filter load innermost loop label input tile input load innermost loop label partial sum tile partial sum load innermost loop label loop whckf filter tile load innermost loop input innermost loop partial sum innermost loop data reuse directly proportional loop filter reuse spatially dimension due input tile halo II consecutive input tile overlap advantage slide reuse fetch overlap dimension loop fetch overlap data dimension slide dimension fetch overlap data dimension dimension parallelize convolution loop hardware accelerator execute loop iteration sequence parallel parallelize iteration entail spatially schedule iteration across processing PEs parallelize dimension algorithm conceptually convert loop parallel loop dimension opportunity parallelism varies across layer dimension varies layer easy parallelize across input layer later layer input dimension later layer fix choice dimension parallelism therefore hurt accelerator efficiency parallelism prior 2D convolution 3D convolution allows parallelization dimension addition spatial filter dimension loop PE parallelism accelerator dataflow motivation detail benefit configuration flexibility impact loop tile PE parallelism  3D cnn inference representative 3D cnn C3D assume accelerator chip buffer flexibly partition input filter partial sum evaluate VI tile memory fragmentation onchip buffer MB KB KB buffer typical parameter accelerator buffer hierarchy inclusive simplicity loop component outer loop buffer allocation across layer opt configuration inner loop access layer C3D various loop tile denote outer loop upper inner loop outer loop refers input tile  filter tile  fetch chip memory chip buffer loop variable spatial channel temporal filter inner loop refers input tile TWT tct tft filter tile  access chip buffer chip buffer compute loop variable spatial temporal channel filter denote distinguish outer loop inner loop schedule tile buffer remark assume input standard across 2D cnn inference knowledge 3D cnns video understand precision assume 2D 3D dram access analyze dram consumption across layer function outer loop dram access fully outer loop tile memory configuration   whckf scenario opt explain frequency iterate outer loop constraint outer loop   illustrate extreme buffer input stationary respectively outer loop whckf achieves overall average across layer finally whereas configuration outer loop across layer opt whichever outer loop optimal layer sweep tile inner loop plot configuration overall chip plus chip isolate outer loop important observation extreme loop observation input varies dramatically across layer specifically outer loop inner loop layer perform later layer filter progressively later layer prudent iterate filter layer trend reverse input spatial dimension progressively later layer loop overall whckf layer individually configuration incur overhead relative opt indicates 3D cnn accelerator flexibly outer loop layer granularity morph sufficient flexibility achieve opt remark layer dram regardless outer loop layer data entirely meaning tile multiple assume accelerator fetch redundant data insight flexible buffer partition input output opt input occupy buffer percentage layer whereas filter occupy later layer performance obtain data fully accommodate layer fully filter allocate input similarly layer fitting output entirely beneficial input layer overall buffer allocation flexibility important accelerator efficiency architecture flexibly buffering data sum tile bound KB chip memory access beyond dram access tile inner loop chip buffer significant impact overall inference methodology inner loop instead outer loop inner loop    average sweep parameter tile yield inner loop takeaway observation valid tile decision chip memory perform inner loop varies layer layer average inner loop layer opt selects optimal inner loop per layer significantly strategy hence 3D cnn accelerator flexible inner loop minimize spent chip buffer chip buffer tile analogous performance via PE utilization beyond accelerator performance maximize PEs earlier 2D cnn acceleration static dataflows PE utilization layer PE utilization later layer vice versa concurrent distribute tile filter spatial dimension layer around enable PE utilization layer PE utilization 2D cnn accelerator volume grows towards later layer input volume shrink 3D cnns previous analysis 3D cnn accelerator flexibly parallelize tile dimension II likewise avoid PE utilization issue away accelerator performs average across layer sub optimal relative accelerator adapt layer optimal inflexible hardware configuration efficiently layer 3D cnn alone across 3D cnns accordingly develops hardware architecture configure evaluate loop tile PE parallelism layer inference develop software infrastructure pre analyzes layer optimal hardware configuration layer component enable highly efficient inference across 3D cnns storage compute requirement sweep relative advantage multi buffer hierarchy buffer hierarchy 3D 2D convolution graph assumes input HWC frame convolve filter rsc temporal depth 2D convolution IV hardware architecture principle inflexible morph architecture modification architecture enable flexible tile loop PE parallelism architecture 3D cnns computationally 2D cnns additional data reuse opportunity dimension filter 2D cnns slide spatial along dimension input reuse dot per filter ignore assume stride exploit reuse 2D cnn accelerator architect custom logic input buffer processing without load input buffer 3D cnns generalize convolution spatial temporal dimension slide dimension increase input reuse factor 3D cnn stock 2D cnn accelerator sub optimal efficiency due lack temporal reuse 2D cnn accelerator perform 2D convolution frame separately merge resultant partially compute frame generate output frame output introduces overhead chip buffer transfer per frame mitigate overhead accelerator exploit  reuse chip buffer PEs analogous fashion 2D cnns exploit spatial reuse buffer hierarchy pre requisite exploit data reuse chip buffer hierarchy temporal locality data access perform analytic 2D 3D cnns input reuse filter dimension filter spatial spatial temporal input factor 2D 3D cnns focus discussion slide reuse  dimension filter reuse buffer dram buffer PE PE input psums input psums cluster PEs per cluster cluster per chip input psums processing PE morph accelerator datapath buffer buffer morph architecture morph architecture IV logic buffer logic configurable loop tile PE parallelism sufficient buffer hierarchy buffer hierarchy etc sweep loop tile fix physical buffer tile isolate hierarchy whichever configuration yield consumption representative layer 2D 3D cnns benefit onchip buffer hierarchy accordingly  memory hierarchy throughout interestingly additional memory hierarchy 3D cnns pronounce hierarchy yield improvement hierarchy relative improvement 2D cnns due additional halo 3D cnns II prevent halo overhead 3D cnns prefer tile buffer tile per access another memory hierarchy brings access distribute data locality across hierarchy efficiency beyond buffer reuse already sufficiently capture additional simply buffer writes buffer cannot additional data reuse architecture buffer hierarchy morph architecture morph built compute cluster cluster consists processing PEs accelerator consists chip buffer statically partition input filter partial sum psums partition pre tile across target 3D cnns 3D cnns VI buffer reside PEs cluster 2D cnns psums wider bitwidth input activation buffer dram buffer static logic cod FSMs governs data  perform etc input filter psums transfer broadcast network cluster cluster local broadcast network local PEs data consume ALUs PE alu lane vector accumulate MACC operation vector lane provision across output channel dimension PE accumulator register per lane reduce psum traffic alu finally remove processing tile buffer logically buffer dataflow morph architecture implement fix loop tile average performance watt across 3D cnns methodology analogous inflexible accelerator 2D cnn inference elaborate outer loop morph implement outer loop whckf implies input tile fetch dram dimension traversal filter spatial dimension inner loop chip buffer morph inner loop  PEs filter slide spatial temporal dimension maximize input reuse finally channel reuse psums iteration finally morph parallelizes across PEs fix chip network network chip NoCs morph architecture broadcast network implement unicast multicast broadcast style data transfer mask destination argue data reuse 3D cnns allows architect NoCs without starve compute PEs byte input activation PE consume unique input per cycle bus transfer byte input per cycle input reuse stride however bus input per cycle rate PEs steady argument implies rate strictly easy 3D cnn accelerator relative 2D cnns 3D cnns additional factor reuse input argument allows reduce bus bandwidth PEs bus bandwidth psums significant psum reuse respectively factor reuse input FSM FSM addr sel data mux data demux input psums network assign psum input psums network psum update configurable buffer shade programmable assign logic output vector indicates assign input filter psums concrete dataflow IV broadcast bus bandwidth 3D cnn parameter desire compute throughput filter stride typical desire compute throughput  cycle PEs cluster steady bus bus rate PEs morph flexible architecture propose morph architecture 3D cnn layer aspect impart flexibility rigid cnn accelerator namely configurable buffer PE logic NoCs datapaths flexible buffer partition enable tile per 3D cnn data without introduce buffer fragmentation flexible logic enable outer inner loop finally minor modification morph noc PE datapath enable PE parallelism additional flexibility frequency overhead previous consume chip buffer cnn accelerator dominates logic ALUs datapath hence despite logic flexibility overall overhead significant VI configurable buffer goal buffer configurability tile input filter psums buffer hierarchy minimize internal fragmentation physical buffer configurable buffer morph reuse chip memory input filter psums within buffer buffer memory configuration purpose expose software allocates memory granularity across input filter psums allocate data contiguously register assign configure layer denote data parallel mux demux logic index data output mux replicate output input filter psums data per access conflict programmable FSMs IV generate address address along assignment register responsible address derive local address signal reading data activates however internal fragmentation tile cleanly banking risk increase due dense SRAM array evaluation memory decompose sufficient variable tile efficiently regime overhead due banking minimal MB overhead buffer data width psums wider input activation specifically precision per activation psum rst avoid overflow due  per dot II handle disparity buffering wider width sufficient psum activation per access benefit decrease wider SRAM width buffer width appropriate activation psums multiple cycle access however seriously degrade performance psum writes infrequent relative  local accumulator register access priority morph buffer logically buffer avoid tile data buffer broadcast network psums writes psum update cycle priority standard pipeline backpressure stall psum update psum update relatively infrequent accumulator register alu filter psum writebacks logic morph architecture fix function FSMs implement logic specific loop memory hierarchy IV FSMs generate address data buffer loop transition loop dimension enable increment counter reset loop bound loop mask address trigger loop programmable FSM responsible generate address buffer signal tile etc shade register configurable layer  perform psums relative perform  processing tile etc signal significantly tile loop loop loop bound memory access tile along frequency load unload tile memory enable flexibility architecture configurable FSM FSM programmed configurable register denote loop bound loop parameterizable loop FSM loop loop bound accumulates output register loop user specifies bound FSM corresponds iteration loop iteration index iteration index behave software counterpart FSM output output register register iteration loop currently terminate loop terminate loop bound loop appropriately accumulator address sequence buffer loop logic FSMs trigger derive non address signal tile unload reload psum alu accumulator register loop iteration boundary logic programmable mask mask derive signal loop reset signal already generate FSM PE parallelism finally flexible PE parallelism architect mask register unicast multicast broadcast bus NoCs within layer PE parallelism fix exception tile occupy PEs due handle counter mask register  noc software  software optimization framework pre analyzes 3D cnns optimal tile loop parameter per layer knowledge underlie morph architecture popular software library tensorflow caffe theano program interface user mask optimization specific hardware software optimizer library 3D convolution request user chosen device morph optimization perform per cnn parameter configuration file recall instead analysis input layer parameter input activation parameter stride etc architecture detail morph PEs cluster memory configuration optimizer return configuration performance performance watt etc user configuration specifies tile loop spatial PE parallelism generate configuration layer parameter architectural detail optimizer enumerates configuration parameter generate inner outer loop buffer tile parallelization parameter chosen tile later heuristic sub tile remain buffer reduce tile PE parallelism discretized optimizer cartesian parameter enumerate configuration configuration outer loop inner loop generate configuration generate metadata configuration generates metadata calculation iteration chosen tile perform convolution storage requirement tile overlap tile slide halo output etc memory allocation optimizer heuristic tile data input psums chip memory configuration define chip memory hierarchy buffer tile data input psums allocate heuristic nth buffer tmin tmin minimum tile data perform 3D convolution sub tile tile  tile sub tile etc  inner loop buffer maximize  function return amount reuse tile inner loop buffer num sum tile exceed physical buffer tile candidate configuration information  calculates ratio buffer buffer update input tile reuse slide reduces halo output channel input output channel filter tile reuse spatially filter spatial input allocate specify configuration tile specify allocate percentage overall tile dimensional tile allocate generates 2D tile tile corresponds dimension tile minimum maximum dimensional tile dimension XY correspond dimension max min max max max min min max min min tile dimension meaning dimension  max dimension occurs  minimum II tile generate data cartesian generate candidate  performance calculation memory allocation information compute operation perform PE PEs active PE utilization writes buffer linear model convert writes operation consume layer analytic model convert PE utilization configuration metadata tile compute per PE per tile etc accelerator configuration generation optimizer performs configuration correspond performance available straightforward optimize performance performance etc configuration derive configure morph IV assignment FSM VI evaluation measurement setup evaluate morph chip performance model synthesize morph PEs rtl verilog evaluate ghz model arithmetic SRAM CACTI SRAMs assume   decrease per access yield SRAMs timing ghz dram network chip noc extrapolate estimate PE SRAM estimate CACTI assume noc swing however consume cycle regardless data transfer via differential signal comparison evaluate variant morph mechanism endow hardware flexibility IV morph software analysis loop buffer partition morph baseline average loop generate morph optimizer specifically outer loop whckf inner loop  static partition chip buffer partition average efficiency across dnns chip buffer  hierarchy input output eyeriss optimize 2D cnn accelerator simulate eyeriss  simulator density input filter normalize parameter eyeriss morph maximum compute available chip memory II eyeriss evaluates 3D cnn frame frame described IV consumption various 2D 3D cnns eyeriss morph morph normalize eyeriss buffer access compute across configuration comparison morph morph chip buffer described IV sub II simulation PARAMETERS parameter morph eyeriss PEs per cluster cluster vector width per cluster per PE per PE cnns evaluate evaluate propose network C3D owe popularity adoption action recognition I3D currently kinetics video dataset 3D resnet 3D version popular resnet 2D network multiple input frame alexnet popular  image recognition analysis consumption I3D C3D 3D resnet alexnet morph morph eyeriss 3D architecture adaptive memory requirement 3D cnns exploit data reuse layer accelerator fix strategy effective data reuse memory hierarchy access buffer expensive chip memory morph average improvement morph morph morph significantly outperform eyeriss consumption 3D cnns mainly eyeriss cannot exploit temporal data reuse loop 3D cnn layer frame increase efficiency gap widens evident I3D frame frame C3D due increase temporal data reuse opportunity frame morph exploit interestingly eyeriss outperforms morph alexnet morph buffer provision 3D cnns tile extract proportionally data reuse however 2D cnns alexnet benefit tile additional per byte outweighs improvement reuse C3D configuration  morph software analysis layer outer loop inner loop configuration layer   layer        whckf   whckf  whckf    whckf   whckf performance watt comparison morph morph morph however slightly outperforms eyeriss alexnet owe improve tile parameter flexible loop insight broken component dram access global buffer cluster buffer compute increase compute memory ratio 3D cnns chip consumption morph effectively reduce dram component respectively average relative morph demonstrates effectiveness flexibly configure loop hierarchy flexibility buffer partition tile morph software analysis C3D configuration chosen layer C3D tune minimal consumption morph architecture outer inner loop tile parameter reflect loop tile across layer improve efficiency performance per watt analysis morph morph theoretical maximum gflops performance improvement improve PE utilization performance per watt characteristic morph normalize morph morph delivers average improvement morph attribute improve PE utilization achieve adaptive loop parallelization morph adaptively chooses spatial parallelization parameter across layer utilization compute resource tile integer multiple dimension parallelization across filter PEs input later layer 2D accelerator suffer PE utilization challenge later layer due diminish input morph performance watt improvement alexnet 2D cnn suggests morph adapts chooses adequate parallelization improve overall performance hardware implementation finally IV overhead morph mechanism PE implement morph morph PEs verilog precision activation synthesis commercial met timing ghz SRAM obtain CACTI logic synthesize technology IV morph PE breakdown component morph morph buffer arithmetic logic morph additional overhead statically partition monolithic SRAMs morph arithmetic flexible loop increase logic relative increase due increase complexity FSMs logic buffer partition however overall increase almost negligible occupy chip memory vii related cnn accelerator due recent popularity 2D cnns image recognition related task plethora propose architecture accelerate 2D cnns accelerate 3D cnns ASICs although recent explore hardware acceleration 3D cnns FPGAs FPGAs reconfigurable directly adapt cnn configuration FPGAs reconfigurability reduce compute density morph architecture introduces flexibility allows adapt cnns retain efficiency benefit ASIC lastly recent trend exploit sparsity input compute compress model sparsity 3D cnns future sparse 2D cnn accelerator apply adaptive accelerator recent community towards flexible accelerator 2D cnns  proposes flexibility dimension parallelization PE utilization loop static dna proposes reconfigure datapaths dataflows MAERI proposes reconfigurable noc dataflows mention however target 2D cnns comparison morph flexibility tile loop parallelism achieve flexible buffering programmable logic cnn accelerator ASICs morph software component cnns prior optimal hardware configuration fpga accelerator entire parameterized configuration employ technique loop unroll loop interchange morph disadvantage reliance FPGAs incurs performance due fpga fabric conclusion propose morph novel 3D cnn accelerator video understand adaptively tile loop propose morph optimizer determines efficient hardware parameter setting layer target 3D cnn accelerator significantly outperforms inflexible similarly provision baseline accelerator eyeriss 2D cnn accelerator 3D cnns towards enable video understand device advocate impart flexibility cnn accelerator improve performance efficiency