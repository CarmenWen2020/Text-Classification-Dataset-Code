spite recent advancement data movement cnn accelerator remains significant bottleneck architecture eyeriss implement scratchpad within individual processing architecture tpu implement systolic array monolithic cache data movement prior therefore across account consumption aware cnn accelerator wax employ distribute memory hierarchy enable data movement array computational register adjacent subarray cache tile shift operation register reuse traversal overhead approach optimizes register fetch access kilobyte buffer perform operation beyond tile traversal cache interconnect uncommon reuse operand introduce data mapping dataflows dataflow WAXFlow achieves improvement performance reduction relative eyeriss wax tile performance tile introduction neural network accelerator emerge recent accelerator expend significant fetch operand various memory hierarchy eyeriss architecture  dataflow non trivial storage scratchpad register per processing PE maximize reuse therefore intra PE inter PE access eyeriss data movement across register file accelerator access monolithic buffer cache hierarchy eyeriss KB global buffer google tpu MB input buffer architecture implement grid systolic PEs increase cached data PEs PEs memory hierarchy cnn accelerator focus reduce frequently traverse data movement magnitude expensive compute floatingpoint consumes transmit correspond operand across chip consumes access MB cache consumes fetch chip LPDDR consumes initial comparison dnn accelerator switch fix  arithmetic compute magnitude recently technology HBM reduce memory per magnitude meanwhile chip wiring onchip cache benefit technology response relative shift bottleneck target chip traversal aware accelerator wax implement distribute memory hierarchy approach leveraged startup  implement array PEs beside cache subarray PE assign handful register register shift capability implement efficient version systolic dataflow PE therefore minimal wiring micro october columbus usa  access register adjacent register  cache subarray data movement within wax tile minimum layer cnns tile aggregate tile increase computational wax tile introduce novel dataflows perform slice computation reuse data movement largely confine within tile explore dataflows adapt reduce problematic partial sum update subarray reduces reuse data structure adder worthwhile analysis additional wax component contribute tile reduces relative eyeriss wax consumes hence distribution eliminate register file eyeriss dataflow WAXFlow enables overlap computation operand load subarrays compute utilization throughput eyeriss tile computational throughput increase tile wax tile therefore basis efficient device throughput latency orient server background commercial academic highlight extent data movement architecture eyeriss eyeriss monolithic grid processing PEs PE scratchpad register file kilo byte operand filter scratchpad entry implement SRAM partial sum activation entry register file respectively PE performs operation entire passing partial PEs stationary dataflow increase reuse PE combine input feature kernel partial sum output feature grid PEs fed data monolithic KB global buffer chip dram eyeriss grid PEs occupies nearly chip PEs PE kilobyte scratchpad register file per PE systolic dataflow PEs traversal span relatively distance mid register file per PE problematic load grid PEs stationary dataflow eyeriss tailor convolution accelerator execute fully classifier layer cnns layer exhibit limited reuse price span PEs scratchpad register google tpu google tpu commercial inference processor capable TOPs peak throughput operating tpu core compose grid mac operand MACs systolic dataflow allows input operand convolutional kernel multiple kernel mac fed register MACs computation register pre load operand computation buffering fetch chip memory ddr tpu HBM tpu fifo input output feature MB buffer notable tpu monolithic grid MACs occupies chip input output feature fetch MB cache occupies chip operand traverse width grid MACs navigate within cache traversal propose approach motivate premise  traversal efficient traversal quantify premise scratchpad register file eyeriss PE promotes reuse increase scratchpad register access increase distance adjacent PE increase distance global buffer breakdown baseline eyeriss execute conv layer alexnet nearly eyeriss consume scratchpad register file hypothesis storage per PE shorten distance reduce data movement efficient dataflows construct hierarchy implement deeper hierarchy kilo byte global buffer adjacent PEs global buffer hop away understand relative various structure summarize data difference KB global buffer correspond version eyeriss architecture KB subarray employ propose wax architecture accord CACTI subarray consumes similarly gap byte SRAM scratchpad filter scratchpad eyeriss register file entry register access consumes magnitude scratchpad estimate CACTI register methodology detailed methodology implement synthesize wiring load configuration fully deplete silicon   technology node verilog code model behavior register synthesize synopsys compiler leakage library frequency mhz innovus perform backend register     file obtain innovus accurate layout metric  account spice simulation register file estimate report aware architecture dataflow cnn accelerator micro october columbus usa register file entry SRAM scratchpad eyeriss breakdown consume register file varied register consume register file increase linearly register register consume logic gate relatively register file overall increase due factor increase complex decoder flip flop signal address signal load  data therefore rough guideline aware accelerator extent replace KB buffer access KB buffer access reduction replace byte scratchpad access register access reduction replace entry register file access register access reduction another consideration account eyeriss architecture eyeriss google tpu SRAM buffer systolic array primarily span systolic array employ tile architecture intersperse SRAM compute across chip offset advantage localize data movement aware accelerator therefore impact distribution network model layout propose accelerator consumes baseline eyeriss primarily achieve eliminate register file per PE PROPOSED architecture focus inference operand google tpu apply operand backward training aware accelerator wax goal reduce data movement memory hierarchy deeper distribute achieves data reuse storage per PE overview propose wax architecture conventional cache typically partition subarrays KB network propose neural array subarray wax tile neural array register maintain input activation partial sum respectively register subarray register shift capability register input operand array MACs computation directly subarray wax tile subarray KB   register partial sum register shift register activation MACs subarray neural array wax architecture overview feature reuse systolic dataflow achieve shift register ensures operand distance processing PE mac register compact PE eyeriss tpu hierarchy adjacent subarray KB cheaper access tpu eyeriss traverse writes MB KB buffer respectively feature central principle implement hierarchy impede data movement across data structure register per mac enable dataflow MACs mac operand fetch operand micro october columbus usa  kernel kernel kernel input feature kernel cycle cycle register register subarray input feature kernel partial sum shift data mapping computation WAXFlow activation kernel subarray register file partial sum subarray hierarchy eyeriss tpu vital hierarchy compact KB subarray network layer computation across multiple subarrays aggregation computation data movement localize  chip resource computation accelerator resembles cache mac scatter across subarrays wax tile compose KB subarray MACs register per mac assume SRAM subarray subarray mac subarray cycle pipelined addition overhead MACs register   subarray interface overhead tile quantify efficient dataflow wax WAXFlow wax tile essentially neural network computation array MACs fed data KB subarray discus data mapped wax tile computation structure maximize reuse efficiency within tile discus neural network layer partition across multiple tile propose dataflow WAXFlow convolutional layer explain accompany convolutional layer input feature assume kernel KB subarray operand subarray input feature kernel subarray similarly kernel subarray finally subarray partial sum computation input feature activation register kernel register wise multiplication register partial sum shade diagonal output feature subarray refer cycle operation diagonal pas activation register performs shift wraparound another wise multiplication andw perform yield shift diagonal partial sum yield partial sum entire slice output feature input activation subarray reuse enable relatively inexpensive shift within  register cycle WAXFlow slice perform slice kernel register register unchanged exhibit reuse computation perform slice partial sum compute slice computation proceeds slice gradually activation register remain partial sum slice output feature initial description slice WAXFlow subsequent slice exhibit reuse activation slice perform adjacent subarray multiple shift operation engage subsequent slice perform reading additional subarray drawback partial sum subarray register subarray non trivial overhead alleviate later alternative dataflows subarray access per byte comparable eyeriss partial sum scratchpad access byte data wax WAXFlow tailor aware architecture dataflow cnn accelerator micro october columbus usa input feature kernel kernel kernel kernel cycle cycle shift register register cycle cycle register subarray kernel data mapping computation WAXFlow reduce overhead activation kernel achieve moderate overhead partial sum update sub define computation cycle recap diagonal pas cycle slice pas cycle slice pas neuron layer output feature perform multiplication kernel  register slice pas perform slice pas cycle dimension kernel reuse maximize refer accumulate pas discard input feature load remote tile subarray perform operation load input feature accumulate pas cannot overlap computation subarray perform partial sum writes subarray cycle overcome drawback dataflows subsection register input feature register kernel kernel accumulate pas accumulate entire slice input feature accumulate accumulate pas accumulate pas consume cycle perform MACs slice output neuron similarly MACs perform slice output neuron engage tile parallel accumulate perform parallel tile partial sum accumulate accumulate pas yield output neuron slice partial sum tile adjacent tile link tile accumulation cycle kernel parallel accumulate accumulate pas involve reduction involve tile sequential accumulate computation layer output neuron output tile entire slice output neuron cycle involve parallel accumulate sequential accumulate input load output compute slice output neuron input feature replace input feature input feature fetch output tile previous layer remain exhibit reuse within subarray processing slice output feature cycle increase reuse partial sum WAXFlow  algorithm described previous sub reuses kernel consecutive cycle reuse cycle input activation reuse consecutive cycle discard reuse activation meanwhile partial sum cycle update cycle partial sum access subarray cycle significant overhead access subarray register WAXFlow slice cycle activation filter contribute subarray access partial sum subarray access slice WAXFlow overall usually reduce access data structure balance alternative dataflow reduce psum access increase activation filter access overall subarray access precisely goal dataflow WAXFlow micro october columbus usa  cycle shift cycle kernel kernel kernel cycle register register cycle register kernel subarray data mapping computation WAXFlow modify data mapping subarray split partition partition input feature correspond channel wraparound shift operation activation register perform locally partition WAXFlow slice consumes cycle exploration minimize activation contains ifmap channel filter partition channel wise multiplication cycle multiplier contribute output feature similarly multiplier yield ofmap cycle partial sum diagonal slice register subarray cycle register performs shift shift perform within channel wraparound happens cycle multiplication partial sum entry register cycle register partial sum subarray subarray perform cycle psum aggregation data mapping introduce input adder reduce psum activity cycle channel register undergone shift load register subarray activation filter increase summarize worthwhile mac operation per subarray access increase WAXFlow WAXFlow due access register mac operation per register decrease subarray access consume register access overall significant reduction WAXFlow subarray partial sum cycle therefore data movement fetch  perform accumulate pas output cannot overlap mac computation however WAXFlow partial sum subarray access cycle subarray idle cycle data movement overlap slice computation WAXFlow latency WAXFlow WAXFlow introduce adder intra cycle aggregation perform reduce psum update subarray opportunity psum access subarray reduce data mapping computation structure WAXFlow WAXFlow subarrays split partition ifmap organize WAXFlow partition kernel kernel therefore opportunity aggregate within partition WAXFlow kernel kernel partition kernel therefore kernel byte partition empty data mapping multiplication perform cycle undergo intra partition aggregation inter partition aggregation cycle partial sum cycle fully register subarray approach partial sum contribute subarray subarray writes cycle activation filter access unchanged introduce another layer adder enable partial sum increment subarray adder detail another significant mac operation per subarray access minor increase mac operation per register access WAXFlow kernel partition empty MACs utilized kernel dimension multiple kernel partition aware architecture dataflow cnn accelerator micro october columbus usa hierarchy WAXFlow WAXFlow WAXFlow activation subarray filter partial sum mac subarray access subarray activation register file filter partial sum mac register file access register file access subarray register file wax dataflows execute cycle empty slot kernel dimension dnns modify wax tile configuration tune WAXFlow dnns adjust width tile subarray byte subarray capacity KB tile MACs etc detail model feature mac utilization feature split feature multiple subarray activation multiple feature subarray performance kernel dimension adjust tile  imposes constraint occasionally upto compute utilization conv layer kernel dimension convolutional layer FC layer exhibit utilization accumulate pas computation idle cycle subarray WAXFlow overlap data movement computation emphasize benefit upgrade dataflow WAXFlow WAXFlow dataflows filter load remain stationary subarray fully exploit activation subarray buffer activation fetch remote subarray hence remote subarray access activation WAXFlow WAXFlow WAXFlow baseline eyeriss architecture partial sum scratchpad multiplication operation mac operation partial sum meanwhile WAXFlow WAXFlow adder accumulate multiple multiplication update partial sum utilization WAXFlow reduces partial sum register access WAXFlow reduces relative WAXFlow scratchpad access dominant contributor eyeriss scratchpad attribute partial sum access register file introduce adder tile target partial sum update update fully dataflow execute fully FC layer  slightly data mapping disable shift operation perform register emulates static register file register FC layer allows activation reuse kernel reuse shift operation pointless kernel subarray comprise correspond output neuron whereas activation input correspond cycle activation fetch register cycle kernel fetch register wise multiplication perform register generate psums kernel WAXFlow correspond output neuron psums accumulate register mac operation perform kernel prefetched register activation fetch register reuse across available kernel subarray activation utilized across available kernel psums compute output neuron multiple subarrays parallel generate remain psums output neuron iteration output neuron compute methodology evaluation wax architecture eyeriss fairness attempt iso resource comparison resource MACs cache capacity accurate model wax eyeriss verilog synthesize synopsys compiler innovus route commercial  technology node typical typical layer account layout synthesis   constrain wax tile width SRAM subarray align ensure input pin wax tile align SRAM output wax tile fully digital perfectly initiate fabrication effort chip gain access memory compiler foundry target  technology node lib  layout micro october columbus usa  file SRAM array model SRAM subarrays interconnects CACTI properly account layout CTS mainly extract CACTI define  rout placement blockage account SRAM placement backend wax eyeriss architecture logic modify subarray anticipate relative metric CACTI baseline propose sufficiently accurate eyeriss analysis assume leakage LP layout eyeriss wax wax occupies significantly eyeriss primarily eyeriss register file per PE summarize distribution wax eyeriss compute localize chip synthesis perform innovus chip wax eyeriss account although eyeriss wax network register file eyeriss network global buffer PE array PEs eyeriss wax wax wax tile layout eyeriss wax evaluate performance eyeriss wax respective dataflows developed simulator capture latency resource contention access various component architecture access per component derive circuit model assume dram interface baseline HBM eyeriss assume operand version eyeriss analysis eyeriss resource register bus width buffer accordingly summarize global buffer KB register storage per PE byte overall eyeriss chip model chip storage global buffer scratchpad KB MACs bus PE array global buffer iso resource comparison model wax KB SRAM storage MACs bus width across KB SRAM wax organize KB subarrays wax tile KB subarray array MACs byte register PE PEs arithmetic precision fix GLB SRAM memory KB feature bus width filter partial sum scratchpad PE feature filter partial sum spad PEs KB eyeriss reconfigured parameter wax architecture subarrays subarrays mac subarrays output tile inactive mac wax mac configuration activation register filter register partial sum register wax parameter assume fix adder output truncate route estimate mac register tile account tile significant overhead overall wax chip eyeriss wax tile implement MACs remain KB subarrays output tile output neuron cnn layer output tile partial sum prefetch load individual subarrays iso resource analysis assume architecture mhz wax parameter summarize cycle assume data load chip wax split bus subarray introduce additional mux ing split data adjacent subarray steer central controller adjacent subarray implement subarray subarray transfer load subarrays cycle data subarray adjacent subarray cycle propose architecture considers interconnect individual hence fetch data output tile cycle data central controller cycle subarray  execute popular cnns vgg resnet mobilenet vgg layer neural network convolution layer fully aware architecture dataflow cnn accelerator micro october columbus usa layer resnet layer neural network convolution layer fully network mobilenet depthwise separable convolution  depthwise pointwise layer counting depthwise pointwise layer mobilenet layer peripheral logic component wax denotes channel border denotes kernel explain partial sum correspond kernel channel accumulate partial sum RESULTS data already highlight benefit WAXFlow WAXFlow therefore focus WAXFlow performance analysis analyze performance wax relative eyeriss convolutional layer wax normalize eyeriss behavior across convolution layer breakdown layer vgg iso resource configuration eyeriss wax capable roughly peak throughput therefore performance difference underutilization computation PEs load various structure latter dominant eyeriss data movement computation PEs cannot overlap therefore spends non trivial amount fetch kernel feature scratchpad MACs execute partial sum PEs GLB processing pas WAXFlow dataflow introduce wax spends consecutive cycle MACs register subarray opportunity load activation subarray MACs execute ability WAXFlow subarray idle cycle therefore overlap computation data load across layer vgg wax eyeriss breakdown data movement partial sum accumulation wax cannot completely hidden increase later layer WAXFlow faster eyeriss vgg resnet faster mobilenet primarily filter exhibit reuse GLB fetch bottleneck WAXFlow advantage WAXFlow filter dimension resnet mobilenet wax throughput TOPS eyeriss throughput TOPS execution comparison eyeriss wax fully layer vgg batch fully layer vgg wax eyeriss batch wax faster wax eyeriss bus bandwidth eyeriss statically allocates PE bus bandwidth across  psums fully layer entirely limited bandwidth available transfer eyeriss longer PEs analysis consume wax eyeriss conservatively assume wiring distance register summarizes consume individual operation architecture eyeriss hierarchy global buffer access byte feature register file byte filter SRAM scratchpad byte partial sum register file byte wax hierarchy remote sub array access byte local sub array access byte register file access byte feature filter partial sum access breakdown eyeriss wax breakdown dissipate wax eyeriss scratchpad register file eyeriss dominant consistent breakdown eyeriss local subarray access SA dominant contributor wax without micro october columbus usa  wax execution various convolutional layer vgg execution wax normalize eyeriss execution wax breakdown execution wax comparison wax eyeriss component conv layer resnet vgg mobilenet GLB global buffer rsa remote subarray access SA local subarray access RF register file limited partial sum update enable WAXFlow component overall significant benefit trading subarray register scratchpad offering SRAM capacity lieu scratchpad per PE wax reduces chip dram access wax efficient eyeriss resnet vgg mobilenet reuse mobilenet remote subarray access wax dram access wax relative eyeriss depthwise layer mobilenet yield improvement filter dimension stride contribute overall pointwise layer resnet mobilenet wax  throughput per watt TOPS eyeriss TOPS comparison eyeriss wax fully layer vgg batch component broken across activation filter partial sum representative workload resnet breakdown across operand eyeriss balance partial sum filter scratchpad thanks dataflows introduce roughly amount dissipate operand wax highlight various reuse unbalanced WAXFlow balance WAXFlow partial sum repeatedly local subarray dominate local subarray access meanwhile activation fetch remote tile repeatedly subarray remote fetch dominates activation partial sum access cheaper wax eyeriss register file partial sum accumulation layer adder accumulate cycle update register wax reduces dram chip dram significant contributor eyeriss chip contributor tpu chip chip reuse layer wise breakdown component execute resnet wax deeper layer activation reduces kernel increase increase remote subarray access kernel fetch remote subarray limited reuse activation fetch kernel aware architecture dataflow cnn accelerator micro october columbus usa wax convolutional layer resnet increase wax throughput delay breakdown activation partial sum wax eyeriss hierarchy convolutional layer resnet breakdown component wax convolutional layer resnet comparison fully network bach batch WAXFlow consumes almost although remote subarray access expensive GLB access eyeriss activation reuse wax batch overhead masked benefit wax nearly efficient impact hence MACs wax throughput consumption throughput image per combination assume bus width exploration reserve tile remote subarray access bus width throughput throughput tile reduce network bottleneck replicate  across multiple subarrays sequential improve scalability interconnect grid parallelism efficient communication throughput peak tile GOPS google tpu related eyeriss baseline introduce difference eyeriss implement primitive per PE exploit activation kernel reuse within wax significantly MACs register per tile reduce wiring overhead increase reuse introduce data mapping shift register per tile computation reuse wax neural cache architecture neural computation closer data cache subarrays introduce cache operator perform wise operation cache wise operator cycle perform mac approach focus per operation reduce wiring overhead maximize reuse neural cache involves SRAM subarray access computation accelerator leverage analog dot operation within resistive crossbar achieve data movement promising analog likely technology roadmap dadiannao shidiannao architecture focus data processing dadiannao tile architecture neural functional eDRAM within tile however eDRAM kilobyte extensive wiring eDRAM mac shidiannao later data shuffle reuse wax computation subarrays achieves reuse shift register recent commercial effort   adopt tile architecture compute SRAM per tile exploit locality data movement micro october columbus usa  tpu eyeriss tesla  ibm core  architecture implement monolithic systolic array fed buffer although storage per PE eyeriss  training therefore maintains buffer activation pas implement finer grain tile tpu eyeriss context gpus numa modular chip employ distribute gpus local memory communicate interconnects target gpu performance standpoint moore era unlike multi module gpus wax deeper hierarchy distributes computational across memory finer granularity reduce recent dnns exhibit sparsity activation quantize eyeriss proposes  exploit sparsity activation improve throughput efficiency eyeriss flexible noc accommodate varied bandwidth requirement orthogonal approach likely compatible wax sparsity technique tile index generation logic correctly steer partial sum integration technique wax future minimum specific datapaths wax gate estimate width increase throughput width configurable MACs datapaths shift register CONCLUSIONS cnn accelerator boundary data execution data movement wax hierarchy relatively resource layer hierarchy entry register file shift operation array register adjacent subarray efficiently operand mac operation various dataflows define WAXFlow balance reuse various data structure reduces expensive access local remote subarrays wax ability perform compute simultaneously load subarray compute utilization improves performance relative eyeriss wax yield improvement relative eyeriss remove collection bulky register file per PE eyeriss overall chip reduce reduce distribution architecture scalable tile increase compute storage increase proportion wax increase throughput tile wax tile therefore efficient primitive server accelerator