package integration multi chip module MCMs promising approach building monolithic mcm combine chiplets substantially reduce fabrication MCMs typically handful coarsegrained chiplets due performance overhead associate inter chiplet communication investigates quantifies benefit MCMs grain chiplets inference application compute chip storage requirement evaluate approach architected implement fabricate simba chiplet prototype mcm  inference chiplet achieves TOPS peak performance chiplet mcm package achieves TOPS TOPS mcm configurable flexible mapping dnn layer distribute compute storage mitigate inter chiplet communication overhead introduce tile optimization improve data locality optimization achieve speedup baseline layer mapping evaluation simba image resnet batch deliver inference latency keywords multi chip module neural network accelerator architecture introduction DL become critical address complex neural network dnns demonstrate effectiveness across application image recognition detection translation audio synthesis autonomous dnns billion operation megabyte activation trend towards deeper network ensue compute storage requirement motivate compute capability DL hardware currently address combination monolithic chip homogeneous multi chip previously propose multi chip DL accelerator focus improve compute throughput chip storage address scalability challenge associate building multiple discrete component micro october columbus usa shao input activation IA output activation OA convolutional layer recently compute throughput era transistor motivate advance multi  mcm integration CPUs gpus mcm packaging approach reduce employ chiplets fabrication yield loss fabrication super linearly packaging technology organic substrate silicon  assemble largescale mcm addition recent advance package signal bandwidth signal chiplet chiplet MCMs improve performance efficiently integration monolithic chip MCMs compute apply MCMs performance dnn inference algorithm previously examine specific challenge stem non uniformity chip package bandwidth latency multi chip exhibit non uniformity focus specific characteristic mcm progression monolithic chip inference accelerator semiconductor slows simba scalable inference accelerator employ multi chip module integration simba chiplets standalone inference accelerator multiple simba chiplets package deliver data compute throughput specifically examine implication non uniform latency bandwidth chip package communication significant latency variability across chiplets latency variability latency execution individual inference layer overall performance layer limited slowest chiplet limit scalability address challenge propose latency aware non uniform tile optimization target improve locality minimize inter chiplet communication non uniform partition balance compute latency communication latency communication aware data placement minimize  traffic layer pipelining improve resource utilization explore challenge evaluate benefit  inference accelerator architecture implement fabricate prototype simba consist chiplets mesh network mcm chiplets OA IA listing dnn loop nest fabricate FinFET technology accumulate mac SRAM memory chip peak performance tera ops per TOPS activation accumulation chiplet mcm achieves TOPS efficiency TOPS operating voltage simba flexible mapping resource allocation efficient dnn inference execution across workload thoroughly characterize simba motivate importance task data placement simba mcm performance difference individual resnet layer motivate observation propose non uniform data placement layer pipelining improve utilization presence batch communication latency background motivation application device data demand efficient inference latency throughput requirement dnn inference application typically highly programmable inefficient cpu programmable gpus ISA extension accelerate tensor operation fix function dnn inference accelerator recent fix function accelerator magnitude efficiency performance CPUs efficiency gpus inference application moderately chip demand performance monolithic chip multi chip partition application across multiple chip easy enormous difference bandwidth latency chip communication inter chip communication packaging approach multi chip module inter chip interconnect closer chip interconnect offering straightforward dnn dnns construct series layer convolutional layer pool layer activation layer fully layer convolutional layer algorithmically formulate dimensional nest loop input activation IA tensor tensor output activation OA tensor listing convolution computation embed dimensional loop nest formulation simba inference multi chip module architecture micro october columbus usa simba architecture package processing PE applies fully layer widely multilayer perceptrons MLPs recurrent neural network rnns activation layer applies non linear function relu  pool layer sample input activation convolutional layer activation pool layer typically merge convolutional layer execution reduce data movement multi chip module packaging package mcm integration promising alternative assemble building chiplets consist multiple chiplets via package link silicon interposer organic substrate employ efficient intra package signal circuit monolithic MCMs reduce logic verification physical easy chip chip fabrication manufacturing yield chip expensive chip addition merely adjust chiplets package without chip  MCMs recently apply purpose cpu alternative building multi core CPUs reticle limited active research multi cpu multi gpu however package communication density chip consequently mcm architect software developer non uniform bandwidth latency achieve efficient application performance non uniformity mcm mcm heterogeneous interconnect architecture available intra chiplet bandwidth significantly available inter chiplet bandwidth addition data remote chiplets incurs additional latency latency chip delay data chiplet synchronizer delay domain serialization deserialization latency communication link package delay inter chiplet link communication latency mcm heavily depends spatial locality package mapping dnn layer tile architecture research dnn tile typically assumes architecture uniform latency bandwidth across processing focus data reuse reduce global bandwidth demand assumption acceptable  communication latency variability computation tolerant communication latency however dnn inference performance execution decrease latency related become important furthermore heterogeneous interconnect architecture MCMs assumption uniform latency bandwidth dnn tile degrade performance efficiency simba quantitatively highlight challenge mapping dnn layer non uniform mcm dnn accelerator proposes communication aware tile strategy address challenge simba architecture understand challenge opportunity MCMs building implement fabricate characterize simba chiplet overview simba architecture default uniform tile strategy simba silicon prototype detailed characterization simba micro october columbus usa shao simba communication capability packet source unicast destination multicast destination PE local PEs global PE controller remote PEs global PE controller global PE local PEs controller local PEs remote PEs controller remote PEs controller local PEs global PE remote PEs global PE controller simba architecture tile architecture frequently propose  accelerator target accelerator scalable data inference data accelerator deliver around tera operation per TOPS generation tensor processing tpu delivers TOPS inference application approach achieve goal increase tile monolithic chip however building network tile tile tile communication latency examine multi core cpu accelerator research simba adopts hierarchical interconnect efficiently processing PEs hierarchical interconnect consists network chip noc connects PEs chiplet network package nop connects chiplets package illustrates hierarchy simba architecture package chiplet PE simba package consist array simba chiplets via mesh interconnect simba chiplet contains array PEs global PE nop router controller chiplet interconnect enable communication PEs global PEs controller latency insensitive across interconnection network noc nop router simba PE microarchitecture simba PE distribute buffer input buffer parallel vector mac accumulation buffer processing simba PE version NVDLA DL accelerator simba PE array parallel vector mac optimize efficiency flexibility simba PE stationary dataflow remain vector mac register reuse across iteration input cycle vector mac performs dot along input channel dimension exploit efficient spatial reduction flexible tile option simba PE PE reduction configurable producer consumer PE PE reduction chain sends partial sum local processing performs relu truncation pool bias addition output activation target global PE computation layer simba global PE global PE serf storage input output activation data PEs flexible partition computation global PE unicast data PE multicast multiple PEs across chiplet boundary global PE multicast manager oversees producer consumer relationship global PE serf platform memory computation dnns feature computation data reuse wise resnet depth wise convolution mobilenet global PE perform computation locally reduce communication overhead operation simba silicon prototype simba controller simba chiplet contains RISC processor core responsible configure manage chiplet PEs global PE via memory mapped register axi communication protocol configure RISC trigger execution active PEs global PEs notification via interrupt synchronization chiplet processor across package implement via memory mapped interrupt simba interconnect efficiently execute neural network diverse layer dimension simba flexible communication across noc nop simba communication capability across component noc nop mesh topology hybrid wormhole specifically unicast packet wormhole packet multicast packet avoid wormhole deadlock simba PE unicast local remote PE PE partial sum reduction local remote global PE transmit output activation local remote chiplet controller signal execution completion PE multicast packet computation communication addition unicast communication global PE multicast packet local remote PEs flexible data tile implement fabricate silicon prototype simba microarchitecture simba chiplet simba package simba silicon prototype simba inference multi chip module architecture micro october columbus usa simba microarchitecture parameter package chiplets core voltage PE frequency ghz chiplet chiplet interconnect reference signal nop interconnect bandwidth GB chiplet nop interconnect latency hop nop interconnect chiplet PEs technology FinFET voltage PE frequency ghz global PE buffer KiB router per global PE noc interconnect bandwidth GB PE noc interconnect latency hop microcontroller RISC PE buffer KiB input buffer KiB accumulation buffer KiB vector mac width vector MACs dataflow stationary input precision partial sum precision parameter chose parameter simba chiplet efficient diannao eyeriss simba package comparable data tpu synthesis breakdown component simba chiplet architecture simba chiplets implement TSMC FinFET technology simba package contains array chiplets organic package substrate reference signal GRS technology intra package communication chiplet  chiplet GRS transceiver macro macro configure receiver transmitter transceiver macro data lane lane configurable gbps pin gbps pin consume peak chiplet bandwidth GB chose GRS communication mechanism delivers bandwidth per per mcm interconnects prototype chiplets implement globally asynchronous locally synchronous  methodology breakdown simba partition component PE vector MACs buffer input buffer accumulation buffer noc router global PE distribute buffer noc router RISC processor nop nop router package parallel parallel chiplet parallel parallel PE vector mac parallel parallel OA IA listing simba baseline dataflow independent rate individual PEs global PEs RISC processor nop router chiplet configuration simba prototype correctly lab minimum voltage mhz PE frequency achieve TOPS core efficiency peak utilization convolution micro benchmark chiplet operates ghz PE frequency peak throughput TOPS chiplet simba functional slightly narrower voltage achieve mhz chiplet achieves ghz PE frequency TOPS simba baseline tile dnn layer onto hierarchical tile architecture dnn tile strategy uniformly partition spatially leverage model parallelism listing default tile loop nest dimension dnn layer tile temporally spatially hierarchy package chiplet PE vector mac loop bound ordering listing configurable simba user flexibly computation simba default dataflow uniformly partition along input channel output channel dimension parallel loop addition simba uniformly partition along height width dimension output activation across chiplets PEs flexible tile highlight limitation approach mapping network onto non uniform network access architecture mcm integration developed caffe dnn inference application simba primarily determines efficient tile strategy dataflow exploit data reuse memory hierarchy facilitate evaluation mapping alternative developed analytical model simba quantifies mapping methodology prior micro october columbus usa shao compilation mapper data regard available resource PEs global PEs buffer parameter layer caffe specification mapper determines PE portion loop nest buffer activation mapping logical mapper  decides physical resource simba topology loop nest data structure random algorithm sample mapping performance model mapping placement finally generates configuration binary chiplet implement execution mapper  simba characterization detail performance characterization simba focus achieve scalability uniform tile baseline evaluation prototype methodology experimental setup performance simba prototype silicon prototype attach host pci xilinx fpga performance simba prototype software RISC query cycle counter built RISC microcontrollers runtime software designates chiplet RISC microcontroller RISC execution chiplets assign performance measurement load PE buffer input load global PE buffer measurement conclude partition signal completion RISC unless otherwise chiplets core voltage PE frequency ghz GRS bandwidth gbps resistor digital acquisition module execution chiplets independent frequency PEs global PEs RISC nop router frequency compute bandwidth ratio noc nop rout dimension rout inter chiplet communication although simba chiplets functional evaluation chiplets easy partition computation input channel output channel typically focus application measurement resnet representative network evaluate layer simba batch latency inference highly critical deployment scenario data inferencing compile layer independently multiple layer physical partition simba execute pipelined manner network pre quantize TensorRT without accuracy loss focus resnet measurement  demonstrate simba weak performance alexnet bench measurement setup simba prototype layer exhibit behavior diversity layer resnet sufficient breadth behavior across convolutional network overview summarizes performance measurement across resnet layer unique mapping layer chiplets active mapping latency normalize hypothetical  latency realize PEs utilization communication synchronization overhead simba mapping option drastically performance profile highlight importance strategy efficiently mapping dnns hardware demonstrates highly variable behavior layer efficient configuration layer branchb achieve almost magnitude efficiency resa brancha data reuse highly influence efficiency layer reuse factor convolution branchb tend perform computation efficiently layer data movement finally although increase chiplets improves performance increase chiplet  communication synchronization efficiency nearly magnitude layer emphasizes data movement overall efficiency understand offs remainder characterizes sensitivity simba mapping alternative layer parameter bandwidth latency weak comparison gpus mapping sensitivity performance comparison mapping resnet brancha layer onto multiple PEs chiplet span multiple chiplets mapped chiplet execution latency decrease linearly PEs improve compute throughput PEs performance flattens beyond PEs due memory bandwidth contention global PE SRAM however simba inference multi chip module architecture micro october columbus usa performance resnet fabricate simba prototype valid workload mapping onto cluster performance achieve mapping workload active chiplets mapping mapping across chiplets execution beyond PEs additional latency communicate across multiple chiplets inter chiplet communication latency synchronization latency ultimately execution relative employ chiplet mapping strategy MCMs characteristic noc nop deliver efficient utilization hardware layer sensitivity performance scalability layer resnet across chiplets performance branchb improves increase chiplet performance gain cease beyond chiplets layer network layer cannot fully utilize compute throughput simba performance degrades chiplets inter PE communication overwhelm limited parallelism contrast performance resa layer chiplets plateau layer compute parallelism branchb fully overcome overhead inter chiplet communication branchb layer demonstrates performance improvement chiplets however performance slows significantly chiplets due communication overhead layer resnet behavior resa branchb remain behavior micro october columbus usa shao performance comparison chip  communication synchronization simba latency normalize PE execution latency simba scalability across layer resnet latency normalize latency  tile chiplet branchb measurement demonstrate amount compute parallelism mcm leverage varies layer layer communication hinder ability exploit parallelism chiplet nop bandwidth sensitivity examine bandwidth sensitivity layer adjust bandwidth nop relative intra chiplet compute performance adjustment reduce frequency PE global PE RISC partition nominal maintain constant nop frequency PE cycle computation frequency reduction effectively corresponds increase nop bandwidth execution affected nop bandwidth representative resnet layer mapped chiplets branchb increase bandwidth chiplets decrease execution layer bound nop bandwidth inter chiplet communication latency however simba scalability chiplet chiplet communication bandwidth simba scalability chiplet chiplet communication latency resa chiplets tile selection active chiplets axis active chiplets highlight resa increase bandwidth decrease execution layer bottleneck communication chiplets mcm intrinsically  architecture intra chiplet inter chiplet PEs mapping policy latency bandwidth parameter deliver performance efficiency nop latency sensitivity addition bandwidth nop latency noc due inter chiplet signal overhead isolate nop latency mapping layer chiplets adjust location chiplets package modulate latency increase inter chiplet latency hop hop resa execution normalize configuration adjacent chiplets chiplet selection active chiplets apart overall execution simba inference multi chip module architecture micro october columbus usa characterization weak simba  increase execution adjacent chiplets communication latency typically pronounce significant role achieve performance efficiency mcm simba weak sensitivity weak trend simba  dnn  significantly network resnet chiplet mapping optimal configuration batch  demonstrate weak trend simba instead distribute amount computation multiple chiplets resnet fix amount per chiplet increase amount computation increase batch increase active chiplets simba achieves throughput improvement perfect weak incur latency increase due synchronization across multiple chiplets comparison gpus simba nvidia gpus resnet batch gpu publish due simba limited package storage capacity input activation simba batch unlike gpus simba latency inference batch motivates distribute persistent storage reduce data movement simba package mcm interface substantially silicon due difference math precision chip storage dram interface computation architecture throughput simba resnet simba delivers throughput batch respectively illustrates correspond efficiency improvement simba resnet throughput efficiency simba resnet batch batch instead exploit batch parallelism gpus simba batch sequentially throughput simba batch simba non uniform tiling novel dnn workload tile technique target non uniform latency bandwidth mcm stress importance communication latency aware tile mapping dnn workload hierarchical non uniform partition efficient parallel load balance component failure properly balance load latency resource slowest increase latency execution broken component communication latency compute latency stateof dnn tile strategy typically assign amount available resource however approach PEs spatially distribute communication latency address limitation propose non uniform partition strategy considers communication latency instead uniformly assign amount PE non uniformly partition across PEs PEs closer data producer perform maximize physical data locality PEs away decrease latency illustrates non uniform partition chiplet assume input activation IA physically global PEs chiplet chiplet partition across chiplets execution global PE chiplet multicast slice IA micro october columbus usa shao illustration communication aware nonuniform partition tensor tensor input activation IA tensor output activation OA IA chiplet chiplet PEs chiplet chiplet communication latency chiplet global PE PEs chiplet chiplet chiplet prevent longer communication chiplet increase latency execution adjust amount computation chiplet assign manner inversely proportional communication distance source chiplet chiplet chunk  chiplet chiplet chunk iдht schedule completion across chiplets thereby improve overall performance simplicity nonuniform partition respect input activation technique mitigate communication latency output activation destination chiplets non uniform partition along dimension variation communication latency pronounce simba spatially distribute PEs dynamically adjust amount PE performs performance counter within PE accurate latency utilization information initial execution layer adjust distribution subsequent execution layer latency variation across PEs illustrates performance improvement non uniform partition layer performance uniform tile baseline execution chiplets identify layer latency layer non uniform partition resnet speedup normalize perform tile non uniform partition shift computation PEs PEs closer data layer dimension achieve performance improvement uniform tile layer achievable performance improvement highly sensitive compute communication ratio mapping compute communication significantly dominate overall execution latency resa incrementally modulate amount PE performs performance improvement however compute communication latency comparable typically desire achieve mapping performance improvement pronounce branchc communication aware data placement communication latency parallel overall performance multi core multi gpu characterization due limited accelerator unified global buffer data PEs however mcm chip buffer spatially distribute chiplets communication latency becomes highly sensitive physical location data illustrates data placement affect communication distance latency src chiplet broadcast data chiplets arrival data greatly distance chiplets src amount computation chiplet performs variation communication distance significantly limit achievable speedup distribute tile simba motivate data placement optimization optimal data placement NP practical greedy algorithm iteratively input output activation data simba algorithm perform placement input activation data input activation greedy algorithm execute tile output activation simba inference multi chip module architecture micro october columbus usa data placement simba assessment relative latency chiplets data src default input activation IA output activation OA placement data sequentially global PE chiplet improve IA placement package data multicast chiplets OA placement distribution along periphery package minimize OA communication latency previous stage mapping already data tile stage focus data placement tile naive data placement sequential allocation input activation chiplets output activation chiplets assignment IAS chiplets algorithm iterative data placement algorithm placement input activation data input activation precompute communication source destination input activation chiplet placement calculate communication pre compute chiplet minimizes communication chiplet ram source chiplet data placement resnet layer speedup normalize perform tile minimize aggregate multi cast hop chiplets finally placement output activation chiplets OA accumulation performance improvement resnet layer optimize data placement although layer chiplets communication layer branchc communicate frequently within chiplets chiplets minimize communication contrast layer resa broadcast chiplet chiplets instead source chiplet sequentially upper package performance improvement data placement optimization improve performance achieve baseline layer pipelining challenge mapping dnn layer achieve utilization layer computation limited amount parallelism address challenge recent dnn accelerator pipelined execution improve overall utilization  wise pipelining PE array assign pipelined layer overhead implementation utilization layer cannot easily mapped across tangram flexible partition across PE array layer non uniformity communication latency bandwidth illustrates residual resnet pipelined across simba package simba hierarchical interconnect flexible communication assign cluster chiplets layer resa branchb chiplets resa brancha chiplets achieve throughput improvement pipelining residual within residual instead execute layer sequentially entire package partition package cluster assign layer cluster execute layer pipelined fashion pipelined execution micro october columbus usa shao pipelining residual resnet simba simba improves overall throughput sequential execution baseline pipelining overall throughput limited pipelining stage achieves significant speedup due relatively balance input activation earlier layer input activation later layer dominate throughput improvement related dnn inference application typically highly programmable inefficient CPUs programmable gpus ISA extension accelerate tensor operation fix function dnn inference accelerator mcm fix function dnn inference accelerator prototype capable highly complex dnn model throughput latency aim simba explore challenge opportunity without incur inference latency previous developed hardware accelerator architecture focus efficient execution DL inference accelerator target network challenge associate network propose accelerator structural throughput improvement pipelined execution residual resnet performance chip diannao originally propose context dnn layer dadiannao aim network propose multi chip network eDRAM activation contrast simba inference specifically employ package integration tpu data inference accelerator lack mapping flexibility target simba tpu restricts communication wise wise multicast systolic computation core communication flexibility simba opportunity perform data locality mapping optimization MAERI dnn accelerator PEs switch explore inference MCMs chiplets simba MCMs explore cpu gpu circumvent fabrication associate monolithic chip contribution simba implementation evaluation mcm dnn accelerator previous explore efficient data movement multi chip purpose workload focus efficient data movement mcm dnn inference tangram efficient mapping tile accelerator buffer loop application pipelining future apply tangram mapping technique simba architecture CONCLUSIONS simba scalable mcm inference accelerator architecture simba heterogeneous  architecture hierarchical interconnect developed silicon prototype consist chiplets achieves TOPS efficiency prototype characterize overhead non uniform network mcm architecture load imbalance communication latency contribute noticeable latency non uniform improve performance technique nonuniform partition communication aware data placement layer pipelining apply optimization performance increase naive mapping