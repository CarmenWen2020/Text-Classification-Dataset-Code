data width neural network quantize width per layer amount shorter data width target adjust data width finer granularity propose shapeshifter activation encode width specific typical per width statically dynamically hardware activation application shapeshifter applicable shapeshifter reduces chip storage communication shapeshifter memory compression reduces chip traffic model respectively sustain performance chip memory interface boost efficiency application shapeshifter implement surgical extension exploit variable precision introduction training model float arithmetic model data inference task image classification fix arithmetic proven sufficient  fix int arithmetic generally applicable conversion rudimentary quantization quantization shortest data width appeal workload expenditure due data transfer data reduces volume data data input numerous accumulate mac operation exhibit data parallelism shorter data functional deploy chip performance efficiency immediate benefit commodity hardware accelerator alike quantization naturally attract attention quantization int usually image classification model selectively application domain quantization shorter data width demonstrate regardless aforementioned quantization target fix data width per network per layer data width trim per layer distribution model activation zero magnitude assume magnitude input target specific data width adjust per layer disproportionately exceedingly magnitude incur micro october columbus usa  significant overhead vast majority data transfer computation however data width accommodate activation specific input activation transfer concurrently accordingly propose mechanism exploit distribution neural net deliver memory footprint traffic computation throughput improvement generally applicable network regardless specific quantization apply propose per data width adaptation calculate upon transfer memory chip memory simd style execution propose data width dynamically activation statically int network maximum magnitude goal per plus metadata memory another maximum magnitude happens instead per develop compute processing maximum magnitude processing aforementioned instead respectively float  narrow float format advantage distribution neural net adapt data width float representation expand moreover float proposal target training generally fix alternative inference contribution adjust data width per dynamically activation statically yield shorter effective data width per layer width selection without affect numerical fidelity accuracy contribution attention popular quantization successfully squeeze broader within target data width expand shorter data target data width expansion unnecessary obscures opportunity per data width reduction deploy aware quantization preserve benefit per data adaptation novel hardware technique lossless memory compression plug compatible hardware shapeshifter hardware compress  activation offchip reduce considerably boost effective offchip bandwidth compression pre processing software activation perform dynamically output previous layer source compress data decompress fetch offchip technique compatible hardware demonstrate benefit accelerator novel hardware technique adapts width activation runtime execution proportionally reduce per concurrently compatible exploit data width variability oppose spatially SStripes surgical extension stripe accelerator advantage proposal modest hardware performance communication storage efficiency improvement anything moreover overhead exceedingly shapeshifter approach quantization affect numerical accuracy simply adjusts data container width accommodate programmer transparent granularity quantization transform data width virtually quantization width accommodate within data typically layer network narrower width opportunity shapeshifter naturally quantization width relative opportunity unless becomes quantize network data width shapeshifter delivers proportional benefit network unique quantization  quantization data width shorter width remain data width data width fix parameter network retrain explicitly shapeshifter directly benefit quantization additional benefit reduce data width however contrary outlier aware quantization shapeshifter model quantize imposes runtime constraint data width relative  spatial distribution highlight finding shapeshifter chip memory compression reduces traffic respectively aware quantize model tensorflow quantization benefit traffic reduce model shapeshifter compression robust increase traffic shapeshifter compression boost effective memory bandwidth yield saving boost performance performance BitFusion improves ddr memory model shapeshifter stripe SStripes improves compute performance stripe model model quantization  tensorflow respectively faster fusion model shapeshifter compression delivers virtually memory traffic reduction outlier aware quantize model despite specialized boost compression rate average shapeshifter enable grain data width adaptation micro october columbus usa width activation googlenet conv width activation googlenet width activation resnet resa width activation resnet resa per per layer activation width model width static googlenet conv width static googlenet width static resnet resa width static resnet resa per per layer width model data width  demonstrates variety model width finer layer granularity considerably shorter layer maximum width layer width varies input quantization unnecessarily expand afford target width activation data width henceforth refer width measurement convolutional layer googlenet  resnet prune network originally quantize trend model layer measurement randomly image imagenet dataset graph cumulative distribution width per activation various width favorable activation vertical report width activation profile derive static image detect dynamically dynamic randomly image illustrate width per input report measurement conv layer googlenet profile width specific image within layer improvement width cumulative distribution width per activation reduction width reduce effective width however difference modest width layer exacerbates importance magnitude activation layer usually exhibit behavior input image discover visual feature whereas inner layer tend correlation feature layer pronounce improvement effective width behavior persists sparse network measurement demonstrate dynamic width variability phenomenon orthogonal sparsity potential target sparsity moreover dynamic width variability limited activation report measurement layer googlenet profile width dynamic precision determines pronounce profile behaviour persists sparse network quantization report distribution width  quantize quantization tensorflow TF aware RA tensorflow unnecessarily expand micro october columbus usa  width activation reduction RA icp RA icp RA reduction TF icp TF icp TF  activation width reduction RA icp RA icp RA reduction TF icp TF icp TF  width activation conv RA  RA softmax RA conv TF  TF softmax TF segnet activation width conv RA  RA softmax RA conv TF  TF softmax TF segnet per data width model tensorflow TF aware RA quantization average width activation average data width activation per layer per layer whereas aware quantization longer layer representative behavior layer average aware quantization considerably reduction icp activation reduction icp demand layer activation reduction tensor quantization model exhibit data width reduction unnecessary expansion tensorflow quantization icp tensorflow contrast aware quantization behavior segnet model demonstrates width requirement considerably layer individual neural network model average effective width per layer profile per detection activation variety model activation corresponds per layer per width respectively measurement randomly image imagenet chip memory chip memory decode encode memory controller chip memory decode encode data conversion zero vector prec VN chip data container msb lsb detector detect per width activation per width reduce chip memory bandwidth storage image classification model image CamVid segnet segmentation input pascal voc respectively yolo detection fcn segmentation image  input IRCNN denoising VDSR super resolution input WMT seqseq translation input coco  caption input  directional lstm caption shapeshifter enable grain data width adaptation micro october columbus usa measurement across network report reduction axis activation per width illustrate width per layer grossly overestimate finally report effective data width achieve per layer due limitation report model along channel dimension effective width fractional average within layer accordingly runtime  chip storage communication bulk dnns expend chip memory access chip storage limited chip bandwidth easily bottleneck fortunately variable width activation reduce amount storage bandwidth chip limit attention chip compression scheme chip memory data container encode activation balance compression rate metadata overhead memory container width statically dynamically hardware output previous layer input source layer activation prefix lo–¥ maximum data width addition avoid zero altogether zero vector per non zero chip scheme metadata per sixteen uncompressed sixteen occupy metadata overhead benefit obtain compress typical decode approach detect per width activation hardware adjusts width runtime activation chip output layer trim unnecessary prefix detect significant within activation hardware calculates signal per correspond across activation implementation generate signal detector identifies significant report detector extend handle negative convert magnitude representation rightmost significant useful network activation function attenuate negative memory layout access strategy memory compression bandwidth amplification purpose program random access cache align address quickly compress handle compress uncompressed advantage unique requirement neural network shapeshifter compression considerably looser requirement easy understand chip memory buffer activation access chip per layer practical network layer accelerator chip input array activation writes output array output activation chip access array sequential shapeshifter access container activation array sequential access within container within container data compress desire without concern occupies incoming decode sequentially chip memory sufficiently access activation multiple reduce chip traffic appropriate dataflow naturally dataflow access data chip minimize address access handle container easily identify statically access within container remain sequential generally dataflow identify access handle statically dynamically activation pre decompression compression clarity decompression sequential assume maximum  per accelerator memory request issue request memory controller access maintains register plus input array normally activation plus sequential address generator array rate request interleave metadata layer uncompressed model activation array decompressor metadata decompressor output zero memory expand data width chip upon decoder header sequential decompressor multiple organize hierarchy detailed decompressor L1D identifies per decompression   per chip memory specifically upon inspect header L1D immediately determines buffer payload decompression  communicate addr addr address offset copying relevant buffer  private buffer L1D micro october columbus usa  encode memory layout memory decoder decoder decoder addr addr addr  decode architecture shapeshifter encode decode sufficient encode metadata introduce vector identifies zero zero memory remain non zero memory decode decoder encode fetch chip memory per memory interface buffer assumes external memory interface clarity chip controller inspects header internal buffer decoder decoder along index decoder expands assign output register output register contains shapeshifter completely transparent chip execution average per layer width shapeshifter network effective activation width per layer reduction alexnet googlenet vgg vgg resnet yolo mobilenet network effective width per layer reduction alexnet googlenet vgg vgg resnet yolo mobilenet address update logic  update fetch address decoder compress memory inspects header sends correspond another  upon payload header L1D  expands serially narrow L1D  buffer avoids crossbar shuffle network finally output layer output activation assemble encode accordingly width detect hardware memory controller writes data container become available shapeshifter enable grain data width adaptation micro october columbus usa  execution discus shapeshifter reduce execution exploit data width variability representative explains shapeshifter benefit stripe illustrate goal shapeshifter stripe SStripes adjust cycle activation accommodate magnitude within stripe cycle profile derive precision dictate cycle review stripe exploit per layer precision improve performance stripe serial  sip sip multiplies accumulates activation sip activation execution varies accord data width chosen data width layer precision grey sip cycle per activation introduce precision detection prior activation sip becomes adjust cycle per basis specific cycle despite precision chosen layer overall stripe architecture identify shapeshifter extension stripe comprises tile tile filter per filter parameter local tile memory WM tile contains SIPs organize SIPs along output channel reuse local activation memory scratchpad  activation via per activation SIPs along across output channel filter activation reuse spatially reuse tile computation partial sum accumulate within SIPs eventually per tile partial sum memory PM activation memory broadcast activation tile dispatcher per activation memory transpose communicate serially tile centralize distribute along tile identify shapeshifter extension per dispatcher width detection inspects activation communicates data width via EOG signal processing proceeds stripe significant simplicity terminate accord EOG signal optional extension stripe composer contains adder allows SIPs adjacent along prior PM SIPs instead SIPs originally stripe SIPs activation accumulator SIPs configure SIPs processing layer profile determines cycle cycle timing  processing sip sip sip sip PM dispatcher WM composer width detector  tile SStripes tile SStripes extend stripe shapeshifter adjust along SIPs upper respectively partial sum accumulate SIPs correctly calculate output activation processing sip array prior PM stripe exploit precision domain fusion instead exploit precision spatially fusion spatial cannot adapt precision granularity composer allows SStripes exploit precision spatially accordingly adapt per precision adapt spatial adjust precision granularity future simplicity advantage proposal overhead per width adaptation negligible tile SStripes affect accuracy numerical stripe micro october columbus usa  neural network prune model dataset application int alexnet imagenet classification alexnet imagenet classification alexnet imagenet classification googlenet imagenet classification googlenet imagenet classification vgg imagenet classification vgg imagenet classification resnet imagenet classification resnet imagenet classification yolo imagenet detection mobilenet imagenet classification int tensorflow quantize alexnet imagenet classification googlenet imagenet classification resnet imagenet classification mobilenet imagenet classification int aware quantize alexnet imagenet classification googlenet imagenet classification BiLSTM  caption segnet CamVid segmentation int int int int outlier aware quantize resnet imagenet classification mobilenet imagenet classification evaluation accelerator model methodology consistency custom cycle accurate simulator model execution estimate synthesize synopsys compiler TSMC library unfortunately due licensing constraint presently technology access laid cadence innovus circuit activity capture  fed innovus estimation 1GHz report layout measurement however fusion report technology SRAM activation buffer activation memory model CACTI library prior layout typical library chosen layout serial affected chip memory layer chip memory per layer chip access dominate consumption appropriately chip memory primary consideration MB activation MB model model configuration allows input activation chip layer measurement report layer network detail neural network outlier aware model demonstrate shapeshifter compression naturally model deliver memory traffic reduction without specialize quantization shapeshifter memory compression compression rate report relative chip traffic scheme baseline compression compress per layer profile derive width profile shapeshifter compression shapeshifter  zero compression zero compression eyeriss scnn compression profile shapeshifter adjacent along depth dimension model shapeshifter compression reduces traffic average tensorflow quantize network benefit technique quantize dynamic precision reduction cannot apply effectively however aware quantize network reduction memory traffic shapeshifter compression achieves traffic reduction zero compression report relative chip traffic non profile network profile feasible data available shapeshifter memory compression achieves traffic reduction non profile network non profile network traffic reduce tensorflow quantize network traffic dynamic precision compress  quantize network without profile precision sparse aware quantize network  chip traffic reduction zero compression  network demonstrate reduce chip memory traffic shapeshifter compression benefit dadiannao fusion scnn improve overall performance efficiency shapeshifter compression saving offchip memory regardless rating demonstrate considerably improve performance report speedup ddr chip memory ddr chip memory halfway ddr chip memory dadiannao performance dadiannao variant dadiannao chip compression scheme shapeshifter compression speedup execution aware quantize model respectively compression ddr average speedup modest tensorflow quantize model speedup pronounce network  vgg variant fully layer memory bound layer benefit compression bandwidth memory node shapeshifter delivers benefit shapeshifter ddr achieves speedup compression aware quantize model respectively shapeshifter greatly reduces compression profile compression naturally reduce memory access due compression reduces memory stall expend idle computation saving dadiannao without compression segnet mainly shapeshifter enable grain data width adaptation micro october columbus usa compression profile shapeshifter zero compression reduction profile network compression shapeshifter zero compression reduction non profile network offchip traffic reduction compression scheme leftmost network network tensorflow quantize rightmost network aware quantize compute bound therefore memory compression benefit fusion BitFusion target precision profile network compose systolic array processing grouped layer precision allows BitFusion natively per layer precision activation model decompose multiplication performs sequentially report respectively performance efficiency BitFusion chip compression scheme chip memory technology node demonstrate BitFusion benefit shapeshifter compression scnn performance scnn shapeshifter compression versus encode compression scheme scnn prune neural network scnn target prune model average scnn shapeshifter performs faster specifically newer resnet network performs report efficiency scnn shapeshifter efficient scnn baseline compression layer fusion evaluate benefit shapeshifter compression combination layer fusion combination fusion shapeshifter compression considerable reduction external memory bandwidth oppose layer fusion alone shapeshifter stripe SStripes stripe fusion  constraint stripe stripe SStripes configure chip stripe tile serial processing peak compute bandwidth multiplication per cycle stripe per layer profile compression originally propose chip activation memory MB model SRAMs model model practical appropriate memory server comparison tpu chip MB chip storage dual channel ddr memory interface prof sufficient deliver bandwidth SStripes configure activation SIPs per plus composer per tile SStripes tile contains SIPs stripe SIPs report speedup SStripes stripe report compute memory breakdown SStripes measurement dual channel ddr memory model SStripes boost performance average recent resnet speedup model alexnet vgg model memory bound benefit memory compression relatively fully layer mobilenet accelerates compute bound tensorflow quantize model benefit average  opportunity precision execution breakdown network mostly convolutional segnet googlenet  network BiLSTM relatively fully layer memory bandwidth requirement mobilenet benefit reduction memory traffic shapeshifter tensorflow quantize mobilenet chip data benefit average aware quantize model BiLSTM benefit due shapeshifter memory compression particularly fully lstm layer memory bound segnet predominantly compute bound SStripes faster benefit per compute adaptation report efficiency SStripes stripe benefit generally performance trend specifically efficiency network increase aware quantize model average micro october columbus usa  ddr ddr ddr speedup shapeshifter profile compression dadiannao performance ddr ddr ddr relative compression profile shapeshifter dadiannao ddr ddr ddr speedup shapeshifter profile compression fusion performance ddr ddr ddr relative compression profile shapeshifter fusion performance efficiency shapeshifter compression ddr ddr ddr chip memory relative ddr compression     resnets geomean relative speedup shapeshifter speedup shapeshifter speedup relative scnn shapeshifter ddr chip memory normalise scnn baseline compression benefit negligible overhead stripe detection logic additional plus compute core core chip memory similarly L1D  decoder comparison fusion SStripes fusion iso constraint fusion profile per layer precision compute memory compression conservatively stripe constant voltage constraint fusion configure per tile report relative execution SStripes alexnet vgg vgg geomean compression rate BW saving fuse layer BW saving fuse layer SS layer fusion compression ratio without shapeshifter oppose neither fusion fusion suffers significant overhead processing layer report network SStripes faster fusion model benefit pronounce aware quantize model SStripes faster average SStripes benefit per width adaptation delivers benefit precision however model quantize fusion unfortunately currently investigate performance model fusion precision profile available whereas SStripes access regardless fusion benefit shapeshifter enable grain data width adaptation micro october columbus usa relative efficiency speedup shapeshifter stripe speedup shapeshifter stripe speedup relative efficiency SStripes stripe iso comparison compute memory shapeshifter compute memory breakdown shapeshifter compression benefit persist model explicitly quantize fusion moreover future investigate modify fusion reconfigure compute finer granularity layer content SStripes efficient fusion benefit closely performance improvement benefit tensorflow quantize  difference profile dynamic precision model memory bound due relatively fully layer chip memory configuration SStripes performs limited chip buffer chip storage diminishes performance becomes limited chip bandwidth SStripes benefit regime shapeshifter loom dynamic width adaptation benefit loom architecture exploit width variability activation loom faster efficient stripe configuration loom serial processing stripe due limited report summary RA model shapeshifter loom SIPs composition performs faster average  relative efficiency speedup shapeshifter stripe speedup shapeshifter stripe speedup relative efficiency SStripes fusion iso comparison googlenet vgg vgg resnet mobilenet combine speedup alexnet compression KB compression KB compression KB profile compression KB profile compression KB profile compression KB shapeshifter compression KB shapeshifter compression KB shapeshifter compression KB performance stripe shapeshifter limited chip buffer ddr external memory outlier aware quantization compression shapeshifter apply outlier aware quantize network MobileNetV prune resnet quantize outlier respectively parameter chosen maintain comparable accuracy precision report memory traffic shapeshifter outlier aware  zero skip ZS scheme outlier aware outlier index index efficient index per furthermore outlier aware ZS compress zero allocate additional non outlier shapeshifter compression outperforms outlier aware scheme comparison outlier aware ZS scheme approach performs dense MobileNetV maintain compression ratio sparse activation demonstrate shapeshifter delivers chip memory benefit model quantize outlier aware quantization demonstrate shapeshifter quantization scheme collectively demonstrate shapeshifter effectively delivers memory micro october columbus usa  outlier aware outlier aware ZS shapeshifter outlier aware outlier aware ZS shapeshifter resnet memory footprint activation mobilenet performance SStripes compression outlier aware quantize activation traffic footprint reduction benefit various quantization without mandate network specific demonstrate compute performance benefit  quantize network comparison accelerator specifically tailor outlier aware quantize model future network quantize outlier aware technique specialized preferable related efficient inference EIE prune zero activation elimination network retrain drastically reduce storage communication processing fully layer aggressive quantization compression greatly outperform shapeshifter however shapeshifter complementary compression EIE shapeshifter benefit activation layer exploit phenomenon mostly orthogonal EIE exploit moreover recent cnns fully layer drastically decrease cnn model codebook compression compression apply convolutional layer retrain codebooks datatypes intel  tensorflow bfloat precision alternative float wider fix representation target primarily training model processing recommendation achieve accuracy float arithmetic float datatypes statically mantissa exponent regardless magnitude model exhibit  distribution exhibit variable precision requirement per layer accordingly apply shapeshifter mantissa exponent adjust significant profile trim precision per layer quantization precision reduces memory computation neural network however quantization without drawback although fix precision suitable network fix quantization scheme optimal sufficient rnns computational image task perform per pixel prediction raw sensor data image classification task processing therefore flexible precision presently important generalize across application quantization attenuates benefit shapeshifter underlie phenomenon shapeshifter exploit persists benefit quantize network SStripes benefit network quantize non precision without network quantize obtain benefit applies model quantize binary activation compress dma vector zero compression effective reduce chip traffic storage training tartan extends stripe advantage per layer precision fully layer shapeshifter directly compatible tartan increase benefit adjust precision per instead due limited evaluation future diffy improves upon shapeshifter encode activation delta diffy exploit spatial correlation activation neural network implement computational image task adapt operand precision application exhibit algorithmic fault tolerance propose nam transmit fix shift amount per accuracy loss conclusion per precision adaptation direction future combine accelerator boost effectiveness algorithm prune precision reduction quantization activation accelerate training accelerator propose input network deliver benefit however reward advance precision reduction quantization deployed incentive innovation