accelerate finite automaton processing benefit regular expression workload application obviously regular expression mining bioinformatics machine exist memory automaton processing accelerator suffer inefficient rout architecture incapable efficiently route highly automaton excessive amount hardware resource propose compact overhead flexible interconnect architecture efficiently implement rout activation apply exist memory automaton processing architecture eAP embed automaton processor throughput scalable memory automaton processing accelerator performance benefit eAP achieve exploit subarray parallelism memory compact memory interconnect architecture optimal pipeline transition efficiently mapping appropriate memory technology overall eAP achieves throughput per cache automaton micron automaton processor respectively consumption CCS CONCEPTS computer organization multiple instruction data hardware emerge architecture theory computation formal automaton theory keywords processing memory embed dram automaton processing reconfigurable compute interconnect introduction data collection analytics rapidly important operation data application network security data mining genomics complex variety inexact methodology inexact therefore regular expression equivalent finite automaton identify complex consequent demand accelerate motivate recent utilize automaton processing hardware accelerator packet inspection processing bioinformatics mining machine particle physic patent publish automaton processing application researcher increasingly exploit memory accelerator performance growth conventional processor finite automaton processing CPUs gpus exhibit irregular memory access disable prediction data technique unpredictable memory bandwidth therefore von neumann architecture struggle bigdata rate processing requirement micron automaton processor AP cache automaton CA propose memory hardware accelerator automaton processing native execution non deterministic finite automaton NFAs efficient computational model regular expression reconfigurable substrate hardware allows execute parallel hardware capacity NFA processing memory centric architecture model input processing phase input decode input detect reading memory transition successor activate propagate signal interconnect interconnect exist automaton processing accelerator incapable efficient route highly automaton provision hardware resource interconnect expense resource however benchmark hardware usually multiple reconfiguration processing data incurs significant performance penalty resource scarce resource AP repurposes dram array proposes hierarchical interconnect diverse automaton benchmark reveals congestion AP rout matrix cripple efficient utilization route automaton micro october columbus usa  resource utilized  automaton remain cannot rout resource moreover although density dram memory AP chip MB data whereas conventional dram MB data implies majority likely spent interconnect hiding dram latency recently propose SRAM automaton processing accelerator cache automaton CA repurposing cache SRAM interconnect address rout congestion AP CA proposes crossbar fcb topology interconnect connectivity automaton meaning implies crossbar switch hardware resource CA spent interconnect however automaton benchmark reveals average maximum switch utilized therefore crossbar extremely inefficient costly automaton processing application expensive interconnect opportunity address interconnect inefficiency exist memory automaton processing architecture  RCB overhead flexible interconnect architecture efficiently implement transition RCB inspire intrinsic automaton connectivity RCB switch fcb CA reduces shorter latency consumption addition efficiency RCB opportunity denser resource accommodate reconfiguration processing data across application ANMLZoo regex benchmark suite entirely RCB fcb interconnect connectivity topology reconfigurable memory array repurposed fcb connectivity expense capacity addition automaton global switch inter connectivity   efficiently transition automaton underlie memory technology eAP logical functionality within memory subarray memory non destructive output stable logical multiple  bitline SRAM gain embed dram GC eDRAM feasible memory implement eAP conventional dram reduce latency dram cannot purpose destructive  cannot recover phase CA SRAM evaluate eAP T1D transistor diode memory T1D GC eDRAM fabricate T1D transistor incurs overhead density therefore throughput due reduce reconfiguration  input scalability gain FinFET technology gain potential technology node  interestingly capability T1D memory array utilized situ computation important kernel neural network graph processing recent explore potential processing binary neural network computation SRAM alternative summary contribution propose compact overhead interconnect architecture efficiently implement transition stage automaton processing eAP embed automaton processor  scalable memory automaton processing accelerator performance benefit eAP achieve exploit subarray parallelism memory optimal pipeline transition compact interconnect architecture choice memory technology evaluate eAP T1D memory array overall eAP T1D achieves throughput per eAP CA AP respectively technology route algorithm magnitude faster AP compiler source cycle accurate automaton simulator perform software optimization automaton propose architecture background related storage NFAs node NFA DFAs deterministic finite automaton sometimes suffer exponential blowup equivalent NFAs dfa active NFA concurrently active hardware accelerator automaton processing NFAs exploit parallel transition benefit NFA compact representation non deterministic finite automaton NFA tuple finite finite transition function initial transition function determines currently active input input automaton accept input report homogeneous automaton representation model  homogeneous automaton transition input nice aligns hardware implementation cycle allows label independent interconnect eAP scalable efficient memory accelerator automaton processing micro october columbus usa performs input homogeneous automaton transition ste classic NFA equivalent homogeneous representation automaton accept alphabet classic representation accept homogeneous label ste ST ST ST ST ST accept ST architecture analyze without incur extra overhead NFA representation simplify inmemory automaton processing model memory automaton processing automaton processor AP cache automaton CA reconfigurable memory directly implement NFAs memory simplify  pipeline automaton processing memory centric model AP CA memory configure homogeneous ST ST generally automaton processing involves input transition phase input decode label input detect reading memory vector potentially combine active vector indicates currently active initiate transition vector  transition phase potential cycle active currently active active vector propagate signal interconnect update active vector input operation memory mapped label NFA mapped memory label assign STEs ST correspond assume ST active potential cycle active enable signal ST ST ST ST enable signal ST ST ST specifically input vector bitwise vector potential enable signal determines ST ST active automaton processor AP input roughly dram cycle however conventional dram cycle latency  trp generation observation majority AP chip spent rout matrix hiding dram latency  duplicate resource critical cache automaton automaton processing architecture repurposes portion cache llc proposes  automaton processing accelerator target NFAs  phase AP model crossbar interconnect SRAM memory array hierarchical switch topology local switch propose intra partition connectivity global switch sparse inter partition connectivity CA crossbar topology interconnect connectivity automaton mention earlier excessive application fpga  fpga implementation automaton processing advantage mapping spatial distribution automaton hardware resource LUTs ram  achieve mhz AP estimate CA eAP fpga chip approximately ste capacity AP chip capacity CA utilize MB llc capacity eAP utilize MB T1D embed ram moreover consumption fpga AP CA eAP recent fpga automaton processing fail complex route automaton rout resource due logical interconnect complexity ASIC implementation ASIC implementation propose accelerate automaton processing unified automaton processor  hare demonstrate rate automaton processing regular expression network intrusion detection processing benchmark hare array parallel RISC processor emulate  dfa representation regular expression  automaton model transition pack multi processing ASICs rate limited parallel transition automaton hare implement dfa limitation regex incurs processing  rate NFAs parallel active importance capacity CA author ANMLZoo benchmark calculate cache utilization report MB cache usage average micro october columbus usa  automaton ANMLZoo benchmark suite portion actual application normalize AP chip however application independent automaton comprise various application magnitude report illustrate issue sequential mining spm benchmark ANMLZoo spm iterative algorithm iteration algorithm sequence candidate automaton checked input relatively realistic dataset spm capacity spm benchmark ANMLZoo application execute iteration algorithm parallel automaton accelerator AP chip capacity reconfigure hardware subset overall input incurs overhead repeatedly input reconfiguration reduce specialized hardware resource per NFA increase capacity demonstrate NFA enable execution configure AP propose technique compact interconnect architecture utilize T1D complementary technique improves efficiency whatever hardware resource allocate automaton processing interconnect architecture implementation interconnect memory subarrays fcb efficiently compact reconfigurable interconnect RCB feasibility hardware discus potential memory switch reduce crossbar interconnect interconnect functionality ste wake successor cycle trigger successor cycle implies interconnect statically programmed ensure  non contention conventional interconnects activation bus simultaneously cannot client mesh hypercube multihop contention crossbar interconnect fcb straightforward interconnect topology STEs automaton model transition multiple STEs ste output multiple input logical active input ST potential ST ST ST active therefore dynamic arbitration cache automaton CA fcb interconnect topology local global switch NFAs application typically compose independent manifest component CCs transition CC usually CCs execute parallel independently crossbar switch utilized pack CCs densely greedy approach crossbar utilization however confirms fcb inefficient rout resource assume fcb switch greedy approach CCs sort assign interconnect resource assume CCs mapping CCs fcb switch switch configure correspond CC unused switch within CC transition sparse meaning switch synthetic benchmark average maximum levenshtein switch utilized fcb interconnect fcb model extremely inefficient automaton processing application overhead consumption delay transition phase motivate efficient compact interconnect visualize connectivity matrix automaton benchmark image label node automaton unique index breadth bfs numeric label bfs assigns adjacent index node image model transition node index automaton pixel coordinate brill levenshtein snort entity resolution union heatmap switch bfs label graph union overall connectivity image CCs benchmark chose union transition rare connection snort entity resolution benchmark nice diagonal connectivity union average image benchmark diagonal connectivity motivates compact interconnect numeric bfs label label node closely CCs mostly graph cycle node motivate observation propose reduce crossbar interconnect RCB switch github com   micro heatmap eAP scalable efficient memory accelerator automaton processing micro october columbus usa union image RCB overhead consumption delay fcb moreover apply CA AP without reduce computation feasibility RCB via RCB compact memory array preserve input output signal fcb switch complicate layout wiring congestion compact array automate layout generation sometimes clever compact scheme regular RCB therefore propose scheme compact fcb array RCB array simply flip diagonal interconnect horizontal vertical congestion dimension utilize available dimension contribute signal rout however squeeze diagonal significantly compact subarray burden signal rout dimension toy fcb subarray diagonal width index index index switch location input signal ste label bfs ste label initial naive mapping diagonal memory waste switch nearby memory reduce array calculation fcb diagonal width reduce RCB approximately switch RCB input output placement guarantee maximum input output RCB input signal signal output signal bitline signal fcb RCB compression diagonal width margin accommodate transition entity resolution snort rout subarrays decode input active vector array register directly wordlines therefore RCB incur extra overhead extra decoder moreover RCB due compression potentially shorter memory access cycle mapping memory technology earlier implement propose interconnect memory underlie memory technology logical functionality memory subarray memory non destructive data maintain operation output stable logical multiple  bitline clearly conventional dram reduce latency dram  cannot adopt destructive destroys node participate operation furthermore SRAM perform bitline unstable undefined SRAM gain embed dram GC eDRAM suitable memory technology implement eAP gain embed DRAMs GC eDRAMs comprise logic transistor optionally additional  diode recent adoption GC eDRAMs cache realization eDRAM acceleration application transistor transistor  particularly beneficial  non destructive splitting latter useful interconnect functionality adopt memory technology reconfigurable switch evaluate architecture SRAM CA T1D transistor diode GC eDRAM SRAM T1D substantially transistor leakage functionality scalability gain extensively FinFET technology gain promising technology node  maintain advantage T1D switch T1D dram connectivity switch switch disconnect switch implement exist transition STEs machine detail T1D consists PMOS transistor operation NMOS operation gate diode reduce couple T1D switch mode mode route mode mode  bitline node  switch STEs disconnect bitline DD switch gnd otherwise route mode connection source ste active destination STEs potential transition vertical  horizontal bitlines switch dot switch switch source ste active correspond bitlines activate micro october columbus usa  potential detail bitlines discharge therefore amplifier bitline convert gate switch adopt switch CA consists SRAM additional transistor bitline allows bitlines input signal implies functionality   processor explain eAP eAP T1D eAP replicate accommodate automaton overall capacity eAP T1D eAP eAP overview eAP consists multiple subarrays global decoder global amplifier global bitlines subarray local amplifier local decoder subarray parallelism salp global decoder access reduce resource enable activation subarrays parallel therefore activation precharging locally within subarray utilize salp phase input multiple automaton parallel memory mode normal mode NM data storage cache automaton mode NM global decoder selects subarrays input address selects within subarray local decoder address input global decoder activate subarray parallel input entire correspond amplifier yield vector accept input vector arrangement  address local amplifier vector propagate transition stage operation configuration STEs memory  normal mode operation automaton processing array APA maximum capacity STEs APA consists tile tile contains automaton processing  APU host memory subarray RCB subarray aggregate node local interconnect inside APA tile collaboratively global switch component APU choice parameter prior organization global fcb switch allows APU node PNs communicate PNs  APA global fcb APA minimize  topmost  uncommon CC RCB interconnect  eAP repurposes subarrays fcb interconnects specifically combine subarray  tile crossbar interconnect APU tile subarray configure fcb instead regular operation fcb SM signal tile signal selects wordlines target subarray driven vector register instead decoder output mode capacity contribute tile ability accept CCs without limitation interconnect functionality array multiplexer subarrays tile fcb SM multiplexer overhead mux reconfiguration promotes tile embed component plus PNs communicate  APA flexible interconnect topology pipeline input memory access vector phase potential vector transition phase vector register pipeline register overlap transition rout previous input cache automaton proposes stage pipeline automaton processing SM GS global switch LS local switch however pipeline data hazard issue input cycle transition LS GS previous cycle stage however LS output stage pipeline stall input resolve hazard decrease throughput factor another avoid data hazard merge GS LS stage sequentially stage memory access whereas stage consecutive memory access refine version CA pipeline verify author unlike CA propose pipeline balance amount stage pipeline frequency slowest stage interconnect organization global local switch parallel stage global switch  correspond local switch perform additional operation memory access pipeline optimization parallel GS obtain standard library nda cannot identify vendor eAP scalable efficient memory accelerator automaton processing micro october columbus usa abstraction physical implementation overview eAP architecture inside tile datapath communication local RCB global switch fcb LS apply CA performance CA eAP pipeline input output eAP asynchronous FIFOs input input buffer IB report output buffer OB host cpu communicates IB OB interrupt trigger memorymapped IO dma interrupt service routine isr responsible IB evict OB assume ghz ghz frequency eAP T1D eAP respectively mhz frequency interrupt IB KB data eAP IB interrupt recently characterize reporting statistic ANMLZoo benchmark benchmark report per cycle average investigation motivates entry OB byte report meta data interrupt rate IB automaton configuration normal operation mode NM eAP switch automaton mode consume input IB buffer output signal empty signal IB  interrupt signal cpu service device automaton mode cycle IB popped address bus contribute eAP APU equip report vector mask identify report cycle simply perform bitwise operation active vector report aggregator rad mechanism propose improvement micron AP reporting procedure OB report IDs cycle information rad adaptively shrink report message active OB efficiently OB interrupt signal service host cpu future report integration discus integration eAP T1D GC eDRAM eAP T1D SRAM eAP bandwidth package memory opm introduces  memory layer chip dram chip cache conventional memory hierarchy intel eDRAM opm haswell broadwell skylake architecture gap chip chip memory bandwidth haswell broadwell processor eDRAM TC cache eAP T1D replace TC eDRAM T1D repurpose portion cache automaton processing eAP assume integration cache automaton cache automaton repurposes cache slice automaton processing access cache leverage cache allocation technology eAP automaton micro october columbus usa  mode compiler generates configuration array interconnect configuration writes eAP memory address offload input task compiler compiler task component RCB switch template mapped fcb mapping automaton hardware representation ste accomplish decision RCB fcb fix matrix representation RCB interconnects generate diagonal matrix DM assign switch location RCB interconnect automaton node bfs traversal fake node node automaton calculate connectivity matrix automaton bfs assign calculate matrix subset DM diagonal switch RCB otherwise automaton fcb diagonal automaton  RCB interconnect capacity automaton bfs label assign input assign interconnect offset input interconnect instead automaton component assign interconnect partially interconnect spare capacity initialize RCB interconnect pool available interconnect evaluation methodology application evaluate eAP architecture ANMLZoo regex benchmark suite ANMLZoo application machine data mining security standard MB input ANMLZoo experimental setup evaluate eAP memory array T1D eAP T1D eAP eAP interconnect memory array sometimes repurpose  array interconnect logical functionality SRAM unable functionality multiple cannot bitline eAP T1D eAP CA AP technology CA interconnect SRAM array calculate cycle memory array standard memory compiler T1D analysis rely fabricate chip develop cycle accurate automaton simulator perform software optimization automaton eAP architecture extract per cycle statistic estimation RESULTS architectural contribution interconnect fcb performance evaluation contact author source code interconnect efficiency overall architectural benefit propose interconnect RCB CA interconnect architecture fcb earlier CA fcb memory RCB interconnect memory meaning RCB consumes switch memory fcb evaluate fcb apply interconnect reduction technique fcb subarrays conclude RCB subarray reduce fcb baseline subarrays RCB switch fcb baseline fcb RCB faster cycle shorter consumes applicability RCB synthetic automaton application calculate RCB fcb application compiler iterates component CCs RCB switch fcb switch accommodate connectivity rout ultimate interconnect approach hybrid RCB fcb versus baseline fcb propose CA assumes connectivity component evaluate  component application entirely RCB fcb RCB switch memory application fcb respectively confirms fcb provision automaton application CC application however CCs  global switch  local switch connectivity  distance loop none CCs RCB switch snort interconnect accommodates CCs RCB fcb RCB whereas baseline  levenshtein route automaton AP compiler benchmark AP chip however levenshtein implies STEs interconnect resource AP chip waste rout congestion however interconnect model RCB switch rout resource eAP rout resource eAP accommodate automaton levenshtein compiler optimization constraint fan fan node sensitivity analysis automaton maximum fan fan minimum switch interconnect optimization apply  interconnect variation gain non volatile memory memory implement functionality rout eAP scalable efficient memory accelerator automaton processing micro october columbus usa comparison interconnect approach hybrid RCB fcb CA interconnect fcb subarrays switch memory CA switch CA benchmark transition baseline switch baseline switch component fcb fcb RCB reduction fcb fcb RCB reduction component brill dotstar  none none fermi ham levenshtein   randomforest snort spm  dotstar dotstar dotstar  bro tcp clamav component  snort RCB connectivity distance loop purpose fcb tcp clamav  component subarray therefore global switch  transition local switch overall overhead discus overhead array interconnect array overhead capacity equivalent STEs eAP furthermore architectural contribution technology contribution analysis subarray T1D fabricate image estimate spent memory spent decoder amplifier project T1D estimate calculate architecture T1D FinFET technology accord T1D memory consistent assumption overhead interconnect overhead architecture assume CA eAP reduces overhead interconnect architectural contribution RCB eAP T1D reduces overhead interconnect architectural contribution RCB technology choice overall overhead rout eAP T1D eAP CA AP respectively technology overall performance report cycle frequency SRAM array twice T1D gain array technology overhead eAP CA AP normalize CA interconnect eAP architectural contribution eAP T1D assume ratio estimate access frequency T1D array fcb access frequency SRAM array calculate standard SRAM compiler nominal voltage T1D architecture T1D FinFET technology accord T1D memory array 2GHz consistent assumption despite reduction RCB assume delay RCB fcb CA proposes amplifier cycling technique assumes reduction access delay however access delay calculate delay local global switch CA assumption SRAM memory compiler fix switch delay calculation pipeline data hazard CA reduces frequency 2GHz 3GHz verify author spice simulation CA delay calculate cache slice switch delay estimate assume assume micro october columbus usa  overall consumption eAP T1D eAP CA opt ideal AP overall consumption eAP T1D eAP CA opt AP report micron delay fcb RCB eAP assumption RCB shorter delay pipeline stage CA eAP CA refine pipeline  switch sequentially stage pipeline delay eAP optimize pipeline switch RCB switch fcb parallel therefore pipeline delay eAP T1D eAP respectively optimization propose eAP apply CA CA opt improves CA frequency 3GHz 2GHz therefore architectural contribution optimize pipeline improves frequency eAP T1D CA pipeline stage delay switch switch freq max freq eAP T1D ghz ghz eAP ghz ghz CA ghz ghz CA opt ghz ghz commodity dram T1D periodic refreshes retain refresh operation sequence dummy memory eAP T1D refresh calculate refresh meaning writes retention refresh perform subarrays parallel normal operation throughput per AP CA eAP input cycle therefore deterministic throughput input per cycle independent input benchmark another important metric addition frequency capacity capacity accommodate automaton iteration input reconfiguration overhead throughput architecture normalize throughput define parallel frequency  per AP operates mhz frequency CA eAP architecture semiconductor technology node technology projection AP throughput normalize per eAP T1D performs due interconnect technology benefit eAP T1D eAP CA CA opt AP AP overall eAP T1D achieves throughput per eAP CA CA opt AP respectively technology eAP throughput per CA opt CA array array interconnect interconnect overhead eAP adopts array interconnect interconnect overhead resource consumption discus consumption eAP T1D eAP prior calculate consumption active partition switch transition local switch consume gate memory array cycle cycle basis gate subarrays potential beforehand however pipeline potential calculate simultaneously prevents gate gate array unoccupied observation CA update CA observation AP adopt ideal AP model CA statistic per cycle extract compiler static consumption eAP T1D consists component leakage refresh data alive refresh  gain dominant portion static moreover static T1D memory array static SRAM array ratio calculate static eAP scalable efficient memory accelerator automaton processing micro october columbus usa eAP T1D estimate dynamic consumption RCB fcb standard memory compiler per input eAP T1D eAP CA opt ideal AP model benchmark  dotstar snort spm consume benchmark utilized switch array accommodate furthermore  cannot utilize RCB resource local interconnect fcb consumption overall consumption eAP T1D eAP CA opt efficiency eAP T1D density compact RCB consume dynamic due shorter switch average consumption across benchmark consumption eAP T1D eAP CA opt respectively eAP T1D performance scalability benchmark performance increase automaton ANMLZoo constrain non constrain scenario assume CA CA opt eAP utilize MB cache accommodate STEs intel generation core processor haswell broadwell 5GB 1GB embed memory cpu cache eAP T1D assume 1GB eDRAM therefore eAP T1D STEs summarizes eAP T1D relative eAP CA CA opt AP eAP T1D density advantage allocation automaton processing limit factor density advantage apply allocate becomes limit factor eAP capacity advantage summary memory automaton architecture interconnect eAP T1D eAP CA CA opt AP freq ghz however benchmark ANMLZoo portion actual application normalize AP chip regex application snort brill dotstar density advantage pertain application magnitude multiple input pas implement portion overall automaton reconfiguration overhead apply mention costly CA performance CA CA opt eAP T1D eAP average ANMLZoo normalize AP performance without constraint non constrain scenario assume CA CA opt eAP utilize maximum capacity scenario relationship additional factor reconfiguration overhead speedup eAP CA CA opt eAP T1D respectively eAP constrain scenario assume maximum reduces allowable active processing eAP performance CA CA opt eAP T1D benchmark eAP frequency others however increase benchmark reconfiguration multi processing input become limit factor CA CA opt due capacity frequency eAP due consumption eAP T1D performance CA CA opt eAP increase benchmark eAP T1D density consumption performance benefit eAP increase automaton furthermore advantage eAP T1D CA CA opt eAP increase increase input performance benchmark CONCLUSIONS propose eAP dense reconfigurable architecture automaton processing exploit inherent parallelism memory multiple concurrent transition NFA utilize subarray parallelism memory automaton parallel motivate connectivity automaton benchmark propose reduce crossbar interconnect transition compact switch crossbar interconnect reduction switch reduces consumption delay due shorter overall eAP throughput normalize previously memory automaton accelerator cache automaton CA automaton processor respectively benefit eAP application multiple