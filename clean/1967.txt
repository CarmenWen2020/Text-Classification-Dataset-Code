processing memory pim concept enable massively parallel dot operand memory pim ideal computationally demand neural network dnns recurrent neural network rnns processing resistive ram RRAM particularly appeal due RRAM density limitation pim  analog digital conversion defeat efficiency performance benefit pim demonstrate cascade architecture connects accumulate mac RRAM array buffer RRAM array extend processing analog memory dot partial sum buffering accumulation implement dnn rnn layer choice interface enable variation tolerant robust analog dataflow memory mapping scheme mapping devise enable RRAM accumulation partial sum analog summation scheme reduce conversion obtain sum cascade recent RRAM computation architecture dnn rnn benchmark demonstrate cascade improves efficiency maintain competitive throughput CCS CONCEPTS computer organization neural network instruction multiple data keywords memory resistive ram neural network accelerator introduction recent machine become dominant workload generation computation important machine kernel neural network dnns recurrent neural network rnns widely deployed task image analysis recognition dnn workload typically highly vectorized define dataflow digital dnn accelerator achieve performance efficiency continued increase dnn complexity demand faster machine efficient chip employ application autonomous mobile device application advanced nonvolatile memory resistive ram RRAM important role thanks storage density leakage wake beyond density storage recent RRAM computation processing memory pim enable massively parallel dot RRAM crossbar array without operand core computation dnn layer easily mapped RRAM crossbar array input activation dnn layer apply wordlines WL RRAM crossbar voltage pulse conductance RRAM crossbar dot obtain bitline BL RRAM crossbar physic ohm multiplication  accumulation elegant RRAM dot project achieve impressive performance efficiency RRAM dot analog computation digital digital input RRAM crossbar convert voltage pulse digital  converter DACs output RRAM crossbar analog integrate digitize analog digital converter ADCs RRAM computation resolution requirement analog computation accommodate multi WL pulse multi RRAM conductance sum  ADCs significant overhead RRAM crossbar device resolution increase adc resolution increase surprising analog digital conversion eventually dominate consumption RRAM computation extent render RRAM computation impractical survey recent RRAM computation highlight overhead conversion severe limitation fledge dnn  RRAM isaac employ ADCs micro october columbus usa estimate silicon prime amplifier sas instead conventional ADCs reduce however SA capable resolve obtain digital output SA cycle decision latency exponentially dependent resolution SA interface limit throughput demand application limitation RRAM computation layer dnn rnn practical RRAM crossbar twofold despite rapid progress RRAM technology development RRAM crossbar demonstrate partial sum accumulate BL comparison output feature fmap fully FC layer alexnet partial sum accumulation output fmap convolutional layer googlenet partial sum accumulation easily exceed analog accumulation practical RRAM crossbar therefore kernel computation mapped multiple RRAM crossbar partial sum multiple crossbar digitize accumulate digital domain impractical assume reliably RRAM multi mlc complex DACs ADCs easily affected variation practical multi multiple RRAM similarly practical input apply serially simplify circuitry reduce variation uncertainty practical activate subset WLs instead WLs RRAM crossbar practical approach partial sum digitize digitally accumulate essence RRAM computation consists RRAM dot conversion digital accumulation partial sum currently RRAM conventional CMOS circuit besides aforementioned overhead resolution conversion estimate digital partial sum accumulation surpass RRAM dot yield core RRAM computation insignificant cascade RRAM computation architecture dnns rnns specifically address conversion digital partial sum accumulation associate RRAM computation approach contribution practical robust memory dot reduce effective BL resolution ensure variation tolerance resolution analog output reliably cascade analyze resolution inference accuracy lower BL resolution accuracy reliably achieve propose mapping scheme buffer RRAM perform RRAM partial sum accumulation replace digital partial sum accumulation analog summation bypass dot implement RRAM crossbar parallel dot implement RRAM crossbar array input vector convert voltage pulse DACs RRAM crossbar BL sample convert digital conversion sum reduce conversion mac RRAMs buffer RRAMs  amplifier  interface convert mac RRAMs BL output analog analog voltage directly buffer RRAMs input cascade mac RRAMs buffer RRAMs enables analog dataflow computation requirement dnn rnn layer intermediate analog memory obtain efficiency performance background RRAM  mim device information via programmable conductance typical RRAM device construct crossbar array dense storage fulfill demand nonvolatile memory addition storage RRAM crossbar array perform parallel dot dot parallel dot RRAM crossbar matrix RRAM crossbar conductance input vector DACs convert voltage pulse apply WLs crossbar voltage pulse RRAM conductance voltage input conductance RRAM along aggregate BL dot illustrate voltage pulse apply WL RRAM activate multiple WLs RRAM crossbar enables dimension parallelism throughout dot operation operand matrix memory significant data movement cascade RRAMs extend analog dataflow memory processing paradigm micro october columbus usa mapping convolution operation RRAM crossbar array addition dot RRAM device accumulation apply consecutive pulse reset pulse RRAM increase decrease conductance benefit RRAM accumulation significant typical digital accumulation memory fetch temporary sum writes memory update sum eliminate RRAM accumulation workload mapping RRAM dnns rnns emerge important machine workload dnn consists layer convolution conv pool normalization fully FC layer conv FC layer computation intense memory intense layer conv layer input activation channel convolve kernel channel output fmap channel kernel training algorithm output fmap calculate input activation kernel activation function elementary computation involves input activation compute output fmap describes layer loop involve accumulation partial sum increase throughput layer loop unrolled partially unrolled entire output fmap additional outer loop FC layer conv layer dimension input activation kernel FC layer typically conv layer dimension input activation output fmap perform inference conv FC layer remain static input activation therefore advantageous RRAM reuse input activation apply efficient RRAM crossbar array kernel conductance kernel across illustration assumes RRAM array consists RRAM device sufficient resolution mapping partial sum accumulate BL computation output fmap practical RRAM array nearly layer dnn rnn easily exceed furthermore practical RRAM distinguish technology limitation multiple RRAM accumulation perform multiple BLs RRAM array multiple array BL digitize adc SA accumulation digital domain underlie computation rnn mapped manner memory lstm rnn input sequence input typical lstm layer define input gate forget gate output gate candidate memory tanh memory hidden tanh parameter training algorithm denotes wise multiplication sigmoid function tanh hyperbolic tangent function tanh wise nonlinear activation function computation intensive matrix vector rewrite formulation dot lstm implement RRAM approach RRAM computation isaac prime pipelayer recently publish architecture implement dnn rnn RRAM computation SRAM computation dram computation micro october columbus usa isaac prime pipelayer cascade input bitwidth encode input per cycle bitwidth encode resolution array ows  BL resolution  output bitwidth encode truncation encode output interface adc SA spike integrate TIA RRAM mac architecture comparison concept comparison aspect isaac prime pipelayer circuitry prime applies width modulation WL input convert voltage pulse precise voltage challenge complicate DAC circuit isaac input serial fashion binary input WL driver voltage pulse onto WL driver simpler pipelayer adopts isaac serial input BL resolution RRAM computation BL resolution prime per RRAM RRAM array BL resolution output truncate practical isaac serial input per array reduce BL resolution serial input adopt pipelayer however pipelayer per BL resolution circuitry prime SA BL SA cycle perform comparison output output bitwidth isaac adc costly adc BLs array amortize pipelayer integrate component BL generate spike serial integration latency physical implementation challenge pim chip demonstrate challenge peripheral circuitry BL resolution realistic resolution input lower subset WLs activate pim chip SRAM RRAM chose digitize significant  reduce conversion output severely limit application pim WLs activate per resolution pim affected voltage pvt variation critical variation account pim conversion RRAM computation conversion integral RRAM computation contributes consumption conversion choice adc SA adc complexity consumption sample rate resolution RRAM computation relatively relaxed sample rate due intrinsic RC delay WL BL propagation however RRAM computation resolution depends resolution analog pulse resolution RRAM  activate parallel nrows desire multi mlc RRAM parallelism activate parallel resolution constantly consumption conversion exponentially resolution conversion RRAM computation challenge SA commonly peripheral circuitry  SLC memory SRAM dram SA adc BL voltage reference voltage output SA multi adc sweep reference voltage reference voltage ramp SA output flip counter SA circuitry compact SA  conversion latency cycle resolution cascade architecture cascade RRAM computation architecture target inference iot device stringent envelope cascade chip analog processing  consists RRAM crossbar array RRAM array tasked perform memory dot buffering memory accumulation cascade executes dnn rnn model layer layer layer load  memory input assume dot quantize isaac prime pipelayer cascade efficient analog dataflow TIA interface output accumulate mac RRAM convert analog voltage voltage apply buffer RRAM directly accomplish partial sum accumulation sum amplifier ADCs convert digital cascade RRAMs extend analog dataflow memory processing paradigm micro october columbus usa illustration cascade architecture bold propose dataflow zoom analog processing APU activation normalization pool output memory layer processing cascade mac RRAMs buffer RRAMs realizes core computation conv FC layer analog RRAM conversion occurs computation redundant conversion intermediate input mapping mac RRAM cascade adopt serial input mac RRAM input lsb msb WL pulse WL driver simpler compact consumes multi DAC mac RRAM  per mapped  RRAM limit BL resolution impact variation mapping  moderate RRAM array nrows  BL resolution previous encode BL resolution reduce serial input binary mapping mac RRAM voltage reference simplify rout driver circuitry cascade mac RRAM per mac RRAM effectively subsection subsection vector mac RRAM performs dot input vector vector buffering partial sum buffer RRAM RRAM dot BLs mac RRAM analog partial sum associate comparison RRAM mac digital accumulation partial sum RRAM mac  buffering accumulation partial sum dash boundary inner loop highlight serial input input vector analog partial sum align accumulate partial sum accumulation due mapping dot multiple mac RRAMs previously dot mapped multiple mac RRAMs mac RRAM multiplexing partial sum accumulate obtain digital accumulation previous accumulation partial sum digital domain dataflow illustrate input vector convert BL output mac RRAM digital partial sum ADCs sas temporary sum SRAM register shift accumulate partial sum truncate lsbs sum maintain bitwidth update sum SRAM register illustrate input partial sum accumulation incurs conversion data movement SRAM register significantly worsens efficiency performance conversion wasteful due lsb truncation analog RRAM buffering accumulation cascade architecture employ analog buffering RRAM accumulation cascade mac RRAM buffer RRAMs via micro october columbus usa illustration mapping input input serially lsb mac RRAM arrow align partial sum  input driver partial sum ith jth cycle buffer RRAM TIA interface dataflow illustrate input vector convert BL output mac RRAM analog voltage  align voltage input buffer RRAMs analog partial sum mac RRAM BL resolution propose mlc RRAM buffer RRAMs serial input analog partial sum buffer RRAM accumulate conversion RRAM accumulation partial sum propose mapping scheme illustrate lsb  input subsection mac RRAM vector dot compute input vector vector output analog partial sum per BL analog partial sum buffer RRAM address dot compute input vector vector output analog partial sum shift buffer RRAM address completion serial input partial sum buffer RRAM input mapping scheme allows accumulation buffer RRAM described cascade assume input partial sum compute subsection mac RRAM buffer RRAM mac RRAM buffer RRAMs buffer RRAM partial sum subsection mac RRAM normalize consumption digital  accumulation analog RRAM partial sum buffering accumulation breakdown digital accumulation partial sum analog RRAM buffering accumulation analog RRAM buffering accumulation estimate digital partial sum accumulation significant saving mainly attribute elimination SRAM access digital approach analog RRAM buffering accumulation conversion partial sum accumulation illustrate addition conversion limited output eliminate redundant conversion intermediate buffer RRAM consideration standard RRAM relatively voltage primary limited endurance propose voltage buffer RRAMs TR RRAM transistor improve endurance voltage RRAM non deterministic simulation error variation due non deterministic incorporate analysis allocate avoid pipeline stall TIA interface mac RRAM buffer RRAM TIA convert input proportional output voltage conventional TIA construct operational  amplifier ota resistor feedback connection conventional consume output voltage appropriate buffer RRAM TIA circuit convert mac RRAM BL voltage voltage buffer RRAM TIA circuit operates phase transfer phase SW SW enable feedback loop convert BL voltage SW SW detach cout precharge vdd phase SW SW detach BL TIA SW precharge cascade RRAMs extend analog dataflow memory processing paradigm micro october columbus usa schematic TIA convert input proportional output voltage simulation waveform TIA CMOS technology leakage output capacitor buffer RRAM zoom TIA transfer phase transfer phase SW sample voltage NMOS reproduce BL discharge cout transfer SW voltage cout buffer RRAM conventional TIA propose TIA faster per conversion flexibility output voltage stage TIA circuit simulated obtain realistic parameter evaluation  capacitor cout simulation leakage cout negligible voltage RRAM accumulation conversion mapping scheme analog partial sum subsection mac SRAM buffer RRAM sum conversion resolution accumulation illustrate BLs efficiently obtain output resolution output BLs msb directly contribute output resolution digitize suitable ADCs BLs lsb analog sum amplifier BL appropriately sum analog sum  fed input compress msb digital sum analog accumulation scheme conversion reduce digital summation reduce accumulation efficiently analog domain although analog accumulation precise digital accumulation msb imprecision becomes negligible conversion adc sample frequency reduce per conversion conversion ADCs reduce tolerance cascade architecture connects mac RRAMs buffer RRAMs relies analog dataflow mac RRAMs buffer RRAMs critical variation tolerance analytically lump variation non  analog circuit effective mac RRAM BL signal ratio SNR SNR affect classification accuracy layer mlp workload configuration SNR SNR margin tolerance suppose aim classification accuracy BL resolution input cycle mac RRAM mac RRAM cascade minimum SNR BL resolution input cycle mac RRAM mac RRAM pipelayer minimum SNR tolerance pipelayer configuration cascade configuration cascade architecture adopts BL resolution ensure robustness analog memory computation evaluation establish reference architecture isaac prime exploration cascade capability architecture limitation finally instance cascade architecture evaluate realistic workload comparison reference methodology reference architecture reference architecture adc architecture adapt isaac micro october columbus usa illustration accumulation schematic sum amplifier layer mlp classification accuracy configuration  nrows  alexnet classification accuracy cascade configuration isaac pipelayer SA architecture adapt prime comparison architecture employ RRAM crossbar array utilize serial input binary mapping summarizes architecture comparison difference cascade performs RRAM buffering accumulation reference architecture perform digital cascade adc SA RRAM dot input serial binary mapping RRAM array partial sum accumulation analog RRAM digital sum SRAM digital sum SRAM resolution activity per cycle per cycle per cycle normalize latency ADCs ADCs per array ADCs per array sas per array configuration cascade adc SA reference architecture accumulation convert analog partial sum ADCs sas efficient TIA interface analog buffering accumulation cascade reduces conversion per cycle per cycle component model evaluation technology RRAM model SRAM model construct obtain memory compiler adopt circuit component model isaac analog component adc SA sum amplifier obtain recent literature successive approximation sar adc isaac sar adc resolution SA model adapt sum amplifier TIA CMOS technology simulated cadence spectre obtain latency variation cascade RRAMs extend analog dataflow memory processing paradigm micro october columbus usa alexnet vgg vgg vgg MSRA MSRA MSRA   FC FC FC FC FC FC FC benchmark evaluation convolution layer denote correspond notation denotes layer computation density across notation RRAM array  per array ADCs array benchmark benchmark dnns rnn evaluate cascade architecture reference detail dnns rnn imagenet image classification dataset alexnet resnet vgg googlenet MSRA recognition  image caption  cascade exploration cascade architecture parameterized variable RRAM array simply denote RRAM array denote  per array denote byT ADCs denote  assume storage capacity KB MB ddr bandwidth 6GB cascade chip external memory computation density GOPS RRAM array computation density due dot accumulation perform RRAM array however array bandwidth adc resolution interface circuitry  sum amplifier ADCs optimal RRAM array APU optimal  limited bandwidth peak performance GOPS achieve RRAM array  per array central ADCs denote performance consumption APU evaluate performance comparison reference APU contains RRAM array  per array ADCs array consumption cascade reference architecture dnn rnn benchmark cascade architecture achieves average adc architecture SA architecture across benchmark breakdown architecture shed competitive advantage cascade input buffer RRAM dot consume amount across architecture however cascade TIA interface consumes adc interface SA interface latter due latency SA conversion throughput cascade reference architecture dnn rnn benchmark average cascade architecture achieves throughput adc architecture micro october columbus usa consumption cascade reference architecture dnn rnn benchmark throughput cascade reference architecture dnn rnn benchmark breakdown cascade reference architecture throughput SA architecture due latency conversion summary cascade improves upon adc architecture adc architecture isaac already demonstrate improvement throughput computation density dadiannao chip dadiannao demonstrate speedup nvidia KM gpu therefore benefit cascade previously demonstrate gain ASIC chip gpu extension spike neural network cascade architecture adapt spike neural network SNNs TIA output capacitor integration capacitor comparator generate spike voltage integration capacitor exceeds threshold approach implement SNN mac RRAMs  additional comparators buffer RRAMs bypass conclusion cascade architecture connects mac RRAMs compute dot buffer RRAMs  buffering accumulate partial sum efficient TIA interface dot partial sum accumulation essential operation implement dnn rnn RRAM analog ensures efficiency remove overhead conversion digital accumulation cascade RRAMs extend analog dataflow memory processing paradigm micro october columbus usa demonstrate mapping scheme efficiently accumulate partial sum analog summation approach bypass conversion cascade architecture minimizes conversion conversion entire computation cascade architecture pipelined achieve performance consumes adc RRAM computation architecture processing dnn rnn workload built realistic RRAM technology constraint cascade architecture SNR margin variation tolerance CMOS periphery circuitry