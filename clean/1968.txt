neural network dnns become prevalent approach computer vision machine processing recognition application although dnns perceive compute intensive task apply intense pressure capacity bandwidth memory hierarchy primarily due intermediate data communicate across network layer prior hardware dnn accelerator leverage layer data sparsity via fully customize datapaths however dynamically compress expand data challenge task purpose multi processor virtual memory hardware manage coherent cache hierarchy dnn intermediate data sequentially reshaped regular transformation layer hence access data tolerate sequential sequential compression expansion without random retrieval insight propose zcomp cpu vector ISA extension tailor dnn layer communication zcomp compactly zero compression expansion fully automates metadata generation storage retrieval eliminates extra instruction execution register usage zcomp target inference training dynamically compress expand layer data memory evaluation individual layer dnn network demonstrate zcomp substantial data traffic reduction chip across cache hierarchy chip dram performance improvement compression exist avx compression approach CCS CONCEPTS computer organization parallel architecture instruction multiple data neural network keywords memory sparsity compression ISA cpu introduction data era neural network dnns quickly emerge killer app compute hardware data service provider rapidly deploy dnns variety task recognition processing computer vision training inference task become complex dnn workload demand compute memory capability trend turbo growth purpose dnn accelerator dnn specific optimization purpose CPUs research dnn hardware focus compute throughput operation per memory subsystem quickly become bottleneck availability resolution data batch increase input intermediate datasets importantly cope accuracy demand data scientist developed complex sophisticated dnn model network layer training parameter layer resnet ILSVRC winner parameter vgg ILSVRC winner layer alexnet ILSVRC winner network dynamically generate megabyte intermediate data activation feature per network layer buffer communicate across multiple layer data  chip cache capacity exerts immense pressure memory bandwidth dnn hardware expensive memory HBM google tpu alleviate bandwidth bottleneck dnns prior dnns exhibit significant data sparsity amenable data compression technique data sparsity dnns model sparsity feature sparsity model sparsity leveraged via offline technique model prune quantization feature generate dynamically unique input harder exploit feature sparsity offline technique promising technique leverage feature sparsity compress feature runtime prior hardware dnn accelerator propose zero skip zero compression layer feature communication technique rely fully customize datapaths encode skip zero activation data communicate network layer technique highly effective custom built accelerator challenge purpose multi processor CPUs preserve legacy virtual memory hardware manage coherent cache hierarchy micro october columbus usa  akin     intense ongoing debate regard hardware approach suitable dnn training inference custom ASIC accelerator FPGAs gpus purpose CPUs accelerator gpus CPUs raw compute throughput CPUs unique advantage recent facebook tighter integration desire dnn non dnn task CPUs preferable due flexibility latency availability datacenter recent simd extension avx enhance cpu performance dnns CPUs already widely deployed data client device effective vehicle dnn inference training lower ownership TCO development accelerator therefore important develop cpu tailor optimization address memory challenge impose dnn workload explore instruction architecture ISA extension efficiently compress dynamic data generate dnns propose technique motivate observation feature dnn layer population zero activation intersperse non zero activation feature access almost exhibit perfect whereby activation data sequentially layer later sequentially another layer without random retrieval propose zcomp cpu vector ISA extension tailor dnn  communication function zcomp dynamically compress expand data memory memory hierarchy throughput parallel execution transparent interaction cache virtual memory flexible software interface specifically zcomp enables zero data feature compactly encode fully automate meta data generation storage retrieval eliminates extra instruction execution register usage contribution evaluate feature memory footprint sparsity impact memory subsystem dnn workload CPUs analyze exist avx ISA approach feature compression demonstrate limitation propose zcomp efficient vector ISA extension tailor feature compression explain microarchitecture software interface evaluate zcomp detailed experimental intel caffe relu activation layer deepbench configuration network google tensorflow zcomp average reduction memory traffic average performance improvement training popular dnns background motivation overview dnns discus memory dnns sparsity  data transfer motivates neural network recent neural network dnns massively deployed compute various data mining recognition synthesis task dnns specific application multilayer perceptrons MLPs backpropagation training apply supervise recurrent neural network rnns memory lstm task exhibit dependent sequential behavior recognition text synthesis convolutional neural network cnns suitable computer vision task image recognition image classification detection cnns explain structure operation dnns cnn consists multiple network layer input layer output layer hidden layer hidden layer responsible extract feature input data passing extract information layer earlier hidden layer extract feature whereas later layer detect detailed feature eventually classify image category layer cnn convolution layer conv layer building cnns conv layer applies filter input generates feature layer filter conv layer comprises parameter apply portion input convolution activation function typically rectify linear relu generate feature input layer pool layer layer reduce activation data combine information multiple adjacent activation activation average avg pool maximum max pool computation fully layer FC layer input output generally layer cnn data cnn classify category input input image network classifies recognizes inference task input data depends image resolution batch cnn cnn learns network layer training output output adjust accordingly data depends complexity network layer precision activation feature layer generates output feature sum input passing filter relu input activation layer input image conv layer activation propagate previous layer activation data generate layer function input batch filter layer pool layer reduce activation data latter layer cnn typically activation footprint earlier layer zcomp reduce dnn layer memory footprint vector extension micro october columbus usa MBs memory footprint vgg input feature zero ratio vgg zero ratio memory footprint vgg batch dnn training inference dnn training amount input data image output iteratively update network layer entire training broken series training epoch epoch involves network traversal propagation backward backward propagation direction processing input data broken batch batch propagate backward epoch network update training epoch training desire accuracy training classify input inference sparsity dnns sparsity dnns model sparsity feature activation sparsity model sparsity arises due redundancy zero precision model sparsity leveraged via prune quantization training generally compression technique sparse model input analyze quantize compress offline achieve model prune model reduce precision inference possibly precede training feature compression cannot achieve offline processing activation generate uniquely input training inference hence activation data dynamically compress generate challenge model compression feature sparsity arises due dnns commonly employ rectify linear relu activation function relu negative input zero output hence activation generate relu layer population zero technique randomly discard output activation algorithmic avoid fitting discard activation zero feature feature exhibit sparse behavior zero network layer extract feature input data location zero significantly across network layer input combination hence dynamic mechanism capture sparsity feature layer dnn pipeline activation function output local response normalization LRN layer simply normalize input pool layer selectively pas input although layer apply relu activation sparsity generate earlier layer demonstrates feature sparsity characteristic vgg popular dnns network layer percentage zero data generate training epoch analysis profile tensorflow implementation vgg batch execute core skylake cpu feature sparsity exists network layer generally pool layer reduce sparsity available input whereas conv layer mostly enhance analysis merge LRN layer previous layer separately reporting sparsity layer however LRN layer simply available sparsity previous layer insight conclude fully utilize available feature sparsity dynamic compression mechanism generalize apply layer data transfer activation generate relu function memory requirement dnns although prior dnns focus compute bottleneck dnns significant amount data movement throughout memory hierarchy quantify memory impact dnn performance profile popular dnns implement tensorflow framework sniper simulator model avx capable core cpu detail micro october columbus usa  akin     alexnet googlenet inc resnet vgg  cpu cycle spent sync memory compute cpu cycle breakdown dnn benchmark methodology breakdown cpu stall cycle compute memory synchronization operation worth dnn workload execution stall due memory access data motivates memory optimization tailor dnns memory bottleneck become critical future due processor incorporate hardware customizations ISA extension improve raw compute throughput reduce compute stall cycle future trend dnns likely pressure chip cache capacity memory bandwidth accurate implementation task image understand detection action recognition video inference feature furthermore distribute inference batch prefer improve throughput increase memory bandwidth capacity analyze memory capacity usage dnn benchmark memory capacity consume data dnn analysis batch network resnet  feature data account majority memory footprint another source memory usage gradient communicate layer backward propagation layer layer analysis feature footprint vgg network earlier dnn layer generally generate  layer feature data whereas data dominant fully layer moreover batch increase feature footprint relative footprint worth feature data generate layer exhibit distinct reuses feature layer layer input due inter layer communication activation data layer fully calculate layer reuse stress chip cache capacity bandwidth however complex network batch feature layer exceed chip cache capacity thereby expose memory bandwidth bottleneck feature calculate alexnet googlenet inc resnet vgg dnn memory footprint feature gradient input memory footprint data structure dnn benchmark pas backward pas calculate feature generate layer accumulate remain buffer memory pas reuse exerts pressure memory capacity bandwidth CPUs substantial performance improvement workload achieve various accelerator gpus FPGAs custom ASIC however CPUs unique advantage workload recent facebook highlight availability flexibility latency cpu platform highly valuable CPUs prefer tighter integration workload business service CPUs effectiveness due additional deployment accelerator consequently inference workload primarily CPUs whereas training target CPUs gpus various service recent simd ISA extension avx enhance cpu performance data parallel compute CPUs attractive dnns vector neural network instruction  avx extension specifically target dnn workload requirement hardware customizations CPUs ISA extension instruction improve CPUs compute capability performance impact memory becomes important zcomp micro architecture zcomp simd instruction vector extension instruction within avx ISA define zcomp instruction zcompl zcomps zcompl load decompress whereas zcomps compress instruction multiple variant data int int etc throughout assume float default data implement framework evaluate tensorflow caffe limited CPUs however propose concept easily extend data variant zcompl zcomps interleave header header zcomp reduce dnn layer memory footprint vector extension micro october columbus usa interleave header zcomps zcompl zcomps simd vector instruction operates data avx instruction generalize target vector width micro architecture zcomps interleave header  zcomps input register reg reg comparison flag CCF zcomps reg reg CCF input vector via reg register operand simd register compress version memory location reg operand register indirect access finally operand CCF immediate comparison input vector comparison flag configure simply zero compress layer data interestingly configure zero input vector useful relu activation layer fuse activation function essentially performs comparison compression instruction zcomps zcomps execution perform wise comparison operation vector reg comparison flag CCF CCF vector comparison creates mask per compression metadata header meanwhile uncompressed vector non zero non negative CCF calculate simply counting comparison vector lane uncompressed output memory concatenate header uncompressed sequentially output memory address reg execution reg remains unchanged reg automatically incremented account amount data plus metadata memory compression zcomps iteratively within loop compress data array reg compress data pointer auto increment feature enables iteration location iteration demonstrate non zero vector assume CCF configure zero lane non zero vector comparison becomes vector constitutes compression metadata header vector active input lane shift non zero header nonzero concatenate output memory address reg addition reg compress data pointer automatically incremented account data header compress data calculate compress data perform population popcount operation comparison vector popcount output non zero nnz  header lane shift reg avx register input zcomps reg reg CCF reg pointer output header reg execution reg memory byte CCF micro architecture zcomps instruction interleave header vector byte amount data byte shift nnz compress non zero another byte increment header byte assume byte addressable memory content reg incremented amount becomes initial content zcomps loop iteratively compress memory address operation zcomps implementation detail regard memory allocation virtual memory data alignment explain zcompl instruction performs decompression zcompl dual zcomps data compress via zcomps retrieve zcompl zcomps zcompl input register reg reg immediate CCF zcompl reg reg operand reg destination register simd register load vector expand operand reg source pointer load vector vector expansion source data arbitrary compression ratio however information regard source data already header zcompl byte correspond header address reg header depends precision vector width nevertheless vector mask wise comparison header zero vector determines active lane micro october columbus usa  akin     reg memory zcompl reg reg header reg pointer input  header reg execution byte byte reg avx register output lane expand micro architecture zcompl instruction interleave header vector non zero popcount operation comparison determines non zero nnz memory consequently zcompl nnz address reg plus header expand reg register active lane zero vector comparison inactive lane lane correspond comparison expand vector reg execution reg expand vector reg automatically incremented account amount compress data plus metadata memory zcomps zcompl iteratively within loop reg address vector header zcomps zcompl interleave header approach aim header compress data memory datasets sparsity guarantee approach efficient however compressibility data completely unknown separately explicitly allocate memory metadata desirable achieve goal variation zcomp instruction header decouples metadata header compress data storage retrieval ISA header zcomps zcompl interleave header variant instead input register operand input header instruction zcomps reg reg reg CCF zcompl reg reg reg additional register reg pointer header skip detailed microarchitecture description  variant zcomps execution differs stage generate header memory location reg instead concatenate compress vector reg reg auto incremented enable continuous iterative operation zcompl execution reading header location reg instead reading reg pointer reg  amount adjust account compress data micro architecture detail execution pipeline zcomps zcompl instruction operation execution operation category logic micro ops memory micro ops detailed critical overview logic ops logic component pipeline cycle correspond micro latency report CPUs zcomps execution vector comparison feature compression usage zero zero operation assume achieve via xor zero operation per lane implement via input gate input gate per lane operation popcount binary vector achieve hierarchical ahead adder popcount pad zero equivalent shift byte per parallel popcount active lane concatenate constitutes pipeline cycle cycle ahead adder popcount pad zero header virtual address reg although logic micro execution cycle latency pipelined implementation achieves instruction per cycle throughput accept instruction cycle memory micro ops variable latency location data llc dram transfer logic latency nevertheless due prefetching parallel execution explain bulk transfer feature zcomp usage becomes  effectively hide latency logic component cycle logic latency variant overall performance almost identical cycle version due  operation prefetching due sequential zcomp compression expansion memory latency issue expansion reading header compress data header compress data sequentially dependent due sequential memory layout access compress data header easily  prefetcher zcomp generate memory micro ops prefetcher trigger subsequent prefetches consequently core vector zcomp reduce dnn layer memory footprint vector extension micro october columbus usa memory request vector flight improves memory parallelism mitigates memory latency workload analyze prefetcher accuracy coverage alignment available sparsity enables compress data header within memory allocation boundary individual vector dataset compress target simd operation header plus data occupy byte cacheline handle regular unaligned instruction cacheline boundary moreover data precision compress vector various byte byte header byte byte header guarantee byte align memory transfer compress vector already exist micro architecture precision alignment issue redundant data transfer zcomp usage dnn processing virtual memory allocation impact zcomp software friendly interface replacement load instruction zcomps zcompl dnn layer data transfer zcomp software worry manage compression metadata data actually compressible however another important aspect memory allocation virtual memory interaction compress data programmer friendly effective feature compression ratio dynamic parameter input therefore allocate memory passing input layer virtual memory allocation somehow compress estimation account compression unless intermediate buffer allocate deallocated input batch performance issue zcomp instruction rely virtual memory allocation aim reduce virtual memory footprint instead goal reduce actual physical memory usage without virtual memory allocation interleave header header variant zcomp instruction interact differently virtual memory estimate compression ratio metadata compress data originally allocate virtual memory interleave header zcomp instruction prefer simd instruction overall compressibility sufficient fully amortize metadata memory allocation remain unchanged whereby metadata interleave compress data without exceed allocation compress data pointer remain within allocate address however memory violation without compressibility compressibility completely unknown option memory allocation option interleave header modify memory allocation intermediate data buffer account uncompressed void zcomps ptr inp vec uint CCF zcompl inp ptr void zcomps ptr inp vec  header uint CCF zcompl inp ptr  header zcomp software interface intrinsics thread thread thread thread memory allocate layer data compress data metadata unused thread thread thread thread unused compress data metadata na√Øve parallelization partition compression zcomp parallelization approach data plus metadata option header variant metadata completely decouple data additional memory allocation header however allocation intermediate data buffer remains unchanged option ensure compress data pointer within allocate address eliminate memory violation possibility technique irrespective additional virtual memory allocate usage physical memory reduce compression ratio reduce physical memory footprint decrease bandwidth capacity usage throughout cache hierarchy dram dnn benchmark profile average sparsity within hence throughout analysis interleave header allocation unchanged compression ratio sufficient amortize metadata zcomp seamlessly header compress data within allocation boundary physical memory footprint software api via intrinsic function effective apply zcomp correspond intrinsic function emit zcomps zcompl instruction intrinsic function interfacing simd ISA extension avx propose intrinsic function zcomps zcompl demonstrates interleave header vector input output pointer pas  construct auto incremented vector pointer reg operand header header pointer  enable auto increment reg propose function software interface intrinsic function compress expand simply replaces vector load operation programmer worry generate mask perform masked load handle metadata maintain pointer compress micro october columbus usa  akin     data however away capability access randomly compression expansion perform vector vector sequentially slice dataset dnn intermediate data buffer communicate layer backward pas bulk reasonable reading intermediate data buffer dnns clearly identify layer propose intrinsic function insert insertion minor dnn software nevertheless easily incorporate framework burden application developer optimization enable transparent performance improvement parallel execution focus compression expansion zcomp perform sequentially vector vector compress vector embed header operating vector completion hence zcomp throughput issue without parallel execution naive parallelization approach aim compress feature contiguous chunk serialization occurs pas compress data pointer thread instead slice feature chunk thread compress portion independent parallelization strategy partition compression thread amount memory compress data pointer isolated within chunk compress sequentially chunk compress parallel independently chunk sustain available cache memory bandwidth throughput mitigate expansion compression parallelization strategy correctly retrieve data similarly expansion chunk expand separately parallel expansion sequential within chunk iterative usage zcomp instruction compiler optimization loop unroll become challenge due  dependency partition compression technique extend address chunk assign thread slice sub iteration multiple zcomp instruction execute target sub sub determines unroll perform however chunk sub fragmentation overall loop unroll minor impact feature feature improve instruction throughput therefore zcomp unroll default unroll perform compiler baseline loop tile reorder traversal baseline data layout hence expansion loop traversal compression compress data explicit data reorganization perform parallel zcomps relu activation layer pragma omp  ptr pragma omp parallel thread slice ptr  num thread pragma omp parallel int vec  tvec load vec  zcomps ptr tvec  relu generate feature propose zcomps instruction parallel zcompl retrieve data compression pragma omp  ptr pragma omp parallel thread slice ptr  num thread pragma omp parallel int vec  tvec zcompl ptr retrieve input tvec retrieve layer feature propose zcompl instruction avx zcomp usage comparison usage efficiency zcomp pre exist compression avx ISA purpose code retrieve sparse feature intrinsic function parallelization express via openmp pragma directive tackle relu activation layer omit straightforward parallel vectorized baseline implementation avx load vector comparison activation demonstrates interleave header zcomps via partition compression parallelization assume relu activation input array target memory slice chunk thread compress data pointer ptr zcomps performs zero LT EZ comparison non positive zero compress shot generic layer perform relu comparison flag  zero zcomps similarly sparse feature compress feature zcomps retrieve zcompl demonstrates zcompl retrieve expand compress data previous thread slice expand xpt comparison flag zcompl expands non zero lane embed header insert zero lane vector apply recent avx   instruction generic instruction apply previous consumes additional instruction register zcomp reduce dnn layer memory footprint vector extension micro october columbus usa parallel avx  relu activation layer   pragma omp  index pragma omp parallel thread slice index  num thread pragma omp parallel int vec  tvec load vec   mask cmp mask tvec  neq uint nnz cnt  mask mask  index mask tvec index nnz cnt header mask relu generate feature avx compress instruction parallel avx  retrieve data compression pragma omp  index pragma omp parallel thread slice index  num thread pragma omp parallel int vec   mask header tvec   mask index uint nnz cnt  mask index nnz cnt retrieve input tvec retrieve layer feature avx expand instruction usage avx   instruction relu activation layer compress data retrieval respectively partition compression parallelization zcomp omit detail additional intrinsic function upon disassemble code snippet avx   extra static scalar vector instruction inside loop additional register zcomp hence iteration avx implementation incur dynamic instruction overhead quantify detail EVALUATIONS simulation methodology evaluation sniper multi core simulator extend version sniper simulator detailed micro architecture model recent ISA extension avx target architecture detail summarize zcomp micro architecture sniper model memory access micro ops zcomp instruction exist compiler cannot generate zcomp instruction methodology avx load instruction instead zcomps zcompl label differently simulator label avx load encounter default behavior overridden instruction treat zcomps zcompl architecture configuration core core avx ghz issue KB private lru MB private SRRIP MB SRRIP prefetcher stride IP noc 2D mesh XY rout cycle hop memory channel ddr GB BW relu activation layer evaluation apply avx compress expand instruction propose zcomp instruction relu activation layer relu layer implementation intel optimize caffe DL framework vectorized avx instruction relu layer typically convolutional fully layer dnns input relu layer various tensor baidu deepbench benchmark suite axis input training inference server suite convolutional fully layer although default deepbench benchmark random initialization uncompressed snapshot evaluate dnn feature average sparsity initialize input tensor accurately capture compression addition baseline avx vectorized version avx vec implement version avx   instruction perform  relu activation avx comp finally implement zcomp version approach methodology described evaluation benchmark sort accord input tensor KBs MBs data traffic core cache hierarchy avx comp zcomp capture available sparsity reduce data traffic average moreover data movement chip dram feature substantial dram traffic compulsory access avx comp zcomp significant reduction average explicit management mask storage retrieval memory location execute dynamic instruction data traffic avx comp zcomp traffic input within benchmark conv infer suggests sweet chip traffic reduction happens feature chip cache compression therefore heavily reduces chip dram access inference benchmark generally batch feature therefore conv infer benchmark feature layer almost cache chip access avx comp exist avx instruction extra operation induce latency overhead decrease instruction throughput feature micro october columbus usa  akin     geomean relu activation layer runtime avx vec avx comp zcomp conv conv infer infer relu activation layer chip data movement avx vec avx comp zcomp relu activation layer chip data movement avx vec avx comp zcomp zcomp benefit exist avx instruction relu activation layer deepbench input alexnet googlenet inc resnet vgg alexnet googlenet inc resnet vgg training inference memory traffic reduction zcomp baseline avx comp zcomp zcomp data traffic reduction network avx comp viable approach saving due reduce data movement amortize instruction overhead however medium feature approach severe performance degradation although runtime mechanism selectively applies avx compression viable approach knowledge hardware platform estimate feature cache memory resident moreover network simply layer feature sufficient data structure prior layer dynamically memory contrast avx comp zcomp performance upside feature average zcomp achieves performance improvement avx vec avx comp outlier zcomp avx vec performance headroom amortize overhead input datasets easily cache runtime correlate reduce chip data traffic zcomp achieves superlinear ups alexnet googlenet inc resnet vgg alexnet googlenet inc resnet vgg training inference speedup zcomp baseline avx comp zcomp zcomp network network evaluation network dnns alexnet googlenet inception resnet resnet vgg implement within tensorflow framework configure intel mkl math kernel library training oxford image image subset imagenet dataset dnns memory stall cycle account overall runtime moreover majority memory access originate layer feature transfer account data traffic feature monitor evaluate network average sparsity overall creates opportunity apply zcomp network evaluation training benchmark batch network resnet inference generally batch prefer reduce latency therefore batch benchmark due batch data movement training dominate feature contrast inference transfer become factor however due lack backward gradient inference feature account allocate zcomp reduce dnn layer memory footprint vector extension micro october columbus usa alexnet googlenet inc resnet vgg compression ratio zcomp cache compression zcomp   zcomp cache compression memory average average data traffic reduction training inference zcomp avx comp inference generate feature discard layer therefore data movement saving target reuse data transfer training feature accumulate layer layer active data potential performance improvement consequently performance improvement zcomp average training inference respectively comparison exist avx compression slowdown benchmark due overhead additional instruction register usage metadata management average avx comp achieves training slowdown inference conclude zcomp reliable approach substantial improvement memory intensive layer overhead opportunity limited comparison cache compression cache compression propose increase effective cache capacity reduce memory traffic cache latency effective compression ratio zcomp cache compression analyze compression ratio random static snapshot dnn workload snapshot average compression ratio achieve zcomp upper bound cache compression ratio  assume compress cache arbitrary byte granularity compress cache regardless physical cache boundary policy approach advanced cache compression architecture skewed compress cache average compression ratio practical tag architecture  combine logical physical cache compression option frequent compression limited FPC algorithm extent frequent compression limited achieve compression ratio latency complexity geometric compression ratio zcomp  architecture unrestricted cache compression architecture  FPC capture zero however attribute compression ratio  overhead compression prefix byte per cache FPC byte per cacheline zcomp  architecture compress cache physical complementary compress harder achieve average compress related multiple prior target dnn sparsity effort focus model prune quantization model optimization reduce memory usage remove reduce precision however correspond portion memory usage prior zero skip activation compression reduce memory transfer effort primarily target hardware accelerator fully customize datapaths memory hierarchy avoid challenge interact automate cache hierarchy virtual memory gpus CDMA leverage feature sparsity reduce cpu gpu data transfer customize dma  performs data fission multiple reduce data movement memory footprint originate feature identify prior virtualized dnn cpu memory virtually expand memory capacity gpus offload feature fuse layer cnn accelerator merges multiple layer minimize intermediate data buffering another prior proposes discard feature compute backward pas reduce overall memory footprint dnn accelerator fpga ASIC processing inmemory propose cpu optimization mainly around improve simd compute capability data layout parallelization CONCLUSIONS memory usage dnn training originates  feature transfer sparse although hardware accelerator leverage sparsity compress feature fully customize datapaths memory hierarchy challenge purpose processor efficiently capture opportunity due automate cache coherence fabric virtual memory boundary introduce zcomp vector ISA extension mitigate layer memory footprint CPUs target bulk sequential feature compression zcomp instruction throughput parallel execution transparent interaction cache hierarchy virtual memory flexible software interface network uncompressed baseline zcomp reduces average memory traffic training inference respectively average performance improvement performance improvement training inference achieve avx vector extension