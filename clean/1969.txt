proposes grain dynamic prune technique cnn inference channel gate accelerator architecture effectively exploit dynamic sparsity intuitively channel gate identifies feature cnn layer contribute classification subset channel compute activation important unlike static network prune remove redundant neuron prior inference channel gate exploit dynamic sparsity specific input structure manner maximize compute saving minimize accuracy loss channel gate learns gate threshold automatically training experimental propose approach significantly network marginal accuracy loss enable performance accuracy channel gate extension cnn accelerator implement prototype quantize resnet model accelerator average speedup imagenet theoretical flop reduction hardware effectively exploit dynamic sparsity expose channel gate CCS CONCEPTS compute methodology neural network computer organization neural network keywords neural network dynamic prune algorithm hardware introduction convolutional neural network cnns demonstrate  accuracy vision related task increasingly adopt application task autonomous robotic manipulation unfortunately cnns highly compute intensive typically demand float operation FLOPs per inference deploy cnns broader application embed mobile setting reduce computational without noticeably sacrifice inference accuracy propose dynamic prune technique channel gate remove ineffectual computation specific input hardware accelerator architecture effectively exploit dynamic sparsity introduce channel gate illustrates intuition channel gate normalize computational sample image decision computation substantially prune channel gate subset input channel output activation intuitively correspond important input feature background prior propose statically prune ineffectual feature magnitude static approach prune network input cannot exploit dynamic input specific characteristic static sparsity introduce exist prune approach reduces constant amount computation regardless input dynamic prune approach recently propose previous approach focus limited dynamic sparsity zero relu activation channel gate aim achieve computation reduction accuracy loss exploit dynamic sparsity prune algorithm training hardware architecture channel gate identify ineffective receptive input feature reduce computation gate portion input channel specifically compute output activation convolutional fully layer perform partial computation subset input channel partial sum strongly correlate sum indicator spatial location important partial sum learnable threshold gate function generates binary decision output activation decision compute convolution channel otherwise simply skip remain computation partial sum normalization activation function cnn inference mostly micro october columbus usa  hua yuan zhou christopher  zhang edward suh height input image width input activation decision lenet mnist dataset height width input image input activation decision resnet cifar dataset input feature decision spatial location decision important computation decision obtain average gate decision CGNet cifar spatial location output channel illustration channel gate subset input channel generate decision prune away unnecessary computation input channel compute bound currently focus minimize computational instead reduce chip memory access achieve accuracy reduce computation gate threshold network cnn channel gate CGNet training training allows channel gate target prune explore performance accuracy experimental channel gate significantly reduce amount computation theart network marginal impact accuracy channel gate reduces computation FLOPs resnet imagenet accuracy loss accuracy loss knowledge distillation channel gate computation accuracy loss translate sparsity prune meaningful performance benefit prune scheme realize efficient software hardware implementation unfortunately exist grain prune scheme irregular sparsity significant exist cnn accelerator contrast channel gate structure sparsity prune contiguous input channel predetermine decision channel gate maintains locality regularity computation data access efficiently implement augment conventional cnn accelerator introduce cnn accelerator architecture channel gate structure sparsity effectively exploit hardware significantly improve performance efficiency cnns propose architecture extends stationary architecture handle dense sparse computation channel gate network challenge accelerate sparse computation efficiently load convolutional feature propose architecture banking scheme allows parallel access activation arbitrary convolutional address challenge architecture exploit loop tile reorder strategy dense sparse computation maximize processing utilization overall throughput moreover propose architecture effectively regular cnns without prune channel gate network evaluate channel gate accelerator implement rtl prototype quantize resnet without channel gate target TSMC standard library ASIC implementation demonstrate channel gate hardware moderate overhead sparsity introduce channel gate efficiently exploit cnn inference actual speedup accelerator theoretical computation reduction CGNet accelerator achieves average speedup baseline accelerator imagenet channel gate reduces computation flop contribution introduce novel dynamic grain prune approach significantly reduce computation cnn selectively prune computation output activation propose unified accelerator architecture efficiently execute regular cnns channel gate network  accelerator prototype rtl experimentally demonstrate benefit channel gate resnet cifar imagenet cnns channel  introduces channel gate neural network CGNet CGNet neuron generalize structure convolutional layer equip channel gate although CGNet within scope convolutional neural network CGNet extend multi layer perceptrons neuron input channel split contains channel remain channel boost performance cnn accelerator dynamic grain channel gate micro october columbus usa structural parameter CGNet parameter description input channel filter conv layer output channel conv layer height output feature conv layer width output feature conv layer dot channel correspond gate channel channel contribute output neuron importantly dot correspond compute amount computation gate reduce channel gate CGNet cnns neuron organize tensor without loss generality input feature output feature  respectively summarizes parameter typical cnn convolution batch normalization BN activation function output feature BN apply channel gate split input feature statically along channel dimension tensor consists input channel channel  similarly associate  decomposition partial sum fed gate generate binary decision tensor binary indicates gate channel output activation index inference CGNet frequency execution refer grey conditional gate activation output activation combination output conditional index component tensor rank  BN BN otherwise propose CGNet observation partial sum psum predictor sum  reduce computation subset output activation skip approximate activation psums pearson correlation coefficient linear correlation psum  average bias ignore batch normalization CGNet neuron pearson correlation coefficient convolutional layer random sample resnet respectively psum  convolutional layer moderately correlate gate function minimize computational gate function output activation conditional gate relu activation baseline network partial sum magnitude important magnitude gate remain channel output activation partial sum activation negative partial sum likely zeroed relu contribution output introduce learnable threshold per output channel broadcast gate function define heaviside function comparison implement otherwise activation conditional satisfies batch normalization without shift normalize batch normalization normalizes input feature variance var inference eliminate extra parameter computational merge batch normalization psums gate function merge gate contains threshold performs pointwise comparison psums threshold training CGNet  learnable essential minimize accuracy degradation prune reformulate output channel gate without expression equation instead output gate binary mask tensor rank express combine addition operator differentiable gate function addition discus important mechanism reduce computation minimal accuracy loss approximate non differentiable gate function induce sparsity conditional subset channel input micro october columbus usa  hua yuan zhou christopher  zhang edward suh parameter CGNet accelerator parameter explanation multiplier per PE PEs per PE PE accelerator non differentiable gate function heaviside function differentiable gradient batch normalize partial sum BN threshold cannot compute directly propose approximate gate smooth function differentiable respect backward propagation propose approximate gate backward propagation relu activation hyperparameter tune adjust difference approximate function gate implement custom operator mxnet batch normalize partial sum input generate gate decision sparsity induce mechanism batch normalize input gate standard normal distribution therefore FLOPs prune increase monotonically reduce computation equivalent motivate reduce computational training target denote entire network difference channel specific loss function factor tune achieve flop reduction gate function differentiable per channel threshold optimize sgd minimize accuracy loss actual gate threshold automatically channel layer channel selection manually channel instead simply channel fix channel selection scheme network training channel improve accuracy  channel shuffle equalize importance channel channel shuffle improve accuracy fix channel selection scheme imagenet  accelerator architecture propose unified architecture accelerate ordinary cnns channel gate network propose architecture extends stationary architecture handle conditional without significant baseline architecture executes regular cnns hereafter refer computation dense convolution conditional sample feature convolution architecture overview overall architecture accelerator communicates cpu soc bus chip dram ddr channel host cpu issue command layer description accelerator channel gate network model chip dram execution layer prefetched chip global buffer local buffer PE array maximize data locality reuse output feature intermediate layer buffer chip minimize chip data transfer chip feature split dense sparse buffer buffering apply overlap computation chip transfer batch normalization layer activation function pool layer combine previous convolutional fully layer depicts propose dense sparse accelerator architecture adopt widely stationary architecture  cnn architecture dense convolution baseline propose dense sparse architecture capable accelerate regular cnns  hardware platform baseline accelerator exploit parallelism output channel input channel spatial dimension component baseline architecture convolution data fetch feature parameter dense sparse architecture convolution consists processing PEs PE contains multiplier network convolutional filter maximize PE utilization network contains convolutional filter technique optimal PE computes convolution input channel per cycle assume convolution input channel mapped PEs baseline architecture PE array computes output activation spatial location output channel minimum input channel layer network feature chip feature buffer minimize chip memory access chip memory access fetch maximize efficiency accelerator stationary architecture adopt kernel exactly chip memory leverage parallelism output channel dimension PE apply kernel input feature generate output channel parallel data fetch broadcast input buffer attach PE exploit feature reuse cnn accelerator specialized buffer buffer global feature buffer exploit data locality reuse feature adder accumulates partial sum PE per PE psum buffer entire feature output channel obtain feature writes output channel global feature buffer baseline architecture fully utilize PEs across layer achieve  throughput respect PEs boost performance cnn accelerator dynamic grain channel gate micro october columbus usa output channel sample feature convolution challenge accelerate sparse computation conditional unlike static grain prune approach compression channel gate friendly hardware acceleration preserve regular parallelism input channel dimension spatial dimension per filter illustrate output pixel CGNet entirely skip conditional performs sample feature convolution remain input channel hence induced sparsity CGNet structure nonetheless exists dynamic behavior across output channel sample feature convolution spatial location output activation conditional compute multiple output channel parallel duplicate feature buffer spatial location feature access concurrently instead compute output channel sequentially explore parallelism dimension avoid overhead channel gate complicates fetch feature access fix stride access irregular access receptive within feature exploit specialized banking scheme memory bandwidth access feature moreover imbalance output channel limit benefit exploit parallelism output channel dimension micro architecture activation output channel conditional drain sample feature convolution pipeline nontrivial overhead individual output channel separately mitigate pipelining overhead execute conditional multiple output channel architectural sample feature convolution baseline architecture execute channel gate network compute sample feature convolution conditional extend baseline architecture introduce hardware module gate function comparator partial sum threshold generate binary decision conditional compute specific output activation decision fix task queue width height output feature sample feature convolution conditional reuse array PEs baseline architecture leverage parallelism within output channel avoid unpredictable unbalanced across output channel conditional PE computes convolution input channel per cycle convolution input channel mapped PE however instead exploit parallelism across multiple output channel PEs input channel parallel therefore entire PE array computes partial sum output activation input channel partial sum psum buffer obtain output activation sample feature convolution PE array output activation irregular activation conditional irregular processing irregular access input feature irregular feature access imply convolution necessarily overlap consecutive cycle buffer effective data fetch load input feature buffer directly conditional load arbitrary convolution PE correspond accelerator crossbar connects sparse feature input PE irregular access challenge reading feature chip memory fully utilize PE array sample feature convolution fetch arbitrary input feature per cycle entire feature onchip memory fetch input sample feature convolution consume multiple cycle starve convolution enable cycle access arbitrary propose specialized banking scheme spatial dimension specifically partition feature fashion activation sparse feature partition spatial dimension propose feature layout assume feature partition spatial dimension throughput reading activation cycle activation chip memory input access parallel feature ID memory activation although capacity chip memory remains baseline architecture memory increase prototype implementation chip memory width multiple output channel access parallel activation along channel dimension memory addition PE input channel cycle sample feature convolution partition global feature buffer along channel dimension access chunk input channel parallel micro october columbus usa  hua yuan zhou christopher  zhang edward suh psum buf  SPS  task queue buf   task gen task   SPS    psum buf SPS  mux buf buf  array PE PE PE PE PE PE  mux fmap dense sparse architecture   sparse dense sparse feature buf sparse feature buf dense feature buf dense feature buf   dense feature buf dense feature buf     host cpu dram soc bus decoder output inst chip buffer overall architecture dense sparse accelerator detail memory layout feature assume channel gate accelerator architecture exploration dense sparse architecture explore dense sparse architecture parallelize output input channel dense convolution input channel sample feature convolution differently objective maximize performance data reuse architecture  width feature filter channel layer average activation conditional respectively latency formulate latency  dense convolution MR  sample feature convolution overhead drain sample feature convolution pipeline significant PD pipeline depth sample feature convolution overhead  PD calculates pipeline drain within layer stationary architecture memory access feature evaluate benefit data reuse memory access feature characterize mem  dense convolution sample feature convolution impose resource constraint exploration maximum multiplier chip buffer multiplier assume activation partial sum spatial index task chip buffer max buf feature buf psum buf task queue assume feature memory access latency cycle infeasible feasible optimal exploration CGNet accelerator assume resnet architecture imagenet dataset exploration maximum multiplier storage MB available resource xilinx zynq device  subset explore depict parameter choice baseline dense sparse architecture another alone convolution handle convolutional layer resnet model evaluation algorithm evaluation evaluate  cifar imagenet datasets demonstrate channel gate significantly reduce compute minimal impact accuracy network architecture apply channel gate modify resnet vgg cifar modify resnet baseline error cifar  imagenet alexnet resnet baseline network uniform layer  resnet variant architecture resnet architecture imagenet filter convolutional layer output fully layer boost performance cnn accelerator dynamic grain channel gate micro october columbus usa flop reduction error scp shuffle CGNet scp shuffle CGNet cifar accuracy flop reduction cifar CGNet CGNet model parameter target threshold CGNet model resnet vgg respectively network model target threshold error flop reduction resnet baseline shuffle CGNet CGNet vgg baseline CGNet CGNet accuracy computational FLOPs CGNet resnet static channel prune scp resnet grouped convolution channel shuffle  technique propose shufflenet grouped convolution shuffle static prune approach CGNet achieves flop saving accuracy scp shuffle accuracy flop reduction CGNet model cifar CGNet reduce computation almost accuracy degradation ofthe architecture CGNet reduces FLOPs accuracy shuffle cifar CGNet CGNet CGNet model target threshold multiple entry CGNet model baseline network dataset CGNet prior alexnet resnet imagenet CGNet outperforms prune technique offering accuracy flop reduction comparison  alexnet error baseline alexnet report CGNet  respectively CGNet achieves speedup error accuracy whereas  report speedup error accuracy static channel prune remove fix channel statically prune model scratch  error  report error error CGNet CGNet alexnet respectively sample flop reduction cifar easy sample refer image flop reduction sample refer image flop reduction comparison accuracy flop reduction prune model imagenet CGNet CGNet model parameter target threshold CGNet model CGNet alexnet respectively CGNet resnet respectively model accu flop reduction error baseline alexnet caffe alexnet baseline  baseline  CGNet CGNet alexnet model accu flop reduction error baseline network slimming discrimination aware prune feature boost suppression shuffle CGNet CGNet resnet CGNet prune technique resnet imagenet shuffle outperforms network slimming discrimination aware channel prune resnet feature boost suppression FBS channel dynamic prune approach achieves flop static prune approach channel gate simpler FBS achieves accuracy slightly flop reduction CGNet brevity apply knowledge distillation CGNet achieves flop accuracy loss sample max min flop reduction category exists difference flop reduction sample demonstrates CGNet prune adaptively sample micro october columbus usa  hua yuan zhou christopher  zhang edward suh error quantize  cifar imagenet CGNet model cifar imagenet refer CGNet dataset model activation error cifar CGNet float float fix fix fix fix imagenet CGNet float float fix fix quantization improve efficiency performance accelerator widely quantization configuration quantizes activation avoid significant accuracy activation adopt pact quantization scheme introduces layer wise trainable clip threshold pact clip activation perform linear quantization within quantize activation express integer function  pact regularization minimizes quantization therefore improve resolution regularization adopt prevent model overfitting directly apply linear quantization max max quantize channel gate network cifar imagenet accuracy degradation float counterpart respectively hardware evaluation evaluate performance improvement hardware overhead apply channel gate implement hardware prototype target TSMC standard threshold voltage transistor standard library SRAMs generate SRAM compiler apply channel gate resnet model cifar imagenet resnet model channel gate achieve flop reduction average batch image baseline resnet model cifar imagenet respectively refer dense sparse architecture propose CGNet xcel baseline architecture described methodology implement baseline CGNet accelerator rtl generate HLS fpga synthesis obtain resource utilization route useful identify resource overhead generate rtl simulation trace rtl source file synopsys compiler DC standard library SRAM information statistic signal switch activity obtain timing estimation gate netlist switch activity interchange format  format rtl simulator  synopsys DC  standard library SRAM compiler synopsys PT netlist   latency timing cycle accurate performance model vcd HLS generate rtl CGNet resnet rtl simulation comp wgt buf comp wgt buf LD wgt buf comp wgt buf comp wgt buf comp wgt buf LD wgt buf comp wgt buf comp wgt buf LD wgt buf LD wgt buf LD wgt buf LD wgt buf residual residual layer load output channel compute output channel timing diagram residual compute load coarse grain pipeline synopsys DC gate netlist rtl signal trace synopsys primetime PT analysis performance estimate combination rtl simulation accelerator DRAMSim chip access performance model extract memory trace compute accelerator cycle accurate rtl simulation DRAMSim dram latency estimate memory access combine obtain overall execution simulate ddr channel mhz channel contains rank capacity simulated dram 8GB dram parameter verify verilog timing model micron timing diagram compute residual resnet applies baseline CGNet xcel accelerator twice residual chip storage accelerator load output channel input channel conditional subset output activation accelerator amount chip memory baseline CGNet extend reduce memory access prune entire output channel apply buffering baseline  computation load perform parallel commonly cnn accelerator convolutional layer residual resnet model compute parallel performance performance baseline CGNet xcel theoretical execution cifar imagenet baseline  baseline CGNet xcel average batch image obtain theoretical execution CGNet CGNet theoretical calculate multiplication boost performance cnn accelerator dynamic grain channel gate micro october columbus usa cifar imagenet execution baseline CGNet xcel CGNet theoretical comparison baseline CGNet xcel theoretical performance cifar imagenet blk blk blk blk blk blk blk blk execution baseline CGNet xcel CGNet theoretical cifar blk blk blk blk blk blk blk blk execution baseline CGNet xcel CGNet theoretical imagenet execution residual blk blk blk blk blk blk blk blk execution CGNet xcel mem CGNet xcel compute cifar blk blk blk blk blk blk blk blk execution CGNet xcel mem CGNet xcel compute imagenet compute memory residual datasets input image multiplier CGNet xcel theoretical execution assumes memory overhead PE utilization CGNet xcel significant performance improvement baseline cifar imagenet moreover performance CGNet xcel imagenet theoretical optimal indicates CGNet xcel architecture effective leverage sparsity channel gate cifar noticeable performance gap CGNet xcel CGNet theoretical detail execution residual CGNet xcel achieves optimal performance residual imagenet cifar fpga resource utilization baseline CGNet xcel baseline CGNet xcel lut FF dsp bram lut FF dsp bram overhead ASIC comparison baseline CGNet xcel baseline CGNet xcel capacity KB capacity KB SRAM dense sparse feature dense sparse combinational  overhead CGNet xcel achieves optimal performance residual performance gap CGNet xcel  wider later residual residual CGNet xcel marginal performance improvement understand source performance gap CGNet xcel theoretical limit breakdown execution residual compute memory access residual compute bound cifar memory bound residual explains CGNet xcel cannot improve performance blk blk channel gate significantly reduce amount computation inference reduce memory access later residual resnet become memory bound CGNet model imagenet computation cifar feature flop reduction ratio channel gate residual  memory bound understand overhead propose CGNet architecture baseline fpga resource usage  report target xilinx zynq device  extension CGNet xcel bram usage due complex memory banking CGNet xcel increase lut flip flop usage extra multiplexing deeper pipeline overhead DSPs CGNet xcel PEs baseline micro october columbus usa  hua yuan zhou christopher  zhang edward suh performance comparison platform imagenet report cpu thermal platform ASIC intel nvidia gtx baseline CGNet xcel frequency mhz watt throughput fps img ASIC CGNet NM dataset freq throughput img cifar mhz fps imagenet fps dsp overhead additional index calculation sparse computation ASIC comparison baseline CGNet xcel baseline ASIC implementation CGNet xcel noticeably increase combinational non combinational logic however dominate SRAMs baseline CGNet xcel CGNet xcel overhead capacity SRAMs implement feature buffer SRAM capacity CGNet xcel significantly baseline SRAM consume complex memory banking scheme banking scheme wider multiplexer increase pipeline depth consumption combinational non combinational logic comparison compute platform throughput comparison platform CGNet xcel baseline cpu gpu cpu gpu perform regular resnet inference batch baseline CGNet xcel outperform cpu throughput magnitude efficient cpu gpu baseline CGNet accelerator efficient respectively baseline CGNet architecture noticeable impact frequency consumption CGNet xcel baseline throughput CGNet xcel efficient baseline bandwidth compute constrain platform CGNet xcel efficiently exploit reduce computation channel gate significantly improve performance cnn inference performance improvement CGNet xcel limited cnn performance constrain memory bandwidth layer resnet channel gate  cifar CGNet xcel particularly blk blk blk blk blk blk blk blk execution CGNet NM CGNet NM theoretical execution breakdown blk blk blk blk blk blk blk blk execution CGNet NM mem CGNet NM compute compute memory residual CGNet NM cifar flop reduction speedup CGNet xcel CGNet pim performance cifar attractive platform memory bandwidth limited compute resource memory memory compute platform hybrid memory cube HMC exploit 3D stack technology computation perform logic stack dram dram traditional DRAMs HMC internal memory bandwidth computational resource logic silicon vias TSVs however portion logic occupy dram peripheral serial link vault controller interconnect TSVs previous estimate peripheral occupy logic HMC addition limited budget constrains frequency computational resource logic applicability CGNet across multiple platform evaluate memory NM compute scenario accelerator logic HMC conservatively target frequency mhz budget  cycle HMC simulator estimate memory latency simulate 4GB HMC device 5GHz frequency vault per vault configuration file  assume accelerator communicates dram stack width lane serial link logic constraint variant CGNet xcel pipeline initiation interval II multiplier refer configuration boost performance cnn accelerator dynamic grain channel gate micro october columbus usa CGNet NM ASIC CGNet NM normal CGNet xcel  accelerator consumption performance CGNet NM cifar execution breakdown breakdown compute memory memory bandwidth frequency compute resource residual slightly memory bound performance CGNet NM theoretical optimal CGNet constraint apply cnns multiple platform performance accuracy CGNet performance accuracy tradeoff memory bandwidth flop reduction ratio channel gate tune training flop reduction channel gate prune away computation accuracy performance normal CGNet xcel CGNet NM flop reduction ratio cifar memory bandwidth performance CGNet NM almost linearly computation prune performance improvement CGNet xcel saturate earlier due memory bandwidth limit CGNet xcel performance compute bound network resnet imagenet CGNet xcel significantly improve cnn performance traditional platform related recent proposal statically prune unimportant filter feature static prune technique identify ineffective channel filter feature examine magnitude activation channel prune ineffective subset channel model prune model retrain mitigate accuracy loss prune prune retrain iteratively compress model reduce computation however reduce amount computation input cannot exploit dynamic input specific sparsity contrast channel gate achieves performance accuracy identify unimportant input reduce computation channel gate complementary static prune approach exploit input dependent sparsity feature  introduces spatially adaptive computation technique residual network adjust residual input feature propose reinforcement recurrent neural network decision prune output channel approach additional extra computation decision comparison channel gate generates grain prune decision without incur overhead computation cnvlutin  propose dynamically prune  pixel relu activation pixel magnitude input feature inference zero prune training effort strictly rely sparsity output feature layer apply relu activation  extends propose predict relu zero partial sum subset input channel partial sum prune decision approach channel gate enables aggressive prune scheme identify unimportant input feature target zero specific relu activation function importantly channel gate introduces training gate policy threshold critical achieve accuracy degradation inference approach  significant accuracy loss target flop reduction predictive execution propose predicts zero execute significant however information significant batch normalization moreover performance gain limited sparsity zero baseline network contrast channel gate exploit various sparsity target threshold fusion proposes reduce computational width dynamically applicable CGNet CIRCNN PERMDNN structure matrix fourier transform permutation achieve hardware friendly structure sparsity research focus static sparsity although potentially complementary approach reduce CGNet conclusion introduces grain dynamic prune technique cnn channel gate reduce computational cnn inference along hardware accelerator architecture efficiently realize dynamic prune experimental channel gate significantly reduce FLOPs minimal accuracy loss flop reduction without accuracy loss cifar flop reduction accuracy degradation imagenet resnet proposes unified dense sparse accelerator dense sparse computation mapped onto processing propose architecture achieve optimal performance improvement channel gate overall optimize cnn algorithm hardware architecture CGNet architecture performance gain accuracy degradation prune technique