convolutional neural network cnns emerge powerful image processing recent machine reduce cnns compute data volume exploit  actively transform zero feature filter previous semi sparse architecture exploit sparsity feature filter recent fully sparse architecture sparse cnn scnn exploit sparsity improve performance dense architecture however sparse vector vector dot primitive sparse cnns inefficient representation adopt scnn dot access non zero sparse vector inner scnn avoids inner perform cartesian capture relevant multiplication however scnn approach incurs considerable overhead applicable  stride convolution exploit reuse sparse cnns fundamentally systematic load imbalance address scnn propose SparTen achieves efficient inner native sparse execution memory storage tackle load imbalance SparTen employ software scheme greedy balance filter density via variant software filter density software hardware hybrid finer grain density simulation average SparTen performs dense architecture sparse architecture scnn respectively fpga implementation SparTen performs dense architecture sparse architecture respectively CCS CONCEPTS computer organization neural network purpose keywords convolutional neural network sparse tensor accelerator introduction advance convolutional neural network cnns highly accurate recognition image data cnns comprise layer employ numerous filter identify feature compute intermediate data compute memory bandwidth concern performance reduce compute data volume previous exploit naturally zero feature output cnn layer input layer due rectifier linear relu convert negative zero scheme exploit sparsity zero feature filter unchanged target sparsity recent actively prune filter threshold zero prune retrain ensure accuracy maintain sparsity memory reduction compute reduction zero tensor independent quadratic compute reduction however architecture dense cnns inefficient sparse tensor computation gpgpu SIMT systolic array respectively divergence irregularity sparse microarchitectures advance goal avoid transfer zero feature filter avoid compute zero feature filter maintain accuracy achieve efficient sparse computation proposal handle sparsity feature achieve partial compute data reduction EIE target sparsity filter feature fully layer EIE performance equivalent scheme discard zero filter incurs micro october columbus usa        goal applicable goal cambricon cnvlutin cambricon scnn SparTen avoid transfer zero avoid compute zero maintain accuracy efficient fully sparse computation compute idle due discard cambricon employ  prune regularity filter sparsity avoid zero affect accuracy elaborate retrieves zero feature discard zero computation serial scheme skip zero compute leverage booth encode transfer zero incur chip SRAM memory bandwidth incur load imbalance sparse scheme issue semi sparse scheme summarize contrast sparse cnn scnn fully sparse scheme target sparsity filter feature however shortcoming inefficient sparse microarchitecture sparse vector vector dot primitive sparse cnns nonzero sparse vector access inner implement inner straightforward inner challenge scnn avoids inner filter slide height width dimension feature convolution non zero filter channel dimension non zero feature channel inner multiplication scnn performs cartesian multiplication parallel without explicit inner however concurrent multiplication unrelated destine output multiplier destine output consequently cartesian approach incurs overhead numerous implicit barrier channel incur barrier output bandwidth route partial sum crossbar address calculator adder multiplier compute underutilization input border filter filter buffering filter approach applicable non stride convolution cnns resnets non convolutional neural network load imbalance scnn employ eyeriss input stationary approach input feature processing PEs capture input reuse across filter broadcast capture filter reuse across input reuse however input inevitably sparsity filter PEs denser lag sparser filter broadcast buffering later filter address systematic load imbalance load imbalance alleviate PE independently fetch filter loss filter reuse via broadcast alternative filter broadcasting input filter stationary incurs EIE input buffering absorbs temporary imbalance across filter address systematic imbalance fundamental reuse imbalance tension address previous scheme sparse blas library  gpus sparse linear algebra performance compute hpc however hpc hardware software optimization sparse linear algebra address shortcoming propose SparTen fully sparse tensor accelerator architecture contribution SparTen microarchitecture achieves efficient inner sparsity instead avoid inner scnn SparTen native sparse execution memory storage contrast previous architecture scnn semi sparse whereas scnn inefficient SparTen confines output multiplier distributes output multiplier parallelism SparTen avoids scnn cartesian overhead SparTen employ cluster asynchronous compute handle sparsity SparTen sparse linear algebra accelerator applicable sparse cnns convolutional stride non convolutional neural network dnns sparse hpc focus unlike scnn address load imbalance across computer cluster input stationary filter stationary approach equivalent capture reuse SparTen employ latter filter recognition observation SparTen employ offline software scheme greedy balance GB variant software GB software hardware hybrid GB GB filter density granularity filter whereas GB finer granularity balance GB variant occurs software shuffle filter output within cluster GB filter granularity implies static offline  rearrange layer suffices GB  combine CC shuffle criterion completely GB filter density whereas improve systolic utilization CC merges sparse filter jigsaw fitting filter filter non zero tensor unlike CC GB finer granularity multi stage permutation network within cluster dynamically  finer grain partial sum appropriate output sum within cluster unlike scnn bandwidth bandwidth network route per partial sum assume compute cluster partial sum compute sparsity SparTen efficiently achieves filter reuse load balance simulation average SparTen performs dense architecture sparse architecture scnn respectively SparTen achieves compute memory architecture fpga implementation SparTen performs SparTen sparse tensor accelerator convolutional neural network micro october columbus usa neural network dense architecture sparse architecture respectively sparse cnns mention layer cnn filter extract feature layer output feature layer input layer input tensor height width depth channel filter previous layer filter tensor typically height width channel input layer output dimension assume filter layer output scalar tensor input filter express vector vector multiplication output dot filter  stride along height width dimension dimensional convolution  slide along channel dimension layer dense cnn  assume input output channel ignore boundary dense cnn filter channel reuse input correspond channel similarly input reuse filter sparse cnns dramatically amount compute data avoid zero reuse remain reuse decrease however implement sparse vector vector multiplication primitive sparse cnns challenge scnn cnvlutin others exploit  sparsity feature filter scnn extends sparsity scnn employ cluster asynchronous processing PEs comprise multiplier accumulator PE tensor input per input stationary approach filter broadcast PEs relevant tensor input scnn capture filter reuse across input input reuse across filter ensure utilization multiplier accumulator PE operates filter sparse vector vector multiplication csr scnn employ hpc compress sparse csr representation sparse tensor pointer non zero csr tensor non zero tensor access correspond inner csr incrementally tensor pointer pointer tensor upon access correspond previous non zero tensor input tensor implement inner straightforward inner csr csc inefficient scnn avoids implement inner via observation filter slide height width dimension feature convolution non zero filter channel dimension non zero feature channel inner multiplication scnn performs cartesian multiplication parallel without explicit inner strategy guarantee unnecessary multiplication zero avoid inner PE appropriate output accumulator buffer output tensor compute dense tensor zero apply rectify linear relu convert negative zero output tensor convert sparse csr format input tile along dimension PE assign tile limited accumulator buffer tile output inter PE communication handle input tile contribute output tile boundary scnn author argue instead tile input tile output incur replication input contribute output tile cartesian unnecessary input filter however extract tile dimension variable due sparsity scnn overhead scnn cartesian strategy however concurrent multiplication unrelated destine output multiplier destine output consequently approach incurs considerable overhead micro october columbus usa        unrelated rout sum accumulator buffer rout destine partial sum multiplier rout bandwidth multiplier adder scnn crossbar compute address partial sum calculation amount compute difference coordinate input filter computation adder adder multiplier per PE latency overhead hidden complexity cartesian strategy assumes filter input stride convolution stride convolution filter input filter input fully layer rnns lstms MLPs cartesian strategy restricts scnn applicability cnns stride convolution scnn report intra PE underutilization due nonzero input tile filter multiplier array PE sparsity filter filter despite operating multiple filter mention filter alleviate accumulator buffer fourth filter tensor broadcast PEs capture filter reuse unrelated output buffering consequently limited filter buffering PEs implies numerous global inter PE barrier channel input feature incur barrier output barrier expose inter PE load imbalance due feature sparsity truncate input tile input due input tile input tile laterally invert remainder input tile decrease imbalance reduce intra PE utilization reuse specific scnn input tile capture filter reuse across input scnn input PEs filter broadcast PEs however sparsity input induces systematic load imbalance PEs PE denser repeatedly longer filter PE sparser amount buffering address imbalance filter instead input imbalance broadcasting filter PE independently fetch filter balance PEs across filter loss filter reuse scnn address fundamental tension reuse load imbalance due scnn report significant PE idle barrier  recall SparTen contribution SparTen achieves efficient inner native sparse execution memory storage instead avoid inner scnn avoid scnn execution overhead due cartesian approach SparTen confines output multiplier distributes output efficient sparse vector vector multiplication multiplier parallelism address load imbalance achieve reuse SparTen employ offline software scheme greedy balance filter density via variant software filter density software hardware hybrid finer grain density efficient inner instead hpc csr csc representation SparTen bitmask representation sparse tensor tuple mask SparseMap non zero SparseMap non zero otherwise implementation tensor broken chunk  correspond variable non zero data vector vector dot computation tensor processing implement inner therefore sparse vector vector dot mask representation efficient inner non zero tensor access correspond achieve  tensor   compute multiplies correspond tensor zero topmost identify topmost  identify priority encoder priority decrease prefix sum circuit input tensor  newly identify tensor non zero address offset access upon compute accumulates partial sum locally fortunately prefix sum priority encoder efficient implementation lookahead logarithmic delay SparseMap width instead ripple linear delay recall inner csr csc involves inefficient non zero counting non zero obtain address offset surprisingly mask representation efficient addition compute efficient efficiency SparTen sparse tensor accelerator convolutional neural network micro october columbus usa representation depends sparsity hpc sparse data extremely sparse pointer representation efficient non zero contrast sparsity machine model non zero consequently mask representation analysis assume nonzero pointer representation logn whereas mask representation additional pointer manage variable sparse data representation pointer scheme logn implies logn cnns filter pointer scheme whereas around csc csr format zero encode compress pointer however shorter achieve compression incur redundant pointer zero longer representation efficiency sparsity zero distribution redundant zero compute redundant pointer mask representation avoids overhead data comprises array tuples chunk SparseMap pointer chunk non zero data array chunk filter feature array layer filter input output non zero variable layout involve non feature layer feature output concurrently cluster compute SparTen contiguous layout output feature chunk  serialize cluster writes memory extreme chunk  another chunk fragmentation grain memory management issue issue arises sparse architecture exploit sparsity feature SparTen compromise contiguously cluster output apart cluster output memory ensure cluster serialize memory cluster fragmentation management issue alleviate cluster contiguous sub tensor output slice dimension channel axis intact data pointer SparseMap chunk mask vector contiguous cluster adequate allocate average pad watermark fallback additional allocation background cluster data format ensures  input tensor filter contiguous compute access pad  SparTen microarchitecture channel non multiple chunk extreme initial channel input image dense zero input format SparTen representation simply mask pad pointer dense data pad finally scnn cnvlutin cambricon csr EIE variant hpc compress sparse representation csc cambricon architecture mask representation cambricon mask coarsely prune filter construct offline software sparse representation feature retrieves zero feature instead  skip zero feature computation contrast SparTen native sparse execution retrieves consumes non zero SparTen microarchitecture envision SparTen sparse tensor accelerator attach cpu memory bus accelerator expose blas interface matrix vector matrix matrix multiplication simplification interface allows incremental construction vector handle non contiguous layout tensor effectively tensor linearize vector matrix vector matrix matrix operation sparse computation efficient simd SIMT systolic organization SparTen employ asynchronous compute compute comprise multiplier accumulator inner circuitry buffer input output performs sparse vector vector dot cluster perform sparse matrix vector multiplication sparse output vector representation SparTen implementation typically cluster compute bandwidth accumulate feature SparTen cluster output independently efficiency compute employ machine instead program capture filter reuse across input input reuse across filter compute local buffer micro october columbus usa        filter chunk reuse across multiple input chunk computation proceeds chunk chunk memory transfer granularity cnn layer cpu instructs compute cluster fetch chunk filter  non zero data cpu issue fetch input chunk chip chip memory broadcast cluster compute computes filter dot inner circuitry compute locally accumulates output partial sum input channel remain cpu issue input chunk compute output relu operation zero consecutive compute consecutive output channel output collector compute output discard zero sparse output tensor SparseMap non zero data broken chunk channel multiple output collector pad SparseMap zero compute cpu pad multiple permute network explain later illustrates hardware efficient conversion per zero detection  gate generates SparseMap output compact eliminate interleave zero compact easily achieve invert prefix sum zero SparseMap shift non zero accordingly illustrates operation sixth zero shift assumes fourth already shift unlike issue queue compaction issue processor output compaction output parallel assume compute cluster filter compute sparsity cluster output output mask data array memory pointer cpu cluster return non zero output cpu increment output array pointer output ensure cluster contiguous sub tensor output cluster memory cpu issue correspond input sub tensor filter cluster capture input reuse filter reuse SparTen assigns output per compute unlike scnn communication cpu request compute request buffering output buffering achieve filter reuse across multiple input filter chunk entire filter reuse filter chunk across multiple input implies buffering correspond incomplete output completion processing filter input fortunately entire input filter chunk output buffer modest output per compute however buffering cannot address systematic load imbalance across filter address hide memory conversion sparse representation latency input filter output buffer later input chunk fetch broadcast previous output data processing input chunk output data input filter mask SparseMap byte data output data byte cluster buffering byte input byte filter byte output buffering KB per multiplier comparison scnn KB per PE accumulator buffering KB per multiplier SparTen buffering increase due load balance scheme SparTen closer scnn buffer capacity recall scnn cartesian strategy concurrent multiplication unrelated destine output multiplier destine output approach incurs numerous barrier bandwidth crossbar address calculator per PE cannot handle non stride convolution non convolutional dnns  PEs input border filter buffering filter contrast SparTen leverage efficient inner confine output multiplier distribute output multiplier parallelism consequently SparTen avoids scnn SparTen inner output compute SparTen applies convolution stride non convolutional dnns avoids scnn SparTen compute output address calculation chunk input buffering hence implicit barrier input broadcast output chunk capture channel filter achieve compute utilization without increase output buffering channel contribute output SparTen assigns output per compute underutilization due input tile greedy balance filter cluster compute inevitably sparsity filter input denser filter lag sparser filter input broadcast broadcast impose implicit barrier across compute expose load imbalance scnn across collection layer filter resnet utilization SparTen sparse tensor accelerator convolutional neural network micro october columbus usa greedy balance GB buffering later input address systematic load imbalance alleviate compute independently fetch loss input reuse reuse imbalance tension fundamental achieve reuse load balance propose offline load balance approach greedy balance GB data dependent estimate feature filter non zero load balance solely density filter effective proxy filter execution option sort offline layer filter density filter within cluster density input compute online input stationary approach amenable offline processing SparTen employ filter stationary approach offline filter processing amortize numerous input image GB variant software GB  hybrid GB GB simply sort filter layer filter density filter assign cluster density however sort shuffle filter output channel axis filter layer GB statically  layer software image input offline processing proceeds layer layer  layer previous layer sort layer filter load balance filter density varies across chunk GB incurs  load imbalance execution recall chunk impose implicit barrier GB address issue offline sort filter per chunk basis however per chunk sort implies partial sum output chunk filter shuffle shuffle cannot fix statically accordingly GB employ multi stage permutation network  partial sum chunk appropriate output sum however filter layer distribute across SparTen cluster parallelism per chunk sort filter layer global network cluster restrict sort filter within cluster local per cluster network filter density within cluster dilemma GB assigns twice filter cluster compute  compute per chunk densest sparsest filter within cluster densest sparsest filter load imbalance across across individual filter filter chunk density chunk density SparseMap density sort rank across filter filter pairing GB colocation respectively utilization compute without GB GB shade useful cycle correspond SparseMap whereas  cycle waste due load imbalance rare filter assign filter per compute GB  cluster improve load balance fortunately detect statically GB GB collocation alleviate residual imbalance GB employ filter collocation dense sparse filter described filter collocation  offline software described hence network unlike GB per chunk collocation instead GB dynamically dispatch filter idle compute filter movement loss filter reuse unlikely perform GB statically  appropriate filter cpu simply instructs compute fetch collocate filter chunk compute sequentially multiplies input chunk filter chunk partial sum rout permutation network output collector permutation network latter complicate former unlike scnn bandwidth bandwidth network route per chunk micro october columbus usa        hardware parameter accumulate architecture MACs cluster cluster buffer mac dense scnn KB SparTen KB assume compute cluster chunk compute sparsity permutation latency hidden chunk computation unlike bandwidth permutation network  network  network bandwidth network significantly resource however collocation buffering filter output collocate filter output SparTen buffering increase byte input byte collocate filter byte collocate output buffering KB per multiplier buffering scnn due bandwidth demand SparTen network significantly  link switch traditional permutation network fully provision network accommodate possibility traverse bisection concurrently instead limit bisection bandwidth batch rout cycle traverse bisection schedule later cycle vacant bandwidth demand analysis reveals modest bandwidth provision adequate handle GB demand similarly GB collocate output per cluster bandwidth demand implies output collector sequentially output  methodology evaluate SparTen cycle simulator verilog implementation realize fpga simulator cycle performance simulator dense accelerator scnn SparTen SparTen simulator configure sparsity dense accelerator simulator capture zero computation opportunity sparse architecture without impose sparse computation overhead inner permutation network output compaction scnn simulator faithfully capture source performance loss intra PE idle  barrier scnn input tile performs tile accumulator buffer output SparTen simulator capture residual load imbalance greedy balance idle additionally configure SparTen  sparsity proxy previous scheme cnvlutin cambricon EIE zero idle detailed estimate verilog implementation simulated simulate aggressive configuration alexnet VGGNet configuration googlenet model hardware parameter comparison ensure architecture closely resource compute chip buffering memory bandwidth performance difference benchmark bench input input filter filter density filter density alexnet layer layer layer layer layer googlenet inc inc  inc inc  inc inc  inc inc  inc inc  inc inc  VGGNet layer layer layer layer layer layer layer layer layer layer layer layer layer stem architectural difference resource disparity tpu dense accelerator byte buffering per accumulate mac achieve performance buffer hurt accelerator without improve performance dense accelerator tensor guaranteed mac systolic pipeline buffer input output sparse architecture SparTen however statically conservative buffering per mac fpga implementation rtl implement SparTen cluster compute verilog realize  DE fpga development intel altera cyclone IV fpga interfaced external SDRAM  mhz fpga dsp  logic KB ram intel  prime  builder synthesis integrate accelerator core  II numerical correctness implementation SparTen sparse tensor accelerator convolutional neural network micro october columbus usa alexnet speedup excludes layer estimate ASIC synthesis perform synthesis verilog implementation CU SparTen cluster synopsys compiler technology FreePDK library FreePDK memory compiler buffer synthesize flip flop expensive SRAM array avoid artificial bloat exist implementation separately model buffer cactus cactus estimation assume per cycle activity SparTen benchmark benchmark network previous obtain sparse version network apply prune network filter per layer sparsity information retrain accuracy ensure sparsity previously report simulation mini batch RESULTS performance comparison scheme execution breakdown scheme explain performance impact greedy balance comparison SparTen speedup fpga implementation performance performance dense architecture dense sparse architecture input sparse cnvlutin SparTen without variant greedy balance SparTen GB SparTen software variant greedy balance SparTen GB SparTen hybrid variant greedy balance SparTen scnn scnn input sparse scnn dense scnn scnn dense scnn scnn dense incur scnn overhead whereas dense scnn scnn variant sanity scheme performance normalize dense alexnet googlenet VGGNet respectively network geometric scheme scnn alexnet excludes layer scnn derivative perform poorly due non convolutional stride alexnet dense SparTen GB exploit  sparsity alleviate load imbalance SparTen GB SparTen GB SparTen SparTen GB due grain load balance overall SparTen achieves speedup dense due efficient microarchitecture inner greedy balance load balance contribution scnn hinder overhead consistent expectation scnn however scnn scnn scnn dense inherit scnn overhead VGGNet googlenet trend layer VGGNet shallow channel depth channel hurt SparTen SparTen GB SparTen GB SparTen googlenet layer reduce layer inception inception filter respectively filter multiple layer collocation SparTen GB SparTen leaf compute  remove filter collocation SparTen GB performance benchmark improvement closely per benchmark density alexnet layer VGGNet layer googlenet layer inception inception reduce inception pool proj density input filter improvement SparTen variant execution breakdown understand speedup architecture execution normalize dense alexnet googlenet VGGNet respectively omit alexnet layer scnn non stride issue mention earlier scnn scnn dense sanity analyze execution non zero computation zero computation intra cluster intra PE scnn loss inter cluster inter PE scnn loss intra cluster loss capture SparTen variant scnn SparTen variant intra cluster loss within cluster load imbalance underutilization due lack filter SparTen  former whereas intra PE idle scnn due non zero VGGNet layer suffers intra cluster loss shallow channel depth inter cluster loss SparTen variant due insufficient amount input filter cluster inter PE loss scnn occurs due load imbalance expose global inter PE barrier alexnet dense incurs zero computation motivation sparse architecture due inevitably imperfect inter cluster load balance dense incurs  loss layer insufficient cluster idle significant overall runtime cluster layer idle relative overall runtime reduces zero computation significant remains SparTen variant eliminate micro october columbus usa        googlenet speedup VGGNet speedup excludes layer alexnet execution breakdown remain zero variant cluster hence idle idle cluster  GB overhead within cluster load imbalance  loss due lack filter reduce  nearly eliminate SparTen scnn remove zero computation incurs significant intra inter PE loss trend googlenet VGGNet googlenet SparTen variant incur  loss reduce layer inception inception due filter non multiple interacts poorly collocation layer inception due residual load imbalance greedy balance variant incur inter cluster loss due insufficient inception layer isolate impact buffering dense dense naive dense configure SparTen buffering instead dense extremely buffering fpga dense naive dense SparTen  SparTen GB SparTen GB normalize dense naive scnn performs complexity model detail meaningful due lack average layer model verilog synthesis toolchain estimate dram cannot SparTen sparse tensor accelerator convolutional neural network micro october columbus usa googlenet execution breakdown VGGNet execution breakdown normalize easily accelerator therefore compute memory separately compute memory zero non zero component alexnet dense compute others due dense buffering highlight dense naive dense dense compute dominate zero component reduce eliminate SparTen variant however non zero computation incurs architecture dense sparse computation overhead extra buffering inner output compaction extent incur dense accumulate SparTen variant non zero compute inner inner account SparTen permutation network due GB relatively visible recall GB bandwidth network due demand sparse computation latency overhead hurt performance due pipelining overhead cannot pipelined away overall SparTen incurs compute increase dense achieves reduction nevertheless SparTen dense performance per joule performance compute ignore SparTen memory advantage dense buffering affect compute memory memory dense naive dense identical unlike dense compute memory dominate non zero compute volume reduces quadratically sparsity memory volume reduces linearly SparTen incur mask pointer overhead non zero data compute reduces zero component SparTen variant eliminate SparTen variant memory due identical memory volume trend compute memory googlenet VGGNet SparTen dense zero component compute memory hence opportunity sparse architecture overall SparTen achieves memory reduction dense dense buffering imply advantage SparTen however SparTen memory advantage dense SparTen chip SRAM dense offset SparTen buffering bloat chip SRAM tends tpu MB offset likely substantial impact greedy balance curve plot non zero chunk filter density axis filter axis alexnet layer representative filter sort density visual clarity observation absolute density fairly median density approximately significant variation density variance load imbalance micro october columbus usa        SparTen saving impact greedy balance curve display distribution hybrid variant greedy balance GB axis treat filter GB wise collocation axis extent filter filter filter density variation improve load balance fpga performance speedup SparTen GB SparTen dense obtain fpga alexnet googlenet VGGNet respectively accelerator rtl multiple disparate consume speedup increase SparTen GB SparTen network exception googlenet layer simulation SparTen achieves speedup model SparTen performs dense architecture sparse architecture respectively speedup trend simulation absolute speedup obtain fpga slightly report simulation discrepancy stem fpga become memory bound computation decrease quadratically sparsity memory traffic linearly sparsity ASIC synthesis estimate SparTen cluster synthesis achieves mhz comparison scnn achieves ghz technology enjoys faster fan inverter FO delay technology SparTen scnn comparable alexnet speedup fpga googlenet speedup fpga VGGNet speedup fpga ASIC SparTen component buffer prefix sum priority encoder MACs permute network SparTen mac technology favorably scnn MACs denser technology scnn estimate mention accelerator consume SparTen sparse tensor accelerator convolutional neural network micro october columbus usa related architecture dense cnns optimize compute memory reuse however sparse architecture significantly reduce compute data volume achieve performance discus  architecture extensively scnn fully sparse architecture diffy improves nonzero computation exploit difference serial architecture reduce compute leverage booth encode elide zero approach opportunity issue scheme transfer zero memory incur chip SRAM memory bandwidth unlike SparTen fundamental tension reuse load imbalance remains laconic array laconic processing incurs implicit barrier encode multiplication load balance SparTen greedy balance laconic speedup opportunity model meaning radix radix encode multiplication operand encode non zero average operand non zero encode average finally sparse scheme SparTen conservative buffering incurs overhead dense architecture sparse scheme conservative buffering booth encode PERMDNN transforms sparse filter permute diagonal matrix fully layer whereas SparTen target convolutional layer CIRCNN circulant matrix filter complex fft hardware capture sparsity memory accelerator leverage analog logic achieve dense matrix multiplication accelerator leverage sparsity analog logic incurs wellknown issue scalability variation affect digital logic accuracy analog logic transistor parameter increase cnn inaccuracy degrade accuracy unacceptable due undo effort machine expert account variation chip separately maintain accuracy cambricon coarse grain prune clamp zero contiguous filter clamped cannot retrain similarly  proposes coarse grain prune granularity hardware width maintain regularity contrast compression prune filter independent retrain zero coarse grain prune degrade accuracy unlike compression cambricon  evaluate accuracy accuracy model combine CC proposes merge sparse denser improve systolic array utilization merge prune non zero multiple non zero merge CC lower accuracy loss accuracy report increase inaccuracy explain SparTen GB  software CC permutation shuffle criterion SparTen GB CC completely density versus jigsaw avoid conflict GB explores chunk granularity instead CC filter granularity previous performance compute hpc sparse data relevant SparTen sparse representation hpc csr widely adopt sparse blas library sparse blas  nvidia sparse blas library implementation improve sparse blas performance purpose processor implementation remain significantly dense accelerator gpus instead SparTen mask representation efficient implementation SparTen proposes GB tackle fundamental tension load imbalance reuse sparse cnns address previous conclusion exploit sparsity zero feature filter convolutional neural network cnns reduce compute data volume challenge sparse vector vector dot primitive cnns inner sparse vector non zero avoid inner scnn performs cartesian capture relevant multiplication however approach incurs overhead applicable non stride convolution exploit reuse sparse cnns fundamentally systematic load imbalance address scnn propose SparTen achieves efficient inner native sparse execution memory storage tackle load imbalance SparTen employ software scheme greedy balance filter density via variant software filter density software hardware hybrid finer grain density simulation average SparTen performs dense architecture sparse architecture scnn respectively SparTen achieves compute memory architecture fpga implementation SparTen performs dense architecture sparse architecture respectively SparTen broadly applicable convolutional layer stride non convolutional neural network dnns memory lstms recurrent neural network rnns multi perceptrons mlp sparse linear algebra performance compute hpc extend SparTen dnns hpc future SparTen simplicity efficiency performance attractive architecture sparse matrix computation