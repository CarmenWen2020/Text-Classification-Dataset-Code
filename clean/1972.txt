convolutional neural network cnns recently demonstrate superior quality computational image application therefore potential revolutionize image pipeline camera display however conventional cnn accelerator ultra resolution video due considerable dram bandwidth consumption therefore memory computation efficient microarchitecture crucial revolution approach goal inference network model instruction processor jointly optimize hardware performance image quality apply inference eliminate dram bandwidth feature accordingly propose  network model ERNet optimize image quality hardware constraint devise coarse grain instruction architecture FBISA hungry convolution massive parallelism finally implement embed processor eCNN accommodates ERNet FBISA flexible processing architecture layout quality  super resolution denoising ultra HD fps ddr consume average comparison diffy  ddr consumes quality VDSR HD fps lastly application performance style transfer recognition demonstrate flexibility eCNN CCS CONCEPTS computer organization embed hardware compute methodology machine keywords convolutional neural network computational image inference hardware accelerator ultra definition introduction convolutional neural network cnns recently attention computer vision computational image hardware accelerator become emerge enable application performance pixel throughput inference quality jointly model structure processor architecture inference cnn model structure evolve mainly recognition shallow alexnet VGGNet filter resnet residual connection hardware orient variant depth wise convolution feature squeeze propose reduce model complexity inference cnns dominant performance computational image application image denoising super resolution image deblurring synthesis novel application achieve traditional style transfer DSLR quality conversion algorithm mimic however seldom discussion hardware orient model computational image despite potential enable generation image pipeline device hardware accelerator propose neural network dadiannao cambricon acc tpu  purpose inference contrast shidiannao eyeriss morph  optimize classification cnns sparsity cnns reduce computation complexity sparsity micro october columbus usa activation deployed another approach complexity precision computation dynamic fix format binary network however accelerator optimize computational image resolution video dram bandwidth compute capability recently diffy attack utilize sparsity activation difference reduce dram access compute diffy tile dram setting HD video finally inference cnn model determines data reuse scheme accelerator memory access efficiency systematic approach partition cnns computation sub introduce energyefficient dataflows analyze layer fusion avoids external traffic feature apply pyramid inference overlap feature reuse scheme chosen fuse cnn layer however buffer increase linearly model depth image width channel MB SRAM VDSR HD resolution offs chip SRAM chip dram bandwidth widely image processing application estimation discrete wavelet transform aim enable quality inference ultra HD UHD fps computational image task target dram setting effective efficient integration embed device challenge goal achieve directly accelerate ofthe model mostly feature layer instead expand inference model structure instruction processor jointly optimize hardware performance image quality propose truncate pyramid inference eliminate dram bandwidth feature chip buffer avoid chip storage recompute overlap buffer proportional model width recomputation overhead almost increase quadratically model depth factor  thumb simply feature layer enhance model quality instead propose novel ERNet model optimize cnns hardware constraint construct feature instruction architecture FBISA highly parallel convolution specifies buffer operation fashion instruction multiple data simd addition flexibility programmer compiler optimize compute constraint finally implement embed cnn processor eCNN flexibly accommodates ERNet FBISA highly parallel filter locally distribute parameter summary contribution finding propose enable resolution inference dram bandwidth analyze computation bandwidth overhead propose hardware aware ERNet optimize image quality hardware constraint training procedure model optimization dynamic fix precision devise coarse grain FBISA parallel parameter bitstreams massive computation parallelism efficiently flexibly embed processor eCNN FBISA highly parallel convolution multiplier  image super resolution SR denoising precision quality SR outperform VDSR psnr eCNN delivers HD UHD fps respectively layout eCNN achieve TOPS tera operation per technology highquality  UHD fps consume ddr comparison diffy consumes dual channel ddr VDSR HD fps computer vision task  model style transfer recognition motivation recent research cnn accelerator mainly focus recognition detection network therefore specific feature computational image network optimization spatial resolution feature aggressively downsampled model sparse former dramatically amount memory bandwidth latter introduces extremely demand compute aggressive downsampling recognition extract feature reduce data amount feature volume cuboid deeper layer conventional accelerator apply frame inference perform convolution layer layer limited dram bandwidth however induce amount dram bandwidth computational image network  feature generate texture detail mild downsampling network without downsampling correspond dram bandwidth feature input output image derive image height image width feature channel model width model depth frame rate feature factor per layer feature dram load layer accordingly layer channel VDSR GB memory bandwidth HD fps feature compression diffy dual channel ddr GB HD specification resolution UHD eCNN highly parallel cnn accelerator inference micro october columbus usa cnn per layer feature recognition computational image quality degradation computational image network sparsity technique prune   depth wise convolution residual EDSR baseline SR SR dram bandwidth unaffordable factor limited device sparsity recognition model deployed develop complexity technique prune depth wise convolution however computational image network rely variety parameter extract local feature generate texture image quality highly related model sparsity technique significant degradation prune denoising  model prune away psnr gain benchmark  datasets  become negative another depth wise convolution EDSR baseline model although complexity quality datasets bsd urban  therefore confront computation demand computational image cnns furthermore resolution image generation demand challenge VDSR already demand TOPS HD application TOPS UHD issue dram bandwidth compute motivate novel approach ultra resolution cnn acceleration propose inference hardware orient ERNet resolve memory issue computation issue address coarse grain FBISA correspond highly parallel eCNN processor propose inference truncate pyramid inference cnn BASED inference propose input image partition independently output stitch output image contrast layer fusion recompute  feature avoid chip SRAM however induces additional bandwidth computation reduce overhead propose truncate pyramid inference output pixel detailed analyze overhead inference network consists conv layer receptive link directly depth convolution deeper upper layer effective become truncate pyramid input generate output 2D depth receptive increase input dram bandwidth output pixel generate input bandwidth overhead evaluate normalize bandwidth ratio nbr bandwidth input output output image BR rgb image depth input ratio similarly computation overhead recomputed feature evaluate normalize computation ratio NCR computation complexity frame intrinsic NCR volume truncate pyramid volume cuboid volume proportional amount feature therefore compute operation micro october columbus usa bandwidth computation overhead truncate pyramid inference nbr NCR versus depth input ratio network NCR versus buffer layer VDSR layer SRResNet ratio rapidly respect  ratio eventually infinity valid output pixel induced bandwidth overhead generally acceptable bandwidth nbr bandwidth overhead frame derive VDSR however computation overhead become compute feature recomputation approach avoid situation adopt buffer reduce deeper network evaluation extend complicate model conclusion bandwidth computation overhead network layer filter channel feature therefore buffer  reduce NCR suffer significant computation overhead buffer usually buffer switch input output layer severe NCR buffer VDSR SRResNet outperforms VDSR NCR layer VDSR MB buffer layer SRResNet around MB NCR buffer SRResNet NCR  quickly therefore NCR buffer simultaneously quality network achieve goal hardware constraint model construction accordingly introduce ERNet ERNet introduce building module ERNet procedure model optimization quantization adopt dynamic fix precision model structure network buffer increase capacity without enlarge NCR buffer explore another direction model construction temporarily expand model width achieve  conv layer expand model width conv reduce residual connection robust training operation perform internally without access buffer therefore pump complexity  improve image quality buffer model depth integer expansion ratio guarantee hardware utilization increase model flexibility construct building  module assign incremented overall expansion ratio accordingly model hyperparameters network increase depth pump complexity model SRERNet perform  SR basically replaces residual SRResNet EDSR baseline  image width height pixel shuffle  restore resolution addition reduce channel buffer model optimization hardware constraint overall computation complexity NCR intrinsic complexity bandwidth overhead usually complexity target correspond throughput aim optimize image quality constraint model selection procedure illustrate SRERNet assume input computation constraint kop pixel operation per output pixel derive expansion ratio module constraint upper bound decrease quickly model depth grows increase NCR kop pixel NCR correspond intrinsic complexity kop pixel deeper network necessarily perform intrinsic complexity scan candidate model lightweight training patch mini batch image quality validation datasets model constraint finally polish model retrain  SRERNet  outperform SRResNet thinner complex network dynamic fix precision quantize polished model computation logic chip memory multiplication buffer eCNN highly parallel cnn accelerator inference micro october columbus usa building module ERNet   SRERNet SR model scan SRERNet computation constraint format  precision internal partial sum accumulate precision preserve quality adopt fix format illustrate  unsigned respectively fractional effective apply dynamic fix precision optimize image quality convolution layer format bias feature output respectively stage procedure quantization tune quantization stage fractional precision format collection correspond float norm norm error optimization  arg min quantization  performs clip precision distribution parameter directly derive float model feature inferencing training dataset quantization induces psnr loss denoising SR tune refine quantize parameter calculate gradient accurately clipped relu rectify linear function model clip behavior  fix ERNet psnr degradation average FBISA simd instruction FBISA  inference fully convolutional network increase flexibility zero pad inference variant   FBISA massive parallelism coarse grain instruction feature internal access parameter memory introduce sequentially instruction instruction format opcode specify convolution task specific attribute inference operand feature parameter respectively attribute format feature operand mandatory inform source src destination dst convolution addition supplementary srcs  feature accumulation instruction skip residual connection partial sum finally parameter operand specifies access correspond bias parameter memory opcode operand expression instead conventional improve readability overview instruction compute task FBISA leaf module performs conv filter feature opcode micro october columbus usa FBISA instruction format FBISA instruction overview leaf module attribute opcodes mainly usage purpose processing output opcode upx shuffle pixel spatial upsampling  performs stride max pool downsampling ER devise specifically  leaf module additional conv feature reduction wider filter conv construct opcodes accumulate partial sum via srcs regard feature operand apply strategy efficient data movement highly parallel convolution specify basis buffer BBs instead conventional register vector therefore internal partial sum instruction accumulate inside  datapaths without access SRAM dram mostly bandwidth limited hungry another advantage program avoid complex compiler quality SRERNet  instruction strategy conventional load instruction external feature reading instead devise operand DI virtual buffer data input output respectively implement fifo interface data normal buffer therefore processor pipeline fully optimize  instruction strategy decouples FBISA data structure memory integration portability bitstream filter parameter format data movement parameter paramount importance cnn acceleration avoid retransmit parameter internal parameter memory reuse feasible thanks computational image network parameter VDSR SRResNet respectively resnet FBISA split filter  enable parallel load distribution processor conv conv spatial filter corresponds bitstreams output channel leaf module format bitstream compress increase model adopt DC huffman cod jpeg cod algorithm enables parallel decode hardware overhead mostly uncorrelated differential encode unnecessary filter bias another  compress decode restart mechanism devise enable parameter reuse instruction byte align address refer bias bitstream specify restart attribute parameter operand huffman encode bitstream bitstreams restart address synchronize restart attribute contains coefficient leaf module bias  finally  synchronize restart pad shorter regard compression efficiency huffman restart bitstream sufficient quantize parameter distribution compression ratio around denoising SR  eCNN embed processor implement FBISA highly parallel convolution performance  compute architecture embed integration eCNN highly parallel cnn accelerator inference micro october columbus usa processing eCNN introduce functional distribute parameter perform convolution respectively architecture processing target model program parameter load eCNN inference image perform model hierarchy sub model instruction leaf module image pixel hierarchy tile feature hierarchy interleave flexible processing model partition shallower sub model reduce computation overhead however intermediate feature increase dram bandwidth sharply performance tradeoff eCNN embed integration processor dma controller via fifo interface transaction handle basis without induce burden eCNN accelerates computation instruction  fashion instruction calculates leaf module tile cycle tile  instruction calculate consecutively accumulate partial sum without precision loss SRAM access specify tile eCNN acceleration instruction diagram diagram implement mention processing consists functional information decode IDU cnn inference ciu IDU responsible decode instruction parameter ciu computes correspond convolution enable highly parallel compute deploy massive amount multiplier convolution ciu LCONV LCONV perform conv conv respectively leaf module latter  model parameter IDU avoid excessive external bandwidth parameter retransmission however multiplier access leaf module cycle throughput affordable bandwidth parameter memory therefore devise instruction pipelining scheme distribute parameter efficiently IDU pipeline stage progressively decode parameter instruction meanwhile sequentially locally distribute register inside multiplier ciu convolution pipeline stage introduce implementation detail IDU ciu information decode IDU instruction IDU decode opcode operand trigger parameter decompression procedure parameter bitstreams mention correspond memory decode parallel decoder leaf module decoder responsible decode bias generates bias parameter ping pong distribution scheme IDU ciu distribution scheme output channel conv decoder deployed filter decodes cycle input channel decode distribute stage network stage output input channel respectively local register file 2D accompany correspond 2D filter filter 2D register file decode parameter ping pong fashion ciu convolution instruction pipeline switch leaf module consecutive computation instruction IDU decodes leaf module cycle completes instruction faster ciu proportional tile cnn inference ciu computation ciu inference datapath closely couple convolution buffer BBs detail tile pipelined inference datapath highly parallel convolution prone inefficiency data movement inflexibility model carefully inference datapath alleviate issue tile pipeline mainly consists function input preparation LCONV output processing opcodes operand function prepares input tile filter however tile tile induces bandwidth buffer reduce bandwidth tile buffer fifo buffer leaf module correspond tile rearrange register file RF leaf module addition data reorder circuit src reorder address tile misalignment issue function output processing model flexibility sub function  LCONV accumulation adder  calculate instruction partial sum another adder  internal upsampling writes micro october columbus usa eCNN diagram instruction pipelining scheme distribution scheme output channel LCONV inference datapath LCONV data pixel shuffle dst reorder downsampling stride max pool dst reorder quantization quantizes feature partial sum format buffer output fifo interface addition quantization circuit inside LCONV reduce input bitwidth LCONV highly parallel convolution LCONV LCONV tile cycle filter respectively employ stationary strategy optimize data reuse inference conventional accelerator multiplier massive parallelism enables efficient accumulation internal partial sum communication locally hardwired without additional register file SRAM dram LCONV contains 2D filter filter 2D 2D filter tile reuses buffer mapping highly parallel data movement brings misalignment issue buffer feature tile access eCNN highly parallel cnn accelerator inference micro october columbus usa BB implementation access reading tile normal mapping interleave mapping eCNN configuration tile align address issue implement buffer sub buffer normal mapping sufficient pixel shuffle upsampling sub buffer conflict therefore another interleave mapping devise resolve issue evaluation configuration implementation computation constraint model optimization correspond specification UHD fps UHD HD fps HD HD fps HD ERNet model layout performance eCNN processor introduce application computer vision style transfer recognition demonstrate flexibility approach ERNet model model structure addition SRERNet SR implement SRERNet  SR denoising respectively model derive accordingly remove  SRERNet batch normalization layer pad zero channel rgb image input eCNN training hyper parameter stage training procedure model scan  tune quantization scan lightweight setting apply setting improve image quality ERNet training setting psnr performance polished ERNet model datasets divk  exploration comparison network SR denoising respectively polished model psnr performance picked model polished comparison VDSR SRResNet implementation SR  FFDNet denoising HD specification hardware constrain ERNet model achieve quality SRResNet FFDNet increase specification psnr performance intrinsic complexity however UHD SRERNet outperform VDSR SRERNet  comparable benchmark VDSR  respectively fix precision entropy cod norm norm quantization polished model SRERNet HD bitstreams exceed capacity parameter memory precision therefore perform quantization parameter capacity norm quality degradation cropped recover tune chose optimize model psnr quality despite entropy dynamic compression ratio around psnr limited dynamic precision micro october columbus usa model quantization entropy cod program   UHD KB parameter memory addition entropy shannon limit justifies usage encode program coarse grain FBISA instruction concise program program layer  UHD attribute opcodes specify output tile identify dynamic format eCNN performance implementation implement eCNN verilog hdl TSMC technology memory compiler generate SRAM macro synopsys IC compiler placement rout perform layout essential pipelined macro circuit constitute eCNN collectively exhaustive accurate estimation rtl simulation generate signal activity waveform propagate layout  extract  layout performance eCNN processor mhz achieves TOPS inference performance average consumption detail summarize LCONV delivers inference performance occupies resource LCONV responsible performance another buffer KB parameter memory KB contributes feature parameter respectively consume thanks constrain depth highly optimize SRAM macro computation  profile inference indicates capability NCR consumption eCNN inference NCR consumption breakdown ERNet model circuit compute overhead tradeoff inference image quality breakdown consumption ERNet model variation specification related quality difference  variation psnr HD UHD contrast  variation consumption psnr breakdown circuit combinational circuit contribute consumption highly parallel convolution sequential circuit constantly occupy locally distribute parameter register tile pipeline register consume SRAMs dram bandwidth dram access via data input output FIFOs highly regular optimize deterministic dram bandwidth dynamic consumption ERNet model  eCNN highly parallel cnn accelerator inference micro october columbus usa dram bandwidth dynamic bandwidth specification 6GB UHD 4GB HD 5GB HD  respectively therefore eCNN application dram configuration ddr 2GB ddr 1GB ddr 6GB sufficient UHD HD HD respectively regard consumption micron ddr SDRAM calculator evaluation ddr bandwidth eCNN consumes dynamic activation leakage consumes comparison eCNN theart processor computational image application ideal BM3D diffy cnn HD specification already dram setting dual channel ddr ddr however eCNN deliver UHD performance ddr another advantage eCNN constant pixel throughput facilitate application contrast performance ideal diffy highly varies input image statistical deployed acceleration consumption report ideal diffy cannot superiority directly highly related technology node implementation detail deployed model algorithm denoising HD ideal BM3D diffy demand tile FFDNet however eCNN consumes   comparable FFDNet SR HD diffy demand tile VDSR eCNN consumes SRERNet VDSR cnn accelerator simulator sim simulate performance  processor configuration classical tpu tpu highperformance processor TOPS MB SRAM feature parameter data reuse simulation UHD fps HD fps achieve SRERNet   respectively dram bandwidth GB GB eCNN throughput efficiency fps TOPS arithmetic intensity TOPS GB respectively model demonstrates advantage joint approach computational image task computer vision model FBISA style transfer recognition computer vision application model structure model flexibility approach built FBISA compatible model style transfer recognition computational image mainly respect spatial downsampling wider channel batch normalization layer FBISA concatenate leaf module stabilize model pre training merge convolutional layer quantization inference style transfer  increase receptive increase NCR significantly split model sub model reduce compute overhead training quantization model deliver style transfer performance eCNN HD fps model nvidia titan gpu generates fps addition dram bandwidth approach 1GB enable advanced application embed device recognition devise layer residual network eCNN perform recognition reduce amount parameter avoid  instead computation thinner layer model achieves accuracy imagenet parameter performance comparable resnet vgg model increase parameter memory eCNN become performance achieves fps per image MB dram bandwidth image eCNN consumes KB dram access comparison eyeriss core technology delivers fps batch image consumption MB dram bandwidth vgg demand MB dram access image demonstrate model flexibility eCNN joint hardware model approach benefit recognition task micro october columbus usa technology outperforms counterpart consumption operation comparison computational image processor related instruction previous simd usually devise load instruction parameter adopt medium grain operand feature vector matrix 2D tile compute tile flexibility contrast apply  approach grain feature operand optimize consumption compute capability highly parallel convolution model structure previous hardware orient model aim reduce complexity squeezenet temporarily reduces model width expands residual connection MobileNetV connection thinner layer reduce storage expansion reduction structure ERNet however goal increase complexity hardware constraint implementation detail winograd convolution efficient algorithm reduce multiplier conv recently advantage gpu fpga embed processor however increase overhead internal  additional pre processing become significant implementation therefore implementation instead frame optimization research direction reduce cnn computation exploit temporal redundancy input similarity across video audio frame moreover concept apply compensate unreliability prune direction complementary approach enhance performance eCNN conclusion investigate hardware framework computational image cnns ultra HD application device instead accelerate exist model devise hardware orient  adopt inference eliminate dram bandwidth feature compute capability construct coarsegrained FBISA enable highly parallel convolution finally implement performance eCNN processor incorporate ERNet FBISA training layout framework superior hardware performance image quality addition flexibility demonstrate usage style transfer recognition future cnn variant application unleash device