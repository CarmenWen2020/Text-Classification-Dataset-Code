recent advancement technique facilitate  diverse application content image retrieval audio texturing unlike conventional query intelligent query lack efficient index complex compute operation feature achieve highperformance intelligent query massive datasets compute employ gpus conjunction solid SSDs data access parallel data processing however characterization various intelligent query workload developed neural network dnns storage bandwidth bottleneck contributes query execution DeepStore storage accelerator architecture intelligent query consists efficient storage accelerator specifically  intelligent query resource constraint ssd controller similarity storage query cache exploit temporal locality user query performance improvement lightweight storage runtime query software abstraction intelligent query DeepStore exploit ssd parallelism exploration achieve maximal efficiency storage accelerator validate DeepStore ssd simulator evaluate variety vision text audio intelligent query gpu ssd approach DeepStore improves query performance efficiency CCS CONCEPTS computer organization purpose secondary storage organization information information retrieval keywords intelligent query storage compute solid hardware accelerator information retrieval introduction thanks recent advancement neural network dnns explosive increase ubiquitous accessibility data enable intelligent query versatile data retrieval application improve quality data service typical identify image database retrieve input style instrumentation online shopping garment item increase accuracy performance technique envision intelligent query dominate emerge data service unlike conventional query transactional database organize data structure manner intelligent query leverage feature vector extract unstructured data facilitate similarity comparison technique intelligent query image text others user intention data retrieve intelligent query query extract query feature vector executes similarity comparison feature vector source data query sort similarity user core component intelligent query similarity comparison neural network improve comparison accuracy however neural network highly non linear cannot preserve geometric micro october columbus usa  qureshi inequality feature vector therefore efficient index feature vector scan entire database fulfill query accuracy furthermore feature  item image relatively KB extract feature memory query datasets facebook photo video service application intelligent query exacerbate therefore achieve performance intelligent query massive datasets approach gpus conjunction SSDs data retrieval parallel similarity comparison conduct characterization typical intelligent query workload recent generation nvidia gpus query nvme ssd data storage evaluate profile intelligent query workload visual audio text intelligent query bottleneck storage increase computational resource bottleneck becomes severe core function similarity comparison intelligent query mainly involve convolutional fully neural network layer ideal candidate hardware acceleration overcome aforementioned bottleneck DeepStore storage acceleration intelligent query unlike exist storage compute rely embed multi core CPUs ssd controller perform computation DeepStore employ  accelerator compute similarity comparison operation however develop accelerator intelligent query ssd controller easy due limited resource SSDs resize storage accelerator exploration methodology memory bandwidth budget abstract neural network operation similarity comparison customize storage accelerator maximize resource efficiency ssd controller achieve maximum efficiency DeepStore explore ssd parallelism ssd  chip mapping storage accelerator channel parallelism energyefficient achieves performance resource constraint accelerator guideline storage accelerator improve performance intelligent query develop storage query cache leverage  technique conduct query lookup cache insight dnn query already tolerate error therefore query cached query cache DeepStore directly return cached query without conduct similarity comparison entire feature database useful intelligent query cannot indexed hash finally enable applicability flexibility intelligent query DeepStore software abstraction program apis enable developer deploy model similarity comparison DeepStore develops runtime storage accelerator responsible dispatch intelligent query knowledge DeepStore storage acceleration intelligent query overall contribution conduct characterization typical intelligent query quantify performance bottleneck storage bandwidth bottleneck intelligent query develop efficient storage accelerator similarity comparison feature vector facilitates offload intelligent query SSDs exploit ssd parallelism storage accelerator conduct thorough exploration ssd channel chip parallelism channel efficiency similarity query cache intelligent query inherently tolerate accuracy loss cache benefit query implement DeepStore ssd simulator construct ssd sim sim evaluate DeepStore variety visual audio text application intelligent query query trace application workload experimental DeepStore improves query performance efficiency gpu ssd organize introduction intelligent query ssd architecture characterization typical intelligent query workload discus DeepStore implementation detail evaluate DeepStore discus related summarize background brief overview intelligent query internal architecture SSDs intelligent query recent advancement neural network dnns explosive increase ubiquitous accessibility data enable intelligent query multiple application recent popular application identification style retrieval application accuracy dnns commonly similarity comparison query furthermore user input query longer restrict text transactional database comprise image image patch sketch context intelligent query leverage dnns automatically extract feature query input bridge semantic gap query source data DeepStore storage acceleration intelligent query micro october columbus usa intelligent query application characteristic application description feature KB conv layer FC layer wise layer FLOPs dataset identification reid visual identify across database image MB CUHK information retrieval mir audio retrieve style instrumentation MB  ESTP visual online shopping garment item garment item MB  text image retrieval tir text image retrieve image description query MB mscoco  TextQA text rerank text closely related query MB trec QA feature vector DB bag owl offline phase online phase  similarity comparison network feature vector DB bag owl workflow dnn intelligent query offline phase dnn model extract feature vector storage device SSDs online phase feature vector extract query feature vector database similarity comparison network obtain finally dataset image sort response typical intelligent query neural network model architecture demonstrate training network learns similarity fed positive negative query feature vector source dataset network deployed perform intelligent query execution phase offline online offline phase image dataset feature vector extract intermediate layer dataset representation network model feature database online phase query feature representation network model extract feature vector intelligent query query feature vector fed similarity comparison network scn model scn model computes similarity query dataset feature vector generate similarity similarity sort item query data item retrieve application fetch correspond content database traditional query rely structure data intelligent query extract feature vector unstructured data technique dnn model highly non linear preserve geometric inequality input feature vector effective index dataset traditional fix metric euclidean cosine distance scan entire database feature vector scn scn execution involves compute intensive dnn operation intelligent query typically employ gpus efficient operation accelerate scn computation optimal gpu resource utilization batch database feature vector intelligent query gpu feature database usually billion feature vector vector upto kilobyte SSDs feature vector database terabyte capacity access latency microsecond focus storage acceleration intelligent query SSDs ssd architecture internal architecture organization ssd ssd controller contains embed cpu core execute ssd management flash translation layer FTL functionality parse command garbage collection leveling terabyte capacity SSDs dense nand flash memory organize multiple hierarchy channel chip ssd channel channel consists flash chip flash controller access via channel flash chip consists multiple flash access granularity buffer cache access flash operation buffer associate SSDs massive internal bandwidth however external bandwidth SSDs limited flash channel arbitration weak processor core ssd controller bandwidth pcie interface intelligent query intelligent query cater diverse application envision dominate emerge data service however prior systematically intelligent query workload depth variety intelligent query workload identify performance bottleneck opportunity optimization experimental setup intelligent query workload span across visual audio text identification micro october columbus usa  qureshi reid image dataset ESTP query across dataset text image retrieval tir textual query retrieve image understand user description information retrieval mir sample audio clip text TextQA query relevant corpus document identify intelligent query application generality emerge intelligent query implement intelligent query tensorflow framework detail model application model accuracy within advertised accuracy extract feature vector application ssd recent generation highend nvidia gpus titan pascal titan volta implement online phase extract feature vector query query feature representation network model feature database load multiple batch perform similarity comparison gpu gpu ssd optimize batch feature prefetched host memory gpu computes scn previous batch batch gpu utilization nearly similarity comparison operation breakdown query latency component gpu compute compute cpu gpu data transfer  ssd cpu data transfer ssd profile storage constitutes query execution although feature dataset prefetched ssd computation executes gpu significant prefetching barely improves performance furthermore newer generation gpus nvidia pascal volta compute intensive layer scn perform faster however overall performance intelligent query workload improve limited storage bandwidth observation emerge intelligent query application primarily bottleneck storage bandwidth emergence faster gpus dnn accelerator performance gap compute storage widen understand intelligent query workload characterize computational scn summarize quantify  mainly comprise convolutional fully wise layer text image retrieval tir consists vector dot fully layer observation emerge intelligent query workload involve complex operation convolutional fully layer wimpy processor sufficient perform computation similarity comparison network significant overhead query latency address challenge observation computation closer data inside storage however wimpy processor ssd controller performant execute scn operation therefore address challenge observation propose develop storage accelerator perform scn computation SSDs  accord intelligent query workload storage bottleneck similarity comparison critical intelligent query observation motivate pursue storage acceleration  intelligent query however develop accelerator resource constrain ssd non trivial goal principle goal DeepStore achieve performance intelligent query massive dataset maximal efficiency resource constraint ssd controller specific principle ssd controller limited budget memory capacity explore achieve maximal resource efficiency storage accelerator DeepStore storage accelerator scalable exploit internal parallelism SSDs storage capacity increase pack flash chip DeepStore achieve maximal efficiency conventional database DeepStore efficient query cache exploit locality user query performance fourth DeepStore diverse intelligent query application enables programmer manipulate feature database specify neural network model execute query interface overview DeepStore architecture DeepStore exploit internal parallelism ssd accelerator ssd channel chip respectively DeepStore executes lightweight query par incoming query manages query cache QC reduce computation model execute  across accelerator inside storage scn computational model accelerator accelerator generate query response query feature vector extract host DeepStore par query specific information query exists query cache comparison query cached query query comparison network qcn executes  accelerator schedule scn cached entry report update QC query load model ssd dram DeepStore storage acceleration intelligent query micro october columbus usa batch percentage compute pascal compute volta  pascal  volta ssd pascal ssd volta pascal volta batch percentage reid batch percentage mir batch percentage ESTP batch percentage tir batch percentage TextQA performance breakdown compute intelligent query workload generation gpus pascal volta application execution spent reading feature dataset ssd ssd controller firmware embed core dram interface flash controller flash controller flash controller flash controller internal bus query interface DeepStore ssd acc query channel acc channel acc channel acc channel acc flash flash flash flash channel flash flash flash flash flash flash flash flash channel flash flash flash flash flash flash flash flash channel flash flash flash flash flash flash flash flash channel flash flash flash flash flash flash flash flash channel flash flash flash flash flash flash flash flash channel flash flash flash flash flash flash flash flash channel flash flash flash flash flash flash flash flash channel flash flash flash flash chip accelerator DeepStore augments traditional ssd ssd channel accelerator interfaced nand flash controller chip accelerator interfaced nand flash chip ssd embed core execute query execution scn storage accelerator individual accelerator execute computational model parallel accelerator writes ssd dram query merges generate similarity query insert query QC application request query host specify memory location memory access dma operation associate ObjectID physical address feature vector reading respective raw data query neural network model programmer interact DeepStore query DeepStore api specify scn computation model query application storage accelerator intelligent query workload classify core intelligent query operation fully convolutional wise operation sort intelligent query accelerator operation efficient manner demonstrate architecture DeepStore accelerator consists component systolic array processing PEs scratchpad memory controller rectangular systolic array spatial architecture enables efficient mapping fully convolutional layer modify regular systolic array architecture wise operation dot subtraction addition intelligent query  PE PE  PE   systolic array  output scratchpad controller sorter input EW  EW input PE PE  PE   flash    query feature vector  dataset feature vector EW wise storage accelerator DeepStore enable input systolic array systolic array throughput wise operation systolic array scratchpad memory implement SRAM buffer query feature vector  database feature vector  scn model intermediate output scratchpad memory highly multiple parallel request systolic array systolic array scratchpad memory controller finite machine controller responsible load model ssd dram location query  controller prefetches  flash chip schedule scn computation sort controller equip priority queue temporary intelligent query execution priority queue implement sort tag array mapping mapping indexed tag entry consists accuracy feature ID systolic array computes similarity controller binary tag array accuracy mapping entry identify entry tag array priority shift tag entry mapping appropriately update interaction flash memory DeepStore accelerator database feature vector flash memory query feature vector accelerator controller responsible program flash controller data flash chip accelerator scratchpad generate flash micro october columbus usa  qureshi flash flash flash flash accelerator scratchpad accelerator controller flash  queue ctrl addr buffer  chip nand flash controller DeepStore accelerator interaction flash memory DeepStore employ queue isolate prefetching data feature vector flash chip perform scn computation address feature vector address query flash controller issue request flash chip flash chip associate buffer afterward flash controller buffer flash  queue accelerator computation feature scratchpad accelerator controller consumes feature flash  queue addition flash  queue isolates computation accelerator data load flash chip enables feature vector prefetched accelerator computes feature perform computation accelerator dataset feature vector  exploit internal parallelism SSDs DeepStore stripe feature database application across channel chip feature vector align DeepStore employ regular FTL FTL physical address database DeepStore physical address along metadata specify feature vector feature vector metadata persist reserve flash cached ssd dram query execution query execution query database query database metadata along channel chip ssd accelerator controller accelerator controller information compute offset physical address feature vector fetch avoid FTL address translation overhead controller schedule scn computation systolic array DeepStore exploit ssd internal parallelism improve performance exploit internal parallelism SSDs mapping accelerator parallelism inside ssd optimization along dimension budget memory bandwidth SSDs usually GBs dram memory  memory bandwidth ssd controller flash chip  bandwidth furthermore SSDs constrain limited budget pcie interface consume exist ssd hardware peak operation budget DeepStore maximize DeepStore efficiency conduct exploration storage accelerator parallelism PEs speedup convolution fully performance systolic array accelerator PEs convolution fully aspect ratio performance SSDs constrain exploration available budget dram bandwidth 0GBps flash chip bandwidth mbps simulation platform described intelligent query workload perform systolic array configuration PEs aspect ratio systolic array assumption infinite memory bandwidth gradually increase PEs aspect ratio FC convd layer application performance gain beyond PEs respectively neural network layer float  operation per cycle feature vector systolic array aspect ratio intelligent query workload perform aspect ratio FC layer PEs convd layer PEs application layer ratio bound reduce introduce memory bandwidth constraint dram flash scratchpad accelerator eliminate choice cannot budget allocate accelerator consume PEs onchip chip memory access scn computation summarize exploration decision accelerator parallelism SSDs ssd ssd accelerator budget dram bandwidth available maximize reuse compute FC layer output stationary OS data ssd accelerator accelerator access MB scratchpad minimize dram transaction scratchpad avoids unnecessary offchip memory access remain within allocate budget increase scratchpad obtain performance improvement application reid scratchpad fetch dram compute scn accelerator fully pipelined allocate budget ssd accelerator systolic array PEs organize maximize performance wise operation DeepStore systolic array application predominantly consist FC layer accelerator width impact performance layer DeepStore storage acceleration intelligent query micro october columbus usa channel assume ssd channel channel accelerator budget dram memory bandwidth  accelerator SRAMs expensive channel accelerator scratchpad ssd accelerator channel accelerator OS data however OS data data access memory bandwidth input fetch frequently bandwidth access input multi scratchpad memory hierarchy channel accelerator ssd scratchpad memory minimize dram traffic improve across multiple channel accelerator reduction dram access requirement instead ssd accelerator configuration channel accelerator systolic array PEs organize configuration due channel accelerator limited budget application convd FC layer reid channel accelerator limited performance execute scn input feature vector application layer TextQA flash channel bandwidth becomes bottleneck access flash channel mbps fetch input feature vector chip assume ssd channel chip per channel chip accelerator budget channel accelerator chip accelerator scratchpad scratchpad increase complexity recall chip accelerator access data channel bus flash interface chip minimal bandwidth frequency chip accelerator stationary WS data maximize reuse minimize bandwidth requirement across channel bus chip accelerator cannot bus  accelerator additional responsibility schedule lockstep manner perform execution across chip accelerator channel chip accelerator systolic array PEs organize configuration PEs chip accelerator chip scratchpad memory chip memory bandwidth however increase consumption chip therefore chip accelerator DeepStore mainly limited compute capability accelerator placement DeepStore regular operation access application specific feature vector DeepStore accelerator perform operation flash accelerator implement flash chip multiplexed regular accelerator response accelerator query operation ssd controller responds regular operation signal DeepStore accelerator introduce overhead regular storage operation   ObjectID query qcn max scn topk confidence max query cache query feature extractor query feature extractor threshold query cache query cache dataset query cache discus leverage temporal locality semantic similarity query improve overall performance DeepStore programmable similarity software query cache QC resides dram commodity ssd controller QC entry tag query feature vector  valid valid database feature vector  address location ObjectID ObjectID physical address feature vector ssd exploit temporal locality query cache recent query incoming query exists QC cached return without scan dataset however cannot exploit semantic similarity exist query described beach traditional cache cache checked QC exploit semantic similarity query boost performance intelligent query workload QC intuition intelligent query tolerate error detect similarity query confidence improve performance highly accurate model guarantee confidence comparison avoid unnecessary query therefore propose query similarity query comparison network qcn structure scn described although neural network comparison additional overhead scan dataset scn qcn query return similarity qcn along qcn accuracy qcn acc quantify cached query confidence algorithm query  cached entry query offloads execution qcn DeepStore channel accelerator comparison generates qcn qcn acc occurs query  complement within specify threshold threshold hyper parameter depends model tune deployment query selects entry maximum complement beyond threshold  micro october columbus usa  qureshi variable QC qcn acc dataset procedure lookup FV thr  max index index max QC valid qcn qcn FV QC qcn qcn acc max max index max max index max thr  QC promote max index return scn FV QC max index feature feature scn FV dataset QC insert FV feature return feature algorithm query algorithm query cache qcn scn query similarity comparison network respectively entire dataset scn qcn accuracy qcn similarity threshold metric exploit QC entry depends query feature vector application database feature vector cached DeepStore query database feature vector KB KB user define ObjectID byte reid application query feature vector query database feature vector KB query query cache KB QC update lru replacement policy developer similarity  model  define upper bound error threshold tolerate application DeepStore runtime discus DeepStore diverse intelligent query application discus query schedule DeepStore accelerator discus program api developer manipulate feature database specify scn model query query software ssd embed core responsible consume query manage QC schedule DeepStore accelerator aggregate accelerator computation due QC query accelerator physical address feature database query cache metadata database ssd dram facilitate access dataset query execution reduce parallel model schedule scn computation DeepStore accelerator QC query user specify model address accelerator informs accelerator location model ssd dram computation perform per feature vector query distributes physical DeepStore api api description  addr num num feature addr database specify  addr num feature vector database num feature feature byte source data location specify addr memory newly database return  addr num appends num feature feature vector database specify source data location specify addr memory  load scn computational model model specify byte DeepStore load model model return query  model accel submit query feature vector specify  byte specifies retrieve scn model specify model sub database specify location accel specifies accelerator query return retrieve  query addr retrieve byte query specify query location addr  qcn qcn thr configures QC qcn model specify qcn qcn feature vector byte threshold thr address feature database accelerator query instructs accelerator ssd dram contains dataset feature vector associate ObjectID similarity query merges generate program api DeepStore enables programmer feature vector database accelerator propose apis    query  described apis internally nvme command interact query application developer database feature vector DeepStore   respectively feature database DeepStore generate byte metadata byte physical address database byte feature byte feature byte mapping database metadata along DeepStore guarantee feature database format feature database user feature however intelligent query generally  workload typically database query upon database typically update database feature vector data item feature vector data item DeepStore handle append fashion   apis DeepStore buffer writes ensure alignment criterion fulfil metadata database update accordingly  api transfer computational graph model specify onnx format register ssd enables DeepStore apis integrate framework successful execution api return model ID application target query specific model query api transfer query feature host ssd dram query api application developer specify feature database query information model transfer ssd DeepStore storage acceleration intelligent query micro october columbus usa DeepStore accelerator configuration DeepStore exploit output stationary OS dataflow ssd channel accelerator stationary WS dataflow chip accelerator ssd channel chip technology configuration systolic OS systolic OS systolic WS PEs arithmetic precision FP frequency mhz mhz mhz scratchpad MB KB KB accelerator query accel accelerator query successful transfer ssd api return query  api return query specify query host memory location api argument  api allows user configure QC qcn feature vector threshold application  implementation implement DeepStore simulator construct SSDSim sim modify sim query cache wise layer memory hierarchy described sim modify generate access memory hierarchy trace load dataset feature vector flash feature database multiple flash access depends feature vector flash flash access trace generate modify sim input ssd sim modify ssd sim generate overall execution query batch accelerator multiple channel chip accelerator sim ssd sim modify generate accept parallel access flash channel chip simulator implement query trace query simulated baseline comparable query trace application baseline gpu ssd pas input query simulator parameter simulation assume access latency ssd scratchpad cycle channel chip scratchpad cycle float maintain accuracy application evaluation evaluation DeepStore improves performance intelligent query application remove storage bottleneck exploit parallelism inside ssd remains performant flash chip latency efficiency intelligent query query cache improve overall performance exploit temporal locality semantic similarity intelligent query experimental setup DeepStore intelligent query processing consist ssd feature database gpu execute similarity comparison assume query feature extract pre processing application described evaluation float operation DeepStore gpu ssd nvidia titan volta gpu server machine core skylake intel cpu 6GHz 4GB dram TB intel DC pcie ssd external bandwidth ssd  simulator described evaluate DeepStore assume flash array access latency channel flash chip per channel per chip per per flash KB flash channel bandwidth mbps DeepStore accelerator evaluate frequency mhz ssd channel accelerator mhz chip accelerator compute overall accelerator utilization exist model described arithmetic operation access memory PE utilization factor flash chip access sim linear model convert metric consumption accelerator neural network layer application arithmetic CACTI estimate utilization SRAMs technology node assume  model SRAMs ssd channel accelerator  model SRAMs chip accelerator due constraint dram assume consumption flash access intel DC ssd compute consume flash access extrapolate network chip estimate CACTI disable query cache evaluate performance individual component DeepStore architecture ssd feature database 5GB feature vector summarize experimental discus query cache performance DeepStore performance evaluate performance accelerator DeepStore wimpy core inside ssd gpu ssd core ARMA cpu wimpy core inside ssd controller batch reid mir ESTP tir TextQA respectively batch picked gpu utilization maximize scn computation performance gain wimpy core DeepStore gpu application chip accelerator execute reid due limited compute chip memory resource micro october columbus usa  qureshi application evaluation summarize DeepStore improvement performance traditional gpus application parallelism speedup efficiency improvement reid ssd channel mir ssd channel chip ESTP ssd channel chip tir ssd channel chip TextQA ssd channel chip reid mir ESTP tir TextQA speedup wimpy core ssd channel chip performance comparison wimpy core ssd channel chip accelerator traditional gpu ssd DeepStore performs faster gpu ssd chip channel accelerator respectively wimpy core gpu ssd baseline storage accelerator parallelism wimpy core obvious DeepStore performs wimpy core henceforth exclude comparison wimpy core remain evaluation ssd accelerator accord DeepStore ssd accelerator performs application ssd accelerator performs gpu ssd intelligent query workload although ssd accelerator access ssd internal bandwidth suffer storage bottleneck performance becomes limited performance similarity comparison query feature database lack parallelism channel accelerator channel accelerator performance channel accelerator perform gpu ssd baseline ssd accelerator respectively performance gain attribute removal storage bottleneck exploitation ssd internal channel parallelism reuse scratchpad chip accelerator DeepStore chip accelerator limited compute chip memory resource cannot execute model reid limitation latency channel accelerator application fully layer mir ESTP tir however due channel chip parallelism exploit removal latency pcie chip accelerator performs gpu ssd mir ESTP tir TextQA respectively DeepStore channel accelerator achieves performance parallelism exploit resource utilized per accelerator ssd sensitivity analysis ssd parameter evaluate sensitivity DeepStore impact flash latency latency flash array  model ssd  model commodity ssd remain performant flash chip performance channel chip accelerator respectively performance flash latency  baseline evaluation flash latency affect performance ssd accelerator gpu ssd bound compute latency external ssd bandwidth respectively channel chip accelerator decrease flash latency improve performance accelerator significantly however flash latency increase factor  performance reduce channel chip accelerator respectively variation performance accelerator bound compute latency DeepStore accelerator cheaper latency flash chip obtain reasonable performance impact external internal ssd bandwidth evaluate performance internal external bandwidth mir evaluation however intelligent query application mention exhibit behavior internal ssd bandwidth channel inside ssd channel increase beyond performance gpu ssd limited external ssd bandwidth pcie  due limited external bandwidth internal ssd bandwidth expose gpu ssd ssd accelerator performance bound compute latency application exploit parallelism however performance channel chip accelerator linearly channel improves internal bandwidth parallelism exploit gpu ssd exploit multiple SSDs aggregate bandwidth reading dataset batch SSDs host memory although performance traditional improves SSDs rate SSDs DeepStore gpu ssd storage performance improves compute scn remains constant compute capability DeepStore linearly SSDs efficiency evaluate efficiency DeepStore evaluate efficiency acceleration DeepStore volta gpu consumption DeepStore storage acceleration intelligent query micro october columbus usa latency ratio speedup reid mir ESTP tir TextQA traditional latency ratio speedup DeepStore ssd latency ratio speedup DeepStore channel latency ratio speedup DeepStore chip flash latency performance normalize performance flash latency  flash latency quadruple  DeepStore remains within performance flash latency  channel speedup traditional ssd channel chip internal ssd BW SSDs speedup external BW internal ssd bandwidth BW channel ssd external bandwidth SSDs performance mir normalize traditional performance ssd consist channel reid mir ESTP tir TextQA normalize perf watt ssd channel chip efficiency DeepStore normalize volta gpu traditional volta gpu nvidia smi channel accelerator efficient performance per watt volta gpu channel accelerator systolic array chip SRAM utilization scratchpad ssd accelerator efficient gpu reid compute intensive workload gpu workload however efficient gpu data intensive workload TextQA gpu chip accelerator efficiency volta gpu however due stricter resource constraint systolic array chip SRAM achieves efficiency channel accelerator breakdown DeepStore ssd accelerator mainly consumes memory access flash access channel accelerator consumption dominate memory access mainly due reuse MB scratchpad reid mir ESTP tir TextQA CP CP CP CP CP percentage compute memory flash consumption breakdown DeepStore ssd accelerator channel accelerator chip accelerator CP application channel accelerator chip accelerator consume access flash reid ssd channel accelerator flash access feature vector flash query cache performance demonstrate benefit query cache intelligent query application tir evaluation  entity dataset without affect truth image 2GB feature vector query universal encoder SNLI corpus incoming query query cached query cache universal encoder similarity intelligent query encoder average accuracy define threshold generate query sample dataset query distribution uniform zipfian query cache query trace query performance evaluate performance query cache cache entry across query distribution error threshold entire query cache entry application millisecond significantly millisecond scan entire feature database scn query cache gpu ssd DeepStore performance gain respectively gpu ssd without query cache although gpu ssd baseline benefit query cache DeepStore benefit significantly penalty intelligent query performance DeepStore configure without query cache gpu ssd baseline cache dataset relax query comparison error threshold micro october columbus usa  qureshi traditional  traditional DeepStore traditional DeepStore  traditional rate error threshold speedup rate uniform error threshold speedup rate zipfian query cache performance query cache rate uniform zipfian distribution query entry query cache rate uniform zipfian zipfian query cache rate function cache distribution locality benefit cache reduces improves query performance rate decrease evaluate zipfian performance trend evaluate impact query cache rate cache query comparison threshold evaluation although query cache reduce rate query distribution exhibit locality zipfian benefit reduces cache suffices query cache MB tir cache entry dram ssd related storage compute explore storage storage processing application database query processing mapreduce workload signal processing data analysis leverage embed cpu ssd controller perform compute operation avoid data transfer overhead pcie unlike application intelligent query workload complex compute operation FC convd layer although execute workload wimpy embed core significantly DeepStore prior application specific hardware accelerator ssd however knowledge explore intelligent query workload storage acceleration discus offs exploit parallelism ssd dnn accelerator accelerator propose training inference computation popular dnn model additional optimization quantization prune data optimization computation improve efficiency however none accelerator incorporate flash storage optimize intelligent query workload although perform optimization quantization precision operation others optimization accelerator community incorporate DeepStore architecture gain performance efficiency possibility extension DeepStore exploit query conventional query leverage similarity data index lookup intelligent query index cannot built due non linearity introduce extract feature vector dnn model query diversity gain performance traditional exploit cache query linear cache however cannot highly accurate semantic similarity query unnecessary expensive penalty DeepStore query cache dnn similarity comparison network perform similarity lookup cache recent explore reorganize feature vector storage efficient operation technique exploit DeepStore improve performance query cache propose DeepStore leveraged domain explore future conclusion diverse representative intelligent query workload gpu ssd application limited storage bandwidth address propose DeepStore storage accelerator consists efficient storage accelerator runtime query similarity storage query cache various offs associate accelerator intelligent query parallelism ssd detailed implementation DeepStore DeepStore improves query performance efficiency gpus