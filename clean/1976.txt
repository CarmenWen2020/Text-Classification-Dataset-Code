storage critical role server highly data intensive application satisfy performance capacity demand application storage deploy array SSDs per server reduce storage employ SSDs per server storage actively perform inline data reduction data deduplication compression exist inline data reduction achieve performance scalability offload computation intensive data reduction operation dedicate hardware accelerator however exist suffer limited workload scalability reduce data incur IO request data reduction rate offload overlook memory intensive operation  scalability propose FIDR highly scalable storage enable inline data reduction grain data identify limitation exist storage server effectively resolve limitation employ optimal offload mechanism FIDR achieve applicability enable grain data reduction scalability distribute data reduction operation optimal device host processor accelerator network interface propose offload mechanism considers computation memory capacity memory bandwidth requirement altogether evaluation implement FIDR prototype FPGAs prototype outperforms stateof data reduction significantly reduce computation memory resource requirement CCS CONCEPTS information information storage computer organization architecture hardware communication hardware interface storage keywords deduplication compression fpga ssd array chunk management memory management introduction massive data generation recent data intensive application database machine gain significant popularity datacenters along component datacenters storage server evolve denser capacity per node faster throughput latency recent commercialization extremely  capacity IO device pave storage server multi terabit per tbps performance petabyte PB capacity PB capacity server rack array TB  SSDs another node flash appliance  PB capacity along tbps performance array GB SSDs minimize ssd array commercial storage server typically apply inline data reduction deduplication compression data SSDs data deduplication compression remove data redundancy database datasets virtual desktop infrastructure significant reduction data footprint improves ssd micro october columbus usa lifetime limited writes flash reduces initial per GB ssd array exist completely rely CPUs data reduction scalable therefore propose offload compute intensive task CPUs hardware HW accelerator CIDR fpga acceleration hash computation deduplication compression achieve GB data reduction throughput FPGAs unfortunately exist HW accelerate limited applicability workload access limited scalability due overlook memory management overhead identify limitation exist approach profile CIDR HW data reduction saturate tbps pcie bandwidth BW cpu socket CIDR demand GB host memory bandwidth theoretical BW GB socket identify cpu bottleneck inefficient memory management accelerator schedule straightforward computation around tbps throughput CIDR demand xeon core significantly core cpu socket core scalable data reduction perform detailed profile data reduction observation observation data reduction operation memory bandwidth capacity requirement characterization data reduction operation accelerator schedule data buffering consume host memory bandwidth GB memory capacity contrast operation data reduction metadata cache consume host memory bandwidth demand GB dram capacity observation data movement host memory extract metadata temporary data buffering complex processing compression client data happens HW accelerator host data movement become unnecessary buffering processing offload accelerator observation integration hash deduplication compression accelerator host accelerator schedule CIDR aim reduce hardware integrate hash compression core accelerator deduplication compression perform series hash core compute hash chunk compression core data eliminate deduplication non duplicate data unscalable host accelerator interaction expensive buffering accelerator CIDR address host software predict non duplicate data advance efficiently schedule batch accelerator however predictor source cpu memory bottleneck observation neither host cpu memory approach accelerator approach scalable data reduction maximum pcie BW cpu socket tbps cache cache data reduction metadata GB memory PB host memory capacity easily simply cpu index cache bottleneck detailed profile  task metadata cache management access data structure GB index GB metadata content cache contrast cpu utilization access cached metadata content GB knowledge identify accelerator management accelerator schedule cache realize scalable data reduction observation propose FIDR scalable data reduction grain inline data reduction hash offload NIC NIC buffering pcie peer peer transfer hybrid cpu memory fpga metadata cache offload hash accelerator NIC NIC detect non duplicate chunk capability enables FIDR remove cpu memory intensive predictor exists CIDR addition enable transfer unique chunk instead data chunk device minimizes pcie bandwidth requirement offload buffering client request host memory NIC advantage pcie peer peer PP feature FIDR completely bypass host memory directly transfer data across device NIC compression accelerator data SSDs scalable accelerator management data reduction finally propose hybrid host cpu memory fpga acceleration ensure scalable data reduction cache FPGAs cpu intensive memory management task index cache replacement host memory cpu access cached content prototype FIDR multiple FPGAs performance effectiveness evaluation FIDR significant speedup HW data reduction FIDR successfully reduces memory bandwidth utilization cpu utilization respectively performance analysis FIDR  PB storage server contribution critical identification knowledge identify accelerator management accelerator schedule cache realize scalable data reduction guideline building scalable storage characterize HW storage detail insightful observation resolve identify scalability limitation effective architecture implementation propose carefully data reduction architecture hash offload NIC NIC buffering pcie PP hybrid host cpu memory fpga acceleration cache prototype validate performance effectiveness organize explains data reduction baseline FIDR scalable storage grain inline data reduction efficient memory handle micro october columbus usa motivate observation propose architecture implementation explain evaluation related discussion finally concludes background data reduction component improve effectiveness storage server data reduction technique commonly deduplication eliminate data redundancy across data inter redundancy compression eliminate data redundancy inside intra redundancy data reduction component mostly deduplication briefly explain component data chunk component split client data data chunk chunk parameter fix variable chunk chunk granularity various workload commercial fix chunk variable chunk due computational overhead variable chunk fix KB chunk chunk hash compute hash chunk signature data chunk deduplication signature instead comparison raw data enables detection duplicate chunk functionally deduplication data signature hash collision ensure exist hash function sha practical collision petabyte data hash PBN metadata hash chunk chunk physical PBN existence chunk hash hash PBN storage server determines chunk duplicate non duplicate unique implementation hash PBN  bucket upon client request server modular function calculate bucket index chunk hash correspond bucket scan respective hash chunk hash unique chunk hash newly allocate PBN insert respective bucket entry hash PBN byte byte hash byte PBN KB chunk PB unique chunk storage hash PBN TB available dram typical therefore server cache dram SSDs SSDs LBA pba another metadata client logical address LBA chunk respective physical address pba chunk variable compress mapping LBA pba efficient data storage ssd server hash computation reduction hash PBN compression storage device SSDs DG LG DG unique chunk compress chunk hash request deduplication data chunk KB request handle storage device SSDs compress chunk request LBA decompression request data pba LBA PBN PBN pba  reduction access request handle overview data reduction storage server usually container compress chunk LBA pba internally LBA PBN mapping array index LBA PBN container PBN pba mapping array index PBN offset address container compress chunk PBN container offset container server simply calculates pba multi TBs PB storage entry byte PBN byte offset byte compress workload usually exhibit address locality dram cache LBA pba performance therefore focus hash PBN cache LBA pba cache operation data reduction data reduction deduplication compression series upon client request storage server split data chunk server applies deduplication remove data chunk already SSDs chunk duplicate server computes hash chunk hash hash PBN server detects chunk duplicate update LBA pba chunk unique server compress chunk content storage device SSDs server update hash PBN LBA pba metadata unique chunk handle request storage server receives client request data specific LBA specific server pba request chunk LBA LBA pba compress data chunk pba SSDs  sends client baseline HW data reduction exist proposes HW data reduction accelerate compute intensive operation modify recent HW data reduction CIDR baseline architecture CIDR array FPGAs accelerate hash compression decompression data micro october columbus usa       NIC host device SSDs host memory cached content data SSDs batch host memory data buffer hash fpga hash comp compression device manager data reduction access batch scheduler unique chunk predictor host memory compress buffer compress predict unique chunk cache fetch flush compress unique chunk unique detection update       NIC host device SSDs host memory LBA pba cache data SSDs hash fpga hash comp hash fpga hash comp fpga decompression LBA pba cache task host memory decompress buffer cache fetch flush compress chunk request correspond pba LBA decompression chunk data buffer data chunk simplify operation baseline HW accelerate hash computation compression decompression reduction metadata host multiple interaction FPGAs hash detect unique chunk informs FPGAs compress unique chunk acceleration batch chunk accelerator expensive memory host detects unique chunk informs accelerator compression unique chunk alternative data transfer FPGAs hash compression approach limit scalability therefore CIDR software module predicts unique chunk deduplication enables transfer batch FPGAs fpga hash hash chunk compression simultaneously compress predict unique chunk due CIDR target capacity TBs chunk author assume data reduction memory PB target capacity KB chunk data reduction multi TB memory capacity therefore assume data reduction dedicate SSDs SSDs software module manages cache host memory modify CIDR request handle baseline server NICs client data buffer host memory unique chunk predictor data buffer predicts chunk unique batch scheduler chunk prediction sends fpga array FPGAs perform hash chunk compress predict unique chunk FPGAs transfer compress unique chunk along hash chunk host memory software management validity prediction hash hash PBN cache host memory KB KB KB KB  mail normalize IO workload significant increase IO request chunk due modify writes deduplication degradation target bucket unavailable host memory server fetch bucket SSDs finally baseline writes compress validate unique chunk data SSDs request handle simpler upon request client server NICs LBA request host host software LBA LBA pba respective pba compress chunk compress chunk data SSDs host memory data host memory fpga array decompression FPGAs decompress data host memory NIC data sends client motivation motivate chunk profile baseline hardware data reduction identify bottleneck memory cpu limitation chunk data chunk KB chunk CIDR reduces data reduction access frequency however chunk suitable workload chunk suffers frequent modify writes increase IO overhead simulated deduplication chunk request trace mail server  assume MB request buffer deduplication due existence KB access randomness LBA access deduplication KB chunk cannot easily KB chunk client request deduplication module fetch KB SSDs KB chunk applies deduplication KB chunk unique writes chunk storage operation significant additional writes furthermore chunk deduplication degrades duplicate detection capability writes overall chunk bottleneck ssd bandwidth therefore KB chunk limitation exist HW approach profile naively extend CIDR recent HW data reduction extension enables chunk storage capacity focus offload straightforward compute intensive operation hardware accelerator memory related bottleneck profile workload workload FIDR scalable storage grain inline data reduction efficient memory handle micro october columbus usa memory BW utilization GB throughput GB mixed  max memory bandwidth bottleneck baseline  data reduction physical cpu core throughput GB mixed cpu socket throughput bottleneck mixed unique chunk predictor data reduction cache management NIC fpga data ssd driver cpu utilization breakdown cpu bottleneck baseline data reduction request deduplication compression ratio another workload writes mixed request deduplication compression ratio CPUs GB tbps theoretical pcie IO bandwidth conservatively target throughput GB per socket throughput theoretical GB conservatively various IO overhead dma management socket independent cpu core independent memory IO bus simplicity measurement memory bandwidth cpu utilization per socket basis memory bandwidth bottleneck achieve GB  data reduction baseline memory bandwidth socket theoretically memory bandwidth baseline data reduction throughput GB GB linearly project trend measurement GB baseline GB memory bandwidth GB mixed socket maximum memory channel theoretical bandwidth socket GB conservative estimation limit baseline throughput GB target throughput cpu bottleneck HW baseline infeasible cpu core mostly non computational task memory IO device management measurement projection throughput assume memory bandwidth cpu core socket GB core typical cpu core xeon cpu utilization breakdown cpu utilization cpu utilization mixed due memory management accelerator schedule related task majority overhead data reduction cache management remain unique chunk predictor fpga batch schedule CHALLENGES  perform detailed characterization baseline memory cpu bottleneck reveal challenge address mention scalability limitation baseline resolution memory bottleneck naive replace host dram emerge bandwidth memory HBM however approach effective practically feasible due limited HBM capacity price generation HBM GB memory capacity package however PB ssd array multi TB data reduction cache GBs host memory amount memory HBM impractical due modification motherboard therefore storage typical dram memory analysis memory bandwidth usage baseline observation observation data reduction operation memory bandwidth capacity requirement characterization baseline memory bandwidth operation GB memory capacity contrast data reduction cache memory bandwidth GB memory capacity observation breakdown memory bandwidth utilization memory capacity requirement baseline component data memory BW memory BW mixed memory capacity NIC host memory KBs MBs host memory unique prediction MBs host memory FPGAs MBs cache management GB host memory data ssd KBs MBs micro october columbus usa bandwidth capacity intensive operation memory scalable data reduction observation baseline consumes host memory bandwidth data processing data buffering operation data reduction software buffer data host memory IO device another NIC fpga data hostside processing unique chunk predictor host operates buffer data data fpga operation data reduction host memory complex data processing compression FPGAs observation potential reduce host memory achieve scalable storage resolution schedule overhead observation integration hash compression accelerator host accelerator task schedule CIDR aim reduce hardware integrate hash compression accelerator however integration host software predict unique chunk advance efficiently schedule HW compression core throughput predictor source bottleneck utilize cpu resource memory bandwidth resolution cache overhead observation neither host cpu memory approach accelerator approach scalable data reduction cache data reduction management component cpu utilization overhead naive fully offload cache management accelerator however approach scalable PB capacity tbps throughput accelerator usually memory limited pcie bandwidth GB dram GB pcie bandwidth VCU fpga limit cache rate cache therefore host approach accelerator approach severely limit storage capacity throughput characterize cpu utilization memory capacity requirement component data reduction cache management cpu overhead consume operation data structure operation structure cache index GB normalize cpu utilization memory capacity requirement component cache component cpu util normalize memory data structure memory capacity cache index node GB accelerator ssd access IO queue KB MBs accelerator cache content access cache content GB host cache item replacement management lru MBs host accelerator GB cache ssd software stack cache replacement KB queue operation directly GB cache content scan cached content bucket cpu overhead characterization suggests scalable cache data reduction cpu host memory cache content offload index management operation HW accelerator goal challenge observation achieve  PB data reduction goal scalable accelerator schedule address cpu memory intensive task schedule HW data reduction suffers scalable accelerator management address hostside memory bandwidth utilization client data buffering data across accelerator NIC SSDs scalable data reduction cache address limited throughput scalability straightforward cache cpu bottleneck cpu cache accelerator memory IO bandwidth bottleneck accelerator cache FIDR scalable grain inline data reduction FIDR achieve goal ensure performance capacity scalability significantly simplify accelerator schedule offload hash dedicate accelerator NIC enable detection unique chunk FIDR NIC hash cpu memory intensive unique chunk predictor redundant remove predictor accelerator schedule addition detection unique chunk NIC transfer unique chunk instead data chunk device minimizes pcie bandwidth usage improves scalability achieve scalable accelerator management offload client request data buffering host memory NIC advantage pcie peer peer feature data transfer across NIC compression accelerator data SSDs operation almost completely bypass host memory maximizes performance scalability propose hybrid host cpu memory fpga acceleration ensures scalable data reduction cache hybrid approach fpga logic fpga memory cpu intensive operation memory capacity cache index host memory cached content cpu lightweight operation directly access cached content propose hybrid acceleration cpu overhead mostly eliminate data operation cpu fpga realize boost scalability cache FIDR scalable storage grain inline data reduction efficient memory handle micro october columbus usa FIDR NIC array host device compression comp comp client data LBA scheduler FIDR device manager chunk buffer index cache cache fetch flush management SSDs host memory lru cached content hash   chunk IDs cache content access unique   data SSDs cache fetch flush  cache  hash unique data compress unique data compress LBA FIDR NIC array host device compression decomp decomp client LBA FIDR device manager chunk buffer index cache cache management SSDs host memory cached LBA pba LBA pba cache access pba cache fetch flush pba  data  data data SSDs nvme command request LBA chunk buffer data FIDR NIC array host device compression comp comp client data LBA scheduler FIDR device manager chunk buffer index cache cache fetch flush management SSDs host memory lru cached content hash   chunk IDs cache content access unique   data SSDs cache fetch flush  cache  hash unique data compress unique data compress LBA FIDR NIC array host device compression decomp decomp client LBA FIDR device manager chunk buffer index cache cache management SSDs host memory cached LBA pba LBA pba cache access pba cache fetch flush pba  data  data data SSDs nvme command request LBA chunk buffer data propose architecture FIDR component FIDR component hardware accelerator module FIDR software SSDs briefly explain  component FIDR NIC NIC scalable data reduction FIDR NIC NIC buffering client request hash data request task batch schedule unique chunk transfer dedicate accelerator FIDR compression decompression FIDR dedicate accelerator compression decompression FIDR compression decompression request handle accelerator receives batch unique chunk compression request handle  batch compress chunk FIDR cache HW consists hardware module FIDR accelerate cache hardware index management fetch flush cache FIDR software host software FIDR orchestrates communication FIDR HW device addition software module FIDR cache management scan cached content host memory simplify operational simplify FIDR FIDR NIC receives client request data LBA buffer request NIC immediately acknowledges completion client client suffer latency chunk buffer FIDR NIC calculates hash chunk sends hash FIDR device manager module FIDR software manages communication hardware device FIDR hash calculates location information bucket location sends FIDR cache HW FIDR cache HW cache index location target bucket cache hash FIDR cache HW fetch respective bucket SSDs host cache location information FIDR software scan target cache host memory determines duplicate unique status chunk FIDR NIC receives unique duplicate status chunk schedule batch unique chunk sends FIDR compression compress chunk compression threshold MB sends compress metadata associate chunk LBAs host finally FIDR software selects empty location specific data ssd informs ssd metadata compress data batch metadata compress batch compression pcie address ssd destination address specify data ssd fetch compress data memory FIDR compression writes data flash finally FIDR software update cache content newly data request handle FIDR complexity request upon request client FIDR NIC NIC micro october columbus usa ethernet tcp IP storage protocol iSCSI chunk buffer pcie dma hash host meta pba etc uniq dup flag LBA buffer hash LBA lookup compression scheduler LBA buffer data chunk buffer propose NIC microarchitecture minimize inter device communication overhead buffer previously request LBA buffer sends data client otherwise LBA FIDR software host software LBA pba pba compress request LBA FIDR device manager software orchestrates data transfer data SSDs FIDR decompression without host memory involvement transfer decompress data FPGAs NIC finally NIC sends request data client microarchitecture detail FIDR NIC FIDR cache HW FIDR NIC microarchitecture FIDR NIC performance NIC data reduction acceleration client request arrives performance NIC NIC usually task processing ethernet packet instead packet data host processing NIC par content deeper layer performance NIC par tcp IP packet decodes client request embed storage protocol layer iSCSI FIDR NIC additional layer buffering processing data reduction FIDR NIC specific operation request handle FIDR NIC buffer data LBAs respective NIC buffer hash chunk batch request sends hash host via pcie dma controller host determines status chunk unique duplicate FIDR NIC receives information uniqueness flag along additional metadata destination chunk compression scheduler scan unique duplicate flag prepares batch unique chunk finally FIDR sends batch unique chunk along correspond metadata pba compression pcie dma controller request handle FIDR NIC buffer LBAs incoming request LBA buffer LBA lookup module scan LBA buffer request lookup FIDR NIC associate data data buffer chunk buffer encodes storage protocol iSCSI sends encapsulate data tcp IP core ethernet core finally client lookup FIDR NIC sends LBAs pcie dma controller host host orchestrates transfer request compress data SSDs FIDR decompression  data FIDR software informs FIDR NIC fetch pcie dma controller host lookup bucket index bucket nvme controller request request cache manager cache insert delete lookup delete lookup delete fpga propose FIDR cache HW microarchitecture accelerate operation data reduction cache decompress data decompression memory finally FIDR NIC load decompress data encodes storage protocol sends client FIDR cache HW microarchitecture FIDR cache HW reduces host pressure hardware index hash PBN cache hardware acceleration manage cache replacement hash PBN access request FIDR cache HW handle client request microarchitecture FIDR cache HW host calculates target bucket location access data reduction sends batch bucket index cache HW cache HW update bucket index ssd location host cache handle ssd access fetch cache flush dirty cache ssd upon replacement batch request handle cache HW sends batch cache location host therefore FIDR host software access cache content duplicate unique chunk detection bucket index exist structure host cache FIDR cache HW selects cache queue request ssd bucket fetch insert bucket index cache index ensure empty FIDR cache  periodically delete cache flush content ssd dirty host software access cache content cache lru host minimize interaction host FIDR HW periodically receives batch lru item deletion handle remain operation completely HW optimization concurrent HW update throughput data reduction throughput index efficiently cache lookup replacement requirement becomes important locality hash PBN access however challenge resource limited FPGAs fully concurrent update request insert deletes due node conflict update request modify node FIDR scalable storage grain inline data reduction efficient memory handle micro october columbus usa request queue pipeline update pipeline request crash replay controller command generator memory replay request update signal propose optimization FIDR cache HW concurrent update fpga index merge split regard exist significantly limited concurrent update concurrent deletes handle concurrent update request propose improve hardware index HW command generator signal request structure manage entry crash replay controller maintain HW pipeline update stage pipeline height concurrent update request speculative execution recovery hash highly random stage GB cache index multiple request update node extremely workload therefore speculatively issue multiple update request recovery speculation rate speculation performance penalty negligible HW guarantee correctness concurrent update delete request cache replacement command generator issue update request update pipeline series pipeline request information traverse node update pipeline reverse leaf reverse traversal request prior request node algorithm node exist speculation request invalidate HW postpones apply request speculation resolve commit request crash replay controller examines crash request speculation algorithm speculation crash replay controller sends update signal applies otherwise crash replay controller ignores request insert request queue replay replay incurs negligible performance overhead replay rarely happens logic pcie bandwidth consideration ensure scalability FIDR choice pcie bandwidth utilization pcie transfer direction multiple device NIC compression data SSDs algorithm issue update request nth node update node request node request speculatively update node spec update node node node spec update node request crash speculative execution request update node pcie switch ensures maximum peer peer pcie bandwidth choice offload metadata cache management cache HW cache content host memory guarantee scalable metadata cache communication cache HW negligible pcie bandwidth MB GB data reduction byte cache index per KB request cache content SSDs access content host memory access pcie complex massive bandwidth GB amd epyc cache rate complex bandwidth cache rate FIDR incurs pcie bandwidth overhead GB pcie bandwidth GB data reduction assume cache rate implementation overall implement HW accelerator FIDR fpga custom NIC another compression cache HW FIDR compression baseline HW remove hash core offload NIC fpga transfer metadata compress data etc host compress data transfer data SSDs explain implementation detail FIDR NIC along FIDR cache HW respectively micro october columbus usa data SSDs SSDs FIDR cache HW FIDR compression HW FIDR NIC active socket cpu FIDR prototype operation implement software module FIDR linux kernel device driver software management nvme data SSDs implement hardware management nvme SSDs request data SSDs compress chunk sequential tolerable overhead therefore simplicity nvme submission completion queue data SSDs host memory default data compression instead host memory contrary ssd access random significant management overhead cpu therefore ssd submission completion queue HW cache modify ssd driver FIDR NIC implement FIDR NIC fpga target throughput gbps tcp offload implementation consist gbps instance optimize network packet scenario storage environment client request data KB simplify protocol instead protocol iSCSI storage access protocol protocol client acknowledgment data request acknowledgment request encode mainly operation acknowledgment request address LBA data hash instance source sha core FIDR cache HW implementation FIDR cache HW consists component index cache management nvme ssd controller rely exist fpga balance core index apply modification boost capacity throughput unlike maximum per node stage node leaf stage gap leaf stage non leaf stage enables non leaf stage  accessible chip memory VCU fpga increase leaf node allows index GB cache fpga chip summary workload workload dedup ratio comp ratio cache rate trace medium IOs mail IOs 4GB medium medium IOs mail IOs 4GB medium medium IOs  IOs 0GB mixed writes writes random valid address memory leaf node fpga dram modification concurrent pipelined update explain cache management implement circular buffer fpga dram due cache dram access sequential entry therefore access ddr controller bus entry negligible dram access overhead evaluation environment methodology evaluate scheme endto involve machine client storage server cpu utilization host memory bandwidth utilization rate network bandwidth limitation prototyping environment machine performance propose hybrid cache acceleration simulation model previous measurement simulate behavior tbps per socket target throughput baseline CIDR HW data reduction capability chunk software cache  cache KB cache correspond KB bucket mapping bucket index cache source perform baseline server machine active socket intel cpu samsung pro TB SSDs data SSDs SSDs VCU xilinx fpga FIDR NIC FIDR compression FIDR cache  TB unique compress data storage data SSDs GB SSDs assume deduplication ratio compression ratio evaluate scheme request however mainly focus intensive workload become widespread enterprise environment directly affect ssd lifetime generate IO workload trace enable various workload extract portion trace workload building factor index intel palm  reduction operation depends data content access due security concern public IO trace data content IO FIDR scalable storage grain inline data reduction efficient memory handle micro october columbus usa normalize utilization host memory BW workload baseline NIC acceleration D2D NIC acceleration D2D cache acceleration reduce host memory BW utilization FIDR normalize cpu utilization workload baseline NIC acceleration D2D NIC acceleration D2D cache acceleration FIDR eliminate cpu utilization bottleneck memory IO management overhead data reduction generate workload extract portion trace achieve target cache rate fix cache replicate extract trace generate workload prevent replication duplicate workload apply minor systematic modification trace content across replicate deduplication ratio trace replicates replicate compressibility concatenate compressible trace request reduction assume GB unique compress storage memory cache detail workload mixed memory bandwidth utilization FIDR reduces host memory bandwidth utilization workload mixed workload perform data reduction task NIC along exploit PP capability pcie device NIC fpga ssd transfer enable FIDR eliminate host memory bandwidth pressure cache rate memory bandwidth cache FIDR becomes effective remove memory bandwidth pressure  FIDR remove pressure host memory handle request portion overhead handle request successful reduction memory bandwidth utilization cpu utilization FIDR reduces cpu utilization workload mixed workload FIDR NIC hash remove unique chunk predictor trace applicable deduplication compression  IO trace hash data writes along IO address throughput GB workload     boost throughput propose  mechanism fpga index throughput GB workload baseline NIC acceleration D2D cache acceleration FIDR multi update index overall throughput evaluation technique scheme baseline reduces cpu utilization hardware offload memory IO management task cache FIDR reduces cpu utilization workload cache rate  frequent update ssd writes therefore baseline cpu core achieve throughput however FIDR eliminates increase overhead memory management cache acceleration throughput FIDR fpga cache index cache allocation achieve GB update almost linear scalability multi update optimization workload cache replacement index update update index GB throughput propose crash replay optimization enables concurrent update HW structure almost linear throughput scalability GB linear performance increase assign negligible overhead concurrent update crash replay propose scheme workload cache rate benefit optimization throughput becomes limited GB due saturate fpga dram bandwidth overall throughput FIDR achieves significant throughput improvement workload mixed workload simulation model cpu utilization memory bandwidth throughput FIDR cache HW project throughput assume core cpu intel xeon micro october columbus usa resource utilization FIDR custom NIC workload data reduction NIC tcp offload LUTs flip flop brams mixed workload data reduction NIC tcp offload LUTs flip flop brams resource utilization FIDR cache HW ssd access medium BW cache MB MB MB chip chip ssd BW GB estimate max throughput GB GB GB fpga resource LUTs flip flop chip bram chip URAM propose NIC acceleration data transfer across pcie device speedup baseline  cache management throughput reduction throughput degradation due throughput HW index update capability optimization multiple concurrent update HW index throughput boost optimization increase throughput mixed due inherent cpu utilization overhead data ssd software stack handle request offload nvme software stack fpga future request latency latency FIDR affect commit latency request latency data reduction achieve assume battery enables non volatile buffering FIDR NIC server buffer client request non volatile buffer server immediately sends completion client hiding backend operation latency commercial storage server hide latency backend NVRAM battery latency FIDR shorter datapath SSDs NICs improves latency baseline HW data reduction latency client KB request batch request measurement FIDR reduces server latency SSDs NICs  baseline FIDR normalize throughput GB TB normalize throughput GB TB normalize throughput GB TB scalability FIDR throughput significantly reduce storage axis fpga resource utilization FIDR NIC data reduction module FIDR NIC consume negligible fpga resource practically sha hash core ddr controller data buffering consume fpga resource minimal workload propose architecture bram LUTs fpga resource utilization mixed workload hash resource utilization becomes LUTs brams default operation NIC storage ethernet packet handle tcp offload iSCSI processing implement fix ASIC data reduction easily implement FPGAs FIDR cache HW FIDR cache HW achieves GB lut utilization medium index estimate GB index capability cache workload suitable cache MB cache index completely fpga chip memory PB cache GB becomes throughput closer PB cache leaf node fpga dram medium index acceleration GB estimate GB performance achievable rely fpga SRAM chip URAM increase depth chip memory stage analysis calculate FIDR amount data SSDs data reduction deduplication compression treat remain data SSDs data reduction data reduction cpu fpga dram SSDs price online amazon vendor website assume GB SSDs GB dram core cpu intel fpga xilinx  cpu fpga resource utilization assume fpga resource practically usable FIDR scalable storage grain inline data reduction efficient memory handle micro october columbus usa FIDR baseline  reduction normalize data SSDs SSDs dram cpu fpga effectiveness FIDR breakdown component GB TB effective capacity FIDR significantly reduces storage data reduction throughput benefit ssd reduction outweigh CPUs FPGAs throughput increase FIDR overhead increase however target storage capacity usually increase performance FIDR effective capacity TB target capacity FIDR GB GB FIDR baseline throughput FIDR throughput GB per socket baseline fails achieve therefore baseline partial data reduction throughput significantly increase FIDR discussion focus data reduction acceleration properly load balance workload environment  environment workload environment skewed access resource contention exist focus contention easily integrate exist modify FIDR software address cache contention instead lru replacement policy prioritize lru policy considers workload locality imbalanced access data SSDs extend FIDR software LBA pba maintain frequently access memory related exist data reduction plenty focus implement optimize softwarebased data reduction improves deduplication throughput sample chunk apply deduplication aim reduce cache latency exploit content locality adopt metadata prefetching overall disk HDDs backup storage environment furthermore exist software data reduction cannot throughput ssd array therefore recent focus offload compute intensive data reduction operation cpu accelerator gpu fpga accelerate variable chunk fpga compression fpga deduplication offload hash compression FPGAs adopt exist baseline scalability limitation due memory overhead ssd deduplication ssd compression propose another offload cpu overhead IO device however CIDR ssd data reduction approach suffer deduplication compression opportunity ssd array lastly exist modify cache algorithm improve rate  statistical request enables request prefetching improve cache rate mostly aim reduce access latency storage HDDs technique orthogonal adopt scalable storage optimize data movement exist PP capability pcie device data transfer across device technique technology enablers data transfer across NIC fpga ssd adapt data reduction environment device device transfer GPUDirect async enables data transfer gpus NICs exist focus improve performance unify couple software stack reflex tightly couple network storage stack guarantee stable access SSDs remote node smart NICs programmable NICs become recent offloads host networking stack NICs focus load balance efficient microservice execution server smart NICs unlike NIC offload scalable accelerator management accelerator schedule data reduction conclusion conclusion propose scalable storage enable grain inline data reduction propose data reduction friendly NIC architecture data transfer across NIC fpga SSDs minimize host dram bandwidth utilization propose hybrid cpu fpga acceleration data reduction cache scheme significantly improve memory management scalability boost overall throughput