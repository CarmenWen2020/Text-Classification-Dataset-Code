neural network become compelling application image classification detection recognition machine translation however excessive computation due provision parameter improve computation efficiency neural network prune technique propose reduce amount accumulate mac operation sparsity network unfortunately sparse neural network dense counterpart gpus due device utilization rate sophisticated hardware primitive tensor core deployed boost performance dense matrix multiplication magnitude performance sparse neural network lag significantly propose algorithm hardware methodology accelerate sparse neural network novel prune algorithm devise improve workload balance reduce decode overhead sparse neural network meanwhile instruction micro architecture optimization propose tensor core adapt structurally sparse neural network experimental prune algorithm achieve performance gain model accuracy sustain furthermore hardware optimization additional performance gain negligible overhead CCS CONCEPTS computer organization instruction multiple data keywords neural network graphic processing prune introduction neural network dnns achieve performance task image recognition recognition processing underlie representational neural network parameter extremely amount computation memory usage plenty prior improve performance efficiency neural network various hardware platform gpus FPGAs ASICs prior sparsity centric optimization technique exploit sparsity activation achieve outstanding convolutional neural network cnns recurrent neural network rnns leverage intrinsic redundancy neural network various sparsifying technique prune technique sparsity matrix instance prior report sparsifying retrain sparsity negligible impact model accuracy nonetheless sparsity necessarily guarantee sparse neural network performant dense counterpart due irregularity data layout sparse neural network hardly obtain performance gain graphic processing gpus sparse library  encodes sparse matrix compress sparse csr format sparse matrix prune sparsifying random non zero csr format workload balance consequence gpu extremely underutilized sparse library kernel matrix multiplication gemm contiguous optimization gpus micro october columbus usa  zhu tao zhang  yuan xie fundamental primitive popular neural network nvidia built performance gemm kernel tune machine code entity  library addition tensor core introduce volta architecture peak tflops FP cuda core tflops tflops unfortunately tensor core focus acceleration dense matrix multiplication sparse gemm cannot advantage tensor core speedup sparse neural network sparsifying address performance issue structural sparsifying propose remove entire matrix structurally prune matrix sustain  tensor core achieve performance however coarse grain prune matrix negative impact model accuracy although prior report comparable accuracy structurally prune network datasets mnist significant accuracy neural network structural prune apply detail propose VectorSparse simd instruction multiple data friendly sparsifying tackle VectorSparse matrix multiple vector prune vector sparsity sparse matrix generate VectorSparse exhibit workload balance parallelism prune matrix improve performance extend instruction volta VectorSparse neural network tensor core extension minor enable index register file simulation VectorSparse neural network faster dense sparse counterpart negligible accuracy impact knowledge exploit efficiency sparse neural network tensor core contribution comprehensive performance analysis demonstrate inefficiency gpu sparse neural network propose VectorSparse novel sparsifying algorithm achieve performance improvement negligible accuracy extend instruction volta gpu operand index register file detail micro architecture mitigate performance bottleneck achieves performance gain negligible overhead background motivation review prior sparsity centric optimization neural network exist sparsifying technique detail sparsity centric optimization dnns recently dnns demonstrate significant redundancy parameterization parameter norm generic sparsifying unified sparsifying generic sparsifying unified sparsifying enforce sparsity matrix sparsity neural network addition parameter activation layer network posse sparsity factor stem mainly activation function relu sparsity parameter input data refer static sparsity sparsity activation depends input data therefore sparsity activation denote dynamic sparsity focus exploit static sparsity neural network accelerate inference phase application effort exploit static sparsity concentrate prune neural network sparsifying prune achieves compression ratio however randomness non zero prune matrix unable leverage sophisticated software library  hardware resource tensor core gpus hence exhibit data throughput correspond dense neural network improve efficiency sparse neural network gpus propose structural sparsifying structural sparsifying spatial constraint non zero  output matrix sparsifying generate dense matrix parameter advantage dense gemm library structural sparsifying performance incurs severe accuracy commercial model restrict spatial constraint network exist sparsifying gpus sparsifying classify category generic sparsifying unified sparsifying generic sparsifying illustrate neural network achieve speedup accuracy structural sparsifying model however cannot widely adopt sparse tensor core algorithm hardware vector wise sparse neural network gpus micro october columbus usa normalize throughput normalize performance dense counterpart sparsity alexnet conv perf alexnet conv perf alexnet conv throughput alexnet conv throughput normalize performance cache throughput generic sparse conv layer dense conv layer tesla gpu absolute matrix highlight unchanged zero sparsity matrix coordinate along arbitrary due flexibility sparsifying although generic sparsifying achieves compression ratio expose inefficiency gpus firstly variation generic sparse matrix partition workload evenly gpus secondly non zero unknown runtime optimal tile scheme data reuse thirdly computation amount highly sparse matrix hide memory access latency therefore benefit sparsity vanishes reveal sparse convolutional layer conv conv alexnet nvidia tesla gpu convolution operation layer convert gemm imcol transformation implementation sparse layer  gpu library sparse linear algebra performance gpu execution throughput cache normalize dense implementation  intuitively layer sparsity performance computation however sparse layer performant dense layer sparsity sparse layer achieve dense layer performance throughput indicates device memory bandwidth underutilized performance sparse layer bound computation compute underutilized utilization due workload balance performance conv layer exhibit workload balance refer detail setup ceil ceil ceil ceil matrix dim vector locality characterization unified sparsifying illustrate distinct generic sparsifying sparsity achieve wise sparsifying entire remain unchanged zero therefore easy encode decode coordinate information unified sparsifying evaluates norm consequence selection probability generic sparsifying remove unified sparsifying unified sparsifying coarse grain sparsifying remove entire matrix approach allows flexibility adjust cannot highly optimize dense library performance boost dense counterpart therefore opportunity sparsifying achieve performance generic sparsifying meanwhile eliminate accuracy unified sparsifying purpose highly flexible structural sparsifying desirable preserve comparable model accuracy generic sparsifying workload balance ensure performance gpus characterization sparsity sparsifying algorithm characterize spatial locality non zero sparse neural network prune resnet NMT generic sparsifying compression prune network sparsity comparable accuracy dense reference network prune split matrix multiple dim vector vector overlap split vector contains index index matrix divisible residue vector pad zero vector within sparse matrix zero vector compute local sparsity vector local sparsity define zero cumulative distribution local sparsity vector dim vector sparsity dim vector indicates dim vector non zero moreover dim vector sparsity micro october columbus usa  zhu tao zhang  yuan xie vector sparsity sparsity resnet NMT resnet NMT resnet NMT dim vector sparsity dim vector sparsity dim vector sparsity cumulative distribution vector local sparsity vector vertical axis vector sparsity horizontal axis sparsity vector non zero observation spatial distribution non zero sparse neural network generally retain non zero vector instead location unaware selection generic sparsifying approach generates sparse matrix balance spatial distribution non zero similarly experimental dim dim vector demonstrate vector sparsity cumulative distribution sparsity vector vector increase local sparsity scope likely resembles global sparsity accord observation inspires matrix dim vector vector sparsified independently achieve sparsity balance comparable model accuracy   characterization spatial locality opportunity avoid accuracy penalty splitting matrix vector sparsifying vector sparsity however encode format generic sparse matrix csr format information associate vector propose balance vector wise encode format sparse matrix simplifies workload partition gpus novel vector wise sparsifying algorithm prune dense network sparse network maximize vector wise encode efficiency vector wise sparse matrix encode improve workload balance sparse neural network gpus propose phase vector wise encode sparsify matrix phase matrix dim vector matrix NZ NZ NZ NZ NZ NZ NZ encode offset NZ NZ NZ NZ NZ NZ NZ vector wise sparse matrix encode non zero vector compress compact vector associate index vector encode vector non zero compact vector empty entry pad zero vector divisible residue vector pad zero phase non zero nnz vector denote maximum nnz vector matrix phase compress vector dim vector along associate index vector vector non zero compression vector non zero NZ vector empty entry zero assure vector dim vector wise encode wise wise without lose generality vector unless specifically illustrate theoretically  encode index dim vector consequently overall compression ratio encode  encode matrix FP matrix offset index array index therefore compression ratio maximum non zero vector encode achieve ideal compression ratio vector non zero however non zero compression ratio ideal fortunately neural network prune allows tailor topology achieve spatial distribution ideal vector wise encode instead pursue overall sparsity prune vector wise encode minimize maximum non zero vector VectorSparse methodology iterative vector wise sparsifying retrain achieve spatially distribution non zero neural network matrix propose VectorSparse prune methodology iterative vector wise sparsifying retrain cnns rnns vector wise sparsifying local sparsifying advantage aforementioned vector wise sparse encode convolutional layer cnns refer CHW matrix generate imcol transformation matrix filter sparse tensor core algorithm hardware vector wise sparse neural network gpus micro october columbus usa algorithm VectorSparse algorithm prune vectorwise sparsifying input matrix NN layer vector maximum accuracy output prune vector wise sparse matrix  nzero  nzero nzero sort absolute vector ascend sort sort tij sort nzero vector remove tij tune prune  channel filter height width filter respectively vector wise sparsifying sort vector absolute absolute unchanged prune phase vector non zero encode dim vector vector wise sparsity encode although easily increase overall compression ratio directly prune dense matrix vector wise encode significant accuracy neural network address accuracy issue propose progressive prune gradually decrease vector wise sparsifying algorithm VectorSparse prune dense neural network algorithm prune network layer layer dense   vector input parameter maximum accuracy acceptable error rate sparsity matrix gradually validation error prune neural network exceeds error rate dense matrix VectorSparse prune absolute within vector tune prune topology training dataset tune algorithm evaluates validation error dense network relative difference validation error prune network dense network prune spatial distribution non zero neural network prune generic sparsity vector wise approach sparsity respectively pixel non zero vector wise prune achieves regularity VectorSparse flexibility specify acceptable error rate usually varies application application sensitive latency accuracy maximum accuracy gain sparsity otherwise maximum accuracy ensure accuracy additional spatial constraint VectorSparse usually chooses generic prune tune prior difference prune negligible impact convergence generic prune prune synapsis retrain phase affect accuracy prune neural network factor vector sparsity nzero algorithm spatial distribution non zero prune resnet vector wise prune workload balance generic prune gpu kernel aware spatial distribution non zero vector wise encode allows sparse matrix multiplication efficiently mapped gpus workload balance correspondingly minor modification gpu kernel vector wise sparse matrix multiplication vector wise encode sparse matrix multiplication kernel defines user interface function standard gemm api extra parameter vector maximum non zero vector assume matrix multiplication matrix respectively traditional dense matrix multiplication kernel compute encode offset NZ NZ NZ NZ NZ NZ NZ dense dense NZ NZ vector wise sparse matrix multiplication micro october columbus usa  zhu tao zhang  yuan xie instruction cache memory IO unified data cache memory texture memory warp scheduler simd dispatch FP int cuda core sfu tensor core tensor core register file LD ST warp scheduler simd dispatch FP int cuda core sfu tensor core tensor core register file LD ST warp scheduler simd dispatch FP int cuda core sfu tensor core tensor core register file LD ST warp scheduler simd dispatch FP int cuda core sfu tensor core tensor core register file LD ST cache architecture multiprocessor SM volta gpus instruction cache constant cache omit brevity regardless sparsity matrix multiplication prune VectorSparse algorithm matrix becomes vector wise sparse matrix associate offset matrix consequence vector wise sparse matrix multiplication kernel subset matrix compute matrix sparse matrix non zero calculation equivalent NZ NZ correspond multiplication execute multiplication reduction performance gpu matrix multiplication kernel tile widely matrix multiplication warp responsible computation tile matrix propose vector wise sparse matrix multiplication orthogonal tile technique tile apply tile multiplication performs index data dependent matrix carefully topic sparse tensor core benefit VectorSparse prune algorithm theory generic sparse matrix suffers workload balance cumbersome coordinate decode overall performance VectorSparse issue hardware unaware algorithm detail hardware adaptive algorithm modify tensor core VectorSparse refer sparse tensor core baseline tensor core architecture tensor core hardware functional dense matrix multiplication introduce nvidia volta mapping matrix multiplication  warp computation task matrix evenly partition  architecture tensor core execute matrix multiplication addition cycle tensor core volta gpus execution mode FP mode mixed precision mode FP mode matrix FP mixed precision mode tensor core FP accumulator writes FP matrix architecture multiprocessor SMs volta gpu illustrate SM consists   warp scheduler math dispatch processor array multiple data cuda core function SFUs tensor core LD ST register file data cache memory  within SM program execution tensor core concurrently warp cuda program model tensor core expose programmer cuda wmma warp matrix accumulate api wmma api dedicate matrix load primitive matrix accumulate operation tensor core wmma matrix load operation data register file memory hierarchy matrix warp computes matrix accumulate nvidia disclose detail tensor core reveal thread within warp collaborate conduct matrix accumulate operation efficiently execute wmma thread warp  threadgroup thread  thread threadgroup compute tile multiplication furthermore data reuse  worktuple worktuple consists threadgroup threadgroup worktuple wmma operation worktuple responsible compute tile worktuple computes achieve worktuple multiplies tile compilation wmma operation machine HMMA instruction mixed precision mode HMMA instruction computes tile tile sparse tensor core algorithm hardware vector wise sparse neural network gpus micro october columbus usa threadgroup threadgroup abc dense HMMA HMMA RD RA RB RC HMMA RD RA RB RC HMMA RD RA RB RC HMMA RD RA RB RC sparse HMMA SHMMA  RO SHMMA exec RD RA RB RC  worktuple dense mode sparse mode dense sparse wmma ptx instruction respectively tile HMMA instruction worktuple HMMA instruction threadgroup compute submatrices respectively execution HMMA instruction  worktuple tile tile private  respectively illustrates wmma operation mapped tensor core architecture octet tensor core inside octet dot DP compute dim vector dot per cycle execution worktuple mapped octet threadgroup DP respectively octet operand buffer worktuple via tile data execute HMMA instruction threadgroup dedicate operand buffer operand operand operand buffer tile operand buffer dedicate operand tile data inside  worktuple threadgroup computes multiplication tile tile HMMA instruction dim dot DP compute dim vector dot per cycle HMMA instruction cycle compute matrix multiplication extension HMMA instruction  worktuple computes matrix multiplication HMMA instruction HMMA instruction mixed precision mode HMMA instruction HMMA RD RA RB RC HMMA RD RA RB RC HMMA RD RA RB RC HMMA RD RA RB RC pipeline register DP dot operand bus operand bus octet octet octet writeback buf buf register file octet threadgroup tensor core tensor core FP multiplier FP adder accumulator buffer operand bus threadgroup buf tensor core architecture register HMMA instruction register register contains FP operand register RA mapped register tile data tile load RB RD instruction FP mode FP accumulator instead FP accumulator without loss generality mixed precision mode prior profile latency FP mode vector wise sparse matrix multiplication tensor core vector wise sparse mode refer execution mode dense mode sparse HMMA SHMMA tensor core execute vector wise sparse matrix accumulate operation matrix encode matrix encode worktuple worktuple computes therefore vector wise sparse mode tensor core computes matrix accumulate operation warp sparse matrix encode data matrix associate offset matrix offset encode offset memory therefore encode offset register dense mode  compute vector wise sparse mode illustrate compute non zero correspond respectively accumulate offset index offset register implement vector wise sparse mode extend tensor core instruction SHMMA instruction offset register SHMMA  RO SHMMA exec RD RA RB RC instruction SHMMA  fetch offset index RO implicit dedicate offset micro october columbus usa  zhu tao zhang  yuan xie dense worktuple worktuple worktuple worktuple dense offset instruction SHMMA  RO SHMMA exec RD RA RB RC sparse wmma  execution tensor core architecture operand vector wise encode matrix dim offset array entry offset array contains offset index associate register instruction SHMMA exec decodes offset register fetch RB data load operand buffer instruction SHMMA exec computes dim dot accumulates instead HMMA instruction dense mode SHMMA instruction sparse mode instruction vector wise sparse mode threadgroup computes SHMMA instruction therefore SHMMA instruction sufficient vector wise sparse wmma operation  denote  mma ptx instruction sparse wmma api  instruction non zero sparse matrix load instruction dense matrix instruction apis dense wmma load  load load offset  load offset math  mma micro architecture sparse tensor core SHMMA instruction modification tensor core highlight dedicate offset register register file offset register implicitly access SHMMA instruction baseline tensor core architecture operand buffer FP octet load tile HMMA instruction improve utilization DP buffer accommodate buffer another buffer hide load latency addition enable broadcasting operand buffer DP connects DP octet threadgroup compute dot per cycle execution timeline mode illustrate performance benefit SHMMA dense mode operand buffer cycle fetch compute fetch compute decode offset fetch fetch fetch offset fetch compute decode offset fetch fetch offset cycle cycle cycle explicit register fetch implicit offset decode dot operation execution timeline dense mode vector wise sparse mode without ping pong buffer vector wise sparse mode ping pong buffer tensor core execution HMMA instruction cycle gemm warp cycle gemm computation dense mode vector wise sparse mode instruction SHMMA  cycle load offset offset register another cycle register file decode offset register signal operand buffer datapath operand buffer FP operand cycle load data operand buffer thanks vector wise sparsity computation reduce cycle vector wise sparse mode cycle computation speedup observation dense mode compute bound vector wise sparse mode memory bound data operand buffer vectorwise sparse matrix multiplication operational intensity FLOPs byte dense matrix multiplication explains sparsity reduce latency inference improve performance another buffer hide register fetch latency buffer  buffer buffer DPs load data register file latency vector wise sparse wmma reduce cycle cycle additional speedup dense mode benefit buffer however compute bound dense mode moderate latency reduction cycle cycle overhead worthy ping pong buffer dense mode experimental methodology algorithm hardware aim accelerate inference phase neural network minimal impact quality model evaluate model accuracy speedup generic sparse neural network dense neural network assume buffer load bandwidth cycle fetch FP buffer sparse tensor core algorithm hardware vector wise sparse neural network gpus micro october columbus usa picked popular neural network domain image classification image caption machine translation neural network generic sparsifying propose VectorSparse respectively training DGX nvidia tesla gpus evaluate performance vector wise sparse mode tensor core extend wmma ptx code model gpgpu sim simulator SHMMA instruction simulator timing parameter configure simulator model tesla gpu tensor core simulated gpu SMs tensor core cuda core inside equip 6GB HBM GB device memory bandwidth image classification application prune retrain cnns imagenet dataset comprises training validation verify vector wise sparsifying commercial application popular cnns alexnet vgg resnet resnext imagenet ILSVRC dataset alexnet vgg popular network achieve accuracy ILSVRC dataset resnet residual layer training easy neural network furthermore evaluate resnext configuration accuracy impact network network tensorflow model repository reference addition cnns examine memory lstm representative rnns model image caption image caption model consists inception model lstm layer attach layer cnn lstm layer default lstm layer pre inception model randomly initialize parameter lstm layer training dataset mscoco mini batch model epoch default configuration retrain phase quantify quality generate caption calculate bleu training configuration mscoco dataset code tensorflow model zoo machine translation application  architecture attention mechanism perform neural machine translation NMT architecture layer lstm encoder layer lstm decoder attention module layer lstm encoder bidirectional lstm layer unidirectional unidirectional bidirectional layer lstm bleu metric neural machine translation model WMT english german dataset training instruction reproduce NMT training source framework epoch retrain phase simplification stem observation validation bleu becomes stable epoch workload default training achieve reference model accuracy apply VectorSparse prune reference dense model FP activation gradient training cuda core inference kernel tensor core kernel dynamically downsize FP input activation FP layer avoid accuracy loss mixed precision mode output activation FP software implement vector wise sparse matrix multiplication kernel CUTLASS  due unavailability source code CUTLASS source performance gemm template library achieve  performance gemm library gemm interface allows data customize gemm computation cuda core vector wise sparse matrix multiplication kernel built sgemm kernel sparse tensor core kernel wmma gemm kernel sparse wmma api instead dense wmma api convolution operation cnn workload convert gemm imcol besides dense baseline workload unified prune VectorSparse sake comparison iteratively reduce channel retrain network VectorSparse evaluate accuracy neural network model experimental RESULTS validate propose vector wise prune accuracy model prune VectorSparse various configuration performance gain prune vector wise sparse network sparse tensor core finally overhead analysis sparse tensor core impact accuracy demonstrate generality apply VectorSparse prune various workload validation accuracy cnn rnn model prune VectorSparse generic prune unified prune respectively badly model accuracy workload sparsity increase marginal impact accuracy regardless sparsity accuracy insensitive vector optimal offset index enables finer grain tile strategy prune retrain VectorSparse prune unified prune validation accuracy accuracy relative deviation reference dense model illustrate cnn model prune VectorSparse retain accuracy sparsity similarly accuracy rnns comparable reference model sparsity exceed micro october columbus usa  zhu tao zhang  yuan xie relative accuracy vector sparsity alexnet vector wise vgg vector wise resnet vector wise resnext vector wise alexnet vector wise vgg vector wise resnet vector wise resnext vector wise alexnet generic vgg generic resnet generic resnext generic alexnet unified vgg unified resnet unified resnext unified diff sparsity accuracy sparsity matrix cnn workload workload generic unified prune described VectorSparse prune respectively accuracy normalize dense baseline although generic prune outperforms VectorSparse sparsity generic sparse matrix necessarily encode vector wise sparse format concentrate vector model accuracy quickly sparsity scheme suffers accuracy loss usually unacceptable application spatial constraint removal rnns resilient prune cnns however accuracy prune incomparable sparsity accuracy vector sparsity achieve performance sacrifice accuracy accuracy metric opt scheme however useful application domain scope recommend another observation unified prune incurs significant accuracy cnns rnns cnn rnn model accuracy loss sparsity due spatial constraint unified prune option remove retrain lack flexibility limit representative network contrast vector wise prune freedom remove topology generic prune extremely relative bleu vector sparsity vector wise NMT vector wise vector wise NMT vector wise generic NMT generic unified NMT unified diff sparsity bleu sparsity matrix rnn workload summary vector wise prune sparsity assure accuracy non zero dim vector warp multiple favorable cuda core therefore sparsity optimal configuration performance evaluation performance evaluation workload configuration evaluate performance VectorSparse prune hardware tensor core baseline cuda core dense dense NN workload FP cuda core evaluate cuda core generic cuda core unified NN workload prune generic unified sparsifying respectively cuda core generic sparsity cuda core unified sparsity examine tensor core dense dense nns tensor core input activation dynamically convert FP layer accumulator FP conversion negligible accuracy impact furthermore cuda core vector wise sparse evaluate justify VectorSparse prune finally evaluate tensor core vector wise sparse proposal vector wise sparse nns sparse tensor core tensor core generic unified sparse GEMMs opt option generic unified sparse nns tensor core cuda core vector wise sparse tensor core vector wise sparse sparsity relatively sparsity accuracy bleu vector wise sparse cuda core unified inference performance cnns rnns prune normalize cuda core dense unsurprisingly tensor core amd terminology wavefront wavefront consists thread multiple sparse tensor core algorithm hardware vector wise sparse neural network gpus micro october columbus usa alexnet vgg resnet resnext NMT geomean cuda core generic sparsity cuda core dense cuda core unified sparsity cuda core vector wise sparse sparsity tensor core dense tensor core vector wise sparse sparsity normalize speedup cuda core dense nns gpu vector wise sparse nns sparsity generic prune sparse nns sparsity unified prune sparse nns sparsity vector wise sparse nns accuracy generic prune nns unified prune nns dense faster cuda core dense tensor core tflops cuda core cuda core generic cuda core dense former sparsity computation slowdown  inefficiency gpu generic sparse nns motivation alternatively cuda core unified average speedup cuda core dense gain mainly dense wmma operation average tensor core vector wise sparse achieve speedup baseline performance gain fold relaxed spatial constraint vector wise sparse nns benefit sparsity cuda core vector wise sparse performance gain baseline secondly customize SHMMA instruction microarchitecture sparse vector wise nns advantage powerful tensor core contributes additional performance improvement versus cuda core vector wise sparse tensor core vector wise sparse speedup tensor core dense overhead analysis vector wise sparse mode tensor core hardware buffer operand FP enable ping pong buffer index operand offset register octet operand buffer octet tensor core octet buffer vector wise sparse mode buffer buffer tensor core SM tensor core KB buffer CACTI evaluate timing overhead KB SRAM node cycle nominal cycle mhz incur timing overhead overhead analysis via CACTI SRAM cycle KB fabricate addition tensor core extra register offset register octet SM extra offset register fetch operand buffer overall overhead negligible summary discussion experimental VectorSparse prune generic prune although sparsity generic sparse nns vector wise sparse nns enable gpus efficiently exploit benefit prune csr format vectorwise encode eliminates index allows split evenly guarantee thread parallelism evaluation vector wise prune network vector wise sparse nns sparsity negligible accuracy promising speedup dense counterpart boost performance nns hardware tensor core enable vectorwise sparse matrix multiplication vector wise sparse tensor core easily extend variant evolution neural network sparse tensor core become compute bound future conclusion generic sparse neural network hardly dense neural network gpus highly optimize software hardware dense gemm efficiently exploit intrinsic redundancy neural network propose VectorSparse vector wise prune guarantee prune network balance workload encode vector wise sparse matrix dim fix offset index instead dim variable index generic sparse matrix VectorSparse prune prune neural network sparsity negligible impact model accuracy workload balance vector wise sparse matrix multiplication faster dense counterpart gpu cuda core improve performance vector wise sparse matrix multiplication enable tensor core sparse mode extend instruction hardware negligible overhead sparse tensor core achieve speedup dense mode tensor core comparable model accuracy