binary instrumentation framework widely implement profilers performance evaluation error bug detection dynamic binary instrumentation pin  CPUs gpu architecture currently limited capability static compile prohibits instrumentation dynamically load library foundation highperformance application nvbit dynamic portable binary instrumentation framework allows user instrumentation cuda selectively apply functionality pre compile binary library execute nvidia gpus dynamic recompilation SASS nvbit analyzes gpu kernel register requirement generate efficient abi compliant code without developer detailed knowledge underlie gpu architecture nvbit allows instrumentation multiple function injection location inspection ISA visible dynamic selection uninstrumented code permanent modification register source code correlation instruction removal nvbit recent nvidia gpu architecture kepler maxwell pascal volta pre compile cuda OpenACC OpenCL cuda fortran application CCS CONCEPTS computer organization instruction multiple data software engineering dynamic compiler compiler keywords dynamic binary instrumentation cuda gpgpu gpu compute introduction binary instrumentation powerful technique allows transformation application binary variety purpose application profile performance model error fault injection binary instrumentation backbone computer architecture research education binary instrumentation framework develop custom instrumentation apply specific transformation application binary unique notable binary instrumentation framework target cpu architecture intel pin  HP caliper framework employ dynamic binary rewrite without application source code advent gpu compute equivalent framework desirable target gpu architecture compiler instrumentation   void nvidia gpus cuda program model  expose api user custom instrumentation characterize application explore gpu architectural however compiler instrumentation significant practical limitation source code recompilation problematic application source code available specific compiler compiler version preclude alternative toolchains target application cannot target code generate gpu driver via  ptx code cannot target proprietary accelerate library cuBLAS   cudnn source code publicly available popular machine framework caffe torch cuBLAS cudnn compile selection instrumentation site function parameter therefore cannot adapt instrumentation dynamic application behavior compiler approach  gpu instrumentation comparable dynamic instrumentation available CPUs micro october columbus usa    david  stephen  knowledge purpose binary instrumentation gpus publicly available nvbit dynamic binary instrumentation framework target nvidia gpus nvbit apis allows instruction inspection callback cuda driver apis injection arbitrary cuda function application kernel launch operating SASS dynamic recompilation nvbit analyzes gpu kernel register requirement generate abi compliant code without developer architectural knowledge underlie gpu implementation nvbit enables instrumentation multi function injection location inspection ISA visible dynamic selection uninstrumented code permanent modification register correlation source code instruction removal nvbit nvidia gpu architecture kepler maxwell pascal volta pre compile cuda OpenCL OpenACC cuda fortran application binary embed ptx gpu accelerate library primary contribution develop nvbit user api inspect instruction intercept cuda driver api nvbit underlie principle mechanism implementation detail instrumentation highlight unique feature nvbit ability proprietary library sample reduce instrumentation overhead instruction emulation allows architect systematically ISA extension nvbit enables development gpu instrumentation par cpu counterpart nvbit immediately applicable numerous architecture performance compute community gpu architectural simulator cmp FM sim error checker valgrind debugger fault injection framework background nvbit briefly review nvidia gpu architecture software stack along terminology relates capability nvbit gpu architecture gpus highly multi thread accelerator execute parallel target application improve efficiency gpu thread grouped warp warp gpu thread program counter execute instruction multiple thread SIMT fashion gpu thread purpose register perform access global local constant memory via load semantics divergence handle via predication per warp active mask enables disables thread specific warp assign execute concurrently gpu core multiprocessor SM cuda driver software stack thread cooperative thread array cta gpu consists multiple SM building along memory hierarchy SM local scratchpad memory cache cache multiple memory controller gpus deploy nvbit allows passing numerous parameter instrumentation function affect program execution register predicate active mask constant gpu software stack nvbit target cuda compute stack user parallel program cuda compiler nvidia nvcc generate intermediate code virtual ISA parallel thread execution ptx ptx expose gpu data parallel compute device stable program model instruction purpose parallel program ptx directly gpu backend compiler optimizes translates ptx instruction SASS machine code specific device backend compiler invoked ahead via  ptx assembler jit compiler embed gpu driver nvbit interacts directly cuda driver handle machine code already compile SASS gpu compute program adhere define application binary interface abi defines interface caller callee register caller versus callee pas parameter register resort passing parameter stack nvbit dynamic assembler generate abi compliant code inject generic cuda device function application compute program nvidia gpus interface gpu driver define cuda apis cuda driver api access runtimes cuda OpenCL OpenACC cuda fortran directly user nvbit interface directly cuda driver seamlessly interpose software stack exist application context  cuda driver maintain gpu device module  function  load demand kernel dependent function launch nvbit callback cuda driver apis plus specific callback application terminate additional driver callback nvbit nvbit dynamic binary instrumentation framework nvidia gpus micro october columbus usa nvbit framework overview nvbit compile library inject preloaded LD preload  regard interface function enumeration parameter nvbit nvbit framework nvbit core compose static library  header file nvbit apis implement nvbit nvbit pseudonym instrumentation developed nvbit nvbit develop file nvbit api compile nvidia nvcc link static library  generates library linux typically file extension library correspond nvbit library inject target application nvbit library developer inject library functionality application cuda driver injection mechanism standard LD preload optional environment variable linux contains library loader load library nvbit compilation  user perspective typical nvbit implement gpu device function inject application gpu kernel accord user define injection dynamic instrumentation binary typically kernel launch although within cuda driver callback listing nvbit thread instruction execute counter application listing implementation injection function atomically increment cuda manage variable counter execute macro nvbit export dev function define inside nvbit prevent compiler perform code elimination injection function unused compiler callback trigger target application executes cuda driver callback return immediately entry kernel launch cuda driver nvbit nvbit counter variable instruction manage counter kernel already std CUfunction kernel implementation instrumentation function  device  void  counter  counter nvbit export dev func  counter callback trigger cuda driver void nvbit cuda driver  ctx int exit cbid cbid const char void params   return entry kernel launch cbid api cuda  exit return parameter kernel launch  params  params params return kernel already kernel insert func return instruction kernel auto nvbit instrs ctx func nvbit insert  counter  callback trigger application termination void nvbit cout thread instruction counter listing nvbit thread instruction application terminates cast callback parameter specific parameter kernel launch already encounter kernel return already iterate instruction compose kernel retrieve nvbit instrs insert instrumentation function instruction nvbit insert finally application terminates counter variable inject device function pedagogical fully functional nvbit allows user arbitrarily complicate skilled cuda programmer optimize variety technique reduce radix counter per thread warp reduction improve overhead binary nvbit user apis overview nvbit user apis category callback inspection instrumentation device callback api callback apis trigger nvbit core target application encounter application termination entry exit cuda driver api listing callback micro october columbus usa    david  stephen  trigger application void nvbit init void nvbit trigger entry exit exit exit cuda driver  cbid identifies cuda driver enumeration  params pointer structure parameter driver cast struct specific cbid  return status cuda driver valid exit void nvbit cuda driver  ctx int exit cbid cbid const char void params   listing callback api trigger instruction CUfunction const std vector instr nvbit instrs  CUfunction CUfunction const std vector std vector instr nvbit  CUfunction related  CUfunction std vector CUfunction nvbit related   CUfunction listing inspection api function retrieve instruction related  apis detail user inspect  application cuda driver callback instance kernel launch callback nvbit cuda driver occurs parameter  launch kernel nvbit callback interface enumeration  nvbit easy gpu programmer inspection api listing inspection api allows user retrieve inspect instruction compose CUfunction inspection api CUfunction vector CUfunction instruction program absence indirect icf instruction api alternatively return vector vector instruction sub vector uninterrupted sequence instruction predicate instruction thread execute without generate static instruction kernel consecutive program counter PCs PC instruction PC target instruction icf instruction exception register compute target address discover instruction execute return simpler CUfunction icf instruction uncommon rarely encounter CUfunction  inspection api retrieve nvbit related  function instruction retrieve instrumentation function inspect understand nvbit instr abstract actual machine SASS instruction across gpu disassemble transform instruction user friendly intermediate representation listing instr public memory operation enum  none local generic global SHARED texture constant operand enum  imm val immediate reg val register pred val predicate register  val constant val constant offset  val register  val register val imm immediate addr   imm operand structure  struct  operand bool neg negative bool absolute val operand return SASS  const char  return offset byte instruction within function uint  return instruction within function uint  return instruction predicate bool  return predicate valid  int  return predicate negate valid  bool  return opcode  const char  return memory operation    return memory operation load bool  return memory operation bool  return byte memory operation int  return operand int  specific operand const operand  int num operand info binary compile option generate info  void  char file uint listing inspection api instr abstract machine SASS instruction instrs nvbit ensures mapping SASS instruction instrs instr correlate application source code instr  file information strip application binary instrumentation api instrumentation instrumentation api listing inject multiple device function CUfunction instruction inject function nvbit insert specify location instr function inject instrumentation function inject code location program counter unknown argument inject function register predicate immediate nvbit arg argument passing positional signature nvbit dynamic binary instrumentation framework nvidia gpus micro october columbus usa enumeration nvbit insert specify insert device function instr  enum    function insert device function dev func instruction instr device function identify oppose function pointer export macro nvbit export dev func void nvbit insert const instr instr const char dev func  parameter  inject  enum pred val predicate instruction pred reg predicate register thread imm immediate val imm imm immediate val imm reg val register val reg num  val constant val val offset arg parameter inject void nvbit arg arg arg val val remove instruction void nvbit remove orig const instr instr listing instrumentation api function injection argument passing code function flag void nvbit enable  ctx CUfunction func bool flag reset instrumentation function  instrumentation void nvbit reset instrumentation  ctx CUfunction func listing api enable disable function reset instrumentation device function register writes permanent application device int nvbit reg int reg device nvbit reg int reg device void nvbit reg int reg int val device void nvbit reg int reg val listing device api register inject function inject function instance inject function argument nvbit arg function inject remove replace instruction nvbit remove orig instance replace functionally equivalent inject function later replace instruction nvbit cannot guarantee application unless inject functionality identical replace instruction api nvbit ability instrumentation application dynamically execution non version function nvbit framework version user nvbit enable listing user reset apply instrumentation instrumentation selection apply operation perform inside callback nvbit cuda driver nvbit core component user api application execute allows function rewrite dynamic application execution significant advancement static compiler gpu instrumentation device api finally nvbit device api within instrumentation inject function importantly register application kernel device function api arbitrary writes register catastrophic application error ability modify gpu requirement fault injection instruction emulation listing describes nvbit device api nvbit implementation nvbit core responsible interact cuda driver user functionality described describes internal implementation detail nvbit core discus overhead related  software component detail component nvbit core detail driver interposer driver interposer nvbit core layer intercept cuda driver apis function overload mechanism LD preload cuda driver load application function CUfunction driver interposer maximum register usage maximum stack usage dependent function function function memory location instruction load consume component within nvbit core library instance maximum register consumption compute amount register jumping instrumentation function driver interposer responsible propagate cuda driver callback api nvbit user callback api function loader function loader responsible load device function within dynamic library nvbit automatically application cuda driver unaware device global function nvbit library micro october columbus usa    david  stephen  code generation load device function export macro nvbit export dev function associate function structure function attribute register request stack location code load gpu memory information code generator code instrumentation function function loader responsible load pre built device function embed  restore register jumping user inject function efficiency nvbit implement fix restore function target specific purpose register hardware abstraction layer hal hardware abstraction layer hal initialize  specific device hal initialization device specific information instruction byte alignment requirement register available per thread abi version within gpu instruction unique fix kepler maxwell pascal encoding volta encoding abi version specifies register register convergence barrier volta restore exit instrumentation function hal initializes device specific assembly disassembly function function assemble code code generator disassemble code instruction  component strictly hal improves portability nvbit across gpu generation SASS ISA guaranteed remain constant instruction  instruction  responsible retrieve raw buffer SASS instruction application CUfunction user request inspect instruction CUfunction nvbit instrs nvbit instruction  convert instr listing explain previously instr machine independent SASS instruction disassemble instruction instr vector subdivide vector user api usage code generator exit cuda driver callback instrumentation apply code generator function nvbit instrumentation code generation occurs code CUfunction inspect user instrumentation function foo user inject highlight instruction arrow code generator executes code memory refer code generates code allocate gpu memory trampoline modifies code substitute highlight instruction sts trampoline jmp insert trampoline elegantly preserve instruction layout expansion significantly complicate possibly additional runtime data structure generate trampoline typically contains instruction routine thread execute instrumentation function nvbit minimum amount purpose register appropriate routine analyze register requirement code inject function purpose register conditional code predicate routine stack sequence instruction mov pas argument specify user argument passing convention register stack define specific abi target device initialize handle hal actual program counter instrumentation function foo retrieve access injection function function loader routine restores thread stack inverse function routine execution relocate instruction sts critically relocate instruction relative instruction offset adjust account target location finally return code program counter relocate instruction jmp NPC trampoline per instruction however efficiency purpose allocation trampoline handle bulk custom memory allocator content trampoline injection function insert gpu instruction injection happens location function nvbit remove orig listing relocate instruction convert nop code loader  user enable disable instrumentation CUfunction code loader  swap code code demand api nvbit enable operation identical cudaMemcpy host device nvbit dynamic binary instrumentation framework nvidia gpus micro october columbus usa byte code swap code code byte occupy location gpu memory nvbit guarantee absolute program target CUfunction regardless version trampoline device memory gpu resident remove unless api nvbit reset  CUfunction  code loader  computes stack register requirement kernel launch version code execute jit compilation overhead binary instrumentation intrinsically introduces overhead execution slowdown nvbit overhead stem aspect generate instrumentation code  overhead instrumentation code jit compilation overhead drawback binary instrumentation versus compiler instrumentation code generation happens offline however later demonstrate nvbit allows perform task cannot achieve compiler instrumentation focus quantify jit compilation overhead later discus overhead specific nvbit exemplify sample technique reduce overhead instrumentation code depends user accomplish instrumentation function extremely broadly quantify within nvbit core jit compilation overhead component retrieve gpu code disassemble gpu program convert binary format developer via nvbit api execute user code inject instrumentation function argument code generator code swap code code component characteristic application component application evaluation assume kernel instruction approximates rough upper bound jit compilation overhead nvbit user upper bound theoretical extrapolation demand breakdown jit compilation overhead component previously apply nvbit described listing selection nvbit arbitrary jit compilation overhead depends amount newly generate code due trampoline argument passing instruction instrumentation function compile offline another instrumentation function jit compilation overhead benchmark OpenACC  suite medium benchmark nvidia titan gpu relatively execute benchmark ensures jit compilation jit compilation overhead respect native execution OpenACC  benchmark overhead  execution application obviously per application  benchmark cuda native implementation opt OpenACC implementation highlight nvbit agnostic binary average jit compilation overhead evaluation kernel jit compilation overhead application compose unique kernel  application exacerbate kernel execute instance launch thread application contributor  overhead disassembly phase convert gpu binary code intermediate representation internally nvbit nvbit  EXAMPLES nvbit emphasize unique capability pre compile library sample instruction emulation memory access address divergence understand memory access important optimize application memory subsystem nvbit allows easily extract information memory operation reference address analyze directly gpu cpu processing entire cache simulator built around mechanism listing nvbit computes unique cache request warp global memory instruction application cache per warp memory instruction perform efficiently instrumentation function  argument predicate register immediate argument memory instruction false predicate instruction actually execute micro october columbus usa    david  stephen  counter unique cache access memory instruction execute manage float uniq manage mem instrs  device  void  int pred int int int imm return predicate false pred return construct address addr imm compute active mask warp int mask ballot active thread warp increment memory instruction counter lane ffs mask  mem instrs thread warp access cache cache addr addr cache int cnt  sync mask cache addr thread contributes proportionally cache counter  uniq cnt nvbit export dev func  void nvbit cuda driver  ctx int exit cbid cbid const char void params    code listing iterate kernel instruction auto nvbit instrs ctx func instr global memory operation skip  instr global iterate operand instruction int  operand  operand memory reference skip instr  inject instrumentation argument nvbit insert   nvbit arg pred val nvbit arg reg val val nvbit arg reg val val nvbit arg imm val void nvbit printf average cache request per memory instruction uniq mem instrs listing memory access address divergence computes average cache request per warp memory instruction  return immediately memory reference address construct combine register immediate mask active thread warp compute leader thread atomically increment global memory reference counter thread computes cache address access reduces warp cuda intrinsics indicates thread access cache within warp thread atomically increment cache counter proportionally reduce thread access cache increment cache counter inspection instruction memory operation address divergence analysis ML workload without pre compile library global instr global instruction memory reference operand instr  inject function  passing argument finally program termination ratio unique cache request warp memory instruction execute listing implement efficiently inject trampoline instruction passing multiple reference argument function however implementation discussion opt simpler approach instrumentation listing variety machine ML workload implement torch alexnet enet googlenet resnet vgg imagenet dataset ML workload pre compile library developed nvidia cuBLAS cudnn  approach capture memory reference emit within library incomplete analysis analysis enable disabled instrumentation  library disable instrumentation pre compile library reproduce behavior compiler approach access library source code exclude library distorts reality considerably overestimate memory divergence application understand extent apply optimize version instruction listing percentage instruction execute workload inside pre compile library percentage average execute instruction across various ML workload accelerate library distinct kernel instance cuBLAS dozen kernel precision transposition access library source code significant amount compile nvbit instead source code application binary library apply instrumentation kernel actually invoked kernel sample instruction histogram jit compilation overhead additional overhead incur due execution code overhead due nvbit framework instead nvbit dynamic binary instrumentation framework nvidia gpus micro october columbus usa instruction execute   application extract nvbit solely code execute complex naturally instrumentation invasive computationally intensive instrumentation site application reduce execution overhead highly optimize instrumentation function optimization apply cuda program nvbit mitigate overhead standard technique sample reduce frequency instrumentation callback nvbit implement sample user kernel uninstrumented version sample kernel launch version unique grid dimension instance kernel foo launch grid dimension grid dimension version foo twice unique grid dimension remain foo uninstrumented perform selection nvbit api nvbit enable within instrumentation kernel launch information kernel execution approximate information uninstrumented execution highlight approach instrumentation performs analysis instruction execute construct histogram instruction execute OpenACC  benchmark execution benchmark nvidia titan gpu instrumentation entire application execution yield slowdown approach benchmark slowdown instrumentation approach sample approach respect native execution average instrumentation approach native execution sample approach incurs slowdown although target application cannot directly jit compilation overhead generate code explain jit compilation overhead independent content inject function primarily trampoline generate slowdown instrumentation sample approach respect native execution error kernel sample respect instrumentation report benchmark average across instruction category speedup sample approach decrease accuracy error kernel sample approach report benchmark average across instruction category error compute without sample definition sample technique average error combination sample approach instrumentation sample error depends characteristic kernel execute kernel compute function grid dimension sample error application grid mesh compute alter algorithm target application instrumentation advanced sample technique adaptive statistical profile implement underlie mechanism instruction emulation warp fft nvbit instrumentation semantic preserve default demonstrate nvbit device api listing modify ISA visible prior functionality fault injection automatic conversion discus instruction emulation typically employ architectural exploration pre silicon compiler demonstrates hypothetical warp fft instruction wfft listing nvbit micro october columbus usa    david  stephen  compute warp fft across lane  device  void wfft  int reg dst num int reg src num input register nvbit reg reg src num implementation warp fft function shuffle fft warp destination register nvbit reg reg dst num nvbit export dev func wfft  void nvbit cuda driver  ctx int exit cbid cbid const char void params    code listing iterate kernel instruction auto nvbit instrs ctx func operand ops operand identify proxy instruction asm    ops val  nvbit insert wfft   nvbit arg imm ops val nvbit arg imm ops val remove proxy instruction nvbit remove orig listing nvbit emulate warp fft instruction wfft functionally equivalent function wfft  replaces hypothetical wfft instruction device function wfft  emulates functionality nvcc compiler cannot emit encode nonexistent wfft instruction proxy instruction instead   proxy instruction  logical instruction input register output register signature wfft along immediate operand immediate operand magic differentiate proxy instruction instance  instruction hijack exist instruction application instance proxy instruction mitigate possibility nvbit native instance proxy instruction application graphic related proxy instruction likely cuda application proxy instruction generate inline assembly ptx instruction listing listing implementation wfft  function clarity omit implementation fft focus device apis nvbit reg nvbit reg modify register nvbit identify proxy instruction wfft instruction inject emulate function pas argument source destination register proxy instruction finally remove proxy instruction purpose injection emulation function implementation warp FFTs warp shuffle operation global void fft kernel float float thread identifier int tid blockIdx blockDim threadIdx insert proxy instruction wfft translates   nvbit replace fft  asm  tid tid listing fft kernel proxy ptx instruction hypothetical wfft kernel listing nvbit listing inline assembly ptx instruction replace wfft  function nvbit combine instruction emulation instruction trace trace instruction exist potentially enable future trace gpu simulator illustrative purpose combine fft instruction emulation instruction listing execute kernel listing computes fft per warp wfft warp executes instruction however replace wfft instruction cuda code performs warp fft instead warp executes instruction nvbit gauge impact ISA LIMITATIONS discussion nvbit generality arbitrary injection cuda device function limitation constant memory usage inject function constant memory memory application instrumentation program fail program commonly memory capacity instrumentation library regardless library instrumentation function instrumentation function cannot accelerate library target application recursion instrumentation function becomes non deterministic application nvbit architected minimally invasive additional instruction register pressure cache user instrumentation nvbit framework alter behavior application rely specific schedule timing assumption behavior application already susceptible non deterministic behavior nvbit likely exacerbate non determinism instance memory address nvbit application memory synchronization via spin loop memory instruction execute application multiple worth limitation instrumentation approach static dynamic specific nvbit execution overhead nvbit active thread enters leaf instrumentation function overhead paid within instrumentation function however implement thread specialization thread identifier instance subset thread return immediately planning explore predicate nvbit dynamic binary instrumentation framework nvidia gpus micro october columbus usa jumping instrumentation function finer gain selection thread explain jumping instrumentation function specific register memory thread although fully parallelize cycle destroy cache locality examine carve instrumentation register limit however challenge decrease occupancy hoc unsupported abi information accessible nvbit information obtain via nvbit inspection apis comparable nvidia  cuda gdb instance developer  SASS code gpu binary SASS cuda gdb translation mapping embed ptx code SASS furthermore cuda gdb memory register manual inspection entire ISA visible machine nvbit primary advantage nvbit specific information analyze performance code magnitude faster user interactively  cuda gdb additionally dynamic instrumentation aspect nvbit user inject generic cuda function application dynamic instrumentation explain nvbit allows instrumentation kernel launch however kernel execute code cannot launch contrast exist approach cpu ability breakpoint modify code worth mention limitation nvbit gpus cannot compile code rely cpu execution related introduce dynamic binary instrumentation nvidia gpus CPUs capability available framework HP caliper  intel pin introduce target alpha processor dynamic binary instrumentation framework propose HP caliper initial implementation target IA  processor later  release intel later ibm architecture finally intel pin target entire intel arguably widely dynamic binary instrumentation framework intel pin developer writes instrumentation  specify program pin  target program pin performs jit compilation instrumentation  directs inside  developer inspect instruction application function inject arbitrary function nvbit separation concern nvbit core nvbit inspire intel pin pin  nvbit beyond pin functionality handle runtime api  complication translate mapping  SASS unique ISA feature gpus hardware abstraction layer knowledge prior instrumentation approach gpus focus compile instrumentation  operates ptx code ingest ptx emit frontend compiler modify compilation emit ptx gpus assembly code CPUs  originally architecture nvidia gpus leverage parallelism ptx program perform instrumentation gpu program implement gpu simulator  useful limited functionality operates ptx exactly correspond binary code execute gpu consequently  interferes compiler optimization invasive precise ability program nvbit  enable compile instrumentation directly SASS leverage nvidia production compiler compile restriction limit usefulness demonstrate approach decode encode strategy hardware abstraction layer hal nvidia gpu conclusion introduces nvbit dynamic binary instrumentation framework target nvidia gpus nvbit improves upon prior compiler instrumentation enable efficient instrumentation gpu code regardless application source code availability directly SASS nvbit application user nvcc via jit compilation ptx inclusion  library cudnn cuBLAS  gpu accelerate library  likelihood application rely highly optimize library increase evidence machine framework gpu acceleration via cudnn nvbit overcomes prior portability limitation operating directly SASS utilizes hardware abstraction layer enables seamlessly across recent generation nvidia gpus without specific compiler hal enables nvbit user obtain information gpu hardware without reverse engineer SASS assembly guaranteed remain constant gpu generation generation development nvbit enable performance analysis optimization architectural exploration gpus exist subset nvbit functionality