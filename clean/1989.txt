despite hardware predictor model impractically cache replacement policy cache replacement powerful lstm model offline accuracy hardware predictor perform analysis interpret lstm model derive insight allows online model offline model accuracy magnitude glider cache replacement policy evaluate memory intensive program spec spec gap graph processing benchmark suite core glider outperforms  cache replacement championship reduce rate lru reduction hawkeye  core glider improves ipc lru improvement hawkeye  CCS CONCEPTS computer organization processor memory architecture architecture keywords cache cache replacement introduction extract useful corpus data dramatic breakthrough computer vision recognition processing nlp promise advance engineering therefore innovation computer architecture domain microprocessor hardware predictor prediction cache replacement data prefetching etc training predictor moreover cache replacement data prefetching heavily seek advance typically refers multi layer neural network multiple layer significant ability non linear relationship multiple abstraction ability recurse recurrent neural network rnns model allows rnns effectively layer powerful machine model rnns extremely successful powerful model variant attention mechanism transformer constantly propose unfortunately powerful model exploit architect instead built hardware predictor simpler technique perceptrons perceptrons however layer neural network approach model rnns model ill hardware predictor model enormous resource training iterates multiple training data training perform offline reasonable application nlp computer vision prediction target offline training effective hardware predictor computer program exhibit phase behavior prediction target program executes behavior program model describes machine algorithm neural network vector machine perceptron micro october columbus usa zhan shi  huang akanksha jain calvin lin wildly another model compress implement chip prediction finally model typically millisecond prediction hardware predictor typically prediction within nanosecond novel approach address mismatch model hardware predictor demonstrate strength approach advance cache replacement approach powerful offline machine develop insight improve online hardware predictor specifically approach powerful unconstrained rnn model offline individual program interpret offline model reveal important insight described shortly useful cache designer insight online model offline model accuracy magnitude simpler online model implement hardware dynamically exist predictor cache replacement unconstrained offline model assumption important aspect program behavior posit offline model unconstrained offline model formulate cache replacement sequence label goal assign sequence memory access binary label indicates access data cached  min algorithm oracle label training data inspire recent lstm attention mechanism sequence model task attention lstm offline model offline model improves accuracy significantly prior analysis attention inside lstm reveals insight lstm benefit input load instruction conclude optimal cache decision program beneficial optimal decision primarily presence sequence insight craft feature program compactly simpler linear model vector machine svm svm online hardware lstm offline accuracy significantly overhead craft feature online svm equivalent perceptron commercial predictor summarize contribution behavior program behave wildly input another behavior gcc differs compile matrix multiplication oppose OS lstm memory variant recurrent neural network effectively dependence sequential data improve hardware cache replacement policy attention lstm model significantly improves prediction accuracy interpret attention mechanism derive important insight cache insight glider cache replacement policy svm predictor outperform cache replacement policy cache replacement championship glider significantly improves upon hawkeye previous winner cache replacement championship  glider reduces rate lru reduction hawkeye multicore glider reduces rate lru hawkeye remainder organize context previous describes hawkeye accessible empirically evaluate conclude remark related discus related cache replacement discus applies machine microarchitecture cache replacement prior applies hardware cache replacement replacement policy evolve sophisticated heuristic trend heuristic driven prior cache replacement policy heuristic commonly access variation lru policy  policy combination heuristic frequency counter reference interval prediction heuristic estimate reuse distance incoming reuse distance expires drawback heuristic policy customize limited cache access approach cache behavior predict future cache priority policy cache replacement binary classification goal predict incoming cache friendly cache averse   monitor eviction sampler load instruction likely insert cache friendly instead behavior heuristic policy hawkeye learns optimal glider  aircraft built principle flight underlie powerful complex  aircraft glider simplicity stem reliance powerful aircraft altitude apply cache replacement micro october columbus usa access oracle label access hawkeye cache replacement supervise hawkeye predictor hawkeye PC predictor machine previous applies machine cache replacement closely related online perceptron improve accuracy cache replacement predictor perceptron predictor program counter input feature glider perceptron program counter differs input representation glider unordered unique PCs benefit duplicate occurrence PC glider effectively longer glider perceptron hardware budget relax requirement unique PCs glider faster behavior distinct predictor entry recently perceptron outperform offline genetic algorithm relevant feature comprehensive craft feature beyond information differs identify insight effective feature representation glider outperforms feature within neural network model promising avenue future research finally earlier  genetic algorithm modulate cache priority cache enlighten unifies notion cache insertion promotion generalize offline program machine computer architecture extremely machine algorithm perceptron dynamic prediction prediction perceptrons enable inspire future academic commercial predictor implementation complex machine algorithm adopt directly implement hardware predictor resource management prefetching dram schedule largely impractical hardware complexity training latency background recurrent neural network attention mechanism background topic supervise machine model label data hawkeye cache hawkeye cache replacement policy cache replacement supervise predictor optimal cache cache access overall structure hawkeye component  simulates optimal behavior training label hawkeye predictor learns optimal hawkeye predictor binary classifier goal predict data load memory access likely cached optimal algorithm cache friendly data insert cache priority cache averse data insert priority  hawkeye predictor cache computes opt decision remembers opt decision cache access opt insertion priority PC overview hawkeye cache hawkeye PC feature maintains counter memory access PC tend cache friendly cache averse hawkeye cache successful cache replacement championship predictor achieves accuracy challenge benchmark recurrent neural network recurrent neural network extremely popular achieve performance sequential prediction nlp recognition sequential prediction prediction entire sequence sequence classification within sequence sequence label technically rnns internal hidden sequence hidden timestep depends previous hidden input lstm widely variant rnns complex within sequence complex exhibit non linear correlation sequence agreement english complicate prepositional layer singular prepositional layer lstm successfully apply formulation cache tag entity recognition nlp sequence label task aim assign label micro october columbus usa zhan shi  huang akanksha jain calvin lin attention mechanism lstm recently couple attention mechanism enable sequence model focus attention input perform machine translation source french target english attention mechanism correlation french correspond english translation another visual attention mechanism focus important image relevant mathematically typical attention mechanism quantifies correlation hidden previous sequence function normalize softmax function exp exp attention impact hidden hidden function chosen attention apply hidden obtain context vector  context vector cumulative impact hidden along hidden output improves accuracy hawkeye hawkeye learns optimal cache improvement hawkeye predictor accuracy replacement decision closer  optimal cache rate improve predictor accuracy replacement policy hawkeye limited program context namely PC repetitive cache behavior load PC tend cache friendly policy predict future access PC cache friendly aim improve prediction accuracy exploit richer dynamic program context specifically sequence memory access memory access formulate cache replacement sequence label goal label access sequence binary label specifically input sequence load identify PC goal PC tends access cache friendly cache averse identify load PC instead memory address PCs frequently memory address training importantly lstm proportion unique input unique address typical input lstm memory address infeasible lstm summarize approach unconstrained offline cache model unconstrained cache model offline offline model lstm attention mechanism identifies important PCs input sequence model significantly outperforms hawkeye predictor offline analysis analyze attention layer discover important insight cache decision primarily presence memory access sequence encode input feature PCs compactly important memory access easily identify hardware hardware friendly linear model practical online model insight analysis practical svm model online identify important PCs svm model accuracy lstm online version svm model essentially perceptron lstm attention introduce lstm model cache replacement lstm input sequence load instruction assigns output binary prediction sequence prediction indicates correspond load cached PC PC pcn pcn lstm input sequence output label lstm sequence label model input sequence PCs output  cache averse label PC  binary cache decision PC pcn PC warmup sequence attention layer embed layer lstm lstm lstm lstm attention lstm network architecture lstm handle sequence input recurrently apply cache replacement micro october columbus usa network architecture lstm model consists layer embed layer layer lstm attention layer PCs categorical model representation PCs PC vocabulary PCs program however representation ideal neural network treat PC equally learnable representation categorical feature PC embed layer lstm layer lstm layer learns cache behavior lstm attention layer correlation PCs layer attention layer memory access trace  model preprocess trace slice  sequence avoid lose context slice trace overlap consecutive sequence sequence sequence warmup sequence context prediction sequence memory access sequence PC pcn sequence PC PC context PC pcn training output decision  sequence specific hyper parameter insight lstm model lstm model effective impractical conduct understand lstm lstm accuracy improves increase PC accuracy benefit saturate observation observation model benefit PCs PCs gain deeper insight attention lstm model attention mechanism source accuracy analysis attention layer reveals PCs valuable feature completely representation PC unnecessary explain attention layer reveal insight attention layer attention layer theart attention mechanism equation attention layer dot compute attention vector capture correlation target source sequence define target load instruction model prediction define source load instruction sequence model prediction specifically compute function hidden target source factor normalize distribution softmax function attention vector compute context vector concatenate representation vector exactly output decision dot function exp exp attention factor originally magnitude dot input dimension factor moderate increase factor sparsity attention vector minimal influence accuracy sparse attention vector indicates source sequence influence prediction cache model attention quantify correlation target memory access source memory access timesteps unfortunately attention layer without factor nearly uniform attention distribution useful information avoid uniform distribution increase factor sparsity attention distribution thereby reveal dependence source access target access attention distribution cumulative probability acc acc acc acc acc cumulative distribution function attention distribution omnetpp insight attention layer benchmark cumulative distribution function attention distribution factor surprisingly without lose accuracy attention distribution becomes bias towards source access model prediction source memory access attention distribution consecutive memory access respectively attention vector target memory access correlation source target weak correlation typically source memory access dominant micro october columbus usa zhan shi  huang akanksha jain calvin lin offset source relative target target index consecutive memory access offset source relative target target index consecutive memory access attention vector consecutive memory access axis index target memory access axis offset source memory access target target memory access strongly correlate source memory access mcf omnetpp soplex sphinx astar lbm average accuracy sequence shuffle sequence accuracy sequence randomly shuffle sequence attention source memory access influence cache decision target memory access zoom source memory access dominant attention nearly target oblique offset increase index oblique source access important almost target access therefore obtain observation observation model achieve accuracy attend source observation posit optimal cache decision sequence presence important PCs confirm conjecture randomly shuffle sequence randomly shuffle sequence marginal performance degradation observation observation prediction accuracy largely insensitive sequence observation important insight cache important insight optimal cache decision predict load instruction primarily presence PCs sequence appropriate feature cache simplify sequence label binary classification understand program semantics insight source target PCs source code model application specific semantics cache behavior target PCs integer svm sparse binary feature insight reveal substitute lstm simpler model capture information within input sequence therefore simplify input feature representation remove duplicate PCs forego information simplify craft feature hardware friendly integer vector machine ISVM remove duplicate PCs craft feature capture effective PCs denote compress sparse binary feature PCs memory access sequence sparse binary feature vector elsewhere specifically sparse binary feature vector PCs tth entry indicator denote PC within sequence vector unique PCs memory access representation sparse binary feature sequence regardless PC sparse representation sequence identical feature exploit important optimal cache decision thereby simplify prediction apply cache replacement micro october columbus usa sequence PC PC PC sequence representation sparse binary feature sequence PC PC PC sequence representation sparse binary feature sparse binary feature simplicity PCs svm sparse binary feature integer operation cheaper hardware float operation integer svm ISVM integer margin rate variation exist hinge loss objective function optimization define max vector svm output binary feature gradient descent rate integer equivalent optimize objective function rate max suppose optimize initial vector rate trace optimize initial vector rate prediction training sample therefore rate update integral avoid float operation ISVM online manner equivalent perceptron threshold prevent inadequate training ISVM amenable hardware implementation vanilla svm simpler model likely converge faster lstm model achieve performance online manner glider consists ISVM model sparse feature hardware hardware implementation glider predictor component PC register  ISVM  maintains unordered PCs core model  lru cache recent PCs ISVM PC ISVM model mapped cache indexed hash PC return ISVM PC PC ISVM consists PCs register correspond content  hash  index retrieve correspond index  contains PC PC PC PC PC retrieve     ISVM ISVM  PC PC PC PC PC ISVM cache friendly cache averse PC register  ISVM glider predictor  training prediction detailed storage latency analysis discus operation glider predictor detail detail aspect replacement policy insertion eviction refer reader hawkeye policy training glider behavior sample access sample glider retrieves correspond PC  incremented  determines cached decremented otherwise perceptron update update sum threshold threshold glider predictor dynamically selects fix threshold adaptive threshold benefit core workload performance largely insensitive choice threshold multi core workload prediction prediction correspond PC  sum summation threshold simulation predict cache friendly insert priority RRPV summation predict cache averse insert priority RRPV remain sum cache friendly confidence insert medium priority RRPV reference prediction RRPV counter replacement policy indicates relative importance cache micro october columbus usa zhan shi  huang akanksha jain calvin lin baseline configuration cache KB cycle latency cache KB cycle latency cache KB cycle latency llc per core MB cycle latency dram trp tRCD tcas mhz GB core GB core statistic benchmark offline analysis ave ave program access PCs  access access per PC per addr mcf omnetpp soplex sphinx astar lbm evaluation evaluate offline ISVM model powerful lstm model glider online model policy cache replacement championship practicality methodology simulator evaluate model simulation framework release  cache replacement championship crc ChampSim model processor stage pipeline entry reorder buffer cache hierarchy parameter simulated memory hierarchy benchmark evaluate model  application spec cpu spec cpu gap define application llc per kilo instruction MPKI benchmark reference input crc simpoint generate benchmark sample billion instruction cache instruction behavior billion instruction multi core workload multi core simulate benchmark core workload simulate simultaneous execution simpoint sample constituent benchmark benchmark execute instruction benchmark  application instruction benchmark simultaneously throughout sample execution multi core simulation methodology crc evaluate performance report speedup normalize lru benchmark metric commonly evaluate cache overall performance avoids domination benchmark ipc metric compute program cache compute ipc environment  ipc execute isolation cache IPCs  compute ipc sum  IPCs  benchmark normalize ipc ipc lru replacement policy setting offline evaluation lstm svm typically offline multiple iteration entire dataset evaluate model trace llc access generate application ChampSim llc access trace contains PC optimal decision tuple optimal decision obtain efficient variant  algorithm model significant training offline model instruction subset core benchmark benchmark statically summarize offline evaluation trace training model evaluate insensitive split ratio training offline evaluation model iteratively convergence baseline replacement policy offline setting accuracy attention lstm offline ISVM model hardware cache model namely hawkeye perceptron hawkeye statistical model assumes memory access PC cache behavior hawkeye counter counter associate PC incremented decremented optimal decision PC perceptron linear perceptron model feature PC memory access perceptron model respect PCs longer PC effective without comparison PC feature implement svm hinge loss perceptron PC memory access respect learns  optimal evaluate glider practical replacement policy glider hawkeye  fourth  recent cache replacement championship crc technique code publicly available crc thread benchmark simulate  optimal replacement policy min although implementation  become implementation feature model label refer model perceptron offline comparison inspire apply cache replacement micro october columbus usa comparison offline model accuracy model offline attention lstm improves accuracy hawkeye baseline accuracy improvement hawkeye offline ISVM performance lstm confirm insight approximate powerful attention lstm simpler hardware friendly predictor mcf omnetpp soplex sphinx astar lbm average accuracy hawkeye perceptron offline ISVM attention lstm accuracy comparison offline predictor bwaves mcf omnetpp wrf pop rom bfs bzip cactusADM gemsfdtd lbm leslied mcf omnetpp soplex sphinx wrf average accuracy hawkeye glider accuracy comparison online predictor comparison online model accuracy speedup practical model online program executes glider hawkeye  online training accuracy glider accurate online model hawkeye subset benchmark training offline model accuracy improves offline improvement glider effective offline attention lstm model insight offline training online predictor core performance glider significantly reduces llc rate comparison replacement policy glider achieves average reduction memory intensive benchmark hawkeye  reduction respectively glider achieves speedup lru contrast hawkeye  improve performance lru respectively improvement insight derive offline attention lstm model practical online cache replacement policy multi core performance glider performs core improves performance improvement hawkeye  respectively feature insight applicable private cache effective sequence relationship offline accuracy sequence attention lstm unique PCs offline ISVM PCs perceptron observation lstm benefit PCs significantly previous offline ISVM unique PCs approach accuracy attention lstm sparse feature representation ISVM effectively capture representation linear model accuracy curve perceptron PC repetition ISVM saturates linear model representation practicality glider lstm practicality attention lstm glider along dimension hardware budget training overhead hardware budget glider lstm glider replace predictor module hawkeye ISVM module hawkeye MB llc hawkeye budget replacement per sampler  KB KB KB respectively overhead glider predictor replaces hawkeye per PC counter ISVM ISVM ISVM consumes byte PCs glider predictor consumes KB  access KB glider hardware budget KB attention lstm model magnitude expensive storage computational glider predictor lookup perform training prediction latency easily hidden latency access cache micro october columbus usa zhan shi  huang akanksha jain calvin lin bwaves mcf lbm omnetpp wrf cam fotonikd rom astar bwaves bzip cactusADM calculix gcc   lbm leslied libquantum mcf milc omnetpp soplex sphinx tonto xalancbmk zeusmp bfs spec spec GA rate reduction lru hawkeye  glider rate reduction core benchmark bwaves mcf lbm omnetpp wrf cam fotonikd rom astar bwaves bzip cactusADM calculix gcc   lbm leslied libquantum mcf milc omnetpp soplex sphinx tonto xalancbmk zeusmp bfs spec spec GA speedup lru hawkeye  glider speedup comparison core benchmark model computation lstm float operation model integer ops model model KB computational per sample operation training lstm predictor glider perceptron hawkeye convergence rate glider lstm model lstm typically multiple iteration converge cache training multiple iteration imply trace llc access training iteration  expensive instead cache replacement machine model online manner pas input data offline training offline ISVM apply cache replacement micro october columbus usa speedup lru core hawkeye  glider speedup core MB llc accuracy PC sequence attention lstm offline ISVM perceptron sequence attention lstm unique PCs offline ISVM sequence perceptron achieves accuracy iteration lstm iteration converge online model perceptron hawkeye converge limited accuracy accuracy iteration entire dataset attention lstm offline ISVM perceptron hawkeye convergence model practicality cache barrier model hardware prediction model computational offline training model attention lstm megabyte significantly exceeds hardware budget hardware cache addition lstm typically float operation model perceptron ISVM integer operation fortunately recent potential reduce model computational model compression technique quantization prune  binarization however model pre offline compress deployed hardware prediction program behavior varies benchmark benchmark input another input benchmark underfitting performance iteration compression technique model hardware predictor program semantics attention lstm model program semantics predict optimal cache omnetpp benchmark simulates network protocol http model discovers network message tend cache friendly message tend cache averse furthermore model discovers relationship distinguish message attention lstm model improves accuracy target PCs scheduleAt target PCs attend source PC target PC source PC hawkeye attention accuracy lstm accuracy specifically scheduleAt frequently inside omnetpp schedule incoming message scheduleAt argument message pointer dereferences pointer memory access reside pointer location model accuracy target load instruction PCs access attention lstm model significantly improves accuracy target PCs target PCs anchor PC source PC attention understand accuracy improvement target PCs scheduleAt scheduleAt invoked various location source code invocation passing message pointer anchor PC belongs  imply load instruction scheduleAt tend cache friendly scheduleAt   pointer micro october columbus usa zhan shi  huang akanksha jain calvin lin scheduleAt message  scheduleAt   scheduleAt   scheduleAt  function anchor PC function target PCs anchor PC belongs context target PCs code source PC int  scheduleAt simtime  msg simtime    msg  simtime msg   msg simulation  insert msg return assembly code target PC  mov rbx rsp mov rbx rax mov rbp rsi  mov rbx source code assembly code target PC scheduleAt bold whereas tend cache averse scheduleAt message pointer correlate load instruction scheduleAt model discover  cache locality   model specification hyper parameter attention lstm model glider explain identify hyper parameter namely sequence  lstm model unique PCs glider perceptron offline ISVM multiple correspond glider model update threshold fix avoid perform float operation decay offline model specification lstm split embed network optimizer adam rate glider unique PCs CONCLUSIONS cache replacement powerful  lstm model behavior program significantly improve prediction cache behavior impractical hardware cache replacement interpret lstm model derive important insight insight  predictor accuracy lstm model dramatically simpler lstm model performs online hardware predictor glider practical cache replacement policy accuracy performance superior previous policy broadly approach glider suggests crucial role systematically explore feature feature representation improve effectiveness simpler model perceptrons hardware designer already however domain knowledge critical approach domain expert formulate appropriately relevant feature effective offline model indirect interpret model illustrates instance cache replacement approach successful insight technique inspire microarchitectural prediction prediction data prefetching prediction