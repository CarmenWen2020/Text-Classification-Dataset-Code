machine perception application increasingly manipulate processing 3D focus registration primitive 3D data processing widely task odometry simultaneous localization mapping 3D reconstruction application routinely deployed constrain environment efficient registration critical tigris algorithm architecture specialized registration extensive exploration registration pipeline vastly offs accuracy performance KD performance bottleneck ideal candidate architectural specialization KD inherently sequential propose acceleration amenable data structure algorithm expose parallelism KD context registration accelerator systematically exploit parallelism incorporate architectural technique improve accelerator efficiency overall tigris achieves speedup reduction KD RTX gpu translates registration performance improvement reduction CCS concept computer organization purpose compute mixed augment reality keywords perception registration KD architecture algorithm artifact configurable pipeline http github com   pipeline introduction enable machine perceive understand visual data vital role promise intelligent future traditional machine perception focus mostly processing 2D visual data image video 3D data dimensional become increasingly important proliferation 3D data acquisition lidar flight camera structure scanner stimulates development processing algorithm algorithm become central application domain robotics navigation augment virtual reality 3D reconstruction data frame data frame align frame illustration registration frame align unified frame important building 3D  application registration align frame data globally consistent scene illustrates registration frame augment reality application align sequence frame 3D model environment virtual similarly mobile robot estimate orientation odometry align consecutive frame translational rotational transformation application increasingly deployed embed micro october columbus usa   tian  zhu limited performance budget enable 3D data registration tigris software hardware specialized 3D registration tigris achieves efficiency specialized datapaths logic mitigate inefficiency purpose processor combination acceleration technique exploit unique characteristic registration tigris identifies exploit parallelism capture unique data reuse reduce overall compute demand critically enable technique data structure algorithm accelerator architecture understand performance characteristic registration identify acceleration opportunity central challenge however registration expose parameter collectively optimize target obtain conclusion without overly specialize construct configurable registration pipeline perform thorough exploration surprisingly although significantly registration accuracy compute efficiency KD dominant kernel across constitute registration lucrative specialization target KD however inherently sequential due recursive traversal enable effective hardware acceleration propose parallel KD algorithm introduce finegrained parallelism amenable hardware acceleration algorithm stage KD data structure variant KD parallelism balance recursive brute however stage KD necessarily introduces redundant computation increase parallelism mitigate redundancy registration resilient imprecision introduce KD due noisy data algorithm incorporates approximate KD procedure reduces workload massive parallelism hardware data structure algorithm conjunction uniquely expose parallelism KD query parallelism QLP node parallelism nlp principle hardware accelerator exploit parallelism architectural mechanism specifically accelerator incorporates parallel processing PE exploit QLP apply pipelining exploit nlp within query parallel PEs pipelining establish technique effectively apply KD architectural optimization leverage compute data access specific KD evaluate tigris purpose consist intel xeon cpu nvidia RTX gpu tigris achieves speedup reduction KD gpu translates speedup reduction registration knowledge focus architecture specialization processing summary contribution identify KD inherently performance bottleneck registration carefully navigate algorithmic parametric registration demonstrate registration tolerant error introduce KD propose acceleration amenable KD algorithm building novel stage KD data structure algorithm expose massive parallelism hardware reduce compute accelerator architecture algorithm accelerator incorporates architectural optimization specific KD effectively exploit parallelism organize sec introduces background processing sec performs extensive algorithmic exploration identify KD performance bottleneck registration sec acceleration amenable KD data structure algorithm sec describes correspond tigris accelerator architecture sec experimental methodology evaluation sec tigris context related sec concludes background introduces data sec registration task application domain data sec data collection 3D cartesian coordinate coordinate 3D directly preserve 3D geometric information scene spatial relationship avoid estimate information 2D image proliferation 3D sensor emerge 3D geometry application robotics massively increase 3D data facto representation data obtain 3D sensor conventional stereo structure camera estimate scene 3D geometry computational active sensor lidar  principle mechanism sensor eventually data structure focus fundamental processing algorithm independent data obtain registration building virtually application registration transformation matrix aligns frame globally consistent specifically source frame target frame goal registration tigris architecture algorithm 3D perception micro october columbus usa detection correspondence estimation  correspondence rejection normal estimation source raw correspondence estimation RPCE transformation error minimization target descriptor calculation detection normal estimation descriptor calculation converge converge initial estimation initial estimation tune registration pipeline consists initial estimation phase tune phase pipeline expose knob accuracy performance analysis algorithmic parametric choice shade stage KD dominant kernel algorithmic parametric choice registration pipeline initial estimation tune stage normal estimation detection descriptor calculation  rejection RPCE transformation estimation algorithm choice   dnn sift  harris  shot  thresholding RANSAC normal shoot projection error metric solver parameter radius radius reciprocity distance threshold ratio threshold reciprocity convergence criterion estimate transformation matrix transforms minimizes euclidean distance error transform apply transformation matrix MX homogeneous coordinate respectively transformation matrix consists rotation matrix translation matrix freedom significance registration registration primitive application domain registration machine perception application odometry mapping instance autonomous navigation capture consecutive frame register obtain transformation matrix navigation estimate trajectory rotation translation odometry ego estimation similarly registration 3D reconstruction frame align another merge global scene registration application pipeline collaborate modality camera slam visual data focus improve efficiency core registration operation independent application registration performance characterization characterizes performance registration configurable registration pipeline expose accuracy performance sec exhaustive exploration algorithmic parametric choice performance bottleneck KD ideal acceleration candidate sec pipeline implementation publicly available http github com horizon research  pipeline registration pipeline exist implementation registration offs accuracy performance intuitively achieve registration accuracy increase workload vice versa goal however overly specialize implementation derive generalpurpose benefit obtain generally applicable conclusion observation registration implementation decision pipeline substrate allows construct purpose pipeline configurable knob implementation instance critically pipeline expose knob tune algorithmic choice parametric choice within algorithm pipeline adopts phase consist initial estimation phase tune phase phase performs initial estimation transformation matrix tune phase accuracy converges rationale  tune phase usually iterative solver minimize global registration error solver easily trap local minimum poorly initialize carefully initial estimation phase significantly improve efficiency accuracy tune illustrates highlevel architecture pipeline tbl algorithmic parametric knob expose pipeline goal initial estimation phase calculate initial transformation matrix salient micro october columbus usa   tian  zhu translational error normalize DP DP DP DP DP DP translational error rotational error normalize DP DP DP DP DP rotational error quantify accuracy performance tradeoff annotate pareto optimal execution normalize source salient target image registration 3D normal estimation calculates normal normal 3D vector perpendicular tangent normal important metadata later stage calculate feature descriptor estimate correspondence detection stage selects contains representative information target source operating improves compute efficiency explore feature extraction algorithm  sift  parameter sift feature  feature feature descriptor calculation stage computes feature descriptor feature descriptor dimensional representation encodes neighborhood information therefore richer information registration essentially stage convert 3D dimensional feature dimension feature depends specific feature descriptor explore descriptor  shot algorithmic parameter radius calculate descriptor correspondence estimation  stage establishes correspondence source target frame feature descriptor specifically  establishes correspondence source frame target frame feature feature feature generate previous stage explore reciprocal perform correspondence rejection stage frontend remove incorrect correspondence previous stage generates correspondence initial transformation matrix estimate explore correspondence rejection algorithm classic RANSAC algorithm simply threshold distance DP DP DP DP DP DP DP DP ratio normal estimation detection descriptor calculation error minimization correspondence rejection RPCE  distribution across stage DP DP DP DP DP DP DP DP ratio KD KD construction operation distribution KD operation distribution registration pareto optimal denote dpi obtain initial transformation matrix allows source transform tune phase estimate transformation matrix target effectively refining initial tune phase popular iterative closest framework iterate stage raw correspondence estimation RPCE stage establishes correspondence source target RPCE  RPCE 3D transformation estimation stage formulates error correspond identify previously minimizes error optimization solver transformation matrix transforms becomes source fed RPCE stage explore error formulation error solver singular decomposition levenberg marquardt algorithm another parameter explore convergence criterion determines termination icp impact accuracy compute performance bottleneck analysis exploration configurable pipeline performs exploration DSE identify representative performance bottleneck specify algorithmic choice parameter described tbl tigris architecture algorithm 3D perception micro october columbus usa canonical KD data structure  stage KD data structure comparison canonical stage KD data structure shade node node prune stag data structure exactly correspond portion classic data structure leaf node organizes unordered sub enable exhaustive expose parallelism stage data structure node node oppose node classic data structure widely adopt kitti dataset perform xeon processor sec detailed experimental setup translation error execution rotational error execution DSE confirm vast offs expose configurable pipeline importantly identify pareto optimal frontier annotate meaningful conclusion focus analyze pareto optimal performance bottleneck goal identify universal performance bottleneck accelerate improvement overly examine per stage performance pipeline registration distribution across stage described pareto optimal DP normal estimation descriptor calculation RPCE dominate stage constitute however dominant stage consistent across instance normal estimation stage contributes execution DP ideal acceleration target contributes execution DP DP diversity stage wise distribution indicates accelerate stage yield operation within stage however normal estimation descriptor calculation RPCE instance calculate normal normal estimation stage identify normal calculate similarly definition correspondence RPCE stage identify KD arguably efficient data structure widely average complexity logn majority registration implementation KD KD inherently algorithmic requirement pipeline stage KD operation dominates registration across DPs KD operation consistently contributes accelerate KD performance optimization generally applicable registration implementation acceleration amenable KD data structure algorithm KD inherently sequential traversal enable hardware acceleration propose mechanism expose massive parallelism KD reduce compute negligible accuracy loss KD data structure approximate algorithm describes parallelism expose KD data structure sec quantify error tolerate KD sec algorithm sec stage KD data structure briefly classic KD data structure associate algorithm stage kdtree data structure expose parallelism introduce redundant canonical KD KD data structure organizes dimensional binary enable efficient node dimensional non leaf node implicitly generates splitting hyperplane sub subtree essentially non leaf node corresponds bound dimensional encapsulates node sub usually median generate splitting KD balance registration mainly involves radius NN query dimensional former return within radius query latter return query without lose generality NN explanation KD algorithm node recursively traverse query algorithm micro october columbus usa   tian  zhu leaf redundancy radius NN redundancy ratio leaf operation radius NN node stage KD introduces redundant node redundancy quantify ratio node stage KD classic KD redundancy increase leaf grows leaf define leaf node unordered node node return distance algorithm sub node critically bound sub intersect hypersphere surround query entire sub skip node guaranteed outside technique prune enables efficient KD KD shade prune prune reduces redundant computation skip unnecessary node serializes algorithm node obtain distance allows prune node later stage KD balance parallelism redundancy slight variant canonical KD data structure stage KD stage KD organization canonical KD stage KD split height  exactly  classic KD leaf node organizes unordered oppose sub canonical data structure leaf node organize unordered node leaf node parallel essentially stage KD enables exhaustive sub extreme  stage KD equivalent exhaustively fundamentally stage KD introduces parallelism redundancy canonical KD data structure stage data structure node traverse exhaustively leaf node oppose node classic data structure intuitively shorter expose parallelism introduces redundancy kitti odometry dataset sec detailed experimental setup redundancy introduce exhaustive varies leaf radius NN redundancy quantify ratio node translational error  sparse RPCE dense sensitivity translational error NN return instead translational error NE dense sensitivity translational error radius return within registration error axis varies error axis error robust  KD dense NE RPCE sensitive sparse  stage KD classic KD leaf define leaf node unordered classic KD leaf stage KD leaf leaf increase height decrease exhaustive redundancy increase leaf stage KD introduces redundant node NN radius redundancy grows faster NN radius NN benefit prune radius suffers exhaustive redundancy introduce radius NN sheer node radius NN absolute node leaf increase redundancy introduce stage KD significant radius quantify error tolerance stage KD data structure expose parallelism introduces redundancy leaf node mitigate redundancy observation KD entire registration pipeline error tolerant perform inexact stage KD reduce amount computation retain parallelism quantitatively demonstrates error resilience mechanism exploit resilience registration resilient inexact KD acquire data inherently approximation due sensor movement sensor acquisition uncertainty data acquisition registration algorithm strives minimize global error local  compensate global error injection understand impact inexact KD registration accuracy manually inject error KD quantify registration accuracy varies KD accuracy specifically tigris architecture algorithm 3D perception micro october columbus usa algorithm approximate KD input   LF threshold thd LF leader closest leader   LF leader dist  thd approximate   precise leaf node LF LF LF leader  inject error NN replace return query query similarly inject error radius replace return within sphere delineate radius within spherical delineate radius parameter error inject radius NN KD respectively error tolerance multiple stage KD mainly inject error stage normal estimation NE raw correspondence estimation RPCE contribute heavily execution former radius latter NN registration error varies error inject RPCE NE respectively due limit translational error trend rotational error error denote standard deviation frame error sequence registration error statistically robust error introduce radius NN potential relax KD accuracy instance registration error virtually radius return precise return within critically instance KD equally amenable approximation NE stage RPCE stage dense error introduce KD operates sparse data detrimental registration accuracy instance correspondence estimation  stage operates sparse feature data overlay registration accuracy varies error introduce  stage return registration accuracy loss overall KD dense amenable approximation opportunity greatly reduce amount computation KD focus NE RPCE stage dominate performance approximate KD motivate error resilience registration pipeline propose approximate KD algorithm reduces computation overhead accuracy loss observation query leaf node 3D partition therefore likely leverage insight split query leaf node leader follower query leader perform exhaustive leaf node query follower return closest leader dynamically adjust leader introduce discriminator thd distance query closest leader thd query leader algo pseudo code algorithm algorithm relies efficiency allows follower query closest leader oppose leaf node incur closest leader assume leaf node leader return leader consists follower query algorithm succeed model understand performance gain sec KD accelerator describes accelerator overview architecture sec component sec sec accelerator overview data structure algorithm expose parallelism query parallel exhaustively leaf node query parallelism QLP exhaustive stage node parallel within query node parallelism nlp hardware architecture mechanism parallelism exploit data locality overview accelerator accelerator consists FE responsible responsible leaf node FE recursion RU processing query exploit QLP incoming query insert FE query queue  RU  query query RU sends query micro october columbus usa   tian  zhu global buffer query buffer PE PE PE PE PE PE query buffer query buffer recursion  RS RN CD PI CL bypass query distribution network recursion  RS RN CD PI CL bypass recursion  RS RN CD PI CL bypass FE query queue query buffer buffer query stack buffer buffer tigris accelerator architecture overview FE consists recursion RU query consists SU query exhaustively leaf node query distribute FE buffer  query FE query queue global buffer maintains metadata SU responsible leaf node query FE insert SU query buffer  SU schedule query execute SU processing PEs exploit QLP nlp processing query input output metadata global buffer specifically buffer partition metadata input buffer query buffer query buffer return query stack buffer recursion stack query reserve maximal stack entry query height buffer recursion FE query query FE recursively leaf node upon query exploit QLP FE consists rus RU independently query popped  rus exploit QLP processing within query sequential due inherent depth RU node proceed node challenge RU expose intra query parallelism improve performance hardware gradually introduce architectural optimization exploit pipelining baseline processing query iteratively traverse dfs manner stack maintain traversal status iteration node stack node stack therefore processing query consists stage  fetch query  obtain query information address query stack global buffer RS stack tos query stack tos structure contains address node RN data global buffer CD calculate distance dist PI node stack dist insert buffer CL issue query leaf node baseline stage RU illustrate stage prepares query data query stage iteration query processing critically data dependency PI stage data stack RS stage  data stack dependency stall pipeline cycle consecutive node propose architectural optimization eliminate stall improve performance node PI stage node node stack whichever node later necessarily popped iteration PI stage directly RN stage eliminate stall cycle completely remove stall logic node CD stage PI stage decision logic earlier CD stage completely eliminates stall node bypassing eliminates stall node PI stage node bypassing aim node pipeline node immediately specifically node deem  entire sub skip sec node bypass bypassing prune node particularly significant NN exhaustive stage obtain tighter distance expose prune node bypassing augment node metadata distance information decode RN stage generates bypass signal RS stage node immediately query arrives leaf node query exhaustively leaf node node tigris architecture algorithm 3D perception micro october columbus usa stack entry stack entry RS query stack buffer node addr node RN calc dist CD compute logic insert PI idx  bypass logic insert  logic stack logic input buffer stack logic bypass buffer dist node node addr threshold threshold query buffer query query logic query FE query queue fetch query  cleanup CL query logic issue query buffer node addr node query node stack entry  tos addr query status logic node addr node addr recursion RU pipeline overview RU recursively traverse dfs manner query traversal status maintain stack introduce data dependency PI RS stage node bypassing eliminate pipeline stall processing compute logic insert  logic threshold buffer processing compute logic insert  logic threshold buffer dist dist node query node query memory access logic query buffer input buffer node access query buffer query access query issue logic node cache buffer leader logic leader buffer SU overview SU processing PE operating simd fashion PEs organize 1D systolic array improve data distribution efficiency SU adopts query stationary data query pin PE node PEs reuse query exhibit QLP nlp exploit QLP incorporates PEs handle query query access logic fetch query query buffer query processing query PE local register PE PE exploit nlp within query pipelined fashion PE datapath pipelined stage node node stage node driven node access logic stage computes distance dist query stage decides insert buffer dist pipeline guaranteed proceed stall dependency across node  MQSN naive PE handle arbitrary query distribute FE maximize PE utilization however memory bandwidth requirement PE potentially node multiple query multiple   alternative PEs query leaf node lower memory bandwidth requirement PEs consume node multiple query  MQSN MQSN memory efficient query issue logic perform associative query buffer query leaf node ensure PE array utilization however increase complexity pipeline cycle hierarchical MQSN enable complexity effective PE achieve PE utilization PEs responsible leaf node issue logic schedule PEs occupy increase schedule efficiency PE utilization SU specifically SU PEs query buffer query SU FE query issue logic issue query PEs address generation logic access node query hierarchical MQSN achieve PE utilization  complexity effective adopt MQSN quantify performance  later micro october columbus usa   tian  zhu associative perform query issue logic query  remain entry  query parallel terminates query PEs associative amortize across execution query typically magnitude longer associative overall performance relatively insensitive exactly leaf node mapped SU policy target SU ID query distribution network FE logic systolic PE organization reduce data distribution organize PEs SU 1D systolic array PEs node node PEs reuse query dataflow naturally query stationary query pin PE alternatively PEs organize simd lane data distribution fabric bus multicasts PEs utilized approximate SU approximate sec allows follower query return leader query instead leaf node node augment memory access logic leader logic determines query approximate leader node access logic fetch node buffer input buffer actual compute distance incoming query exist leader reuse PEs SU computation leader query leaf node local leader buffer cap entry profile kitti dataset leader buffer rare simplify hardware implementation worth cap leader buffer improves accuracy query exactly without rely leader node cache MQSN significantly reduces node load traffic load node contributes memory traffic query consecutively issue FE likely leaf node propose cache capture locality load node reduce memory traffic node cache organize entire node node node entry organize fifo queue node node access sequentially  entry node cache node within entry access fifo greatly simplify hardware implementation evaluation evaluation methodology sec analyze tigris accelerator sec performance consumption accelerator baseline KD alone pipeline sec tease apart contribution optimization sec analyze performance tigris sensitive resource configuration sec experimental methodology hardware implementation synthesize route accelerator datapath  cadence technology memory generate SRAM compiler estimate synopsys  annotate switch activity datapath clocked mhz dram estimate micron ddr specification calculator cycleaccurate simulator parameterized synthesis memory estimation performance analysis dataset evaluate tigris widely kitti odometry dataset sequence dataset truth sequence consists frame kitti dataset obtain popular velodyne hdl lidar representative acquisition report average across frame unless otherwise metric evaluate tigris performance accuracy KD registration frame entire sequence accuracy standard rotational translational error baseline performance characterization perform cpu implementation sec registration pipeline implement cpu evaluation gpu cuda implementation KD popular  library KD gpu faster cpu cpu gpu setup baseline KD gpu operation cpu cpu core xeon processor gpu nvidia geforce RTX widely library PCL develop registration pipeline integrate  implementation KD gpu nvidia smi utility cpu intel RAPL counter via directly reading processor MSRs demonstrate generally applicability tigris evaluate pareto optimal registration pipeline sec DP optimizes performance DP optimizes accuracy analysis configure tigris accelerator rus sus PEs per SU chip SRAM accommodate per frame representative density acquire input buffer query buffer MB query stack buffer MB accommodate maximal height sufficient kitti dataset FE query queue MB query buffer KB per SU query node cache configure KB finally buffer MB buffer tigris architecture algorithm 3D perception micro october columbus usa speedup KD skd acc KD acc skd reduction accuracy orient DP speedup KD skd acc KD acc skd reduction performance orient DP KD speedup reduction pareto optimal DP accuracy orient DP performance orient interface dram traffic critical overall SRAM estimate datapath RU SU PE mostly dominate logic computes euclidean distance float arithmetic combinational logic occupies overall SRAM occupy compute logic performance comparison speedup tigris achieves significant speedup KD baseline accuracy orient DP KD speedup tigris accelerator KD acc KD stage KD height leaf acc skd gpu baseline KD leaf KD comparison purpose speedup gpu  KD height skd acc KD acc skd apply approximate accuracy loss acc skd achieves speedup KD skd faster KD acc KD however faster KD baseline KD accelerator performance almost completely bottleneck recursive sus almost idle resource utilization confirms accelerator data structure expose parallelism cpu implementation KD acc skd achieves speedup KD translates significant toend performance improvement specifically acc skd reduces overall registration gpu baseline KD cpu implementation respectively tigris achieves speedup performance orient DP performance comparison across DP acc skd achieves speedup skd KD alone translates performance improvement speedup DP DP optimize performance DP tight criterion exhaustive instance normal estimation stage DP radius radius DP speedup opt bypass  acc skd variant reduction MQSN speedup reduction architectural optimization memory traffic dist acc skd acc KD FE query query buf query stack buf query node cache buf memory traffic distribution cache alleviates buffer traffic relaxed radius expose exhaustive benefit SU tigris overall performance improvement demonstrate applicability tigris reduction overlay reduction axis acc skd achieves reduction KD KD DP DP respectively reduction along speedup translates significant saving efficiency instance acc skd reduces consumption KD factor DP consumption DP PE contributes consumption contribute SRAM SRAM leakage dram pipeline acc skd achieves reduction KD consumption acc KD acc skd acc KD expose exhaustive leaf node exclusively rus sus idle performance overall consumption acc skd approximate empirically meter approximate threshold sec NN radius threshold radius setting approximate impact translation error increase rotational error meter DP meter DP DP approximate KD achieves performance improvement acc skd translate performance improvement improvement compute reduction approximate algorithm reduces node NN contributes radius contributes optimization bypassing RU bypassing allows prune node exit pipeline allows node query stack immediately technique reduce pipeline stall improve performance speedup KD acc skd variant without technique opt bypassing bypass bypassing micro october columbus usa   tian  zhu RU RU RU RU performance SU PE sus sus sus sus performance comparison performance sensitivity hardware parameter rus sus PEs per SU legend bypassing improves performance achieves improvement  MQSN  allows PEs SU query leaf node additional memory traffic sec speedup  organization acc skd KD MQSN variant  performance MQSN variant however additional memory traffic significantly increase consumption axis overlay reduction various scheme KD MQSN consumption almost node cache node cache reduces global buffer traffic memory traffic distribution across data structure acc skd buffer traffic account traffic without node cache reduce node cache memory traffic memory node cache reduces acc KD exhaustive buffer traffic buffer traffic contribute traffic node cache sensitivity analysis performance tigris hardware resource software parameter hardware configuration parameter RU SU PEs per SU sweep parameter KD configuration overall performance improves consumption increase detailed performance comparison configuration curve RU axis sweep SU PE RU performance bottleneck improve capability increase SU PE improves overall marginally RU increase accelerator becomes balance choice rus rus PEs per SU sits knee curve complexity efficient decision height height software configuration assume height per leaf node KD consumption function height performance increase height increase redundancy exhaustive however performance diminish return height around beyond performance decrease recursive RU reduce node parallelism within query exploit sus optimal height largely consistent across KD instance pipeline optimal height mainly depends factor data frame hardware organization specific registration pipeline KD instance factor optimal height related registration registration pipeline generally category density registration extreme algorithm registration tolerate outlier computationally prohibitive extreme algorithm sample feature efficient compute suffer local minimum registration pipeline extreme predominant choice feature coarsegrained initial estimation tune prior proposes specific specific accuracy offs construct flexible pipeline perform exploration reveals pareto optimal performance bottleneck analysis recent neural network dnn registration dnns susceptible limited specific registration estimation dnns mostly replace stage registration pipeline detection normal estimation description calculation tune icp rely overall pipeline architecture described sec knowledge proposes hardware accelerator registration prior mostly focus algorithmic development KD acceleration KD widely application domain beyond registration graphic data analytics image video processing tigris accelerates fundamental KD algorithm applicable application domain tigris architecture algorithm 3D perception micro october columbus usa accelerate KD mostly explore context reduce gpu fpga tigris accelerator differs prior attempt systematic comprehensive exploitation parallelism KD specifically tigris accelerator exploit  parallelism QLP node parallelism nlp traversal exhaustive prior exploit QLP without nlp buffer kdtree allows nlp leaf node permit nlp traversal expose nlp traversal expose nlp leaf node accelerator incorporates architectural mechanism node bypassing MQSN systolic PE organization  purpose hardware gpus approximate KD approximate robotics graphic application information spur approximate KD knn algorithm approximate KD algorithm differs prior quantify extent KD approximate context registration accuracy prior mostly focus accuracy KD alone approximate algorithm applies NN radius prior limited NN conclusion proliferation 3D sensor ubiquitous 3D perception processing increasingly become cornerstone machine perception application architect knowledge comprehensively characterizes address performance bottleneck registration approach data structure algorithm accelerator compute kernel promising direction research