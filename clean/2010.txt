optimize bandwidth focus network decade optimization trend traditional internet application however emergence datacenters computer entity latency important bandwidth datacenter network pcie interconnect latency bottleneck communication network latency overhead contribute overall communication latency despite overhead pcie facto interconnect standard server establish maintain decade addition pcie overhead data movement network software stack consume processor cycle ultra latency networking challenge tackle pcie data movement overhead architect netdimm memory network interface capable memory buffer clone netdimm network interface chip buffer device dual memory module leverage asynchronous memory access capability ddr memory module host processor memory NIC evaluation netdimm average improves per packet latency baseline network deploy pcie NICs CCS CONCEPTS hardware networking hardware dynamic memory computer organization client server architecture keywords network architecture memory compute introduction traditionally requirement network bandwidth ensure fairness avoid congestion network transport protocol tcp thrive decade network architecture throughput orient internet application file email server interactive web application web sensitive per packet delivery response millisecond acceptable satisfy service objective define percentile response throughput orient network driven development bandwidth network device 0Gb ethernet network interface NIC proliferation datacenters emerge application network requirement addition bandwidth latency communication become primary metric evaluate generation network ultra latency application memory cache performance compute financial trading benefit sub microsecond latency improvement network hardware software stack ethernet backbone datacenter networking technology tightly couple tcp IP protocol ensure reliable communication node datacenter deployment tcp offload along efficient implementation software stack significantly reduce computational overhead software stack ethernet network instance RDMA converge ethernet  protocol technically offloads network software stack ethernet NIC device implement priority inside NIC ethernet lossless  network achieve node node latency minimize software stack overhead technological advancement achieve network latency hardware limit pcie widely establish server interconnect technology pcie chip storage network accelerator device processor chip bleeding pcie gen theoretical bandwidth  pcie layer architecture protocol overhead layer reduces usable bandwidth latency overhead therefore pcie interconnect bottleneck latency communication network frequent transaction pcie interconnect contributor network latency software stack optimize network pcie subsystem contributes overall network latency packet various  0Gbps NIC besides pcie overhead data copying dma buffer application memory micro october columbus usa  NIC DIMM NIC DIMM cpu cpu cpu NIC DIMM pcie mem channel pcie NIC integrate NIC netdimm network interface architecture netdimm bottleneck network subsystem constitute per byte operation overhead network protocol confront pcie memory movement bottleneck previous propose reduce pcie transaction packet transmission reception integrate network interface processor chip integrate memory buffer NIC processing NIC offload software NIC logic develop zero networking although technique alleviate network overhead drawback proposal mostly minimize pcie transaction packet moreover NIC processor pcie interconnect pcie packet costly due overhead processor chip furthermore NIC processor chip manufacture vendor practical integrate chip lastly integrate NIC pollute chip cpu resource packet sec accelerate application technique cannot benefit purpose application manage program moreover NIC architecture suffer pcie overhead regard although zero networking eliminates data copying dma application buffer introduces security breach memory exhaustion extra virtual memory operation overhead nullify benefit propose network attach DIMM netdimm novel memory network interface utilizes ddr channel interconnect memory NIC processor netdimm integrates NIC buffer device dual inline memory module DIMM latency highbandwidth memory channel communicate processor netdimm leverage asynchronous memory access ddr specification seamlessly expose local memory capacity host processor host processor address furthermore netdimm memory buffer clone performance zero networking without drawback specifically netdimm contribution eliminate pcie bottleneck network subsystem netdimm memory channel instead pcie link interconnect NIC processor memory acceleration network stack data movement netdimm accelerates dma NIC dram NIC dram module furthermore netdimm performs memory buffer clone accelerate data movement network stack application transparent network stack acceleration netdimm kernel software stack minimal modification linux kernel therefore netdimm unmodified userspace application reduce memory interference network traffic netdimm reduces host memory channel utilization local memory channel netdimm transfer packet memory NIC netdimm split header payload packet reduces chip resource pollution netdimm NIC architecture netdimm significantly improves communication latency eliminate costly pcie transaction leverage physical proximity NIC dram data movement evaluation across various packet netdimm average reduces network latency server server employ pcie integrate NICs respectively replay trace facebook production cluster average per packet latency reduction replace pcie NICs netdimm across cluster lastly network application server application memory channel netdimm memory access latency memory access latency workload integrate NIC background information conventional network architecture ddr asynchronous memory access memory management linux kernel sec motivates netdimm sec explains netdimm architecture sec evaluation related sec sec conclusion background network architecture despite research innovation internet network architecture limited incremental update architecture remain creation internet resistance multi provider network  exist architecture consensus stakeholder moreover network architecture reliably decade radical become increasingly overall network hardware architecture server NIC processor pcie link NICs data DDIO technology reduce memory bandwidth utilization network packet packet NIC dma transfer packet buffer inside processor cache llc instead dram transmit packet DDIO enable NIC packet buffer allocate llc dma packet llc netdimm latency memory network interface architecture micro october columbus usa core core core core llc mem ctrl ctrl processor NIC dma int pcie dram memory channel pkt pkt RX TX server network architecture however DDIO usually llc capacity megabyte exhaust NIC RX TX rate moreover DDIO network function phenomenon dma leakage DDIO cache pollution application upper limit llc ethernet NIC employ circular buffer descriptor inside memory processor NIC consume packet rate interrupt handle interrupt moderation delay packet processing microsecond ultra latency network usually deployed adaptive polling mode explain NIC cpu memory interaction transmit TX RX packet ethernet NIC polling driver transmission reception boot NIC driver allocates RX TX descriptor initializes sends information NIC driver transmit function driver driver status NIC driver driver dma transfer NIC configuration register NIC dma device fetch available TX descriptor dram llc DDIO enable performs another dma transfer packet NIC NIC packet transmit ethernet link TX pointer update NIC packet destination NIC NIC available RX descriptor fetch dram llc NIC packet  RX descriptor buffer NIC RX descriptor information update driver polling driver notify packet reception driver socket buffer skb initialize data RX buffer ethernet header remove packet upper network layer asynchronous memory access subsection discus nonvolatile dual inline memory module nvdimm protocol specifically nvdimm ddr specification manages interact memory technology nvdimm technology persistence memory capacity memory channel interconnect interface processor JEDEC standard NVDIMMs nvdimm consists byte addressable dram module backup nand flash device nvdimm host ddr memory controller address dram  nvdimm access regular ddr DIMM host perspective nvdimm directly expose nand flash storage processor remove dram device nvdimm cannot CA DQ RD data deterministic dram operation CA DQ  adr data nvdimm operation rdy ID non deterministic rsp ID buffer device controller nand flash nvdimm dram dram dram dram dram dram dram dram nvdimm architecture asynchronous memory access nvdimm access regular ddr timing memory channel nvdimm timing nvdimm novel memory channel protocol allows asynchronous completion memory access feature nvdimm  nvdimm expose dram nand flash host processor address nand flash persistence memory technology 3D xpoint access timing dram conventional ddr protocol cannot access persistent memory ddr specification comprehend heterogeneous medium mixture  DIMM nvdimm facilitate nvdimm access ddr specification asynchronous memory transaction timing cacheline dram nvdimm ddr standard access cacheline nvdimm location data cached buffer device nvdimm access non deterministic latency request nvdimm request  command address request data request ID unlike dram operation nvdimm request ID facilitate access completion  command nvdimm medium data command immediately issue data medium command rdy issue response pin rsp ID request memory controller issue command data data append request ID available data bus DQ specific amount linux memory management memory address mapping physical memory address mapping decode physical address calculate channel rank address location dram DIMMs instal multiple memory channel memory mapping mode channel multi channel flex channel mode channel mode memory channel mapped significant physical address sequential address mapped memory channel multi channel mode sequential memory micro october columbus usa  address interleave multiple memory channel flex mode flexible memory mapping configuration address multi channel mode channel mode flex mode useful asymmetric memory configuration DIMM ddr nvdimm instal memory channel linux kernel memory allocation due hardware limitation physical memory treat differently linux kernel linux physical memory location zone linux primary memory zone zone dma contains dma zone dma contains dma device zone  contains memory cannot mapped kernel address machine zone normal contains regularly mapped  allocate memory kernel malloc userspace  allocate memory specific memory zone input argument apis allocate memory granularity linux apis network stack allocate network socket buffer core function allocation alloc wrapper apis allocate specify numa node memory zone motivation sec packet conventional NIC pcie memory channel transaction specifically client server application pcie transaction request response transfer research propose NIC dma architecture reduce pcie transaction network packet packet although architecture improve network latency pcie packet NIC respectively cpu NIC integration promising approach overhead mention latency packet node another 0Gb ethernet link information evaluation methodology refer sec evaluate NIC configuration discrete NIC dNIC conventional pcie gen NIC dNIC zero transmission reception dNIC zcpy NIC integrate cpu chip iNIC iNIC zero transmission reception iNIC zcpy pcie contribution overall packet transmission reception pcie  iNIC improves network latency dNIC latency improvement signify packet mainly faster access register clearly benefit remove pcie link cpu NIC latency networking enable zero copying NIC access application buffer dma buffer zero improves iNIC network latency byte byte packet respectively latency packet byte dNIC dNIC zcpy pcie  iNIC iNIC zcpy latency comparison NIC configuration packet various discrete NIC dNIC discrete NIC zero dNIC  integrate NIC iNIC integrate NIC zero iNIC zcpy pcie  overhead pcie interconnect discrete NIC configuration memory overhead increase packet packet benefit zero networking pcie overhead packet dNIC zcpy overall network latency spent pcie interconnect transfer byte byte packet respectively although iNIC zcpy ideal ultra latency network architecture limitation zero networking introduce security breach pin application memory memory exhaustion overhead virtual memory operation buffer management nullify gain zero networking integrate blown NIC cpu significantly increase processor specifically challenge NIC cpu manufacture vendor importantly iNIC pollute chip resource llc network rate memory interference application furthermore payload packet processor chip waste precious chip resource network function packet header cpu specific iNIC dNIC illustrate memory cache interference network packet sensitivity network bandwidth cache memory interference depicts sensitivity network bandwidth pressure memory machine equip xeon processor ddr memory channel intel 0Gbps XL QDA NIC intel memory latency checker mlc inject dummy memory request memory subsystem rate ratio memory request axis delay inject memory request interference memory subsystem axis achieve iperf tcp bandwidth memory interference iperf bandwidth significantly memory pressure mlc increase maximum memory pressure netdimm latency memory network interface architecture micro october columbus usa bandwidth gbps delay memory request cycle iperf memory pressure iperf bandwidth memory pressure corresponds  per memory channel iperf delivers achieve bandwidth without interference mlc sensitive network bandwidth interference memory subsystem moreover interpret another angle network traffic severe interference memory subsystem however tcp iperf regulate transmission rate processing capability receiver node therefore degradation local application performance iperf bandwidth decrease illustrate inefficiency network architecture server ideally completely remove pcie transaction exchange data processor NIC interconnect latency without  network bandwidth furthermore reduce memory interference decrease host memory subsystem utilization packet NIC involves prevent NIC inject traffic llc instead mechanism collectively brings byte packet processor application demand pcie standard developed interconnection technology around decade requirement replacement standard establish interconnection technology introduce specialized interconnect costly error prone interconnect seamlessly memory channel processor cache hierarchy facilitate data delivery cpu memory channel latency amongst chip interconnects server besides latency memory channel bandwidth ddr channel   bandwidth latency transfer KB ddr channel pcie link respectively importantly memory channel standard maintain interconnect motherboard server leverage unique feature memory channel propose memory network interface architecture NIC buffer device DIMM solves limitation dNIC iNIC eliminate pcie overhead utilize memory channel internal DIMM interconnects packet transmission reception memory buffer clone packet application dma buffer vice versa decouple header payload packet reduce llc pollution memory channel access network buffer dram reduce host memory channel interference network  DIMM motivate explanation sec propose netdimm latency memory network architecture building atop nvdimm architecture sec memory processing concept netdimm improves data transfer latency processor memory NIC explain hardware software component netdimm detail netdimm hardware architecture inspire asynchronous memory access ddr specification sec architect NIC buffer device DIMM overview overall architecture netdimm memory channel memory channel occupy DIMMs DIMMs conventional ddr DIMMs netdimm requirement  memory channel netdimm instal ddr slot ddr asynchronous memory request completion allows dram netdimm memory channel organization netdimm organization nvdimm depict internal architecture netdimm buffer device consists component nNIC integrate network interface nMC memory controller access netdimm local dram module nController logic extends nvdimm controller netdimm rout management logic ddr phy interface ddr physical interface protocol ddr physical interface contains protocol dram CA DQ rsp signal typical nvdimm device nCache dual SRAM buffer cache RX data reside local dram module  prefetcher pre load RX packet nCache local dram module RowClone enable dram dram device memory data copying expose local dram capacity netdimm host memory address therefore local netdimm memory manage host operating unified address nvdimm explain netdimm memory management sec nNIC phy independently access local dram module nMC arbitration memory access nNIC phy nController arbitration priority nNIC access access local dram host MC non deterministic host MC netdimm local dram module nMC nNIC phy access local dram module depends local dram module nNIC traffic request phy micro october columbus usa  nNIC ddr phy interface nCache nMC netdimm mem ctrl global memory channel ethernet connector netdimm netdimm buffer dev dram dram dram dram dram dram eth ddr DIMM ddr DIMM host MC netdimm ddr DIMM ddr DIMM host MC host global memory channel local memory channel  memory clone capable dram device local dram device nController netdimm architecture observation memory access host processor NIC regular spatial temporal locality plot relative address relative arrival memory request generate dma 0GbE NIC byte packet detailed experimental setup refer sec illustrate packet arrival generates burst memory request dma buffer burst consists cachelines byte host memory controller interval packet nCache  component exploit unique characteristic memory access improve host MC access latency netdimm address packet nNIC outside nNIC notifies nController nController implement functionality dma conventional NIC upon notification nNIC nController available descriptor buffer nMC  RX buffer nNIC descriptor reside netdimm local dram module sec explain descriptor allocate netdimm transfer RX packet netdimm local dram nController writes cacheline packet unless otherwise assume cacheline byte throughout relative relative address cachelines pkt pkt pkt pkt pkt pkt spatial temporal locality NIC memory access host processor perspective nCache rationale cache cacheline packet transport protocol header byte cacheline header packet processing packet network software stack moreover explain sec network function firewall packet payload application decision header information maximum header tcp IP packet byte cache byte packet header packet access copying userspace buffer entire packet nCache efficient reuse distance payload packet longer header assume maximum transmission MTU byte ethernet packet cachelines payload packet access application buffer consecutive request netdimm phy access access easy predict prefetcher prefetcher  netdimm  prefetches cachelines nCache therefore netdimm cache payload RX packet nCache reading entire RX packet nCache disable  cacheline RX packet contains header pollute nCache header packet access host processor flag cacheline nCache cacheline newly packet nCache  flag prefetches cachelines flag nCache reset flag access cacheline request global memory channel nController request data cached nCache data nCache immediately host MC otherwise nController creates request sends nMC data local dram nMC host asynchronous protocol explain sec request global memory channel nController construct memory assume nNIC checksum offload netdimm latency memory network interface architecture micro october columbus usa request nMC request nCache immediately queue nMC queue upon arrival nCache inclusive associative cache structure nCache data buffer data remove access RX packet netdimm host processor cache another location memory memory address unlikely access future therefore data nCache random replacement policy nCache occupy cachelines nCache victim cacheline nMC ensure coherency nCache local dram data nController snoop address request phy nNIC invalidates cachelines nCache conventionally copying memory location another involves processor data memory channel cache hierarchy memory channel destination memory location memory copying expensive operation copying KB ddr memory channel limitation zero driver sec envision memory data acceleration mechanism swiftly clone application buffer dma buffer vise versa netdimm extent utilize extend implementation RowClone mechanism RowClone memory bulk data copying mechanism utilizes dram internal architecture accelerate memory memory copying DIMM illustrates overview memory clone capable dram device location source destination address mode clone parallel mode FPM source destination sub array buffer clone activation source destination FPM mode highlight arrow pipeline serial mode PSM source destination dram device clone happens pipelining cacheline operation internal bus dram chip PSM mode highlight arrow clone mode gcm otherwise netdimm source netdimm buffer device writes pipeline mode destination address highlight arrow gcm operation conventional dma memory chip FPM gcm slowest mechanism important intelligently allocate source destination sub array within dram device extract maximum benefit memory clone sec explain netdimm implement intelligent memory allocation scheme efficiently data dma buffer application buffer netdimm software architecture subsection explain software stack enable netdimm overall minimum amount network software stack linux kernel software stack implementation buffer chip memory clone capable dram dev netdimm buffer device buffer PSM buffer chip memory clone capable dram dev buffer netdimm FPM memory buffer clone acceleration linux memory allocation api physical memory address mapping implementation netdimm driver tcp IP layer remain unchanged api skb allocation developed userspace netdimm driver evaluation however feasibility generality implementation developed linux kernel netdimm driver linux kernel software stack unmodified userspace application linux kernel driver explanation handle netdimm local memory netdimm driver discus local dram module netdimm leverage operating memory management functionality amount software stack minimum netdimm  expose local memory capacity netdimm host processor host physical memory address local memory capacity netdimm memory node numa despite access timing netdimm memory host global address reveal heterogeneity memory linux memory zone  netdimm multiple  instal memory channel memory zone define memory zone linux expensive memory zone linux addition define memory zone important intelligently allocate dma application buffer sub array extract maximum performance netdimm memory buffer clone capability achieve expose internal memory organization netdimm memory scheduler assumption organization memory rank netdimm micron  dram device rank consists dram device device consists sub array sub array consists capacity rank device subarray 8GB MB KB KB respectively organization physical memory address mapping netdimm assume KB illustrates geometric location consecutive memory rank physically sub array KB easy sub array implement alloc netdimm zone hint allocates netdimm zone sub array hint address hint api considers zone micro october columbus usa  rank device chip device 1GB MB subarrays sub arr sub arr sub arr sub arr buffer KB KB sub arr ID sub arr ID col ID byte address KB ID KB sub arr ID MB sub arr KB ID KB sub arr per rank address mapping configuration memory rank netdimm physical memory address mapping illustration physical location requirement effort api allocate sub array hint address another complexity handle local memory netdimm memory channel interleave physical address multiple memory channel memory channel interleave increase memory throughput parallelize memory access memory channel however disable memory channel interleave netdimm address global memory channel visible nNIC therefore netdimm address expose host channel mode host processor netdimm physical address continuous memory chunk leverage flex channel interleave mode sec physical address contains conventional ddr DIMMs multi channel mode another contains  address operating channel mode depicts unified address conventional DIMMs  memory channel interleave mode netdimm driver intel GbE driver develop netdimm driver netdimm pcie device  api configuration netdimm configuration conventional pcie NIC technique configure feature blown NIC without driver scratch NIC interface initialize creates transmit TX RX descriptor buffer initializes buffer pointer allocate dma buffer NICs  dma operation dma buffer span multiple physically contiguous netdimm physical location descriptor correspond dma buffer memory zone correspond netdimm benefit memory clone acceleration application allocate network data buffer netdimm memory zone TX RX DIMM DIMM interleave netdimm ddr DIMM mem ctrl netdimm ddr DIMM mem ctrl host netdimm netdimm physical address memory address channel interleave mode mixture ddr DIMMs  alloc netdimm  allocate descriptor data structure  RX TX dma buffer allocate location application buffer however alloc netdimm packet deteriorate network latency bandwidth netdimm rank distinct sub array accelerate demand memory allocation netdimm pre allocates distinct sub array hash  netdimm memory rank netdimm pre allocates MB demand dma buffer allocation corresponds capacity overhead 6GB netdimm  immediately return allocate specific sub array netdimm driver refill  concurrently background  allocation dma buffer critical packet RX TX complication application knowledge physical layer netdimm packet resolve flag skb header network data structure networking  allocate  belong connection establishment regular kernel address  flag skb header transmit function netdimm driver  flag driver skb data allocate TX dma buffer correspond netdimm initiate packet transmission skb pointer socket packet associate struct sock struct zone struct skb zone  netdimm driver therefore packet transmission connection information allocate skb buffer TX packet correspond netdimm memory zone  flag fallback mechanism memory  zone exhaust skb TX buffer allocate memory zone rare frequently packet netdimm pcie NIC nNIC packet dma buffer notify host processor notify processor newly packet packet transmission completion NIC typically interrupt signal polling agent interrupt approach mostly bandwidth network connection network latency critical polling mechanism mainly userspace network stack netdimm latency memory network interface architecture micro october columbus usa algorithm packet TX RX handle netdimm driver TX desc nex dma  SK dat dma buffer allocation SK CO PY  desc nex dma SK dat skb zone ET  desc nex dma memory  SK dat memory desc nex  aдs  desc nex  aдs kick transmission RX   desc nex fetch data netdimm SK dat  desc nex dma RX buffer allocation   SK dat desc nex dma desc nex  memory buffer clone SK upper    polling agent ter  ion newly  packet RX latency network prevent interrupt processing context switch overhead sec netdimm driver implement efficient polling agent resolution kernel timer polling netdimm efficient polling pcie NIC access register netdimm faster pcie NIC polling driver detects packet arrival RX routine driver alg netdimm memory flush invalidate instruction enforce coherency processor cache netdimm local memory  dst src function alg api memory buffer clone writes dst src netdimm register netdimm clone src dst buffer inside memory physical feasibility netdimm feasible integrate blown NIC buffer device DIMM thermal specification academic research proposal processing buffer device conventional DIMMs centaur DIMM  buffer DIMM ibm memory capacity processor  comprises ddr dram device centaur device consists MB cache memory controller logic TDP ibm centaur buffer device technology  intel pcie ethernet controller incorporate 0Gbps TDP therefore specification DIMM feasible integrate NIC chip buffer device DIMM lastly external cable DIMMs nvdimm moreover connector network cable netdimm evaluation methodology evaluate netdimm gem along analytical model pcie interconnect memory controller overhead linux kernel software stack fade latency improvement netdimm implement bare driver pcie NIC integrate NIC netdimm model gem resemble latency userspace driver latency evaluation configure gem model netdimm memory access latency instantiate isolated memory controller model nMC nMC model access netdimm memory zone memory request host netdimm queue host MC chosen dram instead perform regular memory access  delay host MC memory request correspond nMC memory request access nMC sends response host MC network dma operation memory access directly nMC model performance evaluation network trace facebook production cluster cluster packet traffic patter cluster database application packet uniformly distribute byte byte MTU byte cluster webserver packet byte cluster hadoop server packet byte byte traffic database cluster mostly inter cluster inter datacenter webserver mostly inter cluster intra datacenter hadoop intra cluster trace publicly available facebook randomly node cluster dummy node replay ingres egress data traffic node simulate  network topology facebook datacenter dist gem switch model assume network device datacenter bandwidth 0Gbps implement LF packet inspection dpi network function network function extremely packet processing behaviour evaluate impact netdimm performance server memory subsystem facebook trace network function LF packet header information dpi entire header payload decision configuration parameter core core freq 4GHz superscalar rob IQ LQ SQ entry int FP physical register predictor BTB entry  cache assoc KB KB MB LI L1D latency MSHRs cycle MSHRs dram ddr mhz 6GB channel network switch latency netdimm 0GbE pcie performance pcie micro october columbus usa  latency packet byte   reg acc   latency packet byte     reg acc   latency packet byte   reg acc   network latency breakdown packet various pcie NIC integrate NIC netdimm axis drawn network latency bandwidth network latency breakdown various packet node directly pcie NICs     respectively overhead memory allocation RX TX driver   dma overhead NIC hardware physical layer overhead reg acc overhead cpu NIC register access   cache flush cache invalidate overhead netdimm driver respectively netdimm reduces network latency packet pcie NIC translates network latency respectively eliminate pcie interconnect reg acc significantly reduce iNIC netdimm pcie NIC netdimm   overhead network latency component combine overhead network latency nonetheless average netdimm delivers latency iNIC across packet memory buffer clone overhead cpu cache operation improves overall network latency integrate NIC caveat netdimm unlike pcie NIC memory channel cannot utilize multiple memory channel communicate host processor memory however simulation netdimm delivers 0Gbps bandwidth pcie integrate NIC model nominal bandwidth ddr memory channel   0Gbps ddr memory channel project bandwidth twice ddr channel sustain bandwidth development pcie NICs deliver performance evaluation average per packet network latency cluster server netdimm normalize latency pcie NIC iNIC configuration latency network switch inside simulated  network performance sensitivity netdimm network configuration average across cluster netdimm improves packet latency pcie NIC configuration switch latency respectively netdimm improves average packet latency cluster employ iNIC switch configuration netdimm latency reduction highlight latency network switch fortunately latency network switch improve ultra latency network switch latency cluster webserver benefit netdimm packet byte netdimm effective transfer packet addition webserver traffic intra datacenter traverse lesser hop destination database traffic mostly inter  although hadoop traffic local cluster packet skewed MTU packet therefore netdimm latency reduction hadoop amongst cluster normalize memory access latency application dpi LF server netdimm normalize iNIC dpi decision packet payload processor fetch entire packet cache header payload iNIC directly brings packet llc consume memory channel bandwidth processor congest packet evict dram however LF packet header packet naturally nCache netdimm packet processing behaviour dpi LF packet processing spectrum application netdimm increase memory access dpi improves LF iNIC configuration average netdimm improves memory access latency database webserver hadoop cluster respectively related WORKS novel network architecture propose cache mechanism inside NIC reduce data communication pci channel NIC cache implement dram device manage operating although network architecture reduces pcie traffic incoming packet traverse pcie interconnect cpu furthermore netdimm latency memory network interface architecture micro october columbus usa normalize pcie normalize iNIC normalize latency database webserver hadoop normalize acc dpi LF gmean per packet normalize network latency network switch latency normalize memory access latency application packet inspection dpi LF server netdimm replay facebook cluster trace efficient software manage data cache challenge perform detailed source latency overhead network stack minimize pcie transaction latency NIC propose NIC architecture  reduce communication latency packet employ technique embed packet inside buffer descriptor custom polling creative cache policy  network dma interface reduces packet processing overhead enable NIC perform operation packet exchange memory offload optimization orthogonal netdimm apply netdimm improve network performance proposal decouples dma descriptor management NIC functionality processor aim reduce pcie transaction handle dma buffer efficiently introduce integrate dma minimize descriptor management overhead pcie transaction propose  integrates NIC processor  pio exchange data processor memory NIC although  effective reduce network latency furthermore suite bandwidth communication due lack dma capability NICs netdimm completely remove pcie link NIC processor blown NIC memory implement memory buffer clone overhead conventional network subsystem propose memory integrate NIC mini NIC memory dram module mini implement pseudo dual dram dram host NIC arbitration signal host NIC memory controller mini redesign dram memory controller interface architecture   plug NIC memory channel slot although pcie bottleneck NIC host address explicitly packet host memory channel packet transmission reception furthermore NICs memory channel netdimm seamlessly expose local memory address host minimizes data movement host NIC multichannel memory lastly netdimm processor architecture memory subsystem novel interconnection technology introduce memory channel network mcn concept generalpurpose mobile processor DIMM expose memory processor host processor ethernet interface memory channel interconnect remote node host processor netdimm concept NIC processor memory coherent accelerator processor interface  cache coherent interconnect accelerator  gen interconnect standard development mainly tightly couple processor accelerator gpus FPGAs  developed pcie specification pcie drawback combination ddr interconnection technology unprecedented bandwidth reduces data movement overhead directly access memory although   gen standard introduce emerge merge abandon future however ddr standard maintain developed decade standard interconnection technology memory moreover latency serial interconnects   gen cannot ddr memory channel conclusion decade focus network optimize bandwidth however emergence ultra latency datacenter application latency network unfolded building upon memory processing concept leverage asynchronous memory access nvdimm protocol evaluate  NIC architecture netdimm netdimm integrates blown NIC buffer device memory  capable DIMM developed logic device driver memory NIC available application host processor finally implement memory zone netdimm local memory developed linux kernel apis facilitate memory allocation memory zone memory allocation significantly reduce amount data movement processing network packet conventional pcie NIC netdimm improves network latency without compromise network bandwidth