garbage collection GC standard feature productivity program programmer   bug however productivity benefit application throughput latency consumption introduction GC lisp program myriad hardware software technique propose reduce accelerate GC hardware appeal impact limited due narrow coverage lack flexibility intrusive significant hardware specialized hardware GC performance eventually limited memory bandwidth bottleneck fortunately emerge 3D stack dram technology shed decade enable efficient memory processing ample memory bandwidth propose charon 3D stack memory GC accelerator detailed performance analysis hotspot jvm derive algorithmic primitive GC coverage implementation complexity hardware devise specialized processing substantially improve memory parallelism throughput hardware evaluation charon production hotspot jvm data analytics framework spark graphchi demonstrates geomean speedup saving GC baseline core processor CCS CONCEPTS computer organization heterogeneous hybrid purpose keywords garbage collection memory processing domain specific architecture memory management java virtual machine introduction garbage collection GC automatic memory management technique widely utilized program java javascript python GC programmer explicitly deallocate instead runtime garbage collector automatically identifies data cannot access future garbage automatically performs deallocation reclaim memory ability automatically deallocate memory GC naturally improves productivity programmer completely eliminate substantially reduce memory related bug memory leak dangle pointer etc unfortunately advantage GC noticeable performance GC prohibitively expensive data analytics manipulate scatter across memory multiple source report GC account execution memory intensive data analytics moreover latency sensitive application suffer GC induced latency significantly degrade quality service management distribute amount GC overhead closely related application available heap recent trend slowdown dram technology rapid increase dataset compute parallelism GC continuously increase future counter unavoidable increase GC overhead critical improve throughput GC unfortunately GC identify graph traversal migrate contiguous memory memory intensive workload involves ill operation purpose processor purpose micro october columbus usa processor achieve limited memory parallelism mlp due limited instruction load queue limited bandwidth chip memory become bottleneck average ipc intel xeon core garbage collection various data analytics workload cpu effective GC research focus exploit opportunity specialization memory computation overcome challenge GC purpose processor accelerate GC hardware explore introduction GC lisp program however limited impact various proposal target specific lisp smalltalk specific hardware fpga specialized memory algorithm reference counting narrow coverage others attempt implement GC fully hardware lack flexibility GC accelerator relatively sweep algorithm hence cannot fully offload popular generational GC copying collector  generation offload invasive processor incur hardware verification analysis primitive dominate GC argue offload primitive entire GC approach future proof primitive likely  continuously evolve GC algorithm furthermore GC algorithm bottleneck memory bandwidth therefore without address bandwidth building specialized hardware fortunately emerge 3D stack dram technology HBM HMC uncover intrigue opportunity decade enable efficient processing memory logic ample memory bandwidth architect evaluate charon memory accelerator specialized specific operation GC unlock massive memory parallelism specialized hardware exploit abundant memory bandwidth available memory logic layer charon accelerates primitive GC derive hotspot jvm popular production jvm enables purpose processor offload GC efficiently minimal processor summary contribution perform detailed analysis GC behavior data processing framework production grade hotspot jvm identify algorithmic primitive GC specialized hardware processing substantially improve  parallelism throughput primitive prototype charon hotspot jvm demonstrate effectiveness production grade GC algorithm evaluate charon data analytics framework spark graphchi detailed cycle simulator execute billion instruction demonstrate charon achieves speedup saving baseline core processor eden survivor survivor generation generation reference traverse copying copying promotion compaction overview  GC hotspot background garbage collection algorithm garbage heap allocate future reachable chain pointer consists accessible outside heap stack variable pointer global variable garbage collection GC reclaim garbage longer program handle manually programmer java GC automatically handle exist GC algorithm goal java parallel compact collector throughput orient collector focus GC throughput collector incurs application thread mutator thread GC core devote resource GC thread contrast concurrent garbage collector concurrent sweep CMS hotspot jvm application thread parallel GC thread concurrent garbage collector typically incurs pause throughput orient garbage collector however concurrent garbage collector incurs overhead due synchronization maintain consistent memory application thread interference hardware resource concurrent thread GC application achieves overall throughput throughput orient generational garbage collection generational GC standard algorithm multiple generation advantage weak generational hypothesis assumes heap lifetime exist relatively remain typically generational collector heap generation minimize garbage collection operation entire heap perform lightweight garbage collection relatively generation lifetime operation  popular generational throughput orient collector hotspot jvm allocate eden reserve eden fails allocate minor GC MinorGC trigger collector traverse graph consist specifically MinorGC traverse eden charon specialized memory processing architecture clearing memory micro october columbus usa BS KM LR CC PR ALS geomean spark graphchi GC mutator     GC overhead normalize mutator useful data processing workload heap MajorGC pop pop stack destination bitmap compact compact gen scan traverse reachable MinorGC marked scan traverse reachable promote scan pop pop stack marked simplify execution GC survivor empty survivor promotes generation survive  generation MinorGC eden survivor designates vice versa eventually generation promote trigger GC MajorGC MajorGC traverse MinorGC trigger compaction reduce heap fragmentation analysis GC inefficiency GC overhead data application data processing application heap suffer noticeable performance due GC unlike traditional java application data processing application exhaust heap due massive amount volume data manipulation shuffle cannot reclaim operation completely traverse GC physical memory data processing application traverse becomes bottleneck cpu cycle GC useful computation performance impact GC actual computation intel processor minimum heap enables application without memory  error insufficient heap  heap GC overhead across memory provision substantial  MinorGC breakdown MajorGC breakdown ALS PR CC LR KM BS graphchi spark bitmap scan others ALS PR CC LR KM BS graphchi spark scan others runtime breakdown GC allocate memory actually GC slows application overhead easily  memory approach towards minimum actually heap spent GC exceed actual application runtime mutator runtime furthermore trend rapidly increase dataset compute parallelism likely increase minimum memory faster increase physical memory indicates GC likely portion runtime future GC execution breakdown runtime breakdown MinorGC MajorGC algorithm enable identify operation account portion GC core intel processor ghz profile multithreaded portion parallel  GC collector  openjdk hotspot jvm minor garbage collection MinorGC simplify operation MinorGC MinorGC collector local stack variable global variable etc stack traverse reside collector pop stack pop already collector survivor promotes generation finally collector reference scan reference stack stack completely drain complication reside generation reference generation default MinorGC graph tracked metadata hotspot jvm addition mention micro october columbus usa scan MinorGC reference generation stack MinorGC runtime breakdown spent operation MinorGC spark  application GC execution operation scan operation account MinorGC spark graphchi respectively specifically spark account MinorGC similarly GC graphchi spends scan MinorGC data analytics application manipulate data chunk partition RDDs spark shard graphchi application relatively spent traversal spent garbage collection MajorGC simplify MajorGC operation MajorGC hotspot jvm mainly consists distinct phase compaction MinorGC MajorGC stack phase collector pop stack pop already collector scan reference scan MinorGC operation compact phase collector scatter across heap sequential contiguous memory collector calculate destination location basically sum currently heap linear computation bitmap data structure bitmap bitmap utilized hotspot jvm bitmap structure heap bitmap MB entire 6GB heap bitmap indicates correspond heap address similarly bitmap indicates correspond heap address counting distance obtain utilize structure collector identifies destination address address compaction heap densely packed empty MajorGC runtime breakdown application spends MajorGC operation spark graphchi application GC execution operation MinorGC workload significant portion MajorGC operation account technically summary phase compaction phase phase MajorGC target accelerator MajorGC spark graphchi respectively spark graphchi demonstrate application behavior scan portion graphchi spark tends allocate memory reference graphchi allocates reference spark spends copying graphchi spends primitive scan bitmap ALS graphchi exception algorithm matrix data GC primitive offload runtime breakdown suggests GC dominate handful primitive closer primitive memory processing perspective primitive scan dominate MinorGC hotspot jvm similarly overlap primitive scan bitmap account MajorGC primitive perform memory operation without computation unfortunately purpose processor workload limited mlp due limited instruction load queue scan bitmap primitive traverse graph sequentially limited parallelism GC algorithm utilize multithreading improve throughput purpose processor achieve mlp performance limited chip memory bandwidth primitive GC temporal spatial locality primitive memory without temporal reuse scan primitive traverse reference stack involves indirect memory access locality fetch cache remain unused data access data locality cache hierarchy purpose processor cannot effectively alleviate memory latency bandwidth bottleneck motivates offload primitive  logic layer 3D stack dram stack memory uncovers intrigue opportunity GC offload abundant memory bandwidth fully utilized specialized processing maximize mlp minimize operation latency however operation benefit offload offload operation GC traverse link relatively benefit limited parallelism latency bound characteristic link traversal operation allocate MajorGC essentially atomic instruction potential benefit offload outweigh overhead due offload granularity exclude operation propose offload primitive MinorGC scan MajorGC bitmap scan charon specialized memory processing architecture clearing memory micro october columbus usa return packet host offload request packet host overview propose architecture dram ret queue cmd queue bitmap scan bitmap cache memory access interface mai ID request metadata charon cmd queue diagram charon HMC logic layer tlb cmd queue cmd queue host processor serial link HMC dram layer HMC HMC logic layer HMC charon overview offset load request request response request generator src request generator begMap load request bitmap cache   endMap unmarked update metadata stack GC bitmap cache request generator obj ptr load request request response iterator bitmap scan minor GC false hardware diagram processing description primitive optimize specialized processing implement memory processing logic layer charon architecture overview overview charon architecture assume hybrid memory cube HMC baseline platform internal bandwidth efficiency although non HMC platform flexibly charon elsewhere memory controller buffer setup host processor directly HMC cube HMC cube topology cube communicate without host HMC cube topology multiple HMC cube around central cube throughout architecture necessarily specific topology HMC cube logic layer beneath stack DRAMs implement charon logic layer cube  hotspot jvm primitive employ accelerate GC algorithm host charon interface charon intrinsics host interface initialize program launch constant address globally access data structure address heap bitmap configuration memory mapped register processing generates offload request val offload val addr src addr dst val arg invoked HMC controller generates packet destination cube address request offload request packet consists standard HMC header destination cube offload primitive address extra operand offload request packet destination cube exist inter HMC rout logic buffer command queue appropriate per primitive command queue unless processing offload request packet available processing execution execution offload primitive charon host thread remains offload primitive return packet return packet response contains return memory access processing processing logic layer access local remote stack processing access memory address memory location memory access interface mai cube processing resides along optional request metadata processing buffer mai response return mai empty request buffer request metadata mai issue access request request tag index request buffer memory rout appropriate destination cube request address request response packet handle mai retrieves request metadata response packet along request metadata requester role mai MSHR status handle register host core memory access mai virtual physical address translation via accelerator tlb detailed micro october columbus usa void  src  dst int dst src bool   return return false pseudocode primitive processing clflush host cache hierarchy writes avoid stale data cache retrieve stale data memory however memory access trigger host cache probe clflush execute bitmap memory access bitmap update host GC code primitive pseudocode primitive processing primitive heap another specifically MinorGC primitive eden survivor survivor generation MajorGC primitive compact addition primitive perform operation MinorGC existence described within specify receives address source destination integer address host processor offloads primitive schedule cube source address exploit abundant internal bandwidth HMC implementation optimization diagram achieve performance crucial maximize mlp flight memory request primitive embarrassingly parallel memory operation execute parallel exploit abundant parallelism request granularity maximum granularity HMC cycle receives offload command packet host mai accept request trigger load response return issue request performs comparison increase mlp increase throughput due limited memory bandwidth conventional memory contrast charon exploit internal bandwidth stack dram achieve speedup bitmap primitive primitive heavily compact phase MajorGC compaction collector relocates location bitmap primitive int    begMap  endMap offset int num int    num begMap      num endMap        num  num  return pseudocode bitmap primitive begMap endMap begMap bitmap propose algorithm  begMap endMap  begMap endMap begMap endMap bitmap primitive location sum within memory hotspot jvm primitive bitmap begMap endMap begMap endMap location respectively begMap endMap byte primitive computes sum occupy within specify receives address address begMap correspond address endMap derive constant offset constant configure program launch static primitive schedule cube bitmap address exploit abundant internal bandwidth HMC primitive implementation optimization software version simply iterates begMap endMap granularity charon optimizes primitive implementation modify algorithm explain efficient besides processing identifies amount data issue memory request charon specialized memory processing architecture clearing memory micro october columbus usa header header header stack pop scan reference reference non reference reference iterate traversal processing improves performance employ bitmap cache detailed access bitmap frequently ample temporal locality accord evaluation bitmap cache rate hide memory latency significantly reduces latency primitive optimize algorithm processing utilizes optimize algorithm expression  begMap endMap  begMap subtracts endMap begMap interpret binary binary  begMap endMap begMap previous obtain outcome illustrates assume begMap endMap intuitively endMap begMap yield begMap endMap begMap counting outcome outcome algorithm begMap account compensate  begMap implementation handle begMap endMap description handle omit due limited scan primitive primitive performs graph traversal operation GC primitive scan non static reference reachable stack graph recursively collector identifies heap primitive utilized MinorGC MajorGC MinorGC content hotspot jvm MinorGC collector pop stack minor stack collector survivor iterates inside scan reference load traverse marked unmarked minor stack later otherwise update metadata  skip collector recursively minor stack empty distinct iterate strategy void iterate  reference objptr reference MinorGC content objptr MajorGC content objptr void content objptr obj objptr unmarked obj minor stack obj update metadata obj void content objptr obj objptr unmarked obj obj obj atomic modify stack obj pseudocode scan primitive eventually marked survivor MajorGC content hotspot jvm phase collector pop stack stack traverse scan reachable marked unmarked correspond bitmap obj newly marked stack collector recursively stack totally drain eventually marked metadata hotspot jvm   etc distinct metadata layout scan inside iteration strategy simplicity focus handle dominant data host processor correspond address metadata chooses iteration strategy charon schedule primitive central cube HMC random data access reading content reference input processing central location minimizes overall delay bandwidth usage implementation optimization iterator generates sequential load request reference inside layout memory load request reference arrives performs memory operation arrival request indirect memory access sequence performance conventional cpu limited instruction specifically dependent instruction initial load easily clog instruction cache core stall cache contrast charon amortizes latency initial load exploit mlp processing micro october columbus usa memory load request fetch reference address generates batch memory load request cycle response performs appropriate action response perform minor stack obj update metadata obj MinorGC unmarked obj MajorGC finally MajorGC response unmarked obj performs obj obj stack obj bitmap cache reading specify bitmap memory essential bitmap benefit cache entire bitmap bitmap loop multiple bitmap scan iteration perform generation bitmap access bitmap frequently MajorGC demonstrate temporal locality specify bitmap typically overlap previous cache besides obj operation benefit bitmap cache performs atomic  rmw bitmap although processing bitmap fetch minimum memory access granularity  without cache writeback cache KB dedicate bitmap access bitmap scan bitmap happens compact phase MajorGC scan happens phase MajorGC access bitmap cache simultaneously flush cache primitive MajorGC coherence issue discussion applicability analysis throughput orient GC  hotspot jvm however primitive fundamental operation GC commonly utilized collector applicability charon primitive popular collector hotspot jvm primitive scan operation GC algorithm applicable latency orient concurrent sweep CMS GC latency throughput optimize garbage GC processing bitmap GC scheme slight modification code scan bitmap identify entire heap charon primitive readily applicable concurrent GCs additional challenge maintain consistent memory  collector sequence code barrier jvm terminology execute mutator incurs significant runtime overhead  orthogonal charon issue carefully address efficiency scan bitmap remark  throughput latency CMS compaction applicable applicable minor fix applicable applicability charon primitive programmer effort offload charon primitive minimal programmer effort advantage offload modification hotspot jvm code replace primitive charon intrinsic plus initialization easy verify deploy charon various GC algorithm virtual memory multi processing charon efficient mechanism virtual physic address translation purpose utilize numa memory lock mainstream intel amd architecture application launch jvm allocates 1GB entire heap pin  facilitate hotspot jvm configurable option XX  XX  interleave cube numa alloc  originally allocate memory specific numa node remain memory code  KB conventional demand nearly api function memory allocation already introduce linux numa charon leverage virtual memory efficient address translation protection maintain duplicate tlb entry dram pin pre allocate heap within physical memory oversubscription manage runtime jvm otherwise significantly increase demand GC pin maintain throughout execution program tlb fault multiple jvm charon standard protection mechanism virtual memory mainstream architecture already distinct identifier PCID tlb straightforward extension charon currently oversubscription physical memory attempt pin fail beyond capacity physical memory effectively serf admission mechanism charon capacity 3D stack dram evaluate propose architecture processor  prevents apply propose architecture multi processor multi stack dram memory capacity desire recent demonstrate HMC stack dram terabyte capacity interconnect multiple processor dram module latency penalty bandwidth contention effectively alleviate efficient topology data placement data application scalable technology charon specialized memory processing architecture clearing memory micro october columbus usa HBM memory capacity utilize conventional DDRx DIMM backing storage charon integrate cpu accelerator memory controller demonstrates effectiveness charon cpu accelerator scalability charon increase HMC naturally allows processing GC throughput increase memory increase potential bottleneck charon structure bitmap cache TLBs currently charon employ cube topology utilize bitmap cache cube however structure employ proposal owner cache cube cache slice local data similarly tlb slice cube cache mapping associate local memory request cube virtual address VA OS VA specific cube numa alloc  demonstrate scalability charon increase primitive host cache flush cache GC memory processing obtain data memory host cache technically degradation application thread performance GC however target workload amount memory GC substantially exceeds cache unlikely useful cache application thread remain cache GC bulk flush fully utilize HMC bandwidth incur relatively overhead flush MB llc 0GB sec HMC bandwidth average GC duration exceeds millisecond evaluation methodology evaluation model extend zsim model performance impact proposal configuration host processor HMC memory zsim memory channel interleave col rank ddr cube col rank vault HMC 1GB effectively evaluation host processor integrate McPAT zsim evaluation charon hardware structure implement structure chisel functionally verify realistic input synthesize synopsys compiler TSMC standard library lastly CACTI technology estimate buffer queue structure command queue bitmap cache etc charon workload spark graphchi hotspot jvm openjdk propose framework carefully application framework characteristic hence GC behavior specifically machine workload spark naive bayes cluster logistic regression graph algorithm machine host processor core ghz  OoO core entry IW entry rob issue tlb LI entry per core entry LI cache KB cycle KB cycle cache KB cycle cache MB cycle ddr memory organization 2GB channel rank per channel 4GB per rank timing tck tRAS tRCD tcas twr trp bandwidth 4GB 7GB per channel HMC memory organization 2GB cube vault per cube timing tck tRAS tRCD tcas twr trp bandwidth 0GB per cube serial link 0GB per link latency charon configuration per cube bitmap per cube scan cube bitmap cache KB mai tlb KB entry per cube architectural parameter evaluation algorithm workload graphchi component pagerank alternate evaluation specifically focus charon improves GC performance consumption roi GC default hotspot jvm heap policy max heap application minimum heap application reliably without encounter memory  error summarizes workload correspond input dataset heap performance overall speedup throughput GC across platform host processor conventional ddr memory ddr host processor hybrid memory cube HMC host processor charon logic layer hybrid memory cube charon imaginary ideal scenario host processor ideal offload device execute offload primitive zero cycle ideal host processor achieve speedup simply replace ddr memory HMC memory chip memory bandwidth however host processor fully exploit benefit HMC HMC memory bandwidth host limited memory parallelism despite core cannot fully utilize available bandwidth although HMC memory chip bandwidth serial link host processor alone cannot utilize abundant internal bandwidth bandwidth micro october columbus usa workload input heap spark bayesian classifier BS kdd 0GB cluster KM kdd 8GB logistic regression LR url reputation 2GB graphchi component CC mat 4GB pagerank PR mat 4GB alternate ALS matrix format 4GB workload BS KM LR CC PR ALS spark graphchi geomean normalize speedup ddr HMC charon ideal normalize GC performance charon host cpu execution logic layer dram stack HMC charon overcomes limitation fully exploit benefit achieve average speedup host ddr dram average speedup host HMC dram charon ideal scenario performance charon indeed handle offload primitive efficient charon speedup varies across workload GCs portion offload primitive substantially workload demonstrate charon benefit ALS primitive account portion runtime charon benefit bandwidth analysis graph bandwidth usage GC across platform without charon host access memory utilize offchip link conventional ddr memory HMC memory charon processing logic layer utilize highbandwidth tsv silicon via access data memory internal bandwidth addition charon processing access remote cube serial link cube traffic exploit internal bandwidth tsv link access chip bandwidth host hybrid memory cube beneficial charon effectively utilizes bandwidth available chip memory bandwidth 0GB HMC baseline memory request service locally reserve sizable headroom external link bandwidth LR CC exception memory request remote however bandwidth intensive application hence alleviate BS KM LR CC PR ALS spark graphchi average local access ratio bandwidth GB ddr HMC charon ratio local access utilized bandwidth GC ratio local access per primitive analysis average speedup cpu ddr memory charon achieve primitive charon achieves maximum speedup average MinorGC MajorGC average primitive exploit abundant internal bandwidth HMC maximize memory parallelism granularity memory access addition primitive scan primitive achieves maximum speedup average additional memory parallelism speedup bitmap primitive combination novel algorithm specialized hardware primitive charon improves throughput bitmap primitive average ddr memory charon offloads minimal relatively primitive avoid invasive software hardware charon handle GC however handful offload primitive charon achieves substantial speedup scan primitive relatively speedup degrades application BS KM LR ALS application machine algorithm allocate reference within matrix lifetime usually discard reference amount parallelism charon achieves throughput however charon obtains benefit primitive application application manipulate generate memory access benefit excessive mlp charon modest speedup CC PR explainable manner CC PR graph algorithm traverse node reference cycle reference utilize benefit sufficient mlp scan primitive GC scalability multiple  chain capacity charon primitive logic layer evaluate GC scalability correspond charon primitive increase GC thread scalability unified bitmap cache tlb cube cube distribute slice bitmap cache tlb distribute across cube discus charon specialized memory processing architecture clearing memory micro october columbus usa SP SP SP SP SP SP SP SP SP SP SP SP MinorGC MajorGC MinorGC MajorGC MinorGC MajorGC MinorGC MajorGC MinorGC MajorGC MinorGC MajorGC BS KM LR CC PR ALS normalize speedup ddr charon per primitive speedup analysis SP scan BC bitmap MinorGC spark BS MinorGC graphchi CC normalize speedup GC thread GC thread MajorGC spark BS MajorGC graphchi CC normalize speedup GC thread ddr charon unified charon distribute GC thread GC throughput scalability performance scalability increase GC thread charon significantly ddr utilize plenty internal bandwidth ddr hardly due limited memory bandwidth max 4GB charon distribute structure generally unified contention cube alleviate MajorGC graphchi CC exception memory pressure relatively contention issue penalize distribute remote tlb access finally performance scalability improve adopt bandwidth scalable HMC topology  interleave policy reduce remote traffic charon cpu accelerator charon primitive flexible utilized HMC charon HBM cpu accelerator throughput configuration cpu ddr memory charon cpu HMC memory charon logic HMC memory charon cpu achieves performance baseline processor aggressive mlp optimize bitmap operation however charon  abundant internal dram bandwidth HMC logic layer throughput charon memory accelerator bandwidth bottleneck partly alleviate bandwidth dram technology HBM charon dram BS KM LR CC PR ALS spark graphchi geomean normalize speedup cpu ddr charon cpu HMC charon dram HMC memory implementation speedup  chip implementation advantage efficiency scalability logic bandwidth contention memory controller thermal analysis normalize consumption charon GCs host execution charon substantial improvement throughput increase moderate consumption substantial reduction consumption charon achieves reduction host ddr memory average reduction host HMC baseline across workload consumption component queue metadata array tlb bitmap cache negligible consumption charon maximum ALS usage component charon charon average per cube around assume hybrid memory cube logic layer charon logic layer charon HMC logic layer incurs overhead regard thermal issue primitive charon computation memory access impact thermal constraint negligible evaluation indicates average consumption workload maximum ALS previous proposal computation logic HMC stack therefore maximum density logic maximum allowable density passive sink related GC optimization accelerator accelerate GC specialized hardware propose decade overcome GC overhead conventional processor micro october columbus usa ddr HMC charon ddr HMC charon ddr HMC charon ddr HMC charon ddr HMC charon ddr HMC charon BS KM LR CC PR ALS spark graphchi normalize consumption cpu dram component charon consumption charon GC host cpu execution component per component component command queue request queue request queue metadata array bitmap cache tlb processing bitmap scan average per cube usage charon cube active memory processor amp integrates  processor standard 2D dram array improve performance predictability dynamic memory management function allocation reference counting GC propose hardware GC accelerator reference counting algorithm hardware collector  conventional GC algorithm complement limitation reference counting GC cyclic dependency however accelerator tightly couple host processor invasive host processor charon propose hardware GC technique fully offloads sweep GC  chip accelerator memory utilizes abundant mlp specialized hardware GC GC algorithm limited applicability cannot fully offload generational GC widely production hotspot copying collector  generation charon demonstrate tradeoff specialization flexibility primitive approach likely efficiency loss  rvm target broader applicability moreover charon proposal leverage 3D stack memory internal memory bandwidth address issue dram offload memory processing 3D stack dram introduction 3D stack dram technology prior research propose various memory processing architecture technique focus hardware architecture accelerate variety application graph processing vector operation mapreduce computation aim leverage ample internal dram bandwidth massive parallelism reduce data movement overhead conventional cpu memory hierarchy goal utilize memory parallelism algorithmic primitive efficient hardware concrete proposal offload GC stack dram GC optimization emerge application optimize GC emerge application data analytics memory footprint distribute environment facade yak structure heap optimize GC algorithm  behavior data analytics framework utilize data  distribute improve locality cache coherent numa machine ensure GC thread memory  advocate holistic approach coordinate GCs  runtimes improve performance propose bias reference counting algorithm BRC reduces execution non defer reference counting RC RC operation partially non atomically achieve latency RC proposal complement software optimization grain offload primitive potentially proposal conclusion charon novel memory accelerator offload GC although adoption hardware GC scarce due various limitation finally arrival 3D stack memory various GC algorithm perform performance analysis  hotspot jvm detail analysis identify algorithmic primitive memory bitmap scan commonly trace GC algorithm specialized processing logic layer 3D stack memory executes primitive efficiently evaluation production hotspot jvm data analytics framework apache spark graphchi demonstrates substantial performance efficiency gain purpose cpu without intrusive software