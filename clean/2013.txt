recent  pinpoint embed layer memory intensive DL algorithm deployed datacenters address memory capacity bandwidth challenge embed layer associate tensor operation vertically integrate hardware software custom DIMM module enhance memory processing core tailor DL tensor operation custom DIMMs inside gpu centric interconnect remote memory pool gpus utilize scalable memory bandwidth capacity expansion prototype implementation proposal DL average performance improvement dnn recommender CCS CONCEPTS computer organization parallel architecture keywords architecture memory architecture memory processing DIMM machine neural network graphic processing gpu neural processing npu introduction machine ML algorithm neural network dnns DL rapidly satisfy computation DL practitioner gpus  accelerator dnns neural processing NPUs widely deployed accelerate DL workload despite prior enhance compute throughput gpus NPUs research theme topological structure emerge DL application reproduce facebook keynote compute project summit architectural memory bandwidth capacity embeddings specifically address important memory emerge DL application non mlp portion relatively attention computer architect tackle memory DL emerge DL algorithm demand memory capacity bandwidth limit application deployed practical constraint recent  pinpoint embed lookup tensor manipulation aka embed layer memory intensive algorithm deployed datacenters already GBs memory footprint inference wisdom conventional DL workload convolutional recurrent neural network cnns rnns convolution matrix multiplication account majority inference however emerge DL application employ embed layer exhibit drastically characteristic embed lookup tensor manipulation feature interaction account significant execution facebook instance embed layer execution DL workload deployed datacenters landscape focus address memory capacity bandwidth challenge embed layer specifically focus attention recommender embeddings DL workload deployed datacenters numerous application domain advertisement movie recommendation news etc detailed model embed layer typically GBs micro october columbus usa  kwon  lee   exceeds memory capacity gpus vendor entire embed lookup inside capacity optimize bandwidth cpu memory deploy application CPUs entire computation dnn inference employ hybrid cpu gpu approach embed lookup conduct cpu handle gpu approach significant performance average slowdown hypothetical gpu version assumes entire embeddings gpu memory detailed application characterization performance loss factor embed vector bandwidth cpu memory incur significant latency overhead embeddings bandwidth optimize gpu memory computation throughput CPUs significantly slowdown computation dnn execution solely rely CPUs whereas hybrid cpu gpu version suffer latency copying embeddings cpu gpu memory pcie channel tackle challenge vertically integrate hardware software fundamentally address memory capacity bandwidth embed layer proposal encompasses multiple hardware software stack detailed micro architecture TensorDIMM commodity buffer DIMMs enhance  processing NMP customize DL tensor operation embed reduction NMP TensorDIMM conduct embed reduction operation memory drastically reduces latency fetch embed vector reduce significant improvement effective communication bandwidth performance additionally TensorDIMM leverage commodity dram device another advantage proposal oppose prior memory architecture practicality implementation ISA extension runtime building TensorDIMM architecture propose custom tensor ISA runtime scalable memory bandwidth capacity expansion embed layer proposal entail carefully ISA tailor DL tensor operation TensorISA efficient address mapping scheme embeddings runtime effectively utilizes TensorDIMMs tensor operation TensorISA conduct DL tensor operation memory bandwidth efficient manner important challenge conventional memory regardless DIMMs rank physically available per memory channel maximum memory bandwidth memory controller fix TensorISA carefully  TensorDIMM address mapping scheme aggregate memory bandwidth NMP increase proportional TensorDIMMs proposal platform scalable memory bandwidth expansion embed layer baseline default TensorDIMM configuration average increase memory bandwidth DL tensor operation architecture proposition aggregate pool TensorDIMM module disaggregated memory node henceforth refer TensorNode scalable memory capacity expansion proposal TensorNode interfaced inside NVLINK compatible gpu bandwidth interconnect DL gpus bandwidth switch NVSwitch allows bandwidth latency data transfer gpus proposal utilizes pool memory architecture inside bandwidth gpu interconnect fully capacity optimize memory DIMMs TensorDIMMs benefit interfacing TensorNode within gpu interconnect embed lookup inside TensorNode gpus embeddings faster conventional cpu gpu approach approximately faster pcie assume NVLINK furthermore couple memory bandwidth amplification TensorISA TensorNode scalable expansion memory capacity bandwidth overall vertically integrate average speedup inference cpu hybrid cpu gpu implementation recommender respectively summarize contribution computer architecture community primarily focus accelerate computationally intensive dense dnn layer cnns rnns MLPs knowledge identify analyze explore architectural sparse embed layer highly important building significant industrial importance emerge DL application propose TensorDIMM practical memory processing architecture built commodity DRAMs scalable increase memory capacity bandwidth embed tensor operation TensorNode TensorDIMM disaggregated memory DL efficiency demonstrate proof concept software prototype gpu achieve significant performance improvement conventional approach background buffer dram module balance memory capacity bandwidth commodity dram device utilized unison compose rank rank package memory module popular factor dual inline memory module DIMM data DQ pin memory channel typically multiple DIMMs cpu memory controller dram device deliver command address signal memory channel demonstrate TensorDIMM merit context bandwidth gpu interconnect effectiveness proposal remains intact NPUs facebook  interconnect TensorDIMM memory processing architecture sparse embed layer micro october columbus usa DRAMs ghz dram device driven handful memory controller signal integrity issue consequently server DIMMs typically employ buffer device per DIMM register DIMM load reduce DIMM signal reduce capacitive load resolve signal integrity issue prior academia explore possibility utilize buffer device custom logic address specific application ibm centaur DIMM instance utilizes buffer device MB eDRAM cache custom interface ddr phy ibm proprietary memory interface architecture DL complexity DL application  trend towards dense node multiple pcie attach processor device DL accelerator gpus NPUs address growth multi accelerator device typically parallel occasional inter device communication intermediate data inter device communication critical parallelize DL application vendor employ bandwidth interconnection fabric utilize custom bandwidth signal link nvidia DGX facebook  interconnect fabric nvidia DGX instance contains gpus interconnect NVLINK compatible radix crossbar switch NVSwitch NVLINK GB sec duplex uni directional bandwidth per link gpu within DGX communicate gpu uni directional bandwidth GB sec via NVSwitch uni directional bandwidth GB sec cpu gpu pcie bus bandwidth gpu interconnect enables magnitude faster data transfer DL application embeddings dnn recommender conventional DL application inference cnns rnns generally overall memory footprint within GBs gpu npu physical memory however recent  imminent systemlevel challenge emerge DL workload extremely memory capacity bandwidth limited specifically  pinpoint embed layer memory intensive algorithm deployed datacenters widely deployed DL application embeddings recommender numerous application domain advertisement amazon google ebay social networking service facebook instagram movie image recommendation youtube spotify fox pinterest news linkedin others recommendation typically formulate predict probability probability facebook user click ML model estimate likelihood item probability ranked recommend user user user user user  item embed lookup user item item item item  batch batch batch batch batch batch batch batch embeddings batch embed lookup instruction tensor OP reduce batch batch batch batch batch batch batch batch tensor manipulation reduce average instruction embeddings reduce embeddings concatenate dnn computation dnns gender gender dnn recommender embed layer typically consists embed lookup stage embeddings potentially multiple batch batch embed tensor tensor tensor manipulation operation embed tensor fed dnns proposal utilizes custom tensor ISA extension reduce average accelerate detail without comprehensive review numerous prior literature recommendation model emphasize recommender evolve utilize surprisingly dnns exists variation regard dnns construct commonly employ topological structure dnn recommender neural network collaborative filter algorithm advance development complex model wider deeper vector dimension successfully apply deployed commercial user embed lookup tensor manipulation illustrates usage embeddings recommender incorporate dnns input dnns typically construct fully layer multi layer perceptrons FCs MLPs construct combination dense sparse feature dense feature commonly vector whereas sparse feature index encode vector index query embed lookup project sparse index dense vector dimension content lookup embeddings extract feature facebook embeddings extract information regard user utilized recommend relevant content user embeddings lookup combine dense embeddings feature interaction lookup tensor concatenation tensor reduction wise addition multiplication average etc generate output tensor dnns derive probability memory capacity limit embed layer embed layer consume significant memory capacity user item unique embed vector micro october columbus usa  kwon  lee   inside lookup embeddings therefore proportional user item render overall memory footprint exceed GBs model inference despite memory requirement embed layer DL application improve model quality memory capacity user seek increase model embed layer embed dimension multiple embed combine multiple dense feature tensor reduction operation motivation memory capacity challenge gpus NPUs commonly employ bandwidth optimize package 3D stack memory HBM HMC deliver memory bandwidth chip compute capacity optimize DDRx commonly adopt cpu server bandwidth optimize stack memory capacity limited available GBs storage future benefit memory density technological constraint challenge increase capacity 3D stack memory scalable manner stack dram layer vertically constrain chip  dram stack  silicon  thermal constraint generation gpus NPUs already reticle limit processor 3D stack module within package inevitably sacrifice budget compute memory limit recommender model embed lookup GBs exceed memory capacity limit gpus NPUs due memory limit package stack DRAMs vendor embed lookup cpu embeddings capacity optimize  cpu memory implementation beyond cpu version cpu inference cpu without rely upon gpu hybrid cpu gpu approach cpu gpu embeddings gpu memory pcie cudaMemcpy cpu gpu data transfer gpu initiate various tensor manipulation operation input tensor dnn actual dnn computation landscape DL practitioner designer conundrum deploy recommender inference DL algorithm developer perspective seek embeddings embed lookup combine various embed vector tensor nvidia instance already reticle limit researcher explore alternative option multi chip module computational embed dimension model GB mlp dimension model growth neural collaborative filter NCF recommender mlp layer dimension axis embed vector dimension axis assumes embed lookup contains user item per lookup embeddings mlp dimension dramatic increase model manipulation tensor reduction increase embed dimension embeddings complex feature interaction improves model quality unfortunately fulfil algorithm developer bloat overall memory usage inevitably resort capacity optimize cpu memory embed lookup detailed characterization factor limiter prior approach rely cpu memory embeddings embed lookup bandwidth cpu memory reading embeddings embed operation significant latency  oracular gpu version gpu assumes infinite gpu memory capacity gpu entire embeddings locally bandwidth gpu memory gathering embeddings faster cpu memory gathering embeddings memory bandwidth limited operation cpu version sidestep latency hybrid cpu gpu version pcie communication transfer embeddings relatively computation throughput CPUs significantly lengthen dnn computation hybrid cpu gpu version reduce dnn computation latency additional cpu gpu communication latency copying embeddings pcie cudaMemcpy goal scalable memory overall challenge source operand tensor manipulation embeddings tensor concatenation tensor reduction inside embed lookup inside  bandwidth limited cpu memory consequently prior TensorDIMM memory processing architecture sparse embed layer micro october columbus usa NCF youtube fox facebook average normalize performance cpu cpu gpu gpu performance baseline cpu hybrid cpu gpu version recommender normalize oracular gpu version inference batch cpu exhibit performance advantage cpu gpu version batch inference scenario cpu cpu gpu suffers significant performance loss oracular gpu detail evaluation methodology suffer bandwidth embed operation cpu memory significant latency furthermore cpu cpu gpu version computational bottleneck throughput CPUs communication bottleneck pcie additional latency overhead future projection DL application utilize embed layer assume embed lookup embeddings complex tensor manipulation combine embed feature improve algorithmic performance overall future memory requirement embed layer urgent scalable memory capacity bandwidth expansion detail propose address computational bottleneck throughput CPUs communication bottleneck bandwidth cpu gpu data transfer  NMP DIMM embeddings tensor ops propose approach proposal observation opportunity tackle bottleneck deploy memory limited recommender important tensor operation combine embed feature wise operation equivalent tensor reduction embeddings embeddings individually gpu memory reduction operation initiate gpu conduct embed reduction memory reduce tensor gpu memory memory processing NMP approach reduces latency embeddings data transfer factor alleviates communication bottleneck cpu gpu cudaMemcpy DL vendor deploy gpu npu centric interconnection fabric decouple legacy host device pcie technology allows vendor employ custom bandwidth link NVLINK embed cpu gpu pcie embed cpu gpu pcie embed cpu gpu pcie reduce embed reduction conduct gpu embed locally within TensorDIMM reduce embed locally within TensorDIMM reduce embed locally within TensorDIMM reduce embed TensorNode gpu NVLINK hybrid cpu gpu approach TensorDIMM TensorNode embed lookup  sizeof embed  embed transfer  sizeof embed  embed lookup  sizeof embed  embed transfer  sizeof embed  propose approach hybrid cpu gpu version conduct embed tensor reduction locally memory transfer reduce tensor bandwidth NVLINK consequently proposal significantly reduces latency gathering embeddings lookup  transfer embeddings  cpu TensorNode memory bandwidth optimize gpu memory cpu version suffers significantly longer dnn execution cpu gpu propose bandwidth pcie inter gpu npu communication importantly tightly integrate non accelerator component disaggregated memory pool interconnect endpoint node accelerator non compute node bandwidth link observation propose TensorDIMM custom DIMM NMP core tailor tensor tensor reduction operation propose disaggregated memory TensorNode fully TensorDIMMs proposition software architecture effectively parallelizes tensor reduction operation across TensorDIMMs scalable memory bandwidth capacity expansion significant performance limiter conventional cpu hybrid cpu gpu recommender embed operation bandwidth cpu memory compute limited dnn execution CPUs cpu approach cpu gpu embed operation pcie bus hybrid cpu gpu severe latency overhead vertically integrate fundamentally address thanks innovation approach entire embed lookup inside TensorDIMMs embed operation conduct ample memory bandwidth available across TensorDIMMs cpu bandwidth cpu memory TensorDIMM NMP core conduct tensor reduction operation gpu reduce TensorNode gpu communication latency factor effectively overcome communication bottleneck furthermore bandwidth TensorNode gpu link NVLINK micro october columbus usa  kwon  lee   vector alu ddr phy NMP local memory controller input input output protocol local dram DDRx memory channel dram buffer device dram dram dram dram buffer device dram dram dram dram buffer device dram dram dram dram buffer device dram dram dram NMP core TensorDIMM NVSwitch TensorNode TensorDIMM TensorDIMM TensorDIMM TensorDIMM gpu  architecture TensorNode NVLINK GB sec overview propose NMP core implement inside buffer device TensorDIMM multiple employ disaggregated memory pool TensorNode TensorNode integrate inside bandwidth gpu interconnect combination NVLINK NVSwitch enables gpus data TensorNode communication bandwidth pcie enable latency reduction proportional bandwidth difference pcie NVLINK approximately lastly dnn computation conduct gpu overcome computation bottleneck cpu implementation detailed proposal achieves average performance improvement cpu hybrid cpu gpu respectively  oracular gpu implementation infinite memory capacity detail component proposal TensorDIMM memory tensor ops TensorDIMM architected objective TensorDIMM leverage commodity dram chip capable utilized normal buffer DIMM device DL acceleration tensor reduction operation conduct  lightweight NMP core incur minimum overhead buffer DIMM device addition memory capacity amount memory bandwidth available NMP core proportional TensorDIMM module employ architecture TensorDIMM architecture consists NMP core associate dram chip depict TensorDIMM commodity DRAMs modification limited buffer device within DIMM NMP core ddr interface vector alu NMP local memory controller input output SRAM queue stage source destination operand tensor operation ddr interface implement conventional ddr phy protocol TensorDIMM usage non DL processor memory controller sends receives dram DQ signal ddr interface directly interacts dram chip allows TensorDIMM applicable normal buffer DIMM device utilized conventional processor architecture service load transaction upon TensorISA instruction tensor reduction operation however instruction NMP local memory controller translates TensorISA instruction dram command dram chip specifically TensorISA instruction decode detailed calculate physical memory address target tensor location NMP local memory controller generates RAS CAS activate precharge etc dram command data TensorDIMM dram chip data dram chip temporarily inside input SRAM queue alu tensor operation NMP compute minimum data access granularity dram chip byte burst amount sixteen byte scalar detailed tensor operation accelerate NMP core wise arithmetic operation average exhibit data parallelism across sixteen scalar therefore employ vector alu conduct wise operation data SRAM queue vector alu newly submit data byte pop tensor operation output SRAM queue NMP memory controller output queue newly insert drain dram finalize tensor reduction tensor NMP core data input queue output queue commit dram implementation overhead TensorDIMM leverage exist dram chip associate ddr phy interface additional component introduce TensorDIMM NMP local memory controller vector alu memory controller decode TensorISA instruction series dram command implement FSM logic overhead input output SRAM queue buffer bandwidth delay memory source data remove idle output tensor generation TensorDIMM memory processing architecture sparse embed layer micro october columbus usa conservative estimate latency  memory controller request data arrives SRAM queue SRAM queue capacity assume baseline PC DIMM GB sec memory bandwidth NMP core GB sec KB SRAM queue KB overall input output queue vector alu clocked mhz computation throughput seamlessly conduct wise tensor operation data input queue ibm centaur buffer device approximately TDP NMP core negligible overhead quantitatively evaluate memory bandwidth important objective TensorDIMM scalable memory bandwidth expansion NMP tensor operation challenge conventional memory maximum bandwidth per memory channel fix signal bandwidth per pin data pin per channel regardless DIMMs rank DIMM per channel instance maximum cpu memory bandwidth available baseline cpu nvidia DGX exceed GB sec GB sec across memory channel channel per socket irrespective DIMMs actually utilized DIMMs DIMMs physical memory channel bandwidth multiplexed across multiple DIMMs rank detailed subsection TensorDIMM utilized building construct disaggregated memory TensorNode innovation proposal combine TensorISA address mapping function amount aggregate memory bandwidth NMP core within TensorNode increase proportional TensorDIMM employ aggregate GB sec memory bandwidth assume TensorDIMMs increase baseline cpu memory memory bandwidth NMP core access TensorDIMM internal dram chip locally within DIMM local memory bandwidth TensorDIMMs naturally TensorDIMMs employ inside memory pool aggregate memory bandwidth becomes available NMP core conduct embed reduction architecture overview propose architecture propose construct disaggregated memory pool within bandwidth gpu interconnect refer TensorNode function interconnect endpoint node gpus data TensorNode NVLINK compliant phy interface grain CC numa coarse grain data transfer PP cudaMemcpy depicts TensorNode multiple TensorDIMM device advantage TensorNode threefold TensorNode CC numa access PP cudaMemcpy NVLINK compatible device already available commercial gpus within DGX TensorNode leverage technology minimize complexity platform increase memory capacity scalable manner disaggregated memory pool independently expand density optimize DDRx irrespective gpu local bandwidth optimize capacity limited 3D stack memory multiple embed lookup entirely inside TensorNode multitude TensorDIMM device equip density optimize LR DIMM enable scalable memory capacity increase aggregate memory bandwidth available TensorDIMM NMP core proportional DIMMs provision within TensorNode allows TensorNode TensorDIMM fulfill future memory capacity bandwidth recommender combine multiple embeddings become recent projection  memory capacity requirement embed layer increase already GBs memory footprint TensorNode scalable future proof address memory bottleneck embed layer communication channel TensorNode implement bandwidth NVLINK phys transfer embeddings gpu TensorNode becomes faster pcie approximately software architecture discus software architecture proposal address mapping scheme embeddings TensorISA conduct memory operation runtime address mapping architecture objective address mapping function scalable performance improvement whenever additional TensorDIMMs DIMMs NMP core TensorNode achieve goal important NMP core within TensorNode concurrently distinct subset embed vector reduce tensor operation illustrates address mapping scheme utilizes rank parallelism maximally utilize TensorDIMM NMP computation throughput memory bandwidth TensorDIMM NMP core maximally utilize aggregate NMP compute throughput TensorDIMMs parallel address mapping function accomplishes consecutive byte within embed vector interleave across rank TensorDIMM independently slice tensor operation concurrently target algorithm contains abundant data parallelism address mapping technique effectively partition load balance tensor operation across TensorDIMMs within TensorNode quantitatively evaluate efficacy address mapping function maximum dram bandwidth utilization worth address mapping function address future projection DL practitioner seek embeddings capability DL practitioner willing assume TensorDIMM built DIMM burst minimum data access granularity becomes byte micro october columbus usa  kwon  lee   index embed index embed vector rank embed dram address bitmask virtual address virtual offset input embed input embed output embed input embed input  wise operation rank rank rank NMP core DIMM NMP core DIMM NMP core DIMM propose dram address mapping scheme embeddings assume embed KB dimension split evenly across rank DIMMs rank parallelism utilized interleave embed across TensorDIMMs rank parallelism centric address mapping scheme enable memory bandwidth available NMP core increase proportional TensorDIMMs employ adopt embed vector dimension improve algorithmic performance translates reference embed vector enlarge embed KB KB increase embed width memory footprint embeddings computation memory bandwidth demand conduct tensor reduction effectively handle user architect provision TensorDIMM rank within TensorNode increase memory capacity proportional increase embed naturally accompany increase NMP compute throughput memory bandwidth simultaneously TensorISA memory tensor operation initiate custom ISA extension TensorISA TensorISA primitive TensorDIMM instruction embed lookup reduce average instruction wise operation summarize instruction format instruction pseudo code functional behavior assumes embed layer embed lookup batch compose tensor wise operation reduction embed layer embed lookup phase multiple embeddings batch embed lookup various tensor manipulation propose gpu executes embed layer opcode  aux     reduce    average    instruction format wise reduction wise average embed lookup instruction format reduce average  void src byte address src  void dst  byte address dst  TensorDIMMs within TensorNode tid unique ID assign TensorDIMM byte vector register byte array access byte granularity pseudo code embed lookup      tid    tid pseudo code wise operation reduce    tid    tid OP    tid pseudo code wise average average      tid     tid pseudo code explain functional behavior reduce average gpu sends instruction instruction reduce TensorNode TensorISA instruction broadcast TensorDIMMs NMP core responsible locally conduct embed lookup slice tensor operation instance assume address mapping function TensorNode configuration TensorDIMMs instruction TensorDIMM byte byte data contiguous physical address orchestrate TensorDIMM NMP local memory controller dram transaction undertaken twice tensor slice per TensorDIMM rank tensor reduce instruction execute NMP core runtime DL application typically encapsulate acyclic graph dag data structure DL framework node within dag dnn layer DL framework compiles dag sequence host cuda kernel launch gpu executes layer focus recommender utilizes embed lookup various tensor manipulation propose embed layer execute normal cuda kernel launch kernel wrap around specific information TensorDIMM runtime TensorDIMM memory processing architecture sparse embed layer micro october columbus usa baseline TensorNode configuration dram specification ddr PC TensorDIMMs memory bandwidth per TensorDIMM GB sec memory bandwidth across TensorNode GB sec utilize memory tensor operation specifically embed layer cuda kernel context information lookup embed dimension tensor reduction input batch etc encode per TensorISA instruction format gpu cuda kernel launch gpu runtime receives instruction TensorNode memory processing TensorNode remote disaggregated memory pool gpu perspective runtime allocate memory inside remote memory pool proposal upon prior proposes cuda runtime api extension remote memory allocation gpu disaggregated memory refer interested reader detail runtime cuda apis memory allocation within disaggregated memory evaluation methodology architectural exploration TensorDIMM implication within TensorNode cycle simulator challenge batch inference DL application millisecond gpus cycle simulation batch inference intractable amount simulation proposal multiple hardware software stack cycle hardware performance model TensorDIMM TensorNode alone properly reflect complex interaction micro architecture runtime software potentially mislead conclusion interestingly DL operation utilized embed layer completely memory bandwidth limited allows utilize exist DL hardware software emulate behavior TensorDIMM TensorNode DL recall embed lookup tensor reduction operation reduce average extremely compute memory ratio operation effectively application render execution operation bottleneck available memory bandwidth consequently effectiveness proposal primarily effectiveness address mapping scheme maximally utilize aggregate memory bandwidth tensor operation across TensorDIMMs impact pcie NVLINK communication latency copying embeddings across capacity optimize cpu TensorNode memory bandwidth optimize gpu memory introduce novel hybrid evaluation methodology utilizes cycle simulation proof concept prototype developed DL quantitatively demonstrate benefit proposal SM SM SM SM crossbar MC MC MC MC dram dram dram dram gpu architecture NMP core TensorNode NMP local memory channel associate memory bandwidth gpu interconnect gpu normal gpu gpu emulate TensorNode emulation TensorDIMM TensorNode gpu gpu core aka SMs gpu local memory channel bandwidth corresponds NMP core within TensorDIMMs aggregate memory bandwidth available across TensorNode respectively cycle simulation performance TensorDIMM TensorNode bound utilize dram bandwidth evaluation proposal memory bandwidth utilization develop memory trace function hook DL framework generate memory transaction execute reduce average operation embed lookup tensor operation trace fed ramulator cycle accurate dram simulator configure model baseline cpu gpu configuration cpu memory channel propose address mapping function TensorNode configuration utilize effective memory bandwidth utilization execute tensor operation baseline TensorNode proof concept prototype emulate behavior propose nvidia DGX machine gpus gpu contains GB sec local memory bandwidth  communicate gpus GB sec emulate bandwidth NVLINK communication channel TensorNode gpu gpus DGX treat propose TensorNode normal gpu tensor operation accelerate TensorDIMMs memory bandwidth limited workload performance difference hypothetical TensorNode TensorDIMMs emulate TensorNode local memory bandwidth assume TensorNode configuration evaluate effective memory bandwidth utilization TensorNode configure rank TensorDIMMs aggregate memory bandwidth available within TensorNode approximately GB sec GB sec validate effectiveness TensorNode utilize memory bandwidth commensurate implement software prototype recommender configurable DL application intel math kernel library mkl version cudnn version cuBLAS cuda implementation embed layer micro october columbus usa  kwon  lee   evaluate benchmark default configuration network lookup max reduction FC mlp layer NCF youtube fox facebook reduce average layer mkl cudnn cuBLAS validate implementation memory bandwidth limited layer performance upper bound ideal performance exhibit variation cuda implementation tensor reduction operation gpu core SMs cuda effectively function NMP core within TensorNode cuda kernel reduce average stage tensor SM gpu local memory fashion TensorDIMM TensorNode gpu communication orchestrate PP cudaMemcpy NVLINK evaluate proposal sensitivity TensorDIMM TensorNode gpu communication bandwidth artificially increase decrease data transfer emulate behavior communication bandwidth implication performance benchmark neural network recommender application embeddings evaluate proposal neural collaborative filter NCF recommender available mlperf youtube recommendation youtube fox movie recommendation fox facebook recommendation facebook realistic inference batch representative inference scenario refer recent facebook datacenter recommender commonly deployed batch prior batch default configuration sweep batch sensitivity workload configure default embed vector dimension application configuration summarize specifically deviate default configuration sensitivity implementation overhead TensorDIMM synthesize implementation verilog hdl target xilinx virtex ultrascale VCU acceleration dev overhead TensorNode evaluate micron ddr calculator detail evaluation explore recommender  version cpu hybrid cpu gpu version cpu gpu TensorNode style pool memory interfaced inside highbandwidth gpu interconnect utilizes regular capacity optimize DIMMs NMP enable TensorDIMMs pmem facebook recommendation model release submit peer review micro implement evaluate model proof concept emulation framework facebook source pytorch version consistency model bandwidth utilization GB sec batch average TDIMM reduce TDIMM TDIMM average cpu reduce cpu cpu memory bandwidth utilization tensor operation TensorNode assumes default configuration TensorDIMMs cpu cpu gpu tensor operation conduct conventional cpu memory memory channel DIMMs rank per memory channel assume DIMMs DIMMs DIMMs DIMMs DIMMs DIMMs DIMMs DIMMs DIMMs DIMMs DIMMs DIMMs DIMMs DIMMs DIMMs DIMMs DIMMs DIMMs reduce average reduce average embeddings inside cpu embed inside TensorNode bandwidth utilization GB sec memory throughput function DIMMs employ within cpu TensorNode evaluation assumes embed increase default proportionally increase embed lookup proportional increase memory capacity DIMMs lookup propose TensorNode TensorDIMMs TDIMM  oracular gpu version gpu assumes entire embeddings inside gpu local memory obviate cudaMemcpy memory bandwidth utilization validate effectiveness TensorNode amplify effective memory bandwidth aggregate memory throughput achieve TensorNode baseline cpu utilizes DIMMs TensorNode significantly outperforms baseline cpu average increase memory bandwidth utilization max GB sec conventional memory multiplex memory channel across multiple DIMMs DIMMs enlarge memory capacity bandwidth TensorDIMM ensure aggregate memory bandwidth proportional TensorDIMMs achieve significant memory bandwidth benefit proposal pronounce future scenario enlarge embed dimension effective memory bandwidth tensor operation embed dimension increase necessitate DIMMs proportionally increase embed lookup depict baseline TensorDIMM memory processing architecture sparse embed layer micro october columbus usa cpu cpu gpu pmem TDIMM gpu cpu cpu gpu pmem TDIMM gpu cpu cpu gpu pmem TDIMM gpu cpu cpu gpu pmem TDIMM gpu NCF youtube fox facebook latency normalize embed lookup cudaMemcpy computation breakdown latency inference batch normalize slowest cpu cpu gpu cpu memory memory bandwidth saturates around GB sec fundamental limit conventional memory TensorNode TB sec bandwidth increase baseline performance TensorDIMM significantly reduces latency execute  embed layer thanks memory throughput communication bandwidth amplification memory tensor operation bandwidth NVLINK latency breakdown workload assume batch propose application enjoy significant reduction embed lookup latency embed latency bandwidth TensorDIMM tensor operation communication link utilized embeddings gpu memory summarizes normalize performance across batch cpu occasionally achieves performance cpu gpu batch inference scenario oracular gpu consistently performs average highlight advantage dnn acceleration gpus NPUs TensorDIMM achieves average performance  oracular gpu demonstrate performance merit robustness cpu cpu gpu average speedup TensorDIMM embeddings motivation scalable memory embeddings tensor operation assume default embed workload summarize  pool memory architecture DL practitioner provision embeddings develop recommender superior model quality evaluation conduct emulate version TensorDIMMs demonstrate model quality improvement embed cannot  algorithm memory capacity constrain gpu memory nonetheless conduct sensitivity TensorDIMMs embed embeddings embed layer serious performance bottleneck TensorNode performance benefit setting achieve average maximum performance improvement cpu cpu gpu respectively TensorDIMM bandwidth interconnects highlight maximum potential TensorDIMM merit context bandwidth gpu interconnect nonetheless TensorDIMM utilized conventional cpu centric disaggregated memory concretely envision contains pool memory TensorNode interfaced bandwidth interconnect pcie hybrid cpu gpu distinction tensor operation NMP core reduce tensor gpu communication channel summarizes sensitivity proposal TensorNode gpu communication bandwidth pmem disaggregated memory without TensorDIMMs TDIMM bandwidth interconnects highlight robustness TensorDIMM brings overall pmem sensitive communication bandwidth TDIMM maximum performance loss benefit NMP reduction lose pmem TensorDIMM TDIMM performance loss average communication bandwidth highlight robustness applicability TensorDIMM overhead TensorDIMM dram chip lightweight NMP core inside buffer device implement synthesize component NMP core xilinx virtex ultrascale fpga verilog hdl confirm overhead NMP core mostly negligible dominate KB SRAM queue vector alu perspective utilizes gpus overhead disaggregated TensorNode assume TensorDIMM GB load reduce DIMM consumption becomes estimate micron ddr calculator TensorNode TensorDIMMs amount overhead recent specification accelerator interconnection fabric endpoint compute project accelerator module employ TDP overhead TensorNode acceptable related disaggregated memory typically deployed remote memory pool pcie increase cpu accessible memory capacity prior propose systemlevel embrace memory disaggregation within bandwidth gpu interconnect similarity proposition TensorNode however focus prior DL training whereas primarily focus DL inference TensorDIMM prior explore possibility utilize DIMM buffer device custom acceleration logic instance lightweight cpu core inside buffer device construct memory channel network mcn seek micro october columbus usa  kwon  lee   cpu cpu gpu pmem TDIMM gpu cpu cpu gpu pmem TDIMM gpu cpu cpu gpu pmem TDIMM gpu cpu cpu gpu pmem TDIMM gpu cpu cpu gpu pmem TDIMM gpu cpu cpu gpu pmem TDIMM gpu cpu cpu gpu pmem TDIMM gpu cpu cpu gpu pmem TDIMM gpu cpu cpu gpu pmem TDIMM gpu cpu cpu gpu pmem TDIMM gpu cpu cpu gpu pmem TDIMM gpu cpu cpu gpu pmem TDIMM gpu cpu cpu gpu pmem TDIMM gpu batch batch batch batch batch batch batch batch batch batch batch batch geometric NCF youtube fox facebook performance normalize performance recommender normalize oracular gpu gpu batch batch batch batch batch batch batch batch batch batch batch batch batch batch batch batch batch batch batch batch batch batch batch batch embed embed embed embed embed embed embed embed cpu hybrid cpu gpu performance normalize TensorDIMM performance embed average across benchmark GB sec GB sec GB sec GB sec GB sec GB sec GB sec GB sec GB sec GB sec GB sec GB sec GB sec GB sec GB sec GB sec GB sec GB sec GB sec GB sec GB sec GB sec GB sec GB sec embed embed embed embed embed embed embed embed pmem TDIMM performance normalize performance sensitivity pmem pool memory without NMP acceleration TensorDIMM communication bandwidth average across benchmark normalize default configuration GB sec data amplify effective memory bandwidth expose overall TensorDIMM memory processing approach similarity mcn application embed layer tensor operation likely sub optimal performance mcn inability maximally utilize dram bandwidth embeddings concretely performance embed lookup operation CPUs reporting irregular sparse memory access embed operation render cpu cache rate become extremely latency traverse cache hierarchy maximum dram bandwidth utilized oppose TensorDIMM NMP core maximum dram bandwidth average scope prior significantly focus knowledge fpga utilization NMP core SRAM queue vector alu precision float FPU fix alu xilinx virtex ultrascale VCU acceleration development lut FF dsp bram SRAM queue FPU alu academia identify address memory capacity bandwidth challenge embed layer  deem crucial challenge emerge DL workload closely related prior explore gpu npu architecture DL recent leverage sparsity efficiency improvement acceleration platform training DL algorithm propose network centric DL training platform propose prior orthogonal proposal adopt additional enhancement conclusion propose vertically integrate hardware software address memory capacity bandwidth embed layer important building emerge DL application TensorDIMM architecture synergistically combine NMP core commodity dram device accelerate DL tensor operation built disaggregated memory pool TensorDIMM memory capacity bandwidth embeddings achieve average performance improvement conventional cpu hybrid cpu gpu implementation recommender knowledge TensorDIMM quantitatively explores architectural tailor embeddings tensor operation