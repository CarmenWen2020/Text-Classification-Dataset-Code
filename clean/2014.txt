data partition schedule strategy dnn accelerator leverage reuse perform stag dataflow directly impact performance efficiency dnn accelerator accelerator microarchitecture dictate dataflow employ execute layer dnn dataflow layer impact utilization efficiency lack understand choice consequence dataflows methodology architect explore optimization introduce data centric directive concisely specify dnn dataflow compiler friendly directive analyze infer various reuse exploit hardware capability codify analysis analytical model maestro model accelerator efficiency via spatio temporal reuse occupancy estimate various benefit tradeoff dataflow execution efficiency dnn model hardware configuration demonstrate maestro hardware exploration across identify valid average rate per pareto optimal throughput optimize CCS CONCEPTS computer organization neural network hardware model parameter extraction keywords neural network dataflow model introduction neural network dnns deployed increase across iot platform complex regression classification image recognition recognition translation accuracy surpass tight latency throughput constraint dnns  increase hardware accelerator dnn accelerator achieve performance exploit parallelism processing PEs efficiency maximize data reuse within PEs chip scratchpad specific dnn workload hardware accelerator achieve utilization data reuse directly depends schedule dnn computation choice loop transformation computation across PEs component collectively refer dataflow accelerator literature data exceeds computation understand optimize dataflow critical component dnn accelerator directly determines data transfer multiplier stag local buffer global buffer hierarchy beyond performance efficiency dnn accelerator target dnn model layer dimension dataflow available hardware resource connectivity dimension tightly couple optimize dnn accelerator across dimension challenge task dataflow exploit input channel parallelism convolutional neural network cnns achieve utilization layer channel alternatively dataflows transfer bandwidth network onchip noc utilization hardware increase scratchpad dataflow data bandwidth micro october columbus usa  kwon   michael       krishna increase consumption optimize hardware microarchitecture dataflows primary optimization target accelerator remains challenge novel dataflows microarchitectures propose recently  proposal dataflows exhaustive reference architect custom accelerator within variety constraint contrast recent proposal compilation analysis dnns analyze software mapping dnn workload onto architecture relationship software mapping hardware dataflows elucidate architect intellectual intuition consequence dataflow selection impact reuse dataflow inconsistent manner across architecture analysis proposal architect incomplete unstructured intuition dataflows complex interplay dataflow microarchitecture choice seek remedy situation thorough insight choice consequence dataflow selection interplay microarchitectural alternative structure mechanism quantitatively specific contribution introduce data centric notation various accelerator dataflows data mapping reuses entity unlike compute centric notation prior proposal infer data reuses loop nest representation data centric directive express data reuses across arbitrary hierarchy PEs dense sparse dnn layer convolution lstms fully layer data centric notation complement commonly  notation notation intermediate representation IR extract  notation specify directly data centric directive reuse structure manner demonstrate relationship directive specific algorithmic reuse expose directive potential exploit reuse hardware capability improve efficiency analysis dataflow exploit reuse introduce analytical model maestro model accelerator efficiency via spatio temporal reuse occupancy programmatically implement analysis maestro input dnn model layer dataflow description layer specify propose directive hardware configuration input maestro output estimate execution compute buffer interconnect activity noc challenge propose approach estimation efficient sufficiently precise effectively exploration demonstrate maestro abstract hardware model analytic model within accuracy actual source rtl filter input activation output activation convention input batch output channel input channel input input filter filter output output tensor dimension notation output activation input activation filter tensor tensor index upper index partial sum text accumulate dimension generate output convolutional layer input centric loop nest representation output centric loop nest representation convolutional layer faster maestro versus equivalent rtl simulation workstation xeon processor 4GB memory finally demonstrate maestro model accelerator designer pareto optimal parameter accelerator throughput budget NVDLA dataflow KC partition vgg conv layer difference consumption versus throughput optimize optimize employ SRAM PEs throughput optimize delay improvement throughput concrete significance accelerator architect background understand benefit tradeoff various approach compute convolution discus core concept related data reuse dataflows context dnn accelerator tensor dnns multi channel 2D convolution involves data dimension across data structure input output activation tensor although approach apply various dnn layer CONV2D fully FC lstm separable convolution focus CONV2D variant convolutional neural network cnns popular CONV2D account overall computation cnns tensor dnns address dimension complex manner index output deduce input filter index input centric convolution loop nest input channel index filter input activation output channel filter output activation dimension couple index data index modify specific data access transform loop nest understand reuse performance hardware dnn dataflows data centric approach micro october columbus usa PE buffer scratch pad network chip noc private buffer scratch pad alu mac PE private buffer scratch pad alu mac PE private buffer scratch pad alu mac dram parameter buffer bandwidth dram bandwidth latency bandwidth average latency spatial temporal multicast reduction implementation PEs scratch pad vector width alu precision alu abstract dnn accelerator architecture model pervasive accelerator illustrate architecture hierarchically organize data structure stationary unchanged local buffer significantly reduce global local buffer access dnn accelerator consumption local  dnn accelerator dnn accelerator specialized architecture dnn application throughput efficiency described dnn accelerator employ processing PEs exploit inherent parallelism dnn application PEs typically scratchpad memory ALUs perform accumulate operation MACs reduce  consume dram access dnn accelerator scratchpad buffer stage data PEs buffer PEs interconnect network chip noc approach interconnect noc module systolic array 2D array unidirectional link hardware parameter approach architecture efficiently execute dnn operation convolution enables exploit parallelism data reuse via buffer multicasting NoCs data reuse taxonomy data reuse originates behavior dnn accelerator multicasting input tensor reduction output tensor multicasting spatial multicasting data buffer spatially replicates data via delivers data multiple spatial destination PEs reduces expensive remote buffer access likewise temporal multicasting data remote buffer temporally replicates data via local buffer delivers data multiple temporal destination instance PE reduces expensive remote buffer access reduction spatial reduction accumulates partial output multiple spatial source spatially accumulates via multiple compute adder reduce similarly temporal reduction accumulates partial output multiple temporal source partial sum compute temporally accumulates via accumulation register buffer accumulation buffer tpu input output spatial dimension PE PE PE PE PE data usage timeline temporal reuse spatial reuse multicasting spatial reuse reduction temporal dimension cycle partial sum output slide operational stationary style accelerator PEs simplicity input output channel batch omit kernel dataflow definition leverage opportunity accelerator schedule operation PEs proceed data tensor coordinate fashion transformation tile apply convolution along partition data PEs schedule dataflows prior categorizes dataflows tensor schedule frequently stationary output stationary input stationary stationary dataflow PEs multicast across temporal multicasting multicast across PEs spatial multicasting reduce across accelerator temporally reuses spatially reuses stationary conveys intuition characterization schedule strategy detailed insight analysis precise description refine definition dataflow additionally specify schedule concrete bound instance mapping dataflow important distinction allows accelerator categorize buffer mobile chip datacenter chip traversal ordering despite difference tile brevity remainder distinction schedule fully specify partially unspecified concrete bound refer dataflows exist expression dataflow convey schedule decision architecture dataflows express loop nest syntax resembles imperative program explicit parallelism eyeriss loop nest notation compute centric representation data movement implicit loop explicit parallelism specify micro october columbus usa  kwon   michael       krishna PE PE PE PE cluster par target PE buffer par target PE buffer reg target chip global buffer TemporalMap TemporalMap cluster  global buffer target PE buffer SpatialMap TemporalMap cluster PE buffer target PE buffer reg TemporalMap TemporalMap  dataflow intra PE dataflow program 1D conv output stationary data orchestration loop nest output stationary dataflow data centric directive SpatialMap TemporalMap abbreviate data orchestration description data centric directive mapping PEs data dimension mapping PEs mapping iteration  description 1D convolution output stationary dataflow convolution dataflow loop nest data centric directive  description infer upper affect data reuse PEs abbreviate dataflow description data centric directive mapping PEs iteration dot correspond computation partial sum programmer loop dictate schedule computation explicit annotation loop parallel capture parallelism combination loop tile parallelism enables data reuse therefore architect explore optimize loop nest encompass aspect loop parallelism tile eyeriss describes dataflow dimensional loop nest compute centric representation polyhedral model compiler estimate reuses optimal loop transformation parallelism locality sufficiently accurate estimation series loop transformation compiler however precisely model data reuse therefore compute throughput efficiency accuracy challenge developed analytical model accurately estimate cache behavior thereby compute reuses affine program precisely analyze polyhedral model compile however heavyweight linear algebra framework within polyhedral model compute reuse thereby impractical technique application challenge polyhedral framework compute reuse arise array subscript involve non affine expression complex subscript modulus operation stride convolution addition although exists compiler performs reuse analysis sequential program lack ability analyze loop nest explicit parallelism dnn dataflows multiple parallelism spatial reuse refer spatial locality cache architecture data reuse via across PEs leverage multicasting reduction accelerator role estimate overall throughput efficiency spatial dnn accelerator limitation challenge motivate explore alternative intermediate representation IR dataflows data centric representation data movement organization entity data movement explicit data centric representation analytical model becomes simpler relatively faster leverage heavyweight linear algebra framework precisely estimate data movement reuse behavior  dataflows data centric representation consists directive spatial temporal data movement cluster briefly explain directive 1D convolution pedagogical discus various hardware implementation choice data reuse across data centric representation define dataflow accelerator consist aspect schedule dnn computation choice loop transformation across exploit reuse mapping dnn computation across PEs parallelism representation component briefly discus component fourth component cluster introduce spatial offset specifies distribution dimension data structure across PEs refers index mapped dimension PE offset describes shift index across consecutive PEs temporal offset specifies distribution dimension data structure across PE mapped chunk dimension index across PEs refers index mapped dimension PE offset describes shift index across consecutive PE data movement sequence spatial temporal dataflow specification dictate understand reuse performance hardware dnn dataflows data centric approach micro october columbus usa SpatialMap TemporalMap TemporalMap SpatialMap TemporalMap SpatialMap SpatialMap TemporalMap iteration partial sum mapping temporal multicast stationary temporal reduction output output stationary temporal multicast stationary temporal reduction output output stationary temporal reuse spatial reduction output spatial reduction output spatial multicast filter spatial multicast filter spatial reuse input  data PE PE PE PE PE PE PE PE PE PE PE PE filter data output  data SpatialMap TemporalMap temporal multicast stationary partial temporal multicast input PE spatial reduction output PE PE PE TemporalMap SpatialMap cluster SpatialMap TemporalMap temporal multicast stationary spatial reduction output cluster PE PE PE cluster PE PE PE collaborative stationary collaborative output stationary output stationary stationary informal dataflow tile collaborative stationary cluster tile collaborative stationary dataflow ID impact directive spatial temporal tile cluster 1D convolution mapping described data centric directive iteration correspond partial sum data mapping data structure finally temporal spatial reuse opportunity mapping data movement data mapping PEs across demonstrate reuse opportunity various dataflows 1D convolution unique dataflow program loop nest representation assume accelerator hierarchy register PE local scratchpad buffer loop enclose indicative mapping PEs correspond data centric representation data correspond output dimension spatially distribute across PEs PE receives chunk output data distribution capture spatial directive offset parameter SpatialMap dimension output data structure data correspond dimension replicate across multiple PEs PE receives chunk iteration receives chunk iteration replicate temporal distribution capture temporal directive offset parameter TemporalMap dimension data structure spatial temporal capture data mapping movement behavior across PEs correspond loop loop nest version directive enclose data centric representation description unique dataflow dataflow playground dataflows upon 1D convolution demonstrate dataflow expose various reuse spatial temporal illustrates dataflows consists dataflow variant modify directive spatially temporally mapped dimension mapping PE cluster discus impact data reuse directive directive entirely temporal reuse stationary behavior sequence directive mapping indicates data index explore chunk index temporally reuse data correspond index partial sum index therefore dataflow informally refer micro october columbus usa  kwon   michael       krishna output stationary partition across multiple output parallel impact interchange directive stationary dataflow PEs temporally reuse correspond index index chunk index similarly spatial distribution instead impact data movement temporal reuse dataflow variation indicates informal dataflow precise specification behavior spatially temporally mapped dimension directive SpatialMap refers dimension output data structure spatially distributes index dimension chunk parameter across PEs offset offset parameter PE output data PEs sufficient index dimension mapped mapping fold across PEs offset overlap index across consecutive PEs useful mapping input activation dimension iteration skewed similarly TemporalMap refers dimension filter data structure distributes index dimension chunk across offset PE data PEs data index correspond temporally mapped dimension creates opportunity spatial reuse multicasting data across PEs mapping mapping mapping argument output temporal reuse temporal reuse output mapping vice versa mapping temporal reuse input mapping increase spatial temporal opportunity partial temporal reuse capture convolutional reuse input cnn layer spatial correspond dimension exploit partial temporal reuse input data across PE cluster multi dimensional spatial distribution data mapping related outer update exploration inner inherent assumption limit dataflow behavior interested simultaneously exploit spatial distribution data dimension address introduce another directive cluster simultaneous spatial distribution multiple data dimension cluster directive logically multiple PEs nest sub cluster dataflow multiple cluster directive parameter cluster arranges available PEs cluster PEs mapping directive specify cluster directive perform mapping across logical cluster cluster directive mapping directive specify cluster directive perform mapping across PEs logical cluster inside logical cluster cluster directive mapping directive cluster directive logical cluster cluster directive inside logical cluster mechanism specify complex dataflows multiple parallelization dimension multiple  directive cluster dimension spatially distribute across cluster dimension spatially distribute within cluster cluster directive enable exist accelerator dataflows eyeriss involves spatial distribution dimension simultaneously NVDLA involves spatial distribution dimension another advantage cluster directive notion multiple PEs coarse grain PEs accelerator simd matrix tensor accelerator gpu tensor core summary transformation capture aspect dataflows schedule tile mapping data centric directive concisely aspect envision data centric representation auto generate loop nest version dataflow affine constraint manually hardware implication reuse various data reuse opportunity dataflow summarizes opportunity relationship spatially mapped dimension within cluster inner temporally mapped dimension  output channel spatially mapped decouple data structure input feature PEs input feature implies spatial reuse opportunity broadcast inner temporally mapped dimension input channel input channel iteration temporal reduction opportunity output although dataflow temporal spatial data reuse opportunity appropriate hardware actually exploit phenomenon summarizes reuse category correspond hardware implementation reuse spatial temporal data structure communication multicast input tensor reduction output tensor multicast communication delivers data multiple target PEs PE therefore multicast communication fan network chip structure bus stationary buffer data deliver future contrast reduction communication applies partial sum generate output reduction spatial temporal hardware spatial reduction reduction reduce chain systolic array temporal reduction modify buffer summary dataflows express via directive expose reuse spatial temporal multicasts reduction multiple hardware understand reuse performance hardware dnn dataflows data centric approach micro october columbus usa reuse opportunity spatially mapped dimension combination innermost temporally mapped dimension filter input output separately brevity interpret appropriate mapped dim couple reuse opportunity multicast reduction multicast multicast spatial innermost mapped dim couple reuse opportunity reduction multicast multicast multicast temporal multicast multicast multicast multicast reduction multicast reduction multicast hardware implementation choice spatial temporal reuse temporal multicast refer stationary buffer data reuse spatial communication HW implementation choice multicast reduction PE PE PE PE GBM fanout bus  reduction GBM PE PE PE temporal multicast fwd systolic array PE PE PE reduce fwd systolic array PE PE PE PE buf mac multiple buffer reduction PE buf mac multiple modify buffer implementation dataflows structure manner expose insight potential microarchitectural discussion focus 1D convolution expose dataflows reuse opportunity extend convolution loop analyze reuse opportunity within specific dataflow extend stationary dataflow detailed mapping reuse across stationary dataflow PE accelerator accelerator PE cluster PEs cluster layer previously compute data centric representation stationary dataflow mapping across PE cluster actual coordinate tensor across cluster replicate data imply reuse opportunity replicate data infer data reuse PE array data reuse mapping input activation replicate across cluster skewed manner within implies spatial reuse opportunity diagonal direction PE array similarly replicate within PE implies temporal reuse opportunity stationary style dataflow granularity dataflow stationary coarse grain although stationary define finally output activation PEs PE cluster PEs cluster cooperate generate output activation data PE PE cluster generates partial sum output activation accumulate across PEs PE cluster generate output activation analysis data reuse exactly reuse horizontal direction filter vertical output partial sum accumulation reuse diagonal direction input activation summary reuse opportunity replicate data across PEs implies temporal spatial reuse opportunity respectively demonstrate accurate quantitative methodology compute reuse complex dataflows quantitative dataflow analysis approach quantitatively estimate runtime efficiency dataflows target dnn model hardware configuration approach implement analysis framework maestro consists tensor cluster reuse performance analysis analysis overview discus algorithm without handle multiple layer multiple cluster detail source repository preliminary tensor analysis described tensor analysis identifies dimension couple tensor specify layer operation depth wise convolution output activation couple output channel dimension couple input channel dimension depth wise convolution understood manner eliminate input channel dimension convention aligns maestro input centric model maestro allows user specify tensor arbitrary dimension couple micro october columbus usa  kwon   michael       krishna TemporalMap TemporalMap TemporalMap SpatialMap TemporalMap TemporalMap TemporalMap cluster TemporalMap TemporalMap TemporalMap SpatialMap SpatialMap TemporalMap TemporalMap data centric representation parallel parallel stride stride loop nest representation filter input activation output activation filter input activation output activation filter input activation output activation filter input activation output activation visualize data mapping cluster cluster   automatically infer specify input activation input PE PE cluster PE PE PE cluster PE dim PE PE PE PE PE PE PE cluster cluster PE PE PE PE PE PE cluster cluster PE PE PE PE PE PE cluster cluster PE PE PE PE PE PE cluster cluster PE PE cluster PE PE PE cluster PE dim PE PE PE PE PE PE PE cluster cluster PE PE PE PE PE PE cluster cluster PE PE cluster PE PE PE cluster PE dim PE output data mapping reuse tensor tensor filter output activation mapping data reuse extend stationary style dataflow mapped PE accelerator tile specify apply additional mapping optimization minimize PE utilization data replication across PEs directive asterisk fully unrolled directive entire data dimension mapping multi cluster analysis recursive tensor analysis couple dim analysis cluster analysis cluster structure analysis cluster data mapping analysis cluster dataflow augmentation reuse analysis temporal spatial data reuse analysis sub cluster array temporal spatial data reuse analysis performance analysis spatial ingres egress traffic analysis communication delay analysis analysis buffer access mac analysis buffer analysis cluster dataflow cluster data dim cluster structure tensor couple dim mapping data reuse array mapping array data reuse data iteration analysis computation delay analysis data iteration occurrence ingres egress traffic PEs noc BW latency buffer performance report runtime bottleneck info noc BW requirement report activity buffer requirement overall data reuse latency hiding analysis maestro analysis framework analysis analysis description tensor analysis extract tensor tensor input output couple dimension couple  cluster analysis extract dataflow directive cluster extract data dimension cluster infer directive apply stride analyze sub cluster cluster reuse analysis analyze mapping amount reuse across combination data iteration computes temporal reuse consolidate reuse analysis compute reuse entire sub cluster array cluster computes spatial reuse performance analysis identifies data iteration occurrence computes ingres egress traffic data reuse computes communication ingres egress computation delay latency hiding outstanding delay analysis data iteration computes buffer access mac operation identify buffer requirement analysis dnn model description HW resource description  dataflow description data dimension layer operation stride overview maestro analysis framework simplicity omit component analysis understand reuse performance hardware dnn dataflows data centric approach micro october columbus usa input tensor information tensor tbl cluster information cluster info tbl mapping information mapping reuse data iteration cluster mapping info tbl abstract hardware model target dnn accelerator model output statistic target dnn accelerator dataflow stats compute mapping amount data reuse data iteration procedure  initialize stats extract data iteration init steady data dimension iteration  tensor tbl cluster info tbl iter iteration num occurrence  iter tensor tbl cluster info tbl iteration compute partial sum PE num psums  iter cluster info tbl mapping info tbl reuse iteration compute amount input tensor data fetch buffer upper cluster cluster ingres traffic  iter tensor tbl cluster info tbl mapping info tbl reuse iteration compute amount output tensor data commit buffer upper cluster cluster egress traffic  iter tensor tbl cluster info tbl mapping info tbl core analysis tensor tensor tbl stats  buffer tensor cluster ingres traffic tensor stats  buffer tensor cluster ingres traffic tensor stats upstream buffer tensor cluster egress traffic tensor stats downstream buffer tensor num psums stats upstream buffer req tensor max stats upstream buffer req tensor cluster ingres traffic tensor cluster egress traffic tensor stats downstream buffer req tensor max stats downstream buffer req tensor num psums cluster egress traffic tensor core performance analysis ingres delay  cluster ingres traffic model egress delay  cluster output traffic model compute delay  num psums model compute delay  iter tensor tbl cluster info tbl mapping info tbl considers buffering treat initialization   iter outstanding delay ingres delay compute delay egress delay outstanding delay max ingres delay egress delay compute delay stats outstanding delay num occurrence stats num mac num psums num active cluster num occurrence return stats  overview algorithm performance analysis couple relationship input generality maestro cluster analysis PE cluster refers PEs data dimension parallel specify cluster directive describes analysis cluster analysis cla cla analyzes dataflow description dataflow directive identify sub cluster extract cluster dataflow directive data dimension augment dataflow description directive stride handle cluster reuse analysis description analysis data reuse analysis RA RA identifies amount temporal spatial reuse across adjacent data iteration correspond inner  spatially unrolled mapping directive performance analysis overview performance analysis algorithm performance analysis PA utilize reuse information compute RA PA computes runtime data dimension dataflow compute runtime occurrence accumulate compute runtime runtime dnn accelerator consists communication delay local computation delay PE directly related accelerator hardware parameter PA considers buffering computes outstanding delay delay communication computation delay directly contributes runtime estimate communication delay maestro relies analytical network chip noc model pipe model analytic model pipe model utilizes parameter pipe width bandwidth average delay estimate communication delay via noc model incorporates pipelining packet switch NoCs behavior various combination bandwidth average delay enables model noc structure reasonable accuracy eyeriss hierarchical bus dedicate channel input output tensor therefore bandwidth properly model noc average latency depends implementation detail user appropriate implementation detail ingres egress buffer cycle delay complicate noc architecture user bisection bandwidth average latency uniform communication PEs global buffer 2D mesh network injection bisection bandwidth average latency assume user access noc implementation information noc model precise noc bus crossbar analysis describes analysis CA computes buffer access estimate buffer requirement tensor data reuse compute RA data iteration utilize access mac operation information maestro computes maestro model activity cactus simulation replace model activity  complex dataflow analysis multi cluster analysis multi cluster split  data dimension mapping correspond mapping directive upper cluster outstanding delay cluster becomes computation delay cluster handle various affect cluster maestro recursively performs performance analysis illustrate recursive analysis inner cluster subclusters actual PEs although maestro performs recursion micro october columbus usa  kwon   michael       krishna CC vgg convolutional layer alexnet convolutional layer runtime cycle MAERI maestro eyeriss runtime model validation MAERI rtl simulation PEs eyeriss runtime report PEs complexity PE cluster typically cluster recursively however across cluster tractable dnns although dense convolution simplicity maestro model variety layer lstm hidden layer pool fully transpose convolution generality data centric approach data centric approach operation loop nest input tensor output tensor wherein tensor index couple data dimension affine function maestro model uniformly distribute sparsity dataflow complex statistical sparsity distribution future model validation validate maestro performance model rtl simulation accelerator MAERI eyeriss vgg alexnet respectively runtime estimate maestro within absolute error cycle accurate rtl simulation report processing delay average STUDIES summarizes feature frequently dnn operator dnn model layer refer layer resolution activation shallow channel vice versa respectively label layer layer classification network input channel input activation height identify maestro perform deeper benefit various dataflows apply dnn operation evaluate distinct dataflow style preference dataflow dnn operator estimation activity cactus simulation KB scratchpad MB buffer distinct layer shallow layer narrow dramatically hardware preference dnn operator style dataflow MAERI rtl source eyeriss report runtime alexnet detailed mapping parameter described alexnet layer layer dataflows evaluation conciseness omit redundant directive automatically infer maestro YX YR CK dataflows motivate shidiannao eyeriss NVDLA respectively dataflow spatial dimension upper cluster partition partition YX partition YX KC partition KC TemporalMap TemporalMap TemporalMap TemporalMap TemporalMap SpatialMap TemporalMap TemporalMap TemporalMap TemporalMap TemporalMap SpatialMap TemporalMap SpatialMap TemporalMap TemporalMap TemporalMap TemporalMap cluster SpatialMap TemporalMap TemporalMap SpatialMap TemporalMap TemporalMap TemporalMap cluster SpatialMap SpatialMap SpatialMap TemporalMap TemporalMap TemporalMap TemporalMap TemporalMap cluster SpatialMap partition strategy dataflow YR partition YR characteristic spatial reduction opportunity output activation reuse input activation filter reuse input channel parallelism local reuse temporal reuse filter spatial reuse opportunity via halo input activation input parallelism stationary temporal reuse filter spatial reuse opportunity via 2D halo input activation 2D activation parallelism output stationary motivate shi diannao temporal reuse input activation filter spatial reduction opportunity spatial reuse output activation activation filter parallelism stationary motivate eyeriss spatial reuse input activation spatial reduction factor input channel input output channel parallelism stationary motivate NVDLA dataflow offs dnn operator granularity estimation runtime dataflow across dnn model comparison dataflows actual implementation difference custom implementation logic memory technology KC dataflow style dataflow overall runtime however efficiency vgg YR eyeriss style dataflow runtime YX shidiannao style dataflow unet preference dataflow dnn operator YX runtime segmentation network unet activation input layer recovers activation dimension via convolution transpose convolution preference YX style mainly parallelization strategy exploit parallelism dimension activation efficiency YR dataflow vgg reuse factor local access per fetch layer YR dataflow activation filter reuse factor respectively layer however layer reuse factor YR KC dataflow almost difference KC understand reuse performance hardware dnn dataflows data centric approach micro october columbus usa NLR WS shi RS DLA NLR WS shi RS DLA NLR WS shi RS DLA NLR WS shi RS DLA NLR WS shi RS DLA NLR WS shi RS DLA adaptive runtime cycle NLR WS shi RS DLA NLR WS shi RS DLA NLR WS shi RS DLA NLR WS shi RS DLA NLR WS shi RS DLA NLR WS shi RS DLA adaptive resnet vgg resnext MobileNetV unet average dataflow YX YR KC YX YR KC adaptive runtime cycle dataflow legend layer layer wise residual FC depth wise aggregate residual transpose YX YR KC YX YR KC YX YR KC YX YR KC YX YR KC YX YR KC YX YR KC YX YR KC YX YR KC YX YR KC adaptive plot runtime estimation dataflows respectively apply PEs  noc bandwidth evaluate dataflows dnn model resnet vgg resnext MobileNetV unet average across model dnn operator adaptive dataflow operator dnns feature implication bottleneck depth wise separable convolution  grain operator wise convolution depthwise convolution residual link notable network  DCGAN network MobileNetV resnet resnext dnn operator conv 2D layer conv 2D layer wise convolution CONV2D depth wise convolution residual link skip connection vgg conv MobileNetV conv unet conv vgg conv MobileNetV conv unet conv resnet conv bottleneck MobileNetV bottleneck MobileNetV bottleneck resnet conv bottleneck MobileNetV bottleneck characteristic activation height width shallow input output channel activation height width input output channel parallelism filter convolutional reuse opportunity reduce computation CONV2D reduce data reuse opportunity noc BW requirement extra global buffer dram access fetch previous activation buffer entire activation transpose convolution unet  DCGAN conv  output activation structure sparsity output activation fullyconnected vgg FC resnet  resnext  gemm operation aggregate residual resnext conv data parallelism via branching structure concatenation reduction activation dataflow efficiency YR layer plot although KC YX dataflows runtime noc bandwidth requirement highlight operator dataflows dramatically noc bandwidth others YX bandwidth pointwise convolution convolutional reuse overlap activation data slide layer layer depthwise pointwise activation reuse factor activation reuse YX YR KC filter reuse factor filter reuse noc BW requirement bandwidth gbps dataflow YX YR KC YX YR KC reuse noc bandwidth requirement dataflows PEs dnn operator representative operator dnn model layer conv resnet layer conv vgg depth wise convolution   conv resnext wise convolution conv bottleneck  YX YR KC refers YX YR KC dataflows refers algorithmic maximum reuse lrd lrd input lrd vgg conv vgg conv mac WS shi NLR WS shi DLA RS DLA RS lrd input DLA mac normalize NLR lrd sum  sum YX YR KC YX YR KC KC breakdown consumption mac scratchpad access dataflows access generate maestro appropriate cactus normalize mac micro october columbus usa  kwon   michael       krishna kernel YX optimize exploit convolutional reuse via spatial reuse diverse preference dataflows dnn operator motivates employ optimal dataflow dnn operator refer approach adaptive dataflow benefit average analysis across entire model dnn operator granularity employ adaptive approach potential runtime reduction optimization opportunity exploit flexible accelerator  MAERI via heterogeneous accelerator employ multiple sub accelerator various dataflow style dnn accelerator chip II hardware parameter implementation analysis maestro implement hardware exploration DSE hardware parameter PEs buffer buffer noc bandwidth optimize efficiency throughput  EDP within hardware constraint DSE receives input maestro hardware constraint building synthesize target technology building implement float fix multiplier adder bus bus arbiter global local scratchpad rtl synthesis technology bus arbiter linear quadratic model regression bus increase linearly arbiter increase quadratically matrix arbiter DSE sweep target specify parameter granularity however skip iteration hardware parameter minimum inner loop hardware parameter optimization allows skip invalid various granularity reduces futile effective DSE rate per statistic DSE explore  machine cpu 2GB memory operating linux mint OS DSE machine terminate within effective DSE rate per average analysis DSE explore KC YR dataflow accelerator constraint report chip eyeriss plot entire explore accelerator achieve peak throughput depends PEs noc bandwidth although accelerator sufficient PEs exploit maximum parallelism dataflow allows noc sufficient bandwidth accelerator suffers communication bottleneck noc throughput plot YR dataflow noc bandwidth behavior KC dataflow however stringent constraint YR dataflow behavior DSE maestro report buffer requirement dataflow DSE amount buffer maestro report contrary intuition buffer throughput buffer throughput plot plot optimal regard throughput per buffer buffer throughput plot existence indicates tile strategy dataflow mapping directive representation significantly affect efficiency buffer impact hardware data reuse KC dataflow vgg conv layer throughput optimize bandwidth throughput significantly remains however lack spatial multicast reduction approximately increase fourth throughput optimize moderate PEs buffer imply hardware resource distribute PEs noc buffer PE utilization likewise buffer amount directly increase throughput efficiency imply component intertwine balance obtain highly efficient accelerator related WORKS hardware DSE dataflow optimization dataflow optimization optimization target recent dnn accelerator eyeriss  scnn NVDLA brain  analyze  tradeoff dataflows explore opportunity adaptive dataflow tradeoff construct analytic model convolution FPGAs focus loop transformation interchange unroll tile although analytic model intuition offs dataflows model focus dataflow style propose regional spatial reuse spatio temporal reuse opportunity dnn accelerator communication delay noc dominate dataflows tile target dataflow optimize HLS express complex annotate loop nest HLS synthesis directive  propose automatic fpga pragma annotation program dataflow optimization framework DSE FPGAs analytic model define loop tile unroll however dataflow limited due fix loop presets straightforward input mapping related data centric approach related explore data centric approach approach data memory hierarchy instead structure centric analysis  transformation multi data data  data centric approach understand reuse performance hardware dnn dataflows data centric approach micro october columbus usa PEs constraint throughput mac cycle vgg conv constraint vgg conv constraint vgg conv accelerator KC dataflow accelerator YR dataflow throughput mac cycle throughput mac cycle constraint throughput mac cycle vgg conv throughput optimize optimize buffer KB normalize mac DSE statistic DSE stats valid explore DSE execution min DSE rate sec vgg conv vgg conv vgg conv vgg conv KC dataflow YR dataflow buffer KB normalize mac accelerator KC YR dataflow highlight layer significantly hardware preference apply eyeriss constraint DSE data indicates PEs PEs buffer budget throughput optimize impact multicasting capability bandwidth buffer vgg conv noc BW data cycle reference spatial reuse bandwidth  reduction num PEs throughput mac cycle MACs multicast reduction temporal reuse buffer KB explore context optimization multi cache estimate throughput input kernel precisely discus related loop nest notation reuse analysis compiler discussion future motivate observation optimize dnn accelerator microarchitecture internal dataflow crucial accelerator designer achieve performance efficiency introduce data centric directive specify dnn dataflows compact understand data reuse opportunity analytical model maestro estimate execution efficiency hardware dataflows evaluate analytical model relative MAERI eyeriss accelerator model highly consistent cycle accurate simulation report runtime soundness analytic model benefit dataflow choice dnn model focus dnn operator diverse preference dataflow hardware motivates adaptive dataflow accelerator heterogeneous accelerator finally demonstrate maestro exploration dataflows layer dramatically hardware preference layer DSE maestro enable DSE optimization skip invalid average DSE rate per future leverage maestro implement dataflow auto tuner optimal dataflow specify dnn model hardware configuration optimal dataflow extend infrastructure automatically generate rtl facilitate dnn acceleration