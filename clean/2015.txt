deeply embed application hardware within stringent constraint potential domain introduces significant inefficiency stem chip dram access model ideally model entirely chip however compression memory requirement model chip inference impractical due increase density emerge eNVMs promising MaxNVM principled sparse encoding protective logic fault prone mlc eNVM technology RRAM ctt enable highly efficient dnn inference reduction technique cluster sparse compression increase vulnerability fault limit capability mlc eNVM circumvent limitation improve storage density per minimal overhead protective logic tradeoff density reliability balance technique network reasonably chip naive eNVM highly optimize mlc memory reduce technique NVDLA grade cnn accelerator demonstrate reduce reduce per resnet inference CCS CONCEPTS hardware memory dense storage keywords eNVM memory neural network RRAM ctt introduction dnns everywhere wireless sensor node implant medical device deeply embed environment efficiency paramount dnn hardware accelerator fetch dram performance bottleneck limit inference efficiency ideally dnns entirely chip however aggressive compression capacity requirement unrealistic storage chip SRAM emerge embed non volatile memory eNVM technology promising eliminate dram inefficiency eNVMs capacity latency storage significantly dense SRAM eNVMs RRAM pcm ctt multi mlc capability multiple packed device increase density eNVMs perfect drawback decrease reliability latency achieve ultra density eliminate dram dnn inference aggressive mlc eNVM however mlc incur memory access fault rate eNVMs access typically SRAM however writes magnitude alter physical storage dnns robust issue infrequent update implicitly fault tolerant hypothesize mlc eNVMs significantly improve dnn inference efficiency chip eliminate dram access demonstrates mlc eNVMs highly efficient dnn inference chip consideration factor MaxNVM principled micro october columbus usa algorithm circuit maximize benefit efficient baseline algorithmic apply cluster prune sparse encoding csr bitmask dnn accommodate fault prone mlc storage optimize sparsity encode dnns storage density mlc eNVMs directly impact reliability storage medium reduce memory requirement without sacrifice accuracy sparsely encode vulnerable dense limit mlc eNVM density storage density apply protective mechanism ecc  propose programmed per mlc demonstrate efficacy approach evolve eNVM landscape evaluate fundamentally technology RRAM ctt dense mlc capable CMOS compatible ctt latency incurs  latency ctt RRAM faster latency efficient eNVM model built NVSim RRAM parameter fault model derive publish construct model project demonstrate RRAM ctt parameter fault distribution derive chip prototype demonstrate benefit MaxNVM conduct memory NVDLA  cnn accelerator demonstrate model vgg resnet reasonable chip codesign approach ctt per inference reduce per inference update frequently RRAM compromise magnitude faster approximately efficiency constrain chip memory within sweep percentage SRAM eNVM concludes balance eNVM SRAM ideal maximize performance efficiency research contribution fault prone mlc eNVMs sparse encode reduce memory requirement however tension sparse encode increase fault vulnerability limit efficacy MLCs reduce overall memory footprint sparse encode raw per configuration without accuracy loss increase storage density improve dnn fault tolerance protective logic  propose fault mitigation technique ecc judicious memory dnn decrease propose technique ecc overhead dnn storage optimal mlc reduction relative SLC eNVM propose memory NVDLA  cnn accelerator RRAM ctt approach cnns chip eliminate dram baseline NVDLA implementation mlc eNVMs enable per inference enable entirely chip resnet inference characterization publish eNVM proposal latency extrapolate characterize fix capacity MB latencyoptimized NVSim  modeling   EXAMPLES characteristic eNVM technology discussion foundation model approach specific memory evaluate technology parameter extract develop fault model performance estimate model evaluate MLCs maximal storage density minimal latency selectively evaluate correspond SLC competitive baseline comparison model eNVMs evaluate varied landscape non volatile memory device interested identify implementation achieve latency storage density proven ability advanced node dnn inference constrain compute context spin transfer torque stt memory phase memory pcm resistive ram RRAM trap transistor ctt memory summary implement eNVMs sufficient publish data purpose model span technology node memory array architecture crossbar  evaluate offs memory extrapolate memory characteristic generate equivalent MB memory array optimize delay NVSim stt boundary dense chip storage evaluate memory capability multiple per exclude spin transfer torque stt memory evaluation report bandwidth mlc implementation structure memory device restrict pcm array demonstrate recently technology maximal density MaxNVM micro october columbus usa characterization non volatile memory chip reference eNVM technology access device capacity macro latency latency RRAM CMOS RRAM CMOS RRAM diode 2GB mlc pcm CMOS pcm CMOS pcm pram diode 8GB stt CMOS eNVM proposal pcm configuration latency evaluate RRAM compelling RRAM diode access crossbar CMOS access traditional memory array architecture crossbar array access chip CMOS access device competitive latency overcome increase storage density via mlc program addition RRAMs CMOS access transistor demonstrate node evaluate optimistic RRAM memory evaluate maximum potential promising technology advance node ctt memory standard CMOS transistor memory measurement per mlc program fabricate chip absence dedicate access device ability advanced technology node ctt impressive density latency however latency multi ctt characterization previous demonstrate standard NMOS device effective embed non volatile memory memory refer trap transistor ctt standard gate CMOS transistor data trap gate oxide carrier injection hci alters threshold voltage device transistor programmed exhibit saturation ctt memory array fabricate industrial grade standard CMOS technology zero manufacturing addition ctt desirable memory array analogous programmable rom array latency device display leakage information preserve transistor threshold voltage retention independent apply voltage density extremely  rely transistor gate terminal access additional selector device benefit  expense undesirable limit deployment purpose compute task extremely latency potential fault rate  programmed iteratively chip schematic IV curve mlc programmed ctt photo ctt chip IV curve mlc programmed ctt histogram inject increment reading desire shift achieve additionally injection random inevitably distribution translates increase likelihood  despite limitation demonstrate ctt ideal enable efficient embed dnn inference careful architectural embed dnn update infrequently repeatedly inference dnns fault tolerant extend previous demonstration ctt multi programmed fabricate chip mlc ctt careful algorithmic architectural overcomes fault rate accompany mlc storage mlc ctt chip measurement feasibility fabricate mlc ctt structure commercial FinFET photo contains internal scan chain driver circuit mimic wordline driver additional detail report bump expose bitlines flexibly individual via external equipment micro october columbus usa performance evaluation accelerator performance model NVDLA  fault injection memory characterization NVSim prune cluster error correction mitigation chip measurement circuit simulation previously publish sparse encoding dnn model estimate optimal storage per eNVM characterize eNVM array performance technology technique evaluation eNVM fault model MaxNVM summary optimization intermediate evaluation distribution wordline voltage programmed  mlc programmed unique device distribution nominal wordline voltage cluster corresponds  intrinsic vth variation program exhibit tighter distribution due iterative approximate gaussian distribution encode per distribution tend overlap increase probability  programmed wider distribution intrinsic variation  programmed minimize error  fault model develop fault model eNVM implementation focus primary source uncertainty intrinsic randomness associate programmed memory circuitry intrinsic distribution RRAM implementation extract publish data mlc program mlc ctt inter fault rate directly distribution chip technology distribution model gaussian distribution overlap distribution determines rate  adjacent programmed spice simulation derive distribution output  voltage converter likelihood  mlc ctt RRAM fault rate mlc dnn model baseline classification error compute error bound model lenet vgg vgg resnet dataset mnist cifar imagenet imagenet layer parameter classification error error bound cluster index sparsity zero MB MB MB MB prune cluster KB MB MB MB csr KB MB MB MB bitmask KB MB MB MB error typically adjacent data encode MLCs explore impact per dnn classification error fault mitigate additionally circuitry mlc fault rate focus specific amplifier SA characterize static input refer offset primarily input differential transistor therefore evaluate input refer offset voltage monte carlo spice simulation variety input transistor width simulation SA overhead incur overall array exceeds inherent inter fault rate alter readout architecture parallel scheme flash adc bitline sas per SA appropriate reference voltage parallel scheme decodes conversion sas increase exponentially overhead mitigate multiplexing memory array integrate NVSim characterize overall memory architecture evaluation methodology propose principled incorporates optimization technique algorithmic architectural develop fault model technology specific device characteristic spice model circuitry  fault injection framework quantify impact fault dnn accuracy leverage model performance propose interaction contribute evaluation summarize model optimization propose memory evaluate competitive realistic baseline enforce iso accuracy incorporate dnn optimization summarize dnns cnn model mnist handwritten digit dataset cnns vgg resnet imagenet dataset another  topology mid cifar dataset span gap  probability non adjacent MaxNVM micro october columbus usa iso training previous trading accuracy efficiency convincing demonstration practical address preserve baseline model accuracy guarantee via iso training  intuition accuracy varies dnns repeatedly identical hyperparameters variance accuracy bound classification error model alteration error exceed bound indistinguishable  therefore maintain iso accuracy performance improvement maintain model accuracy within  bound simplification technique magnitude prune retrain sparsify dnn model widely parameterized proportion zero popular technique reduce dnn reduce precision fix quantization dynamic dnn integer fractional drastically reduce per loss model accuracy another reduce cluster layer dnn model within layer unique cluster loss accuracy encode cluster index per layer index cluster strictly per fix quantization without significant training dnns sparse encoding employ lossless sparse encoding reduce storage strictly beneficial traditional memory technology SRAM however sparse encode longer particularly fault tolerant due vulnerability encode structure resilience therefore additional analysis optimal mlc storage scheme sparse encoding explore compress sparse storage csr csr data structure encode sparse matrix non zero data index nonzero index counter non zero per counter relative overhead csr varies proportionally sparsity csr apply per layer basis worthwhile convolution layer typically filter width height channel mapped csr NVDLA specific requirement data format convolutional core dictate compatible mapping bitmask sparse format NVDLA framework natively sparse storage utilizes indicator bitmask zero non zero data packed byte align refer  encode  maintain compatibility define NVDLA sparse format effective compression bitmask vector IdxSync counter data index retrieve index synchronization index synchronization sync index counter reconstruction bitmask sparse decode index retrieve index synchronization IdxSync prevents error bitmask propagate dynamically index non zero counter non zero data decode IdxSync fault corrects misalignment subsequent encode relative overhead sparsity error correction mitigation fault tolerance sparse encoding explore strategy mitigate error vulnerable structure per SLC mlc additionally explore beneficial incorporate ham style parity ecc configuration ecc nand flash binary encode mlc  fault equivalent flip cod ecc MLCs enable correction overhead error mitigation  encode propose counter non zero zero byte align error bitmask decode counter dynamically update index packed data array error mitigation strategy index synchronization bitmask IdxSync IdxSync error prevents error previously propagate matrix reconstruction demonstrate decode memory model integrate memory definition NVSim performance measurement chip publish spice simulation circuitry topology NVSim evaluates memory configuration capacity optimization target per pareto optimal chosen NVSim output within performance constraint NVSim baseline NVDLA parameter NVSim parameter NVDLA baseline NVDLA NVDLA data width convolutional buffer KB KB MACs mat SRAM capacity KB MB optimization target frequency ghz 1GHz latency datapath EDP SRAM BW GB GB dram BW GB leakage LPDDR dram micro october columbus usa architecture performance model nvidia accelerator NVDLA grade source architecture accelerate dnn inference baseline parameter diagram NVDLA cnn FC layer execution computation perform datapath demonstrate propose memory integrate highlight opportunity eNVM integration exist efficient dnn inference fault tolerance sparse  dnn inference model optimization sparse storage scheme significantly impact dnn fault tolerance carefully developed mlc eNVM fault model enable dense efficient storage previously validate application fault injection framework quantify impact encode strategy dnn classification error incorporate error correction mitigation technique maximize effectiveness mlc eNVM storage dnn inference fault injection framework validate application fault injection framework quantify dnn fault tolerance modify model mlc eNVM fault accommodate sparse encode simulate error mitigation generate inter fault probability mlc configuration technology fault probability modify amp shift distribution accord characterize input refer offset fault injection perform convert mlc representation eNVM sample gaussian random variable appropriate distribution threshold fault memory update adjacent modify perform inference entire dataset trial average unique generate fault sparse encoding fault injection structure bitmask non zero per structure implement ecc ensure fault correlate correctable flip error cod binarized MLCs dynamic error correction mitigation strategy integrate fault detect update appropriate prior evaluate model accuracy enables evaluation dnn classification error randomly trial various mlc eNVM configuration encode strategy analysis definitively optimal per mlc minimal memory encode strategy dnn loss accuracy impact lightweight error correction ecc mitigation IdxSync classification error sample dnn mnist lenet data structure ctt SLC mlc mlc assume perfect storage structure isolate impact fault ecc enables storage mlc csr format ecc IdxSync enable mlc storage bitmask encode IdxSync lighter overhead mechanism vulnerability encode strategy sparse encode strategy dnn reveal compact data storage tolerant inter fault dnns safely cluster index mlc longer safely mlc sparse encode structure layer  csr encode ctt storage mlc preserve accuracy improvement dense storage mlc instance due extreme vulnerability sparse encode data structure highlight mnist lenet demonstrates counter index data structure csr exhibit exceptionally vulnerability mlc storage roughly equivalent fault per counter structure pronounce degradation dnn accuracy  offset reading non zero data array remain data incorrectly assign reconstruction similarly index relative index previous non zero within  offset remain data impact restrict vulnerability index mitigate absolute index strictly overhead integrate lightweight ecc relative csr bitmask style sparse encode vulnerable portion flip bitmask remain non zero data assign reconstruct matrix bitmask cannot safely MLCs without protective technique impact error correction mitigation technique index counter ecc error reduction ham style parity ecc employ error csr structure amount relative ecc overhead per dnn correctable error encounter consistently MaxNVM micro october columbus usa minimal eNVM per dnn per encode strategy loss classification accuracy mlc ctt mlc RRAM orange SLC storage exhaustive exploration combination per evaluate trial eNVM technology overhead configuration parity KB counter sufficient safely mlc ctt ecc storage overhead strictly per layer dnns additionally ecc error error detect sec ded probability ded model imagenet vgg probability standard standard memory accepts risk classification error sample model ecc IdxSync enable mlc bitmask without degrade model accuracy bitmask account significant portion storage per dnn layer enable denser storage structure significant benefit dnns apply IdxSync sufficient enable MLCs bitmask configuration storage overhead ecc reduces decoder complexity ham style ecc however optimal storage strategy per dnn depends model per layer sparsity per desire optimization target dictate performance continually leverage fault tolerance propose memory optimal storage configuration described dnn style sparse encode without error correction mitigation fault model mlc ctt mlc RRAM minimal memory dnn storage configuration summarize reflection limitation mlc storage due fault rate technology determines effective capacity per characterize memory array saving stem sparse encoding pack per enable error correction mitigation minimal memory validate importance exhaustive exploration encoding error protection scheme optimal mlc eNVM storage bitmask sparse encode index synchronization  IdxSync imagenet vgg memory without error mitigation despite overhead counter IdxSync enables mlc storage bitmask data decrease reduces chip memory although storage capacity resnet csr prune cluster memory without sparsely encode csr prune cluster safely per mlc vector non zero data leverage mlc without loss classification accuracy however minimum memory resnet mlc ctt mlc RRAM bitmask sparse encode index synchronization mitigate propagation error bitmask chip dnn inference completely inference accelerator chip eNVM external dram envision scenario important deeply embed application iot device implant medical device component performance constraint extreme evaluate performance ctt RRAM per dnn model optimization per encode perform fault tolerance analysis determines appropriate per minimum memory without loss classification accuracy extension NVSim characterize memory array parameterized accommodate model optimal encode strategy finally pareto optimal characterize memory eNVM proposal identify per model minimize delay performance evaluate NVDLA performance model performance baseline configuration rely chip dram storage summary performance optimal storage eNVM evaluate NVDLA performance resnet inference leverage various eNVM micro october columbus usa memory interface convolution core MACs convolutional buffer KB KB fix DLA datapath component dram chip SRAM MB MB baseline NVDLA memory interface convolution core MACs convolutional buffer KB KB fix DLA datapath component SRAM MB MB mlc eNVM chip inference memory interface convolution core MACs convolutional buffer KB KB fix DLA datapath component chip memory SRAM eNVM dram chip hybrid memory schematic NVDLA without eNVM interface baseline SRAM intermediate storage chip dram storage chip memory eNVM storage hybrid partition fix chip memory dram overflow optimal per memory characterize chip optimistically mlc RRAM optimistic mlc RRAM mlc ctt mlc extrapolation SLC RRAM quantifies increase enable entirely chip memory maintain competitive performance achieve reduce reduce per inference cycle execute inference per input image frame compute NVDLA performance model characterize bandwidth latency NVSim eNVM array chip SRAM NVDLA KB MB designate NVDLA configuration manage storage intermediate layer dnns latency ctt prohibitively magnitude mlc RRAM SRAM latency explore quantify relative impact non volatility per inference evaluate performance fix budget dynamic interface eNVM replaces NVDLA interface chip dram dnn demonstrate aggressive mlc configuration ctt optimistically RRAM sparsely encode ecc imagenet vgg MB capacity respectively exhaustive exploration rigorous evaluation scheme dnn reasonably chip mlc eNVM equivalent MB SRAM node SLC RRAM resnet MB capacity sparse encode additional model optimization relaxed error bound mlc configuration leveraged aggressively loss dnn classification accuracy strictly avoids loss accuracy model consume reasonable chip local memory storage mlc eNVM summarizes dynamic per access distinct chip memory model delay optimal characterize extension NVSim dnn parameter optimal encode memory technology relative optimize sparse encode SLC RRAM mlc ctt array average maintain competitive performance within NVDLA baseline furthermore mlc extension SLC RRAM achieves average benefit optimistically mlc RRAM optimistic mlc RRAM enables average benefit relative benefit mlc ctt optimistic  cifar vgg derives inherent storage density optimistic mlc RRAM per safely optimistic mlc RRAM consistently per mlc ctt per due respective fault characteristic impact classification error expose dynamic memory proposal varies magnitude equivalent capacity characterize array mlc ctt consistently MaxNVM micro october columbus usa NVDLA per resnet inference average frame per configuration NVDLA described replace LPDDR dram baseline propose eNVM leverage specific memory array characterize resnet per access optimistic mlc RRAM tends maintain latency bandwidth GB performance mlc RRAM proposal surpass maximal fps NVDLA performance model consistently exceeds frame per NVDLA configuration image processing frame rate standard latency overhead mlc tends negate effective bandwidth increase mlc storage SLC RRAM competitive mlc ctt frame inference per characterize eNVM array optimistic mlc RRAM exhibit bandwidth latency mlc ctt fps performance estimate decode overhead sparse encoding cluster index however overhead reconstruct decode strategy minimal consistent baseline eNVM average consumption per inference resnet operating maximum performance fix NVDLA baseline datapath configuration consumption publicly available assume additional consumption LPDDR dram 1GHz conservative estimate potential improvement via integration onchip eNVM NVDLA representative extremely resource constrain dnn accelerator consumption memory access fetch MaxNVM reduce dram chip mlc ctt overall average reduction NVDLA considerably performance dynamic per inference average scenario trend mlc ctt reduce consumption due fetch however due baseline convolutional core buffer relative reduction evaluate mlc proposal mlc ctt achieves performance per inference optimistically mlc RRAM bandwidth per overall per inference mlc ctt NVDLA demonstrate benefit relative performance characteristic convolutional model highly optimize cnn accelerator datapath maximizes fetch reduction due memory fetch increasingly beneficial resource constrain context exhibit fetch parameter recurrent neural network benefit non volatility frequently inference frame rate image processing task eNVMs inherent relative benefit virtue reload alternatively dram avoid summarizes average per inference evaluate mlc eNVM baseline remain fully retain dram conservative estimate wake inference dram wake baseline  overhead load dnn memory dram increase execution load dnn layer inference task computation applicable eNVMs retain inference dram constant frequency inference increase fps average per inference decrease spends idle conversely  inference average per inference constant frequency fps met NVDLA clearly fps frame rate fps relevant detection task security camera efficient wake baseline per inference propose mlc eNVM maintain relative mlc RRAM mlc ctt reduce per inference saving typical frame rate video conferencing standard image processing task fps average per inference baseline approach performance mlc eNVM alternative frame rate operating fps mandate micro october columbus usa chip mlc eNVM LPDDR dram average per resnet inference frame rate virtual reality headset fps mlc eNVM proposal achieve mlc RRAM mlc ctt per inference optimize mlc eNVM particularly compelling application frame rate requirement benefit exaggerated frequent wake ups hybrid memory dnns tend improve accuracy increase model dnn chip eNVM hybrid fix chip memory budget split SRAM eNVM activation chip fetch dram chip memory budget accommodate MB SRAM various NVSim optimization target eNVM dnns within constraint dnn  highlight performance impact incorporate amount chip eNVM eNVM cache dram chip mlc eNVM dram mutually exclusive model fed directly accelerator datapath partition fix chip memory SRAM intermediate storage mlc eNVM summary optimal storage per eNVM proposal characterize per dnn BPC max per MB max capacity fps max frame per NVDLA latency eNVM array model memory tech encode BPC MB fps cifar opt mlc RRAM  IdxSync mlc ctt bitmask mlc RRAM  IdxSync SLC RRAM bitmask vgg opt mlc RRAM csr ecc mlc ctt csr ecc mlc RRAM csr ecc SLC RRAM csr resnet opt mlc RRAM  IdxSync mlc ctt  IdxSync mlc RRAM  IdxSync SLC RRAM bitmask relative performance vgg chip split eNVM SRAM storage reduce costly dram access drastically increase chip memory capacity partition NVSim characterize maximal capacity minimal latency within constraint pareto optimal chip memory partition inference execution bottleneck fetch dram eNVM traffic intermediate SRAM extend NVDLA performance model selectively eNVM dram maximize eNVM benefit greedily otherwise dram bottleneck layer initial benefit alleviate retrieval dram bottleneck amount eNVM allot dram access reduces access eNVM magnitude access dram per inference onchip devote mlc eNVM technology however performance sharply degrades chip SRAM storage longer intermediate convolutional core execution becomes bottleneck fetch activation dram characteristic mlc eNVMs sufficient buffer intermediate inference explore discussion performance per model emphasize comprehensive codesign methodology storage density mlc advantage due non volatility unoptimized dnns reasonably fault tolerant application leverage memory however optimal encode strategy mlc configuration error mitigation technique varies model eNVM technology rigorous evaluation per dnn methodology effectively executes maximizes efficiency gain latency approximate characterize memory array model entire eNVM scenario latency previous application MaxNVM micro october columbus usa optimistic dnn model memory technology approximate cifar opt mlc RRAM mlc ctt mlc RRAM SLC RRAM resnet opt mlc RRAM mlc ctt mlc RRAM SLC RRAM vgg opt mlc RRAM mlc ctt mlc RRAM SLC RRAM resource constraint reasonable dnn desire frequency rewrite constrain endurance memory however sensor node mobile device constraint driven device dnn inference workload periodic synchronization permissible related research concern specialized chip accelerator dnn image classification prevalent across propose chip dram access consistent performance efficiency bottleneck levy optimization sparsity external mitigate chip SRAM dense embed dram incurs frequent refresh cycle limitation motivate target alternative storage dnns related application eNVMs abundant prior exists related fault tolerance dnns fault tolerance confirm expound upon previous additionally resiliency sparsely encode dnn demonstrate extent fault tolerance dnn sparse encoding impact error correction mitigation technique eNVM technology overview principle eNVMs consists replace  storage inherently volatile ability encode information alter electrical device implementation exist proposal emerge mlc capability ctt applicability dnn storage explore preliminary proof concept without consideration evaluation performance advantage dnn inference storage fix cluster MLCs sparse encoding error protection analog eNVM application secondary encode information physical characteristic device programmable strictly binary scenario  device analog memory leveraged reconfigurable interconnects consequence eNVMs analog memory computation perform directly analog domain basis propose neuromorphic mapped directly resistive device crossbar array perform memory matrix vector multiplication crossbar array implement RRAM pcm limited array recently neuromorphic array propose promise scalability architecture however application dnns demonstrate numerous neuromorphic proposal leveraged eNVMs particularly RRAMs processing memory architecture conclusion comprehensive methodology MaxNVM reveal collection observation offs integration eNVMs enable efficient dnn inference publish measurement compelling mlc programmed eNVM maximize storage density MaxNVM framework evaluate codesign choice circuit algorithm evaluation propose eNVM technology model encode strategy error mitigation rigorously explore offs storage density memory reliability performance accuracy dnn inference various constraint component MaxNVM publicly available empower continued research essential offs evaluate propose eNVMs application driven context demonstrate sparse encode strategy implication fault tolerance dnns propose integrate lightweight protection mechanism maximize per mlc eNVM storage without accuracy loss demonstrate mlc encoding saving relative SLC eNVM emphasize benefit exhaustive exploration finally evaluate dnn inference accelerator integrate propose mlc eNVM per inference respectively maximum frame rate image classification task per inference frame rate