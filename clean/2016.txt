recurrent neural network rnns technology application automatic recognition machine translation unlike conventional dnns rnns information improve accuracy future prediction therefore effective sequence processing application recurrent layer execute processing potentially sequence input image audio frame etc observation output neuron exhibit consecutive invocation exploit neuron fuzzy memoization scheme dynamically cache output neuron reuses whenever predict output previously compute avoid output computation challenge scheme neuron output input sequence recently compute extend recurrent layer simpler bitwise neural network bnn bnn rnn output highly correlate bnn output correspond output rnn layer likely exhibit negligible bnn effective mechanism fuzzy memoization apply impact accuracy evaluate memoization scheme accelerator rnns variety neural network multiple application domain technique avoids computation saving speedup average CCS CONCEPTS computer organization neural network compute methodology machine keywords recurrent neural network memory binary network memoization introduction recurrent neuronal network rnns sequence processing recognition machine translation automatic caption generation surprisingly data recently publish around machine workload google datacenters rnns whereas convolutional neuronal network cnns application unlike cnns rnns information previously input improve accuracy output variable input output sequence although rnn training perform efficiently gpus rnn inference challenge batch input sequence per batch data dependency recurrent layer severely constrain amount parallelism hardware acceleration achieve performance efficient rnn inference rnn accelerator recently propose neuron rnn recurrently execute processing input sequence analysis output reveals neuron output consecutive input sequence average relative difference previous output neuron rnns whereas previous report rnns inherently error tolerant propose exploit aforementioned computation neuron fuzzy memoization scheme approach output neuron dynamically cached local memoization buffer output predict extremely previously compute neuron output memoization buffer recalculate avoid correspond computation memory access micro october columbus usa  threshold WER loss computation reuse  WER loss computation reuse threshold WER loss computation reuse  WER loss computation reuse threshold accuracy loss computation reuse imdb sentiment accuracy loss computation reuse threshold bleu loss computation reuse machine translation bleu loss computation reuse accuracy loss rnns versus relative output error threshold oracle predictor difference previous output predict threshold memoized output employ instead calculate potential benefit memoization scheme oracle accurately predicts relative difference output neuron previous output memoization buffer memoized difference threshold axis rnns tolerate relative error output neuron negligible impact accuracy threshold memoization scheme oracle predictor computation challenge memoization scheme predict difference output previous output memoization buffer without perform correspond neuron computation propose extend recurrent layer bitwise neural network bnn reduce input described bnn output highly correlate output recurrent layer bnn output indicates likelihood rnn output although bnn output rnn output bnn extremely hardware friendly effective predict memoization safely apply simply input predict input output accurate input introduce significant output neuron bnn approach account input propose neuron hardware fuzzy memoization scheme output neuron execution dynamically cached memoization output correspond bnn input sequence bnn compute bnn output memoization difference bnn output cached output threshold neuron cached output output avoid associate computation memory access rnn otherwise neuron evaluate memoization update bnn accuracy loss report elsewhere completely approach bnn predict memoization safely apply negligible impact accuracy inexpensive bnn compute sequence neuron whereas rnn evaluate demand bnn maintain accuracy rnn computation contribution evaluation output neuron recurrent layer exhibit consecutive execution propose fuzzy memoization scheme avoids neuron evaluation reuse  compute memoization buffer propose bnn memoization apply impact accuracy bnn rnn output highly correlate neuron fuzzy memoization rnns micro october columbus usa structure lstm denotes elementwise multiplication vector denotes hyperbolic tangent structure gru implement neuron memoization scheme rnn accelerator hardware introduces negligible overhead speedup saving average rnns background recurrent neural network recurrent neural network rnn machine approach achieve tremendous application machine translation video description characteristic rnns loop recurrent connection information persist execution hence potential unbounded context information future prediction another important feature rnns recurrently execute input sequence handle input output variable characteristic rnns effective framework sequence sequence application machine translation outperform neural network dnns rnn architecture capture exploit dependency input sequence however capture dependency challenge useful information tend dilute exploit dependency memory lstm gate recurrent gru network propose rnns successful widely rnn architecture achieve tremendous variety application recognition machine translation video description subsection detail structure behavior network rnns rnns compose multiple layer stack rnns layer consists lstm gru addition layer unidirectional bidirectional unidirectional layer information prediction whereas bidirectional lstm gru network future context input sequence compose lstm gru direction backward layer bidirectional rnns input sequence evaluate backward direction lstm structure lstm component memory update fully layer neural network gate input gate computation equation decides input information forget gate equation determines information erase updater gate equation amount input information candidate update gate execute update accord equation finally output gate equation decides amount information emit output computation lstm neuron gate connection connection recurrent connection input evaluation neuron gate dot connection another dot recurrent connection peephole connection bias apply computation activation function typically sigmoid hyperbolic tangent gru analogous lstm gru gate information inside however gru independent memory gru update gate candidate information update activation reset gate modulates amount information remove previous compute grus output gate hence expose timestep computation gate gru equation omit