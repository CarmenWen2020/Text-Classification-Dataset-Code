memory dictate workload memory requirement memory underutilized workload cognizant memory underutilization prior memory utilization explore improve context perform systemlevel memory utilization context hpc machine measurement across hpc memory underutilization hpc severe subsequently perform exploration architectural technique improve memory utilization specifically hpc propose expose compute node currently unused memory cpu via novel architectural OS enable microarchitecture technique abundant memory boost microarchitecture performance transparently without user code modification recompilation refer memory aware microarchitecture technique FMTs detailed fmt memory aware memory replication FMR average across hpc benchmark suite FMR performance improvement highly optimize baseline representative memory performance benefit simulation report emulate FMR  simulation emulation FMTs applicability CCS CONCEPTS hardware dynamic memory computer organization grid compute software engineering memory keywords memory architecture memory management hpc operating dram supercomputing introduction physical memory worstcase memory usage scenario memory requirement however memory usage varies workload memory workload memory prior quantify memory underutilization context correspondingly prior explore technique mitigate memory underutilization propose intelligent colocate heterogeneous workload memory hungry database workload compute intensive workload machine improve memory utilization  memory computation workload machine   VM memory   micro october columbus usa  zhang pang workload program memory computation disaggregated memory  perform hpc memory usage compute node physical memory usage encompass everything memory OS disk buffering file cache user etc span operation production hpc los  national laboratory virginia tech machine observation billion memory usage measurement average memory usage active node user memory utilization hpc accord prior memory utilization enhancement technique effective OS file cache workload location ineffective hpc hpc workload typically  storage intensive database workload minimizes effectiveness file cache hpc workload highly parallel workload already sufficient thread core compute node workload colocation unsuitable hpc hpc national laboratory deliberately disallow colocation independent workload compute node minimize negative impact increase thread  performance variation parallel workload resultant inter workload interference perform exploration architectural technique improve memory utilization hpc propose expose compute node currently unused memory cpu via novel architectural operating user transparent manner user code modify  memory aware hardware leverage hpc abundant memory arbitrary data boost microarchitecture performance detailed memory aware microarchitecture technique memory aware memory replication FMR FMR effectively hide dependent memory latency memory location replicate logical memory cpu fetch memory location faster llc FMR readily deployed commodity memory commodity memory chip commodity memory module applicability contribution perform memory utilization hpc hpc suffer severe memory underutilization explore architectural technique address memory underutilization hpc propose expose compute node OS visible memory node cpu improve performance propose concept  microarchitecture technique FMTs opportunistically arbitrary data memory location boost microarchitecture performance detailed fmt memory aware memory replication improves performance systemlevel efficiency average across hpc benchmark suite performance benefit simulation report emulate FMR  simulation emulation  memory underutilization hpc SYSTEMS prior hpc workload characteristic program memory usage quantifies memory usage memory usage memory OS disk buffering file cache user etc memory utilization hpc grizzly badger los  national laboratory lanl cascade virginia tech describes hpc raw measurement data lanl publicly available http  lanl gov data LA UR php hpc grizzly midrange supercomputer deploy  slurm scheduler node memory usage  monitoring    output report memory node currently completely user OS disk buffering file cache anything refer idle memory memory compute node memory usage calculate node physical memory minus memory breakdown active node maximum amount memory across interval grizzly maximum memory usage node active 2GB interval refer node description utilization compute node user average across node cluster compute compute hardware memory per node per node node utilization grizzly core 8GB badger core 8GB core 8GB cascade core 8GB node node node node 2GB 4GB 7GB 0GB 3GB 8GB grizzly node badger node node cascade node distribution active node hourly memory usage node hourly memory usage maximum usage hourly memory usage active node grizzly 2GB 8GB node quantify memory underutilization hpc improve performance via architecture micro october columbus usa user active node filter node interval node average node memory utilization active node grizzly badger cascade respectively active node average memory equally weigh node memory utilization node maximum memory utilization percentile memory utilization node memory utilization interval memory utilization node interval percentile memory utilization considers active interval node user gap node maximum memory utilization memory utilization memory utilization report memory usage account memory exist OS optimization important OS optimization opportunistically memory transparently cache access file OS community optimization disk buffering cache etc OS file cache clearly distinguish cpu cache OS file cache expand occupy memory accumulate file access node particularly intrigue heavily utilized none enforce cap OS file cache empirically verify measurement hpc node OS file cache typically grows slowly unlike data workload  computation access file hpc workload tend compute intensive compute reading input file input node participate distribute directly mpi message passing file therefore insert participate node OS file cache exception checkpoint file fault tolerance however network file nfs sys node memory usage normalize memory percentile percentile max memory usage node grizzly node badger node cascade maximum percentile percentile memory utilization node active vertical slice belong distinct node node within sort percentile utilization  lustre commonly hpc file eviction client cache preserve cache coherence hpc node OS file cache shrink multiple input file compress archive delete preserve precious storage storage server delete file evicts content OS file cache allocate amount memory OS evicts cached file program memory OS  evict file program avoid costly storage network access overhead OS  evict file benefit access file executables individual user file node available user submits hpc growth rate OS file cache couple factor shrink OS file cache hpc potential address memory underutilization unused memory memory utilized however memory reduce performance reduce memory rank parallelism furthermore memory voltage steadily decline reduce memory contribution another potential reduce memory however reduces maximum solvable therefore reduces hpc capability important metric merit hpc prior technique colocate heterogeneous workload node improve memory utilization inadequate hpc individual hpc workload highly parallel therefore occupy core node deliberately thread individual workload across node colocate workload thread node increase network communication overhead therefore reduces parallel performance tend parallel workload hpc workload sensitive performance variation thread significantly execution parallel program multiple workload node network access cpu budget etc workload colocation substantial performance variability performance isolation hpc national laboratory deliberately disallow workload colocation architectural OS expose memory cpu ideally OS utilize hpc abundant unused memory effectively boost performance OS data database workload via OS file cache propose novel architectural OS expose node OS visible memory cpu enable memory aware microarchitecture technique FMTs microarchitecture technique opportunistically arbitrary data memory boost microarchitecture performance advanced configuration micro october columbus usa  zhang pang interface acpi OS exist already inform cpu underutilization various hardware resource cpu opportunistically boost microarchitecture performance OS instruct cpu core cache via acpi conceptually cpu core cache allows cpu exploit knowledge opportunistically boost microarchitecture performance turbo boost frequency core observation propose piggyback acpi enable OS inform hardware physical memory location currently software overview OS propose architectural OS maintains variable continuous memory address within OS communicates continuous memory hardware piggyback exist OS acpi interface cpu leverage OS expose memory arbitrary data boost microarchitecture performance describes architectural OS describes detailed fmt enables architectural acpi defines processor hardware register OS processor similarly propose processor hardware memory register OS continuous memory address within processor physical memory address register upper address continuous physical memory FMTs autonomously arbitrary data arbitrary location within continuous physical memory register refer register cpu visible memory register  register refer physical memory cpu visible OS expand shrink cpu visible update  via acpi runtime node cpu visible expand gigabyte almost entire memory software memory usage expand cpu visible OS acpi address upper address  acpi quickly simply  hardware acpi register manage cpu mode update within microsecond afterwards cpu asynchronously initializes cpu visible without interrupt program spare bandwidth initialization fmt initialize linearly cpu uninitialized via register allows FMTs access initialize memory parallel cache llc evict dirty physical address within cpu visible program handle request llc sends MC dirty eviction MC simply request address within  physical freed virtual memory shrink cpu visible allocate physical memory request OS acpi address upper address  cpu visible cpu memory aware microarchitectures cpu visible memory  register opportunistically performance boost data piggyback acpi regular KB OS overview propose architectural OS expose memory hardware acpi quickly simply  afterwards FMTs cease data physical address outside update address  content away cpu visible preserve opportunistically data exist without  OS zero physical allocate program exist OS zero physical allocate enforce inter memory protection OS architectural OS cpu visible runtime OS expands cpu visible periodically periodic expansion enables node hpc expand cpu visible minimize OS jitter concern hpc propose expand cpu visible OS compact physical outside cpu visible acpi expand widen continuous memory  8GB node expand cpu visible limit maximum amount data migrate per node per 8GB pessimistically estimate compact 8GB memory 8GB 4GB 4GB memory compaction throughput via  detail translates overhead overhead evaluation OS shrink cpu visible memory allocation request OS satisfy memory allocation request OS decrease cpu visible physical memory away cpu visible satisfy memory allocation request OS shrink cpu visible acpi address  OS away cpu visible maintain contiguity OS regular acpi shrink cpu visible allocation expensive OS effectively handle overhead reduce memory compaction exist OS feature MB 1GB memory improve tlb rate program migrate shift compact physical address regardless regular maximum data movement 8GB quantify memory underutilization hpc improve performance via architecture micro october columbus usa cpu visible MB therefore acpi per MB KB allocation OS deduct MB regular quickly satisfy future allocation request discussion node multiple cpu socket node OS maintains cpu visible cpu cpu typically assign contiguous physical memory address address cpu cpu visible within cpu physical memory address another challenge migrate continuous cpu visible virtual kernel  boot physical address within 4GB reserve physical address 4GB memory mapped MMIO backward compatibility legacy address issue OS cpu cpu visible upper address physical address cpu maximum physical address cpu visible downward decrease address memory aware microarchitecture due fundamental tradeoff computation expose memory hardware enable microarchitecture technique boost performance describes detail microarchitecture  memory replication FMR dram access latency heavily dependent dram location llc llc incurs dram refresh latency memory location request currently refresh memory location replicate memory  across dram location hide dependent latency cpu location faster llc organize background dependent latency dram describes hide dependent latency request describes efficiently memory FMR describes memory layout detail background dependent latency refresh latency tRFC dram periodic refresh dram leak DDRx memory chip hpc refresh per rank basis rank memory chip access lockstep rank cannot access refresh bus turnaround delay   rank cpu reconfigure rank circuitry mode rank bus turnaround similarly reading rank cpu reconfigure rank mode prevent frequent stall due frequent bus turnaround typically batch microarchitecture parameter exploration batch maximizes average performance baseline memory addition minimize bus turnaround batch scheduler improve request rate request access already unfortunately batch stall request delay tRRD  tFAW cpu activate dram rank tRRD activate rank similarly cpu activate rank within tFAW tRRD tFAW memory chip constraint delay tRCD precharge delay trp prior access dram cpu activate dram delay furthermore cpu activates dram otherwise cpu issue precharge command trp activate reading memory FMR hide dependent latency FMR logical location rank enables MC fetch rank faster llc avoid inconsistency cache hierarchy MC insert cache hierarchy llc physical address MC fetch replicate location refresh nonblocking request exist typically refresh rank memory channel rank connection memory bus processor memory reside rank channel inaccessible due refresh MC completely hide refresh latency llc satisfy reside rank currently refresh batch nonblocking request prevent batch writes request MC rank channel rank mode MC another rank satisfy llc latter former mitigate tRRD tFAW logical rank MC freedom fetch rank tRRD tFAW constraint already met satisfy llc sooner mitigate tRCD trp logical rank therefore mitigate precharge induced stall MC currently instead reading currently identical content due memory replication MC improve rate thereby mitigate activation induced stall scenario MC purposefully cease speculatively without suffer increase conflict rate llc buffer policy policy timeout policy predict currently access future predict MC speculatively micro october columbus usa  zhang pang rank refresh mode req via rank  via rank precharge activate activate rank req cmd rank cmd activate rank met tRRD tFAW neither rank  rank req via rank req via rank  req cmd precharge access recently schedule request schedule speculative precharge predict via default buffer policy rank content currently precharge refresh FMR schedule choice request buffer policy access currently future llc minimize speculatively strictly improves rate currently llc MC recently access recently access longer strictly improves rate summarizes schedule choice FMR request speculative precharge operation buffer policy memory FMR hide refresh latency request request critical program execution stall request slows performance channel buffer cpu stall prevent cpu stall due request refresh rank clog buffer writeback cache channel llc channel buffer cache request refresh rank writeback cache storage buffer request unlike regular buffer memory scheduler scan writes increase rate request writeback cache later drain buffer writeback cache drain request buffer rank request currently refresh refresh 6GB ddr memory chip organize writeback cache KB associative writeback cache rely writeback cache rank hide stall due batching writes future access due future llc misprediction reduces rate however writeback cache associativity buffer highly associative fully associative writeback cache access access memory per access obtain cactus per memory access calculate micron ddr datasheet addr cmd bus chip rank data bus memory channel chip rank rank DIMM cpu rank memory controller organization memory channel rank dedicate chip CS writeback cache drain writes buffer exceeds watermark writeback cache selects rank occupancy drain buffer request belonging rank robin manner drain writeback cache drain writes longer rank buffer request logical memory replica MC update ensure consistency entry writeback cache update writeback buffer remove request currently false finally memory twice request memory bus utilization writes incur performance overhead intensive application address bandwidth overhead rank multiple rank channel bus bus interconnection topology benefit unique message broadcasting capability exploit chip network broadcast multicast coherence message propose exploit multicasting capability memory bus simultaneously update logical memory bus transaction thereby avoid bandwidth overhead replication server cpu physically broadcast message rank simultaneously bus logically directs message intend rank assert intend rank dedicate chip CS unintended rank ignore message bus CS  enhance cpu multicast message rank channel simply assert rank CS simultaneously transmit message confirm micron technologist commodity memory bus currently rank channel reuse rank incurs minimal impact bus signal integrity explains detail exploit memory bus multicasting simultaneously update logical bus transaction MC identical dram location ID ID ID across rank channel otherwise cpu issue command communicate dram location bus command subsequent data completely quantify memory underutilization hpc improve performance via architecture micro october columbus usa writes furthermore mapping identical dram location identical ID creates identical content optimization mitigate tRCD trp replica identical dram location across rank channel multicasting writes rank MC synchronize issue  command synchronization overhead however amortize writes batch unfortunately MC cannot multicast writes eliminate bandwidth overhead rank batch writes nonblocking benefit reduce bandwidth overhead benefit writes nonblocking bandwidth utilization propose dynamically switch mode accord instantaneous bandwidth utilization MC switch nonblocking writes multicast writes writeback cache becomes cannot incoming request llc MC switch multicast writes nonblocking writes writeback cache incoming request continuous memory layout detail cpu cpu visible cpu physical address assign physical address replicate logical physical address cpu instal physical memory assignment function socket multiple socket cpu MC quickly replicate within address cpu  register MC local  register lookup OS acpi expand cpu visible MC initialize physical memory newly cpu visible MC uninitialized cpu visible MC perform asynchronously background whenever slack memory bandwidth utilization reap performance benefit described MC rank furthermore multicasting writes MC identical dram location across rank channel finally simultaneously rank memory module exceed thermal budget limited memory module DIMMs rank MC preferably rank rank IDs rank per channel DIMMs preserve DIMM thermal profile multiple DIMMs per channel DIMM per channel multicasting writes DIMM configuration pin JEDEC compliant DIMM socket DIMM configuration maximum rank DIMMs rank DIMMs identical DIMM pin DIMMs peak confirm simultaneously rank commodity DIMM feasible micron technologist propose mapping satisfy requirement define address mapping terminology MC translates physical address dram location static mapped function physical address dram address collection function channel ank   compute memory channel ID rank ID ID ID ID collectively dram address brevity refer function ank collectively  propose transformation derive arbitrary ank differs ank   propose transformation restriction rank per channel ank periodic ank mod function interleaf across rank adjacent memory multiple clarity socket easily adapt socket multi socket CPUs preserve appearance socket purpose calculate physical dram address mapping within socket adaptation MC physical address  input  physical memory address assign socket derive ank visualize periodic sequence ank ank ank ank ank ank ank ank ank circular initial derive ank ank ank simply apply constant offset initialize periodic sequence periodic sequence concisely ank ank ank constant ank function ank mod mod transform ank ank obtain mod ank mod otherwise proof equality ank micro october columbus usa  zhang pang channel channel channel channel channel channel physical dram address mapping rank refer memory physical dram address mapping apply rank ID transformation ank physical dram address mapping apply transformation dram location IDs hash rank ID identical channel IDs ank pmod ank ank  ank mod mod therefore similarly  mod ank ank ank mod therefore graphically illustrates transform mapping scheme interleaf adjacent address across channel rank finally clarify propose address mapping function static cpu FMR switch discussion memory bus signal integrity simultaneously rank channel impact signal integrity memory command bus already broadcast command command bus dram chip rank memory channel unintended rank upon broadcast command chip  simultaneously rank channel impact signal data bus due modular chip external data bus insulate rank dram chip internal digital logic IO receiver decides bus signal ignore core core ghz OoO tlb entry entry rob split data instruction assoc cycle latency per core assoc cycle latency MB assoc latency stride prefetcher prefetch MC latency MC llc ddr channel rank channel rank xor address mapping entry queue chan entry buffer chan KB interleave across channel FR FCFS memory schedule policy timeout policy dram cycle evaluate baseline conventional rank dram array multicasting exist rank channel data pin IO receiver active gate IO receiver whenever rank idle incur costly latency overhead workload rank data pin termination ODT impedance affect signal bus rank ODT dynamic ODT rank ODT  exception handle tune  reconfigurable uncommon dynamic ODT application disabled memory configuration recent memory amd ryzen methodology simulate core cpu gem mode llc latency report recent server tlb entry tlb entry skylake processor simulate ddr memory incorporate ramulator gem dram timing parameter micron ddr datasheet JEDEC tRFC refresh latency generation future dram chip simulate channel rank per channel memory configuration hpc interleave adjacent physical address across channel rank xor mapping channel choice model processor summarizes simulated microarchitecture parameter evaluate hpc benchmark suite nasa parallel benchmark NPB graph hpcg linpack gap evaluate benchmark suite NPB EP gap evaluate input random input hpc benchmark identify representative simulation important benchmark gem kvm cpu native execution checkpoint benchmark program execution subsequently simulate quantify memory underutilization hpc improve performance via architecture micro october columbus usa kron twitter web bfs kron bfs bfs twitter bfs web kron twitter web kron twitter web kron wsg twitter wsg web wsg     linpack graph bfs hpcg NPB avg gap avg graph avg avg suite avg benchmark NPB gap bandwidth utilization bandwidth bandwidth workload characterization bandwidth utilization breakdown prefetch writes primary baseline kron twitter web bfs kron bfs bfs twitter bfs web kron twitter web kron twitter web kron wsg twitter wsg web wsg     linpack graph bfs hpcg NPB avg gap avg graph avg avg suite avg benchmark NPB gap rate demand rate demand conflict rate workload characterization rate conflict rate demand request baseline checkpoint benchmark memory bandwidth utilization program execution representative simulation checkpoint exhibit median bandwidth utilization across checkpoint cycle accurate simulation cache via atomic simulation predictor prefetcher via cycle accurate simulation report performance cycleaccurate simulation benchmark parallel thread mpi FLOPs float operation per performance float benchmark commit instruction characterizes memory bandwidth utilization benchmark chosen checkpoint baseline memory finally model cpu via McPAT model output linearly model model memory via  chip rank optimize primary baseline cheaper core simulation perform multi dimensional exploration prefetch policy policy policy timeout policy threshold buffer batch yield maximum average baseline performance evaluate benchmark implement baseline optimization switch channel mode pending request channel due refresh finally FMR evaluation FMR performance benefit depends significantly bandwidth utilization FMR performance benefit bandwidth utilization simpoint serial benchmark checkpoint verify research research contact checkpoint ensure scheme execute instruction cycle accurate warmup dram latency constant additional KB writeback cache per channel KB writeback cache baseline comparison improves baseline average performance optimistically model prior nonblocking memory refresh refresh latency setting identical primary baseline model prior duplicon cache statically reserve MB chip dram channel replicate data dram simulated cpu channel model MB duplicon cache evaluate FMR assume cpu visible fully replicate memory discus validity assumption FMR primary baseline prefetcher timeout buffer etc wherever RESULTS performance FMR normalize primary baseline FMR improves performance weigh benchmark suite equally weigh benchmark equally FMR improves performance average across average average benefit benchmark benefit FMR  kron kron benchmark characterize memory bandwidth utilization spatial locality benchmark benefit FMR linpack   benchmark characterize memory bandwidth utilization spatial locality performance nonblocking memory refresh normalize primary baseline nonblocking memory refresh FMR additional average performance improvement primary baseline average equally across benchmark suite average equally across benchmark respectively FMR another important benefit nonblocking memory refresh FMR deployed commodity memory commodity memory chip module whereas nonblocking memory refresh modifies processor memory interface protocol evaluate duplicon cache baseline MB duplicon cache simulated millisecond gem atomic cpu mode eliminate demand kron twitter web bfs kron bfs bfs twitter bfs web kron twitter web kron twitter web kron wsg twitter wsg web wsg     linpack graph bfs hpcg NPB avg gap avg graph avg avg suite avg benchmark NPB gap perf norm primary baseline FMR nonblocking refresh performance normalize primary baseline micro october columbus usa  zhang pang avg bfs avg avg avg avg avg linpack graph bfs hpcg avg suite avg benchmark rate NPB gap duplicon cache demand rate kron twitter web bfs kron bfs bfs twitter bfs web kron twitter web kron twitter web kron wsg twitter wsg web wsg     linpack graph bfs hpcg NPB avg gap avg graph avg avg suite avg benchmark NPB gap epi norm baseline cpu contribution dram contribution FMR per instruction epi normalize baseline stack epi stack cpu dram contribution epi rate duplicon cache simulated warmup average across benchmark equally average across suite equally rate average memory footprint hpc benchmark 0GB comparison average footprint benchmark duplicon cache evaluates 2GB mostly desktop workload duplicon cache improve rate expensive memory duplicon cache hidden unusable OS duplicon cache replicate memory via chip SRAM tag increase linearly duplicon cache duplicon cache performance benefit FMR ideal actual benefit nearly writebacks duplicon cache dirty evict MB llc almost MB duplicon cache writes memory nearly bandwidth duplicon cache cannot multicast writes duplicon incurs overhead memory request request duplicon cache insert duplicon cache duplicon cache described replicates across rank mitigate rank latency FMR node speedup FMR translate directly speedup distribute workload across multiple node however node faster distribute workload node network communication overhead amount avg bfs avg avg avg avg avg linpack graph bfs hpcg NPB gap percentage memory replica spent multicasting writes request satisfied memory memory location spent multicasting writes mode kron twitter web bfs kron bfs bfs twitter bfs web kron twitter web kron twitter web kron wsg twitter wsg web wsg     linpack graph bfs hpcg NPB avg gap avg graph avg avg suite avg benchmark NPB gap norm baseline demand rate demand conflict rate rate conflict rate demand FMR normalize primary baseline occupy node achieve workload performance occupy node improve workload efficiency node workload therefore improve hpc throughput cpu dram efficiency FMR reduces per instruction epi primary baseline although FMR increase dram idle due perform dram writes memory request FMR reduces epi cpu dram addition cpu idle dominates dynamic accord  reduction cpu idle due improve performance outweighs overhead due writes memory behavior analysis request satisfied duplicate memory FMR average request satisfied duplicate memory reside memory location rank faster another rank incoming request spent multicasting writes oppose nonblocking writes benchmark bandwidth utilization rate conflict rate demand request exclude prefetch request FMR normalize primary baseline average across quantify memory underutilization hpc improve performance via architecture micro october columbus usa kron twitter web bfs kron bfs bfs twitter bfs web kron twitter web kron twitter web kron wsg twitter wsg web wsg     linpack graph bfs hpcg NPB avg gap avg graph avg avg suite avg benchmark NPB gap avg demand lat FMR baseline average dram demand latency ADDRL FMR primary baseline benchmark FMR increase rate reduce conflict rate average dram latency demand primary baseline FMR simplicity refer average latency average dram demand latency ADDRL primary baseline average dram latency demand FMR reduces ADDRL baseline ADDRL latency llc MC latency per memory access closely load memory latency prior report server workload FMR substantially improves performance ADDRL baseline ADDRL kron FMR improves performance ADDRL FMR normalize baseline FMR improves performance substantially amount FMR reduces ADDRL seemingly counter intuitive phenomenon likely occurs improve application performance increase memory access rate memory queue delay increase ADDRL FMR significant ADDRL reduction FMR performance memory access rate somehow artificially throttle baseline sensitivity analysis evaluate sensitivity FMR memory configuration rank per channel performance FMR nonblocking memory refresh normalize primary baseline quad rank configuration FMR improves performance weigh benchmark suite equally weigh benchmark equally average difference performance benefit FMR memory configuration dual rank quad rank average difference weigh benchmark suite equally weigh benchmark equally respectively FMR robust benefit across memory configuration emulation memory compaction overhead expand cpu visible memory compaction expensive estimate overhead throughput memory compaction node microbenchmark fragment memory node kron twitter web bfs kron bfs bfs twitter bfs web kron twitter web kron twitter web kron wsg twitter wsg web wsg     linpack graph bfs hpcg NPB avg gap avg graph avg avg suite avg benchmark NPB gap perf norm primary baseline FMR nonblocking refresh performance normalize primary baseline quad rank configuration repeatedly mmap KB granularity consume nearly node memory request random non contiguous request finally invoke linux built memory compaction routine via echo proc sys compact memory  examine compaction latency report linux trace utility compaction latency amount fragment memory slowest memory compaction throughput compact 4GB memory per memory compaction throughput steadily increase memory cpu visible expansion policy memory compaction throughput memory usage measurement emulate performance overhead propose hourly expansion estimate amount memory compact node positive increase node memory memory define node memory node minimum memory average amount memory compact per node per hpc estimate average performance overhead average amount memory compact per 4GB compaction throughput estimate average performance overhead due migration infrequently expand cpu visible effectively minimizes performance overhead due migration reduce cpu visible memory measurement emulate hourly expansion policy calculate cpu visible memory memory node occupy throughout lifetime FMR fully replicate memory across node occupy average across grizzly badger simulation silicon simulated performance improvement emulate performance impact reduce ADDRL reduce ADDRL cpu cycle reduce cpu cascade IDs micro october columbus usa  zhang pang grizzly badger cascade memory compact GB node grizzly badger cascade avg perf overhead grizzly badger emulation propose hourly expansion policy cpu visible average amount memory compact per per node average performance overhead cpu visible memory fully replicate memory across node occupies entire lifetime kron twitter bfs kron bfs twitter kron web kron twitter hpcg NPB avg gap avg avg suite avg benchmark NPB gap improvement simulated performance emulate performance simulated ADDRL comparison simulated emulate performance improvement sample benchmark frequency emulate performance impact reduce ADDRL execution cpu cycle emulate speedup execution cpu cycle execute benchmark normal cpu frequency execution cycle execute benchmark reduce cpu frequency emulate ADDRL reduction emulate performance benefit FMR benchmark reduce cpu frequency factor FMR reduces ADDRL report simulation benchmark emulation meaningful cache latency remain constant cpu cycle cpu frequency extensive microbenchmark identify core intel core PC satisfy criterion simulated benchmark thread per benchmark simulation adopt evaluate PC microarchitecture parameter PC mhz dram frequency PC 8GB dram chip tRFC channel rank channel latency mem FMR ADDRL improvement FMR ADDRL baseline ADDRL accord simulation emulate FMR performance benefit individual benchmark unfortunately suitable server machine machine cpu frequency access machine gain access server cluster PC PC cpu frequency benchmark ADDRL FMR benchmark ADDRL baseline performance improvement report simulation performance improvement report emulation emulate speedup minus average performance benefit across within however individual benchmark performance improvement report simulation emulation significantly kron exhibit mismatch FMR improves performance accord simulation accord emulation inspection kron reveals FMR performance normalize baseline FMR ADDRL baseline discussion surround describes ideally emulation ADDRL FMR hypothetically achieve performance memory access rate throttle baseline however effort emulation underestimate performance benefit kron emulate performance improvement simulated performance improvement kron twitter twitter generality potential FMTs beside hiding dram dynamic latency microarchitecture memory aware enable potential improve performance enable effective correlate prefetching application correlate prefetching amount address sequence predict future memory address memory overhead particularly application gigabyte memory footprint microarchitecture memory aware enable multi gigabyte correlation enable effective correlate prefetching memory application impossible exist correlate prefetchers static memory location metadata another reduce overhead tlb expensive memory access naively reduce overhead tlb quantify memory underutilization hpc improve performance via architecture micro october columbus usa entry cpu incurs static overhead microarchitecture memory aware opportunistically enable gigabyte chip TLBs potentially eliminate access chip tlb incurs overhead memory access faster overhead memory access another improve hardware memoization recent hardware memoization speedup hardware memoization fully transparent instruction memo lookup memo update accelerate memoization memoization computation available effective memoization become microarchitecture memory aware enable hardware manage memoization maximize effectiveness hardware memoization applicability microarchitecture memory aware benefit PCs prior report memory underutilization FMTs potentially utilize unused memory improve performance exist physical memory limited return memory diminish quickly already sufficient physical memory workload program memory cache frequently access file enable CPUs extract performance gain additional memory FMTs encourage service provider increase memory per node PC user willing memory improve performance via FMTs related server memory mirror across memory channel improve memory reliability memory mirror improves reliability performance xeon processor report suffer slowdown memory mirror memory mirror enable disabled boot unlike FMR adjusts dynamically runtime exist OS multi socket replicate kernel text across CPUs physical memory hide latency bandwidth overhead access kernel text remote memory memory already account memory usage due exist OS optimization reduce overhead remote memory access prior purely OS domain replicates mostly program memory across multiple socket restriction replicate incurs expensive fault however evaluation benefit NPB due replicate mostly comparison FMR substantial benefit NPB due efficiently replicate mitigate performance overhead due refresh recent prior propose refresh dram frequently however reduce refresh rate dram memory security reliability therefore undesirable important application scenario nonblocking memory refresh prior maintains  dram refresh frequency multi mlc technology mlc flash mlc pcm exist prior explore dynamically switch mlc faster SLC storage mode usage switch mlc SLC hardware hardware restrict constant unused location propose architectural however allows hardware autonomously arbitrary memory location enable boost microarchitecture performance conclusion perform systemlevel memory utilization hpc machine measurement across production conclude memory underutilization hpc severe correspondingly perform exploration architectural technique improve memory utilization hpc propose expose compute node currently unused memory cpu via novel architectural OS enable microarchitecture technique abundant memory boost microarchitecture performance transparently without user code modification recompilation refer  microarchitecture technique FMTs detailed fmt memory aware memory replication FMR evaluation FMR improves performance efficiency average across hpc benchmark suite