gradient optimization enable dramatic advance computational image technique nonlinear optimization gradient mathematical function program encode complex transformation image graphical data unfortunately practitioner traditionally limited derive gradient complex computation compose program limited coarse grain operator framework program performance image prohibitively programmer extend image processing halide  automatic differentiation AD ability automatically optimize implementation gradient computation enables automatic computation gradient arbitrary halide program performance programmer effort challenge structure gradient code retain parallelism define algorithm automatically schedule pipeline halide exist schedule primitive express extend AD optimization checkpointing easily define neural network layer automatically compile performance gpu implementation nonlinear inverse computational image finally differentiable program enables dramatically improve quality traditional image processing algorithm blurring distinction classical CCS concept compute methodology graphic interface machine image processing additional image processing automatic differentiation introduction optimization rapid progress graphic image output image pipeline parameter unknown progress surprising gradient optimization nonlinear objective unknown unfortunately computation gradient remains challenge performance paramount training neural network image via optimization practitioner manually derive gradient limited composition building library inefficient user stray exist operator implementation gpu derivative code undertaking glance machine framework pytorch tensorflow CNTK appeal environment  graphic algorithm limited pre coarse grain operation framework performance kernel implementation automatic differentiation AD chain operation program however image application building algorithm  complex tangle composition exist building successfully implementation memory inefficient reload entire array intermediate costly cache recent neural network image processing approximation algorithm built around bilateral slice layer bilateral grid publish neither pytorch tensorflow capable practically express computation author define entirely operator cuda pas manually derive gradient sizeable program task significant expertise operation submission implement operation pytorch yield performance input memory realistically image challenge efficiently derive compute gradient custom node remains serious obstacle ubiquitous custom node effort implement correctly efficiently similarly image processing pipeline toolbox researcher limit operation already exist framework nvidia framework developer constantly expand native operation currently convolution operator alone tensorflow alternative invest magnitude technically tensorflow graph turing thanks inclusion loop node however implement algorithm incredibly complex    math   device float diff float float eps return sqrt eps device float diff float float eps return sqrt eps device float float float  diff return max  device float float float  diff  return return  return diff global void  int nthreads const float grid const float const float input const int const int const int const int const int const int const int input chans const int output chans float sample boundary int grid chans input chans output chans int coeff stride input chans const int idx blockIdx blockDim threadIdx idx nthreads int idx int idx int idx output chans int idx output chans float float float int static cast int int static cast int int static cast int grid stride int int int int int grid chans float int coeff stride float coeff sample int int max min float max int int max min float max int int max min float int grid idx coeff stride coeff sample grid grid idx grid trilinear interpolation input chans int input idx input chans coeff sample input input idx offset coeff sample idx global void  int nthreads const float grid const float const float input const float output const int const int const int const int const int const int const int input chans const int output chans float int grid chans input chans output chans int coeff stride input chans const int idx blockIdx blockDim threadIdx idx nthreads int idx int idx int idx int idx grid chans int idx grid chans float float int static cast int int static cast int ceil int static cast int int static cast int ceil stride output int int int int output chans stride input int isx int  int isc int isb output chans int coeff stride int coeff stride float int int mirror boundary float float max int int mirror boundary float float max int idx float idx float int idx input chans int input idx isc isx  isb output idx input input idx offset output idx idx global void  int nthreads const float grid const float const float input const float output const int const int const int const int const int const int const int input chans const int output chans float int grid chans input chans output chans int coeff stride input chans const int idx blockIdx blockDim threadIdx idx nthreads int idx int idx int idx float float float int static cast int int static cast int int static cast int grid stride int int int int int grid chans float sum int output chans float sum int coeff stride float grid sum int int max min float max int int max min float max int int max min float  int grid idx coeff stride grid sum grid grid idx  grid trilinear  input chans sum grid sum input input chans input chans offset sum grid sum sum sum output output chans idx sum global void  int nthreads const float grid const float const float input const float output const int const int const int const int const int const int const int input chans const int output chans float int grid chans input chans output chans int coeff stride input chans const int idx blockIdx blockDim threadIdx idx nthreads int idx int idx int idx input chans int idx input chans float float float int static cast int int static cast int int static cast int grid stride int int int int int grid chans float int output chans float chan val int int max min float max int int max min float max int int max min float int grid idx coeff stride chan val grid grid idx grid trilinear  chan val output output chans idx kernel  void  int int int int int input chans int output chans int int const float const grid const float const const float const input float const int output chans const int const int      grid input input chans output chans   void  int int int int int input chans int output chans int int const float grid const float const float input const float output float grid float float input int coeff chans input chans output chans const int int grid coeff chans grid const int  grid     grid grid input output input chans output chans grid int const int      grid input output input chans output chans int input input chans input const int  input     input grid input output input chans output chans input cuda   runtime variable  cuda variable  cuda clamp clamp min clamp min clamp min          clamp max clamp max clamp max batch idx  cuda idx  cuda grid batch idx idx grid batch idx idx grid batch idx idx grid batch idx idx grid batch idx idx grid batch idx idx grid batch idx idx grid batch idx idx sum input append  backward adjoints input input grad grid grid grad grad pytorch runtime  memory  slice affine matrix grid transform expr cast float sigma expr cast float sigma expr clamp grid channel expr cast int expr cast int expr cast int expr expr tent RDom func affine affine grid tent func output expr nci input channel RDom nci output affine nci nci output affine nci propagate gradient input auto propagate adjoints output adjoints func func func grid grid halide runtime   implementation gradient computation bilateral slice layer halide pytorch cuda automatic differentiation schedule extension halide implementation concise pytorch implementation modestly complex input fails memory 2GB nvidia titan input thanks operator pytorch publication cuda implementation developed author complex magnitude halide pytorch dominate derive gradient computation faster pytorch input halide version code beyond core logic halide pytorch acm trans graph vol article publication date august differentiable program image processing halide effort develop custom operation derive  debug gradient code development algorithm recently halide domain specific enable implementation performance image processing pipeline effective implement custom node image processing pipeline manual derivation gradient furthermore computation derivative differs code exist automatic performance optimization halide fail critically built halide  gpu schedule extend halide automatically efficiently compute gradient arbitrary halide program reverse mode automatic differentiation sec transformation exist feature building atop halide advantage concise express image processing computation already library exist algorithm halide compiler  target numerous processor accelerator architecture mobile CPUs image processing DSPs data gpus compilation highperformance code finally halide exist schedule construct compose reverse mode AD naturally express generalize essential optimization traditional AD literature sec compiler transformation scatter conversion algorithm preserve parallelism sec automatic schedule algorithm specialized generate gradient code sec halide exist powerful dependence analysis essential contrast traditional halide automatic schedule critical complexity  gradient code automatic gradient computation automatic scheduler easily implement  neural network layer code faster significantly simpler author custom node cuda sec aforementioned bilateral slice layer express halide compute extract gradient compile automatically implementation faster author handwritten cuda faster limited version pytorch implementation performance tune dramatically facilitate prototyping deliver automatic gradient performance outset experimentation usefulness node establish experimentation argue approach gradient optimization arbitrary program useful outside traditional application popularize vision image processing pipeline benefit automatic tune internal parameter currently usually user trial error availability automatic derivative systematically optimize internal parameter image processing pipeline output objective appeal gradient available performance code deployment significantly improve performance traditional image processing algorithm automatically optimize parameter filter sec implement novel joint burst demosaicking superresolution algorithm invert image formation model warp unknown  image  simultaneously sec finally versatility approach implement lens optimization differentiate optical simulator sec related automatic differentiation automatic differentiation collection technique numerically evaluate derivative computer program automatic differentiation distinct finite difference symbolic differentiation exploit structure computation graph recursively apply chain synthesizes program computes derivative instead algebraic expression compute gradient scalar output traverse computation graph backwards output propagate adjoints input complexity program although complexity gradient computation program backward traversal significantly memory pas traditional automatic differentiation memory checkpointing strategy allows user explore offs schedule mechanism halide sec automatic differentiation framework developed program program cumbersome image processing application efficient image processing code enormous effort parallelism locality memory consumption bandwidth account difficulty compound compute derivative recent package highly optimize differentiable building user assemble program package efficient algorithm implement conveniently express combine building user custom operator cuda extend package functionality user code program gradient consistent reasonably efficient tedious error prone challenge maintain approach simply program algorithm generates derivative thanks halide decouple algorithm schedule automatic scheduler convenient handle easily efficient code acm trans graph vol article publication date august  mao   andrew adam frédo durand jonathan   image processing halide image processing briefly introduce sec opt focus nonlinear construct automatically generates solver algorithm generate derivative proximal focus inverse proximal gradient algorithm function correspond proximal operator generates halide code optimization generate adjoints proximal operator focus specific solver namely nonlinear proximal interface gradient program unknown image optimize hyperparameters algorithm jointly optimize image parameter sec demonstrates recently attempt automatically image processing pipeline developed automatic scheduler halide specialized mechanism parallel reduction derivative image processing code benefit future development automatic code optimization optimize image gradient optimization commonly image processing image restoration image registration optical estimation stereo vision image prior complex inverse alleviates manually derive gradient application enables faster experimentation  building differentiable image processing pipeline parameter tune stochastic gradient descent successful instance image restoration photographic enhancement application colorization style transfer custom operator typically available mainstream framework custom operator gradient operation implement manually convenient explore custom computation halide programming extends halide program brief overview construct halide relevant detail halide documentation http halide lang org halide easy highperformance image array processing code halide separation program algorithm specifies compute schedule dictate computation storage algorithm express pure functional pipeline arithmetic operation multidimensional grid schedule address concern tile vectorization parallelization mapping gpu etc guarantee output program depends algorithm schedule user worry optimization algorithm explore optimization strategy without unintentionally alter output automatic differentiation halide philosophy differentiable pipeline user longer worry correctness efficiency gradient code sole specification algorithm synthesizes gradient algorithm optimization strategy explore manually auto scheduler code halide program performs gamma correction image computes norm output target image param float gamma parameter buffer float  input target buffer var integer variable pixel coordinate func halide function declaration halide function definition pow reduction variable loop target domain RDom  func loss compute mse loss  loss initialize sum expr diff  loss diff diff update definition halide embed halide pipeline stage function code func halide function define dimensional grid definition function comprises initial specifies grid optional recursive update modify function definition specify halide expression expr halide expression arithmetic logical expression conditionals halide function input buffer external code sin exp reduction operator summation convolution implement recursive update halide function domain reduction code RDom implies loop domain loop halide implicit domain function reduction schedule express expose func schedule operator transform computation memory bandwidth parallelism redundant computation halide lower schedule algorithm loop nest kernel compile machine code various architecture cuda backends application demonstrate acm trans graph vol article publication date august differentiable program image processing halide halide program request derivative backward cpu gpu code  loss synthesize backward program automatic differentiation manual schedule halide compiler automatic scheduler loss   overview compiler user writes halide program normally specify output gradient automatic differentiation generates halide function implement request gradient user manually schedule pipeline automatic scheduler finally halide compiler generates machine code schedule backward algorithm programmer writes halide algorithm request derivative scalar loss respect halide function image buffer parameter pipeline automatic differentiation graph function describes algorithm synthesizes halide function implement gradient computation sec programmer specify schedule function manually automatic scheduler sec unlike halide built auto scheduler recognizes arise reverse computation graph sec illustrates workflow strategy assume compute derivative scalar typically function minimize implement reverse mode automatic differentiation computes gradient complexity function propagate adjoints function pipeline input adjoints input component gradient specifically halide program graph halide function traverse graph backwards output accumulate contribution adjoints chain halide function definition expression within function perform backpropagation expression propagate adjoints leaf difference algorithm traditional automatic differentiation arises expression halide function construct computation accumulates adjoints onto function non trivial data dependency function sec describes detail illustrate algorithm sec performs gamma correction image computes distance output target image compute gradient distance respect input image gamma parameter obtain gradient respect image gamma parameter auto loss propagate adjoints loss func loss loss func loss loss throughout convention prefix function refers gradient halide function extension propagate adjoints halide scalar halide function generates gradient halide function halide function buffer parameter output depends component automatic differentiation compute gradient user specify  halide function buffer adjoints function computational graph backward gradient computation differentiate halide function difference automatic differentiation halide traditional automatic differentiation halide function define multi dimensional grid function grid non trivial aggregate interaction input output halide function synthesize halide function definition accumulates adjoint output function onto adjoint input performance definition parallelizable scatter conversion correctness efficiency important occurs output combine multiple input happens convolution operation compute gradient reverse automatic differentiation reverse scatter operation input writes multiple output scatter operation however naturally parallelizable convert scatter whenever shear iteration domain illustrate transformation code convolves 1D signal kernel illustrate func output output input kernel assume interested propagate gradient input achieve reverse dependency graph input output variable code transformation yield RDom input output kernel acm trans graph vol article publication date august  mao   andrew adam frédo durand jonathan   iterates iterates domain output argument input replace pure variable reduction variable iterate domain output rename merge reduction variable reduction domain update definition cannot compute parallel multiple memory location efficient compute update illustrate rewrite computation output output input output kernel bound output shear iteration domain variable substitution input parallelizable halide iterates rectangle shear iteration domain longer rectangle zero pad boundary output iterate conservative bound shear domain halide equation deduce variable substitution apply argument function construct equation importantly interval multiple introduce reduction variable upsampling operation output input integer input output accordingly converter generate adjoint code RDom loop output input procedure fails scatter operation parallelize scatter atomics atomic operation halide gpu backend handle scatter atomics usually remains significantly efficient transform code instance backward pas 2D convolution layer apply input atomics scatter conversion listing derivative generate bilateral slice handle partial update arises reverse partial update function code update overwrites  adjoints propagate correctly chain update definition depends via update definition hide previous dependency correspond gradient code parallel parallel 1D convolution backward  backward conversion scatter conversion enables efficient parallel code 1D tap convolution dot input resp output array computation output input fade dot account boundary tap reduction easily parallel output buffer dot compute adjoint operator simply reverse dependency graph loop parallel output node orange input location input adjoint buffer highlight issue scatter operation scatter conversion convert backward operation reduction adjoint convolution correlation transform computation readily parallelize domain update propagate update update propagate initial definition mask unwanted dependency propagate detect update argument consecutive function update mask adjoint update zero update argument update checkpointing reverse mode automatic differentiation complex pipeline traditionally memoizing evaluation reuse reverse pas compute memory unlimited memory bandwidth limited efficient recompute automatic differentiation address checkpointing reduces memory usage recomputing expression however specific instance recomputation memory already address halide schedule primitive function intermediate buffer later reuse compute construct recompute site compute inline construct compute intermediate granularity computation somewhere loop nest consumer compute construct halide allows checkpointing across halide pipeline global cache memoize construct useful pas backward pas separately compile 2D convolution implementation halide RDom convolve convolve kernel loss define optimization objective loss pow convolve target auto propagate adjoints loss func acm trans graph vol article publication date august differentiable program image processing halide listing derivative generate algorithm bilateral slice code output contains adjoint output propagate derivative output affine RDom nci adjoints channel output affine nci affine nci output variable convert reduction variable rco RDom rco adjoints channel affine rco nci nci output rco derivative propagate affine grid RDom sigma sigma expr inv sigma expr inv sigma grid affine inv inv tent tent tent replace inv inv scatter operation transform inv sigma inv sigma inv inv finally obviously affine depends RDom  adjoints channel expr    expr  affine   grid channel interested gradient loss respect correlation convolve target kernel depends convolve schedule handle halide easily cache convolve gradient computation convolve compute convolve compute fetch memory derivative convolve compute inline convolve compute buffer allocate advantageous convolution kernel preserve memory locality pipeline longer cannot afford intermediate buffer halide schedule primitive binary checkpointing decision grain schedule allows exploration memory recomputation offs gradient code instance interleave computation storage convolve computation another halide function consumes code instructs halide compute tile convolve tile compute potentially faster balance compute convolve backpropagation recomputing pixel demand compute tile convolve compute compute tile schedule compute multi thread vectorization cpu image kernel compute inline schedule millisecond compute schedule millisecond compute schedule millisecond image kernel compute inline schedule millisecond compute schedule millisecond compute schedule millisecond automatic schedule halide built auto scheduler navigates performance offs stencil pipeline struggle arise reverse computational graph sec optimize reduction compute scalar loss generate gpu schedule therefore implement custom automatic scheduler gradient pipeline halide built auto scheduler user estimate input output buffer infer extent intermediate function domain automatic scheduler checkpoint compute stage scatter reduces along function function recomputed  compute inline checkpointed function tile function domain parallelize computation tile specifically CPUs split function domain 2D tile launch cpu thread tile vectorizing innermost dimension inside tile gpus split domain 3D tile tile mapped gpu within tile gpu thread tile resp dimension function domain split domain dimensionality function domain tile function performs associative reduction transform parallel reduction halide  schedule primitive allows factorize reduction partial reduction compute parallel serial reduction dimension reduction domain tile reduce tile parallel cpu thread resp gpu within 2D tile vectorize resp parallelize gpu thread wise reduction implement multi parallel reduction schedule unnecessary application compile gpus function domain reduction domain tile recursive update pure variable parallelism parallelize reduction atomics checkpointing automatic scheduler decision overridden user optional halide function inline currently compute automatic scheduler APPLICATIONS RESULTS generate gradient pipeline application integrate exist easily develop custom operator improve exist image processing pipeline optimize internal parameter dataset training image finally derivative inverse image optimize image acm trans graph vol article publication date august  mao   andrew adam frédo durand jonathan   unless otherwise specify automatic scheduler sec schedule application throughout code derivative generate therefore implementation programmer specify pas algorithm custom neural network layer computation expressible library caffe pytorch tensorflow CNTK increasingly nonetheless practitioner custom node tailor instance tensorflow bilinear interpolation layer separable 2D convolution layer however extension operation 3D implement custom operator cuda link library already tedious error prone furthermore algorithm developed adjoint derive sync operator experimentation prototyping finally backward implementation ought reasonably optimize model finite amount verify implement pytorch backend halide derivative plug pytorch  backend implement custom operator recently propose literature transformation layer spatial transformer network warp layer flownet bilateral slice layer bilateral performance automatically schedule code highly optimize primitive cuda faster unoptimized code runtime pytorch CNTK cuda code spatial transformer network spatial transformer network applies affine warp intermediate feature neural network function halide code exclude comment empty function declaration due popularity operator framework implement specialized function layer cudnn library implementation version publication another pytorch implement wrapper around cudnn code performance pytorch grid sample affine grid function cudnn implementation gpu image channel batch cpu code around faster pytorch implementation gpu code around percent highly optimize version implement cudnn currently halide texture sample gpu slowdown performance CNTK implementation spatial transformer operation gpu code around faster CNTK implementation performance approach custom neural network operator runtime latency backward evaluation spatial transformer transforms batch flownet node warp batch image 2D warp  layer image grid measurement intel core cpu 0GHz 6GB ram nvidia titan pascal gpu GB ram operator  flownet  pytorch cpu cpu pytorch gpu CNTK gpu manual cuda gpu gpu fix function affine grid problematic user slightly modify model interpolation scheme bicubic lanczos instead bilinear interpolate dimension transform volume data implement custom operator modification minor code algorithm generates derivative automatically automatic scheduler performance without effort warp layer flownet target optical application introduce 2D warp layer previous spatial transformer layer warp layer transform per pixel warp instead parametric transformation function halide code warp function implement custom node caffe author reverse code cpu gpu backends comprises code custom node handle 2D warp adapt dimensional warp semi parametric warp challenge easy addition pytorch CNTK performance gpu code highly optimize reimplementation nvidia performance code comparable  cuda code bilateral slice layer bilateral performance image processing architecture inspire bilateral grid processing local affine transforms approximate complicate image processing pipeline throughput algorithm splatting 2D image onto 3D grid convolutional network voxel grid contains affine transformation matrix  guidance slice grid unique interpolate affine transform apply flownet http github com   flownet blob src caffe layer warp layer nvidia flownet http github com nvidia flownet pytorch acm trans graph vol article publication date august differentiable program image processing halide input pixel implementation tensorflow implement custom node slice operation due lack efficient perform trilinear interpolation grid custom node applies affine transformation avoid instantiate resolution image affine parameter pixel reference custom node around cuda code exclude comment empty recently introduce scatter functionality implement operation directly pytorch comparison halide code reference cuda code pytorch code pytorch CNTK implementation modestly complex code pytorch CNTK input grid batch CNTK faster pytorch due implementation choice operation manual cuda code aim clarity performance complicate code argue training resolution image capture frequency feature image processing algorithm approximate pytorch CNTK code memory input grid titan gpu GB memory almost impossible resolution input code faster author reference implementation parameter optimization image processing pipeline traditionally develop image processing algorithm programmer manually tune parameter pipeline image parameter manually parameter becomes contrast achieve impressive parameter training image demonstrate apply strategy image processing algorithm augment algorithm parameter tune parameter offline training gradient optimization user code halide optimize parameter code training image demonstrate image demosaicking algorithm adaptive homogeneity demosaicking  park non blind image deconvolution algorithm sparse adaptive prior propose  oliveira image demosaicking demosaicking seek retrieve  image incomplete sample capture filter array pixel contains traditional demosaicking algorithm exhibit structure aliasing artifact   recent achieve impressive http github com   blob  ops bilateral slice  filter truth automatic gradient relax  demosaicking algorithm filter interpolate channel instead footprint instead tweak optimize filter automatically generate derivative obtain sharper image  demosaicking nevertheless inherits limitation  artifact complex  prone image demosaicking dataset psnr demosaicking technique evaluation methodology implement version  demosaicking algorithm despite simplicity approach relax algorithm specification filter channel reconstruction footprint optimize parameter achieve fidelity computational rival technique significantly faster avenue optimize  parametrized algorithm tailor timing report megapixel image timing algorithm non optimize matlab code kodak mcm vdp  bilinear adobe camera raw   filter filter filter  however execution issue practical usage relax adaptive homogeneity demosaicking algorithm  variation default algorithm adobe camera raw  increase filter interpolate channel tune chrominance interpolation filter  reference filter filter explore runtime versus accuracy optimize acm trans graph vol article publication date august  mao   andrew adam frédo durand jonathan   blur truth  automatic gradient enhance  oliveira non blind deconvolution algorithm iteration automatically threshold filter parameter sharper randomly image achieve average psnr algorithm parameter psnr image imagenet filter training dataset gradient illustrate modification obtain significant improvement datasets  vdp filter obtain visually sharper image challenge limited footprint filter complexity optimize demosaicking struggle  prone texture user complex without implement derivative instance quickly ultimately discard alternative algorithm filter ratio account 1D directional filter non blind image deconvolution task non blind image deconvolution function blurry image latent image convolve function recover underlie image highly ill therefore quality reconstruction heavily depends prior image important parameter prior implementation sparse adaptive prior propose  oliveira stage fashion stage conventional output   frame frame automatic gradient inverse  demosaicking burst image user implement model burst raw image capture  camera jointly align  image respectively initialize  bilinear interpolation inverse recover   image capture data reprojected   algorithm output sharper  deconvolution discrete derivative filter prior aware filter cleanup image stage another deconvolution discrete derivative prior stage masked smooth thresholding function extend increase stage instead filter prior stage optimize prior filter smoothness parameter aware filter bilateral grid thresholding parameter smooth thresholding function demonstrate ability handle nest derivative implement generic non linear conjugate gradient solver linear algorithm newton raphson deconvolution conjugate gradient loop pytorch implement gradient vector hessian vector halide implement bilateral grid filter halide optimize parameter differentiate gradient non linear conjugate gradient algorithm imagenet function generation scheme described initialize parameter recommend parameter described  oliveira acm trans graph vol article publication date august differentiable program image processing halide inverse image optimize image derivative automatic differentiation algorithm readily employ inverse computational photography user quickly model prior demonstrate burst demosaicking inverse pipeline misalign bayer raw image goal reconstruct image estimate homography parameter align reconstruction input data minimize function min   sample accord bayer mosaic  align reconstruction input data gradient descent minimize function locally equation highly non convex initialization critical initialize RANSAC sift feature pairwise fashion initialize implement opencv jointly refine alignment estimate image minimize loss function individual image reconstruction sharper suffer  artifact adam gradient descent optimizer iteration rate algorithm gradient loss respect reconstruct image  image compute initial  initialize reconstruction minimize function code generate automatic scheduler titan pascal gpu lens optimization focus image halide express pipeline arithmetic multi dimensional array numerous non image application derivative useful implement ray tracer spherical lens halide construct derivative sharpness respect lens curvature exist   compact maintain sharpness future application demonstrate automatically delivers performance compute gradient image processing pipeline direction future derivative non scalar output optimization derivative non scalar output hessian matrix derivative  opencv http github com opencv opencv halide augment gradient useful wider application image processing machine express ray tracer optical halide derivative sharpness respect lens parameter  classic  lens compact maintain sharpness  nest application differentiation however differentiates respect scalar dimensionality input output automatic differentiation algorithm efficient  incorporate algorithm develop interface non scalar output derivative broaden application automatic schedule manually schedule synthesize reverse computation challenge non trivial rely automatic scheduler entirely performance gradient pipeline inspect generate code reveals plenty improvement halide automatic schedule unsolved conclusion gradient optimization revolutionize image processing efficient computation derivative conform limited building error prone manual derivation challenge performance optimization contrast automatically generate performance gradient code image processing pipeline implementation operator concise easy maintain portable automatically derives gradient code reverse automatic differentiation automatic performance tuner handle computation exhibit derivative code compiles variety platform gpus critical deployment efficient training demonstrate enables application custom neural network node tune acm trans graph vol article publication date august  mao   andrew adam frédo durand jonathan   internal image processing parameter inverse dramatically simplifies exploration custom neural network node automatically performance reserve advanced cuda programmer easy optimize internal parameter image processing pipeline practitioner afford due implement gradient algorithmic exploration stage inverse unknown image parameter addition unknown image user worry implement model demonstrate application implement evolve rapidly gradient performance implementation automatically opportunity rapid research development optimization image application