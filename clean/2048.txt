physic locomotion dicult typically exploit prior knowledge various aim variety environment aware locomotion limited amount prior knowledge adopt hierarchical framework controller timescale achieve robust gait satisfy target style objective controller timescale invoke desire target controller controller decision directly dimensional input terrain suitable representation surroundings policy reinforcement demonstrate simulated 3D biped controller variety style demonstrate robustness respect  disturbance terrain variation style interpolation controller demonstrate capable trail terrain  soccer towards target location navigate static dynamic obstacle CCS concept compute methodology animation physical simulation reinforcement additional physic animation locomotion introduction physic simulation movement promising avenue animation dicult develop strategy robust balance locomotion challenge complexity locomotion task  soccer navigate obstacle hierarchical approach towards controller llc desire timescale goal predominately balance limb timescale controller HLC suitable movement achieve longer goal anticipate obstacle leverage capability reinforcement RL policy timescales RL allows dened via objective function enable policy dimensional input local terrain abundant sensory information hierarchy enables controller reuse multiple task enables controller reuse dierent controller principal contribution demonstrate  3D  locomotion limited amount prior structure impose policy introduce hierarchy reinforcement locomotion hierarchy identical style actor critic algorithm knowledge demonstrate capable dynamic 3D model knowledge equation kinematics abstract feature priori specic feedback structure limitation discus acm transaction graphic vol article publication date july xue bin peng    yin  van  related model movement locomotion computer animation robotics  recently signicant machine community challenge domain reinforcement focus closely related  computer animation reinforcement physic recent survey  animation technique comprehensive overview endure approach controller structure policy around  machine FSMs feedback  abstract model feedback apply  variety style controller developed physic animation optimization improve controller developed around FSM structure FSM dene phase dependent objective inverse dynamic optimization policy stochastic local cma optimize parameter structure achieve richer variety ecient muscle driven locomotion policy successfully apply directly indexed spline neural network variety bicycle  alternative approach trajectory optimization compute oine adapt online model predictive compute optimize action quadratic program  propose hierarchical framework incorporates footstep planner model predictive  locomotion across irregular terrain improve quality enrich repertoire data driven model incorporate capture construct controller  trajectory reinforcement simulated locomotion  demonstration policy gradient recurrent neural network apply 3D movement recent resurgence  towards tackle reinforcement simulated agent continuous continuous action consist agent 2D 3D physic simulation hop numerous apply task planar biped locomotion planar trajectory optimization trust policy optimization TRPO actor critic approach progress recently harder achieve 3D biped locomotion model policy without priori structure tackle  without hint particularly challenge recent generalize advantage estimation TRPO demonstrates sustain 3D locomotion biped another promising approach trajectory optimization supervision recurrent neural network generate movement 3D biped quadruped nal optimization directly dynamic simulation hierarchical 3D locomotion recently propose closest  controller rst pretraining phase conjunction provisional controller access task relevant information communicate lowlevel controller via  signal pretraining controller structure xed highlevel controller 3D humanoid navigate  impressive feat  locomotion demonstrate signicantly locomotion ability multiple style interpolate locomotion signicant  robustness controller task dimensional input ability surroundings ego centric terrain soccer  biped controller bilinear phase transform controller reference optional style reward controller communicate controller reference previously utilized training neural network policy 3D  locomotion controller physic successfully demonstrate locomotion obstacle avoidance task alternatively planning dynamic model propose locomotion task however controller policy signicant insight recent demonstrate planar planning planning typically investigates robot constrain environment robot usually reduce nding robot planning legged robot signicantly challenge due increase freedom tight couple underlie locomotion dynamic quadruped equip robust mobility classic planner compute steer command locomotion controller navigate environment however skilled balance dicult achieve biped acm transaction graphic vol article publication date july  dynamic locomotion hierarchical reinforcement overview harder robotics emphasizes footstep planning generation possibility graph propose highlevel approximation constraint manifold rapidly explore possibility action thereby planner utilized eciently hierarchical planning framework target HLC partly inspire previous humanoid robotics planning animation manipulation task behaviour behaviour planner kinematic planning physic achieve abstract dynamic model dimensional hybrid approach adopt kinematic planner directs dynamic specic overview overview  partition component dierent timescales controller HLC operates coarse timescale timescale controller llc operates timescale action PD target angle finally physic simulation perform khz HLC llc  hierarchy HLC task goal llc intermediate goal towards overall task objective intermediate goal HLC llc coordinate various joint intermediate goal hierarchical partition allows controller explore behaviour span dierent spatial temporal abstraction thereby enable ecient exploration task relevant strategy input HLC consist highlevel goal specied task output action serf goal llc proprioceptive information conguration  information environment framework action consists footstep llc llc receives intermediate goal specied HLC output action unlike consists mainly proprioceptive information action specie target angle PD controller joint compute torque action llc apply simulation update extract relevant feature HLC llc respectively environment reward signal HLC llc  progress towards respective goal controller actor critic algorithm policy actor positive temporal dierence update scheme model  function critic bellman backup policy representation learning deterministic policy goal action stochastic policy conditional probability distribution action distribution model gaussian parameterized xed covariance matrix policy query sample action distribution accord generate apply gaussian action covariance   action parameter neural network parameter excessively noisy prone approximately joint eective training stochastic policy enables explore action promising addition exploration impact performance runtime therefore runtime deterministic policy selects action instead choice stochastic deterministic policy denote addition binary indicator variable indicates stochastic policy exploration deterministic policy selects action training greedy exploration incorporate randomly enable disable exploration accord bernoulli distribution ber probability action exploration apply action reinforcement objective optimal policy maximizes cumulative reward express discount sum immediate reward discount factor acm transaction graphic vol article publication date july xue bin peng    yin  van  horizon  reward function agent feedback regard desirability perform action goal reward function therefore interface user behaviour agent assign reward desirable behaviour reward desirable model parametric function parameter cumulative reward express goal formulate nding optimal parameter arg max policy gradient popular algorithm perform gradient ascent objective empirical estimate policy gradient gradient respect policy parameter framework algorithm illustrates algorithm llc HLC purpose summarize tuples goal action reward application exploration action perform tuples replay memory update policy policy actor critic framework policy  parameter tandem function predict cumulative reward policy goal update function minibatch tuples sample perform bellman backup  critic stepsize function update policy policy improvement perform  style positive temporal dierence update policy gradient dened stochastic policy policy update perform tuples exploration  actor stepsize equation interpret stochastic gradient ascent along estimate policy gradient gaussian policy controller controller llc responsible coordinate joint torque mimic overall style reference satisfy footstep goal maintain balance reference algorithm actor critic algorithm positive temporal dierence update random random goal ber apply simulate reward update function sample minibatch tuples  update policy sample minibatch tuples  keyframes specify target timestep llc query query input feature consist link relative arrow rotation linear velocity arrow angular velocity terrain feature consist 2D heightmap terrain sample regular grid height express relative height immediately heightmap resolution occupies approximately acm transaction graphic vol article publication date july