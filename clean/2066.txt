longstanding goal animation combine data driven specification behavior execute behavior physical simulation enable realistic response perturbation environmental variation reinforcement RL adapt robust policy capable imitate clip complex recovery adapt morphology accomplish  goal handle  highly dynamic action capture flip spin retargeted combine imitation objective task objective react intelligently interactive setting desire direction user specify target approach combine convenience quality clip define desire style appearance flexibility generality afford RL physic animation explore integrate multiple clip develop multi skilled agent capable perform repertoire diverse demonstrate multiple atlas robot   dragon variety locomotion   CCS concept compute methodology animation physical simulation reinforcement additional physic animation reinforcement introduction physic simulation passive phenomenon cloth fluid become nearly ubiquitous however adoption physically simulated modest model remains challenge currently exist simulate diversity behavior exhibit endure challenge domain generalization  rely manually controller compelling ability generalize situation limited availability insight adept perform articulate internal strategy  proficiency challenge encode controller  another obstacle impede adoption simulated author simulated remains notoriously interface cannot user effective elicit desire behaviour simulated reinforcement RL promising approach synthesis whereby agent learns perform various trial error reduce insight reinforcement demonstrate complex behavior prior quality generate lag kinematic manually controller controller RL exhibit severe sometimes humorous artifact extraneous upper peculiar gait unrealistic posture direction improve quality controller incorporate capture author animation data prior typically layer physic controller kinematic animation approach challenge kinematic animation reference http youtu  acm trans graph vol article publication date august xue bin peng    levine  van  feasible physic controller limited ability modify achieve plausible recovery accomplish task goal deviate substantially kinematic furthermore tend complex implement ideal animation artist capture actor reference style generate goal physically realistic behavior reference approach directly reward controller resemble reference animation data achieve additional task objective demonstrate construct controller multiple clip training multi clip reward max operator training policy perform multiple diverse trigger user sequence multiple clip policy function estimate feasibility transition central contribution framework  animation combine goal reinforcement data capture clip  animation although framework consists individual component combination component context data driven physic animation novel demonstrate quality robustness substantially exceed prior incorporate capture data phase aware policy physic behavior nearly indistinguishable appearance reference absence perturbation avoid artifact exhibit previous reinforcement algorithm presence perturbation modification remain recovery strategy exhibit robustness without engineering knowledge demonstrate capable physically simulated ablation identify specific component reference initialization termination critical achieve highly dynamic demonstrate integrate multiple clip policy related model skilled movement articulate  robotics animation recent machine algorithm mature increase machine community focus closely related animation RL kinematic model kinematic endure avenue animation effective amount data available dataset clip controller built appropriate clip situation gaussian latent representation synthesize runtime extend model autoencoders  network apply develop generative model kinematic quality data data driven kinematic quality simulation approach however ability synthesize behavior novel situation limited task environment become complex data sufficient coverage behavior quickly becomes untenable incorporate physic source prior knowledge presence perturbation environmental variation physic model controller simulated remains challenge rely insight implement task specific strategy locomotion considerable robust controller developed nonhuman controller underlie simplify model optimization compact parameter tune achieve desire behavior dynamic aware optimization quadratic program apply develop locomotion controller model effective variety tend struggle dynamic planning contact trajectory optimization explore synthesize physically plausible variety task synthesize extend horizon offline optimization equation enforce constraint recent extend technique online model predictive although remain limited quality capacity planning principal advantage approach generality demonstrate model framework capable wider highly dynamic kick flip ability sequence ability combine imitation task related demand compact compute policy ability leverage dimensional environment description reinforcement optimization technique develop controller simulated reinforcement iteration develop kinematic controller sequence clip context task approach explore simulated recently introduction neural network model RL simulated agent perform diverse array challenge task liu   acm trans graph vol article publication date august  reinforcement physic policy gradient emerge algorithm choice continuous although RL algorithm capable synthesize controller minimal task specific structure behavior generally manually engineer counterpart challenge stem difficulty specify reward function movement particularly absence  model objective achieve simulated locomotion na√Øve objective torque actuate locomotion progress maintain desire velocity gait exhibit extraneous limb asymmetric gait  artifact mitigate artifact additional objective effort impact penalty discourage undesirable behavior craft objective function substantial insight yield modest improvement alternatively recent RL imitation capture  address challenge reward function data induce objective improve quality generate favorably standard computer animation  approach namely imitation reward function although significant limitation fix initial capable highly dynamic demonstrate locomotion task define placement goal compute controller apply  biped model lastly multi clip demonstration involves craft procedure suitable target clip imitation imitation reference computer animation instantiation  locomotion planar controller tune policy model reference demonstrate locomotion 3D humanoid reference reward function RL locomotion gait flap flight demonstrate capability perform significantly broader highly dynamic spin kick flip intermittent contact reference initialization termination critical explore option multi clip integration sequence reminiscent capability sample controller  impressive array reproduce  knowledge  demonstrate diverse corpus highly dynamic  simulated however complex component iterative define dimensional representation synthesize linear feedback structure controller excel mimic reference extend task objective particularly involve significant sensory input recent variation introduces policy selects precomputed collection  fragment flexibility execution fragment demonstrate capable challenge non terminate task balance  propose alternative framework RL conceptually simpler  nonetheless highly dynamic  task objective multiple clip overview receives input model correspond kinematic reference task define reward function synthesizes controller enables imitate reference satisfy task objective strike target desire direction irregular terrain reference sequence target  policy  task specific goal action compute torque apply joint action specifies target angle proportional derivative PD controller torque apply joint reference define imitation reward goal defines task specific reward policy enables simulated imitate behaviour reference fulfil specify task objective policy model neural network proximal policy optimization algorithm background task structure standard reinforcement agent interacts environment accord policy maximize reward brevity exclude goal notation discussion readily generalizes policy model conditional distribution action timestep agent observes sample action environment responds sample dynamic scalar reward reflect desirability transition parametric policy goal agent optimal parameter maximizes return  distribution trajectory induced policy initial distribution  return trajectory horizon acm trans graph vol article publication date august xue bin peng    levine  van  infinite discount factor ensure return finite popular algorithm optimize parametric policy policy gradient gradient return estimate trajectory sample policy policy gradient estimate accord est distribution policy advantage action  denotes return trajectory function estimate average return policy subsequent policy gradient therefore interpret increase likelihood action return decrease likelihood action return classic policy gradient algorithm policy empirical gradient estimator perform gradient ascent reinforce policy proximal policy optimization algorithm demonstrate stateof challenge function multi return TD advantage policy gradient compute generalize advantage estimator GAE depth review supplementary policy representation reference clip sequence target  goal policy reproduce desire physically simulated environment satisfy additional task objective reference kinematic information target policy responsible action apply timestep realize desire trajectory action describes configuration feature consist relative link respect designate pelvis rotation express quaternion linear angular velocity feature compute local coordinate frame origin axis along link direction target reference phase variable feature denotes denotes cyclic reset cycle policy achieve additional task objective direction target goal treat similarly fashion specific goal action policy specifies target orientation PD controller joint policy query target orientation spherical joint axis angle target  joint scalar rotation angle unlike standard benchmark directly torque PD controller abstract away detail local damp local feedback torque PD controller improve performance task network policy neural network goal distribution action action distribution model gaussian dependent specify network fix diagonal covariance matrix treat hyperparameter algorithm input fully layer linear output layer relu activation hidden function model network exception output layer consists linear vision task input augment heightmap surround terrain sample uniform grid around policy network augment accordingly convolutional layer heightmap schematic illustration  policy network heightmap series convolutional layer fully layer feature concatenate input goal fully network task vision reward reward consists encourage reference satisfy schematic illustration  policy network heightmap convolutional layer filter filter filter feature fully feature concatenate input goal fully layer output layer linear relu activation hidden layer task heightmap network consist layer acm trans graph vol article publication date august  reinforcement physic additional task objective imitation task objective respective task objective incentivizes fulfill task specific objective detail imitation objective encourages reference  decompose reward characteristic reference joint orientation velocity reward encourages joint orientation reference compute difference joint orientation quaternion simulated reference equation orientation jth joint simulated reference respectively denotes quaternion difference computes scalar rotation quaternion axis radian exp velocity reward compute difference local joint velocity angular velocity jth joint target velocity  compute data via finite difference exp  effector reward encourages reference denotes 3D meter effector exp finally penalizes deviation reference exp training policy ppo clipped surrogate objective maintain network policy another function parameter respectively training proceeds  episode initial sample uniformly reference rollouts generate sample action policy episode simulated fix horizon termination trigger batch data minibatches sample dataset update policy function function update target compute TD policy update gradient compute surrogate objective advantage compute GAE refer supplementary detailed summary algorithm persistent challenge RL exploration formulation assume unknown MDP agent interaction environment infer structure MDP discover endeavor algorithmic improvement propose improve exploration metric novelty information gain however attention structure episode training potential mechanism exploration decision initial distribution termination treat fix RL appropriate choice crucial challenge highly dynamic kick spin flip default choice fix initial fix episode imitation unsuccessful initial distribution initial distribution determines agent episode choice agent fix however task imitate desire strategy initialize proceed towards episode policy sequential manner phase incrementally progress towards later phase earlier phase progress later phase problematic  prerequisite return policy cannot successfully jumping actually return another disadvantage fix initial exploration challenge policy receives reward retrospectively therefore reward policy favorable disadvantage mitigate modify initial distribution RL task fix initial convenient challenge initialize agent physical robot obtain richer initial distribution imitation task however reference informative distribution leveraged agent training episode sample reference initialize agent refer strategy reference initialization rsi strategy previously planar  manipulation sample initial acm trans graph vol article publication date august xue bin peng    levine  van  humanoid atlas rex dragon 3D simulated framework policy morphology reference agent encounter desirable along policy acquire proficiency challenge perform  fix initial discover perform rotation mid return perform carefully coordinate however motivate perform aware yield reward highly sensitive initial takeoff strategy failure agent unlikely encounter successful flip discover reward rsi agent immediately encounter promising stage training instead access information reference reward function rsi interpret additional channel agent access information reference informative initial distribution termination cyclic task model infinite horizon MDP training episode simulated finite horizon episode terminates fix termination trigger locomotion termination ET detection characterize torso contact link height threshold strategy prevalent mention passing impact performance evaluate termination episode terminate whenever link torso contact termination trigger zero reward remainder episode instantiation termination another reward function discourage undesirable behavior another advantage termination function curating mechanism bias data distribution sample relevant task flip challenge recover return nominal trajectory humanoid atlas rex dragon link height freedom feature action parameter without termination data stage training dominate sample struggle  capacity network devote model futile phenomenon analogous imbalance encounter methodology supervise terminate episode whenever failure encounter imbalance mitigate multi integration discussion focus imitate individual clip ability compose sequence multiple clip vital perform complex task propose application restrict reference clip define desire style instead richer flexible multi clip reward user behavior trigger training selector policy user specify clip selection input avoid training policy clip combination instead construct composite policy exist clip policy setup multiple policy independently runtime function policy activate multi clip reward utilize multiple reference clip training define composite imitation objective calculate simply max previously introduce imitation objective apply clip max acm trans graph vol article publication date august  reinforcement physic imitation objective respect jth clip composite objective sufficient integrate multiple clip unlike manually craft kinematic planner clip objective policy flexibility appropriately clip situation ability switch clip whenever appropriate without kinematic planner selector besides simply policy multiple clip accomplish goal user clip approach policy simultaneously learns imitate diverse execute arbitrary sequence demand policy goal vector entry corresponds execute goal perform correspond nonzero entry additional task objective optimize imitation objective compute currently training random sample cycle policy therefore transition within clip composite policy previously described policy collection clip network multiple jointly challenge increase policy fail adequately alternative adopt conquer strategy policy perform integrate composite policy function estimate policy performance function leveraged appropriate execute policy function composite policy construct boltzmann distribution exp exp parameter policy therefore likely repeatedly sample composite policy perform sequence library diverse without additional training composite policy resembles mixture actor critic expert model  propose although simpler sub policy independently specific  3D humanoid atlas robot model rex dragon illustration available detail model articulate rigid link attach link via freedom spherical joint knee elbow attach via traverse randomly generate terrain mixed obstacle dense gap balance beam stair trace trajectory freedom  joint PD controller joint manually specify gain constant across task humanoid atlas structure morphology distribution actuator PD gain torque limit significantly atlas almost humanoid rex dragon behavior  animation mocap data available illustrate readily apply non  humanoid 7D 6D action complex dragon 8D 4D action standard continuous benchmark RL typically action 3D 7D significantly dimensional action TASKS addition imitate clip policy perform variety task preserve style prescribed reference task specific behavior encode task objective task evaluate target steerable controller introduce objective encourages target direction 2D vector horizontal reward task exp max specifies desire along target direction velocity simulated objective therefore penalizes acm trans graph vol article publication date august