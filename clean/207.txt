video dimensional emotion recognition aim affect dimensional emotion visual signal fundamental challenge affective compute computer interaction novel encoder decoder framework tackle adopts fully convolutional cascade 2D convolution spatial encoder 1D convolution temporal encoder decoder joint spatio temporal model address issue capture discriminative dynamic dependency temporal model refer temporal hourglass convolutional neural network TH cnn extract contextual relationship integrate encode decode clue temporal intermediate supervision TIS introduce enhance affective representation generate TH cnn multi resolution strategy TH cnn macroscopic trend refine fluctuation progressively furthermore thanks TH cnn TIS knowledge learnt intermediate layer customize application adjust decoder depth extensive conduct benchmark database recola SEWA omg superior indicates effectiveness propose approach introduction perceive important artificial intelligence AI understand emotion consistently increase attention academia recent emotion recognition technique apply computer interaction hci deployment AI collaborate deeply  ongoing demand variety application humanoid robot healthcare etc emotion typically mapped discrete universal category accord dimensional theory model continuous contributes another understand structure function emotion arousal valence dimension arousal describes active calm remains valence positive negative emotion recognition conduct multiple modality data audio video text physiological signal video become popular video conveniently acquire repute information facial expression gesture due annotation video dimensional emotion recognition categorize frame clip former aim predict emotion variance frame clue latter render label entire clip utterance information frame spite difference processing basically issue effectively model spatio temporal visual clue meanwhile clip recognition frame dynamically model affect grain carefully procedure literature approach propose address video emotion recognition however overwhelm majority spatio temporal feature employ craft suffer limitation aspect feature strongly dependent prior expertise cannot guaranteed incomplete cognition feature built within relatively fix temporal incapable capture sufficient temporal contextual information recently investigate directly discriminative feature static sequential data convolutional neural network cnns effective tackle visual task emotion recognition filter encode local spatial image 3D cnns extend cnn version capture temporal dependency conveyed consecutive frame adopt action recognition emotion recognition although promising achieve 3D cnn constrain expensive resource consumption temporal model video dimensional emotion recognition intra sample affective difference facial due movement sometimes inter similarity interval span illustration contextual dependency model recurrent neural network rnns competency theoretically handle input arbitrary commonly rnn model due gradient vanish explode advanced version rnn memory lstm network developed compensate shortcoming unfortunately hyperbolic tangent sigmoid activation function incurs gradient decay layer performance degrades sequence therefore remains unsolved issue extract discriminative affective representation recognize emotion video propose novel approach video dimensional emotion recognition model contextual affect encoder decoder fully convolutional network specifically spatio temporal convolutional encoder consists spatial convolutional encoder sce temporal convolutional encoder  cascade capture spatial temporal cue serial manner temporal decoder symmetrical structure  integrates feature skip connection encoder decoder temporal model refer temporal hourglass cnn TH cnn account temporal intermediate supervision TIS introduce enhance affective representation generate TH cnn multi resolution strategy overall pipeline rnn contextual regressors widely topic propose TH cnn TIS pipeline allows acquire dependency stack temporal convolutional layer coarse meanwhile explicit intermediate layer encoder decoder structure propose temporal model flexible adapt application adjust decoder depth efficiency overall architecture propose tackle dimensional emotion recognition fully convolutional encoder decoder framework detail frame recognition video sequence facial image extract align raw frame fed spatial encoder generate static feature temporal encoder spatial feature input hierarchical temporal convolution pool representation obtain frame decoder symmetrical structure temporal encoder linear interpolation adopt upsampling feature integrate feature skip connection encoder decoder explicitly propose temporal model affective fluctuation coarse temporal intermediate supervision TIS apply decode layer demonstrate effectiveness propose approach frame emotion recognition report benchmark datasets recola SEWA moreover approach generalization ability clip emotion recognition task simply replace temporal decoder temporal global pool evidence evaluation omg database significant accuracy improvement representative baseline summary contribution fold propose fully convolutional spatio temporal model video dimensional emotion recognition frame clip task propose temporal hourglass convolutional neural network contextual representation cue respectively layer encoder decoder identical temporal resolution propose supervision strategy temporal intermediate supervision TH cnn semantic emotion representation coarse enhance discriminative remainder organize briefly review related formulation video emotion recognition describes propose spatio temporal encoder decoder detail experimental evaluation analysis conclude future perspective related decade explore categorical model classify emotion discrete although category facial expression accord emotional response scenario complex compound ambiguous alternatively another model emotion dimensional embeds affect dimensional vector arousal valence dominance dimensional affective model continuous signal facilitate realistic application typical video dimensional emotion recognition approach frame clip task stage local spatio temporal visual feature extraction contextual non contextual regression briefly review technique stage respectively local spatio temporal feature handcraft feature appearance feature geometric feature extensively emotional model propose visual feature facial deformation global local facial appearance continuous prediction dimensional emotion generate histogram MHH facial image sequence orientation histogram  local binary LBP calculate dynamic representation compute local gabor binary orthogonal  appearance combine static dynamic texture cue employ facial landmark geometric feature avec baseline extract local phase quantization orthogonal LPQ capture spatial temporal dependency within fix balance consumption adopt feature selection technique reduce risk overfitting exploit feature gaze direction intensity tilt advantage signal facial feature feature basically prior knowledge incapable encode complex temporal information variation feature data driven cnn model ability discriminative feature recently demonstrate performance computer vision task attempt dimensional emotion recognition exploit model residual convolutional neural network pre training conduct fer dataset pool layer regard frame spatial feature apply layer cnn extract frame feature lightweight generates comparable feature handcraft investigate feature cnn model vgg densenet pre  dataset convolutional layer analyze layer convey affective information propose integrate cnn framework jointly perform facial attribute action dimensional emotion recognition video clip utilize  acquire static information frame cnns illustrate tremendous improvement topic static emotion feature extraction however efficient discriminative temporal representation emotion regression model  regressors straightforward perform emotion prediction video frame apply regression model without contextual information introduce linear regressor frame recognition although weak linear relation feature label bias performance improvement complex baseline vector regressor SVR popular  model widely adopt phase baseline avec challenge accuracy  approach restrict due  dependency spatial feature sequence variation affective successive context sensitive contextual regressors rnn structure conduct contextual emotional regression previous utilize lstm model temporal dependency adopt bidirectional lstm BiLSTM encode precede subsequent information introduce BiLSTM clip recognition achieves promising apply echo network esn extend esn bidirectional version improve regression accuracy propose framework leverage spatio temporal attention video frame spatial appearance temporal facial video sequence simultaneously 3D cnns recently jointly spatial temporal dependency within stage framework employ 3D convolutional network ConvLSTM extract spatio temporal information although rnns lstm theoretically handle video various difficulty training due gradient vanish explode  3D cnn suffer computational representation shortcoming greatly limit extensive application aforementioned model formulation goal video dimensional emotion recognition input video dimensional emotion naturally formulate multi label regression specific continuous frame emotion recognition task aim predict emotion variance frame date information mapping raw video sequence frame input function vector dimensional vector emotional description parameter clip recognition difference function vector dimensional emotional description parameter model mapping function frame clip task unified encoder decoder framework consists spatio temporal encoder task specify decoder detailed framework overview instead rnns model temporal dependency literature dimensional emotion recognition fully convolutional encoder decoder structure illustrate frame task video sequence facial image extract align raw frame subsequently fed spatial encoder squeeze excitation generate static feature apply temporal convolution pool hierarchically spatial feature representation obtain temporal encoder temporal decoder symmetric structure temporal encoder linear interpolation adopt upsampling feature integrate skip connection encoder decoder entire temporal model refer temporal hourglass cnn account structure temporal intermediate supervision apply decode layer resolution thanks TIS TH cnn affective fluctuation coarse explicitly furthermore generalization ability propose framework replace decoder temporal global pool decoder address clip emotion recognition encoder spatial convolutional encoder employ 2D cnn spatial encode facial sequence spatial convolutional encoder frame input across frame video squeeze excitation SE propose achieves performance ILSVRC adaptively  channel wise feature spatial average pool channel feature due simplicity SE network accuracy gain employ layer residual network SE spatial cnn perform dynamic channel wise feature recalibration generates discriminative static feature input propose temporal model worth cnn architecture vgg googlenet resnet spatial affective information encode comparison network architecture focus emotion global grain expression SE operation empowers 2D convolutional network model channel relationship temporal convolutional encoder temporal convolutional network tcn series model proven capable capture increasingly employ action segmentation action localization sequence model task temporal convolution dim convolution conduct conv operation along axis adopt stack  temporal contextual encoder tcn computation perform layer wise meaning update simultaneously update sequentially per frame moreover convolution compute across prediction frame function fix receptive illustrate encoder compose encode convolutional sequence temporal convolution normalization spatial dropout temporal max pool rectify linear relu inspire residual scheme residual connection reduces training difficulty encode simply input output residual function output  convolution alter depth identity mapping wise addition another relu layer calculate activation temporal convolutional encoder paradigm encode network decrease output convolutional layer demonstrates receptive variation encode kernel temporal convolutional max pool layer interval learnt automatically temporal encoder decoder illustrates module consist temporal convolution layer temporal max pool layer temporal convolution encoder  TH cnn stack module hierarchical temporal contextual feature display temporal receptive RF increase  input neuron temporal convolution RF enlarge TL temporal max pool RF augment TL stride kernel convolution respectively convenience convolution pool layer module temporal sample layer temporal convolution layer temporal convolution decoder TCD TH cnn compose stack module increase temporal resolution feature generate  depicts temporal resolution increase TCD output neuron  temporal convolution temporal max pool temporal resolution enlarge sample rate temporal sample layer convenience sample convolution frame decoder temporal hourglass network frame recognition decoder correspond structure encoder temporal model symmetrical illustrate decoder integrate feature obtain incorporate decode feature encode feature input perform temporal upsampling operation via linear interpolation due simplicity efficiency interpolation rate temporal max pool output temporal resolution correspond encode layer visualization propose temporal model resembles hourglass partly inspire estimation model propose refer temporal encoder decoder model temporal hourglass cnn TH cnn filter stride temporal convolution temporal pool TL TL sourcewhere TL denotes temporal receptive lth layer stride temporal convolution pool filter convolution equation filter grows exponentially layer tune hyper parameter TH cnn temporal intermediate supervision spontaneous emotion variation interval intra difference dimensional emotion recognition identity confuse temporal regressors easy overfit furthermore dimensional emotion distance inversely proportional spatial feature similarity within local temporal depict model global emotion variance trend local affective detail hierarchical dimensional emotion variance random sample recola database dimensional emotion variance sample expression frame almost valence however contrast appearance respectively valence sharply emotion difference correspond appearance feature similarity random frame anchor dimensional emotion variance random sample recola database dimensional emotion variance sample expression frame almost valence however contrast appearance respectively valence sharply emotion difference correspond appearance feature similarity random frame anchor intermediate supervision launch propose model actually intermediate supervision proven effective practical training computer vision task inspire propose spatial intermediate supervision strategy temporal domain namely temporal intermediate supervision conduct temporal decode layer illustrate temporal resolution intermediate layer temporal resolution reduce temporal max pool encode stage increase temporal upsampling decode stage output intermediate layer target label rescale target average convolution adapt temporal resolution correspond intermediate layer specifically kth layer decoder depth correspond truth emotion label supervision define loss  sourcewhere predict ith layer  correspond truth label parameter balance layer detailed supervision  TIS TH cnn strengthen coarse manner emotional temporal resolution account decode phase clip decoder demonstrate  flexibility propose framework extend fully convolutional network clip emotion recognition task replace decoder clip decoder clip task global information video clip instead naively average frame apply global pool decoder generate video representation temporal max pool adopt temporal fusion operates channel wise manner input contextual feature formulate maxi  sourcewhere denotes representation clip encoder output entire channel temporal contextual feature evaluate propose video dimensional emotion recognition approach extensive comparison counterpart database metric implementation detail subsequent subsection datasets popular benchmark database recola SEWA omg evaluation recola SEWA frame dimensional emotion recognition omg clip dimensional emotion recognition recola remote collaborative affective interaction dataset multimodal corpus monitor remotely collaborative task modality audio video electro  ECG electro  activity eda signal french recording training validation duration frame dataset contains dimensional label arousal valence annotate SEWA automatic sentiment analysis spontaneous naturalistic interaction consist audio video text modality signal interaction behavior duration dataset annotate dimension namely arousal valence omg gradual emotion dataset video average variety youtube channel video automatically specific related  video clip utterance utterance annotate independent amazon mechanical turk dataset release standard arousal valence individual annotation reviewer label readily available treat development clip sample training validation model performance fold validation training statistical information partition datasets statistic dimensional emotion database evaluation statistic dimensional emotion database evaluation metric arousal valence mse RMSE therefore series avec omg challenge metric widely evaluate performance continuous emotion recognition  correlation coefficient pcc concordance correlation coefficient protocol evaluate performance pcc pcc define cov sourcewhere cov covariance series prediction truth variance series define   SourceRight click MathML additional feature pcc variance series equation pcc linear correlation variable penalize offset correlation relationship protocol evaluate performance pcc implementation detail implementation pytorch detail network parameter network training feature similarity network parameter apply layer resnet squeeze excitation static emotion recognition activate output average pool layer spatial feature detailed sce structure spatial dropout encode rate perform fully convolutional filter instead individual initialize gaussian distribution parameter TH cnn adjust slightly task specific recola output channel temporal encoder kernel SEWA output channel temporal encoder kernel detailed parameter TH cnn structure kernel output omg output channel temporal encoder kernel TH cnns output channel temporal decoder remains correspond encoders max pool upsampling rate kernel dataset video frequency computation efficiency consideration recola SEWA omg respectively output kernel parameter sce structure output kernel parameter TH cnn TIS structure network training OpenFace detection alignment landmark spatial encoder pre  task categorical emotion classification model tune correspond dimensional emotion datasets avoid overfitting randomly perform rotation zoom factor horizontal flip input image data augmentation phase standardization apply network training phase mini batch zero padding video sequence batch attempt adam optimization minimize loss function initial rate model achieve performance employ exponential decay rate training feature similarity spatial feature similarity cosine similarity SourceRight click MathML additional feature spatial feature vector obtain anchor frame obtain frame comparison frame task propose model handcraft feature cnn feature recola propose model recent SEWA cnn achieve comparable superior handcraft feature  LPQ geometric feature reproduce previous directly cite standard protocol adopt rnns lstm BiLSTM  temporal information model propose approach fully convolutional network model spatial temporal dependency achieves benefiting context capture without attention mechanism moreover rnn temporal model TH cnn convenient parallel compute thanks characteristic convolution operation comparison pcc recola dataset comparison pcc SEWA dataset analyze advantage temporal convolution network rnn apply replace lstm popular encode temporal clue previous continuous emotion recognition simply TH cnn instead lstm sce remain unchanged improve percent arousal percent valence demonstrate effective continuous emotion recognition SEWA superiority relatively significant recola mainly due SEWA recola shortest video SEWA correspond recola video capability TH cnn model longer temporal contextual information fully highlight additionally lstm rnn suffers gradient explosion vanish contrast TH cnn recurrent structure axis enables easily training rnn counterpart clip task replace decoder TH cnn temporal global pool decoder described tackle clip emotion recognition task omg performance report promising obtain baseline cnn cascade cnn lstm  propose approach comparison pcc omg dataset comparison pcc omg dataset validation intermediate layer validate coarse decode explore issue analyze knowledge intermediate layer demonstrate predict layer rescale label visualization feature layer temporal decoder decode grain affective detail hierarchically acquire moreover compute intermediate performance pcc pcc become decode layer pcc comparable output increase grain detail via decoder layer hence specific application scenario selectively balance accuracy computation complexity intermediate layer valence performance recola SEWA ablation demonstrate effectiveness propose TH cnn TIS pipeline perform ablation recola SEWA omg lstm adopt baseline spatial feature input TH cnn achieves improvement lstm datasets clearly indicates advantage TH cnn temporal contextual model frame clip emotion recognition task apply intermediate loss supervise TH cnn frame recognition TH cnn ability contextual dependency recognition ameliorate illustrate ablation propose model pcc recola SEWA omg ablation propose model pcc recola SEWA omg confusion heatmap visualization visualize prediction propose correspond label via confusion matrix heatmap illustrate prediction distribute interval label prof effectiveness approach confusion heatmaps correspond label distribution recola SEWA heatmap visualization proportion histogram label prediction confusion heatmaps correspond label distribution recola SEWA heatmap visualization proportion histogram label prediction intermediate layer valence prediction versus truth random sample recola dev dev layer prediction input feature decoder prediction decode layer intermediate layer valence prediction versus truth random sample recola   layer prediction input feature decoder prediction decode layer indeed propose approach performs phenomenon obvious SEWA recola imbalance distribution datasets frequency distribution histogram valence distribution SEWA recola sample limited model easily ignore error account portion loss function training stage regard difficulty propose approach conclusion future propose novel effective framework video dimensional emotion recognition specifically encoder decoder fully convolutional network integrate spatio temporal encoder temporal decoder leverage spatial contextual dependency temporal intermediate supervision introduce enhance affective representation performance frame recognition meanwhile thanks flexible structure propose display generalization ability address clip recognition task replace temporal decoder temporal global pool decoder extensive conduct popular benchmark database namely recola SEWA omg superiority addition temporal model TH cnn accept dynamic input modality audio text physical signal etc actually network conveniently extend modality dim signal replace sce modal specific coder  audio encoder bert textual encoder moreover feature modality input concatenate feature capture multi model clue future extend propose approach multi modal analysis improvement overall performance