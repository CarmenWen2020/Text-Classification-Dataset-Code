regression algorithm denoising monte carlo MC rendering leverage inexpensive feature buers however model handle complex technique overt input supervise propose collection reference explicit lters limit denoising ability address propose novel supervise approach allows ltering kernel complex leverage convolutional neural network cnn architecture embodiment framework cnn directly predicts nal denoised pixel highly non linear combination input feature approach introduce novel kernel prediction network cnn estimate local kernel compute denoised pixel evaluate network production data improvement theart MC denoisers generalize variety scene conclude analyze various component architecture identify research MC denoising CCS concept compute methodology computer graphic render ray trace additional monte carlo render monte carlo denoising global illumination introduction recent physically image synthesis become widespread feature animation visual eects acm transaction graphic vol article publication date july  fuel desire photorealistic imagery production studio switch render algorithm   architecture  monte carlo MC trace MC render algorithm satisfy strict quality requirement immense computational convergence characteristic render image scene complex transport fortunately recent  image MC denoising algorithm demonstrate achieve highquality considerably reduce sample rate commercial renderers incorporate technique chaos  renderer corona renderer   integrate denoisers moreover production develop internal  denoiser although variety image MC denoising approach propose technique regression framework improvement achieve thanks robust distance metric regression model diverse auxiliary buers tailor specic transport component advance however increase complexity  progressively diminish return partially  regression model prone overtting noisy input circumvent  recently propose MC denoiser supervise noisy input correspond reference output however approach relatively multi layer perceptron mlp model scene importantly approach hardcoded lter joint bilateral joint non local limited  address shortcoming propose novel supervise framework allows complex ltering kernel leverage convolutional neural network cnns increase amount production data oers diverse dataset training cnn complex mapping collection noisy input correspond reference advantage cnns powerful non linear model mapping leverage information entire training image input previous approach moreover cnns evaluate manual tune parameter tweak finally robustly cope noisy rendering generate quality variety MC eects without overtting although approach application physically image synthesis focus highquality denoising static image production environment specically contribution contribution rst denoising MC rendering evaluate actual production data architecture performs par exist denoising inspire standard approach estimate pixel average noisy neighborhood propose novel kernel prediction cnn architecture computes locally optimal neighborhood regularization training convergence rate facilitates production environment finally explore analyze various processing decision network framework denoising diuse specular component image separately normalization procedure signicantly improves approach previous image dynamic previous MC denoising focus extensive research scope therefore MC denoising restrict directly related posteriori treat renderer overview refer reader review focus convolutional neural network image monte carlo denoising image denoising lter distribute monte carlo eects depth eld blur glossy reections global illumination successful generic non linear image lters auxiliary feature buers improve robustness ltering development introduce sen  leverage noisy auxiliary buers joint bilateral ltering scheme bandwidth various auxiliary feature derive sample statistic later propose estimate lter error metric lter bandwidth asymptotic bias analysis training procedure implicitly learns appropriate various auxiliary buers particularly successful application non local lter joint ltering scheme endure appeal non local lter denoising MC rendering largely due versatility indeed powerful image lters BM3D MC denoising notable exception due successfully extend leverage auxiliary buers component propose machine instead xed lter perform par image lters allows network auxiliary buers leverage robustness recently joint ltering cite interpret linear regression  model generally MC denoising technique linear regression  transaction graphic vol article publication date july kernel predict convolutional network denoising monte carlo rendering rst model leverage rst model useful MC denoising model explore carefully prevent overtting input contrast cnn oer powerful non linear mapping without overtting complex relationship noisy reference data across training recently propose ltering approach closely related however network xed lter therefore inherits limitation contrast propose implicitly learns lter therefore finally concurrent applies denoise monte carlo rendering target dierent application focus interactive rendering sample instead production quality rendering facilitate comparison approach previous baseline respective sec convolutional neural network recent convolutional neural network cnns emerge ubiquitous model machine achieve theart performance diverse task image classi cation processing others cnns variety image processing task image denoising highly related image super resolution however na√Øve application convolutional network MC denoising expose issue handle framework training network compute denoised raw noisy buer  network cannot distinguish scene scene detail moreover render image dynamic training unstable extremely artifact nal image preprocessing feature exploit diuse specular decomposition preserve important detail denoising image furthermore introduce novel kernel prediction architecture sec training tractable stable sec motivate explore decision aect performance theoretical background introduce propose denoising framework rst dene notation interpretation denoising supervise sample output typical MC renderer average vector per pixel data rgb channel auxiliary feature normal depth albedo correspond variance goal MC denoising obtain   truth obtain sample  estimate usually compute operating per pixel vector around neighborhood  output pixel denoising function parameter ideal denoising parameter pixel  argmin denoised dcp  loss function truth denoised clearly optimize impossible truth available instead MC denoising algorithm estimate denoised pixel replace function possibly non linear feature transformation parameter regression around neighborhood  argmin nal denoised pixel compute  regression kernel ignore corrupt feature bandwidth joint bilateral lter potentially patch pixel joint non local lter previously previous  zero rst enumerates polynomial formulation limitation individual approach understood bias variance  zero equivalent explicit function joint bilateral non local lter restrictive function reduction variance model bias although chosen kernel yield performance approach fundamentally limited explicit lters seek remove limitation lter kernel  powerful furthermore rst regression increase complexity function prone overtting  estimate locally image easily address propose supervise approach estimate dataset noisy image patch correspond reference information XN corresponds reference patch pixel input image goal parameter denoising function minimize acm transaction graphic vol article publication date july  albedo irradiance reconstruction diffuse cnn specular cnn diffuse component specular component denoised image exponential transform reconstruction albedo logarithmic transform normalization gradient extraction albedo preprocessing filter postprocessing renderer normalization gradient extraction overview framework preprocessing diuse specular data render independently information network denoise diuse specular illumination respectively output network undergoes reconstruction postprocessing combine obtain denoised image average loss respect reference across patch argmin parameter optimize respect reference noisy information estimate representative training data adapt variety scene characteristic however approach limitation important function hardcoded joint bilateral joint non local lter bandwidth multi layer perceptron mlp lter xed lack  handle monte carlo encounter production environment address limitation extend supervise approach handle signicantly complex function  avoid overtting reduce model bias simultaneously ensure variance estimator suitably enables denoiser generalize image training issue inherent supervise framework develop MC denoising function  capture complex relationship input data reference scenario model convolutional network choice loss function critical ideally loss capture perceptually important dierences estimate reference however easy evaluate optimize absolute loss function sec explore benet sec model avoid overtting training dataset reference image render sample obtain data extremely computationally expensive furthermore generalize network representative various eects denoised data sec convolutional denoising approach model denoising function convolutional neural network cnn layer cnn applies multiple spatial kernel learnable entire image naturally denoising task indeed previously traditional image denoising furthermore layer activation function cnns highly nonlinear function input feature important obtain quality output illustrates entire denoising pipeline rst focus ltering core denoiser network architecture reconstruction lter later data decomposition preprocessing specic MC denoising network architecture fully convolutional network fully layer parameter reasonably reduces overtting training inference stack convolutional layer eectively increase input receptive eld capture context dependency layer network applies linear convolution output previous layer constant bias applies wise nonlinear transformation activation function output tensor bias appropriately linear convolution kernel output previous layer rst layer per pixel vector around pixel input cnn layer  linear relu activation max layer acm transaction graphic vol article publication date july kernel predict convolutional network denoising monte carlo rendering identity function despite discontinuity ReLUs achieve performance task encourage non convex optimization procedure local minimum bias WL trainable parameter layer cnn dimension layer xed training described sec reconstruction function output denoised architecture prediction convolutional network  novel kernel prediction convolutional network KPCN prediction convolutional network  denoised image prediction straightforward simply nal layer network ensure pixel correspond network output denoised dcp  prediction achieves however unconstrained complexity optimization dicult magnitude variance stochastic gradient compute training slows convergence obtain performance  architecture training kernel prediction convolutional network KPCN instead directly output denoised pixel dcp nal layer network output kernel scalar apply noisy neighborhood  neighborhood around pixel dimension nal layer chosen output kernel speci training along network hyperparameters layer cnn kernel apply rgb channel  entry vector obtain  compute nal normalize kernel wpq exp exp denoised pixel dcp   kernel interpret softmax activation function network output nal layer entire neighborhood enforces wpq wpq specic benet ensures nal estimate within convex hull respective neighborhood input image vastly reduces output prediction avoids potential artifact shift ensures gradient error respect kernel behave prevents oscillatory network parameter dynamic input intuitively encode relative importance neighborhood network absolute reparameterization scheme recently proven crucial obtain variance gradient convergence potentially denoising across layer frame production apply reconstruction component analyze behavior propose architecture sec converge overall error dierent training data kernel prediction converges roughly faster reconstruction due faster convergence KPCN architecture analysis unless otherwise diuse specular decomposition denoising output MC renderer ltering operation prone  sec various component image dierent characteristic spatial structure  denoising constraint mitigate issue decompose image diuse specular component component independently preprocessed   recombine obtain nal image illustrate diuse component preprocessing diuse outgo radiance due diuse  behave typically training diuse cnn stable network yield performance without preprocessing however factor noisy albedo renderer preprocessing cnn eective irradiance    wise hadamard implementation allows ltering kernel irradiance buer smoother postprocessing inverts procedure multiplies albedo thereby restore texture detail specular component preprocessing denoising specular challenge due dynamic specular glossy reections image span magnitude variation arbitrary correlation input iterative optimization highly unstable apply transform channel input image   signi  reduces transformation greatly improves avoids artifact dynamic sec component denoised separately apply inverse preprocessing transform reconstruct output network compute nal denoised image   exp  acm transaction graphic vol article publication date july  reference image frame training sample dory film wise hadamard pre specular diuse network separately specular diuse reference respectively afterwards apply tune framework minimize error nal image additional iteration allows recover detail obtain sharper experimental setup data training neural network obtain representative dataset complex relationship input output avoid overtting training representative frame sample entire movie dory generate  tracer meanwhile consists diverse frame coco eects blur depth eld glossy reections global illumination signicantly  style content generalizes input mostly outdoor scene palette dierent dory reference image training render sample per pixel spp although remove residual standard MC denoisers cnns perform image uncorrelated residual correlate error artifact introduce additional denoising therefore reference image although amount visible converge properly evaluate propose approach validate input render xed spp production quality spp pre visualization scene renderer output diuse specular rgb buers   correspond per pixel variance diuse specular feature buers consist normal channel albedo channel depth channel correspond per pixel feature variance implementation convert variance channel channel compute luminance channel variance diuse specular channel feature variance commonly machine raw data network useful feature facilitate convergence depth arbitrary linearly frame preprocess buers described previously sec   finally gradient direction buers highlight important detail facilitate training preprocess buers apply appropriate transformation variance valid apply transformation random variable approximate correspond transformation taylor series expansion variance respectively derivative respect diuse specular component modied variance  diuse   specular  processing construct network input  diuse specular processing data pixel split image patch sample  network although uniform sample patch frame suboptimal network frequently smooth straightforward denoise instead network expose handle dicult sample strategy inspire patch frame dart candidate patch prune pdf variance noisy buer shade normal ensures target detail texture normal buer geometric complexity finally ensure balance easy avoid biasing network automatically accept patch reject training hidden layer convolution kernel layer network KPCN output kernel spp network initialize xavier specically generate random uniform distribution variance node layer specular diuse network independently absolute error metric loss function  perceptual quality compute optimize sec additional  loss diuse network compute reconstruct irradiance albedo acm transaction graphic vol article publication date july kernel predict convolutional network denoising monte carlo rendering input spp  apr NFOR LBF RF ref spp relative ssim relative ssim relative ssim relative ssim relative ssim demonstrate favorable relative denoisers spp production quality data oen remove detail beer preserve highlight supplemental comparison spp data typically stage production LBF modification suboptimal performance text albedo factorize reference image loss specular cnns compute domain network optimize adam optimizer tensorflow rate mini batch network pre approximately iteration nvidia quadro gpu afterwards combine tune sec another iteration RESULTS evaluate stateof  apr NFOR LBF supplemental  denoiser production training metric evaluate relative relative structural similarity index ssim supplemental description compute conciseness report relative ssim commonly supplemental resolution sample per pixel spp metric web interactive viewer allows inspection denoisers input buer albedo normal depth buers correspond rst ray intersection feature buers rst diuse intersection handle specular useful information previous preprocessing report apply diuse specular decomposition albedo diuse component transform specular component interestingly transform signicantly increase robustness denoisers halo supplemental http doi org  acm transaction graphic vol article publication date july  transform transform  apr NFOR LBF RF KPCN  apr NFOR LBF RF KPCN  apr NFOR LBF RF KPCN spp spp relative ssim average performance  apr NFOR LBF RF KPCN across scene spp spp boom input relative noisy input express percentage beer performance prior decomposition irradiance factorization without transform specular component performance transform increase robustness relative error compute trim remove pixel per image artifact supplemental raw specular component denoisers albedo buer extract sample rate pas obtain nal image albedo generate sample render ignores illumination calculation alternatively denoising  furthermore currently ignore alpha channel ltering generate nal image simply alpha zero appropriate avoid bleeding finally production data   shadow ray rst bounce sample estimate illumination noisy rendering correlate sample discrepancy sample cannot directly estimate accurate variance per pixel sample instead  output buer variance previous properly evaluate  NFOR apr data training data raw sample variance directly renderer buer variance aforementioned default setting author LBF network data joint non local lter mlp architecture described training dataset buer variance LBF cannot pre lter feature fairer comparison substitute pre  feature relatively reference image denote LBF RF reference feature however distinct dierences implementation LBF  dataset primary feature LBF namely secondary albedo visibility useful feature lter compensate data instead replace LBF secondary feature correspond primary feature feature calculate noisy buers however buers overtting residual issue exacerbate substitute noisy sample variance joint non local lter instead  buer variance LBF LBF tend excessive residual described sec cnn frame dory render uniform sample rate spp reference spp network sample rate apply data correspond sample rate subset frame coco spp data supplemental sample rate overall perform technique perceptually quantitatively previous residual  headlight respectively approach remove preserve detail furthermore approach generates smooth specular highlight meanwhile approach tend introduce lter artifact lose comparison average performance across scene respect error metric spp network consistently improves across error metric demonstrate  processing input spp network spp data despite sample rate network successfully extrapolate data improve previous approach tend excessive residual relative approach along cable facilitate future comparison demonstrate network ability perform noisier data dierent render publicly available tungsten scene approach baseline NFOR slight residual NFOR spp approach closely resembles reference gure concurrent allows reader relative improvement baseline facilitate comparison tungsten training scene sec training specically tungsten scene randomly modied various swap camera parameter environment generate unique training scene supplemental tungsten scene generate training acm transaction graphic vol article publication date july kernel predict convolutional network denoising monte carlo rendering input spp  apr NFOR LBF RF ref spp relative ssim network spp data spp data performs relative approach demonstrates technique successfully extrapolate sample rate supplemental additional spp input spp NFOR ref spp relative ssim relative ssim relative ssim retrain network data render tungsten tracer baseline approach NFOR scene publicly available camera parameter concurrent timing HD image network evaluate output denoised image comparison timing gpu approach approximately  apr LBF cpu version NFOR worth image core render spp additional sample render evaluate denoisers analysis analyze various choice network architecture frame dory frame examine choice loss function crucial aspect determines network deems important MC denoising ideally loss function  perceptual quality image relative reference evaluate behavior various error metric optimize network evaluate performance training data dory validation data evaluate metric relative relative ssim optimize network metric consistently error across metric datasets due robustness chose error metric sometimes network optimize error perform acm transaction graphic vol article publication date july  iteration loss relative relative ssim iteration relative loss relative relative ssim iteration loss relative relative ssim iteration relative loss relative relative ssim iteration ssim loss relative relative ssim relative relative ssim convergence plot network optimize error metric evaluate data dory error dataset network relative relative ssim network consistently performance across error metric behavior validation image supplemental loss  KPCN loss  KPCN diuse specular comparison optimization  KPCN architecture although approach converge error validation KPCN converges faster network error performs network optimize sensitive outlier  extremely specular highlight signicantly contribute error compensate  performance elsewhere network dierent loss robust outlier validation loss  KPCN reconstruction scheme function specular diuse network training KPCN average loss training horizontal dash convergence  considerably variance average longer loss therefore impose reasonable constraint network output greatly training without  average performance previous machine image denoising evaluate performance  apply cnn MC denoising specically raw buer without decomposition albedo directly output denoised network  feature information distinguish scene detail furthermore input output dynamic cannot properly handle artifact around highlight moreover hdr domain instability network dicult properly evaluate eect various addition framework alleviate aforementioned issue vanilla cnn hyperparameters report nal architecture hidden layer explore eect extra feature input signicant advantage network denoising photograph utilize additional information output render shade normal depth albedo architecture without additional feature sec network buer cannot  scene detail  approach training dynamic data introduce issue namely input output instability training dicult transform buer correspond transform variance reduces artifact interestingly domain benet previous denoising technique reduce halo issue supplemental previous approach without transform diuse specular decomposition albedo factorization improve signicantly decomposition allows network separately handle fundamentally dierent diffuse specular furthermore albedo diuse illumination thereby denoising eective irradiance preserve texture detail easily retrain without albedo   become   without albedo moreover perform albedo without decomposition network preserve detail artifact specular perform transform handle dynamic demonstrates ability network generalize scene dierent artistic style training frame photorealistic piper denoised network without additional training modi cation dory suggests network overtting specic style instead learns robust relationship input output enable performance variety data various inherent limitation approach however lose scene detail properly capture input feature training  screen remove auxiliary feature network mistake acm transaction graphic vol article publication date july kernel predict convolutional network denoising monte carlo rendering input spp vanilla cnn ref spp naively apply cnn MC denoising unprocessed buer input directly  denoised image dynamic data creates artifact around highlight additional feature  detail boom input spp feature feature ref spp training diuse specular buers without additional feature network  detail input spp ref spp dynamic image artifact specular highlight approach correspond transform variance handle diicult beer input spp decomposition albedo decomposition albedo decomposition albedo decomposition albedo ref spp retrain network without diuse specular decomposition albedo factorization  texture   decomposition without albedo  albedo without decomposition creates artifact specular boom approach preserve text clearly closely resembles reference scene patch training dataset network cannot resolve buer however potentially alleviate additional training likewise distribute eects training otherwise network cannot properly denoise volumetric eects detail smoke training typically  another limitation occurs apply render network dory data  data tungsten renderer although renderers output feature inevitable dierences dynamic artifact issue largely disappear training tungsten data although approach generates artifact input severe spp scene acm transaction graphic vol article publication date july  input spp ref spp demonstrate network denoise photorealistic frame film piper significantly  training data dory sample rate network generalizes quality denoised input spp NFOR ref spp input spp NFOR ref spp input spp retrain retrain ref spp input spp NFOR ref spp demonstrate various limitation approach input feature fail capture important scene detail network mistake remove training tends  apply network data dierent render artifact due inherent dierences sample strategy significantly improve network instead data render however data network struggle extremely noisy input boom future CONCLUSIONS although demonstrate robust MC denoising algorithm decision explore extensively improve performance facilitate exploration enable others publicly available tungsten scene release code community rst potential topic investigate choice error metric perceptually important feature capture standard loss metric behave  notable sec sec important training thorough investigation perceptual loss function improve network training principled perceptual evaluation furthermore sample approach important patch image training although performance approach optimal feature metric sample patch acm transaction graphic vol article publication date july kernel predict convolutional network denoising monte carlo rendering network converge faster complicate relationship network hyperparameters optimal explore various layer kernel setting thorough parameter reveal dierent architecture concept yield improve performance explore recurrent residual connection benet however potentially useful explore deeper network improve performance model parameter tractable moreover generative model variational autoencoders generative adversarial network promise image super resolution denoising although resolution image computational hurdle avenue future research finally demonstrate denoising image useful handle animate sequence extension non trivial involves exploration architecture preserve temporal coherency across denoised frame concurrent focus denoising sequence interactive rate summary rst successful towards practically convolutional network denoising monte carlo render image production environment specically demonstrate approach recognize fundamental underlie relationship noisy reference data without overtting withstand strict production demand quality although relatively straightforward architecture robust stable evaluate performs favorably respect denoising algorithm