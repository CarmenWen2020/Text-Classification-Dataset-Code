machine technique 3D facial animation audio input latency neural network learns mapping input waveform 3D vertex coordinate model simultaneously discovers compact latent code  variation facial expression cannot explain audio alone inference latent code intuitive emotional puppet network quality animation data obtain traditional vision performance capture primary goal model style actor model yield reasonable driven audio speaker gender accent demonstrate user applicable dialogue localization virtual reality avatar telepresence CCS concept compute methodology animation neural network supervise regression latent representation additional facial animation audio introduction expressive facial animation essential  movie digital currently vision performance capture animate actor integral component production pipeline quality obtainable capture steadily improve quality facial animation remains computer vision elaborate setup labor intensive cleanup processing obvious issue whenever shot actor location ideally retain appearance challenge another role beard audio performance capture algorithm unlikely quality vision complementary strength importantly dialogue acm transaction graphic vol article publication date july spoken expensive vision consequently animation  vision rely audio transcript bulk unfortunately quality animation currently leaf desire emerge application telepresence virtual reality avatar additional challenge due lack transcript variability user physical setup stringent latency requirement goal generate plausible expressive 3D facial animation exclusively vocal audio animation account complex  phenomenon phoneme  lexical stress interaction facial muscle tissue hence focus entire lip adopt data driven approach neural network fashion replicate relevant training data intractable inherent ambiguity  vastly facial expression audio simply information distinguish variation convolutional neural network proven extremely effective various inference classification task tend regress ambiguity training data tackle contribution convolutional network architecture tailor effectively generalize speaker novel enable network discover variation training data cannot explain audio alone apparent emotional loss function ensure network remains temporally stable responsive animation highly ambiguous training data expressive 3D facial audio latency retain maximal independence detail downstream animation output per frame vertex  facial mesh alternative encoding blend non linear rig introduce later pipeline stage compression render  model quality footage obtain traditional vision performance capture goal model style actor model yield reasonable driven audio speaker gender accent technology dialogue localization virtual reality telepresence useful accommodate script  related review prior input audio text output 2D video 3D mesh animation approach linguistic machine model review apparent emotional model linguistics literature exists analyze understand structure translate  credible facial animation typically audio accompany transcript explicit knowledge phoneme content animation visual counterpart phoneme visemes complex  dominance model mapping phoneme visemes implement dynamic visemes model recent dubbed   factor facial animation lip jaw movement psycholinguistic consideration convincingly reproduce style apparent emotional independently actual content core strength explicit entire explicitly guarantee properly puppet  lip upper   capture weakness accumulate complexity specific  transcript quality inability react convincingly non phoneme lack principled animate besides jaw lip   com widely commercial package implement widely linguistic model dominance model model machine primarily machine  driven exclusively audio perform explicit analysis structure training stage estimate hidden markov model hmm dynamic video inference hmm sample sequence synthesize trajectory optimization considers entire utterance subsequent improve trajectory sample replace hmm piecewise linear approximation alternative representation gaussian latent variable model hidden semi markov model recurrent network alternatively machine  concatenation stage synthesize animation mapping various stage phoneme classification mapping text phoneme phoneme visemes acm transaction graphic vol article publication date july audio driven facial animation joint emotion mapping input audio feature parameter gaussian mixture model goal 3D animation audio inherently interested intermediate representation instead formulate entire mapping optimization task neural network audio directly parameter animate 3D mesh network necessarily trivial complexity revisit formulation convolutional network training specific contribution unfortunately comparison previous machine majority focus reuse capture video frame concatenation blending warp image realistic typically corpus frame directly applicable application VR animate render 3D model viewpoint typically flexible identity 3D text input instead audio aware publicly available implementation video quality standard risen dramatically sync available compute extract emotional automatic separation emotional author  freeman bilinear model apparent emotional visemes later generalize closely related topic multilinear non linear model independent component analysis extract emotion vector machine synthesize 3D animation apparent emotional compute mapping pre define emotional user specify animation synthesis compute  expression pre define emotional user specify emotional cluster adaptive training derive basis emotional interpolate extrapolate user rating realism emotion synthesis neural network mapping pad pleasure displeasure arousal  dominance  parameter facial expression distinguishes effort instead pre define category emotion network latent dimensional descriptor allows explain data descriptor parameter combination formant analysis network layer kernel stride output activation autocorrelation convolution relu convolution relu convolution relu convolution relu convolution relu articulation network layer kernel stride output activation conv concat relu conv concat relu conv concat relu conv concat relu conv concat relu output network layer kernel stride output activation fully linear fully linear detailed breakdown network autocorrelation layer performs fix function analysis input audio clip articulation network concatenate dimensional vector emotional output convolution layer relu activation max fully layer expand abstract feature 3D vertex network scalar later assign semantic meaning sad role residual almost appearance perceive particularly  uncanny author alleviate deduce additional movement saccade blink audio assume residual driven procedural limit scope directly related articulation network architecture architecture network along detail audio processing separation emotional content audio task network infer facial expression expression directly per vertex difference vector neutral fix topology mesh network animate mesh slide vocal audio evaluate network independently network memory animation frame temporally stable acm transaction graphic vol article publication date july architecture overview neural network consists purpose layer convolutional layer fully layer conceptual illustrate input audio formant analysis network sequence feature subsequently articulation network extract raw formant information fix function autocorrelation analysis refines convolutional layer training convolutional layer extract  feature relevant facial animation intonation emphasis specific phoneme abstract representation output convolutional layer articulation network consists convolutional layer analyze temporal evolution feature eventually abstract feature vector describes facial audio secondary input articulation network accepts description emotional disambiguate facial expression style emotional dimensional vector concatenate directly onto output layer articulation network enable subsequent layer alter behavior accordingly  output activation abstract feature dimension axis dimension formant axis stride convolution formant analysis network gradually reduce increase raw formant information abstract feature similarly convolution articulation network decrease subsample axis combine information temporal neighborhood chose specific parameter consistently perform datasets reasonable training hugely sensitive layer feature organize convolution distinct phase avoid overfitting importantly formant analysis network performs operation along axis benefit training sample offset articulation network output abstract feature desire facial feature output network 3D vertex mesh output network implement fully layer perform linear transformation data layer input feature linear basis layer calculates vertex sum correspond basis vector initialize layer precomputed pca component explain approximately variance training data principle fix basis effectively earlier layer output pca coefficient however basis vector evolve training yield slightly audio processing input network audio signal convert khz mono network normalize volume vocal utilize dynamic employ processing dynamic compression reduction pre emphasis filter autocorrelation layer convert input audio compact 2D representation subsequent convolutional layer approach inspire source filter model production audio signal model combination linear filter vocal tract excitation signal vocal cord resonance frequency formants linear filter essential information phoneme content excitation signal indicates timbre characteristic speaker hypothesize important facial animation focus primarily formants improve generalization speaker standard perform source filter separation linear predictive cod LPC LPC signal frame solves coefficient linear filter frame autocorrelation coefficient performs inverse filter extract excitation signal resonance frequency filter entirely autocorrelation coefficient skip processing simply autocorrelation coefficient directly representation instantaneous formant information intuitive autocorrelation coefficient essentially compress version signal frequency content approximately spectrum signal representation convolutional network layer easily estimate instantaneous specific frequency worth audio input future sample respect desire output chose capture relevant phoneme  without data overfitting input audio audio frame overlap frame corresponds sample consecutive frame sample apart audio frame remove DC component apply standard  reduce temporal aliasing finally calculate autocorrelation coefficient yield scalar input audio although  suffice identify individual phoneme retain information signal subsequent layer detect variation approach differs previous recognition analysis typically specialized technique mel frequency cepstral coefficient MFCC perceptual linear prediction PLP  filter technique enjoy adoption mainly linear separation phoneme consequently hidden markov model acm transaction graphic vol article publication date july audio driven facial animation joint emotion silence frame training actor input data representation autocorrelation coefficient clearly superior representation emotional infer facial animation inherently ambiguous facial expression eyebrow causal relationship production ambiguity problematic neural network training data inevitably nearly identical audio input output indeed conflict training data input audio clip consists entirely silence network besides audio output statistical conflict output approach resolve ambiguity introduce secondary input network associate amount additional latent data training sample network information unambiguously infer output ideally additional data encode relevant aspect animation neighborhood training sample cannot infer audio facial expression style  informally secondary input emotional actor besides resolve ambiguity training data secondary input highly useful inference allows emotional vocal powerful animation implement emotional manually label categorize training sample apparent emotion approach ideal however guarantee pre define label actually resolve ambiguity training data sufficient instead rely predefined label adopt data driven approach network automatically learns succinct representation emotional training allows extract meaningful emotional footage sufficient emotion emotional dimensional vector tunable parameter initialize component random drawn gaussian distribution vector allocate training sample refer matrix latent variable emotion database illustrate emotional append activation layer articulation network computation graph loss function trainable parameter update along network backpropagation dimensionality tradeoff emotional fail disambiguate variation training data weak audio response emotional tend become specialized useful inference potential concern emotion database unless constrain meaningful explicitly information audio pathological blend facial expression diminish role audio network useless processing training information audio limited within interval consequently prevent emotional duplicate information forbid variation emotional focus longer highly desirable inference network reasonable animation emotional remains fix express requirement introduce dedicate regularization loss function penalize variation emotion database incremental smooth emotional training important limitation approach cannot model blinking correctly correlate audio cannot slowly emotional redundant append emotional layer articulation network improve significantly suspect emotional aim animation multiple abstraction abstraction generally earlier layer nuanced subtle animation feature  whereas later layer output intuitive stage training concentrate latter later stage concentrate former individual reasonably emotional data without semantic meaning sad etc defer discussion semantics role network architecture training training aspect relevant training network training target obtain dataset consists dataset augmentation loss function training setup acm transaction graphic vol article publication date july video frame mesh video frame mesh training target obtain vision pipeline HD camera output 3D animate vertex frame training target obtain 3D vertex training target commercial  pro com employ synchronize video camera directly capture nuanced interaction skull muscle  actor exclude frequency detail wrinkle benefit allows bypass complex expensive facial rig tissue simulation digital input output data illustrate unstructured mesh texture optical data reconstruct image capture frame fix topology template mesh prior capture photogrammetry pipeline project unstructured mesh associate optical template mesh tracked across performance issue fix semi automatically  software artist orientation stabilize vertex mesh finally vertex mesh export frame shot precisely delta neutral desire output network audio training limitation video capture setup cannot capture tongue typically visible image tongue highly relevant production rarely visible clearly animate detail tongue wrinkle residual procedural animation training dataset actor training consists  inference quality increase training grows training highly desirable due capture quality training data per actor practical sweet  attempt facial normal target english actor speaks  phoneme emotional coverage expression leverage actor performance heavily bias emotional expressive various dramatic narrative movie production compose preliminary version script shot deem aspect ensure network output inference perfect completely novel encounter training consists frame min  additionally frame reserve validation training consists frame min  additionally frame validation loss function ambiguous training data define meaningful loss function optimize specialized loss function consists distinct ensure overall location output vertex roughly ensure vertex exhibit movement animation regularization discourage emotion database variation simultaneous optimization multiple loss tends wildly magnitude balance unpredictable training typical associate pre define ensure none neglect optimization however optimal tedious trial error typically whenever training overcome issue introduce normalization scheme automatically balance loss respect relative importance automatically amount effort optimize consequently specify additional primary error metric difference desire output output network training sample express output vertex denotes ith scalar component output component network output 3D vertex ensures output network roughly sufficient quality animation training network alone considerable amount temporal instability response acm transaction graphic vol article publication date july audio driven facial animation joint emotion individual phoneme generally weak motivates optimize network vertex output vertex training data vertex loss function brand HMMs velocity optimize standard approach training neural network iterate training data minibatches minibatch consists randomly training sample account vertex sample temporal consist adjacent frame define operator finite difference frame define factor evaluate per temporal regularization finally ensure network correctly attribute audio signal longterm emotional described conveniently define regularization emotion database finite differencing operator denotes ith component emotion database training sample definition explicitly forbid emotion database variation merely discourages excess variation average important training data contains legitimate emotional occasionally network incorrectly explain audio signal caveat arbitrarily zero simply decrease magnitude increase correspond network batch normalization remove trivial normalize respect magnitude EB normalization balance loss leverage adam optimization training network adam update network accord gradient loss function normalize component wise fashion accord estimate raw normalization training resistant difference magnitude loss function loss function individual perform normalization loss individually estimate raw minibatch maintain average across consecutive minibatches denotes minibatch index decay parameter average initialize estimate account startup bias calculate average minibatch normalize accord constant avoid zero equation express loss function sum individual although tune importance loss additional beneficial training data augmentation improve temporal stability reduce overfitting employ random shift training sample whenever minibatch network randomly shift input audio direction frame fps compensate apply shift desire output linear interpolation shift training sample temporal amount random shift amount cubic interpolation output linear crucial improve generalization avoid overfitting apply multiplicative input convolutional layer magnitude layer apply per feature basis activation feature factor apply identical training sample meaningful formula apply augmentation training sample besides shift input output multiplicative inside network adjust volume  perform shift apply non linear distortion random equalization scramble phase information none improve training setup parameter implement training setup theano  internally cudnn gpu acceleration network epoch adam default parameter epoch training sample randomize minibatches training sample temporal rate  tenfold geometric progression training epoch acm transaction graphic vol article publication date july decrease gradually accord schedule epoch ramp rate zero smooth curve simultaneously ramp adam parameter ramp remove occasional glitch network ramp ensures network converges local minimum training min min nvidia pascal titan network initialize emotion database initialize zero gaussian tune parameter per actor parameter tune difference probably explain mostly expression throughout training lively various extraneous movement inference RESULTS network evaluate arbitrary appropriate audio facial animation desire frame rate theano implementation inference frame frame batch frame theano overhead confident frame performance efficient framework cudnn directly latency audio currently future  bound ahead confirm experimentally limit ahead training minor degradation quality  longer shorten ahead perceive responsiveness realistic bound latency therefore around rely explicit trajectory optimization substantially latency emotional infer facial novel audio network emotional vector secondary input training network latent dimensional vector training sample strategy emotion database robust emotion vector inference training network attempt latent information everything  audio alone emotion database however decomposition perfect amount crosstalk remains articulation overall expression emotion vector applicable neighborhood correspond training frame necessarily useful inference training data necessarily limited phoneme  emotional emotion vector opening response rapid closure visualization opening audio clip axis constant emotion vector input axis emotion vector extract database clip respectively emotion vector opening closing properly database mining rapidly cull emotion vector respond audio pain understandable semantic meaning emotion vector assign manually examine animation neural network interested semantic meaning concerned emotion vector data disambiguate audio manually robust emotion vector emotion vector  vector constant input perform inference novel audio apparent  therefore audio validation  respectively scan emotion database vector exhibit desire behavior chosen perform preliminary cull candidate emotion vector consideration illustrates response varies emotion vector depict training shot contains highly responsive emotion vector candidate chosen consideration cull validation audio inspect facial infer acm transaction graphic vol article publication date july audio driven facial animation joint emotion emotional animation accompany video infer audio emotion vector candidate emotion vector stage discard emotion vector  spurious unnatural emotion vector taint stage narrow candidate emotion vector inference audio speaker eliminate emotion vector mute unnatural response severely attenuate response speaker lack generalization tends speaker varied articulation style emotion vector examine output network novel audio clip remain emotion vector assign semantic meaning neutral amuse surprised etc emotional convey semantic emotion remain depends entirely training extract emotion training data generalizable novel audio infer facial audio emotion vector remove perform emotion vector substantial variation performance capture inference inference ensemble graph coordinate vertex marked dot function performance capture inference ensemble prediction remove jitter inference neutral neutral transfer deformation transfer interestingly emotion vector behave interpolation sweep emotion vector another tends emotional inference information manual  temporal stability accompany video stable animation primary source temporal stability shift augmentation technique amount jitter lip timescale input likely due aliasing audio frame around  fix via ensembling network evaluate twice animation frame apart prediction average related approach timescales illustrates shift ensembling ambiguity remain eyebrow employ additional temporal smooth upper gaussian filter retargeting model output network becomes specialized mesh application mesh audio via deformation transfer acm transaction graphic vol article publication date july PC PC DM DM clip clip clip clip clip clip clip clip clip clip clip clip clip vote ratio blind user video performance capture PC dominance model animation DM pairwise quality comparison input audio clip validation dataset correspond actor text detail setup evaluation ass quality conduct blind user participant professional animation technique output video performance capture  PC dominance model animation  software DM audio clip input validation dataset correspond actor training data training network feature animate audio clip clip video render PC PC DM DM video participant choice random randomize individual participant unaware video originate progress video animation approximately video frame per audio assign audio clip constant emotion vector emotion database explain  creates animation interpolate pre define target neutral target tongue rotation blink  eyebrow ignore tongue rotation blinking  target manually training spent per target  documentation closely male female male female DM DM DM DM english english english french german italian spanish vote ratio user dominance model DM speaker male speaker female speaker evaluate separately related target upper smoothly masked unwanted similarly masked related  transcript audio clip basis viseme analysis sum participant output video performance capture generally perceive animation synthesize dominance model fifth clip par performance capture exception dominance model performance capture clip surprising unnatural performance capture data instead exceptionally performance clearly outperforms dominance model pairwise comparison clip participant prefer vote additionally fare comparison video performance capture estimate objective quality rank elaborate user vote performance capture closer vote dominance model generalization ass capability network generalize speaker conduct user representative audio clip extract public domain audio host   org output network audio clip training network tune parameter mining emotional clip english french german italian spanish organize clip gender consist male speaker female speaker setup user participant acm transaction graphic vol article publication date july audio driven facial animation joint emotion video dominance model DM video participant audio clip video approximately summarize male speaker participant prefer roughly somewhat female speaker however likely explanation comparison video feature male audio female speaker participant perceive output equally unnatural variation considerably variation speaker suggests overly sensitive input per capability generalize novel audio likely related style tempo speaker accompany video impossible demonstrate quality animation text image refer reader accompany video video comparison video performance capture dominance model render dynamic visemes  video audio footage comparison network speaker training fluent expressive temporal stability entire generalizes speaker synthetic audio audio synthesize WaveNet neural network generates audio input text video  animation male female synthetic probe limit speaker feature emotional video english network responds surprisingly input audio tempo training data future primary focus generate quality facial animation dialogue evaluate practical production perform informal gauge suitability purpose audio clip casual consumer grade equipment background remain responsive input volume normalize roughly training data animation plausible displayed sync audio tempo future principled related realistic interactive speaker shortcoming lack detail performance capture data combine approach generative neural network enable synthesis detail possibly residual plausible emotional min training data increase dataset likely improve particularly network simultaneously attempt latent unified representation identity conceivably deduce emotional automatically inference longer audio context