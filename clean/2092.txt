render gradient domain estimate finite difference gradient image intensity correlate sample combine estimate pixel intensity screen poisson fundamental benefit merely sample pixel intensity trace frequency content transport integrand interplay gradient operator however yield performance algorithm monte carlo sample alone gradient domain render algorithm generally competitive technique combine monte carlo sample hoc removal sophisticated non linear filter convolutional neural network propose novel reconstruction gradient domain render technique replaces screen poisson solver previous gradient domain technique novel dense variant net autoencoder additionally auxiliary feature buffer input optimize network minimize perceptual image distance metric calibrate visual significantly improve quality obtain  trace overtake comparison technique denoise traditional monte carlo sampling correlate gradient sample information smoothness integrand unavailable standard monte carlo sample notably improve image quality equally powerful neural model gradient sample CCS concept compute methodology neural network ray trace additional gradient domain render  reconstruction screen poisson ray trace introduction realistic image synthesis seek realistic virtual photograph computationally render equation randomly sample source sensor render sample leaf image visually distract unsurprisingly practical application constantly struggle strike balance complexity content available computational resource monte carlo sample quality image leaf approach render faster sample faster evaluate gpu render ray trace hardware optimize algorithm contribution nearby photon mapping clever sample bidirectional trace adaptive importance sampler finally denoising reconstruction attempt sample rely various smoothness assumption analytic model transport phenomenon model despite continuous research progress significant remain naturally quality obtain pure technique rely assumption heuristic tends lag assume instance acm trans graph vol article publication date july kettunen diffuse albedo normal cheap  estimate helper variable predictor smoothness image heuristic naturally yield however algorithm assumes anyway reconstruction overly smooth blurry generally validity assumption novel scene concrete presence particularly indirect shadow cannot reliably detect easily available auxiliary variable denoising algorithm struggle shadow fidelity recently introduce gradient domain render algorithm combination approach addition estimate pixel intensity compute monte carlo estimate image gradient correlate sample combine sample image screen poisson combine gradient sample integration exploit frequency distribution transport integrand quality within sample budget choice norm screen poisson equation technique remain unbiased consistent exploit smoothness reconstruction without rely heuristic auxiliary helper variable seek combine smoothness information gradient sample performance denoising technique auxiliary feature aim adapts myriad transport configuration input signal trust correlate coherence highly  task convolutional neural network minimize perceptually motivate loss function network novel hybrid densely convolutional network densenet net autoencoder skip connection model relatively implement exist gradient domain tracer splitting contribution diffuse specular channel kernel predict convolutional network KPCN surpass performance KPCN nonlinearly regression NFOR denoisers scene gradient sample clearly improve otherwise model fed primal image reconstruction significantly improves quality reconstruct shadow despite sample comparison related briefly summarize important related refer reader discussion denoising shea nash introduction convolutional neural network denoising non local filter NL  pixel average pixel image local neighborhood extend NL filter monte carlo rendering reconstruction sample placement combine  filter feature buffer stein unbiased risk estimate evaluate regression NL  feature buffer regression data local polynomial regression heuristic optimal polynomial pixel multilayer perceptron compute optimal filter parameter bilateral NL filter primal auxiliary feature buffer closest kernel predict convolutional network KPCN convolutional neural network predicts smooth filter kernel pixel split input diffuse specular transport pipeline extend animation modular multiple sampler denoising network  convolutional neural network multi resolution net network structure enhance recurrent connection animation tailor extremely sample gradient domain render introduce gradient domain render beneficial metropolis transport context estimate finite difference adjacent pixel sample highly correlate reconstruct image typically noisy noisy gradient screen poisson equation screen poisson equation unbiased recommend slightly bias consistent version typically visually intuitively performance gradient domain renderers applicable explain examine screen poisson information spatially non trivially image patch adapt trace gradient domain render subsequently render adapt gradient domain gradient domain bidirectional trace  reuse gradient domain stochastic progressive photon mapping gradient domain vertex connection merge render homogenous volume gradient domain adaptation beam radiance estimate progressive photon beam photon enforce exploit temporal coherence extend gradient computation acm trans graph vol article publication date july convolutional reconstruction gradient domain render domain mapping random successive animation frame temporal extension screen poisson equation somewhat related regularize screen poisson reconstruction constraint expressible locally linear combination truncate svd auxiliary feature buffer reconstruction gradient domain render formulate reconstruction variate formulate ideal feature local regression denoising approximate  render effectively gradient domain render adaptive polynomial render combine gradient domain sample powerful model neural network convolutional neural network powerful parametric non linear model apply image restoration task denoising tune performance training dataset noisy image correspond target image attempt model prediction target noisy input net network architecture powerful modification classical autoencoder encoder decoder model widespread image restoration execution proceeds towards resolution processing convolutional layer resolution downsampling bottleneck layer mirror net innovation addition skip connection link faraway encoder decoder significantly task pixel accurate facilitate training introduce densenet architecture layer previous layer input facilitates reuse computation improves gradient improve accuracy easy training network previous medical image partially combine densenet net architecture hybrid architecture defer discussion perceptual loss image similarity corresponds judgment recently formalize accumulate anecdotal knowledge hidden variable convolutional image classifier network perceptual introduce perceptual image patch similarity LPIPS image image classification network vgg squeezenet return distance network activation concurrent LPIPS robust optimization ensembled modification LPIPS retains predictive LPIPS robust optimization target insight apply randomize geometry transformation image feature robust LPIPS training model perceptual loss image restoration task novel approach neural reconstruction gradient domain render algorithm addition monte carlo estimate pixel intensity primal estimate finite difference pixel gradient idx  correlate sample sample image reconstruct argmin idx  denotes finite difference gradient operator pixel parameter derive relative variance primal gradient sample norm unbiased image outlier suppress norm prefer norm efficient solver incorporate auxiliary feature information depth normal albedo without potentially complex smoothness prior equation optimize complex non linear perceptual image distance metric route cast reconstruction regression convolutional neural network cnn cnn idx  additional input auxiliary feature buffer detail precise input model earlier image restoration model parameter obtain minimize empirical risk argmin cnn training comprise noisy primal gradient input correspond target variant stochastic gradient descent loss function difference prediction target lose analytic tractability equation prior solver training ahead numerical optimization leaf complex loss function admit analytic overall strategy leaf choice network architecture choice loss function directly affect characteristic loss model powerful remain trainable treat network architecture model popular net architecture hourglass autoencoder non local skip connection link correspond resolution encoder acm trans graph vol article publication date july kettunen decoder novel combination net  diagram describes network structure overall net computation proceeds resolution resolution skip connection link correspond layer resolution consists chain convolutional layer non linearity latter resolution receives data resolution resolution image along describes convolution kernel feature resolution neural network densely densenet layer receives input previous layer activation concatenation loosely previous layer resolution resolution dense net architecture logical conclusion resolution entirety dense decoder additional input resolution detailed improves somewhat standard net loss function dynamic dynamic neural network reconstruction monte carlo render magnitude input widely indeed dynamic input unbounded particularly  regularly extremely outlier stark contrast behavior neural network tend input uniform strategy network input logarithmic domain apply network domain invert mapping network exp approach immediate challenge apply gradient domain gradient negative consequently logarithmic mapping directly however behave origin extension gradient odd reflection sgn monotonous function inverse sgn exp extension negative input negative target pixel filter negative lobe perceptual loss goal efficient render normally image residual error judged observer widely pixel wise norm correspond image similarity perceive  LPIPS perceptual image distance metric image non linear feature pre image classifier cnns precisely employ ensembled variant LPIPS robust optimization target comparison loss logarithmic decompression network yield dynamic output desire evaluate perceptual LPIPS loss yield complication input apply  mapping evaluate loss LPIPS image non linear sRGB image gamma sRGB conversion formula evaluation sRGB approximation sRGB due singularity derivative zero training unstable regularization loss judgment sensitive error LPIPS loss somewhat insensitive brightness reconstruction accurately predict complement training loss component apply  image gradient gradient negative odd reflection mapping avoids instability hdr input combine component loss function LPIPS sRGB sRGB network detail convolution stride convolution pool convolution transpose unpooling input transformation  network apply exp rgb layer nonlinearity leaky ReLUs factor convolutional layer purpose channel image parameterization optimization schedule normalization initialization optimize adam parameter rate  geometrically minibatches image slowly decrease minibatches halve approximately curve LPIPS loss stochastic evaluate  predict image training training sample somewhat expensive significantly acm trans graph vol article publication date july convolutional reconstruction gradient domain render conv  conv  conv pool  conv   processing input feat output rgb linear resolution feature network architecture gradient feature multiple resolution repeatedly concatenation previous resolution processing contract feature convolution applies convolution contract feature input future layer resolution resolution previous resolution concatenate pool stride convolution resolution  concatenate convolution transpose leaky ReLUs apply convolution convolution network exp undo initial mapping input reduces  convergence earlier finding optimization schedule hyperparameters resolution chosen approximately minimize LPIPS loss training orthogonal LPIPS loss training data augmentation augment dataset randomly flip mirror transpose  training mirror pad image randomly amount pad direction pixel essentially network differently offset version image internal working LPIPS loss training data augmentation serf purpose randomly permute channel sample nonnegative random multiplier  apply gradient albedo buffer sample normal brightness multiplier apply gradient buffer input data format input standard gradient buffer gradient domain renderer along albedo normal depth feature useful feature variance significant improvement simplicity transform normal camera perspective normalize depth radiance quantity compress detailed normalize depth buffer training image consist normalize network unrealistic depth distribution instead normalize sample uniformly slightly training non issue network encounter slightly RESULTS discussion evaluation seek usefulness gradient sample context denoising technique neural otherwise comparison technique algorithm kernel predict convolutional network KPCN generalpurpose supervise technique yield denoising performance moderately sample NFOR algorithm machine technique instead relies priori model furthermore screen poisson solver previous gradient domain renderers baseline although strictly information utilize auxiliary feature compensate overhead gradient sample non gradient input buffer sample scene scene almost diffuse sponza chosen establish easy baseline algorithm perform harder bookshelf kitchen feature glossy specular transport additionally feature depth bedroom yield gradient variance due transport transparent curtain bookshelf kitchen din generalization within convex hull training training scene complicate sponza contrast generalization outside convex hull training data depth blur rarely training data gradient variance curtain bedroom training data acm trans graph vol article publication date july kettunen sponza easy bookshelf kitchen din bedroom failure spp input poi NFOR KPCN truth spp LPIPS RelMSE comparison trace baseline screen poisson reconstruction feature denoiser NFOR kernel predict convolutional denoising propose trace NFOR KPCN sample parenthesis acm trans graph vol article publication date july convolutional reconstruction gradient domain render metric quality metric relative error RelMSE LPIPS RelMSE error pixel relative brightness judgment avoid measurement bias LPIPS vgg version LPIPS squeezenet version  image equation LPIPS measurement overall performance analysis screen poisson NFOR KPCN across variety scene training model focus particularly sample regime previous gradient domain render algorithm employ feature buffer struggle KPCN NFOR sample equalize generate input inset sample numeric evaluation sample per pixel gradient poisson generally overall subjective quality previous numerical error measurement steady improvement sample increase attribute training model input sample scene difficulty model estimate remove strict consistency guarantee nonlinear model easy strictly ensure consistency simply blending reconstruction screen poisson increase sample sample pixel RelMSE bedroom bookshelf kitchen din sponza convergence gradient sample per pixel despite training mostly target sample regime steady convergence sample slight slowdown din scene sample per pixel network input image gradient sample supplemental contains image html viewer facilitates detailed comparison sponza feature relatively easy transport visible significantly noisy input perform relatively yield sample regime visible difference fade relatively quickly increase sample bookshelf kitchen din scene typical scene specular glossy transport geometric detail bookshelf kitchen feature source din lit blind shadow glossy reflection din feature complexity clearly improves scene sample scene kitchen depth feature animate blur scene relatively glance metallic  scene ceiling challenge scene bedroom enters scene translucent curtain extremely gradient sampler indeed image relative variance gradient primal sample increase numerical performance KPCN sample dense net without gradient overall interestingly error model LPIPS loss visible hallucinate detail none exist visible numerical detail previous gradient domain render algorithm gradient variance relative primal sample detrimental quality scene training useful apart pathological bedroom generally improves KPCN particularly  regime scene advantage decrease slightly sample author KPCN originally specialized network sample KPCN network variable amount training concentrate sample besides generally artifact trend tends capture shadow accurately scene bookshelf kitchen din attribute smoothness information implicitly gradient sample demonstration invite reader html viewer detailed inspection ablation gradient useful demonstrate performance overall across comparison architectural difference network KPCN warrant role gradient domain sample cnn precisely architecture difference gradient input loss training procedure etc remain unchanged supplemental image viewer contains image label grad KPCN NFOR input acm trans graph vol article publication date july kettunen RelMSE RelMSE grad var grad var primal input spp grad truth var var RelMSE RelMSE image RelMSE distribution reconstruction bedroom scene challenge network without gradient gradient beneficial correlate highly gradient variance relative primal variance originally screen poisson inset gradient reconstruction error grad relative gradient variance vice versa RelMSE NFOR KPCN variation gradient truth din kitchen visual comparison shadow NFOR KPCN network without gradient shadow cannot predict traditional feature buffer gradient denoisers tend reconstruct shadow  capture gradient sample gradient domain reconstruction typically reconstructs shadow accurately sample compensate reduce sample maintain demonstrate gradient domain version performs consistently non gradient version exception curtain bedroom gradient domain trace scene render without gradient scene depth without gradient improves gradient difference gradient bedroom scene noisy due gradient shift implementation partially renderer handle  curtain increase bias fix acm trans graph vol article publication date july convolutional reconstruction gradient domain render  kit din LPIPS RelMSE sample pixel network gradient typically improves network without gradient orange input sponza din bedroom scene gradient sampler blur text relative variance gradient abundance sub pixel geometric feature complex obey assumption shift mapping overall benefit obtain sample gradient addition pixel radiance non linear neural reconstruction smoothness information finite difference sample algorithm unfortunately complex non linear network analysis style earlier gradient domain render perceptual loss useful earlier perceptual error metric training network LPIPS loss instead traditional loss yield improvement perceptual visual quality likely slight per pixel accuracy reconstruction generally slightly blurry detailed training network predict pixel wise median potential truth image network loss apply  tonemapped image generally representative ups LPIPS loss yield structurally image exception actually suppose blurry due defocus LPIPS network hallucinate spurious detail whereas network ambiguous situation instead hallucinate blur careful distribution training alleviate issue remains future variation loss actual LPIPS loss truth kitchen spp bookshelf spp network loss slightly blur image LPIPS capture structure image prediction variation loss actual LPIPS truth spp scene depth blur noisy input output suppose smooth network loss naturally resolve input blur network LPIPS prone spurious detail network  satisfactory numerically training LPIPS loss slightly improve numerical blur improve clarity generalize sample visual inspection numeric slightly network recommend reader judge sample regime viewer supplemental dense net network structure novel hybrid densenet net network perform consistently traditional net capacity temporal stability algorithm image orthogonal technique exist exploit temporal coherence neural denoising gradient domain reconstruction temporal behavior algorithm relevant acm trans graph vol article publication date july kettunen  kit din LPIPS RelMSE sample pixel improve image structure clarity gain training LPIPS loss oppose loss visible medium sample typical scene sponza din training loss handle blur sample trace equivalent supplemental sample sample pixel LPIPS RelMSE bookshelf RelMSE LPIPS net dense net architecture consistently improves traditional net structure model parameter perceptual LPIPS loss solid RelMSE dash scene conduct temporal stability render video sample static camera random frame video available supplemental  temporal flicker KPCN earlier reprojection technique apply alleviate issue sample pixel LPIPS RelMSE bookshelf RelMSE LPIPS average max bilinear lanczos RelMSE LPIPS average max bilinear lanczos network stride convolution downsampling transpose convolution upsampling option average max pool upsampling bilinear lanczos resampling convolution pool convolution transpose unpooling usually performs although margin LPIPS solid RelMSE dash bookshelf scene choice pool empirically validate decision stride convolution pool correspond convolution transpose unpooling training generalization training consists sample supplement training augmentation relationship training scene training consists scene scene predefine guaranteed occlusion sample random camera normally distribute velocity vector simulate blur camera towards predefined target randomly chosen aperture simulate depth sample pixel render sample target image sample input image sample sample uniformly emphasize sample image quality varies faster training render gradient domain trace implementation built mitsuba renderer validation reconstruction quality overly sensitive scene training fix training split instead supplemental training implementation detail software hardware training implement algorithm tensorflow network KPCN acm trans graph vol article publication date july convolutional reconstruction gradient domain render truth spp KPCN spp spp KPCN spp frame variance shutter comparison KPCN render bookshelf scene shutter image construct image animation frame flicker animation render sample variance pixel independent render temporally unstable KPCN supplemental comparison video nvidia tesla gpu training network around training network significant qualitative improvement model optimization schedule author procedure pre KPCN diffuse specular target tune non target image pixel network per image KPCN per image publicly available NFOR implementation image approx intel cpu model network trainable parameter whereas KPCN model chosen improvement KPCN default parameter author input data format KPCN NFOR feature buffer albedo depth normal KPCN NFOR additionally sample variance buffer NFOR render image sample bucket KPCN separation transport diffuse specular component neither compensate gradient NFOR KPCN sample  input render CONCLUSIONS gradient domain trace combine powerful model replace screen poisson solver prior consistently visibly improves monte carlo render reconstruction model without gradient generally gradient sample beneficial despite increase  visual inspection numerical error measurement indicates smoothness information gradient sample benefit non linear feature driven reconstruction domain additional neural model ability optimize metric judgment visual similarity traditional per pixel norm extension MS ssim optimize network minimize variant  hyper parameter training generally improve image quality blurring unfortunately non linear LPIPS metric cannot employ recently propose  training technique converge target image instead noisy data alone seek overcome limitation remains future combine technique orthogonal ensure temporal consistency render animation  keen investigate potential usefulness kernel prediction gradient domain reconstruction instead prediction approach