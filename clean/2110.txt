article trace massive scene multiple gpus approach analyzes memory access tracer defines scene data distribute across gpus minimal performance concept scene amount memory access replicate gpus propose maximize performance trace partially distribute scene data memory management therefore tracer data structure redesign approach applicable tracer minor code proof concept enhance source blender cycle tracer approach validate scene GB scene data replicate machine scene scene verify performance render fully replicate scene scalability achieve parallel efficiency gpus CCS concept compute methodology computer graphic ray trace distribute algorithm additional multi gpu trace NVLink cuda unified memory data distribute trace distribute memory trace introduction article trace massive scene combine performance importantly memory multiple gpus propose approach applicable production render recent advance gpu technology gpu render popular superior cpu render however significant drawback gpus CPUs limited memory CPUs dominant role production render described author render film production instance scene  coco movie GB scene memory gpu technical contribution replication amount scene data chosen distribution data memory gpus replication distribution data memory access analysis tracer spp  data memory access replicate memory gpu access minimizes penalty associate reading data remote memory effective linear scalability knowledge article multi gpu trace massive scene scene data split memory gpus contribution demonstration propose approach memory management trace code gpu acceleration cuda adopt approach without redesign internal data structure proof concept propose mechanism implement complex source blender cycle trace approach relies technology NVLink gpu interconnect enables multiple gpus efficiently content memory due bandwidth latency cuda unified memory UM programmer data placement across memory interconnect gpus professional computer graphic sector gpus NVLink however performance compute hpc NVLink already interconnect gpus nvidia DGX GB gpu memory merely server equip gpus hardware acceleration trace ray trace RT core article organize describes information blender cycle modification tracer multi gpu render describes hardware platform scene benchmarking describes propose analysis memory access trace aspect data management memory affect performance performance scalability evaluation massive scene finally summary article future related majority film production render rely trace almost exclusively CPUs deliver image scene limited memory gpu limited local memory core render distribute parallel render discus approach attention distribute render propose belongs core render gpu accelerate render scene mostly handle core technique approach dealt core data management heterogeneous architecture consist multiple cpu gpu node address trace various device configuration approach core render heterogeneous cluster cpu gpu node network interconnection multi gpu render photorealistic render core approach texture dynamic schedule efficient parallelization implement  render gpus mainly contribute rasterization trace propose render capable iteration scene  gpu ray trace algorithm computation directional occlusion spherical integral gpu render framework handle massive scene core geometry complex gpu apply instead trace distribute render distribute render technique massive scene distribute memory standard classification propose category sort imagebased partition sort related rasterization sort scene data distribution image parallel render sort screen partition workload distribute processor machine per pixel render image acm transaction graphic vol article publication date april gpu accelerate trace massive scene efficient scene data fully replicate local memory ray trace embarrassingly parallel ray trace complex scene local memory approach demand scene data movement ray remains fix propose scene data communication ray gpu data parallel render sort data parallel distribute workload subdivide scene data distribute ray trace approach transfer ray data processor machine scene data initial distribution approach production render cluster workstation recently approach scientific visualization massive data scientific simulation supercomputer propose hybrid combination image parallel data parallel approach distribute memory supercomputer dynamic ray schedule propose dynamic situation ray communication exceed data load quality render resolution distribute memory memory processor SMP amount distribute memory dsm become commercially available machine processor local memory cache hardware software layer transparently creates illusion global memory application multiple processor memory preserve coherent data various cache coherent mechanism dsm exhibit non uniform memory access numa latency access remote data considerably latency access local data machine data locality therefore critical performance improve data locality cache coherency replication migration technique summarise concept propose approach addition data replication performance dsm improve application specific technique reduce memory bandwidth requirement ray trace incoherent ray technique propose   dsm  origin  stanford dash ray trace scene   respectively author screen partition propose distribution load balance synchronization interactive render cuda unified memory multi gpu nvidia unified memory UM manages communication multiple gpus CPUs transparently adopt dsm technique alternative approach propose UM simplifies core processing gpus CPUs multi gpu processing combination previously application focus data processing gpus algorithm specific technique memory handle HW technology NVLINK interconnect enabler dsm multi gpu thorough evaluation variant NVLink interconnects pcie bus evaluate performance advanced UM feature prefetching user data placement platform pcie interconnect CPUs gpus NVLink interconnect cpu critical mechanism UM prefetching eviction due memory subscription migration gpus propose algorithm improve UM performance transparent memory management contrast approach placement replication manually analysis memory access CG UM multi gpu efficiently NVLink image composition distribute render gpus utilized multiple gpus scalable split frame render SFR assigns disjoint frame gpus finally multi gpu render virtual reality exploit data locality scene reduce inter gpu memory traffic blender  tracer blender production renderer cycle unbiased renderer unidirectional trace cpu gpu render ray trace acceleration bound volume hierarchy bvh bvh code implementation nvidia additional code adaptation  gpu render cycle cuda optix OpenCL technology cuda OpenCL implementation feature cpu shade  advanced volume sample optix enables hardware acceleration ray trace gpus RT core gpus turing architecture extension multi gpu previous extend cycle parallel hardware architecture intel xeon phi processor processor hpc technology distribute render mpi extend cycle version although none extension cycle version directly approach render routine CPUs cuda kernel gpus gpus server acm transaction graphic vol article publication date april  implement propose approach gpu cycle importantly core trace cuda kernel render remains unchanged modification inside kernel concern implementation software counter memory access statistic render functionality kernel remains completely unchanged modification split data structure tracer chunk predefined access per chunk performance overhead extra recommend modification kernel significant cpu code cuda unified memory replicate distribute chunk scene data multiple gpus  instead cudaMalloc  data placement hint introduce cuda cuda UM mechanism thoroughly described shorter description appendix overall workflow distribute data structure evenly gpus kernel memory access counter memory access statistic redistribute data structure gpus memory access statistic trace kernel redistribute data extend version cycle distribute renderer render image multiple server replicate scene data command blender preprocess scene generate scene description trace file file cycle command parameter multi gpu benchmark article multi gpu platform perform massive scene render  NVLink blade server tesla gpus GB memory NVLink interconnect server instal barbora hpc cluster  therefore refer barbora advanced platform nvidia DGX massive scene GB memory tesla gpus GB memory uniqueness platform enhancement NVLink interconnect  enable connection gpus bandwidth machine portfolio provider hpc hardware parameter platform related article summarize detail appendix benchmark scene scene benchmarking moana scene parameter HW platform validation propose approach server gpus local memory bandwidth latency remote memory bandwidth latency gpu memory barbora GB GB GB DGX GB GB GB bandwidth latency benchmark due uniqueness acceptance research community production grade benchmark museum scene extend sculpture model increase geometric complexity painting increase texture finally remain scene recent movie animation studio blender institute namely agent operation  scene subdivision functionality increase texture resolution data  multi gpu  propose minimize negative impact remote memory access algorithmic maximize trace performance scalability propose approach splitting data structure tracer approach granularity data placement evaluate data distribution affect overall performance complex methodology entire data structure introduce advanced methodology data structure chunk chunk memory individual gpus description methodology demonstrate moana GB moana GB scene barbora DGX server respectively GB refers overall data structure cycle trace scene memory gpu respective platform therefore scalability evaluation gpus distribution entire data structure approximately data structure cycle scene structure access reading data structure independently replicate distribute across gpus memory access analysis define data structure replicate ratio memory access data structure analyze behavior cycle modify access data structure analysis sample render scene resolution pixel acm transaction graphic vol article publication date april gpu accelerate trace massive scene structure memory access per byte structure data structure MB important svm node shader virtual machine svm data code bvh node bvh without leaf leaf structure prim tri  coordinate vertex scene prim tri index scene contains index prim tri  examine moana GB scene DGX server evidently important data structure bvh node responsible memory access replicate memory gpus memory access local memory solid structure GB entire scene remain trace performance affected distribution replication data structure scene replication data structure split MB chunk distribute robin fashion gpus naive distribution almost longer render gpus performance penalty decrease gpus trace fully replicate scene baseline relative render evaluation relative render decrease replicate structure bvh node gpus addition structure bvh node prim tri index prim tri  replicate relative render scene replicate distribute consequently memory allocation per gpu GB instead GB performance scalability evaluation scalability propose approach evaluate data structure replicate serf baseline achieves performance scalability data structure evenly distribute structure bvh node replicate data structure distribute structure bvh node prim tri index prim tri  replicate data structure distribute data distribution evaluate continuous distribution structure chunk structure gpus gpu chunk robin distribution distribute structure chunk MB distribute robin fashion GB replicate GB gpus distribute GB per gpu MB chunk performance optimal scene chunk yield performance  memory hint replicates chunk gpus analysis memory access important data structure solid impact data structure distribution replication render performance replication distribution data structure render sample per pixel image resolution pixel conclusion robin distribution chunk performs continuous distribution chunk therefore distribute non replicate data structure trace fully distribute data structure platform reasonable scalability gpus DGX beyond structure bvh node replicate scalability significantly improve barbora speedup gpus DGX speedup gpus gpus moana 2GB moana 7GB scene respectively structure bvh node prim tri index prim tri  replicate scalability improve barbora speedup gpus DGX speedup gpus moana GB moana GB scene respectively advanced distribution memory access statistic advanced data placement algorithm advantage unified memory mechanism data placement hint introduce cuda   acm transaction graphic vol article publication date april  scalability cycle tracer data distribution entire structure barbora gpu server DGX moana GB scene render sample resolution data placement chunk hint chunk individually optimal chunk identify experimentally benchmarking tracer performance chunk MB scene GB optimal chunk MB chunk recommend scene around GB optimal chunk MB scene GB optimal chunk MB workflow data placement strategy summarize detail subsection distribute data structure across gpus robin fashion chunk optimal trace kernel memory access counter spp statistic statistic cpu propose algorithm optimal data chunk distribution  migrate replicate chunk unmodified trace kernel memory access analysis identify memory access per chunk access counter implement gpu trace kernel cycle independent counter data structure chunk therefore memory access per chunk gpu memory analysis data structure evenly distribute robin distribution modify trace kernel memory access counter execute gpus sample kernel chunk structure access gpus data input data placement algorithm described render analysis render workload distribute gpu image workload distribute gpus horizontal stripe gpu stripe load balance height stripe analysis memory access moana scene scene data memory access scene significantly entire structure illustration comparison dash solid another important moana GB GB GB GB scene respectively scene data chunk access conclusion drawn moana GB scene structure bvh node prim tri index prim tri  memory access scene replication GB data per gpu gpus scene memory access replicate data GB data per gpu analysis candidate chunk replicate gpus portion data access infrequently distribute acceptable impact performance algorithm decides placement chunk analysis described data placement algorithm memory access algorithm dimensional array memory access counter counter gpus data structure chunk within data structure output algorithm optimal location chunk gpu chunk GB replicate GB gpus distribute GB per gpu GB replicate GB gpus distribute GB per gpu acm transaction graphic vol article publication date april gpu accelerate trace massive scene analysis memory access cycle tracer moana GB scene scene data broken MB chunk memory access scene comparison data dash advantage decides chunk distribute replicate gpus per gpu counter sum access asum chunk data structure asum tuple tuple chunk scene data tuples 1D array  array sort asum access array input algorithm chunk replicate  manually automatically formula  amount memory per gpu MB available scene data scene MB gpus chunk MB MB scene define threshold  sort array evaluate tuples array counter asum correspond chunk  therefore replicate chunk  assign gpu access chunk memory gpu gpu fourth access gpu memory counter zero without access correspond chunk distribute robin fashion across gpus memory performance evaluation performance propose algorithm evaluate ratio replicate distribute data MB chunk granularity moana GB scene replicate chunk fully distribute replicate chunk difference chunk distribute approach previously chunk naive analysis trace performance ratio replicate distribute data moana GB GB scene runtime sample per pixel observation barbora server gpus GB memory render moana GB scene DGX gpus GB memory scene fully replicate robin fashion however chunk memory gpu access chunk gpu memory gpu amount data whilst attempt simultaneously chunk memory gpus access explains performance fully distribute scene replication trace performance sample per pixel pixel resolution scene platform available gpus baseline evaluation runtime gpu DGX scene scene moana GB scene speedup fully replicate scene gpus platform parallel efficiency DGX speedup gpus parallel efficiency moana GB scene gpus speedup parallel efficiency positive scenario trace embarrassingly parallel platform performance difference fully distribute scene chunk replicate performance almost identical moana GB scene gpu memory barbora platform gpus chunk replicate however parallel efficiency gpu baseline gpu DGX GB memory gpus chunk replicate barbora server yield parallel acm transaction graphic vol article publication date april