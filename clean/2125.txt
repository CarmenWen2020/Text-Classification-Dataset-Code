introduce novel neural network architecture encode synthesis 3D particularly structure insight 3D effectively characterize hierarchical organization reflect fundamental intra relationship adjacency symmetry develop recursive neural net RvNN autoencoder unlabeled arbitrary layout compact code code effectively capture hierarchical structure 3D structural complexity despite fix dimensional associate decoder code hierarchy bidirectional mapping tune adversarial setup yield generative model plausible structure novel structure sample finally structure synthesis framework augment module grain geometry global local structural context generative pipeline 3D demonstrate without supervision network learns meaningful structural hierarchy adhere perceptual principle compact code enable application classification partial synthesis interpolation significant variation topology geometry CCS concept compute methodology computer graphic analysis additional analysis synthesis structure symmetry hierarchy recursive neural network autoencoder generative recursive autoencoder generative adversarial training introduction recent progress training neural network image synthesis achievable generative model 3D image naturally 2D signal pixel sample 1D audio canonical representation 3D voxels mesh multi image yield consensus unlike image 3D parameterization regular dimensional grid 3D artifact highly structure hierarchical decomposition nest symmetry exhibit structural variation within variety hence stationarity compositionality acm transaction graphic vol article publication date july RvNN encoder RvNN decoder code random generator discriminator generate structure generate structure 2D code voxelization RvNN auto encoder pre training RvNN gan training 2D code volumetric geometry synthesis training training overview pipeline stage pre training RvNN autoencoder obtain code gan module actual manifold within code network convert synthesize  detailed geometry assumption neural net image longer applicable interested generative neural net structure representation 3D structure define arrangement relation develop neural net structure representation significant departure exist convolutional neural network cnns volumetric representation primarily adapt classical cnn architecture image analysis explicitly encode synthesize arrangement relation symmetry goal generative neural net structure characterize  challenge fold properly jointly encode synthesize discrete structure continuous geometry due intra structural variation treat structure graph foremost enable generic neural network graph combinatorial structure challenge unique neither address network input unstructured fix dimensional grid data image volume insight structure naturally hierarchical hierarchy jointly encode structure geometry importantly regardless variation across structure cod scheme recursively contract hierarchy node attains unification finite structure eventually collapse node code possibly fix neural network recursively encode hierarchy code invert via decode distribution code code generate decode synthesize structure specifically 3D symmetry hierarchy defines recursively grouped symmetry assemble connectivity neural net architecture learns infer hierarchy unsupervised fashion inspire recursive neural net RvNN developed text image understand treat text image superpixels RvNN learns parse recursively merges text image difference challenge  merge adjacent model network node however symmetry hierarchy symmetry assembly connectivity  merge operation network structure node accommodate assembly  symmetry rotational translational symmetry goal generative RvNN structure explicitly discrete structural combination geometric entity accomplish goal focus abstraction symmetry hierarchy compose spatial arrangement orient bound   define  code geometry code leaf hierarchy internal node hierarchy characterize fix code encode geometry  detailed mechanism connectivity symmetry pre unsupervised RvNN  arrangement endow connectivity various symmetry neural network autoencoder RvNN recursively assembles symmetrically   code decodes reconstruct input network comprises node handle assembly handle symmetry merge operation  input RvNN learns organize structure symmetry hierarchy compact minimal loss code accounting geometry structure synthesize 3D extend pre autoencoder RvNN generative model distribution code construct structure 3D acronym rnn rnn frequently refers recurrent neural network acm transaction graphic vol article publication date july generative recursive autoencoders structure utilizes generative adversarial network gan VAE gan dimensional manifold code sample project code onto manifold synthesize  arrangement stage geometry another generative model learns mapping feature voxel grid refer overall generative neural network generative recursive autoencoder structure overview architecture contribution summarize generative neural network model structure 3D representation realize autoencoder RvNN learns encode decode structure via discover symmetry hierarchy generative model synthesize symmetry hierarchy volumetric geometry respectively novel RvNN architecture extends RvNN generative capable encode variety merge operation assembly connectivity symmetry grouping unsupervised autoencoder RvNN jointly learns encodes structure geometry layout fix vector demonstrate network learns meaningful structural hierarchy adhere perceptual principle compact code enable application classification partial generative model synthesis interpolation significant variation topology geometry related related prior statistical model 3D structure recent apply neural network representation model  generative capture continuous discrete variation review relevant focus synthesis emphasize generative model discussion statistical representation capture statistical variation explore smooth deformation fix template later address discrete variation employ stochastic grammar cod training multiple training parallel explore bayesian network modular template continuous discrete variation however severely limited variety complexity layout generate typically consistently restrict layout approach probability distribution generate procedure operating fix parameter inspire non statistical representation van extract hierarchical structure goal consistent probabilistic hierarchical representation automatically unlabeled datasets overview statistical structure aware representation model 3D recently neural network computer vision recognition processing inspire researcher apply model 3D analysis statistical representation immediate relevance merit focus extend computer vision technique developed image 2D grid pixel 3D grid voxels propose generative model belief network  database voxelized 3D application model synthesis probabilistic completion prediction jointly convolutional encoder 2D image convolutional decoder voxelized 3D chain vector output encoder serf input code decoder 3D reconstruction 2D image propose encoder decoder network application  mitra 3D convolutional network voxelized plus semantic modification intent deformation realize intent departure voxel grid powerful classifier multiple project tune standard image cnns 2D datasets apply novel pool mechanism convolutional network directly non euclidean discus improve performance volumetric multi cnns classification recent develop discriminative cnn approach consistently parse bound volumetric primitive inspire develop boltzmann machine model 3D approach  successor modular template incorporate grain deformation model addition fully generative model sample entirely automatically refines correspondence boundary training however prior approach limited variety layout exploit generative adversarial net gan improve upon model core model generative decoder input code voxel grid output decoder adversarially chain prior encoder 2D image correspond acm transaction graphic vol article publication date july code arithmetic interpolation code enable instance topology  complementary seek develop powerful model layout variation accurately synthesize complex hierarchical structure beyond representational resolution grid relatively independent voxel resolution neural model graph structure layout inherently induces non euclidean graph topology define adjacency relative placement concern geometric analysis explore neural network operating graph domain domain linear chain define text signal domain recurrent neural network rnns convolutional neural network cnns slide temporal successful linear model adapt generate non linear output image van den image pixel pixel model however limited adaptation enforce graph organizational structure propose convolutional network directly arbitrary graph define convolution operation radial neighborhood vertex however none enable generative model approach directly inspires recursive neural network RvNN propose sequentially collapse graph yield hierarchy upon autoencoder version network adapt organizational principle characterize 3D structure extend deterministic model probabilistic generative overview hierarchical symmetry aware generative model 3D stage summarize stage highlight important component neural network geometry structure encode define abstraction symmetry hierarchy compose spatial arrangement orient bound   define  code geometry fix code encodes geometry  detailed mechanism connectivity symmetry stage recursive autoencoder stage autoencoder layout  autoencoder layout arbitrary arrangement component  code implicitly capture salient feature encode accomplish via recursive neural network RvNN repeatedly fashion collapse code merge code yield hierarchical organizational structure code entire layout decode recover plus entire hierarchy inverse training loss reconstruction error propagate update network stage manifold plausible structure extend autoencoder generative model structure distribution code describes manifold occupy code correspond meaningful within code generative adversarial model gan dimensional manifold code decode structure indistinguishable adversarial classifier training randomly code project gan manifold synthesize plausible structure stage geometry synthesis stage synthesize convert actual synthesize layout compute structure aware recursive feature context simultaneously compact invertible encode voxel grid geometry mapping contextual feature encode voxelized geometry yield procedure synthesize detailed geometry structure chain hierarchical structure generation geometry synthesis obtain pipeline recursive synthesis structure recursive model structure encode structure fix dimensional code encode fully invertible structure reconstruct code adversarially tune structure decoder random code structure likely combine generator sample plausible structure synthesize geometry individual obtain probabilistic generative model 3D observation component commonly perceive hierarchically organizational principle accepted theory cognition extensively leveraged computationally perceptual functional hierarchy component proximity symmetry hence primary goal structural code successfully encode hierarchical organization symmetry adjacency important metric hierarchy consistent across category achieve via compact model recursive component aggregation consistently identify substructure model recursive autoencoders rae unlabeled binary developed rae framework propose consists encoder neural network dimensional input dimensional output decoder network recovers vector vector binary descriptor leaf rae recursively compute descriptor internal node code code invert recover acm transaction graphic vol article publication date july generative recursive autoencoders structure decoder training loss formulate reconstruction error leaf raes originally intend parse discriminative unlabeled parse adapt framework task synthesize hierarchical structure important technical contribution extend framework accommodate multiple encoder decoder handle non binary symmetric probabilistically generate described subsequent criterion recursive merge model hierarchical organization perceptual cognitive criterion recursive merge  subset adjacent adjacency criterion symmetry symmetry criterion adjacent bound constituent stage interested gross layout discard grain geometric information orient bound earlier layout grain geometry synthesis described recognize symmetry bound generator plus parameter pairwise  symmetry parametrized reflection fold rotational symmetry parametrized axis rotation fold translational symmetry parametrized translation offset scenario illustrate generate training hierarchy respect criterion autoencoder learns synthesize hierarchy synthesize training data recursive autoencoder synthesize training hierarchy dataset assume pre constituent unlabeled truth hierarchy adopt iterative randomize strategy generate plausible hierarchy satisfy merge criterion described iteration merge  subset adjacent symmetric randomly sample satisfies criterion currently reasonable assumption adjacency symmetry disconnect asymmetric merge criterion merge criterion model demonstrate 3D bound relevant highlight adjacent translational symmetry rotational symmetry reflective symmetry merges generate training hierarchy fashion none hierarchy intend truth sample plausible grouping relatively unbiased fashion training purpose autoencoder model handle adjacency symmetry relation recursive autoencoder comprises distinct encoder decoder adjacency encoder adjacency module neural network  merges code adjacent code input output parameter matrix  bias vector bae obtain code merge node formula tanh  bae correspond decoder  split code code reverse mapping tanh   symmetry encoder symmetry module neural network  merges code generator symmetry parameter symmetry output code generator parameter compute tanh   correspond decoder  recovers generator symmetry parameter tanh  bsd    bsd implementation encode symmetry parameter comprise symmetry 1D repetition rotational translational symmetry 1D mirror reflective symmetry rotation axis rotational symmetry displacement translational symmetry 6D encoders decoder adjacency symmetry implement layer network dimension hidden output layer 0D 0D respectively input recursive merge collection bound mapped vector autoencoder employ additional layer neural network  2D parameter concatenate dimension code  recovers 2D parameter code network non recursive simply translate input internal code representation lastly jointly auxiliary classifier  module apply recursive decode classifier neural network hidden layer input code node hierarchy output acm transaction graphic vol article publication date july   code vec 2D parameter code vec code vec code vec code vec 2D parameter    code vec symmetry params code vec code vec symmetry params   leaf adjacent symmetry 3D vector autoencoder training setup ellipsis dot code output    input    2D parameter  code vec   code vec code vec code vec symmetry params  leaf adjacent symmetry prediction autoencoder decode setup node adjacent symmetry leaf node output classifier    invoked training recursive autoencoder BFGS propagation random initialization sample gaussian distribution loss formulate reconstruction error training hierarchy encode leaf bound  recursively apply correspond encoder   internal node obtain code finally invert code recover leaf parameter recursively apply decoder   application  loss formulate sum difference input output parameter leaf training input hierarchy decode hence decoder apply unfolded node mapping input output simultaneously   softmax classification entropy loss recover topology training setup illustrate address distinct challenge infer plausible encode hierarchy novel without hierarchical organization decode code recover constituent bound infer plausible hierarchy encode module resort greedy local specifically subset  perform recursive encode decode reconstruction error merge sequence reconstruction error encode hierarchy merges adjacency symmetry symmetry adjacency illustrate decode code input parameter training merge hierarchy reconstruction error lookahead employ infer hierarchy mode training minimize reconstruction loss hierarchy entire subtrees encoder decoder tune locally globally reconstruction relatively lookahead suffices decode code obtain encode hierarchy infer fashion recursively  decoder expand node correspond decoder    recover code node hierarchy expand leaf correspond parameter decode setup illustrate reconstruction procedure encode novel code reconstruct code RvNN perceptually reasonable symmetry hierarchy 3D structure minimize reconstruction error structure  error spike merge fold rotational symmetry apply rotational symmetry spike respectively apply learning manifold plausible  recursive autoencoder computes compact fix dimensional code infer hierarchical layout recover layout code associate hierarchy however autoencoder developed adjacent reflective sym adjacent fold rot sym encode minimize reconstruction error symmetry reflective adjacency adjacency symmetry fold rotational acm transaction graphic vol article publication date july generative recursive autoencoders structure input structure recover structure  sym  sym input structure fold trans sym  sym fold  sym rot sym recover structure reconstruct without hierarchy successively encode code decode encode hierarchy infer RvNN encoder fold rot sym fold rot sym fold rot sym RvNN encoder perceptually reasonable symmetry hierarchy 3D structure minimize reconstruction error input structure reconstruction error grouped adjacency symmetry instead symmetry adjacency generative model reconstruct layout code arbitrary random code unlikely plausible layout generative model jointly capture distribution statistically plausible structure convert autoencoder model fully generative tune autoencoder relatively dimensional manifold probability structure prior approach feasible manifold parametrized 3D landmark exemplar kernel density estimation multidimensional piecewise primitive fitting however essentially reduce interpolation landmark hence assign probability parameter vector correspond implausible  random code vec code vec 2D param  2D param   code vec code vec 2D param  2D param  code vec 1D vector generate generator network discriminator network architecture generative adversarial network reuse autoencoder module recently generative adversarial network gans introduce overcome precisely limitation instead directly interpolate training exemplar gan synthesis procedure arbitrary parameter vector vector classifier deems plausible classifier arbitrarily sophisticated jointly identify exemplar plausible others fake refine mapping latent implausible eliminate construction completely random parameter gan snap plausible manifold generate meaningful sample addition enable synthesis novel statistically plausible structure manifold interpolation code application feature morph gan architecture architecture generative adversarial network comprises generator network transforms random code hierarchical structure estimate manifold discriminator network generate structure training observation directly reuse tune autoencoder module previous instead introduce component decoder component comprise     exactly estimate structure code constitutes network encoder component comprise    exactly estimate code generate structure code code training structure additional fully layer binary softmax layer probability structure constitutes network hence initialize gan autoencoder module tune minimize gan loss architecture illustrate training procedure described training gan stochastic gradient descent loss function discriminator generator iteration sample mini batch training structure associate hierarchy random code sample hierarchy discriminator yield whereas sample network sequence yield acm transaction graphic vol article publication date july random code plausible hierarchy plausible hierarchy classification loss   geometric training fix structural training training gan model random code plausible hierarchy decode structure fool hierarchy training split geometric structural tune loss function loss function discriminator JD logd loss function generator  logd minimize loss function network encourage discriminator output training sample random sample minimize loss function network encourage generator fool random sample actually training standard adversarial training setup detail straightforward training however converge suitable balance network despite initialization autoencoder due mapping random code manifold network recursive decoder infer grossly incorrect hierarchy network easy reject implausible hierarchy hence generate useful training signal implausible hierarchy generate random code reasonable pathway propagate loss tune properly decode network split geometric  topological  tune separately loss deduce devise training strategy prior constrain training structure prior initial stage prevent mapping random code severely implausible hierarchy achieve introduce structure prior constrain hierarchy infer plausible hierarchy autoencoder hierarchy infer autoencoder mode training plausible hierarchy fool discriminator minimize  hierarchy propagate loss  geometric structural training hierarchy tune geometric decoder via propagate correspond loss  hierarchy tune fool discriminator estimate thatG hierarchy newly update tune structural component  minimize classification loss  node hierarchy node truth hierarchy fool loss constrain random code sample prior constraint reconstruct plausible hierarchy arbitrarily random code therefore instead directly random code normal uniform distribution code drawn gaussians around training sample mixture approximates standard normal distribution secondary network project latent code potential code easy specifically sample multivariate gaussian distribution enc enc enc recursive encoder originally autoencoder adversarial tune mode approximate neural network minimize reconstruction loss addition generator loss gan network enc andG constitute variational autoencoder VAE tune enc parameter gaussian distribution architecture VAE gan propose consequently impose loss function VAE variational distribution training sample towards prior standard normal distribution summary minimize loss function    gan loss  logd loss minimize maximize respectively reconstruction loss define  kullback leibler divergence loss  dkl mixture local gaussians approximate standard normal distribution gan training tune RvNN decoder module network network project random vector drawn standard normal distribution potential code tune decoder acm transaction graphic vol article publication date july generative recursive autoencoders structure   VAE gan confine random code sample gaussian distribution code enc jointly distribution training gan VAE gan network project vector structure plausible manifold module generate grain geometry described constitutes recursive generative model 3D geometry synthesis previous described generative model layout component framework generative model grain geometry bound layout component develop fix dimensional feature vector capture gross dimension context within layout dimensional manifold plausible geometry simultaneously mapping feature vector manifold mapping obtain synthesize geometry generate layout detail structure aware recursive feature  recursive generator network hierarchy internal node hierarchy code exploit structure define feature vector contextual feature concatenate RvNN code node leaf node however variable yield fix dimensional vector instead approximate context concatenate code leaf node immediate feature vector code capture dimension bound latter code capture local global context respectively concatenate code construction structure aware recursive feature  hierarchy concatenate RvNN code immediate fix dimensional vector FC  code input volume 2D  code training training output volume output volume FC FC FC training setup geometry synthesis geometry synthesis structure generate structure synthesize geometry inside volumetric representation per volume embed global volume reconstruct mesh model  geometry stage  feature vector synthesize geometry prototype voxel grid mapping function directly output dimensional plausible span dimensional manifold within output instead adapt strategy inspire convolutional autoencoder consist encoder  voxel grid compact 2D code decoder  reconstruct grid code efficiently dimensional manifold plausible geometry architecture reconstruction error sigmoid entropy loss simultaneously network  input  code 2D code network access code neuron mapping network employ euclidean loss function network jointly loss stochastic gradient descent backpropagation chain mapping network  decoder  obtain function mapping  code synthesize geometry training setup illustrate synthesis overall geometry predict wise 3D volume embed global volume reconstruct mesh model RESULTS evaluation evaluate generative recursive model structure focus validate autoencoder RvNN learns symmetry hierarchy correctness qualify code useful application classification acm transaction graphic vol article publication date july   trans  rot trans  RvNN encoder correctly par 2D arrangement handcraft perceptually violate fold translational symmetry highlight precedence  partial generative capability VAE gan network built RvNN dataset dataset 3D model category bike    model ShapeNet princeton ModelNet model  accord mesh component symmetry aware segmentation utilized average per bike    symmetric distinct utilize label RvNN autoencoder dataset generative VAE gan per category training involves structure within category geometry synthesis category recursive symmetry hierarchy precedence label appendix reproduction handcraft assembly symmetry operation stipulates symmetry preserve assembly precedence symmetry assembly assembly symmetry assemble belong symmetry posse equivalent symmetry inspire gestalt perceptual  razor seek simplest explanation perceptual cognition intrigue RvNN unsupervised replicate cognitive capability arrangement 2D per illustrate  rot rot infer hierarchy consistent across  involves connectivity strength simply geometric proximity arrangement RvNN encoder correctly par fold translational symmetry precedence  consistency infer hierarchy RvNN framework infers hierarchy consistently across demonstrate augment category dataset  semantic label label relatively hierarchy etc subdivide hierarchy consistent across label consistent merge merge merge denote label another label denote shortest distance ancestor label definition label label probability regularly grouped indicator function sum additionally restrict label overall consistency estimate minus average entropy label triplet average consistency category training category RvNN infers hierarchy consistently across consistent infer hierarchy classification structure autoencoder generates compact encoding arbitrary via recursively infer hierarchy code effectively characterize similarity conduct grain classification airplane bike  sub airplane jet  delta acm transaction graphic vol article publication date july generative recursive autoencoders structure MV cnn  recall precision MVCNN  recall precision  MV cnn  recall precision airplane MV cnn  recall precision bike precision recall plot classification task swept  fold  sofa bike motorcycle casual bicycle  bike  average code hierarchy standard protocol category sort remain increase distance average code terminate variable upper limit distance query positive precision recall plot average accuracy subclass classification baseline performance descriptor task entirely comparison leverage prior segmentation unlabeled whereas baseline however grain geometry orient bound parameter considerable improvement baseline demonstrates gross structure significantly important recognition grain geometry accurate consistent identification layout foundation powerful retrieval classification partial structure previous retrieval explore subtree code sufficiently descriptive average code subtree feature subtree contains partial retrieval correctly retrieves subpart query synthesis interpolation framework generative synthesize manifold VAE gan network sample random hierarchical bound layout leaf node hierarchy mapped grain voxelized geometry subsequently mesh synthesize model interpolate topologically geometrically task compute code via infer hierarchy linearly interpolate code reconstruct intermediate synthesis procedure although intermediate code correspond code plausible synthesis procedure project onto valid manifold virtue VAE gan training demonstrate interpolation model successfully handle topological layout within maintain symmetry constraint unlike prior knowledge hierarchy unlike correspondence handle smooth topological individual implementation timing RvNN VAE gan implement matlab geometry synthesis model implement  neural network library pre training autoencoder adversarial tune training geometry synthesis network mapping random code vector manifold plausible structure synthesize hierarchy augment synthesize grain geometry additional per discussion limitation future towards develop structure aware generative neural network 3D apart previous attempt neural net 3D synthesis ability query ranked structure partial structure retrieval bicycle query highlight acm transaction graphic vol article publication date july synthesize without supervision synthesize structure satisfy generate 3D posse cleaner structure symmetry regularize geometry voxel generate previous unsatisfying however decouple synthesis structure geometry hint obvious integrate synthesis code RvNN combine structural geometric information vector demonstrate  RvNN conform perceptual principle reflect precedence handcraft code enable application grain classification partial retrieval reasonable however internal mechanism code precisely structural geometric information unclear encode  arbitrary depth fix vector somewhat mysterious future visualize code gain insight insight steer code towards separation reflect structure reflect geometry network fully mapping generative structure manifold cannot extrapolate arbitrarily limited VAE gan setup sample code exemplar hence synthesis interpolation confine local patch elusive manifold completely generative structure 3D collection sufficiently structural variation dimensional manifold along discover flexible mechanism generate valid code apply algebraic crossover operation available code direction await future investigation thoroughly investigate code structure encode finally worth explore recent development gans wasserstein gan VAE generative adaptation