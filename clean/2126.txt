extensively convolution operation input data convolution clearly define structure data 2D image 3D volume data sparse previous technique developed approximation convolution restrict unfortunately applicability limited cannot propose efficient effective convolution non uniformly sample obtain acquisition technique enable novelty convolution kernel multilayer perceptron phrasing convolution monte carlo integration notion combine information multiple sampling fourth poisson disk sample scalable hierarchical across contribution guarantee adequate consideration underlie non uniform sample distribution function monte carlo perspective propose concept applicable  furthermore propose efficient implementation significantly reduces gpu memory training employ hierarchical network architecture outperform network establish segmentation classification normal estimation benchmark furthermore contrast exist approach demonstrate robustness respect sample variation training uniformly sample data application concept tensorflow implementation layer http github com    CCS concept compute methodology neural network analysis additional convolutional neural network monte carlo integration introduction convolutional neural network achieve unprecedented performance structure data application unstructured data fairly fully approach  permutation acm transaction graphic vol article publication date november      pau    timo  rotational invariance unfortunately non uniform sample typically associate data instance projective lidar scan focus previous research underlie sample severe implication illustrate propose approach developed focus non uniformly sample achieve competitive performance uniformly sample data progress towards goal account definition convolution integral unstructured monte carlo MC estimation integral handle underlie sample density crucial surpass contribution convolution kernel multilayer perceptron mlp convolution kernel spatial offset laplacian scalar mapping mlp choice monte carlo integration compute convolution unstructured data adequate handle non uniformity density monte carlo average formally sample MC estimate integrand MC probability sample neglect uniform constant however sample density non uniformly sample fail perform normalization bias consequently reduce ability therefore approach robust sample invariance instance simply duplicate estimate integrand convolution MC integration allows tap machinery MC quasi randomization importance sample consequently convolution becomes invariant ordering typically receptive variable allows generalize convolution sample convolution sample resolution transpose convolution detail pool sample operation introduce convolution multiple input sampling desire output sample combine information multiple fourth introduce poisson disk sample construct hierarchy favorable scalability farthest sample allows bound maximal sample receptive usefulness novelty demonstrate approach technique segmentation classification normal estimation task outperform non uniform achieve performance uniformly sample previous enable resample regular grid apply approach originally developed structure data extension multiple resolution exist octrees solely focus technique enable directly unstructured data PointNet pioneer unstructured datasets fully network clever machinery achieve rotation permutation invariance PointNet extension localize sub network fully convolutional pcp net inference local curvature normal convolutional   convolutional learner however leaf node fix sensitive density tight logic building query contrast approach regular grid access constant multiple translation invariant  convolution replace correlation local neighborhood graph graph template performance remain particularly sensitive underlie graph structure dynamic graph cnns wang convolutional employ notion learnable operation graph contrast approach neighborhood approach slightly complex implement efficient PointCNN convolutional mlp entire neighborhood transformation matrix later permute input feature standard image convolution apply transform feature  sought inspiration  lattice convolution efficiently perform sparse data dimension filter kernel discrete mask lattice uneven sample distribution lattice address convolve convolution signal achieves without complication lattice radial basis function define discrete convolution kernel computationally demand invariance global uniform resampling construction however non uniform sample setting concurrent  function convolution author mention MLPs perform function nevertheless architecture MLPs perform acknowledge explore continuous representation parametrize convolution acm transaction graphic vol article publication date november monte carlo convolution non uniformly sample receptive scene non uniform sample geometry linear function representation unstructured convolution kernel consideration dissimilar 2D convolution kernel linear function mlp kernel worth explore thanks simplicity approach demonstrate excellent scalability non uniform sample upon exist convolve hierarchical structure approach robust non uniform sample setting reduce volume around capture feature kernel shrink densely grows sparse prevent construct sample invariant detector although non uniformity transform volumetric function robust non uniformly sample however computation quadratic scalable PointNet computes feature locally scalable moreover non uniformly sample simulate lidar scan nevertheless selects fix random sample within radius around density computation demonstrate error convolution  recall definition convolution sec introduce kernel representation MLPs sec efficient irregular data convolution integral recall definition convolution integral scalar function convolve convolution kernel scalar function feature function discrete sample data information besides spatial coordinate binary function evaluates sample otherwise however input information evolution mlp kernel naïve 2D offset dot scalar orange dot extend multiple output computation reduces learnable parameter 3D implementation hidden layer neuron output kernel operation compute naïve approach normal etc internal convolution subsequent input layer feature previous convolution translation invariance depends relative convolution translation invariant invariance evaluate integral entire domain prohibitive datasets limit domain sphere radius multiple radius normalize input receptive scene bound diameter instance invariance construction compactly kernel evaluate rotation invariance achieve seek achieve rotation invariance typical image convolution rotation invariant succeed nonetheless multilayer perceptron kernel kernel multilayer perceptron definition multilayer perceptron mlp input spatial offset comprise coordinate normalize receptive output mlp scalar balance accuracy performance hidden layer neuron sec detail denote hidden parameter vector layer input output feature kernel therefore parameter network input output address mlp output reduce mlp factor illustration mlp coordinate input output propagation propagation derivative convolution respect parameter mlp   acm transaction graphic vol article publication date november      pau    timo  MC convolution within receptive retrieve monte carlo integration sample probability density function compute kernel density estimation bandwidth pink disk computation pink gradient multi feature convolution convolution consumes input feature function output convolve feature function convolve feature function calculate define layer feature spatial convolution multi feature spatial convolution  spatial convolution output scalar feature convolve scalar input feature therefore layer output feature input feature kernel multi feature convolution contrary layer standard convolutional neural network output feature compute sum input feature function convolve layer computationally demand convolution kernel monte carlo convolution convolution monte carlo estimate rely sample density function ultimately robust non uniform sample distribution monte carlo integration compute convolution sample evaluate integral equation sample function propose compute integral MC integration random sample compute integral definition sample comprise input data quasi random subset therefore estimate convolution neighborhood index sphere radius receptive probability density function pdf fix convolution compute illustration computation input data non uniformly distribute worth pdf depends sample likely depends likely others receptive finally arbitrary output input later sample irregular regular domain propagation propagation respect mlp parameter estimate MC   estimate pdf unfortunately sample density estimate sample employ kernel density estimation function estimate sample dense sparse compute bandwidth determines smooth sample density function density estimation kernel non negative function integral gaussian dimension pdf respect relative sample receptive therefore density pre compute receptive define radius uniform sample constant MC convolution multiple  construction handle sample density perform convolution sec multiple sampling sec acm transaction graphic vol article publication date november monte carlo convolution non uniformly sample dierent sample multiple feature monte carlo convolution sample horizontal multiple feature channel vertical sample denote dot whereby sampling sampling convolution involve sample illustrate sample input output identical previous detailed account sample density MC convolution seamlessly handle sampling sampling generalize input sample output sample illustration mapping sample sample upsampling transpose convolution deconvolution principle reduce sample resolution pool entire successively reduce resolution scalar classification combination downsampling net encoder decoder architecture segmentation application previous sampling multiresolution hierarchy fix non interpolation inverse distance upsampling operation approach allows mapping instead procedure explain previous simply sample kernel define continuous offset vector compute output sample input sample density estimation perform respectively input explain previous multiple sampling another unique advantage construction relax sample requirement input output sample input multiple input channel sample remains identical input channel output layer layer layer feature upsampled layer respect previous layer receptive content respective density pink layer MC convolve individual mlp kernel concatenate feature vector layer guarantee maximum receptive maintain ratio poisson disk radius convolution sample mutually input output typical application multiple sample approach consume information multiple resolution hierarchy sample sample sample combine sample multiple sampling density estimation perform relative convolve sample relative processing combine output multiple previous layer densenet classic tabulate kernel admit construct dense link sampling limited receptive sampling sample receptive remains roughly constant deviation desire constant sample variation density inside receptive compensate density estimation embodiment multi sample MC convolution information 2D resolution sample resolution receptive grows resolution poisson disk hierarchy image processing routinely reduces later increase image resolution local global information achieve processing acm transaction graphic vol article publication date november      pau    timo  sample tenth algorithm poisson disk farthest usually farthest FP sample poisson disk PD sample instead scalability ability preserve sample bound sample per realization PD realize network layer parallel poisson disk sample algorithm input layer sample  PD radius output sample minimum distance bound distance distance remain multiple PD layer combine multi resolution hierarchy combination multi sampling convolution sec encoder decoder network contrary sample approach technique generates non fix sample therefore network cannot advantage acceleration technique commonly framework memory pas reserve advance however achieve performance scalability practical PD scalability evaluate tbl PD sample compute model linear model FP sample sample model sample bound moreover PD allows limit within receptive convolution kepler conjecture upper bound inside receptive illustrate non uniform sample PD retain  whilst maintain minimal distance therefore receptive approach pink retain bound poisson disk  marked receptive radius limit sample 3D illustrate advantage compute feature maintain constant ratio ratio sample receptive input farthest poisson disk farthest FP fix sample poisson disk PD sample non uniformly sample input dot sample pink denotes receptive grey FP sample fulfil minimal distance overlap PD around sample packed inside receptive commonly implementation FP sample selects fix sample input contrary PD cannot bound receptive generate sample PD modify variable distance sample subset already implementation implementation detail building propose approach pdf computation implementation differs average individual contribution requirement probability therefore MC approach compute query MC convolution compute voxel grid enables perform desire computation linear wrt scalability hash instead regular grid propose lookup performance index indexed index grid additionally compute arbitrarily arbitrarily density decrease performance computation convolution ofr however maintain ratio receptive PD radius upper bound construct data structure index precomputed pdf constant compute information kernel voxel grid distribute compute parallel gpu ordering within execution introduces randomness output sample PD sample acm transaction graphic vol article publication date november monte carlo convolution non uniformly sample strategy prefer sec evaluates randomness accuracy multilayer perceptron evaluation evaluate mlp MC convolution considerable amount gpu memory implementation standard framework limit compute feature per convolution network architecture address limitation implement mlp evaluation gpu kernel network expand feature coordinate intermediate mlp layer however implementation feature batch normalization improve batch processing layer variable moreover described convolution account computation choice standard tensor approach batch model parallel batch processing extra vector integer denote model identifier belongs acceleration data structure access described model evaluation layer appropriate data structure update query model identifier approach allows batch model variable parallel increase memory consumption linearly however configuration reduce gpu memory sophisticated approach per model perform extra computation within layer evaluation report obtain machinery explain network specific data relevant task report comparable introduce dataset non uniform sample sec benchmark data specific task sec sec reporting actual quantitative evaluation sec moreover report apply network datasets sec lastly introduce additional sec computational efficiency network sec non uniformly sample data non uniform data typical scan scene however exist benchmark data uniformly sample model evaluate performance network generate data artificially non uniform sampling allows explicitly sample data uniformly sample perform rejection sample whereby rejection probability compute accord protocol uniform lambertian gradient split occlusion sample protocol apply uniform uniformly random lambertian depends orientation arrow location direction likely gradient likelihood generate decrease along direction arrow split split dot sample uniformly random density occlusion depends orientation location visible direction uniform rejection split probability random constant random gradient probability proportional projection onto bound axis lambertian probability proportional clamped dot normal fix direction occlusion probability invisible direction sample protocol apply training phase data benchmark model load apply sample technique currently random generate probability direction applicable task task evaluate network classification segmentation normal estimation classification task assigns label performance percentage prediction resampled version ModelNet dataset compose uniformly sample category official split compose model model sample accord protocol sec segmentation task assigns label quantify intersection union iou metric ShapeNet uniformly sample compose standard split training assume task input input network per normal estimation task computes continuous orientation analyze cosine distance ModelNet evaluation model standard split acm transaction graphic vol article publication date november      pau    timo  comparison segmentation uniform non uniform sampling truth non uniform sample gradient lambert fourth split fifth sixth protocol performance task text classify normal MC additional input network per segmentation task appendix sec training network detail architecture architecture task variant PointNet PN multi msg baseline architecture without MC convolution kernel average inside receptive denote avg architecture monte carlo convolution MC additionally report uniform data evaluation variant training uniformly sample data non uniformly sample data dataset sample protocol sec uniform model sample protocol described sec counterbalance randomness introduce sample algorithm measurement average across independent execution discus uniform non uniform sample summarize tbl tbl respectively task classification illustrate tbl approach generates competitive classification uniform sample achieve accuracy importantly network robust performance non uniformly sample tbl training uniformly sample MC performance PointNet uniformly non uniformly sample achieve uniform split gradient sample protocol contrast achieve PointNet lambert occlusion protocol network performance around protocol model task furthermore MC slightly performance PointNet protocol training non uniformly sample achieve protocol whilst PointNet achieve performance lastly avg network MC convolution obtain performance protocol avg network difficulty generalize MC network severe fitting prevent fitting avg network data augmentation strategy PointNet variance accuracy execution MC accuracy significant segmentation truth model tbl performance achieve segmentation network uniform data competitive theart slightly surpass PointCNN achieve whilst PointCNN achieve acm transaction graphic vol article publication date november monte carlo convolution non uniformly sample performance comparison task sample protocol task training uniform non uniform data accord protocol classification segmentation normal estimation uniform non uniform uniform non uniform uniform non uniform PN avg MC PN MC PN avg MC PN MC PN avg MC PN MC uniform split gradient lambert occlusion    sofa shelf sink  desk semantic segmentation approach ScanNet truth PointCNN convolution sec approach handle non uniformly sample segmentation performance non uniform sample tbl uniformly sample MC network obtain protocol PointNet moreover  sample although PointNet competitive performance MC network obtain PointNet protocol worth classification task PointNet obtain non uniformly sample indicates propose sample protocol data augmentation technique avg network MC accuracy uniformly sample protocol MC avg gradient protocol MC avg however avg perform split MC avg lambert MC avg occlusion protocol MC avg nevertheless difference network remain normal estimation task tbl uniform data network outperform achieve cosine distance improve accuracy report non uniform data tbl MC approach outperforms PointNet protocol uniformly non uniformly sample network avg convolution obtain performance however MC convolution obtain data apply data semantic segmentation task ScanNet dataset compose scan training evaluation task classify input category report performance per voxel accuracy meaningful ScanNet metric overall voxel accuracy PointNet approach achieves comparison PointNet ScanNet propose dai nießner image information dai nießner achieve network generate consistent prediction  predict incorrect annotate truth acm transaction graphic vol article publication date november      pau    timo  classification accuracy timing mlp mlp accuracy epoch task classify normal sofa handle training without splitting chunk variant poisson disk hierarchy evaluate MC convolution without poisson disk hierarchy network compose convolution normal estimation task avg convolution MC convolution avg perform MC uniform split gradient lambert protocol similarly performance MC without PD limited uniform split gradient lambert indicates normal estimation PD hierarchy essential mlp classification accuracy mlp tbl maximal accuracy obtain neuron whilst neuron achieve timing neuron accuracy execution computational efficiency tbl epoch network compute pas model training epoch pas individual model due parallel implementation algorithm acceleration data structure network competitive performance performance segmentation network compute feature initial measurement nvidia gtx LIMITATIONS besides benefit propose approach limitation limitation rely KDE obtain pdf carefully bandwidth parameter obtain pdf approximation future investigate shortcoming inspect advanced pdf estimation selection KDE validation  otherwise automate choice approach unbiased estimate convolution assume KDE reliable variance locality locality receptive computation noisy estimate net receptive compute localize noisy estimate CONCLUSIONS phrasing convolution MC estimate superior typical learningbased processing non uniform segmentation classification normal estimation enable convolution kernel multilayer perceptron accounting sample density function poisson disk pool realize MC sample preserve sample density moreover demonstrate network robust fitting generalization obtain performance without data augmentation technique something mandatory classification task density although competitive performance task non uniformly sample data predict sample future input data scenario model ability generalize become robust unseen sampling importance network task future apply input dimensionality animate attribute another direction future research non uniform density triangular tetrahedral mesh typically uniform sample