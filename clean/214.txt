despite remarkable advance emotion recognition severely restrain essentially limited employ modality synchronous presence involve multiple modality motivate propose novel crossmodal emotion embed framework EmoBed aim leverage knowledge auxiliary modality improve performance emotion recognition framework generally component joint multimodal training crossmodal training tend explore underlie semantic emotion information recognition network emotion embed respectively enhance approach efficiently complementary information modality nevertheless presence auxiliary modality demand inference empirically investigate effectiveness robustness propose framework perform extensive benchmark database recola omg emotion task dimensional emotion regression categorical emotion classification respectively obtain propose framework significantly outperforms related baseline monomodal inference competitive superior recently report emphasis importance propose crossmodal emotion recognition introduction automatic emotion recognition endows machine capability empathic communication essential sustain machine interaction critical shift artificial intelligence generation enhance emotional characteristic decade significant advance improve accuracy robustness emotion recognition monomodal multimodal scenario monomodal emotion recognition normally independently explore prominent feature emotion specific modality audio video image text physiology advent modality specific continually achieve promising performance contrast multimodal tend jointly utilise modality aim advantage complementary supplementary information medium cue benefit consistently evaluate superior monomodal numerous previous albeit notable advantage evaluation phase multimodal emotion recognition synchronous presence modality employ previous training phase severely impedes application information modality camera fix user darkness invalid visual signal likewise user although emotional audio data absence involve modality corruption performance degradation pre multimodal straightforward address issue integration additional component activity detector detector multimodal recognition absence modality detect prediction automatically another via accordingly reduce modality nevertheless normally inferior modality aforementioned embrace advantage avoid shortage article propose novel crossmodal emotion EMBEDDING framework namely EmoBed underlie framework transfer knowledge auxiliary modality target modality enhance performance monomodal emotion recognition model basically consists  multimodal training  training former utilises data multiple modality jointly network assumption knowledge modality implicitly transfer fuse network meanwhile latter utilises triplet loss minimise distance intra representation maximise inter regardless correspond modality extract representation modality intra representation distance inter distance framework advantage traditional multimodal emotion recognition data auxiliary modality training stage knowledge suppose transfer target modality inference stage auxiliary modality anymore therefore overcomes synchronous presence traditional multimodal training network demand data signal modality unnecessarily align data heterogeneous corpus modality mismatch training framework advantage largely release signal alignment requirement traditional multimodal furthermore partially inspire multi task paradigm multiple task jointly network task specific network repeatedly demonstrate affective compute generalisation representation learnt network similarly assume multiple modality benefit training monomodal framework optimise parameter network overall contribution propose novel framework EmoBed explore knowledge auxiliary modality emotion recognition jointly network heterogeneous data however unnecessary extract modality invariant emotion embeddings latent via triplet loss although triplet loss implement emotion recognition literature merely utilised distil discriminative representation within signal differs propose aim distil modality invariant emotion embeddings finally comprehensively investigate effectiveness robustness model dimensional continuous emotion regression categorical discrete emotion classification sake clarification define traditional emotion recognition without crossmodal training classic emotion recognition whereas enhance propose crossmodal technology enhance emotion recognition despite propose approach modality sake simplicity article mainly focus visual audio modality emotion recognition camera microphone pervasive audio video assume monomodal remainder article structure brief related crossmodal multimodal emotion recognition elaborately propose EmoBed framework detail perform comprehensive audiovisual emotional database ass performance propose approach emotion regression classification task finally conclusion promising related research direction related summarise related recently report crossmodal training multimodal emotion recognition emotion recognition respectively crossmodal training recently increase investigate approach transfer knowledge across domain modality notable relevant  representation teacher training procedure teacher vision network network recognise acoustic unsupervised manner training KL divergence posterior probability teacher network minimise knowledge transfer establish visual recognition model audio relevance recent author propose align embeddings emotion recognition distil knowledge teacher network facial emotion recognition approach differs propose mainly aspect although network learnt without access label audio limitation approach complex teacher network arguably developed emotion recognition however network scratch report performance network performance teacher network cannot compete fully supervise network contrast primary focus additional knowledge modality assist target modality consequence performance approach superior fully supervise monomodal network multimodal emotion recognition mention multimodal fusion approach widely conduct exploit complementary information improve performance robustness emotion recognition  aim various data fusion strategy extensively utilised normally classify category namely feature fusion decision model fusion typically feature fusion fusion straightforwardly concatenates audio visual feature combine feature vector input model contrast decision fusion fusion combine prediction feature modality specific model decision suitable criterion addition model fusion fuse intermediate representation instead manner learns model potential hidden correlation feature former fusion strategy model fusion supposedly bio inspire cognitive perspective overall fusion strategy practical helpful extent audiovisual emotion recognition however approach focus multimodal scenario rely heavily existence signal sensor contrast exploit hidden correlation multiple modality implicit fusion manner later implement flexible information auxiliary modality inference crossmodal emotion embed aim attain embed explore latent correlation audio video signal monomodal emotion recognition training stage propose EmoBed framework depict typically extract audio video descriptor via standard essential processing jointly modality specific network project multimodal descriptor representation apply predict emotion propose crossmodal emotion EMBEDDING EmoBed framework monomodal emotion recognition mathematically embed function RM RN aim mapping audio input RM visual input RN onto embed representation accordingly coordinate embed function introduce joint training audiovisual data moreover useful semantic representation employ crossmodal triplet loss lastly propose EmoBed framework integrate merit joint training crossmodal training joint training audiovisual data subsection demonstrate embed monomodal emotion recognition joint training loss audiovisual emotional data briefly differentiate joint training related structure depict structure comparison propose joint audiovisual training related multimodal framework fusion fusion model fusion multi task conventional multimodal fusion paradigm demonstrate although information multiple modality fuse contribute multimodal emotion recognition concretely denote monomodal input feature learnt hidden layer representation output prediction audio video respectively combination audio video knowledge feature fusion average decision fusion model fusion model utilised available input model dimension propose joint training model albeit constraint exist remains training model apply monomodal furthermore model dissimilar multi task illustrate multi task training phase auxiliary task benefit task update parameter feature network contrast propose model assume input auxiliary modality improve emotion prediction modality optimise parameter predict network denote audio feature vector RM correspond visual feature vector RN dimension audio visual vector respectively depict fed modality specific subnetworks formulate sourcewhere function RM function RN input modality subspace correspond dimensional representation layer apply estimate prediction formulate sourcewhere function estimate prediction separately efficiently aggregate advantage modality monomodal emotion recognition emotion recognition facial emotion recognition model audiovisual feature model apply emotion recognition joint loss function calculate LA LV sourcewhere denotes network parameter optimise LA LV loss audio video data respectively denotes video prediction loss regulate contribution LV enforces optimisation auxiliary modality information account similarly facial emotion recognition joint loss function alter LV LA  optimise development achieve performance modality crossmodal triplet primary focus subsection emotion discriminative embeddings crossmodal data via triplet loss function triplet loss project descriptor latent instance semantics instance dissimilar semantics away consequently similarity instance semantic information preserve learnt representation triplet constraint investigate intensively image text gain traction audio video motivate propose crossmodal triplet framework adopt crossmodal triplet loss supervise triplet tri embed instance embeddings another instance denote positive embeddings denote negative belonging calculate semantic similarity instance batch embeddings typically embeddings similarity compute sourcewhere denotes euclidean distance embeddings therefore pairwise euclidean distance matrix generate compute distance embeddings diagonal obtain matrix null distance embed zero subsequently explore distance matrix batch hardest positive hardest negative emotion label accordingly particularly embed maximum distance embeddings label conversely another embed minimum distance embeddings label obtain apply construct triplet embed triplet loss constraint LT estimate triplet LT dei dei source compute crossmodal triplet loss  combine audio embeddings video embeddings batch embeddings pairwise euclidean distance matrix obtain compute distance embeddings afterwards embed audio video another embeddings chosen batch triplet worth mention generate hardest positive negative   similarity consideration manner model narrow distribution gap embeddings modality specific emotional semantics intact meantime supervise crossmodal triplet loss  model minimise optimisation objective formulate   sourcewhere  denotes conventional monomodal discriminative loss LV facial emotion recognition LA crossmodal emotion embed subsection illustrate propose EmoBed framework integrates triplet constraint joint training approach overall training demonstrate generally extract monomodal descriptor standard pre processing procedure embed function estimate embed neural network respectively project audio video descriptor latent subsequently audio visual embeddings fed emotion recognition neural network via joint training loss concurrently training supervise triplet loss audio visual embeddings mathematically apply EmoBed framework audio emotion recognition objective function format LA LV  sourcewhere LA LV discriminative loss function audio visual data respectively  triplet loss function audio visual data moreover hyperparameters introduce contribution video data triplet loss furthermore apply importance  similarly training EmoBed framework facial emotion recognition objective function modify exchange LA LV model training component associate auxiliary modality discard retain utilised recognise emotion experimental implementation evaluate approach comprehensively conduct extensive multimodal emotional datasets task respectively specifically recola dataset chosen dimensional emotion regression whereas omg emotion dataset categorical emotion classification briefly introduce datasets experimental setup detail sake replication evaluation metric performance comparison evaluate database feature recola database widely audiovisual dimensional emotion recognition standard database previously apply avec contains audiovisual recording spontaneous interaction french participant investigate socio affective behaviour context remote collaborative task moreover continuous dimensional emotion annotation arousal valence constant frame rate average annotator meanwhile inter evaluator agreement consideration dataset equally disjoint balance gender tongue participant therefore consists unique recording training development conduct recola employ acoustic visual feature feature utilised compute avec baseline comparison specifically acoustic feature extend geneva minimalistic acoustic parameter  extract audio recording source openSMILE toolkit acoustic feature per relation visual feature utilised appearance geometric standard feature avec challenge investigate handcraft video feature learnt feature pre vgg net input visual embed net comparison acoustic feature arithmetic standard derivation compute sequential handcraft visual feature frame slide appearance geometric visual feature omg emotion gradual emotional omg emotion behavior dataset employ categorical emotion classification omg emotion dataset compose emotional  video youtube average video utterance clip annotate annotator categorical emotion neutral happiness sadness disgust majority voting apply compute standard annotation moreover dataset split training development partition respectively perform report performance development label accessible extract acoustic feature omg emotion dataset  feature feature utterance visual descriptor multi task cascade cnn apply detection alignment frame frame intermediate feature extract layer vgg model pre facial image lastly average pool conduct frame clip deliver utterance video descriptor experimental setup propose EmoBed network implement gru rnns lstm employ grus parameter owe lack memory output gate faster training training data demand achieve generalisation moreover empirical evaluation grus perform competitively lstm recola fix hidden layer modality specific subnetworks audio video modality subnetwork respectively hidden layer hidden node network utilised adam optimisation algorithm initial rate moreover employ  decay improve model generality furthermore facilitate training mini batch additionally online standardisation apply input data variation training setting empirically recommend previous recola database grid evaluation strategy omg emotion network training hyper parameter hidden layer modality specific modality subnetworks due limited omg emotion dataset mini batch training network crossmodal scenario randomly chose audio video data align data across audio video mini batch advantage synchronous presence modality training phase principally audio video database network training additionally continuously assess emotion annotator perceive acoustic understand report emotional address annotation delay widely explicit compensation approach shift standard respect feature modality task assumption delay invariant annotator annotator modality task refine obtain prediction continuous emotion recognition perform chain processing median filter centre shift filter rate shift delay optimise grid processing parameter optimise development apply evaluation metric evaluate performance continuous emotion regression model concordance correlation coefficient officially recommend avec challenge define  sourcewhere pearson correlation coefficient pcc series prediction standard denote series correspond variance pcc considers similarity series precision relevant estimate performance continuous emotion prediction model trend absolute prediction relevant performance model metric perfect concordance discordance concordance discrete emotion classification task chose evaluation metric mainly due overview performance multi calculate harmonic unweighted precision recall employ omg emotion challenge indicates prediction performance evaluate statistical significance performance improvement unless otherwise undertook fisher transformation dimensional continuous emotion regression categorical discrete emotion classification significant difference trigger experimental discussion sake performance comparison report conduct emotional database recola omg emotion correspond standard testbeds avec omg emotion challenge recola visual feature appearance geometric acoustic feature  aforementioned experimental scenario audio video app video app audio audio video geo video geo audio modality parenthesis employ auxiliary modality correspond modality addition evaluate dimensional arousal valence regression classic monomodal independently perform training audio video data achieve respectively obtain arousal valence prediction respectively classic monomodal outperform challenge benchmark SVR offset approach exception arousal prediction audio signal probably attribute fix network structure optimise arousal prediction employ confirm gru rnns powerful capability capture context dependence performance comparison arousal prediction propose EmoBed related baseline performance comparison valence prediction propose EmoBed related baseline furthermore jointly training audio video data correspond monomodal audio  video appearance video geometric deliver classic monomodal arousal prediction valence prediction respectively observation implies joint training somewhat transfer semantic information heterogeneous data target modality thanks implementation subnetwork multi task framework joint audiovisual training perform triplet constraint across audio video modality obtain introduce enhance monomodal generally significantly outperform classic monomodal fisher transformation suggests implementation triplet constraint helpful distil emotional discriminative representation monomodal scenario crossmodal scenario investigate article finally simultaneously crossmodal triplet training joint audiovisual training EmoBed achieve performance arousal regression valence regression obtain audio model boost arousal regression valence regression respectively integrate video appearance video geometric feature training furthermore achieve video model appearance geometric feature respectively absolute increase classic monomodal arousal regression observation however cannot video valence regression model exception possibly attribute distribution mismatch development partition overall conclude propose EmoBed largely additional knowledge audio signal alleviate shortage video signal vice verse meanwhile EmoBed achieve comparable performance curriculum multi task dynamic difficulty awareness training  although report utilised variety multimodal fusion approach boost performance reference stage demand simultaneous occurrence modality training stage knowledge achieve deliver arousal prediction audio signal valence prediction video signal observation confirms optimise network raw signal target benefit efficient extraction representation therefore future implement EmoBed fashion enhance performance monomodal emotion recognition omg emotion omg emotion database conduct categorical emotion classification task audio visual signal performance model development annotation publicly available database classic monomodal model outperform report literature vector machine svm random RF specifically classic monomodal model yield svm versus percent audio RF versus percent video performance propose EmoBed related baseline report development omg emotion dataset additionally propose joint audiovisual training model classic monomodal former approach outperforms latter margin versus percent audio versus percent video experimental propose joint audiovisual training approach plausible promote performance monomodal emotion classification furthermore obtain utilise triplet training approach distil salient representation across multiple modality nevertheless achieve EmoBed deliver percent absolute performance gain classic monomodal audio video signal respectively observation confirm finding discover recola database visualisation emotion embeddings investigate propose crossmodal framework benefit emotion recognition extract learnt representation classic monomodal propose EmoBed illustrates distribution learnt representation development recola database distribute stochastic neighbour embed sne classic monomodal learnt representation easily distinguish modality stem arousal valence prediction specifically representation learnt modality almost overlap albeit belong emotion stark contrast representation extract EmoBed visibly cluster emotional arousal valence respectively visualisation learnt representation development recola database propose EmoBed classic monomodal marker representation audio  video appearance video geometric modality marker arousal valence observation noticeable omg emotion database sake simplicity merely chose emotional category sad neutral visualisation likewise representation belonging emotional category almost latent visualisation learnt representation development omg emotion database propose EmoBed classic monomodal marker representation audio video modality marker neutral sad category finding representation learnt propose EmoBed somewhat invariant modality emotion embed emotional representation extract audio video signal implicitly fuse knowledge exploitation mutual information possibly performance improvement monomodal discussion demonstrate importance auxiliary modality monomodal emotion recognition independently investigate impact respect counterpart modality joint audiovisual training crossmodal triplet training depict relationship obtain recola database model performance improve increase video arousal regression model cyan observation audio valence regression model therefore behaviour indicates modality indeed enhancement traditional monomodal audio arousal video valence regression model almost remain without obvious performance improvement transfer information modality richer knowledge sparse knowledge easy around audio signal arousal regression video signal valence regression impact joint auxiliary modality loss joint audiovisual training impact crossmodal triplet loss crossmodal triplet training recola database arousal regression perform alpha beta impact joint auxiliary modality loss joint audiovisual training impact crossmodal triplet loss crossmodal triplet training recola database arousal regression perform impact joint auxiliary modality loss joint audiovisual training impact crossmodal triplet loss crossmodal triplet training recola database valence regression perform alpha beta impact joint auxiliary modality loss joint audiovisual training impact crossmodal triplet loss crossmodal triplet training recola database valence regression perform illustrate relationship obtain recola database obviously obtain remarkably increase arousal valence regression specifically triplet training contributes equally traditional emotion regression training yield arousal regression nevertheless audio video valence regression deliver respectively contribution triplet loss implies distil valence salient representation arousal salient representation triplet training moreover conduct investigation omg emotion database categorical emotion classification explicitly quantifies contribution joint audiovisual training crossmodal triplet training crossmodal framework joint audiovisual training contribution auxiliary modality model learnt loss modality separately increase contribution auxiliary modality training increase performance monomodal emotion recognition audio video improve contribution auxiliary modality actually  objective harm modality performance decrease observation crossmodal triplet training impact joint auxiliary modality loss joint audiovisual training impact crossmodal triplet loss crossmodal triplet training omg emotion database perform identify task performance audio video emotion classification joint audiovisual training whereas performance audio video emotion classification achieve respectively crossmodal triplet training conclusion previous emotion recognition focus traditional monomodal modality specific multimodal article propose enhance explore information across auxiliary modality implement exemplary audio visual modality utilised emotion recognition network audio video data complementary information auxiliary modality implicitly transfer target modality apply triplet constraint acoustic visual representation distil emotional embeddings invariant modality propose framework systematically evaluate benchmark database recola omg emotion experimental demonstrate propose significantly improve prediction performance monomodal fuse additional modality training albeit efficiency propose framework developed future triplet training annotation uncertainty information utilised distance learnt representation addition worth model heterogeneous datasets variety domain moreover independently conduct feature extraction perform crossmodal helpful combine framework