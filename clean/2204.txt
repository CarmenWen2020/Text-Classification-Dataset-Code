convolutional neural network DCNNs dominate performer almost computer vision task however remains challenge deploy powerful DCNNs resource limited environment embed device smartphones cnns emerge feasible resource efficient unfortunately suffer significant performance precision counterpart propose novel bayesian optimize compact cnns BONNs model advantage bayesian improve performance cnns significantly BONNs incorporate prior distribution precision kernel feature filter bayesian framework construct cnns comprehensive manner propose bayesian algorithm optimize network simultaneously kernel feature filter largely improves compactness capacity cnns introduce bayesian prune cnns significantly increase model efficiency competitive performance enables variety practical scenario extensive imagenet cifar LFW datasets BONNs achieve classification performance variety cnn model bonn achieves generalization performance detection task access auckland library introduction convolutional neural network DCNNs exhibit superior performance computer vision task however superiority accompany significant computation storage requirement exist powerful DCNNs compose extensive amount float parameter convolution operation implement matrix multiplication float operand computation storage consume prohibits DCNNs deployed resource limited device smartphones drone tackle various approach explore compress DCNNs coarsely categorize quantization prune quantization approximate precision parameter precision simultaneously accelerate convolution operation storage convolutional neural network cnns regard extreme quantization binarize convolution kernel activation recently dorefa net exploit convolution kernel width parameter gradient accelerate training inference abc net adopts multiple binary activation approximate precision degradation prediction accuracy reduce modulate convolutional network binarize kernel borrow ADMM compress model network net explores variant residual structure preserve activation function tight approximation derivative non differentiable function quantization scheme stage approach alternately quantize activation optimal memory efficiency performance furthermore wage propose discretize training inference quantizes activation gradient error quantization introduce discrete backpropagation algorithm cnns evolution prior distribution observation posterior latent variable precision parameter quantization error parameter initialize accord mode gaussian distribution algorithm converges ideal becomes gaussian distribution corresponds minimum reconstruction error gaussian mixture distribution mode binarized image neural network prune focus remove network connection non structure structure manner non structure prune   propose saliency measurement remove redundant derivative matrix loss function propose iterative thresholding remove unimportant absolute connection splice avoid incorrect prune reduce accuracy loss prune network exist non structure prune typically rely specific hardware accelerate online inference contrast structure prune achieve rapid inference without specialized package propose magnitude prune remove filter correspond feature calculate norm filter layer wise manner taylor expansion criterion propose prune filter tune prune network iteratively unlike multistage layer wise prune huang wang uncontrolled sparse mask prune network manner recently kernel sparsity entropy introduce indicator prune network efficiently however hardware dependent therefore restrict application despite exist progress quantization network prune combine quantization prune unified framework reinforce however clearly introduce prune technique cnns filter kernel equally important worth quantize validate subsequently potential prune network conduct quantization remain network compress network however fails difference binarized precision parameter prune  cnns tend easily prune cnns redundant binarization promising alternative conduct prune  however remains unified framework calculate network prune due deteriorate representation capability network backpropagation sensitive parameter update exist optimization scheme fail tackle investigate possibility bayesian establish global optimization scheme prune cnns bayesian  precision kernel quantization obtain cnns quantization error minimize precision kernel gaussian mixture model gaussian correspond quantization cnns gaussians mixture model employ model precision kernel subsequently bayesian framework establishes prune operation prune cnns filter assume gaussian distribution average replace filter illustrates overall framework innovative introduce procedure cnns compression minimize reconstruction error parameter quantization model parameter distribution gaussian mixture mode binarized prune quantize network maximize posterior probability analysis loss correspond algorithm refer bayesian kernel loss bayesian feature loss bayesian prune loss loss jointly apply conventional entropy loss within propagation pipeline advantage bayesian intrinsically inherit model quantization prune propose loss comprehensively supervise training cnns concern kernel distribution feature distribution finally direction cnns prune explore improve compress model applicability practical application summary contribution threefold novel bayesian framework calculate extremely compact cnns integrates quantization prune joint bayesian framework fully investigates intrinsic relationship precision cnns kernel feature filter cnns binarized prune innovative loss propose simultaneously kernel distribution feature distribution filter distribution supervise training comprehensive efficient model achieve classification performance cnns imagenet cifar LFW datasets BONNs consistently achieve performance prior detection task pascal voc coco extension conference essential innovation improve extend bayesian task prune cnns validate effectiveness propose elaborate detail asynchronous backpropagation optimization conduct extensive apply model task classification recognition prune validate BONNs detection task achieve performance  pascal voc coco performance cnns datasets LFW  FP  coco others strongly effectiveness related extensive report compress accelerate DCNNs quantization rank approximation network prune recently architecture amongst network prune quantization relevant review neural network quantization knowledge bnn attempt binarize activation convolution layer cnns bnn maintain classification accuracy datasets cifar cifar effective apply datasets imagenet instead binarizing kernel layer wise scalar reconstruct binarized kernel prof absolute mav layer optimal inspire scalar reconstruct binarized kernel  adopts binarization scheme achieve accurate approximation preserve advantage binary operation abc net adopts multiple binary activation approximate precision alleviate degradation prediction accuracy decouple continuous parameter discrete constraint network ADMM therefore achieves extremely rate recently net explores variant residual structure preserve activation function tight approximation derivative non differentiable function  introduces neural architecture NAS binarization  det fully  angular amplitude minimization technology narrow gap convolution convolution explores influence refer introduces rectify clamp  revive update   apply restart rate schedule quantize network achieves peak performance cifar quantize kernel activation binary extreme neural network quantization prone unacceptable accuracy degradation accordingly sufficient attention paid quantize DCNNs specifically ternary introduce reduce quantization error  dorefa net exploit convolution kernel width parameter gradient accelerate training inference  precision coefficient quantize ternary quantization scheme stage approach alternately quantize activation optimal tradeoff memory efficiency performance  quantization interval obtain optimal directly minimize task loss network accuracy degeneration width reduction uniform mixed precision quantization optimize distil dataset engineer statistic batch normalization across layer network introduce transfer network quantization obtain accurate precision model utilize kullback leibler KL divergence enables accurate approximation tensor distribution entire minimize quantization error investigate data quantization remove data dependence burden furthermore wage propose discretize training inference activation gradient error quantize despite excellent efficiency cnns seriously limit network representation capability inevitable loss accuracy handle  utilizes projection algorithm enhance backpropagation correspond projection loss improve accuracy cnns significantly neural network prune network prune unstructured prune structure prune unstructured prune remove unimportant independently proposes prune absolute sparse structure compress sparse format aware prune approach introduce prune unimportant layer layer minimize error reconstruction network speedup achieve specific sparse matrix multiplication hardware contrast structure prune directly remove structure kernel filter layer compress simultaneously cnns various shelf library sparsity regularization introduce yoon hwang exploit correlation feature network average percentage zero  filter exploit prune percentage zero output feature correspond filter recently prune filter minimal impact convolutional layer propose lasso regression channel selection scheme reconstruction prune filter recently propose global dynamic training algorithm prune  filter propose novel optimization sgd filter collapse  parameter  prune identical filter without performance loss propose filter prune via geometric median instead traditional norm criterion introduce budget aware regularization prune network via learnable mask layer knowledge distillation although filter prune approach reduce memory footprint dimensional mismatch encounter popular multi network resnets differs approach reduce redundancy 3D filter modify output convolutional layer avoid dimensional mismatch channel prune widely resnet densenet remove unimportant input feature convolutional layer avoid dimensional mismatch impose regularization factor batch normalization unimportant feature prune convolution sparsify network however channel prune obtain sparse network complex training procedure offline binarization compress accelerate DCNNs prune binary operation however unlike previous  IR net latent distribution kernel feature filter consideration propose bayesian loss improve cnns capacity significantly furthermore bayesian prune bayesian prune loss reduce storage computation cnns bayesian bayesian paradigm construct statistical model bayes theorem useful algorithm understand algorithm bayesian advantage probabilistic graphical model achieve information exchange perception task inference task conditional dependency dimensional data effective model uncertainty comprehensively bayesian neural network  recent development establish efficacy  reference therein estimation posterior distribution bayesian inference information uncertainty data parameter however analytical posterior distribution intractable parameter functional neural network lend integration approach propose posterior distribution  optimization technique variational inference VI sample approach markov chain monte carlo MCMC MCMC technique typically obtain sample estimate posterior distribution indeed  MCMC widespread adoption due computational storage dataset contrast MCMC VI tends converge faster apply popular bayesian model factorial model topic model   VI defines variational distribution minimizes kullback leibler KL divergence concern variational recent application variational inference  besides bayesian widely subareas principle bayesian approach neural network tune hyper parameter fitting training data recently bayesian apply estimate layer network depth neural architecture employ sparsity induce prior obtain model depends subset kernel function linear model tip neural network neuron prune ingoing outgo bayesian apply obtain sparse compress network latter variational inference dropout rate exist employ bayesian structure prune filter prune moreover attempt binarize prune cnns framework important topic prior distribution kernel feature bayesian framework achieve bayesian loss optimize cnns bayesian kernel loss improves layer wise kernel distribution convolution layer bayesian feature loss introduces intra compactness alleviate disturbance induced quantization bayesian prune loss  channel gaussian distribution prune bayesian feature loss apply fully layer image brief description notation propose bayesian apply building analyze standard neural network computer vision task  leverage efficacy bayesian construct cnns manner propose bayesian algorithm optimize cnns improve efficiency stability unified framework considers specific kernel distribution cnns supervises feature distribution loss detailed bayesian framework enables prune model clarity describes notation bayesian formulation compact cnns cnns involve optimization continuous discrete training cnn involves pas backward pas parameter update gradient calculation binarized pas inference gradient calculation update parameter precision reveal performance quantize network propose probabilistic framework optimal cnns bayesian loss bayesian kernel loss network parameter quantize code precision code quantization error minimize define precision quantize vector respectively denotes vector reconstruct hadamard reconstruction error assume obey gaussian prior zero variance correspond minimum reconstruction error maximize optimize quantization cnns  bayesian bayes theorem conditional probability hypothesis limited observation calculation  optimize binarization function complicate due unknown bayesian perspective resolve via maximum posterior   min  exp exp assume component quantization error simplify cnns usually quantize absolute neglect overlap model gaussian mixture mode  ΨΨ exp  exp   ΨΨ exp  exp  accord dimension accordingly rewrite min    det ΨΨ independently det ΨΨ accordingly determinant matrix ΨΨ ΨΨ bayesian kernel loss bayesian feature loss bayesian feature loss alleviate disturbance extreme quantization cnns intra compactness feature  supposedly gaussian distribution  reveal loss bayesian kernel loss define      min   bayesian feature loss    respectively latent distribution kernel feature consideration framework introduce bayesian loss improve capacity cnns bayesian prune binarizing cnns prune cnns bayesian framework channel distribution channel combine prune mathematical aspect achieve bayesian formulation bnn prune directly extend actually systematic calculate compact cnns kernel layer  tensor     denote output input channel respectively height width kernel respectively clarity define       dimensional filter  simplicity omit prune cnns assimilate filter  algorithm replace filter average optimization assumption  gaussian distribution training prune becomes average replace  distribution actually worth gaussian distribution constraint widely accordingly bayesian prune cnns denote difference filter  gaussian distribution simplicity calculate minimize bayesian framework       exp exp   mode min      det ΨΨ bayesian prune loss brief summary bayesian prune solves prune assume kernel gaussian distribution finally prune viewpoint obtain prune suitable binary neural network exist prune moreover latent distribution kernel feature filter consideration framework introduce bayesian loss bayesian prune improve capacity cnns comparative experimental model prune demonstrate superiority BONNs exist prune BONNs employ bayesian loss optimize cnns bayesian optimize cnns BONNs reformulate bayesian loss cnns                 det       vectorization kernel matrix convolutional layer  vector modulate    covariance kernel vector layer respectively  bayesian optimization loss furthermore assume parameter kernel independent  becomes diagonal matrix identical   variance kernel layer calculation inverse   identical  implementation  replace average accordingly scalar instead matrix involve inference computation significantly accelerate training cnns bayesian prune loss  optimization feature channel       det  gaussian cluster layer   belong implementation define int  pre define prune rate layer gaussian sample     matrix BONNs entropy loss  bayesian optimization loss  bayesian prune loss  aggregate loss    binarization training becomes prune bayesian kernel loss constrains distribution convolution kernel symmetric gaussian mixture mode simultaneously minimizes quantization error   meanwhile bayesian feature loss modifies distribution feature reduce intra variation classification bayesian prune loss converges kernel compress cnns bayesian BONNs propose cnn elaborate procedure parameter propagation backpropagation propagation propagation binarized kernel activation accelerate computation convolution reconstruction vector essential cnns described denotes vector reconstruct precision vector layer mention sec propagation  becomes scalar layer  calculate online convolution  denotes binarized feature layer feature layer depicts actual convolution binary obtain binarization convolution layer float multiplication negligible BONNs addition gaussian distribution bayesian prune update filter specifically replace filter    prune asynchronous backward propagation minimize update       stochastic gradient descent sgd asynchronous manner update instead elaborate update  define  gradient precision kernel                               indicator function widely estimate gradient non differentiable parameter  vector  update  unlike propagation calculation gradient calculate asynchronous manner specifically  compose                           update     kernel sec gradient scalar gradient   compute                       denotes  update  strategy loss tune update  straightforward elaborate brevity update  prune purpose filter gradually converge replace filter  correspond  gradient                     update filter prune redundant filter filter remove others however operation distribution input channel batch norm layer dimension mismatch convolutional layer maintain batch norm layer correspond remove filter zero remove information retain extent summary propose trainable manner procedure detailed algorithm evaluate propose BONNs widely task image classification recognition detection evaluate bonn model prune image classification task image classification BONNs evaluate cifar imagenet ILSVRC datasets backbone resnets   komodakis resnets  recognition LFW celebrity frontal profile   datasets utilized verify effectiveness resnets extend detection pascal voc coco ssd faster rcnn framework besides model prune bayesian algorithm validate image classification task cifar imagenet ILSVRC datasets favorable generalization capability BONNs integrate dcnn variant comparison cnns resnet WRN  komodakis resnet MobileNetV precision backbone network kernel activation binarized performance report verify superiority BONNs image input image chosen imagenet ILSVRC dataset image feature binary feature layer BONNs feature fourth correspond binary feature although binarization feature information loss BONNs extract essential feature accurate classification image datasets implementation detail datasets cifar image classification dataset compose training image respectively image span airplane automobile deer frog comparatively cifar comprehensive dataset cifar  employ backbone BONNs imagenet ILSVRC classification dataset diverse challenge contains training image validation image across CASIA webface image dataset individual nearly facial image private datasets  VGGFace facenet training data contains image challenge label LFW dataset celebrity photo web photo organize split contains image celebrity frontal profile  consists image dataset contains image frontal image extreme profile evaluate performance cop variation data split frontal frontal frontal profile comparison  image various image categorize distinct accord identity gender attribute pascal voc dataset contains image model voc  voc  consist approximately image evaluate voc image average precision  evaluation criterion coco dataset consists image category conduct coco detection model combination image coco image sample coco val coco  remain image coco minival average precision AP iou denote  report AP AP    analyze  WRN structure WRN resnet depth factor introduce feature depth expansion stage spatial dimension feature brevity channel stage another important parameter WRN network configuration network dropout layer ratio prevent overfitting rate decay per epoch maximum epoch cifar quantization error WRN bayesian feature loss tune training detail described  komodakis WRN denotes network convolutional layer similarly WRN evaluate accuracy BONNs WRN WRN cifar bayesian feature loss however optimal loss bayesian kernel loss training accuracy imagenet superiority propose bonn xnor net backbone network resnet image resnet implement resnets image classification task resnet resnet resnet binarize feature kernel backbone convolution layer without convolution layer shortcut setting network modification net sgd algorithm momentum decay rate     parameter rate quantization error resnet strategy rate decay employ degrades epoch algorithm maximum epoch evaluate bonn tune pre  bonn  respectively denote implementation bonn resnet architecture net resnet binarizing activation convolutional layer along activation convolutional layer binarizing convolutional layer layer net sub epoch batch rate epoch respectively identical comparison mobilenet  MobileNetV module validate effectiveness bonn specifically MobileNetV architecture    bonn training protocol ssd stage ssd framework extra layer  vgg simonyan zisserman backbone pre imagenet ILSVRC comparison  data augmentation technique training bonn ssd detection framework ssd model epoch rate cosine annual rate decay faster rcnn stage faster rcnn framework resnet backbone pretrained imagenet ILSVRC comparison  data augmentation technique training bonn faster rcnn detection framework  rcnn model epoch rate decay epoch ablation hyper parameter selection evaluate hyper parameter performance BONNs bayesian kernel loss bayesian feature loss balance respectively adjust distribution kernel feature WRN WRN implementation detail zero validate influence bayesian kernel loss kernel distribution utilization bayesian kernel loss effectively improves accuracy cifar however accuracy increase reasonably balance relationship entropy bayesian kernel loss obtain optimal balance classification accuracy hyper parameter dominates intra variation feature bayesian feature loss feature investigate illustrate classification accuracy varies verify bayesian feature loss classification accuracy chosen evaluate convergence performance comparative counterpart resnet imagenet ILSVRC plot training curve xnor net oscillates  suspect trigger sub optimal contrast bonn achieves training accuracy demonstrate kernel distribution binarized convolution layer BONNs training initialize kernel mode gaussian distribution epoch epoch fix distribution kernel becomes compact mode confirms bayesian kernel loss regularize kernel promising distribution binarization image effectiveness bayesian binarization imagenet ILSVRC understand bayesian loss imagenet ILSVRC dataset examine loss affect performance accord bayesian kernel loss bayesian feature loss independently improve accuracy imagenet apply accuracy without bayesian loss imagenet dataset backbone resnet distribution illustrates distribution kernel fix training distribution gradually approach mode GMM assume previously confirm effectiveness bayesian kernel loss intuitive kernel distribution xnor net bonn kernel xnor net distribute tightly around threshold bonn regularize mode GMM style evolution binarized training xnor net bonn binarized bonn diverse distribution xnor bonn WRN convolutional layer epoch distribution difference xnor bonn indicates kernel regularize propose bayesian kernel loss across convolutional layer image evolution binarized training xnor bonn WRN convolutional layer curve axis binarized xnor net tend converge bonn  image effectiveness bayesian feature loss model apply bayesian feature loss model resnet resnet retrain backbone bayesian feature loss epoch hyper parameter sgd optimizer initial rate rate schedule degrades epoch bayesian feature loss boost performance model margin specifically promotes performance resnet resnet accuracy respectively image classification cifar datasets evaluate BONNs xnor net WRN backbone report accuracy precision  cifar cifar WRN variant chosen comprehensive comparison layer  kernel stage data augmentation image pad randomly cifar indicates BONNs outperform xnor net datasets margin precision  BONNs reduce accuracy degradation acceptable backbone verifies advantage building cnns moreover WRN WRN obtain classification accuracy xnor net perform difference layer xnor net fails maintain accuracy becomes deeper BONNs accuracy deeper wider WRN backbone imagenet ILSVRC evaluate performance evaluate BONNs imagenet dataset curve training accuracy notably adopt data augmentation training image random location flip image horizontally simply image resnet backbone slight structure adjustment described without bayesian feature loss imagenet dataset backbone resnet resnet accuracy cifar datasets BONNs  calculate parameter model refer model cifar visualize feature across resnet model imagenet dataset extract essential feature accurate classification performance bonn quantize network resnet backbone  dorefa net TBN xnor net abc net bnn net  IR net  indicates bonn obtains accuracy cnns  IR net perform baseline bonn outperforms accuracy respectively moreover due application clip function net stage procedure extra worth mention dorefa net TBN quantize activation propose model performance comparison accuracy imagenet refer activation bitwidth respectively moreover evaluate bonn tune pre  bonn denote bonn  respectively demonstrates bonn improves pre  accuracy however  gain improvement pre totally bonn obtains accuracy activation resnet backbone bonn xnor net net bonn surpasses prior quantitatively bonn outperforms xnor net net respectively demonstrate bonn promotes resnet backbone accuracy imagenet refer activation bitwidth respectively MobileNetV backbone MobileNetV  comparison    mid achieve architecture depth wise binary convolution replace convolution   denotes input channel bonn surpasses baseline  mid accuracy imagenet ILSVRC demonstrate superiority bayesian  backbone bonn improve performance  employ backbone respectively BONNs limited datasets dataset verifies generalization capability propose BONNs recognition precision cnns xnor net  BONNs recognition task model resnet resnet kernel stage model FC layer CASIA webface dataset training LFW   datasets hyper parameter strategy cifar despite difference rate decay per epoch maximum epoch accuracy resnet resnet recognition datasets demonstrate achieve cnns LFW bonn accuracy degradation precision model verify potential network deeper backbone architecture BONNs layer increase recognition accuracy  dataset detection pascal voc propose bonn neural network xnor net net  framework task detection pascal voc datasets report detection performance multi quantize network dorefa net  illustrates comparison  across quantization detection framework bonn significantly accelerates computation storage various detector binarizing activation comparison  cnns stage stage detection framework voc faster rcnn voc summarize significant performance improvement bonn resnet backbone bonn outperforms xnor net net   width activation memory usage FLOPs illustrate bonn achieves performance counterpart prior ssd framework vgg backbone quantitatively bonn surpasses xnor net net  respectively moreover bonn achieves comparable performance dorefa net demonstrates superiority bonn achieve performance cnns various detection framework various backbone pascal voc achieve closer performance precision model demonstrate extensive clearly validate superiority bonn coco coco dataset challenge detection pascal voc due diversity propose  det neural network xnor net net  coco report detection performance quantize dorefa net  AP iou threshold AP limited width memory usage FLOPs conduct faster rcnn ssd framework bonn outperforms significant margin resnet backbone bonn improves  xnor net net  respectively similarly APs iou threshold bonn outperforms obviously ssd framework vgg backbone bonn achieves  outperforms xnor net net   respectively conclude baseline network quantization achieves performance AP iou threshold AP coco demonstrate bonn superiority universality application setting comparison  AP iou threshold AP various binarized detector faster rcnn ssd detection framework coco minival performance detector report reference prune resnet cifar gal proportion prune regularization prune BONNs bonn parameter visualize filter resnet bayesian prune principal component analysis pca demonstrate distribution filter epoch principal component filter  horizontal vertical coordinate respectively image prune implement bayesian prune BONNs specifically prune resnet model potential application remains challenge prune compact model prune rate layer denote bonn prune model prune rate cifar resnet validate performance prune cifar dataset prune correspond pre bonn model baseline implementation prune rate validate effectiveness bayesian prune resnet cifar bayesian prune prune wider layer channel layer storage computation model prune rate comparison besides prune rate chosen bonn bonn bonn demonstrate achieve  prune rate prune bonn obtains performance gal lightweight prune model ops prune rate bonn prune model outperforms baseline ops prune prune resnet imagenet ILSVRC parameter visualize evolution filter training principal component analysis pca distribution filter nearly gaussian distribution hypothesis bayesian prune imagenet ILSVRC bonn prune evaluate imagenet ILSVRC resnet prune network epoch initial rate factor epoch attempt prune FLOPs model implement imagenet ILSVRC hence prune rate comparison comparison dcp   uniform denotes training bonn resnet width multiplier kernel stage becomes bonn outperforms prior imagenet ILSVRC resnet bonn achieves accuracy exceed adapt dcp   respectively moreover uniform bonn improves accuracy report experimental resnet backbone generalization bayesian prune bonn prune network instance bonn achieves accuracy exceed  adapt dcp respectively efficiency analysis resnet MobileNetV backbone employ memory usage efficiency analysis efficiency bonn quantization network parameter storage cnns parameter memory usage compute summation float parameter  binary parameter  network memory usage   operation ops calculate  FLOPs resnet image classification imagenet ILSVRC propose bonn accelerates resnet theory furthermore memory likewise bonn achieves acceleration memory resnet backbone respectively tab BONNs compress accelerate lightweight MobileNetV highly efficient convolution  mid backbone bonn achieve ops framework accelerate MobileNetV likewise bonn accelerate MobileNetV  backbone firmly efficiency BONNs compress lightweight network recognition task network architecture image classification input resolution fully layer quantization efficiency calculate similarly situation image classification task efficiency analysis model employ report FLOPs  computation   memory usage evaluate ops faster rcnn resnet backbone ssd vgg backbone detection task bonn achieve theoretical acceleration rate precision faster rcnn backbone significant efficient detection memory consumption likewise bonn realize impressive theoretical acceleration model compression ssd framework conclude BONNs vital significance compute due highly efficient xnor operation comparison theoretical realistic acceleration imagenet ILSVRC bonn resnet backbone efficiency bonn prune bayesian prune prune cnns improve memory footprint acceptable performance loss theoretical realistic acceleration inference propagation baseline prune model nvidia titan gpu bonn baseline bonn gpus quantization acceleration hence bonn inference resnet gpus theoretical acceleration compute FLOPs prune rate bonn acceleration gpus theoretical realistic model gap hardware limitation IO delay buffer switch conclusion future proposes bayesian optimize cnns BONNs precision kernel feature distribution unified bayesian framework bayesian algorithm incorporate prior distribution precision kernel feature filter bayesian framework cnns comprehensive manner bayesian algorithm improve compactness capacity cnns largely extensive cifar imagenet demonstrate BONNs achieve classification performance  resnet superior performance cnns achieve promising performance recognition detection model prune validate generality propose future combine neural architecture NAS data adaptive cnns bayesian optimization optimal prune rate compact cnns