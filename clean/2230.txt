recently attention spent neural architecture NAS aim outperform manually neural architecture vision recognition task inspire attempt leverage NAS technique automatically efficient network architecture image restoration task propose memory efficient hierarchical NAS HiNAS apply task image denoising image super resolution HiNAS adopts gradient strategy flexible hierarchical inner outer architecture width respectively inner propose layer wise architecture strategy flexible architecture performance outer strategy memory considerably accelerate propose HiNAS memory computation efficient  gpu denoising network bsd dataset super resolution structure divk dataset architecture HiNAS parameter enjoy faster inference achieve highly competitive performance code available http github com  HiNAS access auckland library introduction classical task computer vision image restoration aim estimate underlie image degrade measurement ill inverse procedure degradation image restoration categorize various sub denoising super resolution   inpainting etc focus image denoising image super resolution image denoising aim restore image noisy owe corruption occurs image inevitably degrades visual quality image image denoising various computer vision task purpose image super resolution estimate resolution image resolution image super resolution mainly interpolation bilinear bicubic computationally efficient yield satisfactory recent image denoising image super resolution shift approach mapping function quality image desire correspond quality image convolutional network transformer outperform conventional significantly date focus improve quality restore image largely neglect inference image restoration model typically parameter involve recurrent optimization improve restoration quality inference effort spent employ compact model faster inference however trivial manually compact model enjoy accuracy inference recently witness develop automate manual architecture architecture automatically algorithm achieve highly competitive performance vision task image classification detection semantic segmentation recently NAS algorithm architecture image restoration propose instance  CAE  manually architecture architecture NAS algorithm achieve performance parameter drawback computationally demand tesla gpus CAE  tesla gpus promising architecture summary image restoration promising performance efficient architecture substantial effort NAS image restoration approach overcome architecture introduce relatively compute resource addition attention inference important attempt memory efficient NAS algorithm specifically propose memory efficient NAS algorithm propose NAS algorithm automatically neural architecture efficiently image processing task solves architecture substantial effort propose memory efficient significant amount compute resource addition inference consideration deformable convolution flexible powerful conventional convolution deformable convolution useful improve performance however later deformable convolution consume standard convolution deformable convolution abandon improve inference unfortunately although abandon deformable convolution improve inference accuracy gap propose layer wise architecture strategy adopt residual structure benefiting architecture enjoys faster inference achieve performance demonstrate effectiveness image processing application namely image denoising image super resolution contribution summarize built upon gradient NAS algorithm propose memory computation efficient hierarchical neural architecture approach image denoising image super resolution HiNAS knowledge attempt apply differentiable architecture algorithm vision task propose layer wise architecture strategy improve flexibility propose memory strategy contribute efficiency propose HiNAS architecture image restoration task gtx gpu apply HiNAS image denoising various mode image super resolution evaluation network HiNAS achieves highly competitive performance algorithm parameter faster inference conduct analyze network architecture NAS algorithm internal structure offering insight architecture NAS review relevant related image denoising super resolution currently due popularity convolutional neural network cnns image restoration algorithm image denoising image super resolution achieve significant performance boost image denoising notable network model DnCNN IRCNN predict residue instead denoised image FFDNet attempt address spatially append input DnCNN  incorporates non local operation recurrent neural network rnn image restoration  formulates differentiable version improve DnCNN recently algorithm focus noisy image denoising model previously almost model synthetic noisy image risk overfitting model synthetic data along  simulated camera pipeline generate realistic training data proposes camera simulator aim accurately simulate degradation transformation perform image pipeline camera image super resolution attempt propose built cnn model consists convolutional layer traditional approach neural network model achieve impressive performance later consistent development cnn vision task cnn super resolution approach tend employ deeper model instance propose VDSR DRCN VDSR DRCN convolutional layer employ residual strategy address issue lack memory usually influence specific prior memory recursive gate built  stack propose skip connection addition deeper network improve accuracy spatial correlation consideration researcher attempt embed attention mechanism network  incorporates non local module recurrent network senet built residual channel attention network super resolution recently extend channel attention channel attention residual module via skip connection aggregate feature apart image restoration quality inference efficiency critically important image restoration task image restoration algorithm restore quality image quality image highly valuable network propose cascade residual network combine information distillation recently boost inference FSRCNN knowledge distillation typically accuracy inference network architecture NAS NAS aim automate approach discover performance neural architecture procedure tedious heuristic manual neural architecture largely eliminate pipeline optimization objective NAS complex non differentiable approach employ evolutionary algorithm EA optimize neural architecture parameter architecture obtain iteratively mutate population candidate architecture alternative EA reinforcement RL technique policy gradient RL recurrent neural network meta controller generate potential architecture typically encode sequence explore predefined EA RL suffer drawback inefficient amount computation zero optimization technique therefore propose remedy issue exemplar hyper network network morphism recently increase NAS architecture via gradient optimization relax architecture representation supernet via continuous relaxation optimize architecture parameter supernet via gradient descent propose adopts gradient strategy besides strategy important NAS algorithm defines architecture NAS approach employ complex chain structure chain structure sequence layer candidate operation choice convolution operation depth wise separable convolution dilate convolution pool operation convolution operation hyperparameters kernel kernel stride conduct architecture complex computationally expensive architecture instance  gpus architecture cifar later motivate craft architecture usually consist module propose structure chain structure overall architecture directly structure architectural building construct network stack building pre outline structure chain structure structure advantage structure model consists layer faster adopt structure achieve  architecture easily transfer datasets increase reduce therefore structure gradually gain popularity recent propose integrates chain structure structure propose layer wise architecture strategy previous algorithm construct overall network stack repeatedly layer architecture sacrifice flexibility network consist sequence propose algorithm architecture layer flexible consistent inception series benefiting algorithm gradient architecture layer introduce additional computational continuous variable propose enjoys structure flexible chain structure closely related DARTS   auto deeplab architecture optimization supernet consists layer correspond parameter optimize parameter supernet via gradient descent finally derive architecture accord parameter DARTS firstly propose continuous relaxation architecture representation efficient architecture gradient descent achieve competitive performance motivate efficiency DARTS successor HiNAS employ gradient approach strategy DARTS later NAS algorithm kernel width addition later NAS algorithm discard operation reduction   auto deeplab adjustment spatial resolution integrate candidate operation HiNAS reduction pool operation discard retain resolution feature adjustment receptive relies operation receptive  discovers sequential structure chooses kernel width within manually invert bottleneck  extends chain structure insert various width stage densely subsequent   HiNAS structure structure auto deeplab HiNAS dense prediction task introduce multiple width auto deeplab extends kernel width propose HiNAS resembles auto deeplab difference candidate operation discard pool operation retain resolution feature rely automatically operation receptive adapt receptive propose layer wise architecture strategy improve flexibility strategy propose improve memory efficiency relevant NAS algorithm neural network architecture image restoration task related  CAE   network medical image denoising via EA CAE employ EA architecture convolutional auto encoders image inpainting denoising  propose image super resolution task  combine RL EA hybrid controller model generator mention relatively amount computation amount gpu propose HiNAS employ strategy significantly efficiency detail propose HiNAS gradient architecture algorithm specifically computation denotes architecture built stack width topological architecture width HiNAS hierarchical inner responsible inner topological architecture outer width improve efficiency HiNAS propose strategy feature outer introduce architecture continuous relaxation inner explain width via multiple candidate outer elaborate residual framework image denoising super resolution strategy detail loss function inner inner architecture  contains layer architecture compact node important input input node operation image overall framework supernet supernet contains component manually pre architecture fix automatically algorithm consists layer contains width propose HiNAS layer architecture layer architecture improves flexibility performance introduce computation overhead activate update via gradient descent layer marked arrow image inner architecture super node node  contains operation operation corresponds denotes importance layer purpose architecture continuous variable continuous variable update via gradient descent optimization discrete architecture obtain layer discard node explain detail inner architecture denote  layer output previous previous input output tensor  operation acyclic graph consists node inside node input output previous node input output tensor layer architecture output ith node calculate    input node output layer respectively layer correspond operation denotes operator layer wise architecture  NAS employ complex entire network directly enormous gpus significant amount computation instance gpus architecture dataset cifar later propose  architectural building overall network stack pre outline structure  enjoys faster  widely recent  significantly reduces complexity useful improve RL EA NAS algorithm reduce complexity training network  accelerates price restrict flexibility restricts layer network architecture propose HiNAS adopt flexible architecture layer gradient HiNAS introduce continuous variable without additional computational output node compute    related continuous variable layer layer continuous variable concatenation output node express concat employ operator conv convolution sep separable convolution sep separable convolution dil convolution dilation rate dil convolution dilation rate skip skip connection none connection return zero preserve pixel information image processing abandon sample operation pool layer stride convolution layer convolution operator employ convolution standard convolution separable convolution dilation convolution convolution operator consists LeakyReLU activation layer convolution layer batch normalization layer kernel separable dilation convolution algorithm adapt receptive convolution operator kernel compact comparison feature feature image outer specific architecture inside beside architecture inner heuristically width automatically width overall network multiple candidate conventional cnns width convolution layer related spatial resolution instance feature sample width convolution layer HiNAS instead sample operation pool layer stride convolution layer rely operation receptive dilation convolution operation separable convolution operation adjust receptive automatically conventional adjust width longer applies employ flexible hierarchical task width NAS algorithm NAS algorithm literature outer layer width mostly image understand task   expansion rate inside module discover compact network image classification introduce outer layer width determines width layer similarly supernet contains  width layer illustrate supernet mainly consists consist input layer convolution layer relatively shallow feature extract input data convolution layer input data propagate via skip connection layer layer  width supernet shallow feature extract fed layer output layer fed layer concatenate output layer convolution layer generate residual finally wise sum residual output skip connection obtain layer contains width supernet width correspond decision reduce width previous width increase width layer continuous relaxation strategy mention architecture reuse layer width width width factor output feature layer  output  channel width   node residual framework residual framework denoising residual framework super resolution image   previous layer  layer output layer convolution feature width input  output ith layer compute       combine output accord correspond  input feature   compute  similarity auto deeplab feature stride image segmentation however auto deeplab output sum output     comparison input layer simplicity hierarchical structure candidate candidate  memory consumption factor supernet advantage improve applicability NAS consumes memory computation improve memory efficiency enables broader application improve efficiency memory consumption supernet batch convergence enables deeper wider supernet accurate approximation memory efficiency important NAS previous already propose efficiency memory efficient strategy adopt propose strategy differs core  core load pretrained generate network reduce training scratch   NAS  shot NAS avoid traverse operation PC DARTS training  subset channel activate iteration core reduce duplicate operation processing sum output candidate processing output separately adopt concurrent calculate sum kernel output feature outer layer width extension inner however obtain width layer obtain compact architecture inner architecture previous action inner width adjacent layer network drastically negative impact efficiency explain avoid probability viterbi decode algorithm maximum probability target classification task considers interaction multi feature receptive preserve resolution simultaneously hierarchical rout inner choice width operation multi supernet subnet encode powerful multi feature network previous algorithm width amc   assign width operation within sequential worth essentially operation wise width trivial addition  width via channel mask TAS prune channel accord learnable distribution transfer knowledge  network prune network via knowledge distillation topology flexible residual HiNAS instead restoration directly residual information quality image quality improve performance residual framework image denoising image super resolution image denoising input data propagate framework directly output network generate restoration image super resolution residual straightforward spatial resolution input output typically image input network generates output denote spatial resolution input image upscale factor super resolution respectively pixel shuffle operation convert output   residual finally residual  input image generate restoration gradient descent HiNAS optimization supernet loss HiNAS inspire psnr ssim widely evaluation metric image restoration task loss consist correspond evaluation metric respectively optimization function loss     ssim    input image correspond truth  loss enforce visible structure  supernet learns residual input image  skip connection operation image denoising bicubic sample operation image super resolution ssim structural similarity coefficient empirically optimization supernet gradient descent performance network HiNAS collapse epoch becomes recent dart concurrent observation collapse issue challenge pre epoch tackle issue supernet obtain performance evaluation specifically split training disjoint validation sub datasets optimize supernet kernel convolution layer layer width optimize periodically evaluate performance supernet validation dataset performance supernet corresponds performance architecture detail setting datasets implementation detail denoising widely dataset bsd combination image training image validation training image dataset generate noisy image gaussian image visual image denoising denoising layer wise architecture strategy residual strategy strategy truth denoising image bsd image layer wise architecture  residual  strategy denoising image super resolution resolution image divk dataset widely benchmark datasets bsd urban manga bicubic degradation model resolution image respectively degrade spatial resolution visual image super resolution super resolution layer wise architecture strategy residual strategy strategy truth super resolution image bsd super resolution image urban image setting  image denoising image super resolution image denoising supernet contains consists node perform architecture bsd specifically randomly training sample validation validation equally subset subset update kernel convolution layer subset optimize parameter neural architecture image super resolution supernet involves node perform divk similarly training sample subset validation batch training supernet image denoising image super resolution supernet maximum epoch optimize parameter kernel architecture optimizers parameter convolution layer employ standard sgd optimizer momentum decay respectively rate decay cosine anneal strategy architecture parameter adam optimizer rate decay epoch update parameter kernel alternately optimize kernel convolution layer architecture parameter epoch training randomly patch network evaluation split image adjacent patch network finally correspond patch obtain image epoch evaluate supernet epoch update performance training setting sgd optimizer denoising super resolution network iteration initial rate batch respectively data augmentation random random rotation horizontal vertical flip random patch randomly cropped input image ablation layer wise architecture  residual  strategy super resolution focus ablation analysis component propose analyze layer wise architecture strategy residual benefit outer layer width finally verify loss  indeed improves image restoration layer wise architecture residual evaluate layer wise architecture strategy  residual  architecture setting setting employ layer wise architecture strategy residual strategy strategy comparison image denoising super resolution respectively correspond visual layer wise architecture strategy residual strategy improve performance architecture strategy obtain performance image  layer wise architecture strategy residual strategy equally important instance layer wise architecture strategy improves psnr ssim respectively residual strategy improves  ssim respectively layer wise architecture residual strategy improves psnr ssim strategy improves  ssim respectively denoising detail image super resolution psnr ssim slight improvement psnr ssim significantly improve residual fourth achieves performance bsd psnr ssim improve layer wise architecture residual layer wise architecture residual achieves performance psnr ssim accord strategy closest truth benefit outer layer width evaluate benefit outer layer width apply HiNAS bsd setting denote HiNAS HiNAS HiNAS HiNAS inner architecture layer width HiNAS algorithm latter setting inner architecture algorithm outer layer width manually width HiNAS HiNAS width width setting comparison denoising bsd HiNAS HiNAS psnr ssim decrease respectively addition parameter increase HiNAS HiNAS psnr ssim slight improvement psnr ssim meanwhile correspond parameter increase outer layer width HiNAS achieves performance parameter benefit  loss comparison setting image comparison setting benefit  loss item image denoising visual image denoising denoising image bsd denoising image bsd screen image analyze loss item  improves image restoration implement baseline HiNAS mse loss HiNAS combination mse loss  experimental image denoising bsd dataset HiNAS achieves  denoising image HiNAS reverse texture information HiNAS mse loss HiNAS outperforms competitive model   etc experimental image super resolution report correspond visual image denoising loss  benefit psnr ssim metric however image super resolution loss  improve psnr ssim bsd loss  improves ssim decrease psnr urban HiNAS combination loss HiNAS dataset HiNAS performance HiNAS conjecture  ssim evaluation metric  improve ssim directly affect psnr indirectly ssim psnr consistent image bsd image image HiNAS ssim psnr HiNAS however visual difference image cleaner grid structure image summary image super resolution  improves performance benefit  loss item image super resolution visual image super resolution image bsd image urban image architecture analysis image denoising conv sep dil denote conventional separable dilate convolution skip skip connection architecture HiNAS modify modify operation marked denote modification image architecture analysis image super resolution architecture HiNAS modify modify operation marked denote modification image architecture analysis image denoising architecture analysis denoising  resolution architecture HiNAS denoising  resolution network HiNAS width layer maximum channel consistent previous manually network instead node convolution HiNAS connects node operator HiNAS operator network HiNAS consist fragment network performance previous denoising model explain fragmentation structure beneficial accuracy verify HiNAS improves accuracy architecture simply integrate various structure convolution operation modify architecture HiNAS modify architecture unmodified architecture architecture analysis super resolution comparison NAS image denoising CAE training compute gpus author comparison NAS image super resolution modification replace conventional convolution architecture convolution verify operation HiNAS irreplaceable modification topological structure inside aim verify connection relationship built HiNAS indeed appropriate propose modification modify operation connection node modify architecture achieve performance however limited modification marked comparison mention modification operation denote modification degrade accuracy image denoising super resolution replace convolution operation slightly reduces psnr ssim replace convolution operation topological structure influential performance comparison conclusion HiNAS structure selects convolution operation instead simply integrate complex network various operation slight perturbation architecture deteriorates accuracy indicates architecture indeed local optimum architecture denoising visual denoising   HiNAS denoising image bsd denoising image comparison NAS inspire recent advance NAS NAS propose image restoration task CAE propose image inpainting denoising   propose super resolution  network medical image denoising HiNAS CAE architecture image denoising bsd HiNAS   super resolution network divk detail previous NAS image processing task HiNAS faster instance tesla gpus  super resolution architecture divk  contrast HiNAS gtx gpu HiNAS benefit advantage HiNAS gradient strategy CAE   network gene update population controller instance  model contrast HiNAS supernet stage outer layer width across feature memory consumption supernet batch training supernet propose strategy significantly accelerates without negative influence performance comparison super resolution factor HiNAS network recent psnr ssim quantitatively restoration performance comparison denoising bsd  HiNAS model margin propose HiNAS achieves performance ssim  slightly HiNAS psnr  nearly HiNAS overall HiNAS achieves performance others addition model  network HiNAS parameter faster inference HiNAS network parameter  CAE  HiNAS network reduces inference bsd comparison super resolution super resolution experimental parameter model parameter HiNAS achieves psnr highly competitive ssim faster inference parameter instance network HiNAS parameter san inference san super resolution model HiNAS achieves performance faster inference recent FSRCNN KD leverage distillation improve performance FSRCNN parameter faster inference HiNAS however psnr ssim FSRCNN KD super resolution HiNAS comparable FSRCNN KD inference performance consideration conclusion cnn super resolution model achieve significantly visual bicubic slightly psnr guarantee plausible visual san  achieve psnr HiNAS difference psnr ssim san slightly HiNAS visual san HiNAS summary visual HiNAS highly competitive visual super resolution bicubic san  HiNAS super resolution image bsd super resolution image bsd image inference performance denoising bsd super resolution bsd marker encode parameter HiNAS superior performance recent image inference accuracy visualize performance comparison HiNAS inference ssim parameter image denoising HiNAS achieves advantage denoising network image super resolution HiNAS achieves inference performance inference HiNAS FSRCNN KD performance HiNAS FSRCNN KD super resolution network HiNAS achieve ssim faster inference parameter super resolution conclusion propose HiNAS attempt NAS automatically effective network architecture image restoration task image denoising super resolution HiNAS hierarchical inner outer inner topological structure outer layer width respectively propose layer wise architecture strategy improve flexibility improve performance propose strategy memory accelerate finally instead network restoration directly HiNAS network residual information quality image quality image HiNAS memory computation efficient gpu extensive experimental demonstrate propose HiNAS achieves highly competitive performance model parameter faster inference propose apply image processing task