recent neural network successful academia computer vision task mainly due scalability encode data maneuver billion model parameter however challenge deploy cumbersome model device limited resource mobile phone embed device computational complexity storage requirement variety model compression acceleration technique developed representative model compression acceleration knowledge distillation effectively learns model teacher model rapid increase attention community comprehensive survey knowledge distillation perspective knowledge category training scheme teacher architecture distillation algorithm performance comparison application furthermore challenge knowledge distillation briefly review comment future research access auckland library introduction basis artificial intelligence variety application computer vision reinforcement processing recent technique residual connection batch normalization easy model layer powerful gpu tpu cluster resnet model popular image recognition benchmark image powerful bert model understand model achieve overwhelm however computational complexity massive storage requirement challenge deploy application device limited resource video surveillance autonomous generic teacher framework knowledge distillation image develop efficient model recent usually focus efficient building model depthwise separable convolution   model compression acceleration technique category parameter prune focus remove inessential parameter neural network without significant performance category model quantization model binarization structural matrix parameter rank factorization identify redundant parameter neural network employ matrix tensor decomposition transfer compact convolutional filter remove inessential parameter transfer compress convolutional filter knowledge distillation KD distill knowledge neural network network comprehensive review model compression acceleration outside scope focus knowledge distillation increase attention research community recent neural network achieve remarkable performance scenario data parameterization improves generalization performance data however deployment model mobile device embed challenge due limited computational capacity memory device address issue propose model compression transfer information model ensemble model training model without significant accuracy model model later formally popularize knowledge distillation knowledge distillation model generally supervise teacher model model mimic teacher model obtain competitive superior performance transfer knowledge teacher model model basically knowledge distillation compose component knowledge distillation algorithm teacher architecture teacher framework knowledge distillation schematic structure knowledge distillation relationship adjacent survey mainly contains fundamental knowledge distillation knowledge distillation scheme teacher architecture distillation algorithm performance comparison application discussion challenge future direction abbreviate sec image although theoretical empirical understand knowledge distillation specifically understand mechanism knowledge distillation   obtain theoretical justification generalization bound convergence distil network scenario linear classifier justification learns reveals factor distillation successful distillation relies data geometry optimization bias distillation objective monotonicity classifier quantify extraction visual concept intermediate layer neural network explain knowledge distillation zhu theoretically explain knowledge distillation neural network respective risk bound data efficiency imperfect teacher cho  empirically analyze detail efficacy knowledge distillation empirical model teacher model capacity gap distillation adversely affect empirical evaluation knowledge distillation knowledge distillation mutual affection teacher cho  knowledge distillation explore label smooth assess accuracy teacher obtain prior optimal output layer geometry knowledge distillation model compression inspire recent knowledge distillation extend teacher mutual assistant lifelong extension knowledge distillation concentrate compress neural network lightweight network easily deployed application visual recognition recognition processing nlp furthermore knowledge transfer model another knowledge distillation extend task adversarial attack data augmentation data privacy security motivate knowledge distillation model compression knowledge transfer apply compress training data dataset distillation transfer knowledge dataset dataset reduce training load model schematic illustration source response knowledge feature knowledge relation knowledge teacher network image comprehensive survey knowledge distillation objective survey overview knowledge distillation typical knowledge distillation architecture review recent progress knowledge distillation algorithm application scenario address hurdle insight knowledge distillation perspective knowledge transfer knowledge training scheme distillation algorithm structure application recently survey knowledge distillation comprehensive progress perspective teacher vision challenge wang yoon survey mainly focus knowledge distillation perspective knowledge distillation scheme distillation algorithm performance comparison application organization knowledge distillation summarize sect respectively exist teacher structure knowledge distillation illustrate sect knowledge distillation approach comprehensively summarize sect performance comparison knowledge distillation report sect application knowledge distillation illustrate sect challenge future direction knowledge distillation conclusion sect knowledge knowledge distillation knowledge distillation strategy teacher architecture crucial role focus category knowledge knowledge distillation vanilla knowledge distillation logits model teacher knowledge activation neuron feature intermediate layer knowledge model relationship activation neuron sample information teacher model furthermore parameter teacher model connection layer another knowledge discus knowledge category response knowledge feature knowledge relation knowledge intuitive category knowledge within teacher model response knowledge generic response knowledge distillation image specific architecture benchmark knowledge distillation image response knowledge usually refers neural response output layer teacher model directly mimic prediction teacher model response knowledge distillation effective model compression widely task application vector logits output fully layer model distillation loss response knowledge formulate  indicates divergence loss logits logits teacher respectively typical response KD model response knowledge model prediction response detection task logits offset bound semantic landmark localization task estimation response teacher model heatmap landmark recently response knowledge explore address information truth label conditional target popular response knowledge image classification target specifically target probability input belongs estimate softmax function exp  logit factor introduce importance target target informative knowledge teacher model accordingly distillation loss logits rewrite  generally employ kullback leibler divergence loss clearly optimize logits teacher easily understand response knowledge distillation benchmark model vanilla knowledge distillation joint distillation loss loss define entropy loss  truth label logits model  feature knowledge response knowledge straightforward easy understand context knowledge another perspective effectiveness target analogous label smooth regularizers however response knowledge usually relies output layer target fails address intermediate supervision teacher model important representation neural network logits probability distribution response knowledge distillation limited supervise generic feature knowledge distillation image feature knowledge neural network multiple feature representation increase abstraction representation therefore output layer output intermediate layer feature knowledge supervise training model specifically feature knowledge intermediate layer extension response knowledge training thinner deeper network intermediate representation introduce   improve training model directly feature activation teacher inspire variety propose feature indirectly specific  komodakis derive attention feature express knowledge attention generalize huang wang neuron selectivity transfer   transfer knowledge probability distribution feature easy transfer teacher knowledge introduce factor understandable intermediate representation reduce performance gap teacher propose route constrain hint supervises output hint layer teacher recently propose activation boundary hidden neuron knowledge transfer interestingly parameter intermediate layer teacher model response knowledge teacher knowledge semantics teacher propose layer knowledge distillation adaptively assigns teacher layer layer via attention allocation generally distillation loss feature knowledge transfer formulate  feature intermediate layer teacher model respectively transformation function usually apply feature teacher model indicates similarity function feature teacher model feature KD model summarize feature knowledge perspective feature source layer distillation loss specifically   norm distance norm distance entropy loss maximum discrepancy loss respectively feature knowledge transfer favorable information model effectively hint layer teacher model layer model remains investigate due significant difference hint layer properly feature representation teacher explore relation knowledge response feature knowledge output specific layer teacher model relation knowledge explores relationship layer data sample explore relationship feature propose  define gram matrix layer  matrix summarizes relation feature calculate inner feature layer correlation feature distil knowledge knowledge distillation via singular decomposition propose extract information feature knowledge multiple teacher zhang peng graph respectively logits feature teacher model node specifically importance relationship teacher model logits representation graph knowledge transfer multi graph knowledge distillation propose lee graph knowledge intra data relation feature via multi attention network explore pairwise hint information model mimic mutual information hint layer teacher model distillation loss relation knowledge relation feature formulate    feature teacher model respectively feature chosen teacher model  model  similarity function feature teacher model indicates correlation function teacher feature traditional knowledge transfer involve individual knowledge distillation individual target teacher directly distil distil knowledge contains feature information mutual relation data sample specifically propose robust effective knowledge distillation via instance relationship graph transfer knowledge instance relationship graph contains instance feature instance relationship feature transformation layer propose relational knowledge distillation transfer knowledge instance relation manifold network feature embed preserve feature similarity sample intermediate layer teacher network relation data sample model probabilistic distribution feature representation data probabilistic distribution teacher knowledge transfer propose similarity preserve knowledge distillation similarity preserve knowledge arises activation input teacher network transfer network pairwise similarity preserve propose knowledge distillation correlation congruence distil knowledge contains instance information correlation instance correlation congruence distillation network correlation instance generic instance relation knowledge distillation image described distillation loss relation knowledge instance relation formulate  feature representation teacher model respectively similarity function correlation function teacher feature representation typical instance relation KD model summary relation knowledge distil knowledge categorize perspective structure knowledge data privileged information input feature summary  category relation knowledge specifically   mover distance huber loss angle wise loss frobenius norm respectively although relation knowledge recently model relation information feature data sample knowledge deserves distillation scheme discus distillation scheme training scheme teacher model accord teacher model update simultaneously model scheme knowledge distillation directly category offline distillation online distillation distillation distillation pre network distillation network distillation image offline distillation previous knowledge distillation offline vanilla knowledge distillation knowledge transfer pre teacher model model therefore training stage namely teacher model training sample distillation teacher model extract knowledge logits intermediate feature training model distillation stage offline distillation usually knowledge distillation assume teacher model pre define attention paid teacher model structure relationship model therefore offline mainly focus improve knowledge transfer knowledge loss function feature distribution advantage offline easy implement teacher model model software package possibly machine knowledge extract cache offline distillation usually employ knowledge transfer phase training procedure however complex capacity teacher model training avoid training model offline distillation usually efficient guidance teacher model moreover capacity gap teacher exists largely relies teacher online distillation although offline distillation effective issue offline distillation attract increase attention research community overcome limitation offline distillation online distillation propose improve performance model capacity performance teacher model available online distillation teacher model model update simultaneously knowledge distillation framework trainable variety online knowledge distillation propose specifically mutual multiple neural network collaborative network model model teacher training improve generalization ability mutual extend ensemble logits introduce auxiliary peer leader mutual diverse peer model reduce computational zhu gong propose multi architecture indicates model backbone network ensemble logits introduce feature fusion module construct teacher classifier replace convolution layer cheap convolution operation model employ online distillation distribute neural network propose variant online distillation distillation distillation parallel multiple model architecture model transfer knowledge model recently online adversarial knowledge distillation propose simultaneously multiple network discriminator knowledge probability feature adversarial distillation lately devise gan generate divergent online distillation phase training scheme efficient parallel compute however exist online mutual usually fails address capacity teacher online setting topic explore relationship teacher model online setting distillation distillation network teacher model regard online distillation specifically propose distillation knowledge deeper network distil shallow distillation attention distillation propose lane detection network utilizes attention layer distillation target layer snapshot distillation variant distillation knowledge earlier epoch network teacher transfer later epoch supervise training within network reduce inference via exit   propose distillation training scheme exit layer mimic output later exit layer training recently distillation theoretically analyze improve performance experimentally demonstrate zhang  furthermore distillation recently propose specific propose teacher knowledge distillation analysis label smooth regularization  choi propose novel knowledge distillation knowledge consists predict probability instead traditional probability predict probability define feature representation training model reflect similarity data feature embed propose wise knowledge distillation output distribution training model intra sample augment sample within source model addition distillation propose adopt data augmentation knowledge augmentation distil model distillation adopt optimize model teacher network architecture network  knowledge previous network teacher optimization besides offline online distillation intuitively understood perspective teacher offline distillation knowledgeable teacher knowledge online distillation teacher distillation knowledge oneself moreover distillation combine complement due advantage distillation online distillation properly integrate via multiple knowledge transfer framework teacher architecture knowledge distillation teacher architecture generic carrier knowledge transfer quality knowledge acquisition distillation teacher teacher network habit teacher capture distil knowledge knowledge distillation structure teacher important recently model setup teacher almost pre fix  structure distillation easily model capacity gap however  architecture teacher architecture model setup nearly discus relationship structure teacher model model illustrate relationship teacher model image knowledge distillation previously compress ensemble neural network complexity neural network mainly dimension depth width usually transfer knowledge deeper wider neural network shallower thinner neural network network usually chosen simplify version teacher network layer channel layer quantize version teacher network structure network preserve network efficient operation network optimize global network structure network teacher model capacity gap neural network neural network degrade knowledge transfer effectively transfer knowledge network variety propose reduction model complexity specifically introduce teacher assistant mitigate training gap teacher model model gap reduce residual assistant structure residual error recent focus minimize difference structure model teacher model combine network quantization knowledge distillation model quantize version teacher model   propose structure compression involves transfer knowledge multiple layer layer progressively perform wise knowledge transfer teacher network network preserve receptive online teacher network usually ensemble network model structure structure recently depth wise separable convolution widely efficient neural network mobile embed device inspire neural architecture NAS performance neural network improve global structure efficient meta operation furthermore dynamically knowledge transfer regime knowledge distillation automatically remove redundant layer data driven reinforcement optimal network teacher network previous focus structure teacher model knowledge transfer scheme model teacher model improve knowledge distillation performance adaptive teacher architecture recently neural architecture knowledge distillation joint structure knowledge transfer guidance teacher model future distillation algorithm effective knowledge transfer directly response knowledge feature knowledge representation distribution feature teacher model model algorithm propose improve transfer knowledge complex setting review recently propose typical distillation knowledge transfer within knowledge distillation adversarial distillation knowledge distillation teacher model perfectly data distribution simultaneously model capacity cannot mimic teacher model accurately training model mimic teacher model recently adversarial attention due generative network generative adversarial network gans specifically discriminator gan estimate probability sample training data distribution generator fool discriminator generate data sample inspire adversarial knowledge distillation propose enable teacher network understand data distribution category adversarial distillation generator gan training data improve KD performance teacher discriminator discriminator gan ensures generator mimic teacher teacher generator online knowledge distillation enhance discriminator image adversarial distillation gans category category adversarial generator generate synthetic data directly training dataset augment training dataset furthermore   utilized adversarial generator generate knowledge transfer generally distillation loss gan KD category formulate  output teacher model respectively indicates training sample generate generator random input vector distillation loss predict truth probability distribution entropy loss kullback leibler KL divergence loss teacher discriminator category introduce distinguish sample teacher model logits feature specifically unlabeled data sample knowledge transfer multiple discriminator furthermore effective intermediate supervision squeeze knowledge mitigate capacity gap teacher representative model propose category formulate     network  indicates typical loss function generative adversarial network output teacher category adversarial knowledge distillation online manner teacher jointly optimize iteration besides knowledge distillation compress gans gan network mimic gan teacher network via knowledge transfer summary conclude adversarial distillation gan effective enhance via teacher knowledge transfer joint gan KD generate valuable data improve KD performance overcome limitation unusable  data KD compress gans multi teacher distillation teacher architecture useful knowledge network multiple teacher network individually  distillation training network typical teacher framework teacher usually model ensemble model transfer knowledge multiple teacher simplest average response teacher supervision signal multi teacher knowledge distillation recently propose generic framework multi teacher distillation generic framework multi teacher distillation image multiple teacher network effective training model usually logits feature representation knowledge addition average logits teacher incorporate feature intermediate layer encourage dissimilarity training sample utilize logits intermediate feature teacher network teacher transfer response knowledge teacher transfer feature knowledge randomly teacher pool teacher network iteration transfer feature knowledge multiple teacher additional teacher network mimic intermediate feature teacher network address multiple teacher manner teacher efficiently perform knowledge transfer explore multiple teacher alternative propose simulate multiple teacher teacher stochastic skip connection multiple teacher model feature ensemble knowledge amalgamation knowledge amalgamation public available model teacher reuse interestingly due characteristic multi teacher distillation extension domain adaptation via knowledge adaptation privacy security data summary multi teacher distillation knowledge distillation scheme response knowledge feature knowledge relation knowledge abbreviate    respectively summary typical multi teacher distillation knowledge distillation scheme generally multi teacher knowledge distillation knowledge tailor versatile model diverse knowledge teacher however effectively integrate knowledge multiple teacher modal distillation data label modality available training important transfer knowledge modality typical scenario modal knowledge transfer review teacher model pretrained modality rgb image annotate data sample transfer knowledge teacher model model unlabeled input modality depth image optical specifically propose relies unlabeled sample involve modality rgb depth image feature obtain rgb image teacher supervise training sample transfer annotation label information via wise sample registration widely modal application perform estimation occlude image synchronize signal camera image knowledge transfer across modality estimation  gall obtain sample modality rgb video skeleton sequence transfer knowledge rgb video skeleton action recognition model improve action recognition performance rgb image perform modality distillation additional modality depth image generate  rgb image modality introduce contrastive loss transfer wise relationship across modality improve target detection propose modality distillation available  gans generic framework modal distillation generic framework modal distillation simplicity modality image summary modal distillation modality knowledge distillation moreover propose knowledge distillation visual knowledge trilinear interaction teacher model image input distil bilinear interaction model image input probabilistic knowledge distillation propose   knowledge transfer textual modality visual modality propose modality  architecture modality distillation improve detection performance besides model distillation transfer knowledge multiple domain  summary modal distillation modality knowledge distillation scheme specifically knowledge distillation performs visual recognition task modal scenario however modal knowledge transfer challenge modality gap lack sample modality graph distillation knowledge distillation algorithm focus transfer individual instance knowledge teacher recent propose explore intra data relationship graph graph distillation graph carrier teacher knowledge graph message passing teacher knowledge generic framework graph distillation described sect graph knowledge relation knowledge introduce typical definition graph knowledge graph message passing distillation algorithm specifically zhang peng vertex supervise teacher graph construct logits intermediate feature logits graph representation graph transfer knowledge multiple supervise teacher graph maintain relationship sample dimensional knowledge transfer propose locality preserve loss function lee analyse intra data relation multi graph vertex feature layer cnns directly transfer mutual relation data sample teacher graph graph   similarity matrix mutual relation activation input teacher model similarity matrix teacher furthermore response feature knowledge graph knowledge instance feature instance relationship model vertex graph respectively generic framework graph distillation image graph knowledge knowledge transfer graph specifically modality discrepancy incorporate privileged information source domain graph refer distillation graph introduce explore relationship modality vertex modality connection strength modality another propose bidirectional graph diverse collaborative explore diverse knowledge transfer introduce GNNs knowledge transfer graph knowledge besides knowledge distillation topological semantics graph convolutional teacher network topology aware knowledge transfer graph convolutional network graph distillation transfer informative structure knowledge data however properly construct graph model structure knowledge data challenge attention distillation attention reflect neuron activation convolutional neural network attention mechanism knowledge distillation improve performance network attention KD attention transfer mechanism define distil knowledge teacher network network core attention transfer define attention feature embed layer neural network knowledge feature embed transfer attention function unlike attention attentive knowledge distillation propose attention mechanism assign confidence data distillation data KD propose overcome unavailable data arise privacy legality security confidentiality concern data implies training data instead data newly synthetically generate generic framework data distillation image specifically transfer data generate gan propose data knowledge distillation transfer data network reconstruct layer activation layer spectral activation teacher network propose  knowledge distillation generate synthesize image data knowledge transfer propose zero shot knowledge distillation exist data transfer data model softmax parameter teacher network target data generate information feature representation teacher network zero shot knowledge distillation shot distil knowledge teacher model neural network teacher limited label data besides distillation data distillation data distillation data distillation training annotation unlabeled data generate teacher model employ model summary synthesis data data distillation usually generate feature representation pre teacher model although data distillation potential unavailable data remains challenge task generate quality diverse training data improve model generalizability quantize distillation network quantization reduces computation complexity neural network convert precision network float precision network meanwhile knowledge distillation aim model yield performance comparable complex model KD propose quantization teacher framework framework quantize distillation specifically propose quantize distillation transfer knowledge quantize network mishra  propose quantize KD apprentice precision teacher network transfer knowledge precision network ensure network accurately mimic teacher network precision teacher network quantize feature knowledge transfer quantize teacher quantize network propose quantization aware knowledge distillation quantize network teacher network knowledge transfer furthermore empirical analysis neural network distillation quantization account hyper parameter knowledge distillation teacher network distillation recently unlike quantize distillation distillation training scheme improve performance quantize model teacher model parameter generic framework quantize distillation image lifelong distillation lifelong continual continuous meta aim accumulates previously knowledge transfer knowledge future knowledge distillation effective preserve transfer knowledge without catastrophic forget recently increase KD variant lifelong developed propose adopt meta meta transfer network transfer teacher architecture propose framework leap meta task manifold transfer knowledge another knowledge transfer network architecture shot image recognition architecture simultaneously incorporates visual information image prior knowledge propose semantic aware knowledge preservation image retrieval teacher knowledge obtain image modality semantic information preserve transfer moreover address catastrophic forget lifelong global distillation knowledge distillation lifelong gan multi model distillation KD developed extract knowledge network task performance comparison knowledge distillation cifar indicates performance improvement network correspond baseline model performance comparison knowledge distillation cifar indicates performance improvement network correspond baseline model NAS distillation neural architecture NAS popular auto machine automl technique aim automatically identify neural model adaptively appropriate neural structure knowledge distillation knowledge transfer depends knowledge teacher architecture however capacity gap teacher model model teacher address issue neural architecture adopt appropriate architecture oracle architecture aware knowledge distillation furthermore knowledge distillation employ improve efficiency neural architecture  NAS distil architecture knowledge teacher architecture  shot NAS  architecture mimic intermediate feature representation teacher network structure efficiently feature transfer effectively supervise teacher performance comparison knowledge distillation excellent technique model compression capture teacher knowledge distillation strategy teacher effective performance lightweight model recently knowledge distillation focus improve performance image classification task clearly demonstrate effectiveness knowledge distillation summarize classification performance typical KD popular image classification datasets datasets cifar cifar compose rgb image respectively training image image training image comparison experimental classification accuracy KD directly derive correspond cifar cifar report performance knowledge distillation scheme structure teacher model specifically accuracy parenthesis classification teacher model individually accuracy  dcm  performance teacher online distillation performance comparison observation summarize knowledge distillation realize model model compression model achieve knowledge distillation online knowledge distillation collaborative significantly improve performance model knowledge distillation improve performance model offline online distillation transfer feature knowledge response knowledge respectively performance lightweight model improve knowledge transfer capacity teacher model performance comparison knowledge distillation easily conclude knowledge distillation effective efficient technique compress model application effective technique compression acceleration neural network knowledge distillation widely artificial intelligence visual recognition recognition processing nlp recommendation furthermore knowledge distillation purpose data privacy defense adversarial attack briefly review application knowledge distillation KD visual recognition variety knowledge distillation widely model compression visual recognition application specifically knowledge distillation previously developed image classification extend visual recognition application recognition image video segmentation action recognition detection lane detection identification pedestrian detection facial landmark detection estimation video caption image retrieval shadow detection saliency estimation depth estimation visual odometry text image synthesis video classification visual anomaly detection knowledge distillation classification task fundamental task briefly review knowledge distillation challenge image classification setting recognition action recognition exist KD recognition focus efficient deployment competitive recognition accuracy specifically knowledge chosen informative neuron hint layer teacher network transfer network teacher strategy loss feature representation hint layer knowledge transfer avoid incorrect supervision teacher recursive knowledge distillation previous network initialize recognition perform recognition identity unknown training recognition criterion usually distance metric feature representation positive  sample angular loss correlate embed loss improve resolution recognition accuracy knowledge distillation framework developed architecture resolution teacher resolution model acceleration improve classification performance specifically propose selective knowledge distillation teacher network resolution recognition selectively transfer informative facial feature network resolution recognition sparse graph optimization resolution recognition realize resolution invariant model unify  heterogeneous recognition sub net efficient effective resolution recognition model multi kernel maximum discrepancy teacher network adopt feature loss addition KD recognition extend alignment verification loss knowledge distillation recently knowledge distillation successfully complex image classification incomplete ambiguous redundant image label label  model distillation label progression propose informative collective dynamic label complex image classification address catastrophic forget cnn variety image classification task without forget cnn knowledge distillation lifelong propose recognize image task preserve task improve image classification accuracy propose feature knowledge distillation gan transfer knowledge feature knowledge distillation visual interpretation diagnosis framework unifies teacher model interpretation generative model diagnosis image classifier KD resolution recognition propose feature distillation resolution image classification output feature teacher argue sect knowledge distillation teacher structure transfer preserve modality knowledge efficient effective action recognition modal task scenario successfully realize spatiotemporal modality distillation knowledge transfer action recognition mutual teacher network multiple network spatiotemporal distil dense connectivity network graph distillation multi teacher multi network distill knowledge information multiple modality teacher summarize observation distillation visual recognition application knowledge distillation efficient effective teacher variety visual recognition task lightweight network easily guidance capacity teacher network knowledge distillation knowledge complex data source modality data multi domain data multi task data resolution data flexible teacher architecture knowledge transfer KD nlp conventional model bert consume resource consume complex cumbersome structure knowledge distillation extensively processing nlp obtain lightweight efficient effective model KD propose numerous nlp task exist nlp task KD neural machine translation NMT text generation detection document retrieval text recognition KD nlp belong understand  KD  task specific distillation multi task distillation KD research neural machine translation extend typical multilingual representation model entitle bidirectional encoder representation transformer bert  processing neural machine translation hottest application however exist NMT model competitive performance obtain lightweight NMT extend knowledge distillation neural machine translation recently empirically performance KD non autoregressive machine translation nat model largely relies capacity distil data via knowledge transfer gordon  explain performance sequence knowledge distillation perspective data augmentation regularization effective knowledge distillation extend sequence sequence generation scenario NMT sequence generation model mimic sequence distribution teacher overcome multilingual diversity propose multi teacher distillation multiple individual model handle bilingual teacher multilingual model improve translation quality ensemble  NMT model teacher supervise model data filter improve performance machine translation machine reading task propose novel online knowledge distillation address  training decrease performance validation online KD evaluate model training chosen teacher update subsequent model model performance teacher model multilingual representation model bert attract attention understand cumbersome model easy deployed address lightweight variation bert bert model compression knowledge distillation propose propose patient knowledge distillation bert model compression bert  sentiment classification paraphrase similarity inference machine reading comprehension patient KD feature representation CLS token hint layer teacher transfer accelerate inference propose  stage transformer knowledge distillation contains domain task specific knowledge distillation classification propose task specific knowledge distillation bert teacher model bidirectional memory network BiLSTM lightweight model distilbert generic structure bert variety task nlp simplify bert propose internal representation teacher bert via internal distillation furthermore typical KD nlp perspective improve efficiency robustness machine reading comprehension propose attention distillation fuse generic distillation distillation avoid confuse task specific distillation performance knowledge distillation interaction pre training distillation tune compact model propose pre distillation performs sentiment classification inference textual entailment multi task distillation context understand propose multi distillation neural network task teacher multi task multilingual representation knowledge distillation transfer knowledge multi lingual embeddings bilingual induction resource knowledge transfer effective across ensemble multilingual model observation knowledge distillation processing summarize knowledge distillation efficient effective lightweight model capacity teacher model transfer knowledge data model quickly task effective performance teacher knowledge transfer easily effectively multilingual task knowledge multilingual model transfer model sequence knowledge effectively transfer network network KD recognition recognition neural acoustic model attract attention due powerful performance however recognition deployed embed platform limited computational resource response complex model cannot satisfy requirement recognition scenario satisfy requirement knowledge distillation widely apply recognition task knowledge distillation lightweight acoustic model recognition KD recognition application spoken identification audio classification text independent speaker recognition enhancement acoustic detection synthesis exist knowledge distillation recognition teacher architecture improve efficiency recognition accuracy acoustic model recurrent neural network rnn temporal information sequence knowledge teacher rnn acoustic model transfer dnn model recognition accuracy obtain combine multiple acoustic mode ensemble rnns individual training criterion model knowledge transfer model performs vocabulary continuous recognition  task strengthen generalization spoken identification lid model utterance knowledge feature representation utterance teacher network transfer utterance network discriminate utterance perform duration utterance lid task improve performance utterance lid interactive teacher online distillation propose enhance performance feature representation utterance lid performance utterance improve distil internal representation knowledge teacher longer utterance utterance meanwhile audio classification multi feature distillation developed adversarial strategy adopt optimize knowledge transfer improve robust recognition knowledge distillation employ enhancement audio visual multi modal knowledge distillation propose knowledge transfer teacher model visual acoustic data model audio data essence distillation modal knowledge teacher efficient acoustic detection quantize distillation propose knowledge distillation quantization quantize distillation transfer knowledge cnn teacher model detection accuracy quantize rnn model unlike exist traditional frame KD sequence KD perform sequence model recognition  temporal classification CTC sequence KD introduce  temporal classification output label sequence training teacher model input frame distillation wong gale recognition performance frame sequence teacher training sequence teacher training propose teacher ensemble construct sequence combination instead frame combination improve performance unidirectional rnn CTC recognition knowledge bidirectional lstm CTC teacher model transfer unidirectional lstm CTC model via frame KD sequence KD moreover knowledge distillation issue recognition overcome overfitting issue dnn acoustic model data scarce knowledge distillation employ regularization adapt model supervision source model adapt model achieves performance acoustic domain overcome degradation performance non native recognition advanced multi accent model distil knowledge multiple accent specific rnn CTC model essence knowledge distillation realizes domain knowledge transfer complexity fuse external model LM sequence sequence model seqseq recognition knowledge distillation employ effective integrate LM teacher seqseq model seqseq model reduce error rate sequence sequence recognition summary observation knowledge distillation recognition conclude lightweight model satisfy practical requirement recognition response limited resource recognition accuracy teacher architecture built rnn model temporal sequence rnn model chosen teacher preserve transfer temporal knowledge acoustic data model sequence knowledge distillation apply sequence model performance frame KD response knowledge sequence KD usually transfer feature knowledge hint layer teacher model knowledge distillation teacher knowledge transfer easily domain modal recognition application multi accent multilingual recognition KD application leverage external knowledge user review image important role effectiveness recommendation model reduce complexity improve efficiency recommendation model recently knowledge distillation successfully apply recommender model compression acceleration knowledge distillation introduce recommender rank distillation recommendation express rank propose adversarial knowledge distillation efficient recommendation teacher review predication network supervises user item prediction network generator adjust adversarial adaption teacher network unlike distillation tang wang enhance collaborative denoising autoencoder  model recommender via knowledge distillation capture useful knowledge user feedback reduce unified  framework contains generation network retrain network distillation layer transfer knowledge reduces generation network characteristic knowledge distillation teacher architecture knowledge distillation effective strategy adversarial attack perturbation model issue unavailable data due privacy confidentiality security concern specific perturbation adversarial sample overcome robust output teacher network via distillation avoid expose private data multiple teacher access subset sensitive unlabelled data supervise address issue privacy security data network generate layer activation layer spectral activation teacher network via data distillation data privacy prevent intellectual piracy propose private model compression framework via knowledge distillation model apply public data teacher model apply sensitive public data private knowledge distillation adopts privacy loss batch loss improve privacy compromise privacy performance developed shot network compression via novel layer wise knowledge distillation sample per application knowledge distillation neural architecture interpretability neural network federate conclusion discussion knowledge distillation application arouse considerable attention recent comprehensive review knowledge distillation perspective knowledge distillation scheme teacher architecture distillation algorithm performance comparison application discus challenge knowledge distillation insight future research knowledge distillation challenge knowledge distillation extract knowledge teacher transfer knowledge teacher training therefore discus challenge knowledge distillation following aspect quality knowledge distillation teacher architecture theory knowledge distillation KD leverage combination knowledge response feature relation knowledge therefore important influence individual knowledge knowledge complementary manner response knowledge motivation label smooth model regularization feature knowledge mimic intermediate teacher relation knowledge capture relationship across sample challenge model knowledge unified complementary framework knowledge hint layer influence training model response knowledge layer feature knowledge deeper hint layer suffer regularization transfer knowledge teacher knowledge distillation generally exist distillation categorize offline distillation online distillation distillation offline distillation usually transfer knowledge complex teacher model teacher model model comparable setting online distillation distillation improve efficacy knowledge transfer relationship model complexity exist distillation scheme novel distillation scheme investigate currently KD focus knowledge distillation loss function teacher architecture poorly investigate apart knowledge distillation algorithm relationship structure teacher significantly influence performance knowledge distillation recent model teacher model due model capacity gap teacher model model theoretical analysis capacity neural network shallow network capable representation neural network therefore effective model construction teacher model challenge knowledge distillation despite knowledge distillation application understand knowledge distillation theoretical explanation empirical evaluation remains insufficient distillation privileged information assumption linear teacher model enables theoretical explanation characteristic via distillation furthermore empirical evaluation analysis efficacy knowledge distillation perform cho  however understand generalizability knowledge distillation quality knowledge quality teacher architecture attain future direction improve performance knowledge distillation important factor teacher network architecture knowledge teacher network distil network model compression acceleration neural network usually category namely parameter prune rank factorization transfer compact convolutional filter knowledge distillation exist knowledge distillation related combination knowledge distillation compress quantize knowledge distillation parameter prune integrates network quantization teacher architecture therefore efficient effective lightweight model deployment portable platform hybrid compression via knowledge distillation compress technique compress technique training tune furthermore apply compress topic future apart model compression acceleration neural network knowledge distillation characteristic knowledge transfer teacher architecture recently knowledge distillation apply data privacy security adversarial attack model modality multiple domain catastrophic forget accelerate model efficiency neural architecture supervision data augmentation another knowledge transfer teacher network network accelerate vanilla knowledge distillation feature representation unlabelled data model supervise target model via distillation extension knowledge distillation purpose application meaningful future direction knowledge distillation practicable popularize knowledge transfer classic traditional machine traditional stage classification  cast teacher knowledge distillation furthermore knowledge distillation flexibly deployed various excellent scheme adversarial auto machine label filter lifelong reinforcement therefore useful integrate knowledge distillation scheme practical challenge future