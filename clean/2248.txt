multi channel visual signer continuous 3D communicate production slp automatic translation spoken embody continuous articulation morphology truly understandable deaf community previous slp concatenation isolated focus primarily manual feature robotic non expressive production propose novel progressive transformer architecture slp model translate spoken continuous 3D multi channel sequence manner transformer network architecture introduces counter decode enables variable continuous sequence generation production progress predict sequence extensive data augmentation technique reduce prediction drift alongside adversarial training regime mixture density network mdn formulation realistic expressive sequence propose translation evaluation mechanism slp benchmark quantitative challenge  dataset baseline future research user evaluation slp model understand deaf reception production introduction production slp correspond spoken gloss representation sequence text gloss gloss text translation task highlight slp translation spoken skip gloss intermediary manuscript text denote spoken sequence image visual multi channel medium communication deaf around population hearing loss UK alone estimate deaf hearing deaf native signer spoken meaning spoken immensely therefore prefer communication deaf community posse grammatical structure syntax spoken highlight translation spoken structure due non monotonic relationship 3D visual movement relative important communication convey complex meaning context employ multiple mode articulation manual feature combine non manual feature facial expression  upper posture research vision community previous research focus recognition subsequent translation spoken although useful technology applicable hearing understand deaf helpful deaf community task production slp relevant deaf automatically translate spoken increase content available predominately hearing focus useful deaf community slp sequence understandable akin translator previous slp limited production concatenate isolated focus solely manual feature approach fragment text  gloss production important context lose gloss bottleneck however production sequence challenge task alignment sequence spoken ignore non manual feature disregard contextual grammatical information fully understand meaning 2D skeleton data lack depth channel truly model realistic continuous 3D multi channel production model slp network translate spoken continuous 3D multi channel sequence manner translation source spoken without gloss intermediary propose progressive transformer architecture alternative formulation transformer decode continuous sequence pre define vocabulary introduce counter decode technique predict continuous sequence variable production progress predict sequence production manual non manual feature increase realism comprehension reduce prediction drift continuous sequence production data augmentation robust model reduce erroneous auto regressive prediction continuous prediction articulate output due regression propose addition adversarial training discriminator model source spoken introduce prompt realistic expressive production progressive transformer additionally due multimodal mixture density network mdn model utilise progressive transformer output  gaussian mixture model evaluate quantitative performance propose translation evaluation slp translation SLT translate production spoken evaluate challenge  phoenix  dataset benchmark gloss text configuration underpin future research user evaluation production evaluate comprehension slp model finally qualitative reader insight model performance accurate sequence unseen text input contribution summarise slp model translate spoken continuous 3D sequence enable novel transformer decode technique application conditional adversarial training slp production realistic combination transformer mixture density network model multimodal continuous sequence benchmark slp  dataset translation evaluation metric alongside comprehensive deaf user evaluation preliminary version extend manuscript additional formulation introduction mdn model expressive production extensive quantitative qualitative evaluation explore capability approach alongside user deaf participant comprehension sequence organise outline previous slp surround sect progressive transformer network propose model configuration sect sect experimental setup quantitative evaluation sect qualitative evaluation sect finally conclude sect finding future related understand computational research landscape outline recent literature recognition slr SLT detail previous slp reside intersection vision review recent development neural machine translation NMT finally background application adversarial training mixture density network MDNs sequence task specifically apply generation recognition translation goal vision research develop capable recognition translation production prominent computational research initial focus isolated recognition recent expansion continuous recognition  however majority rely manual feature representation statistical temporal model recently datasets release  phoenix phoenix greek GSL chinese recognition dataset enable application approach  convolutional neural network cnns recurrent neural network rnns expand upon  introduce task SLT aim directly translate video spoken due grammar spoken SLT challenge task  majority utilised NMT network SLT translate directly spoken via gloss intermediary transformer model SLT jointly recognition translation task inclusion multi channel feature reduce dependence gloss annotation SLT production previous research slp focus avatar technique generate realistic production rely pre expensive non manual feature production avatar generation  stiff   mocap approach successfully realistic production expensive statistical machine translation smt apply slp rely processing encode recently increase approach automatic slp slp model combination NMT generative adversarial network gans author independent separately concatenation isolated 2D skeleton mapped gloss via production isolated without realistic transition robotic animation poorly deaf contrary focus automatic production mapping text skeleton sequence directly instead priori closest   neural translator synthesise skeletal text frame input generate sequence fix disregard syntax contrast model allows dynamic output sequence correspond data whilst progress counter sequence generation unlike proprietary dataset publicly available  benchmark future slp research previous slp solely manual feature ignore important non manual convey crucial context meaning  vital comprehension differentiate otherwise  expansion non manual challenge due temporal coherence manual feature intricacy facial movement expand production non manual feature generate synchronise  facial movement model expressive production neural machine translation NMT automatic translation source sequence target sequence neural network tackle sequence sequence task rnns introduce iteratively apply hidden computation across token sequence later developed encoder decoder architecture sequence intermediate embed encoder model drawback fix representation source sequence overcome attention mechanism facilitate source useful context transformer network recently propose achieve performance NMT task transformer attention mechanism generate representation entire sequence global dependency multi attention  layer model combination sequence improve representational model mapping source target sequence representation encoder decoder attention sequence sequence task transformer achieve impressive classic processing nlp task model representation alongside domain image caption action recognition related transformer network apply continuous output task synthesis production recognition apply sequence sequence continuous output task relatively  sequence continuous output previous fix output binary sequence eos flag continuous representation eos token propose novel counter decode technique predicts continuous sequence variable production progress implicitly sequence adversarial training adversarial training inclusion discriminator model improve realism generator critique production gans generate data adversarial technique impressive apply image generation recently video generation task conditional gans extend gans generation upon specific data input gans apply task specific NMT adversarial NMT compliment NMT model cnn adversary propose gan setup translation input sequence specific generation adversarial discriminator production realistic sequence task generate skeleton suffers regression adversarial discriminator improve realism gesture production discriminator smooth diverse conditional discriminator expressive output source spoken mixture density network mixture density network MDNs multimodal prediction model distribution model fully density distribution MDNs combine conventional neural network mixture density model model arbitrary conditional distribution via  neural network estimate density component predict statistic distribution MDNs continuous sequence generation task due ability model sequence uncertainty combine mdn rnn continuous handwrite generation expand sketch generation reinforcement MDNs apply synthesis future prediction prediction MDNs estimation predict multiple hypothesis model uncertainty occlusion knowledge combine transformer MDNs sequence model employ MDNs capture variability model production multiple distribution architecture detail progressive transformer conditional discriminator network progressive transformer sequence respective counter source spoken auto regressive prediction conditional discriminator input truth sequence alongside respective source spoken predicts realism scalar network via combination regression loss  adversarial loss  PT progressive transformer PE positional encode CE counter encode disc discriminator image continuous 3D production introduce slp model learns translate spoken continuous sequence objective conditional probability sequence frame spoken gloss source input replace spoken intermediary sequence continuous skeleton model 3D coordinate signer manual non manual feature target sequence reference spoken challenge firstly exists non  relationship spoken due grammar syntax respective domain secondly target inhabit continuous vector representation discrete text disable classic sequence token finally multiple channel encompass within concurrently manual non manual feature  facial expression address production continuous sequence propose progressive transformer model enables translation symbolic continuous sequence domain PT introduce counter decode enables model progress sequence generation implicitly sequence source propose data augmentation technique reduce impact prediction drift enable production expressive introduce adversarial training regime slp supplement progressive transformer generator conditional adversarial discriminator disc enhance capability model multimodal distribution propose mdn formulation slp network remainder component propose architecture detail progressive transformer upon classic transformer model mapping symbolic source target modify architecture continuous output representation alongside introduce counter decode technique enables sequence prediction variable slp model progress continuous sequence production hence progressive transformer progressive transformer translate symbolic domain gloss spoken continuous 3D sequence sequence signer model output express accurate translation input sequence embody realistic sequence model consists encoder decoder architecture source sequence encode latent representation mapped target output decode auto regressive manner source embeddings per standard NMT pipeline embed symbolic source token via linear embed layer vector dimensional token meaning closer embed bias formulate vector representation source token transformer implementation apply temporal encode layer source embed temporal information network encoder apply positional encode   predefined sinusoidal function relative sequence target embeddings target sequence consists 3D joint signer due continuous apply novel temporal encode refer counter encode CE counter frame relative sequence target joint concatenate respective counter formulate counter frame proportion sequence counter predict alongside skeleton sequence generation conclude counter counter decode progress sequence generation predict sequence without  vocabulary counter model information relate sequence duration inference sequence generation replace predict counter linear timing information stable output sequence counter encode joint linear embed layer formulate embed 3D joint coordinate frame counter decode simultaneous auto regressive prediction continuous counter counter denotes sequence decode image encoder progressive transformer encoder  consists stack identical layer sub layer temporally encode source embeddings  sub layer generates contextual representation perform multiple projection dot attention aim relationship token sequence relevant context sequence formally dot attention output vector combination relevant query dimensionality attention softmax   multiple attention generate parallel mapping query varied learnt parameter allows representation input sequence generate complementary information sub output concatenate project via linear layer      attention        related input variable output  fed sub layer non linear projection residual connection subsequent layer norm employ around sub layer aid training encoder output formulate  contextual representation source sequence data augmentation technique reduce prediction drift robust slp model future prediction prediction multiple future frame counter counter input gaussian applies input skeleton PT progressive transformer image decoder progressive transformer decoder  auto regressive model frame alongside previously described counter distinct symbolic transformer decoder continuous sequence counter concatenate joint embeddings frame firstly initial  sub layer apply joint embeddings encoder extra mask operation mask future frame prevents model attend subsequent decode  mechanism symbolic representation encoder continuous domain decoder sub layer sub layer residual connection layer normalisation encoder output progressive decoder formulate  corresponds 3D joint frame respective counter decoder learns generate frame predict counter sequence model error mse loss predict sequence truth  inference sequence auto regressive manner predict frame input future predict counter decode sequence data augmentation auto regressive sequential prediction suffer prediction drift erroneous prediction accumulate transformer model predict truth input robust predict input impact drift heighten slp model due continuous skeleton neighbour frame content model previous truth input loss penalty inference prediction previous output error quickly propagate throughout entire sequence production overcome prediction drift propose various data augmentation approach namely future prediction counter gaussian future prediction data augmentation conditional future prediction model predict frame sequence future prediction input due neighbour frame movement frame model predict previous frame predict frame future movement learnt simply copying previous frame inference frame prediction production counter inspire  capability transformer model propose pure  approach production contrary input skeleton joint counter target input demonstrates input oppose model decode target sequence solely counter knowledge previous frame halt reliance truth joint embeddings previously access deeper understand source spoken robust production network setup identical training inference model generalise data prediction input gaussian augmentation technique application input sequence training increase variety data input sum epoch distribution statistic joint randomly sample apply input epoch addition gaussian model become robust prediction input error augment input target output inference model noisy input increase ability adapt erroneous prediction sequence generation adversarial training naturally varied movement signer sequence slightly articulation movement realistic consists subtle precise movement easily lose training solely minimise joint error slp model solely regression lack articulation suffer regression specifically average lack comprehensive due variability joint highlight average valid blur articulate production convey meaning average multiple valid blur articulate production due regression image address articulation propose adversarial training mechanism slp introduce conditional discriminator alongside slp generator frame slp min max network evaluate realism production previously described progressive transformer architecture sequence convolutional network scalar realism sequence correspond source input sequence model adversarial manner formalise   truth sequence equates sequence source spoken generator generator learns sequence source spoken sequence integrate progressive transformer gan framework contrary standard gan implementation sequence generation specific source input therefore remove traditional input generate sequence source sequence inspiration conditional gans propose training combination loss function namely regression loss  adversarial loss  loss function combination loss      importance loss function training architecture detail conditional discriminator model concatenate source text project scalar realism sequence image discriminator conditional adversarial discriminator differentiate generate sequence truth sequence source spoken sequence overview discriminator architecture source target sequence generate aim scalar probability sequence originates data sequence counter remove input discriminator critique content due variable frame sequence apply pad transform fix  maximum frame target sequence data    sequence pad zero vector enable convolution upon fix tensor source spoken embed source token via linear embed layer variable sequence embeddings pad fix  maximum source sequence     bias source embed respectively zero pad centre source representation concatenate pad sequence feature   1D convolutional filter sequence analyse local context temporal continuity signing effective frame discriminator realism valid frame consistently temporal leaky relu activation apply layer promote healthy gradient training linear layer sigmoid activation project combine feature scalar probability sequence maximise likelihood sequence generate sequence objective formalise maximise loss function   inference discard sequence auto regressive manner sect mixture density network previously described model architecture generate deterministic production model predict non stochastic prediction unable model uncertainty variation continuous sequence generation task slp deterministic model sequence articulate production expression variability overcome issue deterministic prediction propose mixture density network mdn model variation multiple distribution  entire prediction subspace mixture component model valid movement future enables prediction valid signing correspond uncertainty expressive production formulation MDNs neural network  mixture distribution subset network predicts mixture whilst generates parameter individual mixture distribution previously described progressive transformer architecture amend output model mixture gaussian distribution source token model conditional probability frame  mixture component mdn mixture  distribution regard prior probability frame generate mixture component conditional density  mixture express gaussian distribution  denote variance  distribution respectively parameter mdn predict directly progressive transformer mixture coefficient softmax activation function ensure sum exponential function apply variance ensure positive output overview mixture density network mdn network multiple mixture distribution  progressive transformer PT output input source spoken previous frame output sample mixture distribution expressive variable sequence network negative likelihood  image optimisation training minimise negative likelihood truth data predict mixture distribution formulate     frame sequence mixture component sample inference sample production mixture density compute firstly likely distribution source token mixture   chosen distribution sample predict  valid ensure jitter prediction avoids variation joint sigma particularly predict sequence multiple sample frame mixture density model auto regressive manner sect sample frame input future transformer sequence mdn adversarial mdn combine adversarial training regime outline sect mdn model formulate adversarial generator unchanged conditional discriminator sample discriminator input loss function combination negative posterior loss adversarial generator loss    inference discriminator model discard sequence sample mixture distribution previously explain sequence output model configuration sequence source spoken input animate video skeleton sequence trivial task plot joint relevant timing information progressive transformer counter 3D joint subsequently animate avatar gan sequence valid translation text reference data incorrect signer varied model cadence however visual comparison reference sequence apply dynamic warp dtw temporally align sequence action amend content production temporal coherence visualisation although focus building implementation spoken translate video within however translation delay context translate delay introduce automatic significant delay experimental setup outline experimental setup detail dataset evaluation metric model configuration introduce translation evaluation metric evaluation protocol dataset publicly available  dataset introduce continuous SLT extension phoenix corpus become benchmark SLT research corpus parallel german    video german translation sequence redefine segmentation boundary generate alignment approach video signer vocabulary german gloss training validation split propose skeleton extraction 2D estimation 2D 3D mapping image slp network generate sequence 3D skeleton 2D upper joint facial landmark extract openpose skeletal model estimation improvement   2D upper joint 3D finally apply skeleton normalisation coordinate consistent around joint translation evaluation evaluation continuous sequence generation model task previous slp evaluation metric mse understand propose translation slp evaluation translate sequence spoken automatic understandable production amount translation content preserve correspondence translation visual production quality  inception generative model pre classifier similarly recent slp slr discriminator evaluate isolated skeleton translation performance translation relative evaluation metric model configuration chosen SLT model amend absolute model performance likely however experimentation relative performance comparison model remain consistent ensures comparison model remains valid SLT translation model modify sequence input transformer model layer embed  dataset ensure robust translation text generate spoken translation sequence compute bleu rouge bleu gram completeness truth translation manual non manual manual non manual skeleton representation multiple SLT model various skeleton representation namely manual non manual manual non manual evaluate translation performance configuration understandable representation amount spoken recover manual non manual configuration achieves translation non manual achieve significantly demonstrates manual non manual feature complementary information translate spoken multi channel representation text gloss translation transformer architecture quantitative sect production sequence achieve translation performance truth skeleton data due smooth training data production data contains artifact 2D estimation 2D 3D mapping quality data model learns generate temporally continuous production without artifact significantly smoother truth explains translation performance production truth data evaluation protocol translation evaluation metric slp evaluation protocol  dataset ablation benchmark future text gloss  evaluation protocol symbolic translation spoken representation task translation grammar initial task production bleu rouge comparison without translation gloss GP evaluation protocol evaluates  model capability continuous sequence symbolic gloss representation task production capability network without translation spoken text TP evaluation protocol translation spoken input sequence performance slp consist jointly perform translation production sequence task enables slp application domain expensive gloss annotation available model configuration progressive transformer model built layer embed unless otherwise network xavier initialisation scratch adam optimization default parameter rate plateau rate scheduler patience epoch decay rate minimum rate code NMT toolkit  implement pytorch future prediction gloss task counter gloss task quantitative evaluation thorough quantitative evaluation slp model subsequent discussion conduct text gloss setup evaluate gloss text setup finally user deaf participant text gloss translation baseline evaluates performance classic transformer architecture translation spoken gloss sequence vanilla transformer model predict gloss intermediary layer embed performance encoder decoder network layer gate recurrent grus translation architecture transformer model achieves significantly outperform propose transformer architecture understand gloss production evaluate progressive transformer gloss task outline sect baseline progressive transformer model translate gloss without augmentation data augmentation model suffers prediction drift erroneous prediction accumulate transformer model predict robust target input therefore multiple data augmentation technique introduce sect namely future prediction counter gaussian future prediction data augmentation conditional future prediction model predict frame sequence model future frame prediction multiple future frame increase model performance bleu bleu model cannot rely copying previous frame minimise loss instead predict future prediction exists benefit complexity increase predict frame performance prediction frame sufficient encourage planning understand without averse model complexity counter inspire  capability transformer model evaluate pure  approach counter target input model oppose 3D skeleton joint performance increase approach considerably increase bleu counter model  drift model decode target solely counter cannot rely truth joint embeddings previously access halt erroneous prediction longer fed model setup training inference identical model generalise data gaussian augmentation evaluation examines apply skeleton sequence training joint randomly sample apply input factor augmentation gaussian augmentation achieves performance bleu amount input model become robust auto regressive prediction error augment input target output however increase degradation affect model training subsequent performance overall propose data augmentation technique significantly improve model performance fundamental production understandable sequence gaussian augmentation gaussian gloss task adversarial training evaluate adversarial training regime outline sect training generator discriminator compete min max realistic production fool sequence input source text adversarial progressive transformer generator layer embed performance achieve regression  adversarial  loss   respectively reflect relative adversarial loss adversarial training gloss task mixture density network gloss task conduct non conditional adversarial training regime sequence critique without conditioning upon source input discriminator architecture weak perform generator bleu previous augmentation adversary apply solely sequence negatively affect performance discriminator prompt realistic production regard source text affect quality central translation task evaluate conditional adversarial training regime introduce critique source input evaluate discriminator architecture cnn layer strength adversary finely balance generator min max setup increase increase performance peak bleu discriminator enforce realistic expressive production generator however increase discriminator becomes generator performance negatively affected overall conditional adversarial training regime demonstrate improve performance model solely regression loss bleu considerably previous performance inclusion discriminator model increase comprehension production source sequence input due discriminator generator towards expressive production accurate translation deceive adversary increase content generate sequence understandable output performance mixture density network gloss evaluation mixture density network mdn model configuration outline sect training multimodal distribution model data sample inference progressive transformer model built layer embed evaluate mixture component increase allows multimodal prediction subspace model sequence variation achieve validation performance bleu regression deterministic prediction reduce expressive production subtlety restore particularly variable joint increase model complexity outweighs benefit performance degradation propose mdn formulation achieves performance previous deterministic approach progressive transformer comparison adversarial configuration slight increase performance bleu respectively however translation evaluation perfect performance mdn adversarial model within error margin SLT reduce regression architecture increase articulation additionally evaluate combination mdn loss previously described adversarial loss explain sect creates network mixture distribution generator conditional discriminator sect mdn  adversarial  loss respectively combination mdn adversarial training actually performance individually dev bleu however combination slightly performance mdn alone configuration aim alleviate regression adversely affect performance due goal text production evaluate model text task outline sect translation task source spoken sequence without gloss intermediary model configuration evaluate various model configuration propose sect namely architecture gaussian augmentation adversarial training mdn configuration gloss task gaussian augmentation increase performance architecture bleu due reduction prediction drift previously explain addition adversarial training increase performance bleu conditioning discriminator important task input spoken context production text task model configuration text text gloss network configuration text task text performance bleu mdn model mention earlier performance adversarial mdn setup equivalent utilized SLT perfect due increase context source spoken variety production therefore multimodal model mdn enhance highlight performance gain addition adversarial training mdn model increase performance previous evaluation text text gloss evaluates network configuration production text text TP via gloss intermediary text gloss  task outline  TP TP model outperforms  development information available within spoken gloss representation token per sequence predict predict gloss sequence intermediary information bottleneck information production gloss therefore contextual information source text lose however achieve performance gloss intermediary due limited training sample vocabulary gloss generalisation capability network TP network progressive transformer model powerful sub task firstly mapping spoken sequence representation accurate recreation important future slp model architecture domain gloss availability furthermore bleu outperform text utilise gloss information bleu unfair comparison indication model performance quality sequence user evaluation evaluate production discussion deaf community user output sequence understand understandable native deaf signer perform evaluation skeletal output model confuse translation ability visual aesthetic avatar however assess skeleton directly lose information conveyed image shadow occlusion therefore relative comparison truth sequence ass production fairly although infancy understand important feedback deaf community deaf community empower involve development technology target native conduct user evaluation native  speaker estimate comprehension sequence survey consist comparison production truth data visual task translation task evaluates comprehension animate sequence explain sect video online survey user evaluation conduct collaboration  factor consult gmbh evaluate model configuration adversarial training MDNs user sequence randomise video deaf participant evaluation production quality comprehension qualitative evaluation sequence source input truth video frame sequence model configuration image visual task evaluation visual task video production alongside correspond truth sequence user rate video implicit comparison comparison adversarial mdn model configuration user evaluation visual task percentage user rat truth GT sequence prod visual quality overall user feedback mainly truth video slightly participant prefer production highlight quality video smoothly generate without visual jitter contrary sequence suffer visual jitter due blur video artifact introduce 3D estimation mdn configuration rating participant adversarial setup user prefer mdn production truth sequence adversarial model demonstrates participant prefer visuals mdn model quantitative translation model sect user feedback suggests MDNs production quality translation task evaluation translation task translation accuracy production automatic production alongside spoken translation sequence user likely translation user evaluation translation task percentage participant chose spoken translation choice adversarial user chose translation mdn configuration drastic difference understand model configuration demonstrate mdn production visual translation task alongside quantitative performance conclude propose mdn configuration generates realistic expressive production qualitative evaluation report qualitative slp model snapshot sequence visually output propose model configuration gloss task correspond unseen spoken sequence input alongside frame truth video sequence slp model visually realistic correspondence truth video smooth accurate whilst meaningful express specific non manual feature correspondence truth video alongside accurate movement slight articulation  comparison model configuration gaussian production express specifically adversarial training improves significantly expressive production frame due discriminator production towards realistic output inclusion mdn representation accuracy production visually closer truth due mixture distribution model uncertainty continuous sequence remove production gaussian production visual comparison adversarial mdn production reflect quantitative performance sect demonstrate contrast increase comprehension overall regression diminish realistic production achieve highlight importance propose model configuration failure production due complex  occlusion image regress continuous 3D sequence successfully achieve attention approach predict joint location neighbour frame closely model learnt subtle signer movement smooth transition highlight difference discrete generation spoken qualitative evaluation sequence source input truth video frame sequence model configuration image failure approach complex classifier replicate occlusion affect quality training data production occurs specific entity due lack grammatical context training data conclusion continuous 3D multi channel production model slp model translate text continuous 3D sequence manner enable propose progressive transformer architecture alternative formulation transformer decode variable continuous sequence introduce counter decode technique predict continuous sequence variable production progress predict sequence reduce prediction drift continuous sequence production data augmentation significantly improve model performance predict continuous articulate output propose addition adversarial training network introduce conditional discriminator model prompt realistic expressive production propose mixture density network mdn model utilise progressive transformer output  mixture gaussian distribution evaluate approach challenge  dataset propose translation evaluation metric slp importance data augmentation technique reduce model drift improve model performance addition adversarial training regime mdn output representation furthermore text translation configuration outperform gloss intermediary model meaning slp model limited domain expensive gloss annotation available finally conduct user deaf response production understand comprehension propose model configuration production perfect improve reduce smooth inherent data approach however highlight production improvement fully understandable deaf slp infancy potential growth improvement future 3D skeleton representation affect comprehension sequence future increase realism production generate photo realistic signer gan image image translation model expand skeleton representation feedback user evaluation improve articulation via classifier increase comprehension automatic viseme generator pipeline improve feature deterministic manner data