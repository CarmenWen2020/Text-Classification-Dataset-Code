although synthetic training data beneficial task estimation rgb action recognition relatively unexplored goal synthetic improve performance action recognition focus generalization unseen viewpoint recent advance monocular 3D reconstruction action sequence automatically render synthetic training video action label contribution investigate extent variation augmentation beneficial improve performance viewpoint clothing individual action relevant augmentation non uniform frame sample interpolate individual perform action introduce data generation methodology  allows training spatio temporal cnns action classification substantially improve action recognition performance ntu rgb UESTC standard action multi benchmark finally extend augmentation approach video subset kinetics dataset investigate shot training data available demonstrate improvement introduction action representation rgb video data widely recent advance convolutional neural network cnns excellent performance benchmark datasets ucf however cnns rely heavily availability training data address lack training data explore complementary synthetic data task computer vision optical estimation segmentation estimation synthesize video action recognition limited data viewpoint shot available training surveillance ambient assist living dataset already action camera camera environment viewpoint annotate data appearance action drastically perform viewpoint action recognition network fail drastically distinct viewpoint specifically model video benchmark dataset ntu rgb camera network video obtain  video performance sect motivates action recognition novel viewpoint exist address action recognition challenge setup viewpoint training split introduce challenge protocol viewpoint training recent assume multi training data become inapplicable naive achieve generalization data impractical due combinatorial explosion instead augment exist data synthetically increase diversity viewpoint appearance synthetic relatively easy render task estimation arbitrary capture mocap resource however action classification semantics challenge generate synthetic data action label typical mocap datasets cmu mocap database target diversity suitable action recognition due lack action annotation mocap dataset limited pre define category propose efficient scalable approach generate synthetic video action label target category employ 3D estimation HMMR vibe automatically extract 3D dynamic rgb video sequence SMPL parameter combine randomize generation component viewpoint clothing render diverse complementary training data action annotation overview pipeline demonstrate advantage data training spatio temporal cnn model action recognition unseen viewpoint training shot data boost performance unseen viewpoint ntu UESTC dataset augment limited training data propose  dataset furthermore depth analysis importance action relevant augmentation diversity viewpoint non uniform frame sample strategy substantially improves action recognition performance code data available project  related action recognition establish research review literature action recognition recent survey kong focus relevant synthetic data action recognition briefly 3D estimation synthetic simulate date extensive overview approach recently synthetic image visual model 2D 3D estimation segmentation depth estimation multi estimation pedestrian detection identification estimation recognition synthetic datasets built task recent surreal dataset however action label previous focus synthetic data tackle action recognition synthetic 2D sequence synthetic trajectory invariant action recognition however rgb synthetic training action recognition relatively attempt manually define action jointly estimate category synthetic category multi task however category easily scalable necessarily relate target unlike automatically extract sequence data flexible category recently generate  dataset simulation environment programmatically define synthetic activity source focus generalization data relevant generates synthetic training image achieve performance unseen viewpoint extension   rgb input instead depth formulate frame classification synthetic data feature action recognition feature necessarily discriminative target action category direction explicitly assign action label synthetic video define supervision directly action classification action recognition due difficulty building multi action recognition datasets standard benchmark environment rgb datasets   II ucla availability ntu rgb dataset ntu allows training neural network unlike previous datasets recently dataset UESTC coverage around performer although lab multi action datasets typically capture depth device kinect accurate estimate 3D skeleton skeleton action recognition therefore attention decade variant lstms widely recently spatio temporal skeleton image dimensional standard cnn architecture apply rgb action recognition comparison transform rgb feature invariant trivial transform 3D skeleton transfer appearance feature source target explore maximum margin cluster joint codebook temporally synchronous video   approach focus building global codebooks extract invariant representation recently approach information guidance action recognition formulate adversarial classifier achieve invariance propose fuse specific feature multi cnn approach cannot handle training differs compensate lack diversity synthetic video augment data automatically training model involve extra unlike moreover assume multi video training 3D estimation recover mesh image explore model fitting regress model parameter cnns regress non parametric representation graph volume recently cnn parameter regression approach extend video HMMR image HMR dynamic 1D temporal convolution recently vibe adopts recurrent model frame estimate spin vibe incorporates adversarial loss penalizes estimate sequence realistic indistinguishable amass mocap sequence recover 3D parameter video HMMR vibe employ SMPL model comparison purpose action recognition proxy task evaluate estimation synthetic action label goal improve performance action recognition synthetic data data limited domain mismatch training viewpoint data regime stage obtain 3D temporal model action training sequence viewpoint sect 3D temporal model generate training sequence viewpoint render pipeline augmentation sect training spatio temporal cnn synthetic data sect 3D estimation generate synthetic video graphic technique sequence articulate 3D model employ parametric model SMPL statistical model 3D scan SMPL generates mesh disentangle parameter parameter kinematic deformation due skeletal posture parameter identity specific deformation height hypothesize action capture sequence parameter parameter largely irrelevant necessarily interaction category reliable 3D sequence action recognition video datasets transfer associate action label synthetic video recent namely mesh recovery HMMR unless otherwise HMMR extends image reconstruction HMR video multi frame cnn account temporal neighborhood around video frame HMMR learns temporal representation dynamic incorporate 2D pseudo truth video   multi 2D estimation pre processing input cnn estimate weak perspective camera parameter refer reader detail robustness video ability capture multiple smoothness recover important generalization synthetic video 3D animate synthetically sample video frame recent estimation vibe improvement estimation proportionally affect action recognition performance pipeline parameter HMMR vibe randomly parameter camera parameter factor augmentation synthetic data generation  dataset component detail synthetic dataset  synthetic     render 3D SMPL sequence randomize cloth texture animate model automatically extract dynamic described previous explore various augmentation technique increase intra diversity training video incorporate multi video important interaction category systematically sample viewpoint around perform augmentation illustrate sample synthetic frame visualization  generate video automatic truth 3D joint location segmentation optical SMPL parameter action label training video 3D cnn action classification truth modality input action recognition oracle optical truth estimator segmentation randomly augment background pixel  dataset differs surreal dataset mainly action label explore augmentation automatically extract sequence instead mocap recording cmu mocap database moreover exploit temporal aspect dataset cnns image input employ multi video systematic viewpoint distribution synthetic action estimate 3D video automatically render synthetic video action label explore various augmentation viewpoint appearance training temporal cnns data significantly improves action recognition unseen viewpoint image augmentation illustrate augmentation  dataset action modify joint angle additive parameter augmentation systematically camera viewpoint diversity sample parameter background clothing randomize appearance image  visualize sample  action ntu UESTC datasets estimate HMMR video frame accompany synthetic augmentation variation clothes background camera height distance viewpoint variation viewpoint action video project  project image augmentation automatic extraction 3D sequence 2D video additional challenge dataset quality mocap sequence reduce jitter temporally smooth estimate SMPL parameter linear average SMPL axis angle rotation joint convert quaternion apply linear operation normalize quaternion norm convert axis angle processing remain noisy inevitable monocular 3D estimation task finding interestingly synthetic video beneficial noisy increase diversity perturb parameter various augmentation specifically video additive quaternion joint slightly intra individual augmentation inter individual augmentation interpolate sequence action sequence individual align dynamic warp linearly interpolate quaternion align sequence generate sequence refer interpolation visual explanation significant gain increase diversity multi 2D information video image 3D translation independently frame constant global loses information action jumping translation estimate information potentially increase domain gap exists appendix insert additional model render translate image translate xyz component translation estimation reliable due depth ambiguity therefore explanation omit component appendix temporally smooth translation reduce translation across video across roughly frame therefore relative distance important action towards viewpoint systematically render sequence randomize generation parameter camera rotate azimuth angle respect origin denote distance camera origin height camera randomly sample predefined meter distance meter height adjust accord target background access target dataset estimation extract background pixel directly training dataset without obtain static background ntu UESTC datasets experimentally benefit target dataset background appendix kinetics render unconstrained video non overlap action benefit static background background video pixel training 3D cnns non uniform frame 3D cnns video recognition employ spatio temporal convolutional architecture operates multi frame video input unless otherwise specify network architecture 3D resnet randomly initialize appendix pretraining generalization capability synthetic data across input modality cnn rgb another optical simonyan zisserman average reporting fusion subsample fix input video spatio temporal resolution frame width height respectively optical input rgb input dimensional estimate estimate stack hourglass architecture synthetic data estimation consecutive frame refer reader qualitative optical estimation non uniform frame sample adopt frame sample strategy context 3D cnns instead uniformly sample fix frame rate video clip consecutive frame randomly sample frame across temporal refer non uniform sample although recent explore multiple temporal resolution regularly sample frame rate randomly frame rate sample frame equidistant tsn eco employ hybrid strategy regularly sample temporal randomly sample frame restrict strategy moreover tsn 2D cnn without temporal model 2D convolutional feature frame stack input 3D cnn network none quantify sample strategy concurrent experimental analysis dense consecutive sample hybrid sample tsn consecutive sample non uniform sample report improvement latter video temporally trim around action therefore video span training randomly sample video frame fix input 3D cnn convolutional kernel become invariant data augmentation technique capture cue frame sample illustrate non uniform frame sample strategy 3D cnn training commonly adopt consecutive uniformly sample fix frame rate non uniform sample random skip augmentation context image datasets sample video frame multi datasets ntu UESTC datasets viewpoint respectively ntu correspond UESTC around performer image sample frame clip average softmax uniform sample non overlap consecutive clip slide non uniform randomly sample non uniform clip slide uniform sample clip proportional video precisely frame entire video input frame per clip stride parameter sample clip apply slide uniform non uniform sample clip clip random without replacement frame subset important sample scheme temporal important detail appendix synth video augment multiple synthetic data synthetic data training balance synthetic datasets epoch randomly subsample synthetic video synthetic minimize entropy loss RMSprop mini batch initial rate fix schedule augmentation rgb implementation detail appendix training jointly synthetic data substantially boost performance training ntu CVS protocol unseen improvement rgb fusion marginal improvement addition unlike task reduce synthetic domain gap render version synthetic dataset HMMR vibe estimation improvement vibe moreover training synthetic video alone obtain accuracy baseline training CVS protocol ntu dataset rgb video correspond training respectively training viewpoint performance diagonal domain gap viewpoint viewpoint training account domain gap non uniform frame sample consistently outperforms uniform frame sample action recognition datasets sect extensive ablation action recognition unseen viewpoint sect completeness sect finally illustrate approach video sect datasets evaluation protocol briefly datasets evaluation protocol employ inputting raw parameter performs significantly unseen viewpoint synthetic rendering ntu CVS protocol various input representation increase independence joint coordinate SMPL parameter SMPL parameter without global rotation SMPL model recover rgb HMMR vibe depth kinect joint 2D resnet architecture parameter input architecture significant gain synthetic rendering video text interpretation image ntu rgb dataset ntu dataset capture action synchronous camera video dataset allows training neural network sequence frame average standard protocol report accuracy split CV split considers training training CS training remain training report standard protocol however introduce protocol task challenge training split viewpoint training split protocol CVS focus mainly improve unseen distinct UESTC rgb 3D action dataset UESTC UESTC recent dataset systematically equally viewpoint around dataset action category video frame dataset allows action unusual official protocol CV suitable task viewpoint performance evaluate average across completeness report II CV II protocol concentrate multi training training viewpoint FV odd viewpoint vice versa shot kinetics dataset kinetics formulate shot scenario kinetics video pre model feature extractor model pre mini kinetics subset kinetics define novel remain category described procedure subset kinetics bending clap hug jogging   shake skip swing sweep  category din snake cannot recognize solely additional contextual cue action randomly sample training video per video synthetic augmentation training therefore consists video report accuracy validation video limitation protocol sensitive choice training video 3D estimation fails video model benefit additional synthetic data future multiple training sample video 3D estimation confident report average performance ablation synthetic synth mixed synthetic synth training explore estimation quality inputting raw parameter oppose synthetic rendering synthetic data generation parameter analyze viewpoint diversity evaluate model video baseline protocol ntu training data summarizes training model domain gap viewpoint naturally reduce training however available performance significantly remain ntu assume frontal viewpoint available non uniform frame sample consistent improvement non uniform frame sample uniform consecutive sample setting additional video frame sample optical appendix non uniform sample strategy rgb remainder unless specify otherwise synth training report improvement obtain synthetically increase diversity action ntu combine training data synthetic data augment viewpoint synth synth training rgb combination performance generally rgb possibly due grain category cannot distinguish coarse training synthetic data synth accuracy data indicates generalization capability synthetic combine synthetic training video synth performance rgb increase training challenge unseen viewpoint additional synthetic video obtain without extra annotation confirm noisy estimate sufficient obtain significant improvement discriminative action information synthetic data amount data sequence per action synth synth training ntu CVS split generalization unseen viewpoint significantly improve addition synthetic data training pink training contains additional synthetic data text interpretation image advantage controllable data generation procedure analyze component synthetic data important examine aspect quality estimation input representation amount data diversity diversity additional appendix quality estimation HMMR vibe 3D estimation monocular video recently demonstrate convincing performance unconstrained video opening possibility investigate action recognition synthetic video progress 3D estimation improve synthetic data synthetic data factor source  extract HMMR  extract vibe consistent improvement accurate estimation vibe HMMR propose pipeline potential improve progress 3D recovery raw parameter input another estimation output parameter directly input action recognition model instead synthetic rendering implement 2D cnn architecture input frame sequence 3D joint coordinate joint SMPL joint kinect 3D joint rotation axis angle relative rotation SMPL without global rotation resnet architecture HMMR vibe SMPL parameter input kinect joint ntu dataset comparison report various representation performance synthetic rendering observation remove viewpoint dependent factor parameter joint degrades performance viewpoint consistently improves unseen viewpoint synthetic video rendering viewpoint significantly improve raw parameter challenge unseen viewpoint vibe outperforms HMMR rgb estimation competitive depth kinect joint architecture comparison explore influence architectural improvement action recognition model 2D resnet temporal convolution versus ST gcn graph convolution SMPL parameter obtain vibe ST gcn improves 2D resnet performance synthetic training rendering remain superior unseen viewpoint significant boost rendering parameter despite source information difference potential architecture 3D resnet 2D resnet capacity estimation non frontal viewpoint challenge negatively affect performance affect 3D resnet estimation rendering advantage standard data augmentation technique image pixel apply unlike parameter augment importantly rendering advantage mixed video substantially improve performance explore architectural capacity action recognition model recent ST gcn model graph convolution vibe estimate ST gcn 2D resnet architecture although improvement ST gcn synthetic rendering significantly generalization unseen amount data ntu CVS training split sequence per action subset sequence per action scenario synth synth subset plot performance versus amount data scenario rgb consistent improvement complementary synthetic training unseen viewpoint effective synthetic data sequence per action viewpoint increase sequence data improvement synthetically augment exist sequence per action obtain synth accuracy without spending extra annotation effort diversity confirm improvement mainly due viewpoint variation synthetic data synth plot indicates viewpoint synthetic data improvement consistent therefore important augment viewpoint obtain improvement moreover synthetic training sequence per action performance synthetic training however viewpoint training benefit viewpoint diversity synthetic training ntu CVS split synthetic video obtain data sequence per action subset synthetic data synthetic performance viewpoint training performance obtain viewpoint combine diversity diversity synthetic training subset ntu CVS split clothing diversity important diversity significantly improve performance augmentation video additive joint rotation sixth dataset render training perform synthetic data render randomly sample clothing etc UESTC dataset protocol training viewpoint others plot individual performance rgb network matrix correspond training respectively obtain significant improvement due non uniform frame sample synthetic training UESTC dataset II protocol training odd viewpoint viewpoint vice versa split average rgb rgb fusion synth training consistently outperforms baseline diversity investigate diversified beneficial synthetic training attempt towards direction synthetic data mainly static image recently introduce interpolation distinct synthetic data training 3D estimation however contribution exist experimentally validate preserve action information therefore cannot generate unconstrained generate realistic challenge research scope augmentation increase diversity explain sect generate sequence interpolate additive parameter analysis synthetic data ntu CVS protocol baseline sequence per action render per viewpoint render sequence without augmentation obtain marginal improvement sequence per action significantly improves upper bound augmentation clothing appearance diversity important diversity generate sequence interpolation improves baseline moreover perturb joint rotation across video additive effective performance increase render without augmentation justify video frame frame hybrid version independently sample frame interpolate frame rendering qualitatively remain noisy reduce performance return comparison report standard protocol ntu completeness improve previous rgb due non uniform sample synthetic training additional cue extract rgb modality denote parenthesis perform par skeleton without kinect sensor comparison employ standard protocol UESTC ntu datasets performance report recently release UESTC dataset CV CV II protocol augment UESTC dataset vibe estimation outperform rgb joule 3D resnext margin 3D resnet architecture resnext architecture implementation obtain resnet architecture CV CV II contradicts report improvement attribute non uniform frame sample strategy therefore report uniform baseline significant performance boost later obtain mixture synthetic training data rgb input obtain improvement challenge CV protocol data rgb obtain improvement report odd split CV II protocol access multi training data synthetic data benefit baseline ntu contains interaction simulate UESTC dataset focus  movement convincingly demonstrate generalization capability efficient synthetic data generation video standard ntu split synthetic video generate HMMR estimation split achieve performance rgb modality comparison information training modality kinect depth skeleton training approach non uniform sample boost performance moderate gain synthetic data rgb training already sample video frame shot kinetics dataset illustration frame synthetically augment version viewpoint render synthetic static background computational efficiency augment training random video segmentation mask image shot training limit approach unconstrained video kinetics dataset video challenge 3D estimation fails due complex blur resolution occlusion scene exist cue action context simulate interaction bias towards clothing environment action assume alone noisy discriminative information action augment training video shot kinetics subset synthetically HMMR render viewpoint shot kinetics training data consists training sample per category video random baseline performance setup augment training video viewpoint synthetically render SMPL sequence extract data video blend random background mini kinetics training video obtain improvement training data linear layer resnext 3D cnn model pre mini kinetics pre feature extractor model linear layer feature fitting capacity model due limited shot training data pre model obtain rgb model 3D resnext architecture pre mini kinetics category resolution consecutive frame sample baseline pre feature slightly random rgb training linear layer synthetic data obtains performance training data obtains baseline performance obtain improvement synthetic data static background image LSUN dataset importance realistic noisy background generalization video conclusion effective methodology automatically augment action recognition datasets synthetic video explore importance variation synthetic data viewpoint analysis emphasizes diversify within action category obtain significant improvement action recognition unseen viewpoint shot training however approach limited performance 3D estimation fail clutter scene future direction action generative model sequence simulation contextual cue action recognition