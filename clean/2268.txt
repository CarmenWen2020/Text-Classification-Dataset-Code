winograd minimal filter algorithm widely convolutional neural network cnns reduce multiplication faster processing however effective convolution kernel stride suffers significantly increase FLOPs numerical accuracy kernel fails convolution stride extension ND convolution intensify numerical accuracy severely obstruct winograd minimal filter algorithm application video analysis propose novel decomposable winograd DWM ND convolution acceleration limitation winograd minimal filter algorithm convolution DWM decomposes kernel stride kernel stride apply winograd algorithm DWM reduce multiplication numerical accuracy enables exploration kernel stride dimension cnns performance accuracy potential cnns winograd algorithm propose DWM ND convolution speedup without affect numerical accuracy access auckland library introduction convolutional neural network cnns emerge premier algorithmic technique video related task development severely impede notorious massive computation recognize video frame cnns C3D gflops I3D gflops gflops convolution convolution important component cnns related video analysis reduce FLOPs ND convolution essential meaningful winograd algorithm widely reduce computation  apply winograd minimal filter algorithm reduce multiplication convolution kernel extension winograd algorithm propose version winograd algorithm accelerate convolution kernel unfortunately algorithm limitation cannot apply situation convolution kernel otherwise winograd algorithm introduce multiplication serious accuracy efficiency decline stride convolution principle otherwise winograd algorithm invalid dimension convolution increase ND become ND convolution kernel stride frequently cnns restriction severely limited application winograd algorithm tackle propose decomposable winograd DWM ND convolution ND convolution without accuracy loss DWM inspire observation winograd algorithm observation scalability winograd algorithm winograd algorithm convolution kernel convolution kernel irregular observation decomposability convolution convolution kernel equivalent convolution kernel convolution decompose convolution decomposability convolution apply acceleration algorithm friendly convolution kernel convolution kernel DWM propose observation core DWM lego kernel assemble kernel decomposability convolution apply winograd acceleration algorithm lego kernel calculate kernel scalability winograd algorithm DWM ND convolution without accuracy degradation convolution kernel stride specifically kernel DWM decomposes convolution kernel kernel apply ND winograd algorithm successfully kernel situation DWM reduce multiplication numerical accuracy convolution moreover stride situation DWM split kernel stride 3D convolution stride split convolution kernel stride corresponds split polynomial winograd minimal filter algorithm odd compute respectively guarantee equivalence theoretically finally derive winograd algorithm  apply winograd algorithm ND convolution kernel convolution dimension difference coefficient transformation matrix harm accuracy efficiency DWM kernel stride restriction traditional winograd algorithm extend  convolution DWM advantage efficiency numerical accuracy DWM efficiently reduce multiplication regular convolution kernel numerical accuracy negligible advantage DWM enables explore ND convolution kernel stride cnns performance accuracy video related task limited computational complexity application DWM inspire potential ND cnns video analysis network neural architecture NAS convolution kernel verify effectiveness efficiency DWM convolution kernel network multiple task related video DWM achieves acceleration convolution acceleration convolution numerical error numerical accuracy DWM numerical accuracy FP convolution traditional winograd algorithm accuracy kernel convolution furthermore estimate influence accuracy video architecture video super resolution video denoising optical estimation DWM inference convolution traditional winograd algorithm harm seriously additionally another comparison convolution reveals increase dimension intensify numerical traditional winograd algorithm influence DWM extends previous additional contribution theoretically extend DWM ND convolution theoretical analysis acceleration reveal relationship decomposition FLOPs practically implement DWM numerical accuracy convolution operation accuracy performance network video analysis organize sect discus winograd algorithm cnns video analysis introduce background information winograd algorithm principal DWM theoretical analysis illustrates experimental ND DWM sect conclusion related winograd algorithm winograd algorithm popular algorithm convolution acceleration kernel  apply winograd minimal filter algorithm reduce multiplication convolution propose novel winograd algorithm convolution gain speedup cudnn despite dimension extension researcher effort overcome defect winograd algorithm researcher attempt numerical accuracy kernel   investigate wider winograd algorithm dnns significantly improve float accuracy feature although approach accuracy FP suffers accuracy application decrease numerical error tile winograd algorithm polynomial however limitation cannot accuracy thoroughly meng extend winograd algorithm tile introduce complex winograd transformation complexity calculation complex impede scalability implementation researcher focus hardware implementation winograd algorithm due memory ceiling gpu hardware winograd algorithm cpu convolution operation achieves fold improvement throughput previous implementation besides mobile cpu acceleration winograd algorithm achieves performance improvement network imcol  optimization technique research mention focus scalability winograd algorithm extend winograd algorithm wider situation kernel stride dimension ND efficiently reduce multiplication calculation numerical accuracy stably cnns video analysis cnns widely video related task important effectively capture temporal information video cnns convolution widely adopt capture spatio temporal information respectively simonyan zisserman propose convolutional network video classification network fuse spatio temporal remains strategy spatio temporal fusion temporal dimension video dimension convolution researcher apply convolution video task successfully convolution capable exploit spatio temporal information simultaneously C3D convolution chose action recognition video detection version resnets obtain deeper cnns achieve superior performance action recognition I3D extend inception network version succeed action recognition however amount computation limit application convolution furthermore ignore efficiency consideration convolution kernel importance capture interaction recently slowfast network combine convolution resnets achieve performance action recognition adopt convolution kernel neural architecture NAS addition action recognition video detection VV video semantic segmentation optical estimation video equip C3D convolution  kernel apply convolution kernel video inpainting besides ND convolution dimension potential video related task propose cnns minkowski convolutional neural network video analysis  employ residual model evolution spatio temporal representation sparse 4D convolution 4D grid generate perception conclusion ND convolution convolution kernel potential advantage video analysis computation complexity restricts application accelerate ND convolution kernel significant benefit development research video related task preliminary winograd algorithm winograd algorithm equivalent multi dimensional fir filter convolution implement efficiently winograd minimal filter algorithm denote compute output tap fir filter convolution stride output kernel correspond convolution algorithm multiplication traditional winograd algorithm derive relationship polynomial multiplication convolution chinese remainder theorem crt fix algorithm contains fix transformation matrix situation filter input signal feature polynomial convolution obtain calculate coefficient polynomial multiplication apply crt transformation matrix convolution formulate   denotes convolution output denotes wise multiplication derive gradient neuron denote gradient denote winograd algorithm chain     gradient layer ND convolution nest    correspond        drawback winograd algorithm become accurate inefficient invalid situation kernel stride detail illustrate kernel benefit winograd algorithm simplicity transformation matrix apply winograd algorithm transformation matrix convolution output kernel  however kernel grows beyond transformation matrix become complicate kernel complicate transformation matrix convolution output kernel transformation matrix becomes something  transformation matrix winograd transformation consumption accurate stride another winograd algorithm cannot apply stride convolution stride convolution cannot equivalent polynomial multiplication derivation sect invalid winograd algorithm cannot apply convolution stride directly therefore although winograd algorithm implement convolution efficiently stride convolution ND decomposable winograd propose series technique decomposable winograd DWM apply winograd algorithm mention  kernel stride ND winograd algorithm firstly introduce ND version winograd algorithm convolution operation winograd transformation linear winograd algorithm  situation decompose combination dimension without loss generality suppose feature nth dimension ND feature split dimensional similarly suppose convolution kernel nth dimension ND convolution kernel split dimensional ND convolution decompose correspond convolution slide along nth dimension analogy ND convolution decompose combination transformation ND winograd algorithm consists iteration dimension data correspond transformation matrix described algorithm although ND winograd transformation introduce serial version calculation dimension independent coefficient transformation matrix fix therefore implementation usually parallel accelerate transformation graphic processing gpus accelerate hardware significantly reduce transformation ND winograd algorithm combination winograd algorithm accurate kernel situation invalid stride situation mention sect apply winograd algorithm convolution kernel transformation matrix easy loss accuracy seriously matrix multiplication ND winograd algorithm intensify numerical accuracy loss accuracy enlarge exponential accord stride situation derivation ND winograd algorithm invalid ND winograd algorithm fail convolution stride kernel denote convolution filter convolution filter derivation becomes split polynomial apply traditional winograd algorithm   apply separately multiplication reduce ND convolution split kernel apply winograd algorithm separately illustrate version kernel convolution splitting split convolution kernel kernel input signal feature slice redundant convolution convolution transformation apply correspond winograd transformation  algorithm calculation wise multiplication channel wise summation usually implement transpose batch matrix multiplication practise  inverse transformation algorithm intermediate spatial domain aggregation sum calculation equivalent convolution splitting stride convolution convolution splitting stride convolution stride convolution denotes convolution correspond image splitting stride convolution convolution splitting stride convolution stride convolution denotes convolution correspond image convolution winograd algorithm dot frame denote convolution procedure split procedure splitting transformation calculation  aggregation image split situation apply apply convolution split kernel convolution kernel illustrate split kernel advantage instead reduces amount multiplication transformation matrix therefore DWM implement multiplication winograd transformation efficiently winograd algorithm accuracy furthermore advantage dimension extension ND convolution accelerate algorithm without accuracy loss stride normal polynomial multiplication indicates stride convolution  barrier polynomial denote convolution stride convolution kernel split  input signal feature split similarly stride convolution polynomial multiplication apply winograd algorithm valid DWM reduces multiplication convolution stride successfully ND convolution nest convolution decompose kernel contains splitting transformation calculation  aggregation instance stride convolution convolution kernel input signal feature parity   stride convolution activation split stride convolution splitting stride convolution activation split stride convolution occasionally stride convolution kernel combine technique mention apply technique efficiently reduce multiplication ND convolution backpropagation apply winograd algorithm calculate gradient reduce multiplication employ chain derive        denotes winograd transformation  denotes winograd transformation denotes update iteration  normal data transformation instead winograd data transformation training winograd algorithm equivalent substitute normal convolution DWM technique mention derive correspond propagation denote DWM output technique aggregation derivative output backpropagation furthermore derive splitting backpropagation denotes DWM ND backpropagation derive easily algorithm nest decomposition strategy decomposition strategy convolution kernel image relationship theoretical FLOPs decomposition strategy convolution kernel calculate theoretical FLOPs assumption winograd transformation convolution kernel ignore simplicity transformation almost transformation coefficient manipulation exponent transformation computation resource reuse feature kernel convolution operation input transformation reuse filter transformation reuse batch largely reduces average transformation optimization implementation transformation largely optimize implementation technique pipeline theoretical analysis upper bound finally DWM inference purpose instead training transformation offline separately reduce transformation adopt assumption dimension convolution kernel fix output ignore batch channel calculate theoretical FLOPs convolution kernel DWM decomposes kernel kernel relationship theoretical FLOPs decomposition strategy  convolution kernel  decompose dimension theoretical FLOPs detail proof without loss generality assume dimension  FLOPs calculate decompose kernel computation ignore implementation detail decomposition strategy kernel constraint memory consume drawback DWM memory consume stride convolution theoretical analysis convolution operation memory strongly correlate parameter feature memory expansion DWM transformation convolution kernel split feature former winograd transformation expand convolution kernel convolution kernel expand winograd transformation furthermore DWM  equation derive transform ND convolution kernel notation parameter  denotes input channel  denotes filter expansion ratio numerator obviously denominator addend denotes split jth dimension split memory parameter dimension kernel expansion memory acceptable latter winograd algorithm apply feature feature split convolution stride feature split overlap overlap expansion memory DWM  splitting feature denote activation derive denotes batch denotes output ith dimension overlap traditional convolution expansion ratio equation however implementation substantial impact memory practically overlap convolution reuse easy DWM moreover non fuse implementation implement algorithm cuda kernel device memory fuse implementation implement algorithm kernel memory device memory comparison discussion exist model acceleration channel prune grain prune quantization tensor factorization compact network discus relationship DWM structure prune structure prune aim prune important structure loss minimization layer layer approach regularization combination approach orthogonal DWM DWM apply channel prune network DWM pure convolution acceleration algorithm anything computation stage convolution channel prune compress accelerate computation model DWM accelerate compute stage convolution grain prune aim reduce parameter amount calculation grain prune driven parameter important DWM utilizes decomposable convolution calculation perspective channel prune grain prune destroys structure inside kernel computation stage convolution cannot efficiently combine DWM researcher attempt quantization quantization aim accelerate compute network convert float arithmetic integer transformation DWM conflict quantization however skip transformation computation DWM quantize achieves speedup ratio therefore quantization combine DWM technique tensor factorization tensor factorization attempt decompose convolution kernel  singular decomposition svd rank expansion aim amount parameter gain acceleration driven decompose DWM equivalent accelerate risk accuracy loss DWM equivalent substitute convolution compact network goal compact network performance network craft structure neural architecture NAS DWM compatible architecture DWM equivalent substitute convolution moreover NAS potential kernel convolution video analysis  suitable training strategy convolution kernel important role video related task action recognition DWM accelerate ND convolution kernel benefit development research video related task winograd kernel apply winograd algorithm implement winograd algorithm FPGAs kernel pad kernel however pad non zero winograd transformation padding extra calculation DWM precisely convolution operation without pad avoids redundant float multiplication winograd transformation achieves acceleration without numerical accuracy loss additionally extends DWM  scalability DWM error mse convolution acceleration algorithm FP convolution error mse convolution acceleration algorithm FP convolution overall setting graphic processing gpu nvidia tesla implement DWM tensorflow pytorch platform implement DWM performs plug operator convenient data layer generate randomly standard normal distribution convenience network FLOPs analysis accuracy pytorch pytorch requirement source github model architecture gain github implementation mention later numerical accuracy layer estimate mse FP convolution operation setting assume application situation feature channel feature channel feature convolution random generate standard normal distribution numpy convolution data distribution consistent convolution layer network numerical accuracy implementation data format convolution DWM FP DWM FP numerical accuracy winograd FP winograd FP almost situation evident winograd algorithm serious numerical accuracy kernel increase kernel error FP winograd algorithm approach FP accuracy contrast DWM numerical error amplify kernel increase DWM FP FP meaning DWM FP apply convolution operation without accuracy winograd FP incurs overflow DWM FP FP winograd algorithm nan intermediate winograd transformation kernel becomes transformation matrix intermediate transformation become FP advantage DWM instead winograd algorithm accuracy influence network later analysis network situation DWM FP accurate FP convolution reasonable DWM consumes multiplication numerical accuracy convolution dimension multiplication DWM convolution algorithm convolution FP DWM numerical accuracy FP FLOPs estimation layer calculate FLOPs convolution kernel stride setting ignore influence easily implement shift FLOPs speedup winograd algorithm DWM DWM computation traditional winograd algorithm situation algorithm FLOPs kernel however kernel becomes FLOPs traditional winograd algorithm increase heavily FLOPs concentrate winograd transformation transformation becomes non sparsity matrix multiplication contrary speedup DWM steady transformation matrix DWM stride convolution traditional winograd algorithm cannot stride convolution due decomposition DWM speedup stride convolution stably advantage stably speedup almost convolution speedup accelerate algorithm convolution speedup accelerate algorithm convolution layer convolution operation naive implementation DWM setting nvprof footnote profile nvidia cudnn cudnn recommend algorithm fourier transform fft convolution algorithm cudnn fft tile naive implementation 3D DWM optimization technique kernel fusion pipeline accord conclusion DWM outperforms winograd algorithm kernel efficiency winograd algorithm amount computation transformation almost DWM fft computation fft complex transformation DWM fft fluctuates activation kernel reasonable accord fft algorithm activation kernel pad waste extra closer activation kernel pad contrast DWM redundant pad output cannot DWM faster cudnn situation around convolution totally DWM performs kernel becomes cudnn DWM becomes DWM kernel kernel DWM performs cudnn convolution acceleration mismatch theoretical analysis implementation DWM optimal cannot algorithm potential convolution algorithm integrate cudnn algorithm cudnn behave differently circumstance related factor kernel feature channel filter accelerate algorithm convolution nvprof accelerate algorithm convolution nvprof bottleneck ND illustrate weakness winograd algorithm ND convolution conduct numerical accuracy ND convolution error mse ND convolution kernel FLOPs theoretical speedup accelerate algorithm network setting kernel fix output convolution fix convolution convolution batch filter involve calculation output meaning mse moreover channel although increase enlarge mse imprecise algorithm numerical accuracy ND winograd algorithm random summarize winograd algorithm performs DWM dimension convolution convolution accuracy DWM winograd algorithm dimension increase numerical accuracy winograd algorithm decrease performance DWM stable phenomenon expectation sect increase dimension enlarge drawback winograd algorithm convolution analysis network analyse representative network analysis comparison FLOPs network accuracy performance setting FLOPs analysis convolution algorithm conv convolution algorithm winograd replace stride convolution winograd algorithm conv winograd replace stride convolution winograd algorithm acceleration DWM replace convolution DWM network accuracy performance convolution algorithm baseline convolution winograd replace stride convolution winograd algorithm DWM replace convolution DWM TOFlow baseline indicates baseline github baseline baseline reproduce FLOPs theoretical FLOPs analysis footnote winograd speedup network kernel DWM performs stable specifically kernel network spynet VV DWM performs winograd conv winograd network consists conv winograd efficient TOFlow peak signal ratio psnr structural similarity ssim  data format acceleration algorithm dataset vimeo task interpolation denoising super resolution DWM slightly winograd algorithm however potential risk winograd algorithm spynet spynet error   data format acceleration algorithm dataset mpi sintel training validation winograd FP nan DWM  baseline spynet mostly compose convolution convolution winograd transformation matrix consist complicate compute data distribution spynet FP overflow transformation intermediate overflow correspond convolution output winograd algorithm become nan spynet visualization optical estimate spynet mpi sintel dataset acceleration algorithm FP data format image accuracy average precision  slowfast standard  illustrate nan although winograd FP output baseline overflow DWM FP output almost baseline nearly difference winograd FP DWM FP FP FP robust representation precision winograd fail precision detail however risk winograd exists image precision calculation FP calculation winograd fail representative network slowfast performance  dataset something something charade winograd stride convolution slowfast convolution winograd already integrate cudnn pytorch totally difference baseline winograd slowfast DWM accuracy convolution indicates DWM accuracy loss slowfast something something default configuration num spatial  num spatial  footnote memory comparison convolution DWM  psnr ssim  video frame interpolation task data format acceleration algorithm dataset vimeo  spynet winograd FP nan situation DWM FP  however difference nan polynomial interpolation baseline nan totally broken unlike  network calculation nan overflow broadcast calculation layer discussion typical task video analysis video inpainting stride convolution network mainly convolution winograd algorithm accuracy winograd algorithm cannot apply stride convolution accuracy comparison winograd DWM meaningless overall layer illustrate potential risk winograd kernel convolution DWM aim typical network demonstration network representative network industrial cudnn DWM setting gpu nvidia tesla pytorch cuda cudnn strategy gpu cudnn DWM transformation computation winograd kernel implement matrix multiplication computation DWM implement compute unified device architecture cuda program nvidia gpus pytorch packaging outside comparison winograd DWM practical improvement winograd algorithm kernel convolution comparison cudnn DWM DWM potential exceed convolution library nvidia gpus accord conclude DWM outperforms winograd 3D ShapeNet 3D ShapeNet consists kernel 3D convolution without operation pool interpolation reflect pure convolution network DWM performs cudnn speedup ratio theoretical analysis gap mainly difference implement cudnn nvidia implement parallel thread execution ptx parallel thread execution virtual machine instruction architecture ISA assembly implementation convolution highly tune footnote DWM implement compute unified device architecture cuda implementation detail ptx optimization DWM gpus optimization DWM future researcher memory consume finally illustrate memory consume DWM setting gpu nvidia tesla pytorch implement DWM non fuse manner kernel conclude DWM memory convolution expectation theoretical analysis kernel memory ratio DWM conv analysis split significant memory usage convolution kernel ratio becomes decrease output increase convolution memory overall DWM sacrifice memory efficiency gain convolution acceleration conclusion propose novel DWM extend winograd minimal filter algorithm ND convolution winograd minimal filter algorithm widely reduce multiplication faster processing cnns cnns however winograd algorithm drawback suffer significantly increase FLOPs numerical accuracy kernel fail convolution stride effective convolution kernel stride numerical accuracy intensified dimension grows ND propose DWM limitation traditional winograd algorithm convolution kernel stride dimension DWM decomposes kernel stride kernel stride apply winograd DWM reduce multiplication numerical accuracy moreover coefficient transformation matrix guarantee extension DWM ND experimental propose DWM ND convolution speedup without accuracy loss DWM enable explore kernel stride dimension cnns performance accuracy improves potential discover cnns however discover drawback DWM memory decomposition transformation future another implementation optimization