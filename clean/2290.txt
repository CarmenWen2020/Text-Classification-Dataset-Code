transformer architecture fundamental computational linguistic dominate recurrent neural network implies drastic modal task vision researcher already tackle issue review critical milestone overall trend transformer architecture incorporate  modal task furthermore discus limitation speculate upon prospect imminent access auckland library introduction advent revolution facto standard modal task vision convolutional neural network cnn extract feature visual domain vgg resnet frequently cnn architecture employ recurrent neural network rnn memory lstm gate recurrent gru representation plethora variation exist specific extract feature blend embed fundamental pipeline almost invariably restrict combination cnn rnn brief timeline important model vision upper domain respectively affected approach modal domain image  landscape introduction transformer architecture transformer demonstrate capacity processing nlp achieve performance countless nlp task rapidly replace recurrent neural network application expand recognition domain variety transformer model exist bert particularly gain attention performance unique approach pre training adaptability downstream task GPT demonstrate pre training corpus tune model target task outperform conventional model margin GPT pre training extremely amount corpus parameter easily extends performance shot task without perform tune transformer architecture domain naturally application modal task involve vision  model demonstrate pre training objective bert extend modal obtains comparable performance model conventional cnn rnn approach model approach assumption pre training amount data superior performance modal domain important argument regard limitation prospect transformer modal model model image video tokenized serialize fundamentally rely cnn model extract feature visual token remains  transformer fundamentally superior embeddings performance simply due amount computation data issue computational efficiency furthermore blending transformer architecture generative model challenge explore brief timeline important model vision domain respectively influence approach modal task important milestone trend project transformer architecture vision representation modal task performance vision domain comparable cnns sect attempt review representative transformer modal model emphasis pre training scheme characteristic various model along difference investigate model novel promising direction transformer vision representation image generation text goal acquire date insight various aspect modal prospect influence organize review conventional  modal task recent benchmark task along frequently evaluation metric sect review elementary transformer architecture variation mostly restrict domain around bert sect introduce inspect recent model modal task employ transformer architecture sect focus architectural modification pre training scheme introduce sect mostly employ transformer architecture acquire linguistic representation sect introduces examine vision transformer architecture possibility replace convolutional neural network discus prospect transformer architecture modal task finally conclude sect summarize important limitation future preliminary  task sect briefly preliminary understand topic implication review representative task  domain namely image caption visual commonly approach important task become benchmark task transformer modal task evaluation metric task classical task prior era model frequently tackle  modal task template model rank retrieval model advent however mainstream paradigm tackle modal task rapidly shift towards approach incorporate convolutional recurrent neural network image caption visual VQA conventionally representative modal task involve vision image caption model image caption image learns generate descriptive caption unseen image rely straightforward combination cnn lstm advanced model dense localization semantic attention incorporate generative adversarial network gans image caption gans predict caption VQA task image model variation VQA task exist VQA benchmark task usually refer MS coco roughly source VQA approach categorize primary component image representation text representation embed scheme attention mechanism image representation mostly rely cnns employ detection model text representation rely rnn model  classical model wordvec glove modal embed scheme concatenation wise addition multiplication sophisticated scheme propose compact bilinear pool rank bilinear pool modal tucker fusion various attention mechanism propose demonstrate effectiveness footnote image caption VQA extend modal domain involve video video caption video QA respectively image caption VQA model video domain heavily rely cnns rnns video benchmark task image caption visual exemplary  modal task important intrigue variation emerge novel variation along VQA task consolidated benchmark task pre modal task interestingly image caption task despite representative  task rarely tackle recent transformer model conjecture attributable image caption task employ array evaluation metric intuitive performance along accuracy task sect however actively examine video likely remain important indicator modal model performance frequently benchmark task visual commonsense  extends VQA refer agent action model chose notably attempt accomplish logical inference subtasks difficulty exist within task model predicts model predicts intention QA model predicts AR  dataset contains roughly multiple choice accompany bound semantic mask agent candidate refers visual  advanced understand vision judging statement regard juxtaposition image dataset consist roughly statement image unique task image brings challenge embed image embed separately decision affect embed text  frequently benchmark task goal identify image refer linguistic statement dataset consist refer expression  refer expression caption image retrieval embody visual recognition noteworthy task involve vision finally image generation text important largely unexplored axis  task mostly limited target domain promising sect sect benchmark task consolidated video domain although video caption task  ActivityNet frequently evaluation metric evaluation metric  task categorize classification task generation task image caption classification task mostly rely straightforward accuracy generation task usually employ multiple metric evaluation bleu originally propose machine translation frequently evaluation metric computes portion gram candidate caption truth bleu emphasis precision rouge recall orient approach counting occurrence gram bleu rouge meteor evaluates gram synonym stem wordnet CIDEr specifically image caption task consensus generate image caption reference caption assign gram specifically image gram frequently image spice popular metric image caption task tuples extract semantic parse graph metric widely video caption task benchmark task apart caption evaluate performance straightforward accuracy although accuracy define slightly differently task VQA defines accuracy respect truth available  proposes supplement accuracy propose additional metric consistency validity plausibility straightforward accuracy metric risk loss accountability subtle aspect performance model ability yield usually report frequently task aggregate evaluation usually retain mutual agreement extent accuracy recall image retrieval task preliminary II transformer model transformer architecture multi attention mechanism introduce bert transformer model become crucial component recent surge transformer modal model emphasis unique pre training objective introduce pre training objective model bert reference transformer architecture propose transformer architecture demonstrate outperforms dominant rnn cnn approach sequential transduction task machine translation transformer comprises encoder decoder consist series attention module differently cnn rnn adopts attention operation model instead convolution memory gate obtain suitable handle sequential data later attention input vector transform vector query output compute sum assign accord similarity query correspond matrix query extract input vector respectively attention formulate attention softmax  dimensionality boost flexibility attention transformer adopt multi attention mechanism instead attention multi attention attention conduct parallel output integrate concatenation linear projection obtain output vector equation multihead concat    attention     projection matrix integrate output attention query attention compute linear projection projection matrix    jointly optimize  trainable parameter via training attention input vector affect output vector model prediction inappropriate nlp task avoid information input encode input embed token fed transformer model specifically information encode PE sin  PE  target token input sequence  dimensionality embed token index respectively positional embed embed propose relative sinusoidal positional embed outperforms absolute positional embed longer distance modify attention matrix  propose positional embed bias attention distinctive merit transformer previous model model dependency flexibility parallel compute instance rnns generally preserve context distance token becomes longer execute sequential manner becomes non trivial parallel distance dependency challenge cnns proportionally layer attention mechanism model dependency distance within pre define sequence sequential input highly suitable parallel distribute computation bert variation transformer exist bert bidirectional encoder representation transformer particularly important topic architecture employ modal model pre training task extend account modal transformer bert applies layer normalization multi attention residual connection applies propagation unique choice non linear activation function namely gaussian error linear  bert unique choice pre training task namely masked model prediction masked model token randomly masked probability model predict masked token prediction actual succeed random probability model task binary classification actual succeed objective masked model aim token dependency motivation task inter relationship although prediction crucial model performance objective along bidirectional architecture characterize differentiate bert transformer model reference highlight notable pre training objective propose model XLNet attempt overcome drawback masked model bert absence masked token data inability model joint probability autoregressive model specifically propose permutation model model maximizes likelihood input sequence permutation factorization thereby bidirectional context retain merit autoregressive model MT dnn performs multi task leverage supervise data multiple related task pairwise text classification relevance rank model pre training prevents model overfitting specific task complementary model pre training proposes pre training objective emphasis capture granularity semantics query document namely inverse cloze task capture local semantic context selection capture global context within document wiki link prediction capture inter article context propose pre training objective perform significantly masked model analysis modal embeddings modal model employ transformer bert architecture review pre training objective directly inspire bert albeit notable variation speculate relate model performance model attempt construct network architecture inter modal dependency along distinct modality mutual compatibility pre training task primary argument laid transformer model bert pre training successful performance evidently choice pre training task essential obtain quality model modal model employ transformer architecture generally presumption emphasis pre training objective challenge consequently replicate pre training modal sect bert unique pre training task masked model classification solely inevitably adjustment extend modal model adopt pre training task modify model propose additional pre training task specifically modal masked model pre training task bert linguistic input token modal model almost without exception notable variation  performs masked model replace masked continuous random model BT extends task training model image another pre task bert prediction mostly convert binary classification input image semantically perform modal model exception VL bert  explicitly opt perform pre training task  unique variation explicitly involve negative challenge pre training task modal embed model implement masked model task visual input cannot naively extend vision domain straightforward manner due non sequential vision model BT  simply opt perform extend masked model task visual input model propose novel apply masked model visual token  handle challenge propose mask image extract faster cnn model predict distribution distribution output faster cnn truth VL bert  VL notable perform masked visual model prediction VL bert proposes masked roi classification linguistic clue randomly masked model predict category masked roi solely rely linguistic clue  drinking bottle RoIs obtain via cnn derive concern nearly impossible identify masked VL bert opts perform image task unique setup intend modal dependency model rely prediction distribution beneficial incorporate feature regression masked visual model task  masked classification label masked roi classify visual input linguistic input perform roi feature regression loss  report combine prediction feature prediction improves performance  performs prediction feature regression propose task masked visual model perform prediction KL divergence model propose novel pre training task category frequently depends target downstream task   propose image task pre training whereas  employ pixel random sample proposes contrastive pre training encourage model representation sequence semantics sequence correspond corrupt sequence model rely faster cnn extraction  employ extraction module inspire  attempt enhance visual feature perform image classification detection dataset modal representation subsequently tune caption tag caption tag obtain exist model comparison performance modal model various task pre training task perform various modal model report model report respective comparison frequently datasets pre training  model visualization representative modal model dataset pre training performance VQA task pre training objective employ model respective model corresponds model approximate implementation detail report respective VM VM refer distinct task masked visual model image image analysis performance performance various model representative modal task model datasets model comparison performance necessarily imply superiority model others pre training task perform model rough category pre training task setting model datasets pre training implementation detail visualizes representative model dataset pre training performance VQA task pre training objective perform respective model remains  attribute aspect respective model performance model performance characteristic pre relatively amount data model pre training objective observation generic consistent recent trend pre training model parameter dataset affirm GPT frequently datasets image video along description content annotation although generalize model described easy overfitting observation respect dataset pre training objective likely reference datasets image affirm indispensable role played pre training dataset model relatively attain performance pre dataset mini VLM pre training dataset overcome  label obtain via weak web supervision   pre conceptual caption  caption  supervise demonstrates fairly performance throughout task however setting pre training manually annotate datasets pre training datasets web supervision particularly dataset obtain desirable performance inevitable presence web supervision efficiency annotation dataset illustrate  variation lamp MS coco display reliable performance  conceptual caption although conceptual caption MS coco however highlight equivalent alternatively pre training web supervision demonstrate comparable performance annotation sufficient amount data model   sect tendency video model  pre youtube video annotation obtain youtube video annotation annotate  evaluation model directly pre   infer pre training dataset task target task boost performance target task  oscar  dataset image QA task demonstrate performance VQA task similarly correlation pre training objective performance specific task beneficial perform pre training task target downstream task   notable propose unique pre training objective image QA indeed achieve performance VQA task  performs additional pre training objective image caption task demonstrate reliable performance image caption task furthermore video model pre  annotation performance transformer modal model video caption  ActivityNet video extend vision domain modal task image video naturally brings challenge truly reliable supervision annotation frame necessitates prohibitive amount manual labour exist video datasets youtube sport rely user attach tag noisy label supervise become frequently approach video representation transformer modal model tackle video relatively newly emerge research topic displayed approach task orient principle model adopt approach employ model image bert inspire pre training objective model opt diverge align video supervision extract correspond audio  raw video frequent semantic video align speaker video speaker cooking video probability visual linguistic semantics temporally align exploit examine video caption generation frame prediction  extend bert pre training objective video domain obtain visual token hierarchical vector quantization video feature masked model masked visual token model classification extend alignment classification visual linguistic cooking video however alignment noisy concatenate subsampling rate video token model robust variation video similarly  cbt employ slide approach extract visual token  network visual linguistic token concatenate shallow layer modal transformer mutual information compute pre training objective masked model masked visual token model employ  dataset crawl video youtube  unlike  feature cluster vector quantization task estimate cluster masked feature belongs cbt attempt directly regress masked feature model rely densely sample visual token  notably demonstrates sparse visual token sufficient video representation novel pre training objective specific video domain  proposes masked action classification objective predict action label linguistic feature whereas hero proposes frame model   incorporate generative model approach propose regard modality translation model explicitly employ bert inspire approach proposes multi modal transformer calibrate video retrieval task directly extend pre training objective bert rely bidirectional max margin rank enforce similarity video caption notably extract video feature combination pre model domain differentiates model generally employ pre model modality similarly cooperative hierarchical transformer  proposes modal cycle consistency loss enforce semantic alignment alignment loss simply extend pre training objective bert proposes masked transformer dense video caption task simply caption decoder input visual encoder proposal decoder without explicit pre training objective modal limited transformer model video tend specific task domain approach model employ approach modal model image capture definite model remains explore summarizes model performance video caption task frequently task video modal domain network architecture comparison modal transformer refer embed visual token respectively output layer image network architecture modal embed transformer roughly category model transformer modality specific model input transformer inter modal  representative model propose attention transformer mechanism input modality input transformer another modality input transformer vision vice versa query modality correspond modality transformer embed feature modality modality tokenize vision extract image faster cnn resnet backbone employ dimensional spatial location vector  approach embed modality encode transformer separately modality encoder apply query vector  modality context vector  another modality input respective modality encoders   exchange query equivalently modality  notably performs modal upper semantic alignment   mostly  incorporates scene graph approach input text parse node relation attribute text mention scene graph representation improve semantic alignment vision author hint possibility extend modal task graph neural network modal fairly intuitive conventional transformer extend straightforward manner without architectural modification concatenate input  VL   notable modal model notably nearly model perform fusion vision embeddings concatenate prior fed modal transformer BT specifically VQA  task modifies dual encoder perform fusion text bound fusion feature entire image report fusion outperforms fusion  aim building modal model unlike model employ cnn faster cnn extract visual feature propose stage efficient feature extractor tee consist EfficientNet compact bert fusion model visual feature extraction consists EfficientNet bidirectional feature pyramid network  non maximum suppression  unique propose triplet input modal consist visual feature tokenized tokenized motivation inputting label explicitly enforce dependency correspond text approach employ oscar input tag along token feature apart modal mechanism input format positional embed another source variation model  VL extract image faster cnn along vector notably embed image random permutation  embed visual token correspond input token whenever alignment image input token available VL bert proposes format input consists embeddings namely token embed visual feature embed embed sequence embed notable visual feature embed combination visual appearance feature correspond output fully concatenate feature cnn image visual geometry embed vector coordinate linguistic token entire image visual feature embed sequence embed image token randomize video domain  proposes tangle transformer query modality  employ encoder video encoding combine along dimension sequence  relies modality architecture partially employ modal attention transformer summary observation model model explicitly attention scheme exchange input attention model explicitly attention scheme simply rely pre training objective modal dependency image tokenized without exception although entire image replace model embed mostly accompany dimensional spatial vector coordinate information feature extract pre detection model classification model model  employ EfficientNet backbone detection model attach embed image source variation model model employ coordinate model simply random permutation embed visual token visual representation transformer tackle modal task transformer architecture primarily apply linguistic representation scheme embed visual representation mostly tokenizing visual representation tokenization visual representation rely pre convolutional neural network  relies faster cnn extract image token however recent research convolution safely replace transformer architecture arguably imply fundamentally obtain visual representation explicitly modal task critical implication prospect modal task sect briefly introduce recent important vision representation transformer proposes vision transformer  suggests pure transformer achieve comparable performance image classification task closely transformer architecture split image patch sequence linear embeddings patch input transformer 2D image sequence flatten patch spirit masked model bert perform masked patch prediction model predict patch ablation variation dataset pre training dataset  performance implicate domain training transformer model massively amount image data model outstanding performance pyramid vision transformer expands upon vision transformer demonstrates convolution model extend wider computer vision task detection semantic segmentation instance segmentation  video handle without convolution  proposes another direction applicability transformer vision proposes sequence transformer pixel prediction reshape pixel 1D sequence perform pre training objective auto regressive pixel prediction masked pixel prediction model architecture GPT outperform resnet cifar pixel prediction task implies promising research direction image generation image enhancement transformer image transformer image processing transformer ipt demonstrate transformer model outperform conventional model various task image super resolution introduce transformer architecture already tackle computer vision task mostly conjunction cnns  employ transformer encoder decoder cnn backbone detection whereas max deeplab performs panoptic segmentation dual architecture cnn mask transformer detail application transformer computer vision task refer reader survey prospect concern transformer model massively training model inevitably accompany prohibitive financial afford training procedure  estimate billion parameter model model billion parameter aggravate concern regard moreover tendency proven vision modal task sect training model image data noticeable performance boost nearly modal model ablation benefit immensely training model rapid inflation training concern affordability limited handful corporation potential alternative community wise discussion unambiguous direction develop efficient network architecture research effort focus lessen computational burden transformer model subsequently modality domain  reduces parameter bert retain nearly identical performance knowledge distillation integrate loss function  similarly proposes leverage knowledge distillation report comparable performance model vision domain data efficient image transformer  built upon  introduce introduction distillation token utilizes teacher strategy attention demonstrates performance comparable cnns imagenet reformer attempt aim improve efficiency transformer approach goal locality sensitive hash reversible residual layer reduce complexity exponential logarithmic sparse transformer switch transformer strive diminish computational complexity sparse attention mechanism respective domain transformer architecture anticipation towards subsequent transformer model modal task sect transformer architecture effective vision convolutional neural network involve speculate modal model solely transformer architecture vision representation acquire solely transformer without convolution recurrent neural network imminent possibility important architectural integrity likely development hardware optimize transformer architecture recent transformer superiority cnns computational efficiency  memory efficient resnets memory efficiency couple distillation model performance issue described alleviate substantial extent architectural transition wider likely   propose demonstrate convolution model transformer modality accomplish comparable performance faster  raw video audio text convolution transformer model pre training objective evolve emergence transformer exclusive model modal model transformer architecture modality future predict likely important role transition described sect  demonstrate transformer employ generate image extend accomplishment modal domain encounter task image synthesis text description image generation text aspiration machine community limited specific domain however  GPT model demonstrate image synthesis text variety realistic image illustration encompass geographic temporal knowledge along text token tokenized image discrete latent code via discrete VAE  demonstrates image style manipulate text input leverage contrastive image pre training  image generation pipeline particularly intrigue modal employ transformer architecture tackle task image video input along text text label output image synthesis text task reverse direction along combination exist generative model highly promising future research direction conclusion review recent trend transformer model modal task vision emphasis pre training scheme network architecture domain performance modal model strongly aspect model dataset pre training objective reinforce recent trend model data performance witness GPT pre training datasets annotate manually efficient dataset pre training datasets annotation obtain weak supervision via crawl although discrepancy overcome sufficient amount data pre training directly datasets target task achieve performance target task introduce apply transformer architecture vision representation potential prospect regard transformer modal model potential transformer exclusive model data efficient model model generative task topic relatively stage serf useful reference summarizes phase topic potentially increasingly important