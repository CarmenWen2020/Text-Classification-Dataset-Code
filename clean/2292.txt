introduces qmdp net neural network architecture planning partial observability qmdp net combine strength model model planning recurrent policy network policy parameterized task model planning algorithm solves model embed structure planning network architecture qmdp net fully differentiable allows training  task generalize parameterized task transfer task beyond preliminary qmdp net performance robotic task simulation interestingly qmdp net encodes qmdp algorithm sometimes outperforms qmdp algorithm introduction decision uncertainty fundamental importance computationally partial observability partially observable agent cannot exactly observation optimal action integrate information action observation model approach formulate partially observable markov decision POMDP  exactly computationally intractable approximate POMDP algorithm dramatic progress  however manually construct POMDP model data remains model approach directly optimal within policy restrict policy difficulty data computational efficiency parameterized policy effectiveness policy constrain priori choice neural network unprecedented domain distinct approach decision uncertainty network DQN consists convolutional neural network cnn fully layer successfully tackle atari complex visual input replace  fully layer DQN recurrent lstm layer allows partial  however planning approach fails exploit underlie sequential decision introduce qmdp net neural network architecture planning partial observability qmdp net combine strength model model planning qmdp net recurrent policy network policy POMDP model algorithm solves model embed structure planning network conference neural information processing beach CA usa robot navigate partially observable grid robot belief initial initial local observation ambiguous insufficient policy expert demonstration randomly generate environment generalizes environment transfer environment lidar architecture specifically network qmdp approximate POMDP algorithm sophisticated POMDP algorithm qmdp net consists network module bayesian filter integrates agent action observation belief probabilistic estimate agent qmdp algorithm chooses action belief module differentiable entire network qmdp net expert demonstration randomly generate environment policy generalizes environment transfer complex environment preliminary qmdp net outperform network architecture robotic task simulation successfully  hallway domain interestingly qmdp net encodes qmdp algorithm sometimes outperform qmdp algorithm background planning uncertainty POMDP formally define tuple action observation respectively transition function defines probability agent action observation function defines probability observation action reward function defines immediate reward action partially observable agent maintains belief probability distribution agent initial belief update belief bayesian filter normalize constant belief recursively integrates information entire decision POMDP planning seek policy maximizes discount reward action policy chooses discount factor related policy decision partially observable domain approach model model planning alternative policy directly model usually policy exploit model information effective generalization propose approach combine model model embed model planning algorithm recurrent neural network rnn policy training network rnns earlier partially observable domain  extend DQN convolutional neural network cnn replace convolutional fully layer recurrent lstm layer similarly navigate partially observable maze policy generalizes goal fix environment instead generic lstm approach embeds algorithmic structure specific sequential decision network architecture aim policy generalizes environment embed specific computation structure neural network architecture gain attention recently implement iteration neural network iteration network vin markov decision mdps fully observable domain agent filter address related integral optimal allows continuous action neither address issue partial observability drastically increase computational complexity decision   developed trainable bayesian filter probabilistic estimation introduce  estimation markov reward decision planning address planning partial observability former focus model policy model fix environment generalize latter proposes network approach robot navigation unknown environment focus mapping network architecture contains hierarchical extension vin planning partial observability planning qmdp net extends prior network architecture MDP planning bayesian filter imposes POMDP model computation structure prior entire network architecture planning partial observability overview policy enables agent effectively diverse partially observable stochastic environment robot navigation domain environment correspond building robot agent location directly estimate noisy reading laser finder access building model dynamic sensor building significantly layout underlie effective navigation building training robot building robot building navigate effectively specify goal formally agent learns policy parameterized task partially observable stochastic environment parameter parameter capture variety task characteristic within environment goal agent robot navigation encodes environment goal belief robot initial assume task action observation agent prior model dynamic sensor task objective training task subset agent learns policy solves issue representation policy without specific parametrization introduce qmdp net recurrent policy network qmdp net policy parameterized POMDP model approximate POMDP algorithm embed differentiable neural network embed model allows policy generalize effectively embed algorithm allows entire network model compensates limitation approximate algorithm embed POMDP model action observation manually task transition observation reward function data perfect policy qmdp planner bayesian filter qmdp planner qmdp planner qmdp planner bayesian filter bayesian filter bayesian filter qmdp net architecture policy action observation action qmdp net rnn imposes structure prior sequential decision partial observability embeds bayesian filter qmdp algorithm network hidden rnn encodes belief POMDP planning qmdp net unfolded underlie model dynamic observation reward task embed POMDP algorithm agent alternative model mitigate approximate algorithm limitation obtain overall policy qmdp net embeds POMDP model network architecture aim policy model qmdp net consists module encodes bayesian filter performs estimation integrate agent action observation belief encodes qmdp approximate POMDP planner qmdp chooses agent action correspond fully observable markov decision MDP perform ahead MDP belief evaluate propose network architecture imitation expert trajectory randomly chosen task parameter parameter expert trajectory consist sequence demonstrate action observation agent access truth belief along trajectory training define loss entropy predict demonstrate action sequence RMSProp training appendix detail implementation tensorflow available online http github com  qmdp net qmdp net assume task parameterized underlie action observation qmdp net policy parameter qmdp net recurrent policy network input qmdp net action observation task parameter output action qmdp net encodes parameterized POMDP model qmdp algorithm selects action model approximately manually prior knowledge specifically prior knowledge model action observation abstraction counterpart task robot navigation robot continuous grid finite reduce representational computational complexity transition function observation function reward function data training assume task simplify network architecture training feasible qmdp net encodes model associate algorithm fully differentiable neural network embed algorithm neural network linear operation matrix multiplication summation convolutional layer maximum operation max pool layer detail qmdp net architecture consists module filter planner bayesian filter module qmdp planner module qmdp net consists module bayesian filter module incorporates action observation belief qmdp planner module selects action accord belief filter module filter module implement bayesian filter belief action observation belief belief update account action observation observation action normalization factor implement bayesian filter transform layer neural network discussion grid navigation task agent observes access task parameter encodes obstacle goal belief initial task belief tensor implement convolutional layer convolutional filter denote convolutional layer kernel encode transition function output convolutional layer tensor encodes update belief action belief correspond action agent directly index cannot index instead index encode action action function index vector distribution action along appropriate dimension incorporates observation observation model tensor probability observation grid navigation task observation obstacle location task parameter function neural network mapping cnn encodes observation probability observation observation probability observation cannot index directly instead index encode observation observation function mapping index vector distribution finally obtain update belief wise normalize initial belief task encode initialize belief qmdp net additional encode function planner module qmdp planner performs iteration core compute iteratively apply bellman update maxa action belief implement iteration convolutional max pool layer grid navigation task tensor express max pool layer input output convolution convolutional filter addition operation reward tensor denote convolutional layer kernel encode transition function similarly filter reward navigation task goal obstacle reward task parameter cnn implement iteration bellman update stack layer iteration  approximate action belief obtain action  finally output action policy function mapping action output qmdp net naturally extends dimensional discrete maze navigation task dimensional convolution restrict discrete handle continuous task simultaneously discrete planning action observation objective understand benefit structure prior neural network policy alternative network architecture gradually relax structure prior evaluate architecture simulated robot navigation manipulation task task simpler atari visual perception challenge sophisticated handle partial observability future reward robot unknown successful policy information improve estimation partial noisy observation information gathering reward distance future experimental setup qmdp net related alternative architecture qmdp net variant  qmdp net relaxes constraint planning module  transition function cnn layer lstm qmdp net replaces filter module generic lstm module architecture embed POMDP structure prior cnn lstm cnn lstm  architecture propose reinforcement partially observability rnn recurrent neural network fully hidden layer rnn contains structure specific planning partial observability experimental domain contains parameterized task parameter encode environment goal belief robot initial policy generate random environment goal initial belief construct truth POMDP model generate data apply qmdp algorithm qmdp algorithm successfully goal retain sequence action observation expert trajectory correspond environment goal initial belief important truth  generate expert trajectory qmdp net comparison network expert trajectory domain perform training parameter layer hidden network architecture briefly experimental domain appendix implementation detail grid navigation robot navigates unknown building goal robot uncertain location equip lidar detects obstacle neighborhood uncertain robot fail execute desire action possibly slippage lidar false reading implement simplify version task discrete grid task parameter image channel channel encodes obstacle environment channel encodes goal channel encodes belief robot initial robot grid action canonical direction lidar observation compress binary correspond obstacle deterministic stochastic variant domain stochastic variant action observation uncertainty robot fails execute specify action probability observation faulty probability independently direction policy expert trajectory random environment trajectory environment random environment highly ambiguous observation maze observation despite robot maze navigation differential robot navigates maze domain grid navigation significant challenge robot contains orientation robot cannot freely kinematic constraint action observation relative robot orientation increase ambiguity localize robot initial highly uncertain finally successful trajectory maze typically longer randomly generate grid expert trajectory randomly generate maze grasp simplify grasp training grasp robot gripper novel noisy sensor tip gripper perform compliant maintain contact grasp  maybe database however relative relies sensor localize implement simplify variant task model POMDP task parameter image channel encode grasp belief gripper initial gripper action canonical direction unless environment boundary binary sensor tip distinct observation expert demonstration randomly sample previously unseen random qmdp net component task task appropriate neural network representation specifically representation function opportunity incorporate domain knowledge principled local spatially invariant connectivity structure convolution kernel grid navigation maze navigation robot orientation task grasp task cnn component kernel task enforce probability distribution softmax sigmoid activation convolutional kernel respectively finally fully component encode function softmax layer identity function adjust amount planning qmdp net allows propagate information without affect parameter however deeper network computationally expensive evaluate transfer policy environment increase execute policy representation task parameter isomorphic chosen architecture restrict rely convolution kernel direction future discussion report additional report appendix domain report task rate average task completion completion meaningful rate qmdp net successfully learns policy generalize environment evaluate environment qmdp net rate faster completion alternative nearly domain understand performance difference specifically architecture fix environment navigation initial goal across task instance environment remains qmdp net alternative comparable performance rnn performs fix environment network feature optimal policy directly towards goal contrast qmdp net learns model planning generate optimal policy arbitrary environment POMDP structure prior improve performance complex policy across gradually relax POMDP structure prior network architecture structure prior weaken overall performance however prior sometimes constrain network degrade performance filter planner policy underlie transition dynamic allows approximation flexibility shed issue visualize POMDP model appendix qmdp net learns incorrect useful model planning partial observability intractable rely approximation algorithm qmdp net encodes POMDP model qmdp approximate POMDP algorithm solves model network opportunity incorrect useful model compensates limitation approximation algorithm reward reinforcement indeed qmdp net achieves rate qmdp nearly task qmdp net performs hallway domain expose weakness qmdp myopic planning horizon planning algorithm qmdp net qmdp qmdp net learns effective model expert demonstration qmdp generates expert data training expert data successful qmdp demonstration successful unsuccessful qmdp demonstration training qmdp net perform qmdp qmdp net policy environment transfer directly environment policy environment scratch scalable approach performance comparison qmdp net alternative architecture recurrent policy network SR rate percentage average task completion denote deterministic stochastic variant domain environment qmdp qmdp net  lstm cnn rnn qmdp net qmdp net lstm domain SR SR SR SR SR SR grid grid grid grid maze maze hallway grasp intel lab  fix grid policy environment transfer environment transfer qmdp net policy simply expand planning module recurrent layer specifically policy randomly generate grid apply policy environment intel lab  lidar robotics data repository environment additional setting building available appendix conclusion qmdp net recurrent policy network embeds POMDP structure prior planning partial observability generic neural network mapping input output qmdp net learns model planning task network fully differentiable allows training simulated robotic task qmdp net policy successfully generalize environment transfer environment POMDP structure prior training substantially improve performance policy interestingly qmdp net encodes qmdp algorithm planning qmdp net policy sometimes outperform qmdp direction future exploration limitation approach representation iteration algorithm qmdp iterates entire suffer curse dimensionality alleviate difficulty qmdp net training abstract representation planning incorporate hierarchical planning qmdp approximation reduce computational complexity explore possibility embed sophisticated POMDP algorithm network architecture algorithm planning performance algorithmic sophistication increase difficulty finally restrict imitation extend reinforcement earlier indeed promising