robust optimization goal optimize objective function develop reduction robust improper optimization stochastic optimization oracle return approximate distribution objective compute distribution approximate derandomizing NP statistical task apply robust neural network training submodular optimization evaluate approach experimentally corrupt classification robust influence maximization network introduction task uncertainty loss aim optimize classification task recognition perform various distortion environment recognize photo classifier handle rotation background environment resolution image likely encounter noisy pixelation artifact instead training classifier scenario seek optimize performance corruption combination thereof available trainer generally goal minimax optimizes function individual function optimize effectively perform individual objective non convex hence approximate stochastic optimization optimize distribution loss function approximate stochastic optimization straightforward loss function commonly convex combination approximately optimal stochastic yield approximately optimal robust develop reduction robust optimization stochastic optimization approximate oracle stochastic optimization implement approximate robust optimization extension illustrate effectiveness application approximate stochastic oracle distribution potentially nonconvex loss function approximate robust optimization  outcome improper outside non convex interpret compute distribution relaxation improper NP achieve robust optimization respect outcome stochastic optimization exactly polynomially loss function complement statistical scenario loss convex predict dependent variable deterministic performance guarantee conference neural information processing beach CA usa technical overview approach employ execution regret dynamic zero sum played learner equip approximate stochastic oracle adversary aim distribution loss function maximizes learner loss converges approximately robust learner adversary upon approximate minimax convergence additive regret converges rate dynamic application illustrate reduction statistical via neural network arbitrary training reduction generates net optimizes robustly loss function evaluate experimentally recognition task loss function correspond corruption model available learner verify experimentally approach significantly outperforms various baseline optimize average performance optimize loss separately apply reduction influence maximization goal maximize concave function independent cascade model influence non convex subset vertex network previous robust influence maximization directly focus function chosen within establish hardness approximation comparison agnostic function achieves approximation return distribution evaluate synthetic datasets goal robustly optimize suite random influence instantiation verify experimentally approach significantly outperforms baseline related recently robust optimization machine continuous optimization closest shalev shwartz    robust optimization convex loss function difference assume loss function non convex access stochastic oracle hence proof technique apply generalizes distributional oracle approximation optimal theorem applies realizable statistical oracle mistake bound application frame optimization access approximate oracle approximation multiplicative respect optimal submodular optimization robust optimization closest  slightly objective  apply influence extend submodular function finally unlike recent non convex optimization goal optimize non convex function abstract non convex guarantee via approximate stochastic oracle robust optimization approximate stochastic oracle model optimization robust objective uncertainty optimize finite loss function function intuitively goal achieves loss loss function maxi loss minimax optimum min min max goal approximate robust optimization extension infinite loss function version extend naturally goal maximize minimum reward function oracle framework ben approximation multiplicative additive algorithm oracle efficient improper robust optimization input objective apx stochastic oracle parameter exp output uniform distribution distribution maxi loss drawn weaker version robust approximation improper robust optimization distribution reduction approximate stochastic oracle approximately minimizes distribution loss function definition approximate stochastic oracle distribution approximate stochastic oracle computes EL min EL improper robust optimization oracle access approximate stochastic oracle efficiently implement improper approximate robust optimization vanish additive loss theorem access approximate stochastic oracle algorithm  computes distribution define uniform distribution max moreover distribution compute algorithm satisfies max proof proof defer version interpret algorithm define zero sum learner adversary learner action adversary action loss learner adversary define correspond payoff adversary regret dynamic zero sum iteration adversary distribution function subsequently learner simpler notation denote probability density function associate distribution adversary probability function adversary distribution arbitrary regret algorithm action concreteness adversary distribution multiplicative update algorithm exp  easily extend oracle computes approximately optimal additive error multiplicative simplicity exposition multiplicative error literature approximation algorithm subsequently learner output approximate stochastic oracle distribution adversary guarantee regret algorithm adversary EI LI max combine guarantee stochastic oracle min max min EI LI min EI LI EI LI oracle guarantee max regret adversary define uniform distribution derive max corollary theorem convex objective function convex function compute approximately minimax optimal calculate optimize maximum loss directly proportional therefore bite function corollary convex loss function convex function PT output algorithm satisfies max proof theorem uniform distribution max convex moreover convex therefore conclude max max robust statistical apply theorem statistical regression classification setting data vector feature dependent variable hypothesis function assume convex subset finite dimensional vector loss function functional theorem implies approximate stochastic optimization oracle compute distribution hypothesis achieves approximate minimax guarantee loss functionals convex hypothesis compute ensemble hypothesis possibly hypothesis non convex achieves guarantee theorem suppose convex functionals ensemble hypothesis PT hypothesis output algorithm approximate stochastic oracle satisfies max min max proof proof proof corollary emphasize convexity theorem hypothesis feature parameterization neural network mild applies statistical theory instance loss loss function distribution function convex respect argument predict dependent variable satisfied loss function machine multinomial logistic loss entropy loss  multi classification loss  regression setting theorem improper robust hypothesis ensemble hypothesis underlie optimization arbitrarily non convex parameter hypothesis apply approach robust training neural network stochastic oracle simply standard network training neural network achieve improper oppose standard corresponds training neural network extra layer relative network generate oracle robust submodular maximization robust submodular maximization reward function monotone submodular function function assume monotone submodular goal mini factor minimax optimum maxt mini robust optimization reward loss subset objective function stochastic oracle instantiate asks convex combination submodular function compute maxS compute maximum NP submodular function greedy algorithm computes approximate  iteration   marginal contribution   moreover approximation ratio polynomial convex combination monotone submodular function monotone submodular function immediately exists approximate stochastic oracle compute polynomial algorithm formally algorithm combine theorem corollary corollary algorithm stochastic oracle  computes poly distribution define uniform distribution ST min ES algorithm greedy stochastic oracle submodular maximization  input objective distribution objective  arg      sample mnist image corruption apply background corruption shrink corruption pixel corruption mixed corruption version compute achieves approximation NP function additive however randomize achieve constant factor approximation polynomial function monotone implies construct deterministically achieves constant factor approximation latter simply union ST distribution return algorithm criterion approximation scheme corollary suppose reward version algorithm return ST ST satisfies min robust classification neural network classic application robust optimization framework classification neural network corrupt perturbed datasets data image label corrupt data hypothesis neural net fix architecture assignment denote hypothesis parameter neural net uniform distribution corrupt data interested minimize empirical entropy aka multinomial logistic loss distribution latter robust statistical framework training neural network non convex optimization guarantee performance instead assume distribution image label loss function training neural net stochastic gradient descent image drawn achieve approximation optimal loss min implies approximate stochastic oracle code implement algorithm available http github com robust classification corrupt dataset robust training distribution corruption stochastic oracle asks approximation minimization min latter simply another loss distribution image mixture distribution define corruption index corrupt image distribution hence oracle assumption implies sgd mixture approximation linearity expectation alternative stochastic oracle training neural net distribution image loss function combination loss function corrupt version image implement interpretation stochastic oracle hybrid composite respectively neural network training scheme version detail finally entropy loss convex prediction neural net apply theorem ensemble neural net average prediction neural net iteration robust optimization achieve loss refer ensemble bottleneck loss setup mnist handwritten digit data training image validation image image image pixel grayscale image intensity pixel input neural network node hidden layer output layer softmax function distribution digit activation function relu network gradient descent parameter iteration mini batch corruption corruption image corruption version detail corruption baseline baseline individual corruption corruption construct oracle neural network training data perturbed corruption return network baseline corruption split baseline alternate training corruption iteration previous baseline oracle baseline oracle mod uniform distribution advanced baseline robust optimization scheme hybrid appendix without distribution update instead distribution corruption fix discrete uniform iteration allows multiplicative update robust optimization algorithm benefit hybrid composite superior baseline difference substantial magnitude statistically significant sophisticated composite outperforms hybrid increase improves performance diminish return largely sufficiently distribution corruption initial uniform distribution optimal stable distribution version detail consistent across corruption ensemble bottleneck loss empirically individual bottleneck loss perform algorithm composite ensemble bottleneck loss individual bottleneck loss background shrink pixel mixed combine classifier obtain robust optimization practical prediction data robust influence maximization apply robust influence maximization graph goal node maximize influence function comparison independent confidence criterion individual bottleneck loss min uniform baseline significantly loss individual influence opinion member node reachable extend model robust influence maximization goal maximize influence bottleneck influence function correspond graph fix robust submodular maximization rescale setup graph graph randomly probability graph parameter wikipedia vote graph parameter graph vertex parameter baseline algorithm baseline uniform individual greedy apply greedy maximization algorithm graph separately return uniform distribution greedy uniform distribution graph return output greedy submodular maximization algorithm uniform distribution influence function maximize influence uniform greedy multiple perturbed distribution generate distribution function randomly perturb uniform distribution perturbation magnitude chosen distance uniform distribution return robust optimization iteration graph robust optimization outperforms baseline bottleneck influence difference statistically significant magnitude moreover individual generate iteration robust optimization achieve empirically influence version detail