propose novel adaptive approximation approach  prediction motivate mobile iot health security application constraint computation communication latency feature acquisition arise adaptive training gate prediction model limit utilization model input instance gate easy handle input instance model adaptively approximate model model suffice highly accurate prediction empirical loss minimization constraint jointly gate prediction model benchmark datasets outperforms achieve accuracy introduction resource arise prediction machine application feature internet healthcare surveillance application arise due feature extraction feature sensor acquisition addition feature acquisition communication latency challenge mobile compute internet  iot application sensor camera phone device adaptive device constantly transmit measurement image centralize model prediction efficient approach device prediction locally whenever communication reduce latency due memory compute battery constraint prediction model device limited complexity consequently maintain accuracy adaptive desirable identify easy handle input instance local model suffice limit utilization service instance propose adaptive training fully annotate training data objective maintain accuracy meeting average resource constraint prediction promising approach focus reduce improve overall accuracy adaptive  resource feature computation etc allocate adaptively difficulty input model manner namely attempt model selectively effective feature improve accuracy contrast propose novel approach adaptive model annotate training data selectively identify input accuracy maintain principle advantage twofold approach readily apply desirable reduce exist legacy training model feature fundamental combinatorial issue multi stage feature subset sec contrast bypass issue adaptive approximation objective partition input easy upper stage schematic approach gate LPC model adaptively approximate hpc model insight adaptive approximation axis feature axis conditional probability prediction LPC hpc prediction input correspond gate threshold performs poorly otherwise goal gate function attempt LPC hpc legacy available consists accuracy model minimizes empirical loss regardless prediction model hpc readily exist neural network achieves accuracy jointly gate function prediction LPC model adaptively approximate accuracy model identify input gate LPC model adequate achieve accuracy iot application complexity model deployed device perform gate prediction input instance gate function decides LPC model adequate accurate classification intuitively easy correctly classify LPC model hpc model identify input instance classify accurately  bypass utilization hpc model reduce average prediction upper schematic approach feature vector predict label aim LPC model adaptively approximate hpc observation depict probability classification hpc model highly complex function LPC model exists input LPC competitive accuracy gate threshold accord gate function LPC loss prediction accuracy reduce prediction simpler task primarily partition input LPC model suffice difficulty gate function capable identify input instance LPC suffices prediction gate account strategy feature decision architecture gate function LPC model discriminative empirical risk minimization jointly optimizes gate prediction model joint margin objective function objective separately convex gate prediction function propose alternate minimization scheme guaranteed converge appropriate choice loss function instance logistic loss optimization amount probabilistic approximation projection projection projection onto probability recursively apply multiple stage successively approximate adaptive obtain previous stage thereby refining accuracy benchmark datasets stage outperforms accuracy performance related decision minimize error budget constraint prediction active pre model instantiation assume exists collection prediction model amortize prediction model impose instance feature dimension assume sufficiently admit exhaustive enumeration combinatorial possibility policy amongst prediction model contrast impose restriction dimensional exist approach focus complex adaptive decision function conceptually training acquire feature utility exploration partition input combinatorial feature subset accuracy multi stage exploration combinatorially novel relaxation greedy heuristic developed context somewhat related propose prune fully random RF reduce nevertheless contrast adaptive perspective compress model utilize prune alone model prediction furthermore specifically tailor random another related classifier cascade decision dag aim pre learner reduce prediction budget pre accuracy model jointly learns model approximate therefore complementary exist teacher framework related approach model learns approximate teacher model budget however goal alone model contrast  accuracy teacher model prediction via gate function learns limitation model consult accuracy teacher model thereby avoid accuracy loss composite related  learns composite max likelihood estimation model difference  address budget constraint fundamental aspect budget constraint asymmetry whereby hpc model sequentially approximate  asymmetry propose strategy accuracy predictor separately estimate critical empirical loss minimization setup standard scenario resource constrain prediction feature training sample generate unknown distribution feature vector acquisition assign feature label multi classification stage training formalize setup model prediction hpc model priori accuracy regardless consideration alternative prediction LPC model option model utilize prediction accuracy prediction model model loss function exclusively employ logistic loss function binary classification exp  although framework allows loss model assume acquire feature efficiently cached subsequent incur additional utilize prediction model denote compute sum acquisition unique feature oracle gate gate likelihood function output likelihood input prediction model overall empirical loss esn esn esn excess loss depends perspective constant average loss average assume gate negligible esn esn esn reduction constant characterize optimal gate function minimizes overall average loss average constraint excess loss reduction suitable choice characterization encodes important principle marginal reduction excess loss opt hpc model nevertheless characterization generally infeasible LHS depends hpc performs input instance information unavailable target unreachable gate gate approximation directly enforce structure decouple constraint introduce parameterized gate function attempt mimic approximate ensure approximation minimize distance choice approximation metric kullback leibler KL divergence although choice KL divergence dkl sgn sigmoid function besides KL divergence propose another symmetrize metric fitting directly odds ratio  detail budget constraint gate function predict depends denote feature passing sum acquisition unique feature similarly denotes independent depends primarily model linear model collection feature budget simplifies esn esn esn budget depends quantity esn model feature constant budget constraint accuracy model esn therefore split budget constraint objective ensure penalty   tradeoff parameter indicator variable denote feature respectively model parameterization approximate sparse norm stage wise manner algorithm ensure  via constraint esn  optimization min esn loss gate approx opt esn   objective function penalizes excess loss ensures excess loss enforce admissible gate function penalizes feature usage budget constraint limit costly model remark directly parameterizing non convexity average loss sum loss hpc LPC probability distribution convex  parameterization generally non convex sigmoid non parametric avoid non convexity parameterize via KL opt convex respect fix convex fix otherwise introduce non convexity prior instance non convex inner loop iteration algorithm remark stage approximation however straightforward recursively composite predictor gate prediction model approximate composite remark limit scope focus reduce feature acquisition prediction challenge combinatorial however prediction computation encode choice functional opt surrogate upper bound composite insight objective opt latent variable composite standard application jensen inequality reveals dkl  therefore  composite bound loss function overload notation random variable format dkl  implies objective attempt bound loss composite objective constraint enforce budget limit composite sparsity feature zero encourage feature gate prediction model fundamental sparse subset feature effective gate accurate prediction combinatorial contribution address optimization framework opt algorithm concrete instantiate framework opt algorithm via parameterizations adapt lin linear adapt gbrt non parametric algorithm adapt lin input  initialize opt opt convergence algorithm adapt gbrt input  initialize opt cart minimize feature cart minimize feature convergence KL divergence distance algorithm adapt  symmetrize distance  algorithm perform alternate minimization opt convergence alternate minimization algorithm parameter  swept generate various  tradeoff satisfy budget validation data adapt lin linear classifier feature correspond component non zero minimization solves min PN  PN  opt shorthand notation optimization non negative constant constraint satisfied optimization projection information geometry entropy optimize constant minimize respect opt relaxed non convex   norm sparsity tradeoff parameter feature budget satisfied constant minimize respect adapt lin summarize algorithm min XN opt adapt gbrt non parametric classifier gradient boost PT PT  regression limited depth assume feature utility independent feature otherwise similarly optimization solves opt modify minimize denote loss essentially objective opt minimize respect gradient boost naturally adopt stage wise approximation objective define impurity function approximates negative gradient loss penalizes initial acquisition feature capture initial acquisition penalty indicates feature already previous update impurity respectively   minimize impurity function balance minimize loss already acquire feature classification regression cart construct decision impurity function adapt gbrt summarize algorithm impurity  interestingly  adapt gbrt exactly recovers   algorithm thanks approach adapt gbrt benefit accuracy initialization perform accuracy tradeoff accuracy beyond  baseline ALGORITHMS baseline approach perform regularize logistic regression data identify relevant sparse subset feature training data restrict identify feature finally correctness prediction pseudo label assign pseudo label agrees label otherwise stateof feature budget algorithm  ensemble gradient boost feature budget  prune random feature budget   omit perform  datasets detailed setup  visualize verify adaptive approximation ability adapt lin adapt gbrt synthetic dataset without feature illustrate difference adapt lin baseline approach synthetic datasets finally adapt gbrt resource constraint benchmark datasets input data lin initialization lin iteration rbf contour gbrt initialization gbrt iteration synthetic without feature input data decision contour rbf svm decision boundary linear initialization iteration adapt lin decision boundary boost initialization iteration adapt gbrt  adaptation construct 2D binary classification dataset synthetic rbf svm accuracy classifier visualize adaptive approximation 2D feature opt adapt lin adapt gbrt synthetic adaptive feature acquisition data distribute cluster feature correspond coordinate respectively accuracy tradeoff curve algorithm recover optimal adaptive whereas approach cannot initialization prediction iteration adapt lin adapts local assign sends  similarly initialization prediction  identify ambiguous via algorithm maintain prediction accuracy classify via simpler model joint optimization return prediction feature budget constrains illustrate baseline approach 2D dataset synthetic data distribute cluster label feature acquisition complex classifier acquires feature achieve accuracy synthetic cluster data regularize logistic regression vertical dash cluster others feature acquire adaptive cluster cluster complex classifier incur average sub optimal adapt lin optimize alternate manner recover horizontal sends cluster classifier cluster correctly classifies cluster correctly classify adaptive feature acquire cluster overall average feature solid curve accuracy tradeoff plot baseline approach sub optimal  optimize selection feature subset jointly average feature accuracy adapt gbrt    average feature accuracy covertype average feature average precision yahoo rank average feature accuracy cifar comparison adapt gbrt   benchmark datasets RF adapt gbrt rbf svm adapt gbrt achieves accuracy tradeoff gap significant accuracy  exceeds precision slowly limit clearer comparison dataset statistic dataset validation feature feature uniform  uniform uniform cifar uniform yahoo cpu datasets various aspect algorithm stateof feature budget algorithm benchmark datasets  particle identification covertype datasets uci repository cifar yahoo rank yahoo rank dataset associate feature  relevance rank document query feature associate acquisition cpu extract feature yahoo employee label binarized relevant relevant task model query associate document relevance rank relevant document feature performance metric average precision datasets unknown feature assign feature aim adapt gbrt successfully selects sparse subset  feature summarize statistic datasets highlight insight dataset generality approximation framework allows approximation powerful classifier rbf svm random curve respectively adapt gbrt maintain accuracy reduce advantage algorithm approximate achieves accuracy adapt lin adapt lin outperforms baseline dataset confirms intuition synthetic adapt lin iteratively subset feature jointly  adapt lin adapt gbrt significantly performance adapt lin approximate rbf svm RF non parametric non linear classifier powerful linear adapt gbrt  approach benefit initialization adapt gbrt RF  adapt gbrt maintain accuracy longer budget decrease adapt gbrt improves around  spike precision initial prune improve generalization performance RF adapt gbrt maintains accuracy  furthermore adapt gbrt freedom approximate rbf svm adapt gbrt achieve accuracy  average feature accuracy adapt gbrt RF adapt lin RF RF adapt gbrt rbf adapt lin rbf rbf  baseline approach adapt lin adapt gbrt rbf svm RF dataset adapt gbrt  adapt gbrt outperforms  datasets gap significant significant reduction without sacrifice accuracy within adapt gbrt reduces average feature around  yahoo cifar datasets respectively conclusion adaptive approximation approach account prediction arise various application gate function identify prediction model collection model adapt input overall goal reduce without sacrifice accuracy gate prediction model strategy prediction model approximate prediction model model suffice benchmark datasets average reduction without sacrifice accuracy within outperforms budget algorithm significant margin