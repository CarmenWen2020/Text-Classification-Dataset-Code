affine invariant optimization algorithm online lazy newton regret online lazy newton independent conditioning algorithm performance depends precondition retrospect intrinsic dimensionality application online lazy newton achieve optimal regret rank expert improve factor previously bound resolve introduction online convex optimization learner convex function bound convex domain learner convex function aim learner minimize cumulative regret define min online gradient descent achieves optimal regret rate GD bound lipschitz constant bound diameter domain respect euclidean norm simplicity restrict exposition linear loss bound maximal euclidean norm kgt convex easily reduce useful obtain faster convergence optimization employ precondition namely apply linear transformation gradient update online optimization access preconditioner hindsight achieve regret rate   DP diameter  GP bound norm gradient refer quantity  conditioning preconditioner however directly assume bound magnitude loss assume bound bound typical guarantee gradient online gradient descent directly apply principle preconditioner  assumes intrinsic dimensionality rank loss vector conditioning optimization improve however approach access transformation typically data dependent retrospect conference neural information processing beach CA usa address achieve regret rate without explicit prior knowledge preconditioner affirmative algorithm achieves rate online lazy newton algorithm variant online newton algorithm due employ lazy projection online newton algorithm exploit curvature loss function specifically exp concavity adaptation aim possibly linear online convex optimization exploit latent dimensional structure adaptation algorithm achieve regret logarithmic factor without prior knowledge optimal preconditioner crucial algorithm affine invariance online lazy newton invariant affine transformation gradient algorithm gradient apply inverse transformation decision decision obtain apply algorithm directly vector application establish regret rate rank expert introduce rank expert variant classical prediction expert advice assumes expert linearly dependent loss span dimensional rank challenge achieve regret rate independent expert depends rank bound regret upper bound algorithm achieve suboptimal regret bound apply online lazy newton algorithm improve upon latter bound achieve logt regret bound optimal logt factor improves upon prior bound unless exponential rank related adaptive regularization important topic online optimization considerable attention recent adagrad algorithm closely related algorithm analyze dynamically adapts geometry data adagrad learns preconditioner trace bound mahalanobis norm detailed discussion comparison guarantee  algorithm van   dynamic regularization technique obliviously adapt curvature loss function bound precondition domain unbounded bound inapplicable however loss bound assume generally beyond analysis exploit latent structure data active research within online direction adaptation stochastic data exploration various structural assumption leveraged guarantee online lazy newton algorithm algorithm regularize leader   iteration minimizer loss additional regularization algorithm closely related approximate leader  algorithm  algorithm achieve logarithmic regret rate exp concave exploit curvature information function contrast algorithm aim optimize convex function possibly curvature  performs FTL approximation function online lazy newton instead utilizes approximation additional rank quadratic regularizer another algorithm closely related perceptron algorithm  closely related   warmuth forecaster variant classical perceptron algorithm adapt data skewed ill similarly algorithm perceptron employ adaptive whiten data address skewness finally algorithm propose enhance version online newton utilizes sketch improve previous online algorithm propose version completely invariant linear transformation regret bound depends linearly dimension ambient quadratic rank loss matrix contrast regret bound dimension ambient linear rank loss matrix achieve optimal regret bound rank expert highly inspire motivate rank expert optimal algorithm introduce author establish regret rate rank expert loss regret bound expert investigate cohen  establish tight upper bound logarithmic factor independent expert setup recall standard framework online convex optimization learner chooses decision bound convex subset dimensional adversary chooses convex function learner suffers loss performance learner regret define difference accumulate loss incur learner loss decision hindsight namely regret learner RegretT min typically assumes diameter bound convex function lipschitz continuous respect norm typically norm dual however refrain explicit assumption geometry optimization algorithm oblivious notation positive definite matrix denote norm induced namely  dual norm define kgk  kgk  finally non invertible matrix denote moore penrose  inverse affine invariant regret bound online lazy newton algorithm bound linear loss regret intrinsic dimensionality bound loss theorem online convex optimization linear loss assume algorithm regret bound RegretT DH  rank PT  DH max  GH max kgt standard reduction analogue statement convex loss assume dot gradient decision vector bound corollary arbitrary sequence convex function suppose algorithm maxt maxx regret bound RegretT DH  rank PT DH max  GH max theorem bound txt constant significantly weaker requirement assume bound diameter  norm gradient norm quantity DH GH properly bound stress importantly algorithm matrix achieve correspond bound theorem assume max max rank PT algorithm regret algorithm logt discussion worth previously adaptive regularization algorithm technique popular gradient employ adaptive regularization adagrad algorithm introduce adagrad algorithm enjoys regret bound depict competitive fix regularization matrix RegretT adagrad inf  RegretT  inf  matrix generalize cauchy schwartz inequality plug tune bound competitive fix regularization matrix depict bound improves adagrad regret bound bound intrinsic dimension underlie dimensionality dimension ambient online lazy newton enjoys superior regret bound furthermore demonstrate dependence adagrad regret ambient dimension artifact analysis actual regret grows polynomially rank online lazy newton bound superior adagrad exists conditioning matrix improves norm gradient respect euclidean norm norm respect optimal norm induced generally whenever PT PT tighter bound online lazy newton algorithm focus affine invariant algorithm online lazy newton  algorithm algorithm maintains vector vector update iteration gradient via vector guaranteed hence actual prediction  via projection onto vector however similarly ons algorithm transforms via pseudo inverse matrix sum outer projection respect context notation arg min denote projection onto respect semi norm induced positive semidefinite matrix algorithm  online lazy newton parameter initial initialize incur gradient rank update online newton projection return motivation  demonstrate analysis algorithm becomes invariant linear transformation gradient vector indeed linear transformation algorithm instead transform cumulative regret invariant transformation theorem invariance indeed algorithm improve regret bound input representation data poorly algorithm ons important aspect unlike ons lazy version maintains vector update without projection projection apply calculate gradient descent lazy projection analogous dual average ons gradient descent greedy projection reminiscent mirror descent algorithm decouple future conditioning projection transformation matrix lazy approach allows project iteration independently ons initialization  invariant affine transformation difference negligible typically chosen recall matrix  eigenvalue meaningful poorly application rank expert rank expert online lazy newton algorithm obtain nearly optimal regret rank expert variant classic prediction expert advice learner advice expert learner chooses distribution expert probability vector denotes dimensional simplex thereafter adversary chooses vector assign loss expert player suffers loss contrast standard expert assume hindsight expert rank structure rank stochastic drawn fix distribution leader algorithm enjoy regret bound min author achieve regret bound online affirmative theorem rank expert rank expert rank algorithm obtain regret satisfies RegretT logt bound bound logt factor improves upon upper bound exponential differently aim ensure average regret  algorithm iteration oppose iteration algorithm remark hedge algorithm obtain regret rate obtain algorithm regret bound minp logt treat hedge  meta expert apply hedge analysis proof theorem rely technical lemma lemma lemma convex function define convex domain arg minx arg minx assume strongly convex respect norm furthermore convex lemma slight strengthen lemma sequence vector define  positive definite matrix kgt rank matrix  proof det GT det det   matrix determinant lemma det uut det det  det det det inequality sum obtain det det det GT det yield  PT   rank matrix PT  PT  hence eigenvalue matrix  sum denote latter concavity jensen inequality conclude det  lemma theorem proof theorem fet atx choice algorithm iteration minimizer fet indeed span constant fet const algorithm equivalent leader algorithm function fix positive definite matrix DH maxx  GH max kgt fet atx kxk kxk kxt kxk kxt  function strongly convex respect norm kgt  minimize lemma fet fet kxt kgt  kgt kvk kuk kgt max kgt kgt  overall obtain kxt FTL btl lemma PT PT hence obtain kxt plug rearrange obtain kxt finally obtain inequality matrix rescale matrix  DH obtain matrix diameter DH GH DH GH plug inequality yield proof theorem simplify notation assume corollary RegretT 2G GH DH obtain regret bound RegretT hence exists matrix indeed span denote XS projection  XS  projection define XS definition symmetric convex hence ellipsoid approximation obtain positive semidefinite matrix positive eigenvalue restrict TB duality XS TB PS projection    arbitrary  TP SB  kxk