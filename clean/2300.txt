initialization network dramatic impact ensure singular network input output jacobian essential avoid exponentially vanish explode gradient moreover linear network ensure singular jacobian concentrate yield dramatic additional dynamical isometry however unclear achieve dynamical isometry nonlinear network address employ powerful probability theory analytically compute entire singular distribution network input output jacobian explore dependence singular distribution depth network initialization choice nonlinearity  relu network incapable dynamical isometry sigmoidal network achieve isometry orthogonal initialization moreover demonstrate empirically nonlinear network achieve dynamical isometry magnitude faster network indeed properly initialize sigmoidal network consistently outperform relu network overall analysis reveals entire distribution jacobian singular important consideration introduction achieve performance domain computer vision machine translation education neurobiological model determinant training network appropriately initial indeed genesis upon initial observation unsupervised pre training initial subsequent tune backpropagation moreover seminal appropriately gaussian prevent gradient explode vanish exponentially achieve reasonable random initialization primarily driven principle singular network jacobian input output remain implies average randomly chosen error vector preserve norm backpropagation however guarantee growth shrinkage error vector requirement demand jacobian singular remain requirement error vector approximately preserve norm moreover angle error vector preserve error information conference neural information processing beach CA usa  faithfully  network requirement dynamical isometry theoretical analysis nonlinear dynamic linear network reveal initialization satisfy dynamical isometry yield dramatic increase initialization linear network orthogonal initialization achieve dynamical isometry remarkably epoch becomes independent depth contrast random gaussian initialization achieve dynamical isometry achieve depth independent training remains unclear however nonlinear network indeed empirically gaussian orthogonal initialization nonlinear network yield mixed important theoretical practical entire distribution singular network input output jacobian upon depth statistic random initial nonlinearity combination ingredient achieve dynamical isometry nonlinear network neither vanish explode gradient addition achieve dynamical isometry achieve faster detailed summary discussion theoretical derive expression entire singular density input output jacobian variety nonlinear network width limit compute singular equivalently  JJT deduce rescale examine metric quantify conditioning jacobian smax maximum singular equivalently max maximum eigenvalue JJT JJT variance eigenvalue distribution JJT max JJT jacobian ill dynamic setup layer neural network width synaptic matrix RN bias vector pre activation activation feedforward dynamic network pointwise nonlinearity input RN input output jacobian RN diagonal matrix entry input output jacobian closely related backpropagation operator mapping output error matrix layer former latter tends layer therefore understand entire singular spectrum network randomly initialize bias bias drawn zero gaussian standard deviation random matrix ensemble random gaussian drawn gaussian variance random orthogonal drawn uniform distribution orthogonal matrix obey  review signal propagation random matrix eqn empirical distribution pre activation nonlinearity eqn propagation empirical distribution layer limit empirical distribution converges gaussian zero variance obeys recursion relation induced dynamic eqn  initial PN exp denotes standard gaussian recursion fix obey input chosen fix distribution becomes independent fix scenario rapidly approach layer assume depth approximation compute spectrum another important quantity signal propagation network DW DW derivative distribution singular matrix DW pre activation fix distribution variance phase chaotic gradient exponentially explode vanish respectively indeed singular simply critical initialization neither vanish explode gradient chaotic vanish gradient explode gradient chaos transition tanh critical determines boundary phase chaotic phase signal propagation expands fold chaotic manner propagate gradient exponentially explode phase signal propagation contract manner propagate gradient exponentially vanish along critical phase heatmap probability random matrix theory network previous reveal singular obtain detailed information entire singular distribution eqn consists random matrix probability becomes relevant powerful compute spectrum review random matrix limit spectral density define denotes respect distribution random matrix GX definition  transform invert lim GX linear gaussian relu orthogonal  orthogonal spectrum criticality nonlinearities depth excellent agreement empirical simulation network width dash theoretical prediction solid relu tanh orthogonal linear gaussian gaussian linear orthogonal relu similarly distribution depth conditioning singular orthogonal tanh  transform GX related generate function MX MX  kth distribution  denote functional inverse MX definition satisfies MX MX finally transform define SX utility transform arises behavior multiplication specifically freely independent random matrix transform random matrix ensemble AB simply transforms sab SA SB eqn implicit definition spectral density JJT supplementary   SD SL wwt SL identical distribution define   pre activation distribute independently depth implies SD SD eqn compute spectrum JJT WT compute respective transforms sequence equation  eqn reverse sequence  JJT inverse  calculate transforms wwt attack specific nonlinearities ensemble principle procedure numerically arbitrary choice nonlinearity postpone investigation future linear network linear network QL criticality eqn implies eqn reduces critical orthogonal orthogonal singular thereby achieve perfect dynamic isometry gaussian behave differently singular eigenvalue JJT wishart matrix spectral density recently compute singular density sin    sin  demonstrates theoretical density empirical density obtain numerical simulation random linear network depth increase density becomes highly anisotropic concentrate zero develop extend corresponds minimum singular smin corresponds maximum eigenvalue max max max eqn yield variance eigenvalue distribution JJT JJT linear gaussian network smax JJT linearly depth signal conditioning breakdown dynamical isometry relu tanh network discus criticality finite eqn eqn nonlinear network network slope nonlinearity eqn reduces probability neuron linear regime  limit distribution pre activation zero gaussian variance therefore relu independent whereas tanh  erf depends approach relu eqn reduces imply critical tanh contrast depends eqn criticality yield curve tanh network along curve direction decrease curve approach monotonically decrease towards critical relu network parameter critical tanh network neither vanish explode gradient due nevertheless entire singular spectrum network behave differently eqn spectrum depends non linearity SD eqn depends distribution eigenvalue equivalently distribution derivative distribution bernoulli distribution parameter insert distribution sequence eqn eqn eqn yield GD MD SD calculation  eqn compute  gaussian orthogonal subsection gaussian derive expression transform random gaussian matrix variance supplementary  combine eqn SD eqn  eqn yield  JJT eqn eqn define polynomial  transform satisfies equation eqn spectral density obtain imaginary max singular smax versus gaussian orthogonal relu dash tanh solid network gaussian relu tanh smax grows predict eqn contrast orthogonal tanh orthogonal relu smax remain predict eqn essence fix neuron linear regime smax grows tanh network linear curve corresponds prediction linear gaussian network eqn curve simply correspond linear orthogonal network spectral namely location minimum maximum eigenvalue JJT deduce imaginary eqn vanishes discriminant polynomial eqn vanishes detailed  calculation max max recall exponential growth max exponential decay moreover criticality max grows linearly depth obtain variance JJT eigenvalue density JJT compute employ lagrange inversion theorem  JJT relates expansion generate function  functional inverse JJT substitute expansion JJT eqn expand coefficient generically exponentially vanish however criticality variance JJT exhibit linear growth depth neuron operating linear regime relu tanh network choice gaussian initialization prevent linear growth JJT max imply critical gaussian initialization failure dynamical isometry depth network orthogonal orthogonal wwt transform SI supplementary  SI combine eqn eqn yield  eqn yield JJT  JJT combine eqn eqn obtain polynomial  transform satisfies  sgd momentum adam RMSProp dynamic generalization performance network depth width cifar optimizers tanh tanh relu solid orthogonal dash gaussian initialization relative curve robustly persists across optimizers strongly correlate dynamical isometry initialization smax network smax closer faster network initialize critically isometric orthogonal tanh magnitude faster isometric relu network extract eigenvalue singular density JJT respectively eqn demonstrate excellent theoretical prediction numerical simulation random network modest depth singular peaked max depth distribution accumulates develop fix critical relu tanh network ill  orthogonal matrix obtain maximum eigenvalue JJT discriminant polynomial eqn vanishes calculation yield max max max exponentially  decay criticality behaves max compute variance JJT expand JJT eqn apply eqn criticality JJT asymptotic behavior max JJT depends crucially neuron linear regime relu network max JJT linearly depth dynamical isometry  relu network critical orthogonal contrast tanh network erf therefore along critical towards thereby reduce increase decrease arbitrarily  linear growth max JJT unlike relu network gaussian network achieve dynamical isometry depth essence strategy increase neuron operating linear regime enable orthogonal tanh net mimic successful dynamical isometry achieve orthogonal linear net however strategy unavailable orthogonal relu network demonstration establish theory entire singular distribution dynamical isometry empirical evidence presence absence isometry impact training summarize critical neural network tanh tanh relu chosen appropriately achieve critical initial empirical measurement sgd training define accuracy orthogonal tanh network curve reflect depth fix  collapse onto universal curve rate rescale rescale implies optimal rate remarkably optimal grows curve reflect fix reveal associate increase dynamical isometry enables faster training optimal rate function function qualitative agreement connection smax boundary chaos network singular neither vanish explode gradient infinite width limit therefore probe specific dynamical isometry entire spectrum explore sophisticated optimizers overcome initialization sgd momentum RMSProp adam network depth width batch additionally average instantiation network reduce nonlinearity initialization optimizer obtain optimal rate grid sgd sgd momentum logarithmically rate adam RMSProp explore optimal rate threshold accuracy performance exceeds qualitative conclusion fairly independent report version cifar theory performance advantage orthogonal gaussian initialization significant somewhat negligible prediction verify solid dash curve furthermore extent dynamical isometry initialization strongly predicts isometric orthogonal tanh faster isometric relu network magnitude moreover conclusion robustly persist across optimizers  dynamical isometry tanh initialization orthogonal versus gaussian impact choice optimizer insight quantitative analysis relation dynamical isometry orthogonal tanh network summarize focus sgd lack dependence optimizer  demonstrates optimal training grows  depth reveals increase dynamical isometry enables faster training available faster rate finally similarity positive correlation training max singular overall dynamical isometry correlate entire distribution jacobian singular important consideration explore relationship dynamical isometry performance beyond initialization evolution singular throughout training dynamical isometry initialization persists training  standard cifar dataset augment random flip random saturation brightness contrast perturbation singular evolution orthogonal tanh network sgd training average distribution network sgd eigenvalue ill conditioning JJT equality sgd initial interestingly optimal maintains dynamical isometry later stage training simply accuracy function sgd generalization accuracy function initial reveal optimal nonzero maintains dynamical isometry training yield generalization accuracy perfect dynamical isometry initialization choice preserve isometry throughout training instead nonzero optimal moreover generalization accuracy peak nonzero bolster relationship dynamical isometry performance beyond simply initialization discussion summary employ probability theory analytically compute entire distribution jacobian singular function depth random initialization nonlinearity analytic computation yield insight combination ingredient enable nonlinear network achieve dynamical isometry linear gaussian network cannot maximum jacobian singular grows linearly depth remains orthogonal gaussian relu network relu nonlinearity destroys dynamical isometry orthogonal linear network contrast orthogonal gaussian sigmoidal network achieve dynamical isometry depth increase max singular remain former grows linearly latter orthogonal sigmoidal network rescue failure dynamical isometry relu network correspondingly demonstrate cifar orthogonal sigmoidal network magnitude faster relu network performance advantage robust choice variety optimizers sgd momentum RMSProp adam orthogonal sigmoidal network moreover sublinear depth orthogonal linear network depth independent training orthogonal sigmoidal network training depth finally dynamical isometry initialization persists amount training moreover isometric initialization longer persistence yield faster generalization overall yield insight entire distribution network jacobian singular dramatic avoid exponentially vanish explode gradient significant performance advantage moreover pursue principle tightly concentrate entire distribution around reveal  network sigmoidal nonlinearities actually outperform relu network popular nonlinear network future extend network skip connection convolutional architecture generally performance advantage accompanies dynamical isometry suggests explicitly optimize reinforcement architecture