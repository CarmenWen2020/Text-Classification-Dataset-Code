embeddings effective analyze recently extend model data beyond text item recommendation embed model probability target observation item context item conditioning context optimal instead model probability target subset context amortize variational inference automatically subset standard embed model improves prediction quality embeddings introduction embeddings powerful model capture latent semantic structure capture occurrence allows usage meaning embeddings extend dimensional data beyond text item supermarket movie recommendation goal capture occurrence focus exponential embeddings EFE encompasses exist embeddings expressive probabilistic model distribute representation embed model conditional probability target context instance text target corresponds context around embed model item supermarket target corresponds item basket context item purchase shopping conditioning context optimal intuitively item necessarily interact target context instance shopping data probability purchasing chocolate independent bathroom tissue context latter actually purchase shopping generalization EFE model relaxes assumption target depends context model considers target depends subset context refer approach ping liu contribution  researcher columbia conference neural information processing beach CA usa context selection exponential embeddings CS EFE specifically introduce binary hidden vector target depends infer indicator vector embed model related context conditional distribution vector capture underlie item relation introduction indicator price inference embed task amount target context inference avoid inference separately target context amortize variational inference neural network structure perform inference difficulty varied context varied input output structure overcome binning technique detail contribution develop model allows conditioning subset context EFE model develop efficient inference algorithm CS EFE model amortize variational inference automatically infer subset context relevant predict target comprehensive experimental datasets namely movielens movie recommendation  PA grocery data shopping behavior CS EFE consistently outperforms EFE predictive performance datasets movielens embed representation CS EFE model quality model context selection procedure model embeddings adopt formalism exponential embeddings EFE extend embeddings data continuous data briefly review EFE model model efficient inference procedure exponential embeddings exponential embeddings EFE collection text application movie recommendation goal vector representation occurrence dataset typically sparse matrix datapoints text application corresponds location text vector location movie data entry xnj indicates rating movie user EFE model learns vector representation conditional probability observation observation context context cnj index observation conditional probability distribution xnj definition context varies across application text corresponds fix location movie recommendation cnj corresponds movie rat user exclude EFE vector embed vector context vector vector interact conditional probability distribution observation xnj context cnj correspond observation xcnj indexed cnj distribution xnj exponential xnj xcnj  xnj xcnj xnj sufficient statistic exponential distribution xcnj parameter parameter xcnj cnj xcnj  cnj context link function depends application role generalize linear model slightly xcnj EFE intercept average context choice generally improve model performance vector intercept maximize pseudo likelihood conditional probability observation xnj context selection exponential embeddings EFE model assumes context cnj role distribution xnj unrealistic assumption probability purchasing chocolate context vector bathroom tissue latter actually context formally domain context interact selectively probability xnj develop context selection exponential embeddings CS EFE model selects subset context embed model parameter depends truly related target introduce hidden binary vector bnj cnj indicates context cnj distribution xnj parameter xcnj bnj bnj xcnj  bnj  non zero bnj prior distribution assign prior bnj bnj bnj πnj     constraint bnj context bnj satisfy constraint probability proportional independent bernoulli variable hyperparameters   distribution approach categorical distribution  constraint bnj becomes relevant distribution approach bernoulli distribution probability πnj impact context  πnj min cnj global parameter hyperparameter average tends infinity fix recover EFE model objective function objective function regularize pseudo likelihood marginalize variable bnj lreg  bnj xnj xcnj bnj bnj πnj lreg regularization regularization embed context vector computationally marginalize context selection variable bnj particularly cardinality context cnj address issue inference maximize objective function propose algorithm amortize variational inference global inference network local variable  inference detail variational inference variational inference introduce variational distribution bnj νnj parameterized νnj cnj maximize bound objective lreg bnj νnj xnj xcnj bnj bnj πnj bnj νnj maximize bound respect variational parameter νnj corresponds minimize kullback leibler divergence posterior bnj variational distribution bnj νnj variational inference EFE   maximization approach variational distribution dataset online training model generally cannot bnj νnj individually optimization νnj later address former variational inference approximates expectation via monte carlo obtain noisy gradient variational bound tackle latter amortize inference advantage optimize local variable amortization amortize inference avoids optimization parameter νnj local variational distribution bnj νnj instead structure calculate local parameter νnj specifically function input target observation xnj context xcnj index cnj model parameter output variational distribution bnj  xnj cnj xcnj πnj input νnj cnj output νnj  vector logits variational distribution  νnjk sigmoid νnjk νnjk  similarly previous neural network parameterized amortize inference network parameter network typical neural network transform fix input fix output however variable input output output function bnj νnj context cnj varies across target context local variable  varies xcnj depends context propose network address challenge overcome difficulty output split computation component νnjk νnj cnj task task computes logit νnjk function νnjk  input  contains information  depends index specify input  naïve approach index context item correspond sparse vector network input moreover network computation νnjk subset assign non zero input instead input vector  fix regardless context cnj transform input  xnj cnj xcnj πnj vector reduce dimensionality preserve relevant information define relevant transform vector reduce dimensionality fix vector information relevant inspect posterior bnj bnj xnj xcnj πnj xnj xcnj bnj bnj bnj πnj xnj  bnj bnj πnj dependence xcnj  vector cnj contains inner correspond embed context vector variable bin  representation amortize inference network output variational parameter context selection variable  input fix regardless context snjk prior parameter πnj target observation xnj histogram snjk context observation snjk  αjk therefore  sufficient raw embed vector input  cnj reduce dimensionality input transform  cnj fix vector neural network input vector neural network structure transformation differently network output variational parameter νnjk snjk directly input snjk related νnjk network output νnjk ultimately indicates probability  νnjk indicates kth context computation parameter snjk relation νnjk permutation posterior bin snjk bin therefore obtain fix vector instead bin boundary gaussian kernel denote width gaussian kernel denote bin variable xcnj exp snjk finally νnjk  neural network input snjk bin variable summarize information snjk target observation xnj prior probability πnj  snjk xnj πnj variational update denote parameter network bias perform inference iteratively update maximize νnj output network variational expectation maximization EM algorithm gradient respect model parameter gradient respect network parameter obtain noisy gradient respect function variational inference allows rewrite gradient expectation respect variational distribution bnj xnj  bnj bnj πnj bnj bnj estimate gradient via monte carlo sample bnj empirical performance context selection application domain movie recommendation  basket analysis domain context selection improves prediction movie data embeddings interpretable basket analysis motivate variational probability infer network data movielens movielens dataset contains rating movie rating rating remove user rat movie movie rat yield dataset user movie average non zero per user aside data validation  PA  data contains information observation datum corresponds checklist specie report zero extraordinarily treat outlier positive specie observation subset  PA rectangular mostly overlap pennsylvania checklist data unique specie average non zero per checklist split data validation basket dataset contains purchase customer anonymous supermarket aggregate purchase category combine individual upc universal code item item category yield purchase unique item average basket item split data training validation model exponential embeddings EFE model context selection procedure implement amortize inference network described prior hyperparameter movie data rating binomial conditional distribution trial identity link function parameter logit binomial probability  PA basket data poisson conditional distribution link function softplus parameter poisson rate context corresponds movie rat user movielens checklist  PA item basket experimental setup explore dimensionality embed vector report perform validation qualitative difference relative performance non report negative sample ratio positive non zero versus negative sample stochastic gradient descent maximize objective function adaptively stepsize adam validation likelihood ass convergence variance regularization regularization fix context selection exponential embeddings CS EFE model hidden hidden layer bin histogram explore setting network obtain network layer adapt setting bin essential information bin equally distance width code github repo http github com  lab context selection embed softplus function define softplus exp baseline EFE CS EFE movielens baseline EFE CS EFE  PA baseline EFE CS EFE basket likelihood datasets CS EFE model consistently outperforms baseline prior hyperparameter bracket standard error hyperparameter context impact movielens dataset datasets predictive performance predictive pseudo  calculate marginal likelihood report average likelihood datasets average predictive likelihood per item standard error bracket prediction model baseline EFE obtain baseline significance bold performance across setting outperforms baseline datasets improvement baseline significant  PA datasets prior parameter impact model performance evaluation embed quality context selection affect quality embed vector item movielens dataset movie genre label calculate movie similarity genre label similarity derive embed vector consistent genre similarity detail binary vector genre label movie genre define similarity genre vector genre normalize genre sim max vector analogous manner define similarity embed vector cosine distance compute similarity movie movie accord definition similarity genre embeddings query movie correlation metric metric simply spearman correlation ranked metric rank movie embed similarity calculate average genre similarity movie finally average metric across query movie report baseline EFE CS EFE metric  sim correlation embed vector movie genre embed vector CS EFE model exhibit correlation movie genre target taco target taco hispanic salsa  hispanic wet  pet approximate posterior probability CS EFE model basket item broken unrelated cluster basket item item target mexican target posterior probability item mexican probability pet vice versa similarity embed vector obtain model consistent genre similarity compute similarity conclusion suggests context item actually relation movie evaluation posterior insight variational posterior distribution model heterogeneous basket contains item mexican pet related basket item compute variational distribution output neural network target item basket intuitively mexican item probability target item similarly pet CS EFE model basket data report approximate posterior probability query item probability item target contribution context conclusion standard exponential embeddings EFE model vector representation fitting conditional distribution context subset context improve performance subset truly related model consequence embed vector reflect occurrence relation fidelity embed model formulate context selection bayesian inference hidden binary vector context inference due develop inference algorithm leverage amortization stochastic gradient binary context selection vector challenge amortize inference algorithm address binning technique model datasets application domain superiority EFE model direction explore improve performance propose context selection exponential embeddings CS EFE apply context selection technique text data target likely context combine context selection technique context hopefully representation explore variational inference scheme rely improve inference network capture complex variational distribution