introduce loss metric inspire lowe criterion sift propose loss maximizes distance closest positive closest negative batch complex regularization shallow convolution network architecture apply novel loss LNet cnn architecture compact descriptor hardnet dimensionality sift performance baseline stereo patch verification instance retrieval benchmark introduction computer vision task rely local correspondence image retrieval panorama stitch baseline stereo 3D reconstruction despite attempt replace complex classical pipeline model image camera localization classical detector descriptor local patch due robustness efficiency tight integration moreover reformulate task complex pipeline differentiable highly challenge towards craft descriptor sift detector replace   however descriptor gain popularity practical application despite performance patch verification task recent confirm sift variant RootSIFT pca dsp sift significantly outperform descriptor image retrieval 3D reconstruction conclusion local patch datasets diverse quality widely applicable descriptor focus descriptor novel convolutional neural network cnn hardnet additionally descriptor significantly outperforms craft descriptor task image retrieval extreme training standard patch correspondence data available datasets sufficient beyond related classical sift local feature consists distance ratio threshold filter false positive conference neural information processing beach CA usa knowledge local descriptor fully mimic strategy objective simonyan zisserman propose filter plus pool scheme convex optimization replace craft filter  sift propose  siamese architecture embed patch similarity latter network improve performance prevent approximate algorithm  komodakis independently siamese explore convolutional architecture simo harness  mining relative shallow architecture exploit similarity  classical sift scheme triplet margin loss triplet distance loss random sample patch triplet superiority triplet architecture although unlike sift sample negative randomly calculate distance matrix mining positive negative pairwise contrastive loss batch generate negative sample distance truth matchings minimum constraint distance distance ratio enforce instead propose penalty correlation descriptor dimension adopt supervision intermediate feature performance adopt LNet architecture descriptor powerful descriptor significantly simpler objective without auxiliary loss batch input patch descriptor distance matrix      triplet batch   propose sample procedure patch described network distance matrix calculate closest non descriptor patch positive respectively finally negative candidate hardest chosen operation pas propose descriptor sample loss objective mimic sift criterion batch local patch generate anchor positive patch correspond 3D batch exactly originate 3D patch network pairwise distance matrix   calculate denote descriptor patch respectively closest non descriptor respectively anchor descriptor positive descriptor  closest non descriptor  arg minj  closest non descriptor  arg mink quadruplet descriptor   triplet     otherwise goal minimize distance descriptor closest non descriptor triplet distance fed triplet margin loss max min   min   pre compute triplet construction distance matrix calculation gpu overhead random triplet sample distance matrix calculation calculate minimum moreover triplet scheme cnn memory consumption computation unlike neither supervision intermediate layer constraint correlation descriptor dimension experienced significant fitting model architecture conv pad BN relu conv pad BN relu conv pad BN relu conv pad BN  conv pad BN relu conv BN  conv pad BN relu architecture network adopt LNet convolutional layer batch normalization relu dropout regularization convolution layer hardnet architecture identical LNet pad zero apply convolutional layer preserve spatial pool layer decrease performance descriptor spatial reduce stride convolution batch normalization layer relu non linearity layer dropout regularization dropout rate apply convolution layer output network normalize descriptor grayscale input patch pixel normalize per patch per patch standard deviation optimization stochastic gradient descent rate momentum decay rate linearly decayed zero within epoch training pytorch library model training ubc  dataset consists subset liberty    normalize patch keypoints detect  detector verify 3D model consists non sequence setup descriptor subset others metric false positive rate fpr positive recall michel keller evaluation procedure report fdr false discovery rate instead fpr false positive rate avoid  fpr fdr rate estimate comparison propose descriptor outperforms competitor training augmentation without multiscale patch sample surround architecture architectural choice beyond scope already surround consistently improves dataset descriptor hurt performance realistic setup oxford affine dataset descriptor liberty sequence comparison TFeat LNet dataset training patch correspondence verification performance dataset report false positive rate positive rate fpr report false discovery rate fdr instead fpr due bug source code consistency fpr obtain article estimate fdr marked bold training   liberty  liberty  liberty   fdr fpr sift  TFeat LNet hardnet augmentation flip random rotation gloss  LNet hardnet explore batch influence influence mini batch descriptor performance mini batch beneficial faster convergence generalization batch gpu utilization loss function benefit negative patch distinguish positive patch report batch model described liberty sequence dataset model performance improves increase mini batch harder negative although increase batch significant benefit empirical evaluation recently performance patch verification task dataset performance setup vice versa therefore extensively evaluate descriptor task image retrieval RootSIFT TFeat LNet comparison descriptor variety datasets epoch fpr influence batch descriptor performance metric false positive rate fpr positive rate average   validation sequence distractors  hardnet hardnet LNet LNet RootSIFT sift TFeat patch retrieval descriptor performance  distractors evaluate HPatches dataset patch descriptor evaluation HPatches recent dataset local patch descriptor evaluation consists sequence image dataset split viewpoint sequence significant viewpoint illumination sequence significant illumination artificial keypoints detect  hessian harris detector reference image reprojected image sequence geometric easy tough variant HPatches benchmark defines task patch correspondence verification image patch retrieval refer reader HPatches detailed protocol task LNet hardnet performance patch verification task advantage hardnet task non augment version hardnet outperforms augment version LNet noticeable margin difference tough setup illumination sequence challenge geometric descriptor network TFeat architecture propose loss function denote  outperforms version retrieval par patch verification task patch retrieval relative performance descriptor hardnet LNet descriptor significantly outperform previous superiority cnn architecture shallow TFeat model easy tough     patch verification    TFeat LNet hardnet LNet hardnet image   TFeat  LNet LNet hardnet hardnet patch retrieval   TFeat  LNet LNet hardnet hardnet verification retrieval HPatches dataset marker indicates geometrical easy tough marker indicates experimental setup   source negative verification task   sequence none descriptor HPatches comparison loss function sample strategy HPatches task  report cpr regularization penalty correlation descriptor channel propose negative mining perform per epoch bold hardnet hardest batch sample triplet margin loss sample loss  triplet margin contrastive random overfit negative mining overfit random cpr negative mining cpr hardest batch another patch retrieval distractors non patch retrieval dataset TFeat descriptor performance comparable LNet presence distractors degrades quickly database grows performance sift explains TFeat performs relatively poorly oxfordk  benchmark around distractors respectively detail performance hardnet decrease slightly augment version difference  descriptor grows increase complexity task ablation understand significance sample strategy loss function conduct summarize hardnet model architecture exactly LNet model parameter evaluate impact sample strategy random propose hardest batch classical negative mining epoch closest negative training loss function  distance triplet margin margin contrastive margin maximum distance normed descriptor  HPatches task propose hardest batch clearly outperforms sample strategy loss function hardnet performance random sample classical negative mining overfit training loss performance varied behavior loss function random sample report negative mining hardest training surprising due dataset label negative actually positive visual inspection confirms triplet margin  contrastive contrastive contribution gradient magnitude positive negative horizontal vertical distance anchor negative positive respectively  loss gradient quickly decrease unlike triplet margin loss contrastive loss negative contribute zero gradient triplet margin loss contrastive loss margin behave similarly reasonable random negative mining sample additional correlation penalty descriptor channel cpr propose regard loss function  stable across sample strategy marginally outperform contrastive triplet margin loss strategy explanation triplet margin loss contrastive loss margin constant non zero derivative positive negative sample contrastive loss margin negative optimization zero derivative  derivative become distance positive negative baseline stereo validate descriptor generalization ability extreme WBS dataset consists image extreme image appearance difference appearance due seasonal occlusion etc geometry difference camera illumination significant difference intensity wavelength source sensor difference sensor data IR mri moreover local feature WBS dataset detect  hessian affine implementation  detector local structure   patch training descriptor another significant difference HPatches setup absence geometrical patch perfectly reprojected target image protocol HPatches task hardnet LNet perform comparably former perform image geometrical appearance latter  visible infrared outperform sift margin however significant amount domain shift descriptor perform TFeat loses badly sift  significantly outperforms TFeat descriptor WBS dataset superiority propose loss performance patch verification task automatically performance image register therefore descriptor baseline stereo setup metric successfully image average inliers per matcher comparison protocol protocol orb detector descriptor remove sift replacement descriptor focus EF extreme oxford affine datasets saturate descriptor image  average nuisance factor  TFeat TFeat sift RootSIFT LNet LNet hardnet hardnet descriptor evaluation WBS patch dataset precision recall curve report denote nuisance factor appearance viewpoint geometry illumination sensor  satellite photo hardnet slight advantage inliers per image datasets  gdb   image domain photo domain photo hardnet outperforms descriptor par craft RootSIFT hardnet domain domain scenario therefore generalization ability comparison descriptor baseline stereo within  matcher baseline stereo datasets image average inliers report header corresponds image dataset EF    gdb   descriptor inl inl inl inl inl inl inl RootSIFT TFeat LNet hardnet image retrieval evaluate related practical application image retrieval local feature standard image retrieval datasets evaluation oxfordk  datasets datasets image oxfordk  depict landmark distractors landmark query define bound constitute query per dataset performance report average precision  image dataset multi hessian affine feature extract exactly feature described related descriptor per feature approximate visual vocabulary independent dataset evaluate oxfordk vocabulary descriptor  vice versa descriptor dataset assign correspond vocabulary finally image histogram visual occurrence bag bow representation invert file efficient additionally spatial verification SV standard query expansion QE rank refine comparison related patch description hardnet LNet perform comparably across datasets setting slightly performance hardnet average across performance  evaluation bag bow image retrieval vocabulary consist visual independent dataset evaluate oxfordk vocabulary feature  vice versa SV spatial verification QE query expansion highlight bold descriptor sift hardnet liberty sequence dataset hardnet union HPatches datasets oxfordk  descriptor bow bow SV bow QE bow bow SV bow QE TFeat RootSIFT LNet hardnet hardnet hardnet performance  comparison image retrieval local feature vocabulary independent dataset evaluate oxfordk vocabulary feature  vice versa spatial verification query expansion VS vocabulary SA assignment multiple assignment highlight bold oxfordk  VS SA SA sift bow sift bow  RootSIFT  hardnet  average  RootSIFT perform descriptor image retrieval average  across hardnet version available training data union HPatches datasets instead liberty sequence hardnet benefit training data perform setup finally descriptor image retrieval approach local feature fairness local feature detector described vocabulary independent dataset spatial verification SV query expansion QE hardnet  visual vocabulary visual additional ham embed technique refines descriptor assignment binary signature procedure RootSIFT  replace RootSIFT hardnet descriptor specifically vote decrease function ham distance burstiness suppression multiple assignment feature visual QE feature aggregation parameter performance report oxfordk  vocabulary independent dataset  report oxfordk dataset comprise relevant image amount feature  report oxfordk twice local feature conclusion propose novel loss function local image descriptor relies negative mining within mini batch maximization distance closest positive closest negative patch propose sample strategy outperforms classical negative mining random sample  triplet margin contrastive loss descriptor compact dimensionality sift stateof performance standard patch verification retrieval benchmark compute gpu training source code convnets available http github com  hardnet