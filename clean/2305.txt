greedy coordinate descent gcd algorithm regularize improve gcd popular strategy nesterov acceleration stochastic optimization firstly norm approximation propose greedy selection nontrivial convex efficient algorithm  thresholding projection SOTOPO propose exactly regularize norm approximation induced SOTOPO algorithm nesterov acceleration stochastic optimization strategy successfully apply gcd algorithm algorithm accelerate stochastic greedy coordinate descent ASGCD optimal convergence rate meanwhile reduces iteration complexity greedy selection factor sample theoretically empirically ASGCD performance dimensional dense sparse introduction convex optimization widely due cheap iteration improve convergence rate reduce iteration important strategy nesterov acceleration stochastic optimization nesterov acceleration refer technique algebra trick accelerate firstorder algorithm stochastic optimization refer sample training dual coordinate random training data iteration assume objective function convex smooth minx optimal approximate satisfies vanilla gradient descent iteration apply nesterov acceleration scheme accelerate gradient AFG iteration optimal algorithm meanwhile assume finite sum sample convex function sample training stochastic gradient descent sgd variant reduce iteration complexity factor sample alternative sgd randomize coordinate descent rcd reduce iteration complexity factor sample obtain optimal convergence rate nesterov acceleration development gradient descent rcd nesterov acceleration stochastic optimization strategy improve exist algorithm national foundation china grant conference neural information processing beach CA usa partly coordinate descent gauss  selection greedy coordinate descent gcd gcd widely sparse optimization machine optimization sparse suitable counterpart rcd however theoretical convergence rate meanwhile iteration complexity comparable gcd preferable rcd however gauss  selection compute gradient beforehand gcd iteration complexity rcd concrete nonsmooth regularize min def  def  regularization parameter smooth convex function finite average smooth convex function sample def regularize empirical risk minimization erm lasso exp  regularize logistic regression obtain nonsmooth gauss  variant GS GS GS gcd algorithm procedure iteration quadratic approximation minimizes surrogate objective function constraint direction vector update nonzero entry easy nonconvex due cardinality constraint direction vector nesterov acceleration scheme convexity derivation optimal convergence rate therefore impossible accelerate gcd nesterov acceleration scheme exist propose novel variant gauss  norm approximation quadratic approximation involves regularize norm approximation nontrivial convex exactly challenge propose efficient  thresholding projection SOTOPO algorithm SOTOPO algorithm complexity counterpart  euclidean projection SOTOPO accelerate gcd attain optimal convergence rate comb  mirror descent meanwhile compute gradient beforehand sample training compute noisy gradient gradient perform greedy selection stochastic optimization technique reduces iteration complexity greedy selection factor sample accelerate stochastic greedy coordinate descent ASGCD algorithm assume optimal assume smooth   satisfies ASGCD  iteration function varies slowly upper bound dimensional dense sparse ASGCD performance demonstrate theoretical notation denote denote nonnegative  denote norm kxk maxi denote norm vector dim denote dimension denote gradient vector denote denote cardinality denote simplex SOTOPO algorithm propose SOTOPO algorithm aim propose minimize regularize norm approximation def arg min kgk  def denotes iteration variable optimize director vector update iteration nonzero entry denotes coordinate update iteration unlike quadratic approximation GS GS GS coordinate update implicitly sparsity induce norm kgk cardinality constraint kgk nonzero nonsmooth  exist minimizer norm approximation norm steepest descent equivalent gcd  exists generally coordinate update sparsity induce kgk direction vector iterative sparse addition unconstrained feasible variational reformulation involves  nonsmooth kgk nonsmooth nonsmooth directly variational identity kgk  lemma transform  nonsmooth separable smooth optimization simplex lemma define def  def arg ming def def arg  vector function minimization equivalent relation meanwhile coordinate separable expression def  max   def lemma obtain iterative thresholding operator lemma reformulate parameter joint convexity swap optimization fix optimize respect vector function substitute finally optimal obtain explicit expression substitute lemma  constant piecewise structure deduce SOTOPO algorithm  replace minimization convention lemma assume compute denote def def belongs although formulation complicate summarize lemma corollary corollary  non decrease continuous function corollary trivial whichever belong consistent iterative procedure formulation mainly impact criterion SOTOPO optimal lagrangian def lagrange multiplier vector non negative lagrange multiplier due coordinate separable kkt  reformulate kkt lemma lemma stationary optimal meanwhile denote def def kkt formulate maxj lemma lemma beforehand compute simply apply equation therefore optimal equivalent nonzero thresholding projection algorithm lemma negative  variable therefore simpler obtain coordinate positive glance identify coordinate potential subset coordinate clearly exponential dimension however clarify lemma enables efficient procedure identify nonzero lemma derive procedure identify non zero lemma nonzero identification optimal coordinate equivalently lemma sort def uid permutation lemma obtain θij θij compute therefore lemma efficiently identify nonzero optimal sort operation however lemma sort reduce lemma lemma efficient identification assume lemma max lemma filter coordinate satisfy maxj lemma propose  thresholding projection SOTOPO algorithm alg efficiently obtain optimal lemma quantity lemma sort lemma quantity criterion met alg nonzero compute lemma accord optimal correspond algorithm SOTOPO def maxi arg maxi def sort permutation denote def def def denote quantity def min  lemma  otherwise lemma correspond obtain theorem SOTOPO algorithm theorem SOTOPO  alg minimizer regularize norm approximation SOTOPO algorithm complicate indeed efficient dominant operation alg complexity reduction lemma proposition optimization define regularization parameter max max assume define alg proposition max max therefore coordinate satisfy  considerably reduce sort complexity remark SOTOPO extension  algorithm objective function euclidean distance function lemma counterpart objective function euclidean distance addition extension randomize median algorithm linear deserve research due limited discussion ASGCD algorithm motivation accelerate gcd obtain optimal convergence rate nesterov acceleration reduce complexity greedy selection stochastic optimization although coordinate descent algorithm propose minimize performs update coordinate generalize proximal gradient descent norm therefore apply exist nesterov acceleration stochastic optimization framework katyusha efficiently accelerate stochastic greedy coordinate descent ASGCD algorithm described alg algorithm ASGCD randomly sample mini batch probability  SOTOPO   output algorithm   max  output alg gradient descent propose SOTOPO algorithm mirror descent  algorithm norm divergence sec denote mirror descent  alg standard katyusha framework parameter setting instead custom choice define alg minimize varies slowly upper bound meanwhile depends extra constant furthermore define finally unlike alg batch algorithm parameter stochastic deterministic knowledge exist gcd algorithm deterministic therefore exist gcd algorithm efficient SOTOPO algorithm ASGCD nearly iteration complexity standard alg katyusha meanwhile convergence rate theorem convex smooth optimum regularize ASGCD satisfies   alg ASGCD achieves additive error  iteration convergence rate exist algorithm ASGCD regularize acc non acc denote correspond algorithm nesterov accelerate respectively primal dual denote correspond algorithm solves primal regularize dual respectively norm norm denote theoretical guarantee norm norm respectively norm guarantee katyusha approx convergence rate  norm guarantee gcd convergence rate  applicable smooth generalize GS GS GS generally theoretical guarantee gcd bound ASGCD  accelerate version norm guarantee  meanwhile bound depends define erm sample dimensional dense regularization parameter relatively extreme norm guarantee  ASGCD norm guarantee  katyusha approx finally factor bound ASGCD  analysis deserves research remark batch ASGCD deterministic algorithm smooth constant satisfies  remark necessity compute gradient beforehand bottleneck gcd application exists avoid computation gradient perform approximate greedy selection preprocessing incoherence convergence rate regularize empirical risk minimization gcd convergence rate apply algorithm convergence rate non acc primal norm saga  acc primal norm katyusha  acc acc sdca  dual  norm  approx non acc primal norm gcd  acc primal norm ASGCD  dataset somewhat complicate contrary propose ASGCD algorithm reduces complexity greedy selection factor amortize simply apply exist stochastic variance reduction framework numerical demonstrate theoretical empirical performance ASGCD batch deterministic version denote ASGCD ASGCD respectively addition data access cpu recent sgd rcd literature data access algorithm access data matrix algorithm performance nesterov acceleration ASGCD non accelerate greedy coordinate descent GS coordinate gradient descent  nesterov acceleration stochastic optimization strategy ASGCD katyusha alg propose norm approximation ASGCD norm proximal accelerate gradient AFG implement linear couple framework meanwhile benchmark stochastic optimization finite sum structure performance proximal stochastic variance reduce gradient svrg addition katyusha alg empirical performance regularize therefore algorithm  accelerate sdca datasets obtain libsvm data summarize algorithm lasso min    datasets sample vector feature vector prediction vector characteristic datasets dataset  FEATURES leukemia gisette mnist ASGCD katyusha alg tight smooth constant maxj maxj  respectively implementation  gisette mnist loss  AFG ASGCD svrg katyusha ASGCD loss loss loss loss loss  ASGCD  svrg AFG katyusha lasso gcd AFG smooth constant maxi  kak respectively rate  svrg tune factor rate  gisette mnist addition dataset AFG optimum accuracy performance algorithm plot loss axis axis denotes algorithm access data matrix ASGCD access iteration ASGCD access twice entire outer iteration dataset compute rate     acceleration ASGCD non accelerate  algorithm ASGCD katyusha ASGCD AFG   gisette ASGCD dominates katyusha alg ASGCD dominates AFG theoretical analysis relatively around ASGCD alg alg AFG consistency demonstrates theoretical analysis