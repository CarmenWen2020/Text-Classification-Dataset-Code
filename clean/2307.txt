develop slim accurate neural network become crucial realworld application employ embed previous along research promising exist fail significantly compress network retrain prune network boost prediction performance propose layer wise prune neural network propose parameter individual layer prune independently derivative layer wise error function respect correspond parameter prediction performance prune bound linear combination reconstruct error layer layer wise error properly perform retrain prune network resume prediction performance conduct extensive benchmark datasets demonstrate effectiveness prune baseline code release http github com  OBS introduction intuitively neural network approximate predictive function arbitrary complexity amount parameter layer neuron neural network tremendously increase lenet parameter vgg parameter parameter model memory intensive computationally expensive urge researcher dig redundancy neural network neuroscience recent significant redundant neuron brain memory relation  specific synapsis machine theoretical analysis empirical evidence redundancy model therefore compress neural network without loss prediction prune parameter carefully criterion however optimal prune NP prune exponential parameter recent mainly focus develop efficient algorithm obtain optimal prune exit approach parameter prune criterion increase training error magnitude parameter etc exist prune criterion conference neural information processing beach CA usa heuristically guarantee prediction performance neural network preserve prune therefore consume retrain usually boost performance trim neural network instead consume effort network layer wise prune net trim propose sparse parameter minimize reconstruct error individual layer theoretical analysis overall performance network bound sum reconstruct error layer prune network theoretical guarantee error however net trim adopts norm induce sparsity prune fails obtain compression ratio propose layer wise prune neural network aim achieve goal layer parameter highly compress prune reconstruct error theoretical guarantee overall prediction performance prune neural network reconstruct error layer network prune retrain resume prediction performance achieve goal borrow classic prune approach shallow neural network optimal brain damage OBD optimal brain surgeon OBS classic approximate error function via functional taylor series identify unimportant derivative approach proven effective shallow neural network remains challenge extend neural network computational compute derivative inverse hessian matrix parameter restrict computation derivative parameter individual layer hessian matrix parameter specific layer computation becomes tractable moreover utilize characteristic propagation fully layer network reduce computational complexity inverse operation hessian matrix achieve goal theoretical proof bound performance prune reconstruct error layer layer wise prune framework derivative trim parameter layer empirically significantly prune parameter prediction performance prune therefore retrain resume performance achieves goal contribution summarize propose layer wise prune neural network significantly trim network preserve prediction performance network prune theoretical guarantee addition propose consume retrain boost performance prune network  conduct extensive verify effectiveness propose approach related preliminary prune widely model compression neural network neural network relatively training data prune crucial avoid overfitting classical OBD OBS aim prune parameter increase error approximate derivative however computation hessian inverse parameter expensive OBD hessian matrix restrict diagonal matrix computationally tractable however approach implicitly assumes parameter interaction hurt prune performance OBD OBS hessian matrix prune obtains performance computationally expensive woodbury matrix identity iterative compute hessian inverse OBS vgg naturally compute inverse hessian matrix regard prune model propose delete unimportant parameter magnitude absolute retrain remain recover prediction performance achieves considerable compression ratio however pioneer research parameter magnitude absolute error therefore magnitude approach eliminate parameter prediction performance prune robustness retrain variant magnitude criterion significant prediction performance prune remains avoid prune parameter introduce mask matrix network connection dynamically prune gradient decent propose iterative thresholding approach activate prune parameter prune phase besides net trim layer wise prune previous propose induce sparsity rank approximation layer prune however norm norm sparsity induced regularization increase difficulty optimization prune neural network obtain compression ratio prune retrain network prevent accumulation error optimal brain surgeon propose layer wise prune extension OBS neural network briefly review OBS network parameter local minimum error functional taylor series error   denotes perturbation correspond variable hessian matrix parameter  network local minimum error vanishes  ignore OBS goal parameter zero denote scalar minimize prune iteration resultant optimization min  vector otherwise optimization lagrange multiplier computation bottleneck OBS calculate non diagonal matrix inverse impractical prune model usually parameter layer wise optimal brain surgeon statement training instance neural network layer exclude input layer denote input output neural network respectively layer denote input output layer respectively representation layer  pas matrix parameter layer activation function convenience presentation proof define activation function rectify linear relu denote vectorization neural network fix matrix information neural network goal prune zero layer wise error layer wise prune layer input fix network suppose denote zero parameter vector denote  obtain output layer denote Yˆ simplicity presentation suppose neural network fully network extend filter layer convolutional neural network error Yˆ training data layer wise error vuut  frobenius norm parameter prune compute error prune criterion adopt exist however parameter layer pas training data compute error computationally expensive efficient approach derivative error function identify importance parameter define error function   outcome sum operation perform activation function layer neural network  outcome sum operation prune layer desire output layer activation lemma layer wise error bound error define lemma error function  therefore parameter deletion zero minimizes translate parameter deletion minimizes error function error function approximate functional taylor series       denotes perturbation correspond variable hessian matrix  proven error function define linear  suppose aim parameter zero  minimal OBS formulate optimization min    vector otherwise lagrange multiplier obtain optimal parameter prune resultant minimal error function   refer sensitivity parameter parameter prune sensitivity instead magnitude mention magnitude criterion merely numerator estimation sensitivity parameter moreover inverse hessian matrix training data involve capture data distribution sensitivity parameter prune parameter sensitivity parameter vector update via   lemma layer wise error layer bound    equality obtain worth mention merely focus layer hessian matrix matrix however significantly reduce computation layer layer wise error propagation accumulation prune parameter layer estimate introduce error independently however aim consistence network output  prune layer wise error propagate output layer accumulate error multiple layer explode theorem prune network via layer wise prune introduce layer layer wise error accumulate error ultimate network output  obeys     Wˆ denotes accumulate prune output layer Wˆ theorem layer wise error layer continued multiplication parameter frobenius norm layer propagates output layer layer error ultimate network output bound sum layer wise error proof theorem appendix parameter sensitivity layer prune prune operation finally QL    ultimate network output error worth mention although layer wise error factor QL   propagates layer tractable ultimate network output factor output layer easily estimate norm ultimate network output via   prune operation layer layer wise error relative ultimate output error    relative ultimate output error  controllable network adopt maxout layer ultimate output actually network gain ratio magnitude network output magnitude network input propose algorithm prune fully layer selectively prune parameter approach compute inverse hessian matrix layer sensitivity parameter layer computationally expensive tractable efficient algorithm reduce hessian matrix computation inverse layer accord definition error function lemma derivative error function respect    matrix  respectively hessian matrix define   simply ignore  stage prune difference ignore correspond layer output  hessian matrix calculate via xml illustration hessian neural network activation via propagation hessian matrix parameter denote illustrate zero correspond denote importantly diag compute obtain significantly reduces computational complexity hessian matrix instance layer diagonal matrix specifically gradient output  layer output activation function gradient simply calculate importantly output gradient layer input otherwise illustrate ignore script simplicity presentation diagonal matrix diagonal  inverse hessian matrix diagonal matrix diagonal addition normally degenerate pseudo inverse calculate recursively via woodbury matrix identity reduce computational complexity calculate estimate minimal error function optimal layer wise hessian matrix layer wise hessian matrix correspond layer input prune operation parameter layer wise error prune inflection layer wise error dramatically user incrementally increase prune parameter sensitivity prune ratio performance tolerable error threshold prune ratio procedure prune algorithm fully layer summarize layer input network calculate hessian matrix  pseudo inverse dataset pseudo inverse hessian matrix compute optimal parameter  sensitivity parameter layer tolerable error threshold parameter sensitivity prune parameter parameter via   otherwise prune prune convolutional layer straightforward generalize convolutional layer variant vectorize filter channel fully layer multiple input patch instance vectorized filter channel similarly parameter output fully layer however difference input instance filter slide across extract patch  input volume similarly pixel  dimensional activation response patch corresponds output fully layer hence convolutional layer generalize    diagonal matrix diagonal slightly revise computation hessian matrix extend algorithm fully layer convolutional layer accumulate error ultimate network output linearly bound layer wise error model OBS prune friendly neural network layer wise hessian compute  slight modification however model sizable layer resnet OBS economical computational hessian future verify effectiveness propose layer wise OBS OBS various architecture neural network compression ratio CR error rate retrain iteration retrain resume satisfactory performance CR define ratio preserve parameter parameter conduct comparison OBS prune approach randomly prune OBD lwc dns net trim architecture lenet lenet mnist dataset cifar net cifar dataset alexnet vgg imagenet ILSVRC dataset network apply various prune approach network evaluate performance retrain batch hyper parameter lwc comparison adopt prune related dropout sparse regularizers mnist OBS along technique cifar imagenet overall comparison overall comparison prune layer lenet compression ratio achieve slightly overall compression ratio lwc comparable compression ratio OBS performance retrain lighter retrain lwc performance almost ruin prune classic prune approach OBD hessian matrix model strongly non diagonal besides relative obtain derivative via chain OBD suffers drastic performance directly apply model properly prune layer lenet increase tolerable error threshold relative initial incrementally prune parameter monitor model performance prune encounter prune inflection mention prune layer lenet compression ratio retrain prune model revise alexnet cifar convolutional layer fully layer overall comparison iterative OBS err prune regard prune stage network error CR err prune error iters random lenet OBD lenet lwc lenet dns lenet OBS lenet OBS iterative lenet OBD lenet lwc lenet dns lenet OBS lenet OBS iterative lenet lwc cifar net OBS cifar net dns alexnet err lwc alexnet err OBS alexnet err dns vgg err lwc vgg err OBS iterative vgg err iteration around dns  prune network prune operation report error rate prune network retrain however lwc iteration dns reboot network OBS retrain iteration dns report implement tensorflow addition scenario prune ratio OBS flexibly adopt iterative version performs prune retrain alternatively obtain prune ratio relative prune iteration prune retrain OBS achieve prune ratio dns lighter retrain iteration lenet iteration lenet regard comparison cifar net achieve error dropout batch normalization prune network lwc OBS network architecture lwc retrain rate retrain representation capability prune network parameter damage prune principle parameter important factor representation capability however OBS adopt rate retrain prune network consideration OBS ensures retrain important connection parameter preserve capability representation prune network instead ruin model prune regard alexnet OBS achieves overall compression ratio without loss accuracy intel xeon cpu compute hessian nvidia tian gpu retrain prune model iteration computation hessian inverse OBS negligible retrain analysis complexity mention complexity calculate assume neural network retrain via sgd approximate complexity retrain idm mini batch parameter iteration respectively retrain iteration complexity calculate hessian inverse OBS economic interestingly compression ratio prune retrain OBS compression prune alexnet without substantively impact accuracy prune error without retrain apply OBS vgg parameter achieve promising compression ratio perform prune retrain  twice OBS achieves overall compression ratio without loss compression rate accuracy accuracy OBS resnet compression ratio data sample memory byte net trim memory  OBS  mnist comparison net trim layer wise OBS layer lenet prune error CR prune error CR net trim net trim OBS OBS OBS net trim accuracy intel xeon cpu compute hessian inverse iteration retrain prune model apply OBS resnet knowledge perform prune resnet perform prune layer layer compression ratio compression ratio OBS maintain resnet accuracy compression ratio comparison OBS net trim propose OBS inspire net trim adopts norm induce sparsity conduct comparison net trim network prune formulate layer wise prune optimization    corresponds  OBS due memory limitation net trim prune layer lenet OBS net trim prune error rate CR OBS  net trim addition net trim encounter explosion memory datasets parameter specifically complexity positive semidefinite matrix quadratic constraint net trim optimization 7GB sample mnist illustrate moreover net trim multi layer perceptrons deploy convolutional layer conclusion propose novel OBS prune framework prune parameter derivative information layer wise error function theoretical guarantee overall error reconstruct error layer propose OBS prune considerable parameter performance reduce omit retrain importantly identifies preserve important network prune previous dive neural network