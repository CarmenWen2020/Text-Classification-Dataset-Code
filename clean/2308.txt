propose accelerate geodesically convex optimization generalization standard nesterov accelerate euclidean nonlinear riemannian derive equation obtain nonlinear operator geodesically convex optimization instead linear extrapolation euclidean analyze global convergence accelerate geodesically strongly convex improves convergence rate moreover improves global convergence rate geodesically convex finally specific iterative scheme matrix karcher validate theoretical introduction riemannian optimization min denotes riemannian manifold riemannian metric nonempty compact geodesically convex geodesically convex convex geodesically smooth smooth convex function non convex euclidean convex along manifold global optimization solver convexity convex optimization geodesic metric without attention global complexity analysis topic geometric program convex optimization developed theoretical recognize generate convex function cone theoretic fix optimization algorithm however none global convergence rate analysis algorithm recently global complexity analysis algorithm convex optimization riemannian gradient descent  gradf iterate index  exponential detail rate gradf riemannian gradient correspond author conference neural information processing beach CA usa extend nesterov accelerate gradient descent euclidean nonlinear riemannian introduce nesterov variant convex optimization euclidean euclidean inner nowadays application involve data data accelerate practical theoretical minimize convex function gradient thirty nesterov propose accelerate gradient momentum parameter fix lipschitz constant scheme exhibit optimal convergence rate  convex non strongly convex minimizer contrast standard gradient descent achieve convergence rate improvement relies introduction momentum particularly tune coefficient inspire nesterov momentum development accelerate addition strongly convex nesterov accelerate gradient attains convergence rate standard gradient descent achieve linear convergence rate accelerate nonlinear riemannian convergence rate euclidean counterpart nesterov accelerate motivation challenge zhang sra propose efficient riemannian gradient descent RGD attains convergence rate geodesically strongly convex geodesically convex respectively hence remain gap convergence rate RGD nesterov accelerate nesterov accelerate gradient descent algorithm counterpart nonlinear riemannian standard gradient descent euclidean nesterov accelerate gradient involves linear extrapolation improve convergence rate strongly convex non strongly convex  linear function euclidean counterpart nonlinear  exp  nonlinear function exp inverse exponential  inner detail therefore nonlinear riemannian trivial analogy linear extrapolation although riemannian geometry enable generalization euclidean algorithm mention overcome fundamental geometric hurdle analyze global convergence accelerate contribution mention propose accelerate nonlinear riemannian essence generalization standard nesterov accelerate summarize contribution nesterov accelerate iterative scheme nonlinear riemannian linear extrapolation replace nonlinear operator furthermore derive equation obtain correspond nonlinear operator geodesically strongly convex geodesically convex respectively global convergence analysis accelerate algorithm algorithm attain convergence rate geodesically strongly convex geodesically convex objective respectively finally specific iterative scheme matrix karcher experimental verify effectiveness efficiency accelerate notation preliminary introduce notation definition riemannian geometry detail riemannian manifold smooth manifold equip riemannian metric  denote inner txm norm txm define  metric induces inner structure tangent txm associate geodesic constant curve locally distance minimize txm exponential  txm geodesic unique geodesic exponential inverse exp txm exp geodesic unique shortest   geodesic distance parallel transport txm  vector txm  preserve inner norm     txm geodesic geodesically convex convex equivalent definition formulate  exp gradf riemannian gradient function geodesically strongly convex strongly convex inequality  exp  differential function geodesically smooth smooth gradient lipschitz  exp  accelerate geodesically convex optimization propose acceleration geodesically convex optimization generalization nesterov accelerate euclidean riemannian nesterov accelerate involves linear extrapolation nonlinear riemannian analogy linear extrapolation therefore standard analysis technique nonlinear motivate derive equation bridge gap geodesically  geodesically convex generalize nesterov algorithm propose geodesically convex optimization equation propose replace classical nesterov scheme update geodesically convex optimization riemannian  gradf denotes nonlinear operator obtain propose equation deduce analysis convergence analysis strongly convex convex respectively riemannian gradient descent   nesterov accelerate technique introduce update nesterov scheme difference update update implicit iteration explicit iteration illustration geometric interpretation equation algorithm accelerate strongly convex optimization input initialize compute gradient gradf   output geodesically strongly convex equation respect strongly convex exp gradf exp illustrates geometric interpretation propose equation strongly convex exp  exp vector  parallel transport  sum parallel translation  equation accelerate algorithm geodesically strongly convex algorithm application propose equation manipulate simpler specific equation average symmetric positive definite matrix geodesically convex convex smooth diameter bound maxx variable obtain equation exp  exp  gradf   constant illustrates geometric interpretation propose equation convex exp  accelerate algorithm geodesically convex algorithm lemma nesterov accelerate scheme convex euclidean role convergence analysis nesterov accelerate algorithm   algorithm accelerate convex optimization input initialize compute gradient gradf  kgk kyk   output correspondingly obtain analysis convergence analysis propose equation equation riemannian counterpart lemma strongly convex geodesically strongly convex smooth satisfies equation define exp        convex objective lemma convex convex smooth diameter bound satisfies equation define exp   gradf      proof lemma supplementary convergence analysis analyze global convergence propose algorithm algorithm geodesically strongly convex convex lemma convex smooth sequence algorithm  exp   proof lemma supplementary geodesically strongly convex theorem strongly convex optimal sequence algorithm geodesically strongly convex smooth  define lemma comparison convergence rate geodesically convex optimization algorithm algorithm RGD  strongly convex smooth min convex smooth proof theorem supplementary theorem propose algorithm attains linear convergence rate geodesically strongly convex euclidean counterpart significantly faster non accelerate algorithm geodesically non strongly convex theorem convex sequence algorithm convex smooth diameter bound   define lemma proof theorem supplementary theorem convex objective acceleration improves theoretical convergence rate RGD optimal rate convex setting euclidean detail parameter define application matrix karcher specific accelerate scheme conic geometric optimization matrix karcher specifically loss function karcher symmetric positive definite spd matrix define klog  loss function non convex euclidean geodesically strongly convex inner tangent vector manifold   trace matrix matrix riemannian distance define klog computation accelerate update algorithm compute via equation however specific inner derive simpler loss function inner exp xky gradf PN gradf exp  kuk PN xky proof proof proof supplementary inner riemannian manifold exponential  exp   denotes tangent vector geometry tangent vector express detail sym sym extract symmetric argument sym exp sym  definition exp  exp  sym sym xky therefore exp sym xky equality due inner tangent vector euclidean inner vector reformulate xky obtain numerical perspective approximate exp validate performance accelerate average spd matrix riemannian metric matrix karcher riemannian gradient descent RGD  riemannian BFGS  matrix karcher widely apply application elasticity radar signal image processing medical image geodesically strongly convex non convex euclidean relaxed richardson iteration algorithm approximate joint diagonalization algorithm riemannian stochastic gradient descent  achieve performance RGD data application relatively optimization error report experimental RGD RGD  detail accelerate algorithm initialize arithmetic data iteration dist RGD  dist RGD  iteration dist RGD  dist RGD  comparison RGD  accelerate geodesically strongly convex karcher data vertical axis distance horizontal axis denotes iteration input synthetic data random spd matrix generate technique matrix toolbox matrix explicitly normalize norm report experimental RGD  accelerate data matrix evolution distance karcher iterate dist respect iteration karcher consistently converges faster RGD empirically verifies theoretical theorem accelerate faster convergence rate RGD although  outperforms iteration accelerate converges faster  conclusion propose nesterov accelerate gradient nonlinear riemannian generalization nesterov accelerate euclidean derive equation accelerate algorithm geodesically strongly convex convex optimization respectively theoretical accelerate attains convergence rate standard nesterov accelerate euclidean strongly convex convex finally iteration scheme matrix karcher essence non convex euclidean numerical verify efficiency accelerate extend accelerate stochastic variance reduction technique apply geodesically convex future convex non smooth regularization addition replace exponential mapping computationally cheap retraction correspond theoretical guarantee direction future accelerate scheme non convex optimization riemannian