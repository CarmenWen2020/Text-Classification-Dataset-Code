selective classification technique reject option context neural network dnns technique potentially significantly improve dnns prediction performance trading coverage propose construct selective classifier neural network allows user desire risk classifier reject instance grant desire risk probability empirical cifar imagenet convincingly demonstrate viability possibility dnns mission critical application unprecedented error imagenet classification guaranteed probability almost coverage introduction awareness remains  define concept rudimentary awareness easy grasp ability smarter subfield capability machine selective prediction prediction reject option around motivation selective prediction reduce error rate abstain prediction doubt coverage ultimate manifestation selective prediction classifier equip dial allows precise desire error rate guaranteed probability coverage classifier future task perform predictive model dramatically enhance quality selective prediction autonomous cannot rely advent singularity AI  manage standard machine sometimes errs autonomous network capable respond situation disengage advance alert driver hopefully plenty mission critical application likewise greatly benefit effective selective prediction literature reject option extensive mainly discus rejection mechanism various hypothesis algorithm svm boost reject option rarely context neural network nns nns dnns exist NN rejection model whereby misclassification abstain specify rejection mechanism optimize propose mechanism classification apply carefully threshold maximal neuronal response softmax layer mechanism softmax response SR model useful quantify involve application meaningful appropriate rejection misclassification disengage autopilot conference neural information processing beach CA usa alternative risk coverage selective classification ensemble technique selective confidence rat prediction rejection mechanism typically ensemble statistic however technique presently realize context dnns costly sufficiently ensemble member recently gal  propose ensemble uncertainty dnns bypass ensemble member via sample multiple dropout application pas perturb network prediction randomly monte carlo dropout MC dropout technique mention context selective prediction directly apply viable selective prediction threshold discus classification task goal selective classifier standard classifier rejection function selective classifier guaranteed risk ideal classify sample production desire risk optimal coverage rate reasonable assume optimal performance obtain however simpler neural classifier already goal rejection function guarantee probability desire error rate technique rejection SR MC dropout devise chooses appropriate threshold ensures desire risk classifier confidence desire risk output selective classifier error probability vgg architecture apply cifar cifar imagenet imagenet apply resnet architecture SR dropout extremely effective selective classification cifar datasets mechanism achieve nearly identical however imagenet simpler SR mechanism significantly superior importantly almost desirable risk guaranteed surprisingly coverage unprecedented error imagenet classification guaranteed probability almost coverage standard multi classification feature raw image data finite label distribution classifier function risk EP loss function error label sample empirical risk classifier selective classifier classifier selection function serf binary qualifier selective classifier  prediction iff performance selective classifier quantify coverage risk fix coverage define EP probability non reject selective risk EP clearly risk selective classifier coverage entire performance profile classifier specify risk coverage curve define risk function coverage classifier training sample confidence parameter desire risk target goal selection function selective risk satisfies  probability training sample sample unknown underlie distribution classifier satisfy maximize coverage fix goal selective risk satisfies coverage maximize selection guaranteed risk technique construct selection function guaranteed performance classifier confidence rate function assume anything interpretation rank confidence function indicates confidence prediction confidence prediction concerned goal generate selection function guaranteed performance reminder loss function standard loss function unless explicitly mention otherwise training assume sample unknown distribution confidence parameter desire risk target goal selection function selective risk classifier satisfies define selection function otherwise selective classifier define empirical selective risk respect label sample empirical coverage selection function denote projection selection guaranteed risk SGR algorithm algorithm algorithm receives input classifier confidence rate function confidence parameter target risk training algorithm performs binary optimal bound guarantee risk sufficient confidence SGR algorithm output selective classifier risk bound analyze SGR algorithm lemma tightest numerical generalization bound classifier label sample lemma   distribution classifier error empirical error label sample  equation    emphasize numerical bound lemma tightest analytic bound derive hoeffding inequality concentration inequality approximate numerical bound incur slack whenever triplet infeasible algorithm return vacuous zero coverage algorithm selection guaranteed risk SGR SGR sort accord assume index reflect zmin zmax dlog zmin zmax   dlog lemma zmax zmin output bound selection function projection theorem uniform convergence SGR procedure theorem SGR label sample application SGR procedure dlog selective classifier bound compute SGR ith iteration  pgi  proof sketch random variable accepted ith iteration SGR fix lemma apply project distribution pgi sample smi consist drawn distribution pgi  pgi pgi  sample distribution label SGR sample distribution filter  therefore  pgi  clearly pgi  EP therefore     application union bound completes proof confidence rate function neural network classifier assume unknown distribution confidence rate function previous ideal confidence rate function reflect loss monotonicity obviously cannot ideal confidence rate function useful analyze effectiveness risk coverage curve induced rejection function define risk coverage curve relationship nearly identical risk coverage curve plot confidence rate function ideal empirically extremely effective confidence rate function around NN folklore explicitly mention context reject option function neural network classifier layer softmax denote response output jth confidence rate function define maxj function softmax response SR softmax response treat probability response positive sum author criticize approach purpose ideal confidence rate function coherent rank absolute probability softmax response potentially candidate relative confidence rate familiar rigorous explanation SR intuitively motivate neuron activation depicts average response neuron layer positive false positive mnist dataset qualitatively behavior occurs mnist axis corresponds neuron index layer axis average response average positive boldface highlight response correspond average response false positive evident positive activation response active neuron false positive reflect softmax layer response moreover activation neuron confidence signal arises due numerous detect neuron layer qualitatively behavior deeper layer average response neuron activation mnist dataset positive false negative MC dropout technique recently propose quantify uncertainty neural network estimate uncertainty instance iteration apply dropout fully layer uncertainty variance response neuron correspond minus uncertainty MC dropout confidence rate empirical introduce SR MC dropout confidence rate function define model vgg model cifar cifar imagenet model SR MC dropout confidence rate function induced theorem severely skewed ideal bound selective classifier target risk rejection function risk coverage curve obtain datasets curve obtain compute validation risk coverage evident risk coverage profile SR MC dropout nearly identical cifar datasets imagenet plot curve correspond dash curve task solid curve dataset SR significantly MC dropout task task coverage SR rejection error MC dropout rejection incurs error importantly risk coverage curve selective classification potentially dramatically reduce error datasets due relative advantage SR focus SR rating cifar cifar image net risk coverage curve cifar cifar image net task dash curve task solid  SR MC dropout report SGR routine apply datasets construct probability risk selective classifier datasets risk cifar desire risk risk coverage risk coverage risk bound selective guaranteed risk cifar cifar detail vgg architecture adapt cifar dataset massive dropout exactly described data augmentation horizontal flip vertical horizontal shift rotation sgd momentum initial rate decay multiplicatively rate epoch epoch validation accuracy network basis selective classifier apply SGR algorithm SR confidence rating function training SGR standard cifar validation randomly split consume SGR training reserve bound training approximately sample apply SGR routine desire risk obtain correspond selective classifier risk bound application SGR routine dataset particularly confidence apply selective classifier reserve compute selective classifier risk coverage summarize risk coverage compute selective classifier training risk bound target risk moreover risk bound bound baseline threshold define maximizes coverage error baseline random split bound violate violation relative request finally guarantee  error domain selective guaranteed risk cifar vgg  adapt model cifar apply data augmentation routine cifar  experimental CFAR obtain risk cifar desire risk risk coverage risk coverage risk bound SGR generate tight bound desire target risk bound violate risk dramatically reduce risk moderate compromise coverage architecture coverage easily surpass cifar currently residual network architecture likely residual network architecture obtain significantly selective guaranteed risk imagenet already image net vgg model ILSVRC experimental training approximately SGR classification task summarize respectively implement resnet architecture qualitatively obtain architecture resnet imagenet classification task summarize respectively SGR image net dataset vgg desire risk risk coverage risk coverage risk bound report perform bonferroni correction easily SGR image net dataset vgg desire risk risk coverage risk coverage risk bound SGR image net dataset resnet desire risk risk coverage risk coverage risk bound challenge imagenet vgg resnet architecture selective classifier extremely effective appropriate coverage compromise classifier easily surpasses imagenet surprisingly resnet achieve vgg preserve relative advantage relative vgg conclude remark algorithm selective classifier risk fully guaranteed confidence empirical validate algorithm challenge image classification datasets guaranteed risk achievable immediately practitioner cop mission critical task significant direction research approach neural classifier supposedly optimize risk coverage rejection mechanism extremely effective identify superior mechanism classifier however challenge simultaneously classifier selection function optimize coverage risk selective classification intimately related active context linear classifier explore potential relationship context neural classification selective classification loss importance SGR image net dataset resnet desire risk risk coverage risk coverage risk bound extend technique loss function specifically regression fully false positive false negative rate application classification task risk critical benefit obvious medical application utmost precision rejection handle expert application existence performance guarantee propose essential financial investment application obvious opportunity cherry futuristic application robotic sale representative extremely harmful bot fully understand