hash embeddings efficient continuous vector hash embed interpolation standard embed embed random hash function hash trick hash embeddings token dimensional embeddings vector dimensional vector dimensional representation token fitting embed vector token hash trick pool embed vector hash embeddings easily vocabulary consist token hash embed training perform vocabulary prune training model hash embeddings exhibit performance model regular embeddings across task furthermore parameter embed regular embed standard embeddings embeddings construct hash trick actually hash embed hash embeddings extension improvement exist regular embed introduction contemporary neural network rely loss function continuous model parameter compute gradient training data network data discrete translate continuous textual input distinct dense vector vector jointly model pre corpus beforehand datasets vocabulary easily billion parameter model severe gram token vocabulary pre wordvec vector google vocabulary consist embed moderately dimension parameter billion conference neural information processing beach CA usa embed vocabulary advantage drawback ignore infrequent majority text subset vocabulary zipf ignore anything frequent sometimes preserve text drastically reduce embed vector parameter however task risk remove frequent besides unimportant sometimes task typical training model corpus text logic conversely specialized domain medical rare important remove non discriminative token training model perform efficient feature prune entropy retain token norm reduction vocabulary decrease performance actually avoids fitting increase performance model however prune training algorithm compress embed vector lossy compression technique employ reduce amount memory embed vector quantization vector replace approximation construct sum vector previously centroid online training nuisance feature hash hash function assign token fix bucket embed vector goal hash reduce dimensionality token normally token collide assign bucket multiple token collide vector representation prevents model distinguish token information lose token collide surprisingly obvious improvement feature hash described optimal hash function important token collide however hash function discrete codomain easy optimize gradient training neural network propose article extension feature hash hash function instead hash function trainable parameter hash function token actually combination hash function embed hash embed explain embeddings construct feature hash standard embeddings hash embeddings hash embed efficient hybrid standard embed embed feature hash hash embed advantage described none disadvantage hash embeddings beforehand handle dynamically expand vocabulary hash embed mechanism capable implicit vocabulary prune hash embeddings hash trainable mechanism handle problematic collision hash embeddings perform something quantization instead token codebook token access codebook hash embed typically reduction parameter magnitude bulk model parameter resides embed layer reduction hash function component vector importance parameter hash vector input token illustration hash vector optional concatenate vector importance parameter omit component vector illustration parameter wider ensemble dimensionality vector related propose embed hash occurrence demonstrates correlation embed vector correspond subjective judgement similarity ultimately clever reduction embed occurrence embeddings  mooney multiple embeddings prototype meaning conversely hash treat frequently feature reduce dimensionality bag gram tri gram input feature vector somewhat robust minor difference another approach employ xiao cho input sub individual generally task meaningful representation text input embeddings model increase computational model hierarchical encode technique machine translation input computational hash embeddings construction vector representation token hash embeddings illustrate function component vector token predefined pool component vector combine chosen component vector sum   importance parameter optional vector importance parameter token concatenate  construct hash vector translation token hash vector vector notation denotes concatenation operator  optional token component vector function implement ED token function bucket hash function matrix beforehand enumeration token inconvenient impossible simply hash function importance parameter vector matrix token importance vector mapping implement  Dˆ hash function article Dˆ Dˆ future description construction hash embeddings trainable embed matrix component vector trainable matrix importance parameter vector scalar importance parameter hash function uniformly assigns component vector token trainable parameter hash embed standard embed trainable parameter hash function bucket typically chosen without degrade performance reduction parameter typically description computational overhead hash embeddings instead standard embeddings matrix multiplication matrix importance parameter matrix component vector computational overhead therefore negligible hash embeddings actually marginally faster standard embed vocabulary however embed layer responsible negligible computational complexity model hash embeddings instead regular embeddings difference model furthermore hash embeddings training perform vocabulary prune training reduce training hash function importance parameter fix token hash embeddings equivalent hash trick furthermore component vector hash function identity function hash embeddings equivalent standard embeddings performance difference kera tensorflow backend geforce gtx titan GB memory nvidia geforce gtx 2GB memory performance penalty standard embeddings vocabulary possibly avoid custom embed layer pursue hash theory theorem hash function probability  collides token  approximation  token collision    proof variation birthday hash dimensionality reduction collision unavoidable disadvantage feature hash counteract hash embeddings component vector token hash embeddings independent uniform hash function combination multiple hash function approximates hash function drastically reduces risk collision vocabulary component vector instead token collide token vocabulary reduce approximately exp approximately exp hash function reduce collision token vocabulary usually important task purpose importance parameter implicitly prune unimportant importance parameter reduce collision  exp   important task component vector collide token dimensional subspace span dimensional embed vector hash embeddings consist layer hash layer token simply translate integer hash function collision layer random hash function token collision equation collision cannot avoid collision decrease increase increase vocabulary introduces parameter standard embeddings hash embeddings typical embed parameter saving embed chosen gram model gram model bucket increase increase bucket additional parameter standard embeddings hash embeddings default hash function embed extremely parameter saving benchmark hash embeddings without text classification task data preprocessing evaluate hash embeddings datasets introduce various text classification task topic classification sentiment analysis news categorization datasets balance sample distribute evenly overview datasets significant previous experimental protocol perform preprocessing besides remove punctuation model snippet text convert text sequence gram training sample randomly consecutive gram input input model avoid overfitting entire document input snippet document embed obtain simply embeddings datasets description task AG news english news categorization dbpedia ontology classification yelp review polarity sentiment analysis yelp review sentiment analysis yahoo topic classification amazon review sentiment analysis amazon review polarity sentiment analysis training model minimize entropy stochastic gradient  adam rate patience training data validation data model implement kera tensorflow backend training perform nvidia geforce gtx titan GB memory hash embeddings without standard hash trick embed hash embed hash embeddings importance parameter vector hash function component vector dimension parameter hash embeddings standard hash trick embeddings architecture almost identical gram layer hash bucket embeddings parameter document embed input fully layer softmax activation performance model embed hash embeddings parameter standard embeddings perform standard embeddings across datasets dbpedia standard embeddings perform hash embeddings limit vocabulary frequent gram token uni gram gram token vocabulary embed vector hash embeddings hash function bucket chosen validation maximum standard embeddings chosen validation complex architecture consist embed layer standard hash dense layer hidden relu activation softmax layer batch normalization regularization layer parameter saving without hash embeddings parameter average standard embed complex model actually achieve model described insufficient vocabulary overfitting however model access vocabulary vocabulary therefore explain performance performance difference embed hash embeddings regularize performance layer hash vector importance parameter correspond directly unique correspond absolute importance absolute importance parameter correspond important gram information gram model capture gram model behalf instead gram model ensemble hash embeddings bucket hash embed chosen without severely affect performance bucket typically sufficient obtain performance almost par parameter layer embed embed ensemble model bucket instead model amount parameter training model parallel ensemble particularly useful hash embeddings collision handle effectively importance parameter possibility important suboptimal embed vector model ensemble avoid hash function chosen hash embed ensemble ensemble consist model combine model voting model architecture previous model hidden layer instead model hidden layer diversify model parameter ensemble approximately standard embed model standard embed model vocabulary parameter accuracy datasets without shallow network network hash emb std emb hash emb std emb ensemble AG amazon dbpedia yahoo yelp amazon  yelp  future hash embeddings complementary address vocabulary attractive possibility hash embeddings embed context sensitive model  token function component vector importance parameter hash bucket layer identical component vector importance parameter effectively become indistinguishable model instead token function Dˆ accuracy split bow embed approach complex rnn cnn approach category dataset bolded AG dbp yelp yelp    char cnn char     lstm virt adv net fasttext bow gram gram tfidf hash embeddings  hash embeddings  hash embeddings  ensemble vocabulary importance parameter yelp polarity amazon important token joke lack  instead unimportant token service  style importance parameter severely reduce collision initial finding hash function index importance parameter consistent improvement hash function article vector weigh sum component vector however aggregation simply concatenate weigh component vector dimensional vector equivalent weigh sum orthogonal vector finally pre training lean quality hash vector distribute alternative wordvec vector around GB almost billion parameter conclusion described extension improvement standard embeddings empirical comparison hash embeddings standard embeddings across classification task performance hash embeddings par standard embeddings hash embeddings easily vocabulary hash embeddings without particularly useful online cannot construct training hash embeddings inherent regularize performance standard regularization regularization parameter regularize parameter closer contrast regularization hash embeddings parameter bucket determines regularization parameter model hash embed model article achieve performance previous bag model standard embeddings furthermore datasets performance hash embeddings