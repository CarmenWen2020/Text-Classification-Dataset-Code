latent model become default choice recommender due performance scalability however research primarily focus model user item interaction latent model developed recently achieve remarkable excellent diverse input inspire propose neural network latent model  address recommender unlike exist approach incorporate additional content objective instead focus optimization neural network model explicitly dropout model apply exist latent model effectively capability architecture empirically demonstrate accuracy publicly available benchmark code available http github com  lab  introduction popularity online content delivery service commerce social web highlight important challenge relevant content consumer recommender proven effective task increasingly attention approach building accurate recommender model collaborative filter CF CF prediction individual preference preference information user CF across various domain successful web service netflix amazon youtube CF deliver highly personalize recommendation user majority exist approach CF category model model approach latent model typically prefer choice compact representation data achieve accuracy representation optimize retrieval handle user concentrate latent approach latent model typically apply variant rank approximation target preference matrix preference information available degrade highly sparse setting extreme sparsity occurs preference information available user item personalize recommendation generate incorporate additional content information latent approach cannot incorporate content hybrid model propose combine preference content information however hybrid introduce additional objective considerably complicate inference moreover content objective typically generative model explain content maximize recommendation accuracy recently achieve remarkable computer vision processing  conference neural information processing beach CA usa  network dnn model achieve accuracy virtually feature engineering highly effective model content recommender however recent progress apply CF investigation address propose model address gap approach observation equivalent data preference information hence instead additional objective model content modify procedure explicitly model input approach apply dropout input mini batch dnns generalize input appropriate amount dropout dnn latent model performs comparably significantly outperform model simpler hybrid approach objective function jointly optimize component maximize recommendation accuracy additional advantage approach apply exist latent model enhance capability virtually modification model minimize implementation barrier production environment already latent model detailed description approach empirical publicly available benchmark framework typical CF user item user feedback item preference matrix  preference item user  explicitly user rating dislike etc infer implicit interaction purchase explicit typically contains grade relevance rating implicit binary preference information available   denote user express preference  denote item express preference preference information available formally define user item additionally domain access content information user item item information text audio image video user profile information gender location device etc social medium data facebook twitter etc data highly useful signal recommender model particularly effective sparse setting preference information available apply relevant transformation content information fix feature vector ΦU ΦV denote content feature user item respectively ΦU ΦV content feature vector user item content correspond feature vector goal preference information content ΦU ΦV accurate robust recommendation model ideally model handle stage user item journey stage sparse preference stage define preference profile relevant hybrid latent approach propose address CF popular model collaborative topic regression ctr combine latent dirichlet allocation lda matrix factorization WMF ctr interpolates lda representation WMF preference available recently related approach propose collaborative topic poisson factorization  interpolation architecture replaces lda WMF component poisson factorization collaborative CDL another approach analogous architecture lda replace stack denoising autoencoder  architecture diagram user preference content ΦU input correspond dnns  layer activation concatenate tune network output latent representation Uˆ item handle fashion  Vˆ component optimize jointly propagation fix inference retrieval latent Uˆ Vˆ replace representation model achieve highly competitive performance disadvantage incorporate preference content component objective function highly complex CDL contains objective tune combine addition WMF autoencoder parameter challenge tune model datasets parameter expensive consume formulation model assumes item applicable user online service frequently incorporate user item model handle principle derive analogous model user jointly optimize model however complex objective nearly parameter aim address develop simpler model applicable user item addition CDL approach propose leverage dnns CF earlier approach DeepMusic aim predict latent representation latent model content dnn recently described youtube stage recommendation model input user session recent profile information latent representation item session average concatenate profile information dnn output session dependent latent representation user average item address variable input loose temporal aspect session accurately model user preference recurrent neural network rnn approach propose rnn apply sequentially item item hidden layer activation latent representation model benefit apply architecture CF however investigate sparse performance content information available arguably beneficial scenario due excellent generalization various content propose approach aim leverage advantage latent representation preference feature input user item combine content hybrid dnn model unlike focus primarily user develop analogous model user item model explicitly handle approach architecture model  inference procedure input representation aim develop model handle scenario consequently input model content preference information option directly raw however become prohibitively user item grows instead approach latent representation preference input latent model typically approximate preference matrix rank matrix   latent representation user item respectively dense dimensional rank min performance latent approach CF datasets adequate assume latent representation accurately summarize preference information user item moreover input dimensionality significantly reduces model complexity dnns activation hidden layer directly proportional input advantage input ΦU ΦV user item respectively model architecture joint preference content input propose apply dnn model latent incorporates content preference information formally preference content ΦU input correspond dnns  layer activation concatenate tune network output latent representation Uˆ item handle fashion  Vˆ component preference content input handle complex structure content image directly concatenate preference input raw another advantage split architecture allows publicly available proprietary pre model   training significantly accelerate update layer pre network domain vision model exceed layer effectively reduce training content input compatible preference representation remove  directly apply concatenate input ΦU avoid notation clutter omit sub network denote user item model subsequent training component optimize jointly propagation model fix Uˆ Vˆ retrieval Uˆ Vˆ relevance estimate  Uˆ  model architecture user item component training training aim generalize model preserve accuracy exist hybrid model approach additional objective training model content representation preference available however complicates  balance multiple objective addition training content representation moreover content objective typically generative model explain data instead maximize recommendation accuracy waste capacity model content aspect useful recommendation approach borrow denoising autoencoders training model reconstruct input corrupt version goal model accurate representation input achieve propose objective reproduce relevance input model  ΦU ΦV  Uˆ  minimizes difference input latent model dnn input available objective trivially minimize content identity function preference input desirable apply input dropout stochastic mini batch optimization randomly sample user item compute gradient update model mini batch user item random preference input passing mini batch model model reconstruct relevance without preference input user   ΦU ΦV item   ΦU ΦV training dropout fold dropout encourage model content information without dropout encourage ignore content simply reproduce preference input net balance extreme model learns reproduce accuracy input latent model preference data available generalize dropout hybrid preference content interpolation objective simpler architecture easy optimize additional advantage dropout originally developed regularize model additional regularization rarely deeper complex model algorithm algorithm input ΦU ΦV initialize user model item model dnn optimization sample mini batch apply user dropout ΦU ΦU item dropout ΦV ΦV user transform ΦU  ΦU item transform ΦV  ΦV update convergence output parallel model denoising autoencoders dimensionality reduction analogous denoising autoencoders model reproduce input noisy version dropout fully remove subset input dimension however instead reconstruct actual uncorrupted input minimize pairwise distance reconstruct relevance  Sˆ Uˆ  dimensional goal preserve relative Sˆ model focus reconstruct distance flexibility model entirely latent representation another model objective analogous popular dimensionality reduction model project data dimensional relative distance preserve objective function developed dimensionality reduction drawback objective equation depends input latent model accuracy however empirically objective robust model advantage implement additional parameter tune easy apply datasets mini batch mode NM unique user item sample update network moderate datasets billion significantly easy dnns without fitting performance particularly robust sparse implicit datasets commonly CF binary sparse training mini batch sample raw careful tune avoid oversampling avoid stuck local optimum inference training fix model infer latent representation ideally apply model continuously throughout stage user item journey interaction finally establish preference profile however update latent representation Uˆ preference user infer input preference vector latent model complex non convex objective update latent representation preference non trivial task iterative optimization avoid trick user sum item user interact input latent model retrain formally user generate interaction approximate average latent representation item approximation pas user dnn update representation Uˆ  ΦU procedure continuously data input latent model item handle average user representation distribution representation obtain via approximation deviate input latent model explicitly dropout throughout preference input randomly chosen subset user item mini batch replace equation alternate dropout transformation relative frequency transformation dropout algorithm outline procedure validate propose approach conduct extensive publicly available datasets CiteULike acm recsys challenge dataset datasets chosen content information evaluation implement algorithm tensorflow library conduct server core intel xeon cpu cpu nvidia titan gpu 8GB ram model CF approach WMF ctr DeepMusic CDL described baseline DeepMusic code release respective author extensively tune model optimal hyper parameter DeepMusic modify version model replace objective function equation comparison dnn architecture hidden layer layer DeepMusic model dnn model mini batch fix rate momentum algorithm apply directly mini batch alternate apply dropout inference transforms denote dropout rate batch randomly batch user item batch apply dropout user item batch inference transform procedure across datasets CiteULike CiteULike register user scientific article library future reference goal leverage library recommend relevant article user subset CiteULike data user article user article binary article library otherwise sparse user average article addition preference data article content information title abstract comparison approach vocabulary idf item content matrix ΦV user content available ΦU model evaluation fold fold nearly identical report fold modify evaluation accuracy generate recommendation article user exclude training interaction challenge evaluation model performance evaluation remove subset article training data generate recommendation article CiteULike dropout rate WMF ctr DeepMusic CDL DN WMF DN CDL CiteULike recall fix rank model consistent setup model hidden layer architecture hidden tanh activation performance deeper significantly improve model apply dropout preference input outline apply dropout item preference item content available recall accuracy dropout rate probability accuracy remains virtually unchanged decrease dropout rapidly degrades accuracy steadily increase dropout moreover without dropout performance dropout improves indicates dropout significant gain accuracy achieve without loss datasets validate propose approach apply dropout generalization achieves desire recall verify model conjunction exist latent model version denote DN WMF DN CDL WMF CDL input preference model respectively model preference input dropout rate baseline virtually model WMF objective model exception DeepMusic performs significantly baseline attribute DeepMusic item latent representation function content lack preference information DN WMF DN CDL perform comparably baseline preference information input model significantly improves performance content model DeepMusic moreover suggests aggressive dropout affect performance model recover accuracy input latent model diverse baseline DeepMusic unlike ctr CDL unsupervised semi supervise content component DeepMusic toend supervise representation tailor target retrieval task dnn WMF outperforms baseline improve recall baseline indicates incorporate preference information input training improve generalization moreover WMF apply model effectively capability WMF excellent generalization without affect performance DN CDL improves performance CDL almost without affect recsys acm recsys dataset release acm recsys challenge data collection user interaction career orient social network xing european analog linkedin importantly publicly available datasets contains user item content information enable evaluation user interaction interaction impression click bookmark delete  interaction correspond timestamp addition user access profile information education location similarly item recsys user item recall truncation increment code release author ctr CDL applicable item baseline exclude user evaluation location title tag career related information description data cleaning transform categorical input representation user feature item feature ΦU ΦV respectively interaction data remove duplicate interaction multiple click item deletes collapse remain interaction binary matrix user interact otherwise split data interaction evaluate scenario simultaneously interaction split user item approximately interaction respectively user item training contains interaction user item obtain remove training interaction randomly subset user item goal model handle task simulates scenario online service xing user item daily recommend exist user item rank model model denote DN WMF latent representation WMF training alternate apply dropout inference approximation user item mini batch rate ctr CDL code release respective author item evaluate model item task network architecture user item WMF recsys recall user item dnn architecture tanh activation batch norm layer appropriate dnn architecture conduct extensive increasingly deeper dnns approach pyramid structure network gradually compress input  successive layer architecture fully layer batch norm tanh activation function activation function relu sigmoid significantly model WMF input latent model however WMF cannot apply user item user item recall layer increase layer accuracy task steadily improves additional layer accuracy remains approximately deeper architecture highly useful task layer model recsys baseline perform comparably DeepMusic content model unlikely perform user item respectively DeepMusic perform baseline significantly beating baseline ctr item DN WMF significantly outperforms DeepMusic relative improvement truncation despite DeepMusic layer architecture objective function DN WMF incorporate preference information input model highly important goal user inference interaction increase user inference randomly subset user training interaction training interaction remove user training simulate user incorporate training interaction model chronological inference procedure outline latent representation recall interaction increase WMF apply procedure equation WMF representation model seamlessly transition preference without retrain moreover model WMF input significantly outperform WMF interaction item inference omit training inference approximation achieves desire model transition preference without training excellent generalization conclusion  neural network model recommender  applies input dropout training preference information optimization data model leverage preference content information without explicitly rely excellent generalization scenario moreover unlike exist approach typically complex  objective function objective easy implement optimize  apply exist latent model effectively capability leverage architecture content model empirically demonstrate public benchmark future investigate objective function directly incorporate preference information aim improve accuracy beyond input latent model explore dnn architecture user item model leverage diverse content