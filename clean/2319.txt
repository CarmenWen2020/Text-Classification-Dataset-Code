sequential decision important useful user understand underlie correspond decision however typical reinforcement algorithm seldom information due probabilistic model lda uncover latent text sequential decision model understood variant latent topic model tailor maximize reward connection approximate maximum likelihood estimation lda celebrate algorithm demonstrate text domain propose viable mechanism uncover latent decision obtains reward introduction reinforcement important role sequential decision considerable application however understand examine underlie sequence decision interpretable user insight potentially useful downstream application investigate approach uncover underlie text sequential decision text interactive fiction experimental domain specifically focus choice hypertext literature action characterize decision maker agent observes text document observation text describes observation environment text document action text characterize action observation agent selects action transit immediate reward agent receives terminal reward probabilistic model lda tailor maximize reward decision specially observation text action text characterize topic model variant latent dirichlet allocation lda topic model topic proportion chain model dependency action  wang  deng microsoft research conference neural information processing beach CA usa proportion partially responsible generate immediate terminal reward connection maximum likelihood parameter estimation model algorithm empirically demonstrate propose viable mechanism uncover latent decision obtains performance text contribution contribution seamlessly integrate topic model uncover latent interpretable text sequential decisionmaking contemporary reinforcement model algorithm seldom information due knowledge prior achieve topic model fashion maximize reward related lda variant lda capture observation action text text decision model dependence immediate reward topic proportion supervise topic model chain topic proportion model dependency previous action observation dynamic topic model novelty approach model estimate aim maximize reward optimal policy hence topic model reinforcement algorithm furthermore connection DQN variant text setup previous observation action described challenge representation difference previous author treat observation text markovian contrast model capture partial observability dependence observation text decision dialogue finally choice reward function lda similarity gaussian temporal difference organization describes detail probabilistic model connection algorithm algorithm mirror descent propagation demonstrates empirical performance model conclude discussion future probabilistic model text sequential decision text sequential decision probabilistic model relate variant sequential decision text text episodic task proceeds discrete across episode agent receives text document observation environment observation text agent receives text document describes action agent denote feasible action text action text agent action environment transit immediate reward dynamic reward generation stochastic unknown reveals observation text action text transition agent receives terminal reward reward depends text positive reward ending negative reward goal agent maximize cumulative reward optimally environment observation text action text previous action reward agent policy conditional notation simplicity assume text graphical model representation sequential decision observation topic model topic topic distribution action topic model action topic topic distribution dependence variable consecutive plate observation text observation text document standard notation graphical model shade  topic distribution dirichlet parameter observable estimate feasible detail characterizes reward non zero reward probability action maximizes reward PT discount factor simplicity exposition focus reward nonzero algorithm generalize complexity important RL policy independent simplify setup previous observation action described action described action inherently discrete due exponential complexity respect reinforcement action continuous probabilistic model approach challenge variable observation text action text action reward assume generate probabilistic latent variable model examine latent variable aim uncover underlie sequence decision model related estimation model reward maximization lda model graphical representation model lda depict instance topic model observation text action text chain topic proportion influence topic proportion future capture action detail generative model observation topic model dir denote topic observation text action topic model dir denote topic action text assume topic initial topic proportion dirichlet parameter observation action text respectively lda proceeds sequentially latent variable topic matrix drawn dirichlet distribution hyper parameter emission probability correspond topic similarly define observation text observation topic proportion dir observation text lda lda denotes standard lda generative topic proportion topic latent variable indicates topic action text action topic proportion dir action text lda latent variable indicates topic action exploration policy data collection chosen model greedy policy instead immediate reward generate accord gaussian distribution function variance defer definition parameter connection likelihood compute topic proportion dirichlet parameter WSS WSA  max positive action agent WSS WSA  model parameter besides topic proportion influence correspond chosen action furthermore generate accord dir dir respectively implicitly chain via generative defines joint distribution random variable depict generative episode suppose already episode directly conditional distribution reward observation model discriminative manner prediction reward action agent obtain policy action obtain apply bayes joint distribution define generative denote model parameter WSS WSA  loss function min denotes prior distribution model parameter dirichlet parameter denotes episode KS KA denote topic observation text action text VS VA denote vocabulary observation text action text respectively learnable parameter lda VS KS VA KA KA KS KS KA model predict reward imply policy define appropriate function reward achieve closely resembles effectively policy iterative fashion lda relate lda brief introduction latter reinforcement algorithm optimal policy markov decision MDP described action discount factor furthermore defines transition probability action immediate reward correspond transition policy MDP define probability action action immediate reward optimal policy maximizes reward trt seek optimal policy estimate function define discount reward action optimal policy thereafter satisfies bellman equation max directly optimal action arg maxa solves iteratively transition update action practical text exponentially hence usually approximate parametric function neural network model parameter update maxa  otherwise denotes delayed version model parameter update periodically update understood apply stochastic gradient descent sgd regression loss function target compute prediction define reward function lda model function TU parameter typical RL approach model neural network probabilistic model define reward function max remains function expression conditional expectation definition reward function relationship bellman equation relates reward immediate reward manner bellman equation identification corresponds immediate reward equation bellman equation function define appendix loss function approximate maximum posteriori estimate denote respectively min  regularization dirichlet prior interpret regression estimate function target construct manner therefore optimize discriminative objective variant obtain greedy policy action maximizes function estimate estimate due intractable marginalization latent variable advanced approximation technique markov chain monte carlo MCMC variational inference exploration future mirror descent propagation intuition align action misalign action introduction allows meaning topic observation action algorithm training algorithm mirror descent propagation input replay sgd update rate randomly initialize model parameter interact environment behavior policy episode data randomly sample episode sample episode compute accord algorithm sample episode compute stochastic gradient respect propagation computational graph define algorithm update WSS WSA  stochastic gradient descent update stochastic mirror descent algorithm recursive inference episode input initialization ˆS  compute exp ˆS ˆS ˆS initialization normalization factor compute exp  initialization normalization factor compute ˆS  accord compute TU develop algorithm lda minimize loss function previous variant algorithm reinforcement algorithm propose model model replay widely technique recent stateof specifically consists multiple stage stage agent interacts environment fix exploration policy episode data replay memory discus choice assumption generative model lda objective update estimate model parameter update randomize data stage replay replay agent behavior policy episode update augment multiple stage model parameter previous stage initialization stage therefore focus stage formulate optimization objective function estimate therefore recursion compute introduce algorithm recursive inference mirror descent estimate topic proportion define arg max however intractable instead develop approximate algorithm recursively estimate develop algorithm rely proof defer appendix proposition estimate approximate recursively arg max ˆS arg max  bag vector observation text action text respectively compute ˆS  ˆS  update accord ˆS WSS WSA   becomes decouple sub inference therefore sub mirror descent inference algorithm compute dirichlet parameter overall inference procedure summarize algorithm remark obtain readily estimate approximate conditional expectation estimate agent extract greedy policy action arg maxa function function greedy policy optimal backpropagation training loss stage finite sum episode inside summation depends model parameter via computational graph define algorithm appendix diagram graph therefore model parameter sample episode data compute correspond stochastic gradient propagation computational graph algorithm update stochastic gradient mirror descent detail algorithm appendix gradient formula text evaluate propose model demonstrate interpret decision john machine appendix brief introduction action define feasible action lda setup comparison replay algorithm softmax action selection exploration policy data appendix detail episode data john machine replay amount episode replay update model epoch replay appendix additional experimental detail evaluate performance propose lda model reward receives apply text random initialization summarize standard deviation reward simulator obtain http github com  text average reward standard deviation model task DRRN DQN topic becomes hidden per layer task topic lda DRRN layer DRRN layer DQN layer john machine reinforcement relevance network DRRN propose hidden layer variant DQN network baseline DQN max action DQN perform therefore DQN lda outperforms approach task machine lda DRRN model margin gain lda john lda DRRN approach upper bound reward machine task due stochastic action upper bound reward definition reward ending therefore lda closer upper bound although improvement finally standard online RL setup model update data newly generate episode therefore report evaluate training dataset truthfully reflect actual average reward model proceed demonstrate analysis latent decision episode machine episode player wander shopping mall peak player approach machine insert coin hint player future development player related   player scar combat cardboard    reveals concern  finally overcomes maintains friendship episode receives reward evolution topic proportion active topic observation text action text dominant observation topic action topic episode wander mall action mall respectively surprising episode mall scenario topic related mall quickly player machine afterwards salient observation topic becomes   combat activation machine enters scenario player combat cardboard  towards episode observation topic converse  topic kitchen chat correspond action peak decay action topic relieve peak consistent player chooses overcome chat  appendix observation action text stage finally another observation matrix function compute TU matrix positive negative correlation action topic observation topic matrix observation topic action topic interestingly meaning action topic relieve topic converse  positive contribution reward happens topic rarely activate topic becomes therefore active topic explain performance improvement marginal topic grows active topic observation text action text respectively observation topic combat   suddenly  converse    grab towards      insists wrap  wander mall  shopping peak wrap hang attention action topic relieve  away maybe ability easy  kitchen chat tea  classic  kitchen machine coin insert cloth desk apply action mall alarm machine ignore shot  gaze  observation topic observation topic observation topic observation topic observation topic action topic action topic action topic action topic action topic matrix evolution active topic machine conclusion propose probabilistic model lda uncover latent text sequential decision model latent topic model chain topic proportion interestingly model function immediate reward discriminative lda likelihood closely related approach variant sequential topic model evaluate lda text task demonstrate reward furthermore viable approach latent decision