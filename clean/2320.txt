online reinforcement average reward stochastic SGs SG model player zero sum markov environment transition payoff simultaneously learner adversary propose UCSG algorithm achieves sublinear regret compete arbitrary opponent improves previous regret bound dependency diameter intrinsic related SGs opponent optimistic response learner UCSG maximin stationary policy sample complexity poly gap policy introduction scenario computer network cast multi agent framework multi agent reinforcement MARL target traditional reinforcement RL markov decision mdps widely model agent interaction environment stochastic SGs extension mdps multiple agent simultaneous interaction environment SGs model MARL player zero sum SGs proceed mdps exception player action simultaneously jointly transition probability reward zero sum restricts player payoff sum zero player player maximize reward player minimize amount mdps reward discount  episodic non episodic literature SGs typically setting online offline setting respectively offline learner player centralize manner goal equilibrium optimality player maximin minimax policy sample complexity sample estimate optimality error threshold online learner player arbitrary opponent SGs player action contains null action conference neural information processing beach CA usa learner regret difference benchmark learner reward earn benchmark define reward player optimal policy player stationary response player online algorithm equilibrium simply previous offline sample complexity discount SGs bound heavily chosen discount factor however discount suitable SGs planning finite relevant reward function defines knowledge offline sample complexity bound poly average reward  non episodic error parameter difference algorithm previous player asymmetric role algorithm focus player optimal policy sample efficient resembles strictly extends maximin action stage online aware max algorithm average reward SGs regret bound scenario adopt regret definition significantly improve bound appendix detail another difference algorithm output currently stationary policy stage fix horizon policy input parameter former optimal policy stationary policy technique related RL mdps optimism principle appendix optimism principle built concentration inequality automatically strike balance exploitation exploration eliminate manually adjust rate exploration ratio however import analysis mdps SGs challenge opponent  non stationarity prevents learner freely explore previous analysis relies stationary distribution perturbation analysis useless develop novel replace opponent non stationary policy stationary analysis introduce facilitates technique perturbation analysis technique benefit future analysis concern non stationary agent MARL related topic robust MDP MDP action adversarial reward transition assume robust MDP adversarial choice environment directly observable player SG assume action player however difficulty SG address previous robust MDP recent robust MDP adversarial stochastic action propose  algorithm distinguish scenario environment fully adversarial counterpart transition reward reveal learner learner perform maximin planning however transition reward opponent arbitrary action hinder learner information contribution orthogonal research related SGs mdps adversarially reward function adversarially transition probability assumption difference therefore comparable however indeed viewpoint stochastic preliminary model policy SG tuple denotes player joint action denote initial suppose player player joint action player receives reward player player transition probability simplicity deterministic reward extension stochastic straightforward shorten notation abbreviation without loss generality player assume action policy probability distribution action policy dependent policy denote  stationary policy denote  selects action function joint policy average return player joint policy define reward RT PT average reward limt RT whenever limit exists exists sup inf lim RT initial simply bias vector stationary policy bias vector define coordinate bias vector satisfies bellman equation vector describes relative advantage model joint policy advantage disadvantage policy define difference accumulate reward initial converges difference asymptotically notation span vector define maxi mini therefore model policy induces model MDP regret inevitable dependency optimal policy closely related passage markov chain induced actually max denotes model player joint policy intuitive proof remark notation equation vector matrix vector inequality matrix matrix denote val max min min max probability simplex dimension SGs estimate function matrix equation max min abbreviate vector val denote optimal addition indicator function denote unlike player mdps sup inf definition necessarily attainable moreover player stationary optimal policy setting overview assume proceeds meaningful regret bound sublinear assumption SG model assumption assumption max max  max  assumption max max  min  assumption SG model opponent player lock learner player strategy learner totally avoid however stage learner probability lock linear regret learner therefore assume whatever policy opponent executes learner within bound essentially assumption assumption actually implies policy execute player necessarily stationary remark within average assumption asymptotic regret improve assumption algorithm convergence infinitely assumption define notion diameter specific SG model assumption assumption player optimal stationary policy independent initial simply proof refer theorem appendix setting overview focus training player discus setting online player competes arbitrary player regret define reg offline player player action player maximin policy sample complexity define min stationary policy execute player definition player mdps definition upper bound algorithm optimal explain appendix almost algorithm handle setting online challenge mainly focus online discussion offline summarize theorem theorem assumption UCSG achieves reg DS DS theorem assumption UCSG achieves reg  upper confidence stochastic algorithm UCSG probability probability polynomial algorithm UCSG input initialization phase initialize phase max ptk ptk  update confidence conf  conf  optimistic planning maximin  execute policy reward definition confidence conf  SA conf    min   upper confidence stochastic algorithm UCSG algorithm extends  optimism principle balance exploitation exploration proceeds phase indexed learner policy phase phase fix priori depends statistic observation phase algorithm estimate transition probability empirical frequency  previous phase empirical frequency confidence transition probability transition probability confidence constitute plausible stochastic model model belongs probability player optimistically model optimal stationary policy model finally player executes policy occurrence phase phase reset zero phase optimistic model policy  min max denotes error parameter maximin  LHS define player stationary optimal policy MDP induced roughly min approximate max min error picked optimistically  adversarial opponent extend SG maximin  calculation involves technique extend iteration  player version SG player action remain player action transition kernel reward function admissible transition probability action player action equivalent action simultaneously admissible transition probability confidence suppose extend SG satisfies assumption model embed theorem appendix constant independent initial satisfies bellman equation val bound function constant vector iteration schweitzer transform aperiodic transform optimal policy extend EG maximin  detail maximin  refer appendix summarize lemma lemma suppose model estimate model stationary policy output maximin  satisfy min max min dive analysis assumption establish lemma probability model phase appendix lemma fairly assume analysis analysis assumption import analysis technique player mdps develop technique non stationary opponent model player behavior assume dependent randomize policy assume mapping distribution simply hide dependency inside subscript definition applies abuse notation denote phase algorithm policy notation interchangeably phase decompose regret phase   define min stk min stk stk stk   stationary policy player define later action player arbitrary imaginary exists analysis assumption stationary policy induces irreducible markov chain specify initial clearly non positive optimism bound lemma remains bound bound introduction involve artificial policy stationary policy replaces player non stationary policy analysis replacement constant regret facilitates perturbation analysis regret bound selection principle behavior markov chain induced empirical statistic intuitively define ptk ptk however actual trajectory define policy define phase undefined denominator zero however actually lemma define dsa probability bound regret perturbation analysis establish lemma lemma transition probability accurate denotes transition kernel transition probability accurate otherwise probability PT sbt sketch logic proof assume model frequency action induced empirical frequency induced clearly zero expectation becomes difference average reward markov reward slightly transition probability counterpart player version analysis dominant proportional player directly remark unfortunately player version resort perturbation analysis passage developed appendix lemma theorem appendix bound remark implies assumption guarantee 2D approach lemma analysis define notation assumption stationary policy induces irreducible markov chain unique stationary distribution policy execute denote stationary distribution besides denote ptk phase benign model define 2D lemma phase benign DS probability finally benign phase lemma lemma define ST dsa probability argument suppose communicate assumption policy within executes outperform contradiction player SGs argument response however player uncontrollable policy necessarily lemma phase benign DS dsa probability proof theorem regret proof decomposition bound lemma definition lemma bound DS DS analysis assumption ingredient regret analysis bound span bias vector however approach weaker assumption bound passage arbitrary policy hence adopt approach approximate average reward SG sequence finite horizon SGs assumption approximate multiple average reward SG reward hindsight sum episodic SGs resort bound SGs sample complexity translates regret approximation episodic SGs approximation quantity UCSG analysis horizon episode index episode denote episode episode phase define reward joint policy initial VH  decompose regret phase        VH   VH  VH  VH  VH  VH   denotes player policy episode non stationary incomplete episode phase related tolerance maximin  algorithm  error approximate infinite horizon SG episodic SG possibly initial clearly non positive remains bound lemma azuma hoeffding inequality HT probability lemma assumption  sample complexity regret bound contributor regret corresponds inaccuracy transition probability estimation largely reuse player episodic MDP fix initial distribution lemma episode phase VH VH exceed HS initial episode VH VH HS proof allows arbitrarily non stationary policy phase directly utilize analysis summarize theorem appendix algorithm input input remove without affect bound pac bound arbitrarily theorem lemma HSA probability proof theorem decomposition lemma regret bound TD AH  max DT SA sample complexity offline training define sample complexity player maximin policy offline version UCSG phase player optimistic policy player optimistically player optimistically selects policy specifically model policy obtain another extend iteration extend MDP fix player action extend threshold min min iteration halt selection obtain theorem theorem assumption UCSG achieves DS DS theorem assumption assume max max  min  UCSG achieves dsa algorithm output stationary policy player guarantee offline version UCSG algorithm output stationary policy optimal output policy proof theorem obtain regret DS DS  assumption improve bound asymptotic constant bound inherit player MDP  another weaken assumption max   SG argue assumption cannot sublinear regret online however obtain polynomial offline sample complexity player cooperate explore action