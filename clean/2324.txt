introduce agglomerative hierarchical cluster AHC framework generic efficient effective approach embeds sub  williams LW clustering relies inner instead euclidean distance constrain merge procedure sparsified normalize inner matrix snk AHC sparsified normalize kernel matrix AHC snk AHC scalable classic dissimilarity matrix AHC cluster arbitrary artificial benchmark exemplify theoretical standpoint snk AHC another interpretation classic technique relies concept penalize similarity difference average mcquitty centroid median ward explain distinct average strategy aggregate cluster inter similarity intra similarity feature snk AHC examine sufficient monotonic dendrograms elaborate data matrix approach centroid median underline diagonal translation invariance average mcquitty ward extent snk AHC cluster keywords agglomerative hierarchical cluster  williams formula kernel scalability manifold introduction cluster discover homogeneous cluster differentiate approach another classification scheme upon cluster partition hierarchical cluster output nest partition latter classification binary dendrogram hierarchical cluster advantage cluster firstly dendrogram informative partition insight relationship cluster secondly requirement cluster priori unlike cluster technique focus hierarchical cluster procedure agglomerative divisive former dendrogram fashion whereas latter approach focus agglomerative hierarchical cluster AHC suppose pairwise dissimilarity matrix cluster AHC procedure initializes trivial partition compose singleton iteratively merges closest cluster item grouped AHC merge compute dissimilarity newly exist cluster AHC dissimilarity despite approach literature  williams propose parametric formula LW formula generalizes strategy described LW formula dissimilarity AHC AHC framework due simplicity flexibility research implement program successfully apply domain however AHC suffers important scalability issue respect quadratic memory complexity cubic complexity drawback severely limit application AHC data context aim AHC approach equivalent  extend reduce computational furthermore approach define account geometry data unsupervised approach manifold nutshell contribution focus sub LW formula establish model relies inner instead euclidean distance parametric recurrence equation instead model relies inner encompasses reproduce kernel hilbert RKHS kernel function model kernel matrix agglomerative hierarchical cluster AHC dual AHC euclidean distance dissimilarity AHC framework geometric technique centroid median ward data matrix instead distance matrix contrary graph average mcquitty enjoy AHC enables data matrix approach latter scheme median centroid scheme suffer pathological behavior reversal dendrogram phenomenon iteration dissimilarity cluster becomes dissimilarity previous iteration median centroid non monotonic dendrograms ward modification centroid enables non monotonicity issue latter scheme spirit introduce scheme median solves  median technique propose project data hypersphere shift obtain non negative inner obtain normalize kernel NK matrix interpret similarity matrix satisfy maximal similarity interpret model penalize similarity difference classic technique rely distinct average operation inter similarity intra similarity NK matrix apply sparsification procedure remove  similarity relationship output sparsified normalize kernel snk matrix adjacency matrix sparse similarity graph apply AHC snk matrix constraint cluster merge non null inter similarity approach sparsified normalize kernel matrix AHC snk AHC snk AHC computational AHC AHC memory moreover sparsification enables capture intrinsic geometry data unlike NK matrix snk matrix positive semi definite therefore perspective snk AHC interpret geometrical unlike AHC nevertheless average mcquitty ward snk AHC implicitly hilbert due scheme invariant respect translation diagonal snk matrix interpret snk AHC framework graph theory demonstrate procedure emulates operation employ component undirected graph snk AHC automatically cluster latter component similarity graph illustrate aforementioned AHC snk AHC artificial data addition superiority snk AHC AHC realworld benchmark experimental confirm snk AHC scalable classic AHC snk AHC outperform AHC cluster quality approach efficient effective AHC remainder organize introduce notation useful definition review AHC LW formula introduce AHC model establish inner expression embeds LW sub equation interested feature AHC examine afterward snk AHC dedicate artificial data introduce approach exhibit discus related research finally conclude sketch future notation definition item cluster denote cardinal suppose throughout AHC algorithm input pairwise dissimilarity matrix definition dissimilarity matrix pairwise dissimilarity matrix denote matrix satisfy dab non negativity dab dba symmetry denote subset AHC procedure nest partition denote singleton whereas correspond item cluster cardinal denote fusion merge union denote AHC algorithm iterative procedure denote iteration designate denote exist cluster iteration partition subset denote dissimilarity matrix cluster symmetric matrix satisfy definition AHC algorithm nest partition diagram dendrogram definition dendrogram dendrogram hierarchical cluster denote binary leaf node node corresponds subset distinct node besides node assign non negative height denote node dendrogram cluster adopt notation  node designate singleton correspond subset illustration dendrogram definition monotonic dendrogram dendrogram monotonic distinct node dendrogram monotonic indeed height node leaf reversal definition dendrograms efficient effective generic AHC approach height illustration dendrogram definition sequence merges sequence merges hierarchical cluster denote sequence couple disjoint subset satisfy AHC technique monotonic dendrogram cluster iteration cluster fuse AHC procedure regardless height correspondence dendrograms sequence merges sequence merges correspondence dendrogram definition establish equivalence AHC algorithm definition equivalent dendrograms dendrograms equivalent respective sequence merges identical eventually introduce definition similarity matrix definition similarity matrix pairwise similarity matrix denote matrix satisfy sab non negativity sab sba symmetry saa sab maximal similarity maximal similarity pine AHC dissimilarity AHC review concept AHC firstly introduce LW formula dissimilarity detailed explanation fusion mechanism dendrogram secondly review another equivalent express LW formula formulation relies version dissimilarity framework introduce thereafter inspire latter expression LW formula procedure initialize singleton null height dissimilarity matrix iteration AHC merges couple cluster satisfies arg min cluster fuse dendrogram amend node height partition update dissimilarity cluster cluster compute scheme propose cite linkage linkage average  mcquitty  centroid  median  ward technique graph whereas latter geometric despite numerous dissimilarity LW equation introduce parametric update formula generalizes aforementioned define scalar function triple disjoint subset review definition cite scheme symmetric argument unlike couple cluster dendrograms unweighted arithmetic arithmetic unweighted centroid centroid efficient effective generic AHC approach link link  mcquitty centroid median ward setting LW formula ward argument whatever triple constant linkage linkage mcquitty median likewise constant linkage linkage average mcquitty median concern non null linkage linkage sub LW clustering satisfies linkage technique latter scheme peculiar reduce min max operator respectively due specific feature linkage address algorithm consequently interested LW sub formula wrap sub algorithm pseudo code AHC previous LW sub formula equivalent dissimilarity LW sub formula henceforth suppose vector hilbert moreover assume dissimilarity euclidean distance dab context review another dissimilarity LW sub formula equivalent indeed vector hilbert centroid pine algorithm procedure AHC input dissimilarity matrix AHC output dendrogram initialize leaf cluster accord merge update compute apply correspond AHC parameter ward update equation express cluster representative vector cluster cluster dissimilarity centroid scheme regard ward approach version centroid former scheme AHC iterative procedure equivalent algorithm input dissimilarity matrix euclidean distance data iteration minimum dissimilarity merge arg min function disjoint subset definition dissimilarity scheme merge dissimilarity matrix update model function definition important mention ward function argument unlike consequently globally function cluster fuse iteration formally function efficient effective generic AHC approach  mcquitty centroid median ward median setting LW sub formula model define mention previously ward interpret version centroid similarly introduce version median  parameter define  function median function instead uniform median function ward demonstrate later median monotonic dendrograms unlike median technique AHC kernel matrix AHC suppose item hilbert euclidean distance proximity relationship henceforth underlie inner introduce model generalizes LW sub equation framework relies inner amount kernel matrix AHC AHC thereafter introduce model sufficient technique express model monotonic dendrograms furthermore data matrix approach generalizes average mcquitty scheme inner LW sub formula denotes inner geometrical data representation assume formally sab dab saa sbb sab implies matrix kernel gram matrix satisfies sab sba symmetry positive semi definite data representation encompasses reproduce kernel hilbert RKHS accordingly approach kernel pine benefit spectrum kernel function address diverse manifold AHC detect item arbitrary contrast LW sub formula model equation update matrix diagonal diagonal entry suppose input matrix procedure assume iteration cluster merge model update accord recurrence equation function similarly AHC assume update matrix symmetric along procedure matrix couple cluster core role approach establishes connection LW sub equation approach lemma sequence matrix input subsequent define respectively suppose related function proof assume  cluster fuse iteration replace function respectively efficient effective generic AHC approach average mcquitty centroid median ward median setting model define assume ingredient regroup respect replace introduce approach proceeds manner however unlike AHC AHC performs maximum iteration indeed initialize dendrogram leaf AHC fuse cluster satisfies arg max function domain disjoint subset iteration cluster merge latter subset node height update similarly apply clearly model scheme definition respective function furthermore summarize algorithm AHC procedure lemma assume algorithm algorithm merge couple cluster iteration therefore equivalent dendrograms definition difference dendrogram AHC assigns assume algorithm pine algorithm procedure AHC input kernel matrix AHC output dendrogram initialize leaf cluster accord correspond AHC parameter merge update compute apply correspond AHC parameter node height minus height assign node dendrogram obtain AHC consequence theorem suppose lemma satisfied suppose addition algorithm algorithm equivalent dendrograms technique scheme examine AHC equivalent AHC monotonicity hierarchical cluster important reversal building dendrogram indeed non monotonic dendrograms interpret undesirable classic AHC framework described algorithm technique monotonic dendrogram  sufficient express LW equation output monotonic dendrogram satisfy relationship approach described algorithm height depth varies algorithm consequently monotonicity definition previously translates AHC efficient effective generic AHC approach sufficient express model monotonic dendrograms proposition sequence matrix input subsequent define suppose function satisfy  satisfies proof definition obtain assume regroup respect cluster merge iteration accord therefore easy assume easy induction inequality besides previous inequality obtain upper bound pine finally previously sufficient easy technique described satisfy fifth proposition however unlike average mcquitty ward centroid median satisfy former scheme monotonic regard median technique introduce sub proposition median scheme monotonic proof apply median parameter proposition inequality develop equation manipulation obtain equivalent expression max min non negative rational parenthesis becomes easy completes proof data matrix approach AHC dimension hilbert suppose finite data matrix vector assume proximity matrix approach input algorithm however useful data matrix instead indeed inefficient impossible proximity matrix machine data matrix approach input data matrix computes dissimilarity vector cluster merge cluster representative vector latter approach allows alleviate storage complexity computational proximity evaluate efficient effective generic AHC approach data approach dissimilarity scheme formulate representative vector already sub centroid ward perform vector latter representative vector iterative fashion cluster fuse vector compute median median scheme cluster mid denote representative vector cluster singleton newly cluster median median median median dissimilarity satisfy graph average mcquitty equivalent express dissimilarity update equation representative LW sub equation approach obtain latter scheme introduce discus data matrix approach geometric scheme inner model proposition centroid ward scheme proof kernel matrix assume proof simply linear inner concern pine regard proposition merge define median median scheme proof proof proposition proposition centroid ward median median  diagonal entry inner representative vector average mcquitty technique concerned latter valid diagonal indeed average mcquitty respectively vector centroid ward median median regard diagonal entry actually compute average mcquitty efficiently extra vector norm vector vector vector component associate cluster iteration suppose cluster merge recurrence formula function define average mcquitty easy proposition merge define average mcquitty scheme previously summarize algorithm AHC data matrix approach examine similarly loses dimension iteration efficient effective generic AHC approach average mcquitty centroid NA median NA ward NA median NA setting data matrix AHC define representative vector update algorithm procedure AHC data matrix approach input data matrix AHC output dendrogram initialize leaf appropriate compute inner matrix representative vector cluster accord correspond AHC definition merge update compute representative vector apply correspond AHC formula compute apply correspond AHC formula appropriate pine snk AHC sparsified normalize kernel matrix AHC another important AHC address scalability issue proximity matrix AHC procedure distance matrix reasonable assume item distance unlikely grouped stage consequently goal reduce storage complexity discard replace zero sparse matrix however zero distance identical away avoid drawback propose inner matrix instead explain introduce approach sparsified normalize kernel matrix AHC snk AHC firstly introduce normalization procedure transform kernel matrix constant diagonal non negative preliminary interpret inner matrix similarity definition sparsification procedure aim thresholding zero introduce snk AHC algorithm feature average mcquitty ward context snk AHC cluster normalize kernel matrix perspective normalize kernel NK matrix designates kernel matrix constant diagonal non negative assume belong intersection hypersphere positive quadrant saa sbb sab kernel matrix constant diagonal apply cosine normalization generalization propose sab sab  minimal propose perform translation obtain non negative sab sab worth translation algorithm procedure invariant positive linear transformation matrix scheme interested replace constant maximal distance lose sparsity efficient effective generic AHC approach proposition suppose function satisfy algorithm equivalent dendrograms input similarity matrix matrix proof linear transformation tab  assume cluster fuse iteration indeed denote respectively sequence matrix define obtain input kernel matrix respectively arg max arg max moreover remind positive linear transformation positive semi definite matrix positive semi definite matrix therefore remains kernel matrix henceforth assume NK matrix matrix enjoys interpretation inner pine hypersphere hilbert similarity matrix satisfy definition consequence NK matrix similarity matrix interchangeably penalize similarity aggregate inter similarity versus aggregate intra similarity suppose discus interpretation AHC similarity equation maximum couple cluster quality depends define difference arithmetic inter similarity cluster intra similarity cluster couple cluster inter similarity penalize arithmetic respective intra similarity accord inter similarity intra similarity consequently likely merge inter similarity respect intra similarity context important formally function define scheme therefore vector interpret average inter similarity intra similarity respectively viewpoint difference technique understood distinct average strategy precise difference suppose iteration assume couple cluster merge illustrate situation input similarity matrix involve correspond rectangular inter similarity intra similarity inter similarity average dash mcquitty median median assign whereas average centroid ward assign intra similarity average highlight dot solid respectively symmetric equivalent depends depict partition geometric sub contribute median median scheme sub assign uniform amount unweighted regard centroid context maximal similarity due cauchy schwartz inequality efficient effective generic AHC approach illustration inter similarity intra similarity ward distribute respect conversely graph average mcquitty depends consequently diagonal involve computation cluster intra similarity latter scheme observation already underlined sub sparsified normalize kernel matrix cope storage complexity AHC sparsify NK matrix remove similarity obtain sparsified normalize kernel snk matrix sparsification procedure introduce thresholding operator parameter sab sab sab otherwise dab saa sbb sab sab saa thereby entry correspond exactly entry sparsification approach  closest accord equivalently define sab sab   otherwise non null similarity profile sab bound apart closest item  item distance sparsification procedure equivalent epsilon neighborhood pine  sca sac non null consequently instance memory usage necessarily factor besides graph basically however procedure reference therein perform sparsified similarity matrix longer positive semi definite thereby geometric context assume snk matrix nonetheless sub technique concerned issue perform AHC snk matrix efficient effective manner AHC snk matrix however owe distinct interpretation expose sub propose substantial modification algorithm proximity AHC algorithm AHC AHC bottleneck computational cluster fuse operation iteration overall complexity sparse similarity matrix introduce subset sab likewise merge mechanism iteration subset easily update contrast AHC snk AHC merge accordingly replace arg max therefore whatever cluster merge similarity snk AHC constrain AHC procedure snk AHC pseudo code algorithm describes algorithm unlike AHC dendrogram grows consequence output algorithm investigate sub restrict obtain scalable dendrogram building procedure proposition non null entry sparsification algorithm perform procedure algorithm storage complexity processing complexity efficient effective generic AHC approach algorithm procedure snk AHC input kernel matrix sparsification AHC output dendrogram diagonal constant normalize translate sparsify initialize leaf accord cluster accord correspond AHC parameter merge update update compute apply correspond AHC parameter however dense NK matrix sparsified related distance matrix algorithm algorithm output equivalent obtain algorithm accord theorem dedicate snk AHC dramatically efficient AHC computational standpoint enables improve quality cluster challenge diagonal translation invariance highlight sub snk matrix positive semi definite assume belong hilbert however recover feature easily indeed symmetric diagonal entry nonnegative positive semi definite sufficiently augment diagonal entry strictly diagonally dominant horn johnson theorem technique introduce matrix  identity matrix chosen positive semi definite pine sequence merges definition diagonal entry snk matrix easy lemma sequence matrix input  subsequent define suppose respect sequence merges lemma indicates merge sequence cluster  entry similarity matrix identical intra similarity influence diagonal translation average mcquitty ward relationship lemma sequence matrix input  subsequent define suppose respect sequence merges average mcquitty regard ward proof average technique parameter definition assume latter relation suppose iteration cluster merge apply lemma average proof mcquitty ward theorem average mcquitty ward algorithm equivalent dendrograms input similarity matrix  efficient effective generic AHC approach proof denote respectively sequence penalize similarity matrix obtain input similarity matrix couple cluster maximize maximize sufficient constant abuse notation denotes whenever input matrix merge consequence apply lemma sufficient induction concern average mcquitty technique assume iteration cluster merge apply lemma ward lemma consequently scheme geometrical representation hilbert valid apply algorithm snk matrix positive semi definite scheme centroid median median preliminary empirical positive semi definite recommend indeed increase diagonal performance cluster quality transformation distortion latter highly sensitive therefore algorithm diagonal translation default pine cluster component important issue cluster cluster extent algorithm address challenge detail framework graph theory undirected graph node define node respect component latter subset partition cluster viewpoint component cluster component undirected graph disjoint data structure typically node assigns representative item context operation employ creates member representative representative belongs union  disjoint belong remove latter representative review algorithm pseudo code disjoint data structure graph output component algorithm component determination input output component union illustration graph node apply algorithm component representative item subset chosen respect lexical efficient effective generic AHC approach illustration disconnect graph AHC algorithm introduce rely fusion mechanism reproduces operation algorithm difference instead scan unitary loop algorithm AHC procedure consolidated representative item disjoint cluster moreover unlike algorithm picked randomly AHC algorithm chosen goal optimize criterion dissimilarity AHC penalize similarity AHC snk AHC furthermore algorithm input proximity matrix dense underlie graph fully therefore algorithm necessarily output contrary algorithm sparse similarity matrix algorithm output component cluster summarize statement proposition sparse similarity matrix obtain sparsification algorithm define associate undirected graph component algorithm iteration moreover output component accordingly snk AHC output dendrogram demonstrate advantage snk AHC AHC benchmark data artificial freely available respectively firstly dendrograms obtain AHC snk AHC purpose verify sparsification perform snk AHC equivalent  secondly interested assess proximity dendrograms AHC snk AHC thirdly medium data demonstrate algorithm indeed allows reduce AHC computational dramatically finally data sparsifying similarity matrix cluster introduce assessment criterion benchmark obtain pine evaluation proximity dendrograms obtain algorithm  matrix correlation coefficient dendrogram derive  matrix denote pairwise matrix item height AHC depth snk AHC node merges   dendrograms obtain technique  correlation correlation vectorized upper triangular matrix   denote CC CC implies dendrograms equivalent evaluate quality cluster apply external validation methodology partition data obtain dendrogram obtain cluster denote cluster algorithm partition cluster afterward partition truth evaluation adjust rand index denote ari ari truth recover perfectly regard scalability baseline AHC computational therefore memory reduction report comparison performance obtain AHC relative storage processing decrease examine however mainly analyze benchmark indeed synthetic data worth computational gain detail artificial data artificial data goal illustrate ability snk AHC address challenge cluster task compute inner matrix data matrix respect standard deviation variable aggregation data benchmark consists  cluster identify cluster non convex author shortcoming classic AHC linkage linkage average ward scheme algorithm fails recover cluster previous euclidean distance report performance obtain scheme framework snk AHC gaussian kernel sparsification operator  apply efficient effective generic AHC approach aggregation data regard gaussian kernel remind definition sab exp  descriptive variable default popular svm concern  distinct successively integer percent item sequence sparser sparser snk matrix corresponds sparsification apply snk AHC without sparsification equivalent AHC AHC situation corresponds baseline contrast refers sparsest similarity matrix scheme CC obtain dendrograms equivalent AHC observation empirically confirm theorem sparse matrix reduce memory alter quality snk AHC output therefore improve scalability without decrease ari addition CC likely dendrograms obtain thesis equivalent baseline fifty percent performance stable ari behavior depends median median sparsification beyond fifty percent negative conversely average percent recover partition ari pine compound data sparsification ward dramatically improves baseline dense ari increase cluster snk AHC latter corresponds sparsest similarity matrix underlie graph becomes disconnect scheme cluster discover disconnect cluster cyan diamond cluster respectively compound data synthetic data composition cluster task originally propose consists dimensional data distinct identify task particularly challenge cluster highly non convex non linearly separable similarly previous apply gaussian kernel default however sparsification threshold chosen sparsity precisely correspond percentile similarity distribution percentile yield sparsification baseline contrary percentile similarity thresholded zero latter sparsest matrix obtain comment observation actually previous benchmark firstly sparsification apply obtain equivalent dendrograms efficient effective generic AHC approach  CC ari average mcquitty centroid median ward median NA NA NA NA NA NA NA aggregation data gaussian kernel pine CC ari average mcquitty centroid median ward median NA NA NA NA NA NA NA compound data gaussian kernel AHC snk AHC secondly matrix reduce cluster quality dense matrix technique consequence computational without degrade ari similarity discard obtain consistent improvement scheme thresholding ari efficient effective generic AHC approach compound data cluster performance underline obtain sparsest similarity matrix contains similarity ari obtain technique overall performance partition cluster illustration snk AHC output clarity cluster cluster singleton cluster cluster consequently cluster snk AHC discover depict although ari partly due snk AHC output partition cluster truth versus important precise perfect  finally emphasize snk AHC sparsest similarity matrix allows improve AHC viewpoint firstly expose previously ari improvement comparison baseline refinement ward technique ari increase secondly computational AHC largely diminish snk AHC detect core cluster diverse successfully data exemplify snk AHC synthetic data address cluster pine addition cluster quality discus detail gain snk AHC achieve scalability likewise previous data matrix inner matrix landsat data collection landsat data consists item data corresponds contiguous pixel dispose patch pixel spectral integer consequently described vectorial dimension task consists recognize item grey damp grey vegetation  damp grey preliminary experimental sparsification cluster comparison threshold gaussian kernel performance linear kernel consequently report obtain perform setting concern  sequence integer percent previously percentage estimation density matrix obtain depict assessment plot ari CC curve dot respectively moreover relative memory relative respect computational baseline dense correspond solid plus dash respectively outcome report scalability verify sparser snk matrix memory processing curve relative measurement latter criterion clearly decrease non null entry relative reduction linear respect latter variable reduce memory snk AHC reduce initial processing empirically illustrates proposition quality comment firstly ari impact whatever AHC scheme likewise previous benchmark nearly initial memory usage without hurt cluster quality contrary median  approach ari however cluster quality stable nevertheless average mcquitty sparsest similarity matrix performance technique gain concern average scheme ari jumped http archive uci edu datasets  landsat satellite efficient effective generic AHC approach average remove mcquitty remove centroid remove median remove ward remove  remove landsat data gaussian kernel axis corresponds remove axis corresponds belong solid plus relative memory dash relative dot indicates CC dot ari regard median worth mention plainly dominates median scheme non uniform median technique allows obtain monotonic dendrograms enables boost cluster quality similarity graph sparsest similarity matrix thereby whatever snk AHC pendigits data collection pendigits data benchmark consists handwritten digit writer around sample entire collection compose observation sample described numerical feature digit frequency obviously task recognize correspond digit report obtain linear kernel outcome gaussian kernel however sparsification outperform rely threshold therefore report obtain former sparsification technique sequence neighborhood selection obtain benchmark landsat http archive uci edu datasets pen recognition handwritten digit pine average remove mcquitty remove centroid remove median remove ward remove  remove pendigits data linear kernel axis corresponds remove axis corresponds belong solid plus relative memory dash relative dot CC dot ari scheme ari curve stable remove regard scalability technique memory usage processing without degrade performance beyond remove cluster quality evolution depends centroid median decrease whereas approach ari curve positive slope average mcquitty ward respective ari achieve sparsest matrix precisely ari technique clearly outperform respective baseline overall gain ari achieve average cluster quality increase data median superior median moreover ari stable respect sparsification similarly landsat similarity graph remain sparsification apply related discussion approach generic scalable effective respect challenge cluster task belong non linear manifold previous efficient effective generic AHC approach research relevant framework inherent scalability issue hierarchical cluster algorithm introduce data mining community birch cure respect approach random sample pre cluster stage reduce convey hierarchical cluster rely vectorial representation classic distance AHC approach sparse graph goal hierarchy construction vectorial euclidean distance dissimilarity author graph merge closest centroid approximately update research efficient algorithm however generic model hierarchical cluster assume feature description item data approach vectorial representation dissimilarity related euclidean distance snk AHC relies generic model allows proximity relationship cluster standpoint approach developed statistic data analysis overview LW equation core role landscape formally infinite hierarchical cluster technique furthermore algebraically define sub satisfy appeal guarantee output monotonic dendrogram admissibility recently  ben david introduce characterize linkage hierarchical clustering context research address scalability AHC highlight research regard essentially implementation concern LW clustering leverage advanced data structure AHC employ priority queue efficiently minimum reduce maintain priority queue efficient fashion overall complexity AHC reduce cubic contrast previous research avenue research involves subfamily AHC scheme reducibility  algorithm usually chain reducibility satisfied centroid median latter algorithm equivalent AHC procedure complexity instead cubic chain pine procedure implement author recently propose memory efficient online hierarchical cluster  relies reducibility however average linkage latter research focus computational efficiency  framework snk AHC effectively tackle cluster complex data text graph image necessarily linear sub manifold euclidean distance descriptive fail non convex arbitrary capture underlie geometry data approach propose literature context non parametric hierarchical cluster adopts graph cluster task graph derive pairwise proximity allows approximation geometry linkage chain effective arbitrary detection scheme  ross link linkage minimum span mst  analyze detail application mst algorithm detection non convex non linearly separable context removal effective mst capture spectrum approach non parametric proximity rely mutual rank linkage recover arbitrary cluster recently defines hierarchical cluster robust outlier neighborhood another related context chameleon algorithm introduce proceeds stage graph chameleon another snk AHC emphasizes contrast inter intra connectivity cluster dynamic model spirit penalize similarity however author propose generic model unlike parametric recurrence equation another research direction manifold kernel function data description dimensional hilbert feature cluster easy detect knowledge extend AHC kernel contrast model scalability issue previous inner formulation LW equation addition sparse kernel matrix employ nonetheless latter model framework concept penalize similarity theoretical examine framework introduce efficient effective generic AHC approach lastly worth emphasize relationship snk AHC spectral cluster latter technique kernel function employ construct similarity graph sparsification apply laplacian graph theoretical spectral graph theory link eigen decomposition laplacian component graph spectral cluster procedure performs spectral embed subsequently applies cluster algorithm usually context roughly snk AHC classic AHC spectral cluster significant extension conventional cluster sub LW clustering recover non spherical cluster besides approach advantage hierarchical cluster  cluster addition snk AHC scalable AHC drawback hierarchical cluster conclusion future introduce AHC generic AHC model relies inner instead euclidean distance approach recurrence formula embeds sub LW cluster technique model efficient effective challenge cluster task apply AHC sparsified normalize kernel matrix perspective recurrence formula highlight aggregation inter similarity intra similarity dynamic model penalize similarity cluster moreover constrain merge procedure fuse cluster  non null snk AHC scalable AHC boost cluster quality detect cluster however performance snk AHC similarity matrix sparsified relies sparse similarity graph spectral cluster therefore important future research advanced sparsification technique cluster quality standpoint sparsification important address determines component snk matrix cluster approach recover investigate theoretical cluster framework introduce regard overall complexity algorithm technique exactly approximately graph efficient manner important indeed dendrogram building procedure perform snk AHC efficiently computational graph remains quadratic bottleneck pine mention building procedure concerned already immediate improve scalability approach underlined previous direction firstly enhance complexity snk AHC priority queue secondly chain approach regard performance concern scheme satisfy reducibility finally model generic demonstrate parameter setting worth standpoint examine admissibility express framework perspective peculiar diagonal translation invariance average mcquitty ward satisfy technique effective experimental report accordingly characterization sub cluster technique beneficial