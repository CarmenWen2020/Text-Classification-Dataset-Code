introduce neural network model tractable monotone online algorithm model describes network classification output node nonlinear operation rectification relu function bias however rectifier node network positive static associate node rectify network arbitrary boolean function bias parameter network another departure approach standard neural network loss function replace constraint constraint simply output node associate zero model norm minimize parameter update correctly classify training item quadratic program compute network demonstrate training algorithm update sequential deactivation sda mnist synthetic datasets upon adopt choice  sda hyperparameters network structure explore behavior respect network depth sparse expander network keywords neural network online training algorithm rectify linear simplest mode machine monotone representation knowledge  classification algorithm exemplar amass monotonically training monotone memory decrease suppose classify boolean feature vector conjunctive normal formula randomly initialize formula training simply discard formula feature vector inconsistent clause clearly attractive feature monotone algorithm computational desire outcome additional exemplar improve discrimination discard clause accommodate report monotone training fix parameter monotone fashion computationally tractable objective item feature scheme operates network rectifier training normally computationally tractable monotone rectifier network standard paradigm conservative update minimize norm parameter eliminate bias variable parameter although feedforward computation standard neural network model inspire biological neuronal computation training protocol widespread generalize digit without suffer training epoch data capable building rudimentary representation digit relatively conservative principle introduce knowledge minimal disturbance principle closer mode parameter network minimally accommodate rationale attention minimality preserve representation earlier minimal disturbance principle   algorithm context vector machine relatively model aggressive parameter update minimal achieve immediate easily compute emphasize norm minimize characteristic mode conservative approximate implementation conservative successfully discover strassen matrix multiplication standard neural network model enable conservative outline guarantee monotonicity computation conservative update tractable elevate role additive parameter bias metaphor replaces hebbian synapse silt delta myriad channel bias differentially monotonically overview contribution inspire analog implementation logic propose replace conventional rectify sum compute standard neuron relu function max wixi sum rectification max bias parameter static monotone rectify network positive network rectify arbitrary boolean function construction input encode false generalizes symbolic data encode node network non negative propose define membership data correspond output node zero bias non negative increase training initialize zero norm minimize bias parameter output node input zero quadratic program iterative algorithm stochastic gradient descent backward propagation propagation propose approximately norm minimize bias execute minimum iteration output node defines sequential deactivation algorithm sda limit sda network hidden layer learns arbitrary boolean function network multiple layer training sensitive setting static propose balance parameter random sparse expander network introduce explore depth behavior model conservative rectify network sda algorithm demonstrate mnist synthetic datasets despite model capacity depth representation experimental sda algorithm obtain limit deactivation occurs layer model equivalent network hidden layer rectifier gate pursuit fully utilize activation  layer identify goal future research rectify network motivation network model implementation logic analog computation without multiplication analog counterpart false respectively rectify network model network construct bias rectifier gate bias rectifier gate input max elser schmidt yedidia generalizes gate gate realize bias gate saturate output detail fix additional rectifier suitable network negation completely absent rectifier implementation logic circuit  gate circuit output input exchange gate dictate morgan gate remain gate directly input accommodate input logic circuit replace analog false refer encode scheme input relatively straightforward theorem network comprise rectifier gate bias variable parameter mimic logic circuit machine perspective network model capacity define truth arbitrary boolean function however parameter setting realize boolean continuous parameter model claimed directly relevant intend network theorem boolean function input compute binary gate gate implement analog network comprise bias rectifier gate correspond analog input proof circuit function boolean input boolean output associate negation circuit gate output gate panel upon output gate output negation gate render available gate input output fork gate circuit output boolean function diagram gate input derive fork output another gate input circuit proof consists confirm boolean gate relationship input output negation exactly reproduce correspond rectifier gate network replacement gate panel bias parameter arise node replacement gate circuit replacement described others input negate trivially generate insert negation input equivalent swap input fork completeness gate receives input output fork gate replacement applies input input fork applies input choice input obtain swap replacement gate unchanged bias swap monotone rectify network gate replacement rectifier gate analog network gate replacement rectifier gate analog network negation remain gate replace input fork circuit define input analog network fork network rectifier gate gate replacement rectifier gate generalize rectifier gate model observation rectifier input bias equivalent sum input input rectifier bias rectifier elser schmidt yedidia gate replacement input output fork gate exchange bias freedom increase expressivity network boost parameter commensurate standard model network bias completely network thereby disable model capacity architecture generalize model introduce summation static intend positive layer network apply rescale bias restore without classification behavior network therefore role training gain decay impart analog signal propagates input output response elicits bias parameter formally define model definition rectify network model computation acyclic graph associate node node output related max bias parameter output incident node sum node monotone rectify network positive constant node node output bias parameter rectify network output sum node input node zero input node assign output network node zero output node node neither input output node hidden node network connectivity rectify network exists input node output node content theorem express rectify corollary truth boolean function variable compute binary gate gate  rectify network input hidden node input output node encode truth function proof logic circuit binary gate node gate output express rectify network gate output replace node gate additional node hidden node rectify network therefore bound wiring node input output node acyclic graph bias setting  realize network node principle easy rectify network bias modest network promise corollary data generate boolean function network model significant advantage standard model tractable conservative equally valid representation instead likely network render rectify network graph input node output node upward direction render bias parameter thickness bias bias bias rectify zero input network effectively absent zero thickness style render network input hidden output node generalize input output rectify network expand scope beyond boolean function classifier application feature vector finite alphabet false etc boolean alphabet encode scheme expand numerical zero numerical non symbolic data encode style suppose data vector obtain cumulative probability function elser schmidt yedidia rectify network input node hidden node output node  bias component dataset data vector input network correspond convention input network vector sum component data mapping cumulative probability function analog data ensures distribution channel uniform output node data boolean function classifier output node classify truth input classify mnist data network output node output interpret trivial observation lemma rectify network input vector exist setting bias parameter output node proof output node positive zero bias positive output input positive definition exclusively incident bias zero explain zero appropriate indicator function define conservatively rectify network notation network model introduce summarize notational convention variable parameter monotone rectify network network latin index define node network node index node define network bias parameter output node without subscript denote vector indexed vector bias network vector data input node equality inequality relation applies componentwise bracket attach variable identify functional dependence relevant node emphasize dependence bias input data upper reserve data input hidden output node respectively upper continuous convention functional dependence variable bias node zero input cardinality denote vertical network node network counterfactual classification conservative monotone rectify network fix input network node non negative nonincreasing function bias parameter combine lemma zero feasible output node motivates definition classifier definition output node rectify network classifier input bias input definition context conservative suppose stage training output node positive input increase bias output driven zero input increase conservatively output node positive output zero increase bias classification ambiguous amount subsequent training bias increase monotonicity respect ensures valid indicator output propose classifier counterfactual positive node evidence define truth boolean function function return false member correspond analog circuit output node zero separately insist training bias decrease automatically conservative principle assigns bias elser schmidt yedidia definition rectify network bias norm bkp uniqueness minimize bias fix misclassification depends norm exponent mostly uniqueness definition conservative sidestep uniqueness definition suppose output node input network bias conservative bias update minimizes bkp output lemma conservative update proof bias input suppose update others unchanged monotonicity node respect output node zero zero output rectify network composition rectifier function composition convex function convex rectifier function inductive proof lemma convex define max convex proof convex arbitrary arbitrary max max max define arbitrary max max max monotone rectify network obtain max max max max max claimed theorem node rectify network input convex function bias parameter proof input node constant trivially convex network acyclic graph node indexed consecutive integer output incident node input node hidden node induction suppose convex function bias parameter output incident max function bias parameter convex function bias hypothesis lemma therefore convex conclusion applies hidden node numbered input constant node positive multiple sum convex function convex completes induction uniqueness conservative bias update corollary bias rectify network fix input output node non empty convex consequently conservative bias update minimizes respect bias unique proof define feasible function non negative lemma non empty theorem convex convex function suppose distinct minimizers bias convex contradiction implies minimizers elser schmidt yedidia algorithm compute conservative bias update establish complexity task cast mathematical program rectify network bias parameter previous training suppose vector input data associate output node unless bias conservatively update partition active complement definition active rectify network input data bias parameter equivalently bias inactive increase node output node correspond output zero monotonicity non increase therefore network induced active active network mathematical program reduce bias variable definition active rectify network input data bias parameter network define reduce bias conservative update reduce bias reduce output node conservative update already reduce freedom reduction conservative update define mathematical program data bias rectify network correspond active node unknown update bias reduce bias output input hidden node minimize monotone rectify network theorem mathematical program define network active bias data conservative bias update associate output node proof valid assignment node output correspond rectify network realize feasible linear involve reduce bias variable rectify bias consistent reduce bias sufficient satisfy monotonicity ensures optimize constraint define feasible reduce bias conservative update positive semi definite quadratic program grows polynomially active network conclusion applies linear program objective replace sum update bias impose lemma constraint rectify network model tractable conservative application important training nearly linearly network sequential deactivation algorithm sda described goal review feature model training tractable  exactly individual item  feature indeed easy mathematical program compute conservative update data item generalize norm minimize update classifies entire mini batch variable data exception bias data index mini batch replace imposes consistency bias update instantiation reduce bias mini batch mathematical program grown equation inequality tractable convexity variable respect bias parameter clearly important tractable proof lemma theorem relies non negativity static activation function obvious facilitate tractability proposal replace loss function neural network constraint constraint replace constraint inequality fix positive parameter specifies margin avoid ambiguous classification tractable mathematical program substitution unfortunately proposal creates conflict relationship reduce bias actual bias update variable exhaustive parameterization node variable constraint cannot elser schmidt yedidia inequality preserve constraint bias replace bias rectify model monotonic decrease non output exceed output sequential deactivation algorithm describes detail sda algorithm compute bias update rectify model update guaranteed conservative computation significantly faster quadratic program sda algorithm resembles gradient optimize standard model network widely stochastic gradient descent sgd program greatly simplify inequality somehow automatically satisfied active remain active update update bias reduce bias node variable linear function norm bias update optimal update achieves  conservative update simplification gradient descent become inactive monotonicity gradient positive component zero gradient descent update satisfies latter immediately active incident output node non zero derivative respect bias gradient descent cannot guarantee norm minimize update promise program attractive feature computation piecewise constant gradient computation gradient discontinuity deactivation opportunity gradient descent update conservative inactive indicator inactive improve update max sda algorithm efficient implementation gradient descent rectify model attention descent define deactivation implementation sda algorithm elementary operation define algorithm procedure eval simply pas network data input node output node directly encode monotone rectify network algorithm elementary network procedure procedure eval max procedure procedure grad  procedure procedure velocity procedure addition output node eval output active output non zero suppose bias parameter denote negative gradient component bias derivative respect active node node indexed variable define output node  gradient constant bias evolve continuous gradient positive node active node sum active contribute along bias active bias instead apply active node implies recursion procedure grad algorithm corresponds propagation network procedure velocity derive recursion output elser schmidt yedidia derivative obtain velocity output propagation initialization occurs input node sum absent grad construction constant velocity active positive deactivation occurs min iteration sda algorithm bias incremented newly deactivate remove another iteration terminate eval return bias obtain apply gradient descent bias revisit classification definition data declare recall desire supremacy node spoil subsequent training latter negative decrease output increase bias output node zero mitigate bias conservative introduce ultra conservative definition ultra conservative mode sda algorithm iteration terminate output node zero output node termination criterion conservative stage training algorithm candidate latter output node terminate sda criterion met perform training succeed bias criterion data output node spoil training data network retrain data properly combination imply heuristic net bias mode conservative insist output sda algorithm ultra conservative termination criterion summarize algorithm becomes output zero algorithm return conservatively update bias consistent activity network estimate misclassification complicate factor iteration linearly active network iteration average satisfy ultra conservative monotone rectify network algorithm sequential deactivation sda input network initial bias data vector initialize bias eval active node output iter zero iteration counter arg mini iterate node grad gradient respect active bias velocity output velocity mini increase bias eval active node output iter iter increment iteration counter bias active max conservative update inactive output iter update bias iteration termination empirically iteration depends weakly network lessen sparsification active network computation data item iteration active deactivate eliminate subsequent direction future research algorithm solves program conservative update modest amount extra sda connection simplest instance update sda non optimal fragment network bias update analyze isolation optimization bias update zero node additional incident node inactive data item input node zero previous data explains bias zero inactive incident node active previous data elser schmidt yedidia network fragment sda update optimal initial bias zero another parameter node node dash inactive irrelevant responsible prior training account positive bias node node data vector component node relevant fragment easy sda optimal coincides however sda update suboptimal advantage improvement pervasive instance alone seriously impact observation sda sgd apply loss function difference rate parameter deactivation multiple execute data item benefit impart rectify model context gradient computation easy compute aside monotonicity guarantee attribute loss function model territory truth boolean function variable generate binary gate rectify network input output node corollary rectify network exists monotone rectify network hidden node outstanding gradient algorithm appropriate bias setting instead monotone loss function fraught training cannot prevent ambiguity hinge loss function max corollary achieve zero loss margin data correctly unambiguously classify loss function piecewise linear rectify model likewise benefit gradient calculation loss longer monotone decrease bias parameter initialize bias zero loss monotonicity brings zero gradient suspect serious rectify standard network model clause although sda algorithm tractable computation individual data rectify model algorithm function address concern limit sda algorithm define boolean function already rectify network favorable demonstrate rely admittedly impractical network exponentially boolean variable definition boolean network boolean function input node hidden node output node input node correspond literal hidden node clause literal output node correspond truth hidden node constituent literal output node boolean network function variable render zero bias hidden node layer output node fix limit sda algorithm analyze hidden node approach zero refer limit boolean network clause sda bias clause output node hidden node bias hidden node factor relative hidden node recall sda iteration define deactivation bias increase clause occurs layer output hidden node bias output suffices deactivation correspond bias hidden node factor elser schmidt yedidia boolean network variable boolean function function classify output node convention node inactive input node lemma clause hidden node bias proof induction training bias zero input hidden node literal associate node input suppose slightly modify statement iteration sda hidden node happens iteration invoked network prediction deactivate increase bias deactivate hidden node bias therefore difference bias correspond bias hidden node statement hidden node theorem clause succeed boolean function proof output node clause encounter input associate function argument lemma   integer output zero sda bias monotone rectify network identify suppose bias increase sda inactivation hidden node output node decrease due bias increase hidden node become inactive sda iteration desire outcome bound hidden node argument apply however argue arises input correspond unique hidden node combination input literal boolean network sample literal combination sum non negative contribution hidden node proof inconsistent conservative fix hidden node argument bias correctly assign smallness correspond input clause associate unique input achieve contribution bias conservative hypothesize argument interchange output node applies data balance network layer node partition sequence layer network node adjacent layer rectify network layer dependent suitable rescale apply bias eliminate replace without classification behavior network denote layer node definition  equation unchanged replace correspond input node output node respectively classification unchanged output node rescale positive factor elser schmidt yedidia network conservative motivate balance derive node definition choice net gain decay typical node layer appearance associate propagation choice recursion relates bias throughout network conservative bias influence output node net gain decay layer choice bias derive backward propagation become inactive training active however problematic unequal onset become inactive unequal node accumulate bias rate layer layer deactivation uniform network concentrate input layer output layer promote uniform distribution activation network balance choice propagation limit bias denotes average node denotes average node growth rate adjust direction network whereas neither zero layer layer growth systematic imbalance growth monotone rectify network deactivation uniformly across layer non zero layer layer growth decay variable rectify network equation intrinsic suitable  apply training neutralize layer layer growth without classification behavior network hidden node node output node differently prior information frequency data however distinguish prior information output node invariance impose rectify network introduce global multiplier hyperparameter formula replace corresponds balance limit collapse rectify model simpler explain easily generalize arbitrary layer limit layer become inactive bias layer neglect layer hidden layer node combine linear layer model hidden layer away expression output node max aji integer aji network input node hidden layer node model comprise layer rectifier gate easy analyze multi layer rectify model decrease ass depth network compromise performance depth utilized essential network network model rectify training conservative unconventional demonstrate model network standard demonstration sda algorithm boolean network function variable hidden layer network function boolean variable already network learns variable boolean function sda limit vanish hidden layer balance thanks elser schmidt yedidia bias setting network function data ordering symmetry network equivalence function equivalence respect negate input output swap variable operation function correspond  input output node network algebra GF boolean operation representative equivalence verify function network sda balance bias initialize zero render thickness recall network function sda algorithm data vector propagate output node unequal unequal node correspond bias item output positive output sda algorithm executes iteration output zero network iteration output zero irreversible mode classification ambiguity arise network function ultra conservative guarantee successfully data  subsequent training however data bias sda establish function data manifest bias bias setting function define trial training zero bias data prescribed trial function successful network bias monotone rectify network data correctly unambiguously classify successful trial err  classify data invocation sda training iter sda iteration perform network variable function sda iteration sufficient fix incorrect classification surprisingly constant function easy err trial err xor function err function variation err aggressive variant sda iteration output node zero succeed function network balance mode training pas data suffices function network data err iter boolean network network quickly become function boolean variable alternative network node hidden layer clause subset variable incomplete sample clause clause network function limit apply hidden layer suggests hidden layer network bias output limit hidden layer node output hidden layer node interpret proto clause composition literal modify training input bias proto clause sample variable negation rely training bias bias input network alleviates exponential explosion static clause limit departure boolean network increase hidden node layer indeed training tractable primary motivation rectify model explore feature network variable boolean function hidden layer node indegree potentially express relationship variable hidden node layer variable network node hidden layer exhaust input distinct boolean without negation attribute node identity boolean variable sample layer hidden node exhaust combination input hidden layer attribute network wiring hidden layer output node conventional network bias rendering training network comprehensible however statistic err iter correlate naive complexity boolean function function evaluate elser schmidt yedidia rectify network hidden layer conjecture learns boolean function variable hidden node network input frequency ignore easy function logical majority function harder sensitive variable argument parity function hardest trial perform sda network balance successful distribution err iter trial data metric false classification training difficulty rank function extensive trial boolean function successful conjecture network learns variable boolean function regardless data boolean network instance theorem boolean function limit reassure sda network variable boolean function highlight advantage ultra conservative termination sda iteration trial stricter zero monotone rectify network distribution err iter trial variable boolean function network output training successful training successful rate rate improves function another ultra conservative termination greatly reduce iteration zero output iteration average data ultra conservative sparse expander network nice simplification rectify network training algorithm hyperparameters succeed fails mostly basis network however tractability rectify training sensitivity network therefore anticipate network characteristic depth sparsity role hyperparameters improves network bias parameter rectify network appropriate network introduce network depth hidden layer parameter layer network completely conventional suffer symmetry bias zero completely network perfect permutation symmetry hidden node symmetry elser schmidt yedidia extends training bias update hidden node independent setting bias initial bias symmetry data neutral fashion incompletely sparse network extreme symmetry correspond infinite bias setting network sparse expander network generalizes network sparsity hidden node representation data expand layer layer constant growth factor layer expander structure cannot apply layer fix output node layer completely thereby information hidden layer available output node denote hidden layer sparse expander network input parameter introduce uniformity network constant input hidden node integer node hidden layer generate simplest random algorithm pas hidden layer per node layer uniformly node currently pas completion node layer hidden layer input layer construct differently construction implement publicly  program expander report hyperparameter ass role depth performance degrade limit network replace simpler hidden layer model limit apply sparse expander network hidden node sample fix aji rectifier gate conservatively rectify network suffices specify network sda algorithm hyperparameters balance principle parameter sparse expander network convenient behavior respect network depth characteristic focus choice feature multiplier hyperparameter later critical insight network explore publicly available implementation sda algorithm  program user network layer input output compatible data network program expander github com  rectify monotone rectify network although  report regular interval specify data batch bearing actual training bias parameter update response classification batch merely frequency network data classification accuracy data average indicator variable unambiguous classification node fold addition classification accuracy  report quantity already misclassifications err iteration perform sda iter increase classification accuracy however overfitting situation err iter increase accuracy evidence phenomenon directly discern another statistic report  average sda iteration fix incorrect classification encounter batch  define  however misclassifications batch err iter static  report information reveal internal working network layer average activation hαi   layer output node active average data item decrease training bias increase lastly statistic relevant termination training frequency  irreversible misclassifications  latter arise whenever output node zero onset  signal training terminate activation sda multiple output zero target output remedy prevent  increase network fold classification ambiguity mention occurs output node zero ambiguous classification therefore  intel xeon 0GHz core  rate per iteration per network iter estimate nest majority function synthetic data synthetic data advantage data nest majority function NMF data nice feature nest systematically complexity data boolean function define boolean argument easy generate training supervise  parameterized boolean function evaluate frequency parameter prime argument instance define integer nest elser schmidt yedidia instance depth nest majority function variable node majority gate presence absence negation depth function correspond remove layer circuit function GF define via recurrence mod mod mod argument majority function mod mod interpret GF function correspond depth boolean circuit input construct majority gate instance function identity negate argument NMF negates define NMF therefore data generate function easy depends argument function rectify network input identify ignore others argument apply negation function etc become progressively harder monotone rectify network improvement classification accuracy network function rectify network hidden layer argument complicate possibility generalize data clearly generate training data sample boolean variable pseudorandom generator training sample data training algorithm function fix hidden layer increase network via growth factor accuracy plot function err training data bias occurs misclassification network accuracy maximum decline coincides onset  network imperfect classifier terminate training empirical maximum accuracy absence data  exceeds threshold explanation intrigue similarity unsuccessful accuracy plot seemingly random feature accuracy plot nearly reproduce random instance expander network emphasize really surprising generalization performance drastically deteriorate training network conservative fundamentally principle standard approach minimize overall loss function across training conservative merely  account recently negative consequence training unseen network depth although feature network deeper network elser schmidt yedidia network comparable function deeper network successful data successful accuracy plot err faster shallower network plot iter deeper network sda iteration evenly distribute training  plot err iteration training shallow network balance heuristic successful layer average activation uniform training network hαi hαi deeper successful network diminish activation hαi hαi hαi complexity function reflect network network hidden layer successfully misclassifications correspond err network err accuracy achieve roughly err prediction accuracy NMF data network improve decrease multiplier hyperparameter optimal performance achieve model comprise layer rectifier gate fix aji strike function sparse expander graph already encounter decrease balance accuracy network already downside decrease increase training data err codewords aji associate sparse expander network network depth almost rectifier random input monotone rectify network plot sda iteration average iteration sda algorithm fix classification training network feature likely performance model improve codewords beyond generate sparse expander network finding surprising conventional neural network hidden layer easily outperform rectify model NMF data fully relu network layer hidden node sufficient completely sample sample err inform bias rectify model data vast reduction hidden node penultimate layer network doubt attributable elser schmidt yedidia evolution prediction accuracy multiplier hyperparameter network function standard model capacity codewords model static mnist image mnist handwritten digit analog data contrast image approximate pixel alphabet rectify network data report approach however processing mnist non symbolically analog data vector described equation motivation analog approach mnist image highly compressible strength rectify network advantageous mnist image image data vector component nearly statistically independent principal component analysis singular decomposition approximate matrix flatten mnist training image UV matrix orthonormal eigen image associate singular flatten image training mapped analog channel apply component wise cumulative probability distribution function component data vector equation compute cumulative probability function training data function processing data sample minimum maximum training sample mapped respectively monotone rectify network accuracy training sparse expander network analog mnist data channel network conventional network fully layer hidden node err misclassifications misclassifications training data training cease accuracy attains maximum evolution  training generalization accuracy improves decrease hyperparameter balance however improvement modest NMF data  training although data epoch sda anything analog mnist data sda training stochastic gradient descent sgd training model network architecture sparse network sda gpu software sgd optimizer sda core without gpu strictest comparison sgd monotone loss function sda sgd multi hinge loss margin parameter loss spoil tractability monotone bias therefore applicable sda mini batch sgd fix employ standard stochastic gradient descent without momentum sgd poorly sda monotone loss rate training accuracy maximum accuracy mode operation sda adaptive gradient deactivation sgd fold reduction rate training epoch data sda improvement training accuracy interpret conservative principle gain parameter optimally adjust response individual data performance sgd improves significantly rectify model multi hinge loss however due structure expander network loss problematic apply exclusively evaluate initial expander network active partial derivative loss respect bias output identically zero occurs bias layer penultimate layer node completely output leaf hinge loss unchanged avoid zero derivative hybrid approach hinge loss data monotone loss deactivation symmetry layer switch hinge loss epoch accuracy switch epoch improve training accuracy respectively trend improvement training switch epoch attribute modest improvement generalization sda model margin hinge loss insert boundary overfitting sda limit analog data vector channel essentially classification accuracy elser schmidt yedidia classification accuracy mnist image principal component data vector sda algorithm sparse expander network parameter sda iteration per misclassification data network obtain uncompressed symbolic data latter generate raw mnist image thresholding pixel maximum minimum pixel histogram define evolution accuracy fix depth network increase network monotone rectify network classification accuracy binarized mnist image rectify network hidden layer accuracy  onset  however already network situation training terminates training data correctly classify remain ambiguous zero output  termination  contribute reduction accuracy increase network accuracy termination  accuracy obtain analog data although mnist improvement linear classifier accuracy pairwise boost linear classifier appropriate comparison tractable vector machine gaussian kernel already achieve accuracy mnist technique mnist review markov sequence application synthetic symbolic dataset unlike nest majority function data sample generate probabilistically data alphabet generate markov chain extract markov chain define transition matrix elser schmidt yedidia sda iteration per misclassification data network CAD         bda  extract markov chain transition matrix transpose doubly stochastic transpose defines another markov chain source doubly stochastic ensures frequency finally absence zero bigram finite probability machine manage reconstruct data perfect classification accuracy occurs finite probability chain optimal classifier selects chain probability criterion apply positive rate network positive rate maximum achievable classification accuracy classification accuracy network approximately depth network maximum accuracy probabilistic monotone rectify network classification accuracy generate markov chain transpose classification accuracy generate markov chain transpose data accuracy eventually decline probability label encounter deeper network vulnerable phenomenon accuracy achieve optimal classifier shortfall explore decrease hyperparameter data elser schmidt yedidia discussion rectify model significant departure standard model neural network training algorithm derive model sda deserves comparison algorithm standard model stochastic gradient descent sgd algorithm network parameter update amount data item sda minibatch sgd computation update involves propagate data vector pas data associate loss backward pas sda extra pas determines update signal deactivation sgd update rate hyperparameter update average mini batch another hyperparameter characteristic sda differentiates sgd monotone evolution network parameter ostensibly problematic update data tacitly assumes network unique nearly alternative hypothesis worth multiplicity equally viable sda hypothesis bias setting perfect classification obtain data markov chain generate sda  outlier data although network probabilistic classification ambiguity accuracy optimal classifier asymptotic sda successful mnist dataset algorithm suffer overfitting whereas network perfectly classify training data classification accuracy data reflect fundamental deficiency rectify model conservative principle generally execution serious rectify model expose upon decrease multiplier hyperparameter recall primary motivation model tractable training presence network depth significant simplification standard neural network model fix replace loss constraint facilitate tractable training expressivity remain bias parameter remains arbitrary boolean function training algorithm sda fully utilizes activation relu nonlinearities layer network parameter introduce explore behavior respect depth bias output input node limited network sparse expander net experimental uniformly limit rectify model reduces simpler model comprise layer rectifier gate limit model non trivial definitely lack depth rectify model highlight difference capacity depth learnability depth although exist bias setting model efficiently complex boolean function utilize activation bias setting monotone rectify network tractable conservative route conclusion perform direction explore conclude conservatively rectify network lack capacity depth obvious architecture sparse expander network layer although symmetry initialize bias parameter zero specialized architecture inherently exploit depth standard model network convolutional layer recall bias update sda algorithm conservative quadratic program something efficient sda algorithm generalpurpose solver update quality training route aggregate entire mini batch conclusion another avenue define conservative norm bias update choice norm motivate uniqueness optimal update however likely mini batch norm unique optimum finally network slowly training depth parameter sophisticated scheme average