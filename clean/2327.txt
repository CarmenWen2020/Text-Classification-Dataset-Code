propose constructive approach estimate sparse dimensional linear regression model approach computational algorithm motivate kkt penalize generates sequence iteratively detection primal dual information refer algorithm sdar brevity sparse  matrix probability estimation error sequence decay exponentially minimax error bound iteration important predictor relative magnitude nonzero target coefficient mutual coherence estimation error decay optimal error bound iteration moreover sdar recovers oracle estimator within finite iteration probability sparsity computational complexity analysis sdar per iteration adaptive version sdar practical application sparsity unknown simulation demonstrate sdar outperforms lasso MCP greedy accuracy efficiency keywords geometrical convergence kkt  error bound oracle detection introduction linear regression model response vector matrix normalize vector underlie regression coefficient vector random focus model sparse relatively predictor important without constraint exist infinitely highly undetermined linear usually data assumption sparse important nonzero relative estimate minimization min  kβk sparsity however generally NP hence tractable stable algorithm dimensional setting propose constructive approach approximate penalize approach computational algorithm motivate kkt lagrangian approximate sequence kkt equation iteratively detection convergence achieve brevity refer propose approach sdar literature review approach propose approximate lasso norm constraint instead norm popular  matrix sparsity assumption lasso model selection consistent lasso convex minimization algorithm propose lars  coordinate descent proximal gradient descent however lasso tends  coefficient bias estimate adaptive lasso propose  analyze dimension achieve oracle requirement minimum nonzero coefficient optimal nonconvex penalty smoothly clipped absolute deviation scad penalty minimax concave penalty MCP capped penalty propose remedy minimum signal strength achieve recovery penalize regression although global minimizers local minimizers nonconvex regularize model eliminate estimation bias enjoy oracle compute global local minimizers desire statistical challenge optimization nonconvex nonsmooth numerical algorithm nonconvex regularize variant minimization maximization algorithm multi stage convex relaxation local quadratic approximation  local linear approximation  decompose penalty difference convex coordinate descent algorithm coordinate descent gauss seidel version coordinate descent jacobian version iterative thresholding algorithm generate sequence objective function nonincreasing convergence sequence generally unknown moreover sequence generate multi stage convex relaxation lasso converges converges stationary enjoy oracle statistical lasso solver per iteration propose globally convergent primal dual active algorithm nonconvex regularize recently effort  proximal gradient local minimizers desire statistical another research concern greedy orthogonal pursuit omp approximately iteratively variable correlation residual roughly performance omp guaranteed submatrices orthogonal matrix fan propose marginal correlation independence screen SIS equivalent formulation penalize univariate regression screen fan recommend iterative SIS improve finite sample performance iterative SIS core omp feature iteration recently developed greedy aim variable remove variable adaptively iterative thresholding IHT thresholding gradient descent GraDes adaptive backward selection foba liu propose mixed integer optimization mio approach penalize classification regression penalty combination penalty however dimensional mio approach subset selection linear regression constraint approach moderate sample moderate dimension min huang jiao liu  author computation columbia performance compute facility commercial mio solver gurobi comparison propose approach dimensional model simulation personal laptop computer contribution sdar approach fitting sparse dimensional regression model penalize sdar generates sequence kkt penalize criterion primal dual active regularize regularization parameter iteration explain detail sdar achieves estimation error bound within finite iteration specifically sparse  sparsity assumption achieves minimax error bound constant factor probability iteration important predictor relative magnitude nonzero target coefficient definition mutual coherence sparsity assumption achieves optimal error bound iteration probability coincides oracle estimator iteration respectively available minimum magnitude nonzero optimal magnitude detectable signal aspect iteration sdar achieve optimal error bound underlie sparsity appeal feature triple analyze computational sdar per iteration comparable exist penalize greedy summary contribution propose approach fitting sparse dimensional regression model approach seek directly approximate kkt equation penalize sequence generate sdar achieves error bound within finite iteration adaptive version sdar simply ASDAR tune model data driven procedure bic simulation demonstrate sdar ASDAR outperforms lasso MCP greedy accuracy efficiency generate model penalize regression notation vector denote norm  nonzero kβk denote vector matrix XA xab submatrix respectively vector indicator function denote supp denote supp  min kth absolute minimum absolute respectively denote operator norm induced vector norm kxk identity matrix organization develop sdar algorithm penalize establish  error bound sdar adaptive sdar ASDAR analyze computational complexity sdar ASDAR sdar greedy screen conduct simulation evaluate performance sdar ASDAR lasso MCP foba  conclude remark proof appendix derivation sdar lagrangian regularize minimization min   lemma coordinate wise minimizer satisfies thresholding operator define conversely satisfy local minimizer remark lemma kkt regularize minimization derive scad MCP capped regularize model derive replace thresholding operator correspond thresholding operator detail huang jiao liu supp suppose rank XA definition  XA equation iteratively kth iteration approximate obtain update approximation  XAk suppose integer choice initial obtain sequence aspect sdar detect sum primal dual approximation calculate detect therefore sdar iterative kkt equation important modification iteration sdar combine adaptive thresholding primal dual information fitting summarize sdar algorithm penalize regression coe  sdar coe  MCP coe  lasso sdar MCP lasso component sdar gradually increase lasso MCP decrease sdar along MCP lasso data generate model described lasso compute lars sdar function model model comparison MCP lasso function penalty parameter prespecified interval sdar selects component correctly spurious estimate estimate coefficient spurious zero algorithm detection sdar kkt  XAk denote iteration     ensure Aˆ Iˆ estimate remark sdar sequence generate sdar supp sdar active output oracle estimator huang jiao liu  error bound  error bound sequence generate sdar algorithm satisfies sparse  src spectrum bound   denote src src spectrum diagonal sub matrix gram matrix XX spectrum diagonal sub matrix bound sparse orthogonality constant define   another useful quantity mutual coherence define maxi characterizes minimum angle useful quantity summarize lemma appendix addition regularity matrix another sparsity regression parameter sparsity assume regression parameter nonzero zero nonzero coefficient relatively strict sparsity realistic strictly sparse index component typically min max transform non exactly sparse model exactly sparse model component RJ magnitude component outside RJ exactly sparse without loss generality simplicity exactly sparse oracle estimator define arg   generalize inverse XA XA XA rank obtain predictor correspond component model predictor obviously exactly sparse penalize regression error bound integer algorithm assumption matrix error vector input integer algorithm satisfies input integer algorithm src random error independent identically distribute zero sub gaussian exists exp  exp define max define theorem input integer algorithm suppose assume assume probability RJ remark theorem establishes bound approximation error sequence generate sdar algorithm iteration vector bound active iteration upper bound estimation error error bound decay geometrically model error constant factor specializes sub gaussian huang jiao liu remark assumption sdar nonzero feature src analysis lasso MCP sufficient matrix satisfy src proposition zhang huang src mutual coherence closely related rip restrict isometry constant appendix verify sufficient sub gaussian assume literature sparse estimation slightly weaker standard normality assumption calculate probability maximal function vector remark greedy algorithm assumption related sparse  zhang omp zhang analyze backward greedy algorithm foba properly chosen parameter GraDes analyze rip related imply norm estimation error sdar mention greedy achieves minimax error bound remark comparison sdar greedy algorithm corollary suppose JM define furthermore assume JR suppose probability JM define furthermore assume probability JR suppose exactly sparse sdar suppose probability penalize regression KR KR iteration sdar output oracle estimator remark corollary sdar sequence achieves minimax error bound constant factor within finite iteration iteration JR sparsity relative magnitude coefficient important predictor sparsity nonzero coefficient model sdar oracle estimator KR iteration probability remark suppose exactly sparse  corollary implies sufficiently rip constant  solves min kβk  assume normalize normalize however nontrivial task dimensional setting comparison sdar involves computational remark exactly sparse corollary implies sdar achieves minimax error bound probability KM error bound error bound sdar replace mutual coherence satisfies define max define theorem input integer algorithm huang jiao liu assume  assume probability  RJ remark theorem establishes bound approximation error sequence iteration vector bound iteration upper bound estimation error error bound decay geometrically model error constant factor specializes sub gaussian corollary suppose  furthermore assume suppose probability  furthermore assume penalize regression suppose exactly sparse sdar suppose probability iteration sdar output oracle estimator remark theorem corollary derive theorem corollary respectively relationship norm norm separately weaker assumption brings insight sdar error bound iteration complexity sdar underlie sparsity corollary remark mutual coherence omp lasso assumption exactly sparse noiseless    omp recover exactly noisy  omp recover cai wang analysis omp mutual coherence analysis omp however obtain error bound available omp literature furthermore corollary implies iteration sdar sparsity surprising literature greedy remark  zhang derive estimation error bound lasso respectively however nontrivial lasso solver compute approximate sdar involves computational remark suppose exactly sparse corollary implies error bound  achieve probability remark suppose exactly sparse corollary implies probability oracle estimator recover sdar minimum magnitude nonzero optimal magnitude detectable signal remark iteration corollary depends relative magnitude sparsity numerical improves corollary surprising iteration greedy recover depends garg  huang jiao liu average iteration sdar average iteration sdar increase average iteration sdar independent replication data generate model described sparsity increase average iteration sdar remains stable assertion corollary numerical comparison iteration greedy brief description proof detailed proof theorem corollary appendix proof src mutual coherence sdar iteratively detects solves therefore convergence sequence generate sdar sequence active approximate accurately increase norm norm difference kth iteration norm coefficient crucial decay geometrically bound constant factor define intrinsic error due approximate error specifically effort spent establish inequality lemma appendix theorem theorem constant matrix src penalize regression mutual coherence critical role establish clearly inequality useful another useful inequality  positive constant matrix lemma appendix src mutual coherence establish inequality norm norm respectively combine theorem theorem inequality vector sub gaussian assumption sum  approximation error RJ universal probability remain theorem corollary adaptive sdar sparsity model usually unknown data driven procedure upper bound important variable sdar algorithm tune parameter role penalty parameter penalize sdar fan upper bound model consistently estimate sample obtain corresponds null model data driven criterion  Tˆ Tˆ estimate overall computational complexity lnp compute increase along subsequence integer geometrically increase subsequence reduce computational scenario tune tune continuous penalty parameter penalize indeed simply increase along subsequence comparison tune  interval λmin λmax λmax corresponds null model λmin grid λmin λmax λmin λmin corresponds model numerical implementation coordinate descent algorithm lasso MCP scad λmin  somewhat λmin however meaning model meaning λmin explicit option iteration accord criterion sdar gradually increase consecutive propose recover sparse residual sum huang jiao liu prespecified inspire sdar increase gradually residual sum prespecified summarize algorithm algorithm adaptive sdar ASDAR initial integer integer criterion optional algorithm initial denote output criterion satisfied ensure Tˆ estimation computational complexity float operation algorithm clearly flop conjugate gradient CG linear equation iteratively CG iteration operation matrix vector multiplication flop precomputed therefore CG iteration ensures flop calculate matrix vector flop flop overall per iteration algorithm corollary iteration algorithm therefore overall algorithm exactly sparse approximately sparse ASDAR algorithm assume ASDAR discussion overall algorithm bound lnp efficient dimension increase linearly ambient dimension comparison greedy screen comparison sdar greedy screen omp foba IHT GraDes SIS greedy iteratively remove variable project response vector onto linear subspace span variable already penalize regression sdar characteristic however omp foba variable per iteration correlation dual variable notation sdar selects variable sum primal dual information interpretation dimensional clarify difference approach XX   hence sdar approximate underlie accurately omp foba simulation IHT GraDes formulate HK skd HK thresholding operator others chosen rip constant IHT GraDes respectively IHT GraDes primal dual information detect sdar approximate active sdar fitting accurate thresholding simulation propose iterative thresholding algorithm dimensional sparse regression linear regression algorithm propose GraDes stage IHT involves refit detect extend gradient thresholding loss convex loss analyze estimation sparsity recovery performance propose restrict strongly convexity RSS restrict strongly smoothness rsc derive error estimate approximate oracle norm difference sdar stage IHT propose sdar solves iteration stage IHT involves regularity sdar concern submatrices regularity stage IHT involves  applicable approximately sparse model sparse corollary iteration complexity sdar comparison iteration complexity stage IHT establish norm estimation iteration sdar independent sparsity corollary criterion sdar archive finitely corollary corollary however discus issue huang jiao liu fan propose SIS dimension reduction  dimensional liner regression selects variable absolute improve performance SIS fan iterative SIS iteratively selects feature desire variable report iterative SIS outperforms SIS numerically however iterative SIS lack theoretical analysis interestingly sdar initialize exactly SIS sdar iterative SIS active sdar sum primal dual approximation iterative SIS dual simulation implementation implement sdar ASDAR foba GraDes MCP matlab foba matlab implementation package developed zhang optimize rank update greedy implementation MCP iterative  algorithm publicly available matlab package lars  package lars foba variable variable addition default reduce computation algorithm improve accuracy prevent overfitting GraDes optimal gradient depends rip constant NP compute garg  GraDes residual norm maximum iteration compute MCP optimal  iteration residual norm  estimate ASDAR algorithm iteration residual  accuracy efficiency accuracy efficiency sdar ASDAR lasso lars MCP GraDes foba moderately nonzero coefficient sample dimension model nearly limit reasonably estimate lasso generate matrix generate random gaussian matrix entry normalize generate underlie regression coefficient generate nonzero coefficient uniformly distribute observation vector generate independently penalize regression independent replication correlation comparison fourth average relative error define   average cpu standard deviation cpu relative error parenthesis boldface performer numerical relative error cpu data  lars MCP GraDes foba ASDAR sdar lars MCP GraDes foba ASDAR sdar lars MCP GraDes foba ASDAR sdar correlation MCP foba sdar ASDAR average error  sdar ASDAR faster correlation increase foba becomes accurate sdar ASDAR MCP sdar ASDAR accuracy sdar ASDAR standard deviation cpu relative error MCP sdar ASDAR setting influence model parameter model parameter performance ASDAR lars MCP GraDes foba closely huang jiao liu simulation matrix drawn independently σjk error vector generate independently max min underlie regression coefficient vector generate randomly chosen subset observation vector parameter data generate model described ASDAR specify  tune parameter simulation independent replication influence sparsity panel influence sparsity probability recovery ASDAR lars MCP GraDes foba data generate model sample increment ASDAR MCP eliminate maximum sparsity solver perform recover increase lars fail recover  phenomenon garg  MCP fail GraDes foba fail comparison ASDAR influence sample panel influence sample probability correctly estimate data generate model performance becomes increase however ASDAR performs others simulation ASDAR capable handle dimensional data generate model influence ambient dimension panel influence ambient dimension performance ASDAR lars MCP GraDes foba data generate model probability exactly recover underlie coefficient ASDAR MCP solver increase ASDAR MCP robust ambient dimension influence correlation panel influence correlation performance ASDAR lars MCP GraDes foba data generate model performance penalize regression solver becomes correlation increase however ASDAR generally perform increase probability lars MCP ASDAR GraDes foba probability lars MCP ASDAR GraDes foba probability lars MCP ASDAR GraDes foba probability lars MCP ASDAR GraDes foba numerical influence sparsity panel sample panel ambient dimension panel correlation panel probability recovery solver summary simulation demonstrate sdar ASDAR generally accurate efficient stable lasso MCP foba GraDes iteration subsection sdar GraDes IHT iteration independent replication data generate model described average iteration average absolute error norm displayed iteration GraDes increase almost  sparsity increase sdar almost varies average error sdar  accurate GraDes empirical theoretical corollary huang jiao liu comparison dependence iteration panel accuracy panel sparsity data conclude remark sdar constructive approach fitting sparse dimensional linear regression model appropriate establish  minimax error bound optimal error bound sequence generate sdar calculate iteration achieve bound surprising aspect mutual coherence matrix iteration sdar achieve optimal bound underlie sparsity addition sdar computational complexity per iteration lars coordinate descent greedy simulation demonstrate sdar ASDAR accurate stable easy implement competitive outperforms lasso MCP greedy efficiency accuracy generate model theoretical numerical sdar ASDAR useful addition literature sparse model linear regression model generalize sdar model loss function sparsity structure penalize regression develop parallel distribute version sdar multiple core data data distributively implement sdar matlab package sdar available http homepage stat  edu  acknowledgment grateful action editor anonymous reviewer detailed constructive comment considerable improvement patrick  critical reading helpful comment research national foundation china  grant appendix proof lemma proof   suppose coordinate wise minimizer argmin argmin  xik argmin argmin definition thresholding operator conversely suppose definition deduce furthermore XA equivalent argmin      huang jiao liu positive sufficiently minimize deduce completes proof lemma lemma disjoint subset assume src sparse orthogonality constant mutual coherence XT  XT     furthermore  kuk moreover increase function decrease function increase function proof assumption src implies spectrum  identity matrix  submatrix  spectrum norm XX   implies  disk theorem KGA suffices KGA auk kuk kuk KGA auk  kuk kuk assertion definition completes proof lemma lemma suppose max  penalize regression proof lemma sub gaussian assumption standard probability calculation zhang huang  detail define notation useful theorem integer sequence active generate sdar algorithm define quantity difference norm coefficient crucial proof denote cardinality notation easily understood difference active target index incorrect index respectively index lose kth iteration iteration index gain kth iteration iteration algorithm subsection described overall approach theorem proceed proof argument lemma approximation model sum  approximation error RJ universal probability sub gaussian lemma norm norm lemma bound norm lose index norm lose index huang jiao liu lemma orthogonality norm lose index bound norm gain index lemma upper bound norm gain index sum norm combine lemma desire relation lemma theorem lemma lemma suppose probability src define define proof  RJ assumption src arbitrary vector forth      kβk  kβk kβk inequality inequality inequality fourth algebra implies definition RJ inequality probability   RJ penalize regression therefore monotone increase definition arbitrariness argument replace  RJ therefore probability     RJ implies lemma definition arbitrariness completes proof lemma lemma suppose src proof   XAk XAk    XAk huang jiao liu equality definition algorithm equality XAk XAk equality algebra definition therefore  XAk  equality supp equality inequality inequality inequality definition definition prof inequality imply argument proof inequality completes proof lemma lemma furthermore assume  src proof definition prof similarly penalize regression prof similarly XAk  XAk XA  XAk XAk   equality definition equality definition equality algebra inequality inequality definition inequality monotonicity definition prof finally index satisfy  xik XAk    equality derive equality proof replace inequality inequality inequality definition rearrange inequality completes proof lemma lemma max min min proof definition algorithm definition inequality algebra inequality completes proof lemma huang jiao liu lemma furthermore suppose  mutual coherence src proof definition inequality vanishes  XAk equality derive equality proof replace inequality inequality definition inequality monotonicity implies finally similarly completes proof lemma lemma suppose src  penalize regression proof inequality inequality inequality fourth inequality sum inequality implies definition    inequality inequality inequality fourth inequality inequality lemma definition completes proof lemma proof theorem proof suppose repeatedly 1D huang jiao liu   inequality inequality algebra definition completes proof theorem completes proof theorem proof corollary proof JM inequality algebra  JM JR inequality assumption inequality algebra implies JR prof proof omit suppose exactly sparse sdar algorithm algorithm probability KR indeed  assumption completes proof corollary penalize regression proof theorem proof satisfy algebra theorem similarly theorem omit completes proof theorem proof corollary proof proof corollary omit suppose exactly sparse sdar probability  assumption completes proof corollary