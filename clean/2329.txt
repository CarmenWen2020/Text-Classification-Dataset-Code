kernel regression observation sequentially goal recover underlie function assume belong RKHS variance assume context tackle tune regularization parameter adaptively maintain tight confidence bound estimate function generalize exist finite dimensional linear regression fix regularization variance kernel setup regularization parameter measurable function observation appropriate normalize inequality upper bound estimate variance bernstein concentration bound latter define adaptive regularization bound technique valid uniformly observation literature numerical finally potential illustrate application kernelized bandit revisit kernel UCB kernel thompson sample procedure benefit novel adaptive kernel tune strategy keywords kernel regression online adaptive tune bandit introduction application online optimization unknown noisy function define possibly domain kernel regression possibly non linear function information across observation technique variety application hyperparameters optimization active preference reinforcement generally rely kernel regression estimate function decision observation algorithmically standard kernel regression involves regularization parameter account complexity unknown target function variance theoretical approach rely fix regularization parameter heuristic tune parameter adaptively however price loose theoretical guarantee indeed theoretical guarantee concentration inequality exist approach regularization parameter kernel regression fix quantity assume prior tight knowledge variance unrealistic cumbersome assumption adjust regularization parameter kernel regression deterministic quantity choice regularization conveys bayesian interpretation intuition empirical estimate function observation tune regularization automatically however non trivial due data allows measurable function observation concentration bound empirical variance currently unknown kernel setup finally exist theoretical bound regularization parameter deterministic constant parameterization explicitly depends observation goal rigorous perform online tune kernel regularization preserve theoretical guarantee confidence interval context kernel regression unknown adaptive tune practical perspective retains theoretical guarantee gently contribution theorem generalizes exist concentration abbasi wang freitas explicitly regularization parameter pave theorem regularization tune online afterwards introduce variance estimator theorem yield empirical upper bound function plug estimate empirical bernstein concentration corollary kernel regression variance estimate tune regularization parameter application kernelized bandit regret bound kernel UCB kernel thompson sample procedure derive discus approach finally potential previously introduce exist alternative numerical postpone proof appendix kernel regression predictable sequential regression learner observation unknown function assume belong function random assume sub gaussian predictable model kernel regression unknown variance assumption predictability generate observation predictable filtration measurable measurable assumption sub gaussian model sub gaussian predictable model non negative constant exp  kernel function continuous symmetric positive definite compact equip positive finite borel denote correspond RKHS information gain quantity information obtain function sample define mutual information observation difference marginal entropy conditional entropy distribution observation information gain quantifies reduction uncertainty observation multidimensional gaussian  det linear information gain typically information gain effective dimensionality instead dimension effective dimension correspond informative effective dimension observation coverage increase information gain extend information gain regularization definition information gain unknown variance define information gain regularization parameter generalization theorem information gain inversely proportional regularization flexibility regression model regularization limit impact observation model therefore limit information gain durand maillard pineau concentration bound prediction error standard regularize kernel estimate regularization fix parameter theorem kernel assume sub gaussian predictable model parameter define posterior variance   vector probability simultaneously  quantity information gain remark extension abbasi theorem finite dimensional possibly infinite dimensional function specifically linear kernel theorem recovers exactly theorem abbasi generalization non trivial laplace amend apply beyond linear remark uniformly importantly thanks random construction related occurrence normalize inequality handle contrast wang freitas separately def kkt however neither exactly assumption former assumption bound norm RKHS upper bound essentially kernel chosen capture detail   estimate tune sequence regularization parameter tune adaptively observation modify previous valid deterministic statement kernel regression unknown variance theorem kernel online tune assumption theorem predictable positive sequence parameter measurable assume positive constant define modify posterior variance kλt  probability simultaneously kλt proof appendix regularization parameter therefore conjunction previous data posterior regression model variance return acquire observation remark measurable theoretical guarantee virtually adaptive tune procedure regularization parameter remark assumption naturally satisfied choice regularization variance estimation focus estimation variance parameter unknown loosely theorem suggests define sequence min initial loose upper bound upper bound estimate built observation  ensures measurable satisfies probability crux define upper bound estimate variance estimate obviously sub gaussian assumption tight inequality remains valid replace convey minimality assume sequence sub gaussian sub gaussian exp corresponds  generate function chi distribution freedom assumption naturally gaussian variable durand maillard pineau remark avoid technicality assume exactly trivially sub gaussian PT denote slightly bias variance estimate regularization parameter theorem kernel variance estimate assume predictable sub gaussian regression model predictable positive sequence introduce quantity finally max   introduce variance bound define differently deterministic upper bound         maxt  maxt probability simultaneously proof appendix remark absolutely bound challenge intuitive recover sample bound theorem appendix intuition observation correspond bound becomes trivial remark variance bound theorem systematically factor suggests proportional justification target upper bound remark advice bound kernel regression unknown variance quantity computable directly however directly information gain recall information gain inversely regularization hence therefore estimate upper bound bound define max initial bound bound estimate built observation  proceed estimate return compute quantity obtain estimate sandwich estimate procedure allows upper bound without prior knowledge compute predictable sequence described equation theorem hence simultaneously probability replace variance estimate union bound theorem derive confidence bound fully computable empirically context regularization parameter adaptively tune function unknown summarize empirical bernstein style inequality corollary kernel empirical bernstein inequality assume define bound max define correspond bound define upper bound min define regularization parameterizing regression model acquire observation accord equation probability valid simultaneously fλt kλt bλt bλt  proof denote kλt simultaneously denote decompose durand maillard pineau theorem tune propose procedure happens propose procedure simultaneously therefore bound probability simultaneously theorem naturally therefore remark fully empirical confidence envelope function around initial bound tight simply constant deterministic sequence theorem tightness estimate depends parameter compute probability construction adaptive yield tighter bound fix numerical application kernelized bandit application framework stochastic multi bandit structure embed RKHS bandit algorithm recommends sample compact observes noisy outcome  denote optimal goal algorithm sequence minimizes cumulative regret RT kernel regression unknown variance context tight confidence corollary illustrate technique bandit strategy upper confidence bound UCB thompson sample TS adapt kernel unknown variance extension lemma wang freitas variance estimate important role regret analysis algorithm lemma sum variance information gain assume kernel bound supx sequence instance satisfied probability equation kλt sequel useful bound confidence bound bλt equation lemma deterministic bound confidence bound assume constant confidence bound upper bound deterministic quantity bλt remark replace refine thanks confidence bound variance estimate kernel UCB unknown variance upper bound error directly UCB style algorithm formally vanilla UCB algorithm correspond argmax regret proof strategy abbasi minor modification yield guarantee regret strategy theorem kernel UCB unknown adaptive regularization probability regret kernel UCB adaptive regularization variance estimation satisfies recall bλt define equation RT kλt bλt RT durand maillard pineau remark simultaneously horizon extends abbasi kernel regression variance unknown assumes bound observation implies bound bound bound looser bound kernel TS unknown variance another application confidence bound analysis thompson sample kernel scenario TS algorithm kernel algorithm sample posterior distribution gaussian posterior posterior variance kernel estimate however series obtain provable regret minimization guarantee posterior variance inflate although vanilla version without inflation research owe novel confidence bound derive TS algorithm posterior variance inflation factor algorithm kernel TS adaptive variance estimation regularization tune input discrete parameter regularization sequence variance inflation factor compute posterior bft fλt compute posterior covariance  kλt sample bft   outcome remark algorithm variance upper estimate remark assume discrete merely practical otherwise update estimate RKHS memory computational unbounded simplifies analysis regret bound obtain careful easy adaptation agrawal goyal proof appendix independent rigorous somewhat simpler rewrite proof technique agrawal goyal theorem regularize kernel TS variance estimate assume maximal instantaneous pseudo regret maxx finite regret kernel TS algorithm bλt episode kernel regression unknown variance probability precisely probability regret bound RT kλt bλt CR  RT CR  remark confidence interval bound likewise regret probability without bound observation contrary earlier discussion related concentration theorem extends normalize bound abbasi linear function RKHS sub gaussian nontrivial adaptation laplace yield normalize inequality possibly infinite dimension generalizes wang freitas kernel regression already generalization previous bound concentration kernel regression assumption bound observation lemma proposition wang freitas denote function RKHS induced kernel define posterior variance arbitrary data assume sub gaussian variable information gain remark bound valid probability contrast abbasi theorem probability uniformly durand maillard pineau theorem extends theorem regularization tune online observation knowledge exists literature moreover theorem variance estimate confidence bound spirit maurer  theorem finally corollary specifies theorem situation regularization tune accord theorem yield fully adaptive regularization procedure explicit confidence bound bandit optimization apply multi bandit theorem respectively extend linear TS UCB RKHS extension literature GP UCB generalizes UCB linear RKHS gaussian corresponds bound target function belongs RKHS however loose  generalizes UCB linear RKHS kernel regression however analysis algorithm proof technique independence analyze instead arguably appeal variant  analysis GP UCB  agnostic respectively limited bound bound observation illustrative numerical illustrate introduce previous concentration theorem variance estimate theorem combine former estimate tune theorem corresponds corollary finally performance kernelized bandit technique variance estimate adaptative regularization scheme conduct function norm RKHS induced gaussian kernel function linear feature  taylor expansion randomly generate parameter vector zero gaussian upper bound bound kernel concentration bound fix regularization concentration theorem kernel concentration bound wang freitas report lemma assume observation uniformly sample fix confidence feature gaussian kernel kernel regression unknown variance function numerical observation observation observation observation theorem lemma confidence interval theorem lemma theorem recovers confidence envelope wang freitas however confidence bound plot theorem valid uniformly derive wang freitas valid separately theorem generalizes latter illustration illustrates confidence envelope potential benefit tune empirical variance estimate illustrate convergence rate estimate max min compute theorem durand maillard pineau observation observation observation observation theorem theorem confidence interval theorem estimate theorem fix dot indicates observation uniformly sample suggests tighter bound fix indeed adaptive update converges whatever initial bound loose initial upper bound bound theorem knowledge useful illustrate plot upper bound variance estimate minimum bound knowledge agnostic kernel regression unknown variance unknown variance estimate theorem without minimum bound upper bound dot indicates maximum estimate envelope recall kernel concentration bound adaptive regularization combine previous estimate tune regularization recall estimate bound max theorem compute upper bound estimate min theorem compute confidence interval corollary everywhere observation  sample illustrates confidence envelope fully empirical model upper bound recall satisfies plot confidence envelope obtain theorem fix improvement confidence interval observation recall challenge variance unknown regularization parameter tune online confidence bound valid uniformly kernelized bandit optimization evaluate potential kernelized bandit algorithm variance estimate linearly discretized recall goal minimize cumulative regret equation optimize function evaluate kernel UCB equation kernel TS algorithm bλt configuration oracle fix assume knowledge fix without prior knowledge adaptative regularization tune corollary configuration kernel UCB kernel TS regret bound respectively probability recall observation sample bandit algorithm configuration oracle cumulative regret durand maillard pineau observation observation observation observation fix adaptive confidence interval fix theorem adaptive corollary regularization episode cumulative regret kernel UCB oracle kernel UCB fix kernel UCB adaptative episode cumulative regret kernel TS oracle kernel TS fix kernel TS adaptative average cumulative regret along episode kernel UCB kernel TS average repetition oracle corresponds performance kernel UCB kernel TS knowledge plot confirm adaptively tune regularization variance estimate improvement fix non accurate initial phase regret adaptively tune algorithm increase rate oracle algorithm exactly kernel UCB outperforms kernel TS implies inflate variance kernel TS per theory previously optimal attention kernel regression unknown variance episode cumulative regret kernel TS theorem kernel TS lemma average cumulative regret standard deviation along episode kernel TS oracle theorem lemma evaluate benefit concentration bound theorem kernel TS algorithm oracle theorem lemma concentration bound theorem improves performance kernel TS exist concentration highlight relevance  regularization parameter allows advantage regularization rate adapt conclusion address online tune regularization parameter kernel regression online estimation variance extent introduce novel concentration bound posterior estimate kernel regression fix explicit regularization theorem extend regularization parameter tune theorem introduce upper bound estimate variance theorem estimate variance tune kernel regularization online fashion corollary retain theoretical guarantee propose derive kernelized variation bandit algorithm UCB thompson sample regret bound theorem propose illustrate numerical obtain relevance introduce kernel regression concentration interval explicit regularization regularization correspond variance potential propose regularization tune procedure illustrate application kernelized bandit benefit adaptive regularization  variance unknown usually finally strength propose adaptively tune regularization parameter preserve theoretical guarantee regularization tune validation durand maillard pineau future extension technique obtain empirical estimate kernel information assume available although preliminary direction theoretically motivate algorithm address concern important gap theory basis perform thompson sample RKHS extend contextual future  UCB acknowledgment funding engineering research council canada  canada  strategic network FRQ NT   machine inc acknowledges french     grant  CE project  inria lille  europe cper    data advanced data technology french ministry education research appendix laplace tune kernel regression simultaneously resort version laplace carefully extend RKHS proceed kernel function continuous symmetric positive definite compact equip positive finite borel countable sequence  orthonormal basis      belongs RKHS denote  analogy finite dimensional sequel martingale component analysis lemma hilbert martingale assume sequence conditionally sub gaussian exp  kernel regression unknown variance respect filtration generate variable  deterministic positive denote exp quantity define satisfies proof difficulty proof handle indeed thanks conditional sub gaussian immediate non negative super martingale actually satisfies convergence theorem nonnegative super martingale  almost surely define define introduce version min lim  lim   lemma concludes proof refer detail proof theorem kernel feature explicit denote correspond parameter sequence matrix built feature introduce infinite matrix vector decompose estimation indeed feature      morrison formula algebra yield obtain inequality appropriate matrix norm decomposition valid involve finite   durand maillard pineau application  morrison formula yield bound  remain  apply lemma however lemma apply measurable proceed upper bound expression involve    function  non increase lemma   introduce random define later apply lemma precisely infinite gaussian random sequence independent random variable denote  define clearly elementary algebra det det det det eigenvalue matrix eigenvalue kxk correspond det det det finite difficulty proof handle possibly infinite dimension approximation kernel regression unknown variance dimension sequence restriction correspond quantity component gaussian abbasi obtain det exp  obtain application  lemma lim  det lim exp  det lim conclude define abbasi min  det random  det finally combine previous remark obtain probability uniformly det lemma technical lemma function semidefinite positive matrix vector non decrease proof indeed sherman morrison formula obtain semi definite positive lim durand maillard pineau appendix variance estimation proof theorem proceed upper bound bound variance estimate theorem bound derive statement theorem regularize variance estimate sub gaussian predictable assumption random filtration probability   introduce convenience constant proof feature decomposition     via       likewise via  kernel regression unknown variance combine bound     λmin λmax λmin   λmax λmin λmin  λmin lemma probability   lemma obtain probability     combine union bound deduce probability  λmax   λmin   λmin durand maillard pineau derive bound indeed  λmax λmin  inequality inequality   λmax vuut λmin vuut   corollary extension corollary maillard probability simultaneously     maxt max   upper bound derive inequality probability    maxt kernel regression unknown variance proof theorem probability    inequality rewrite inequality AC conclude correspond probability proof theorem remark increase function lemma lemma maillard assume random satisfies almost surely   random introduce appendix application stochastic multi bandit proof lemma min  kλt min durand maillard pineau obtain cauchy schwarz inequality kλt proof lemma quantity bλt recall equation bλt  non increase non decrease bλt alternatively theorem random variable tighter instance theorem easily obtain probability   estimate satisfies implies likewise yield bλt proof theorem UCB algorithm kernel bandit denote instantaneous regret denote optimistic chosen built confidence UCB algorithm probability fλt fλt kλt bλt kernel regression unknown variance deduce probability RT kλt bλt lemma bλt lemma sum kλt yield bound regret RT proof theorem TS algorithm kernel bandit closely proof technique agrawal goyal clarify simplify split saturate unsaturated former designates sample probability dominate latter designates related optimism possibility sample optimum ebt  concentrate around respective precisely confidence introduce ebt fλt cbt fλt quantity cbt define ebt confidence bound cbt kλt bλt ebt ebt fλt introduce notation kλt union bound  kλt motivates definition  kλt durand maillard pineau chosen sequence choice max ensures  obtain summary definition ebt fλt fλt cbt kλt bλt  bλt  saturate convenient introduce saturate argmin remark construction strategy kernel TS algorithm  deduce ebt maxx remark definition kernel regression unknown variance likewise min min deduce ebt min min min bound denominator ebt inclusion combine deduce ebt fλt yield ebt fλt fλt ebt fλt cbt fλt cbt obtain fλt cbt ebt fλt cbt ebt fλt cbt ebt durand maillard pineau anti concentration resort anti concentration gaussian variable precisely inequality fλt cbt introduce measurable random variable cbt   bλt  bλt constant yield fλt cbt def summary proof ebt min  ebt ebt ebt min inequality combine bound definition obtain ebt min   pseudo regret sum previous obtain  kernel TS strategy satisfies ebt probability RT min   kernel regression unknown variance max constant  recall bλt  bλt specific choice satisfies yield RT min bλt min introduce deterministic quantity concentration proof relate sum sum precisely introduce random variable min min construction  application  inequality martingale obtain probability vuut probability RT min vuut replace expression max durand maillard pineau deduce probability RT    kλt bλt  concludes proof bound lemma rewrite regret RT kλt  lemma cauchy schwarz inequality finally obtain RT vuut 