enable remarkable progress variety task image recognition recognition machine translation crucial aspect progress novel neural architecture currently employ architecture mostly developed manually expert consume  automate neural architecture overview exist research categorize accord dimension strategy performance estimation strategy keywords neural architecture automl  strategy performance estimation strategy introduction perceptual task largely due automation feature engineering hierarchical feature extractor fashion data manually accompany however demand architecture engineering increasingly complex neural architecture manually neural architecture NAS automate architecture engineering logical automate machine already NAS outperform manually architecture task image classification detection semantic segmentation NAS subfield automl significant overlap hyperparameter optimization meta categorize NAS accord dimension strategy performance estimation strategy abstract illustration neural architecture strategy selects architecture predefined architecture performance estimation strategy return estimate performance strategy defines architecture principle incorporate prior knowledge typical architecture task reduce simplify however introduces bias prevent novel architectural building beyond knowledge strategy strategy detail explore exponentially unbounded encompasses classical exploration exploitation desirable perform architecture quickly premature convergence suboptimal architecture avoid performance estimation strategy objective NAS typically architecture achieve predictive performance unseen data performance estimation refers estimate performance simplest option perform standard training validation architecture data unfortunately computationally expensive limit architecture explore recent research therefore focus develop reduce performance estimation refer illustration article structure accord dimension strategy outline performance estimation conclude outlook future direction defines neural architecture NAS approach discover principle discus recent relatively chain structure neural network illustrate chain structure neural network architecture sequence layer layer receives input layer input output input output illustration architecture node graph corresponds layer neural network convolutional pool layer layer visualize layer layer denotes receives output input chain structure complex additional layer multiple skip connection output serf input layer parametrized maximum layer possibly unbounded operation layer executes pool convolution advanced operation depthwise separable convolution dilate convolution hyperparameters associate operation filter kernel stride convolutional layer simply fully network parameter hence parametrization fix conditional recent NAS incorporates craft architecture skip connection complex multi network illustrate input layer formally described function lout combine previous layer output employ function significantly freedom multi architecture chain structure network lout residual network previous layer output sum lout  previous layer output concatenate lout concat lout motivate craft architecture consist motif propose motif dubbed respectively input input output output input output illustration normal reduction architecture built stack sequentially combine complex manner multi simply replace layer architecture optimize normal preserve dimensionality input reduction reduces spatial dimension architecture built stack predefined manner illustrate advantage drastically reduce usually consist significantly layer architecture estimate previous achieve performance architecture built easily transfer adapt data simply filter within model indeed transfer optimize cifar imagenet achieve performance architecture building proven useful principle lstm rnns stack residual consequently successfully employ recent however choice arises namely macro architecture actual model sequential model receives output precede input employ structure manually architecture densenet within model principle combine arbitrarily within multi described simply replace layer ideally macro architecture micro architecture structure optimize jointly instead solely optimize micro architecture otherwise easily manual macro architecture engineering perform direction optimize macro architecture hierarchical introduce consists motif consists primitive operation motif primitive operation via acyclic graph motif encode motif hierarchical motif correspond cod macro architecture choice largely determines difficulty optimization fix  optimization remains non continuous relatively highdimensional complex model tend perform choice architecture fix vector dimensional categorical dimension chooses building input unbounded constrain potentially finite layer fix potentially conditional dimension discus strategy strategy strategy explore neural architecture random bayesian optimization evolutionary reinforcement RL gradient historically evolutionary algorithm already researcher evolve neural architecture decade    consists input chosen input operation apply input chosen dimensional representation originally another dimension sum concatenate operation within however choice discard output within sum yao literature review earlier bayesian optimization celebrate NAS vision architecture performance cifar without data augmentation  neural network competition data expert NAS become mainstream research topic machine community  obtain competitive performance cifar penn treebank benchmark strategy reinforcement  vast computational resource achieve gpus variety publish succession reduce computational achieve improvement performance frame NAS reinforcement RL generation neural architecture agent action action identical agent reward estimate performance architecture unseen data RL approach agent policy optimize  recurrent neural network rnn policy sequentially sample encodes neural architecture network reinforce policy gradient algorithm proximal policy optimization instead policy sequentially chooses layer correspond hyperparameters alternative approach sequential decision policy sample action generate architecture sequentially environment contains summary action sample  reward obtain action however interaction environment occurs sequential external intermediate reward intuitive interpret architecture sample sequential generation action simplifies RL stateless multi bandit related approach propose frame NAS sequential decision approach partially architecture reward estimate architecture performance action corresponds application function preserve mutation dubbed network morphisms training phase network variable network architecture directional lstm encode architecture fix representation encode representation actor network sample action combination component constitute policy reinforce policy gradient algorithm approach architecture twice alternative RL neuro evolutionary approach evolutionary algorithm optimize neural architecture approach neural network aware date almost decade genetic algorithm propose architecture backpropagation optimize neuro evolutionary approach genetic algorithm optimize neural architecture however contemporary neural architecture supervise task sgd optimization currently outperform evolutionary recent neuro evolutionary approach therefore gradient optimize solely evolutionary algorithm optimize neural architecture evolutionary algorithm evolve population model possibly network evolution model population sample serf generate offspring apply mutation context NAS mutation local operation remove layer alter hyperparameters layer skip connection alter training hyperparameters training offspring fitness performance validation evaluate population neuro evolutionary sample update population generate offspring tournament selection sample whereas sample multi objective pareto inverse density remove individual population beneficial remove individual decrease  remove individual generate offspring approach initialize network randomly employ  inheritance knowledge network network morphisms offspring inherit parameter affected apply mutation inheritance strictly function preserve random initialization moreover mutate rate optimize rate schedule NAS refer recent depth review neuro evolutionary conduct RL evolution random RS conclude RL evolution perform equally accuracy evolution anytime performance model approach consistently perform RS margin RS achieve error approximately cifar RL evolution approximately model augmentation depth filter increase difference non augment actually approx difference report error cifar validation error imagenet RS evolution respectively recent evolve competitive gradient optimization variance estimate gradient available reinforcement task nonetheless supervise task gradient optimization approach bayesian optimization BO popular hyperparameter optimization apply NAS typical BO toolbox gaussian focus  continuous optimization derive kernel function architecture classic GP BO contrast model parzen estimator random effectively dimensional conditional achieve performance optimize neural architecture hyperparameters jointly comparison lack preliminary evidence approach outperform evolutionary algorithm  gordon  exploit structure monte carlo propose perform algorithm discovers quality architecture greedily direction perform architecture without sophisticated exploration mechanism employ discrete propose continuous relaxation enable gradient optimization instead fix operation convolution pool execute specific layer author compute convex combination operation specifically layer input layer output compute  convex coefficient effectively parametrize network architecture optimize network network architecture alternate gradient descent training data validation data architectural parameter eventually discrete architecture obtain operation arg maxi layer instead optimize operation propose optimize parametrized distribution operation ahmed  employ gradient optimization neural architecture however focus optimize layer hyperparameters connectivity respectively performance estimation strategy strategy aim neural architecture maximizes performance accuracy unseen data strategy estimate performance architecture simplest training data evaluate performance validation data however training architecture evaluate scratch frequently yield computational demand gpu NAS naturally develop performance estimation discus refer overview exist performance estimate fidelity actual performance training denote proxy metric fidelity shorter training ups achieve reference fidelity estimate training reduce training epoch subset data  model  data curve extrapolation training reduce performance extrapolate epoch training inheritance network morphisms instead training model scratch inherit model shot model shot model across architecture subgraphs shot model   overview performance estimation NAS training subset data resolution image filter per layer fidelity approximation reduce computational introduce bias estimate performance typically underestimated problematic strategy relies rank architecture relative rank remains stable however recent relative rank dramatically difference cheap approximation evaluation argue gradual increase fidelity another estimate architecture performance upon curve extrapolation propose extrapolate initial curve terminate predict perform poorly architecture   architectural hyperparameters predict partial curve promising training surrogate model predict performance novel architecture propose employ curve extrapolation predict performance architectural extrapolate architecture training challenge predict performance neural architecture prediction relatively relatively evaluation another approach performance estimation initialize novel architecture architecture achieve dubbed network morphisms allows modify architecture function network unchanged gpu allows increase capacity network successively retain performance without training scratch training epoch additional capacity introduce network morphisms advantage approach without inherent upper bound architecture strict network morphisms architecture overly complex architecture attenuate employ approximate network morphisms shrink architecture shot architecture treat architecture subgraphs supergraph shot model architecture supergraph shot model various architecture subgraphs shot model evaluate without training inherit shot model greatly performance estimation architecture training evaluate performance validation data gpu shot model typically incurs bias underestimate actual performance architecture severely nevertheless allows rank architecture sufficient estimate performance correlate strongly actual performance however currently actually shot NAS shot model  learns rnn controller sample architecture shot model approximate gradient obtain reinforce DARTS optimizes shot model jointly continuous relaxation obtain mixture candidate operation shot model instead optimize operation DARTS  optimizes distribution candidate operation author employ concrete distribution reparametrization relax discrete distribution differentiable enable optimization via gradient descent overcome necessity entire shot model gpu memory   architectural mask per operation probability masked sample binarized architecture  update correspond probability shot model sufficient deactivate model stochastically training dropout  maxpool  illustration shot architecture network input node denote hidden node denote output node denote instead apply operation convolution node shot model contains candidate operation node namely convolution convolution maxpooling illustration shot model across architecture simply subgraphs shot model inspire previously mention approach optimize distribution architecture training approach fix distribution performance obtainable latter indicates combination fix carefully chosen distribution surprisingly ingredient shot NAS related approach   generate novel architecture training  architecture difference strictly generate  conditional sample architecture limitation shot NAS supergraph define priori restricts subgraphs moreover approach entire supergraph resides gpu memory architecture restrict relatively  accordingly typically combination approach substantially reduce computational resource NAS gpu currently understood bias introduce sample distribution architecture optimize along shot model instead fix instance initial bias explore others shot model adapt architecture reinforce bias premature convergence NAS correlation shot performance architecture systematic analysis bias introduce performance estimator desirable direction future future direction discus future direction research NAS exist focus NAS image classification challenge benchmark manual engineering devote architecture perform domain easily outperform NAS relatively easy define exploit knowledge manual engineering unlikely NAS architecture substantially outperform exist considerably architecture cannot fundamentally important beyond image classification apply NAS explore domain notable direction image restoration semantic segmentation transfer machine translation reinforcement optimize recurrent neural network model promising application NAS generative adversarial network sensor fusion additional promising direction develop NAS multi task multi objective resource efficiency objective along predictive performance unseen data highlight multi objective NAS closely related network compression aim perform efficient architecture hence compression NAS vice versa likewise extend RL bandit approach policy encodes task resource requirement contextual bandit direction ramachandran extend shot NAS generate architecture task instance moreover apply NAS architecture robust adversarial intrigue recent direction related research define flexible instance transferability image classification task largely image classification generalize easily domain cod hierarchical structure chain structure apply author optimize architectural choice reinforcement algorithm versatile policy network architecture convolutional recurrent building NAS RL semantic segmentation detection allows identify hierarchical structure NAS broadly applicable direction moreover predefined building convolution pool identify novel building beyond limitation substantially increase NAS comparison NAS reproducibility publish complicate measurement architecture performance factor architecture author report cifar data regard computational budget data augmentation training procedure regularization factor cifar performance substantially improves cosine anneal rate schedule data augmentation  mixup combination factor regularization shake shake regularization  therefore conceivable improvement ingredient impact report performance architecture NAS definition benchmark crucial comparison NAS direction benchmark propose consist approximately unique convolutional architecture pre evaluate multiple data training validation accuracy training model training budget multiple strategy hence computational resource benchmark simply query pre compute data previous pre evaluate joint neural architecture hyperparameters evaluate NAS isolation source automl hyperparameters data augmentation pipeline optimize along NAS NAS achieve impressive performance insight specific architecture architecture derive independent identify motif understand motif important performance investigate motif generalize desirable