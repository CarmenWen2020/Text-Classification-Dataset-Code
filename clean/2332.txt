multi restless bandit distribution stationary version realistic model application cannot optimally PSPACE objective characterize sub approximate tractable approach specifically coefficient modify version UCB effective challenge unlike distribution sample offs characteristic bandit necessarily overcome carefully sample policy distribution proof technique developed generally context online sample dependence propose algorithm accompany correspond regret analysis introduction simplest sequential optimization uncertainty multi bandit arise various application online advertisement internet rout typically assumption offs independently identically distribute independent however assumption necessarily practical situation online advertisement aim garner click user advert category associate category multi bandit dependence across user likely advert related selection recent multi bandit offs dependent evolves regardless played instance restless bandit optimal policy leverage inter dependency sample switch appropriate obtain overall distribution however switch strategy PSPACE distribution markovian dynamic papadimitriou  grünewälder  khaleghi license CC http creativecommons org license attribution requirement http jmlr org html grünewälder khaleghi  therefore useful relaxation aim devise computationally tractable effectively approximate optimal switch strategy approximation markovian restless bandit dynamic previously reference therein focus reward unknown distribution exhibit dependency markov chain interested sub restless bandit offs dependency finite analysis dependence weakens concentration inequality approach assume distribution stationary coefficient sequence random variable  amount dependence sub sequence  dependence vanishes notion formally define markov chain coefficient closely related markov chain distribution approach stationary distribution define distribution within stationary distribution variation distance sec classical  decrease distance distribution markov chain stationary distribution factor coefficient markov chain coefficient related markov chain correspond variety stochastic markov chain earlier optimal notoriously infeasible strategy version switch address relaxation obtain identify stationary viable approximation purpose characterize approximation error amount dependence offs optimum relaxed optimal switch strategy translates directly distribution weakly dependent address optimistic approach devise identify propose UCB algorithm achieves logarithmic regret respect stationary interestingly amount dependence bound offs recover regret bound familiar bandit corresponds recommendation objective personalize advert news massive online mooc user specific application bandit correspond appropriate category genre topic played item correspond category random user dependency offs reflect user memory observation however dependence naturally decay user memory influence dominant coefficient cap maximum frequency specific recommendation user minor modification sequential decision naturally posse dependency structure impose user memory APPROXIMATIONS restless bandit relaxed version straightforward challenge obtain confidence interval around empirical estimate stationary hoeffding concentration bound exist tempt inequality directly standard UCB algorithm however demonstrate unlike policy framework introduce coupling future offs distribution sample sequence standard UCB algorithm setting suitable equip concentration bound oversight previous literature specifically improve UCB approach   refer detailed discussion circumvent difficulty carefully random account policy distribution technical generally address online sample dependence finally multi bandit strongly dependent offs generality beyond scope complementary regime specifically bandit stationary gaussian slowly decay covariance function dependence scenario instance throughput channel slowly channel model bandit strongly dependent offs intuitive efficiently obtain approximately optimal dependency prediction future reward scarce observation switch strategy instance significantly outperforms policy aim regret bound algorithm directly reflect dependence offs dependence regret summary contribution attempt derive computationally tractable restless bandit identify optimal switch strategy approximate stationary loss stationary oppose switch strategy amount inter dependence reflect proposition detailed namely demonstrate challenge non bandit policy framework introduce coupling future offs sequence completely dependency structure develop technical machinery circumvent difficulty introduce inter dependence reward policy distribution derivation concern sample dependence independent propose UCB algorithm namely algorithm deploys identify stationary correspond random variable sample grünewälder khaleghi upper bound regret algorithm respect stationary theorem regret bound function amount inter dependence reflect coefficient offs recover regret bound thm along proposition argue dependence algorithm approximate switch strategy remainder organize introduce preliminary notation definition formulate conclude discussion preliminary useful notation definition concern stochastic bandit involves multiple extend definition jointly model multi bandit indeed setting jointly demonstrate proposition independent markov chain jointly notation denote extend respectively introduce abbreviation sequence finite subset sequence denote indexed XC sequence random variable indexed denote XC algebra generate XC notion dependence concern dependence  define definition probability denote  respectively dependence sup random variable measurable respect simplify notation denote dependence correspond algebra distinction context similarly XA XB finite sequence random variable dependence similarly define XA XB XA XB XA XB maximal difference probability conditional probability random variable indexed respectively notion dependence probability expectation random variable define probability denote information  denote kolmogorov conditional expectation measurable random variable theorem due bradley vol difference EX effectively upper bound APPROXIMATIONS restless bandit theorem bradley probability random variable kxk  sup supremum measurable random variable furthermore kxk trivially measurable theorem implies  stochastic BX measurable denote borel algebra denote infinite sequence indexed stochastic model probability denotes algebra generate cylinder associate stochastic sequence random variable projection onto stationary borel stochastic refers distribution associate sequence random variable reference context definition stationary stationary stochastic coefficient sup dependence gap limn model bandit concerned stochastic joint distribution stationary specifically fix probability probability obtain via cylinder denote borel algebra described associate joint sequence random variable  projection stationary stochastic interchangeably correspond sequence random variable  correspond joint generally finite interval grünewälder khaleghi definition jointly stationary fix stationary  coefficient sup dependence jointly limn denote coefficient correspond joint definition respectively notion apparent context assumption joint stationary dependence requirement fulfil joint assumption jointly fulfil variety model independent markov chain generally proposition independent joint define whereby dependence definition replace sup coefficient define manner analogous definition coefficient upper bound coefficient proposition probability mutually independent define joint coefficient joint upper bound coefficient individual proof appendix independent markov chain jointly specifically corollary independent markov chain jointly mutually independent stationary ergodic finite markov jointly stationary proof theorem bradley moreover proposition mutually independent jointly jointly stationary significance observation jointly mutually independent markov formulation jointly bandit assume sequel bandit corresponds stationary generates series offs furthermore assume joint definition sequence coefficient  kϕk APPROXIMATIONS restless bandit stationary denote max stationary sometimes denote stationary player chooses accord policy receives reward player objective maximize sum offs policy access offs gain earlier stage chosen  filtration payoff obtain policy sequence mapping measurable respect assumption measurable respect equivalent assumption policy function offs chosen thm  measurable denote policy denote filtration information available unobserved offs specifically  measurable zero define maximal achieve sup ext regret strategy simplify notation policy context remark kϕk standard literature empirical theory involve assumption already fulfil stationary ergodic finite markov chain theorem bradley coefficient correspond necessarily stationary markov exponentially rate decay  sequence coefficient focus sequel necessarily markov restless bandit reward distribution jointly  formulate recall optimal strategy switch obtain switch strategy PSPACE address computationally tractable approximation optimal policy obtain former characterize loss stationary oppose switch strategy coefficient optimum relaxed latter devise UCB algorithm identify stationary challenge building confidence interval around empirical estimate stationary indeed demonstrate unlike policy framework introduce coupling future offs sequence standard UCB algorithm suitable grünewälder khaleghi equip hoeffding concentration bound circumvent difficulty policy distribution analysis relies technical outline appendix independent finally weakly dependent reward distribution complement strongly dependent switch strategy significantly outperforms policy aim policy random recall policy function data sample bandit therefore decision sample generate random bandit played naturally model via random formally denote random variable determines sample denote xτi obtain sample random indicator function otherwise challenge devise policy bandit policy dependence structure sequence completely differs simpler distribution characteristic sample sequence illustrate bandit deterministically distribution described markov chain initial distribution stationary distribution transition matrix probability markov chain policy denote sequence random sample accord subsequent random xτn otherwise significantly guarantee distribution xτn xτn stationary distribution markov chain sample sequence generate highly dependent offs observation xτn stationary EX EX xτn due equation hence xτn APPROXIMATIONS restless bandit detailed treatment appendix indeed policy access data coupling future offs framework overlook   relies improve UCB identify stationary eliminate potentially sub optimal elimination depends data played remain hence random policy memory carefully consideration distribution sample sequence correspond notion account algorithm confidence interval involve correspond distribution sample sequence therefore invalid non approximation error translate expectation difference switch strategy achieve stationary prior delve bandit bound stationary  sample random fully define observation xτi difference sample  stationary proposition difference coefficient increment proof appendix proposition assume  stationary coefficient ext  furthermore sequence random xτi measurable fix  sample sample estimate stationary bias estimator bound coefficient implication something leverage switch policy switch policy selects effectively random played sum cannot  stationary policy random intuition underlies proposition proposition jointly stationary bandit formulate stationary distribution max coefficient definition grünewälder khaleghi proof arbitrary policy  arbitrary ext ext recall definition measurable hence ext EE extend joint xtk apply lemma EX disjoint ext ext EX relaxation introduces inevitable linear component regret proposition however argue reward distribution weakly dependent instead switch strategy optimistic approach propose UCB algorithm identify stationary jointly bandit bandit described bound stationary sequence joint stationary suppose weakly dependent policy approximation switch strategy specifically ext denote regret policy respect stationary proposition objective minimize recall argument crucial policy access data account devise strategy bandit framework APPROXIMATIONS restless bandit address challenge induced inter dependent reward sequence obtain random approach relies observation suppose obtain sequence consecutive sample random batch average expectation  become stationary formally lemma lemma fix consecutive sample random sample denote stationary  kϕk proof simplicity notation denote recall denotes filtration information available unobserved offs measurable obtain   ext EE ext ext ext ext ext  kϕk grünewälder khaleghi algorithm UCB algorithm bandit input sum kϕk coefficient initialization update empirical denotes loop selection maximizes UCB min argmax kϕk kϕk min operator precedence index update consecutive iteration update empirical accordingly due stationarity expectation respectively stationarity theorem namely inequality kxk directly definition definition kϕk inspire algorithm sum kϕk coefficient sample initialization played batch exponentially specifically upper confidence empirical played consecutive denotes upper confidence bound calculate hoeffding bound corollary rio sample obtain calculate scratch empirical algorithm individual coefficient sum kϕk upper bound kϕk replace kϕk regret bound theorem analyze regret algorithm recall trivially   played algorithm framework equality necessarily due inter dependency offs however proposition analogous upper bound algorithm sum coefficient individual coefficient APPROXIMATIONS restless bandit proposition regret algorithm  proof denote random sample measurable respect filtration information available EE equality measurable theorem ext min min min  inequality upper bound theorem proof appendix recall difference stationary stationary theorem regret bound regret algorithm kϕk kϕk remark interestingly kϕk bound theorem indeed offs recover constant regret bound thm grünewälder khaleghi strongly dependent reward distribution complementary extreme bandit strongly dependent distribution objective switch strategy obtain leverage inter dependency sample approach overall computationally efficient intuition dependency prediction future reward scarce observation sample stochastic easily dependency choice stationary gaussian recall gaussian fully specify covariance function cov covariance function kolmogorov consistency theorem guarantee existence gaussian covariance function gaussian stationary constant covariance cov cov dependence  continuity covariance function assume exists cov cov assume covariance function non negative correspond highly dependent covariance decrease slowly slowly decrease covariance implies coefficient cov cov rio thm bandit distribute accord stationary gaussian stationary simplicity assume mutually independent unknown covariance function cov cov assume unknown access upper bound rate decay constant cov  continuous assume upper bound stationary cov obtain regret inference switch strategy instead guarantee regret policy respect policy hindsight maxi algorithm namely algorithm exploit dependence offs exploration phase algorithm alternate exploration exploitation denote phase phase II respectively phase sweep correspond offs phase II constant reflect dependence sample estimate stationary distribution algorithm bound difference stationary suffice indeed difference minor relevance unless dependence individual regret bound proof appendix proposition maxi cov  regret algorithm exp APPROXIMATIONS restless bandit algorithm algorithm highly dependent input bound difference stationary maxi  coefficient cov  min argmax phase offs   min argmax phase II interpret bound simplicity highly dependent hence stationary standard bound normal distribution bound exp regret regret algorithm insignificant bracket gap importance phase algorithm selects stable regret bound linear oracle significant advantage oracle chooses hindsight however moderate significantly exp unless considerably advantage switch algorithm vanishes smoothness eventually exploration phase phase dominate smoothness cannot exploit algorithm demonstrates dependence stochastic exploit switch algorithm significant algorithm aim algorithm algorithm outperform switch algorithm outlook initial attempt characterize sub restless bandit approximate computationally tractable approach UCB algorithm approximate optimal strategy offs grünewälder khaleghi jointly stationary weakly dependent derivation bound moreover algorithm knowledge sum coefficient kϕk oppose individual online estimation coefficient useful specifically proposition estimate data algorithm estimate maximum loss respect switch strategy strengthen algorithm adaptively estimate kϕk instead rely input another regime corresponds strongly dependent distribution stationary gaussian switch strategy leverage dependency outperform policy weaken assumption distribution obtain analogous weakly dependent strongly dependent framework appendix proof technical lemma allows coefficient correspond disjoint lemma probability  exists disjoint sequence  proof similarly hence APPROXIMATIONS restless bandit technical lemma proof proposition lemma probability  proof exists dynkin  disjoint sequence correspond iteratively disjoint sequence hence therefore dynkin intersection similarly equivalently furthermore subset obtain finally hence monotone theorem due another important context concern random hoeffding bound batch observation algorithm concisely define xτn xτn xτn xτn notation xτn xτn denotes xτn xτn random variable attain lemma jointly stationary bandit formulate increase sequence  batch played almost surely finite sup grünewälder khaleghi proof define exist sequence   due stationarity  almost surely combine    proof proposition probability mutually independent define joint coefficient joint upper bound coefficient individual proof fix denote individual   furthermore XA XA XB XB proof structure joint construct probability associate approach useful complex easily approximate approximation relate arbitrary union derive applies APPROXIMATIONS restless bandit XA XB mutual independence implies demonstrate approach index independent algebra XC XC restriction XC define XC XC denotes algebra XC XC inverse preserve due  XA important exists XA XC XA standard argument algebra  sequence suitable furthermore XA latter intersection grünewälder khaleghi hence monotone theorem XA similarly link XB demonstrate arbitrary XA XB XA XB inverse preserve advantage approach approximate cylinder allows  thm exist sequence XC denotes symmetric difference obtain respectively moreover inequality similarly obtain furthermore obtain rely elementary manipulation AA AA APPROXIMATIONS restless bandit due apply obtain furthermore grünewälder khaleghi measurable argument hence obtain obtain bound inequality argument inequality arbitrary derive upper bound arbitrary XA XB proof derive precede assume APPROXIMATIONS restless bandit conclude XA XB joint limn proof proposition denote probability  filtration zero random measurable denotes furthermore random variable attain assume BX measurable BX denotes borel algebra define random inductively filtration information measurable random measurable random variable almost surely define xτi xτi   measurable hence   moreover measurable   observation simply verify induction recall statement proposition assume  stationary coefficient ext  furthermore sequence random xτi measurable fix  proof technical important context lemma proof trivially algebra contains measurable almost surely implies grünewälder khaleghi xτi xτi BX   BX  implies directly  induction verify xτi observation scp lemma implies upper bound instead lemma allows extend lemma algebra proof independent independence stationarity  bound attains finite levi theorem integrable upper bound integrable function lebesgue dominate convergence theorem APPROXIMATIONS restless bandit perform induction lemma hence xτi due stationarity implies xτi  measurable xτi EE xτi EE EE argument levi theorem lebesgue dominate convergence theorem expectation operator conditional expectation operator infinite summation outside moreover due measurable finally obtain xτi grünewälder khaleghi due  decrease sequence vol prof statement detail detail construction demonstrates sequence  sample bandit argument standard markov chain perturbation markov chain detail argument completeness assume bandit zero distribution described markov chain transition probability probability player gain markov chain markov chain markov chain irreducible aperiodic furthermore markov chain induces stationary distribution implies jointly stationary coefficient upper bound derive bound eigendecomposition transition matrix yield eigenvalue eigenvectors diagonal matrix entry  stationary distribution vector hence coefficient bound random variable gain realization XA XB algebra finite union xnk XA consists finitely similarly XB APPROXIMATIONS restless bandit argument bound allows conclude XA XB hence markov chain constant reward introduce dependency therefore jointly stationary coefficient coefficient markov chain fix policy policy accord played policy chooses played policy chooses dlog switch sequence offs generate policy sequence random played construction evolution  described transition matrix transition probability probability summarize transition matrix former evolution latter evolution markov chain discus argument apply Tˆ non negative entry Tˆ verify Tˆ markov chain associate Tˆ irreducible aperiodic implies existence stationary distribution associate probability summarize vector convergence  thm exist constant vector    grünewälder khaleghi calculate stationary distribution Tˆ explicitly markov chain slightly perturbed transition matrix stationary distribution due cho meyer exists constant dependent Tˆ independent     maxi Tˆ combine inequality yield  non negative entry xτn equality xτn xτn xτn xτn xτn xτn furthermore matrix define upper bound recall hence arbitrary furthermore arbitrary exists hence appendix proof theorem regret bound bandit regret algorithm kϕk kϕk proof thanks proposition bound regret suffices calculate suboptimal played kϕk kϕk recall algorithm APPROXIMATIONS restless bandit batch exponentially played consecutive denotes denote algorithm estimate stationary superscript refers quantity stationary fix min implies fix EX EX EX kϕk exp kϕk lemma hoeffding bound corollary rio applicable due lemma grünewälder khaleghi moreover kϕk similarly obtain kϕk kϕk kϕk false therefore kϕk appendix proof strongly dependent reward distribution bound independent normally distribute random variable variance derive bound crucial derivation regret notation max exp density function standard normal distribution derivation gaussian random variable recall normally distribute variance therefore  exp exp exp exp exp APPROXIMATIONS restless bandit standard bound cdf lem  exp exp exp exp similarly  exp exp exp apply  lem trivial bound hence inequality inequality obtain upper bound standard normal random variable bound bound straightforward adaptation technique derive bound apply bound upper bound  exp exp exp  exp exp exp exp inequality upper bound derive  exp exp exp grünewälder khaleghi  exp exp exp inequality proof proposition bound regret phase individually technical streamline discussion regret phase sweep etc phase regret regret bound max max difference stationary stationary inequality assumption variance individual therefore bound sum variance individual regret phase II regret building phase observation sweep offs maxi played maxi due stationarity maxi  conditional distribution  xii  marginal appendix APPROXIMATIONS restless bandit max  inner integral bound inequality  due independence posterior gaussian observation respectively posterior related posterior zero gaussian posterior equation zero gaussian observation cov cov therefore cov hence cov cov similarly cov cov define  continuity assumption obtain bound cov  cov cov cov cov bound  appendix conditional variance obtain upper bound cov cov  posterior equation covariance covariance assumption non negative  continuity assumption non negative covariance cov max  hence cov cov max   cov  otherwise   otherwise  grünewälder khaleghi combine obtain  exp  exp  bound maximize substitute bound manipulation appendix max amc exp  combine regret combine combine regret bound amc exp horizon iteration phase II overall regret bound amc exp amc exp regret bound proposition text instead minimizes optimize considerably simpler expression amc minor relevance negative bracket ignore latter lose another constant minimize respect yield APPROXIMATIONS restless bandit proof inequality due stationarity policy depends observation sweep maxi maxi correspond choice observation rewrite regret max max max    independent  depends bound introduce cov cov cov cov cov cov cov cov due  assumption cov cov  cov cov  bound  proof inequality denote recall normally distribute variance exp  grünewälder khaleghi   inequality dependent bound inner integral density  obtain exp exp exp exp exp  exp exp exp bound exp