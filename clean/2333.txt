recent trajectory optimization algorithm alternate linear approximation dynamic around trajectory conservative policy update constrain policy bound kullback leibler KL divergence successive policy approach already demonstrate experimental challenge physical however linear approximation dynamic introduce bias policy update prevent convergence optimal policy article propose model trajectory policy optimization algorithm guaranteed monotonic improvement algorithm  local quadratic dependent function trajectory data instead model dynamic policy update ensures KL constraint satisfaction without simplify assumption dynamic experimentally demonstrate highly non linear task improvement performance algorithm comparison approach linearize dynamic monotonic improvement algorithm additionally conduct theoretical analysis policy update scheme derive bound policy return successive iteration keywords reinforcement policy optimization trajectory optimization robotics introduction trajectory optimization stochastic optimal successful dimensional complex setting physical dependent linearization dynamic model around trajectory obtain update policy linear quadratic regulator  linearization locally policy iteration however iterative convergence guarantee linearization dynamic introduce bias impede algorithm converge optimal policy circumvent limitation propose novel model trajectory policy optimization algorithm moto  approximate policy iteration framework iteration function estimate locally around trajectory distribution dependent quadratic function afterwards policy update accord information theoretic trust bound KL divergence successive policy moto dimensional continuous action policy dependent stochastic linear feedback controller update function propagate backward extend propose domain stochastic notion sequential decision sequential decision policy update KL constraint function quadratic function action maximize sample efficiency rely importance sample reuse transition sample policy previous iteration principled moto complex despite simplicity function thanks function sample policy ensures function valid locally update policy ensures KL constraint satisfied exactly irrespective sample non linearity dynamic ensures function locally experimental demonstrates task highly non linear dynamic moto outperforms rely linearization dynamic additionally simulated robot tennis task moto dimensional task sample complexity relatively amenable application physical report experimental moto TRPO reinforcement algorithm showcase setting dependent linear gaussian policy moto suitable alternative neural network conduct theoretical analysis policy update sec bound increase policy return successive iteration algorithm bound validates KL constraint sec trajectory policy optimization ensure monotonic improvement policy return prior theoretical report maximum KL upper bound enforce leverage standard trajectory optimization assumption extend prior analysis policy update specific moto policy KL bound notation  finite horizon markov decision MDP horizon action transition function model trajectory policy optimization probability density transition upon execution action assume independent dependent reward function policy define dependent density function probability execute action timestep goal optimal policy maximize policy return IEs  expectation random variable distribution initial policy algorithm operates restrict parameterized policy iterative algorithm comprise policy evaluation policy update throughout article assume dependent policy parameterized  linear gaussian  gain matrix matrix bias dimensional vector covariance matrix exploration policy dimension yield parameter across policy iteration algorithm denote standard definition function   function  advantage function furthermore distribution related policy denote notation  iteration occasionally definition applies similarly iteration model policy update trajectory policy optimization moto alternate policy evaluation policy update iteration policy evaluation generates rollouts policy estimate quadratic function sec gaussian distribution sec quantity information theoretic policy update derive KL bound trust obtain policy iteration optimization goal policy update return policy maximizes function expectation distribution previous policy limit policy oscillation iteration KL upper bound KL divergence define policy update already successfully apply prior additionally bound entropy rollout monte carlo simulation trajectory accord execution physical    peter neumann reduction exploration yield non linear program maximize IEs KL IEs KL distribution KL entropy hyper parameter algorithm constant throughout iteration accord entropy policy IEs entropy reduction hyper parameter constant throughout iteration indicates maximizes expectation action distribution distribution bound average policy exploration exploitation ensures exploration action directly link entropy policy reduce quickly constraint introduce stochastic domain avoid premature convergence constraint crucial inherent non stationarity objective function optimize iteration non stationarity objective optimize policy update twofold update policy modify iteration algorithm function hence optimization landscape function policy parameter update policy induce distribution policy unlimited expressiveness optimal arg maxa irrespective however due restrict policy likely optimization landscape optimal policy parameter hence ensures exploration action maintain optimization landscape evolves avoids premature convergence update lagrange multiplier optimization exp optimal lagrange multiplier related KL entropy constraint respectively assume quadratic TQ  model trajectory policy optimization linear gaussian gain matrix bias covariance matrix function matrix vector  qas  invertible positive semi definite defines covariance matrix linear gaussian policy concave  negative semi definite  exists desirable barely yield policy KL divergence negatively impact convergence gradient algorithm model parameter specific semi definite available concave however maximally tolerate KL divergence successive policy remains define without additional constraint  dual minimization lagrangian multiplier obtain minimize convex dual function exp exploit structure quadratic function linear gaussian policy inner integral action evaluate dual simplifies  TMs dual function simplifies additionally assume normality distribution function   efficiently optimize gradient descent obtain expression dual function definition addition partial derivative appendix constant policy update albeit refer article advantage function interchangeably lieu update policy    peter neumann sample efficient policy evaluation KL constraint introduce policy update non linear optimization linear gaussian policy function quadratic subsection introduces supervise policy evaluation remain subsection discus improve sample efficiency function supervise remainder interested parameter linear model feature function contains bias linear quadratic yield parameter subsequently extract  qas iteration rollouts perform assume sample execution rollouts parameter regularize linear regression arg min   target  noisy estimate distinguish obtain estimate  monte carlo estimate estimate obtain sum future reward trajectory yield  PT estimator bias variance variance reduce average multiple rollouts assume reset however assumption severely limit applicability algorithm physical dynamic program reduce variance estimate exploit function reduce reward identity  Vˆ unbiased Vˆ however Vˆ approximate function recursively approximation introduce bias accumulate fortunately restrict algorithm policy update hence bias reduce increase complexity function approximator nonetheless article quadratic function function model trajectory policy optimization function assume zero function subsequently recursively function transition sample parametric function minimize loss PM  addition reduce variance estimate  choice function justified increase possibility reuse sample transition previous iteration sample reuse improve sample efficiency approach reuse sample iteration importance sample loss minimizes assumption infinite sample arg min IE loss inner within sum estimate  expectation respect action reuse sample transition sample rely importance sample importance IW ratio action probability independent action probability PT loss minimize becomes min IE transition probability dependent cancel IW upon computation IW regression minimize empirical estimate data  numerator IW recomputed sample additionally reward dependent estimate  recomputed dependent reward assume reward function reuse sample previous iteration sample previous iteration reuse access sample action distribution computation storage previous policy distribution limit iteration alternatively assume presence reward RT usually formulate task initialize    peter neumann finally sample reuse combine data iteration  regression IW PT estimate distribution compute IW distribution estimate rollouts sample policy sample available estimation necessitate reuse previous sample cope dimensional task propagation distribution investigate estimation distribution propagation estimate identical iteration importance sample sample maximum likelihood sample computation IW depends previously estimate distribution however estimate entail error despite sample iteration propagate degeneracy effective sample latter distribution mixture policy estimation heuristic behave intuition KL constraint policy update yield distribution sec theoretical justification closeness distribution sample previous iteration reuse simpler manner specifically sample mixture policy selects policy previous iteration exponentially decay iteration probability executes rollout decay factor accord dimensionality sample per iteration KL upper bound intuitively yield closer policy henceforth reusable sample estimate distribution gaussian distribution maximum likelihood sample sample iteration moto algorithm moto summarize alg innermost loop split policy evaluation sec policy update sec distribution estimate  transition sample compute function function  dynamic program estimate function conclude policy evaluation subsequently component quadratic model action extract optimal dual parameter respectively related KL entropy constraint minimize convex dual function gradient descent model trajectory policy optimization algorithm model trajectory policy optimization moto input initial policy trajectory per iteration entropy reduction rate output policy sample trajectory estimate distribution sec compute IW sec estimate function sec optimize arg min sec update sec policy update return policy iterate addition simplification policy update rationale local quadratic approximation twofold optimize locally KL constraint quadratic model potentially information hessian matrix gradient descent arbitrarily complex model linear gaussian maximum likelihood however complex linear gaussian exist quadratic model policy update additionally sec hence bias introduce propagate complex quadratic function necessarily improvement policy linear gaussian policy monotonic improvement policy update analyze constrain optimization policy update   approximate policy iteration monotonic improvement policy return obtain successive policy algorithm optimization define sec bound policy KL distribution iteration policy distribution careful analysis conduct analysis   lowerbound policy return policy optimization define sec policy unlike   enforce closeness successive policy KL constraint instead related obtain KL constraint    peter neumann contribution extend trajectory optimization continuous action KL policy bound instead maximum KL achieve denote policy policy respectively denote distribution policy respectively similarly difference policy return advantage function lemma policy denotes advantage function policy difference policy return IEs proof lemma proof lemma lemma express policy return advantage distribution optimize advantage function distribution policy apparent lemma lemma KL KL divergence distribution max  policy IEs proof IEs IEs IEs IEs KL  inequality sum lemma completes proof lemma bound policy return advantage optimize policy update negative quantifies distribution successive policy core contribution lemma model trajectory policy optimization relates distribution KL constraint policy policy update lemma distribution gaussian policy linear gaussian IEs KL KL proof demonstrate lemma induction distribution identical hence KL zero assume KL compute KL distribution KL sum inequality IEs KL hence bound KL distribution KL distribution KL policy previous express KL policy distribution IEs KL KL policy previous distribution IEs KL bound policy update KL assumption distribution policy gaussian linear gaussian demonstration appendix report IEs KL easy combination induction hypothesis yield KL finally combination lemma lemma theorem bound policy return theorem distribution gaussian policy linear gaussian IEs KL IEs  theorem obtain bound derive continuous action trajectory optimization bound KL policy update expectation previous distribution    peter neumann easy apply theorem appropriate generally approximately theorem constrain policy update overall behavior policy successive iteration crucial approximate RL related approximate policy iteration scheme policy update potentially decrease reward policy oscillation unless update policy previous bound policy update approximate policy iteration literature already   propose conservative policy iteration cpi algorithm policy obtain mixture greedy policy mixture parameter chosen bound positive improvement guaranteed however convergence asymptotic policy update sample algorithm optimal refine bound cpi additional capture closeness policy define matrix norm difference policy aggressive update experimental however approach discrete action extension continuous domain dimensional action action continuous typical robotic application stochastic policy update KL constraint ensure closeness successive policy empirical however empirical sample estimate objective function generally optimize typically sample precludes application physical sample complexity reduce model dynamic available latter empirical evidence suggests policy dimensional continuous action episode counter dependent dynamic assume linear simplify assumption sophisticated model gaussian   pan  policy trajectory optimization context challenge task chapter policy update resembles difference without assumption quadratic function additional maximum likelihood fitting sample policy policy KL longer respect secondly entropy constraint cope inherent non stationary objective function maximize policy ensure exploration sustain quality model trajectory policy optimization policy thirdly sample optimization algorithm introduction dual variable typically linearly dimension optimize dual variable irrespective trajectory optimization stochastic optimal linearize dynamic update policy  instance algorithm  DDP  robust variant trajectory optimization algorithm gps algorithm assumption moto respectively gaussian linear gaussian issue maintain stability policy update similarly moto introduce additional constraint regularizers update DDP   regularize update introduce damp matrix inversion gps KL bound successive trajectory distribution however demonstrate sec quadratic approximation function perform moto empirically detrimental quality policy update linearization dynamic around trajectory perform related approach experimental validation moto experimentally validate multi link swing task robot tennis task experimental aim analyze propose algorithm angle quality return policy comparatively trajectory optimization algorithm effectiveness propose variance reduction sample reuse scheme contribution entropy constraint policy update local optimum ability algorithm dimensional experimental concludes comparison TRPO reinforcement algorithm bound KL successive policy showcasing setting dependent linear gaussian policy moto suitable alternative neural network multi link swing task swing task involve multi link pole respectively joint task variant torque joint limit introduce additional non linearity dynamic challenge trajectory optimization algorithm linearize dynamic consists joint joint velocity action motor torque task reward function split action action constant throughout dependent zero quadratic penalizes null vector zero velocity upright successful swing ups moto depict    peter neumann axis axis link swing policy moto comparison gps moto link swing task torque limit apply moto variant link swing task moto without entropy constraint EC importance sample dynamic program DP plot average axis axis quad link swing policy moto comparison gps moto quad link swing task restrict joint limit torque limit moto link swing task rollouts per episode plot average moto trajectory optimization algorithm propose levine  refer gps chose moto gps KL constraint bound policy choice approximate function dependent quadratic model moto policy update instead linearize dynamic around trajectory trajectory optimization algorithm isolated gps moto dependent linear gaussian policy linear slight abuse notation gps algorithm additionally optimize trajectory upper policy however article interested trajectory optimization model trajectory policy optimization model dynamic gps reuses sample gaussian mixture model sample model prior joint gaussian distribution choice linearize dynamic model lack thereof approach sample reuse algorithm sample rollouts per iteration quad link respectively bypass sample reuse algorithm gps configuration moto link swing task initial policy algorithm however gps performs initial variance otherwise action quickly torque limit dynamic model harder dynamic linear gps manages improve policy return eventually swing policy configuration moto entropy reduction constant entropy constraint stochastic domain specifically entropy reduction constant convergence ultimately quality policy task moto manages slightly outperform gps gps moto quad link swing task task significantly challenge link increase difficulty joint limit introduce joint whenever joint angle exceeds absolute threshold desire torque policy ignore linear feedback controller aim joint angle within constrain gps barely improve average return torque limit link task moto performs significantly finally torque limit reduce moto manages swing policy demonstrate comparison importance component moto assess link rollouts per iteration reduce entropy constraint improvement quality policy iteration exchange initial progress importance sample greatly convergence monte carlo estimate  adequate rollouts per iteration exacerbate sample reuse transition monte carlo estimate finally explore link swing task balance perform rollouts per iteration stepsize versus rollouts policy evaluation update initial successively entropy reduction constant rollouts entropy reduce amount discount sample yield    peter neumann comparison robot tennis task initial velocity comparison robot tennis task gaussian bounce comparison robot tennis task initial velocity sample uniformly sample decay rollouts perform tune however complicate non overlap link swing task sample efficiency achieve however improvement becomes negligible decrease effective sample tends limit complexity mixture policy denominator importance ratio increase decrease become representation data fitting simpler action distribution representative data future improve sample efficiency algorithm crucial application physical robot tennis robot tennis task consists simulated robotic float racket effector task robot return incoming  strike freedom comprise joint linear joint 3D movement joint velocity 3D incoming dimension action dimension consists torque command analytical player  generate  stroke subsequently demonstration initial policy  player comprises phase preparation phase phase return phase reset preparation phase replace policy phase although moto model trajectory policy optimization robot tennis  stroke moto upon spin algorithm subsequently frequency factor dependent policy linear gaussian controller demonstration straightforward consists average torque command quantity initial bias controller although capture template  strike correlation action demonstration initial gain matrix null matrix similarly exploration action uninformed identity matrix setting task noiseless launch initial velocity context initial velocity sample uniformly within fix noisy bounce gaussian velocity upon bounce simulate spin moto policy algorithm  stochastic algorithm related information theoretic update algorithm optimize parameter dynamical movement primitive DMP DMP non linear attractor commonly robotics DMP initialize trajectory algorithm optimize goal joint velocity attractor DMP generates trajectory tracked linear controller inverse dynamic moto directly output torque command rely model algorithm converges faster  extent noiseless context somewhat surprising moto dependent linear policy parameter optimize parameter DMP attractor however policy slightly  context contextual variant  learns mapping initial velocity DMP parameter finally policy successfully    peter neumann capable adapt bounce fail trajectory DMP update generate comparison neural network policy recent advance reinforcement neural network policy ability generate processing amount data impressive achievement atari continuous task combine trajectory optimization supervise neural network policy directly optimize policy neural network reinforcement latter trajectory optimization directly optimize neural network policy linear gaussian policy moto related algorithm benefit neural network policy propose multi link swing task sec moto dependent linear gaussian policy TRPO neural network policy chose TRPO reinforcement baseline stateof performance policy update moto bound KL successive policy variant TRPO moto refrain importance sample sec technique policy policy evaluation TRPO moto default version TRPO OpenAI baseline implementation TRPO optimizes neural network policy function default parameter KL divergence constraint TRPO moto reward dependent distance upright penalize sec additional entry description entry interval horizon fed policy function neural network variant TRPO benefit moto dependent linear gaussian policy instead RL implementation neural network policy baseline TRPO algorithm replaces policy evaluation neural network function policy evaluation moto sec propagate quadratic function variant TRPO entry function baseline isolates policy update core algorithm function interchange finally variant TRPO quadratic function dependent linear gaussian policy diagonal covariance matrix standard formulation implementation TRPO covariance exploration entry function policy baseline algorithm bound KL divergence successive policy differentiate factor baseline moto TRPO bound KL policy moto solves policy update independently model trajectory policy optimization approximate policy iteration algorithm sec KL divergence upon update moto hence moto TRPO KL divergence overall policy expectation distribution KL divergence sub policy secondly moto performs quadratic approximation function solves policy update exactly TRPO performs quadratic approximation KL constraint solves policy update conjugate gradient descent TRPO policy update matrix inversion matrix invert dimensionality policy parameter contrast moto afford matrix invert dimensionality action generally significantly policy parameter performance moto TRPO variant link quadruple link swing task sec task moto outperforms TRPO variant albeit TRPO combine quadratic function variant outperforms moto link swing task quadratic function  task quadratic regulation generally reward quadratic function action negative distance upright quadratic action however moto task largely outperforms variant TRPO despite policy evaluation policy conclusion neural network purpose policy demonstrate variety task specific setting quadratic regulator task trajectory policy optimization outperform RL algorithm moto rely linearization dynamic around trajectory handle quadratic reward highly non linear dynamic quadruple link swing task outperform trajectory optimization algorithm sec conclusion propose article moto trajectory policy optimization algorithm rely linearization dynamic efficient policy update derive locally fitting quadratic function additionally conduct theoretical analysis constrain optimization policy update upper bound KL successive policy drift successive distribution approximate policy iteration scheme KL constraint widely trajectory optimization algorithm demonstrate however algorithm increase robustness towards non linearity dynamic closely related trajectory optimization algorithm simplification local linear approximation dynamic detrimental overall convergence algorithm local quadratic approximation function    peter neumann comparison multi link swing task moto TRPO TRPO neural network policy function default quadratic function  linear gaussian policy moto quadratic function task allows moto outperform neural network policy quadruple link swing task reward task  neural network function plot average independent simulated robotics task demonstrate merit approach policy algorithm optimize commonly dimensional parameterized policy strength approach ability reactive policy capable adapt external perturbation sample efficient however exploration scheme algorithm gaussian structure dimensional parameterized policy harmful robot addition transition simulation physical safety exploration scheme algorithm technical function complex function approximator network future extension refine bias variance acknowledgment research funding DFG project  spp autonomous intel corporation european union horizon research innovation programme grant agreement  compute grant  cluster model trajectory policy optimization appendix dual function derivation recall quadratic function action TQ  policy constrain maximization  gain matrix bias covariance matrix function matrix vector  qas optimal lagrange multiplier related KL entropy constraint obtain minimize dual function exp quadratic additionally assume distribution approximate dual function simplifies   define TF      convex dual function efficiently minimize gradient descent policy update perform upon computation gradient cst lin quad cst  lin quad cst lin quad dependency notation compactness dimensionality action    peter neumann appendix bound policy KL distribution distribution policy parameterized distribution KL constraint policy update IEs KL distribution affect KL depends reminder formula KL gaussian distribution KL dim linear gaussian policy policy distribution KL affect TMs matrix suffices bound expectation TMs KL already bound yield IEs KL TMs   exploit gaussian equation bound   matrix immediately non negativity KL negative KL gaussian distribution covariance matrix negative hence bound KL induction hypothesis policy KL positive yield IEs KL TMs   model trajectory policy optimization matrix AB    xxt  due cauchy schwarz inequality positiveness trace finally reverse triangular inequality    concludes bound  bound     bound  bound KL distribution bound equivalent max  constraint   eigenvalue optimization objective  average lambda transformation reduce constraint  jensen inequality hence optimum constraint active  finally constraint minimum hence maximum  equation unique strictly increase function expression exists bound equation yield upper bound equation simplify inequality yield  yield IEs KL