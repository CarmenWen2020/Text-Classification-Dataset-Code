binary representation data arise naturally application appeal hardware implementation algorithm data classification binary data obtain dimensional projection propose framework computation resource illustrate utility propose approach stylize realistic numerical theoretical analysis framework analysis foundation approach keywords binary measurement representation classification introduction focus data classification binary representation data available binary representation arise variety circumstance arise naturally due compressive acquisition distribute bandwidth constraint necessitate extremely coarse quantization measurement binary data representation particularly appeal hardware implementation inexpensive compute promotes hardware device benefit contribute sigma delta converter alternatively binary heavily quantize compress representation classification algorithm data compression goal framework perform inference classification highly quantize data representation focus extreme binary representation mathematical formulation formulation via matrix moreover linear denote operator without risk confusion overload notation operator apply matrix entrywise matrix define matrix entry classification algorithm access training data AX along vector associate label membership exactly matrix define hyperplanes binary information hyperplane data throughout primarily independent identically distribute standard gaussian entry experimental structure matrix algorithm classify signal available binary via matrix label unknown contribution contribution framework classify data binary representation obtain dimensional projection described data framework serf purpose mathematical classification application data already capture binary representation demonstrates classification effectively dimensional measurement suggests approach measurement classification computation technique classification mathematically analyze framework extend utilized novel algorithmic approach classification training data illustrate promise synthetic data theoretical analysis propose approach dimensional signal mild assumption derive explicit bound probability data classify correctly analysis serf foundation analyze complicate setting framework approach classification binary data organization proceed brief overview related propose stage classify data binary representation data stage performs training data membership stage classify data priori unknown membership demonstrate potential propose approach synthetically generate data datasets application handwritten digit recognition facial recognition finally theoretical analysis propose approach dimensional signal conclude discussion future direction prior related classification compress hash quantization due popularity impact research review prior necessarily non exhaustive briefly discus related prior highlight connection stress distinction vector machine svm become popular machine classification training data label svm construct optimal hyperplane hyperplanes data data linearly separable maximize geometric margin data linearly separable although loosely related utilize hyperplanes data approach fundamentally svm instead optimal hyperplane propose algorithm randomly hyperplanes via matrix relationship hyperplanes training data construct classification procedure operates information hyperplanes data classify transform dimensional data dimensional extensively related context pioneer  lemma dimensional euclidean linearly embed dimension without distort distance factor namely johnson lindenstrauss  embeddings motivate signal processing data analysis application focus randomize embeddings matrix associate linear embed drawn appropriate random distribution random embeddings gaussian subgaussian random variable admit implementation usually fourier transform another important related compress demonstrate linear measurement dictate traditional nyquist sam needell saab woolf  dimensional data signal obtains measurement noisy measurement goal recover signal assume signal sparse meaning kxk supp recovery becomes indeed vast literature recovery algorithm random matrix drawn appropriate distribution entry independent gaussian random variable relationship johnson lindenstrauss embeddings compress directional matrix yield johnson lindenstrauss embeddings excellent compress matrix conversely compress matrix minor modification yield johnson lindenstrauss embeddings initial perform inference task classification compress data promising processing digital computer compressive measurement quantize mapped discrete finite extreme quantization acquire compress introduce recently framework measurement objective recover signal developed recover signal normalization measurement although data overall goal signal reconstruction data classification recently binary embeddings embeddings binary cube linear projection apply operator nonlinear largely preserve information angular distance vector sufficiently measurement indeed measurement operator binary embeddings johnson lindenstrauss embeddings compress random gaussian subgaussian matrix admit linear transformation random circulant matrix although limitation embeddings subgaussian non gaussian matrix although binary measurement necessarily concerned geometry preservation dimensional ability perform data classification machine data representation multiple abstraction layer layer essentially function parameter network composition function algorithm neural network recently obtain classification due availability training data couple advancement compute development technique randomization neural network computational advantage shallow network randomization random initialization neural network obtain network optimization neural network extend binary data net boolean function binary input output quantization propose reduce multiplication input hidden layer randomize non linear measurement neural network motivational multi algorithm indeed tune parameter optimization typically necessarily posse structure typical architecture approach potentially simpler easy randomize non linearity simpler optimization latter closely resembles approach author propose function binary training phase classifies data maximization probability function perspective prior approach however bayesian geometric function balance measurement binary component data vector compress style projection approach utilize multi approach geometric framework lends easily obtain binary data simpler analysis propose classification algorithm training phase algorithm detailed algorithm binary data input directly training data AX compute pre processing arbitrary matrix incur computational mnp structure matrix reduce logarithmic dependence dimension training algorithm proceeds index randomly unique achieve multi uniformly random cardinality iteration indexed submatrix define training data hyperplane training data patter hyperplanes training data subsequent denote index binary representation needell saab woolf bin correspondence binary identification image operator index bin corresponds membership index parameter knowledge training calculate heavily dominate signal unknown label corresponds likely classify choice membership index parameter experimentally denotes training selection PG PG PG briefly explain intuition formula indicates proportion training iteration balance others extra selection nonzero receives certainly index binary training data remark membership index index unique actually training data unique iteration input classification phase algorithm algorithm algorithm training input training label binary training data raw training data fix matrix raw data compute AX randomly compute compute algorithm classify signal suppose signal unknown available quantize classification binary data measurement algorithm classification membership index selection iteration available algorithm decision vector initialize zero vector selection hence binary representation correspond index identify update via finally respect measurement entry identifies estimate label bbx actually affect outcome classification simply ensure quantity become unbounded bulk classification due geometry algorithm however phase cheap compute contribute classification accuracy naturally setting hierarchical classification detection remark algorithm classification input binary data parameter algorithm initialize identify identify index update classify bbx  experimental experimental algorithm synthetically generate datasets handwritten digit recognition mnist dataset facial recognition extend  database synthetic data typically gaussian  hyperplanes classify data datasets identical radial distribution around origin gaussian simply easy visualize various geometry structure around origin pre processing framework clearly extend remove future data digit clearly complicate geometry needell saab woolf harder visualize data fully characterize performance remark purposefully related svm data happens linearly separable svm outperform approach precisely data data linearly separable clearly outperform svm svm fail svm appropriate kernel identify kernel highly non trivial without understand data geometry precisely avoids unless otherwise specify matrix standard gaussian entry assume data ensure pre processing raw data perform account data around origin training data matrix calculate data adjust similarly assumption overcome future dither hyperplane dither AX random dither motivate  classification synthetic datasets stylize gaussian training data setup choice equally training data training execute algorithm trial generate perform classification per report average classification rate  trial  simply define correctly classify generate distribution label data average trial generate metric capture false negative positive access label plot nearly perfect classification suite construct gaussian utilize various data geometry training data gaussian execute algorithm classification accuracy increase nearly perfect classification gaussian execute propose algorithm display classification algorithm beneficial complicate data geometry gaussian classification binary data measurement average classification rate training per training per training per synthetic classification gaussian per trial randomly generate training data setup average classification rate versus training per measurement average classification rate training per training per training per synthetic classification gaussian per trial randomly generate training data setup average classification rate versus training per needell saab woolf gain classification accuracy gaussian improvement classification accuracy classification training data improve performance slightly decrease accuracy unexpected happens construction gaussian training data outlier harder measurement average classification rate training per training per training per synthetic classification gaussian per trial randomly generate training data setup average classification rate versus training per handwritten digit classification apply algorithm mnist dataset benchmark dataset image handwritten digit pixel dataset training apply algorithm digit classification rate digit versus equally training data classify image per digit algorithm perform comparison setup digit increase achieve classification accuracy around indicates digit likely mixed understandable due digit classification performance matrix construct dimensional discrete cosine transform dct addition typical gaussian matrix similarly discrete fourier transform instead dct define function complex specifically construct dimensional dct uniformly random classification binary data measurement average classification rate training per training per training per measurement average classification rate training per training per training per measurement average classification rate training per training per training per measurement average classification rate training per training per training per synthetic classification gaussian per trial randomly generate training data setup average classification rate versus training per needell saab woolf measurement average classification rate training per training per training per measurement average classification rate training per training per training per measurement average classification rate training per training per training per measurement average classification rate training per training per training per synthetic classification gaussian per trial randomly generate training data setup average classification rate versus training per classification binary data apply random illustrate difference dct gaussian construction analyze dct challenge limit theoretical analysis gaussian advantage structure matrix dct reduction computation acquire measurement training data measurement average classification rate training per training per training per data classification handwritten digit image mnist dataset per trial randomly generate training data image average classification rate versus training per data image apply algorithm mnist dataset digit utilize training per digit perform classification image per classification training per classification accuracy achieve training slightly improve classification facial recognition considers facial recognition extend  dataset dataset image individual roughly frontal image illumination per individual individual dataset randomly image illumination training illumination individual training data execute algorithm equally training data classify image per displayed needell saab woolf training data data measurement average classification rate training per training per training per measurement average classification rate training per training per training per classification handwritten digit image mnist dataset per trial randomly generate training data image data image average classification rate versus training per gaussian matrix dct matrix measurement classification rate training per training per training per classification rate versus handwritten digit mnist dataset training per per instance randomly generate classification binary data classification achieve training training data measurement average classification rate training per training per training per data classification individual extend  dataset per trial randomly generate training data image average classification rate versus training per data image theoretical analysis theoretical analysis algorithm series simplify assumption development tractable focus signal dimensional belonging moreover assume disjoint cone assume angular density training non uniform density relates complicate geometry dictate training accurate classification direction future however analyze simpler setup foundation generalize analysis future denote angular define max needell saab woolf denotes angle vector define similarly define min angle suppose classify random hyperplanes simplicity assume hyperplanes intersect cone intersect cone impose visualization setup analysis partition disjoint angle location within visualization analysis setup dimension hyperplane intersects hyperplane hyperplane intersects hyperplane within membership index parameter however angle instead training PG PG PG denotes angle index selection throughout denote index hyperplane identification implies hyperplane bbx denote classification label propose algorithm theorem describes probability classify correctly bbx simplicity classification binary data theorem assume assumption convenience clarity presentation already cumbersome proof analogously albeit without easy simplification convenience computation utilize assumption proof technical theorem corollary illustrate usefulness theorem cone define angular respectively suppose angular density training suppose probability data classify algorithm measurement matrix independent standard gaussian entry bound bbx display classification probability bound theorem simulated bbx varied importantly classification probability approach increase theorem behaves similarly simulated probability increase corollary asymptotic situation bbx tends corollary whenever corollary combination intuition grows hyperplanes essentially chop finer finer wedge dependence constant explicit proof corollary setup theorem suppose bbx probability converges exponentially  positive constant corollary setup theorem suppose bbx probability converges exponentially  positive constant proof proof theorem proof setup possibility hyperplane hyperplane completely cone associate needell saab woolf hyperplanes probability label bound bound bound bound bound bound bound bbx versus hyperplanes varied legend solid probability multinomial probability conditional probability simulated trial uniform random variable dash theorem hyperplane hyperplane completely cone hyperplane hyperplane hyperplane via hyperplane via observation define whereby hyperplanes hyperplanes cone hyperplanes hyperplanes hyperplanes easy reference quantity distinguish hyperplanes hyperplanes within hyperplane whereas hyperplanes within hyperplane orientation affect computation membership index definition probability handle bbx probability classify correctly bbx classification binary data latter probability probability density multinomial random variable evaluate conditional probability hyperplane summarizes hyperplane model location hyperplane within random variable define interval assume distribution index denote independent random variable hyperplane summary cone per hyperplane independent random variable define interval computation assume hyperplanes described hyperplanes hyperplanes hyperplanes hyperplanes compute membership index parameter define needell saab woolf θuh θuh θuh θuh θuh θuh θuh θuh θuh θuh simplify assumption conditional probability express θuh θuh θuh imply probably hyperplane configuration probability calculate classification probability assumption simplifies θuh θuh obtain inequality θuh conditioning probability reduces bound summation index bbx classification binary data equivalent implies assume simplifies hyperplane configuration reduces simplifies introduce denote hyperplanes simplify assumption sum sum quantity sum easily multinomial theorem finally equivalent thereby proof proof corollary proof bound bound exclude sum satisfy approach satisfy bound maximum combinatorics appendix satisfy needell saab woolf quantity bound max max maximum ignore constraint upper bound multinomial coefficient trivial upper bound assume assumption strategy satisfy implies combine bound classification binary data tend equivalent implies assume becomes therefore already assume essentially restriction completes proof proof corollary proof probability equivalent multinomial coefficient maximize parameter attain multinomial maximize another additional constraint multinomial maximize possibly ceiling multiple appendix explanation stirling approximation factorial notation denote asymptotic equivalence quantity ratio tends parameter grows assume implies assume therefore bound needell saab woolf summation discussion conclusion supervise classification algorithm operates binary data along encourage numerical theoretical analysis framework analysis approach relevant analyze multi algorithm future direction dither complicate data geometry identify setting measurement worth additional complexity analyze geometry non uniform density data generalize theory dimensional data belonging utilize multiple within algorithm addition framework extend nicely application hierarchical cluster classification detection membership function information data utilized detection structure classification false negative rate framework naturally extend setting simplistic algorithmic approach ability mathematical rigor acknowledgment DN acknowledges  sloan foundation NSF career DMS NSF bigdata DMS RS acknowledges NSF DMS author reviewer suggestion comment significantly improve manuscript classification binary data appendix elementary computation derivation suppose hyperplanes denote recall arrangement satisfy simplify denote arrangement satisfy satisfied reduces suppose reserve remain arrangement satisfy arrangement satisfy remain reserve therefore arrangement satisfy upper limit sum integer algebra software express sum derivation suppose maximize choice  fix equivalent minimize constraint fix optimize minimize convenience suppose multiple optimal choice non negative integer satisfy constraint sum fix readily observes needell saab woolf increase optimal choice choice argument optimize minimize choice therefore desire