structure prediction computer vision processing predict structure output segmentation parse setting prediction perform inference equivalently integer linear program complex function obtain accurate prediction inference typically approximate solver propose theoretical explanation strike observation approximation linear program LP relaxation tight instance LP relaxed inference encourages integrality training instance training tightness generalizes data introduction application machine formulate prediction structure output output variable predict jointly account mutual dependency correlation structural constraint matchings span unfortunately improve expressive model computational indeed prediction become NP despite intractability efficient approximation achieve performance approximation effective application linear program LP relaxation approach prediction cast integer LP ILP integrality constraint relaxed obtain tractable program addition achieve prediction accuracy LP relaxation tight relaxed program happens optimal integral particularly surprising LPs complex function constrain tractable understand instance behave differently theoretical address aim theoretical explanation frequent tightness LP relaxation context structure prediction approximate training objective although accurate predictor induces tightness LP relaxation byproduct interestingly analysis suggests training explain tightness future instance generalization bound relate average tightness training data tightness respect generate distribution bound imply prediction training instance tight prediction instance likely tight moreover prediction training instance integral distance prediction instance likely similarly integral understand previous empirical finding knowledge theoretical justification widespread LP relaxation structure prediction setting training data algorithmically separable background review formulation structure prediction LP relaxation associate prediction task goal input vector discrete output vector focus model linear classifier particularly prediction perform via linear discriminant arg maxy function mapping input output feature vector correspond vector output exponential generally intractable maximize output application function structure specifically assume decomposes sum simpler function assignment non exclusive subset variable decomposition assigns output variable correspond node graph  function prediction max sometimes omit dependence sequel due combinatorial prediction generally NP fortunately efficient approximation propose particularly interested approximation LP relaxation formulate prediction generally non linear model neural factor tightness LP relaxation structure prediction ILP max ML ML indicator variable factor local assignment factor assignment dimension ML local marginal polytope marginalization variable correspondence feasible assignment obtain indicator local assignment consistent  NP easy obtain tractable program relax integrality constraint introduce fractional LP relaxation sometimes linear program relaxation  adam hierarchy hierarchy successively tighter LP relaxation ILP relaxed program obtain remove constraint optimal upper bound ILP optimum finally complex constraint assignment forbidden correspond feasible global structure span achieve prediction accuracy parameter training data supervise model label goodness task specific loss assume commonly task loss ham distance ham structure svm SSVM framework empirical risk upper bound convex surrogate structure hinge loss yield training objective min max brevity omit standard regularization however regularization objective convex function hence optimize various objective maximization output training  prediction task repeatedly training evaluate  training intractable prediction LP relaxation apply structure loss convenience introduce singleton factor bound apply maximization regularization  london    pereira yield relaxed training objective min max ML vector entry integral vector correspond assume task loss decomposes model define vector entry dimension define vector indicator correspond definition generalizes ML review related propose theoretical justification tightness LP relaxation structure prediction complementary argument argue optimize relaxed training objective encourage tightness training instance tightness generalizes data related structure prediction express  despite NP various effective approximation propose approximation LP relaxation  tightness LP relaxation extensively recent demonstrate restrict structure model function pairwise LP relaxation tight structure model supermodular stability guarantee tightness LP relaxation   model cycle relaxation binary pairwise model tight planar ising model external almost balance model hybrid combine structure forbid minor recently guarantee tight relaxation facilitate efficient prediction restrict model tractable supermodular structure however sufficient mention indeed function useful satisfy integral predictor LP relaxation yield tight LPs data dependency parse therein behavior dependency parse LP tight structure prediction non projective dependency parse tightness LP relaxation structure prediction john movie john movie important NP simplest model non projective  parse relaxation   acl  exactly  eng dan dut por slo   percentage dual decomposition percentage integral local LP relaxation tight exactly quickly percentage integral dependency parse lacoste instance tight LP relaxation quadratic assignment formulation alignment fractional overall phenomenon multi label classification task integrality   structure output predictor label data propose various collins formulation generalize training binary classifier perceptron algorithm vector machine SVMs structure output algorithm repeatedly perform prediction necessitate approximate inference within training approach introduce inception structure SVMs LP relaxation purpose closest  pereira approximation equally important inference algorithm author define concept algorithmic separability refers approximate inference algorithm achieves zero loss data author LP relaxation structure generalization bound risk LP prediction however generalization bound  pereira focus prediction accuracy setting tightness instance guaranteed training data algorithmically separable seldom structure prediction task model perfect contrast theorem guarantee instance LP relaxation tight achieve training data allows generalization computation suppose LP relaxation algorithm iteratively tighten relaxation   observes instance training data integral relaxation tighten remain integral generalization bound guarantee approximately ratio assume sufficient training data kindly author  london     various approximate inference context structure prediction theoretical empirical superiority LP relaxation establish guarantee algorithmic separability LP relaxed training derive risk bound algorithm combination relaxed inference finally recently performance structure predictor 2D grid graph binary label information theoretic perspective bound minimum achievable ham error propose polynomial algorithm achieves error focus LP relaxation approximation algorithm handle without assumption model error decomposition concentrate solely computational aspect ignore accuracy concern tightness training relaxed training objective although achieve accuracy induces tightness underlie LP relaxation although focus LP relaxation LP relaxation simplify notation focus training instance index denote relaxed integer LPs arg max ML arg max ML respectively integral vector correspond truth output decomposition relaxed hinge integrality gap hinge equality difference relaxed optimum groundtruth relaxed hinge sum integrality gap difference optimum truth hinge non negative decomposition implication immediately derive bound integrality gap max ML precisely relaxed training objective therefore optimize approximate training objective minimizes upper bound integrality analysis sdp quadratic relaxation future recent    tightness LP relaxation structure prediction gap hence approximate objective reduce integrality gap training instance although loose bound non zero integrality gap integrality gap becomes zero data algorithmically separable relaxed hinge vanishes hinge vanish training integrality assure integral truth integrality indeed verify empirically training model random label attains tightness training truth label accuracy drastically analysis suggests tightness couple accuracy predictor   explain tightness LP relaxation fractional incur loss training analysis suggests alternative explanation emphasize difference loss decouple tightness accuracy presence global constraint constraint express linear inequality local polytope ML redefine constraint tighter polytope satisfies global constraint finally assumption model therefore apply generally factor obtain non linear function input neural network relaxed training training unfortunately bound sometimes loose indeed bound discard hinge task loss maximize loss augment objective precise characterization integrality gap specifically gap difference relaxed hinge hinge implies relaxed hinge zero integrality gap attain hinge integrality gap hinge relaxed hinge likely training relaxed inference training inference relaxed hinge upper bound relaxed training objective respectively latter additionally task loss therefore minimize training objective likely reduce correspond hinge demonstrate empirically insight relaxed training reduces relaxed hinge without directly reduce hinge thereby induces integrality gap suggests training actually increase integrality gap reduces  without reduce directly relaxed hinge consistent previous empirical evidence specifically random model yield loose relaxation  london   hinge relaxed hinge hinge relaxed hinge function scenario dependency parse training relaxed objective achieve integral training achieve integral   multi label classification relaxed training integral instance training attain yeast dataset empirical explanation however limitation counter counter demonstrates despite training relaxed objective hinge actually substantially relaxed hinge loose relaxation although illustrates limitation explanation correspond task construct scenario relaxed training obtains zero hinge non zero relaxed hinge relaxation tight model prediction arg max correspond LP relaxation max ML construct training instance relaxed loss function obtain plug respectively minimizes relaxed objective however vector relaxed hinge instance hinge instance data separable consequently integrality gap instance relaxation loose simplicity ham tightness LP relaxation structure prediction ML illustration local marginal polytope ML vertex partition integral vertex VI fractional vertex VF instance actually tight data output corresponds input generalization tightness argument concern tightness instance however empirical evidence pertains data bridge gap generalization bound tightness implies tightness define loss function lack integrality  LP instance discrete vertex local polytope ML exclude convex hull denote VI VF fully integral non integral fractional vertex respectively VI VF VI VF consists vertex ML vertex reduce generality linear program vertex max VI max VF denote respective integral fractional convention whenever VF  inference quantity whenever LP fractional integral define integrality loss otherwise coordinate fractional belong VF assume feasible integral vertex ML relaxation  london   loss function optimal fractional strictly optimal integral loss whenever non integral integral optimum purpose relaxation tight integrality loss probability obtain fractional LP input loss ignores truth assignment generalization analysis define related loss function integrality ramp loss predetermine margin parameter integrality ramp loss importantly integrality ramp loss upper bound integrality loss ramp loss zero integral fractional margin requirement mere tightness appendix model guaranteed satisfy requirement happens generally compute address interested tightness generalize worry computational efficiency theorem generalization bound tightness proof defer appendix pac bayesian analysis deterministic predictor theorem denote distribution denote feature mapping supx probability accord vector kwk satisfies MBR theorem integrality equivalently  finite sample training data likely integrality data sufficient sample worth focus linear model simplify presentation extend accommodate non linear model compute neural network assumption smoothness model loss corollary theorem actually applies generally disjoint vertex limited VI VF corollary vertex ML fractional vertex ML theorem replace VI VF definition respectively tightness LP relaxation structure prediction vertex fractional vertex ML meaning integrality loss analysis unchanged consequently generalization implies likely portion instance fractional training moreover theorem presence global constraint span constraint mention polytope ML replace intersection global constraint polytope derivation remains unchanged loss function actual fractional distance integrality analyze notion tightness account distance integrality generalization bound  pereira bound tightness prediction ignore label error happens parameter tractable regime supermodular potential stable instance LP relaxation tight training instance generalization bound guarantee probability LP relaxation tight instance contrast  pereira tightness instance guaranteed training data algorithmically separable LP relaxed inference predicts perfectly tight relaxation notion tightness surrogate  loss model satisfy definition LP relaxation tight integral non integral focus binary pairwise model model guaranteed tight proof appendix involves balance model binary pairwise model supermodular supermodular flip subset variable detail appendix proposition balance model unique optimum tight difference integral structure predictor input whereas supermodular model enforce linear inequality tractable ensure model balance instead model LP relaxation model balance training data proposition theorem LP relaxation likely tight data regard model singleton pairwise binary pairwise model minimal representation node θij representation appendix increase bound grows norm via BR therefore assume bound easily generalizes variable  london   detail variable define attractive θij repulsive θij proposition variable satisfy θij θij model tight finally verify efficiently compute efficiently analysis integrality distance structure prediction  important maximum really optimum relaxed integral relaxed inference yield optimal integral assignment lack assignment optimal integral assume relaxed unique integrality gap zero implies assignment however lack assumption multiple disparate assignment characterize distance relaxed assignment function nonzero integrality gap addition integrality gap interested integrality distance define   manhattan distance  relaxed program integrality distance conceptually persistence persistent fractional subset variable zero integrality distance integrality distance related integrality gap although distance sometimes useful integrality distance relaxed ultimately moreover integrality distance zero integrality gap zero regardless assume uniqueness relate integrality distance loss function commonly analyze literature structure prediction integrality gap integrality distance generalizes empirical sample population average moreover integrality distance upper bound constant multiple structure hinge loss convex loss function commonly training importantly unlike bound theorem bound computationally tractable combine obtain probability bound integrality distance efficiently evaluate training data additive error decrease finally argument bound applies integral fractional tightness LP relaxation structure prediction structure loss function focus integrality distance singleton node marginals denote sufficient decode label denote normalize manhattan distance input integral equivalent normalize ham distance model input assignment denote loss loss function generalization ham loss commonly prediction error inference argument reference truth label prediction error approximate inference however argument integral normalize integrality distance latter quantity focus upper bound max ML denote loss function commonly refer relaxed structure hinge loss loss minimize alternate assignment margin structure hinge loss computes loss augment integrality gap manhattan distance loss augmentation related loss function relaxed structure ramp loss max ML normalize version hinge loss bound whereas unbounded feature hinge loss max margin training convex ramp loss convex bound lipschitz convenient relationship ham hinge loss ramp loss analytical derive generalization bound generalization bound integrality distance training sample generalizes data distribution theorem proof appendix pac bayesian analysis  london   theorem denote distribution denote feature mapping supx define probability accord vector kwk satisfies MBR replace theorem integrality distance training generalizes future precisely integrality distance random instance  average integrality ramp hinge loss training plus vanish training grows training data estimate integrality distance remark theorem indeed integral assignment reference label loss function proof replace truth label obtain risk bound approximate inference topic relationship max margin training compute integrality loss generally infeasible inference therefore upper bound theorem cannot evaluate however empirical relaxed hinge loss respect truth label evaluate efficiently minimize quantity actually minimizes integrality distance max margin training approximate inference commonly anyway graphical model reduces prediction error inference approximation error insight enables technical lemma lemma reference truth label meaning integrality hinge loss twice hinge loss respect label proof decompose integrality hinge loss max ML max ML max ML tightness LP relaxation structure prediction hinge loss approximate predictor respect label evaluate efficiently righthand ham loss inference cannot evaluate efficiently however latter quantity upper bound max max ML combine completes proof lemma yield upper bound integrality ramp loss upper  integrality hinge loss lemma obtain corollary theorem corollary denote distribution denote feature mapping supx probability accord vector kwk satisfies MBR denotes integral vector correspond truth label corollary max margin training relaxed inference directly minimizes integrality distance future importantly constant bound efficiently evaluate training data worth corollary actually integral assignment truth label nonetheless bound insightful respect truth label setup define joint distribution bound probability input label worth corollary presence global constraint truth whichever integral assignment feasible decode LP relaxation fractional integral assignment scheme extensively arguably simplest local assignment arises denote relationship apply prior generalization analysis distance upper bound multiple integrality distance  london   relaxed training relaxed hinge hinge relaxed SSVM obj SSVM obj training epoch tightness tightness relaxed training training epoch relaxed training training training yeast multi label classification dataset various quantity function training iteration training LP relaxation training ILP integrality margin bin width differently lemma suppose output variable domain domain fractional proof output variable fractional assigns majority local belief label label chosen inference variable however fractional local belief incorrect label variable distance label incorrect label fractional fractional distance apply logic variable completes proof lemma combine corollary generate bound integrality distance bound simply numerical theoretical analysis multi label classification task image segmentation task training implement coordinate frank wolfe algorithm structure svm  LP solver standard regularizer chosen via validation multi label classification multi label classification adopt experimental   label binary variable model consists tightness LP relaxation structure prediction relaxed training relaxed hinge hinge relaxed SSVM obj SSVM obj training epoch tightness tightness relaxed training training epoch relaxed training training training scene multi label classification dataset various quantity function training iteration training LP relaxation training ILP integrality margin singleton pairwise factor fully graph label task loss normalize ham distance relaxed training iteration yeast dataset label plot relaxed hinge relaxed SSVM training objective respectively instance integral accuracy scheme fractional relaxed inference  nicely correlate relaxed training objective likewise hinge correlate objective relaxed training relaxed hinge hinge integrality gap difference remains almost training integrality gap indeed percentage integral almost relaxed training training histogram difference optimal integral fractional integrality margin training instance relaxed training margin positive although training negative finally integrality almost indistinguishable empirical generalization model random label label data model obtains tight training instance observation integral truth accuracy important tightness finally verify tightness coincidental tightness relaxation induced random vector random model tight trial tightness relaxation displayed objective average instance exclude regularization  london   relaxed training relaxed hinge hinge relaxed SSVM obj SSVM obj training epoch tightness tightness relaxed accuracy accuracy training training epoch training foreground background segmentation  dataset various quantity function training iteration training LP relaxation training ILP proceed perform scene dataset label yeast training specifically relaxed hinge hinge relaxed training consequence integrality gap relaxation tight almost instance illustrate sometimes optimize objective reduce relaxed objective relaxed hinge integrality margin namely integral optimum strictly fractional conjecture LP instance easy due dominance singleton specifically feature signal allows label assignment mostly local influence pairwise conjecture inject gaussian input feature model rely pairwise interaction noisy singleton indeed yeast dataset integrality gap instance tight appendix supplement image segmentation finally conduct foreground background segmentation  dataset data consists image training binary output variable assign pixel variable per image average extract singleton pairwise feature described  quantity multi label accuracy compute percentage correctly classify pixel behavior scene multi label dataset specifically relaxed training integrality gap percentage tight instance ILP training proposition satisfied variable although training instance satisfy variable tightness LP relaxation structure prediction unlike scene dataset variable satisfy proposition LP training model balance proposition although segmentation model balance relaxed training conclusion theoretical analysis tightness LP relaxation structure prediction application analysis careful examination integrality gap integrality distance relation training objective training LP relaxation although accuracy consideration induces tightness relaxation derivation suggests training sometimes increase integrality gap explain tightness tightness generalizes sens training prediction integral instance integral secondly training prediction average integral prediction similarly integral expectation