policy temporal difference TD discount markov decision goal evaluate policy model observation generate without execute policy curb variance issue policy TD propose scheme parameter TD generalize bellman equation scheme accord eligibility trace iterates calculate TD thereby easily trace desire bound prior scheme flexible allows policy TD bound trace soundness markov chain theory ergodicity joint trace  associate scheme generalize bellman equation policy evaluate depends evolution unique invariant probability trace immediately characterization convergence behavior implementation scheme analysis gradient implementation keywords markov decision approximate policy evaluation generalize bellman equation reinforcement temporal difference markov chain randomize introduction discount markov decision mdps policy temporal difference TD approximate policy evaluation linear function approximation goal evaluate policy model observation generate without execute policy policy important reinforcement methodology operation research machine available TD algorithm however tend variance due importance sample issue limit applicability purpose introduce TD scheme address motivate recently propose retrace algorithm ABQ algorithm backup algorithm exist earlier algorithm explain parameter TD curb variance issue policy accord action guarantee boundedness eligibility trace TD reduce significantly variance TD iterates limitation algorithm however tend conservative restrict whereas approximation bias TD propose scheme parameter TD generalize bellman equation scheme accord eligibility trace iterates calculate TD thereby easily trace desire bound scheme previous mention bound trace TD flexible allows policy regard generalize bellman equation context correspond dynamic program equation policy evaluate equation function unique associate operator contraction standard bellman operator refer associate operator generalize bellman operator bellman operator author conceptually broader equation policy evaluation treat policy evaluation parameter estimation statistical framework estimate equation framework equation function unique estimate function generalize bellman equation specific structure generalize  bellman equation associate randomize arise markov detail generalize bellman equation operator powerful classic MDP theory intricate optimality analysis computational however emerge primarily reinforcement parameter eligibility trace TD naturally bellman operator bellman operator choice calculate eligibility trace iterates correspond generalize bellman equation TD bellman operator effort aspect broaden scope TD algorithm analyze algorithm sutton multiple timescales  generalize TD algorithm tabular   chap context policy recent approach utilize connection TD generalize bellman operator TD efficient aim propose scheme parameter analysis TD scheme focus theoretical markov chain theory ergodicity joint trace  theorem associate scheme generalize bellman equation policy evaluate depends evolution unique invariant probability trace theorem corollary immediately characterization convergence behavior implementation scheme corollary remark analysis gradient implementation latter analysis recently remark addition theoretical preliminary numerical  algorithm demonstrate advantage propose scheme flexibility remark although focus exclusively policy evaluation approximate policy evaluation highly pertinent optimal policy mdps apply approximate policy iteration policy gradient algorithm gradient estimation policy addition mdps artificial intelligence robotics application generate model however beyond scope discus application organize brief background introduction scheme TD bound trace establish ergodicity joint trace discus generalize bellman operator associate randomize derive generalize bellman equation associate scheme experimental implementation scheme appendix proof generalize bellman operator approximation TD text policy TD bound trace policy policy evaluation algorithmic TD scheme dependent analyze eligibility trace iterates convergence correspond algorithm mahmood sutton preliminary policy concern markov chain finite chain transition matrix whatever physical mechanism induce chain denote refer target policy behavior policy respectively markov chain however performance markov chain evaluate specifically stage reward function associate discount reward criterion dependent discount factor denote diagonal matrix diagonal entry assume satisfy target behavior policy inverse PΓ exists moreover irreducible performance define discount reward initial notation expectation respect markov chain induced transition matrix function define function standard MDP theory matrix vector notation PΓ PΓ equation bellman equation dynamic program equation stationary policy footnote compute approximation parameter vector dimensional feature representation vector transpose data available computation realization markov chain transition matrix generate reward associate transition function relates suitable parameter approximation policy TD scheme define importance sample ratio initial eligibility trace vector scalar temporal difference approximate function calculate accord important parameter TD choice elaborate shortly exist TD algorithm generate sequence parameter approximate function algorithm LSTD obtains linear equation admits matrix vector LSTD update equation iteratively incorporate observation transition discus primarily algorithm behavior characterize directly subsequent analysis joint trace mention earlier analysis analyze  TD algorithm stochastic approximation theory complexity however delve refer reader recent detail scheme dependent choice trace iterates TD function approximation constant function neither behavior policy constrain unbounded variance unbounded situation policy TD challenge behavior policy target policy variance reduce satisfactory applicability policy seriously limited without restrict behavior policy mention earlier recent closely related exploit dependent variance choice  trace iterates bound reduce variance iterates motivate prior proposal accord directly desire straightforwardly parameter vector  within radius bound focus analyze iteration choice mention precede however dependence trace dependent entire retain markovian useful convergence analysis function previous trace specifically function previous trace memory summary formulation formulation denote memory simplicity assume finite evolution markovian function joint finite markov chain function besides  evolution memory behavior policy markov chain recurrent recurrence  markov chain multiple recurrent recurrent treat separately argument however remark finiteness assumption simplification finite mainly trace continuous joint trace resort markov chain infinite infinite introduce technical essential analysis obscure argument evolve function satisfy norm memory constant transition memory generalize bellman equation TD restrict desire  maxy max continuity function trace variable memory role subsequent analysis ensure trace jointly markov chain appeal defer discussion technical role remark satisfy later experimental mention earlier terminology introduce define function vector whenever exceeds threshold kek kek otherwise satisfied whereas simply euclidean projection onto origin radius therefore lipschitz continuous modulus correspond update becomes  ket cst cst ket otherwise scheme encourages chosen whenever variation scheme another factor   variation simply constant retrace algorithm modifies trace update policy TD truncate importance sample ratio policy TD algorithm constant retrace modifies trace update min retain interpretation bootstrapping parameter TD rewrite update retrace equivalently min function choice parameter automatically satisfies memory mahmood sutton discount factor strictly  bound deterministic constant depends initial initial retrace choice coincides choice framework parameter vacuously satisfied trace encounter retrace framework effectively encompasses choice retrace variation retrace trace update instead truncate importance sample ratio truncate constant scheme bound trace simplest variation memory independent positive constant replace definition min treat  ket  ket otherwise correspondingly instead update becomes min min ket ket otherwise variation retrace satisfy comparison previous policy evaluation retrace algorithm ABQ algorithm ABQ actually developed independently retrace publish although ABQ release later retrace ABQ backup algorithm additional parameter whereas backup specifies implicitly advantage knowledge behavior policy freedom relation algorithm retrace experimental later performance LSTD various scheme retrace retrace eligibility trace update retrace equivalent advantage parameter involve explicitly TD parameter directly affect associate bellman operator meaningfully interpret probability whereas importance sample ratio eligibility memory define accord min treat min  generalize bellman equation TD trace iterates essentially unchanged discrepancy behavior target policy prefer prefer selection parameter occurs apparently importance sample ratio trace update mention connection recognize role parameter explicit derive ABQ algorithm however ABQ discussion presentation algorithm emphasize apparent importance sample ratio trace iterates unsatisfactory clarify mention introduction retrace ABQ backup conservative tend statement precise explain algorithm tend behave effectively TD constant despite due TD TD constant  information cumulative reward horizon estimate encounter zero TD algorithm bootstrap immediately interleave effectively situation TD propose scheme threshold threshold comparison retrace ABQ backup constrain dependent parameter  prone issue mention demonstration retrace approximate policy evaluation actually focus primarily optimal policy MDP tabular demonstrate empirical performance retrace backup purpose despite adequate establish asymptotic optimality algorithm online optimistic policy iteration personal communication  theoretical online TD algorithm MDP algorithm positive rapidly target policy involve mention policy evaluation conceive generalize bellman operator although relate operator explicitly dependent correspond algorithm ergodicity joint trace important understand characterize behavior propose TD scheme subsection importantly establish ergodicity trace useful convergence analysis associate TD algorithm mahmood sutton although discus LSTD algorithm ergodicity relate LSTD equation generalize bellman equation target policy interpret LSTD obtain subsection argument analyze policy LSTD constant however dependent proof longer apply explain detail subsection another introduce nonnegative coefficient memory feature similarly  algorithm update accord apply update directly choice function continuous function depends continuously finiteness ensures weak feller markov chain weak feller markov chain   theorem boundedness ensure implies invariant probability lemma concern behavior initial important implication actually purpose introduce lemma converges almost surely lemma  generate iteration trajectory initial initial respectively  proof proof lemma ket  denote algebra generate assumption generation trace sequence  memory parameter denote trace sequence respectively      ket  hence ket  ket  nonnegative   convergence theorem  theorem lemma converges nonnegative random variable lim  inequality ket  ket   calculation PΓ bound continuous function endow topology continuous function   prop generalize bellman equation TD denotes dimensional vector PΓ converges zero matrix therefore lim  consequently lemma ergodicity weak feller markov chain ergodicity theorem application LSTD immediately theorem proof terminology notation initial probability distribution almost surely respect occupation probability denote random probability borel indicator function interested asymptotic convergence occupation probability weak convergence probability metric converges weakly  bound continuous function markov chain occupation probability define likewise markov chain essentially convenient apply ergodicity TD algorithm temporal difference involves regard invariant probability markov chain obviously invariant probability invariable probability probability compose marginal conditional distribution specify borel notation integral notation theorem weak feller markov chain unique invariant probability initial occupation probability converge weakly  likewise unique invariant probability initial distribution trace stationary denote expectation stationary corollary theorem LSTD theorem sequence equation LSTD definition mahmood sutton fix express continuous function trace hence entire bound weak convergence occupation probability theorem implies sequence equation asymptotic limit express stationary trace corollary initial almost surely sequence linear equation tends asymptotically linear equation random coefficient former equation converge correspond coefficient latter equation theorem broadly argument weak convergence occupation probability invariant probability initial imply uniqueness invariant probability proof comment remark difference proof previous comment remark technical role concern choice function relaxed proof theorem lemma weak feller invariant probability  prop exists initial occupation probability converge weakly invariant probability depends initial theorem invariant probability weak convergence arbitrary recurrent initial converges weakly invariant probability almost surely finite markov chain recurrent evolution affected marginal coincides unique invariant probability distribution recurrent implies exists initial mention earlier converges weakly  almost surely arbitrary generate iterates  accord trajectory lemma  therefore null sample generalize bellman equation TD bound lipschitz continuous function    weak convergence  earlier null   function combine yield almost surely    theorem implies almost surely  weakly initial converges weakly almost surely invariant probability  denote  initial converges arbitrary initial min proof recurrent define markov  theorem probability distribution markov chain therefore precede proof almost surely bound continuous function lim denote min almost surely lim limt lim lim prof converges weakly almost surely initial unique invariant probability suppose another invariant probability bound continuous function stationarity precede proof establish initial  implies initial distribution lim lim equality bound convergence theorem bound continuous function hence  prop uniqueness invariant probability conclusion markov chain argument replace replace transition assertion deduce implication assertion treat latter notation proof simpler remark proof theorem theorem policy LSTD constant analysis applies  technique theorem difference proof argument markov extend weak convergence subset initial initial whereas convergence LSTD iterates establish latter approach due dependence indeed due dependence proof convergence LSTD convergence consequence theorem boundedness trace construction proof ergodicity proof therefore regard alternative proof theorem mention uniqueness invariant probability bound weak convergence occupation probability immediately  prop however evolution depends trace easy directly uniqueness remark function proof theorem rely lemma precede lemma namely weak feller markov chain invariant probability weaken function proof conclusion theorem remain applicable introduce bound trace algorithmic concern ergodicity trace unimportant remove theorem lemma quickly infer invariant probability without   theorem bound probability proof straightforward proof lemma prop actually combine continuity ensure trace weak generalize bellman equation TD feller markov chain instead evolution trace memory function stochastic kernel suitable continuity stochastic kernel ensure trace desire weak feller markov packed lipschitz continuous function modulus somewhat restrictive instead function lipschitz modulus however additional ensure lemma lemma trace ergodic approach characterize sample trace algorithmic perspective desirable flexibility parameter generalization mention replace stochastic kernel introduce occasionally trace instead trace bound variance bound desire generalize bellman equation analysis recall corollary establish asymptotic limit linear equation LSTD linear equation goal relate equation generalize bellman equation target policy interpret compute LSTD approximate version generalize bellman equation description randomize associate bellman operator notion derive bellman operator correspond choice parameter linear equation LSTD discus composite scheme parameter application extension simplify notation subsequent derivation shorthand notation denote treat randomize associate bellman operator markov chain induced target policy recall function definition mahmood sutton  equation standard bellman equation generalize bellman equation randomize notion generalizes naturally depends random outcome toss coin coin regardless correspond bellman equation associate TD constant decision entire define randomize formally probability enlarge account whatever randomization scheme decision enlargement dependent subsection demonstrate enlarge randomize relative increase sequence algebra sequence algebra generate relative remains markov chain transition probability prob   chap prop therein equivalent definition randomize fully determines reduces markov chain definition encapsulate earlier intuitive discussion decision namely decision additional random outcome affect evolution markov chain markov randomize markov chain important algebra associate relative algebra generate conditional distribution probability distribution markov chain initial  theorem abstract definition randomize allows bellman equation without worry detail enlarge important notational simplicity confusion probability enlarge probability denote expectation conditional expectation respectively randomize markov  theorem allows express discount random relative sequence increase algebra generalize bellman equation TD reward prior  bellman equation expectation defines generalize bellman operator associate equivalent expression context expression convenient expression convenient define associate deduce contraction whereas expression explicitly TD scheme dependence parameter bellman operator generalize bellman operator affine involves  matrix function unique fix unique sup norm contraction slightly theorem randomize unique fix generalize bellman operator associate contraction sup norm explain derivation footnote almost surely define equation derive markov  theorem conditional expectation rewrite equality mahmood sutton theorem appendix proof amount evolves accord  matrix involve affine operator transient equivalently spectral radius invertible  appendix conclusion theorem nonnegative matrix theory  theorem specific choice sup norm theorem simply initial proof   prop TD algorithm dependent random correspond bellman operator description TD constant dependent choice TD algorithm associate randomize constant probability similarly dependent function precede probability replace expectation correspond bellman operator express solely model parameter target policy bellman equation propose TD scheme terminology randomize generalize bellman equation associate TD scheme propose corresponds randomize random generalize bellman equation precede subsection indeed bellman equation TD scheme markov chain target policy define randomize evolve accord initial distribute accord unique invariant probability theorem trace induced behavior policy probability dependence initial distribution explicit probability generalize bellman equation TD definition function initial random define hence conditional distribution initial distribution generalize bellman operator associate equivalent denote expectation similarly derivation rewrite expectation derive express integrate construction useful later express temporal difference rearrange remark expression remark expression reflect role whereas expression eliminate auxiliary variable clearly dependence entire randomize markov chain expression rearrange mahmood sutton initial distribution dependence trace dependence trace function describes importance sample ratio behavior policy choice feature representation assert significant role bellman operator target policy contrast policy TD constant behavior policy approximation subspace affect approximates bellman equation underlie TD bellman equation solely furthermore invariant distribution trace associate dynamic behavior trace behavior policy generally explicit expression parameter function cannot express operator parameter scheme TD function proceed bellman equation relates policy TD scheme notation denote invariant probability markov chain induced behavior policy coincides marginal function linear subspace function recall function dimensional feature vector denote subspace span component function approximate function TD scheme recall denotes expectation stationary trace behavior policy theorem theorem linear equation equivalently generalize bellman operator remark LSTD project version generalize bellman equation project  onto approximation subspace euclidean norm theorem corollary LSTD solves limit although generalize bellman operator contraction theorem composition projection contraction appendix cannot contraction argument analyze approximation purpose oblique projection viewpoint  specifically precede project bellman equation admits unique oblique projection approximation error characterize oblique projection viewpoint detail appendix remark gradient TD theorem LSTD algorithm analyze gradient algorithm generalize bellman equation TD LSTD algorithm aim project generalize bellman equation characterize theorem remark average dynamic important analyze convergence ode approach stochastic approximation theory ergodicity theorem essentially LSTD algorithm detail convergence analysis gradient TD algorithm recent subsection corollary theorem defer proof theorem corollary subsection corollary concern composite scheme slightly described bellman operator composition component bellman operator useful variance scheme explain motivation partition nonempty disjoint associate possibly scheme described denote memory function trace vector update accord ergodic trace variable unique invariant probability theorem associate randomize generalize bellman operator subsection define operator concatenate component mapping LSTD algorithm defines trace sum trace vector trace linear equation  corollary linear equation tends asymptotically linear equation corollary composite scheme scheme involve LSTD calculate trace accord limit linear equation associate LSTD equivalently generalize bellman operator fix contraction theorem mahmood sutton composite scheme demonstrate explain informally motivation scheme remark composite scheme motivation composite scheme reveal equation typically implement TD ignore bound trace introduce TD constant bellman operator TD constant extreme partition associate combination operator TD estimate information reward whereas information stage reward relate transition graph information useful TD policy evaluation cannot realize trace sequence evolve indeed interleave algorithm behave effectively TD entire context complex scheme motivation composite scheme implement parameter ith scheme chosen encourage throughout dictate combine component mapping composite scheme cumulative reward transition structure timescales additional flexibility manage bias variance estimate function demonstration finally mention policy LSTD constant composite scheme propose analyze proposition corollary extends convergence analysis  algorithm composite scheme proof theorem corollary proof theorem expression trace vector lemma subtle proof involves mostly calculation extend stationary trace stationary kolmogorov existence theorem  theorem exists markov chain transition probability define behavior policy update marginal distribution notation stationary markov chain generalize bellman equation TD recall shorthand notation introduce addition convention lemma almost surely define finite proof indeed PΓ equality monotone convergence theorem equality calculation inequality PΓ PΓ implies implies max theorem integration rudin theorem almost surely infinite series converges finite limit expression unfold iteration backwards trace bound converges zero converges almost surely expression proof theorem treat expression lemma expression calculate arbitrary function bound measurable finite  stationarity trace derive equality expectation summation equality justified dominate convergence theorem interchange expectation summation proof proceed calculation relate expectation summation expectation probability introduce mahmood sutton recall induced target policy involves randomize denote expectation marginal perform combine marginal distribution obtain calculate function recall equality expression therefore equivalent corollary proof corollary apply theorem trace specifically definition hence  equality definition operator linear equation equivalently unique fix contraction respect sup norm theorem applies suffices satisfies theorem namely generalize bellman operator associate randomize satisfies define random randomize associate bellman operator enlarge generalize bellman equation TD probability regard define probability definition component mapping generalize bellman operator associate definition bellman operator associate randomize numerical toy illustrate behavior trace calculate policy LSTD constant evolves accord propose scheme described behavior LSTD various choice toy behavior trace toy centre split evenly loop topology transition graph target behavior policy drawn transition graph northeast manner transition structure symmetry specify transition matrix target behavior policy respectively suffices specify submatrices central label central northeast clockwise submatrices respectively intuitively central enters diagonally direction probability spending eventually return central behavior policy average spends wander inside target policy target policy tends traverse clockwise quickly definition described initial distribution marginal mahmood sutton northeast central southeast transition graph toy cycle graph importance sample ratio transition cycle infer trace sequence unbounded almost surely reward zero northeast shade northern southern reward discount factor feature aggregate mention earlier central binary feature membership discus illustrate behavior trace toy comparison policy TD constant explain challenge policy TD motivation propose scheme trace constant trace iterates calculate TD identify cycle transition graph infer unbounded almost surely cycle transition graph toy consists central northeast label cycle importance sample ratio transition traverse cycle importance sample ratio discount factor destination infer calculate policy TD unbounded prop plot upper graph euclidean norm  trace iteration TD recur spike exceptionally spike plot consistent   invariant probability trace unbounded despite  bound probability lemma invariant distribution kek prop latter implies invariant distribution probability kek decrease empirical generalize bellman equation TD norm trace histogram trace norm trace norm statistic trace TD toy text detailed explanation distribution trace converges almost surely theorem iteration trace  increase simulation agrees precede discussion plot graph trace  iteration vertical axis indicates horizontal axis indicates despite recur spike  entire trace magnitude sharply increase trace exceptionally magnitude consecutive iteration illustrate histogram histogram concern excursion trajectory outside kek horizontal axis indicates excursion iteration excursion contains vertical axis indicates excursion iteration experimental plot histogram trace consecutive iteration behavior although tolerable LSTD detrimental TD algorithm disrupt motivation parameter bound trace directly trace evolve proceed illustrate behavior trace LSTD evolves accord propose scheme specifically demonstration threshold constant update  ket ket otherwise mahmood sutton simulate trace illustrate ergodicity theorem shortly performance LSTD respectively detail accord ergodicity theorem initial trace generate trajectory accord behavior policy empirical distribution trace trajectory converge distribution marginal invariant probability discrete verify examine empirical conditional distribution trace examine empirical distribution trace sub trajectory stk  stk kth trajectory empirical distribution converges increase trajectory initial kth kth kth kth demonstration convergence empirical conditional distribution trace text detailed explanation limit distribution trace parameter generate trajectory iteration plot normalize histogram trace component sub trajectory associate toy respectively central northeast southeast empirical conditional distribution trace converge along sub trajectory stk  characteristic function distribution characteristic function empirical conditional distribution obtain sub trajectory experimental par generalize bellman equation TD  evaluate complex characteristic function chosen randomly accord multivariate normal distribution covariance matrix maximal difference indicator deviation correspond distribution plot difference curve obtain described respectively mention earlier description horizontal axis plot indicates correspond difference curve tend zero increase consistent predict convergence empirical conditional distribution trace empirical distribution along trajectory obtain another trajectory initial difference curve southeast plot graph curve graph curve tends zero limit distribution empirical distribution initial consistent theorem comparison plot difference curve empirical conditional distribution empirical conditional distribution obtain trajectory specifically northeast upper curve graph clearly indicates associate limit conditional distribution trace characteristic function approach adopt effectively distinguish distribution footnote LSTD evolve LSTD algorithm toy toy previous subsection LSTD algorithm performs parameter function bound trace LSTD trajectory iteration compute euclidean distance LSTD asymptotic TD parameter normalize norm latter calculation independently generate trajectory plot standard deviation normalize distance LSTD obtain recall tight sequence probability distribution converges probability distribution characteristic function converge pointwise characteristic function  lemma recall convergence distribution weaker convergence variation cannot variation metric distribution mahmood sutton normalize distance TD normalize distance TD LSTD evolve constant constant dot dash curve plot quality asymptotic TD LSTD approach curve limit due variance issue impractically iteration exhibit convergent behavior LSTD evolve outperforms LSTD constant effectively archive quality TD constant normalize distance TD approximation error normalize approximation quality asymptotic TD constant toy yield approximation comparison LSTD constant dash dot curve indicates normalize distance asymptotic TD LSTD obtain limit performance LSTD deteriorates due variance issue convergence LSTD constant iteration iteration perform comparison LSTD evolve behaves effectively generalize bellman equation TD approximation quality achieve comparable asymptotic TD constant around quality asymptotic TD constant plot graph normalize distance TD TD plot graph normalize approximation error correspond approximate function error euclidean norm specify invariant distribution behavior policy normalization norm  function target policy considerably approximation accord retrace algorithm performance LSTD comparable TD around specifically retrace normalize distance TD normalize approximation error comparable TD surprising earlier  retrace ABQ conservative overall recall propose scheme instance reduces LSTD subsection demonstrate LSTD evolve adapt detail adaptation target behavior policy involve report detail crucial avoid distraction briefly experimental setup goal  steep consists velocity interval respectively corresponds desire destination destination peaked direction illustration destination available action reward action action respectively dynamic  reward discount factor destination discount factor enters  termination permanently target policy reasonably behave policy slope increase kinetic plus gravitational potential accelerate direction brings otherwise velocity zero backward probability visualizes mahmood sutton illustration estimate visualize image scheme  horizontal vertical image correspond velocity respectively dimensional visualization induced behavior policy image visualizes approximation obtain discretized model scheme quality approximation TD target policy horizontal vertical axis indicates velocity estimate simulate target policy evenly velocity velocity interval evenly subintervals target policy simulated generalize bellman equation TD  correspond discontinuity function behavior policy artificial policy random action chosen probability action explores jumping random restarts destination probability restarts restarts random sample uniformly explain approximation quality continuous actually analysis finite although treat essentially finite simulation finite precision computer calculate approximation error  approximate function various LSTD algorithm subsequent grid calculate euclidean distance function grid  simulate behavior policy image visualizes tile cod generate binary feature approximate function obtain LSTD algorithm piecewise constant comparison discrete approximate model aggregation discretization resolution comparable tile cod scheme dynamic reward model calculate data behavior policy discrete approximate model scheme subsequent image approximate function calculate LSTD image shortly positive approximation quality LSTD improves discrete model approximation approach effective TD report retrace scheme parameter previous composite scheme partition behavior policy exactly described possibility restart whenever destination specifically chose grid evenly velocity behavior policy effective iteration iteration ineffective behavior policy action restart action impossible target policy effective iteration grid boundary disregard due boundary dynamic normalize grid sum tiling comprises uneven rectangle binary feature detail cod scheme described consists velocity velocity belong mahmood sutton visualize image approximation obtain LSTD scheme scheme choice image equivalent LSTD retrace applies scheme parameter respectively refer composite scheme designation LSTD mention trajectory generate behavior policy effective iteration footnote approximate function obtain visualize image retrace scheme increase approximation scheme improves precisely approximation error algorithm independent consists effective generalize bellman equation TD retrace approximation error approximation error LSTD scheme iteration plot algorithm standard deviation approximation error approximate function obtain algorithm improvement approximation quality increase retrace consistent image iteration approximation error retrace iteration approximation error retrace temporal behavior LSTD scheme iteration approximation error retrace iteration approximation error retrace iteration approximation error retrace temporal behavior LSTD scheme mahmood sutton plot approximation error calculate per effective iteration algorithm experimental retrace latter achieve approximation quality retrace without increase variance variance become however iteration scheme  retrace yield approximation composite scheme performs plot plot composite scheme reduce variance iteration scheme  retrace yield approximation clearly bias variance composite scheme similarly previous propose LSTD algorithm constrain variant LSTD retrace constant constrain LSTD constant scheme parameter constant constrain LSTD evolves trace vector policy LSTD solves linear equation instead function  component trace vector within interval algorithm naturally ergodicity trace approximation unbounded integrable function bound detailed discussion retrace approximation error LSTD constrain LSTD approximation error LSTD algorithm algorithm standard deviation approximation error approximate function obtain independent experimental consists effective iteration horizontal axis indicates algorithm parameter constant LSTD perform LSTD retrace comparable constant LSTD fail sensible LSTD unreliable behavior behavior LSTD related toy due generalize bellman equation TD iteration approximation error retrace LSTD constrain LSTD constrain LSTD constrain LSTD iteration approximation error constrain LSTD temporal behavior LSTD algorithm variance issue becomes severe constrain LSTD reliable consistently LSTD evolve achieve slightly approximation quality constrain LSTD temporal behavior algorithm experimental plot approximation error calculate per iteration algorithm comparison constrain LSTD suffer variance constrain LSTD behavior LSTD evolve reasonable scheme retrace scheme additional parameter recall reduce scheme already previous scheme becomes approximation quality variance plot retrace obtain experimental consist effective iteration approximation error calculate per iteration bias variance parameter initial experimental although retrace discern plot variation retrace specifically parameter plot experimental effective iteration comparison plot behavior retrace scheme algorithm approximation quality improves variance tend increase initial variant truncate importance sample ratio perform comparably retrace  retrace achieve approximation quality mahmood sutton iteration approximation error iteration approximation error iteration approximation error retrace approximation error temporal behavior LSTD scheme retrace iteration approximation error retrace iteration approximation error retrace approximation error temporal behavior variation retrace conclusion developed scheme parameter policy TD randomize generalize bellman equation mdps recently propose algorithm retrace ABQ scheme trace bound reduce variance flexible theoretical analyze trace establish convergence associate LSTD algorithm prepared convergence analysis gradient implementation propose scheme addition preliminary numerical propose scheme LSTD outperform exist policy LSTD algorithm demonstrate achieve bias variance offs policy helpful flexibility parameter future research conduct extensive numerical gradient algorithm versatile memory parameter policy application