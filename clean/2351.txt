quadratic discriminant analysis QDA standard classification due simplicity flexibility parameter quadratically variable QDA practical however dimensionality relatively address propose novel procedure DA QDA QDA analyze dimensional data formulate coherent framework  aim directly estimate quantity bayes discriminant function quadratic interaction linear index variable classification appropriate sparsity assumption establish consistency estimate interaction linear index demonstrate misclassification rate procedure converges optimal bayes risk dimensionality exponentially respect sample efficient algorithm alternate direction multiplier ADMM developed interaction faster competitor literature promising performance DA QDA illustrate via extensive simulation analysis datasets keywords bayes risk consistency dimensional data linear discriminant analysis quadratic discriminant analysis sparsity introduction classification central topic statistical data analysis due simplicity quadratic decision boundary quadratic discriminant analysis QDA become important technique classification extra layer flexibility linear discriminant analysis lda despite usefulness parameter QDA  variable quickly inapplicable moderate dimensionality extremely  era data encounter datasets dimensionality substantially sample aim develop novel classification approach DA QDA approach QDA QDA useful analyze ultra dimensional data presentation focus binary observation suppose observation satisfy vector covariance matrix lda assume QDA boundary quadratic variable bayes classifies observation probability density function multivariate normal distribution variance prior probability algebra bayes discriminant function observation centroid difference precision matrix anderson discriminant function becomes lda completely analogous interaction model linear regression linear index variable nonzero entry role whereas nonzero entry understood interaction variable although discriminant function quadratic function discriminant function location invariant respect coordinate easy reference subsequently parameter bayes discriminant function collectively bayes component contribution highlight contribution DA QDA approach sparse QDA dimensional setup bayes discriminant function directly estimate sparse assumption intermediate quantity estimate intermediate DA QDA develop algorithm computationally memory efficient competitor develop theory theoretical attractiveness DA QDA theory estimate intercept considerable theory estimate lack theory convergence estimate approach sparse QDA extensive simulation data analysis DA QDA approach outperforms competitor variable considerable interact literature review datasets dimensional classification context increase attention empirical estimate bayes component longer applicable bickel  highlight lda equivalent random scenario dimensionality sample scientifically practically however component bayes discriminant function assume sparse loosely notion sparsity  interaction representation model admits interaction plethora built suitable sparsity assumption propose estimate lda cai liu mai  related linear dimensional classification leng   others zhao increasingly recognize assumption covariance matrix across lda restrictive practical extra layer flexibility QDA variable interaction extremely attractive shao sparse QDA sparsity assumption propose sparse estimate assumption directly quantity discriminant function addition estimate quantity necessarily translate classification phenomenon similarly argue cai liu lda propose screen identify interaction admits sparse structure permutation apply penalize logistic regression identify interaction estimate sparser model cannot penalize logistic regression important interaction appeal methodological perspective rayleigh fourth predictor estimate despite effort approach QDA stringent assumption shao dimensional analysis propose DA QDA approach aim overcome difficulty mention shao sparsity assumption estimate quantity directly DA QDA interaction without resort stage approach interaction selection fan jiang wang directly QDA variable DA QDA understood novel attempt interaction discriminant function correspond nonzero entry interaction selection importance extensively recently regression hao zhang reference therein estimate alone attract attention lately context understand structure network differs structure gaussian graphical model propose approach estimate formulate procedure via dantzig selector severe limitation linear program procedure constraint memory requirement constraint matrix iteration algorithm computation cardinality apparently dimensional data maximum attempt report contrast lasso formulation develop algorithm alternate direction multiplier ADMM estimate memory requirement algorithm computational per iteration enable DA QDA easily handle organize outline DA QDA methodology estimate novel algorithm ADMM estimate developed investigates theory DA QDA various consistency estimate establish consistency misclassification risk relative bayes risk extensive numerical analysis datasets comparison classification demonstrates DA QDA competitive estimate sparsity parameter discussion outline future direction research proof relegate appendix DA QDA methodology obtain estimator bayes discriminant function propose estimator bayes component appropriate sparsity assumption data estimate via sample version Σˆ Σˆ max Σˆ Σˆ degenerate cannot directly estimate denote DA QDA estimate obtain approach sparse QDA respectively scalar DA QDA procedure classifies observation Ωˆ classifies otherwise emphasize nonzero entry interaction variable contribute classification nonzero entry variable classification linear discriminant analysis becomes lda linear variable scalar grid bypassing estimate determinant strategy implement analytical justification focus estimation sparsity assumption quantity estimate recall attempt estimate intermediate quantity difference however accurate estimation covariance matrix inverse dimension unless additional assumption impose bickel  quantity bayes propose estimate directly proceed   define loss function  loss function minimize satisfies  observation motivates penalize loss formulation estimate replace empirical estimate arg min Σˆ  Σˆ Σˆ   penalty vectorized encourage sparsity tune parameter obtain symmetric estimator simply Ωˆ  obtain derivative loss function Σˆ Σˆ nonnegative definite formulation convex convex optimization algorithm develop ADMM algorithm optimization min Σˆ  Σˆ Σˆ  augment lagrangian Σˆ  Σˆ Σˆ  jiang wang leng frobenius norm matrix parameter ADMM algorithm detail estimate update successively arg min arg min Σˆ  eigenvalue decomposition Σˆ diag dip denote Σˆ Σˆ  organize diagonal matrix   proposition explicit ensures efficient updation algorithm proposition thresholding operator matrix namely entry matrix aij aij aij otherwise update update involves matrix subtraction matrix multiplication therefore update efficiently implement brief derivation obtain explicit proposition derivative respect Σˆ  Σˆ Σˆ Σˆ  Σˆ Σˆ Σˆ Σˆ vec vec Σˆ Σˆ vec vector operator vec Σˆ Σˆ vec Σˆ Σˆ equality vec  BT vec Σˆ Σˆ approach sparse QDA Σˆ Σˆ vec vec vec vec vec vec hadamard therefore examine ignore independent minimize     easily update efficiently implement algorithm summarize initialize fix compute svd Σˆ  Σˆ  compute   convergence compute Σˆ Σˆ update AU update thresholding elementwise update algorithm involves singular decomposition Σˆ Σˆ algorithm involves matrix addition multiplication algorithm extremely efficient algorithm computational complexity memory requirement algorithm memory requirement computational complexity handle convex convergence ADMM algorithm rate iteration convergence analysis ADMM algorithm assumption establish recent optimization literature hong luo verify assumption hong luo establish linear convergence algorithm lemma appendix detail jiang wang leng linear index estimate discus estimation linear index develop procedure avoids estimate   equation derivative motivate estimate lasso regularization arg min Σˆ Σˆ  Σˆ Σˆ vector penalty tune parameter optimization standard lasso easy exist lasso algorithm remark challenge analyze theoretically mai accuracy estimator carefully quantify emphasize framework extremely flexible accommodate additional constraint concrete enforce  principle interaction unless correspond  hao zhang denote exists Ωˆ penalty  variable penalize due limitation research theory consistently nonzero interaction nonzero addition explicit upper bound estimation error norm classification misclassification rate DA QDA converges optimal bayes risk sparsity assumption simplicity assume min instead assume sparse shao assume sparse estimation rate corollary theorem however previously computationally efficient dimensional addition estimation approach sparse QDA importantly estimate estimate plug estimator consequently corollary error rate estimate factor estimate however DA QDA discriminant function define approach sparse QDA component component consequence overall estimation error rate become implies estimate error aggregate estimation affect convergence rate classification error theory estimate quantify overall misclassification error rate theory estimate introduce notation assume ωij sparse ωij denote complement maximum node vector norm define norm define max matrix mij entrywise norm define mij entrywise norm max mij max mij denote norm induced matrix operator norm denote Σˆ Σˆ  Σˆ  definition kronecker matrix indexed vertex  denote Σˆ Γˆ ΓT  max max  max BΓ  ΓT BΓ ΓT max BΓ  establish model selection consistency estimator assume  max SΓ introduce zhao  establish model selection consistency lasso theorem model selection consistency rate convergence estimation theorem assume BB ΣB ΓT BB ΣB ΓT constant probability Ωˆ exists constant Ωˆ ΣB ΓT theorem  satisfied estimate consistently rate convergence estimate norm BB ΣB ΓT depends sparsity kronecker assumption BB ΣB ΓT implies jiang wang leng max diverge rate ΓT proof theorem corollary corollary assume  BΓ ΓT constant constant probability Ωˆ Ωˆ assumption BΓ ΓT corollary closely related mutual incoherence introduce   impose mutual incoherence inverse submatrices indexed maximum node nonzero entry rate obtain corollary rate theorem however covariates interaction important rate theory estimate cardinality denote max define AΣ Σˆ Σˆ Ωˆ establish model selection consistency estimator assume  max  max theorem model selection consistency rate convergence estimation theorem assume   ΣB ΓT assumption theorem  ΣB ΓT constant probability  exists constant ΣB ΣB ΓT approach sparse QDA theorem corollary immediately corollary suppose assumption corollary theorem assume AΣ constant constant probability reduces proportional discriminant variable cai liu variable linear discriminative feature contribute bayes proof theorem rate theorem  consistent obtain theorem misclassification rate subsection asymptotic behavior misclassification rate postpone theory estimate probability observation misclassified bayes DA QDA respectively suppose optimal bayes risk misclassification rate DA QDA   suppose denote density constant define max  sup denotes essential supremum define supremum almost everywhere zero  nonzero theorem establishes upper bound misclassification rate difference theorem assume exist constant  max min max  denotes ith eigenvalue assumption theorem BB ΣB ΓT  ΣB ΓT exist positive constant ΣB ΓT ΣB ΣB ΓT jiang wang leng BB ΣB ΓT  ΣB ΓT positive constant probability constant ΣB ΓT ΣB ΣB ΓT theorem indicates appropriate sparsity assumption DA QDA optimal misclassification rate converges optimal bayes risk probability statement theorem converges overwhelm probability corollary corollary theorem immediately corollary assumption corollary corollary theorem exist positive constant constant probability constant remark assumption constant shao assumption weaker assume density bound neighborhood zero shao density bound everywhere choice scalar critical classification receives attention exist literature detailed discussion lda propose minimize sample misclassification error establish analytical estimation misclassification rate abuse notation data covariates label obtain seek minimize sample misclassification error arg min Dˆ Dˆ hence Dˆ approach sparse QDA function becomes without loss generality assume define index satisfies optimization simplify arg min compute function index minimizes optimal satisfy establish asymptotic misclassification rate denote misclassification rate associate discriminant function discriminant function Dˆ respectively analogously sample misclassification rate Dˆ denote optimality bayes unique minimizer denote correspond optimal bayes misclassification rate minimizer estimation feasible assume exists constant proposition indicates although loss compute misclassification rate neither continuous convex misclassification rate desirable proposition strictly monotone increase strictly monotone decrease proposition theorem van der  establish theorem indicates estimator consistent misclassification rate estimate Dˆ tends optimal bayes misclassification rate probability theorem minimizer assumption theorem probability probability numerical extensive numerical evidence empirical performance DA QDA competitor sparse QDA sQDA jiang wang leng shao innovate interaction screen sparse quadratic discriminant analysis IIS SQDA penalize logistic regression PLR penalize logistic regression interaction PLR approach sparse lda dsda conventional lda lda conventional QDA QDA oracle procedure oracle oracle procedure underlie model serf optimal risk bound comparison evaluate via synthetic datasets datasets addition regularize svm svm kernel svm svm gaussian kernel performance benchmark data analysis DA QDA employ ADMM estimate coordinate wise descent algorithm linear minimize sample misclassification error rate parameter ADMM accord optimal criterion tune parameter estimate estimate chosen fold validation loss function chosen  misclassification rate reduce complexity currently grid tune avoid redundant computation inmemory parallelism distribute computational task worth calculation individual tune completely independent distribute entire calculation multiple thread parallel fashion tune parameter objective function separately however strategy classification tune jointly possibly due complex misclassification function tune parameter implement sQDA matlab validation tune parameter employ matlab built function  lda QDA package dsda dsda PLR PLR stage IIS SQDA penalize logistic regression glmnet package elastic net parameter tune parameter glmnet chosen fold validation minimize sample classification error stage IIS SQDA screen adopt oracle assist approach propose compute transform variable screen seek appropriate screen preserve variable interaction report misclassification error IIS SQDA synthetic data synthetic data setup observation simulated recall dimension parameter model model model matrix ωij symmetric approach sparse QDA sparse matrix nonzero entry symmetry model model model model model matrix define model random sparse symmetric matrix conditional non zero density generate  matlab model model generate covariance sample matrix entry uniform random variable define RT dense matrix matrix generate similarly model model generate random sample uniform distribution model generate matrix model define RT rrt ellipsoidal covariance eigenvalue relatively matrix similarly generate model generate random sample model model model sparse matrix permutation model IIS SQDA model difference diagonal matrix IIS SQDA underperform screen identify variable involve interaction retain variable model obviously model linear discriminant analysis lda sparse lda dsda model simulated sparse QDA satisfactorily situation lda model difference matrix  matrix screen IIS SQDA underperform finally model admits random sparse structure nonzero entry regardless dimension model covariance matrix dense DA QDA scenario sparse assumption fails feature highly correlate model linear implementation IIS SQDA apply model model directly cite model simulate synthetic datasets model comparison misclassification rate MR false positive interaction FP FP inter false negative interaction FN FN inter jiang wang leng summarize model consideration dense model misclassification rate replication summarize phenomenon model setup IIS SQDA IIS SQDA performs variable selection DA QDA performs similarly misclassification rate outperform lda QDA PLR PLR dsda margin classification error model IIS SQDA outperform DA QDA margin interestingly PLR performs linear classifier PLR dsda perform badly model dsda IIS SQDA DA QDA perform similarly DA QDA performs par dsda model clearly sparse linear classifier model DA QDA outperforms margin model DA QDA performs margin becomes model ordinary QDA performs dimensional covariance sufficiently dimension DA QDA achieves precision really surprising matrix dense advantage mostly DA QDA imposes sparse assumption instead precision matrix sQDA addition  performs IIS SQDA summarize DA QDA achieves misclassification rate competitive performance interaction IIS SQDA prefer approach diagonal matrix permutation model PLR generally performs sparse linear classifier interaction exist data investigate performance DA QDA analyze data classifier simulation regularize svm svm kernel svm svm gaussian kernel comprehensive comparison quora classifier data challenge available http quora com challenge classifier training data contains quora annotate feature effective extract goal challenge automatically classify feature dimension relatively DA QDA approach sparse QDA standard error parenthesis various performance classification model replication MR FP FP inter FN FN inter lda QDA PLR dsda sQDA PLR IIS SQDA DA QDA oracle PLR dsda sQDA PLR IIS SQDA DA QDA oracle PLR dsda sQDA PLR IIS SQDA DA QDA oracle standard error parenthesis various performance classification model replication MR FP FP inter FN FN inter lda QDA PLR dsda sQDA PLR IIS SQDA DA QDA oracle PLR dsda sQDA PLR IIS SQDA DA QDA oracle PLR dsda sQDA PLR IIS SQDA DA QDA oracle jiang wang leng standard error parenthesis various performance classification model replication MR FP FP inter FN FN inter lda QDA PLR dsda sQDA PLR IIS SQDA DA QDA oracle PLR dsda sQDA PLR IIS SQDA DA QDA oracle PLR dsda sQDA PLR IIS SQDA DA QDA oracle standard error parenthesis various performance classification model replication MR FP FP inter FN FN inter lda QDA PLR dsda sQDA PLR IIS SQDA DA QDA oracle PLR dsda sQDA PLR IIS SQDA DA QDA oracle PLR dsda sQDA PLR IIS SQDA DA QDA oracle approach sparse QDA standard error parenthesis various performance classification model replication MR FP FP inter FN FN inter lda QDA PLR dsda sQDA PLR IIS SQDA DA QDA oracle PLR dsda sQDA PLR IIS SQDA DA QDA oracle PLR dsda sQDA PLR IIS SQDA DA QDA oracle standard error parenthesis classification rate MR model replication model model model model lda QDA PLR dsda sQDA PLR IIS SQDA DA QDA oracle PLR dsda sQDA PLR IIS SQDA DA QDA oracle PLR dsda sQDA PLR IIS SQDA DA QDA oracle jiang wang leng simulation via fold validation randomly split data model data report misclassification error average misclassification error standard error various interestingly lda performs QDA analysis simply prefer linear classifier lda however becomes sparse model PLR PLR IIS SQDA DA QDA outperform non sparse model significantly DA QDA perform misclassification rate quora data fold validation standard error DA QDA lda QDA PLR dsda sQDA PLR IIS SQDA svm svm gastrointestinal lesion dataset contains feature extract database  video gastrointestinal lesion     task multi classification simplify binary classification task aim identify  data contains sample sample feature feature absolute sample statistic perform fold  validation average misclassification error standard error report data  logistic regression achieves DA QDA runner pancreatic cancer rna seq data dataset rna seq   data random extraction gene expression patient tumor      dataset contains patient gene task aim distinguish  cancer previous gene absolute sample statistic analysis achieve misclassification error fold validation increase difficulty randomly split dataset subset subset procedure obtain regularize svm achieves misclassification error previous dataset difference cancer dominate factor lda approach sparse QDA misclassification rate gastrointestinal lesion fold validation standard error DA QDA lda QDA PLR dsda sQDA PLR IIS SQDA svm svm already achieve surprisingly misclassification rate dataset DA QDA perform difference covariance matrix misclassification rate pancreatic cancer rna seq data standard error DA QDA lda QDA PLR dsda sQDA PLR IIS SQDA svm svm prostate cancer ftp stat  manuscript  prostate rda data contains genetic expression gene individual normal prostate cancer patient detail data  efron goal identify gene link prostate cancer predict potential patient difficulty task interaction gene existence interaction complicate analysis unreliable inference ignore display gene marginal distribution gene patient normal panel important distinguish panel however identify joint distinguishes jiang wang leng patient allocate normal within existence interaction useful classification gene gene gene patient gene density gene patient gene density gene patient plot gene gene joint scatter plot marginal density gene marginal density gene data cai liu retain gene absolute sample statistic average misclassification error standard error fold validation various report lda QDA exclude DA QDA par regularize svm outperforms margin regardless gene analysis misclassification rate prostate cancer data fold crossvalidation std error std error DA QDA lda QDA PLR dsda sQDA PLR IIS SQDA svm svm approach sparse QDA conclusion propose novel DA QDA dimensional quadratic discriminant analysis aim directly estimate quantity QDA discriminant function propose framework implement enjoys excellent theoretical demonstrate via extensive simulation data analysis DA QDA performs competitively various circumstance conclude identify direction future research discussion focus binary extend DS QDA handle multi apply DA QDA approach pairwise manner sample denote DA QDA classifier Dˆ suppose bayes classifier otherwise theoretical optimal finally propose framework extremely flexible concrete sparse matrix permutation penalty  encourages sparsity   sum norm procedure IIS SQDA topic beyond scope pursue elsewhere acknowledgement reviewer action editor prof   valuable comment greatly improve manuscript jiang research hong kong  grant  leng research partially alan turing institute  grant EP appendix appendix technical lemma proof linear convergence ADMM algorithm lemma establishes linear convergence propose ADMM algorithm lemma Σˆ Σˆ suppose ADMM scheme generates sequence converges linearly optimal  converges linearly zero proof equation hong luo Σˆ  Σˆ Σˆ  Σˆ  vec Σˆ Σˆ vec Σˆ Σˆ  eigenvalue decomposition symmetric matrix Σˆ Σˆ denote  function define Σˆ Σˆ function define avec vec vec clearly Σˆ Σˆ gradient jiang wang leng uniformly lipschitz continuous polyhedral lemma immediately theorem hong luo proof theorem introduce technical lemma proof theorem lemma lemma suppose λmax exist constant   exp exp cnv proof denote orthogonal matrix define  independent  XT    lemma bickel  lemma remark denote max   lemma lemma lemma assume BΓ ΓT  SΓ ΓT  ΓT ΓT  ΓT moreover Γˆ  ΓT Γˆ  ΓT proof approach sparse QDA consequently argument appendix max ΓT Γˆ SΓ  ΓT  ΓT prof similarly lemma assume assumption min BBΓ ΓT BBΓ ΓT BBΓ ΓT BΓ ΓT  min  vec Ωˆ  ΓT BB ΓT BBΓ ΓT proof suppose min  Σˆ  Σˆ Σˆ  lemma due convexity derivative zero equivalently Σˆ Σˆ Σˆ Σˆ derivative obtain Σˆ Σˆ Σˆ Σˆ zij zij zij zij therefore vector operator becomes Σˆ Σˆ vec vec Σˆ Σˆ  jiang wang leng equivalently vec Γˆ vec Σˆ Σˆ  Σˆ Σˆ vector operator Γˆ svec svec vec svec vec vec svec Γˆ svec svec vec Γˆ SΓˆ vec Σˆ Σˆ  SΓ svec vec Γˆ SΓˆ SΓ vec Γˆ SΓˆ svec Σˆ Σˆ Γˆ SΓˆ Γˆ SΓˆ SΓ Γˆ SΓˆ consequently  Γˆ SΓˆ SΓ Γˆ SΓˆ  Γˆ SΓˆ proof assumption lemma lemma Γˆ SΓˆ SΓ Γˆ Γˆ Γˆ Γˆ Γˆ Γˆ Γˆ Γˆ BΓ ΓT BΓ ΓT ΓT BBΓ ΓT BBΓ ΓT BBΓ ΓT BΓ ΓT  min consequently  Γˆ SΓˆ  min max Γˆ SΓˆ SΓ Γˆ SΓˆ  approach sparse QDA vec svec Ωˆ vec vec Γˆ vec Σˆ Σˆ  svec Γˆ Γˆ svec Γˆ vec Γˆ Γˆ Γˆ assumption immediately Ωˆ BΓ  ΓT  ΓT BB ΓT BBΓ ΓT proof theorem lemma probability abuse notation denote max BBΓ ΓT BΓ ΓT BB ΣB ΓT assume sample max min ΓT ΓT BΓ ΓT clearly assumption theorem BB ΣB ΓT firstly verify assumption lemma CB immediately min min ΓT implies BΓ ΓT assumption jiang wang leng   BΓ ΓT BBΓ ΓT BBΓ ΓT BBΓ ΓT BΓ ΓT BBΓ ΓT BΓ ΓT  BB ΓT BΓ ΓT implies BBΓ ΓT BBΓ ΓT BBΓ ΓT BΓ ΓT lemma imply Ωˆ  ΓT BB ΓT BBΓ ΓT  ΓT BB ΓT  ΓT BB ΓT proof theorem introduce technical lemma proof theorem lemma lemma assume AΣdδ Σˆ AΣdδ proof lemma easily observation Σˆ Σˆ Σˆ  AΣ Σˆ  lemma   approach sparse QDA proof Σˆ Σˆ Ωˆ Σˆ Σˆ lemma Σˆ Σˆ Ωˆ Ωˆ Ωˆ  Σˆ Σˆ proof theorem argument lemma exists constant max  proof theorem max  AΣ    max BBΓ ΓT BΓ ΓT BΓ ΓT BB ΓT assume max AΣ    min min suppose min  Σˆ Σˆ  jiang wang leng  Σˆ  definition Σˆ  consequently Σˆ        Σˆ  Σˆ     Σˆ   Σˆ   Σˆ   Σˆ   Σˆ   Σˆ   simplicity inequality derive without mention probability AΣdδ probability repeatedly inequality without mention probability theorem  lemma Σˆ   Σˆ Σˆ Σˆ Σˆ Σˆ Σˆ Σˆ Σˆ  Σˆ   AΣdδ combine lemma Σˆ  AΣ AΣdδ    AΣdδ   AΣdδ  approach sparse QDA  hence   assume   AΣdδ  min   Σˆ     proof AΣ    AΣdδ      AΣ     AΣdδ  AΣdδ  Σˆ    Σˆ   Σˆ AΣdδ  AΣ  AΣ AΣdδ  AΣdδ AΣdδ AΣ AΣ AΣdδ     theorem plug  proof theorem proof abuse notation jiang wang leng denote    Ωˆ argument bound  Ωˆ obtain ΣB ΓT ΣB ΣB ΓT assumption theorem ΣB ΓT ΣB ΣB ΓT equality cumulative distribution function standard normal random variable constant lemma theorem proof probability ΣB ΓT ΣB ΣB ΓT proof proof approach sparse QDA proof proposition proof suppose denote  exp easily consequently therefore strictly monotone increase statement similarly proof theorem introduce technical lemma proof theorem lemma constant define min  lemma constant  exp  proof  exp exp increase function exp exp exp  jiang wang leng lemma argument clearly proof lemma bound bound uniformly similarly lemma suppose duc uniformly lemma assumption theorem probability uniformly proof denote sample denote estimator obtain ith sample similarly denote estimator obtain ith jth sample immediately EI EI EI EI lemma  ER uniformly cov cov cov obtain lemma independent immediately uniformly lemma markov inequality uniform convergence bias variance proof theorem obtain proposition lemma theorem van der  statement immediately theorem approach sparse QDA