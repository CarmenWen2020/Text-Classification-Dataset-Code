efficiency restart subgradient RSG periodically restarts standard subgradient SG apply convex optimization RSG optimal complexity SG RSG reduce dependence SG iteration complexity distance initial optimal optimal logarithmic factor moreover advantage RSG SG satisfy local error bound demonstrate advantage specific convex optimization constant local error bound epigraph polyhedron RSG converge linearly local quadratic growth sublevel RSG iteration complexity admit local   constant RSG iteration complexity novelty analysis exploit bound optimality residual novelty allows explore local function local quadratic growth local   generally local error bound develop improve convergence RSG develop practical variant RSG enjoy faster convergence SG without involve parameter local error bound demonstrate effectiveness propose algorithm machine task regression classification matrix completion keywords subgradient improve convergence local error bound machine introduction generic optimization min correspondence  yang  lin license CC http creativecommons org license attribution requirement http jmlr org html yang lin extend  convex function convex dom assume smoothness dom decade linearly convergent optimization algorithm developed smooth strongly convex contrary relatively technique generic non smooth non strongly convex optimization application machine statistic computer vision etc potentially non smooth non strongly convex simplest algorithm subgradient SG lipschitz continuous SG iteration obtain optimal iteration complexity  non smooth non strongly convex oracle model computation however iteration complexity achieve algorithm additional structural information available generic restart subgradient RSG multiple stage stage previous stage within stage standard project subgradient update perform fix iteration constant reduce geometrically stage stage scheme RSG achieve iteration complexity classical SG belongs function summarize RSG mild assumption assumption RSG iteration complexity additional significantly constant SG SG iteration complexity quadratically depends distance initial optimal RSG iteration complexity quadratic dependence distance optimal distance initial optimal dependence initial upper bound initial optimality gap logarithmically epigraph polyhedron RSG achieve linear convergence iteration complexity locally quadratically definition weaker convexity RSG achieve iteration complexity admits local   definition  function RSG achieve complexity SG refer deterministic subgradient literature stochastic gradient upper bound initial optimality gap objective RSG beating subgradient without smoothness convexity derive generic complexity RSG satisfy local error connection exist error bound growth literature spite simplicity analysis RSG additional insight improve iteration complexity via restart restart improve theoretical complexity stochastic SG non smooth strongly convexity assume restart helpful SG weaker assumption remark lemma lemma developed leveraged develop faster algorithm context built groundwork laid developed smooth algorithm improve convergence nesterov smooth algorithm non smooth optimization structure developed stochastic subgradient improve convergence standard stochastic subgradient organize reminder review related preliminary notation algorithm RSG theory convergence considers nonsmooth non strongly convex improve iteration complexity RSG parameter variant RSG experimental finally conclude related smoothness convexity convex optimization affect iteration complexity optimal iteration complexity smooth strongly convex recently emerge surge accelerate firstorder non strongly convex non smooth satisfy develop improve complexity local error bound closely related error bound literature various error bound exploit analyze convergence optimization algorithm luo tseng establish asymptotic linear convergence feasible descent algorithm smooth optimization coordinate descent project gradient local error bound coordinate descent extend objective function constraint tseng  wang lin global error bound non strongly convex smooth objective function feasible descent achieve global linear convergence rate recently error bound generalize leveraged faster convergence structure convex optimization consists smooth function non smooth function recently   generalize error bound establish linear convergence parallel version randomize coordinate descent minimize sum partially separable smooth convex function fully separable non smooth convex function emphasize aforementioned error bound local error bound explore bound distance optimal norm project gradient proximal gradient partial smoothness objective function contrast bound distance optimal objective residual respect optimal broader function recently smooth optimization composite smooth optimization objective function satisfy error bound growth non degeneracy establish linear convergence rate proximal gradient accelerate gradient prox linear relative strength relationship zhang smoothness assumption growth error bound equivalent error bound wang lin attention local error bound closely related metric   establish polyhedral error bound epigraph polyhedral domain bound polytope polyhedral error bound zero sum propose restart firstorder nesterov smooth technique nash equilibrium linear convergence rate difference subgradient instead nesterov smooth technique former broader applicability nesterov smooth technique linear convergence derive slightly domain unbounded polyhedron polyhedral error bound lemma important application subsumes polyhedral error bound bilinear saddle error bound allows derive linear convergence RSG weak minimum coin however earlier convergence subgradient later subsequent finite linear convergence algorithm establish weak minimum gradient projection proximal algorithm ppa subgradient RSG beating subgradient without smoothness convexity choice emphasize difference novel gradient projection finite convergence establish gradient objective function lipschitz continuous objective function smooth polyak chap theorem contrast assume smoothness objective function ppa proximal sub consist objective function strongly convex function iteration therefore finite convergence finite subgradient evaluation contrast linear convergence subgradient evaluation linear convergence subgradient polyak optimal objective convergence distance iterates optimal weaker linear convergence objective gap addition optimal objective instead variant RSG linear convergence multiplicative constant parameter local error bound without parameter develop practical variant RSG achieve convergence rate linear convergence recent  framework apply conic optimization transform equivalent convex optimization linear equality constraint lipschitz continuous objective function framework greatly extends applicability linear inequality constraint algorithm iteration complexity related implies  corollary objective function polyhedral epigraph optimal objective beforehand subgradient linear convergence rate optimal objective  apply objective function necessarily polyhedral obtains improve iteration complexity local error bound recently freund propose SG assume strict bound denote fslb satisfies growth fslb optimal closest growth rate constant fslb novel incorporates fslb non smooth optimization SG achieves iteration complexity fslb fslb fslb initial difference theoretical implementation freund growth inequality function noticeable difference growth constant convergence establish freund relative error absolute error rewrite convergence freund absolute accuracy fslb algorithm complexity depends fslb fslb however freund SG yang lin attractive due parameter algorithm without growth constant RSG freund detail restart multi stage strategy employ achieve uniformly optimal theoretical complexity stochastic SG strongly convex uniformly convex restart helpful without uniform convexity furthermore algorithm propose exist iteration per stage increase stage algorithm iteration stage possibility restart algorithm complexity local error bound preliminary define notation assumption establish denote   objective function necessarily strongly convex optimal necessarily unique denote optimal unique optimal objective denote euclidean norm throughout assumption assumption convex minimization assume constant exists constant maxv kvk remark assumption assumption equivalent assume bound assumption freund machine application usually bound zero satisfy assumption standard assumption previous subgradient denote closest optimal norm arg min uniquely define due convexity strongly convex denote sublevel respectively maximum distance optimal max min max sequel assumption RSG beating subgradient without smoothness convexity assumption convex minimization assume finite remark finite optimal bound objective function  convex  function sublevel bound  corollary nevertheless bound optimal finite max although optimal bound local error bound satisfy assumption denote closest sublevel arg min denote easy optimality denote normal cone NΩ formally NΩ define dist NΩ dist NΩ min NΩ dist NΩ therefore dist NΩ optimality residual define constant min dist NΩ notation lemma analysis lemma proof conclusion trivially assume accord optimality exist scalar lagrangian multiplier constraint subgradient vector NΩ definition normal cone inequality convexity imply yang lin equality due therefore complementary slackness inequality   equality due inequality due definition lemma inequality achieve improve convergence RSG hinge optimality residual bound important depends optimization algorithm apply generalize norm norm  distance correspond dual norm define bound residual generalization allows mirror decent variant RSG knowledge leverage bound optimal residual improve convergence non smooth convex optimization exhibit discus impact convergence sequel abuse notation exists constant independent restart subgradient RSG generic complexity framework restart subgradient RSG convergence lemma algorithmic developed viewpoint however exhibit insight improvement template development improve convergence RSG RSG algorithm SG subroutine project subgradient algorithm  define  arg min RSG reveal later convergence RSG optimal iteration parameter RSG depends parameter influence iteration complexity emphasize RSG generic algorithm applicable non smooth non strongly convex without update scheme tune parameter iteration per stage varies RSG variant subroutine stage RSG beating subgradient without smoothness convexity algorithm SG SG input iteration initial query subgradient oracle obtain update  output PT algorithm RSG RSG input stage iteration per stage assumption subroutine SG obtain SG output optimization algorithm SG subroutine algorithm convergence lemma guaranteed dual average regularize dual average  discussion focus SG subroutine establish convergence RSG relies convergence SG subroutine lemma lemma algorithm iteration omit proof standard analysis cite lemma convergence RSG theorem suppose assumption 2G dlogα algorithm stage algorithm return iteration algorithm optimal  2G remark satisfies 2G iteration complexity algorithm optimal 2G dlogα proof denote closest sublevel induction yang lin conclusion obviously suppose namely apply lemma stage algorithm  kwk  assume  lemma kwk combine 2G implies therefore induction inequality due definition theorem iteration complexity RSG allows leverage local error bound upper bound obtain specialized practical algorithm lemma define closest RSG beating subgradient without smoothness convexity dist dist tan geometric illustration inequality dist proof subgradient vector NΩ convexity definition normal cone closest inequality implies   NΩ equality definition obtain     NΩ definition assume otherwise trivial proof lemma exists NΩ  accord geometric explanation inequality dimension lemma iteration complexity RSG corollary theorem corollary suppose assumption iteration complexity RSG obtain optimal 2GB dlogα 2GB dlogα yang lin SG standard SG improve RSG  issue improve complexity obtain 2GB magnitude address issue unknown admit local error bound requirement relaxed parameter related local error bound RSG non smooth non strongly convex optimization admit local error bound improve iteration complexity RSG standard SG complexity local error bound define local error bound objective function definition admits local error bound sublevel closet constant decrease zero indeed induce notable local error bound extensively community optimization mathematical program variational analysis exhibit computable admits local error bound RSG achieve iteration complexity implies replace corollary upper bound 2G RSG obtain complexity RSG corollary suppose assumption admits local error bound iteration complexity RSG obtain optimal 2G logα 2G dlogα remark 2G 2G iteration complexity remains aim apply RSG RSG beating subgradient without smoothness convexity inequality due assume without loss generality local error bound RSG iteration complexity therefore iteration complexity RSG convex optimization admit local error bound faster convergence RSG apply linear convergence polyhedral convex optimization subsection non smooth non strongly convex epigraph polyhedron polyhedral convex minimization polyhedral convex minimization linear growth admits local error bound constant lemma polyhedral error bound suppose polyhedron epigraph polyhedron exists constant admits local error bound remark inequality weak minimum literature proof lemma burke  proof remark extend valid norm distance lemma generalizes lemma bound polyhedron unbounded polyhedron generalization useful development efficient algorithm error bound unconstrained without artificially constraint lemma basis RSG achieve linear convergence polyhedral convex minimization linear convergence RSG obtain plugin corollary corollary suppose assumption polyhedral convex minimization iteration complexity RSG obtain optimal 2G dlogα 2G dlogα corollary directly replace replace proof theorem derive corollary global error bound yang lin mention  linear convergence rate corollary obtain SG historically non smooth non strongly convex machine satisfy assumption corollary constrain regularize piecewise linear loss minimization machine task classification regression exists data empirical risk minimization min regularization denotes loss function regularizer regularizer indicator function zero piecewise linear loss function hinge loss max absolute loss insensitive loss max etc easy epigraph polyhedron define sum regularization loss function piecewise linear loss function generally max  finitely scalar formulation indicates piecewise affine function epigraph polyhedron addition norm polyhedral function kwk max kwk max max max sum finitely polyhedral function polyhedral function epigraph polyhedron another important objective function polyhedral epigraph submodular function minimization denote submodular function function subset submodular function minimization cast non smooth convex optimization   extension polyhedron define   extension max mina minw submodular function minimization essentially non smooth non strongly convex optimization polyhedral epigraph RSG beating subgradient without smoothness convexity improve convergence locally semi strongly convex definition local semi convexity definition function semi strongly convex sublevel exists closest optimal refer local semi convexity explore semi convexity domain linear convergence smooth optimization literature inequality growth satisfy inequality indicates admits local error bound corollary iteration complexity RSG locally semi strongly convex corollary suppose assumption semi strongly convex iteration complexity RSG obtain optimal 2G dlogα 2G dlogα remark obtain iteration complexity suppresses constant logarithmic local semi convexity obvious convexity implies local semi convexity vice versa function strongly convex compact polyhedral epigraph accord function satisfies constant although smoothness assume lemma lemma suppose assumption satisfies dom strongly convex function compact polyhedral epigraph satisfies proof lemma duplicate analysis exist almost identical proof lemma gong assumes smooth however without smoothness recall yang lin function commonly loss function regularization machine statistic robust regression without regularizer min  denotes feature vector target output objective function matrix accord   strongly convex function compact objective function semi strongly convex improve convergence convex KL lastly non smooth function local   KL definition KL definition function   KL exist neighborhood continuous concave function continuous   KL inequality ming kgk function  function  function reparameterization important  function KL inequality semi algebraic function satisfy KL indeed concrete satisfy   discussion KL refer reader previous corollary iteration complexity RSG unconstrained KL corollary suppose assumption satisfies uniform    function constant RSG iteration complexity 2G dlogα obtain optimal 2G addition iteration complexity RSG 2G dlogα 2G dlogα RSG beating subgradient without smoothness convexity proof corollary proposition appendix accord proposition satisfies KL uniform implies monotonic conclusion similarly corollary conclusion immediately conclusion inequality implies local error bound conclusion corollary hinge convex function continuous semi algebraic  function satisfy KL finite worth mention knowledge leverage KL develop improve subgradient explore non convex convex optimization deterministic descent smooth optimization convergence subgradient descent sequence minimize convex function error bound sequence subgradient descent sequence exist satisfies namely sufficient decrease  relative error exists   however non smooth function sequence generate subgradient necessarily satisfy instead proximal gradient applies function consist smooth component non smooth component assume proximal mapping non smooth component efficiently compute contrast algorithm analysis developed non smooth function variant RSG without constant exponent local error bound local error bound reveal magnitude exhibit however constant estimate render challenge appropriate 2G inner iteration RSG sufficiently however approach vulnerable estimation estimation estimate waste iteration estimation accurate target accuracy addition objective function machine sum piecewise linear loss training data yang lin nuclear norm regularizer overlap non overlap lasso regularizer variant RSG implement without local error bound exponent improve convergence SG RSG without increase sequence another restart RSG detailed algorithm refer RSG RSG complexity RSG theorem theorem suppose dlogα algorithm exists satisfies local error bound Sˆ constant 2G dlog RSG algorithm iteration RSG obtain optimal upper bound TS 2G dlogα proof dlogα dlogα 2G apply corollary RSG algorithm output satisfies RSG initial satisfy setup dlogα dlogα 2G apply corollary output satisfies argument subsequent RSG dlog algorithm ensures iteration RSG bound TS 2G dlogα remark remark algorithm theorem theorem applies increase sequence algorithm practical implementation RSG beating subgradient without smoothness convexity algorithm RSG restart RSG input iteration stage RSG stage RSG initialization RSG iteration complexity theorem implies RSG enjoy convergence rate linear convergence satisfy weak minimum implementation RSG  calibrate improve performance relationship calibration tradeoff exit criterion RSG automatic RSG RSG obtain optimal depends unknown parameter namely criterion terminate algorithm machine application monitor performance validation data terminate algorithm quantity proof implicitly compute apply algorithm finally local convexity Sˆ derive iteration complexity SG SG converges Sˆ iteration independent iterates within Sˆ converge iteration complexity exist analysis SG strongly convex function however local convexity parameter unlike theorem local convexity parameter RSG unknown without sharper local error bound simply render  definition employ trick increase sufficiently RSG dlogα stage increase factor theorem algorithm suppose dlogα assume algorithm exists dlog RSG algorithm iteration RSG obtain optimal upper bound TS dlogα yang lin remark monotonically decrease function theorem exists satisfies KL corollary unknown theorem proof proof theorem Bˆ dlogα dlogα apply corollary RSG algorithm output satisfies RSG initial satisfy setup dlogα dlogα apply corollary Bˆ Bˆ output satisfies argument subsequent RSG dlog algorithm ensures iteration RSG bound TS dlogα discussion comparison discus obtain exist comparison standard SG standard SG iteration complexity  achieve optimal assume appropriately RSG accord corollary iteration complexity GB depends instead logarithmic dependence upper bound initial optimal RSG complexity appropriately 2GB theorem guarantee propose RSG iteration complexity SG sufficiently satisfies local error bound RSG iteration corollary theorem dependency complexity standard SG RSG beating subgradient without smoothness convexity comparison SG freund freund introduce growth fslb fslb strict bound difference distance optimal distance sublevel objective gap respect fslb objective gap respect growth constant varies fslb freund SG iteration complexity  fslb fslb define fslb fslb comparison RSG fslb complexity freund absolute error fslb obtain  fslb fslb gap fslb fslb dominate  due definition complexity magnitude standard SG RSG due generally iteration complexity freund SG reduce GB fslb proof theorem freund depends fslb comparison RSG complexity fslb corollary theorem subsection corroborate addition RSG leverage local error bound enjoy iteration complexity comparison  nesterov  nesterov primal dual subgradient uniformly convex namely iteration complexity uniform convexity implies  admits local error bound sublevel therefore RSG complexity accord corollary  nesterov complexity logarithmic factor however local error bound weaker uniform convexity broader function comparison achieve target optimal algorithm euclidean norm definition replace norm yang lin objective robust regression gap stage objective robust regression gap stage iteration objective gap robust regression SG RSG SG RSG algorithm iteration objective gap robust regression SG RSG SG RSG algorithm comparison RSG algorithm housing data iteration subgradient update algorithm RSG initial RSG propose  nesterov knowledge uniform convexity parameter parameter worth mention  nesterov algorithm fix iteration input achieve adaptive rate without knowledge however corresponds notation apply demonstrate effectiveness RSG application machine regression classification matrix completion focus comparison RSG SG comparison RSG freund SG variant regression algorithm initial unless otherwise specify RSG beating subgradient without smoothness convexity objective robust regression gap stage objective robust regression stage gap iteration objective gap robust regression SG RSG SG RSG algorithm iteration objective gap robust regression SG RSG SG RSG algorithm comparison RSG algorithm data iteration subgradient update algorithm robust regression regression predict output feature vector training linear regression model optimization instance conduct data libsvm website namely housing examine convergence behavior RSG iteration per stage initial RSG proportional parameter variant plot housing data data plot objective stage difference objective converge refer gap clearly RSG converges convergence rate linear stage consistent theory secondly SG verify effectiveness RSG baseline SG implement decrease proportional iteration http csie ntu edu cjlin  datasets yang lin iteration objective SG SG comparison algorithm iteration objective SG initial SG initial SG initial SG initial sensitivity initial svm classification  regularizer objective initial iteration subgradient update algorithm index initial SG tune convergence initial RSG tune around initial SG RSG RSG increase sequence implement RSG restart RSG stage increase iteration factor increase factor respectively RSG quickly converge accurate SG sufficiently iteration RSG relatively converge accurate RSG converges faster SG bridge gap RSG RSG svm classification graph fuse lasso classification predict binary label feature vector training training linear classification model cast min hinge loss max vector machine svm graph fuse lasso  regularizer  fij encodes information variable suppose graph node attribute assign sij similarity attribute attribute denote consists tuple attribute matrix   zero entry  becomes RSG beating subgradient without smoothness convexity iteration objective absolute loss SG SG iteration objective hinge loss SG SG rank matrix completion loss function sij previous carefully  regularization reduce risk fitting generate dependency graph sparse inverse covariance selection generate sparse inverse covariance matrix assign sij non zero entry inverse covariance matrix conduct dna data libsvm website label classify versus comparison algorithm initial zero entry RSG restart stage increase factor initial algorithm tune dependence RSG convergence initial SG initial initial initial generate normal gaussian distribution convergence curve algorithm initial plot initial SG RSG separately tune initial RSG sensitive initial SG consistent theory matrix completion collaborative filter subsection rank matrix completion demonstrate effectiveness RSG without knowledge local error bound movie recommendation data namely movielens data contains rating user movie formulate task recover user movie rating matrix partially matrix objective compose loss function difference entry nuclear norm regularizer enforce http  org datasets movielens yang lin rank min xij yij  user movie denote entry denote loss function kxk denotes nuclear norm regularization parameter loss function hinge loss absolute loss absolute loss hinge loss   introduce threshold due distinct rating assign movie define max xij yij otherwise loss function nuclear norm semialgebraic function satisfies error bound compact however remains local error bound hence RSG SG simply iteration stage RSG baseline SG implement objective iteration plot RSG converges faster SG verify effectiveness RSG predict theorem comparison freund SG subsection propose RSG freund SG algorithm empirically later algorithm fix relative accuracy fslb fslb strict bound maintain objective optimization comparison RSG fix freund SG algorithm input parameter plot objective versus iteration algorithm conduct classification data subsection namely housing data data robust regression strict bound fslb freund algorithm SGR refers freund SG algorithm specify relative accuracy instance data report objective iteration RSG competitive performance converge faster freund algorithm achieve accurate objective gap conclusion propose novel restart subgradient non smooth non strongly convex optimization obtain optimal leverage RSG beating subgradient without smoothness convexity objective gap robust regression RSG SGR SGR SGR SGR iteration objective gap robust regression RSG SGR SGR SGR SGR objective gap robust regression RSG SGR SGR SGR SGR iteration objective gap robust regression RSG SGR SGR SGR SGR comparison RSG freund SG algorithm SGR housing data objective gap robust regression RSG SGR SGR SGR SGR iteration objective gap robust regression RSG SGR SGR SGR SGR objective gap robust regression RSG SGR SGR SGR SGR iteration objective gap robust regression RSG SGR SGR SGR SGR comparison RSG freund SG algorithm SGR data bound optimality residual establish generic complexity RSG improves standard subgradient non smooth non strongly convex admit local error bound derive improve iteration complexity RSG extension parameter variant RSG without knowledge constant local error bound experimental machine task demonstrate effectiveness propose algorithm comparison subgradient