advance inverse reinforcement irl sophisticated inference framework relax model assumption agent behavior reflect intention instead global behavioral model recent irl demonstration data account trajectory correspond intention generate domain expert intuitive concept subgoals upon premise trajectory explain efficiently locally within context globally enable compact representation behavior assumption implicit intentional model agent goal forecast behavior unobserved situation integrate bayesian prediction framework significantly outperforms exist irl smooth policy estimate consistent expert notably framework naturally handle situation intention agent classical irl algorithm fail addition due probabilistic model straightforwardly apply active scenario demonstration expert keywords demonstration inverse reinforcement bayesian nonparametric model subgoal inference graphical model gibbs sample introduction inverse reinforcement irl refers infer intention agent expert behavior markov decision MDP formalism intention encode reward function agent instantaneous feedback situation encounter decision classical irl assume exists global reward model explains entire demonstration expert relax restrictive model assumption recent irl agent intention presume demonstration data inherently compose trajectory reflect intention domain expert premise expert trajectory demonstrate behavior explain efficiently locally within context global reward model illustrative task expert approach intermediate target finally global goal similarly agent eventually return initial cyclic despite simplicity task encode behavior global intention model reward structure comprises comparably redundant action reward alternative model strategy rely  expansion agent representation memorize goal resort decision framework semi mdps option achieve task abstraction substantially simpler model framework minimal adaptation standard MDP formalism hypothesis behavioral model sufficiently expert policy insight motivates approach task decompose subtasks considerably model effort task description building synthesize arbitrarily complex behavioral strategy suitable sequence subtasks possibility comparably task representation intuitive concept subgoals achieve efficiently encode expert behavior task adapt partitioning expert data propose framework upon bayesian nonparametric inverse reinforcement subgoal representation task demonstration data however without underlie subgoal relationship policy model generalize strategy demonstrator address limitation generalize BNIRL model insight previous nonparametric subgoal model policy recognition building compact intentional model expert behavior explicitly describes local dependency irl via nonparametric spatio temporal subgoal model sequence target cyclic behavior behavior motivate subgoal principle grid dynamic described task description global reward function inefficient action reward explain trajectory structure however data described efficiently subgoal encoding scenario analyze detail demonstration underlie subgoal structure integrate bayesian prediction framework exploit spatio temporal context demonstration capable smooth policy estimate consistent expert furthermore capture posterior information data enables apply propose approach active data acquisition posterior predictive distribution model experimental propose approach baseline variety benchmark task scenario reveal approach performs significantly BNIRL model alternative irl task interestingly algorithm outperforms baseline expert reward structure dense underlie subgoal assumption violate related decompose complex behavior around researcher approach overall exist approach clearly categorize accord criterion approach formulation distinguish active algorithm interact freely environment passive behavioral model solely observation furthermore soˇ ˇsic rueckert peter zoubir koeppl discriminate explicit intentional model underlie task directly trajectory latter distinction sometimes refer intentional  approach concise summary relevant restrict passive approach focus intentional considerably overview active approach refer exist literature pursue decomposition behavior global trajectory irl approach   propose hierarchical prior reward function account trajectory data reflect behavioral intention generate domain expert similarly  expectation maximization cluster approach individual trajectory accord underlie reward function choi kim generalize propose nonparametric bayesian model intention priori unbounded expert data global concerned subgoal model conduct option instance propose cluster approach distance minimal option explain expert behavior alternative handcraft option probabilistic treatment data involves hoc choice direction principled probabilistic option framework expectation maximization framework capable infer sub policy automatically reinforcement context intra option however behavioral model estimate policy parameter sub policy specify manually latter propose hierarchical nonparametric irl framework sequential representation demonstrate task transition define local linearity behavior however contrast inference perform jointly isolated stage stage propagates estimate associate model parameter moreover temporal relationship demonstration data identify local linearity hoc fashion windowing function another model explicitly address issue employ hidden markov model hmm structure establish temporal relationship demonstration instance regard generalization model  extends  framework impose markov structure reward model similarly extend hmm demonstration vector autoregressive model suitable movement primitive however primitive processing meaning quality irl via nonparametric spatio temporal subgoal model representation crucially depends initial segmentation stage contrast automatically learns timing subgoals via via assume objective finally encode global function recently related approach probabilistic movement primitive jointly solves segmentation unknown primitive expectation maximization framework model operates purely trajectory cannot reveal latent intention demonstrator another variant approach explicitly address propose  srivastava author propose replace hmm emission model MDP model infer policy model trajectory instead recognize dynamic model later extend augment hmm representation beta model facilitate across trajectory model formulation highly flexible drawback inference becomes computationally expensive involves multiple irl iteration per gibbs contrast hmm sequential focus temporal relationship subtasks approach establishes correlation structure demonstration employ non exchangeable prior distribution subgoal assignment without commit purely temporal factorization subgoals compact model representation avoids estimate latent subgoal transition probability hmm structure flexibility capture temporal spatial dependency subtasks outline organization briefly revisit BNIRL model discus limitation basis introduces intentional subgoal framework address shortcoming BNIRL derive sample inference scheme model explain framework subgoal extraction action prediction experimental synthetic data finally conclude bayesian nonparametric inverse reinforcement purpose  principle bayesian nonparametric inverse reinforcement briefly building model focus limitation framework motivates extend model formulation finally inference approach afterwards revisit BNIRL framework irl paradigm goal BNIRL infer intention agent demonstration data standard MDP model formalize finite assume soˇ ˇsic rueckert peter zoubir koeppl invariant transition model finite action available agent notational convenience integer denotes cardinality BNIRL assume expert demonstration action consists agent correspond action herein denotes demonstration throughout shorthand notation access collection expert action individually BNIRL model assumption temporal demonstration action arisen specific arbitrary agent decision later contrast classical MDP formalism irl framework BNIRL presuppose expert behavior necessarily originates underlie reward function instead introduces concept subgoals correspond subgoal assignment underlie assumption decision expert selects subgoal action subgoal herein reward function define simplest corresponds reward goal identify reward function otherwise indicates subgoal location positive constant although principle legitimate associate subgoal arbitrary reward structure encode complex goal orient behavior restriction reward function equation sufficient behavioral complexity synthesize combination subgoals nonparametric BNIRL subgoals assume unbounded reward model equation advantage however posterior inference expert subgoals becomes computationally tractable explain therefore focus reward model summarize infinite collection subgoals multiset adopt assumption subgoal assignment BNIRL achieve indicator variable annotate demonstration unique subgoal index prior distribution model chinese restaurant subgoal prior distribution BNIRL formulation variable argument nonetheless author BNIRL restrict distribution indeed implies conditioning irl via nonparametric spatio temporal subgoal model assigns indicator jth subgoal prior probability shorthand notation collection indicator variable denotes assignment jth subgoal distinct entry parameter diversity assignment target subgoal expert assume action accord softmax decision weighs return action another exp exp herein denotes action action optimal policy subgoal reward function max  expectation respect stochastic action sequence induced fix policy initial action execute explicit notation disambiguate temporal index decision demonstration index action softmax policy model expert ability maximize future return target subgoal coefficient express expert confidence optimal action combine subgoal prior distribution partition model obtain joint distribution demonstrate action subgoals subgoal assignment structure distribution visualize bayesian network worth emphasize although refer likelihood model  BNIRL really model action conditional contrast distribution equation therefore conditional distribution conditional generative model variable posterior inference BNIRL refers approximate computation conditional distribution allows identify potential subgoal location correspond subgoal assignment available demonstration data detail reader refer soˇ ˇsic rueckert peter zoubir koeppl BNIRL model intermediate model ddBNIRL model ddBNIRL model relationship subgoal model illustrate bayesian network shade node variable deterministic dependency highlight stroke limitation BNIRL subgoal inference motivate approach irl BNIRL framework promising variety scenario model formulation  significant conceptual limitation explain detail limitation subgoal  posterior predictive policy central limitation BNIRL framework restrict pure subgoal extraction inherently reasonable mechanism generalize expert behavior infer subgoals framework treat subgoal assignment exchangeable random variable implication induced partition model agnostic covariate information data behavioral model unable propagate expert knowledge situation illustrate investigate predictive action distribution arises BNIRL formulation simplicity without loss generality assume perfectly infer subgoals correspond subgoal assignment demonstration denote predict action irl via nonparametric spatio temporal subgoal model BNIRL model yield latent subgoal index belonging softmax decision equation optimal deterministic policy subgoal aspire noisy expert behavior optimal action accord infer reward model equality equation conditional independence imply equation easily verify  graphical model equation reveals predictive model characterize posterior distribution latent subgoal assignment intuition generalize expert situation account information likely subgoal target expert however BNIRL distribution model without consideration query variable conditional independence equation distribution effectively reduces CRP prior due intrinsic  considers subgoal frequency readily infer assignment clearly subgoal assignment mechanism solely frequency information predict expert behavior inevitably ignore structural information demonstration return subgoal probability query regardless agent actual situation contrast reasonable assignment mechanism inherently account context agent action author BNIRL discus action selection propose assignment strategy action marginalization approach satisfactory allege conditioning query equation  involve subgoal indicator variable equation remedy without modify model external processing scheme waypoint limitation spatial temporal context waypoint described processing routine convert subgoals identify BNIRL valid option model obtain model reconstructs demonstrator sequence infer subgoals complies spatio temporal relationship expert decision demonstration phase initiation termination option policy soˇ ˇsic rueckert peter zoubir koeppl imperfect demonstration bag cluster noisy trajectory label diagram illustrate implication  assumption BNIRL bag model BNIRL partition mechanism ignores spatio temporal context data discriminate demonstration agent intention diagram illustrates partition simplify prior neglect impact likelihood model latter indeed context action cannot account spatial temporal data action separately construct distance identify subgoals temporal prescribed expert combine BNIRL allows synthesize behavioral model mimic expert behavior however strategy significant drawback waypoint spatio temporal relationship individual demonstration explore hoc fashion largely ignore actual inference procedure information enters via likelihood model partition model explain limitation lack context awareness inference mechanism overly prone demonstration proximity subgoals visitation distance metric define correspond physical location construct metric usually straightforward however encode arbitrary abstract information become metric unfortunately BNIRL framework waypoint cannot apply multiple unaligned trajectory obtain expert data temporal information situation occurs instance expert data action unknown timestamps coherent trajectory irl via nonparametric spatio temporal subgoal model assign visitation infer subgoals meaningful expert eventually subgoals demonstration phase subgoals aforementioned distance metric subgoals guaranteed constrain subgoal prior distribution expert data footnote reduces flexibility model potentially disables compact encoding task limitation inconsistency invariance intention agent behavior encounter agent static strategy optimize fix objective intention agent latter clearly inference identify intention agent understand temporal relationship static scenario contrast implies exists optimal policy task action mapping imprint specific structure inference BNIRL model generally category freely allocates subgoals per decision per flexible agent objective important understand model actually distinguish described scenario explain limitation temporal aspect data explicitly model BNIRL framework waypoint subsequently capture overall chronological consequence model tailor scenario ignores valuable temporal context reliably discriminate demonstration agent intention model agnostic predefined invariant optimal policy static scenario lack structure inference harder allows model inconsistent data representation static potentially assign subgoal violate mention action limitation subgoal likelihood model apart limitation BNIRL partition model problematic issue concern softmax likelihood model equation demonstrate specific model encodes indeed contradictory intuitive understand subgoals critical prediction expert behavior drastically affect localization subgoals somewhat hidden model equation defer detailed explanation soˇ ˇsic rueckert peter zoubir koeppl demonstration local global trajectory subgoals global goal difference local constrain global unconstrained subgoal depict demonstration data solid potential goal subgoal location explain behavior indicates correspond subgoal assignment trajectory trajectory approach goal agent global goal temporarily distract trajectory partition assumption expert subgoals demonstration partition without restriction subgoal location yield compact encode task intention invariant intention schematic comparison behavior illustrate agent trajectory indicates temporal progress intention agent perform action revisit dot contrast invariant intention imply action policy agent incentive perform action already definition underlie objective remain  diverge action subfigure therefore explain suboptimal behavior irl via nonparametric spatio temporal subgoal model limitation action demonstration lastly minor BNIRL framework inference algorithm demonstration data action access expert action assumption restrictive practical confines application model setting laboratory monitoring expert important estimate expert action sequence recover BNIRL additional sample stage omit successor expert decision marginalize inference scheme described correspond sample stage nonparametric spatio temporal subgoal model introduce redesign inference framework analogy BNIRL refer distance dependent bayesian nonparametric irl ddBNIRL derive model series modification BNIRL framework address previously described shortcoming conceptual rethink framework discussion commonly softmax action selection strategy equation context subgoal inference finally redesign subgoal likelihood model limitation focus subgoal allocation mechanism introduce closely related model formulation target behavior described thereby address limitation invariant intermediate model introduces subtle important structural modification BNIRL framework generalize model account spatial structure finally allows extrapolate expert behavior unseen situation generalization metric arises naturally context subgoal inference limitation lastly tackle variant model explicitly considers temporal aspect subgoal limitation later contrast BNIRL model likewise subgoal extraction action prediction moreover bayesian methodology approach posterior information subgoal likelihood model approach RL literature BNIRL exploit softmax equation transform optimal policy valid subgoal likelihood model softmax action origin RL boltzmann exploration strategy commonly apply cope exploration exploitation dilemma recent however become facto standard imperfect decision strategy demonstrator soˇ ˇsic rueckert peter zoubir koeppl focus implication model subgoal extraction contradicts intuitive understand characteristic reasonable subgoal model argue subgoal posterior distribution arise BNIRL softmax model limited infer latent intention agent due subgoal artifact dynamic cannot reconcile evidence demonstration insight propose alternative transformation scheme consistent subgoal principle reward function implication softmax likelihood model concern choice uncertainty coefficient explain agent target subgoal likelihood equation quantifies probability agent decides specific action correspond action linear underlie reward function equation softmax likelihood model implies expert ability maximize reward reflect probability magnitude assume subgoal reward concentrate probability signifies confidence action choice assume goal reward virtually increase confidence expert difficulty underlie task optimal policy remain unchanged nonetheless BNIRL model readjust uncertainty coefficient model consistent however model reference expert uncertainty across scenario choice becomes nontrivial parameter significant impact granularity subgoal model purposeful goal orient behavior random decision described specific subgoal reward model equation really consequence softmax transformation equation occurs model apply regular MDP environment arbitrary reward function agent additional constant reward clearly constant reward information underlie task hence affect agent belief optimal choice action discussion constant reward function transformation reward russell observation intuition seek rationality model invariant affine transformation reward signal meaning reward function intentional representation achieve model behavior agent relative advantage action absolute return impact transition dynamic implication softmax likelihood model immediate inherently dynamic explain scenario precise potential goal expert adopt irl via nonparametric spatio temporal subgoal model grid dynamic described upward trajectory action aspire explain sub goal depict intuitively demonstration goal upper concentrate around vertical moreover away smooth decrease subgoal likelihood rate decay reflect assume confidence expert induced BNIRL subgoal posterior distribution contradicts intuition model yield unreasonably posterior upper border accord intuitive understand cannot justified demonstration pin recall equation likelihood action grows correspond hence demonstrate action subgoal assume upper border bellman principle express optimal function subgoal ET ET ET  maxa arg maxa optimal policy subgoal subgoal reward equation lastly tpt denotes improper discount distribution generate execute policy initial refers probability policy exactly define implicitly via transition model outer expectation equation account stochastic transition successor inner expectation evaluates cumulative reward reachable important construction function agent depends choice action whereas remain argument expectation purely dynamic subgoal policy focus inner conclude regardless chosen action whenever assume subgoal induces visitation frequency location latter fulfil goal discounting transition dynamic induced subgoal goal frequently implies model generally prefers subgoals demonstration cannot justified demonstration simply expert soˇ ˇsic rueckert peter zoubir koeppl goal desire proximity naturally attribute subgoal prior model moreover depends primarily dynamic strongly influence action agent scenario pathological independent agent decision meaning agent illustrate extreme scenario agent driven terminal regardless execute policy although somewhat pointless subgoals context terminal exhibit subgoal likelihood accord softmax model correspond visitation frequency inevitably soften variant border agent freedom hence others transition others naturally exhibit increase visitation frequency due characteristic environment symptom described clearly upward policy imply demonstration induced visitation distribution exhibit increase exactly aforementioned border due reflection agent boundary trajectory proximity normalize likelihood model address modify likelihood model rescale involve denote maximum minimum subgoal maxa mina define normalize action function otherwise arbitrary constant cancel equation contrast bellman action function quantifies return action normalize function ass return action relation return action concept advantage function important difference return normalize indicator relative quality action accordingly interpret relative advantage relative maximum advantage action normalize subgoal likelihood model construct analogously BNIRL likelihood model exp model invariant affine transformation reward function summarize proposition irl via nonparametric spatio temporal subgoal model BNIRL model normalize model probability probability BNIRL model normalize model comparison subgoal posterior distribution induced BNIRL likelihood model propose normalize model grid dynamic described uniform subgoal prior distribution scheme understood per subfigure BNIRL likelihood model yield unreasonably subgoal posterior border due locally increase visitation probability arise reflection trajectory ending implicit proximity model detail mitigate propose normalize likelihood model describes action selection agent relative advantage action instead absolute return soˇ ˇsic rueckert peter zoubir koeppl proposition affine invariance MDP reward function denote correspond optimal action function correspond normalize function hence subgoal likelihood model equation invariant affine transformation proof due linear dependence reward function equation relationship equation immediately propose likelihood model advantage enables generic choice uncertainty coefficient return fix indicates indicates confidence corresponds assumption expert chooses optimal action probability probability favorable action irrespective underlie model moreover reveal induced subgoal posterior distribution notably closer expectation twofold likelihood computation relative advantage mitigates influence transition dynamic described  visitation distribution equation return action reduce propose normalization instance agent grid policy upward induced visitation distribution exhibit increase upper border manipulate action agent bellman function accordingly model increase subgoal likelihood normalize model contrast affected construct likelihood increase visitation frequency relative normalization diminishes discounting subgoal posterior distribution concentrate around trajectory significant along extrapolate agent allows identify potential goal location flexibility infer subgoal constellation illustrate scenario normalize model assigns posterior corridor subgoal corridor explains demonstration equally difference model pronounce transition dynamic impact agent behavior due detail refer additional insight subgoal inference mechanism model invariant intention redesign likelihood model focus partition structure model herein intention agent constant irl via nonparametric spatio temporal subgoal model respect explain limitation consistent standard MDP formalism optimal policy task described action mapping account relation establish link model partition structure underlie replace  indicator variable unlike indicator directly data instead although formally variable distribution CRP yield intermediate model  structure illustrate difference equation subgoals indexed model intermediate model policy  underlie action approximate expert model unable extrapolate information unvisited explain replace exchangeable prior distribution subgoal assignment induced CRP non exchangeable account explicitly covariate information demonstration insight bayesian policy recognition distance dependent chinese restaurant purpose allows intuitive handle context explain alternative survey  williamson contrast CRP assigns partition  assigns pairwise distance assignment described indicator prior distribution otherwise herein link parameter denotes distance monotone decrease function distance obtain via suitable metric define furthermore calibrate function subsequent partition structure component induced  graph joint distribution visualize denotes subgoal label arise indicator highlight dependence underlie subgoal mechanism refer model ddBNIRL soˇ ˇsic rueckert peter zoubir koeppl canonical metric spatial subgoal model  prior model partition equation inevitably notion distance compute involve function distance limitation suitable quasi metric derive transition dynamic canonical choice ddBNIRL model markov chain agent specific policy chain naturally induces initialize eventually min context subgoal quasi metric distance goal correspond optimal subgoal policy arg maxa ddBNIRL waypoint BNIRL choice particularly appeal subgoal policy already available within inference procedure action compute likelihood model correspond distance obtain efficiently policy evaluation corresponds optimal negative return respective target absorb zero reward assign reward choice function equation evident  model partition structure connection nearby context subgoal translates prior assumption likely subgoal approach expert specific localize assumption reasonable task task target approach beneficial model reuse subgoal various context obtain efficient task encode mathematical prerequisite encoding function shrink zero distance remains non zero probability apart achieve convex combination monotone decrease zero approach function constant offset chosen radial basis function implement desire locality model decay function respectively calibrate quantiles distribution distance irl via nonparametric spatio temporal subgoal model model intention expert intention flexibility BNIRL subgoal decision instead restrict policy target unique subgoal per hence retain BNIRL structure define subgoal allocation mechanism data related indicator variable however contrast BNIRL assumption temporal relationship subgoals allows arbitrary expert intention joint distribution smooth action expert persistently subgoal extend  encode underlie smoothness assumption function define temporal distance demonstration purpose additional information namely unique timestamp demonstration accordingly assume data denotes dth demonstration prior distribution data partitioning  otherwise index demonstration herein denotes temporal distance data notation distinguish data related partition variable distance related counterpart ddBNIRL however function independent underlie model chosen described calibrate duration demonstrate task obtain temporal subgoal model refers subgoal label dth demonstration induced assignment analogous spatial subgoal model refer model ddBNIRL structural difference model relationship BNIRL distance dependent CRP contains classical CRP specific choice distance metric function ddBNIRL model strict generalization BNIRL framework neglect likelihood normalization ddBNIRL generalizes timestamps naturally available demonstration trajectory consecutive action temporal information data waypoint limitation author BNIRL formally assume access reduce data action soˇ ˇsic rueckert peter zoubir koeppl intermediate model however although BNIRL model recover ddBNIRL important sample mechanism framework fundamentally whereas BNIRL subgoal assignment sample directly cluster structure ddBNIRL define implicitly via assignment variable respectively explain   markov chain gibbs sampler significantly faster cluster assignment alter effectively realizes gibbs sampler static versus dynamic subgoal allocation model structure described alternative subgoal naturally arises approach application scenario explain previous difference model structure subgoals allocate ddBNIRL relies static assignment mechanism consistently link individual correspond subgoals ddBNIRL allocates subgoals per demonstration latter action explain intentional setting hence decision described via static assignment uniquely characterizes situation flexible model allows account additional information decision theory optimal invariant mdps formulate deterministic invariant markov policy fully static ddBNIRL framework therefore assume transition dynamic constant respect agent rationally knowledge environment exist plausible potentially agent execute variant policy reward model agent decision markovian respect assume model agent decision additional context information explicitly capture representation accordingly assume markov meaning chosen representation sufficiently capture decision strategy agent theoretical justification prefer dynamic subgoal model ddBNIRL static ddBNIRL assume intention agent truly dependent practically however representation fulfill markov requirement obvious explanation actual demonstrator perfectly situation occurs omit rigorous proof intuitively action optimal MDP reward function synthesize via ddBNIRL assume individual subgoal extreme irl via nonparametric spatio temporal subgoal model context available agent observable modeler another potential situation strategy agent depends information independent dynamic hence deliberately exclude variable parameter unaffected action agent  specific strategy generic framework setting described agent learns multiple sub policy trigger context information treat separately external observer unaware context information policy agent potentially dependent disentangle individual sub policy resort dynamic subgoal encode ddBNIRL however context temporal information approach equivalently representation specifically static ddBNIRL model augment variable context information accordingly resort dynamic subgoal allocation scheme ddBNIRL distance metric account context conversely purely invariant context described quantity ddBNIRL ddBNIRL regard coin invariant policy demonstrator information prediction inference introduce ddBNIRL framework explain generalize expert behavior focus task action prediction query explain extract information demonstration data along insight implicit intentional model framework redundancy minimum consideration ddBNIRL model ddBNIRL straightforwardly equation subgoals reference obtain correspond expression simply replace assignment variable cluster definition equation accordingly occurrence becomes replace action prediction  task predict action query optimal respect expert unknown reward model however contrast exist irl approach estimate expert reward function account entire hypothesis reward model allows obtain posterior predictive policy expert data mathematically task formulate compute predictive action distribution capture information expert soˇ ˇsic rueckert peter zoubir koeppl behavior demonstration expand distribution latent assignment conditional distribution express posterior distribution subgoal target query prediction conditionally independent demonstration partition structure correspond subgoal assign joint distribution equation correspond normalize constant supp relationship supp contrast summation subgoal location computational complexity subgoal prior distribution grows linearly marginalization respect indicator variable involves summation becomes quickly intractable therefore approximate operation via monte carlo integration yield supp prediction perform via maximum posteriori policy estimate arg max inference task hence reduces computation posterior sample described irl via nonparametric spatio temporal subgoal model partition inference joint model equation obtain posterior distribution factorize supp supp denotes kth cluster induced assignment cluster define explain   indicator sample efficiently generate gibbs chain  graph define subset indicator insertion additional outcome illustrate loop underlie partition structure unaffected leaf structure unchanged target already cluster creates link cluster latter involve cluster merge corresponds merge associate sum equation accord conditional distribution gibbs procedure obtain cluster merge     cluster   merge herein denotes marginal action likelihood demonstration accumulate cluster supp normalize constant posterior distribution cluster subgoal equation accordingly equation interpret likelihood ratio partition define merge structure insert subgoal inference important inference described collapse sample scheme subgoals model marginalize ddBNIRL framework differs BNIRL irl reward model expert explicit predict action nonetheless desire purpose analyze expert intention estimate subgoal location obtain hoc fashion subgoal posterior distribution equation assignment soˇ ˇsic rueckert peter zoubir koeppl insertion dash arrow  graph cluster membership node define implicitly via component graph loop insert already node alter cluster structure unconnected component merges associate cluster action inference mention BNIRL algorithm knowledge expert action limit potential application scenario generalize inference scheme access information alternative data refers expert immediately inference perform extend gibbs procedure additional collapse sample stage supp fix assignment recovers estimate latent action transition knowledge transition model link expert action successor extension ddBNIRL model transition timestamps computational complexity discus computational complexity approach purpose reminder notation cardinality action respectively demonstration refer kth cluster ddBNIRL data cluster ddBNIRL subsequent additionally notation ND access demonstration data associate irl via nonparametric spatio temporal subgoal model cluster cluster iteration supp shorthand subgoal prior distribution indicator variable ddBNIRL ddBNIRL initialization phase model BNIRL precede planning phase compute potentially parallel action function equation subgoals allows construct subgoal likelihood model equation overall computational complexity procedure   denotes complexity planning routine approximately MDP action iteration algorithm instance achieve  assume expert subgoals demonstration phase restrict subgoal prior upper bound min exist approximation technique computation tractable continuous discussion sample procedure compute cluster likelihood pairwise likelihood accord equation random initial cluster structure likelihood computation kth cluster involves ND data calculate subgoals average execute potentially parallel cluster however demonstration associate exactly cluster directly ddBNIRL via correspond variable ddBNIRL hence ND complexity compute cluster likelihood  irrespective actual cluster structure applies computation pairwise likelihood yield complexity latter cluster combination assume initial cluster pairwise likelihood compute hence overall complexity initialization phase summarize  partition inference partition inference bulk computation construction likelihood equation update whenever cluster structure analyze complexity sample individual assignment variable likewise remove belongs  graph associate cluster cluster likelihood compute upper bound data associate cluster operation complexity  initialization phase irrespective occurs compute pairwise cluster likelihood cluster via choice  operation initialization phase assign indicator variable assume simplicity cluster constant gibbs cycle complexity update cluster assignment hence  pessimistic upper bound obtain assume data defines cluster soˇ ˇsic rueckert peter zoubir koeppl complexity increase  identify cluster structure assignment additionally component underlie  graph explain polylogarithmic action sample compute conditional probability distribution action evaluate involve action belong cluster action equation compute action involve upper bound append belongs action choice another operation subgoals yield upper bound complexity gibbs cycle involves sample action variable overall complexity hence experimental experimental framework evaluation proof concept conceptual comparison BNIRL performance comparison related algorithm data conduct  robot active task proof concept illustrate conceptual difference BNIRL additional insight latent intentional model framework motivate data originally  environment define grid correspond inaccessible marked horizontal valid expert action comprise action initiate noisy transition inter cardinal direction action depict arrow partition BNIRL remain subfigure ddBNIRL framework obtain posterior sample return respective algorithm ddBNIRL simulated anneal schedule obtain difference approach unlike BNIRL propose framework allows spatial temporal encode task possibility account explicitly demonstrate behavior static dynamic explain context unaware principle dynamic vanilla BNIRL inference scheme irl via nonparametric spatio temporal subgoal model subgoal posterior sample partitioning predictive policy BNIRL ddBNIRL ddBNIRL spatial policy phase subgoal phase subgoal phase subgoal subgoals combine temporal policy BNIRL data demonstration data sample partitioning generate inference algorithm subgoal posterior distribution associate partition ddBNIRL ddBNIRL clearer overview correspond BNIRL distribution omit comparison invariant ddBNIRL policy model synthesize detect subgoals temporal phase identify ddBNIRL background meaning highlight structure policy generalization mechanism BNIRL reasonable predictive policy model limitation soˇ ˇsic rueckert peter zoubir koeppl exploit spatial temporal context data ddBNIRL inherently robust demonstration notably smoother partition structure particularly pronounce data later partition trajectory obtain implicit representation associate subgoal posterior distribution without assign estimate strike posterior distribution correspond partition comparably upper explain intuitively subgoal posterior potentially sequence circumvents posterior exhibit boundary subgoal upper likely trajectory approach contrast BNIRL built generalization mechanism limitation return predictive policy model comprise posterior action information policy estimate compute accord equation additional concern posterior uncertainty illustrates synthesis predictive policy differs ddBNIRL ddBNIRL rightmost subfigure ddBNIRL conditionally independent policy model identify behavioral phase ddBNIRL entire subgoal schedule onto  policy representation closer model recognize ddBNIRL realizes spatial combination temporal ddBNIRL component component activate correspond cluster alternative interpretation behavior random MDP scenario insight generalization ability framework purpose randomly generate mdps  transition dynamic sample independently symmetric dirichlet distribution concentration parameter repetition NR uniformly random assign reward sample uniformly interval zero reward compute optimal deterministic MDP policy respect discount factor generate expert trajectory herein expert optimal action probability random suboptimal action probability obtain sequence algorithm compute normalize loss reconstruct policy accord  irl via nonparametric spatio temporal subgoal model NR NR NR NR comparison inference random MDP scenario reward density empirical standard deviation loss obtain monte carlo graph difference BNIRL BNIRL ext ddBNIRL illustrates importance spatial context subgoal extraction  respectively vectorized function optimal policy reconstruction belongs invariant mdps ddBNIRL lends choice model expert behavior baseline adopt  bayesian policy recognition framework maximum margin irl maximum entropy irl vanilla BNIRL due generalization ability BNIRL limitation waypoint straightforwardly apply scenario multiple unaligned trajectory algorithm extension BNIRL refer BNIRL ext mimic ddBNIRL principle account spatial context demonstration soˇ ˇsic rueckert peter zoubir koeppl assign BNIRL subgoal target closest metric action however assignment actual subgoal inference ddBNIRL reference gain spatial relationship data inference ddBNIRL BNIRL ext augment correspond action sample stage action sequence expert discard data enable comparison remain algorithm loss demonstration reward setting NR BNIRL ext ddBNIRL significantly outperform reference sparse reward structure allows efficient subgoal encode expert behavior enables algorithm reconstruct policy minimal amount demonstration data however BNIRL ext drastically deteriorate denser reward structure difference performance account spatial information partition model BNIRL processing BNIRL ext exploit inference ddBNIRL demonstrates importance processing context information  ddBNIRL outperforms baseline dense reward regime although subgoal encode loses efficiency reveal propose approach combine merit model sample efficiency intentional model max margin max entropy data asymptotic accuracy fully probabilistic  bayesian framework BPR robot ddBNIRL framework various data  lightweight robotic via kinesthetic video demonstrate task http spg  jmlr freedom correspond joint joint equip torque sensor angle encoder recording joint angle velocity acceleration cartesian coordinate span transverse compute raw measurement kinematic model data sample rate downsampled factor yield effective sample rate sufficient temporal resolution scenario goal intentional model behavior partition data meaningful predict desire direction expert simplicity demonstrate comparably loss BPR data explain framework policy model expert behavior assume inherently stochastic contrast stochasticity arises merely consequence suboptimal decision irl via nonparametric spatio temporal subgoal model  lightweight robotic algorithm robustness model error adopt simplistic transition model action inter cardinal direction measurement accuracy effector allows extract highlevel action directly raw data direction angular deviation truth underlie obtain discretizing coordinate measurement predefined detail apart discretization aforementioned data downsampling preprocessing apply spatial partition expert behavior described invariant policy model aspire capture via ddBNIRL cycle task video analyze variant ddBNIRL model allows comparison approach task consists approach target marker video eventually return initial regard version loop described  explain classical irl algorithm rely global reward model max margin irl max entropy irl completely fail due periodic task downsampled discretized data arrow obtain expert trajectory visualization purpose discretization chosen ddBNIRL background indicates partition structure compute posterior sample cluster clearly reveal modular structure task intuitive interpretable explanation data however although induced policy soˇ ˇsic rueckert peter zoubir koeppl expert data sample partition policy estimate uncertainty estimate predictive model ddBNIRL cycle task raw measurement discretized demonstration data arrow background indicates partition structure obtain posterior sample maximum posteriori policy estimate visualization model prediction uncertainty entropy correspond posterior predictive action distribution background indicates uncertainty illustration predictive model comprise action information prediction uncertainty model smoothly capture cyclic task cannot obtain trustworthy prediction due lack additional demonstration data unveil expert intention clearly estimate policy cannot reflect prediction uncertainty confidence information bayesian approach naturally quantify prediction uncertainty query correspond posterior predictive action distribution straightforward option prediction entropy irl via nonparametric spatio temporal subgoal model define obtain unbiased approximation non temper predictive distribution gibbs chain unaltered parallel temper chain entropy estimate summarize uncertainty overlaid prediction obtain posterior uncertainty information model active demonstrate temporal partition attention ddBNIRL model vanilla BNIRL approach purpose collection task supplementary video comprises dependent expert behavior complexity obtain quantitative performance evaluation conduct manual segmentation trajectory thereby truth subgoal label decision segmentation depict appendix truth subgoals assume immediately correspond respectively partition structure BNIRL ddBNIRL uniform subgoal prior distribution underlie discretization chosen regular grid background visual comparison structure reveals superiority ddBNIRL vanilla BNIRL quantitative comparison instantaneous subgoal localization error model entire demonstration herein instantaneous localization error action euclidean distance grid location truth subgoal associate correspond subgoal location predict model prediction model entire trajectory data posterior information demonstration ddBNIRL directly return subgoal location estimate instead access subgoal posterior distribution error compute respect subgoal location   arg max supp ddBNIRL version equation dot truth annotation significantly increase localization error model explain truth annotation somewhat subjective around switch label comparably error trajectory stem imperfect synchronization interval execution soˇ ˇsic rueckert peter zoubir koeppl instantaneous subgoal localization error ddBNIRL upper BNIRL data dot subgoal switch correspond truth subgoal annotation depict average localization error ddBNIRL significantly BNIRL approach median qualitative comparison underlie partition structure appendix task recall skip correspond data preprocessing hence capture accuracy performance median localization error series mask outlier realistic error quantification sample obtain error plot ddBNIRL localization error discretization interval BNIRL propose yield error reduction average active posterior predictive action distribution quantify prediction uncertainty model query opportunity apply framework active irl via nonparametric spatio temporal subgoal model comparison random data acquisition active random MDP scenario empirical loss obtain policy model data query obtain monte carlo induced uncertainty indicates model instruction expert effectively demonstrate procedure reconsider random MDP active context active strategy previously random data acquisition scheme initialization procedure request action demonstrator initial data herein drawn uniformly random action generate accord noisy expert policy described active algorithm request series subsequent demonstration induce sequence data query chosen accord specific data acquisition criterion  algorithm evaluate predictive model arg max  purpose acquisition criterion ass uncertainty model query demonstration request uncertainty choice entropy  confidence  max margin  soˇ ˇsic rueckert peter zoubir koeppl denote respectively likely likely action accord distribution arg maxa arg maxa iteration compute loss equation induced policy model correspond loss obtain random data acquisition curve delineate model significantly improve active acquisition scheme reduces expert demonstration successfully task conclusion building upon principle bayesian nonparametric inverse reinforcement propose framework data efficient irl leverage context information demonstration predictive model expert behavior amount training data central framework model architecture spatial subgoal capture intention contrast BNIRL model architecture explicitly covariate information demonstration predictive model inherently robust demonstration BNIRL model recover framework conduct drastic improvement vanilla BNIRL approach achieve subgoal localization accuracy stem improve likelihood model context aware cluster data notably framework outperforms reference analyze benchmark scenario additionally capture posterior information subgoal representation prediction uncertainty expert behavior reflect posterior predictive action distribution basis apply active request additional demonstration data expert limitation approach architecture MDP model discrete action subgoal principle straightforwardly continuous metric construction likelihood model becomes environment knowledge optimal action function potential subgoal location however BNIRL exist approximate likelihood concept apply equally ddBNIRL future efficacy model involve continuous distance approach