aim derive theoretical  behavior crossvalidation procedure apply knn context binary classification focus validation LpO ass performance knn classifier remarkably LpO estimator efficiently compute context formula derive celisse mary huard strategy derive exponential concentration inequality LpO estimator apply knn classifier obtain exploit connection LpO estimator statistic intensive generalize efron stein inequality apply LO estimator important contribution derive quantification discrepancy LpO estimator classification error risk knn classifier optimality bound bound simulation keywords classification validation risk estimation introduction knn algorithm binary classification popular prediction algorithm predict majority vote label although knn classifier successfully apply classification task efficient implementation developed datasets theoretical performance knn classifier already extensively investigate context binary classification preliminary theoretical date  celisse  mary huard license CC http creativecommons org license attribution requirement http jmlr org html celisse mary huard hart  knn classifier weakly universally consistent NN classifier asymptotic expansion error rate derive strategy successfully apply knn classifier  venkatesh influence parameter risk knn classifier asymptotic expansion derive poisson binomial model training recently limitation suffer classical knn classifier deduce improve version local choice semi supervise context contrast aforementioned  dasgupta focus finite sample framework typically upper bound probability risk knn classifier bound distribution alternatively regression kulkarni  derive strategy finite sample bound performance NN extend knn knn estimator estimate entropy refer interested reader  devroye almost thorough presentation knn algorithm various context numerous practical application compute validation CV estimator popular strategy evaluate performance knn classifier CV procedure principle consists splitting sample disjoint subset training respective cardinality training data compute classifier performance evaluate data comprehensive review validation procedure refer interested reader  celisse focus LpO validation CV procedure belongs exhaustive strategy considers average  training usually induced computation LpO prohibitive surrogate fold validation  however  celisse mary huard recently derive formula respectively bootstrap LpO procedure apply knn classifier formula efficient computation LpO estimator moreover  estimator suffers bias variance LpO LpO strictly improves upon  context although assess risk knn classifier CV theoretical guarantee regard performance moreover probably technical exist apply LO LpO LpO procedure estimate risk alternatively classification error rate knn classifier purpose distribution theoretical guarantee behavior LpO respect influential parameter instance aim performance CV estimate risk knn exist regime function LpO estimator consistent estimate risk knn classifier convergence rate LpO estimator contribution contribution fold strategy derive exponential concentration inequality LpO estimator apply knn binary classifier inequality derive convergence rate LpO estimator towards risk knn classifier strategy relies exploit connection LpO estimator statistic rosenthal inequality upper bound polynomial LpO estimator reduces derive bound simpler LO estimator derive upper bound LO estimator generalize efron stein inequality combine previous insight interplay concentration rate finally exponential concentration inequality LpO estimator apply whatever ratio upper bound increase longer bound upper bound cannot improve distribution remainder organize connection LpO estimator statistic clarify recall formula LpO estimator apply knn classifier LpO estimator upper bound LO estimator apply classification algorithm specifies previous upper bound knn classifier theorem characterize concentration behavior LpO estimator respect polynomial derive exponential concentration inequality LpO estimator concern highlight strength strategy inequality concentration inequality derive sophisticated finally exploit previous bound gap LpO estimator classification error knn classifier optimality upper bound distribution framework establish bound upper specific setting empirical report conclusion statistic LpO estimator statistical framework classification tackle binary classification goal predict unknown label observation random variable unknown joint distribution define  denotes reference probability distribution distributional assumption regard predict label aim building classifier basis random variable celisse mary huard training sample drawn independently setting confusion replace strategy classifier classification algorithm formally define function training sample onto correspond classifier ADn measurable function numerous classifier literature scope review instance focus knn propose fix  instance devroye wagner rogers wagner knn algorithm knn classification algorithm denote consists classify observation majority vote decision label closest denote training sample chosen accord distance associate euclidean norm adaptive metric literature scope reference distance training sample emphasize broken index choice lemma  devroye lemma formally index knn classifier define otherwise label denotes bernoulli random variable parameter validation sample performance classifier ADn respectively classification algorithm assess classification error respectively risk define focus estimation expectation LpO validation LpO successively considers split training cardinality cardinality denote subset cardinality defines split training sample sample classification algorithm LpO estimator performance ADn average split classification error estimate rbp ade performance CV estimate risk knn ade classifier built refer reader  celisse detailed description LpO validation procedure sequel lengthy notation rbp replace rbp setting confusion arise algorithm training sample rbp training sample LpO knn classification algorithm usually due seemingly prohibitive computational LpO apply reduces however context density estimation regression formula derive LpO estimator apply projection kernel estimator knn classifier another instance estimator efficiently compute LpO estimator computation complexity linear previously establish celisse mary huard briefly recall formula LpO estimator express sum observation sample probability ade ade integration respect random variable uniform distribution subset cardinality instance proportion subsamples cardinality prescribed index lemma calculation sequence depends dependency skip sake readability derivation respect random variable denotes rank sample instance sum involves candidate training subset probability easily compute lemma ade FH FH celisse mary huard FH FH respectively denote cumulative distribution function denotes hypergeometric distribution computational LpO knn classifier LO NN classifier whatever contrast prohibitive computational complexity seemingly suffer LpO statistic bound LpO purpose strategy derive upper bound polynomial LpO estimator strategy establish connection LpO risk estimator statistic exploit connection derive upper bound LpO estimator upper bound relate LpO estimator LO estimator classifier introduce statistic recall purpose thorough presentation refer    definition statistic average tuples distinct index definition   denote measurable function integer assume symmetric function argument function  statistic kernel clarify connection LpO statistic introduce statistic strategy relies consists statistic average permutation sum independent variable proposition hoeffding notation definition define  denotes integer denotes summation permutation performance CV estimate risk knn remark development expose connection LpO estimator define statistic theorem classification algorithm classifier compute training LpO estimator rbp statistic kernel define AD denotes sample withdrawn instance denotes knn algorithm cardinality satisfy implies proof theorem LpO estimator performance classification algorithm compute satisfies rbp rbp ade ade unique index cardinality rbp adv furthermore fix unique index rbp adv adv celisse mary huard kernel deterministic symmetric function argument reduces LO estimator risk classifier compute context binary classifier error rate already derive upper bound LpO estimator classifier expectation define theorem classifier ADn adm correspond classifier built respectively classifier compute training rbp rbp furthermore rbp rbp rbp rbp max vuut var numeric constant denotes optimal constant define rosenthal inequality proposition proof appendix straightforwardly jensen inequality apply average permutation proposition integer becomes becomes consequence strategy proof classical upper bound variance statistic suggests cannot improve without assumption unlike derive rosenthal inequality enables upper bound sum  independent identically random variable  var remark constant furthermore rosenthal inequality allows advantage integer unlike understand behavior LpO estimator highlight later proposition performance CV estimate risk knn bound LpO knn classifier goal specify upper bound theorem knn algorithm introduce theorem express LpO estimator LO estimator compute consists focus LO derive upper bound LO achieve generalization efron stein inequality theorem  inequality theorem generalization sake completeness recall corollary generalization corollary proposition denote independent random variable measurable function independent  exists universal constant  vuut apply proposition LO estimator compute theorem LO estimator apply knn classifier theorem denote knn classifier learnt correspond LO estimator constant arise lemma lemma grows exponentially dimension define proposition proof detailed relies lemma lemma prof kγd dependence upper bound respect explicit constant induces deterioration dimension grows therefore dimension sample upper bound strategy index chosen ensures lemma celisse mary huard easy enables exploit calculation upper bound variance LO estimator risk knn classifier compute rate strict improvement upon derive sub gaussian exponential concentration inequality theorem contrast arise difficulty derive tight upper bound expectation resp denotes sample resp remove combination theorem LpO LO estimator theorem upper bound LO theorem rbp denote LpO risk estimator knn classifier define exist constant rbp rbp rbp rbp  denotes constant arise lemma lemma furthermore rbp rbp rbp rbp max max performance CV estimate risk knn straightforward proof detailed upper bound deteriorate grows longer specifically setup longer therefore unlike inequality particularly relevant setup investigate shao yang celisse respective convergence rate rate becomes approximately emphasize statistic fix LpO estimator gaussian limit distribution theorem  rbp rbp var therefore upper bound non  respect interplay recovers magnitude variance assume constant finally derive specific version rosenthal inequality optimal constant involve balance factor balance factor optimize relative bracket dependence upper bound respect cannot improve proof however cannot conclude cannot improve technical argument exponential concentration inequality exponential concentration inequality LpO estimator apply knn classifier heavily rely inequality previously derive namely theorem emphasize gain strategy proof successively exponential inequality obtain sophisticated discus strength weakness justify additional refinement introduce along exponential concentration inequality rbp rbp derive bound difference inequality proof theorem originally developed LO estimator proposition integer rbp denote LpO estimator classification error knn classifier define rbp rbp denotes constant introduce lemma lemma celisse mary huard proof appendix upper bound strongly exploit subsample sample cannot lemma lemma rough upper bound denominator exponent exhibit factor role distinguish sample although setup highly probability training sample consequently dependence convergence rate proposition improve confirm forthcoming theorem previous comment sharper quantification influence theorem rbp denote LpO estimator classification error knn classifier define exists numeric constant max rbp rbp rbp rbp exp introduce lemma universal constant proof unlike proposition account rank sample enables considerably reduce denominator exponent assume fix instance influence factor asymptotically negligible recover numeric constant upper bound theorem achieve however upper bound theorem reflect dependency respect polynomial theorem deteriorates increase unlike upper bound derive theorem drawback overcome contribution theorem rbp denote LpO estimator classification error knn classifier ˆfk define max rbp rbp rbp rbp exp performance CV estimate risk knn max define theorem furthermore max rbp rbp rbp rbp exp min arises denotes constant introduce lemma lemma proof postpone appendix involves argument derive inequality firstly argument apply derive ineq correspond inequality theorem characterize sub gaussian behavior LpO estimator lemma secondly exploit appropriate upper bound LpO estimator theorem combine proposition establishes exponential concentration inequality upper bound accordance conclusion drawn theorem upper bound increase grows unlike concentration rate achieve whereas useless however remains strictly theorem constant theorem therefore comment regard dependence respect dimension apply facilitate interpretation ineq derive proposition appendix focus description deviation proposition notation theorem rbp rbp vuut constant arise inequality bernstein inequality deviation instead bernstein inequality respect deviation bernstein inequality deviation somewhat bernstein inequality celisse mary huard nevertheless almost recover rate instance therefore allows interpolate rate dependence sub gaussian deviation respect improves upon ineq theorem instance however remains certainly optimal remains widely stage literature generally strength approach versatility indeed deviation directly upper bound LO establish theorem therefore improvement latter upper bound immediately enhance concentration inequality without proof assess gap LpO classification error upper bound derive upper bound discrepancy rbp rbp classification error ˆfk risk ˆfk ˆfk bound LpO estimator completely extension former specifically derive LO estimator apply knn classifier theorem rbp denote LpO risk estimator knn classifier ˆfk define rbp ˆfk rbp ˆfk  moreover rbp ˆfk contrast previous restriction arises theorem lemma devroye wagner upper bound stability knn classifier observation remove training sample actually upper bound remains meaningful performance CV estimate risk knn proof theorem proof ade lemma immediately rbp ˆfk ˆfk ade ADn proof proof combine previous upper bound establish variance LpO estimator rbp ˆfk rbp rbp rbp ˆfk  concludes proof proof ineq intricate postpone appendix rbp ineq upper bound bias LpO estimator difference risk classifier built respectively therefore upper bound increase reliable classifier become another increase precisely upper bound ineq additional restriction reduces universal consistency knn classifier monotonicity upper upper bound respect somewhat unexpected classifier become increase however dependence cannot improve distribution framework proposition upper bound ineq easily derive price increase constant emphasize ineq allows discrepancy LpO estimator risk knn classifier expectation classification error ideally replace risk ˆfk prediction error ˆfk strategy proof additional distribution concentration inequality prediction error knn classifier knowledge concentration inequality available upper bound difference LpO estimator prediction error precisely purpose ineq latter inequality completely strategy trace earlier proof rogers wagner celisse mary huard proof theorem apply LO estimator mention ineq combine jensen inequality accurate upper bound ineq finally apparent difference upper bound ineq completely scheme proof allows derive upper bound LpO estimator exhibit dependence respect contrast exclusively dedicate upper bound difference prediction error LpO estimator however probably optimal upper bound ineq enables achieve minimax rate proposition bound bias LO estimator purpose counter highlight upper bound cannot improve discrete define generative model refer discrete DS parameter fully joint distribution distribution DS satisfies margin assumption   chosen away however favourable forthcoming bound simplification along calculation proposition DS assume odd exists numeric constant independent knn classifier satisfy proof proposition appendix rate righthand achieve generative model DS consequence rate cannot improve without additional assumption instance distribution related comment empirical illustration illustrate proposition simulated data accord DS display evolution absolute bias function curve absolute bias nondecreasing function upper bound plot dash comparison non decrease behavior absolute bias restrict illustrate performance CV estimate risk knn corresponds DS parameter non decrease behavior absolute bias rough location peak denote  deduce peak arise classifier respectively observation disagree strongly classifier label likely therefore discrepancy classifier observation situation arise occurs observation training resp resp expression boil peak  proposition remarkably yield   location peak suggests  arise tune parameter mention curve obtain gaussian mixture model disjoint report empirically illustrates rate limited DS discrete confirms rate cannot improve distribution framework finally display absolute bias function  coef denotes integer choice proposition implies absolute bias decrease rate plot curve contrast panel illustrates  faster decrease rate error described devroye wagner bound minimal convergence rate error celisse mary huard bias bias bias coef bias coef evolution absolute bias function dash correspond upper bound obtain previous data generate accord DS parameter upper bound displayed absolute bias evolution absolute bias respect chosen  denotes integer correspond coef previous chosen  performance CV estimate risk knn proposition assume independent odd ˆfk ˆfk upper bound ineq rate proposition suggests rate cannot improve distribution framework minimax rate conclude corollary finite sample bound gap rbp ˆfk ˆfk probability restriction previous theorem corollary notation theorem assume exists probability ˆfk rbp vuut ˆfk proof corollary ineq combine exponential concentration derive rbp namely ineq theorem upper bound bias ineq ˆfk rbp ˆfk rbp rbp rbp ineq derive bound ˆfk confidence bound however recommend ineq concentration inequality numeric constant optimize sample deviation explicit numeric constant corollary exhibit dependence becomes exponentially increase dependence weaken remains completely stage nevertheless highlight increase quickly deviation whereas ˆfk rbp belong celisse mary huard ineq bias price cannot improve distribution framework accord proposition besides combine restriction consistency constraint conclusion almost convergence rate LpO estimator weaken restriction potentially nuance conclusion highlight deviation inequality deduce optimality minimax rate define statement corollary uniformly respect LpO estimator rbp risk ˆfk knn classifier remain probability proposition notation corollary exists probability LpO estimator knn classifier satisfies ˆfk ˆfk rbp ˆfk ˆfk denotes classification error bayes classifier furthermore assumes regression function belongs min recall rbp ˆfk ineq uniform gap excess risk ˆfk correspond LpO estimator rbp ˆfk probability decrease rate probability directly related factor upper bound decrease rate faster price increase exponent factor numeric constant precise meaning chosen increase deviation numeric factor instance replace equivalence establish choice knn classifier achieve minimax rate however minimax rate achieve limitation dependence deviation respect optimal improve performance CV estimate risk knn proof proposition define maximum assume constant introduce ˆfk rbp union bound furthermore combine inequality ˆfk ˆfk ˆfk ˆfk hence ineq equivalence namely borel  lemma yang combine theorem  dasgupta minimax rate achieve knn classifier ˆfk exist numeric constant moreover easy ˆfk ˆfk besides rbp ˆfk ˆfk ˆfk rbp ˆfk ˆfk celisse mary huard introduce sequence rbp ˆfk negligible respect ˆfk exists integer ˆfk hence rbp ˆfk ˆfk finally conclusion borel  lemma discussion quantify performance LpO estimator apply knn classifier exploit  LpO  polynomial exponential inequality derive insight concentration LpO estimator around expectation regime instance conclude consistency LpO estimator towards risk classification error rate knn classifier theorem establish asymptotic equivalence LpO estimator shift bayes risk excess risk regression function proposition worth mention upper bound derive instance theorem minimize LO estimator optimal risk estimation apply knn classification algorithm observation corroborates simulation celisse mary huard empirically estimation risk fix whatever data optimality LO risk estimation consistent  celisse LO asymptotically validation procedure perform risk estimation context dimensional regression density estimation respectively alternatively LpO estimator data dependent calibration procedure tune  minimizes LpO estimate instance performance CV estimate risk knn classification LpO prediction performance context splitting ratio knn classifier illustrate simulation summarize celisse mary huard becomes phenomenon limited knn classifier extends various estimation prediction identify predictor candidate selection performance shao linear regression model LpO shao theorem prof model selection consistency recover predictor candidate yang consistency CV relate optimal splitting ratio convergence rate predictor min although focus worth mention concentration establish significant towards derive theoretical guarantee LpO model selection procedure indeed exponential concentration inequality ingredient ass model selection consistency model selection efficiency various context instance celisse   density estimation framework theoretically investigate behavior  dedicate development towards derive tighter upper bound bias LpO estimator risk upper bound currently available derive devroye wagner lemma unfortunately fully capture behavior LpO estimator respect becomes improve emphasize comment theorem another important direction model selection behavior LpO procedure concentration inequality classification error rate knn classifier around expectation concentration establish knn algorithm fix regression framework derive classification context remains challenge knowledge acknowledgment author associate editor reviewer  comment greatly improve presentation partially fund    celisse mary huard appendix proof polynomial upper bound proof theorem proof relies proposition allows relate LpO estimator sum independent random variable distinguish setting calculation upper bound derive proof separately straightforward jensen inequality cautious derive upper bound sophisticated rosenthal inequality namely proposition exploit proposition accord proof proposition arises LpO estimator express statistic rbp nXm  AD AD denotes classifier sample LpO estimator rbp rbp rbp rbp jensen inequality nXm zim nXm zim performance CV estimate risk knn independence rbp rbp var nXm zim nXm var zim var straightforward jensen inequality rbp rbp nXm zim rosenthal inequality proposition introduce symmetric random variable  zim nXm zim nXm implies nXm zim max nXm nXm zim celisse mary huard nXm zim max var hence rbp rbp max var concludes proof proof theorem strategy proof consists proposition consists derive upper bound  lemma lemma upper bound kγd maximum finally technical distinguish tighter bound upper bound  sake readability notation theorem denote coordinate remove  upper bound  AD AD performance CV estimate risk knn furthermore introduce denote index respectively obtains  AD distinguish derive tighter bound lemma lemma  AD kγd sum apply  kγd kγd hence kγd kγd enables conclude obtain slightly upper bound notation  AD AD celisse mary huard lemma implies kγd allows conclude  kγd AD sum introduce independent denote derives kγd AD kγd inequality lemma proof theorem plug upper bound previously derive LO estimator namely ineq theorem inequality LpO estimator theorem proof ineq inequality straightforwardly combination theorem ineq theorem proof ineq upper bound theorem plug ineq derive rosenthal inequality optimize constant namely proposition rbp rbp max vuut max vuut max  performance CV estimate risk knn vuut finally introduce max celisse mary huard appendix proof exponential concentration inequality proof proposition proof relies successive ingredient McDiarmid inequality theorem lemma lemma upper bound rbp rbp rbp rbp ade ade denotes random variable index resp denotes index resp introduce lemma implies hence rbp rbp conclusion McDiarmid inequality proof theorem proof notation proposition goal proof refine version previous proposition account status strategy sub gaussian concentration inequality lemma LpO estimator rbp upper bound derive ineq generalize efron stein inequality amount difference rbp rbp performance CV estimate risk knn precisely evaluate contribution compute quantity denotes probability respect uniform random variable denotes index upper bound rbp rbp rbp rbp ade ade ade absolute jensen inequality rbp rbp ade ade notation integration respect random variable discrete uniform distribution distinct index ade denotes index notation proof proposition rbp rbp celisse mary huard sum quantity rbp rbp evaluate influence successively upper bound partition sum rank sample lemma performance CV estimate risk knn upper bound maxj upper bound apply sum partition sum rank sample apply lemma lemma kγd gathering upper bound previous bound enables conclude rbp rbp celisse mary huard generalize efron stein inequality rbp rbp  hence combine rbp rbp conclusion lemma rbp rbp rbp rbp exp  proof theorem proposition proof theorem exploit characterization sub gaussian random variable lemma apply introduce constant max rbp rbp lemma rbp rbp rbp rbp exp proof relies proposition exponential concentration inequality upper bound random variable combine minj rbp rbp exp min performance CV estimate risk knn arises proof proposition previous proof derivation deviation proposition notation previous proof combine proposition minj rbp rbp vuut celisse mary huard appendix proof deviation upper bound proof ineq theorem proof strategy theorem rogers wagner along proof repeatedly notation briefly introduce define independent reading proof shortcut fbe ade index cardinality finally along proof denote random variable distinct index discrete uniform distribution notation resp integration respect sample random variable resp random variable  correspond expectation sample random variable independent compute instance amount integrate respect random variable proof notation rbp rbp nln rbp nln fbe immediately rbp fbe fbe proof consists successively upper bound equality upper bound fbe fbe fbe fbe fbe fbe performance CV estimate risk knn introduce emphasize random variable discrete uniform distribution fbe fbe fbe fbe NPE fbe fbe fbe fbe furthermore NPE fbe fbe fbe fbe fbe fbe fbe upper bound upper bound simply upper bound obtain upper bound upper bound fbe fbe fbe depends index belongs training celisse mary huard lemma prof lemma combine previous bound lemma deduces fbe upper bound fbe fbe built sample fbe fbe fbe performance CV estimate risk knn lemma obtain inequality conclusion conclusion simply combine bond rbp combinatorial lemma lemma notation introduce lemma proof lemma along proof repeatedly exploit independence random variable distinct index discrete uniform distribution important ingredient probability choice index celisse mary huard lemma notation fbe fbe fbk fbe proof lemma remind sample cannot belong consequently exhaustive formulation fbe fbe fbe fbe built sample hence lemma implies fbe fbe fbe fbe fbe fbe  fbe  fbe fbe lemma notation fbe fbe fbe proof lemma previous lemma fbe built sample observation replace fbe fbe fbk fbe fbe fbe fbk fbk  fbk fbk fbe fbe fbk fbk fbe performance CV estimate risk knn proof proposition bias LO estimator ADn ADn ADn ADn ADn ADn denotes remark simplify expression easy ADn ADn label ADn implies label almost odd assumption respectively denote resp proof theorem  celisse mary huard dasgupta denotes hypergeometric random variable population cardinality recall assumption ADn calculation apply finally conclusion entail denotes numeric constant independent performance CV estimate risk knn appendix technical inequality exponential inequality proposition  lemma denote random variable assume exist   minj mini minj  furthermore minj  minj proof proposition markov inequality apply PN  upper bound PN  maxi  minj  maxi  minj   mini minj  proof PN minj combine minj arises PN PN minj PN minj max PN minj PN minj mink celisse mary huard PN mink minj hence minj  minj minj minj sub gaussian random variable lemma theorem random variable satisfies lemma theorem random variable satisfies efron stein inequality theorem efron stein inequality theorem independent random variable  function var moreover denote independent define performance CV estimate risk knn generalize efron stein inequality theorem theorem independent random variable denote measurable function define independent furthermore   exists constant corollary notation  vuut vuut moreover   McDiarmid inequality theorem independent random variable assume satisfies sup proof theorem rosenthal inequality proposition   denote independent random variable symmetric distribution  max denotes positive constant furthermore optimal celisse mary huard denotes standard gaussian variable random variable poisson distribution proposition denote independent random variable symmetric distribution max   proof proposition lemma  plug previous upper bound rosenthal inequality proposition max   lemma notation proposition proof lemma performance CV estimate risk knn lemma apply lemma technical lemma computation resampling apply knn algorithm lemma proof lemma equality straightforward calculation equality satisfies hence celisse mary huard lemma lemma corollary belongs kγd increase stability knn classifier remove observation lemma devroye wagner denote NN classification algorithm define denote random variable independent exponential concentration inequality LO estimator lemma theorem denote NN classification algorithm define denote LO estimator define exp upper bound LO estimator lemma denote NN classification algorithm define denote LO estimator define kγd proof straightforward combination lemma upper bound optimal constant rosenthal inequality lemma denote standard gaussian random variable proof lemma performance CV estimate risk knn positive integer  implies odd implies lemma denote binomial random variable proof lemma symmetric chernoff inequality odd calculation proof lemma apply  celisse mary huard odd another calculation proof lemma  implies hence lemma random variable poisson distribution proof lemma remark EN  furthermore conditional distribution binomial distribution lemma entail  EN performance CV estimate risk knn remains upper bound expectation poisson random variable EN EN jensen inequality introduce  polynomial classical upper bound EN   vuut finally concludes