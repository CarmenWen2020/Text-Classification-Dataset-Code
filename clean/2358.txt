robust pca widely statistical procedure recover underlie rank matrix grossly corrupt observation considers robust pca nonconvex optimization manifold rank matrix proposes algorithm manifold optimization properly initialization propose algorithm guaranteed converge underlie  matrix linearly previous factorization rank matrix propose algorithm reduce dependence underlie rank matrix theoretically simulation data confirm competitive performance keywords principal component analysis rank model manifold rank matrix introduction underlie data matrix assume approximately rank computer vision machine bioinformatics principal component analysis pca standard statistical procedure recover underlie rank matrix however pca highly sensitive outlier data robust pca cand clarkson woodruff chen  hence propose modification handle grossly corrupt observation mathematically robust pca formulate data matrix sum rank matrix signal sparse matrix corruption nonzero entry recover component accurately robust pca application application zhang yang background detection recognition  jacob rank collaborative filter cand rank matrix nonconvex generally obtain algorithm theoretical guarantee tractable optimization algorithm nonconvex review carefully algorithm theoretical guarantee recovery underlie rank matrix exists cand convex relaxation instead min    nuclear norm schatten norm define sum singular  sum absolute entry convex polynomial addition recovers rank matrix corrupt non zero entry rank incoherence sparsity assume random cand algorithm succeed probability percentage corruption rank min max coherence parameter  matrix defines slightly differently cand comparable however aforementioned algorithm convex relaxation computational complexity min per iteration prohibitive alternatively faster algorithm propose nonconvex optimization   proposes project gradient however assumes sparsity random algorithm computational complexity convex proposes alternate project allows computational complexity per iteration chen  assumes positive semidefinite applies gradient descent cholesky decomposition factor positive semidefinite assumption satisfied application factorizes matrix performs alternate minimization matrix algorithm allows min complexity per iteration applies factorization applies alternate gradient descent algorithm complexity rnn per iteration allows  underlie rank matrix another reduces complexity algorithm subsampling entry observation matrix    partially rank matrix rank rank robust pca manifold optimization matrix  optimize instead computational parameter parameter parametrization technique  popularize     semi definite program SDPs rank matrix estimation phase synchronization  community detection matrix completion recover matrix linear measurement chen  addition associate stochastic gradient descent algorithm contribution novel robust pca algorithm gradient descent algorithm manifold rank matrix theoretical guarantee recovery underlie rank matrix propose algorithm utilizes manifold optimization simpler naturally structure algorithm theoretical guarantee initialization succeed  tolerate corruption factor addition theoretical convergence rate faster factor simulation verify advantage propose algorithm remark manifold optimization apply robust pca   algorithm theoretical guarantee popularity factorization rank matrix manifold optimization apply rank matrix estimation addition implement efficient user friendly package  available http github com   organize algorithm explain propose algorithm derive theoretical previous algorithm simulation data analysis  dataset propose algorithm competitive scenario superior performance algorithm matrix factorization discussion propose algorithm proof appendix algorithm robust pca setting fully partially fully formulate rank matrix sparse matrix recover recover optimization arg min rank zhang yang thresholding procedure define fij aij aij aij otherwise matrix percentile absolute entry remove entry simultaneously correspond absolute threshold user entry entry identical absolute broken arbitrarily motivation sparse percentage nonzero entry zero definition zero nonnegative propose algorithm manifold optimization derivation defer algorithm gradient descent manifold fully input observation rank thresholding initialization initialize rank approximation loop iterate convergence option matrix consist singular vector singular singular vector option QR decomposition QR decomposition RT output estimation rank matrix limk partially addition gross corruption matrix entry denote entry yij define aij aij aij otherwise percentile absolute entry matrix respectively generalization algorithm propose arg min rank robust pca manifold optimization algorithm gradient descent manifold partially input observation entry rank thresholding initialization initialize rank approximation loop iterate convergence sparse matrix nonzero entry correspond entry option matrix consists singular vector singular singular vector option QR decomposition QR decomposition RT output estimation rank matrix limk entry implementation algorithm derivation defer algorithm memory usage due storage algorithm memory usage algorithm algorithm option singular decomposition computationally intensive complexity per iteration rnn algorithm algorithm option computational complexity per iteration rnn respectively derivation propose algorithm derivation algorithm derive manifold optimization review manifold optimization geometry manifold rank matrix manifold optimization purpose review framework gradient descent manifold summarizes mostly framework  refer reader detail smooth manifold differentiable function procedure gradient descent algorithm minx differentiable function calculate euclidean gradient zhang yang visualization gradient descent algorithm manifold solid euclidean gradient solid projection euclidean gradient tangent solid orthographic retraction dash projective retraction calculate riemannian gradient direction steepest ascent direction tangent txm direction   projection operator tangent txm define retraction tangent manifold txm satisfy  definition kyk smooth update gradient descent algorithm define  remark differential geometry standard retraction exponential tangent manifold however manifold optimization generic mapping tangent manifold definition retraction unique visualize gradient descent manifold retraction orthographic projective discus detail retraction robust pca manifold optimization geometry manifold rank matrix apply gradient descent algorithm manifold rank matrix projection  retraction define manifold matrix rank matrix rank explicit expression  tangent txm retraction RX manifold rank matrix   assume svd decomposition UÎ£VT tangent txm define txm  uut accord   explicit formula projection     uut  uut  completeness proof appendix various define retraction manifold rank matrix refer reader   detail retraction projective retraction  txm retraction define rank matrix frobenius norm arg min  rank approximation matrix rank approximation  singular vector improve computation efficiency orthographic retraction   denote rank matrix difference orthogonal tangent txm arg min   txm explicit   UT UT proof appendix derivation propose algorithm derivation algorithm gradient descent algorithm RL  PT define RL define derive explicit algorithm remains gradient zhang yang absolute entry proof defer appendix entry equivalent generate objective function non differentiable however arbitrarily subgradient correspond gradient descent subgradient differentiable projective retraction rank approximation  rank approximation define algorithm option orthographic retraction RL define accord update formula simplify matrix matrix derivation defer appendix implementation algorithm option derivation algorithm argument previous conclude entry apply procedure derive subgradient differentiable observation algorithm partially identical replace implementation algorithm convergence algorithm topic algorithm reasonable convergence critical unfortunately impossible theoretical guarantee fix chosen subgradient fix convergence guarantee objective function non differentiable however accumulation objective function differentiable critical riemannian gradient zero strategy algorithm described relatively repeatedly shrink factor satisfied RL   robust pca manifold optimization prespecified argument convergence argument proof prior manifold optimization optimization manifold investigate literature  summary advance optimization manifold manifold optimization apply matrix estimation recover rank matrix partial entry matrix completion  robust matrix completion   reformulate analyze   comparison algorithm additional theoretical guarantee another aspect matrix completion similarity manifold optimization algorithm theoretical guarantee propose algorithm recover underlie rank matrix exactly partially without corruption proposes arg min rank yij lij theoretical analysis analyze theoretical algorithm previous algorithm goal recover rank matrix sparse matrix avoid identifiability issue assume rank sparse specifically standard assumption assumption contains nonzero entry contains nonzero entry assume kai assumption rank matrix sparse achieve coherent singular decomposition svd assume exists incoherence parameter   norm define kak   kxk maxi zhang yang analysis algorithm assumption theoretical regard convergence rate initialization stability algorithm theorem linear convergence rate fully suppose  singular exists remark choice parameter proof equation LHS increase function zero RHS positive suggests constant theorem due technicality proof potentially improve remark simplify choice parameter exists initialization algorithm algorithm handle addition iteration achieve statement initialization arises initialization rank approximation initialization initialization upper bound accord proof borrow estimation along  theorem initialization fully initialize rank approximation  combination theorem implies fully tolerance propose algorithm corruption stability algorithm statement robust pca manifold optimization theorem stability fully update apply algorithm iteration assume random gaussian sample exist rank   theorem observation contaminate random gaussian properly initialize  algorithm converge neighborhood   iteration probability analysis algorithm partially assume entry probability statement convergence theorem linear convergence rate partially exists max max  min min probability  remark choice parameter RHS RHS assume exists RHS practical choice zhang yang remark simplify choice parameter exists  theorem addition parameter initialization requirement convergence rate weaker suspect dependence subsampling ratio improve estimation estimation lemma future direction obtain initialization theorem combine theorem algorithm allows corruption theorem initialization partially exists min initialize rank approximation  probability singular comparison alternate gradient descent objective function equivalent objective function alternate gradient descent AGD difference algorithmic implementation gradient descent manifold rank matrix alternate gradient descent factor rank matrix aspect accuracy initialization algorithm tolerate initialization satisfy algorithm guaranteed converge convergence rate iteration algorithm convergence criterion corruption perfect initialization suppose initialization sufficiently neighborhood exists satisfies maximum corruption tolerate convergence analysis corruption initialization suppose initialization procedure theorem partially fully maximum corruption tolerate robust pca manifold optimization comparison summarize remove reduce dependence unchanged partially advantage dependence sometimes additional dependence subsampling ratio simulation verify algorithm performance partially significant theorem suspect dependence remove careful analysis assumption criterion accuracy convergence max corruption max corruption initialization rate perfect init init algorithm APG  max  algorithm  APG partial   max  comparison theoretical guarantee alternate gradient descent algorithm criterion explain detail intuition propose gradient descent factorization optimization arg min factor gradient descent factor  update formula becomes aat BBT gradient descent factor direction aat BBT comparison gradient descent variable direction direction decrease aat BBT matrix gradient descent factor generally apply gradient descent factor variable apply variable applies gradient descent applies gradient descent factor zhang yang comparison robust pca algorithm robust pca summarize criterion apply analyzes algorithm specific initialization criterion apply maximum corruption ratio handle computational complexity per iteration convergence rate depends assumption parameter coherence parameter rank matrix alternate projection  iteration achieve accuracy assumption tune parameter chosen alternate minimization guarantee parameter concern coherence nonzero entry min approximately max notation  max restrictive assumption theorem convex usually convergence rate guarantee convexity accelerate proximal gradient   convergence rate convergence rate theorem necessarily converge assumption rank matrix corruption ratio criterion maximum complexity corruption per iteration algorithm  rnn convex min min  rnn comparison theoretical guarantee robust pca algorithm stability theorem comparable analysis stability analysis assumes prof output algorithm satisfies  robust pca manifold optimization error algorithm sample suggests  bound  comparison theorem suggests iteration bound  probability tighter upper bound simulation performance propose algorithm simulated data data matlab implementation algorithm available http ucf edu math  simulated data generate UÎ£VT random matrix sample diagonal matrix entry sample probability zero probability sparsity sparse matrix overall corruption associate corruption wise corruption partially assume entry probability choice parameter investigate performance propose algorithm dependence parameter simulation partially simulation investigates algorithm option option appropriate choice simulation option various visualize option perform similarly usually algorithm converge faster however diverge algorithm algorithm simulation simulation concern choice choice calculate zero algorithm fail converge convergence observation default choice performance propose algorithm analyze convergence behavior parameter overall ratio corrupt entry rank subsampling ratio visualize simulation corruption replace corruption algorithm zhang yang iteration error iteration error iteration error iteration error dependence estimation error iteration algorithm option algorithm option algorithm option algorithm option iteration error iteration error convergence algorithm choice fully partially robust pca manifold optimization converges corruption information available however algorithm converges overall corruption simulation rank replace respectively simulation algorithm rank converges rank simulation replace diag various algorithm converges simulation algorithm converges slowly decrease quickly zero suspect initialization sufficiently algorithm local neighborhood convergence remark generally challenge nonconvex optimization algorithm matrix rank singular manifold matrix rank geometry manifold smooth algorithm performs rank instead underlie matrix approximately rank singular algorithm various matrix algorithm converges quickly choice within iteration simulation algorithm various choice subsampling ratio algorithm converges convergence rate comparison robust pca algorithm algorithm accelerate proximal gradient APG alternate direction multiplier ADMM convex relaxation robust matrix completion algorithm RMC   manifold optimization arg min rank   alternate gradient descent AGD solves optimization implementation matrix factorization manifold optimization implementation APG   implementation ADMM http github com   algorithm choice parameter max default choice implementation   theoretical analysis cand ADMM augment lagrangian parameter default RMC GD default parameter algorithm apply implementation APG ADMM fully setting zhang yang iteration error iteration error rank rank rank rank rank iteration error iteration error iteration error dependence estimation error iteration overall ratio corrupt entry algorithm rank algorithm algorithm matrix algorithm subsampling ratio algorithm robust pca manifold optimization error RMC APG ADMM algorithm AGD error RMC APG ADMM algorithm AGD error RMC APG ADMM algorithm AGD error RMC APG ADMM algorithm AGD comparison performance algorithm fully replace diag matrix replace replace diag algorithm converges faster algorithm advantage AGD algorithm verifies theoretical analysis convergence rate faster analysis factor algorithm RMC AGD algorithm converge faster APG ADMM verifies computational advantage nonconvex algorithm matrix however convex algorithm converge nonconvex algorithm converge local minimizer due nonconvex algorithm minimizer initialize trap local minimizers zhang yang error comparison robust pca algorithm partially RMC algorithm AGD comparison performance algorithm partially initialization chosen algorithm converges quickly performance RMC AGD algorithm partially visualize fully AGD algorithm comparable RMC converges faster achieve accuracy possibly due choice regularization parameter propose algorithm data application video background subtraction adopt public data  frame visualize frame video sequence matrix corresponds frame video corresponds pixel video apply algorithm partially algorithm iteration algorithm obtain desirable rank approximation within iteration algorithm APG convergence objective function relative error define  objective relative error implies rank approximation algorithm obtain objective within iteration fully partially data originally http perception edu model index html available http ucf edu math  robust pca manifold optimization performance algorithm video background subtraction frame video sequence algorithm subsampling ratio iteration relative error algorithm fully AGD fully algorithm partially AGD partially relative error algorithm AGD respect iteration fully partially background subtraction zhang yang conclusion proposes robust pca algorithm fully partially gradient descent algorithm manifold  matrix theoretically gradient descent algorithm matrix factorization approach faster convergence rate tolerance initialization accuracy corruption approach remove reduces dependence algorithm underlie rank matrix numerically propose algorithm performance sensitive choice partially performance propose algorithm significantly affected presence additional dependence observation probability popularity matrix factorization future direction apply manifold optimization rank matrix estimation acknowledgement author editor associate editor referee helpful comment suggestion author david  cod yang research partially   zhang research partially national foundation NSF grant CNS robust pca manifold optimization appendix robust pca manifold optimization technical derivation verification formula verify frobenius inner matrix   uut   uut     similarly  uut    uut  verifies formula  orthogonal txm verification define rank  txm txm simplicity txm  uut  uut  easy verify  VT   similarly easily verify UT UT uut  therefore txm exists unique rank txm verification define operator elementwise aij aij otherwise absolute entry sparsity perturbation definition hadamard elementwise matrix verification sufficient svd decomposition denote     zhang yang projection PT DV DV PT DV DV DV DV DV DV similarly tpt combine update formula verify proof theorem proof investigate RL Î·PTL sufficient  satisfy theorem introduce auxiliary lemma lemma  noisy max max lemma  PTL PTL lemma  kxk kxk robust pca manifold optimization Î·PTL  PTL   PTL   PTL PTL kPTL  PTL  kPTL kPTL  kPTL  fourth obtain PTL PTL fifth PTL TL  inequality    kPTL triangular inequality kPTL kPTL  lemma assumption  imply kPTL combine estimation  lemma Î·PTL RHS positive implies Î·PTL Î·PTL Î·PTL addition kPTL  lemma Î·PTL Î·PTL kPTL  combine therefore theorem chosen zhang yang proof theorem proof noisy similarly proof theorem define PTL proof theorem apply lemma Î·PTL  PTL  PTL  PTL    addition  Î·PTL combine estimation  lemma  lemma theorem lemma  sample probability max max exists probability PTL kPTL  converges proof theorem proof borrows lemma lemma exists  min probability tangent TL BL kxk  kxk lemma min probability entry per interval entry per introduce lemma parallel lemma robust pca manifold optimization lemma lemma PÎ¦  proof theorem parallel proof theorem define slightly differently RL Î·PTL define PÎ¦  xij  analysis Î·PTL   PTL         TL PÎ¦  TL projector subspace orthogonal TL lemma lemma imply  TL  TL  combine estimation lemma RHS  addition lemma implies     combine lemma Î·PTL  zhang yang combine lemma theorem proof lemma lemma proof definition matrix sparse matrix therefore sparse matrix denote location nonzero entry define dij dij addition definition percentage dij assumption therefore percentage combine dij max max apply estimation repeatedly robust pca manifold optimization  max max PTL PTL PTL PTL PTL PTL PTL kxk statistic therefore numerator due percentage lemma implies PTL zhang yang addition exists PTL   kPTL   percentage PTL  ijk  ijk  ijk  ijk   similarly kak   PTL  combine lemma proof apply max max max max inequality proof definition lemma proof svd decomposition  orthogonal matrix col col col col col subspace span   TL TL robust pca manifold optimization rank singular singular decomposition UÎ£VT  apply  kak kbk diagonal matrix    xij UÎ£VT     proof lemma proof svd decomposition  UT UT UT XV XV argument kxk kxk kxk zhang yang lemma definition closest matrix rank lemma lemma proof wlog assume generic similarly estimation distribution maximum gaussian variable max exp exp inequality applies estimation cumulative distribution function gaussian distribution   combine estimation apply union bound inequality probability similarly inequality probability parameterize PL  exists kPTL PTL  apply obtain PTL     davis  theorem davis  assumption  exists       exists kPTL PTL PTL PTL PTL PTL combine apply net argument proof combine probabilistic estimation union bound net argument standard argument probabilistic estimation  estimation robust pca manifold optimization cumulative distribution function gaussian distribution   PTL  exp apply PTL  exp union bound net therefore kPTL PTL  exp CkN probability infinity  kPTL inequality applies assumption kPTL combine recall CkN kPTL CkN PTL kPTL kPTL CkN exp  zhang yang  CkN exp  exp exp CkN exp exp CkN exp exp CkN exp CkN CkN exp inequality exp clearly RHS combine estimation probability CkN exp max  combine  max   dominant lemma lemma proof proof lemma lemma replace arbitrary interval   apply lemma    TL combine estimation  lemma   robust pca manifold optimization