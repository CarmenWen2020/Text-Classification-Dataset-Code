data increase multi core computer architecture developed asynchronous parallel optimization algorithm become essential machine unfortunately conduct theoretical analysis asynchronous notably due introduction delay inconsistency inherently sequential algorithm handle issue resort simplify unrealistic assumption novel perspective revisit clarify subtle important technical issue recent convergence rate proof asynchronous parallel optimization algorithm propose simplification recently introduce perturbed iterate framework resolve demonstrate usefulness framework analyze distinct asynchronous parallel incremental optimization algorithm hogwild asynchronous sgd kromagnon asynchronous svrg asaga novel asynchronous parallel version incremental gradient algorithm saga enjoys linear convergence rate remove problematic assumption obtain theoretical notably asaga kromagnon obtain theoretical linear speedup multi core without sparsity assumption implementation core architecture illustrate practical speedup hardware overhead finally investigate overlap constant ill understood central quantity theoretical analysis asynchronous parallel algorithm encompasses complexity previous magnitude traditionally keywords optimization machine asynchronous parallel sparsity leblond pedregosa lacoste julien introduction unconstrained optimization minimize finite sum smooth convex function min assume convex lipschitz continuous gradient strongly convex data regularize empirical risk minimization define standard finite sum literature flurry randomize incremental algorithm iteration random gradient recently propose linear convergence rate sag sdca svrg saga algorithm interpret variance reduce version stochastic gradient descent sgd algorithm demonstrate theoretical practical improvement sgd finite sum optimization advantage multi core architecture computer aforementioned optimization algorithm adapt asynchronous parallel multiple thread concurrently devote recently propose analyze asynchronous parallel variant algorithm sgd sdca svrg incremental gradient algorithm linear convergence rate optimize svrg asynchronous parallel version propose adaptation attempt saga argue candidate contrarily svrg epoch synchronization barrier extend journal version conference framework asynchronous analysis intricate simplify assumption complex algorithm saga therefore investigation newly propose perturbed iterate framework introduce improve upon properly analyze saga derive framework analysis saga enables highlight deficiency previous framework define alternative approach limited saga investigate algorithm improve exist bound contribution propose simplification perturbed iterate framework basis asynchronous convergence analysis assume individual smooth smooth smoothness constant classical rate complexity gradient evaluation accuracy contrast  batch gradient descent sdca knowledge explicit strongly convex regularizer whereas sag saga adaptive local convexity variant svrg adaptive review variant improve parallel optimization analysis stochastic incremental novel perspective revisit clarify technical literature randomize asynchronous parallel algorithm namely assume unbiased gradient estimate assumption inconsistent proof technique without unpractical synchronization assumption novel sparse variant saga adapt parallel saga algorithm asaga lock asynchronous parallel version sparse saga consistent operation tailor convergence analysis asaga asaga obtains geometric convergence rate per update saga overlap bound core satisfies max sparsity notably implies linear speedup theoretically without sparsity regime contrast previous analysis sparsity assumption revisit asynchronous variant svrg kromagnon remove gradient bound assumption inconsistent strongly convex algorithm enjoys rate convergence svrg asaga whereas analysis rate sequential asynchronous meaningful speedup improve perturbed iterate framework revisit analysis optimization routine proof assume homogeneous computation analysis hogwild algorithm introduce framework allows remove classic gradient bound assumption speedup realistic setting practical implementation asaga illustrate performance core architecture improvement asynchronous variant svrg sgd overlap bound encompasses complexity previous related seminal textbook   foundational parallel distribute optimization algorithm asynchronous variant sgd constant hogwild framework analysis inspire recent literature asynchronous parallel optimization algorithm convergence rate asynchronous variant coordinate descent sdca sgd non convex sgd stochastic optimization svrg although author mention gradient bound assumption enforce thresholding operator explain handle interplay non linear operator asynchrony algorithm theoretical analysis relies linearity operation derive currently theory strongly convex function unbounded domain unbounded gradient leblond pedregosa lacoste julien unbiased gradient assumption consistent proof technique suffers technical highlight perturbed iterate framework knowledge suffer convergence analysis heavily approach simplify author assume strongly convex bound gradient inconsistent assumption unconstrained analyze overcome difficulty tighter inequality remove requirement bound gradient propose convenient label iterates sparse version saga propose inspire sparse version svrg propose hybrid algorithm  saga svrg asynchronous analysis epoch handle fully asynchronous version saga moreover consistent propose efficient sparse implementation saga contrast asaga proposes mini batch algorithm parallelize sgd maintain serial equivalence smart update partition data sparse obtain speedup hogwild implementation svrg saga however hogwild implementation suboptimal leverage data sparsity efficiently adapt lazy update trick asynchronous parallel appendix extremely approximation severely penalize performance algorithm sequential version extend asaga algorithm proximal notation denote expectation respect randomness conditional expectation random index factor chosen sgd algorithm context coordinate vector sequential algorithm update parameter vector algorithm iteration revisit perturbed iterate framework asynchronous analysis recent parallel optimization contribution hardware model multiple core access memory core update central parameter vector asynchronous lock fashion unlike assume vector consistent multiple core coordinate vector implies vector core correspond consistent memory specific easily fix incrementing global counter sample hogwild author asynchronous parallel variant core independently sequential update improve parallel optimization analysis stochastic incremental perturbed iterate framework review perturbed iterate framework recently introduce basis analysis sequential stochastic gradient descent variant characterize update random variable independent unbiasedness recall relevant conditional expectation respect unfortunately parallel manipulate stale inconsistent parameter straightforward relationship instead propose distinguish xˆt actual core compute update virtual iterate analyze define update equation ˆxt interpret xˆt noisy perturbed version due asynchrony formalize precise meaning xˆt reference mention related analyze asynchronous parallel randomize algorithm assume unbiasedness unbiasedness ˆxt xˆt ˆxt convergence proof randomize optimization correctly literature implicit assumption independent xˆt explain assumption incompatible non uniform asynchronous model analysis approach recent literature difficulty label iterates formalize meaning xˆt highlight subtle important difficulty arise analyze randomize parallel algorithm meaning label iterates purpose analysis label randomness consideration interpret meaning expression contrast approach unified framework notably clarify dependency issue label resolve propose simpler label allows simpler proof technique completely formal define conditional expectation precisely define another random vector describes entire randomness writes delay etc conditioning xˆt actually shorthand conditioning relevant defines xˆt label clarity exposition technical detail define appropriate sigma equation fully rigorous notable exception sag bias update significantly complex convergence proof sag unbiased saga simpler convergence proof leblond pedregosa lacoste julien algorithm execute parallel global label define information memory xˆt sample perform computation xˆt update memory approach approach standard global label scheme later mention related notable exception approach virtual global counter successful writes memory incremented content memory update interpretation crucial equation xˆt delayed local core successful update factor sample core update framework xˆt unknown later core writes memory finally local variable label xˆt necessarily independent dependence assignment label yield faster update others influence label assignment define xˆt concrete dependency resolve issue ensure unbiasedness assume computation algorithm core independent sample chosen assumption overly context potentially heterogeneous factor fundamental flaw analyze non uniform asynchronous computation mostly ignore recent asynchronous optimization literature contrary asynchronous algorithm parameter vector sample data choice emphasize xˆt independent crucial analysis reading parameter independent sample data although prefer parameter sample relevant data sake analysis cannot source dependence analysis handle reading parameter sample independence ensure clarity presentation independence explicit presentation choice analysis explicitly assume xˆt independent although explain practical implementation author propose scheme handle dependency directly appendix however fix apply restrict setup hogwild algorithm assumption norm gradient uniformly bound furthermore restrict setup scheme worsen theoretical bound apply complex algorithm kromagnon asaga overcome significant hurdle absence option enforce independence xˆt modify   briefly issue stress analysis sgd schedule computation independent randomness sgd assumption satisfied label propose label resolve issue improve parallel optimization analysis stochastic incremental core core core core core core core core suppose core factor variable variable yield gradient significantly expensive compute initial content memory local core core update memory without incrementing counter possibility define index sample core core sample faster update happens possibility analysis scheme satisfy crucial unbiasedness understand subtle independent dependency label assignment approach address issue propose instead increment global counter core memory framework xˆt inconsistent core computational chosen sample update definition meaning virtual iterate necessarily correspond content memory quantity manipulate algorithm approach xˆt whereas analysis consequently critical quantity vanish ekxˆt independence xˆt simply enforce approach memory reading coordinate fix implies coordinate regardless weaker assumption computation approach reasonable global approach approach complication analysis xˆt global assignment computation core xˆt future analyze simplify analysis propose label iterates xˆt fully incremented approach ensure independent xˆt ensure unlike approach global xˆt iterates independent xˆt definition virtual iterate perturbed iterate framework leblond pedregosa lacoste julien xˆt assume atomic writes ˆxu ˆxt coordinate ˆxu crucial asaga proof obtain bound hogwild kromagnon algorithm perturbed iterate framework handle xˆt analyze convergence virtual quantity suppose exists index memory pre iteration computation xˆt convergence instead convergence directly xˆt avoid unwieldy pre iteration counter enable guarantee entire algorithm remark mention footnote sample data parameter vector advantage option allows reading relevant dimension parameter vector although lose crucial independence xˆt label approach sample equivalent switch sample reading setup sample approach becomes equivalent propose label however framework analysis approach described account troublesome future additional analysis considerably harder ultimately theoretical asynchronous parallel sparse saga sparse saga sparse variant saga algorithm adapt asynchronous parallel introduce asaga asynchronous parallel version sparse saga finally convergence speedup asaga outline proof sparse saga borrowing notation saga algorithm novel sparse variant saga algorithm standard saga algorithm maintains quantity optimize iterate memory historical improve parallel optimization analysis stochastic incremental gradient iteration saga algorithm sample uniformly random index executes update unconstrained optimization version update efficiently online fashion crucially  update direction unbiased furthermore proven reasonable update vanish variance enables algorithm converge linearly constant motivation variant saga update dense individual gradient sparse due historical gradient introduce implementation technique denote lag update iteration proportional however technique involves update easily adaptable parallel appendix therefore introduce sparse saga novel variant explicitly sparsity account easily parallelizable sparse saga algorithm sparse svrg algorithm propose obtain sparse saga modification parameter update replace sparse version equivalent expectation diα diagonal matrix projection precisely gradient function coordinate nonzero diagonal reweighting matrix coefficient diagonal probability dimension belongs sample uniformly random define  psi projection onto reweighting ensures  update unbiased despite sparsifying projection convergence serial sparse saga clarity exposition model convergence corollary rate obtain sparse saga obtain aforementioned reference saga theorem sparse saga converges geometrically expectation rate factor min obtain update  linear predictor model memory scalar initialize convenient typically unlike prescribed analyze leblond pedregosa lacoste julien proof outline reuse proof technique combination classical convexity lipschitz inequality derive inequality   contraction lyapunov function define ensure variant converges rate regular saga inequality verify derive variant equation proof reuse without modification detail appendix comparison lag update lag update technique saga observation update component apply coefficient access iteration refer reader detail interestingly iteration dimension partial gradient probability partial gradient precisely update sparse saga therefore update sparse saga anticipate update whereas implementation lag algorithm sparse saga expectation update whereas lazy implementation random variable expectation sparse saga therefore slightly aggressive strategy explain sparse saga saga lag update convergence iteration sparse saga scheme slightly faster runtime although sparse saga computation probability pas throughout data constant sgd negligible asynchronous parallel sparse saga asaga sparse asynchronous parallel implementation sparse saga algorithm theoretical analyze algorithm practical implementation convergence analyze algorithm improve perturbed iterate framework specific sparse saga additional memory argument perturbed update ˆxt ˆxt ˆxt dit convergence highlight algorithm central assumption improve parallel optimization analysis stochastic incremental algorithm asaga analyze algorithm initialize variable parallel inconsistent  inconsistent sample uniformly   atomic denotes memory update parallel loop algorithm asaga implementation initialize parallel sample uniformly inconsistent  inconsistent inconsistent  atomic atomic atomic parallel loop independence global independent xˆt independence assume parallel optimization literature verify label emulate enforce independence algorithm core data parameter historical gradient iteration although expensive practical data sparse theoretical algorithm analyze independence consequence global instead unbiased estimator update xˆt unbiased estimator gradient xˆt yield conditional expectation crucial analysis related literature independence xˆt computation algorithm ensures    update unbiased recomputing optimal instead introduces potential bias issue proof detailed appendix atomicity parameter coordinate update atomic update addition overwrites core compete resource enforce swap semantics heavily optimize processor minimal overhead non thread algorithm verify appendix swap optimize accuracy leblond pedregosa lacoste julien finally standard literature assumption maximum delay asynchrony partially asynchronous define   assumption bound overlap assume exists uniform bound maximum iteration overlap iteration overlap concurrently iteration reading parameter update bound iteration cannot overlap iteration coordinate update iteration successfully memory iteration linear speedup usually proxy core  however linearly actually depends factor notably data sparsity distribution magnitude upper bound ratio maximum minimum iteration encompasses theoretical aspect hardware overhead detail explicit asynchrony overlap assumption expression iterates obtain explicit asynchrony crucially proof xˆt ˆxu diagonal matrix definition update xˆt already conversely update xˆt lack update whereas global definition cannot update future convergence speedup theoretical detailed outline proof detail appendix define notion sparsity definition sparsity introduce maxv maximum bipartite graph factor dimension maximum data specific feature succinctness define hence improve parallel optimization analysis stochastic incremental convergence speedup statement theorem convergence guarantee rate asaga suppose min unless inconsistent iterates algorithm converge expectation geometric rate min xˆt constant independent define theorem saga convergence theorem maximum extra factor refer theorem rate factor saga min constant factor rate theorem infer maximum overlap asaga rate saga upper bound corollary speedup suppose max asaga converges geometrically rate factor min saga linearly faster sequential counterpart constant factor moreover universal asaga adaptive local convexity rate saga knowledge interestingly regime saga enjoys contraction ratio asaga enjoys rate saga non sparse regime contrast previous asynchronous incremental gradient sparsity theoretical linear speedup sequential counterpart ill regime sparsity linear speedup bound degenerate scenario proof corollary appendix comparison related convergence analysis asynchronous parallel version saga epoch version saga random fairly algorithm theorem directly extend parallel extension svrg version adaptive local convexity rate contrast parallel svrg analysis thm proof technique handle inconsistent non uniform processing across asaga actually converge maximum exp denominator constant appendix leblond pedregosa lacoste julien bound equivalent adaptivity local convexity asaga optimal performance contrary parallel svrg detail contrast svrg analysis thm obtain dependence rate sparsity obtain furthermore remove gradient bound assumption convergence guarantee xˆt algorithm whereas bound error iterate proof outline theorem extend outline proof detail lemma initial recursive inequality xˆt expand update equation define virtual iterate introduce xˆt inner obtain kxt kxt  kxt   gti kxt   gti  gti introduce xˆt inner function xˆt sequential independent obtain unbiasedness perturbed iterate framework instead independent xˆt crucial enables unbiasedness  gti  xˆt expectation unbiasedness obtain expression allows convexity  ˆxt ˆxt kxˆt manipulation expectation standard inequality kak kbk appendix obtain recursive contraction inequality ekgtk  xtk  gti additional asynchrony γet  ˆxt inequality midway derive proof lemma equation tighter convexity bound latter important extra γet sequential crucially negative suboptimality γet cancel variance ekgtk derive departure replace xˆt bound kxˆt relationship equation yield inequality loose afterwards rate svrg improve parallel optimization analysis stochastic incremental bound additional asynchrony negative suboptimality convergence speedup parallel algorithm constraint maximum proof proceeds expansion xˆt bound additional asynchrony update ekguk lemma bound update ekgtk  standard saga inequality carefully analyze update expectation lemma apply lemma lemma obtain contraction inequality define novel lyapunov function  manipulate inequality bound contraction maximum lemma finally unroll lyapunov inequality convergence theorem detail lemma proof sketch pointer relevant appendix detailed proof lemma inequality ˆxt  ekguk γet  lemma bound ekxˆt  gti respect ekguk achieve crucially equation proposition derive combination cauchy schwartz sparsity definition proposition  gti ekguk ekgtk derive essential inequality relevant reuse sparsity constant introduce relate define earlier depends instead attention dependency leblond pedregosa lacoste julien remark constant  kxk  define norm restrict proof  kxk implies kxk minimum constant satisfy inequality max kxk maximizes vector kxk probability simplex equivalent maximization convex combination maximum maximum definition implies indeed constant satisfy proof proposition without loss generality  gti  sparse inner ekguk ekgtk cauchy schwarz expectation ekguk ekgtk remark ekguk ekgtk GM inequality  gti ekguk ekgtk switch improve parallel optimization analysis stochastic incremental lemma suboptimality bound ekgtk ekgtk ekα proof convergence sparse saga appendix ekgtk ekf ˆxt  handle expectation lipschitz inequality equation remains lemma express   expectation adequate probability distribution derive bound algorithm dimension memory vector  contains partial gradient compute  unless replace partial gradient derive bound sum clever conditioning obtain lemma inequality define lemma γet LγC LγC LγC LγC lyapunov function associate recursive inequality contraction additional positive converge optimum classical negative suboptimality unusual variance reduction literature successful approach sequential define lyapunov function encompasses contraction emulate however sequential iterate parallel inequality resolve issue define involve lyapunov function encompasses iterates target contraction rate define later introduce quantity instead handle arbitrary initialization precisely  leblond pedregosa lacoste julien inequality appendix aim bound contraction promising inequality handle basically rearrange sum expose sum factor specific negative couple positive safely sum inequality trickier handle separately obtain bound directly ekxˆt introduce additional γet bound modify negative twice multiplier chose simplicity sake lemma lemma sufficient convergence suppose min coefficient negative furthermore γet obtain carefully derive polynomial inequality simplify appendix argument bound suboptimality error γet linearly contract sum contract linearly rate geometric rate factor define min γet γet max geometric rate factor appendix introduce circumvent problematic improve parallel optimization analysis stochastic incremental asynchronous parallel svrg label asaga asynchronous svrg scenario asaga practically advantageous closely related cousin asynchronous svrg asynchronous svrg synchronization per epoch compute gradient saga memory computation generalize linear model memory reduce svrg asynchronous counterpart asaga synchronization heterogeneous compute environment core application finally asaga optimal convergence sparse regime adaptive local convexity whereas svrg indeed svrg asynchronous variant additional hyper parameter epoch convergence yield effective convergence rate asaga svrg tune additional hyper parameter risk convergence epoch chosen converge chosen motivation analyze asynchronous svrg despite advantage complex model storage saga become expensive practical svrg computation storage suffer drawback apply saga cannot another advantage kromagnon historical gradient fix epoch asaga equivalent update iteration recomputing costly update maintain quantity cheaper ultimately introduce bias update appendix detail subtle issue worthwhile analysis kromagnon asynchronous parallel version svrg although svrg regularly compute batch gradient kromagnon regular synchronization coordinate computation attractive asynchronous parallel extend asaga analysis analyze convergence variant svrg obtain exactly bound variant improves upon initial algorithm tune epoch hyperparameter adaptive local convexity furthermore allows saga contrary svrg svrg variant knowledge adaptive local convexity asynchronous adaptation analyze speedup analysis fully satisfactory achieve convergence svrg kromagnon furthermore remove uniform gradient bound assumption inconsistent convexity leblond pedregosa lacoste julien cleaner analysis contrary svrg replace parameter epoch random iterates label derive convergence speedup proof kromagnon comparable asaga analysis asaga regime kromagnon achieve linear speedup without sparsity assumption svrg algorithm svrg algorithm variant sparse asynchronous parallel adaptation kromagnon svrg algorithm standard svrg algorithm saga difference instead maintain historical gradient svrg reference batch gradient update regular interval typically iteration hyper parameter svrg epoch algorithm epoch reference iterate chosen gradient compute iteration epoch algorithm sample uniformly random index executes update saga update direction unbiased proven reasonable epoch update vanish variance enables algorithm converge linearly constant  svrg variant introduce variant epoch random variable iteration bernoulli random variable sample algorithm update reference iterate computes gradient reference gradient algorithm executes normal svrg inner update variant adaptive local convexity inner loop epoch hyperparameter respect closer saga svrg algorithm adaptive kromagnon kromagnon introduce obtain sparse update technique sparse saga algorithm parallel algorithm extension svrg variant introduce  sparse asynchronous parallel version svrg variant algorithm core stochastic update independently sample inner update coordinate whenever decides batch gradient computation difficulty approach core communicate core inner update compute synchronize batch gradient instead introduce variable computation variable checked core update improve parallel optimization analysis stochastic incremental algorithm kromagnon initialize compute parallel synchronously parallel asynchronously sample uniformly inconsistent ˆxt atomic parallel loop algorithm  initialize compute parallel synchronously parallel asynchronously sample sample uniformly inconsistent ˆxt atomic parallel loop another core batch gradient computation core compute allocate computation core proceeds sample random variable sample performs inner update sample gradient computation update compute allocate leblond pedregosa lacoste julien computation gradient compute core resume loop asaga convergence speedup proof easily adapt accommodate  closer saga initial svrg algorithm convergence modify lemma slightly difference exponent replace proof justification tweak batch svrg fully synchronize detail appendix convergence speedup rate kromagnon theoretical detailed outline proof detail appendix theorem convergence guarantee rate kromagnon suppose epoch chosen    inconsistent iterates kromagnon converge expectation geometric rate initial iterate epoch obtain uniformly random inconsistent iterates previous epoch theorem svrg indeed remove asynchronous exactly rate derive dense asynchronous svrg easy consistent writes flaw framework essentially canonical svrg svrg obtains convergence rate rate max max compute convergence rate therefore linear speedup degenerate data feature speedup asaga regime svrg theorem usually theorem optimal epoch analysis parallel speedup prompt author rate specific parameter fix investigate speedup precisely derive theorem svrg kromagnon model theorem improve parallel optimization analysis stochastic incremental corollary convergence guarantee rate serial svrg svrg converges geometrically expectation rate factor per gradient computation min due svrg structure cannot however express convergence algorithm rate factor per gradient computation per epoch easy convergence rate algorithm saga parallel variant kromagnon speedup obtain parallelize svrg saga difference additional hyper parameter knowledge illustrates svrg adaptive local convexity whereas saga  svrg corollary simplify convergence guarantee rate kromagnon kromagnon converges geometrically expectation rate factor per gradient computation min corollary derive serial maximum additional convergence rate rate maximum allowable setting sufficient linear speedup corollary speedup suppose max suppose kromagnon converges geometrically rate factor min svrg linearly faster sequential counterpart constant factor almost asaga additional regime regime kromagnon rate svrg without sparsity previous furthermore generally kromagnon asaga reset epoch core synchronize leblond pedregosa lacoste julien comparison related corollary rate convergence per gradient computation svrg contrary literature algorithm allows easy comparison saga algorithm contrast svrg analysis thm proof technique handle inconsistent non uniform processing across theorem corollary precise enable finer analysis speedup corollary speedup without sparsity regime contrast kromagnon analysis thm theorem dependence rate sparsity remove gradient bound assumption svrg contrary speedup comparison meaningful finally theorem convergence guarantee xˆt algorithm whereas bound error iterate proof theorem detailed outline proof detail appendix proof technique asaga analysis verify kromagnon asaga analysis assumption bound overlap consequently recursive contraction inequality lemma however derive equivalent lemma slightly prompt difference proof technique lemma suboptimality bound ekgtk ekgtk initial iterate epoch proof appendix derive technique lemma although simpler lemma asaga difference prevent reuse lyapunov function proof technique lemma replace depends epoch geometrically decrease quantity sufficient cancel subsequent inequality issue traditional svrg technique proof substitute lemma lemma contraction inequality similarly asaga kromagnon algorithm analyze parameter sample verify although practical actual implementation improve parallel optimization analysis stochastic incremental johnson zhang sum contraction inequality epoch randomization trick relate obtain contraction inequality proof theorem derive contraction convergence rate proof corollary corollary corollary proof sketch detailed proof appendix inequality asaga analysis apply lemma appendix LγC LγC max LγC contraction inequality previously mention geometrically decrease factor lyapunov function asaga cannot instead apply svrg sum contraction inequality epoch appendix  LγC LγC cancel negative relate randomization trick johnson zhang instead iterate epoch iterates epoch uniformly random combine  remove positive recursion inequality     replace define directly theorem leblond pedregosa lacoste julien algorithm hogwild initialize variable parallel inconsistent sample uniformly atomic parallel loop hogwild analysis improve perturbed iterate framework revisit analysis optimization routine proof assume homogeneous computation analysis hogwild algorithm asynchronous parallel constant sgd introduce hogwild algorithm theoretical convergence speedup proof framework allows easily remove classical bound gradient assumption another literature although inconsistent convexity unconstrained regime allows bound uniform bound replace variance optimum theoretical theoretical analysis hogwild inconsistent writes framework outline proof detail appendix useful definition definition ekf variance gradient estimator optimum reference rate convergence serial sgd theorem convergence guarantee rate sgd sgd converges expectation accuracy geometric rate   sgd converges linearly around optimum accuracy linear convergence accuracy serial sgd min proof appendix improve parallel optimization analysis stochastic incremental theorem convergence guarantee rate hogwild min unless min inconsistent iterates algorithm converge expectation accuracy geometric rate ekxˆt define obtain serial sgd recover constant factor asynchronous algorithm serial equivalent  verify regime interested linear regime stringent impose investigate hogwild linearly faster sgd derive respective convergence rate around optimum algorithm converge quantity theorem corollary speedup suppose min allowable sgd hogwild converges geometrically radius rate factor sgd linearly faster sequential counterpart constant factor moreover universal hogwild adaptive local convexity rate sgd knowledge hogwild obtains convergence rate sgd converges equivalent radius maximum guarantee linear convergence sgd hogwild linearly faster sgd reasonable min remark  supersede theorem restrictive asaga kromagnon explain saga svrg composite rate factor directly proportional algorithm enjoy contraction rate allows asynchronous variant maintain linear speedup sgd rate factor directly proportional hence restrictive function derive directly iterates bound distance xˆt easily obtain function leblond pedregosa lacoste julien bound ˆxt adapt classical smoothness inequality  asynchronous parallel convergence accuracy algorithm converge accuracy additional bound radius converge sgd appendix  hogwild bound converge accuracy appendix active upper bound algorithm regime obtain relaxed linear speedup maximum allowable remove instead enforce  weaker overlap min obtain theorem hogwild analysis although variance optimum instead global bound gradient comparison related convergence analysis hogwild assumption global bound gradient allows replace dependence potentially significantly improve upper bound overlap obtain linear convergence accuracy hogwild previous analysis serial sgd contrast hogwild analysis proof technique handle inconsistent non uniform processing across corollary dependence sparsity various bound gradient assumption contrast hogwild analysis thm remove gradient bound assumption enables potentially significantly upper bound linear speedup convergence guarantee xˆt algorithm whereas bound error iterate proof theorem corollary proof technique asaga analysis verify hogwild asaga analysis assumption consequently recursive contraction inequality lemma kromagnon proof diverges derive equivalent lemma  bach analysis hogwild algorithm parameter sample verify improve parallel optimization analysis stochastic incremental lemma suboptimality bound ekgtk ekgtk proof appendix derive technique lemma simpler lemma asaga lemma kromagnon vanish grows reflect constant sgd converge optimum around however simpler allows simply unroll inequality convergence proof substitute lemma lemma contraction inequality unroll inequality cleverly regroup obtain contraction inequality kxˆt kxˆt xtk obtain contraction inequality directly iterates maximum proof theorem finally derive hogwild converges convergence rate radius serial sgd proof corollary proof sketch detailed proof appendix inequality asaga analysis plug lemma appendix LγC LγC contraction inequality previously mention vanish cannot asaga kromagnon proof technique instead unroll equation appendix LγC leblond pedregosa lacoste julien contraction inequality xˆt kxˆt kxˆt xtk previous bound appendix ekxˆt LγC contraction inequality safely remove enforce LγC directly theorem convergence rate comparison corollary simply min allowable hogwild converge linearly sgd algorithm converge proof remark algorithm rate convergence directly proportional empirical sequential algorithm sparse saga exist alternative saga lag update saga algorithm baseline empirical comparison asaga kromagnon hogwild finally additional convergence speedup respect iteration theoretical speedup constant experimental setup model although asaga apply broadly focus logistic regression model practical importance associate objective function exp  kxk data sample data sparse data rcv url dense covtype statistic covtype standardize dense data hence insightful relate theoretical derive coarse sparsity bound remains hardware software core machine 4GB memory algorithm implement scala chose despite typical slowdown standard library appendix primary concern code easily reuse extend research purpose code available http sierra research asaga improve parallel optimization analysis stochastic incremental data statistic density rcv url covtype implementation detail regularization amount regularization update project gradient regularization vector preserve sparsity maintain unbiased estimate gradient sparse saga update becomes diα  comparison theoretical algorithm algorithm fully detailed algorithm difference algorithm implementation random feature vector ait enables data iteration xˆt although violates performs maintain memory recomputing iteration longer subset data implement algorithm enjoys performance choice subtle update guaranteed unbiased setup appendix detail algorithm picked equally grid boundary interval covtype rcv interval whereas url interval admit fairly constant core asaga kromagnon algorithm rcv url covtype comparison sequential algorithm sparse saga lag update sparse saga variant propose approach naive dense update scheme lag update implementation described datasets parallel subset rcv data  data description appendix reveals sparse lag update per iteration dense counterpart faster convergence sparse data furthermore approach convergence iteration sparse saga scheme slightly faster runtime previously sparse update adapt asynchronous dense data covtype approach exhibit performance leblond pedregosa lacoste julien lag sparse saga update suboptimality respect saga update scheme various data suboptimality function suboptimality data sparse data rcv sim lag sparse update per iteration faster convergence improve parallel optimization analysis stochastic incremental suboptimality function speedup function core convergence speedup asynchronous stochastic gradient descent display rcv url covtype asaga kromagnon hogwild asynchronous variant stochastic gradient aforementioned data asaga kromagnon asynchronous sparse svrg described hogwild chosen convergence suboptimality hogwild asynchronous version hence sequential processor reveals asynchronous version significant speedup sequential counterpart examine speedup relative increase core speedup achieve suboptimality hogwild core achieve suboptimality core average convergence appendix information displayed predict theory linear theoretical speedup iteration however respect speedup taper core phenomenon explain hardware model necessity simplification reality machine memory core cache addition ram faster pool memory fully leveraged core unfortunately core location cache coherency protocol deployed ensure information consistent across core protocol computational overhead core information memory stack overhead costly data cache memory unlikely benefit core although perform grid interval fairly constant core asaga kromagnon leblond pedregosa lacoste julien comparison covtype data suboptimality speedup core legend refers plot sample item repeatedly becomes rare speedup actually improve experimentation quantify potentially increase performance sparsity sparsity important role theoretical ill regime linear speedup  regime confront convergence speedup performance asynchronous algorithm covtype data fully dense standardization significant improvement increase core improvement sparser data speedup consequently taper earlier data however theoretical speedup linear attribute performance hardware overhead update fully dense parameter heavily contend sparse datasets compute constant data fails capture sparsity distribution essentially maximum data obtain sparse data derive coarse bound remains theoretical speedup previous experimental experimental speedup suboptimality function encompasses theoretical algorithmic hardware overhead contention memory account analysis improve parallel optimization analysis stochastic incremental theoretical speedup suboptimality respect iteration asaga kromagnon hogwild core curve almost coincide theoretical speedup almost core hence linear isolate plot convergence suboptimality function iteration abstract away potential hardware overhead experimental algorithm data curve core almost coincide indeed theoretical linear speedup regime indeed plot amount iteration converge accuracy function core obtain horizontal algorithm speedup linear attribute various hardware overhead variable contention swap operation expensive compete request increase cache mention closer constant theory parallel optimization literature refer proxy core however intuitively factor influence quantity attempt qualitative argument factor relate core factor indeed core core indeed scenario core exactly execution iteration iteration insight really encompasses define scenario precede core implement global counter sparsely update iteration modify asynchrony counter plot purpose otherwise leblond pedregosa lacoste julien core stuck overlap eventually grows assume core twice relevant quantity ratio execution slowest execution iteration arbitrarily factor execution core data matrix gradient computation others eventually become computation longer algorithm likely explore potential data matrix overlap upper bound core ratio maximum iteration minimum iteration link sparsity distribution data matrix upper bound really useful factor others probability corresponds upper bound exponentially conjecture useful indicator ratio maximum iteration iteration sum preliminary theoretical exploration encompasses complexity usually imply literature reflect constant magnitude core experimental verify intuition variable data characteristic remind density minimum average maximum factor density max min max rcv url covtype estimate compute average overlap iteration difference label  iteration iteration quantity bound actual overlap maximum average maximum average compute overlap global counter improve parallel optimization analysis stochastic incremental overlap overlap function core asaga hogwild data update iteration heavily contentious quantity susceptible artificially asynchrony algorithm magnitude indeed dismiss mere proxy core carefully analyze plot maximum function core relationship indeed roughly linear respect core core core phase transition slope increase significantly maximum function epoch omit dependency epoch iteration cannot increase function respect stable quickly epoch computation forever eventually upper bound mention actually slowly increase function iteration conclusion future building recently propose perturbed iterate framework propose novel perspective clarify important technical issue recent convergence rate proof asynchronous parallel optimization algorithm resolve introduce novel framework demonstrate usefulness analyze asynchronous parallel incremental optimization algorithm asaga novel sparse fully asynchronous variant incremental gradient algorithm saga proof technique accommodates realistic setting usually literature inconsistent writes unbounded gradient obtain tighter leblond pedregosa lacoste julien previous asaga linearly faster saga mild sparsity linear speedup empirical benchmark confirm speedup sag enjoys improve performance combine non uniform sample constant essentially maximum sometimes fails accurately sparsity distribution data finally algorithm directly distribute worker architecture communication optimize avoid prohibitive limit communication interpret artificially increase delay yield delay influence communication constitute direction future analysis exploration encompasses complexity previously acknowledgment  pan implementation kromagnon alberto  typo proof partially google research award msr inria joint FP acknowledges financial     data joint research initiative   pour  improve parallel optimization analysis stochastic incremental appendix outline appendix adapt proof theorem convergence serial sparse saga appendix detail proof convergence asaga theorem linear speedup regime corollary appendix detail proof convergence kromagnon theorem simpler convergence svrg corollary kromagnon corollary finally latter linear speedup regime corollary appendix detail proof convergence hogwild theorem linear speedup regime corollary appendix explain adapt lag update implementation saga asynchronous appendix additional detail data implementation appendix sparse saga proof theorem proof sketch heavily reuse proof technique sketch author combine classical convexity lipschitz inequality derive inequality   contraction additional  positive variance negative suboptimality suboptimality cancel variance author classical smoothness upper bound variance relate suboptimality however partial gradient compute previous upper bound variance involve suboptimality previous directly relatable suboptimality circumvent issue lyapunov function define encompass proof lyapunov function contraction proof outline fortunately reuse proof sparse saga converges rate regular saga establish lemma verify gradient estimator unbiased derive variant equation remind reader ekf ekf ekα ekα  leblond pedregosa lacoste julien unbiased gradient estimator update estimator unbiased estimator unbiased    diα     vector nonzero component component definition  equation derive equation define diα contrary author define concern sparsity inequality kak kbk ekf ekf ekα equivalent definition differs derive equation ekα  ekα   diα  PS orthogonal decomposition diα diα disjoint orthogonality vector simplify expression  diα  diα   similarly    diα  improve parallel optimization analysis stochastic incremental ekα   version equation proof lemma proof reuse without modification obtain theorem appendix asaga proof theorem corollary initial recursive inequality derivation equation ˆxt kxt kxt  kxt   gti kxt   gti  gti equation bound  gti thanks  gti    ˆxt classical convexity bound inequality  ˆxt ˆxt kxˆt convexity bound kxˆt kxˆt xtk kxt kak kbk  gti   xtk ˆxt initial recursive inequality rewrite explicitly ekgtk  xtk  gti γet  ˆxt proof lemma inequality ˆxt lemma bound ekxˆt xtk  gti respect ekguk leblond pedregosa lacoste julien bound  gti  gti  gti equation  gti diagonal matrix ekguk ekgtk proposition ekguk ekgtk bound ekxˆt xtk respect thanks expansion xˆt kxˆt xtk    proposition    ekguk  expectation ekxˆt xtk ekguk ekguk ekguk ekguk rewrite ekgtk proof lemma introduce specify lemma γet ekgtk ekguk ekguk  γet  ekguk improve parallel optimization analysis stochastic incremental proof lemma suboptimality bound ekgtk derive bound respect suboptimality appendix ekgtk ekf ˆxt  ekf ˆxt ˆxt random variable picked uniformly random whereas fix constant handle  express  definition  independent global expectation xˆt  define rewrite quantity     fix iterate quantity  information split  along dimension handle inconsistency   ˆxu ˆxu ˆxu rewrite indicator obtain independent equality enable distribute expectation suppose handle afterwards replace partial gradient omit clarity  leblond pedregosa lacoste julien picked uniformly random roughly picked refine account collision due asynchrony definition iteration unfinished update memory later update similarly update overwritten update cannot infer discussion conclude implies outside independent upper bound indicator function ˆxu ˆxu ˆxu  ˆxu crucial independence assumption  arise sum dimension ˆxu ekf ˆxu  ekf ˆxu ekf ˆxu ekf ˆxu  equation max improve parallel optimization analysis stochastic incremental exclude formula generic multiplier treat differently bound initial initialize fix picked disappears lose factor evaluate ekα ekα plug lemma ekgtk introduce ekα saga algorithm batch gradient compute lemma thanks initialize fix quantity cannot bound ekα introduce lemma  simpler  described slight variation ekf ˆxu sum dimension gradient computation writes synchronize reference gradient consistent upper bound indicator variable sample update update roughly batch fully synchronize worry update overwrite reference gradient iterates compute gradient contains update core update however update variable variable sample atomic iteration label reference gradient update fortunately update introduce quantity iteration gradient compute label correspond update iterates conceivably another core iteration update issue operation update idempotent reference gradient compute leblond pedregosa lacoste julien regular update parameter assumption applies reference gradient update iteration label ˆxu ˆxu ekf ˆxu prof lemma  actually slightly exponent upperbound lemma proof theorem  exactly manner asaga remark iteration compute reference batch gradient sequential parallel version analysis corollary asaga applies  algorithm obey convergence speedup inequality derivation combine bound ekgtk derive lemma lemma γet  LγC LγC LγC LγC LγC define γet LγC LγC LγC LγC inequality improve parallel optimization analysis stochastic incremental lyapunov function associate recursive inequality define  target contraction rate define later bound  LγC LγC LγC LγC  LγC LγC LγC LγC rearrange sum expose sum factor proof lemma sufficient convergence asaga explicit ensure negative positive safely sum inequality trickier handle separately indeed enforce negative significantly eventually convergence rate factor instead handle directly lyapunov function computation factor explicit assume split  leblond pedregosa lacoste julien  LγC LγC LγC easy derive inequality easy derive LγC trickier insight rewrite sum manner  min define assumption bound min sub sum effectively positive LγC trick min  improve parallel optimization analysis stochastic incremental LγC finally compute complicate indeed factor compute triple sum compute factor inner sum max exponent positive bound sum otherwise simply bound sum LγC LγC min LγC LγC combine LγC LγC  LγC computation recall treat separately initialization saga creates initial synchronization contribution bound ekgtk roughly contribution safely handle lyapunov inequality bound reasonable constant split contribution explain detail leblond pedregosa lacoste julien   LγC LγC LγC safely ignore LγC compute min LγC already compute computation exactly  finally compute min min min improve parallel optimization analysis stochastic incremental LγC LγC LγC LγC  sufficient convergence negative safely reduce polynomial remark upper bound upcoming polynomial sufficient convergence recall define depends expand  bracket rearrange polynomial  discriminant polynomial positive negative relevant analytically positive upper bound overlap guarantee convergence unfortunately upper bound becomes exponentially presence exponent specifically bound exp exp obtain factor exp denominator recall lemma derive instead assumption constant chosen interpretable relate convergence standard saga convergence theorem explain assumption reasonable bernoulli inequality integer bound derive inequality exp valid leblond pedregosa lacoste julien manageable constant slightly restrictive assumption target rate overlap upper bound loosely bracket plug simpler sufficient   positive LC LC  LC simplify inequality recall LC already bound reasonable appendix inequality derive concavity differentiable concave function improve parallel optimization analysis stochastic incremental min sufficient min rewrite proof inequality convergence  suboptimality xˆt xˆt quantity algorithm whereas virtual fortunately easily adapt γet translates replace LγC LγC  LγC easily derive min γet proof lemma multiplier simplicity analysis loose derive tighter bound leblond pedregosa lacoste julien proof theorem convergence guarantee rate asaga lyapunov convergence assumption lemma thanks rewrite constant depends finite crucially reuse argument loose bound assumption lemma γet linearly contract sum contract linearly minimum geometric rate factor define min max γet γet maximal assumption lemma ekα theorem obtain geometric rate min simplify definition parenthesis negative indeed contains positive negative assumption lemma introduce circumvent problematic prevent geometric convergence constant potentially sum becomes annoy linear perform index sum improve parallel optimization analysis stochastic incremental min theorem proof define theorem proof corollary speedup regime asaga refer theorem geometric rate factor saga min corollary considers distinguish regime parallel speedup algorithm obtains derive linear speedup regime regime geometric rate factor sequential saga linear speedup constant factor enforce recall min already verify accord theorem recall min constant factor sufficient alternative rewrite sufficient alternative decrease suppose without loss generality rewrite sufficient suppose initial assumption combine sufficient geometric rate factor asaga sequential saga leblond pedregosa lacoste julien ill regime regime geometric rate factor sequential saga obtain linear speedup reduces sufficient sufficient becomes proof corollary universal universal satisfies rate factor min sequential saga proof corollary appendix kromagnon proof theorem corollary proof lemma suboptimality bound ekgtk lemma serial sparse svrg ekgtk ekf ˆxt ekf  remains kromagnon equation bound manner ekgtk ˆxt proof theorem convergence guarantee rate kromagnon inequality derivation asaga analysis plug lemma lemma max γet improve parallel optimization analysis stochastic incremental inequality LγC LγC max LγC contraction inequality derivation adopt svrg sum inequality epoch randomization trick LγC LγC max LγC upper bound remove telescope max min  LγC LγC randomization trick  LγC LγC finally recursive inequality remove positive bound   standard convexity inequality contraction inequality     leblond pedregosa lacoste julien proof corollary corollary corollary speedup regime simpler svrg standard convergence rate serial svrg  define obtain optimal denominator whereas upper bound replace denominator satisfied lose factor enforce easily algorithm easily frame rate factor per gradient computation verify define estimate bernoulli inequality min min proof corollary simpler kromagnon define theorem upper bound remove denominator reasonable factor enforce obtain rate factor per gradient computation min proof corollary improve parallel optimization analysis stochastic incremental speedup rate factor svrg kromagnon regime convergence svrg slightly complex expression kromagnon gradient computation ensure allowable comparison suppose regime rate factor svrg linear speedup rate factor kromagnon recall rewrite condense suppose ill regime rate factor svrg linear speedup rate factor kromagnon derive sufficient obtain corollary proof appendix hogwild proof theorem corollary proof lemma suboptimality bound ekgtk lemma lemma simply introduce derive bound ekgtk ekf ˆxt ekf ˆxt ekf ˆxt ekf kak kbk leblond pedregosa lacoste julien proof theorem convergence guarantee rate hogwild inequality derivation plug lemma lemma  γet inequality LγC LγC contraction inequality derivation unroll LγC LγC simplify min min  easily bound asaga bernoulli inequality assume assumption restrictive weaker linear obtain analysis corollary improve parallel optimization analysis stochastic incremental geometric sum plug obtain LγC contraction inequality derivation xˆt contraction inequality convergence however quantity exist fix iteration prior algorithm iteration unwieldy xˆt converges inequality kxˆt kxˆt xtk already contraction bound combine lemma ekxˆt xtk LγC easy combine rewrite ekxˆt xtk LγC LγC LγC combine ekxˆt LγC maximum theorem inequality ekxˆt xtk equation leblond pedregosa lacoste julien remove safely enforce negative hence LγC plug  asaga analysis reduces polynomial remark upper bound polynomial sufficient convergence define sufficient   discriminant polynomial positive negative relevant analytically positive upper bound overlap guarantee convergence positive LC LC simplify LC proof theorem proof theorem convergence serial sgd analyze corollary derive maximum allowable serial sgd sgd verifies simpler contraction inequality lemma ekgtk γet contraction factor instead xˆt inequality kxt kxˆt apply convexity bound initial recursive inequality lemma serial sgd plug unroll improve parallel optimization analysis stochastic incremental linear convergence around optimum remove equation safely negative trivially derive achieve linear convergence sgd converges geometric rate radius around optimum accuracy linear convergence accuracy serial sgd min proof corollary speedup regime hogwild convergence rate sgd hogwild directly proportional hogwild linearly faster sgd reasonable maximum allowable ensure linear convergence hogwild theorem sgd recall sufficient definition min linear speedup min proof corollary regardless algorithm  proof corollary weaker min allows bound accuracy appendix difficulty parallel lag update implementation dense update defer instead dense update counter coordinate parameter vector variable update average gradient coordinate whenever component compute gradient without modify algorithm sequential iteration perform update dense fashion coordinate stale execution counter parameter vector counter parallel issue arise core attempt lag update addition replacement ensure leblond pedregosa lacoste julien overwrites lag multiple overly atomically  triplet highly impractical explicit global counter asaga global counter solely proof dense update coordinate coordinate update coordinate fix random variable differs coordinate coordinate whereas lag implementation multiplier constant conditional potentially xˆt trick asynchronous parallel svrg apply relies reference gradient svrg constant throughout epoch saga implementation scheme parallel impractical actually yield algorithm dense version harder analyze confirm author implement parallel version lag update scheme alter algorithm succeed obtain algorithm suboptimal performance appendix additional empirical detail detailed description data data logistic regression purpose binary classification rcv reuters corpus volume rcv data archive manually categorize  available reuters ltd research purpose associate task binary text categorization url data introduce associate task binary malicious url detection data contains URLs obtain random yahoo directory listing benign URLs web mail provider malicious URLs benign malicious ratio feature lexical information metadata data obtain  project http csie ntu edu cjlin  datasets binary html improve parallel optimization analysis stochastic incremental covertype data associate task binary classification originally pre treatment feature  variable contrarily dense data  fourth data non parallel specific swap constitutes  article discussion simulated auto simulated aviation auto aviation implementation detail hardware dell PowerEdge machine intel xeon processor 2GHz core 4GB mhz ram software algorithm implement scala software stack consist linux operating scala java chose expressive experimentation despite typical performance primary concern code easily reuse extend research purpose harder achieve heavily optimize code error prone parallel compute exhibit sub optimal   slowdown consistent across data roughly scala code despite slowdown convergence per iteration furthermore implementation hogwild kromagnon various code available http sierra research asaga necessity swap operation interestingly swap instruction implementation asaga display suboptimality plot non thread operation swap CAS operation non thread version faster fails converge beyond specific suboptimality swap version converges linearly machine precision swap instruction  google library guava  hood package java util concurrent atomic standard java library indeed benefit cpu optimize instruction efficient storage gradient expensive proposition linear predictor model actually scalar per gradient implementation asaga leblond pedregosa lacoste julien swap implementation asaga suboptimality function asaga swap CAS operation standard operation graph reveals CAS indeed practical implementation ensure convergence precision bias update implementation implementation detailed algorithm maintain memory instead recomputed iteration reading data iteration compute iteration however remove unbiasedness guarantee definition expectation  sample uniformly random average  precise component without synchronization coordinate update account conversely writes component precede correspond induce another source bias alleviate issue coordinate lock synchronize lock inexpensive vector lock however previously experimental fix