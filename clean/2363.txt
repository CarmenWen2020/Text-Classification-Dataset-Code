machine task fix input linear regression predict hidden response afford attain response subset construct linear prediction dataset performance prediction evaluate loss response attain remain hidden approximate obtain dimension response joint sample technique volume sample moreover obtain volume sample subproblem unbiased estimator optimal response unbiasedness desirable subset selection technique motivate develop theoretical framework volume sample matrix expectation equality statistical guarantee importance regression numerical linear algebra regularize variant volume sample propose efficient algorithm volume sample technique practical machine toolbox finally experimental evidence confirms theoretical finding keywords volume sample linear regression sample active optimal introduction introductory linear regression dimension non zero hidden response target assume obtain response expensive learner afford request response index request response learner determines approximate linear dimensional homogeneous response learner request loss approximate loss optimal linear knowledge response response suffices index chosen proportional learner approximate loss loss optimum compute response moreover approximate unbiased estimator optimum  expand version conference DEREZINSKI warmuth extend formula dimension sample response joint sample distribution volume sample contribution loss response twice loss optimum dimension response denote rank matrix transpose vector response goal minimize loss  linear vector denote optimal vector minimize loss response attain subset learner fix fix none response random subset index obtains response correspond learner proceeds optimal linear subproblem XS XS subset indexed correspond response response vector generalization dimensional distribution chooses index chosen proportional volume  span XS volume det XS elementary linear algebra volume sample assures approximation expectation loss response loss det XS furthermore sample procedure attains response ratio loss loss optimum cannot bound constant unbiased pseudoinverse estimator connection linear pseudoinverse matrix dimensional response vector optimal argminw similarly XS subproblem XS propose implementation volume sample reverse iterative sample enables novel proof technique obtain elementary expectation formula  volume sample XS XS isx isx matrix index consecutive reverse iterative volume sampling linear regression suppose goal estimate pseudoinverse pseudoinverse subset recall subset index fix XS submatrix indexed version zero matrix isx selection matrix dimensional diagonal matrix otherwise fix index chosen proportional det XS expectation formula equality isx XS isx isx isx indexed XS remain zero expectation matrix XS clearly submatrix expectation formula implies drawn volume sample unbiased estimator XS isx isx expectation formula pseudoinverse estimator isx compute useful notion matrix variance application random matrix theory isx isx isx isx regularize volume sample develop regularize variant volume sample extends reverse iterative sample subset useful extension matrix variance formula namely regularize procedure sample subset satisfies XS def standard notion statistical dimension crucially bound subset dimension additional assumption response vector generate linear transformation distort bound bound XS variance bound ridge regression estimator specifically xwe zero bound variance var sample accord  volume sample  obtain bound prediction error   volume sample proof cramer DEREZINSKI warmuth XS ridge regression estimator subproblem XS bound upper bound regularize volume sample essentially optimal respect choice subsampling procedure algorithm polynomial algorithm volume sample recently propose complexity algorithm framework reverse iterative sample deterministic runtime probability algorithm improve factor volume sample nearly efficient comparable sample technique leverage sample datasets confirm efficiency algorithm sample volume sample effective leverage sample task subset selection linear regression related volume sample  DPP DPP attention literature application machine recommendation cluster approximate efficiently generate sample distribution propose useful randomize algorithm focus sample volume sample propose   motivate application graph theory linear regression matrix approximation subset input matrix linear regression task extensively statistic literature optimal pool active various criterion subset selection propose optimality optimality optimality seek minimize XS combinatorially optimize exactly volume sample XS approximate randomize sample inverse covariance matrix trace computational geometry variant volume sample obtain optimal bound rank matrix approximation task goal subset matrix rank bound rank approximation construct volume sample index obtains optimal multiplicative bound task polynomial algorithm volume sample  rademacher   linear regression rank suffice obtain multiplicative bound focus volume sample recall simplicity assume rank compute approximate linear regression explore domain numerical linear algebra  overview multiplicative bound loss approximate achieve via approach approach relies sketch input matrix response vector suitably chosen random matrix algorithm sketch generate input matrix linear regression computationally efficient  clarkson woodruff reverse iterative volume sampling linear regression response generate sketch suitable goal response approach subsampling input matrix response sample learner optimally solves sample subproblem obtain vector prediction subproblem agnostic minimal coreset without response vector denote vector approach coincides goal focus focus sample multiplicative loss bound volume sample sufficient achieve multiplicative bound fix factor sufficient focus efficiency combinatorics volume sample previous mostly sample statistical leverage leverage marginals volume sample sample sample achieve multiplicative loss bound linear regression obtain volume sample jointly chosen subset informative brings sample focus estimator volume sample unbiased therefore average accurate estimator average immediately unbiased estimator loss optimum sample response construct factor unbiased estimator sample response unbiasedness concern estimator recently outline define volume sample instance procedure reverse iterative sample methodology matrix expression expectation pseudoinverse estimator isx isx isx sample volume sample central volume sample cauchy binet formula determinant proof formula leverage marginals volume sample formulate linear regression response upper bound loss volume sample estimator theorem discussion related bound theorem additional related matrix expectation formula discus unbiased estimator easily average improve loss discus construct unbiased estimator regularize variant volume sample propose along statistical guarantee compute subsampled ridge regression estimator efficient volume sample algorithm reverse iterative sample paradigm experimentally evaluate finally concludes future research direction typically additional rescale subproblem whereas technique propose rescale DEREZINSKI warmuth reverse iterative sample integer dimension subset matrix formula goal sample sample develop concise expression ES formula reverse iterative sample sample acyclic graph dag node correspond proceed along graph iteratively remove concretely dag contains node node node denote label conditional probability vector occurs sample node trace dag node node assign probability shorter node node probability probability probability node via sum probability finally probability node generally probability node layer associate formula node dag equality compute expectation lemma ES proof suffices expectation successive layer summand per summand per equality exactly summand expression equivalent reverse iterative volume sampling linear regression volume sample rank matrix sample volume sample chooses subset probability proportional volume span submatrix XS volume det XS theorem dag setup compute normalization constant distribution subset volume ignore unreachable propose sample procedure theorem det det XS define probability def det iXS det XS XS reverse iterative volume sample probability distribution det XS simply definition probability subset det XS det volume sample rewrite ratio det XS det XS XS sylvester theorem determinant incidentally determinant theorem implies generalization cauchy binet formula det XS det binomial coefficient becomes vanilla cauchy binet formula proof theorem minimalist proof classical formula proof reverse iterative sample node probability sake completeness inductive proof generalize cauchy binet formula appendix proof node det XS probability sum XS xix XS XS remains formula probability node det XS probability superset positive volume superset volume probability assume det XS positive volume determinant sample XS det XS volume span XS DEREZINSKI warmuth probability along telescope additional factor accumulate probability probability det XS det det XS det immediate consequence sample procedure composition volume sample distribution subsampling proof highlight combinatorics volume sample corollary hierarchical sample procedure volume sample XT volume sample XT return distribute accord volume sample proof probability probability formula volume sample theorem probability node node det XS det XT det XT det det XS det det XS det probability competitor volume sample sample statistical leverage input matrix leverage define def recall quantity definition conditional probability theorem leverage compute submatrix XS relationship leverage volume sample sample accord volume sample leverage marginal probability formula marginals volume sample  reverse iterative volume sampling linear regression proposition rank matrix sample accord volume sample proof instead compute det XS det det XT det det det det cauchy binet twice marginal probability expectation formula volume sample expectation remainder volume sample expectation volume sample sample fix expectation formula choice proven theorem lemma suffices volume sample related expectation formula theorem proven later technique recall XS submatrix indexed version zeroed matrix isx dimensional diagonal matrix otherwise theorem rank matrix volume sample isx linear algebra literature elementary determinant cramer proof methodology developed reverse iterative volume composition volume sample corollary theorem reduce however proof theorem DEREZINSKI warmuth sample fundamental formula core volume sample important application focus application linear regression however   discus pseudoinverse submatrix essential application important establish variance bound expectation volume sample concrete guarantee obtain formula estimator theorem rank matrix volume sample XS isx isx volume sample matrix equality replace positive definite inequality volume sample equivalent det XS volume sample implies volume sample theorem immediately expectation formula frobenius norm isx estimator isx isx isx norm formula   numerous application theorem pre trace version norm formula proof technique simpler volume sample becomes inequality mention application theorem context linear regression response vector model noisy linear transformation xwe random vector detailed discussion matrix XS interpret covariance matrix estimator fix theorem formula covariance matrix volume sample extend version guarantee regularize estimator model theorem application arbitrary response vector combine theorem obtain covariance formula pseudoinverse matrix estimator isx isx isx isx isx isx isx isx notion covariance random matrix theory random matrix analyze  reverse iterative volume sampling linear regression background matrix expectation formula volume sample  compute projection matrix onto span matrix define PX def apply theorem immediately unbiased matrix estimator projection matrix isx isx XX PX matrix estimator isx closely linear regression transform response vector prediction vector subsampled isx volume sample covariance matrix expectation formula theorem rank matrix matrix sample accord volume sample isx isx isx PX PX matrix equality replace positive definite inequality expectation formula limited sample consequence theorem relates loss subsampled estimator loss optimum estimator unlike formula theorem proof rely methodology lemma expectation dag associate sample defer proof expectation formula extension formula sample proof theorem apply lemma isx suffices XS isx XS XS apply sherman morrison iXS XS xix XS XS XS xix XS XS isx xie DEREZINSKI warmuth expand factor expectation XS isx isx expectation remain sum XS XS xie XS xix XS isx XS XS appendix alternate proof derivative argument proof theorem XS lemma suffices volume sample XS XS iXS apply sherman morrison iXS XS XS XS xix XS XS XS XS xix XS XS denominator XS zero sum denominator positive matrix equality becomes positive definite inequality linear regression response motivation volume sample suppose dimensional linear regression input matrix response vector minimizes loss  denote loss optimal vector minimizes def argmin compute access input matrix response vector assume access response vector restrict random subset fix response submatrix XS reveal vector subset index input matrix correspond response goal distribution subset function learner define optimal multiplicative constant specialized minc minp maxy EP domain distribution function specialized bound future research reverse iterative volume sampling linear regression fix constant independent throughout argument shorthand function assume attain response expensive response multiplicative bound volume sample attain response sufficient response unbiased estimator expectation suffers loss upper bound volume sample observation subproblem XS vector loss zero algorithm predict consistent vector response extend response vector aim multiplicative loss bound algorithm predict optimum def XS whenever subproblem XS loss XS rank unique consistent subproblem learner function theorem input matrix response vector loss optimal subproblem XS obtain volume sample loss upper bound restriction response bound introduction bound already non obvious dimension multiplicative factor visualization bias dimension factor becomes dimension instructive loss optimum zero vector response linearly independent optimum multiplicative loss formula theorem clearly formula specifies generalizes gracefully noisy volume sample loss obtain response factor loss optimum moreover loss function convex jensen inequality theorem gap jensen inequality coincides regret estimator expectation volume sample schematic gap variance  prediction estimator unbiased summary DEREZINSKI warmuth regret gap jensen  variance observation bound highlight upper bound theorem theorem matrix expectation formula imply theorem equality loss volume sample however equality guaranteed matrix minimal matrix equality theorem strict inequality equality assumption apply infinitesimal additive perturbation matrix matrix equality optimum loss significantly perturbation sample loss sufficiently gap inequality minimal subset sample identical probability sample volume sample subset identical submatrices XS XS therefore equally optimal vector loss bound slightly perturbed input matrix arbitrarily response vector submatrix singular upper bound theorem tight subset probability loss expectation significantly affected component directly calculation correspond perturbed subproblems volume subproblems loss det  det  det  reverse iterative volume sampling linear regression subproblem volume loss loss easily compute gap bound disappears denominator normalize constant volume sample bound importance joint sample factor theorem cannot improve response proposition exists index proof input vector simplex origin response non zero optimal zero vector loss subset index subproblem loss input vector indexed obtain prediction geometric argument simplex origin simplex def input vector prediction vector  loss  moreover easy deterministic algorithm without response guarantee multiplicative loss bound factor sake completeness proposition input matrix deterministic algorithm chooses singleton response vector loss subproblem optimal loss related DEREZINSKI warmuth proof response vector vector index dimensional proof volume sample uniformly distribution multiplicative factor importance joint sample volume sample crucial role achieve multiplicative loss bound randomness deterministic algorithm guarantee bound proposition chosen submatrices rank rank deficient submatrix positive probability multiplicative bound proposition  sample procedure achieve multiplicative loss bound response corollary jointly subset volume sample ensures correspond input vector input volume sample probability rank submatrix XS intuitively rank deficient subset effective choice statement precise randomize algorithm positive probability selects rank deficient subset cannot achieve multiplicative loss bound intuitively algorithm rank deficient subset vector input matrix subset response loss subproblem XS however rank XS choice vector loss unique positive loss response vector proposition input matrix algorithm sample rank deficient subset positive probability loss algorithm cannot bound constant optimum loss response vector rank sample subset positive probability constant factor approximation proof rank deficient subset chosen probability setup bound response vector adversary adversary XS response zero algorithm response XS span XS vector response XS response adversary chooses indexed response loss loss algorithm loss reverse iterative volume sampling linear regression strengthen proposition whenever sample rank deficient loss optimum zero loss algorithm positive however proposition weaker specific input matrix proposition input matrix rank consist standard basis vector randomize algorithm probability selects subset rank XS function response vector satisfy probability proof adversarial response vector construct carefully vector response vector ensures consists standard basis vector component learner discover exactly incur positive loss rank deficient lack standard basis vector rank XS suppose matrix learner function sake concreteness argument shorthand function proof input function rank deficient index response vector consistent fix rank deficient distinct basis vector XS clearly fix subset XT contains basis vector XS exactly basis vector XS duplicate component response clearly choice input function restriction bound  max  function exists adversarial response vector guarantee learner rank deficient therefore receives positive loss proposition sample distribution leverage sample sample multiplicative loss bound probability expectation corollary input matrix rank consist standard basis vector randomize algorithm selects random multiset via sample distribution function response vector satisfy probability DEREZINSKI warmuth proof sample probability unique standard basis vector coupon collector probability submatrix XS rank algorithm proposition consistent adversarial response vector probability loss positive corollary restrict contains duplicate corollary arbitrary rank matrix proof loss expectation formula discus connection linear regression volume proof loss suffer optimum vector  euclidean distance prediction vector response vector minimize distance subspace span feature vector projection onto subspace denote projection PX define PX linear mapping onto span matrix span PX PX geometric interpretation   feature vector input matrix furthermore extend input matrix response vector extra def prediction vector projection onto span feature vector height formula relate volume volume  response vector  hence volume volume distance span distance  projection onto span det det proposition corollary theorem suppose input matrix remain response training proposition relates loss obtain loss proposition index reduce linear regression proven uniform sample theorem  doerr uniform sample coupon collector bound sample reverse iterative volume sampling linear regression det det det def loss algebraic proof proposition essentially proof theorem   sake completeness geometric proof proposition appendix volume stress connection volume sample matrix exactly training matrix rank loss zero training obtain simpler relationship proposition corollary rank define det det proof proposition det det corollary height formula volume proof theorem recall goal loss volume sample proof theorem rewrite expectation corollary matrix XT assume rank XT det XT det det det summand index inner summation becomes multiplication loss det det det det cauchy binet formula application height formula summands rank XT DEREZINSKI warmuth non negative becomes inequality proof theorem expectation matrix matrix expectation formula theorem corollary loss expectation formula theorem observation loss formula arbitrary response vector allows matrix proof theorem loss estimator projection matrix PX  PX PX PX projection matrix PX loss expectation subsampled estimator obtain isx isx isx crucially extract response vector expectation formula allows formula theorem isx PX elementary symmetric matrix matrix expectation formula isx PX expand apply theorem obtain covariance equivalent theorem PX isx isx PX isx PX PX average unbiased estimator response goal sample index construct function response multiplicative factor bound input matrix response vector recall denotes loss optimal response previous subsection goal achieve sample procedure function factor finite drawn proportional similarly positive definite inequality matrix reverse iterative volume sampling linear regression volume XS det XS factor optimal denotes linear subproblem XS goal arbitrarily optimum loss sample sample distribution subset function built related bound leverage sample sample suffices achieve factor probability however imply multiplicative bound expectation conjecture volume sample achieve factor sample expectation technique volume sample achieves factor generalize proof sample however unique volume sample estimator useful unbiased estimator benefit unbiased estimator optimal prediction vector rudimentary version bias variance decomposition   unbiasedness estimator assures therefore factor loss bound equivalent factor variance bound loss bound variance bound  reduce variance unbiased estimator sample independent sample predict average estimator loss bound average estimator satisfies response approximation volume sample achieves factor proof technique response factor approximation vector leverage sample unbiased average estimator independent volume sample regret prediction variance DEREZINSKI warmuth response unbiased estimator achieves factor approximation average equivalent unbiased estimator achieves constant factor unbiased estimator achieves constant factor average factor ideally unbiased estimator version volume sample achieve feat conclude favorite version volume sample achieves constant factor approximation minimal statistical assumption response vector bound assume response vector linear plus bound zero model volume sample achieves constant factor approximation regularize volume sample noisy response algorithm regularize sample det XS det XS sample return volume sample define fundamental limitation namely undefined whenever matrix rank sample subset dimension motivate limitation propose regularize variant  volume sample define generalization reverse iterative sample procedure det iXS det XS normalization factor conditional probability sum compute sylvester theorem det iXS det XS XS XS XS XS regularization trace vanishes recover volume sample however non zero depends entire matrix XS regularize volume sample complicate equality proven previous longer analogous sample probability theorem recover node node graph probability however proof technique developed reverse iterative sample apply extension variance formula theorem recent factor approximation achieve guarantee probability expectation estimator unbiased reverse iterative volume sampling linear regression theorem sample accord regularize volume sample XS def constant notion statistical dimension refer effective freedom eigenvalue decrease rank unlike theorem meaningful bound sample proof obtain theorem essentially methodology described lemma regularize equality replace inequality recall sylvester theorem compute unnormalized conditional probability det iXS det XS XS XS shorthand proof compute unnormalized expectation apply sherman morrison formula iXS  xix xix  finally normalization factor already compute bound statistical dimension matrix bound obtain iXS XS theorem remains chain conditional expectation along sequence subset obtain regularize volume sample DEREZINSKI warmuth ridge regression noisy response apply obtain statistical guarantee subsampling regularize estimator matrix task fitting linear model vector response xwe zero random vector covariance matrix var classical task ridge estimator argmin   consequence theorem sample regularize volume sample ridge estimator subproblem XS XS generalization respect prediction error  error mse theorem suppose xwe zero vector var sample accord regularize volume sample ridge estimator compute subproblem XS  prediction error  error  bound  subsampled ridge estimator statistical guarantee achieve regularize volume sample nearly optimal standard approach non volume sample essential achieve generalization response namely data matrix subsampling procedure leverage sample response achieve  contrast volume sample obtains bound matrix response theorem sufficiently divisible exists matrix statement vector correspond regression xwe var satisfies statement subset reverse iterative volume sampling linear regression multiset sample distribution  proof theorem standard analysis ridge regression estimator perform bias variance decomposition error bias appropriately bound recall calculation fix subproblem XS compute bias ridge estimator fix recall shorthand XS   XS similarly covariance matrix     error ridge estimator fix subset bound    λtr  apply cauchy schwartz inequality matrix trace assumption  expectation sample  ES theorem bound prediction error standard bias variance decomposition fix       DEREZINSKI warmuth expectation subset  ES  ES theorem  bound application theorem mse trace version inequality however obtain bound  positive semi definite inequality proof theorem  divisible define def def statistical dimension def prediction variance estimator   prediction bias estimator   estimator  minimizes expression derivative respect reverse iterative volume sampling linear regression derivative negative positive unique minimum  achieve regardless subset chosen seek bound focus proof assume formula simplifies apply jensen inequality convex function proof assume suppose multiset sample distribution similarly corollary exploit coupon collector probability vector  bound  efficient algorithm volume sample propose algorithm efficiently perform volume sample address   polynomial algorithm  rademacher algorithm later improve   recently algorithm arbitrary complexity propose reverse iterative sample technique achieve faster volume sample algorithm apply regularize volume sample described standard volume sample algorithm deterministic runtime whereas accelerate version probability obtain improvement factor factor algorithm   algorithm implement reverse iterative sample theorem index algorithm remove remove index distribute accord volume sample proceed desire primary procedure update conditional distribution convenient unnormalized define via sylvester theorem compute XS sake generality regularize DEREZINSKI warmuth volume sample naively compute XS matrix overall runtime naive becomes compute XS compute computation matrix inverse efficient matrix XS compute obtain previous sherman morrison formula update instead furthermore propose strategy maintain update sherman morrison rejection sample compute rejection trial avoids compute computation expensive strategy lemma update previous oppose however compute rejection sample explain shortly lemma matrix distinct index iXS XS XS proof XS xix xix iXS sherman morrison formula overall complexity reverse iterative sample strategy factor naive version initialization theorem algorithm RegVol index distribute accord regularize volume sample primarily interested bound assumption however technique easily adapt reverse iterative volume sampling linear regression proof lemma sherman morrison formula invariant loop XS XS runtime compute initial compute initial inside loop update update overall runtime becomes algorithm RegVol  sample  return algorithm FastRegVol max sample uniformly  sample bernoulli  RegVol XS return algorithm FastRegVol rejection sample strategy observation update conditional distribution wasteful distribution slowly throughout procedure moreover unnormalized compute bound sample distribution iteration employ rejection sample sample uniformly compute accept probability otherwise another sample rejection sample employ locally within iteration algorithm rejection revert algorithm moreover probability acceptance strategy compute per iteration algorithm oppose update majority algorithm conditional probability drastically becomes efficient algorithm RegVol theorem algorithm FastRegVol sample accord  volume sample probability DEREZINSKI warmuth proof analyze efficiency rejection sample FastRegVol random variable correspond trial loop FastRegVol conditioning algorithm distribute accord geometric distribution probability XS variable independent upper bound sequence independent variable rbt expectation trial FastRegVol bound rbt obtain bound probability instead expectation variable rbt independent upper bound sum probability standard concentration bound geometric distribution corollary  immediately probability however careful analysis dependence lemma rbt independent random variable rbt trial rejection sample compute overall complexity FastRegVol computation update matrix rejection sample RegVol portion proof lemma  bound sum geometric random variable minimum acceptance probability variable vast majority rbt acceptance probability intuitively advantage improve bound partition variable roughly acceptance probability separately bound sum variable assume integer partition notation partition def def def min def reverse iterative volume sampling linear regression apply theorem  obtain    moreover chosen  denote  apply union bound  obtain desire bound rbt  experimentally evaluate propose volume sample algorithm runtime task subsampling linear regression regularization sample prediction implement algorithm regularize volume sample algorithm FastRegVol RegVol leverage sample LSS popular sample technique dataset RegVol FastRegVol LSS  MSD  abalone regression datasets runtime comparison RegVol FastRegVol runtime sample leverage LSS perform benchmark linear regression datasets libsvm repository datasets along sample dimension dataset MSD RegVol reasonable however FastRegVol plot runtime portion datasets FastRegVol RegVol respect data FastRegVol exhibit linear dependence datasets regularize variant leverage context kernel ridge regression   however regularize leverage improvement DEREZINSKI warmuth data  RegVol FastRegVol data MSD RegVol FastRegVol data  RegVol FastRegVol data abalone RegVol FastRegVol comparison runtime FastRegVol RegVol libsvm regression datasets data subset subset selection ridge regression apply volume sample task subset selection linear regression evaluate subsampled ridge estimator average loss dataset average loss  XS evaluate estimator subset subset sample accord regularize volume sample leverage sample average clarity dataset chosen subsampled ridge estimator perform average sample preselected leverage appropriate rescale instance sample subproblems  detail volume sample rescale datasets response obtainable regularize volume sample estimator leverage sample predict theorem bound theorem dataset  conclusion volume sample joint sample procedure diverse sample sample developed matrix expectation formula volume sample credence fundamental sample procedure significant progress efficient implementation sample procedure sample compute ridge estimator reverse iterative volume sampling linear regression comparison loss subsampled ridge estimator regularize volume sample leverage sample datasets reverse iterative volume sample algorithm within constant factor sample leverage remarkable feat volume sample recently polynomial generalize volume sample matrix expectation formula tensor acknowledgment thanks daniel hsu   valuable discussion research NSF grant IIS appendix inductive proof cauchy binet cauchy binet equation matrix det BS det easy generalize volume sample theorem asymmetric version alternate inductive proof denote respectively consists indexed theorem det det BS DEREZINSKI warmuth proof subset rewrite restriction induct det det clearly det otherwise sylvester theorem det  det induction assume det det ind det BS det BS induction subset restriction clearly equality sum appendix alternate proof theorem derivative determinant petersen pedersen symmetric det CX det CX CX CX proof generalize cauchy binet volume sample det isx det derivative det isx isx det det isx det isx isx reverse iterative volume sampling linear regression appendix proof proposition proof construct variant input matrix relate volume standard determinant proposition matrix det det operation replace  another swap recall goal formula det det det proposition assume proposition feature vector denote moreover optimal prediction vector dataset projection onto subspace span feature denote PX define vector def def optimal prediction vector training rank unique vector minimizes loss training lemma lemma achievable loss decompose  proof projection onto subspace span feature vector corresponds denote projection yen construct vector yen closer span projection incur loss along dimension reduce remain dimension corresponds training definition projection onto span PX linearity projection PX PX PX PX PX already therefore vector orthogonal vector PX PX DEREZINSKI warmuth finally projection onto span span vector orthogonal vector pythagorean theorem   definition  conclude proof lemma proof proposition construct matrix vector extra matrix def apply height lemma compute volume span det det  det volume preserve elementary operation proposition prediction vector linear combination coefficient therefore structure perform operation coefficient negative zero def transformation zero transform matrix transformation zero operation entire sequence operation matrix due diagonal structure volume easily described height formula det det det det det combine obtain desire finally cannot perform transformation however matrix volume moreover det det concludes proof proposition reverse iterative volume sampling linear regression