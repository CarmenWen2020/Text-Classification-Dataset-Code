rank matrix corrupt observation fundamental machine application however role information rank matrix attention approach hoc applicable restrictive propose model exploit information rank matrix corrupt observation propose model apply popular scenario matrix completion robust pca furthermore information sample complexity model efficiency improve sufficiently informative information theoretical insight usefulness information model finally conduct comprehensive application relationship prediction semi supervise cluster noisy image classification propose model properly exploit information effective theory keywords information rank matrix corrupt observation matrix completion robust pca introduction rank matrix noisy dimensional complex data important research challenge machine recent data era assume observation model implicit rank structure prevail approach avoid curse dimensionality various rank matrix arise context domain primary challenge namely reliably rank matrix corrupt observation generic framework machine matrix completion robust pca matrix useful important application recommender social network analysis image processing research related rank matrix promising direction exploit information feature notion information naturally application netflix goal movie recommendation user rating popular approach assume user movie rating sample rank matrix however besides rating profile user genre movie possibly leverage information recommendation additional feature available application model incorporate feature rank matrix becomes important issue theoretical practical motivate realization information rank matrix corrupt observation formally described rank model matrix due various matrix contains corrupt observation addition suppose additional feature matrix information denotes feature representation entity instead recover leverage information effectively important application information naturally framework collaborative filter collaborative filter popular machine application aim predict preference user limited rating netflix mention previously traditional approach partial user rating matrix via matrix completion however per user feature per feature information leverage assemble feature representation user becomes framework link prediction link prediction online social network analysis predict recommend implicit friendship user network snapshot approach network snapshot user user relationship matrix relationship snapshot infer conduct matrix completion similarly user specific information user profile user feature deem image denoising another rank matrix application image denoising image image digit image scene rank structure dimensional useful application image recognition information feature interchangeably throughout rank matrix information background subtraction realistic image corrupt sparse shadow brightness saturation dimensional popular approach robust pca construct matrix vector representation image underlie rank subspace sparse feature image label relevant feature underlie dimensional subspace accurately organization information rank matrix corrupt observation focus important  manner information benefit model incorporate information quantify merit information regard perfect information define equation idealize feature fully informative generalize noisy information feature partially correlate information perfect feature extremely useful noisy feature effective benefit model incorporate information construct subsequently information identify precisely argue perfect feature directly transform rank model matrix bilinear respect feature however validity embed becomes questionable feature noisy therefore noisy feature propose rank matrix capture information feature capture information outside feature model learns rank matrix jointly balance information noisy feature observation addition discus connection model wellknown model rank matrix completion robust pca propose model efficiently establish optimization procedure furthermore theoretical analysis justify merit information propose model quantify quality feature corruption rademacher model complexity generalization analysis tighter error bound derive quality feature observation derive sample complexity guarantee matrix completion observation corrupt matrix completion sample complexity suggests propose model asymptotically observation recover rank matrix standard matrix completion feature sufficiently informative substantially generalizes previous information matrix completion jain dhillon guarantee improve complexity perfect feature chiang hsieh dhillon observation corrupt sample complexity guarantee implies quality information useful entry rank matrix corruption severe justify usefulness information propose model theory finally verify effectiveness propose model experimentally various synthetic data additionally apply machine application relationship prediction semi supervise cluster noisy image classification tackle rank model matrix corrupt observation additional feature therefore employ model exploit information achieve performance application demonstrate propose model indeed exploit information various rank matrix contribution information treatment incorporate information rank matrix corrupt observation perfect information propose transform estimate rank matrix bilinear respect feature moreover noisy information propose rank matrix capture feature information plus capture information outside feature therefore conduct efficiently balance information feature observation theoretically justify usefulness information propose model various scenario quantify effectiveness feature sample complexity asymptotically improve sufficiently informative feature comprehensive experimental confirm propose model properly embeds perfect noisy information rank matrix effectively approach previously exclusively noisy information matrix completion perfect information robust pca respectively propose model exploit information broader rank matrix model exploit noisy information robust pca observation corrupt discus comprehensive theoretical experimental demonstrate effectiveness propose treatment rank matrix information exploit information rank matrix discus incorporate information rank matrix corrupt observation introduce formulation exploit perfect noiseless information introduce propose model exploit noisy information finally optimization propose model corrupt observation rank matrix corrupt observation formally underlie rank matrix min rank matrix denote magnitude unknown structure sparse furthermore ωobs entry cardinality  orthogonal projection operator define  xij ωobs otherwise data matrix   goal accurately estimate underlie matrix without loss generality assume ωobs ωobs extension matrix completion assumes observation undersampled noiseless empty intuitive approach estimate rank matrix structural information specifically cand propose via convex program min   lij sij rij ωobs  nuclear norm define sum singular  sij wise norm regularization useful enforce rank structure sparse structure respectively although enjoy theoretical empirical cannot directly leverage information recovery tailor model resolve issue idealize perfect information suppose addition data matrix feature entity information goal rank matrix information exploit addition observation estimate concrete netflix corresponds partial user movie rating matrix correspond user chiang hsieh dhillon movie feature leverage additional feature along rating predict unknown user movie rating principle information useful instance simply random matrix information gain information therefore incorporate perform structural information explore advantage information information ensure informativeness ideal scenario information perfect implicitly describes latent definition perfect information information perfect information noiseless information satisfy col col col col col col denotes  svd perfect information satisfy col col col col indicates exists matrix xmy express target rank matrix bilinear respect feature cast feature min kmk   sij rij ωobs reduce rank matrix bilinear embed respect perfect feature rank matrix already propose matrix completion indeed cast xmy matrix completion obtain inductive matrix completion IMC model underlie matrix sample perfect information discus improve sample complexity IMC detail however obvious weakness bilinear embed assumes information perfect unfortunately application feature perfect noisy weakly correlate latent longer express xmy translate objective becomes questionable weakness empirically recover matrix XM diverge noisy information nevertheless  noisy feature helpful svd  perturbation entry perturbed imperfect feature informative observation motivates model exploit noisy information rank matrix information propose model exploit noisy information introduce improve model exploit imperfect noisy information model balance feature information observation rank matrix specifically propose jointly capture information feature xmy capture information outside feature feature noisy fail latent capture information pure observation however identifiability issue simply expression xmy infinitely satisfy xmy although theory perfectly recover underlie matrix prefer others efficiency intuitively underlie rank prefer xmy rank recover parameter preference pursue rank conceptually subspace subspace effective jointly  estimate xmy pursue rank enables accurately estimate sample parameter advantage formally justified later therefore incorporate noisy information rank matrix corrupt observation propose min ωobs xmy rij    convex surrogate loss underlie matrix estimate XM optimal rank propose objective nuclear norm regularization variable nuclear norm regularization popular heuristic pursue rank structure tightest convex relaxation rank function rank matrix rank  rij CL   CL rnn nuclear norm regularize constraint  relaxed rank  rij rnn propose formulation exploit information rank matrix corrupt observation equivalent convert loss constraint min    xmy rij ωobs easy become learns rank matrix corrupt observation chiang hsieh dhillon without information perfect information respectively suggests model exploit perfect noisy information parameter model crucial contribution feature observation corruption intuitively ratio corrupt observation relative contribution xmy rank estimate therefore appropriate ratio propose model leverage informative feature xmy robust feature remain pure observation discus connection model model various rank matrix connection model matrix completion matrix completion partially entry corrupt therefore propose reduces objective min ωobs xmy rij   model matrix completion feature disregard becomes standard matrix completion objective becomes IMC model estimation rank matrix completely XM however appropriately estimate rank matrix jointly noisy feature XM pure observation therefore improve model exploit noisy information matrix completion refer IMC noisy feature IMCNF justify effectiveness matrix completion connection model robust pca another robust pca ωobs assume entry observation without entry corrupt scenario propose robust pca information convert loss constraint min    xmy reduce robust pca model equivalent pcp solves robust pca purely structural prior suppose information perfect derive pcp rank matrix information model correspond propose model MC IMC IMCNF LRR ωobs entry pcp ωobs entry pcpf ωobs entry PCPNF ωobs entry setting rank matrix model propose perfect feature pcpf objective min   xmy directly estimate bilinear embed XM however pcp pcpf exploit noisy information recovery refer pcp noisy feature PCPNF examine effectiveness leverage noisy information robust pca summarizes rank matrix model propose model discussion convincing treatment various matrix information sufficient intuition parameter important role various circumstance analytically properly parameter quality feature corruption propose model achieve efficient remark practical application feature quality priori therefore recommend parameter via validation parameter rank model estimate entry validation optimization propose alternative minimization scheme propose algorithm algorithm alternatively update variable fix others iteration update variable via variable minimization sub algorithm apply coordinate descent algorithm convex continuous function model originally propose constrain equivalent constraint become instance propose simplicity loss chiang hsieh dhillon algorithm alternative minimization loss input matrix feature matrix tmax max iteration output estimate rank matrix arg minm ωobs xmy  arg  ωobs nij xmy  arg  ωobs sij xmy  converge tmax xmy cyclic coordinate descent algorithm guaranteed converge global minimum tseng compact satisfied briefly discus optimization subproblems algorithm max thresholding operator denotes wise similarly thresholding operator singular    svd fix minimization becomes standard IMC objective matrix typical proximal gradient descent update   xmy rate feature dimension entity therefore relatively inexpensive compute svd matrix proximal fix subproblem becomes standard matrix completion matrix xmy principle algorithm matrix completion nuclear norm regularization subproblem singular thresholding algorithm proximal gradient descent apply active subspace selection algorithm matrix completion efficiently finally minimize fix   xmy therefore ωobs theoretical analysis information theoretical analysis justify usefulness information model focus sample complexity analysis model aim exploit information accomplish possibly corrupt observation analysis generalization error estimate entry associate rank matrix information sample model complexity model complexity related quality feature sparse error feature quality generalization error sample complexity guarantee concentrate analysis detailed proof theorem corollary lemma appendix generalization bound propose model equivalent constrain min ωobs xmy rij kmk   analysis assume entry ωobs sample unknown distribution index entry upper bound constant CL  circumstance consistent scenario netflix user rate movie feasible kmk   feasible    estimation function parameterized vector axis FΘ feasible function interested empirical risk quantity Rˆ define Rˆ ωobs rij context model  arg minf FΘ Rˆ classic generalization error bound risk Rˆ along measurement complexity model lemma typical bound lemma lipschitz loss function bound respect argument constant FΘ rademacher model complexity function FΘ ωobs define FΘ sup FΘ  probability probability FΘ Rˆ  FΘ observation sample sample replacement model  shamir shalev shwartz sample procedure bernoulli model chiang hsieh dhillon therefore guarantee Rˆ rademacher model complexity  FΘ carefully introduce lemma model complexity related feature quality sparse quality feature model complexity intuition goodness feature quality motivate imperfect information violates feature perturbed mislead correlate latent however feature effective weaken latent information portion latent informative feature somewhat informative helpful recover matrix formally FΘ model complexity  FΘ bound lemma lemma maxi  maxi  max max suppose convex surrogate loss satisfy lemma lipschitz constant FΘ model complexity  FΘ upper bound MX min CL constant lemma lemma carefully construct feasible Rˆ  FΘ reasonably witness thresholding operator otherwise addition  reduce svd   informative informative denote define similarly propose    Mˆ arg minm  XT XT  optimal approximate informative feature lemma trace norm Mˆ function lemma fix constant define min mini  mini  constant define lemma trace norm Mˆ upper bound  cld CL maxi  constant upper bound entry rank matrix information therefore combine lemma derive generalization error bound theorem suppose convex surrogate loss function lipschitz constant bound respect argument assume constraint fix probability risk optimal bound min CL cld CL constant lemma theorem deem measurement feature quality respectively feature quality observation corruption risk quantity measurement consistent intuition feature quality feature latent informative feature  absorb theorem discus information propose model sample complexity important scenario comparison fix feature dimension function discussion sample complexity matrix completion matrix completion observation partial corrupt mention model reduce IMCNF equivalently exploit noisy information matrix completion addition theorem derive sample complexity IMCNF corollary suppose aim approximately recover partial observation  XM  arbitrary constraint min sample sufficient guarantee estimate rank matrix XM recovers probability sufficiently corollary suggests sample complexity IMCNF lower aid sufficiently informative noisy information significance explain sample complexity model feature perfect corollary suggests IMCNF model sample recovery coincides sample complexity IMC researcher perfect feature observation chiang hsieh dhillon recovery however IMC guarantee recovery feature perfect corollary suggests recovery attainable IMCNF min sample analysis suggests sample complexity IMCNF feature apply inequality corollary   CL rank  CL matrix entry explain sample complexity standard matrix completion information glance pure matrix completion matrix completion guarantee  distributional achieve polylog sample complexity approximate recovery recovery however polylog additional distributional assumption entry analysis distributional assumption fairer comparison shamir shalev shwartz pure matrix completion entry sufficient approximate recovery without distributional assumption furthermore bound tight distributional assumption entry therefore corollary indicates IMCNF pure matrix completion distribution reasonable matrix completion bound feature completely useless feature random matrix information exactly standard matrix completion cannot matrix completion bound feature account however application feature random corollary theoretical insight noisy feature useful matrix completion indeed feature informative sample complexity IMCNF model asymptotically standard matrix completion concrete scenario rank matrix generate random orthogonal model corollary generate random orthogonal model random orthogonal singular arbitrary magnitude singular limn noisy feature basis orthogonal sample sufficient IMCNF achieve recovery corollary suggests random orthogonal model feature noisy perturbs subspace associate singular sample complexity IMCNF asymptotically bound standard matrix completion matrix completion observation partial uncorrupted propose reduces IMCNF model moreover corollary rank matrix information suggests attain recovery efficiently exist model exploit noisy informative information sample complexity partial corrupt observation observation corrupt presence corruption theorem corollary matrix XM sufficient observation sample depends quality feature sparse error exists PΩ generalization bound theorem corollary implies XM entry ωobs recover entry underlie rank matrix error moreover apply propose algorithm constrain satisfy PΩ automatically formally recovery guarantee partial corrupt observation corollary suppose data matrix  corrupt observation along information constraint apply algorithm equivalent min sample sufficient guarantee probability XM rij sufficiently satisfies PΩ corollary suggests observation corrupt guarantee rank matrix XM accurate entry sample depends quality feature corruption addition complexity guarantee intuitive matrix completion sample corrupt harder therefore increase sample complexity however suppose corruption severe magnitude error corollary non trivial bound sample entry accurately furthermore quality feature becomes helpful faster corruption suppose corruption budget upper bound sample complexity min implies sample sufficiently feature sample feature remark overall sample complexity analysis justify model information effectively leverage information analysis generalization bound informative information corruption sample accurate estimation justify usefulness information emphasize relatively loose recovery guarantee matrix completion cand tao cand  chiang hsieh dhillon robust pca approximate recovery entry however important recovery guarantee additional assumption incoherence underlie rank matrix distributional assumption ensure sample observation sufficiently representative analysis distributional incoherence assumption generalization analysis ensure average loss entry sufficiently therefore average loss wrongly estimate incoherence however circumstance argument justify usefulness information recovery context robust pca observation grossly corrupt exploit perfect information amount rank matrix cannot recover standard robust pca without feature exactly recover propose model interested reader consult detail theoretical analysis information improve recovery guarantee rank matrix research direction explore future experimental experimental exploit information propose model various rank matrix synthetic model performs aid information observation matrix completion corrupt robust pca corrupt application machine application relationship prediction semi supervise cluster noisy image classification  model matrix corrupt entry information apply model achieve performance application synthetic usefulness perfect noisy information model synthetic setting matrix completion examine information model matrix completion rank matrix UV latent uij vij randomly sample ρobs entry ωobs matrix  addition construct perfect information satisfy generate quality feature parameter derive replace rank matrix information feature relative error SVDfeature MC IMC IMCNF ρobs feature relative error SVDfeature MC IMC IMCNF ρobs feature relative error SVDfeature MC IMC IMCNF ρobs sparsity observation relative error SVDfeature MC IMC IMCNF sparsity observation relative error SVDfeature MC IMC IMCNF sparsity observation relative error SVDfeature MC IMC IMCNF performance various matrix completion fix sparsity observation ρobs upper fix feature quality feature perform standard matrix completion MC perfect feature however IMCNF sensitive feature increase exploit information noisy feature chiang hsieh dhillon orthogonal recover underlie matrix propose IMCNF model instance exploit noisy information matrix completion IMCNF standard trace norm regularize matrix completion MC IMC SVDfeature recover matrix algorithm evaluate standard relative error   parameter report recovery average random trial ρobs upper IMC SVDfeature perform similarly ρobs moreover performance mainly depends feature quality affected observation although performance comparable IMCNF perfect feature performance quickly feature become noisy phenomenon noisy feature IMC SVDfeature easily trap feature perform pure MC another feature quality IMC SVDfeature fails achieve relative error observation increase IMC sensitive feature cannot guarantee recoverability feature perfect performance IMCNF improve feature observation informative feature achieve error MC sensitive feature IMC SVDfeature empirically analysis robust pca examine perfect noisy information propose model robust pca rank matrix UV uij vij sparse matrix entry non zero entry probability non zero entry probability construct noisy feature parameter construction previous feature span recover rank matrix fully matrix along noisy information pcp exploit feature pcpf theoretically exploit perfect feature bilinear embed PCPNF incorporate noisy information pcpf PCPNF instance propose model exploit information robust pca relative error criterion evaluation rank matrix information feature relative error pcp pcpf PCPNF feature relative error pcp pcpf PCPNF feature relative error pcp pcpf PCPNF performance various robust pca feature sparsity corruption PCPNF noisy informative feature recovery performance feature quality feature perfect pcpf PCPNF exactly recover underlie matrix pure pcp fails recover confirms PCPNF pcpf leverage perfect feature recovery however feature become noisy pcpf quickly performs mislead feature PCPNF exploit noisy feature recovery PCPNF recovers noisy reasonably feature whereas pcp pcpf fail recover PCPNF advantage noisy information corrupt observation corrupt observation examine extent information model observation corrupt construction previous generate perfect feature matrix observation ωobs randomly sample ρobs entry index  matrix goal therefore recover along information exploit advantage information propose model zero exploit perfect feature recover structural information ρobs becomes robust pca fully matrix reduces pcp model reduces pcpf objective respectively aspect refer pcp partial observation pcp model pcpf partial observation pcpf relative error criterion evaluate recover matrix regard recovery successful error parameter  recoverability pcp pcpf rank sparsity ρobs chiang hsieh dhillon ρobs ρobs ρobs performance pcp pcpf perfect feature recover corrupt observation ρobs respectively achieve recovery fail pcpf achieves recovery leverage perfect feature pcpf recover corrupt observation apply obtain estimate rank matrix grid recovery attain fail recovery cannot attain pcp attain pcpf grid marked ρobs exists substantial matrix recover pcpf corrupt entry exploit information propose model recover amount matrix cannot recover information application application relationship prediction network semisupervised cluster noisy image classification cast rank matrix corrupt entry additional information consequence rank model matrix propose model achieve performance application model exploit information relationship prediction network relationship prediction online review website epinions review trust distrust others review social network model network treat entity trust distrust relationship model positive negative entity relationship prediction network predict unknown relationship user network snapshot rank matrix information IMCNF IMC MF ALS hoc hoc accuracy auc relationship prediction epinions network noisy user feature IMC performs without feature MF ALS hocs IMCNF outperforms others successfully exploit noisy feature propose approach rank model conduct matrix completion adjacency matrix matrix prediction however developed network structure therefore feature user available extend rank model incorporate user feature completion setup described data user relationship relationship distrust addition trust information user feature matrix user dimensional feature user review positive negative review user prediction cycle hoc hoc rank model matrix factorization completion LR ALS prediction network structure without user feature extend rank model completion replace IMCNF IMC incorporate user feature implicitly prediction entity user IMCNF IMC randomly network fold conduct fold validation fold training fold validation parameter validation chosen average accuracy auc report IMC performs LR ALS IMC feature account user feature partially related relationship matrix IMC mislead noisy feature IMCNF performs prediction performs slightly LR ALS accuracy auc IMCNF exploit weakly informative feature prediction without trap feature semi supervise cluster semi supervise cluster another application translate  matrix partial observation feature matrix item pairwise constraint specify item dissimilar goal cluster item item within cluster sub optimally constraint feature information traditional cluster algorithm purely feature item chiang hsieh dhillon obtain cluster purely pairwise constraint matrix completion similarity matrix construct constraint sij item dissimilar similarity unknown cluster item becomes equivalent cluster graph goal item denote node within positive negative apply matrix completion approach propose graph cluster conduct matrix completion eigenvectors obtain cluster node apparently feature constraint optimal semi supervise cluster algorithm propose item feature constraint account metric approach spectral kernel algorithm algorithm approach essentially solves semisupervised cluster IMC objective pairwise constraint sample entry matrix uut cluster membership matrix  IMC objective furthermore matrix ideally subspace span conduct eigenvectors matrix obtain cluster however IMC performance heavily depends quality feature therefore propose replace IMC IMCNF matrix completion eigenvectors matrix obtain cluster target rank matrix describes similarity item algorithm improve version handle noisy feature algorithm graph cluster matrix completion  data mushroom covtype classification benchmark feature truth label item available truth cluster item define truth label statistic data summarize data randomly sample pairwise constraint input constraint feature algorithm obtain cluster cluster index item evaluate pairwise error truth cluster item cluster various constraint data mushroom data feature perfect training accuracy attain linear svm classification data available http csie ntu edu cjlin  datasets covtype subsample entire data cluster balance rank matrix information item feature dimension cluster mushroom covtype statistic semi supervise cluster data mushroom pairwise error  IMCNF pairwise error  IMCNF covtype pairwise error  IMCNF performance various semi supervise cluster data mushroom data feature perfect IMCNF output truth cluster error rate covtype feature noisy IMCNF model outperforms error decrease constraint IMCNF obtain truth cluster error rate indicates IMC indeed effective perfect feature covtype data performance dominate feature quality although benefit constraint information outperforms clearly constraint performance improve constraint increase error rate  decrease increase however disregard feature suffers error rate constraint finally IMCNF combine advantage  feature constraint leverage constraint information avoid trap feature therefore carefully handle information IMCNF model improve semi supervise cluster algorithm noisy image classification finally noisy image classification application rank matrix corrupt observation correlate image pixel corrupt task denoise image classify image correctly underlie image correlate implicit rank structure standard robust pca identify sparse recover rank approximation image however dimensional feature image available source suppose image principal component chiang hsieh dhillon linear svm classifier kernel svm classifier noisy pcp pcpf noisy pcp pcpf digit classification accuracy pcp pcpf eigendigit feature accuracy noisy accuracy denoised image pcp pcpf achieve accuracy noisy image pcpf outperforms pcp incorporate eigendigit feature  feature feature helpful denoising motivate realization multiclass classification noisy image mnist data data training image image image handwritten digit dimensional vector pre multiclass linear kernel svm classifier training image perturb image generate noisy image precisely image denotes pixel denotes image construct sparse matrix entry randomly picked corrupt noisy image min exploit feature entity denoise noisy image classification exploit eigendigit feature exploit eigendigit feature denoising training image eigendigit feature pca simply feature input pcp derive denoised image pcp input pcpf derive another denoised image pcpf XM pcp pcpf rank approximation image although eigendigit feature satisfy assume derivation pcpf heuristically incorporate pcpf circumstance unbiased information rank approximation digit quality denoised image pcp pcpf input pcp pcpf pre SVMs digit classification report somehow effective denoising sparse accuracy achieve denoised image closer image noisy image furthermore pcpf consistently achieves accuracy pcp incorporate eigendigit feature pcpf helpful denoising classification exploit eigendigit label relevant feature addition eigendigit feature exploit feature entity ideally rigorously truth rank approximately rank rank matrix information accuracy pcp pcpf pcpf PCPNF linear svm accuracy pcp pcpf pcpf PCPNF linear svm accuracy pcp pcpf pcpf PCPNF linear svm accuracy pcp pcpf pcpf PCPNF kernel svm accuracy pcp pcpf pcpf PCPNF kernel svm accuracy pcp pcpf pcpf PCPNF kernel svm digit classification accuracy various eigendigit  feature construct label relevant feature quality PCPNF exploit noisy label relevant feature feature relevant information image extremely useful classification generate label relevant feature entity perfect feature matrix indicator vector digit contains truth label information randomly shuffle correspondingly image noisy relevant information finally feature span quality depends parameter label relevant feature approach denoising baseline pcp pcpf eigendigit feature previous label relevant feature account moreover pcpf PCPNF incorporate eigendigit feature label relevant feature denoising pcpf PCPNF emphasize embed label relevant feature apply denoise noisy image examine quality denoised image accuracy achieve pre SVMs fix sparsity recover image quality perfect label relevant feature extremely useful recover image chiang hsieh dhillon pcpf PCPNF achieve accuracy image report however increase pcpf quickly fails accuracy drastically accuracy become PCPNF performs pcpf exploit noisy label relevant feature achieves accuracy pcpf pcp demonstrate effectiveness propose model exploit noisy information related rank matrix imperfect observation expansive domain machine fundamental principal component analysis pca matrix completion rank matrix robust pca topic independent research burgeon recent focus usefulness information rank matrix observation partial corrupt theoretical practical aspect rank matrix partial observation matrix completion successfully apply machine task recommender social network analysis cluster theoretical foundation establish strike recovery guarantee cand tao cand  author polylog observation sufficient recovery probability entry uniformly sample random recovery non uniform distributional assumption  setting noisy observation research information matrix completion although feature helpful application mainly focus non convex matrix factorization formulation without theoretical analysis information recently inductive matrix completion IMC objective incorporate information IMC trace norm regularization recovery achieve IMC sample complexity perfect feature however imperfect feature IMC cannot recover underlie matrix suffer performance observation develop improve model exploit noisy information robust pca another prominent instance rank matrix imperfect observation goal recover rank matrix matrix entry arbitrarily corrupt sparse sparse structure application image processing bioinformatics researcher investigate approach robust pca theoretical guarantee remarkable milestone guarantee cand author mild rank sparse structure exactly distinguishable extension robust pca robust pca sparse error data compress data however unlike matrix completion research directly exploit information robust pca advantage information robust pca unexplored extend analysis information matrix completion robust pca similarity robust pca essentially harder matrix completion aspect matrix completion mostly estimation emphasis accurately recover entry  partial observation robust pca matrix separation identify corrupt entry  observation difference naturally precludes extension analysis matrix completion robust pca nevertheless recently perfect feature recovery rank matrix becomes attainable robust pca information robust pca exploit extend develop model exploit noisy information robust pca another model similarity robust pca information rank representation LRR emerge subspace cluster data matrix corrupt sparse error LRR model assumes underlie rank matrix linear combination interestingly LRR propose pcpf model serf feature LRR incorporate feature recovery conclusion effectiveness information rank matrix corrupt observation propose model incorporates perfect noisy information balance information feature observation simultaneously derive instance model IMCNF PCPNF matrix completion robust pca leverage information addition formal analysis justify effectiveness information propose model quantify quality feature sample complexity asymptotically improve sufficiently informative feature analysis therefore quantifies merit information model rank matrix theory finally verify model synthetic machine application relationship prediction semi supervise chiang hsieh dhillon cluster noisy image classification application rank matrix corrupt observation additional feature employ model competitive algorithm performance comparable approach consistently demonstrate propose model learns rank matrix corrupt observation effectively properly exploit information acknowledgment acknowledge research CCF IIS CCF appendix proof preliminary lemma introduce lemma proof lemma lemma bound rademacher complexity function bound trace norm norm respectively lemma kwk maxi  sup   proof lemma directly lemma lemma kwk exe vector sup  proof lemma theorem  maxa rank matrix information proof lemma proof standard rademacher contraction principle lemma  zhang bound FΘ FΘ sup xmy  sup kmk   sup    sup    mmax  inequality derive apply lemma lemma moreover maxi  maxj  maxi  upper bound FΘ FΘ MX however circumstance bound loose sample complexity analysis shamir shalev shwartz derive tighter bound trace norm residual rewrite FΘ FΘ sup FΘ  sup FΘ  rij  trick shamir  ωobs threshold later formally hij define aij  hij otherwise bij hij  otherwise construction therefore FΘ FΘ sup FΘ aij rij sup FΘ bij rij rij upper bound sup FΘ aij rij aij chiang hsieh dhillon inequality derive apply lemma shamir shalev shwartz rademacher contraction principle upper bound sup FΘ  sup kmk   sup   sup   upper bound lemma lemma precisely upper bound sup kmk   sup kmk   MX upper bound sup    addition apply inequality upper bound sup   sup   kbk CL inequality derive apply lemma shamir shalev shwartz therefore upper bound CL obtain another upper bound FΘ FΘ MX CL lemma combine proof lemma proof kxt    singular therefore definition Mˆ   XT XT    rank matrix information   non zero singular furthermore construction     bound  kxk kxk min   therefore bound      lemma conclude  CL proof theorem proof directly plug lemma lemma Rˆ  instance Rˆ proof theorem proof matrix completion XM  therefore directly theorem proof theorem proof construction rewrite   therefore upper bound  TL LV    moreover suppose limn limn chiang hsieh dhillon contradiction definition therefore conclude  theorem plug bound theorem proof theorem proof sample complexity directly theorem PΩ directly construction algorithm