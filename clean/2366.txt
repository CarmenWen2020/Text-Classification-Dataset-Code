scalable machine data important attention recent popular distribute environment hadoop cluster commodity machine communication substantial algorithm suitably novel approach distribute training linear classifier involve smooth loss regularization reduce communication iteration node minimize locally approximate objective function minimizers combine descent direction approach freedom formation approximate objective function choice convergence iterative parameter instantiation yield parallel stochastic gradient descent convergence communication node faster  distribute solver statistical query model computes function gradient distribute fashion evaluate recent distribute demonstrate superior performance keywords distribute partition regularization introduction recent machine data become important web related application commonly application data mining amount user data application usually decentralize fashion cluster commodity machine node communication node significantly application involve communication click prediction advertisement data feature billion geo distribute data across data extremely setting partition node distribute machine algorithm data usually iterative iteration involves computation happens locally node iteration communication information node distribute distribute reduce framework involve additional operation per iteration load data disk ram recent framework spark reef avoid unnecessary load data disk communication node iteration unavoidable substantial data therefore development efficient distribute machine algorithm minimize communication node important algorithm minimize iteration distribute batch training linear classifier feature data matrix sparse partition node loss function convex differentiable regularizer employ involves unconstrained minimization convex differentiable objective function vector minimization usually perform iterative descent iteration computes direction satisfies sufficient angle descent angle vector performs along direction tdr arg minw contribution proof convex satisfies additional weak assumption global linear rate convergence glrc satisfy iteration theme flexibility convergence allows useful distribute computation communication capability effective distribute viz SQM statistical query model batch gradient descent gradient compute distribute node compute gradient glrc efficient distribute algorithm component correspond aggregation component interested communication node relative computation node iterative algorithm SQM training training cmp com iter cmp com respectively computation communication per iteration iter iteration com optimal algorithm SQM cmp due iter scenario useful iteration computation node iteration hence communication decrease reduce compute effort literature reduce amount communication node node approximation optimization iteration local decrease average iterate iteration refer parameter PM alternatively iteration refer iterative parameter  convergence theory inadequate prompt devise  another dual distribute fashion denote dual vector associate node optimize parallel combination individual direction generate overall tend convergence detail novel iterative descent mention distribute algorithm positively distribute computation generate direction gradient SQM iteration node iterate gradient information node function ˆfp approximates satisfies ˆfp effective suggestion ˆfp outside node approximation objective function associate data node suggestion ˆfp ˆfp optimize within node glrc trust BFGS etc optimize ˆfp fully constant local node approximate minimizer feature dimension application gain performance feature expand via feature combination explicit expansion nonlinear kernel etc mahajan agrawal keerthi sellamanickam bottou ˆfp direction satisfies sufficient angle descent convex combination direction generate node overall direction iteration satisfies overall distribute satisfy reduces communication SQM intuition ˆfp approximation global direction minimize towards faster SQM summary contribution convex establish glrc iterative descent important propose distribute algorithm converges  convergence efficient SQM communication flexible local optimization node another contribution associate wise stochastic gradient descent sgd inherently sequential parallelize employ sgd local optimizer obtain parallel sgd performance convergence contribution detail summarize subsection validate theory benefit dimensional datasets communication bottleneck discussion unexplored possibility extend distribute conclude descent continuously differentiable function convex gradient satisfy assumption lipschitz continuous   essentially happens twice continuously differentiable upper bound eigenvalue hessian convex function strongly convex kwk convex machine convex risk functionals regularization kwk strongly convex strongly convex satisfies assumption tdr standard   wolfe  future extend theory developed non differentiable convex function sub gradient efficient distribute algorithm algorithm descent exit direction satisfy tdr satisfies  wolfe descent algorithm minimize algorithm proof appendix lemma suppose tdr unique     theorem arg minw glrc satisfy iteration upper bound proof theorem appendix interested convergence easy establish assumption theory classical wolfe glrc harder exist proof gradient descent glrc wang lin applicable descent equation wang lin gradient descent theorem entirely surprising exist literature remark important rate convergence upper bound theorem pessimistic descent algorithm batch gradient descent rate convergence therefore  convergence rate actual rate convergence rate distribute FADL empirical comment remark distribute training discus detail distribute training algorithm training associate binary classification assumption implies unique mahajan agrawal keerthi sellamanickam bottou linear classification model sgn continuously differentiable loss function lipschitz continuous gradient allows loss function logistic loss exp hinge loss max hinge loss theory non differentiable suppose training distribute node index sits node loss associate node loss node aim minimize regularize risk functional kwk kwk regularization constant easy lipschitz continuous approach distribute descent algorithm  architecture partition node distribute compute compute gradient direction iteration gradient communicate node direction node construct approximation information available node ˆfp approximately optimizes chosen convex combination along direction completes iteration involves distribute computation inexpensive detail subsection ˆfp ˆfp outer iteration mention avoid  notation choice ˆfp discus subsection ˆfp depends iterate ˆfp flexibility ˆfp optimize ˆfp satisfy ˆfp strongly convex lipschitz continuous gradient satisfies gradient consistency ˆfp ˆfp strongly convex easily regularizer ˆfp implies ˆfp ˆfp ˆfp  allreduce arrangement node efficient distribute algorithm gradient consistency motivate satisfy angle obtain optimize ˆfp reasonable assume ˆfp ˆfp convex combination later formalize yield precisely approximate functional ˆfp ˆfp kwk Lˆ approximation Lˆ approximation choice within node possibility maintain communication efficiency Lˆ explicitly outside node satisfy Lˆ lipschitz continuous gradient aid satisfy gradient consistency appropriate linear choice ˆfp linear approximation Lˆ taylor series Lˆ zeroth omit everywhere constant role optimization  locally computable node comment hybrid approximation improvement linear approximation quadratic Lˆ loss loss average loss quadratic approximate quadratic Lˆ hessian corresponds subsampling approximate hessian utilize local subsampling hessian approximation effective optimization machine quadratic approximation pure quadratic variant approximation Lˆ comment earlier goodness subsampling hessian hybrid approximation applies mahajan agrawal keerthi sellamanickam bottou nonlinear approximation approximate Lˆ somewhat approximation algorithm deterministic monotone descent algorithm gradient consistency essential establish function descent respect algorithm BFGS approximation approximation approximation Lˆ positive semi definite matrix diagonal approximation limited gradient BFGS approximation remark distribute described instance algorithm theorem theorem mention convergence rate yield rate rate obviously pessimistic applies choice ˆfp satisfy minimal assumption actual rate convergence choice ˆfp suppose ˆfp via hybrid quadratic nonlinear approximation choice mention subsection minimize ˆfp exactly inner optimization approximation invariant coordinate transformation positive definite matrix  unaffected transformation iteration without algorithm analysis coordinate transformation rate linear approximation choice ˆfp enjoy explains hybrid quadratic nonlinear approximation perform rate convergence subsection convergence rate FADL approximation future evaluate approximation detail convergence theory exactly minimize ˆfp infeasible convergence minimizer ˆfp direction satisfies angle chosen discus minimizer ˆfp appendix approximation optimizer glrc minimize ˆfp constant iteration satisfy sufficient angle descent efficient distribute algorithm lemma assume suppose minimize ˆfp optimizer generates sequence glrc ˆfp ˆfp ˆfp exists depends lemma combine theorem yield convergence theorem theorem suppose satisfies lemma iteration iteration apply minimize ˆfp distribute converges satisfy proof lemma theorem appendix practical implementation refer acronym FADL function approximation distribute numerical optimization replace actual usage algorithm terminate  satisfied tdr loss  compute distribute computation tdr derivative respect cheap involve computation involve data explore cheaply approximate optimization decent identify interval lemma backward minimizer tdr  minimizer approximately finally terminate fix computation communication effectively algorithm FADL mention distribute communication computation involve choice deterministic glrc BFGS tron primal coordinate descent etc glrc expectation convergence theorem interpret probabilistic nicely connects recent literature parallel sgd discus subsection briefly outside scope related detail optimize quadratic approximation conjugate gradient non quadratic approximation FADL nonlinear solver tron connection parallel sgd machine wise stochastic gradient descent sgd variation dual update scan mahajan agrawal keerthi sellamanickam bottou algorithm FADL function approximation distribute com communication cmp computation agg aggregation optimizer minimize ˆfp compute com cmp data agg exit  parallel iteration convex combination agg compute com cmp pas data com cmp agg tdr derivative wrt tdr coordinate ascent perform however wise inherently sequential employ sgd local optimizer essence parallel sgd however parameter iterative parameter briefly convergence theory limited complicate analysis limited unanswered parallel sgd convergence glrc instantiation distribute local optimization variation sgd glrc expectation johnson zhang related convergence probabilistic strongly convergent parallel sgd observation machine version instantiation variance reduce sgd johnson zhang discus connection svrg ˆfp node define npl kwk easy ˆfp efficient distribute algorithm sgd update apply ˆfp precisely update svrg node implementation sgd update optimize ˆfp svrg johnson zhang motivate update variance reduction derive functional approximation viewpoint computation communication tradeoff subsection rough analysis understand FADL faster SQM analysis understand role various parameter precise comparison SQM FADL computation optimize ˆfp node FADL outer iteration clearly FADL attractive communication feature dimension distribute compute environment specific implementation choice easy rough analysis understand FADL efficient SQM distribute grid node allreduce tron implement SQM FADL assume outer SQM outer FADL outer FADL outer SQM outer iteration SQM FADL rough analysis SQM FADL appendix detail FADL faster satisfied nonzero data feature dimension relative communication computation node inner iteration FADL dimension sparsity data FADL SQM demonstrate effectiveness exist distribute training data discus experimental setup briefly overall applies mainly choice function approximation ˆfp subsection detail choice finally detail setting clearly demonstrates scenario performs subtle apply svrg ˆfp corresponds sgd former assures glrc expectation mahajan agrawal keerthi sellamanickam bottou experimental setup hadoop cluster node gbit interconnect node intel xeon processor ghz iteration traditional mapreduce setup disk access allreduce binary mapper communication bandwidth gbps gigabit per sec dataset feature non zero kdd url webspam mnistm rcv datasets data publicly available datasets kdd url webspam mnistm rcv feature nonzero data matrix regularizer regularizer dataset chosen optimal performance validation datasets mainly illustrate validity theory utility distribute machine scenario data datasets typically kdd url webspam dimensional mnistm rcv medium dimensional datasets useful communication partition distribute mainly dependent appendix datasets somewhat communication hinge loss function unless differently numerical optimization trust newton tron propose evaluation criterion relative difference optimal function precision recall curve AUPRC evaluation criterion former calculate optimal function obtain tera algorithm iteration comparison pipelined version hence incur extra multiplicative logp communication datasets available http csie ntu edu cjlin  datasets mnistm binary others employ AUPRC instead auc differentiates finely efficient distribute algorithm tera  tera representative SQM ofthe distribute solver therefore important baseline ADMM partition formulation alternate direction multiplier ADMM ADMM dual primal however solves approximate node iteratively batch cocoa distribute dual outer iteration parallel local dual optimization dane newton described function approximation FADL disco distribute newton communication efficiency FADL described detail specifically algorithm tera attractive tera outer iteration remains constant respect distribute node recommend local vector per node minimize local objective function node epoch sgd optimal chosen sgd subset data average node per feature basis explain average vector tera LBFGS trainer whereas tron bias tera LBFGS tera tron progress objective function choice clearly tera tron superior behavior datasets restrict tera tron simply refer tera ADMM ADMM objective function quadratic proximal augment lagrangian penalty parameter performance ADMM sensitive hence choice crucial inexpensive initialization FADL ADMM applicable cocoa cocoa primal objective function others mahajan agrawal keerthi sellamanickam bottou sec rel func diff tera LBFGS tera tron kdd node sec rel func diff tera LBFGS tera tron kdd node plot efficiency tera kdd theory approach adapt iteration equation refer choice  recently deng yin linear rate convergence ADMM assumption ADMM function analysis objective function analytical formula theoretical linear rate constant refer choice analytic choice ADMM analytic neighborhood ADMM iteration objective function additional ADMM progress training objective function choice kdd analytic magnitude choice however significant amount initial spent overall approach  performer choice observation datasets choice ADMM fix  refer ADMM  simply ADMM worth comment related ADMM apart ADMM   discus classic optimization separable convex program proximal augment lagrangian distribute training linear classifier ADMM gauss seidel jacobi   related feature partition partition scenario therefore efficient distribute algorithm sec rel func diff ADMM  ADMM ADMM analytic kdd node sec rel func diff ADMM  ADMM ADMM analytic kdd node plot efficiency ADMM kdd cocoa cocoa parameter approximation inner iteration project dual sub epoch coordinate dual ascent inner iteration crucial role choice progress objective function kdd choice node choice epoch reasonably consistently datasets node fix choice refer simply cocoa primal objective function decrease continuously dual monotone descent objective function assure dane detailed comparison later another dane resemblance FADL newton described spirit function approximation nonlinear approximation briefly subsection dane non monotone fix probabilistic convergence theory dane parameter function approximation coefficient proximal define direction convergence choice practical recommendation choice sub optimal datasets tune dataset improve dane initial tune randomly shuffle presentation epoch inner iteration comment ADMM dual objective function ADMM kdd mahajan agrawal keerthi sellamanickam bottou sec rel func diff cocoa cocoa cocoa kdd node sec rel func diff cocoa cocoa cocoa kdd node plot efficiency cocoa setting kdd initial outer iteration improvement objective function direction improvement outer iteration fix chosen remain iteration associate tune overall dane later subsection spite tune dane converge situation essentially issue adaptive dane choice stage training nearly akin outer iteration oppose dane FADL monotone directly proximal restrict unlike dane parameter FADL fix default datasets choice inner optimizer dane crucial efficiency svrg tron svrg node detailed svrg choice fix epoch local inner optimization epoch local inner optimization local computation node communication choice dane FADL recall subsection various choice ˆfp implement BFGS approximation future focus linear approximation quadratic hybrid nonlinear choice implementation described subsection algorithm efficient distribute algorithm sec rel func diff FADL quad FADL hybrid FADL  kdd node sec rel func diff FADL quad FADL hybrid FADL  kdd node plot efficiency function approximation FADL kdd progress training objective function various choice ˆfp choice quadratic approximation ˆfp performance although hybrid nonlinear approximation reasonably consistently datasets hence subsection FADL quadratic approximation analysis simply refer FADL hereafter quadratic approximation hybrid nonlinear approximation precise argument outer iteration function approximation mainly direction recall subsection choice approximation Lˆ meaning linear nonlinear quadratic Lˆ possibly direction direction sensitive nonlinear approximation quadratic approximation become severe node becomes literature quadratic approximation robustness subsampling hessian computation worsen direction comparison FADL disco disco inexact damped newton inexact newton compute distribute precondition conjugate gradient tera belongs SQM disco described zhang xiao hinge loss therefore subsection separately FADL disco logistic loss communication efficiency communication progress objective function function communication FADL clearly disco mahajan agrawal keerthi sellamanickam bottou difference datasets kdd compute superiority FADL plot communication disco extensive computational FADL within communication pas comparison FADL tera ADMM cocoa dane choice setting evaluate FADL tera ADMM cocoa dane detail plot detail performance varies node communication plot variation training objective function function communication axis prefer communication instead outer iteration latter meaning former uniform plot respectively dimensional datasets medium dimensional medium datasets mnistm rcv plot variation training objective function function actual plot respectively dimensional medium dimensional datasets tera tera establish baseline useful fare relative tera function node performance generalization performance finally quantity within steady AUPRC achieve perfect training communication plot ratio tera correspond function node ratio ratio faster tera plot datasets plot plot dane later due extra tune proximal parameter rate convergence analysis rate convergence behavior training objective function respect communication useful clearly predict theory rate convergence linear efficient distribute algorithm communication rel func diff FADL disco kdd node communication rel func diff FADL disco kdd node communication rel func diff FADL disco url node communication rel func diff FADL disco url node communication rel func diff FADL disco webspam node communication rel func diff FADL disco webspam node plot communication disco FADL datasets mahajan agrawal keerthi sellamanickam bottou communication rel func diff FADL cocoa tera ADMM dane kdd node communication rel func diff FADL cocoa tera ADMM dane kdd node communication rel func diff FADL cocoa tera ADMM dane url node communication rel func diff FADL cocoa tera ADMM dane url node communication rel func diff FADL cocoa tera ADMM dane webspam node communication rel func diff FADL cocoa tera ADMM dane webspam node plot rate convergence various dimensional datasets efficient distribute algorithm communication rel func diff FADL cocoa tera ADMM dane mnistm node communication rel func diff FADL cocoa tera ADMM dane mnistm node communication rel func diff FADL cocoa tera ADMM dane rcv node communication rel func diff FADL cocoa tera ADMM dane rcv node plot linear convergence various medium dimensional datasets tera distribute computation compute gradient plot unaffected plot difference plot initialization average pas sgd node initialization node due variance objective function FADL rate steeper steeper behavior node functional approximation node becomes node decrease recall aim function approximation reduce communication significantly plot clearly confirm reduction mahajan agrawal keerthi sellamanickam bottou convergence rate ADMM generally rate convergence initial stage training useful behavior generalization AUPRC tend achieve steady quickly stage usefulness dane rate convergence associate tune tune affect rate convergence dane tends unstable overall FADL rate convergence training stage stage tera cocoa ADMM dane FADL reduction communication tera node cocoa trend FADL communication cocoa node observation clearly cocoa increase node previous analysis ignore computation within iteration role analyze overall efficiency actual relevant FADL ADMM cocoa dane involve extensive computation inner iteration tera node amount local data node tera fare analysis communication gap tera becomes analysis tera ADMM respect communication although ADMM efficient tera respect communication tera nicely ADMM cocoa sometimes kdd url node otherwise node FADL uniformly ADMM respect overall FADL performance perform equally situation medium dimensional datasets communication issue expectation FADL datasets FADL equally tera dane competitive due involve tune useful recall comment remark goodness convergence rate FADL efficient distribute algorithm sec rel func diff FADL cocoa tera ADMM dane kdd node sec rel func diff FADL cocoa tera ADMM dane kdd node sec rel func diff FADL cocoa tera ADMM dane url node sec rel func diff FADL cocoa tera ADMM dane url node sec rel func diff FADL cocoa tera ADMM dane webspam node sec rel func diff FADL cocoa tera ADMM dane webspam node plot efficiency various dimensional datasets mahajan agrawal keerthi sellamanickam bottou sec rel func diff FADL cocoa tera ADMM dane mnistm node sec rel func diff FADL cocoa tera ADMM dane mnistm node sec rel func diff FADL cocoa tera ADMM dane rcv node sec rel func diff FADL cocoa tera ADMM dane rcv node plot efficiency various medium dimensional datasets rcv mnistm node plot dane invisible tune displayed relative performance relevant cocoa impressive tera kdd datasets unclear cocoa fare kdd datasets ADMM overall decent performance tera FADL consistently faster tera ups anywhere communication scenario reduce communication important FADL ADMM possibility ups tera kdd FADL cocoa node generally dane scenario others efficient distribute algorithm node tera comm pas comm pas FADL cocoa tera ADMM dane kdd node tera comm pas comm pas FADL cocoa tera ADMM dane url node tera comm pas comm pas FADL cocoa tera ADMM dane webspam node tera comm pas comm pas FADL cocoa tera ADMM dane mnistm node tera comm pas comm pas FADL cocoa tera ADMM dane rcv plot communication relative tera function node terminate within steady AUPRC achieve perfect training rcv FADL ADMM curve coincide mahajan agrawal keerthi sellamanickam bottou node tera FADL cocoa tera ADMM dane kdd node tera FADL cocoa tera ADMM dane url node tera FADL cocoa tera ADMM dane webspam node tera FADL cocoa tera ADMM dane mnistm node tera FADL cocoa tera ADMM dane rcv plot relative tera function node terminate within steady AUPRC achieve perfect training efficient distribute algorithm function revisit plot correspond kdd FADL tolerance rel func diff node useful prompt distribute really already mention training data data generate reside distribute node data machine efficient option dataset approximate tolerance AUPRC plot function usually minimum appropriate optimally minimize training data machine application involve periodically model training involve newly data advertising logistic regression click probability model retrain daily basis incrementally datasets scenario worthwhile tune deployment phase minimize choice future computation communication ratio computational communication dimensional datasets ratio tera communication dominates balance FADL ratio varies clearly FADL computation communication significantly reduce communication FADL cocoa tera ADMM kdd url webspam ratio computation communication various terminate AUPRC within AUPRC node FADL comment discussion apply datasets kdd url webspam really data lack availability public datasets medium dimensional datasets rcv mnistm communication latency etc role analysis computation versus communication insight mahajan agrawal keerthi sellamanickam bottou dataset verify goodness FADL evaluate FADL dataset magnitude dataset dataset splice site recognition dataset bioinformatics domain dataset sequence positional feature upto gram dataset feature dataset terabyte employ cluster node various reduction objective function function communication reduction objective function function improvement generalization performance AUPRC function FADL reduction communication FADL performer cluster communication iteration FADL interestingly cocoa performer cocoa convergence FADL equally plot improvement generalization performance earlier datasets cocoa varies scenario splice site recognition node FADL uniformly scenario datasets node etc summary useful summarize finding empirical FADL reduction communication clearly superior communication setting spite computational per iteration FADL overall performance medium dimensional datasets FADL tera actual dataset FADL nicely balance computation communication discussion discus briefly distribute setting algorithm aim flexibility generality approach ensure glrc partition distribute across node worth mention due gradient consistency partition constraint theory allows resampled node arbitrarily reduce outer iteration node efficient distribute algorithm communication rel func diff FADL cocoa ADMM tera dane communication efficiency objective function sec rel func diff FADL cocoa ADMM tera dane efficiency objective function sec AUPRC FADL cocoa ADMM tera dane efficiency AUPRC plot various splice site recognition dataset theory propose feature partition suppose node restrict subset feature constraint denotes feature partition useful important feature node gradient sub consistency ˆfp gradient  modify algorithm propose feature decomposition algorithm glrc feature decomposition approach propose literature closest  synchronize parallel algorithm extends generic approximation algorithm functional approximation sub partition parallel although objective function assume convex approximation satisfy mahajan agrawal keerthi sellamanickam bottou monotone imply approximation convex algorithm asymptotic linear rate convergence feature partition disjoint contrast glrc feature overlap partition moreover exist counterpart partition distribute algorithm recently  developed algorithm MISO MISO spirit EM algorithm  approximation avoid MISO serial develop distribute version MISO future direction inexpensive communication wise unclear benefit approach easily generalize joint feature partition non convex setting detail extension mention related future recently powerful conquer approach training kernel partition input restriction training partition input decouple FADL ability data node gain decouple partition however distribute pre processing shuffle data expensive conclusion conclude propose FADL novel functional approximation distribute algorithm provable global linear rate convergence algorithm flexible local approximation node algorithm optimize local approximation data usage node establish superior efficiency FADL evaluate exist distribute FADL potential machine arise data appendix complexity analysis notation around define overall distribute algorithm inner  outer outer outer iteration inner inner iteration node communication happens denote data dimensional dot per inner iteration respectively communication assume allreduce binary described non convex setting glrc establish simpler convergence theory efficient distribute algorithm pipelining multiplicative factor logp relative computation communication distribute precisely ratio associate communicate float perform float operation usually dimensional vector gradient hessian vector computation etc communicate inner SQM FADL parameter parameter SQM FADL outer SQM overall conjugate gradient iteration plus gradient computation average conjugate gradient iteration inner minimization ˆfp tron per outer iteration FADL typically dense dot extremely  approach ignore simplicity FADL lesser tera  FADL outer SQM outer SQM outer FADL ignore outer SQM inequality SQM rearrange looser outer SQM outer FADL assume outer SQM outer FADL appendix proof proof establishment convergence theory proof lemma tdr  connection quantity involve lemma  corresponds corresponds strictly monotone increase assumption implies strictly monotone increase tend infinity tends infinity actually another communication logp communicate pipeline however typically hence ignore mahajan agrawal keerthi sellamanickam bottou strictly monotone increase unique validates definition monotonicity implies satisfied iff monotone increase exists unique validates definition easily checked iff imply monotonicity lemma proof theorem  bound      implies implies  vishwanathan clearly theorem proof establishment convergence theory establish minimizer ˆfp sufficient angle descent lemma minimizer ˆfp  proof gradient consistency ˆfp ˆfp  efficient distribute algorithm ˆfp ˆfp      convexity fourth proof lemma approximate establish lemma satisfy construction proof lemma equation  vishwanathan ˆfp iteration ˆfp ˆfp ˆfp ˆfp def mahajan agrawal keerthi sellamanickam bottou assume sphere therefore max angle angle geometry easy maxv attain boundary  satisfy geometry yield   wrk   wrk  wrk yield lemma implies separately satisfy def prof lemma proof theorem trivially combination lemma theorem