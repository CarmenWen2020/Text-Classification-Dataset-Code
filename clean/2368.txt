approximate bayesian inference approach estimate intensity inhomogeneous poisson intensity function model gaussian GP prior via sigmoid link function augment model latent marked poisson olya gamma random variable obtain representation likelihood conjugate GP prior estimate posterior variational optimisation framework sparse gps furthermore alternative approximation sparse laplace posterior efficient expectation maximisation algorithm derive posterior mode algorithm inference obtain markov chain monte carlo sampler standard variational gauss approach model magnitude faster furthermore performance competitive another recently propose poisson model quadratic link function limited gps exponential kernel rectangular domain keywords poisson cox gaussian data augmentation variational inference introduction estimate intensity rate discrete continuous application model seismic activity neural data  forth particularly approach bayesian model cox assume generate poisson intensity function model another random prior probability inference model attract bayesian machine community recent    assume intensity function sample gaussian GP prior however restrict intensity function poisson nonnegative strategy nonlinear link function GP argument return valid intensity variational approximation complex gaussian model inference poisson model attract considerable machine community powerful variational gaussian inference algorithm available apply arbitrary link function choice link function crucial define prior intensity important efficiency variational inference standard choice cox exponential link function treat however variational gaussian inference link function disadvantage posterior variance becomes decouple observation choice quadratic link function integration data domain sparse GP inference specific kernel compute analytically model minimisation variational perform gradient descent technique approximate inference model sigmoid link function model introduce MCMC sample algorithm improve  van  model favourable frequentist theoretical prior hyperparameters chosen appropriately contrast variational gaussian approximation posterior distribution latent function introduce alternative variational approximation specially sigmoidal gaussian cox recent bayesian logistic regression data augmentation olya gamma random variable approach already combination gps stochastic discrete extend augmentation latent marked poisson distribute accord olya gamma distribution augment likelihood becomes conjugate GP distribution combination variational approximation sparse GP approximation obtain explicit analytical variational update inference addition augmentation computation maximum posteriori estimate expectation maximisation EM algorithm obtain laplace approximation non augment posterior organise introduce sigmoidal gaussian cox model transformation variable augmentation derive variational EM algorithm obtain estimate laplace approximation posterior sparse GP approximation infinite dimensional tractable demonstrate performance synthetic data monte carlo sample model variational approximation modify cox model sigmoid link function inference  robert propose efficient approximate sample scheme frequentist nonparametric approach model bayesian extension application marked poisson sigmoidal gaussian cox inference algorithm artificial data observation discussion outlook inference assume generate poisson dimensional vector compact domain goal infer intensity function likelihood exp constant density poisson intensity appendix respect poisson intensity bayesian framework prior intensity random doubly stochastic cox reparametrization intensity function sigmoid function maximum intensity rate hence intensity positive everywhere arbitrary function inference function throughout assume model GP sigmoidal gaussian cox likelihood becomes exp bayesian inference define GP prior PGP zero covariance kernel prior density respect ordinary lebesgue gamma density rate parameter respectively hence prior   posterior density respect prior    normalise expectation denominator respect probability  infinite dimensionality gps poisson minimum extra notation introduce density radon  derivative define equation appendix respect infinite dimensional boldface non bold density denote density classical respect lebesgue bayesian inference model doubly intractable likelihood equation contains integral exponent normalisation posterior equation calculate expectation equation addition inference hamper likelihood depends donner opper non linearly sigmoid exponent sigmoid tackle augmentation scheme likelihood becomes conjugate GP prior subsequently derive analytic variational posterior assumption data augmentation latent poisson briefly introduce data augmentation scheme latent poisson basis sample algorithm extend augmentation marked poisson focus exponential equation utilize sigmoid exp exp  characteristic functional poisson generally random ΠZ function define  ΠZ exp  probability poisson intensity equation derive campbell theorem appendix  chap identifies poisson uniquely combine equation obtain likelihood however another augmentation invoke campbell theorem likelihood conjugate model prior simplifies inference data augmentation II olya gamma variable marked poisson inverse hyperbolic cosine gaussian mixture model cosh pPG pPG olya gamma density appendix define tilt olya gamma density pPG pPG parameter explicit density subsequently derive inference algorithm obtain directly generate function calculate straightforwardly equation appendix equation allows sigmoidal gaussian cox inference rewrite sigmoid function cosh pPG define equation substitute equation exp exp pPG  finally apply campbell theorem equation equation Xˆ intensity  representation exponential equation exp Xˆ   ΠXˆ interestingly poisson ΠXˆ  marked poisson  chap latent olya gamma variable denote independent random variable location straightforward sample sample inhomogeneous poisson domain constant rate independently density pPG finally olya gamma augmentation discrete likelihood factor correspond equation obtain joint likelihood model ΠXˆ   ΠXˆ ΠXˆ  define prior augment variable    olya gamma variable observation prior  QN  augment representation likelihood contains function linearly quadratically exponent conjugate GP prior likelihood equation recover  ΠXˆ donner opper inference augment augmentation define posterior density joint model respect   ΠXˆ    ΠXˆ ΠXˆ denominator marginal likelihood   ΠXˆ posterior density equation sample gibbs sample explicit tractable conditional density variational approximation conditional ΠXˆ variable function maximal intensity specific marked poisson independent tilt olya gamma density distribution function ΠXˆ gaussian however sample GP finite random ΠXˆ fix variational approximation variational inference assumes desire posterior probability belongs inference tractable structure assumption fully utilise conjugate structure approximate posterior  ΠXˆ ΠXˆ meaning dependency olya gamma variable marked poisson ΠXˆ function maximal intensity neglect assumption allows derive posterior approximation analytically variational approximation optimise minimise kullback leibler divergence approximate posterior equivalent maximise bound marginal likelihood observation EQ ΠXˆ ΠXˆ probability variational posterior equation introduce approximate likelihood ΠXˆ  ΠXˆ  standard argument variational inference bishop chap equation optimal factor satisfy ΠXˆ EQ ΠXˆ const sigmoidal gaussian cox inference EQ ΠXˆ const respectively iterative scheme optimise increase bound equation structure likelihood derives  ΠXˆ ΠXˆ density define respect   respectively subsequent describes update explicitly optimal olya gamma density equation obtain exp cosh pPG pPG factor tilt prior olya gamma density equation appendix EQ density transformation obtain density respect lebesgue   pPG tilt olya gamma density optimal poisson equation obtain ΠXˆ ΠXˆ EQ exp Xˆ EQ   EQ EQ involves expectation EQ EQ equation marked poisson intensity exp EQ cosh pPG exp EQ pPG EQ proof appendix donner opper optimal gaussian equation obtain optimal approximation posterior likelihood define relative GP prior effective likelihood EQ ΠXˆ EQ expectation variational poisson ΠXˆ olya gamma variable easily evaluate appendix EQ dirac delta function expectation integral EQ tanh exp EQ tanh variational distribution defines gaussian assumption integral equation integration random variable deterministic integral however integral function entire kernel compute marginal posterior density input specific GP kernel operator inverse differential operator linear partial differential equation dimensional  kernel integer parameter fulfill becomes equivalent inference continuous gaussian hidden markov model perform backward algorithm reduce computation ordinary differential equation discus detail approach elsewhere kernel resort instead variational sparse GP approximation induce sigmoidal gaussian cox inference optimal sparse gaussian sparse variational gaussian approximation standard approach generalisation continuum likelihood completeness derivation detailed appendix approximate sparse likelihood GP respect GP prior  depends finite dimensional vector function induce approach  exactly infinitely function outside induce sparse likelihood optimise minimise kullback leibler divergence dkl  computation appendix EP conditional expectation respect GP prior function induce explicit calculation conditional expectation EP kernel matrix induce expectation EP EP const constant conditional variance sparse location sparse finite define ordinary posterior density GP induce respect lebesgue equation conclude sparse posterior induce variable multivariate gaussian density covariance matrix contrast variational approximation obtain analytic variational posterior donner opper covariance arbitrary GP kernel however finite dimensional integral cannot compute analytically sparse approximation poisson model link function integral exponential kernel obtain analytically hence resort monte carlo integration integration sample uniformly integration drawn uniformly finally equation obtain function variance sparse approximation EQ variance identity matrix optimal density maximal intensity equation identify optimal density gamma density gamma EQ gamma function denotes indicator function otherwise integral monte carlo integration defines expectation update EQ EQ digamma function hyperparameters hyperparameters model covariance parameter GP location induce prior parameter maximal intensity covariance parameter optimise gradient ascent gradient bound equation respect appendix gradient ascent algorithm employ adam algorithm perform variational posterior update described location sparse GP principle optimise fix regular grid choice toeplitz matrix kernel  invariant invert instead operation employ finally prior parameter chosen twice standard deviation intensity homogeneous poisson variational procedure outline algorithm sigmoidal gaussian cox inference algorithm variational bayes algorithm sigmoidal gaussian cox init EQ EQ integration EQ EQ converge update PG distribution observation rate latent integration update sparse GP distribution GP integration EQ EQ gamma distribution update kernel parameter gradient ascent laplace approximation variable augmentation compute laplace approximation bishop chap joint posterior GP function maximal intensity alternative previous variational scheme maximum posteriori estimate mode posterior distribution taylor expansion around mode augmentation compute estimator iteratively EM algorithm obtain estimate definition posterior mode GP posterior function infinite dimensional density respect lebesgue possibility avoid  spatial integral likelihood approximate posterior multivariate gaussian density mode compute gradient zero approach defines mode directly function allows utilise sparse GP approximation developed previously computation mathematically derive estimator maximise properly  likelihood  williams chap GP model likelihood finitely input penalty reproduce kernel hilbert RKHS norm corresponds GP kernel hence   kgk kgk RKHS norm kernel penalty understood generalisation gaussian prior density function formal definition heuristic attempt optimisation EM algorithm instead apply donner opper variable augmentation poisson olya gamma variable introduce previous likelihood function EP ΠXˆ  ΠXˆ kgk maximise becomes variational approach likelihood gaussian model GP function hence argue function maximises posterior gaussian model compute without explicit RKHS norm conditional probability ΠXˆ easily obtain optimal average straightforwardly density ΠXˆ ΠXˆ factor   pPG latent ΠXˆ poisson density ΠXˆ   ΠXˆ intensity  pPG function EP ΠXˆ  ΠXˆ EP EP already tackle almost identical likelihood expression equation specific prior precision kernel differential operator treatment ODEs PDEs resort sparse GP approximation instead sparse version obtain replace EP obtain sparse function sigmoidal gaussian cox inference function maximal intensity maximise equation analytically estimate obtain convergence EM algorithm desire sparse equation  variational scheme integral approximate monte carlo integration alternative derivation sparse restrict minimisation function linear combination kernel centre induce definition RKHS norm  williams chap sparse laplace posterior computation laplace approximation evaluate quadratic fluctuation around previously obtain sparse approximation converge define sparse likelihood poisson model via replacement EP sparse likelihood easy compute laplace posterior derivative variable ensure effective normal density maximal intensity rate address hyperparameter selection laplace posterior straightforward approach validation optimise kernel parameter estimate laplace approximation approximate evidence variational induce location regular grid laplace approximation augmentation scheme compute estimate efficient approximation involve implies dependency retain predictive density variational laplace approximation yield posterior distribution GP approximation  normal density posterior intensity function compute EQ EQ donner opper variational laplace posterior expectation compute analytically expectation compute numerically via quadrature evaluate performance inference interested compute likelihood data dtest generate truth sample gps posterior calculate likelihood dtest EP dtest EQ dtest EQ exp dtest integral exponent approximate monte carlo integration expectation approximate average sample infer posterior observation dtest integration instead sample obtain analytic approximation likelihood equation taylor expansion around obtain posterior apply variational posterior dtest  dtest  EQ     EQ EQ   derivative likelihood equation respect  approximation involve neglect uncertainty posterior john  account generate data model evaluate newly developed algorithm generate data accord sigmoidal gaussian cox model PGP poisson density PGP GP density covariance function kernel exponential function exp hyperparameters scalar sample inhomogeneous poisson via assume hyperparameters subsequent data sample generative model sigmoidal gaussian cox inference benchmark sigmoidal gaussian cox inference propose algorithm alternative inference sigmoidal gaussian cox model inference sample approach competitor variational approach propose  regular bin likelihood equation approximate ppo ppo poisson distribution parameter centre bin observation within bin sparse gaussian variational approximation correspond kullback leibler divergence minimise gradient ascent optimal posterior GP estimate originally propose cox elegant  package implementation sigmoid link function straightforward numerical integration sigmoid link function evaluate variational bound spatial bin gradient augmentation scheme discussion propose augmentation model refer inference algorithm variational gauss comparison induce algorithm sampler bin  domain variational gauss algorithm integration MC integration variational laplace data generative model illustrative sample dimensional poisson generative model perform inference sampler sample iteration algorithm laplace approximation variational gauss posterior intensity function standard deviation regularly induce random integration drawn uniformly  bin algorithm recover intensity laplace algorithm posterior variance sampler inference obtain laplace algorithm variational gauss sampler convergence laplace variational algorithm illustrate objective function algorithm minus maximum converge function algorithm plateau iteration performance likelihood dtest sample truth average independent data posterior sampler yield variational approximation equation yield variational gauss laplace yield posterior density maximal intensity increase efficiency GP sample elliptical slice sample donner opper sampler laplace var gauss max laplace  laplace sampler var gauss inference 1D dataset inference sampler algorithm laplace approximation variational gauss solid colour denote intensity function shade standard deviation dash rate function vertical observation convergence EM algorithm objective function bound likelihood EM algorithm shift convergence function iteration respective algorithm infer posterior density maximal intensity variational gauss estimate vertical denotes sigmoidal gaussian cox inference inference 2D dataset truth intensity function dataset dot posterior intensity sampler algorithm laplace variational gauss induce regular grid colour integration bin inference dimensional cox induce integration bin laplace variational gauss algorithm posterior sampler laplace variational gauss algorithm recover intensity rate evaluate role induce integration generate dtest intensity evaluate likelihood equation compute average induce integration integration induce account randomness integration fitting shade minimum maximum obtain approximate algorithm predictive likelihood saturates already induce sparse GP however inference approximation slightly inferior sampler likelihood hardly affected integration approximate likelihood algorithm equation yield estimate sample dash runtime algorithm laplace approximation superior magnitude variational gauss algorithm difference increase increase induce algorithm data sample generative model previous approximate algorithm yield qualitatively performance likelihood sampler superior approximate likelihood equation estimate sample addition approximate error RMSE evaluate grid normalise maximal intensity infer truth laplace algorithm magnitude faster donner opper num induce runtime num integration evaluation inference predictive likelihood average function induce integration bin fix sampler laplace orange variational gauss purple algorithm solid denotes data shade denotes min max dash approximate predictive likelihood algorithm function integration induce fix algorithm function induce integration data sigmoidal gaussian cox inference performance artificial datasets sampler algorithm MF laplace variational gauss  datasets dimension observation corresponds dataset likelihood inference denotes approximate likelihood variational algorithm approximate error normalise maximal intensity rate dataset intractable sampler due observation data correspond  gauss algorithm algorithm slightly faster laplace data comparison approach variational algorithm data generative model data model prior previously alternative model propose link function sigmoidal gaussian cox propose augmentation scheme analytic update variational posterior gaussian cox likelihood integral analytically sample kernel exponential domain rectangular algorithm rely sparse GP approximation empirically dimensional data generate intensity function exp exp interval already propose generate training rate function factor sigmoidal donner opper max 1D observation sample function posterior sigmoidal gaussian cox respectively shade denotes standard deviation  runtime RMSE runtime RMSE benchmark standard deviation runtime RMSE likelihood obtain RMSE standard deviation inference algorithm deterministic correspond variational algorithm training induce variational algorithm integration posterior intensity data model sigmoidal link function infers smoother posterior function variance posterior link function data report standard deviation runtime RMSE likelihood algorithm comparable intermediate data algorithm link function faster data sigmoidal link function converges RMSE comparable intermediate data sigmoidal model superior dimensional data comparison neuronal data spike activity mouse freely arena data mouse observation randomly assign training chris lloyd tom  code infer variational posterior gaussian cox sigmoidal gaussian cox inference observation training variational posterior intensity obtain sigmoidal link function respectively infer regular grid induce sigmoidal posterior smoother difference algorithm apart link function sigmoidal model sample interval investigate integration runtime likelihood regardless integration variational posterior link function yield superior likelihood sigmoidal model likelihood improve significantly integration runtimes algorithm comparable integration chosen algorithm achieve fitting model integration converge  desire integration rerun algorithm dot allows significant without loss likelihood variational algorithm sigmoid link function faster integration equally integration data  taxi data data contains trajectory taxi  john  ups observation poisson taxi randomly split training respectively training induce regular grid variational posterior respective intensity data data difference model subtle likelihood variational posterior sigmoidal model outperforms model link function likelihood variational algorithm faster variational posterior link function choice integration reduces accuracy previous data strategy fitting posterior integration desire integration dot prof significant without loose predictive discussion outlook combination variable augmentation derive conjugate representation posterior sigmoidal gaussian cox approximation augment posterior  yield efficient variational algorithm rationale variational update conjugate model explicit analytical gradient contrast runtime displayed linear meaning algorithm magnitude john  report highly peaked pickup within coordinate exclude donner opper neuronal data max num integration runtime taxi data max num integration runtime data mouse neuron spike posterior obtain variational algorithm sigmoidal gaussian cox variational approximation gaussian cox likelihood runtime function integration algorithm dot obtain fitting sigmoidal model integration axis shade standard deviation obtain data observation taxi ups  sigmoidal gaussian cox inference descent comparison variational algorithm model augmentation approximation posterior gaussian quality inference approach algorithm magnitude faster variable augmentation computation estimate  posterior EM algorithm finally apply calculation laplace approximation yield explicit approximate GP posterior correspond effective likelihood contains continuum GP latent variable computation marginal variance inversion linear operator instead simpler matrix inverse specific prior pde ode resort sparse GP approach induce apply arbitrary kernel spatial integral domain moderate dimensionality approximate monte carlo integration advantage approach limited rectangular domain requirement volume alternative poisson model spatial integral perform analytically within sparse GP approximation limited exponential kernel rectangular domain quadratic link function variational algorithm variational algorithm data algorithm magnitude runtime slight advantage variational algorithm model predictive highly data dependent alternative monte carlo integration approach avoid infinite dimensionality latent GP binning scheme poisson observation straightforward adopt augmentation poisson likelihood augment poisson olya gamma variable donner opper bin approach favourable data becomes  data resolution  however approach spatial  sparse induce become problematic dimensional domain alternative spectral representation kernel promising tackle apply variable augmentation bayesian model sigmoid link function inherent boundedness intensity crucial nonlinear hawkes widely model stock data seismic activity model sigmoid function naturally mention kinetic ising model markov originally introduce model dynamic classical spin physic recently model joint activity neuron finally gaussian density model introduce treat augmentation developed donner opper acknowledgment CD    partially fund   DFG grant crc data assimilation project  bayesian inference model selection stochastic differential equation  sigmoidal gaussian cox inference appendix poisson briefly summarise poisson relevant thorough description recommend concise  particularly chapter countable subset ΠZ definition poisson random countable subset ΠZ poisson sequence disjoint subset cardinality union ΠZ independent poisson distribute constant poisson homogeneous inhomogeneous otherwise campbell theorem ΠZ poisson furthermore define function sum ΠZ ΠZ  ΠZ exp integral converges  probability poisson intensity variance obtain  ΠZ  ΠZ equation defines characteristic functional poisson marked poisson ΠZ poisson intensity  poisson Zˆ drawn independently marked poisson intensity straightforward extend campbell theorem characteristic functional   exp Zˆ    donner opper appendix olya gamma density olya gamma density useful allows inverse hyperbolic cosine infinite gaussian mixture cosh exp pPG parameter furthermore define tilt olya gamma density pPG exp cosh pPG equation generate function obtain definition    differentiate respect yield  tanh appendix variational inference stochastic density random stochastic probability density respect lebesgue infinite dimensional function gaussian however define density respect another reference absolutely continuous respect density expectation EP ER density equation radon  derivative respect poisson density specific prior density poisson equation define respect reference ΠZ   ΠZ exp ΠZ sigmoidal gaussian cox inference  probability intensity expectation define  ΠZ  ΠZ ΠZ calculate expectation ΠZ equation identify characteristic function poisson equation intensity kullback leibler divergence density express kullback leibler divergence probability KL divergence define dkl  EQ absolutely continuous KL divergence reference appendix posterior marked poisson optimal variational posterior equation poisson campbell theorem posterior equation ΠZ  ΠZ ΠZ  ΠZ ΠZ exp ΠZ random random poisson intensity proof density ΠZ poisson calculate characteristic functional arbitrary function EQ ΠZ  ΠZ exp exp exp exp exp  identify generate functional poisson intensity  poisson uniquely characterise generate function  chap proof donner opper appendix sparse gaussian approximation inference function define sparse GP prior effective likelihood depends finite function hence  sparse posterior  equality equation depends KL divergence posterior density EP sparse dkl  EP EP const EP EP const derive directly posterior density sparse GP sparse likelihood EP sigmoidal gaussian cox inference appendix bound hyperparameter optimization bound equation EQ ΠXˆ ΠXˆ Xˆ EQ EQ EQ  Xˆ  EQ EQ cosh EQ trace det  det  EQ  optimise covariance kernel parameter differentiate bound respect parameter perform gradient ascent gradient specific parameter Xˆ EQ  EQ trace det  Xˆ EQ  EQ trace trace derivative function EQ EQ EQ EQ EQ EQ EQ donner opper remain variational hyperparameters update  