considerable bayesian inference scalable data setting focus reduce compute per iteration reduce iteration markov chain monte carlo MCMC article considers data augmentation MCMC DA MCMC widely technique DA MCMC sample tend become highly  sample due calibration conditional posterior distribution augment data concentrate MCMC obtain acceptably MC error combat inefficiency propose calibrate data augmentation algorithm appropriately adjust variance conditional posterior distribution metropolis hastings eliminate bias stationary distribution sampler exist alternative approach dramatically reduce MC error reduce autocorrelation increase effective DA MCMC sample per compute approach applicable variety exist data augmentation algorithm focus popular generalize linear model probit logistic poisson linear dramatic gain computational efficiency application keywords bayesian probit bias subsampling data augmentation linear model logistic regression maximal correlation  gamma introduction  data application scalable computational algorithm inference data uncertainty quantification UQ somewhat surprisingly volume data increase uncertainty remains sizable phenomenon occurs financial fraud detection disease mapping online click bayesian approach useful paradigm quantify uncertainty setting standard approach bayesian posterior computation markov chain monte carlo MCMC related sample algorithm however conventional MCMC algorithm poorly complexity due sequential computational MCMC factor evaluation sample iteration iteration obtain acceptably monte carlo MC error substantial literature developed focus decrease computational per iteration data sample setting  adam others focus reduce MCMC iteration contrast historical focus statistic probability literature improve convergence MCMC traditional moderate sample suggests opportunity improve performance data setting renew focus improve concern apply MCMC algorithm data autocorrelation MCMC increase data markov chain autocorrelation effective sample ESS per computational refer informally ESS asymptotic variance MCMC average estimate standard monte carlo algorithm independent sample effective sample MCMC iteration MCMC algorithm ordinary MC algorithm obtain MC error average scenario unusual data MCMC algorithm burden per iteration increase become iteration sample increase burden member machine community abandon MCMC easily scalable alternative variational approximation unfortunately approach typically lack theoretical guarantee badly underestimate posterior uncertainty hence substantial recent scalable MCMC algorithm focus popular data augmentation DA MCMC algorithm DA MCMC algorithm routinely model algorithm albert  probit model logistic model particularly popular focus improve performance algorithm data setting scalability occurs per iteration deterioration sample increase focus calibrate data augmentation demonstrate popular DA MCMC algorithm effective sample data setting involve imbalanced data data binary proportion zero insight discrepancy rate gibbs width probability posterior converge zero increase conditional posterior augment data simply concentrate relative marginal posterior amplify data sample increase literature accelerate DA MCMC algorithm trick reparameterization parameter expansion however approach fail address  impact worsen rate increase data sample article proposes algorithm address  DA underlie calibrate DA CDA algorithm introduce auxiliary parameter variance conditional distribution parameter auxiliary parameter adapt data sample typical CDA algorithm rate probability posterior contract increase invariant CDA MCMC typically exist unique differs posterior CDA MCMC computationally efficient perturbation markov chain bias eliminate metropolis hastings adaptive metropolis hastings algorithm carefully chosen multivariate proposal complicate adaptation multiple chain CDA MCMC modification gibbs sample generate proposal auxiliary parameter efficiently adapt data augmentation via minimize difference fisher information conditional marginal distribution calibrate data augmentation data augmentation gibbs sampler alternate sample latent data conditional posterior distribution model parameter data sample parameter broken series conditional sample focus simplicity algorithm belongs location gaussian popular data augmentation algorithm sample conduct easily efficiently sample latent data independently simultaneously multivariate gaussian standard distribution effectively avoids tune issue metropolis hastings algorithm particularly dimensional data augmentation algorithm particularly generalize linear model  duan johndrow dunson xiθ conditionally gaussian prior distribution chosen focus poisson linear binomial logistic binomial probit motivate markov kernel invariant update markov chain evolve accord abuse notation lag autocorrelation function stationarity express bayesian information rubin liu var var integral numerator respect denominator respect integrable function maximal autocorrelation sup inf var var geometric convergence rate data augmentation gibbs sampler liu coordinate projection numerator informally average augmentation algorithm stationarity direction denominator width bulk posterior direction consequently whenever average stationarity relative width bulk posterior purpose CDA introduce additional parameter relative posterior width roughly ratio flexibility reparametrization parameter expansion flexibility gain achieve invariant introduce parameter additional parameter denote correspond collection reparametrizations defines distinct likelihood exists gibbs update vector parameter tune increase var var usually coordinate projection although likelihood correspond gibbs update application location parameter shift posterior approximate reparametrization likelihood gibbs sampler refer CDA gibbs marginal invariant prior ultimately interested CDA gibbs efficient proposal metropolis hastings propose calibrate data augmentation denote conditional density gibbs sampler invariant tune adaptation phase reduce  increase metropolis hastings acceptance rate obtain computationally efficient algorithm tune facilitate MH acceptance ratio proposal kernel convenient nice feature gibbs generate MH proposal remark CDA MH acceptance ratio min min strategy tune convergence guarantee CDA MH weak assumption robert smith basically probability absolutely continuous respect remark ergodicity assume density respect lebesgue fix CDA gibbs ergodic invariant metropolis hastings algorithm proposal kernel define fix ergodic invariant proof appendix initial probit intercept illustrate CDA algorithm toy probit regression intercept bernoulli improper prior data augmentation algorithm integral   density normal distribution update duan johndrow dunson subscript denotes truncation interval var approximately width probability posterior introduce parameter update adjust conditional location parameter equivalent adjustment yield  exp  tune parameter scalar modify data augmentation algorithm achieve consistent width posterior probability preserve target generate MH proposal remark MH acceptance probability  acceptance rate corresponds gibbs sampler illustrate increase acceptance rate MH easy compute proposal distribution proposal MLE perform computation data correspond theoretically optimal plot autocorrelation function  sampler without MH adjustment autocorrelation lag increase dramatic improvement gain increase theoretically optimal kernel smooth density estimate posterior without MH adjustment chain minimize impact monte carlo error posterior variance increase somewhat MH adjustment difference remove MH acceptance probability calibrate data augmentation lag acf acf CDA without MH adjustment theta density posterior density estimate without MH adjustment lag acf acf CDA MH adjustment autocorrelation function  kernel smooth density estimate CDA sampler intercept probit model hierarchical gaussian appendix specific algorithm CDA algorithm probit logistic regression probit regression probit regression bernoulli xiθ improper prior data augmentation sampler update xiθ xiθ XX XX liu meng van  others previously algorithm propose rescale parameter expansion however modification impact conditional variance directly increase typical approach fundamentally directly adjust conditional variance intercept model modify var yield update xiθ xiθ XR XR XR diag bernoulli likelihood  exp xiθ  duan johndrow dunson xiθ fix defines bernoulli likelihood conditional parameter therefore transition kernel define gibbs update unique invariant fix denote insight relationship marginal  gibbs sampler evolve accord  XR XR XR cov XR XR  XX conditional variance increase factor uniformly entire  var  var CDA  var  additionally maximize MH acceptance probability defer detail tune algorithm illustration simulation probit regression intercept predictor generate albert  DA algorithm slowly parameter expansion algorithm PX DA propose liu PX DA mildly reduces correlation tune CDA satisfactory acceptance rate dramatically calibrate data augmentation DA PX DA CDA iteration  DA parameter expand DA CDA algorithm lag acf DA PX DA CDA acf DA parameter expand DA CDA algorithm panel demonstrates  panel autocorrelation substantial improvement CDA variance probit regression rare data parameter expand logistic regression focus logistic regression model bernoulli exp xiθ exp xiθ improper prior model propose  gamma data augmentation PG xiθ   diag algorithm relies express logistic regression likelihood xiθ exp xiθ exp xiθ PG  PG denotes density  gamma distribution parameter  tanh goal increase conditional variance  achieve stochastically reduce  replace PG PG update latent data location linear predictor xiθ xiθ exp xiθ exp xiθ PG  exp xiθ exp xiθ duan johndrow dunson update CDA gibbs sampler PG xiθ   defer tune detail illustration parameter intercept slope model iid obtain rare outcome data  CDA tune acceptance rate DA slowly exhibit autocorrelation lag CDA dramatically DA CDA iteration  DA CDA lag acf DA CDA acf DA CDA panel demonstrates  panel autocorrelation substantial improvement CDA logistic regression rare data DA automatic tune calibration parameter illustrate previous subsection efficiency CDA dependent choice calibration parameter propose efficient algorithm calculate parameter utilize fisher information empirical MH acceptance rate although choice calibration parameter relies sample approximation calibration approach modest sample goal adjust conditional variance calibration approximately marginal variance target distribution maintain reasonable MH acceptance rate calibrate data augmentation approximate marginal variance inverse fisher information var maximum posteriori estimate recall CDA proposal density conditional variance bound var  inverse fisher information approximate var via  intractable cumbersome compute instead approximation fisher information evaluate conditional mode choice mode depends  expression adjust reduce distance dist dist distance matrix  frobenius norm difference however increase proposal variance increase variance metropolis hastings transition density acceptance probability substantially depressed relative DA gibbs sampler acceptance probability therefore adjust optimize acceptance rate proposal variance average acceptance rate negative expectation proposal density posterior    max tractable computation function evaluate conditional mode approximate expectation yield max duan johndrow dunson mode mode combine yield optimal tune parameter criterion min optional parameter allows differential acceptance rate variance although default application facilitate automatic tune generic exploit automatic differentiation optimization software tensorflow compute fisher information optimize density conditional estimator tune detail probit logistic regression likelihood update density already conditional estimator probit regression conditional mode available viz xiθ xiθ otherwise XR XR logistic regression conditional expression  tanh   geometric convergence rate CDA MH CDA gibbs although remark guarantee convergence average estimator commonly MCMC goal CDA MH improve upon convergence rate DA gibbs motivation CDA intercept logistic probit regression data increasingly imbalanced sample increase spectral gap DA converges zero  metropolis spectral gap suggests superiority metropolis algorithm sample imbalanced data however implement metropolis effectively moderate covariates efficient construct proposal goal CDA convergence rate CDA CDA MH imbalanced intercept logistic regression spectral gap DA function comparable MH optimally tune proposal grows faster fix difficulty calibrate data augmentation obtain quantitative estimate rate spectral gap converges zero grows underscored complexity argument intercept logistic regression prior scalar fix update CDA gibbs PG theorem intercept logistic regression observation CDA gibbs uniformly ergodic CDA MH uniformly ergodic exist choice CDA MH spectral gap spectral gap CDA MH zero slowly increase DA gibbs moreover prior spectral gap CDA MH independent CDA MH rapidly sample imbalanced unlike DA gibbs spectral gap converge zero rate faster ignore logarithmic factor borne empirically conduct simulation fix increase massive effective sample per DA CDA deterioration DA becomes critical effective sample CDA performs exceptionally limit float accuracy duan johndrow dunson sample logn effective sample DA CDA effective sample pointwise confidence interval per sample logistic regression model intercept simulation performance CDA popular alternative algorithm comparison downsampling algorithm motivate introduction factor MCMC practically useful compute iteration effective sample within iteration potential issue data augmentation latent variable sample iteration strategy avoid sample latent variable observation approximate markov transition kernel subsamples unlike alternative algorithm invariant suitable subsample approximation error challenge usually  instead goal sub sample alone address ESS DA whereas trivially combine propose CDA strategy subsampling DA MCMC enormous data sample illustrate strategy parameter intercept slope model logistic regression described data sample simulate bernoulli outcome bernoulli exp xiθ iid obtain  utilize minibatch  gamma algorithm described apply CDA calibrate calibrate data augmentation variance discrepancy highly imbalanced apply bias sample data sub sample data denote data random subset adjust likelihood contribution via compensate downsampling approximate likelihood exp xiθ exp xiθ exp xiθ latent variable reduce remains calibration algorithmic detail appendix performance approximate algorithm combine CDA sub sample sub sample alone clearly sub sample alone effective sample CDA sub sample excellent computational performance sample logn effective sample DA subsampling CDA subsampling performance CDA DA couple sub sample approximation reduce sample latent variable comparison independence metropolis hastings CDA MH algorithm utilize marginal quantity tune parameter performance CDA proposal alternative MH proposal access information specifically analyze MH independent multivariate proposal variance algorithm acceptance rate relative CDA MH MH acceptance rate min assume prior negligible impact acceptance rate constant posterior density duan johndrow dunson parameter however computational convenience proposal easy sample density ratio decrease rapidly away posterior mode rejection rate illustrate independent multivariate distribution proposal logistic regression XT diag exp exp parameter var exactly induce heavier target likelihood geometric ergodicity MH independent proposal density ratio exp  exp xiθ TI exp  exp xiθ xiθ exp exp denotes constant approximation acceptance ratio focus  DA gibbs exp assume posterior density neighborhood xiθ bound constant constant approximately numerator acceptance ratio approximately exp  decrease exponentially away contrast CDA proposal density target calibration density ratio constant neighborhood mode density ratio logistic CDA proposal exp xiθ exp xiθ constant minimize fisher information distance approximately exp density ratio approximately acceptance ratio performance MH algorithm CDA proposal parameter intercept slope described acceptance ratio intercept approximately average xiθ calibrate data augmentation acceptance rate rapidly proposal CDA proposal theta acceptance rate proposal CDA acceptance ratio multivariate distribution CDA proposal logistic regression variance fix inverse fisher information CDA acceptance ratio multivariate proposal data application bernoulli latent factor model intercept network model apply CDA accelerate estimation intercept latent factor model dataset sparse network  project network data consideration adjacency matrix connectivity macroscopic brain matrix aij binary symmetric aij aij otherwise aii connection ignore therefore effectively binary outcome hemisphere connection within hemisphere pai pai across hemisphere pai quantify phenomenon intercept within across hemisphere fix duan johndrow dunson within bernoulli probit latent factor model aij bernoulli pij pij ψij ψij   wij  inverse gamma wij otherwise wij uir matrix latent factor  assign uniform prior  manifold TU latent dimension latent variable update probit data augmentation algorithm zij ψij aij ψij aij zji zij connection data highly imbalanced connection intercept slowly ordinary DA gibbs algorithm without DA efficient MH proposal develop due restriction DA gibbs relies conditional distribution  zij  wij diag lag acf parameter beta beta  parameter DA lag acf parameter beta beta  parameter CDA  performance model average sparsity network connectivity brain calibrate data augmentation CDA calibrate update gibbs sample unchanged ψij bij rij aij ψij bij rij aij wij rij wij rij bij  wij rij wij rij wij rij bij  wij rij accepted via MH calibrate conditional density ψij aij ψij aij ψij   wij bij tune parameter optimize approach described xiθ replace   wij DA CDA auc  avg compute sec sec avg compute  sec sec parameter estimate compute DA CDA bernoulli latent factor model brain network DA CDA approximately effective sample calculate coda package algorithm initialize estimate CDA significant reduction autocorrelation compute per effective sample sample AUCs compute aij posterior pij CDA estimate clearly data poisson normal model web traffic prediction application apply CDA online browsing activity dataset obtain computational advertising dataset contains user browse website belonging client computational advertising agency traffic site browsing duan johndrow dunson session refer site session browsing client website commercial identify traffic site relatively browsing rate effectively computational advertising valuable understand browsing behavior predict traffic user poisson regression model browsing browsing client website outcome plus browsing website predictor xij raw browsing traffic site client site gaussian random account dispersion relative poisson distribution poisson normal regression model poisson exp  iid assign weakly informative prior overdispersion parameter assign non informative prior sample separately random slowly instead sample jointly matrix  linear predictor sample jointly explanation improve sample liu focus behavior data augmentation review data augmentation poisson normal model propose treat poisson limit negative binomial NB moderate approximation limit relationship omit constant exp  exp exp lim exp  exp finite approximation posterior sample  gamma data augmentation PG ZX ZX diag however approximation data augmentation inherently problematic approximation error approximate denominator exp exp exp exp moderately exp error calibrate data augmentation cannot additional MH acceptance rate practical gibbs sampler extremely associate conditional covariance CDA circumvent issue approximation error fractional calibration proposal likelihood logistic CDA xiθ exp exp  likelihood proposal update PG  ZX ZX proposal accepted probability poisson density approximation xiθ min exp exp exp exp exp  exp  tune parameter optimize described  tanh ZX update parameter sample via inverse gamma ordinary DA algorithm CDA hamiltonian monte carlo sampler default tune implement  algorithm initialize DA CDA HMC approximately effective sample CDA adapt empirical  DA CDA HMC DA parameter poorly HMC affected presence random duan johndrow dunson parameter remain highly correlate within lag CDA substantially improves algorithm CDA efficient compute per effective sample efficient algorithm lag acf autocorrelation parameter DA lag acf autocorrelation parameter CDA lag acf autocorrelation parameter HMC CDA significantly improves parameter poisson lognormal evaluate predictive performance another browsing traffic client site traffic browsing transform client predict client traffic site predict exp client site expectation approximate MCMC sample obtain training validation error prediction actual compute DA HMC estimation parameter prediction error CDA significantly error DA CDA HMC  prediction RMSE  avg comp sec sec sec avg comp  sec sec sec parameter estimate prediction error compute DA CDA HMC poisson regression model calibrate data augmentation discussion data augmentation DA technique routinely enable implementation gibbs sampler avoid expensive complex tune metropolis hastings algorithm despite convenience DA slowly conditional posterior variance augment data substantially marginal variance data sample massive arises rate convergence augment marginal posterior literature strategy improve rate gibbs sampler non parameterizations parameter expansion improvement however exist approach sample address fundamental rate mismatch issue tackle propose calibrate data augmentation directly adjust conditional variance associate CDA likelihood evaluation negligible random generation iteration DA article demonstrate calibration generally applicable belongs location useful outside location pursue CDA HMC involve MH comparison rely proposal difference compute efficiency although HMC generally applicable beyond data augmentation computationally intensive hamiltonian dynamic multiple numeric CDA calibrate gibbs sample efficient leverage exist data augmentation algorithm auxiliary gibbs chain generate MH proposal generally promising attention literature focus sample parameter dimension moderate limitation CDA MH grows maintain reasonable acceptance rate increase conditional variance decrease MH algorithm therefore dimensionality appendix proof remark proof marginal gibbs transition kernel gibbs reversible margin duan johndrow dunson appendix proof remark proof conditionals define therefore gibbs transition kernel correspond marginal kernel define moreover assumption aperiodic irreducible discussion corollary robert smith aperiodic irreducible marginal transition kernel induced irreducible indicates absolute continuity density respect lebesgue  implies  moreover theorem robert smith CDA MH irreducible aperiodic appendix toy hierarchical normal demonstrate toy commonly data augmentation literature marginal normal model improper prior hierarchical model augment data standard data augmentation algorithm update  thanks straightforward compute marginal variance var clearly conditional variance  adjust conditional variance alternative hierarchical model calibrate data augmentation update  deviation alternative model treat proposal target model remark standard normal density proposal variance target marginal variance  var yield proposal   intuitively improve acceptance rate proposal posterior density yield choice  simulate data performance  autocorrelation plot acf DA calibrate DA algorithm initiate CDA significantly improves performance acceptance rate approximately DA CDA iteration  DA CDA lag DA CDA acf DA CDA trace autocorrelation plot DA CDA hierarchical normal model duan johndrow dunson instead rely  directly adjust  var however non gaussian  intractable adjust var useful appendix calibrate  gamma algorithm sub sample adapt randomly sample subset index algorithm generates proposal subset PG  xiθ  XV   XV subscript indicates sub matrix sub vector correspond sub sample accept MH calibrate likelihood exp xiθ exp xiθ exp xiθ target approximate likelihood appendix proof theorem proof proposal kernel CDA MH identically transition kernel CDA gibbs markov transition semigroup CDA MH density respect lebesgue respectively seek constant density inf proceed exists constant density inf conclude CDA gibbs uniformly ergodic calibrate data augmentation exists constant inf combine  restriction conclude CDA MH uniformly ergodic spectral gap tune parameter zero slowly bound constant function exp exp exp exp exp inequality laplace transform PG exp   proceed bound expectation respect exp  cosh  cosh  cosh cosh cosh exp duan johndrow dunson exp inequality lemma choi  cosh fourth combine obtain viz exp exp exp exp exp exp exp exp exp completes  acceptance ratio min exp exp differentiate respect obtain assume calibrate data augmentation therefore unique mode monotonically increase monotonically decrease convenience exp proceed subcases exp exp exp exp exp exp exp exp exp exp exp bound monotone nondecreasing exp exp lim monotone nonincreasing exp exp duan johndrow dunson exp exp exp exp exp exp exp exp exp combine bound min exp therefore bound min exp combine establish bound inf  density specifically calibrate data augmentation standard gaussian density uniformly ergodic spectral gap tune constant slowly minimize rate  gap zero constraint exp exp min exp tune equivalent tune elect former reduce exp decrease rapidly recall assume  constant independent yield duan johndrow dunson meaning exp exp exp min exp exp combine exp exp exp