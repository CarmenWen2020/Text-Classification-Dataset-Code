knn popular classification data mining statistic implementation significant classification performance however impractical traditional knn assign fix expert sample previous assign sample validation usually consume proposes kTree optimal sample involve training stage knn classification specifically training stage kTree learns optimal training sample sparse reconstruction model construct decision namely kTree training sample optimal stage kTree output optimal sample knn classification conduct optimal training sample propose kTree classification accuracy traditional knn assign fix sample moreover propose kTree achieves classification accuracy newly knn assign sample proposes improvement version kTree namely stage extra information training sample leaf node kTree training sample leaf node kNNs kNNs decision enables conduct knn classification subset training sample leaf node training sample newly introduction model model training sample predict sample model model kNNs training stage conduct classification task calculate distance sample training sample obtain conduct knn classification assigns sample label majority label implementation significant classification performance knn popular data mining statistic vote data mining algorithm previous knn assign optimal fix expert predefined sample assign optimal sample  sharma fix optimal sample training sample propose optimal sample via tenfold validation however traditional knn assigns fix kNNs sample fix knn impractical application consequence optimal sample conduct knn classification varied knn become research topic data mining machine effort focus varied knn efficiently optimal sample propose category propose employ monte carlo validation optimal smooth parameter fore sample recently propose sparse knn optimal sample knn suitable sample reconstruction framework previous varied knn usually individual optimal sample employ traditional knn classification majority training sample predict sample optimal however optimal sample scan training sample sample consume therefore challenge simultaneously address issue knn optimal sample reduction performance improvement address aforementioned issue knn propose kTree optimal sample training stage traditional knn output training model building decision namely kTree predict optimal sample specifically training stage propose reconstruct training sample training sample via sparse reconstruction model output optimal training sample construct decision training sample correspond optimal regard optimal training sample label training stage offline leaf node optimal construct kTree stage sample construct kTree model node leaf node optimal assign sample traditional knn classification assign label majority distinguish difference previous knn propose kTree previous knn fix knn varied knn training stage kTree sparse training stage complexity sample noteworthy training stage kTree offline varied knn propose kTree regard varied knn optimal conduct traditional knn classification classify sample optimal previous complexity obtain optimal due involve sparse kTree dimension feature via model kTree noteworthy traditional fix knn assign fix sample via scan training sample sample although kTree enables obtain optimal sample scan training sample conduct knn classification consume extend propose kTree improvement version namely stage extra information training sample node training sample kNNs decision difference kTree training stage optimal leaf node kTree optimal information training sample stage sample output optimal information leaf node traditional knn classification conduct optimal subset training sample node kNNs sample correspond kNNs training sample sample reduce stage organize briefly recall knn detail propose respectively II analyze experimental IV conclusion II related knn enables output remarkable performance approximately achieve error rate bayes optimization mild widely apply application regression classification imputation performance knn affected issue selection selection distance address issue amount machine technique developed previous knn mainly focus optimal sample incorporate certainty factor conduct knn classification fix sample propose subset informative sample neighborhood vincent  local hyperplane distance define similarity data conduct knn classification recently propose enhance fuzzy knn adaptively specify optimal particle swarm optimization approach developed dual voting scheme knn overcome sensitivity optimal   propose robust neighborhood consensus multiple kNNs impractical apply fix sample data mining machine effort focus sample demonstrate category fix across category   propose combine widely empirical approach induction instance respectively optimal propose construct knn model automatically optimal sample statistical confidence propose locally adjust   propose probabilistic infer optimal huang propose adaptive knn algorithm sample optimal optimal training although fix complexity optimal sample approach  denote matrix vector scalar respectively boldface uppercase boldface lowercase normal italic matrix xij denote respectively denote frobenius norm norm norm norm respectively matrix norm entrywise norm xij xij  denote transpose operator trace operator inverse matrix respectively XT framework propose kTree detail specifically interpret reconstruction optimal training sample kTree respectively illustrate flowchart propose flowchart propose kTree flowchart propose kTree flowchart propose flowchart propose reconstruction denote training sample respectively training sample feature training sample reconstruct reconstruct training sample goal distance  denotes reconstruction coefficient matrix loss function minw  minw XW SourceRight click MathML additional feature denotes reconstruction coefficient correlation training sample application norm regularization avoid issue singularity xtx minw XW SourceRight click MathML additional feature norm regularization tune parameter usually ridge   xtx xtx however output sparse generate sparse reconstruction coefficient training sample sample previous literature employ sparse objective function minw XW SourceRight click MathML additional feature norm regularization nonnegative equation sparse absolute shrinkage selection operator moreover generates elementwise sparsity irregular sparsity sparse training sample reconstruct exist relation feature sample generally feature highly related reasonable correspond prediction related devise regularization assumption feature involve regress correspond prediction related correspond prediction   relation utilize relation penalize loss function similarity specifically impose relation training sample reflect relation prediction define embed function    SourceRight click MathML additional feature sij denotes feature similarity matrix sij encodes relation feature vector respect similarity vector throughout radial basis function kernel define exp SourceRight click MathML additional feature denotes kernel width similarity matrix construct data adjacency graph regard feature node kNNs along kernel function define compute similarity feature kNNs feature similarity sij feature node otherwise similarity zero sij mathematical transformation obtain regularization  SourceRight click MathML additional feature laplacian matrix definition laplacian matrix indicates relational information sample laplacian matrix indicates relational information feature successfully manifold finally define objective function reconstruction minw XW SourceRight click MathML additional feature equation sequentially loss function norm regularization graph laplacian regularization nonnegative constraint accord loss function graph laplacian regularization convex smooth norm regularization nonnegative constraint convex differentiable convex nonsmooth therefore objective function convex nonsmooth accord iterative optimize objective function convex satisfy global optimum moreover converge global optimum objective function omit proof directly obtain accord optimize obtain optimal matrix correlation training sample wij denotes correlation training sample training sample positive wij indicates training sample training sample positively correlate negative wij correlation negative zero wij correlation training sample training sample training sample predict training sample consequently correlate training sample training sample nonzero coefficient predict training sample training sample understand characteristic propose reconstruction assume optimal SourceRight click MathML additional feature training sample accord propose correlation training sample training sample due nonzero training sample related training sample fourth training sample fifth training sample specifically knn classification regard training sample training sample correspond optimal meanwhile accord regard training sample training sample correspond optimal obviously training sample predict fifth training sample correspond optimal training sample obtain propose reconstruction model moreover optimal knn algorithm sample hence distribution data prior knowledge account optimal training sample kTree knn graph sparse reconstruction GS knn reconstruct sample training sample yield performance however consume predict sample training sample overcome propose training stage construct decision namely kTree training sample correspond optimal motivation relationship training sample optimal kTree enables output optimal sample stage stage complexity faster GS knn fix knn complexity noteworthy propose training stage involve optimize yield optimal training sample construct kTree respectively fortunately offline training stage kTree optimal obtain optimal training sample nonzero coefficient training sample regard optimal label construct kTree training sample correspond optimal ID greedily construct recursive conquer decision difference ID kTree regard optimal training sample label ID label training sample construct decision item leaf node ID label training sample kTree optimal training sample illustrate difference illustration decision ID kTree respectively decision construct item leaf node specifically ID kTree respectively label optimal training sample leaf node optimal subset training sample mathbf ldots mathbf correspond training sample kNNs mathbf ldots mathbf ldots mathbf training sample mathbf ldots mathbf NN mathbf ldots ldots decision kTree illustration decision ID kTree respectively decision construct item leaf node specifically ID kTree respectively label optimal training sample leaf node optimal subset training sample correspond training sample kNNs eij eik training sample  eij decision kTree stage easily obtain optimal sample leaf node kTree conduct knn classification classify sample training sample optimal sample faster varied knn GS knn fix knn complexity pseudo propose kTree algorithm algorithm pseudo propose kTree training sample sample label training stage optimal training sample ID construct kTree training sample correspond optimal optimal training sample leaf node stage obtain optimal sample kTree predict label traditional knn learnt optimal training sample although kTree enables obtain optimal sample conduct knn classification training sample reduce complexity extend kTree improvement version namely complexity stage cardinality subset training sample classification training stage propose construct decision namely kTree described difference information leaf node kTree optimal leaf node optimal information leaf node subset training sample leaf node kNNs sample subset kNNs specifically construct leaf node contains optimal subset training sample regard optimal besides sample denote  denote  leaf node contains optimal illustration stage sample propose construct output optimal leaf node propose selects subset training sample   assigns label accord majority propose knn classification conduct denote cardinality pseudo algorithm algorithm pseudo propose training sample sample label training stage optimal training sample ID construct training sample correspond optimal optimal training sample leaf node stage obtain optimal sample predict label traditional knn learnt optimal principle knn intuitive assumption sample closer feature sample unknown simply compute distance sample training sample assign kNNs sample propose reduce training training sample subset sample sample almost training sample actually experimental IV verify assumption propose achieve classification accuracy kTree traditional knn training complexity kTree stage conduct knn classification subset training sample complexity stage IV experimental public data uci repository machine data data widely academic research evaluate propose compete classification task classification accuracy data data dimensional data dimensional data binary data multiclass data imbalance data evaluate robust propose abalone balance breast australian climate german sample   CNAE gisette libra  arcene feature climate data positive sample negative sample german data positive sample negative sample regard imbalance data employ tenfold validation specifically randomly partition data subset subset remain subset training avoid bias data partition validation compute average model selection parameter compete knn knn applicability domain approach AD knn knn sparse knn GS knn filter attribute subspace bagging inject randomness  ensemble classifier landmark spectral cluster knn LC knn compete detail knn classical classification literature sample respectively report knn applicability domain approach AD knn integrates salient feature knn approach adaptive kernel conduct probability density estimation literature parameter AD knn monte carlo validation maximum knn sparse knn learns sample sparse loss function apply achieve minimal reconstruction error norm regularization utilized obtain elementwise sparsity laplacian regularization preserve local structure data literature validation conduct model selection knn graph sparse reconstruction GS knn training sample reconstruct sample obtain optimal traditional knn conduct classification task literature validation conduct model selection parameter  building ensemble classifier  integrate perturbation training data input attribute parameter landmark spectral cluster knn LC knn propose conduct cluster data cluster training sample conduct knn classification experimental sample conduct classification task sample uci data aim avoid bias imbalanced sample report classification accuracy average classification accuracy iteration horizontal axis indicates sample vertical axis classification accuracy iteration horizontal axis indicates iteration vertical axis accuracy classification accuracy classification accuracy classification accuracy data sample abalone rho rho balance rho rho rho rho rho rho  rho rho australian rho rho climate rho rho german rho rho  rho rho  rho rho classification accuracy data sample abalone balance  australian climate german   zoom subfigure abalone sample zoom subfigure abalone sample zoom subfigure balance sample zoom subfigure balance sample zoom subfigure sample zoom subfigure sample zoom subfigure sample zoom subfigure sample zoom subfigure breast sample zoom subfigure breast sample zoom subfigure australian sample zoom subfigure australian sample zoom subfigure climate sample zoom subfigure climate sample zoom subfigure german sample zoom subfigure german sample zoom subfigure  sample zoom subfigure  sample zoom subfigure  sample zoom subfigure  sample propose kTree improve classification accuracy average versus knn versus AD knn versus  versus LC knn data kTree almost accuracy GS knn knn optimal sample kTree knn GS knn AD knn outperform fix expert predefined knn knn reduce average classification accuracy data versus AD knn optimal sample regard observation achieve minimal LC knn knn kTree  AD knn GS knn knn faster knn abalone data propose scan subset training sample conduct knn classification knn kTree conduct knn classification scan training sample noteworthy LC knn LC knn conduct cluster data scan subset training data however outperform LC knn classification accuracy moreover LC knn suitable achieve performance standard knn knn GS knn AD knn kTree knn knn GS knn training sample reconstruct sample obtain optimal AD knn expensive calculate AD training sample verify sample inside outside AD propose kTree outperform knn AD knn  classification accuracy improve versus knn versus AD knn versus  respectively  data faster knn AD knn  respectively achieve performance GS knn knn faster average faster GS knn knn reduce classification accuracy data propose scan subset training sample conduct knn classification GS knn knn scan training sample experimental feature employ fisher rank feature data informative feature knn classification goal analysis robustness feature classification accuracy data report iteration accuracy II II classification accuracy II classification accuracy classification accuracy data feature  rho rho  rho rho CNAE rho rho gisette rho rho rho rho libra rho rho  rho rho arcene rho rho musk rho rho arrhythmia rho rho classification accuracy data feature   CNAE gisette libra  arcene musk arrhythmia zoom subfigure  feature zoom subfigure  feature zoom subfigure  feature zoom subfigure  feature zoom subfigure CNAE feature zoom subfigure CNAE feature zoom subfigure gisette feature zoom subfigure gisette feature zoom subfigure feature zoom subfigure feature zoom subfigure libra feature zoom subfigure libra feature zoom subfigure  feature zoom subfigure  feature zoom subfigure arcene feature zoom subfigure arcene feature zoom subfigure musk feature zoom subfigure musk feature zoom subfigure arrhythmia feature zoom subfigure arrhythmia feature II clearly kTree achieve classification accuracy comparison improve classification accuracy average data versus AD knn versus knn versus  versus LC knn moreover propose kTree propose GS knn knn classification accuracy intuitively faster kTree knn  data knn data CNAE musk dimension data training sample faster knn dimension sample knn II propose kTree outperform knn AD knn  feature moreover achieve performance GS knn knn faster propose scan subset training sample conduct knn classification GS knn knn scan training sample although LC knn scan subset training sample increase average classification accuracy faster twice LC knn LC knn suitable conclude experimental propose improve performance knn classification accuracy conclusion future propose knn classification algorithm kTree optimal sample efficient effective knn classification propose training stage reduce stage improve classification performance conduct evaluate propose compete experimental outperform compete classification accuracy future focus improve performance propose dimensional data