processing propel explosion model article brief introduction overview architecture  plethora recent summarizes assortment relevant contribution analyze research core linguistic processing issue addition application computational linguistics discussion along recommendation future research introduction processing nlp encompasses variety topic involves computational processing understand increasingly rely data driven computation involve statistic probability machine recent increase computational parallelization harness graphical processing gpus utilizes artificial neural network anns sometimes billion trainable parameter addition contemporary availability data facilitate sophisticated data collection enables training architecture recent researcher practitioner nlp leveraged anns  pioneer recent considerably  significant advance core nlp directly apply achieve practical useful objective article brief introduction nlp neural network dnns extensive discussion nlp article topic publish none extensively within furthermore survey examine application computational linguistics underlie theory traditional nlp task addition discussion recent revolutionary development article useful reader familiarize quickly embark upon advanced research topic nlp AI introduce II core nlp broken subsection namely model morphology parse semantics application practical IV specifically information retrieval IR IV information extraction IV text classification IV text generation IV summarization IV  QA IV machine translation IV conclusion drawn brief summary prediction suggestion future dynamically evolve II overview nlp significant issue attention researcher practitioner introduce  explanation architecture commonly processing nlp computational linguistics involves engineering computational model practical understand useful software nlp subareas core application although sometimes distinguish clearly issue belong core address fundamental model underscore quantify association naturally morphological processing segmentation meaningful component identify  syntactic processing parse diagram precursor semantic processing semantic processing attempt distill meaning component text application involve topic extraction useful information entity relation translation text summarization automatic infer classification cluster document handle core issue successfully apply procedure practical currently nlp primarily data driven statistical probabilistic computation along machine machine approach na√Øve bayes hidden markov model conditional random CRFs decision random vector machine widely however wholesale transformation approach entirely replace enhance neural model neural network neural network compose interconnect node neuron input output node output layer performs sum computation input node generate output nonlinear transformation function summation correction response individual error loss network exhibit output node correction usually network stochastic gradient descent derivative error node approach backpropagation factor distinguish network node layer network node organize sequential layer node input node earlier layer feedforward neural network  consensus exactly defines dnn generally network multiple hidden layer layer convolutional neural network convolutional neural network cnns built upon   derive convolution operation mathematics signal processing cnns function filter simultaneous analysis feature data cnns extensively image video processing nlp important precisely feature locality therefore pool operation minimize feature output convolutional filter pool generally prevent loss precision recursive neural network cnns recursive network minimize training however whereas cnns horizontally within layer recursive net vertically layer particularly appeal allows easy model structure parse recursive network tensor generalize matrix recursively successively recurrent neural network memory network recursive neural network heavily recurrent neural network rnn nlp dependent phoneme useful memory previous processing sometimes backward dependency exist processing beneficial direction backward rnn layer combine output arrangement rnns bidirectional rnn representation sequence rnn layer input linger longer rnn layer longer setup sequential rnn rnn stack highly engineer rnn memory lstm network lstms recursive node compose individual neuron manner retain forget expose specific information whereas generic rnns neuron technically memory dilute successive iteration oftentimes important information recent information important lstm important information retain longer irrelevant information forgotten slightly simpler variant lstm gate recurrent gru perform standard lstms task attention mechanism transformer task machine translation text summarization caption output textual typically encoder decoder encode ann vector decode ann return variable text vector scheme rnn encode entire sequence finite vector without regard input important others encoder decoder architecture multiple option encoders decoder available rnn variant choice particularly latter network attention mechanism decoder portion encode relevant output encoder decoder architecture multiple option encoders decoder available rnn variant choice particularly latter network attention mechanism decoder portion encode relevant output robust attention attention mechanism dense layer annotate rnn hidden network attention accordance hidden annotation mechanism variant mechanism introduce popular convolutional  gate attention attention involves attention encode input beneficial project variable amount attention decode appropriate attention already attention become widely encoder decoder model transformer transformer model encoders decoder stack attention encoder decoder attention encoders decoder multiple instance attention parallel  recurrence convolution transformer become quintessential component neural network nlp transformer model transformer encoders decoder positional encoder inner working encoder contains attention layer layer inner working decoder contains attention layer attentional encoder decoder layer layer transformer model transformer encoders decoder positional encoder inner working encoder contains attention layer layer inner working decoder contains attention layer attentional encoder decoder layer layer residual connection dropout network via backpropagation gradient error vanish explode mitigate activation function rectify linear relu exhibit  steep  gradient response issue others residual connection connection simply skip layer usually alternate layer layer gradient backpropagate network residual network resnet variant exist highway network  another important training anns dropout dropout connection maybe node deactivate usually randomly training batch node deactivate batch network distribute memory across multiple generalization lessen likelihood overfitting training data core nlp core issue inherently computational linguistic perform translation text summarization image caption linguistic task understand underlie understand broken model morphology parse semantics scholarly decade publication volume core nlp publication indexed google scholar relate topic decade experienced growth model grown publication volume core nlp publication indexed google scholar relate topic decade experienced growth model grown model determines extension however individual weakly meaningful derive interaction morphology considers prefix compound  device display tense gender plurality linguistic construct parse considers modify others constituent  structure semantics considers meaning individual relate modify others context knowledge significant amount overlap therefore model analyze classify belonging multiple relevant logical connection interact model embeddings arguably important task nlp model model essential almost application nlp model model predict linguistic component previous component useful application user input predictive ability text entry however versatility emanate implicitly capture syntactic semantic relationship component linear neighborhood useful task machine translation text summarization prediction program generate relevant neural model statistical model inability synonym vocabulary OOV training corpus progress introduction neural model nlp another decade anns heavily model community immediately advantage continued develop sophisticated model summarize evaluation model neural network breakthrough model quantify improvement desirable evaluate model independently application metric propose perfect commonly metric perplexity inverse probability normalize perplexity reasonable measurement modeling data vocabulary metric becomes meaningful luckily benchmark data comparison data penn treebank PTB billion benchmark memory network attention mechanism model network variation attention mechanism network attention mechanism fully hypothesize predict token encode information attentional decode information attentional hinders network parameter perform distinct task simultaneously therefore network node output encode decode information attentional another predict token explicitly network output encode information attentional decode information retrieve wikipedia corpus attention mechanism improve perplexity baseline successively parameter increase previous token hence selection therefore fourth network simply residual connection previous network comparable rnns lstms reasonable achieve simpler network another recent usage residual memory network  model author residual connection skip layer effective closely skip layer residual connection layer fourth layer fifth layer eighth ninth  increase network depth improve batch memory constraint encounter network width importance performance however network harder  capable outperform lstms convolutional neural network model cnn recently model replace pool layer fully layer layer feature reduce dimensional pool layer however whereas reference location feature lose pool layer fully layer somewhat retain information architecture implement multilayer perceptron cnn  filter simply linear instead MLPs multilayer cnn ML cnn multiple convolutional layer stack combination network com kernel filter varied stack convolutional layer detrimental model  com reduce perplexity combine  kernel com analysis network specific finally cnns capture dependency closer importance farther away significance aware neural model cnns nlp embeddings input recent network analyze input instead network unlike previous network accepted input combine embeddings cnn input representation manner embeddings usually representation fed encoder decoder compose highway network gate network resemble lstm lstm network english PTB data czech german spanish french russian arabic non english russian network outperform previously publish data PTB par exist however network trainable parameter considerably others network focus morphological similarity analysis capable previous model handle rare analysis without highway layer  necessarily semantically addition network capable recognize misspell standard instead recognize vocabulary analysis network capable identify prefix understand hyphenate robust model architecture output model model billion benchmark effective model achieve model perplexity billion trainable parameter previous model billion parameter lstm cnn input network performance however achieve ensemble lstms ensemble perplexity surpass previous ensemble perplexity development embeddings neural model prediction unseen synonymous model relationship vector numeric component individual obtain model technique embeddings usually principle component analysis capture internal neural model standard modeling modeling construct specifically purpose typically embeddings dimension overuse distribute representation queen embed vector computation perform obtain highly sensible vector respectively extremely intuitive recent embeddings standard input nlp recent advance challenge model evolve weekly basis introduce generative pretraining GPT pretrained model transformer model IV dependency longer text immediately surround incorporate  capture backward context addition context embeddings model elmo addition capture  multiple layer multiple encoding information capture empirically significantly boost performance additional unsupervised training task random masked prediction prediction  continuous text another predict bidirectional encoder representation transformer bert built upon multitask dnn MT dnn representation model model stochastic network san  bert model pretraining model task tune task MT dnn model achieve eleven attempt task pretrained model excellent headway understand task entailment inference hypothesize model template syntactic within data unrelated logic inference data remove carefully model perform addition recent model universal model amount address resource morphology morphology concerned within stem prefix infix  prefix infix overtly modify stem gender construct  aware model RvNN model morphological structure neural model RvNN model  data segmentation perform  model construct context model insensitive context  morphological structure stem cluster antonym context sensitive model perform relationship stem accounting feature prefix model popular data significantly outperform previous embed model morphological analyzer important nlp task recent examine extent morphology variety neural machine translation NMT model translation model construct translate english french german czech arabic hebrew encoders decoder lstm model attention mechanism aware cnns model wit corpus decoder replace POS tagger morphological tagger fix encoders preserve internal representation encoders examine decoder attach training conclude attention mechanism decrease performance encoders increase performance decoder furthermore aware model superior others morphology output affect performance encoders specifically  output representation encoders analyze morphological model  japanese construct rnn model beam decoder automatically label corpus manually label corpus model perform task jointly morphological analysis POS tag lemmatization model kyoto text corpus kyoto web document corpus outperform baseline task recent morphology universal morphology task considers relationship morphology relate aim ultimate goal morphological analyzer however author knowledge apply task universal parse apply task data already available CoNLL task addition universal morphology development morphological embeddings considers structure aid  processing possibly across  valuable resourced others addition morphological structure important handle specialized biomedical literature become entrench nlp handle morphological component likely improve performance overall model parse parse examines relate within distinct parse constituency parse dependency parse constituency parse  constituent extract hierarchical fashion dependency parse relationship individual recent parse dependency parse within exists another graph parse construct parse graph approach generative model formal grammar construct popular recent graph approach transition approach usually construct parse modification propose standard transition dependency parse buffer stack label onto stack connection arc item dependency popped stack buffer empty label remains stack approach regulate previously described action arc standard approach dependent arc eager approach regardless finally swap lazy approach arc standard approach modify swap stack graph  neural parse application nlp rnns probabilistic context grammar  author aware neural model achieve performance parse  performance achieve PTB label attachment  unlabeled attachment  inside recursive neural network vector representation inner outer data lstm attention mechanism syntactic constituency parser data domain data english web treebank treebank oppose journal portion PTB neural model generalize domain embeddings dependency parse  approach rnn acyclic graph model within journal portion CoNLL task data difficulty transition dependency parse chen   english chinese data english PTB accomplish FFNN decision maker transition parser subvert sparsity persistent statistical model chen greedy replace beam achieve significant improvement improve upon chen deeper neural network residual connection perceptron layer softmax layer significantly typical  potential data sample fed parser sample upon parser training primary parser another model lstm instead feedforward network unlike previous model model knowledge entire buffer entire stack knowledge entire transition decision prediction generate stanford dependency treebank CTB chinese data finally feedforward network global normalization task POS tag compression dependency parse obtain task journal data notably model significantly computation comparable model  alternative algorithm acyclic graph task semantic parse deeper relationship task seek identify action modify addition typical stack buffer transition parse algorithm employ deque representation  although rare english furthermore multiple label addition graph article novel lstm technique lstm subtraction incremental lstm lstm subtraction built previous buffer subtraction vector lstm addition additional lstm deque incremental lstm extension lstm modify acyclic graph incrementally simultaneously model achieve publish evaluation metric semeval task english semeval task chinese apply semantic parse domain QA author knowledge apply semantic parse generative dependency constituent parse propose model rnn grammar parse model approach approach parse approach input addition parse simply local within model achieve english generative parse model attain chinese generative parse   treat parse model lstm assign probability parse achieve model  simply combine model parser candidate another rank superior parser approach combine parser explicitly preferable parser candidate rerank achieve extend model parser achieve finally ensemble model parser construct achieve PTB model  graph approach attentive network similarly attentional model semantic role label subtask semantic parse achieve excellent recurrent convolutional replacement feedforward portion attention mechanism feedforward variant performance another novel approach active perfect semantic parse availability data universal parse universal morphology universal dependency parse universal parse relatively task parse standardize tag relationship across parse varies drastically attempt uniform easy processing  recent development universal grammar challenge ahead mainly development consistency label task gain traction CoNLL task approach task transition parse graph neural parse competitive model neural model ensemble task examine outside CoNLL apply universal dependency parse tweet ensemble bidirectional lstm remain challenge outside universal parse parse challenge investigate building syntactic structure without  training attempt attention lstms outside inside autoencoders approach successful potential environment context resource domain scenario challenge remain focus semantics semantic processing involves understand meaning document embeddings wordvec glove capture meaning distributional hypothesis meaning corollary vector correspond component text neural network representation loosely semantically representative compute  neural semantic processing research distinct semantic similarity portion text capture transfer meaning constituent particularly semantic comparison efficacy approach compute semantics document judged meaning judged similarly program propose cnns perform semantic comparison task model arc inspire siamese network cnns evaluate parallel network connection cnns approach outperform exist model task english chinese building prior yin  propose cnn MI MI  interaction feature consist pretrained cnn model cnn interaction model logistic regressor modify siamese network dynamic cnns addition feature comparison simply feature achieve microsoft research paraphrase corpus  construct feature similarity measurement layer fully layer softmax output layer within cnn convolutional layer network evaluate data  involve compositional knowledge sick data microsoft video paraphrase corpus  achieve  model RvNN lstm node lstm variation examine constituency dependency sick data stanford sentiment treebank constituency model achieve stanford sentiment treebank dependency achieve sick lin another model outperform sick model matrix apply similarity focus layer layer cnn dense layer softmax output similarity focus layer semantically input apply matrix location relation obtain   semeval task   data model extend neural model model attempt capture meaning vector model  attempt model text generate representation dynamic convolutional neural network dcnn filter dynamic max pool layer due dynamic pool feature identify structure without pad input dependency dependency identify dcnn apply task semantic understand outperform comparison model predict sentiment movie review stanford sentiment treebank identification sentiment tweet performer classify trec database requirement understand examination due typical encoder decoder structure NMT IV  testbeds research internal semantic representation encoders english arabic english spanish english chinese english german decode classifier distinct data multi nli expand version SNLI recast data JHU  semantics initiative  plus FN definite pronoun resolution DPR semantic proto role SPR none particularly although SPR conclusion NMT model capture paraphrase information fail capture inference  resolution resolve gender however model  recipient action concurrent analyze quality data inference   training semantic parser domain effective training across domain conclusion drawn lstm model model model encoder decoder network domain input model model decoder domain encoders domain specific encoder  encoder model model encoder decoder domain model overnight data exceptional achieve model performance exhibit model conclusion drawn lstm encoder decoder network analyze embed vector encoder accept english input decoder decoder replicate decoder reproduce english input decoder translate text german french finally fourth decoder POS tagger combination decoder model replicate decoder others structure  data network fed encoders output analyze cluster correspond structure analysis decoder definitive cluster decoder zero error furthermore researcher confirm hypothesis logical arithmetic perform embeddings perform embeddings semantic challenge addition challenge already mention researcher task actual understand integrate network graph wordnet knowledge graph dbpedia endow understand graph embed active research integrate model graph model recently machine understand summary core issue generally perform surpass exist individual core nlp task foundation useful application built however examine research review  complex topic myriad core task  architecture  execute individual core task synthesize  possibly complex distribute neural architecture competence multiple core task fundamentally task superior performance apply task ultimate engineering goal context building effective efficient model successful architecture apply task IV forgo explicit architectural component core task task implicitly researcher argue relevance amount core issue fully justified others argue extensive research understand develop perfectly perform task explicitly implicitly IV application nlp core nlp important understand neural model meaningless engineering perspective application benefit humanity pure philosophical scientific inquiry approach immediately useful nlp task summarize issue involve processing text processing verbal processing expertise topic acoustic processing generally another commonality nlp decade publication volume apply nlp apply nlp witness growth recent growth publication volume apply nlp apply nlp witness growth recent growth information retrieval purpose IR useful information convenient format issue IR primary address pertains rank document respect query relevance hoc retrieval task happens model hoc retrieval text query text document obtain relevance model focus representation interaction individual query document representation focus approach model representation text representation straightforwardly whereas interaction focus approach local interaction directly dnns text interaction document query relevant portion potentially anywhere document distribute query relates portion document helpful mindful specific IR built neural architecture  enhance interaction focus model quantize histogram local interaction intensity mlp parallel query subnetwork establish importance dependency output parallel network mixed relevance document query  achieve performance neural IR model relevance ranker  document stage efficient traditional ranker deem relevant query representation neural  dense document query document collection relevant query impossible anns rank entire collection document contrast standalone neural rank model  prf sparse representation query document mimic traditional approach query shorter document query information document query representation denser achieve training sparsity objective combine hinge loss gram representation query document embed separately individual mlp perform average pool training approach  document obtain retrieve document exist model TF idf BM lack correctly label document ann model approach invert index document network traditional approach retrieval dot compute query document representation obtain retrieval relevance  prf obtain metric nDCG recall across data robust  mac extract query representation pretrained contextualized model elmo bert representation augment exist competitive neural rank architecture hoc document rank  joint model combine bert classification vector architecture benefit approach  contextualized embeddings document rank  improve performance prior model bert token representation information extraction information extraction extract explicit implicit information text output extract data relationship within relational database commonly extract information entity relation participant temporal information tuples entity recognition entity recognition ner refers identification information date price IDs multitask approach task although report approach feedforward network context fix around presumably capture distance relation lstms ner  model ahead network due lack available compute addition sophisticated numeric vector model available slightly baseline english baseline german dnn architecture  jointly input perform sequential classification article perform  annotate portuguese corpus spa CoNLL annotate spanish corpus portuguese corpus  outperform previous across entity achieve performance spanish author alone neither embeddings embeddings  joint feature important effective ner performance chiu  bidirectional lstm cnn resemble santos  without private lexicon detailed information link entity elaborate craft feature CoNLL ontonotes data developed architecture bidirectional lstms conditional random model input embeddings input combine fed bidirectional lstm output fed layer perform crf computation model dropout obtain performance german spanish lstm crf model english dutch achieve without engineer feature gazetteer achieve performance german english ner pretrained bidirectional model retrieve contextual embed BiLSTM crf sequence labeler perform ner extraction extraction concerned identify refer occurrence along participant agent recipient occurrence extraction usually subtasks identify mention identify trigger usually  specify occurrence identify argument identify argument role argue cnns max pool likely capture important information valuable refer address drawback feature instead maximum maximum stage classify trigger  trigger stage align role argument approach significantly outperform rnn encoder decoder identify trigger role exceed earlier latent variable neural model induce schema extract domain achieve data release relationship extraction another important information extract text relationship   synonymous relationship familial geographic relationship approach cnn classify relationship layer embeddings dimension attain prior approach bidirectional lstm cnn relationship classification entity recognition recently attention gru model mechanism network novel data structure coverage mechanism ensure important information extract achieve performance clinical temporal relation extraction pretrained bert model supervise training biomedical data text classification another classic application nlp text classification assignment text document predefined document classification numerous application kim pretrained vector cnn classification kim motivate cnns convolutional layer dense layer dropout softmax output achieve excellent multiple benchmark hyperparameter tune cnn model propose improve upon task cast classification sentiment analysis classification later network employ convolutional layer document classification hybrid architecture combine belief network softmax regression belief network feedforward network hidden layer resemble restrict boltzmann machine unsupervised increase decrease dimensionality data achieve data backward propagation minimum  loss independent label classification portion task therefore without softmax regression output layer architecture pretrained combine regular neural net backpropagation quasi newton bert obtain classification document data promising nlp text classification necessarily hurdle  kalita task classify genre gradient boost superior neural network cnns lstms text generation nlp task generation summarization machine translation convert text another sequence sequence seqseq fashion task image video caption automatic sport reporting convert  data text task however text without input data convert amount topic task poetry generation joke generation generation poetry generation poetry generation arguably hardest generation subtasks addition creative content content deliver  manner usually specific structure task textual output recurrent model standard however recurrent network internal model structure output adhere style address style issue training poet style chinese poetry training data adequate achieve structure address hopkins  generate  poetry training network ensure adhere  structure evaluator judged quality indistinguishable another approach poetry generation pretrained model specifically GPT model successor GPT model hypothesize alongside sequence sequence attention model inherently text generation training vast data pretrained GPT model arguably effective prolific neural generator  kalita parameter GPT model generate quality english demonstrate elicit emotional response reader GPT model available parameter  billion parameter tucker kalita generate english spanish ukrainian hindi   model astonish GPT pretrained english corpus training another believable generator poetry joke pun generation another attention joke pun generation generate  pun pun multiple meaning lstm network ambiguity introduce multiple meaning although pun humorous generate pun classify evaluator machine generate majority author training pun data alone sufficient generate pun ren yang lstm generate joke training data collection joke   joke pertain network news article context joke  saha generate joke quote tweet neural network additional input specify knowledge  increase quality joke generation poetry humor generation gain traction generation recent rnn variant attention liner description another recent lstms generate input specify sad model successfully coherence  recent attempt task mechanism focus action entity important constraint generate generally become incoherent lose direction shortly address skeleton model important information capture important information modest evaluation approach model date focus overview component convert text generate image tiered network construct conceptual overview convert overview hierarchical approach cnns  approach blind comparison evaluator addition attention perplexity developed fusion model pretrained model improvement concur document hierarchical fashion reproduce hierarchical fashion achieve text generation gans generative adversarial network gans likeness generate text network reading output gans concept minimax player generative network discriminative network discriminator attempt generative network training generator maximize mistake discriminator  gan difference embed output token content evaluate directly without respect specific grammar simply standard metric minimize loss reinforcement text generation model taught model attempt optimize metric generate evaluate meaningful another modify gan refer  text generation employ lstm generator cnn discriminator achieve promising bilingual evaluation  bleu tendency reproduce realistic gans increase text generation recently text generation VAEs another network variational autoencoder VAE gans attempt output indistinguishable model discriminator actual sample VAEs attempt output sample training recent VAEs text generation adapt module topic sequence generation summary text generation humor poetry generation understudied topic machine generate text improve desire personality text almost certainly emerge hence research increase generation improve coherence longer address propose nucleus sample counteract perform GPT model addition issue lack creativity coherence metric sort creative task therefore evaluation norm utilize amazon mechanical turk however recent propose metric reliable automatic evaluation generate text addition creative task survey others previously gatt  image caption survey recently task generate text textual input IV IV summarization summarization document encapsulation important content primary summarization extractive abstractive focus extraction simplification reorder concatenation relay important information document text directly document abstractive summary rely express document content generation style abstraction possibly document introduce summarization FFNN model encoder generative beam decoder initial input directly model convolutional attention encoder contextual importance surround summary performance model comparable model attention mechanism improve performance encoder decoder model  kalita various attention model abstractive summarization approach developed multiple  attention encoder mechanism input text token output token decoder previously generate hybrid entropy loss function propose decrease training execution magnitude finally recommend strategy reinforcement modify gradient reduce exposure bias model exclusively via supervise attention boost accuracy fully convolutional model propose implement attention mechanism layer propose encoder decoder framework generate output sequence input sequence stage manner encode input sequence bert decoder stage stage transformer decoder generate draft output sequence stage masked draft sequence fed bert combine input sequence draft representation generate bert transformer decoder predict refine masked model achieve performance cnn daily mail york data summarization information extraction QA relevant document QA coherently return information response request resemble summarization gate attention recurrent network passage attention mechanism refine machine representation mapping entire passage pointer network predict location boundary network attention pool vector representation passage analyze model critical token  cnns automatically analyze multiple viewpoint parallel network extract pertinent information input network context information relationship return output network combine rank relational network RNs summarization propose RNs built upon mlp architecture focus relational define relationship entity data feedforward network implement function aggregate correlation input RNs lstm representation document input representation information request bert achieve QA squad squad data demonstrate QA integrates bert source  IR toolkit identify corpus wikipedia article fashion obtain standard benchmark collection machine translation machine translation quintessential application nlp involves mathematical algorithmic technique translate document another perform effective translation intrinsically onerous proficiency morphology syntax semantics adept understand  cultural sensitivity associate society consideration attempt NMT  although neural model previously task  convert text  feedforward network input output pad trim ability translate another introduction encoder decoder model model   stem continuous recurrent representation capture syntax semantics morphology addition ability rnns robust model NMT encoder decoder model combination generative convolutional recurrent layer encode optimize source model cast target model quickly rework numerous novel effective advance model encoder decoder model continuously define expand dozen layer residual connection attention mechanism residual attention mechanism decode layer attend encode layer achieve numerous convolutional layer encoder decoder information hierarchical layer multitude recurrent derive model continually improve shortcoming predecessor overcome engineering recent progress effective initialization decoder hidden conditional gate attentional removal bias embed layer alternative decode phase factorization embeddings beam algorithm standard initialization decoder propose backward encoder however average embed annotation layer translation gate recurrent standard sequence sequence task variation conditional gru  effectively utilized attention mechanism  consists component gru transition attention mechanism combine previous hidden along attention context generate hidden alter decode input generate output token update hidden representation update generate simplify decode source attribute morphological segmentation label POS tag syntactic dependency label improves model concatenate factorize embeddings increase robustness dependency vertically stack recurrent standard optimum layer roughly desire input presence density residual connection beam algorithm beside softmax layer multiple target prediction greedy fashion prediction without entire hypothesis direction diverge previous propose discard recurrent convolutional layer instead focus exclusively attention mechanism encode globally input output prefer attention mechanism traditional layer motivate principle reduce complexity computation per layer minimize sequential training finally  input output  dependency sequence task apart increase accuracy across translation task attention model parallelization throughout architecture decrease training minimize sequential model generate english german english french international workshop spoken translation  corpus  kalita modify model propose  parallel attention mechanism stack model addition improve bleu reduce training recently apply bert machine translation task constant model achieve relatively competitive performance   attain perform unsupervised machine translation multiple model pretraining recent model examine model picked apart feature truly responsible strength comparison hybrid model knowledge incorporate previous model outperform previous model addition model attentive component recurrent component model stack parallel technique employ crucial model neutral model examine label smooth multihead attention layer normalization synchronous training another   examine technique recommend adam optimization restart multiple rate anneal perform subword translation ensemble decoder furthermore technique model additional benefit actually hurt model lexicon bias prim output directly translate  translation another model usually quality additional input dropout however data bootstrapping training additional independent sample advantageous model already perform recommend future development perform model realm effectiveness addition recommendation challenge neural machine translation model superior statistical machine translation model model significantly data perform poorly outside domain fail handle rare adequately furthermore attention mechanism perform statistical counterpart align beam decode surely drawback focus research addition mention NMT model struggle semantic concept likely focus upcoming examine  NMT predict future research development nearly impossible model report daily advancement survey become outdated notable recent advancement cache network context simply individual translate ability handle rare ability translate understudied  addition conduct selection sensitivity tune hyperparameters denoising data important topic surround NMT finally machine translation groundbreaking research multilingual translation fairly recent neural network convert automatically recognize source simply input token identify output furthermore model capable understand somewhat multilingual input mixed output multiple token sometimes related actually suggests dnns capable universal representation information independent possibly capable  relationship summary nlp application numerous application nlp exist grammar correction processor author mimic sufficient data generates text replicate style writer application infrequently understudied expose however sentiment analysis become increasingly popular utilize semantic task extraction writer sentiment positive negative neutral inclination application varied research future prediction social medium analysis classification spam ensemble lstms cnns apply usage nlp countless conduct  facilitate successful variety application  refer survey specific recommendation practitioner individual trend model application pretrained stack transformer configuration encoder decoder configuration encoders attention  transformer become norm along attention encoder decoder decoder recent article transformer replace lstm  pretraining transformer model become accepted endow model generalize knowledge model bert corpus billion available practitioner model posse amount knowledge already practitioner corpus desire training enormous pretraining model model perform task practitioner available downloadable task specific corpus task specific corpus training usually supervise recommend task perform multitask training wherever conclusion application nlp   algebra solver program severely constrain conversational   another converse manipulate  nowadays highly advanced application nlp ubiquitous google microsoft machine translator translate  device command respond emergence sophisticated application particularly deployed setting  impressive accomplishment domain sixty without doubt incredible progress particularly recent progress causal relationship remarkable advance anns technology decade machine construct  progress unprecedented rate performance myriad task miscellaneous neural architecture instill model performance task imperfect metric consolidate analysis model survey trend  convolutional recurrent specimen contribute recent however stack attention transformer encoders decoder consistently superior across terrain nlp model generally heavily pretrained knowledge unsupervised supervise manner somewhat lightly specific task supervise fashion attention mechanism alone without recurrence convolution connection encoders decoder network examine feature perform multiple task usually improves finally highly engineering network usually optimizes substitute cultivate network quantity quality data although pretraining generic corpus immensely observation useful research effort pretraining methodology develop highly specialized component squeeze performance complex model numerous stellar architecture propose highly competitive  identify architecture evaluation complexity data evaluate model generate specifically model although consolidated data encompass task glue emerge feature data highly variable comparison subfields nlp benefit extensive discussion regard content data compilation addition variability evaluation data numerous metric evaluate performance task oftentimes model metric report agreement metric ensure comparison furthermore metric usually report mention average variability important understand performance model important understand standard performance model highly variable attempt report undesirable model consistently relatively performance preferable increasingly randomize parameter reduce variation performance variance exist necessitate reporting metric recommendation future wider variety currently vast majority research nlp conduct english another sizeable portion mandarin chinese translation task english almost input output usually dozen european eastern asian neglect entire linguistic intricacy express therefore capture nlp software furthermore spoken throughout spoken meaning research excludes immense  collection validation data  nlp model data tremendous contribution nlp society due amount data available author foresee  traditional nlp model future model shallow anns extremely data hungry  traditional model relatively amount training data however anticipate model become norm computational linguistics pretraining transfer highly impactful role spark revolution nlp although contribution unified model realize widely instead neural network introduce traditional nlp task  parse model implement  structure simply anns decision previously probability model versatile architecture obviously become reality understand abstract concept handle network important understand network furthermore abstraction hallmark intelligence understand abstraction inside ann aid understand intelligence underlie linguistic ability  linguistic processing artificial intelligence understand component interrelate important construct AI unified nlp architecture another reality goal aid advance computational equipment gpus significantly improve ability network direction wider availability chip specifically purpose google tensor processing tpu microsoft  intel crest ultimately anns implement traditional von neumann style computer potential luckily another computer engineering  recent neuromorphic compute neuromorphic chip implement neural structure hardware widely continuation longevity highly anticipate ensure opportunity sustain progress nlp