variant memory lstm architecture recurrent neural network propose inception recent network become model variety machine renew understand role utility various computational component typical lstm variant analysis lstm variant representative task recognition handwrite recognition  model hyperparameters lstm variant task optimize separately random importance assess powerful functional analysis variance framework summarize experimental cpu lstm network none variant improve upon standard lstm architecture significantly demonstrate forget gate output activation function critical component hyperparameters virtually independent derive guideline efficient adjustment introduction recurrent neural network memory lstm emerge effective scalable model related sequential data earlier attack tailor specific dependence lstms effective capture temporal dependence suffer optimization hurdle plague recurrent network SRNs advance handwrite recognition generation model translation acoustic model synthesis protein secondary structure prediction analysis audio video data others central lstm architecture memory maintain nonlinear gate regulate information incorporate improvement lstm architecture formulation however lstms apply significantly improvement systematic utility various computational component comprise lstms gap systematically address improve lstm architecture detailed schematic srn lstm hidden layer recurrent neural network detailed schematic srn lstm hidden layer recurrent neural network evaluate popular lstm architecture vanilla lstm II variant thereof benchmark acoustic model handwrite recognition  model variant differs vanilla lstm allows isolate performance architecture random perform hyperparameters variant enable reliable comparison performance variant insight gain hyperparameters interaction functional analysis variance  II vanilla lstm lstm setup commonly literature originally described  schmidhuber refer vanilla lstm reference comparison variant vanilla lstm incorporates  schmidhuber lstm gradient training description lstm schematic vanilla lstm feature gate input forget output input constant error  output activation function peephole connection output recurrently input gate pas input vector lstm input lstm layer input RN recurrent RN peephole RN bias RN vector formula vanilla lstm layer pas       input    forget     output  output SourceRight click MathML additional feature pointwise nonlinear activation function logistic sigmoid gate activation function hyperbolic tangent tanh usually input output activation function pointwise multiplication vector denote backpropagation delta inside lstm calculate                    SourceRight click MathML additional feature vector delta layer loss function formally corresponds recurrent dependence delta input layer training compute      SourceRight click MathML additional feature finally gradient calculate denotes outer vector      tct  SourceRight click MathML additional feature lstm initial version lstm possibly multiple input output gate forget gate  peephole NP connection output gate bias input activation function omit training mixture recurrent backpropagation BPTT gradient propagate gradient recurrent connection truncate gradient training another feature version gate recurrence  gate recurrent input gate previous addition recurrent input output feature later forget gate modification lstm architecture introduce forget gate enable lstm reset continual task embed  grammar peephole connection  schmidhuber argue precise timing gate output gate peephole connection connection gate curve architecture precise timing easy addition output activation function omit evidence essential lstm gradient modification vanilla lstm  schmidhuber BPTT training lstm network architecture described II TIMIT benchmark BPTT advantage lstm gradient checked finite difference practical implementation reliable variant introduction vanilla lstm commonly architecture variant introduction BPTT training training extend kalman filter enable lstm pathological computational complexity propose hybrid evolution instead BPTT training retain vanilla lstm architecture evolve lstm architecture maximize fitness context sensitive grammar later introduce linear projection layer project output lstm layer recurrent connection reduce amount parameter lstm network introduce trainable parameter slope gate activation function  improve performance lstm offline handwrite recognition data dynamic cortex memory improve convergence lstm recurrent connection gate propose simplify variant lstm architecture gate recurrent gru neither peephole connection output activation function couple input forget gate update gate finally output gate reset gate gate recurrent connection input perform initial comparison gru vanilla lstm report mixed IV evaluation setup focus empirically lstm variant achieve therefore setup comparison vanilla lstm baseline evaluate variant variant remove modifies baseline exactly aspect allows isolate evaluate data domain account domain variation comparison setup variant variant setting hyperparameters performance interested performance achieve variant chose tune hyperparameters rate amount input individually variant hyperparameter impossible traverse completely random obtain perform hyperparameters combination variant data random chosen benefit data analyze various hyperparameters performance lstm variant data data split training validation optimize hyperparameters evaluation TIMIT TIMIT corpus reasonable acoustic model benchmark recognition manageable focus  classification task data objective classify audio frame phone raw audio extract mel frequency cepstrum coefficient ham stride  coefficient preprocessing standard recognition chosen comparable earlier lstm coefficient along derivative comprise input network normalize zero variance performance classification error percentage training validation split  sequence frame average restrict core establish subset TIMIT corpus split training validation detailed  core SA sample training validation built discard sample IAM online IAM online handwrite database consists english series pen movement mapped IAM  data split training validation contains multiple handwritten consist stroke per sequence validation training validation sequence respectively training IAM  dataset transcription label sequence training IAM  dataset transcription label sequence handwrite accompany target sequence assemble ASCII   label training contains sequence image stroke percent remove subsampled sequence training harm performance frame sequence vector pen stroke fourth dimension contains pen transition stroke zero within stroke explicitly marked additional preprocessing baseline   correction network  temporal classification CTC error function output plus empty label performance error rate cer decode decode JSB chorale JSB chorale collection harmonize chorale bach consist chorale  minor preprocessed piano  piano generate transpose midi sequence minor sample frame quarter network prediction minimize negative likelihood data consists sequence training validation respectively average network architecture training network lstm hidden layer sigmoid output layer JSB chorale task bidirectional lstm TIMIT IAM online task consist hidden layer processing input backward softmax output layer loss function employ entropy error TIMIT JSB chorale IAM online task CTC loss initial network drawn normal distribution standard deviation training stochastic gradient descent nesterov style momentum update sequence rate rescale factor momentum gradient compute BPTT lstms training epoch improvement validation epoch lstm variant vanilla lstm II refer vanilla activation function standard logistic sigmoid hyperbolic tangent derive variant architecture report difference pas formula II   input gate  forget gate  output gate  input activation function  output activation function  couple input forget gate NP peephole       SourceRight click MathML additional feature  gate recurrence             SourceRight click MathML additional feature variant explanatory  variant gate gate input recurrent connection modification lstm refer grus equivalent instead forget gate independently  variant recurrent connection gate formulation lstm additional recurrent matrix significantly increase parameter hyperparameter efficiently hyperparameters random advantage easy implement trivial parallelize uniformly thereby improve analysis hyperparameter importance perform random combination variant data random encompasses trial trial randomly sample hyperparameters lstm per hidden layer uniform sample rate uniform sample momentum uniform sample standard deviation gaussian input uniform sample TIMIT data additional boolean hyperparameters tune data choice traditional momentum nesterov style momentum analysis measurable performance latter arbitrarily chosen clip gradient hurt overall performance therefore gradient clipped data unlike earlier parameter fix variant variant parameter differently fix bias comparison discussion amd opteron CPUs 5GHz average sum cpu computation TIMIT performance trial classification error  report likelihood  JSB chorale data  lstm IAM online data cer NP previously publish cer extensive preprocessing goal comparison lstm variant rough orientation reader performance trial trial accord validation data variant percentile data whisker dot median data variant significantly vanilla lstm histogram background average parameter performer variant performance trial trial accord validation data variant percentile data whisker dot median data variant significantly vanilla lstm histogram background average parameter performer variant comparison variant summary random welch significance performance variant significantly baseline variant highlight performance differs significantly performance vanilla lstm distribution performance conclusion drawn therefore specific choice reasonable hyperparameters setting variant effective variance tend variant data significant difference conclusion restrict analysis perform trial combination data variant finding dependent chosen representative reasonable hyperparameter tune effort important observation remove output activation function  forget gate  significantly hurt performance data apart cec ability forget information squash critical lstm architecture indeed without output activation function output principle unbounded couple input forget gate avoids render output nonlinearity important explain gru performs without input forget gate couple  significantly performance data although performance improve slightly model similarly remove peephole connection NP significant performance improve slightly handwrite recognition variant simplify lstms reduce computational complexity worthwhile incorporate architecture  significantly performance TIMIT IAM online JSB chorale data variant greatly increase parameter generally advise feature proposal lstm absent remove input gate  output gate  input activation function  significant reduction performance handwrite recognition however significant model performance statistically insignificant average performance improvement   architecture model hypothesize behavior generalize model supervise continuous data handwrite recognition input gate output gate input activation function crucial obtain performance impact hyperparameters  framework assess hyperparameter importance observation marginalize dimension efficiently regression allows predict marginal error hyperparameter average others traditionally hyperparameter grid whereas hyperparameter sample random average performance slice hyperparameter obtain training regression sum prediction along correspond subset dimension precise random regression prediction performance average improves generalization allows estimation uncertainty prediction obtain marginals decompose variance additive component  insight overall importance hyperparameters interaction rate rate important hyperparameter therefore important understand correctly achieve performance rate affect predict average performance important average hyperparameters regression shade around curve indicates standard deviation prediction hyperparameters quantify reliability average predict average training predict marginal error marginal rate hidden input data shade standard deviation predict marginals reliability predict performance plot vanilla lstm curve variant significantly predict marginal error marginal rate hidden input data shade standard deviation predict marginals reliability predict performance plot vanilla lstm curve variant significantly plot optimal rate dependent data data basin magnitude rate inside performance related unsurprising observation sweet rate basin performance training therefore rate lstm sufficient coarse performance increase variance rate due interaction rate hidden layer detail suggests rate quickly tune network pie variance performance attribute hyperparameters percentage variance due interaction multiple parameter pie variance performance attribute hyperparameters percentage variance due interaction multiple parameter hidden layer surprisingly hidden layer important hyperparameter affect lstm network performance network perform diminish return training increase network factor increase computation epoch convergence input additive gaussian input traditional regularizer neural network lstm however almost hurt performance slightly increase training exception TIMIT dip error momentum unexpected momentum affect neither performance training significant observation none data momentum account variance performance TIMIT interaction rate momentum account variance rate hidden reveal interpretable structure choice rate dependent momentum IV observation momentum substantial benefit training lstms online stochastic gradient descent analysis variance performance variance attribute hyperparameters obvious rate important hyperparameter accounting variance important hyperparameter hidden layer input momentum variance interaction important role TIMIT important data interaction hyperparameters hyperparameters interact performance individually interaction explain variance performance understand interaction combination hyperparameters visualize interaction hyperparameters marginal performance respective hyperparameters average performance predict decision marginalize hyperparameters therefore version performance plot marginal predict performance hyperparameters variation due interaction plot vertically subplots data TIMIT IAM online JSB chorale subplots horizontally triangular matrix matrix hyperparameters rate momentum hidden input combination encodes performance classification error TIMIT cer IAM online negative likelihood JSB chorale data datasets marginal predict performance hyperparameters variation due interaction plot vertically subplots data TIMIT IAM online JSB chorale subplots horizontally triangular matrix matrix hyperparameters rate momentum hidden input combination encodes performance classification error TIMIT cer IAM online negative likelihood JSB chorale data datasets employ anova illustrate interaction hyperparameters variance performance explain hyperparameter remove hyperparameters interact perfectly independent residual zero hidden rate TIMIT data performance varies strongly along axis rate decrease increase rate along axis hidden performance decrease slightly roughly hidden plot hyperparameters interaction differs completely independent exhibit structure sample properly analyze interplay however observation worth effort plot hidden optimal rate interaction IAM online JSB data rate input rate input marginals however trend revers rate input beneficial practical relevance performance generally rate apart however discern regularity analyze hyperparameter interaction conclude practical attend interplay hyperparameters therefore practical purpose hyperparameters treat approximately independent optimize separately VI conclusion report variant lstm architecture conclude commonly lstm architecture vanilla lstm performs reasonably various data none investigate modification significantly improves performance however modification couple input forget gate  remove peephole connection NP simplify lstms without significantly decrease performance variant attractive reduce parameter computational lstm forget gate output activation function critical component lstm remove significantly impairs performance hypothesize output activation function prevent unbounded propagate network destabilize explain lstm variant gru perform reasonably without bound couple input forget gate rate crucial hyperparameter network surprisingly momentum unimportant online gradient descent gaussian input moderately helpful TIMIT harmful data analysis hyperparameter interaction reveal apparent structure furthermore interaction rate network implies practical purpose hyperparameters treat approximately independent rate tune fairly network experimentation neural network tricky practitioner already understood remain hurdle newcomer practical choice intuition expert gain attempt intuition experimental insight architecture selection hyperparameter tune lstm network emerge choice complex sequence future explore complex modification lstm architecture