vast quantity information data evolve computer hardware encourages machine community meanwhile challenge gaussian regression GPR nonparametric interpretable bayesian model suffers cubic complexity data improve scalability retain desirable prediction quality variety scalable gps however comprehensively review analyze understood academia review scalable gps GP community timely important due explosion data article devote review scalable gps involve category global approximation  entire data local approximation data subspace particularly global approximation mainly focus sparse approximation comprise prior approximation modify prior perform inference posterior approximation retain prior perform approximate inference structure sparse approximation exploit specific structure kernel matrix local approximation highlight mixture expert conduct model average multiple local expert boost prediction review recent advance improve scalability capability scalable gps review finally extension issue scalable gps various scenario review inspire novel future research avenue introduction era data vast quantity information demand effective efficient analysis interpretation prediction explore benefit ahead data machine community challenge gaussian regression GPR kriging  surrogate emulator computer nonparametric statistical model various scenario active multitask manifold optimization data GP community mainly refers challenge volume amount data analyze incur computational complexity GP paradigm worth review mainly focus scalable gps regression gps machine model training observation GP seek infer latent function functional GP define kernel prominent weakness standard GP cubic complexity due inversion determinant kernel matrix knn limit scalability GP unaffordable data hence scalable gps devote improve scalability GP retain favorable prediction quality data extensive literature review summarize classifies scalable gps category global approximation approximates kernel matrix knn global distillation distillation achieve subset training data subset data SOD kernel matrix  remove uncorrelated entry knn sparse kernel sparse kernel matrix zero entry rank representation induce training sparse approximation nyström approximation knn   local approximation conquer focus local subset training data efficiently local approximation tackle local expert data addition smooth prediction equip valid uncertainty model average employ mixture expert percentage category scalable gps global approximation local approximation literature survey percentage category scalable gps global approximation local approximation literature survey depict scalability sparse approximation induce local approximation data expert training complexity parallel distribute compute organize induce kronecker structure sparse approximation reduce complexity meantime reorganize variational bound stochastic optimization available sparse approximation remarkable complexity enable regression billion data comparison scalable gps regard scalability model capability alpha induce sparse approximation subset SOD local approximation comparison scalable gps regard scalability model capability induce sparse approximation subset SOD local approximation notable welcome gps scalability favorable prediction model capability remarkable complexity cannot SOD perform increase model capability global approximation capable capture global spatial correlation filter local due limited global induce contrast due localization local approximation capture local nonstationary feature enable outperform global approximation complicate task solar drawback however ignore global risk discontinuous prediction overfitting recently attempt improve model capability interdomain strategy hierarchical structure hybrid global local approximation neural network nns showcasing performance development scalable gps demand comprehensive review methodological characteristic comparison understand knowledge detailed survey various scalable gps conduct literature review GP community timely important due explosion data skeletal overview classify review analyze scalable gps specifically introduction standard GP regression II category scalable gps global local approximation comprehensively review IV moreover review improvement scalable gps scalability capability thereafter VI discus extension scalable gps scenario highlight potential research avenue finally vii conclude remark skeletal overview scalable gps skeletal overview scalable gps II GP regression revisit nonparametric GPR GP prior latent function GP function zero kernel smoothness exponential SE function equip automatic relevance determination    TΔ sourcewhere diag comprises along dimension signal variance conventional kernel  kernel refer training data obtain model evidence marginal likelihood kϵnn kϵnn knn  hyperparameters infer maximize logp  kϵnn kϵnn SourceRight click MathML additional feature automatically achieves bias variance tradeoff omit conditioning distribution clarity thereafter predictive distribution variance respectively express kϵnn kϵnn sourcewhere alternatively interpret GP extension bayesian linear model SourceRight click MathML additional feature gaussian prior dimensional input dimensional feature equivalently derive kernel  particularly SE kernel recover infinite gaussian basis function everywhere computational bottleneck linear kϵnn determinant kϵnn traditionally cholesky decomposition kϵnn  kϵnn LT kϵnn  prediction variance per  scalable gps extensively recent improve scalability data classify scalable gps global approximation local approximation comprehensively analyze showcase methodological characteristic global approximation global approximation achieve sparsity kernel matrix knn crucial scalability subset training data SOD remove entry knn correlation sparse kernel employ rank representation sparse approximation subset data SOD simplest strategy approximate GP subset  training data hence SOD retains standard GP inference complexity operates  comprises data recent theoretical analyzes error bound prediction generalization SOD graphon framework accuracy tradeoff comparison approximation review sufficiently SOD reasonable prediction redundant data struggle overconfident prediction variance due limited subset regard selection  randomly cluster technique KD partition data subset centroid subset employ active criterion differential entropy information gain pursuit sequentially query data however compute sparse kernel sparse kernel attempt directly achieve sparse representation knn via particularly compactly CS kernel imposes exceeds threshold therefore nonzero involve calculation GP CS kernel challenge construct valid CS kernel positive semidefinite psd   besides GP CS kernel potential capture local due truncation sparse approximation typically eigendecomposition eigenvalue approximate rank kernel matrix knn  inversion calculate via sherman morrison woodbury formula kϵnn      sourceand determinant sylvester determinant theorem kϵnn    SourceRight click MathML additional feature complexity however eigendecomposition limited operation hence approximate eigenfunctions knn data nyström approximation knn qnn   sourcewhich greatly improves kernel enables naive nyström GP scalable GP however negative prediction variance generative probabilistic model nyström approximation impose training data cannot guarantee psd kernel matrix inspire influential nyström approximation sparse approximation generative probabilistic model achieves sparsity via induce refer active  optimally summarize dependence training data introduce induce induce variable akin GP prior  besides assume sufficient statistic variable recover joint prior marginalize  sparse approximation category prior approximation approximates prior performs inference posterior approximation retains prior performs approximate inference structure sparse approximation exploit specific structure kernel matrix prior approximation prior approximation modify joint prior origin cubic complexity independence assumption  sourcewhere training conditionals nyström notation      knn qnn  sourcewhere induce variable dependency induced obtain computational gain modify training conditionals    SourceRight click MathML additional feature logp approximate logq logq  qnn  qnn  sourceit specific selection enable calculate qnn  qnn  substantially reduce complexity particularly subset regressors sor deterministic induce conditional DIC imposes deterministic training conditionals      SourceRight click MathML additional feature equivalent apply nyström approximation training data degenerate GP rank kernel   SourceRight click MathML additional feature alternatively interpret sor GP kernel infinite expansion input feature define dense basis function equivalent bayesian linear model infinite hence relevance vector machine rvm basis function approximation  knn  sourcewhere  σmm consequence rvm GP function kernel   SourceRight click MathML additional feature recovers  σmm   however depict sor approximation rvm model impose restrictive assumption training data overconfident prediction variance training data illustration sparse approximation toy mathrm sinc epsilon epsilon sim mathcal training initial location induce whereas optimize location induce dot curve prediction GP curve confidence interval GP prediction curve prediction sparse approximation shade confidence interval prediction sparse approximation ski optimize induce prior approximation sor suffers overconfident prediction variance training data FITC capture  variance guaranteed converge GP overlap induce differently VFE stochastic variant  approximate GP due posterior approximation finally greatly reduce complexity structure induce ski discontinuous prediction illustration sparse approximation toy sinc training initial location induce whereas optimize location induce dot curve prediction GP curve confidence interval GP prediction curve prediction sparse approximation shade confidence interval prediction sparse approximation ski optimize induce prior approximation sor suffers overconfident prediction variance training data FITC capture  variance guaranteed converge GP overlap induce differently VFE stochastic variant  approximate GP due posterior approximation finally greatly reduce complexity structure induce ski discontinuous prediction reverse uncertainty behavior sor rvm heal augment basis function however compute augmentation alternatively sparse spectrum GP  variational variant elegantly address issue reconstruct bayesian linear model spectral representation fourier feature stationary kernel    SourceRight click MathML additional feature spectral frequency another impose informative assumption instance deterministic training conditional DTC training conditional    SourceRight click MathML additional feature retains conditional hence prediction sor prediction variance sor grows prior induce notably due inconsistent conditionals DTC GP besides DTC sor perform due restrictive prior assumption alternatively fully independent training conditional FITC remove dependence vnn knn qnn training conditional    diag vnn  conditional retains variance identical due correlation diag vnn hence sor DTC away uncertainty FITC partially retains closer approximation prior moreover independence assumption extend derive fully independent conditional  model nondegenerate GP kernel   δij  SourceRight click MathML additional feature δij kronecker delta  constant prior variance stationary alternatively approximation derive minimize kullback leibler KL divergence KL quantifies similarity approximate joint prior improve partially independent training conditional  training conditional     vnn SourceRight click MathML additional feature equivalent partition training data independent subset account joint distribution subset however argue closer approximation  brings improvement FITC issue address extend partially independent conditional pic particularly FITC prediction variance      diag vnn  diagonal correlation diag vnn posterior variance hence variance zero exactly enable FITC capture  invalidate estimation nearly zero variance sacrifice accuracy prediction finally  FITC another approximation attempt achieve desirable predictive accuracy compute faithfully recover standard GP increase indeed prior approximation recover GP however configuration global optimum maximize logq  besides induce via optimization prediction issue address posterior approximation review posterior approximation prior approximation posterior approximation retain prior perform approximate inference elegant variational VFE propose  variational inference VI instead modify prior VFE directly approximates posterior central task statistical model introduce variational distribution KL divergence KL logp logp logp SourceRight click MathML additional feature expectation distribution minimize rigorously define KL equivalent maximize logp constant evidence bound ELBO VFE permit jointly optimize variational parameter hyperparameters maximize respect hyperparameters directly improves maximize respect variational parameter implicitly approximation posterior evidence derive tighter bound calculus variation optimal variational distribution remove dependence relevant derivative zero collapse bound    vnn SourceNote  differs  trace however substantially improves inference quality maximize  decrease trace vnn variance predict latent variable particularly vnn recover GP hence trace regularizer guard fitting seek deliver induce improves increase theoretical analysis implies resource VFE recover GP contrast without trace DTC risk overfitting regard improvement VFE extend continuous discrete input efficient QR factorization optimization induce hyperparameters estimation induce improve augment feature interdomain strategy author argue similarity induce euclidean inconsistent kernel hence assign mixture prior latent feature derive regularize bound induce kernel besides matthew bridge gap variational induce framework KL divergence stochastic interpretation approximate infinite joint prior comprises inferential posterior distribution model evidence minimize KL divergence encourages approximation posterior evidence hence FITC VFE interpret jointly  logq   sourcewhere logq  vnn recover FITC VFE besides hybrid approximation moderate prediction improve scalability VFE retain variational distribution obtain relaxed bound logp KL SourceRight click MathML additional feature sum due observation hence stochastic gradient descent sgd encourages employ obtain unbiased estimation minibatch logp  KL SourceRight click MathML additional feature due difficulty optimize variational parameter euclidean employ stochastic VI svi gradient remarkable complexity interestingly online anytime fashion therefore crucial stochastic variational GP  sparse GP subset training data iteration scalability desirable approximation  drawback bound tight  optimally eliminate optimizes variational parameter epoch training introduction svi brings empirical requirement carefully parameter sgd inspire  derive factorize variational bound gps augmentation allows flexible basis function incorporate various rank structure composite nonconvex bound enable speedup asynchronous proximal gradient algorithm deploy variational model distribute machine platform  author GP billion data similarly cheng boot derive stochastic variational framework difference variance respectively decouple basis function flexible inference besides recent novel unify anytime variational framework akin  variational framework accommodate exist sparse approximation sor DTC pit via efficient sgd achieves asymptotic convergence predictive distribution chosen sparse model conduct reverse VI reverse prior conventional GP prior variational distribution FI PI maximum ELBO finally scalability  reduce nearly introduce kronecker structure induce variance   model improve bayesian treatment hyperparameters traditional estimation risk overfitting hyperparameters non gaussian  hood structure sparse approximation speedup kϵnn standard GP achieve matrix vector multiplication MVM iteratively solves linear conjugate gradient CGs iteration complexity argue MVM issue determination lack meaningful speedup badly kernel matrix alternatively precondition CG PCG employ precondition matrix nyström approximation improve conditioning kernel matrix accelerate CG convergence interestingly kernel matrix knn algebraic structure MVM massive scalability kronecker exploit multivariate grid input tensor kernel xti  kernel matrix decomposes kronecker knn eas eigendecomposition greatly reduce complexity another toeplitz complementary kronecker exploit kernel matrix built regularly complexity  severe limitation kronecker toeplitz grid input prevent apply arbitrary data handle arbitrary data retain efficient kronecker structure structure kernel interpolation ski imposes grid constraint induce hence matrix  admits kronecker structure toeplitz structure whereas kernel matrix  approximate local linear interpolation adjacent grid induce  sourcewhere induce closely bound interpolation insert approximation qnn qnn   SourceRight click MathML additional feature matrix extremely sparse nonzero entire per local linear interpolation impressive complexity kϵnn sparse incurs prediction constant complexity prediction variance complexity precomputing furthermore derive constant prediction variance lanczos approximation admits iteration MVM calculation ski drawback grid induce grows exponentially dimensionality impractical address issue dimensionality reduction manifold induce dimensional latent interestingly hierarchical structure nns extract latent dimensional feature furthermore continual effort directly reduce complexity linear exploit partition khatri rao structure  impose tensor decomposition kronecker variance  variational framework linear complexity permit numerous induce ski discontinuous prediction due local interpolation overconfident prediction variance training data due restrictive sor framework smooth prediction evans nair exploit partition khatri rao structure  local interpolation sensible uncertainty diagonal correlation akin FITC finally usage induce improve model capability however due grid constraint structure sparse approximation fix induce resort dimensionality reduction tackle dimensional task vast majority induce domain boundary increase degenerate model capability induce review sparse approximation regard implementation location induce crucial induce theoretical analysis indicates KL divergence variational approximation posterior arbitrarily grows slowly regression normally distribute input SE kernel location induce alternatively cluster technique finite induce employ query criterion sequentially informative induce flexibly induce regard parameter optimize hyperparameters additionally introduces parameter inference dimensional optimization task besides increase benefit optimization selection training data vanish interestingly recent attempt simultaneously location induce bayesian framework prior IV local approximation inspire local approximation localize expert improve scalability GP besides global approximation localization enables capture nonstationary feature comprehensively review naive local expert directly employ pure local expert prediction mixture expert  expert  inherit advantage naive local expert boost prediction model average naive local expert away  correlate hence local expert subset sensible prediction complexity particularly naive local expert  local expert completely responsible subregion define mathematically predict accord partition classify  category inductive  partition input expert chooses appropriate predict transductive  particularly chooses neighborhood subset around relevant expert predict inductive  employ static partition cluster technique voronoi tessellation independent local GP expert training expert partition expert usually separately jointly bayesian treatment contrast transductive   induce valid stochastic employ dynamic partition around ntm complexity relies issue transductive  neighborhood around simplest geometric closeness criterion selection however optimal without spatial correlation hence GP active employ sequentially update neighborhood enjoy capability capture nonstationary feature due localize structure  discontinuous prediction boundary subregions suffers generalization capability spatial correlation address discontinuity issue patch gps impose continuity adjacent local gps patch nearly identical prediction boundary however patch gps suffer inconsistent negative prediction variance available dimension popularly model average strategy accomplish mixture local gps elaborate smooth prediction multiple expert address generalization issue hyperparameters across expert combine local approximation global approximation review illustration local approximation individual expert toy moe jointly expert gate function simplicity individual expert differential entropy softmax gate function  suffers discontinuity generalization PoE prediction overconfident prediction variance due inability suppress expert alleviate issue gate function moe desirable prediction input dependent  boost prediction mixture expert moe  combine local diverse expert individual parameter improve overall accuracy reliability moe generally express combination gaussian mixture model GMM  SourceRight click MathML additional feature gate function usually parametric softmax probit function probability expert indicator assign expert responsible component mixture gate manage mixture probabilistic partition input define subregions individual expert responsible expert variety machine model linear model vector machine illustration moe illustration moe training moe usually assumes data maximize factorize likelihood logp gate function expert simultaneously gradient optimizers popularly EM algorithm joint permit probabilistic partition input via data expert diverse expert specify overlap subregions finally predictive distribution  sourcewhere regard posterior probability responsibility advance moe layer model extend hierarchical architecture bayesian approach employ instead maximum likelihood rid overfitting underestimation distribution handle outlier finally instead conditional mixture input distribution joint likelihood assignment expert regard mixture GP expert data moe multimodal nonstationary model individual global expert responsible data complexity data assumption GP data dependence joint distribution parametric gate function bayesian nonparametric framework  introduce mixture GP expert employ GP expert respectively capture variance gate parameter nearly complexity unattractive data mixture GP expert data address issue reduce computational complexity model complexity expert model selection address model complexity issue core thread localization expert instance infinite mixture GP expert  localize likelihood rid assumption SourceRight click MathML additional feature instance expert indicator likelihood factorizes local expert expert training  model improve employ joint distribution SourceRight click MathML additional feature fully generative model capable handle partially specify data inverse functional mapping however inference resort expensive markov chain monte carlo MCMC sample alternatively localization achieve EM algorithm truncation representation assigns data expert maximum posteriori expert indicator threshold thereafter operates subset thread combine global expert sparse approximation review variational EM framework dependence input broken VI feasible interpret GP finite bayesian linear model FITC expert factorize induce induce expert complexity  reduce EM thread assign data dynamically accord data expert performance hence denote mixture implicitly localize expert implicit partition determines optimal allocation expert capture interaction expert advantage encourages application data association drawback however competitive expert fail due zero coefficient unreasonable initial parameter relieve thread  input cluster technique assign expert training mixture explicitly localize expert  reduces model complexity explicitly determines architecture moe distinct local expert meantime drawback  cluster partition information data label expert cannot capture interaction expert finally address model selection issue akaike information criterion synchronously balance criterion employ candidate elegantly input dependent dirichlet DP   distribution   introduce expert indicator automatically infer expert data model inference however resort representation DP due complex prior infinite expert moe employ sum probability distribution expert via operation  multiplies probability distribution sidestep assignment moe operation mpi sourcewhere normalizer however inference intractable maximize likelihood logp fortunately GP expert sidestep issue gaussian distribution hence multiple gaussians gaussian distribution factorize marginal likelihood expert SourceRight click MathML additional feature   training expert factorization degenerate kernel matrix knn diagonal matrix diag KM diag hence complexity substantially reduce PoE likelihood estimation moe likelihood moe likelihood average PoE likelihood configuration expert indicator consequently joint gate function expert moe achieve optimal allocation expert generally due sum moe sharper  expert contrary due PoE sharper expert confirm PoE prediction overconfident prediction variance aggregate prediction independent expert due inability suppress expert contrary moe desirable prediction gate function hence improve PoE retain effective training modify predict instead aggregate expert prediction various aggregation criterion propose weaken vote expert particularly aggregation aggregate prediction sensible probability aggregate prediction robust weak expert GP expert predictive distribution  aggregate expert prediction modify  SourceRight click MathML additional feature quantify contribution derive aggregate prediction variance expression aggregation employ constant aggregate precision explode rapidly increase alleviate overconfident uncertainty generalize PoE  introduces define difference differential entropy expert prior posterior increase decrease importance expert prediction uncertainty however flexible  explosive prediction variance training data address issue impose constraint favorable prediction employ  recovers GP prior however  prediction variance alternatively bayesian committee machine BCM aggregate expert prediction another impose conditional independence assumption explicitly introduces prior expert thereafter bayes SourceRight click MathML additional feature prior correlation BCM recover GP prior akin  robust BCM RBCM prediction within BCM however unreliable prediction analyze notably unlike PoE prior BCM expert hyperparameters explicitly conventional   inconsistent aggregate prediction cannot recover GP consistent aggregation nest pointwise aggregation expert  remove independence assumption assume random variable  theoretically consistent prediction complexity due inversion kernel matrix efficient retain consistent prediction generalize RBCM  introduces global communication expert fix GP prior perform correction expert considers covariance global local expert consistent prediction aggregation sourcewhere predictive distribution expert augment data transductive aggregation usually hyperparameters across expert achieves automatic regularization eas inference due hyperparameters allows temporarily ignore GP aggregation instead relieve inconsistency typical aggregation finally  cannot expert individual hyperparameters however hyperparameters limit capability capture nonstationary feature superiority local approximation besides another drawback aggregation kolmogorov inconsistency induced separation training predict unify probabilistic framework extend predictive distribution multiple improvement scalable gps scalability global approximation sparse approximation generally reduce standard cubic complexity induce moreover complexity reduce svi exploitation structure data  sparse approximation however computationally impractical scenario prediction environmental monitoring alternatively implement sparse approximation advanced compute infrastructure graphic processing gpus distribute cluster processor computation actually GP gpu distribute cluster investigate regime distribute implement parallel linear algebra algorithm  algorithm MVM algorithm distribute memory multicore multi gpu hardware instance successfully MVM GP data gpus meantime gpu accelerate sparse gps explore factorize data inference parallelize accelerate gpu moreover relaxed ELBO grid induce tensorflow  library pytorch  library developed exploit usage gpu hardware besides parallel sparse gps parallel pit incomplete cholesky factorization icf developed message passing interface framework distribute computation multiple machine ideally parallelization achieve speedup factor machine comparison centralize counterpart recently unify framework distributes conventional sparse gps DTC FI PI rank  markov approximation  built via correlate structure impressively implement sparse gps distribute compute platform billion training model successfully within local approximation generally complexity global approximation local opinion naturally encourages parallel distribute implementation reduce computational complexity capability originate rank nyström approximation global sparse approximation approximate feature spatial correlation spectral expansion kernel matrix dominate eigenvectors contrast latent function nonstationary feature complicate series task limited global induce exploit local inspire local approximation however capable capture local suffer inability global hence enhance representational capability scalable gps hybrid approximation straightforward thread combine global local approximation tandem alternatively hybrid approximation accomplish additive instance partition input subregions pic stochastic distribute variant extend  retain conditional independence training subregions enables integration local global approximation transductive mathematically suppose jmp SourceRight click MathML additional feature model corresponds GP additive kernel   ψij  sourcewhere ψij belong otherwise ψij pic recovers  subregion pic becomes purely local gps induce zero additive kernel  employ combine CS kernel sparse approximation furthermore extension pic structure GP ignores  dependence induce concentrate dependence adjacent subregions chain structure almost purely localize model reduces complexity linear allows induce hybrid approximation conduct coarse hierarchical structure multiple layer yield multiresolution extend hierarchically partition GP approximation model multiple layer layer localize gps particularly layer individual kernel configuration density induce adjacent layer covariance function convolve relevant kernel similarly park choi layer model GP centroid subset GP construct rough global approximation layer subregion layer local GP global GP prior GP model improve multilayer structure inevitably combination local approximation induce discontinuous prediction inaccurate uncertainty boundary subregions structure gps completely adopt localize predictive distribution suffers severe discontinuity prediction smooth induce boundary subregions however implement pic predictive distribution compose global local partially alleviate discontinuity completely address discontinuity nguyen  combine sparse approximation model average strategy moe finally despite hybrid approximation representational capability sparse approximation enhance powerful probabilistic framework instance interdomain GP employ convolution  GP dimensional GP particularly linear integral transform induce another domain possibly dimension induce variable domain kernel induce richer dependence domain interdomain apply posterior approximation besides encourage employ configuration basis function capture feature nonstationary GP indeed derive interdomain alternatively unlike standard GP  FITC extend diag moreover employ markov correlate distribute variational framework unify framework accommodates exist sparse approximation DTC pic markov structure extend article bayesian treatment hyperparameters guard overfitting elegantly derive scalable heteroscedastic bayesian model adopt additional GP analogous account variance exp differently derive stochastic distribute variant scalable heteroscedastic regression distribute variant expert hybrid parameter improves scalability capability stochastic variant global induce sacrifice prediction heteroscedastic finally  encode augment input latent variable exploit regime density estimation VI extension issue scalable manifold GP scalable GP literature usually focus scenario training whereas input modest dimension however handle task comparable demand dimensional scalable GP impose dimensional constraint restrict dimensional input dimensional manifold embed dimensional dimensional statistical inference solvable input compatible statistical training hence various manifold gps express SourceRight click MathML additional feature mapping matrix developed tackle dimensional data linear nonlinear dimensionality reduction NN input transformation kernel operates data dimensional mapping matrix scalable regression jointly bayesian framework favorable particularly dimensionality manifold estimate bayesian mixture model however induce computational budget recent theoretical intrinsic manifold bypass GP dimensional achieve optimal rate highly smooth motivates usage bayesian model average random compression various configuration reduce computational demand continual theoretical empirical effort specific component convolutional kernel scalable manifold gps urgent demand various computer vision CV scalable GP motivate enormous various scalable gps  investigate recent representative combine structural nns flexible nonparametric GP nns input feature extract nonstationary recurrent feature layer sparse GP conduct standard regression latent parameter nns GP jointly maximize marginal likelihood guard overfitting nns GP structure sensible uncertainty robust adversarial CV task differently   employ nns input dependent hyperparameters additive kernel  algorithm employ GP inference besides   output nns prior  elegantly inspire DGP variant employ hierarchical functional composite SourceRight click MathML additional feature stack multiple layer latent variable model LVM extract feature DGP showcase flexibility supervise scenario however nonstandard GP recently developed convolutional kernel DGP CV task interestingly finite layer layer transformation input extend infinitely infinitesimal differential stochastic differential equation inference DGP intractable expensive efficient training sophisticated approximate inference via induce limit capability easy inference without loss prediction accuracy challenge DGP completely potential beyond regression scalable multitask GP due multitask arose various environmental sensor network structure multitask GP   GP seek latent correlate task RT simultaneously GP  SourceRight click MathML additional feature individual crucial  construction valid multitask kernel  RT built linear model  convolution individual model task loses valuable information joint task enables boost prediction exploit task correlation leverage information across task task training  data task fuse entire kernel matrix complexity hence inference  standard review sparse approximation local approximation apply  improve scalability date scalable  mainly scenario task define label input modest dimension effort extend  handle challenge regime multitask  scalable online GP typically assume entire data available priori conduct training however scenario data arrives sequentially online data unknown batch complicate online regression model adaptation data handle data continuously sparse gps extensible online employ induce summarize training data data interact induce enhance online reasonable update FITC  rely induce data moreover stochastic variant naturally showcase online structure bound minibatch stochastic optimization however issue scalable online gps fix hyperparameters obtain constant complexity per update empirically argue optimization improves model significantly iteration hence advanced compute demand accurate prediction update hyperparameters online iteration scalable online gps implicitly assume data data drawn input distribution however task complex trajectory evolve series address evolve online intuitive local approximation maintains multiple local gps data update specific GP relevant local data local GP away data however information loss available training data extension deployed elegant probabilistic framework update posterior distribution hyperparameters online fashion interaction happens induce primary theoretical bound bayesian online model scalable recurrent GP exist various task recognition identification forecasting robotics data sequential focus recurrent GP handle sequential data popular recurrent GP GP nonlinear autoregressive model exogenous input GP NARX generally express  SourceRight click MathML additional feature external input output observation lag parameter respectively delayed output input regression vector emission function  account observation observation deterministic transform input comprise previous observation external input enables standard scalable gps sparse approximation GP NARX due simplicity applicability GP NARX extend achieve robust prediction outlier local model incorporate prior information frequency response function drawback GP NARX however cannot account observation error variable address issue conduct data preprocessing remove data adopt gps input employ powerful model SSMs introduce GP ssm employ recurrent structure   SourceRight click MathML additional feature internal memory transition function emission function  transition finally  emission GP NARX simplify GP ssm model observable GP ssm considers transition brings flexibility lag parameter however model suffers intractable inference marginalize latent variable approximate inference finally trend combine recurrent gps nns instance recurrent GP attempt mimic recurrent nns rnns layer model GP DGP inference recurrent GP intractable sophisticated approximation hence model retain recurrent capability memory lstm model directly combine scalable gps analytical inference desirable scalable GP classification regression task mainly review article continuous observation classification discrete label binary GP classification GPC model usually formulate GP bernoulli SourceRight click MathML additional feature inverse link function squash probability differently multiclass GPC  GP categorical sourcewhere independent latent function RC due non gaussian likelihood inference GPC however intractable approximate inference approximates non gaussian posterior gaussian motivate scalable GPR directly treat GPC regression task GPR transformation interprets label output dirichlet distribution sidestep non gaussian likelihood principled however adopt  combine approximate inference laplace approximation EP VI sparse strategy derive scalable  challenge scalable GPC  intractable inference posterior training complexity issue stochastic GPC derives model evidence express integration gaussian distribution adequately calculate gaussian  quadrature furthermore GPC equip FITC assumption completely analytical model evidence particularly logit softmax inverse link function  gamma data augmentation analytical inference posterior GPC recent noisy framework derive analytical  scalable binary  conventional likelihood issue complexity  linear alternatively formulate model evidence sum efficient stochastic training vii conclusion although GP nonparametric flexibility interpretability popular challenge era data article attempt summarize scalable gps understand attain insight discovery extensive review seek uncover applicability scalable gps task challenge model theory GP community appendix library data summarizes primary library implement representative gps academia python hardware acceleration become popular GP community specific scalable GP package implement advanced model review VI relevant researcher webpage primary library representative scalable gps primary library representative scalable gps besides uci regression data libsvm regression data II summarizes regression data scalable GP literature researcher assess scalable gps billion training II regression data literature II regression data literature