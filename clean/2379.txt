federate allows multiple jointly model combine data without participant reveal local data centralize server privacy preserve collaborative however significant communication overhead training address compression propose distribute training literature reduce amount communication magnitude exist however limited utility federate compress upstream communication client server downstream communication uncompressed perform idealize distribution client data typically cannot federate article propose sparse ternary compression stc compression framework specifically requirement federate environment stc extends exist compression technique gradient sparsification novel mechanism enable downstream compression  optimal  encode update task demonstrate stc distinctively outperforms federate average federate scenario advocate paradigm shift federate optimization frequency bitwidth communication bandwidth constrain environment introduction development currently transform data advent internet iot intelligent device rapidly grown couple device equip various sensor increasingly potent hardware data unprecedented concurrent development revolutionize information extract data resource groundbreaking computer vision processing recognition others amount data  recent partly attribute availability data training therefore potential harness data iot device training improve model data privacy become concern user multiple data leakage misuse recent demonstrate centralize processing data risk user privacy iot device usually data private environment without explicit awareness user concern particularly therefore generally option data centralize entity conduct training model situation local processing data desirable increase autonomy local agent leaf dilemma combine data iot device training model data cannot centralize location federate resolve issue allows multiple jointly model combine data without participant reveal data centralize server privacy preserve collaborative achieve protocol illustrate participate client model server client improve model local training data stochastic gradient descent sgd finally participate client upload locally improve model server aggregate model update ΔW wnew  communicate instead model equivalent client remain synchronize convergence criterion satisfied protocol training data local device model update communicate although adversarial setting information training data infer update additional mechanism homomorphic encryption update differentially private training apply fully conceal information local data federate parameter server illustrate communication distribute sgd client synchronize server client compute update independently local data client upload local update server average model federate parameter server illustrate communication distribute sgd client synchronize server client compute update independently local data client upload local update server average model issue federate massive communication overhead arises around model update naively protocol described earlier participate client communicate model update training iteration update model gigabyte architecture parameter multiple training iteration data communication client easily petabyte consequently communication bandwidth limited communication costly naive federate become unproductive completely unfeasible amount uploaded client training  niter update  update SourceRight click MathML additional feature niter training iteration backward perform client communication frequency model  entropy update exchange upload respectively inefficiency encode difference update minimal update entropy assume model training iteration fix achieve accuracy task leaf option reduce communication reduce communication frequency reduce entropy update  via lossy compression scheme efficient encoding communicate update reduce II challenge federate environment reduce amount communication account unique characteristic distinguish federate distribute training setting parallel training federate distribution training data computational resource fundamental fix environment entail challenge unbalanced non data training data individual client client local environment usage distribution local data typically heavily client client federate environment constitute multiple participant furthermore quality collaboratively model combine available data client collaborative environment tendency parameter server client grows beyond threshold communication update becomes unfeasible workload communication aggregation update grows linearly client federate therefore unavoidable communicate via intermediate parameter server reduces amount communication per client communication upload local update aggregate update server workload aggregation away client communicate via parameter server however introduces additional challenge communication efficient distribute training upload server server compress reduce communication consumption partial participation federate iot generally guaranteed client participate communication device lose connection battery seize contribute collaborative training limited battery memory mobile embed device grid instead capacity computation limited finite battery perform iteration sgd notoriously expensive neural network therefore gradient evaluation per client mobile embed device typically limited memory memory footprint sgd grows linearly batch device batch mention characterization federate environment conclude communication efficient distribute training algorithm federate fulfil requirement compress upstream downstream communication robust non batch unbalanced data robust client partial client participation contribution article demonstrate none exist propose communication efficient federate satisfies requirement concretely compress upstream downstream communication sensitive non data distribution robust data compress downstream proceed construct efficient communication protocol federate resolve issue requirement convergence analysis extensive empirical neural network architecture data demonstrate sparse ternary compression stc protocol superior exist compression scheme gradient evaluation communicate converge target accuracy IX extend regime communication efficient distribute propose literature none exist satisfies requirement federate environment robust non data federate training converges independent local distribution client data compression rate weak communication efficient distribute propose literature none exist satisfies requirement federate environment robust non data federate training converges independent local distribution client data compression rate weak IV related broader realm communication efficient distribute variety propose reduce amount communication training reference organize substantial exist research communication efficient distribute communication delay reduce communication frequency propose federate average instead communicate iteration client performs multiple iteration sgd compute update convolutional recurrent neural network architecture communication delayed iteration without significantly affect convergence data distribute client manner amount communication reduce longer delay however increase gradient evaluation  combine communication delay random sparsification probabilistic quantization restrict client random sparse update random sparsity afterward structure versus sketch update combine sparsification probabilistic quantization however significantly slows convergence sgd iteration communication delay automatically reduce upstream downstream communication proven client partial client participation sparsification reduce entropy ΔW update restrict subset parameter  approach later modify gradient magnitude predefined threshold server gradient accumulate residual achieve upstream compression rate magnitude acoustic model task however appropriate threshold architecture layer overcome issue aji  instead fix sparsity rate communicate entry magnitude gradient gradient residual sparsity rate slightly degrades convergence accuracy model minor modification aji  performance gap sparsification propose primarily intention parallel training data convergence challenge federate environment investigate sparsification exist primarily compress upstream communication sparsity update client generally participate client inverse sparsity rate easily federate downstream update compress dense quantization reduce entropy update restrict update reduce propose signSGD compression theoretical convergence guarantee data quantizes gradient update binary reduce per update factor signSGD incorporates compression aggregate binary update client majority vote author propose stochastically quantize gradient upload unbiased  quantize stochastic gradient descent   theoretically appeal inherit convergence regular sgd relatively mild assumption however empirical performance compression rate sparsification federate average signSGD compress upstream downstream communication limited utility federate define II communication server client uncompressed notation calligraphic refer entirety parameter neural network regular uppercase refers specific tensor parameter within lowercase refers scalar parameter network arithmetic operation neural network parameter understood elementwise limitation exist compression related efficient distribute almost exclusively considers data distribution client assume unbiasedness local gradient respect batch gradient accord WR SourceRight click MathML additional feature distribution data client empirical risk function combine training data assumption reasonable parallel training distribution data client chosen practitioner typically valid federate generally unbiasedness  WR SourceRight click MathML additional feature individual client gradient bias local data accord  WR SourceRight click MathML additional feature violates assumption non distribution local data render exist convergence guarantee formulate  dramatic practical performance communication efficient distribute training algorithm demonstrate preliminary preliminary simplify version layer vgg network cifar data federate setup client split training data randomly equally shard assign shard client non assign client sample exactly data data split nonoverlapping balance client data detailed procedure generates split data described appendix supplementary perform logistic regression classifier mnist data setup federate environment model momentum sgd comparable compression rate batch convergence gradient evaluation model communication efficient federate compression achieve comparably convergence gradient evaluation data closely uncompressed baseline suffer considerably non training setting trend logistic regression model conclude underlie phenomenon unique neural network convex objective analyze detail compression convergence compression training vgg cifar logistic regression mnist fashion mnist distribute client non data non client exactly respectively data compression suffer degrade convergence non situation sparse affected convergence compression training vgg cifar logistic regression mnist fashion mnist distribute client non data non client exactly respectively data compression suffer degrade convergence non situation sparse affected federate average noticeably federate average orange although specifically propose federate suffers considerably non data observation consistent demonstrate model accuracy non environment attribute loss accuracy increase divergence client propose assign public data client approach indeed accurate model multiple shortcoming crucial generally cannot assume availability public data public data exist  model server consistent assumption typically federate furthermore client public data overfitting data become serious issue particularly severe highly distribute setting data client finally relatively data client accuracy achieve situation cannot fully restore data strategy propose insufficient workaround fundamental federate average convergence issue non data  quantization signSGD suffers stability issue non environment completely fails converge cifar benchmark convex logistic regression objective training plateau substantially degrade accuracy understand convergence issue investigate likely batch gradient  SourceRight click MathML additional feature batch gradient specific minibatch data parameter gradient entire training data define probability  SourceRight click MathML additional feature compute statistic  SourceRight click MathML additional feature estimate average congruence parameter network exemplary distribution within logistic regression mnist training batch predictor gradient variance average congruence slightly random sensitivity signSGD non data becomes apparent inspect development gradient congruence increase batch development batch increase sample non distribution latter sample batch contains data exactly data quickly grows increase batch increasingly accurate update non data however congruence independent batch client highly non subset data signSGD update weakly correlate direction steepest descent batch chosen training distribution alpha layer logistic regression mnist data development alpha increase batch batch sample randomly training data non batch contains sample exactly batch gradient becomes increasingly accurate batch non batch data gradient remain highly incongruent batch gradient batch distribution layer logistic regression mnist data development increase batch batch sample randomly training data non batch contains sample exactly batch gradient becomes increasingly accurate batch non batch data gradient remain highly incongruent batch gradient batch sparsification exist compression sparsification suffers non data vgg cifar training converges reliably client data exactly logistic regression classifier mnist convergence hypothesize robustness non data due mainly frequent communication update client prevents diverge another hence sparsification suffer divergence federate average sparsification destabilize training nearly signSGD stochastic gradient amplify quantization although sparsification promising performance non data utility limited federate directly compress upstream communication summarizes finding none exist compression compression properly non data VI sparse ternary compression sparsification promising performance distribute environment non client data observation construct efficient communication protocol federate protocol prevent application sparsification federate increase efficiency employ quantization optimal lossless cod update incorporate downstream compression efficient communication server client implement cache mechanism client synchronize partial client participation  update regular sparsification propose communicates precision communicate previous already demonstrate imbalance update precision wasteful distribute training compression gain achieve sparsification combine quantization nonzero adopt described federate quantize remain sparsified update population magnitude ternary tensor quantization formalize algorithm algorithm stc input flatten tensor sparsity output sparse ternary tensor max topk mask  mask  return   reduces entropy update   SourceRight click MathML additional feature    regular sparsification sparsity rate additional compression achieve    achieve compression gain pure sparsification increase sparsity rate approximately factor theoretical framework developed convergence stc standard assumption loss function proof relies bound impact perturbation compression operator formalize definition definition contraction parameter contraction operator comp satisfies contraction comp SourceRight click MathML additional feature stc indeed contraction lemma  define algorithm contraction topk SourceRight click MathML additional feature proof appendix supplementary directly smooth strongly convex objective function bound gradient ΔW update   ΔW   SourceRight click MathML additional feature converges accord WT    SourceRight click MathML additional feature stc converges rate regular sgd preliminary theoretical finding accuracy vgg model sparsity without  additional  negligible convergence sometimes increase accuracy model evident combination sparsity quantization efficient communication  pure sparsification  upload sparsity displayed difference accuracy model sparse update model sparse binarized update positive performance model pure sparsity vgg cifar iteration client non data  upload sparsity displayed difference accuracy model sparse update model sparse binarized update positive performance model pure sparsity vgg cifar iteration client non data extend downstream compression exist compression framework propose distribute training compress communication client server sufficient application aggregation achieve via reduce operation however federate client aggregate update server approach feasible communication bottleneck illustrate  ΔW ΔW compression operator flatten update ΔW sparsified  update ΔW accord algorithm local update ΔW update stc ΔW  ΔW δwi ΔW ΔW SourceRight click MathML additional feature empty residual client update ΔW client server sparse nonzero update ΔW downstream grows linearly amount participate client participation rate exceeds inverse sparsity update ΔW essentially becomes dense resolve issue propose apply compression mechanism client server compress downstream communication modifies update ΔW   ΔW δwi SourceRight click MathML additional feature client server residual update ΔW δwi ΔW ΔW SourceRight click MathML additional feature express update upload compression pure upload compression generalize filter mask sparsifying filter mask respective client upload server sparse update ΔW client filter mask hadamard predict training model update behave regular upstream sparsification slightly increase sparsity rate experimentally verify prediction accuracy achieve vgg cifar federate environment client iteration rate upload compression upload sparsity sparsifying harmful convergence decrease accuracy non accuracy achieve vgg cifar distribute client iteration upload sparsity sparsifying update downstream communication reduces accuracy upload sparsity accuracy achieve vgg cifar distribute client iteration upload sparsity sparsifying update downstream communication reduces accuracy upload sparsity update cache partial client participation scenario client participate throughout entire training however elaborate II federate typically entire client population participate communication client model compress model update ΔW introduces challenge client synchronize synchronization reduce workload client propose cache mechanism server assume communication update ΔW server cache partial sum update ΔW global model ΔW client participate communication synchronize server previous communication skip sparse update bound entropy ΔW SourceRight click MathML additional feature attain linearly client skip training average skip inverse participation usually tolerable downlink typically cheaper bandwidth uplink already essentially compression communicate parameter update instead model suffer signSGD although downstream update grows logarithmically delay accord signSGD SourceRight click MathML additional feature partial client participation convergence federate training delayed sparsified update investigate detail vii lossless encode communicate sparse ternary tensor stc transfer nonzero flatten tensor along per nonzero update instead communicate absolute nonzero favorable communicate distance assume random sparsity distance approximately geometrically distribute probability sparsity rate therefore optimally encode distance  code  encode reduces average SourceRight click MathML additional feature golden ratio sparsity rate translates compression naive distance encode fix encode decode scheme appendix algorithm supplementary update encode upload compression framework feature upstream downstream compression via sparsification  optimal encode update described algorithm algorithm efficient federate parameter server via stc algorithm efficient federate parameter server via stc vii evaluate propose communication protocol task performance federate average signSGD variety federate environment model data spectrum evaluate differently convolutional recurrent neural network relevant federate task image classification recognition vgg cifar modify version popular layer vgg network cifar data simplify vgg architecture reduce convolutional filter respective convolutional layer reduce hidden fully layer remove dropout layer batch normalization layer regularization longer batch normalization perform poorly batch non data obscure investigate behavior vgg network achieves accuracy validation iteration training constant rate contains parameter cnn KWS layer convolutional neural network cnn command data command data consists sample specific keywords keywords sample duration restrict subset keywords command extract mel spectrogram fourier transform feature cnn architecture achieves accuracy training iteration parameter lstm fashion mnist memory lstm network hidden layer fashion mnist data fashion mnist data contains validation greyscale image fashion item image treat sequence feature dimensionality fed lstm network training iteration rate lstm model achieves accuracy validation model contains parameter logistic regression mnist finally logistic regression classifier mnist data mnist data contains training greyscale image handwritten digit logistic regression classifier achieves accuracy contains parameter task summarize II primarily discus vgg cifar however described phenomenon benchmark experimental appendix supplementary II model hyperparameters rate constant throughout training II model hyperparameters rate constant throughout training compression propose stc sparsity rate federate average equivalent delay iteration signSGD  sparsity rate stc compress update upload roughly factor delay iteration federate average slightly compression rate analysis sparsity rate delay convergence stc federate average appendix supplementary training related hyperparameters constant compression  training iteration communication federate average iteration local iteration environment federate environment described algorithm fully characterize parameter configuration client participation ratio local batch assign client equally subset training data sample explicitly signify otherwise hyperparameters default configuration summarize notation client refer setup federate environment random subset client participates communication client data exactly configuration federate environment configuration federate environment momentum federate optimization investigate momentum optimization convergence behavior compression accuracy achieve federate average stc signSGD training iteration variety federate environment dash refer momentum training solid signify classical sgd momentum significant influence convergence behavior signSGD performs distinctively momentum optimization stc federate average parameter environment momentum beneficial harmful performance stc participation rate batch training sufficiently momentum improves performance stc conversely momentum deteriorate training performance situation training batch client participation latter increasingly client non subset data surprising issue stale momentum described enhance situation relationship federate average heterogeneity local minibatches momentum positive training performance robustness compression non ness client data benchmark vgg cifar stc distinctively outperforms federate average non data environment configure described dash signify momentum robustness compression non ness client data benchmark vgg cifar stc distinctively outperforms federate average non data environment configure described dash signify momentum maximum accuracy achieve compression training vgg cifar iteration batch federate environment client participation client data exactly client subset data maximum accuracy achieve compression training vgg cifar iteration batch federate environment client participation client data exactly client subset data validation accuracy achieve vgg cifar iteration communication efficient federate training compression relative client participation varied client data exactly client subset data validation accuracy achieve vgg cifar iteration communication efficient federate training compression relative client participation varied client data exactly client subset data validation accuracy achieve vgg cifar iteration communication efficient federate training compression training data split client  gamma validation accuracy achieve vgg cifar iteration communication efficient federate training compression training data split client  federate average signSGD stc ignore whichever version momentum performs non ness data preliminary already demonstrate convergence behavior federate average signSGD sensitive ness local client data whereas sparse communication robust investigate behavior detail maximum achieve generalization accuracy fix iteration vgg cifar non ness additional benchmark appendix supplementary plot partial plot client participation stc outperforms federate average across ness distinct difference non regime individual client stc without momentum outperforms federate average signSGD margin extreme client data exactly stc achieves accuracy partial client participation respectively federate average signSGD fail converge robustness parameter environment proceed investigate parameter environment convergence behavior compression maximum achieve accuracy training vgg cifar iteration federate environment additional benchmark appendix supplementary stc without momentum consistently dominates federate average benchmark environment local batch memory capacity mobile iot device typically limited memory footprint sgd proportional batch training client restrict minibatches influence local batch performance communication efficient federate technique exemplary vgg cifar momentum significantly slows convergence stc federate average batch independent distribution data client training data distribute client manner client participate training iteration federate average suffers considerably batch stc demonstrates robust constraint extreme batch model stc achieves accuracy federate average model training iteration client participation convergence vgg cifar federate environment client participation isolate reduce participation absolute participate client local batch constant respectively throughout client relative participation reduce participation rate negative federate average stc negative however federate average participation rate proportional effective amount data training conduct individual communication  subset client participate communication federate average steer optimization away minimum catastrophic forget previously concept partial participation reduces convergence stc client residual sync increase gradient staleness client participate training outdated accumulate gradient become behavior stc strongly non situation accuracy steadily decrease participation rate however extreme client participate training stc achieves accuracy federate average signSGD client data stc suffers reduce participation rate federate average client participate stc without momentum manages achieve accuracy federate average  accuracy signSGD affected reduce participation unsurprising absolute participate client influence performance behavior benchmark appendix supplementary noteworthy federate usually server rate client participation instance typically increase participation ratio client  perform balance split data client assign amount data however data client typically heavily simulate  split data client client assign SourceRight click MathML additional feature data parameter minimum amount data client parameter concentration data fix amplify unbalanced client data client participation client accuracy achieve iteration interestingly  data significant performance compression data highly concentrate client converge reliably federate average accuracy slightly increase  apparently rare participation client balance communication client benchmark appendix supplementary communication efficiency finally compression respect iteration communicate achieve target accuracy federate task federate average signSGD perform considerably client non data batch meaningful comparison therefore evaluate environment client moderate batch training setup federate average signSGD maximum parameter environment configuration target accuracy achieve maximum amount iteration exceed amount communicate upload vgg cifar cnn keyword KWS lstm model fashion mnist client data stc manages achieve desire target accuracy within communication budget stc converges faster training iteration version federate average comparable compression rate unsurprisingly federate average stc tradeoff training iteration computation communicate communication investigate benchmark however stc pareto superior federate average fix iteration complexity achieves upload communication complexity convergence federate compress communication training iteration uploaded benchmark federate environment client participation readability validation error curve average smooth benchmark stc amount converge target accuracy convergence federate compress communication training iteration uploaded benchmark federate environment client participation readability validation error curve average smooth benchmark stc amount converge target accuracy IV amount upstream downstream communication achieve target accuracy megabyte cifar task stc sparsity rate communicates MB worth data reduction communication factor baseline MB federate average MB federate average delay achieve target accuracy within iteration budget IV upload achieve target accuracy task environment signifies achieve target accuracy within iteration budget environment  described IV upload achieve target accuracy task environment signifies achieve target accuracy within iteration budget environment  described lesson summarize finding article suggestion approach communication constrain federate summarize client non data sparse communication protocol stc distinctively outperform federate average across federate environment client minibatches hardware memory constrain situation stc outperforms federate average client data stc prefer federate average client participation rate converges stable quickly non regime stc generally advantageous situation communication bandwidth constrain costly meter network limited battery achieve target accuracy within minimum amount communicate data IV federate average return communication latency constrain client participation momentum optimization avoid federate whenever client training batch client data non participation rate accuracy achieve vgg cifar iteration federate training federate average stc configuration environment upstream downstream communication achieve validation accuracy federate average stc cifar benchmark data moderate batch accuracy achieve vgg cifar iteration federate training federate average stc configuration environment upstream downstream communication achieve validation accuracy federate average stc cifar benchmark data moderate batch IX conclusion federate mobile iot application challenge task generally exert environment article demonstrate convergence behavior communication efficient federate sensitive variety data model architecture convergence federate average drastically decrease environment client non subset data minibatches client participates communication address issue propose stc communication protocol compress upstream downstream communication via sparsification  error accumulation optimal  encode stc robust mention peculiarity environment federate average moreover stc converges faster federate average respect training iteration amount communicate client data moderate batch training approach understood alternative paradigm communication efficient federate optimization relies frequent volume instead frequent volume communication particularly federate environment characterize latency bandwidth channel client server