investigate anomaly detection unsupervised framework introduce memory lstm neural network algorithm variable data sequence pas sequence lstm structure obtain fix sequence decision function anomaly detector vector machine OC SVMs vector data description SVDD algorithm literature jointly optimize parameter lstm architecture OC svm SVDD algorithm highly effective gradient quadratic program training apply gradient training modify objective criterion OC svm SVDD algorithm convergence modify objective criterion criterion extension unsupervised formulation semisupervised fully supervise framework obtain anomaly detection algorithm variable data sequence performance series data approach generic apply approach gate recurrent gru architecture directly replace lstm structure gru structure illustrate significant performance gain achieve algorithm respect conventional introduction preliminary anomaly detection attract significant contemporary literature due application engineering article variable anomaly detection unsupervised framework seek function unlabeled variable sequence data anomalous although extensively literature exist supervise semisupervised knowledge data label employ unsupervised due obtain accurate label application however extend derivation semisupervised fully supervise framework completeness literature widely approach anomaly detection decision function defines model normality approach defines decision function optimizes parameter function respect predefined objective criterion vector machine OC SVMs vector data description SVDD algorithm however algorithm approach examine series data sufficiently achieve acceptable performance performance significantly approach careful selection satisfactory performance enhance performance series data fisher kernel generative model introduce however drawback fisher kernel model inversion fisher information matrix computational complexity obtain adequate performance generative model hidden markov model hmm carefully structural parameter topology model furthermore training algorithm considerable performance generative model limit usage application neural network recurrent neural network rnns approach introduce thanks inherent memory structure information however rnn architecture structure gate regulate amount information advanced rnn architecture structure memory lstm network introduce however neural network approach cannot directly optimize objective criterion anomaly detection due lack data label unsupervised framework hence predict sequence sample sequence anomaly prediction error anomaly cannot predict nominal data probabilistic model prediction error threshold probabilistic model detect anomaly challenge optimization restricts performance accordingly furthermore neural network approach fix vector sequence significantly limit usage application circumvent issue introduce novel lstm anomaly detection algorithm variable data sequence pas variable data sequence lstm structure obtain fix representation apply OC svm algorithm SVDD algorithm detect anomaly extract fix vector illustrate unlike previous approach literature jointly parameter lstm architecture OC svm SVDD formulation maximize detection performance joint optimization propose training quadratic program algorithm gradient algorithm merit approach detailed article gradient training modify OC svm SVDD formulation convergence modify formulation instead prediction approach literature define objective function anomaly detection lstm architecture optimize parameter lstm architecture via define objective function hence anomaly detection algorithm variable sequence performance series data furthermore introduce generic approach apply rnn architecture apply approach gate recurrent gru architecture advanced rnn architecture lstm architecture simulation extensive demonstrate significant performance gain respect conventional overall structure anomaly detection approach prior comparison introduce anomaly detection OC svm SVDD algorithm generally employ due performance application however algorithm inadequate performance series data due inability capture dependency improve performance algorithm series data convert series data vector replicate sample obtain vector sequence however obtain vector sequence dimension additional information approach inadequate performance series data another approach OC svm acquires vector series data unfold data phase delay embed specifically sample dimensional vector previous sample along sample however obtain satisfactory performance approach dimensionality carefully tune restricts usage application lstm algorithm performance series data highly complex optimization adequate performance lstm anomaly detection algorithm predict series data multivariate gaussian distribution error threshold distribution allocate sequence parameter distribution threshold via maximum likelihood estimation technique conventional lstm approach careful selection additional parameter significantly degrades performance furthermore OC svm SVDD lstm fix sequence circumvent issue introduce generic lstm anomaly detector variable data sequence jointly parameter lstm architecture OC svm SVDD formulation via predefined objective function therefore obtain performance series data enjoy joint effective optimization parameter respect define objective function contribution contribution introduce lstm anomaly detection algorithm unsupervised framework extend derivation semisupervised fully supervise framework literature jointly parameter lstm architecture OC svm SVDD formulation via define objective function introduce joint optimization gradient joint optimization modify OC svm SVDD formulation convergence modify formulation thanks lstm structure introduce variable data sequence addition unlike conventional effectively detect anomaly series data without preprocessing extensive involve simulated data illustrate significant performance improvement achieve algorithm respect conventional moreover approach generic apply recently propose gru architecture organization article organization article II variable anomaly detection introduce lstm structure introduce anomaly detection algorithm OC svm formulation propose joint training lstm svm parameter merit approach detailed manner introduce anomaly detection algorithm SVDD formulation joint training parameter IV demonstrate performance improvement data thanks generic approach introduce gru anomaly detection algorithm finally conclude remark II model description article vector vector denote boldface matrix boldface uppercase vector ordinary transpose ata norm index subscript vector vector zero identity matrix understood context data sequence define SourceRight click MathML additional feature respect assume bulk sequence normal remain sequence anomalous aim decision function anomalous data output desire function nominal anomalous data respectively application framework host intrusion detection handle operating trace data consist generate user program trace belong alphabet however occurrence issue detect anomaly program execute sequence sequence program binary encode sample sequence sequence aim function successfully distinguishes anomalous sequence normal sequence function   OC svm algorithm hyperplane anomaly normal data SVDD algorithm hypersphere enclose normal data anomaly outside hypersphere however algorithm fix sequence hence lstm architecture obtain fix vector representation previously introduce although exist version lstm architecture widely employ architecture lstm architecture without peephole connection lstm architecture demonstrate internal lstm equation SourceRight click MathML additional feature vector input vector output vector lstm addition input forget output gate respectively hyperbolic tangent function tanh applies input vector pointwise similarly sigmoid function operation elementwise multiplication vector furthermore parameter lstm architecture accord dimensionality input output vector basically lstm architecture network previous lstm information consecutive lstm lstm architecture important information amount information output information data purpose compute contains candidate via tanh layer amount generate information information forget input gate respectively finally output obtain output however output compute filter tanh layer output gate obtain output lstm lstm structure obtain fix sequence lstm parameter however presentation simplicity apply lstm architecture data sequence illustrate average lstm output data sequence pool obtain fix sequence denote procedure obtain information demonstrate emphasize vector explicitly calculation computation via pool remark pool obtain fix sequence dij however pool max pool  respectively derivation straightforwardly extend pool novel anomaly detection algorithm formulate anomaly detection approach OC svm SVDD algorithm joint optimization update parameter overall structure anomaly detection OC svm algorithm anomaly detection algorithm OC svm formulation derive joint update lstm svm parameter training quadratic program algorithm introduce gradient training algorithm apply gradient training smoothly approximate OC svm formulation convergence approximate formulation actual OC svm algorithm aim hyperplane anomaly normal data formulate OC svm optimization sequence minÎ¸   wth TW TR  sourcewhere parameter hyperplane regularization parameter slack variable penalize misclassified instance lstm parameter  lstm parameter unknown function parameter minimize function respect optimization function sgn wth SourceRight click MathML additional feature detect anomalous data sgn function return input emphasize minimize respect suffer overfitting  dependency data parameter null circumvent issue introduce constraint norm avoid overfitting trivial boost ability lstm architecture capture dependency remark orthogonality constraint lstm parameter however constraint instead optimization manner choice constraint neural network frobenius norm define  sourcefor matrix aij directly replace frobenius norm constraint lstm parameter optimization manner approach aim regularize parameter however rnns encounter exponential growth decay norm gradient training parameter significantly degrades capability architecture capture dependency moreover regularizes parameter bound norm coefficient matrix article constraint regularize parameter improve capability lstm architecture capture dependency quadratic program training algorithm introduce training approach quadratic program optimization perform consecutive update lstm svm parameter purpose convert optimization dual consecutive update parameter lagrangian svm parameter   nÎ±i wth SourceRight click MathML additional feature lagrange multiplier derivative respect derivative zero nÎ±i   SourceNote optimum inequality become equality nonzero relation compute    source substitute obtain dual constrain minimization minÎ¸    nÎ±i TW TR  SourceRight click MathML additional feature vector representation lstm parameter unknown minimization substitute function dual sgn   SourceRight click MathML additional feature calculate optimal optimization employ procedure lstm parameter minimize sequential minimal optimization SMO algorithm fix update algorithm optimization orthogonality constraint consecutive update procedure converge converge evaluate although convergence algorithm guaranteed obtain carefully tune parameter rate application explain procedure detail lstm parameter vector iteration update vector iteration SMO algorithm due efficiency quadratic constrain optimization SMO algorithm subset parameter minimize fix parameter extreme parameter minimize however due parameter illustrate SMO algorithm update fix parameter nÎ±i sourcewe replace derivative respect derivative zero obtain update iteration MK sourcewhere kij   due update outside project update obtain parameter procedure eventually converges parameter obtain minimize update update update vector purpose employ optimization satisfies reduce dual     TW TR  SourceRight click MathML additional feature update   sourcewhere subscript iteration index rate  define gij SourceRight click MathML additional feature remark compute gradient objective function respect chosen parameter obtain accord chosen parameter update chosen parameter update obtain quadratic program training algorithm algorithm pseudocode lstm anomaly detector algorithm quadratic program training anomaly detection algorithm OC svm initialize lstm parameter dual OC svm parameter threshold convergence criterion obtain accord optimal obtain remark detect anomaly evaluate gradient training algorithm although quadratic program training algorithm directly optimizes OC svm formulation without approximation depends consecutive update lstm OC svm parameter converge local minimum resolve issue introduce training gradient update parameter however approximation OC svm formulation apply convergence approximate formulation OC svm formulation slack variable max sourcewhere wth sourceby substitute remove constraint obtain optimization minw  TW TR  SourceRight click MathML additional feature differentiable function unable optimization gradient optimization algorithm hence employ differentiable function   SourceRight click MathML additional feature smoothly approximate smooth parameter logarithm increase converges hence comparison smooth approximation proposition increase uniformly converges consequence approximation converges svm objective function define source proof proposition proof proposition appendix modify optimization minw  TW TR  SourceRight click MathML additional feature objective function optimization define  sourceto obtain optimal parameter update converge local global optimum update gradient descent algorithm compute gradient objective function respect parameter compute gradient     update    sourcewhere subscript indicates parameter iteration similarly calculate derivative objective function respect   SourceRight click MathML additional feature update   SourceRight click MathML additional feature lstm parameter optimization orthogonality constraint due update calculate gradient objective function   sourcewe update   sourcewhere  mij SourceRight click MathML additional feature remark compute gradient objective function respect chosen parameter obtain accord chosen parameter update chosen parameter remark semisupervised framework optimization svm algorithm minÎ¸ min wth  wth TW TR  sourcewhere slack variable tradeoff parameter label unlabeled data instance respectively label data instance application quadratic program training semisupervised apply optimization XS similarly modify equation accord gradient training semisupervised framework supervise implementation procedure semisupervised implementation hence update parameter algorithm algorithm pseudocode moreover illustrate convergence approximation proposition proposition demonstrate convergence optimal objective function optimal actual svm objective function theorem algorithm gradient training anomaly detection algorithm OC svm initialize lstm parameter OC svm parameter threshold convergence criterion obtain accord obtain remark detect anomaly evaluate theorem fix unique converges minimum proof theorem proof theorem appendix anomaly detection SVDD algorithm introduce anomaly detection algorithm SVDD formulation joint update lstm SVDD parameter however generic formulation OC svm distinct update parameter proof convergence approximate SVDD formulation actual SVDD algorithm aim hypersphere encloses normal data anomalous data outside hypersphere sequence SVDD optimization minÎ¸  RR  TW TR sourcewhere tradeoff parameter misclassification error radius hypersphere hypersphere addition lstm parameter slack variable respectively OC svm constrain optimization detect anomaly function sgn SourceRight click MathML additional feature quadratic program training algorithm introduce training algorithm quadratic program OC svm assume lstm parameter fix perform optimization SVDD parameter fix lstm parameter lagrangian   nÎ±i sourcewhere lagrange multiplier derivative respect derivative zero yield nÎ±i   SourceRight click MathML additional feature obtain dual minÎ¸      nÎ±i TW TR  SourceRight click MathML additional feature modify sgn      SourceRight click MathML additional feature constrain optimization employ approach OC svm fix lstm parameter parameter optimal SMO algorithm fix update algorithm optimization orthogonality constraint procedure convergence finally evaluate converge parameter remark SVDD apply SMO algorithm procedure OC svm parameter minimize fix parameter due chosen parameter obey hence update iteration SourceRight click MathML additional feature definition OC svm obtain obtain update remain parameter procedure convergence remark SVDD update iteration however instead definition gij sourcewhere       iteration remain parameter procedure remark hence obtain quadratic program training algorithm lstm anomaly detector described algorithm pseudocode algorithm quadratic program training anomaly detection algorithm SVDD initialize lstm parameter dual SVDD parameter threshold convergence criterion obtain accord optimal procedure remark obtain remark detect anomaly evaluate gradient training algorithm introduce training algorithm gradient function eliminate constraint minÎ¸  RR  TW TR  SourceRight click MathML additional feature  SourceSince gradient cannot optimize due  function employ instead modify minÎ¸     TW TR sourcewhere objective function obtain optimal update till local global optimum update employ gradient descent algorithm gradient calculation compute gradient   SourceRight click MathML additional feature update  SourceRight click MathML additional feature subscript iteration likewise compute derivative objective function respect   sourcewith update  sourcefor gradient calculation   SourceRight click MathML additional feature update   SourceRight click MathML additional feature  mij SourceRight click MathML additional feature remark compute gradient objective function respect chosen parameter obtain accord chosen parameter update chosen parameter remark semisupervised framework optimization SVDD algorithm minÎ¸    TW TR SourceRight click MathML additional feature slack variable margin label data instance tradeoff parameter label unlabeled data instance respectively label data instance quadratic program training modify remark remark respect manner modify equation accord obtain gradient training semisupervised framework supervise implementation procedure semisupervised implementation algorithm algorithm convergence proof OC svm algorithm gradient training anomaly detection algorithm SVDD initialize lstm parameter SVDD parameter threshold convergence criterion obtain accord obtain remark detect anomaly evaluate theorem fix unique converges minimum define  source proof theorem proof theorem appendix IV simulation demonstrate performance algorithm data evaluate performance data contains variable data sequence digit data anomaly detection performance benchmark data occupancy hong kong exchange  rate http  stock price data perform benchmark data gru algorithm performance lstm moreover training algorithm perform orthogonality constraint introduce algorithm bound function sigmoid function lstm architecture normalize dimension data throughout denote lstm OC svm anomaly detector gradient quadratic program algorithm lstm GSVM lstm  respectively manner lstm GSVDD lstm QPSVDD SVDD anomaly detector moreover label gru algorithm replace lstm prefix gru anomaly detection variable data sequence evaluate performance introduce anomaly detector digit data data pixel sample digit tablet author varies sample digit significantly introduce algorithm sequence thanks generic structure however conventional OC svm SVDD algorithm cannot directly sequence algorithm sequence obtain fix vector sequence coordinate pixel evaluate performance digit normal another digit anomaly emphasize randomly digit illustration obtain performance digit sample digit training allocate sample training training sample sample anomaly training optimize parameter algorithm twofold validation crucial parameter procedure lstm GSVM lstm  lstm GSVDD lstm QPSVDD respectively furthermore output dimension lstm architecture regularization parameter algorithm implementation conventional OC svm SVDD algorithm libsvm library parameter manner via built optimization libsvm receiver operating characteristic roc curve performance metric roc curve plot positive rate tpr function false positive rate fpr curve auc performance anomaly detection task illustrate roc curve correspond auc label digit normal anomaly respectively OC svm SVDD algorithm directly variable data sequence obtain fix sequence achieve significantly auc introduce lstm lstm lstm GSVM slightly outperforms lstm  lstm GSVDD achieves significantly auc lstm QPSVDD quadratic program training depends consecutive update lstm svm SVDD parameter converge local minimum however gradient guarantee convergence local minimum choice rate although performance obtain performance gradient however overall introduce algorithm significantly auc conventional roc curve algorithm digit data digit normal digit anomaly svm algorithm SVDD algorithm roc curve algorithm digit data digit normal digit anomaly svm algorithm SVDD algorithm besides previous scenario scenario label digit normal anomaly respectively illustrate roc curve correspond auc previous scenario svm SVDD introduce algorithm achieve auc conventional algorithm introduce algorithm lstm GSVM lstm GSVDD achieve auc svm SVDD respectively furthermore auc algorithm previous due similarity digit roc curve algorithm digit data digit normal digit anomaly svm algorithm SVDD algorithm addition digit data perform another handle variable data sequence evaluate anomaly detection performance algorithm financial data ford stock price data daily stock price anomaly detection framework artificially introduce anomaly via gaussian distribution variance training data series data apply variable windowing operation obtain variable data sequence moreover unlike previous lstm GSVM lstm  lstm GSVDD lstm QPSVDD respectively lstm algorithm achieve considerably auc svm SVDD algorithm lstm lstm GSVM slightly outperforms lstm  similarly lstm GSVDD achieves slightly auc lstm QPSVDD moreover previous gradient training performance quadratic program thanks capability roc curve stock price data svm algorithm SVDD algorithm roc curve stock price data svm algorithm SVDD algorithm benchmark data auc algorithm benchmark data moreover training evaluate orthogonality constraint data approach article generic addition lstm algorithm implement approach recently introduce rnn architecture gru architecture define equation SourceRight click MathML additional feature output vector input vector furthermore parameter gru accord dimensionality input output vector replace obtain gru anomaly detector lstm anomaly detection approach another benchmark performance criterion series data occupancy detection evaluate performance algorithm occupancy data data feature relative humidity percentage lux carbon dioxide ppm celsius humidity ratio aim occupy feature procedure IV training data moreover training data lstm GSVM lstm  lstm GSVDD lstm QPSVDD respectively gru algorithm parameter lstm algorithm furthermore maximize performance algorithm due inherent memory lstm gru algorithm achieve considerably auc conventional svm SVDD algorithm moreover gru GSVDD achieves auc algorithm lstm algorithm lstm GSVM lstm  comparable auc gradient training auc quadratic program training stem update procedure guarantee convergence local minimum auc algorithm occupancy  rate http  stock price data anomalous exchange rate detection occupancy data perform  rate data examine performance financial scenario data amount hong kong introduce anomaly data artificially sample gaussian distribution variance training data furthermore training data lstm GSVM lstm  lstm GSVDD lstm QPSVDD respectively illustrate auc algorithm  rate data series data lstm gru algorithm naturally outperform conventional thanks inherent memory preserve sequential information moreover lstm architecture memory content via output gate unlike gru architecture obtain auc lstm GSVM previous gradient training performance quadratic program training network anomaly detection evaluate auc algorithm http data data feature duration connection network service byte source destination destination source feature aim distinguish normal connection network attack lstm GSVM lstm  lstm GSVDD lstm QPSVDD respectively demonstrate performance algorithm http data algorithm achieve auc data lstm gru algorithm auc conventional svm SVDD overall gru QPSVDD achieves auc quadratic program training perform gradient training data however auc slight performance improvement algorithm anomalous stock price detection evaluate anomaly detection performance algorithm another financial data  stock price data data daily stock price  rate data artificially introduce anomaly via gaussian distribution variance training data moreover lstm GSVM lstm  lstm GSVDD lstm QPSVDD respectively illustrate auc algorithm  stock price data gru lstm algorithm achieve considerably auc conventional thanks memory structure although lstm algorithm auc obtain auc gru QPSVDD moreover previous gradient training performance quadratic program thanks capability constraint complexity analysis II performance lstm GSVM scenario orthogonality constraint conventional norm regularization constraint without constraint lstm GSVM auc perform orthogonality constraint outperforms improve detection performance article addition training algorithm data gradient algorithm achieve significantly faster training performance quadratic program due highly complicate structure quadratic program optimization II auc lstm GSVM orthogonality constraint norm regularization constraint constraint training algorithm computer processor ghz cpu GB ram training algorithm computer processor ghz cpu GB ram conclude remark article anomaly detection unsupervised framework introduce lstm algorithm introduce generic lstm structure variable data sequence obtain fix sequence via lstm structure introduce function anomaly detector OC svm SVDD algorithm literature jointly optimize parameter lstm architecture function OC svm SVDD formulation jointly optimize parameter algorithm introduce gradient quadratic program training algorithmic merit extend derivation algorithm semisupervised fully supervise framework apply gradient training modify OC svm SVDD formulation convergence modify formulation actual therefore obtain highly effective anomaly detection algorithm series data variable data sequence simulation due generic structure approach introduce gru anomaly detection algorithm extensive illustrate significant performance improvement achieve algorithm respect conventional simulated data appendix proof proposition simplify notation denote     SourceRight click MathML additional feature max       sourceand     sourcethus conclude monotonically decrease function derive upper bound difference derivative difference   SourceRight click MathML additional feature hence difference decrease function therefore maximum occurs similarly derivative difference positive maximum difference occurs obtain bound  SourceRight click MathML additional feature sufficiently hence increase uniformly converges average data obtain  SourceRight click MathML additional feature prof uniform convergence appendix proof theorem hessian matrix respect     SourceRight click MathML additional feature satisfies  nonzero vector hence hessian matrix positive definite strictly convex function consequently global unique addition derivative    SourceRight click MathML additional feature implies strictly convex function global unique fix proof proposition SourceRight click MathML additional feature convergence proposition    SourceRight click MathML additional feature prof equality  SourceRight click MathML additional feature appendix proof theorem II hessian matrix respect   SourceRight click MathML additional feature  implies nonzero vector hessian matrix positive definite strictly convex function global unique addition derivative    SourceRight click MathML additional feature implies strictly convex function therefore global unique convergence proof directly proof theorem