review reinforcement RL feedback optimal regulation multiagent exist RL optimal graphical review RL optimal online data along trajectory discus integral RL algorithm core algorithm discrete DT continuous CT respectively moreover discus direction policy RL CT DT finally review application publish transaction neural network volume issue june introduction optimal theory mature mathematical discipline optimal policy dynamical optimize user define functionals capture desire objective principle  maximum principle PMP dynamic program DP principle PMP optimality DP sufficient optimality partial differential equation hamilton jacobi bellman HJB equation classical optimal offline knowledge dynamic therefore cope uncertainty dynamic machine enable adaptive autonomy machine grouped supervise unsupervised reinforcement amount quality feedback task supervise feedback information algorithm label training data objective model relation input output parameter unsupervised feedback information algorithm objective classify sample similarity input sample finally reinforcement RL goal orient wherein agent decision maker learns policy optimize reward interact environment RL agent evaluative feedback performance action improve performance subsequent action RL psychology computer economic formulation RL approximate DP adp engineering context RL adp bridge gap traditional optimal adaptive algorithm goal optimal policy function potentially uncertain physical unlike traditional optimal RL HJB equation online unlike traditional adaptive controller usually optimal minimize functionals RL algorithm optimal motivate researcher enable adaptive autonomy optimal manner develop RL controller related theoretical origin RL computer attract increase attention seminal RL society date  algorithm impact totally unknown environment extend RL algorithm continuous CT continuous perform knowledge model optimal policy formulate developed theoretic framework CT switch hybrid adp generally task RL algorithm policy evaluation policy improvement policy evaluation calculates function related policy policy improvement ass obtain function update policy RL algorithm perform policy iteration PI iteration VI PI VI algorithm iteratively perform policy evaluation policy improvement optimal PI admissible policy sequence bellman equation optimal policy contrast PI VI initial stabilize policy RL algorithm PI VI algorithm developed optimal mostly survey PI RL algorithm feedback RL algorithm context mainly optimal regulation optimal agent optimal coordination multiagent objective optimal regulation optimal controller assure output converge zero zero optimal desire optimal controller output desire reference trajectory goal optimal coordination multiagent distribute protocol available local information agent agent achieve objective review exist RL algorithm optimal regulation agent coordination multiagent finally RL algorithm optimal categorize namely policy policy policy evaluate improve policy decision policy function policy generate data behavior policy unrelated policy evaluate improve estimation policy target policy target policy online data obtain offline apply behavior policy dynamic policy data efficient obtain execute behavior policy reuse update function correspond estimation policy moreover policy algorithm account probe exploration schematic policy policy RL category RL policy RL policy apply behavior policy generate data policy policy optimal policy RL policy policy RL diagram policy RL diagram category RL policy RL policy apply behavior policy generate data policy policy optimal policy RL policy policy RL diagram policy RL diagram structure survey literature RL autonomy II optimal discrete DT dynamical online RL algorithm optimal regulation linear nonlinear DT discus recent development RL optimal controller CT dynamical nash online RL graph IV finally application research direction VI II optimal DT online nonlinear invariant sourcewhere input output respectively drift dynamic input dynamic output dynamic assume locally lipschitz  standard assumption unique finite initial optimal regulation goal optimal regulation optimal input stabilize minimize predefined functional related functional define sourcewhere RT hence define minu source function define  equivalent bellman equation sourcethe associate hamiltonian define sourcethe bellman optimality principle optimal function minu sourcewhich DT HJB equation optimal derive argminu source DT linear dynamic become sourcewhere constant matrix appropriate dimension assume bellman equation function quadratic sourcethen DT HJB becomes DT algebraic  equation dare     sourceand optimal input   source approximate RL HJB equation generally extremely impossible analytically approximate exist approximate HJB equation dare knowledge dynamic PI algorithm approximate HJB equation dare perform successive iteration offline PI algorithm algorithm offline DT HJB equation knowledge dynamic algorithm PI algorithm HJB procedure admissible policy bellman equation    convergence update policy source procedure actor critic approximators approximate HJB equation obviate requirement knowledge dynamic actor critic structure widely online HJB critic approximator estimate function update minimize bellman error actor approximator approximates policy update minimize function function    sourceand input    sourcewhere   basis function   vector critic approximator basis function   vector actor approximator basis function bellman equation critic approximator source gradient descent tune critic actor approximators     sourcewhere tune gain actor critic structure evaluate function improve policy nonlinear knowledge dynamic synchronous tune actor critic approximators online approximator approach HJB equation without knowledge internal dynamic greedy iterative heuristic DP introduce obtain optimal saturate controller neural network approximate function optimal policy model unknown remark structure function approximator important factor convergence performance algorithm inappropriate function approximator chosen algorithm converge optimal linear function quadratic therefore error approximation consequently algorithm converges global optimal linear linear algorithm dare nonlinear layer neural network function approximation activation function assure approximation error consequently optimal layer neural network achieve approximation error lesser activation function moreover error tolerant adp algorithm DT guarantee stability presence approximation error function approximation trigger RL reduce communication controller trigger algorithm trigger mechanism determines signal transmit execution achieve performance stabilize nonlinear DT neural network trigger approximation HJB equation nonlinear  sourcewhere target model network hidden layer output layer input vector hidden layer bound approximation error estimate ideal  SourceRight click MathML additional feature     sourcewhere matrix input hidden node correspond output  hidden neuron input model network sample vector correspond gradient descent update network minimize function approximate network gradient descent update network minimize sourcewhere output critic network define  sourcewith nch   nch sourcewhere input output hidden node critic network respectively nch hidden node input critic network sample vector input node critic network sample input trigger define  sourcewith nah   nah sourcewhere input output hidden node action network respectively nah hidden node action network gradient descent update actor network trigger finite optimal scheme uncertain nonlinear DT function DT linear quadratic regulation linear propose action dependent function function instead function bellman equation avoid dynamic literature DT function define  dynamic function becomes     ZT  sourcefor kernel matrix apply stationarity   sourceand  source algorithm model approach algorithm converges global optimal persistence excitation PE satisfied PE guarantee uniqueness policy evaluation iteration however information output feedback  RL algorithm derive linear algorithm knowledge dynamic advantage measurement input output data algorithm algorithm dare procedure admissible policy bellman equation ZT   ZT  convergence update policy   source procedure optimal goal optimal input desire reference trajectory define error sourcein input consists feedforward guarantee feedback stabilizes feedforward obtain dynamic inversion concept  functional    sourcewhere rte feedback input apply stationarity  sourcethen optimal input feedback feedforward source obtain feedforward input knowledge dynamic reference trajectory dynamic formulation developed feedback feedforward input simultaneously enables RL algorithm without knowledge dynamic assume reference trajectory generate command generator model sourcewhere augment construct error reference trajectory sourcewhere augment source functional define TQ sourcewhere RT function augment XT  sourcewhere QT sourceand discount factor remark essential discount performance function propose formulation reference trajectory zero application infinite without discount factor input contains feedforward depends reference trajectory zero infinity difference equivalent sourcethe hamiltonian XT  sourcethe optimal minu sourcewhich DT HJB equation optimal argminu  source approximate RL DT HJB equation approximate offline PI algorithm PI algorithm DT HJB iterate bellman equation augment dynamic update input algorithm convergence algorithm algorithm algorithm PI algorithm HJB procedure admissible policy bellman equation XT      convergence update policy  SourceRight click MathML additional feature procedure online actor critic approximators obviate requirement knowledge dynamic reference trajectory dynamic actor critic structure developed nonlinear optimal optimal linear PI VI algorithm linear quadratic tracker  online without knowledge dynamic information input output data DT zero sum controller disturbance input minimize maximize player respectively linear optimal zero sum  dynamic disturbance input sourcewhere measurable vector input disturbance input drift dynamic input dynamic disturbance input dynamic define functional optimize  sourcewhere RT stabilize function feedback disturbance policy define  source difference equivalent  SourceRight click MathML additional feature optimal zero sum differential  SourceRight click MathML additional feature worth minimize player maximize zero sum hamilton jacobi isaac  equation  equation optimal disturbance sourceand  SourceRight click MathML additional feature respectively approximate RL policy policy RL algorithm   successive approximation bellman equation policy RL DT policy RL algorithm  PI algorithm algorithm offline algorithm knowledge dynamic actor critic structure online PI algorithm simultaneously update function policy knowledge drift dynamic function input approximate respectively disturbance input approximate    sourcewhere  basis function approximator   vector basis function algorithm PI algorithm  procedure admissible policy bellman equation     convergence update policy disturbance policy SourceRight click MathML additional feature  SourceRight click MathML additional feature procedure function input approximators update gradient descent respectively gradient descent update disturbance input approximator algorithm optimal input disturbance input linear without knowledge dynamic however disturbance input update prescribed manner policy RL algorithm knowledge dynamic disturbance input update prescribed manner schematic policy RL zero sum policy RL behavior disturbance policy actual disturbance environment cannot specify disturbance policy data policy policy RL zero sum DT description sourcewhere disturbance input constant matrix appropriate dimension derive policy RL algorithm rewrite  kjx kjx sourcewhere  dkj estimation policy kjx kjx contrast behavior policy actually apply generate data taylor expansion function function yield   kjx kjx  kjx   kjx  kjx   SourceRight click MathML additional feature remark explicitly knowledge dynamic simultaneously without knowledge dynamic matrix vec rnm transpose vector stack matrix  vec  dkj policy bellman equation rewrite vec vec kjx vec kjx kjx vec kjx vec kjx kjx vec kjx kjx vec kjx kjx vec    SourceRight click MathML additional feature      1D  1D unknown variable bellman equation vec tvec tvec tvec tvec tvec tvec sourcewhere    ψij define LS algorithm policy PI  procedure iteration admissible policy probe LS vec tvec tvec tvec tvec tvec tvec source update disturbance gain source procedure optimal CT online nonlinear invariant SourceRight click MathML additional feature input output respectively drift dynamic input dynamic output dynamic assume locally lipschitz  optimal regulation goal optimal regulation optimal input assure converge zero minimize functional functional define  SourceRight click MathML additional feature RT function admissible policy define   differential equivalent sourceand hamiltonian SourceRight click MathML additional feature optimal bellman optimality equation SourceRight click MathML additional feature CT HJB equation optimal argminu source approximate RL policy policy integral RL irl algorithm approximate HJB iterate bellman equation policy irl equivalent formulation bellman equation involve dynamic SourceRight click MathML additional feature interval equation irl bellman equation PI algorithm implement iterate irl bellman equation update policy irl algorithm algorithm online knowledge drift dynamic implement algorithm neural network structure approximate function algorithm sequential RL algorithm actor policy improvement critic policy evaluation update sequentially synchronous update actor critic introduce update actor critic simultaneously assure stability later synchronous actor critic structure augment identification avoid knowledge dynamic algorithm policy irl algorithm HJB procedure admissible policy bellman equation   convergence update policy source procedure policy irl replay technique obtain easy convergence irl algorithm recent transition sample repeatedly gradient update propose adaptive gradient decent algorithm minimize instantaneous temporal difference TD error minimize TD error transition sample assume function uniformly approximate irl bellman equation  SourceRight click MathML additional feature RN basis function vector basis function therefore approximate irl bellman equation becomes TW  SourceRight click MathML additional feature TD error critic approximator data stack evaluate define bellman equation error TD error critic estimation TW  source replay gradient decent algorithm critic NN     sourcethe gradient update TD error minimizes sample stack trigger policy RL trigger version optimal controller sample information instead becomes SourceRight click MathML additional feature consecutive sample trigger controller HJB equation becomes  TR TR sourcewhere trigger HJB equation function approximate compact  sourcewhere basis function vector basis function approximation error optimal trigger controller rewrite  sourcethe optimal trigger controller approximate actor  SourceRight click MathML additional feature basis function vector basis function approximation error function optimal policy estimate respectively ideal critic actor approximators   sourcethe bellman error define sourcewith  tune minimize   WTW WTW sourcewith update actor approximator error define  TW sourcethe tune minimize  SourceRight click MathML additional feature equation compute     SourceRight click MathML additional feature convergence stability extend input constraint without knowledge dynamic RL approach interconnect policy irl develop policy irl algorithm dynamic rewrite SourceRight click MathML additional feature policy update contrast behavior policy actually apply dynamic generate data differentiate along dynamic   TR  equation yield policy irl bellman equation  TR source iterate irl bellman equation yield policy irl algorithm remark fix policy policy apply policy irl bellman equation function update policy simultaneously without knowledge dynamic implement policy irl algorithm algorithm actor critic structure approximate function policy algorithm policy irl algorithm HJB procedure admissible policy policy bellman equation  TR  convergence procedure linear policy irl bellman equation     TR SourceRight click MathML additional feature iterate bellman equation yield policy irl algorithm linear robust policy irl uncertain nonlinear SourceRight click MathML additional feature component available feedback  unknown input unknown locally lipschitz function drift dynamic input dynamic respectively develop robust policy irl algorithm dynamic rewrite sourcewhere differentiate along      equation yield robust policy irl bellman equation   source actor critic structure yield         SourceRight click MathML additional feature iterate irl bellman equation robust policy irl algorithm algorithm robust policy irl algorithm procedure admissible policy policy bellman equation          convergence procedure optimal goal optimal input desire reference trajectory define error source DT standard technique feedback feedforward input separately knowledge dynamic formulation developed feedback feedforward input simultaneously enables RL algorithm without knowledge dynamic assume reference trajectory generate command generator model SourceRight click MathML additional feature augment construct error reference trajectory sourcewhere augment sourcethe functional define TQ sourcewhere RT function augment yield XT  sourcewhere QT sourceand discount factor differential equivalent bellman equation XT SourceRight click MathML additional feature hamiltonian XT XT sourcethe optimal XT sourcewhich CT HJB equation optimal argminu XT  source approximate RL function manner policy policy irl algorithm optimal regulation policy policy irl algorithm developed optimal CT HJB equation policy irl algorithm optimal regulation policy irl algorithm CT HJB equation online without knowledge internal dynamic irl algorithm algorithm online algorithm knowledge internal dynamic actor critic structure implement algorithm quadratic function irl bellman equation linear  algorithm policy irl algorithm HJB procedure admissible policy bellman equation  XT     SourceRight click MathML additional feature convergence update policy SourceRight click MathML additional feature procedure policy irl policy irl optimal regulation augment discount function policy irl algorithm developed avoid requirement knowledge dynamic implement policy irl algorithm algorithm actor critic structure approximate function policy algorithm policy irl algorithm HJB procedure admissible policy policy bellman equation    TR  convergence procedure remark exist optimal affine extension RL algorithm nonaffine optimal regulation optimal CT CT dynamic presence disturbance input SourceRight click MathML additional feature measurable vector input disturbance input drift dynamic input dynamic disturbance input dynamic performance index optimize define   sourcewhere RT stabilize function define   source differential equivalent bellman equation   sourceand hamiltonian   source define player zero sum differential   zero sum  equation  SourceRight click MathML additional feature equation  SourceRight click MathML additional feature approximate RL PI algorithm approximate  iterate bellman equation policy irl equivalent formulation bellman equation involve dynamic sourcefor interval equation irl bellman equation PI algorithm algorithm offline algorithm knowledge dynamic actor critic structure implement online PI algorithm simultaneously update function policy knowledge drift dynamic various update developed actor critic disturbance approximators minimize bellman error algorithm policy irl regulation zero sum procedure admissible policy bellman equation vij  tdi vij  vij SourceRight click MathML additional feature convergence vij update policy SourceRight click MathML additional feature procedure policy irl develop policy irl algorithm dynamic rewrite sourcewhere policy update contrast behavior policy actually apply dynamic generate data differentiate along dynamic     TR   equation yield policy irl bellman equation     source iterate irl bellman equation yield policy irl algorithm fix policy actual disturbance policy apply policy irl bellman equation function policy disturbance policy simultaneously without knowledge dynamic implement policy irl algorithm algorithm actor critic structure approximate function disturbance policy algorithm policy irl algorithm  procedure admissible policy policy bellman equation   TR  SourceRight click MathML additional feature convergence procedure policy irl algorithm optimal without knowledge dynamic reference trajectory dynamic nash online RL noncooperative played agent player nonlinear invariant differential  sourcewhere measurable vector  input drift dynamic input dynamic assume locally lipschitz  functional associate player define  SourceRight click MathML additional feature   rij  player define SourceRight click MathML additional feature differential equivalent function bellman equation  source hamiltonian function define  sourceby apply stationarity associate feedback policy   source substitute feedback policy hamiltonian couple hamilton jacobi HJ equation      source approximate RL PI algorithm developed iterate bellman equation approximate couple HJ equation algorithm PI regulation nonzero sum procedure tuple admissible policy   tuple vki couple bellman equation vki      SourceRight click MathML additional feature update tuple policy  vki SourceRight click MathML additional feature procedure policy irl obvious knowledge dynamic equivalent formulation couple irl bellman equation involve dynamic tri SourceRight click MathML additional feature interval function approximate compact  sourcewhere  basis function basis vector basis function approximation error assume estimate output critic  source procedure zero sum update rewrite     TW   sourceand    TW       SourceRight click MathML additional feature respectively  model nonzero sum nash obviate requirement knowledge dynamic identification IV graphical interaction agent model fix strongly graph  EG define finite  agent EG    information exchange link node define node incoming denote EG adjacency matrix AG αij αij EG zero otherwise diagonal matrix graph define diag  agent dynamic model axi  sourcewhere measurable vector rmi input player input matrix respectively leader node  dynamic source agent network seek cooperatively asymptotically leader node  simultaneously satisfy distribute functional define neighborhood error agent  sourcewhere pin gain agent pin leader node dynamic aδi   SourceRight click MathML additional feature functional associate agent uni uni    SourceRight click MathML additional feature user define matrix  rij appropriate dimension detectable desire graphical nash equilibrium agent source express couple distribute minimization  sourcewith dynamic ultimate goal distribute optimal function define minui    SourceRight click MathML additional feature define  associate agent neighborhood error uni  aδi      SourceAccording stationarity optimal  uni   satisfy appropriate distribute couple HJ equation source assume function quadratic neighborhood error  sourcewhere  unique matrix complicate distribute couple equation  aδi      aδi            SourceRight click MathML additional feature optimal player  source approximate RL PI algorithm developed distribute couple HJ equation PI algorithm algorithm offline algorithm develop online RL algorithm couple HJ equation algorithm PI regulation graphical procedure tuple admissible policy   tuple vki couple bellman equation  vki  aδi      source update tuple policy  vki  source procedure assume exist constant function approximate compact  SourceRight click MathML additional feature  basis function vector basis function approximation error assume estimate output critic  source estimate approximate lyapunov equation  aδi biu  SourceRight click MathML additional feature residual error due approximation   SourceRight click MathML additional feature denote estimate ideal critic update minimize residual error guarantee hence tune critic aδi biu  aδi biu      SourceRight click MathML additional feature tune gain actor   aδi biu  TW     aδi biu  TW sourcewhere  tune gain matrix guarantee stability aδi biu  aδi biu  source application RL successfully apply robot navigation decade application RL robotics increase steadily RL kinematic dynamic scheme  mobile robot reference RL network mobile robot pioneer DX neural network RL controller active simulated link biped robot RL actor critic framework fully actuate freedom autonomous underwater vehicle RL robust adaptive optimality  mobile robot synchronization communication graph reference optimal adaptive consensus formation scheme finite horizon networked mobile robot agent presence uncertain robot agent dynamic irl approach employ novel adaptive impedance robotic exoskeleton adjustable robot  irl marine craft unknown  parameter RL along stabilize feedback controller pid linear quadratic regulation improve performance trajectory robot manipulator mobile robot navigation task refers obstacle avoidance specify goal RL mobile robot navigation mainly robot expert demonstration robot autonomous navigation former standard behavior robot learns data generalize potential situation later RL technique robot via interaction surround environment stability concern task continuous autonomous navigation mobile robot   propose LS PI robot navigation task neural network approximate function environment assume static autonomous agent adapt strategy cope surroundings challenge navigation task environment dynamic obstacle exist application RL proposes PI technique actor critic structure automatic voltage regulator model neglect sensor dynamic theory distribute controller desire voltage magnitude frequency load optimal switch typology voltage converter investigate boost converter RL nonlinear strategy attain constant output voltage RL distribute microgrids establish coordination active load collectively respond load RL apply heater sensor VI conclusion review RL technique optimal data along trajectory online RL optimal regulation optimal nash graphical dynamic RL online technique another approach reuse data replay technique RL algorithm data efficient reuse data static RL  controller nonlinear investigate exist RL controller disturbance another important research direction develop novel RL approach feedback nonlinear dimensional input unstructured input data neural network approximate accurate structure function avoid divergence RL algorithm consequently instability feedback RL feedback however develop algorithm assure stability feedback lyapunov