vehicle platoon dominant mode future knowledge reinforcement RL algorithm apply vehicle platoon action RL apply agent tackle multiagent multiagent RL algorithm parameter grows exponentially increase agent involve previous multiagent RL algorithm generally redundant information agent amount useless unrelated information convergence training extraction information random action usually contribute crash training communication proximal policy optimization CommPPO algorithm propose tackle issue specific CommPPO model adopts parameter structure dynamic variation agent handle various platoon dynamic splitting merge communication protocol CommPPO consists widely predecessor leader follower typology platoon adopt transmit global local information agent reward reward communication channel propose spurious reward lazy agent exist multiagent RLS moreover curriculum approach adopt reduce crash training validate propose strategy platoon exist multiagent RLS traditional platoon strategy apply scenario comparison CommPPO algorithm gain reward achieve fuel consumption reduction introduction transportation source consumption greenhouse emission europe transportation responsible consumption emission transport greenhouse emission transport demand therefore reduce transportation related fuel consumption critical issue development vehicle vehicle VV communication technology vehicle platoon dominant mode urban freeway vehicle platoon fleet vehicle specific information typology exist adopt optimal theory model predictive mpc improve fuel efficiency platoon formulate function fuel consumption minimization heuristic algorithm optimal action vehicle however platoon dimension significantly increase computational complexity reduces feasibility application recent reinforcement RL approach apply various microscopic traffic scenario autonomous vehicle attract significant attention performance knowledge RL algorithm apply vehicle platoon vehicle platoon acceleration vehicle simply adopt agent RL obtain vehicle joint action action exponentially increase increase platoon intractable hence centralize RL infeasible platoon due extremely dimension joint action previously numerous multiagent RL algorithm propose action researcher adopt independent learner optimize reward agent perspective environment partially agent spurious reward signal originate teammate unobserved behavior independent RL unsuccessful due nonstationary later researcher adopt parallel actor centralize critic agent structure centralize training decentralize execution RL multiagent context however training fully centralize critic becomes impractical handful agent accord improve performance multiagent RL communication protocol propose enhance cooperation agent cooperate agent action agent communication however agent information agent communication helpful agent differentiate valuable information information addition usually spends enormous computational resource complex neural network information typology article communication proximal policy optimization CommPPO algorithm propose vehicle platoon contribution construct multiagent RL framework vehicle platoon achieve goal guaranteed traffic safety efficiency carefully balance crucial traffic safety efficiency propose RL platoon framework ensures reasonable vehicle platoon mixed traffic autonomous vehicle cavs driven vehicle  exploration rate generate performance usually brings plenty crash vehicle platoon hence adopt curriculum platoon training task platoon knowledge task platoon novel information communication protocol enhance cooperation agent improve performance multiagent information transmission channel propose communication protocol transmission channel observation widely predecessor leader follower typology vehicle platoon adapt transmit carefully information agent address issue nonstationary partial observation avoids inefficient information transmission another reward transmission channel instead directly adopt local global reward reward propagation channel propose allocate explicit representative reward agent avoid lazy agent parameter RL structure adopt reduce computational complexity agent variable agent propose CommPPO algorithm adopts parameter structure agent network decrease computational complexity due various platoon dynamic platoon splitting merge agent varies agent environment parameter structure handle issue agent parameter aim improve efficiency vehicle platoon fuel consumption usually occurs traffic oscillation scenario frequent acceleration deceleration concurrently freeway urban hence various traffic oscillation scenario simulated performance CommPPO exist algorithm article organize II review related methodology CommPPO vehicle platoon strategy IV introduces simulation platform experimental VI concludes discus future direction II related exist vehicle platoon strategy vehicle platoon vehicle leader proactively reaction dynamic vehicle addition predefined communication typology adjacent vehicle effectively adjust inner gap cooperate hence vehicle platoon increasingly prevail future platooning strategy widely apply management propose advice approach reduce consumption recent apply optimal theory mpc algorithm calculate optimal optimal formulate function fuel consumption minimization heuristic algorithm optimal action vehicle approach theoretically obtain mathematical optimal fuel consumption minimization formulate assumption predecessor platoon constant horizon unrealistic accelerate decelerate scenario hence optimal predecessor trajectory specific predecessor disturbance addition mpc algorithm contains online compute workload decay feasibility platoon RL application autonomous vehicle recent various RL algorithm apply autonomous vehicle mainly category category focus training autonomous vehicle smooth efficient policy lane maneuver category VV vehicle infrastructure communication derive data driven policy autonomous vehicle primary approach adopt actor critic RL algorithm formulate model category autonomous vehicle movement complex urban scenario recent model autonomous vehicle decision traffic autonomous vehicle explore parking avoid multiagent RL algorithm multiagent RL adopt decentralize learner independent agent agent access local observation agent optimize joint reward agent network structure correspond parameter agent considers agent environment apply extensive later development RL independent learner network DQN recurrent network  performance obtain recently parameter structure apply continuous action combine trust policy optimization TRPO algorithm empirical multiagent demonstrate parameter TRPO PS TRPO agent perform domain continuous action agent nonstationary dynamic environment effectively teammate behavior due unstable environment agent tends spurious reward originate teammate address nonstationary issue multiagent RL algorithm adopt structure centralize training decentralize execution achieve coordination agent centralize training decentralize execution allows agent construct individual action function optimization individual optimization joint action function agent strategy centralize critic agent action independently execution propose decomposition network  algorithm decompose function agent intuitively  agent impact joint reward assume joint action function additively decompose function agent function relies local action later network  transformation  relaxed assumption neural network express relationship local global recent apply centralize training decentralize execution continuous action multiagent deterministic policy gradient  counterfactual multiagent coma gain performance various cooperative competitive environment enhance cooperation agent various communication protocol integrate multiagent representative differentiable  dial  knowledge agent action adopt network multiagent passing average message agent module layer later policy gradient algorithm multiagent  coordinate network  propose bidirectional recurrent neural network heterogeneous agent communicate parameter  discover effective collaborate combat recent target multiagent communication  nearly decomposable function  adjust communication channel information transmission previous multiagent RL algorithm generally redundant information agent amount useless unrelated information convergence training extraction information limitation inspire propose CommPPO algorithm enhance communication efficiency agent preliminary preliminary knowledge reinforcement RL agent interact environment behave environment without prior knowledge maximize numerically predefined reward minimize penalty action RL agent receives feedback environment performance action feedback reward penalty iteratively update action policy optimum policy successful application RL chess alpha inspire significant amount engineering application RL intelligent transportation automate vehicle classical RL algorithm RL network perform discrete action handle continuous action resolve difficulty policy gradient approach introduce enable agent update policy continuous policy RL algorithm policy proximal optimization formulate policy continuous function policy gradient algorithm objective function decides update direction policy   SourceRight click MathML additional feature average finite batch sample action policy actor parameter estimation advantage function define probability ratio  probability policy parameter update accord encourage vice versa  SourceRight click MathML additional feature however minimization excessively policy update without constraint penalize policy ensure efficiency importance sample propose clipped ppo adopt article   min clip SourceRight click MathML additional feature hyperparameter predecessor leader follower typology predecessor leader follower typology widely communication protocol vehicle platoon adopt enhance cooperation agent traffic network platoon cavs HDV assume cav vehicle sensor detect location predecessor VV communication technology enables cav broadcast information cavs platoon assumption accepted previous assumption modify predecessor leader follower typology apply communication cav receives information traffic predecessor dash traffic platoon predecessor dash focus propose RL framework optimal vehicle platoon strategy reduce communication failure framework CommPPO vehicle platoon strategy framework CommPPO vehicle platoon strategy communication proximal policy optimization algorithm nonstationary multiagent replay sample obsolete negatively impact training address disable replay instead rely asynchronous training hence online update RL algorithm ppo adopt article detail principle propose CommPPO communication protocol vehicle platoon multiagent explicit interaction relationship influence agent agent already concretely vehicle platoon direction precede agent influence agent influence calculate ordinal vehicle influence vehicle however suffer influence vehicle vehicle platoon influence precede vehicle influence vehicle argue algorithm performance communication typology improve adopt specialized communication channel CommPPO structure agent parameter correspond vehicle vehicle platoon vehicle detect global influence platoon broadcast preliminary information agent agent influence predecessor hence agent receives global  agent local global agent respective index vector agent action global local besides actor critic parameter structure potential drawback global reward lazy agent agent learns useful policy agent discourage agent tends objective collaboration simply allocate local reward agent agent spurious reward signal originate agent hence compound reward reflect interaction agent propose reward propagation channel specifically agent platoon agent influence vehicle reward agent calculate equation  discount SourceRight click MathML additional feature reward agent calculate traffic reward agent update critic discount discount factor precede vehicle related vehicle relative acceleration distance assumption influence influence evaluation focus addition influence precede agent attenuates vehicular constant discount factor algorithm curriculum action RL likely randomly distribute predefined training random action implement vehicle platoon vehicle collide training interrupt traffic collision promptly random action platoon scenario agent optimal strategy hence curriculum approach adopt training task knowledge task specifically cav platoon agent sample data stably cavs platoon sixteen curriculum approach agent target agent algorithm describes parameter training approach agent ppo initialize actor critic network parameter agent increase gradually curriculum iteration algorithm sample trajectory agent snext update parameter network unlike distribute ppo  worker computes gradient central ppo CommPPO computes gradient trajectory maximize objective robust performance min clip SourceRight click MathML additional feature agent index platoon algorithm CommPPO initial policy critic network actor agent iteration rollout trajectory agent compute advantage  trajectory  maximize bracket objective function clipped ppo propose OpenAI reflect advantage policy parameter vector inside min function advantage policy policy obtain maximize without constraint excessively policy update decay update stability algorithm hence formula clip introduce modify objective function clip probability ratio remove incentive outside interval finally objective bound minimum transition sample parameter agent update parameter selection agent evenly contributes central update avoid lazy agent crucial interaction CommPPO algorithm traffic network crucial RL sequential decision action reward CommPPO agent perceive traffic simulation network action behavior platoon implement action agent reward traffic simulation environment obtain agent receives transition transition sample agent series transition agent trajectory eventually CommPPO computes gradient trajectory agent hence agent contribution policy gradient reduces correlation sample data ensure algorithm stability optimal policy benefit agent converges collective goal eventually action agent correspond cav adjust acceleration action CommPPO platoon strategy principle cav recommend previous driver comfortable previous apply acceleration  cav adopt CommPPO algorithm variable reflect traffic platoon cav vector consists variable communication protocol difference HDV cav former minus latter difference cav predecessor cav latter minus former cav gap cav predecessor cav ordinal worth mention cav predecessor HDV reward article aim minimize fuel consumption traffic oscillation scenario guaranteed traffic safety efficiency accord frequent acceleration deceleration factor fuel consumption hence agent acceleration penalty define norm ratio acceleration acceleration bound scenario vehicle cannot entirely sacrifice traffic safety fuel efficiency otherwise abnormal behavior crash ensure acceptable avoid penalty gap ahead significant gap threshold  accord vehicle gap ahead  ensure vehicle mode moreover avoid crash maximum conflict acceleration introduce traffic conflict traffic safety index crash risk traffic collision likely traffic conflict index exceeds predefined threshold hence vehicle decelerate proactively avoid crash traffic conflict occurs deceleration rate avoid crash  widely traffic conflict index exist evaluate rear crash risk  define minimum deceleration rate vehicle velocity vehicle avoid crash  SourceRight click MathML additional feature velocity vehicle vehicle precede vehicle vehicle vehicle  assumption reaction computational signal cav cav reaction extent scenario cav decelerate instantaneously avoid crash  extensive threshold introduce  index reflect traffic safety calculate traffic obtain VV communication technology without knowledge driver reaction judge traffic conflict occurs maximum threshold index maximum available deceleration rate  apply vehicle deem conflict vehicle   literature widely accepted  adopt hence maximum acceleration without traffic conflict  calculate accordingly penalty acceleration  acceleration vehicle minimum  overall agent penalize escape mode traffic conflict predecessor reward agent calculate   gap text   SourceRight click MathML additional feature acceleration vehicle gap gap ahead vehicle amax acceleration bound neural network agent neural network actor policy generation critic policy improvement critic network compute policy advantage actor network construct policy parameter update critic layer fully neural network neuron layer adopt input layer variable neuron output layer correspond neuron actor layer fully neural network neuron layer adopt input layer network variable neuron output layer neuron standard deviation neuron action distribution respectively exist approach exist multiagent RL algorithm performance propose CommPPO exist multiagent RL strategy PS ppo  introduce comparison PS ppo originate PS TRPO adopts independent learner parameter agent focus comparison structure replace TRPO ppo  apply multiagent variable agent suitable vehicle platoon  communication latent hidden layer structure parameter however proposes bidirectional recurrent neural network model actor critic network model gain performance reader refer detail traditional platoon strategy validate CommPPO efficiency widely platoon strategy mpc apply scenario mpc algorithm briefly introduce detail refer previous distribute mpc platoon strategy adopt acceptable computational accord mpc platoon strategy function SourceRight click MathML additional feature SourceRight click MathML additional feature objective function SourceRight click MathML additional feature constraint function variable constraint vmax amin amax SourceRight click MathML additional feature coefficient denote location acceleration vehicle vehicle respectively amin amax vmax minimum acceleration maximum acceleration maximum minimum constant gap prediction horizon respectively parameter amin amax vmax  relatively headway adopt oscillation mitigate performance mpc strategy cavs mpc algorithm  intelligent driver model idm calculate optimal function optimal acceleration vehicle refer via information IV development simulation platform traffic simulation software simulation urban mobility sumo adopt simulate dynamic vehicle platoon article sumo emerge widely traffic simulation platform various exist microscopic simulation sumo numerous lane model satisfy research importantly source software sumo user privilege deeper development enables communication interaction software import python package vein future cav HDV exist simultaneously mixed traffic scenario extend widely model namely idm adopt simulate movement HDV model parameter idm quickly accurately calibrate addition simulate reaction mode HDV collision behavior reader refer detail model parameter training environment parameter setting traffic oscillation phenomenon freeway urban behavior indicates vehicle  accelerates vehicle conduct behavior sequentially series behavior contribute traffic disturbance furthermore frequent acceleration deceleration largely contribute fuel consumption accord hence apply CommPPO traffic oscillation scenario fuel consumption minimization scenario vehicle vehicle platoon load lane freeway platoon increase gradually initial headway respectively vehicle deceleration acceleration simulation worth mention episode simulation fuel consumption attempt CommPPO platoon strategy oscillation scenario ensure comparison cav HDV assume physical aerodynamic calculate fuel consumption accord resistance aerodynamic slope sum calculate equation FG FG  SourceRight click MathML additional feature FG refer drag resistance gravity vehicle suffers respectively vehicle physical aerodynamic accord derivation fuel consumption equation complex objective instead approximate differentiable function velocity input develop algorithm ecological typical vehicle nonnegative fuel consumption  per SourceRight click MathML additional feature otherwise fuel consumption SourceRight click MathML additional feature acceleration vehicle model coefficient obtain curve fitting accord physical aerodynamic physical aerodynamic simulation analysis training CommPPO vehicle platoon interaction sumo improve training curriculum adopt agent increase gain reward positive become sufficiently training parameter II conduct sensitivity analysis discount factor average increase discount factor reward convex trend gain reward discount factor determines reward function RL heuristic algorithm performance improve precise adjustment discount factor convergence performance multiagent RL algorithm plot reward training episode CommPPO gain reward algorithm curriculum reward gradually increase agent outperform algorithm training II hyperparameters correspond description II hyperparameters correspond description reward curve reward curve performance algorithm vehicle platoon diagram vehicle trajectory plot vehicle vehicle purple baseline scenario vehicle traditional idm model sumo oscillation propagate continuously upstream vehicle emergent deceleration contrast CommPPO vehicle platoon almost dampen oscillation trajectory vehicle CommPPO proactively slows suitable relatively gap ahead propagation oscillation fuel consumption without CommPPO respectively reduction achieve however scenario  PS ppo oscillation partly dampen vehicle significant gap predecessor oscillation fuel consumption reduce communication channel CommPPO valuable information agent vehicle leader action proactively explore vehicle algorithm acceleration curve vehicle plot scenario CommPPO maximum absolute acceleration initial absolute acceleration vehicle hence traffic oscillation dampen vehicle smoothly diagram vehicle trajectory strategy diagram vehicle trajectory strategy vehicle recover baseline fuel efficient mode ideal limit relatively tends fuel consumption accord fuel consumption function concave curve respect vehicle likely recover initial vehicle tend relatively gap predecessor alleviate future oscillation contributes difference traffic oscillation actually RL heuristic algorithm multiagent RL uniform equilibrium  gap platoon RL necessarily recover initial previous agent RL oscillation mitigate strategy propose traffic oscillation reflect RL algorithm equilibrium influence disturbance performance CommPPO scenario scenario simulated fuel consumption curve fuel consumption CommPPO almost explore traffic disturbance curve plot traffic disturbance define average acceleration disturbance relatively CommPPO oscillation trigger traffic disturbance reflect frequent acceleration deceleration vehicle fuel consumption scenario algorithm vehicle impact factor fuel consumption vehicle CommPPO relatively vehicle algorithm extremely around fuel consumption CommPPO fuel consumption disturbance comparison scenario CommPPO strategy mixed traffic mixed cav HDV traffic scenario extend future moreover ratio cav HDV varies generally platoon likely environment cav penetration rate mixed traffic cav HDV randomly distribute freeway penetration rate cavs necessarily influence CommPPO vehicle platoon traffic scenario cav penetration rate simulated widely vehicle platoon algorithm mpc apply comparison scenario vehicle predefined trajectory load freeway vehicle insert later headway trajectory vehicle define constant  decrease zero vehicle deceleration zero accelerates constant fuel consumption scenario cav penetration rate strategy fuel consumption increase cav penetration rate penetration rate scenario cavs perform sparse information obtain cavs vice versa CommPPO strategy contribute reduction fuel consumption mpc strategy addition environment cav penetration rate propose strategy acceptable performance reduction widely mpc platoon strategy reduction fuel consumption comparison strategy fuel consumption comparison strategy average calculate IV traffic efficiency decrease trail vehicle ideal limit accordingly penalize vehicle gap ahead reward function vehicle traffic disturbance fuel consumption reduction reduce slightly sacrifice traffic efficiency reduction IV IV average comparison strategy IV average comparison strategy cavs navigate  gap gap threshold  belong platoon vehicle leader platoon typical scenario without oscillation cavs precede vehicle platoon relatively constant reduce unnecessary fuel consumption oscillation scenario oscillation trigger vehicle ahead conduct behavior sequentially cavs platoon obtain oscillation information proactively action reduce traffic disturbance specific cav platoon broadcast disturbance information vehicle vehicle decelerate extent gap predecessor largely alleviates traffic disturbance action cavs aim minimize fuel consumption traffic disturbance article analyze computational burden computational validate propose strategy calculate average exist strategy average surge increase cav penetration rate scenario mpc strategy fluctuate around initial scenario CommPPO strategy addition CommPPO strategy spent mpc strategy scenario averagely environment cav mpc decision variable exponentially increase increase cavs objective function decision variable constraint contrast CommPPO neural network parameter directly action without recursive iteration addition parallel agent action independently computational increase penetration rate accord deployability vehicle platoon traffic agent action promptly compute average computational CommPPO algorithm platoon maximum computational hence parameter structure satisfies operation cav statistically comparison mixed traffic boxplot denote maximum quartile sample median quartile minimum data comparison mixed traffic boxplot denote maximum quartile sample median quartile minimum data mpc effectively complex optimization industrial delay applicable platoon communication reaction delay due enormous computational complexity mpc suitable extensive delay platoon scenario operation development advanced communication technology 5G dedicate communication limited delay neglect scenario previous research RL model development assume ideal environment information delay research influence vehicle platoon due parameter structure CommPPO algorithm directly apply vehicle platoon platoon agent performance CommPPO vehicle platoon platoon previous scenario severe traffic oscillation simulated oscillation mitigate performance trajectory vehicle define constant  decrease zero vehicle deceleration zero accelerates constant cav penetration rate consistent scenario platoon simulated vehicle platoon vehicle platoon vehicle platoon vehicle platoon evenly distribute traffic diagram vehicle trajectory vehicle trajectory smoothly decrease platoon statistically platoon separately distribute mixed traffic achieve fuel consumption reduction mixed traffic various unpredictable factor emergent deceleration HDV vehicle platoon sensitive traffic disturbance action promptly hence platoon decomposition dampen severe oscillation heatmap platoon heatmap platoon VI conclusion discussion propose multiagent RL algorithm CommPPO vehicle platoon improve efficiency traffic oscillation agent parameter vehicle CommPPO handle variable agent communication protocol propose reward communication widely predecessor leader follower typology transmit local global information agent reward propagation channel adopt avoid spurious reward lazy agent moreover curriculum approach adopt training CommPPO agent interact sumo recursively explore optimal action communication channel vehicle CommPPO platoon proactively react dynamic vehicle avoid emergent deceleration hence traffic disturbance fuel consumption reduce multiagent RL algorithm mpc algorithm mixed traffic fuel consumption decrease cav penetration rate cavs perform environment abundant information obtain cavs moreover platoon separately distribute mixed traffic achieve fuel efficiency platoon shed apply multiagent RL vehicle platoon guaranteed safety efficiency vehicle parallel agent achieve collective goal predefined communication typology propose CommPPO algorithm communication channel address critical issue exist multiagent RL nonstationary spurious reward lazy agent computational performance mixed traffic algorithm applicable future article relationship reward relative relationship instead absolute agent reward action nowadays autonomous vehicle related roadside communication device popular urban environment commercial hence propose strategy applicable scenario without advanced information transmission device recently numerous advanced  technology developed autonomous vehicle cellular propose strategy applicable scenario technology moreover vehicle platoon unique multiagent explicit interaction relationship agent influence agent CommPPO practical partial limited observation