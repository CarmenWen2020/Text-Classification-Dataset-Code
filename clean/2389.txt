BLS aim alternative structure propose structure suffer consume training parameter filter layer moreover encounter retrain structure sufficient model BLS establish network input transfer mapped feature feature node structure expand enhancement node incremental algorithm developed remodel expansion without retrain network deems expand incremental algorithm increment feature node filter structure increment enhancement node model algorithm versatile model rapidly addition another incremental developed model encounter incoming input specifically remodel incremental without entire retrain satisfactory model reduction singular decomposition conduct simplify structure exist neural network experimental modify national institute standard technology database nyu NORB recognition dataset benchmark data demonstrate effectiveness propose BLS introduction structure neural network learning apply achieve breakthrough application particularly data processing popular network belief network DBN boltzmann machine dbm convolutional neural network cnn although structure powerful network suffer consume training hyperparameters complicate structure involve moreover complication analyze structure theoretically span parameter stack layer accuracy achieve mission powerful compute resource involve recently variation hierarchical structure ensemble propose improve training performance layer feedforward neural network SLFN widely apply classification regression universal approximation capability conventional training SLFN gradient descent algorithm generalization performance sensitive parameter setting rate similarly usually suffer convergence trap local minimum random vector functional link neural network RVFLNN propose RVFLNN effectively eliminates drawback training generalization capability function approximation proven RVFLNN universal approximation continuous function compact therefore RVFLNN employ diverse domain context model although RVFLNN enhances performance perception significantly technique remodel volume variety data data era model moderate data dynamic wise update algorithm propose update output RVFLNN enhancement node pave remodel model encounter incoming data nowadays addition growth data data dimension increase tremendously raw data dimension directly neural network cannot sustain accessibility anymore challenge dimensional data becomes imperative recently alleviate dimension reduction feature extraction feature extraction seek optimal transformation input data feature vector approach advantage easy implementation outstanding efficiency feature selection variable rank feature subset selection penalize random feature extraction nonadaptive random projection random convolution input mapping likewise feature extraction RVFLNN mapped feature input propose BLS RVFLNN addition BLS effectively efficiently update relearn incrementally deems BLS mapped feature generate input data feature node mapped feature enhance enhancement node randomly generate connection mapped feature enhancement node fed output ridge regression pseudoinverse desire connection algorithm network expansion feature node enhancement node incremental algorithm developed remodel expansion without retrain network deems expand model consist redundancy due expansion simplify rank approximation rank approximation establish scientific compute address linear multilinear algebra intractable classical technique comprehensive exposition theory algorithm application structure rank approximation various algorithm singular decomposition svd nonnegative matrix factorization NMF widely exploratory data analysis embed classical rank algorithm propose network svd structure simplify algorithm simplification compress additional feature node insert additional enhancement node insert compress structure algorithm versatile equivalent neural node structure approach excellent approach model selection organize II preliminary RVFLNN ridge regression approach pseudoinverse sparse autoencoder svd BLS detail propose algorithm IV performance modify national institute standard technology database mnist classification nyu recognition benchmark NORB classification various performance analysis proposal algorithm address finally discussion conclusion II preliminary     classical mathematic discussion advantage functional link network training generalization feedforward network capability universal approximation RVFLNN clearly hence omit illustration  characteristic functional link network propose introduce rapid dynamic feature functional link network developed chen wan chen ridge regression approximation algorithm  recall sparse autoencoder svd briefly functional link neural network functional link network redrawn dynamic stepwise update algorithm functional link neural network network classification task refer denote matrix  expand input matrix consist input vector combine enhancement component functional link network model illustrate dynamic version model propose update instantly enhancement node classic model model easy update generally model inspire pseudoinverse partition matrix described denote matrix introduce stepwise update algorithm enhancement node network equivalent input matrix denote pseudoinverse  SourceRight click MathML additional feature  DTA nif SourceRight click MathML additional feature illustration stepwise update algorithm illustration stepwise update algorithm  SourceRight click MathML additional feature enhancement node respectively update easily compute pseudoinverse correspond node rank update pseudoinverse pseudoinverse ridge regression algorithm  network pseudoinverse convenient approach output layer neural network calculate generalize inverse orthogonal projection  iterative svd however expensive training sample input suffer volume velocity variety pseudoinverse estimator linear equation aim output training error generalization error ill optimal alternative pseudoinverse argminw AW SourceRight click MathML additional feature typically norm regularization optimal regular norm regularization convex generalization performance denotes constraint sum equivalent ridge regression theory approximation moore penrose generalize inverse positive diagonal ata aat theoretically inverse degenerate  heavily constrain tends consequently aat  SourceRight click MathML additional feature specifically  aat SourceRight click MathML additional feature sparse autoencoder supervise task classification usually feature representation input achieve outstanding performance feature representation efficient data representation importantly capture characteristic data usually intractable mathematic derivation easy random initialization generate random feature however randomness suffers unpredictability guidance overcome randomness sparse autoencoder regard important slightly tune random feature sparse compact feature specifically sparse feature model attractive explore essential characterization extract sparse feature training data optimization equivalent optimization argminw ZW SourceRight click MathML additional feature sparse autoencoder desire output linear equation XW denote lasso convex consequently approximation dozen orthogonal pursuit svd alternate direction multiplier ADMM iterative shrinkage thresholding algorithm FISTA ADMM actually decomposition decentralize algorithm optimization moreover algorithm norm involve derive ADMM FISTA hence brief review typical approach lasso equivalently argminw SourceRight click MathML additional feature ADMM rewrite argminw SourceRight click MathML additional feature therefore proximal iterative    SourceRight click MathML additional feature  operator define SourceRight click MathML additional feature singular decomposition highlight linear algebra matrix factor UÎ£VT SourceRight click MathML additional feature orthogonal matrix eigenvectors aat orthogonal matrix eigenvectors ata diagonal matrix diag SourceRight click MathML additional feature rank moreover eigenvalue ata singular therefore achieve decomposition matrix effective numerical analysis analyze matrix algorithm reduce matrix involve threshold parameter component associate eigenvalue fix singularity define threshold various requirement svd technique advantage feature selection detail propose BLS model construction suitable expansion introduce incremental dynamic expansion model characteristic model simplification svd model propose BLS construct traditional RVFLNN however unlike traditional RVFLNN input directly establishes enhancement node input construct mapped feature addition develop incremental algorithm update dynamically assume input data project data   become mapped feature wei random dimension denote concatenation mapping feature similarly enhancement node   denote concatenation enhancement node denote differently upon complexity model task furthermore function similarly function without loss generality subscript random mapping random mapping omit BLS advantage sparse autoencoder characteristic apply linear inverse tune initial wei obtain feature detail algorithm assume input data equips sample dimension output matrix belongs RN feature mapping mapping generates node equation   SourceRight click MathML additional feature wei  randomly generate denote feature node denote enhancement node   SourceRight click MathML additional feature hence model equation    SourceRight click MathML additional feature structure easily compute ridge regression approximation network illustration BLS BLS BLS alternative enhancement node establishment illustration BLS BLS BLS alternative enhancement node establishment alternative enhancement node establishment previous expansion enhancement node synchronously connection mapped feature construction mapped feature enhancement node detail described input data mapped feature enhancement construction       SourceRight click MathML additional feature dimensional mapping feature achieve  model structure illustrate obvious difference construction connection enhancement node theorem prof connection enhancement node actually equivalent theorem model feature dimension dimension respectively model feature dimension dimension normalize network exactly equivalent consequently establishment enhancement node adapt network essentially feature node enhancement node hereby model proof proof suppose wei    randomly drawn independently distribution model treat feature mapping RN separately enhancement node denote RN respectively model treat feature mapping RN separately enhancement node denote RN obviously dimension exactly equivalent entrance matrix generate distribution enhancement node therefore equivalent sample chosen data denote  RN   hence enhancement node associate sample XW                   SourceRight click MathML additional feature expectation distribution dimension random vector drawn distribution density scaler sample similarly  RN  deduce model XW      Î²bj           SourceSince drawn distribution expectation composite distribution obviously hence SourceRight click MathML additional feature therefore conclude assumption equivalent normalization operator apply incremental expansion increment additional enhancement node cannot desire accuracy insert additional enhancement node achieve performance detail expansion enhancement node denote   SourceRight click MathML additional feature    bias mapped feature additional enhancement node randomly generate discussion II deduce pseudoinverse matrix  sourcewhere   BT  BT sourceand   amd  SourceRight click MathML additional feature construction model procedure algorithm meanwhile structure illustrate  involve matrix calculate regularization approach specifically algorithm compute pseudoinverse additional enhancement node instead computation entire incremental increment additional enhancement node algorithm increment additional enhancement node incremental expansion increment feature mapping node various application feature mapping dynamic increment enhancement node insufficient feature mapping node extract underlie variation factor define structure input data popular structure network exist model task increase filter increase layer procedure suffer tedious reset parameter structure instead propose BLS increment feature mapping structure easily construct incremental apply without retrain network incremental newly incremental feature node assume initial structure consists feature mapping node enhancement node feature mapping node denote   SourceRight click MathML additional feature correspond enhancement node randomly generate      SourceRight click MathML additional feature   randomly generate denote amn amn  upgrade mapped feature correspond enhancement node relatively upgraded pseudoinverse matrix achieve amn amn  SourceRight click MathML additional feature amn  BT   amn sourceand      source specifically algorithm compute pseudoinverse additional mapped feature instead compute entire amn incremental incremental algorithm increment feature mapping algorithm incremental network additional feature mapping enhancement node increment mapped feature algorithm increment mapped feature algorithm increment mapped feature incremental increment input data input training sample model input correspond output enters model model update reflect additional sample algorithm update easily without entire training cycle denote input neural network denote amn feature mapping node enhancement node initial network respectively increment mapped feature node enhancement node formulate       SourceRight click MathML additional feature     incremental feature update wei    randomly generate initial network hence update matrix   sourcethe associate pseudoinverse update algorithm deduce  amn BDT SourceRight click MathML additional feature DT  BT  amn  SourceRight click MathML additional feature atx  therefore update     sourcewhere  label additional similarly input node update algorithm algorithm network illustration checked incremental compute pseudoinverse scheme perfect incremental incoming input data increment input data algorithm increment feature mapping node enhancement node input algorithm increment feature mapping node enhancement node input remark framework propose BLS selection function feature mapping deserves attention theoretically function explicit restriction choice kernel mapping nonlinear transformation convolutional function acceptable specifically function feature mapping convolutional function network structure classical cnn structure network additional link convolutional layer output layer furthermore additional connection laterally stack feature node explore remark propose BLS construct characteristic  functional link network  functional extension incremental algorithm various network vector machine rbf pseudoinverse computation replace iterative algorithm desire gradient descent approach enhancement node desire structure simplification rank expansion mapped feature enhancement node via incremental structure risk redundant due initialization redundancy input data generally structure simplify series rank approximation adapt classical svd conservative choice structure simplification propose model simplification generation mapped feature generation enhancement node completion svd simplification mapping feature random initial network feature node equation xwe   SourceRight click MathML additional feature similarly previous denote yield  SourceRight click MathML additional feature explore characteristic matrix apply svd  uzi         sourcewhere   singularity parameter motivation achieve satisfactory reduction node compress principal portion  equation  derive     uzi       SourceRight click MathML additional feature model define sourcewe  ZW        FW sourcewhere   SourceRight click MathML additional feature finally linear equation model refine FW SourceRight click MathML additional feature SourceHere pseudoinverse simplify svd simplification enhancement node simplify structure enhancement node network suppose feature mapping node enhancement node network FW sourcewhere     SourceRight click MathML additional feature     SourceRight click MathML additional feature equation  obtain            SourceSimilarly simplify structure obtain substitute  svd simplification insert additional enhancement node without loss generality assumption deduce svd simplification insert additional enhancement node sourcewhere     similarly svd implement previous  SourceRight click MathML additional feature update pseudoinverse conclude  sourcewhere  BT   sourceand  FD  SourceRight click MathML additional feature model FW source svd simplification completion network built however simplify singular component therefore  UF       FP  SourceRight click MathML additional feature algorithm AF  SourceRight click MathML additional feature approximation matrix  sourcewhere WF AF  structure simplify algorithm algorithm generally neural node significantly reduce threshold simplify feature mapping node enhancement node structure respectively algorithm svd structure simplification algorithm svd structure simplification IV discussion experimental verify propose confirm effectiveness propose classification apply popular mnist NORB data effectiveness BLS classification ability exist mainstream stack auto encoders sae another version stack autoencoder sda DBN multilayer perceptron mlp dbm extremely machine elm multilayer structure denote  helm respectively matlab software platform laptop equips intel ghz cpu GB memory classification cite furthermore extend fuzzy restrict boltzmann machine FRBM reasonable solid theoretical foundation establish  layer FRBM propose ghz intel cpu processor PC matlab platform duplicate sever computer equips ghz intel xeon cpu processor accuracy training superscript cnn achieve extremely imagenet challenge however generally structure ensemble various operation comparison cnn lenet comparison propose BLS linear feature mapping related cnn server computer theano platform generally mention helm  structure hyperparameters tune propagation initial rate decay rate epoch elm network regularization parameter  respectively penalty parameter helm detailed parameter checked propose BLS regularization parameter ridge regression meanwhile layer linear feature mapping tune emphasize feature adopt addition associate parameter wei  drawn standard uniform distribution interval enhancement node sigmoid function chosen establish BLS experimental propose algorithm mnist data series focus classical mnist handwritten digital image data consists handwritten digit partition training sample sample digit image pixel typically image mnist data accuracy efficiency propose structure classification priori knowledge feature node enhancement node however exactly normal building network neural network challenge task network construct feature node enhancement node reference structure sae DBN dbm  helm respectively accuracy mention propose although performance sae mlp training server surprising server moreover feature mapping node accord scholar intuition information practical application usually redundancy mapped feature II classification accuracy mnist data classification accuracy mnist data II classification accuracy mnist data enhancement node II classification accuracy mnist data enhancement node effectiveness incremental associate implement sever computer mention structure consists feature node enhancement node initial network incremental suppose initial network feature node enhancement node algorithm apply dynamically increase enhancement node dynamic increment increase dynamically feature node correspond enhancement node additional enhancement node scenario network feature node enhancement node algorithm feature node dynamically increase update correspond enhancement node additional feature increase additional enhancement node increase equivalently feature node enhancement node insert network performance dynamic construction mnist classification incremental version performance shot construction surprising dynamic increment feature node enhancement node perform randomness feature node enhancement node implies dynamic update model incremental compatible meanwhile opportunity adjust structure accuracy desire performance classification accuracy mnist data incremental classification accuracy mnist data incremental additional elapse incremental initial network feature node enhancement node similarly previous feature node dynamically increase update correspond enhancement node additional feature increase additional enhancement node increase training update IV competitive shot network feature node enhancement node proven incremental algorithm effective IV snapshot mnist classification incremental IV snapshot mnist classification incremental finally incremental algorithm input suppose initial network training sample mnist data incremental algorithm apply dynamically input training sample fed structure BLS feature node enhancement node snapshot update increment input enhancement node network feature node enhancement node additional enhancement node increase dynamically additional input input increase attractive update checked VI snapshot mnist classification incremental increment input snapshot mnist classification incremental increment input VI snapshot mnist classification incremental increment input enhancement node VI snapshot mnist classification incremental increment input enhancement node NORB data NORB data complicate data mnist data image pixel NORB contains image toy belonging distinct category airplane sample image various elevation azimuth training contains stereo image per contains image remain network construct shot model consists feature node enhancement node complex structure DBN helm respectively propose BLS faster training vii pleasant performance training propose mnist although accuracy performance previous server  computation exist propose network attractive vii classification accuracy NORB data training training svd structure simplification simulation svd simplify structure model construct mnist data threshold simplification feature node enhancement node generation important principle component simplify network apply svd operation network compression svd algorithm network compression svd algorithm denotes network structure BLS feature node enhancement node specifically summation node network  svd operation apply network network compress desire node restrict boltzmann machine RBM BLS parameter RBM refer rate decay minimal error  average error ate percentage obviously node exceeds BLS model RBM moreover model svd significantly improve accuracy specifically RBM trap around accuracy node network addition propose svd varies narrow RBM performance analysis BLS obviously outperforms exist structure neural network training furthermore mlp training BLS promising performance classification accuracy training epoch iteration performance computer structure BLS easily construct regular PC addition mention IV incremental version lose accuracy classification mnist furthermore structure simplify apply series rank approximation classical svd layer RBM propose svd stable structure reduction rank algorithm developed svd efficient conclusion BLS propose aim alternative structure establishment RVFLNN model expand fashion feature node enhancement node correspond incremental algorithm incremental learning developed remodel expansion without retrain network deems expand incremental experimental IV incremental learning rapidly update remodel shot structure increment version structure however incremental approach remodel model selection model volume mnist NORB data confirm dynamic update propose BLS finally svd approach apply simplify structure simplify network demonstrate promising lastly arrangement feature node propose algorithm incremental learning apply network network compute layer elm