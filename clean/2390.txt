symbolic regression powerful technique discover analytic equation data explainable model ability predict unseen data contrast neural network achieve amaze accuracy image recognition processing task model interpret typically extrapolate poorly article neural network architecture symbolic regression equation learner EQL network integrate architecture backpropagation demonstrate performance substantially task neural network perform symbolic regression function mnist arithmetic task convolutional network extract digit finally demonstrate prediction dynamical unknown parameter extract encoder EQL architecture extrapolate outside training data standard neural network architecture pave apply scientific exploration discovery introduction complex phenomenon engineering reduce model described relatively mathematical equation classical  described maxwell equation  quantum mechanic described  equation model elucidate underlie dynamic prediction machine technique become increasingly powerful task image recognition processing neural network architecture technique model scientific exploration machine widely apply interpretable model extract meaningful information complex data extrapolate outside training data symbolic regression regression analysis mathematical expression model data wider data model linear regression assume mathematical expression correctly describes underlie model data easy interpret extrapolate model neural network symbolic regression typically technique genetic program structure mathematical expression evolutionary algorithm data approach extract underlie physical experimental data however due combinatorial genetic program prone overfitting alternative approach underlie data explore sparsity combine regression technique numerically evaluate derivative partial differential equation PDEs dynamical significant neural network architecture interpretable inductive bias suitable scientific exploration neural network unique activation function correspond function engineering mathematical expression data architecture pde net propose predict dynamic spatiotemporal interpretable differential operator constrain convolutional filter propose neural network module neural arithmetic logic NALU introduces inductive bias arithmetic operation architecture extrapolate specific task neural network architecture extract relevant interpretable parameter dynamical parameter predict propagation addition symbolic regression module discover kinematic equation parameter extract video regard various neural network architecture symbolic regression integrate architecture advantage powerful technique interpretable generalizable symbolic regression backpropagation entire without multiple source code publicly available II EQL architecture symbolic regression neural network equation learner EQL network propose EQL network fully neural network layer neural network described wihi SourceRight click MathML additional feature matrix layer input data layer activation function network hidden layer output network described WL SourceRight click MathML additional feature EQL network symbolic regression neural network activation function identity sine multiplication hidden layer visual simplicity network function hidden layer broader function EQL network symbolic regression neural network activation function identity sine multiplication hidden layer visual simplicity network function hidden layer broader function activation function choice neural network relu tanh consist function component sine function function argument output multiplication function    SourceRight click MathML additional feature additive bias absorbed convenience activation function analogous primitive function symbolic regression function argument allows multiplicative operation inside network schematic activation function hidden layer visual simplicity function exp sigmoid addition activation function duplicate within layer multiple component activation function reduces sensitivity random initialization creates smoother optimization landscape network stuck local minimum easily allows EQL network function detail appendix stack multiple layer EQL architecture complex combination composition variety primitive function analogous maximum depth genetic program approach upper limit complexity expression model conventional symbolic regression powerful function typically engineering importantly EQL network backpropagation integrate neural network model training sparsity ingredient symbolic regression interpretable enforce sparsity regularization simplest equation data goal sparsity parameter parameter inactive remove expression enforce sparsity neural network active research architecture parameter become computationally prohibitive evaluate recent development neural network sparsity technique straightforward popular enforce sparsity loss function regularization function neural network matrix SourceRight click MathML additional feature index layer elementwise matrix  SourceRight click MathML additional feature index matrix regularization penalizes nonzero regardless magnitude sparsity however regularization equivalent combinatorics NP compatible gradient descent commonly optimize neural network recent explore training sparse neural network relaxed version regularization stochastic gate variable regularization compatible backpropagation popular sparsity technique regularization EQL network although sparsity strongly regularization regularization convex optimization optimization technique gradient descent however penalizes magnitude propose enforce sparsity strongly without penalize magnitude regularization compatible gradient descent although longer convex apply neural network experimental regularization performs regularizers imply optimal enforce sparsity regularizers significant overall improvement regularizer agreement addition sparsity strongly simpler expression however regularization singularity gradient training gradient descent avoid smooth version propose label regularizer piecewise function smooth function magnitude SourceRight click MathML additional feature transition standard function smooth function plot regularization smooth regularization avoids extreme gradient improve training convergence EQL network integrate architecture regularization apply EQL network regularization described respectively threshold plot easy visualization threshold regularization described respectively threshold plot easy visualization threshold implement EQL network relaxed regularization propose detail appendix symbolic regression analytic expression validate EQL network ability perform symbolic regression EQL network data generate analytic expression exp sin data generate domain network sensitivity random initialization trial function network converge ignore slight variation coefficient addition equivalent sin instead sin appendix network reasonable trial construct desire equation trial combination equation simplicity extrapolation ability extrapolation ability equation error evaluate domain extrapolation error equation tends magnitude equation network network data EQL network hidden layer expression complicate expression mnist arithmetic demonstrate ability combine symbolic regression image recognition arithmetic task mnist digit mnist popular data image recognition  grayscale image handwritten digit integer label arithmetic function correspond image input learns image architecture input consists mnist digit training randomly drawn mnist training data fed separately encoder dimensional latent variable constrain alternatively architecture encoder digit encoders encoder consists convolutional layer max pool layer fully layer batch normalization layer output detail encoder appendix latent variable EQL network EQL network scalar output label schematic mnist addition architecture encoder consist convolutional layer fully layer operates mnist image extract dimensional latent variable encoders latent variable fed EQL network entire fed without pretraining schematic mnist addition architecture encoder consist convolutional layer fully layer operates mnist image extract dimensional latent variable encoders latent variable fed EQL network entire fed without pretraining entire network error mse loss predict label label encoder separately EQL network encoder closely resembles convolutional neural network classify mnist digit output scalar instead logits encode digit constraint label dynamical analysis apply EQL network analyze physical potentially powerful application exploration discovery discover parameter dynamical unsupervised parameter predict propagation multilayer perceptrons extract relevant bounce constant simultaneously predict trajectory accomplish goal dynamic encoder DE convolutional layer propagate decoder PD deconvolutional layer enable analysis prediction spatiotemporal PDEs DE PD architecture analyze spatiotemporal uncontrolled dynamical parameter varies instance data diffusion constant diffusion equation parameter encode latent variable fed PD along initial PD propagates extract physical parameter dynamic architecture DE PD architecture DE input series txt output dimensional latent variable unlike DE PD architecture DE VAE DE consists convolutional layer fully layer batch normalization layer detail appendix parameter initial fed PD predicts future  dynamic PD consists EQL recurrent structure recurrent structure predicts EQL consists EQL network feature dimension architecture equation propagate dynamical EQL PD consists EQL network dimension mathbf predict mathbf velocity EQL network EQL architecture equation propagate dynamical EQL PD consists EQL network dimension predict velocity EQL network EQL architecture mse loss predict dynamic  target series  architecture DE PD separately restriction bias latent variable explore physical kinematics harmonic oscillator SHO described kinematics kinematics describes physic apply schematic physical scenario described kinematics  apply direction parallel relevant parameter reduce constant velocity    SourceRight click MathML additional feature kinematics describes dynamic apply SHO describes constant displacement velocity kinematics describes dynamic apply SHO describes constant displacement velocity acceleration varies across series data simulated data initial acceleration uniform distribution SourceRight click MathML additional feature initial parameter fed PD correlate harmonic oscillator physical analyze SHO ubiquitous model physic physical pendulum quantum potential circuit dynamic SHO couple ordinary differential equation ode   SourceRight click MathML additional feature velocity resonant frequency constant SHO numerically finite difference approximation derivative euler integrate ODEs   SourceRight click MathML additional feature generate data parameter drawn uniform distribution SourceRight click MathML additional feature variable finite difference propagate equation otherwise learns identity function avoid recurrent architecture prediction explode training propagate training strategy restart training initial parameter fed PD correlate training neural network implement tensorflow network backpropagation RMSProp optimizer loss function SourceRight click MathML additional feature training data hyperparameter balance regularization versus mse introduce  training schedule optional phase network apart EQL evolve freely extract latent parameter training phase increase EQL network become sparse phase EQL network threshold frozen equivalent fix norm phase training training without regularization reduce maximum rate tune specific detail appendix IV mnist arithmetic latent variable versus label digit entire network digit drawn mnist training data evaluate network performance digit drawn mnist data confirm encoder generalizability linear correlation data encoder successfully linear relation despite access digit label factor due lack constraint linear regression relation SourceRight click MathML additional feature ability encoder differentiate digit latent variable versus digit psi digit chi drawn mnist training data data correlation coefficient respectively ability entire architecture label predict sum versus sum digit chi drawn mnist training data data ability encoder differentiate digit latent variable versus digit digit drawn mnist training data data correlation coefficient respectively ability entire architecture label predict sum versus sum digit drawn mnist training data data extract equation EQL network encoder equation encoder conclude EQL network successfully extract additive function predict sum versus sum absolute error prediction model digit mnist training data respectively mnist arithmetic extract equation mnist arithmetic extract equation architecture regression mse loss report accuracy classification task label integer calculate accuracy predict sum integer label achieves accuracy digit drawn mnist training data respectively demonstrate generalization architecture data outside training data scheme mnist digit randomly sample mnist training data training data otherwise discard phase mnist digit randomly sample mnist training data evaluation data otherwise discard comparison generalization encoder abovementioned procedure mnist digit mnist data generalization network II EQL network equation II mnist arithmetic generalization II mnist arithmetic generalization difference accuracy evaluate architecture EQL network accuracy percentage however architecture EQL network replace commonly fully network relu activation function label relu accuracy EQL generalize reasonably regime relu cannot generalize encoder digit accuracy slightly digit drawn mnist data versus digit drawn mnist training data optimize hyperparameters digit extraction network accuracy architecture optimize finally accuracy slightly EQL versus relu network unsurprising symmetric activation function relu network constrain network EQL optimization landscape smoother kinematics extract latent parameter plot function parameter linear correlation correlation coefficient DE extract relevant parameter relation extract linear regression SourceRight click MathML additional feature latent parameter dynamic encoder architecture training plot function parameter linear correlation predict propagation mathbf EQL conventional network relu activation refers propagation mathbf latent parameter dynamic encoder architecture training plot function parameter linear correlation predict propagation EQL conventional network relu activation refers propagation equation EQL training DE equation equation extract relation EQL equation closely kinematics extract equation kinematics extract equation predict propagation plot EQL propagate EQL network comparison neural network architecture EQL replace standard fully neural network hidden layer neuron relu activation function label relu network closely training regime dot relu network quickly diverges outside training regime EQL reasonably extrapolate beyond training data SHO plot latent variable function parameter training SHO linear correlation oppose reflect operation propagate equation latter function addition simplest parameter due sparsity regularization EQL linear correlation correlation coefficient DE successfully extract relevant parameter SHO linear regression relation SourceRight click MathML additional feature training SHO latent parameter dynamic encoder architecture training plot function parameter linear correlation velocity function various model refers analytical EQL refers propagation equation discover EQL network relu refers propagation conventional neural network relu activation function euler refers finite difference euler equation extract EQL IV DE equation DE immediately expression closely euler approximation latent variable relation extract DE IV SHO extract equation IV SHO extract equation normally approximation euler integrate ODEs  SourceRight click MathML additional feature expand approximation expand euler approximation     SourceRight click MathML additional feature equation DE assume expansion label DE IV EQL network euler finite difference another corresponds taylor expansion EQL network likely inside network lack convergence exactly likely disappear another thresholding propagate relu propagate architecture EQL network replace conventional neural network hidden layer relu activation function additional comparison calculate finite difference euler integrate ODEs label euler within training regime reasonably however conventional neural network relu activation function completely fails extrapolate beyond training regime euler EQL network extrapolate reasonably beyond training regime although diverge due accumulate error numerical integration accurate   almost exactly analytical surprising due error bound however complex euler likely EQL network expression   interestingly EQL network error euler demonstrate EQL network correction euler possibly discovery efficient integration scheme differential equation finite difference conclusion integrate symbolic regression architecture entire advantage powerful training technique developed recent namely arithmetic mnist digit identify image image recognition task simultaneously extract mathematical expression relates digit addition simultaneously extract unknown parameter dynamical extract propagation equation SHO discover technique integrate ODEs potentially pave improve integrator integrator stiff ODEs numerical direction future role random initialization sensitive random initialization benchmark EQL network appendix EQL network mathematical expression local minimum EQL network network stuck gradient optimization guaranteed local minimum global minimum local minimum typically concern neural network local minimum typically performance global minimum however EQL network global minimum alleviate issue increase stochasticity rate decrease sensitivity random initialization duplicate activation function addition multiple trial manually automate future global minimum without resort multiple trial  neural network activation function global minimum gradient descent linear regardless random initialization direction future expand architecture EQL network integrate spatiotemporal pde discovery spatial derivative calculate finite difference approximation learnable kernel another extension introduce parametric dependence unknown parameter dependence pde discovery sparsity addition encoder expand capture wider variety data video audio signal text capability apply scientific exploration discovery appendix EQL network detail activation function hidden layer consist sin sigmoid SourceRight click MathML additional feature sigmoid function define sigmoid SourceRight click MathML additional feature activation function duplicate sin sigmoid function multiplier inside function accurately respective inside input domain unless otherwise activation function duplication arbitrary significant impact performance future duplication phase training phase rate regularization iteration frozen phase phase training rate iteration benchmark symbolic regression trial function architecture feasibly construct network trial benchmarking mention network equation construct automatically equation simplicity error benchmark EQL network benchmark EQL network computational efficiency respect task symbolic regression algorithm asymptotic speedup conventional symbolic regression algorithm expression combinatorial expression NP advantage symbolic regression gradient descent integrate symbolic regression architecture nvidia gtx training EQL network hidden layer epoch training EQL network hidden layer computational complexity EQL network conventional fully neural network difference activation function apply iterate node layer however computational complexity neural network dominate matrix multiplication EQL network conventional fully neural network appendix relaxed regularization implement EQL network relaxed regularization neural network introduce briefly review detail refer reader detail neural network  SourceRight click MathML additional feature ideally binary gate however differentiable stochastic variable drawn concrete distribution sigmoid logu  min max SourceRight click MathML additional feature trainable variable describes location concrete distribution hyperparameters distribution binary gate regularization penalty simply sum nonzero however concrete distribution calculate analytical expectation regularization penalty distribution parameter loss function    SourceRight click MathML additional feature advantage regularization enforces sparsity without penalty magnitude allows without stage frozen reparameterization trainable parameter neural network regularization apply EQL network architecture hyperparameters regularization although optimize future addition apply sparsity matrix goal computational efficiency apply parameter sparsity goal simplify symbolic expression benchmark EQL network regularization aforementioned trial function rate network regularization trial function picked integrate EQL network regularization mnist arithmetic kinematics architecture EQL network regularization appendix detail mnist arithmetic encoder network consists convolutional layer filter max pool layer another convolutional layer filter max pool layer fully layer relu activation max pool layer pool stride fully layer layer batch normalization output batch normalization layer standard deviation output decrease input EQL network EQL network construct assume input domain addition output EQL network fed loss function normalize output equivalent normalize relu network EQL network comparison consists hidden layer relu activation phase training phase rate regularization phase rate regularization frozen phase threshold phase iteration kinematics generate kinematics data sample generate series  input series propagate DE consists convolutional layer filter layer hidden layer node relu activation function output layer batch normalization layer standard deviation relu network EQL network comparison mnist task phase training phase rate regularization iteration iteration remainder training frozen phase threshold phase rate regularization iteration SHO generate SHO data sample data generate series  input series propagate output series propagate DE architecture kinematics due propagate EQL network duplicate activation function function function hidden layer consist sin SourceRight click MathML additional feature relu network EQL network comparison consists hidden layer relu activation function phase training phase rate regularization iteration training iteration phase training increase decrease rate increase regularization frozen phase threshold phase training rate regularization appendix additional mnist arithmetic data drawn trial trial network random initialization network due random initialization EQL equation additional trial demonstrate variability behavior robustness random initialization experimental detail described digit drawn entire mnist training data refer trial trial trial trial linear relationship digit latent variable although positive instead negative correlation previously mention bias latent variable positive negative correlation arbitrary depends random initialization architecture expression EQL network SourceRight click MathML additional feature positive coefficient reflect positive correlation network accurately predict sum ability encoder differentiate digit latent variable versus digit psi digit chi drawn mnist training data data ability entire architecture label predict sum versus sum digit chi drawn mnist training data mnist data ability encoder differentiate digit latent variable versus digit digit drawn mnist training data data ability entire architecture label predict sum versus sum digit drawn mnist training data mnist data trial relationship longer linear however encoder mapping EQL network extract information predict sum ability encoder differentiate digit latent variable versus digit psi digit chi drawn mnist training data data ability entire architecture label predict sum versus sum digit chi drawn mnist training data mnist data ability encoder differentiate digit latent variable versus digit digit drawn mnist training data data ability entire architecture label predict sum versus sum digit drawn mnist training data mnist data equation EQL network sin sin SourceRight click MathML additional feature consistent insight curve inverse sine function invert transformation linear mapping digit EQL network equation gain insight analyze latent variable equation