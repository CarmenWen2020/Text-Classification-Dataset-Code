federate obtains central model server aggregate model locally client federate client upload data server thereby preserve data privacy client challenge federate reduce client server communication device typically limited communication bandwidth article enhance federate technique propose asynchronous strategy client temporally aggregation local model server asynchronous strategy layer neural network dnns categorize shallow layer parameter layer update frequently shallow layer furthermore temporally aggregation strategy introduce server previously local model thereby enhance accuracy convergence central model propose algorithm empirically data dnns demonstrate propose asynchronous federate outperforms baseline algorithm communication model accuracy introduction smartphones wearable gadget distribute wireless sensor usually generate volume privacy sensitive data service provider interested mining information data personalize service relevant recommendation client however client usually willing service provider access data privacy federate recently propose privacy preserve machine framework local model client model parameter server aggregate local model server local model upon data locally client data privacy preserve typical federate communication local model client local data client refers participate subset client training sample denote local model model parameter vector communication model client belonging subset parameter central model server initial local model local training participate client update parameter server consequently central model update aggregate update local model agg local model client machine model chosen accord task accomplish exist federate neural network dnns memory lstm employ conduct text text prediction task recent dnns successfully apply complex text classification image classification recognition therefore dnns widely adopt local model federate stochastic gradient descent sgd popular algorithm training local model aforementioned communication parameter client local training client parameter upload client model aggregation server framework distribute machine algorithm federate however model parameter uploaded client server data local client uploaded server exchange client accordingly data privacy client preserve machine paradigm federate challenge unbalanced data data amount client highly imbalanced user non iid data data client strongly non independent identically distribute iid preference user local data overall data distribution local distribution iid assumption distribute training data distribute local client uniformly random usually federate massively distribute data client client mobile phone user enormous unreliable participate client portion participate client offline unreliable connection client mobile phone user communication cannot ensure participation apart mention challenge communication overall performance indicator federate due limited bandwidth battery capacity mobile phone algorithm accuracy mainly local training aggregation strategy importance accordingly motivation article reduce communication improve accuracy central model assume dnns local model inspire observation tune dnns layerwise asynchronous update strategy local model update aggregation propose improve communication efficiency evolutionary optimization evolutionary algorithm adopt asynchronous strategy enhance computational efficiency parallel asynchronous particle swarm optimization  concretely motivation layerwise asynchronous strategy adopt  communication efficiency versus computational efficiency essential difference driven aforementioned motivation propose algorithm address parameter shallow layer asynchronous manner  update particle velocity parallel asynchronously contribution article asynchronous strategy aggregate update parameter shallow layer dnns frequency propose reduce parameter communicate server client temporally aggregation strategy efficiently integrate information previously local model model aggregation enhance performance remainder article organize II related briefly review detail propose algorithm asynchronous strategy temporally aggregation overall framework described IV experimental discussion finally conclusion drawn II related  developed framework federate experimentally exist machine algorithm  propose reduce uplink communication structure update sketch update data compression reconstruction technique recent version federate federate average FedAVG report developed obtain central prediction model google  app embed mobile phone user privacy pseudocode FedAVG algorithm algorithm FedAVG function  server initialize max random client client parallel    function function  client split batch local epoch batch return server function briefly explain component FedAVG server execution consists initialization communication initialization initializes parameter communication obtains participate client indicates local client corresponds participate client per accord randomly selects participate subset   parallel  executes aggregation update client update  local mini batch local epoch respectively rate split data batch corresponds index data client execute local sgd batch return local parameter equation  described aggregation strategy sample client training sample  client however update communication client participate model remain unchanged chosen participate model aggregation parameter uploaded client previous contribute equally central model FedAVG parameter shallow layer synchronously update parameter uploaded client participate previous apart reduce communication federate focus protocol security tackle differential attack propose algorithm incorporate preserve mechanism federate client differential privacy efficient robust transfer protocol secure aggregation dimensional data privacy preserve reduction communication important aspect account federate concern remains enhancement performance central model data loss function federate define loss function client model clearly performance federate heavily depends model aggregation strategy report reduce communication reduce parameter uploaded client server recent report recently article layerwise asynchronous model model update model parameter reduce communication temporally aggregation strategy recent model aggregation enhance performance layerwise asynchronous model update temporally aggregation layerwise asynchronous model update intrinsic requirement decrease communication federate upload data without deteriorate performance central model article layerwise asynchronous model update strategy update local model parameter reduce amount data uploaded primarily inspire observation tune dnns shallow layer dnn feature applicable task data meaning relatively parameter dnns shallow layer feature data contrast layer dnn hoc feature related specific data parameter focus feature specific data mention observation relatively parameter shallow layer pivotal performance central model federate accordingly parameter shallow layer update frequently parameter layer therefore parameter shallow layer model update asynchronously thereby reduce amount data server client layerwise asynchronous model update dnn employ local model client shallow layer feature layer specific feature layer feature denote respectively denote respectively typically propose asynchronous strategy update uploaded frequently assume federate loop loop model update loop update communicate update communicate reduce parameter exchange server client reduce communication communication significantly reduce usually dnns illustration shallow layer dnn asynchronous strategy abscissa ordinate denote communication local client respectively local device server indicates client participate update global model parameter exchange synchronous model update strategy asynchronous model update strategy illustration conventional synchronous aggregation strategy uploaded rectangle aggregation shallow layer participate contrast propose asynchronous strategy compute loop parameter layer exchange reduce parameter communicate temporally aggregation federate aggregation strategy algorithm usually importance local model training data correspond client local client contribute central model diamond previously local model update central model orange dot recently update local model client participate aggregation participate local model orange dot data regardless compute model update local model update important update reasonable conventional aggregation strategy federate however training data participate client therefore model recently update aggregation accordingly model aggregation account timeliness local model propose   sourcewhere logarithm depict  update propose temporally aggregation client participate aggregation denote orange dot others diamond darker local model aggregation illustration temporally aggregation framework framework propose federate layerwise asynchronous model update temporally aggregation detailed description component federate layerwise asynchronous model update temporally aggregation temporally asynchronous federate pseudocode component propose temporally aggregation asynchronous ASTW federate ASTW FedAVG implement server client algorithm respectively algorithm server component ASTW FedAVG function  server initialize client   mod loop  flag flag false max random client client parallel flag  flag     flag    flag   function loop    algorithm client component ASTW FedAVG function  flag client split batch flag local epoch batch flag return server return server function implement server consists initialization communication initialization algorithm central model timestamps  timestamps initialize timestamps timeliness correspond parameter aggregation training loop algorithm flag freq loop assume loop loop randomly participate subset client accord participate client per   parallel  correspond timestamps update flag specify layer shallow layer update communicate aggregation perform update parameter introduce function examine influence weighting article implementation local model update algorithm parameter flag index client flag indicates layer shallow layer update denote local mini batch local epoch respectively algorithm split data batch whereas layer shallow layer local model accord flag local sgd perform return local parameter IV experimental analysis experimental perform popular dnn model convolution neural network cnns image processing lstm activity recognition har respectively empirically examine performance communication propose ASTW FedAVG cnn stack convolution layer fully layer apply modify national institute standard technology mnist handwritten digit recognition data lstm stack lstm layer fully layer chosen accomplish har task mnist har data adapt performance propose federate framework scenario non iid distribution unbalanced amount massively decentralize data detail IV FedAVG baseline algorithm approach propose ASTW FedAVG variant namely temporally federate  adopts temporally aggregation without layerwise asynchronous model update FedAVG employ layerwise asynchronous model update without temporally aggregation algorithm FedAVG ASTW FedAVG TW FedAVG FedAVG important parameter propose algorithm parameter freq frequency update exchange parameter layer server local client loop instance freq parameter layer uploaded server client parameter adjust model aggregation environmental parameter complexity denotes local client participate client per parameter setting setting data federate framework challenge non iid unbalanced massively decentralize data therefore data reflect challenge generation client data described detail algorithm parameter label smin smax local data smin smax specify local data label indicates involve correspond task algorithm generation local data label NC smin smax non iid unbalanced local dataset choice label NC len label zero zero  random sum   num random smin smax   num handwritten digit recognition cnn mnist data digit digit pixel image partition data local client sort digit label shard algorithm perform compute client partition coefficient correspond shard task label randomly chosen smin smax sake easy analysis partition local data namely mnist mnist mnist predefined correspond plot  data mnist mnist mnist mnist mnist architecture cnn mnist task convolution layer channel max pool layer fully layer relu activation output layer softmax parameter setting cnn II II parameter setting cnn activity recognition lstm har data data sequence image label activity operation apply har data data local client label randomly chosen smin smax architecture lstm article lstm layer fully layer relu activation softmax output layer correspond parameter lstm parameter setting lstm analysis perform subsection examines influence important parameter freq performance strategy cnn mnist data algorithm communication accuracy lstm har data har challenge mnist data parameter detailed sensitivity analysis related discussion aim understand parameter setting helpful mainly conduct research parameter investigate influence parameter others default freq freq metric adopt article performance algorithm accuracy central model within central model accuracy compute communication independently average avg standard deviation stdev IV VI IV VI average standard deviation parenthesis IV experimental freq experimental VI experimental scalability algorithm IV VI observation analysis freq IV conclude exchange frequency communication however freq deteriorate accuracy central model analysis parameter influence model aggregation recently update local model heavily aggregate model previously update local model impact central model option cnn mnist data algorithm reduce FedAVG meaning parameter uploaded importance aggregation parameter leverage scalability federate avg stdev calculate recognition accuracy cnn predefined data combination separately assess VI conclusion involve client recognition accuracy ASTW FedAVG outperforms FedAVG fourth fifth VI FedAVG slightly client implies advantage propose algorithm traditional federate become apparent client increase encourage client typically comparison accuracy communication conduct overall performance algorithm comparison default parameter local data predefined mnist har respectively instance local data task mnist mnist importance temporal demonstrate accuracy compute baseline FedAVG TW FedAVG without asynchronous update mnist har data respectively comparative temporally aggregation data mnist cnn mnist mnist mnist mnist mnist comparative temporally aggregation data har lstm har har har har har conclusion propose temporally aggregation central model converge acceptable accuracy mnist mnist data TW FedAVG communication achieve accuracy traditional FedAVG achieve accuracy reduction communication conclusion drawn data mnist mnist mnist although accuracy TW FedAVG becomes fluctuate har data task TW FedAVG mostly achieve accuracy FedAVG har notably TW FedAVG communication achieve accuracy FedAVG nearly reduction communication har TW FedAVG faster convergence FedAVG stage finally temporally aggregation fluctuation performance attribute contribution quality local model communication finally comparative algorithm generate mnist har task vii metric classification accuracy parenthesis communication observation ASTW FedAVG TW FedAVG outperform FedAVG accuracy communication TW FedAVG achieves performance task accuracy temporally aggregation strategy accelerates convergence improve performance ASTW FedAVG performs slightly TW FedAVG mnist communication TW FedAVG ASTW FedAVG har data layerwise asynchronous model update strategy significantly contributes reduce communication per FedAVG performs algorithm performance FedAVG adopt asynchronous strategy ASTW FedAVG asynchronous strategy vii performance conclusion future article aim reduce communication improve performance federate asynchronous model update strategy temporally aggregation empirical performance communication canonical federate propose federate mnist action recognition data demonstrate propose asynchronous federate temporally aggregation outperforms canonical performance communication article assumption local model adopt neural network architecture hyperparameters rate sgd future research develop federate algorithm client evolve local model improve performance reduce communication