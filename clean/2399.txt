attention increasingly popular mechanism neural architecture mechanism realize variety format however pace advance domain systematic overview attention article define unified model attention architecture processing focus vector representation textual data propose taxonomy attention model accord dimension representation input compatibility function distribution function multiplicity input output prior information exploit attention model discus ongoing research effort challenge extensive categorization vast literature domain introduction involve processing compose source text characterize relevance task instance aspect sentiment analysis cue relevant aspect consideration others machine translation source text irrelevant translation visual task background pixel irrelevant regard foreground relevant regard scenery arguably effective factor notion relevance focus computational resource restrict important approach tailor specific genre exploit regularity input feature engineering argumentative analysis persuasive essay emphasis however approach viable input information text summarization output condense version possibly lengthy text sequence another approach increase popularity amount machine relevance input neural architecture automatically weigh relevance input account perform task  mechanism attention attention introduce processing nlp machine translation task however glimpse already propose computer vision  hinton observation biological retina fixate relevant optic array resolution rapidly eccentricity visual attention become popular significantly outperform image classification task dynamic visual due architecture adaptively sequence location resolution progressively resolution pixel besides offering performance gain attention mechanism interpret behavior neural architecture notoriously understand indeed neural network  architecture therefore knowledge numeric interpretation becomes impossible pinpoint output neural architecture interestingly attention partially interpret explain neural network behavior cannot reliable explanation instance compute attention relevant information discard neural network irrelevant input source factor explain surprising output neural network therefore visual highlight attention instrumental analyze outcome neural network specific devise visualization attention visualization context aspect sentiment analysis attention visualization aspect sentiment analysis task highlight accord attention bold relevant task rationale attention become increasingly ingredient neural architecture nlp  neural architecture introduction attention mechanism significant gain grouped nlp task address spectrum task involve remarkably besides nlp computer vision attentive model successfully adopt recognition recommendation series analysis mathematical  exploit attention grouped task address nlp initial exploration seminal pace development attention model attentive architecture ensue highly diversified architectural landscape overall complexity  author independently intuition development almost identical attention model instance concept inner attention attention arguably unsurprisingly introduce author define concept ambiguity literature context vector meaning article systematic overview attention model developed nlp model attention nlp task research activity introduce taxonomy describes exist approach along dimension input representation compatibility function distribution function input output multiplicity knowledge taxonomy attention model accordingly succinct description attention model model another insight moreover regard prior information unison attention debate future attention challenge restrict analysis attentive architecture vector representation data typically nlp reader interested attention model task data graphical representation refer survey comprehensive account neural architecture nlp excellent overview neural architecture nlp attention mechanism impossible rapidly become obsolete sheer volume article feature architecture increasingly rely mechanism moreover purpose synthesis critical outlook listing research activity quantitative evaluation attention mechanism mechanism generally embed neural network architecture devise address specific task pointless attempt comparison standard specific nlp task evaluation attention model experimentation multiple neural architecture extensive hyperparameter tune validation variety benchmark however attention apply multiplicity task data meaningfully variety task empirical evaluation beyond scope article however experimental focus nlp task machine translation argumentation mining text summarization sentiment analysis worthwhile remark occasion attention approach enable dramatic development entire research development immediate performance boost transformer sequence sequence annotation bert currently popular architecture creation embeddings impact attention model pave radically approach task influence machine translation likewise expressive memory network significantly contribute network task survey structure II define model attention component machine translation architecture introduce illustration instance model elaborate attention various nlp task IV taxonomy attention model discus attention combine knowledge task data VI devote challenge trend future direction vii concludes article II attention function attention mechanism neural architecture enables dynamically highlight relevant feature input data nlp typically sequence textual apply directly raw input representation core attention compute distribution input sequence assign relevant illustrate briefly classic attention architecture  chose  historical significance simplicity respect structure machine translation alignment  attention machine translation objective compute output sequence translation input sequence architecture consists encoder decoder architecture  attention model encoder bidirectional recurrent neural network BiRNN computes annotation input BiRNN source decoder consists cascade attention function rnn attention function embed input sequence context vector subsequent rnn characterize hidden computes embed probability distribution output rnn source context vector obtain attention function input previous hidden rnn annotation input alignment model obtain scalar eti input around output around normalize softmax function obtain ati eti ati exp eti exp  source finally context vector compute sum annotation ati  source quote attention relieve encoder burden encode information source fix vector approach information throughout sequence annotation selectively retrieve decoder accordingly unified attention model characteristic attention model structure data  desire output structure unified model propose extends model propose comprises core almost totality model survey literature additional component although universally literature model core attention model model II core attention mechanism sequence vector distribution encodes data feature  attention compute instance embeddings document internal recurrent architecture happens annotation  multiple feature representation encode embed task representation entire document II notation core attention model attention model another input query reference compute attention distribution attention mechanism emphasis input relevant task accord query define attention emphasis inherently relevant task  instance namely rnn hidden architecture entity embeddings actual textual query contextual information background knowledge matrix vector document attentive reader query vector query vector compute compatibility function source function corresponds  alignment model author function transform attention distribution function source outcome core attention mechanism  distribution function softmax function  normalizes probability distribution relevance task respect computation already sufficient task classification task address nevertheless task computation representation another input sequence vector data  attention compute apply corresponds representation data indeed architecture  distinguish distinction introduce representation input compute attention distribution contextual information combine obtain representation merge compact representation usually context vector  obtain summation however alternative propose gate function nevertheless mainly associate attention   source described synthesis frequent architectural choice attentive architecture option explore IV deterministic versus probabilistic attention proceed brief remark relevant convention attention model described sometimes described literature mapping probabilistic interpretation softmax normalization allows interpret attention probability distribution function accordingly recent literature defines deterministic attention model model context  attention focus deterministically constituency parse input however author described model deterministic attention contrast stochastic attention probability distribution hardly sample input context vector reinforcement strategy encode probability distribution stochastic attention model differs model absence replace equation  dks  sourceto avoid potential confusion remainder article abstain characterize attention model deterministic probabilistic stochastic attention attention enables estimate relevance input combine compact representation context vector  characteristic relevant context vector input computational resource later stage yield computational gain summarize attention task relevant attention relevant task task document classification usually input query attention mechanism encode input compact computation embed feature selection apply feature representation applies feature domain multimodal task neural architecture simply aspect input document similarly attention exploit auxiliary task training specific feature model via multitask scenario visual domain classification understand semantic role label generation text sequence machine translation attention enables dynamic representation input sequence whereby input encode vector encode tailor accord task embed previous decoder generally possibility perform attention respect query allows representation input task context specialized embeddings particularly useful task sentiment analysis information extraction attention contextual representation sequence sequence annotator without resort rnns convolutional neural network cnns rely attention mechanism obtain encoder decoder architecture attention specific dependence parse cloze task former attention apply predict dependence latter attention apply textual document vocabulary perform classification finally attention handy multiple interact input sequence combination task input consists textual sequence instance document input encode obtain mutual interaction sequence apply rigid priori define model IV taxonomy attention model attention model described basis orthogonal dimension input IV compatibility function IV distribution function IV distinct input output refer multiplicity IV moreover attention module inside attention model obtain complex architecture hierarchical input model IV multiple input coattention model IV input representation nlp related task generally representation document sequence component usually embed continuous vector representation annotation function  vaf obtain hidden representation typical annotation function rnn layer gate recurrent grus memory network lstms cnns input relative local context layer annotation attention model encode information useful attention model alternatively input isolation context instance encode pretrained embed application attention mechanism directly raw input model inner attention model proven effective author exploit fashion architecture layer hyperparameters reduces computational resource training input sequence attention architecture operates encoding obtain subsequent application rnn layer context embeddings obtain component individually concatenate representation document encodes relevant representation component task aggregate compute average sum input sequence usually however input juxtaposition feature relevant aspect textual instance input compose source input aspect document embeddings input collate fed attention model multiple embeddings representation allows highlight relevant input feature selection reduction dimensionality representation via context embed attention mechanism interestingly propose model textual input sequence mixed output attention model truncate attention model iterates computation attention lstm lstm hidden context vector compute previous query explain detail successful structure become establish building neural approach nlp namely attention hierarchical input architecture attention distinction input source input sequence query however architecture compute attention input sequence architecture attentive  model remark however approach  amount application multiple attention vector vector query ati relevance respect yield context embeddings per attention sequence sequence model alternative cnns rnns sequence influence input incorporate contextual information without locality boundary overcome shortcoming rnns limited ability model dependence distribution emphasis strongly relate analysis distribution therefore information regard relationship inside sequence text sequence generation rely approach another possibility construct query pool operation furthermore input sequence query apply technique IV coattention attentive approach characterize absence query simplify compatibility function IV attention sequence sequence model hierarchical input architecture task portion input data meaningfully grouped structure hierarchical input attention model exploit subsequently apply multiple attention module composition hierarchical input attention model define zhao zhang inside attention indicates application highlight input instance data naturally associate semantic structure micro macro attention apply representation micro aggregate representation macro context vector attention apply sequence  embeddings compute embed document model attention highlight relevant micro within macro relevant macro document instance apply attention compute embeddings apply attention embeddings obtain document representation reference model introduce II embeddings compute embeddings compute document eventually context vector hierarchy extend instance another layer apply attention document representation micro macro available compute attention exploit query compute attention yield   attention enables identify relevant task attention via attention model zhao zhang defines hierarchy micro macro attention compute embeddings  obtain document representation context vector query application attention embeddings KC yield context vector identify target macro necessarily sequence macro KT context vector application attention mechanism KT query application attention mechanism document embeddings  compatibility function compatibility function crucial attention architecture defines query combine presentation compatibility function data model  vector document embed model structure although however architecture consist sequence vector matrix possibility explore IV compatibility function IV approach identify instance similarity attention propose relevant query accordingly author model relies similarity function sim IV compute rely cosine similarity choice query semantic representation widely multiplicative dot attention dot compute complexity computation  variation model multiplicative attention factor introduce improve performance attention propose extends concept accommodate query representation introduces learnable matrix parameter parameter transformation matrix query onto vector transformation increase complexity operation  bias attention introduce learnable bias relevant independently input activate attention employ nonlinear activation function IV placeholder nonlinear activation function hyperbolic tangent tanh rectifier linear relu exponential linear  approach particularly suitable task concept relevance closely related similarity query instance task specific keywords query abusive recognition sentiment analysis IV summary compatibility function literature learnable parameter approach amount combine compute joint representation importance vector  adhere semantic representation vector defines relevance additional query learnable parameter speculate analysis machine importance vector additional information model simplest model approach concat attention joint representation  query additive attention similarly contribution compute separately precomputed contribution reduce computational footprint complexity computation ignore application nonlinear function  indicates  moreover additive attention principle accommodate query additive attention concat attention query fed neural layer instead attention multiple layer employ IV attention function depth neuron reasonably assume parameter complexity becomes  approach suitable representation relevant unavailable available encode significantly encode instance task document classification summarization rationale convolution attention inspiration biological model whereby biological attention template vector  convolutional filter embodies specific relevance template filter obtain apply convolution operation subsequence apply multiple convolution filter enables contextual selection specific relevance template obtain specific filter belongs multiple subsequence yield multiple aggregate obtain per aggregation achieve sum average IV illustrates convolution attention filter complexity operation  approach suitable representation relevant unavailable available encode significantly encode instance task document classification summarization finally model attention distribution ignores depends location attention associate compute function independently content conversely attention compute without compatibility function attention function omit IV empirical comparison compatibility function namely additive multiplicative attention domain machine translation reader refer distribution function attention distribution attention choice distribution function depends distribution instance probability distribution probability boolean enforce sparsity account distribution function logistic sigmoid propose constrain ensure correspond counterpart boundary interpret probability relevant context vector softmax function commonly attention mechanism attention attention interpret probability correspond relevant sigmoid softmax alike relevance argue input completely irrelevant likely introduce contribute useful information exploit attention distribution altogether ignore thereby reduce computational footprint option  distribution  zero threshold exploit geometric probability simplex approach useful setting irrelevant document summarization cloze task model structural dependence output structure attention network exploit neural conditional random model conditional attention distribution attention compute normalize marginals treat potential task machine translation image caption relevant feature neighborhood helpful focus attention specific portion input advance apply positional mask application softmax location advance attention model considers dynamically location expensive inference differentiable advanced training technique reinforcement variance reduction local attention extends preserve differentiability intuition machine translation input relevant local attention considers fix attention focus precise location combine softmax distribution gaussian distribution gaussian distribution dynamic whereas variance fix dynamic selective attention grid gaussian filter considers patch resolution dynamic parameter combine attention apply former filter latter precisely softmax apply subset whereas others zero subset accord random variable variable correspond probability associate variable attention apply  distribution task query define model distribution learnable adaptive parameter softer attention increase smoother distribution happens harder attention finally concept locality define accord semantic temporal possibility multiplicity variation unified model attention mechanism extend accommodate multiple possibly heterogeneous input output multiple output application data interpret multiple ambiguity data stem multiple meaning address multitask model define jointly compute multiple attention distribution data possibility exploit additive attention IV importance matrix instead vector   yield matrix multiple associate regard model relevance context matrix  embeddings concatenate richer expressive representation regularization penalty apply enforce differentiation model relevance frobenius norm multidimensional attention importance matrix matrix attention compute  feature  yield convolution attention multiple distribution accord convolutional filter filter another possibility explore multihead attention multiple linear projection input perform accord learnable parameter multiple attention function compute parallel usually context vector merge embed suitable regularization sometimes impose guarantee sufficient dissimilarity attention propose possibility regularization subspace linear projection attend output context vector multihead attention helpful combine  attention distribution capture local global context finally  attention computes attention distribution improve performance interpretation data isolate data technique mutually exclusive multihead multidimensional attention combine another multiple input coattention architecture query sequence multidimensional matrix  vector setup architecture task query keywords abusive detection useful relevant query accord task straightforward apply attention mechanism query treat query yield independent representation however information interaction alternatively apply attention jointly become input coattention architecture coattention model coarse grain grain coarse grain model compute attention input embed input query grain model input respect input furthermore coattention perform sequentially parallel parallel model procedure compute attention symmetric input treat identically coarse grain coattention coarse grain model compact representation input compute attention model role input query longer focal compact representation query architecture vice versa sequential coarse grain model propose alternate coattention whereby attention compute obtain embeddings attention compute context vector query perform attention another context vector CK query attention apply context vector CQ architecture propose described model adaptation omit factor additional query attention almost identical sequential architecture attention parallel coarse grain model model propose average avg compute input query application attention generate embed input sequential coattention elaborate computation potentially discard irrelevant computational footprint parallel coattention optimize performance expense simpler elaboration output worthwhile average model replace attention filter irrelevant stage coarse grain coattention inside attention besides average operator indicates apply highlight input grain coattention grain coattention model relevance associate query coattention matrix rdq compute  function  function straightforward adaptation compatibility function IV alternatively function define define  linear transformation concatenation decomposable attention input fed neural network output delay processing neural network reduces input network yield reduction computational footprint alternative propose exploit hermitian inner project complex domain hermitian compute finally hermitian noncommutative role played input query  WQ WK source associate compute relevance respect specific query similarly relevance respect specific extract information aggregation function output function vector  aggregation function attention pool parallel model adopt amount attention pool attribute attention coattention respect query attention obtain  max pool whereas query attention obtain  max pool attention regard conservative approach indeed obtain coattention likely irrelevant query furthermore relevant query obtain relevant query applies query attention suitable approach task query keywords disjunction abusive recognition conversely approach compute accounting related attention heavier computational footprint aggregation function obtain apply distribution function IV omit brevity customary placeholder generic nonlinear activation function whereas dist  distribution function softmax grain coattention model dash max pool distribution function apply   multilayer perceptron mapping computation simpler linear transformation instead apply nest model matrix compute separately apply   softmax attention distribution document accord specific query already  average compute attention distribution query finally sum accord relevance query compute dot obtain document attention distribution alternative nest attention model propose whereby fed multilayer perceptron improvement obtain combine multiple coattention model instance compute coarse grain grain attention parallel combine embed combine attention knowledge accord challenge artificial intelligence AI combine   model network approach symbolic knowledge representation perform complex task throughout decade gap AI methodology research avenue popular approach statistical relational neural symbolic application various architecture memory network neural turing machine others perspective attention attempt improve interpretability neural network opportunity plug external knowledge assign attention relevance input respect task context exploit information isolate significant feature network prediction prior knowledge regard data domain specific task whenever available exploit generate information desire attention distribution encode within neural architecture overview technique inject knowledge neural network VI discussion challenge regard combination knowledge attention supervise attention survey attention model neural architecture perform specific task although alongside supervise procedure attention model per unsupervised fashion useful information architecture nevertheless knowledge desire distribution available data label obtain additional information external exploit perform supervise training attention model preliminary training possibility external classifier classifier subsequently plug attention model architecture procedure preliminary training attention model probability contains relevant information relevance rationale snippet text correspond document categorization model preliminarily data estimate reading reading predict model neural model sentiment analysis auxiliary training another possibility attention model without preliminary training treat attention auxiliary task perform jointly task procedure scenario machine translation visual domain classification understand mechanism exploit attention model specific feature linguistic information useful semantic role label attention multitask syntactic structure indeed lisa multilayer  architecture semantic role label attention perform dependence parse auxiliary task transfer furthermore perform transfer across domain task perform preliminary training attentive architecture source domain perform source task mapping input distribution another attentive architecture target domain perform target task pretrained model exploit indeed desire distribution obtain architecture attention therefore treat auxiliary task previously mention difference distribution pretrained model truth instead data label attention attention apply multiple data sequence sequence model useful information relevance input along model iteration indeed attention model assigns input machine translation desirable ensure possibility maintain information suitable structure additional input attention model exploit symbolic information coverage associate input attention compute information fed attention model query update accord output attention representation enhance  representation coverage model distribution function exploit prior knowledge another component attention model prior knowledge exploit distribution function constraint apply computation enforce boundary assign input coverage information exploit constrain distribution function regulate amount attention receives prior knowledge exploit define infer distance domain domain specific distance distribution function instead positional distance distance derive syntactical information distribution function distance along dependence graph VI challenge future direction discus challenge application attention mechanism analysis neural network training enable integration symbolic representation within neural architecture attention network investigation attention explain neural network currently debate recent attention cannot reliable explain interpret neural network nonetheless advocate attention analytic specifically jain wallace attention consistent explainability metric easy local adversarial distribution distribution model outcome   discussion demonstrate effective global adversarial attention model local attention information regard feature importance conclusion attention indeed explanation model explanation plausible necessarily faithful reconstruction decision rudin  context multilayer neural architecture assume deepest compute abstract feature therefore application attention network enable selection feature hint understand complex feature relevant task inquiry computer vision domain application attention feature performance image generation visualization attention reveal attribute proximate image texture query image moreover spatial distribution specific instead model corresponds depict image identify abstract feature input text immediate image analysis greatly aid visual intuition application attention ass correspond specific feature analyze relation attention syntactic prediction  resolution investigate correlation linguistic feature analyze behavior multihead model discover develop behavior related specific specific syntactical confirm deeper neural architecture capture nonlocal aspect textual input application locality depth attentive architecture introduction beneficial apply layer closer input moreover application locality variable layer tend broader popular investigate architecture feature amount architecture perform task happens transfer adopt outside context attention perform syntactic prediction hidden representation machine translation attention input feature model ass relevant information task happens attention distillation network penalize confuse feature accord teacher network efficient robust model task machine reading comprehension similarly transfer attentional heterogeneous transfer exploit  text classification selectively filter input feature heterogeneous source attention outlier detection sample weigh another attention outlier detection task classification creation representative embed specific attention apply sample belonging task sample associate regard outlier respect principle potentially apply data training independently computation sample interpret assess relevance specific data specific task principle assign sample exclude improve model robustness noisy input moreover dynamic computation training dynamic selection training data training phase attention adaptive data selection strategy already proven useful efficiently obtain effective model knowledge attention approach date attention analysis model evaluation impact attention irrelevant exclude input sequence importance relevant properly balance seemingly uniform distribution attention interpret attention model unable identify useful due data useful information task ascribed ability model discriminate information nevertheless attention model unable relevant information specific input sequence error analysis distribution attention therefore architecture confidence perform task input speculate entropy distribution presence threshold correlate probability neural model therefore indicator ass uncertainty architecture improve interpretability clearly information useful user understand model data exploit complex propose exploit uncertainty stochastic predictive model avoid risky prediction healthcare task context architecture relies multiple strategy perform task hybrid model relies symbolic  information uncertainty neural model parameter merge strategy context information relevant multitask reinforcement exploitation uncertainty model although context attention nlp   unsupervised attention properly exploit unsupervised widely recognize important challenge AI already mention attention typically supervise architecture although without supervision attention nevertheless recently attempt exploit attention within purely unsupervised model promising research direction indeed largely unsupervised attention exploit model aspect extraction sentiment analysis aim remove irrelevant sentiment ensure coherence predict aspect zhang attention within autoencoders retrieval task generate semantic representation attention exploit encode decode phase objective reconstruct input sequence traditional autoencoders exploit bidimensional attention recursive autoencoders bilingual embeddings whereas tian fang exploit attentive autoencoders representation perform topic model text instead adopt attention driven approach unsupervised sentiment modification explicitly sentiment content computer vision attention alignment propose unsupervised domain adaptation aim align attention network source target domain respectively scenario nlp neural symbolic recently attention mechanism integrate within neural symbolic model application nlp scenario stage instance context neural logic program exploit knowledge graph combine parameter structure logic logic attention network aggregate information graph network attention moreover prior knowledge exploit enable attention mechanism knowledge representation entity rank neural architecture exploit attention perform task address symbolic approach textual entailment instance  recently propose architecture complex nlp usually target sub task visual architecture attention within model capture dependence attention attempt introduce constraint logical statement within neural network propose attention enforce alignment task machine comprehension inference vii conclusion attention model become ubiquitous nlp application attention apply input representation data feature obtain compact representation data highlight relevant information selection perform distribution function locality dimension semantics attention input data query similarity significance autonomously relevant representation encode important data integrate attention neural architecture yield significant performance gain moreover attention investigate behavior network article introduce taxonomy attention model enable systematically vast portion approach literature another knowledge systematic comprehensive taxonomy attention model nlp role attention address fundamental AI challenge attention instrumental inject knowledge neural model specific feature exploit previously acquire knowledge transfer setting speculate pave challenge research avenue attention exploit enforce combination  model symbolic knowledge representation perform task address understand recent attention ingredient unsupervised architecture focus training supervision advance