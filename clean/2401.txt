federate FL currently widely adopt framework collaborative training machine model privacy constraint albeit popularity FL yield suboptimal local client data distribution diverge address issue cluster FL CFL novel federate multitask  framework exploit geometric FL loss client population cluster jointly trainable data distribution contrast exist  approach CFL modification FL communication protocol applicable nonconvex objective neural network cluster priori mathematical guarantee cluster quality CFL flexible handle client population implement privacy preserve cluster perform FL converge stationary CFL postprocessing achieve performance conventional FL client specialized model verify theoretical analysis convolutional recurrent neural network commonly FL data introduction federate FL distribute training framework allows multiple client typically mobile iot device jointly model combine data communication efficient without participant reveal private training data centralize entity FL realizes goal via iterative protocol communication client synchronize server model client proceeds improve model perform multiple iteration stochastic gradient descent minibatches sample local data update vector  sgd  client upload compute update server aggregate average accord  sourceto model procedure summarize algorithm algorithm FL FL implicitly assumption model client data generate distribution model parameterized loss function formally assumption assumption FL exists parameter configuration locally minimizes risk client data generate distribution minθ  sourcefor hereby  sourceis risk function associate distribution easy assumption satisfied concretely violate model expressive distribution client disagree conditional distribution counterexample toy FL assumption violate belong client orange belong client FL assumes model client data distribution federate xor insufficiently complex model capable fitting client data distribution client conditional distribution diverge model distribution however data client belonging cluster easily client data generate distribution congruent respect satisfy assumption incongruent argue assumption frequently violate FL application FL client arbitrary non data cannot audit centralize server due privacy constraint typically limited hardware restriction model complexity illustration practical scenario preference assume scenario client local data image goal attractiveness classifier joint data client naturally client opinion attractiveness individual corresponds disagree conditional distribution client data assume instance client population attractive unattractive situation model accurately predict attractiveness client confer limited model complexity assume client jointly model prediction private text message scenario statistic client text message likely demographic factor instance text message compose teenager typically exhibit statistic compose elderly insufficiently expressive model data client presence adversary  subset client population behaves adversarial manner scenario adversary deliberately alter local data distribution encode arbitrary behavior jointly model affect model decision client potential harm federate multitask goal federate multitask  client model optimally local data distribution described situation ordinary FL framework client treat equally global model capable achieve goal incorporate incongruent data generate distribution generalize conventional FL assumption assumption CFL exists partition client population subset client satisfies conventional FL assumption cluster FL CFL novel algorithmic framework  satisfy assumption identify hidden cluster structure CFL allows client data profit another minimize harmful interference client dissimilar data contribution contribution highlight important practical limitation conventional FL namely incongruent client data distribution derive computationally efficient cosine similarity client gradient update provably allows infer member client population data generate distribution infer cluster structure II address cluster derive criterion ensures cluster incongruent benefit II theoretical insight CFL algorithm II investigate practical concern client population training formal privacy guarantee communication update instead gradient demonstrate CFL seamlessly adapt constraint IV evaluate theoretical finding neural network demonstrate vast performance improvement conventional FL algorithm optimize incongruent client summary important throughout article additionally refer reader II detailed comparison propose related  summary II qualitative comparison  II cluster federate address distribute satisfy assumption generalizes FL assumption identify partition glance daunt task FL paradigm server access client data data generate distribution meta information thereof cosine similarity bipartitioning easy task directly infer entire cluster structure bipartitioning definition definition  mapping assigns client data generate distribution bipartitioning sourcein bipartitioning client data generate distribution cluster easy cluster obtain exactly  demonstrate exists explicit criterion bipartitioning infer simplify FL client data client sample data generate distribution  client associate empirical risk function  sourcewhich approximates risk arbitrarily data client sufficiently RI sourcefor demonstration purpose assume equality FL objective becomes sourcewith mdi standard assumption FL optimization protocol described converges stationary FL objective  situation simultaneously minimize risk client congruent distribute otherwise sourceand incongruent situation cosine similarity gradient update client RI RI RI RI  bipartitioning SourceThis consideration insight stationary FL objective distinguish client hidden data generate distribution inspect cosine similarity gradient update visual illustration refer optimization FL client belonging cluster incongruent data distribution FL converges stationary FL objective gradient client positive norm direction cosine similarity gradient update cluster constant throughout federate training cosine similarity gradient update cluster quickly decrease equality assumption arbitrary data generate distribution obtain generalize version theorem separation theorem DM local training data client data sample data generate distribution empirical risk client approximate risk stationary FL objective RI RI sourceand define RI RI sourcethen exists bipartitioning client population maximum similarity update client cluster bound accord  minc maxi maxi sin sourcewith   similarity update client data generate distribution bound αminintra mini   mini  theorem appendix remark data generate distribution simplifies  maxi   maxi sourcefor partition respective αminintra mini   mini sourcefor client cluster additionally retain theorem directly deduce separation corollary theorem αminintra  sourcethen partition argminc maxi  sourceis definition proof argminc maxi  sourceand  αminintra mini sourceand hence cannot data generate distribution consideration notion separation gap definition separation gap cosine similarity matrix mapping client data generate distribution define notion separation gap αminintra  minc maxi  source remark corollary bipartitioning definition separation gap zero theorem estimate similarity absolute αminintra typically  typically parameter dimension dim instance magnitude typical neural network assume RI RI normally distribute experimentally cluster   maxi  SourceThis suggests cosine similarity criterion readily bipartitioning data generate distribution empirical risk client data loose approximation risk cluster quality function data generate distribution relative approximation CFL correctly client theorem empirically CFL correctly client probability distinguish congruent incongruent client appropriately generalize classical FL split client incongruent data distribution classical congruent non FL described model performance typically degrade client distribution cluster due restrict knowledge transfer client cluster luckily criterion distinguish realize inspect gradient compute client stationary client distribution incongruent stationary FL objective definition cannot stationary individual client hence norm client gradient strictly zero conversely client distribution congruent federate optimization jointly optimize client local risk function hence norm client gradient tend zero approach stationary observation formulate criterion decision split splitting stationary FL objective mdi  sourceand individual client stationary local empirical risk maxi  sourcewe experimentally verify cluster criterion recommendation selection hyperparameters another viable option distinguish congruent incongruent splitting perform FL converge stationary conventional FL compute CFL splitting client degradation model performance FL CFL improve FL performance perform equally algorithm CFL recursively  client population initial client parameter initialization CFL performs FL accord algorithm obtain stationary FL objective FL converge criterion maxi  sourceis evaluate criterion satisfied client sufficiently stationary local risk consequently CFL terminates return FL criterion violate client incongruent server computes pairwise cosine similarity client transmit update accord server client cluster maximum similarity client cluster minimize argminc maxi  SourceThis optimal bipartitioning core CFL algorithm FL assume server computational client overhead cluster typically negligible algorithm optimal bipartition derive II bipartitioning ensure αminintra   optimal cluster similarity  easily compute computation intra cluster similarity knowledge cluster structure hence αminintra estimate theorem accord αminintra mini    bipartitioning    data generate distribution criterion allows reject  assumption approximation  interpretable hyperparameter criterion satisfied CFL recursively  stationary splitting recursively recursion none subclusters violate criterion anymore mutually congruent client identify CFL characterize assumption communication burden CFL increase linearly split FL entire recursive procedure algorithm schematic illustration schematic overview CFL algorithm recursively bipartitioning client population subgroup maximum dissimilarity CFL hierarchy model increase specificity algorithm CFL related FL currently dominant framework distribute training machine model communication privacy constraint FL assumes client congruent central model client distribution author investigate convergence FL congruent non scenario  empirical investigation jiang agrawal convergence guarantee argue conventional FL challenge incongruent data distribution distribute training framework issue framework incongruent data multitask overview recent technique multitask neural network however technique apply centralize data reside location server knowledge optimization  extends multitask approach FL explicitly model client similarity via correlation matrix however relies alternate biconvex optimization applicable convex objective function limited ability massive client population   model connectivity structure client server bayesian network perform variational inference although handle nonconvex model expensive generalize federate network client model refine sequentially finally propose cluster approach article however differs aspect significantly distance instead cosine similarity distribution similarity client approach severe limitation client risk function convex minimum cluster distinguish congruent incongruent setting incorrectly split client conventional congruent non described furthermore approach adaptive decision cluster already communication contrast apply arbitrary FL nonconvex objective function theoretical consideration systematic understand novel CFL framework detailed qualitative comparison  refer II IV implementation consideration practical implementation detail concretely demonstrate CFL implement without modification FL communication protocol without compromise privacy participate client demonstrate flexible handle client population update generalize gradient theorem statement cosine similarity gradient empirical risk function FL however due constraint communication  client device instead commonly update define compute communicate deviate classical FL algorithm hence desirable generalize update commonly conjecture accumulate minibatch gradient approximate batch gradient objective function indeed sufficiently smooth loss function rate update compute epoch approximates direction gradient taylor expansion split   client data disjoint batch epoch sgd sgd sourcewith sourcein remainder compute cosine similarity update instead gradient accord      demonstrate compute cosine similarity update surprisingly achieves separation compute cosine similarity gradient preserve privacy machine model information data bias layer neural network typically information label distribution training data author demonstrate information client input data infer update sends server via model inversion attack privacy sensitive situation prevent information leakage client server mechanism luckily CFL easily augment encryption mechanism achieves cosine similarity client update norm update invariant orthonormal transformation permutation index          remedy client apply transformation operator update communicate server server average update client broadcast average client simply apply inverse operation   sourceand FL protocol resume unchanged multitask approach access client data hence cannot encryption CFL distinct advantage privacy sensitive situation client population parameter assumption client participate training however CFL flexible handle client population incorporate functionality server CFL parameter contains node intermediate cluster compute CFL correspond stationary obtain FL algorithm cluster cached node  resides FL entire client population  cluster  bipartitioning cluster  CFL node   via    update client sgd    cached exemplary parameter client training assign leaf cluster iteratively traverse parameter leaf contains client update accord algorithm exemplary parameter CFL node resides conventional FL model obtain converge stationary FL objective client layer client population split accord cosine similarity subgroup converge stationary respective branching recursively stationary satisfies splitting criterion quickly assign client leaf model server cache  update client belonging  client along similarity algorithm assign client cluster another feature building parameter allows server client multiple model specificity leaf model specialized model FL model application context CFL client switch model generality furthermore parameter allows ensemble multiple model specificity investigation along promising direction future research previous protocol privacy preserve CFL described algorithm algorithm privacy preservation update practical consideration II cosine similarity criterion distinguish incongruent client FL converge stationary client data empirical risk approximates risk cosine similarity compute gradient empirical risk demonstrate practical none fully satisfied instead CFL correctly infer cluster structure client data approximately stationary FL objective furthermore cosine similarity compute update instead gradient improves performance FL setup perform mnist cifar data client belonging cluster client assign equally random subset training data simulate incongruent cluster structure client data modify randomly swap label cluster client belongs client belonging cluster data label relabeled vice versa client belonging cluster switch relabeling ensures approximately across client conditionals diverge cluster refer label swap augmentation multilayer convolutional neural network adopt standard FL strategy local epoch training report separation gap definition αminintra  sourcewhich accord corollary CFL correctly bipartition client cluster source data investigate data cosine similarity randomly subsample client training data data client mnist cifar local data FL communication training progress mostly halt stationary compute pairwise cosine similarity update separation gap grows monotonically increase data mnist data client sufficient achieve bipartitioning definition cifar around data achieve bipartitioning separation gap function data client label swap mnist cifar corollary CFL bipartitioning mnist already satisfied client data update computation similarity proximity stationary investigate importance proximity stationary cluster previous reduce data client mnist cifar compute pairwise cosine similarity separation gap communication separation quality monotonically increase communication mnist cifar communication obtain cluster separation gap function communication label swap mnist cifar separation quality monotonically increase communication FL separation already achieve around communication compute update update instead gradient abovementioned compute cosine similarity gradient     gradient sourceor federate update     update  epoch interestingly update separation data distance stationary handy allows FL communication protocol unchanged compute cosine similarity update instead gradient distinguish congruent incongruent client experimentally verify validity cluster criterion FL mnist client data incongruent congruent distribution congruent client training digit client training digit incongruent client random subset training data distribution modify accord label swap described earlier development average update norm maximum client norm communication predict theory congruent maximum client norm converges zero incongruent  increase average update norm tends zero convergence stationary experimental verification norm criterion displayed development gradient norm communication FL client data incongruent congruent distribution FL converges stationary average update norm zero congruent maximum norm client update decrease along server update norm contrast incongruent  increase consideration recommendation regard selection hyperparameters quality cluster improves proximity stationary restriction thumb around tenth maximum average update norm maxt  accordance available client prior knowledge heterogeneity client data likely client population CFL obtain cluster federate apply CFL described algorithm FL setup inspire motivate introduction client perform epoch local training batch communication image classification cifar split cifar training data randomly evenly client cluster client belonging cluster apply random permutation label modify training data PI   PI dtest sourcethe client jointly layer convolutional neural network modify data CFL epoch local training batch joint training progression communication client model conventional FL protocol initial training converge stationary FL objective client accuracy stagnate around conventional FL finalize distinct gap αminintra  developed underlie cluster structure communication client population therefore split immediate increase validation accuracy client belonging purple cluster splitting communication cluster zero cluster indicates cluster finalize accuracy client achieve FL underline standard FL novel CFL detect necessity subsequent splitting cluster enable significantly performance addition cluster structure potentially illuminate insight composition complex underlie data distribution CFL apply permute label cifar client permutation accuracy model correspond validation separation gap cluster initial communication separation gap developed split purple client immediate drastic increase accuracy client communication client incongruent distribution split model accuracy client separation gap cluster zero indicates cluster finalize model news news corpus collection news article belonging topic sport business sci tech split corpus   article topic assign corpus client consequently client cluster article client layer lstm network predict local corpus article communication multistage CFL apply distribute FL converges stationary around communication client achieve perplexity around local client population split communication underlie cluster discover communication perplexity client FL amount communication  average perplexity CFL apply news perplexity achieve client local client communication separation perplexity client local FL dot  perplexity VI conclusion article CFL framework  improve exist FL framework enable participate client specialized model CFL theoretical stationary FL objective cosine similarity update client highly indicative similarity data distribution crucial insight allows mathematical guarantee cluster quality mild assumption client data arbitrary nonconvex objective demonstrate CFL implement privacy preserve without modify FL communication protocol moreover CFL distinguish situation model client data client latter convolutional recurrent neural network CFL achieve drastic improvement FL baseline classification accuracy perplexity situation client data exhibit cluster structure finally expose privacy issue FL demonstrates information client data similarity infer update argue privacy loss inflict tolerable situation mere knowledge client similarity reveal anything client data nevertheless implement FL privacy sensitive application future explore extent CFL conjunction differential privacy mechanism parameter update compression furthermore explainable AI technique context cluster FL