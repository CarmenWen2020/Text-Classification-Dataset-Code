revolutionize machine task recent image classification video processing recognition understand data task typically euclidean however increase application data generate non euclidean domain graph complex relationship interdependency complexity graph data impose significant challenge exist machine algorithm recently extend approach graph data emerge article comprehensive overview graph neural network GNNs data mining machine propose taxonomy GNNs category namely recurrent GNNs convolutional GNNs graph autoencoders spatial temporal GNNs discus application GNNs across various domain summarize source code benchmark data model evaluation GNNs finally propose potential research direction rapidly introduction recent neural network boost research recognition data mining machine task detection machine translation recognition heavily rely handcraft feature engineering extract informative feature recently revolutionize various paradigm convolutional neural network cnns recurrent neural network rnns autoencoders domain partially attribute rapidly develop computational resource gpu availability training data effectiveness extract latent representation euclidean data image text video image data image regular grid euclidean cnn exploit shift invariance local connectivity compositionality image data cnns extract local meaningful feature entire data various image analysis effectively capture hidden euclidean data increase application data graph commerce graph exploit interaction user highly accurate recommendation chemistry model graph  identify drug discovery citation network article link via  categorize complexity graph data impose significant challenge exist machine algorithm graph irregular graph variable unordered node node graph important operation convolution easy compute image domain apply graph domain furthermore core assumption exist machine algorithm instance independent assumption longer graph data instance node related others link various citation friendship interaction recently increase extend approach graph data motivate cnns rnns autoencoders generalization definition important operation rapidly developed handle complexity graph data graph convolution generalize convolution illustrate image graph pixel adjacent pixel convolution perform graph convolution average node neighborhood information convolution versus graph convolution convolution analogous graph pixel image node filter convolution average pixel node along node fix graph convolution hidden representation node graph convolutional operation average node feature node along image data node unordered variable limited exist review topic graph neural network GNNs geometric overview non euclidean domain graph manifold although review GNNs article mainly review convolutional GNNs limited GNNs focus address network embed graph network building relational data review GNNs unified framework conduct partial survey GNNs apply attention mechanism summary exist survey GNNs examine limited thereby recent development GNNs article comprehensive overview GNNs interested researcher rapidly develop expert GNN model broader article considers GNNs approach graph data contribution article notable contribution summarize taxonomy propose taxonomy GNNs GNNs categorize recurrent GNNs  convolutional GNNs ConvGNNs graph autoencoders GAEs spatial temporal GNNs STGNNs comprehensive review comprehensive overview technique graph data GNNs detailed description representative model comparison summarize correspond algorithm abundant resource abundant resource GNNs model benchmark data source code practical application article understand develop approach various application future direction discus theoretical aspect GNNs analyze limitation exist future research direction model depth scalability tradeoff heterogeneity dynamicity organization article article organize II outline background GNNs commonly notation defines graph related concept clarifies categorization GNNs IV vii overview GNN model collection application across various domain IX discus challenge suggests future direction summarizes article II background definition outline background GNNs commonly notation define graph related concept background brief graph neural network   apply neural network acyclic graph motivate GNNs notion GNNs outline elaborate category RecGNNs target node representation propagate information iterative manner stable fix computationally expensive recently increase effort overcome challenge encourage cnns computer vision domain redefine notion convolution graph data developed parallel approach umbrella ConvGNNs ConvGNNs spectral approach spatial approach prominent research spectral ConvGNNs developed graph convolution spectral graph theory increase improvement extension approximation spectral ConvGNNs research spatial ConvGNNs earlier spectral ConvGNNs  address graph mutual dependence architecturally composite nonrecursive layer inherit message passing RecGNNs however importance article overlook recently spatial ConvGNNs emerge apart RecGNNs ConvGNNs alternative GNNs developed GAEs STGNNs framework built RecGNNs ConvGNNs neural architecture graph model detail categorization graph neural network versus network embed research GNNs closely related graph embed network embed another topic attracts increase attention data mining machine community network embed aim network node dimensional vector representation preserve network topology structure node content information subsequent graph analytics task classification cluster recommendation easily perform shelf machine algorithm vector machine classification meanwhile GNNs model aim address graph related task manner GNNs explicitly extract representation distinction GNNs network embed GNNs neural network model various task network embed various target task therefore GNNs address network embed GAE framework network embed contains  matrix factorization random graph neural network versus graph kernel graph kernel historically dominant technique graph classification employ kernel function similarity graph kernel algorithm vector machine supervise graph GNNs graph kernel embed graph node vector mapping function difference mapping function deterministic learnable due pairwise similarity calculation graph kernel suffer significantly computational bottleneck GNNs directly perform graph classification extract graph representation therefore efficient graph kernel review graph kernel refer reader definition throughout article bold uppercase denote matrix bold lowercase denote vector unless particularly specify notation article illustrate define minimal definition understand article commonly notation definition graph graph vertex node node throughout article denote node eij denote neighborhood node define adjacency matrix matrix aij eij aij eij graph node attribute node feature matrix feature vector node meanwhile graph attribute feature matrix  feature vector definition graph graph graph node another undirected graph graph inverse direction node graph undirected adjacency matrix symmetric definition spatial temporal graph spatial temporal graph attribute graph node attribute dynamically spatial temporal graph define categorization framework taxonomy GNNs II categorize GNNs RecGNNs ConvGNNs GAEs STGNNs various model architecture brief introduction category II taxonomy representative publication GNNs GNN model built graph convolutional layer gconv denotes graph convolutional layer mlp denotes multilayer perceptron cnn denotes standard convolutional layer  multiple graph convolutional layer graph convolutional layer encapsulates node hidden representation aggregate feature information feature aggregation nonlinear transformation apply output stack multiple layer hidden representation node receives message neighborhood  pool readout layer graph classification graph convolutional layer pool layer coarsen graph subgraphs node representation coarsen graph graph representation readout layer summarizes graph representation sum hidden representation subgraphs GAE network embed encoder graph convolutional layer network embed node decoder computes pairwise distance network embeddings apply nonlinear activation function decoder reconstructs graph adjacency matrix network minimize discrepancy adjacency matrix reconstruct adjacency matrix  spatial temporal graph forecasting graph convolutional layer cnn layer graph convolutional layer operates capture spatial dependence cnn layer slide along axis capture temporal dependence output layer linear transformation generate prediction node future taxonomy graph neural network recurrent graph neural network mostly pioneer GNNs RecGNNs aim node representation recurrent neural architecture assume node graph constantly exchange information message stable equilibrium RecGNNs conceptually important inspire later research ConvGNNs message passing inherit spatial ConvGNNs convolutional graph neural network generalize operation convolution grid data graph data generate node representation aggregate feature feature RecGNNs ConvGNNs stack multiple graph convolutional layer extract node representation ConvGNNs central role building complex GNN model  node classification demonstrates  graph classification graph autoencoders unsupervised framework encode node graph latent vector reconstruct graph data encode information GAEs network embeddings graph generative distribution network embed GAEs latent node representation reconstruct graph structural information graph adjacency matrix graph generation generate node graph output graph GAE network embed spatial temporal graph neural network aim hidden spatial temporal graph becomes increasingly important variety application traffic forecasting driver maneuver anticipation action recognition STGNNs spatial dependence temporal dependence approach integrate graph convolution capture spatial dependence rnns cnns model temporal dependence illustrates  spatial temporal graph forecasting framework graph structure node content information input output GNNs focus graph analytics task mechanism node output relate node regression node classification task RecGNNs ConvGNNs extract node representation information propagation graph convolution  softmax layer output layer GNNs perform node task manner output relate classification link prediction task node hidden representation GNNs input similarity function neural network utilized predict label connection strength graph output relate graph classification task obtain compact representation graph GNNs combine pool readout operation detailed information pool readout review training framework GNNs ConvGNNs semi supervise purely unsupervised within framework task label information available semisupervised node classification network partial node label others remain unlabeled ConvGNNs robust model effectively identifies label unlabeled node framework built stack couple graph convolutional layer softmax layer multiclass classification supervise graph classification graph classification aim predict label entire graph task realize combination graph convolutional layer graph pool layer readout layer graph convolutional layer responsible node representation graph pool layer role downsampling coarsens graph substructure readout layer collapse node representation graph graph representation apply multilayer perceptron softmax layer graph representation framework graph classification unsupervised graph embed label available graph graph embed purely unsupervised framework algorithm exploit information adopt autoencoder framework encoder employ graph convolutional layer embed graph latent representation upon decoder reconstruct graph structure another popular utilize negative sample approach sample portion node negative exist node link graph positive logistic regression layer apply distinguish positive negative summarize characteristic representative RecGNNs ConvGNNs input source pool layer readout layer complexity various model detail complexity message passing graph convolutional operation model eigenvalue decomposition complexity complexity due node pairwise shortest computation incur equivalent complexity graph adjacency matrix sparse otherwise computation node representation involves sum node exactly complexity lack complexity analysis article report complexity overall model algorithm summary RecGNNs ConvGNNs pool readout layer node task IV recurrent graph neural network RecGNNs mostly pioneer GNNs apply parameter recurrently node graph extract node representation constrain computational earlier research mainly focus acyclic graph GNN propose extends prior recurrent model handle graph acyclic cyclic undirected graph information diffusion mechanism GNN update node exchange neighborhood information recurrently stable equilibrium node hidden recurrently update sourcewhere parametric function initialize randomly sum operation enables GNN applicable node differs neighborhood ensure convergence recurrent function contraction mapping shrink distance project latent neural network penalty impose jacobian matrix parameter convergence criterion satisfied node hidden readout layer GNN alternate stage node propagation stage parameter gradient computation minimize training objective strategy enables GNN handle cyclic graph graph echo network  extends echo network improve training efficiency GNN  consists encoder output layer encoder randomly initialize training implement contractive transition function recurrently update node global graph convergence afterward output layer fix node input gate GNN GGNN employ gate recurrent gru recurrent function reduce recurrence fix advantage longer constrain parameter ensure convergence node hidden update previous hidden hidden define gru sourcewhere GNN  GGNN backpropagation BPTT algorithm model parameter problematic graph GGNN recurrent function multiple node intermediate node memory stochastic steady embed SSE proposes algorithm scalable graph SSE update node hidden recurrently stochastic asynchronous fashion alternatively sample batch node update batch node gradient computation maintain stability recurrent function SSE define average historical  sourcewhere hyperparameter initialize randomly conceptually important SSE theoretically node gradually converge fix apply repeatedly convolutional graph neural network ConvGNNs closely related recurrent graph neural network instead iterate node contractive constraint ConvGNNs address cyclic mutual dependency architecturally fix layer layer distinction illustrate graph convolution efficient convenient composite neural network popularity ConvGNNs rapidly recent ConvGNNs category spectral spatial spectral approach define graph convolution introduce filter perspective graph signal processing graph convolutional operation interpret remove graph signal spatial approach inherit RecGNNs define graph convolution information propagation gcn bridge gap spectral approach spatial approach spatial developed rapidly recently due attractive efficiency flexibility generality RecGNNs versus ConvGNNs RecGNNs graph recurrent layer  update node representation ConvGNNs graph convolutional layer gconv update node representation spectral ConvGNNs background spectral solid mathematical foundation graph signal processing assume graph undirected normalize graph laplacian matrix mathematical representation undirected graph define AD diagonal matrix node dii normalize graph laplacian matrix posse symmetric positive semidefinite normalize laplacian matrix factor  matrix eigenvectors eigenvalue diagonal matrix eigenvalue spectrum  eigenvectors normalize laplacian matrix orthonormal mathematical  graph signal processing graph signal feature vector node graph ith node graph fourier transform signal define utx inverse graph fourier transform define signal graph fourier transform graph fourier transform project input graph signal orthonormal basis eigenvectors normalize graph laplacian transform signal coordinate graph signal input signal iui exactly inverse graph fourier transform graph convolution input signal filter define utx  sourcewhere denotes elementwise denote filter diag  spectral graph convolution simplify    ConvGNNs definition difference choice filter spectral cnn assumes filter learnable parameter considers graph signal multiple channel graph convolutional layer spectral cnn define   sourcewhere layer index input graph signal input channel output channel diagonal matrix learnable parameter due eigendecomposition laplacian matrix spectral cnn limitation perturbation graph  filter domain dependent cannot apply graph structure eigendecomposition computational complexity ChebNet gcn reduce computational complexity approximation simplification chebyshev spectral cnn ChebNet approximates filter chebyshev polynomial diagonal matrix eigenvalue  λmax chebyshev polynomial define recursively xti convolution graph signal define filter   utx sourcewhere λmax uti UT proven induction ChebNet   sourceas improvement spectral cnn filter define ChebNet localize filter extract local feature independently graph spectrum ChebNet mapped linearly  applies cayley polynomial parametric rational complex function capture narrow frequency spectral graph convolution  define   sourcewhere return complex  complex  imaginary parameter spectrum cayley filter preserve spatial locality  ChebNet  graph convolutional network gcn introduces approximation ChebNet assume λmax simplify   AD sourceto restrain parameter avoid overfitting gcn assume definition graph convolution  AD sourceto  input output gcn modifies compositional layer define   sourcewhere AD activation function AD empirically numerical instability gcn address gcn applies normalization trick replace AD spectral gcn interpret spatial spatial perspective gcn aggregate feature information node neighborhood equation express   source recent incremental improvement gcn explore alternative symmetric matrix adaptive gcn  learns hidden structural relation unspecified graph adjacency matrix construct residual graph adjacency matrix learnable distance function node feature input dual gcn  introduces dual graph convolutional architecture graph convolutional layer parallel layer parameter normalize adjacency matrix positive pointwise mutual information  matrix capture node occurrence information random sample graph  matrix define  max sourcewhere  function return frequency node node sample random ensembling output dual graph convolutional layer  encodes local global structural information without stack multiple graph convolutional layer spatial ConvGNNs analogous convolutional operation conventional cnn image spatial define graph convolution node spatial relation image graph pixel node pixel directly nearby pixel filter apply patch average pixel central node across channel similarly spatial graph convolution convolve central node representation representation derive update representation central node another perspective spatial ConvGNNs information propagation message passing RecGNNs spatial graph convolutional operation essentially propagates node information along neural network graph  propose parallel GNN spatial ConvGNNs distinctively RecGNNs  learns graph mutual dependence compositional neural architecture independent parameter layer neighborhood node extend incremental construction architecture  performs graph convolution sum node neighborhood information directly applies residual connection skip connection memorize information layer  derives layer node  sourcewhere activation function equation matrix XW AH sourcewhich resembles gcn difference  unnormalized adjacency matrix potentially hidden node extremely contextual graph markov model  proposes probabilistic model inspire  maintain spatial locality  benefit probabilistic interpretability diffusion cnn dcnn regard graph convolution diffusion assumes information transfer node node transition probability information distribution equilibrium dcnn defines diffusion graph convolution   sourcewhere activation function probability transition matrix compute dcnn hidden representation matrix remains dimension input feature matrix function previous hidden representation matrix dcnn concatenates model output stationary distribution diffusion summation series probability transition matrix  sum output diffusion instead concatenation defines   sourcewhere RD activation function transition probability matrix implies contribute information central node  DGCNN increase contribution shortest defines shortest adjacency matrix shortest node node otherwise hyperparameter receptive  DGCNN introduces graph convolutional operation sourcewhere concatenation vector calculation shortest adjacency matrix expensive maximum partition graph convolution  partition node criterion limited shortest  construct adjacency matrix accord define neighborhood  applies gcn parameter matrix sum QA sourcewhere message passing neural network  outline framework spatial ConvGNNs treat graph convolution message passing information node another along directly  message passing iteration information propagate message passing function namely spatial graph convolution define  sourcewhere function learnable parameter derive hidden representation node output layer perform node prediction task readout function perform graph prediction task readout function generates representation entire graph node hidden representation generally define sourcewhere readout function learnable parameter  exist GNNs assume however graph isomorphism network gin previous  incapable distinguish graph structure graph embed amend drawback gin adjusts central node learnable parameter performs graph convolution mlp sourcewhere mlp multilayer perceptron node inefficient node neighborhood graphsage adopts sample obtain fix node performs graph convolution SN sourcewhere aggregation function SN random sample node aggregation function invariant permutation node ordering sum max function graph attention network gat assumes contribution node central node neither identical graphsage predetermine gcn difference illustrate gat adopts attention mechanism relative node graph convolutional operation accord gat define  sourcewhere attention connective strength node softmax sourcewhere LeakyReLU activation function vector learnable parameter softmax function ensures attention sum node gat performs multihead attention increase model expressive capability impressive improvement graphsage node classification task gat assumes contribution attention gate attention network  introduces attention mechanism computes additional attention attention apart apply graph attention spatially  proposes lstm gate mechanism information across graph convolutional layer graph attention model however belong  framework difference gcn gat gcn explicitly assigns nonparametric aij deg deg aggregation gat implicitly capture aij via neural network architecture important node mixture model network monet adopts approach assign node introduces node  relative node relative node function relative relative node parameter graph filter across location monet framework exist approach manifold geodesic cnn GCNN anisotropic cnn  spline cnn graph gcn dcnn generalize instance monet construct nonparametric function monet additionally proposes gaussian kernel learnable parameter function adaptively another distinct achieves across location rank node criterion associate rank learnable patchy san node accord graph labelings selects graph labelings essentially node derive node centrality weisfeiler lehman WL node fix graph structure data convert grid structure data patchy san applies standard convolutional filter aggregate neighborhood feature information filter corresponds node rank criterion patchy san considers graph structure computation data processing gcn  rank node node feature information node  assembles feature matrix consists neighborhood sort feature matrix along sort feature matrix input data central node improvement training efficiency training ConvGNNs gcn usually graph data intermediate node memory batch training algorithm ConvGNNs suffers significantly memory overflow graph contains node memory graphsage proposes batch training algorithm ConvGNNs sample node recursively expand node neighborhood fix sample sample graphsage computes node hidden representation hierarchically aggregate hidden node representation gcn FastGCN sample fix node graph convolutional layer instead sample fix node graphsage interprets graph convolution integral transforms embed function node probability monte carlo approximation variance reduction technique employ facilitate training FastGCN sample node independently layer layer connection potentially sparse propose adaptive layerwise sample approach node sample layer achieves accuracy FastGCN employ complicate sample scheme another stochastic training GCNs  reduces receptive graph convolution arbitrarily historical node representation variate  achieves comparable performance per node however  intermediate node memory consume graph cluster gcn sample subgraph graph cluster algorithm performs graph convolution node within sample subgraph neighborhood restrict within sample subgraph cluster gcn capable handle graph deeper architecture memory cluster gcn notably straightforward comparison complexity memory complexity exist  training algorithm analyze IV IV memory complexity comparison  training algorithm summarize node layer batch sample node simplicity dimension node hidden feature remain constant denote IV gcn baseline conduct batch training graphsage memory sacrifice efficiency meanwhile memory complexity graphsage exponentially increase complexity sto gcn bottleneck memory remains unsolved however sto gcn achieve satisfactory performance complexity cluster gcn remains baseline introduce redundant computation cluster gcn realizes memory complexity comparison spectral spatial model spectral model theoretical foundation graph signal processing graph signal filter  ConvGNNs however spatial model prefer spectral model due efficiency generality flexibility issue spectral model efficient spatial model spectral model perform eigenvector computation handle graph spatial model scalable graph directly perform convolution graph domain via information propagation computation perform batch node instead graph spectral model rely graph fourier basis generalize poorly graph assume fix graph perturbation graph  spatial model perform graph convolution locally node easily across location structure spectral model limited undirected graph spatial model flexible handle multisource graph input input graph graph heterogeneous graph graph input incorporate aggregation function easily graph pool module GNN generates node feature task however feature directly computationally challenge downsampling strategy objective role network strategy pool operation aim reduce parameter downsampling node generate representation avoid overfitting permutation invariance computational complexity issue readout operation mainly generate graph representation node representation mechanism pool refer downsampling strategy apply GNNs earlier graph coarsen algorithm eigendecomposition coarsen graph topological structure however suffer complexity issue  algorithm alternative eigendecomposition calculate cluster version graph recent employ pool operation coarsen graph nowadays max sum pool primitive effective implement downsampling calculate max sum pool max sum sourcewhere index graph convolutional layer perform max pool network important reduce dimensionality graph domain mitigate expensive graph fourier transform operation furthermore attention mechanism enhance sum pool attention mechanism reduction operation sum pool satisfactory embed inefficient fix embed generate regardless graph propose  generate memory increase input implement lstm intend integrate dependent information memory embed reduction apply otherwise destroy information address issue another rearrange node graph meaningful devise efficient pool strategy approach ChebNet input graph coarsen multiple  algorithm coarsen node input graph coarsen version rearrange balance binary arbitrarily aggregate balance binary node pool rearrange signal efficient pool propose DGCNN pool strategy  performs pool rearrange node meaningful ChebNet DGCNN sort node accord structural role within graph graph unordered node feature spatial graph convolution treat continuous WL sort node addition sort node feature unifies graph truncate extend node feature matrix delete otherwise zero aforementioned pool mainly graph feature ignore structural information graph recently differentiable pool  propose generate hierarchical representation graph previous coarsen  simply cluster node graph learns cluster assignment matrix layer refer  node kth layer probability matrix generate node feature topological structure softmax  sourcethe core comprehensive node assignment topological feature information graph implement standard ConvGNNs however drawback  generates dense graph pool thereafter computational complexity becomes recently  approach propose considers node feature graph topology learns pool attention manner overall pool essential operation reduce graph improve effectiveness computational complexity pool investigation discussion theoretical aspect discus theoretical foundation GNNs perspective receptive receptive node node contribute determination node representation compositing multiple spatial graph convolutional layer receptive node grows ahead  finite spatial graph convolutional layer exists node receptive node node graph  extract global information stack local graph convolutional layer VC dimension VC dimension model complexity define shatter model analyze VC dimension GNNs model parameter node derive VC dimension GNN sigmoid tangent hyperbolic activation piecewise polynomial activation function suggests model complexity GNN increase rapidly sigmoid tangent hyperbolic activation graph isomorphism graph isomorphic topologically identical  graph GNN embeddings graph identify  WL isomorphism GNNs gcn graphsage incapable distinguish graph structure aggregation function readout function GNN injective GNN powerful WL distinguish graph  invariance GNN equivariant function perform node task invariant function perform graph task node task GNN permutation matrix node GNN equivariant satisfies  QX graph task GNN invariant satisfies  QX achieve  invariance component GNN invariant node ordering theoretically characteristic permutation invariant equivariant linear layer graph data universal approximation  feedforward neural network hidden layer approximate borel measurable function universal approximation capability GNNs seldom cascade correlation approximate function structure output  approximate function preserve unfold equivalence precision node unfold equivalent unfold identical unfold node construct iteratively extend node neighborhood depth ConvGNNs framework message passing universal approximators continuous function define multisets invariant graph network approximate arbitrary invariant function define graph VI graph autoencoders GAEs neural architecture node latent feature decode graph information latent representation GAEs network embeddings generate graph characteristic GAEs summarize brief review GAEs perspective network embed graph generation characteristic GAEs network embed network embed dimensional vector representation node preserve node topological information GAEs network embeddings encoder extract network embeddings decoder enforce network embeddings preserve graph topological information  matrix adjacency matrix earlier approach mainly employ multilayer perceptrons GAEs network embed neural network graph representation  stack denoising autoencoder encode decode  matrix via multilayer perceptrons concurrently structural network embed SDNE stack autoencoder preserve node proximity proximity jointly SDNE proposes loss function output encoder output decoder separately loss function enables network embeddings preserve node proximity minimize distance node network embed network embeddings loss function lst define lst  enc enc sourcewhere enc encoder consists multilayer perceptron loss function enables network embeddings preserve node proximity minimize distance node input reconstruct input concretely loss function lnd define lnd dec enc sourcewhere dec decoder consists multilayer perceptron DNGR SDNE node structural information connectivity node ignore node feature information depicts attribute node GAE leverage gcn encode node structural information node feature information encoder GAE consists graph convolutional layer enc gconv gconv sourcewhere denotes network embed matrix graph relu activation function gconv function graph convolutional layer define decoder GAE aim decode node relational information embeddings reconstruct graph adjacency matrix define dec  sourcewhere embed node GAE minimize negative entropy adjacency matrix reconstruct adjacency matrix simply reconstruct graph adjacency matrix overfitting due capacity autoencoders variational GAE VGAE variational version GAE distribution data VGAE optimizes variational bound logp KL sourcewhere KL kullback leibler divergence function distance distribution gaussian prior aij dec  diag vector ith encoder output define  derive similarly another encoder accord VGAE assumes empirical distribution prior distribution enforce empirical distribution approximates prior distribution adversarially regularize VGAE  employ training scheme generative adversarial network gans gan competition generator discriminator training generative model generator generate fake sample discriminator attempt distinguish fake sample inspire gans  endeavor encoder empirical distribution indistinguishable prior distribution GAE graphsage encodes node feature graph convolutional layer instead optimize reconstruction error graphsage relational information node preserve negative sample loss dec  dec  sourcewhere node node node node node sample negative sample distribution negative sample loss function essentially enforces node representation node dissimilar representation  alternatively local network embeddings capture global structural information maximize local mutual information distinct improvement graphsage experimentally aforementioned essentially network embeddings link prediction however sparsity graph positive node negative node alleviate data sparsity network embed another convert graph sequence random permutation random approach applicable sequence directly graph recursive network embed  assumes node network embed approximate aggregation neighborhood network embeddings adopts memory lstm network aggregate node reconstruction error  define lstm sourcewhere network embed node obtain lookup lstm network random sequence node node input  implicitly learns network embeddings via lstm network lstm network generate network embeddings avoids lstm network invariant permutation node sequence network representation adversarially regularize autoencoders  propose graph encoder decoder framework loss function define  dist dec enc sourcewhere dist distance node embed reconstruct encoder decoder  lstm network random node input   regularizes network embeddings within prior distribution via adversarial training although  ignores node permutation variant lstm network experimental validate effectiveness  graph generation multiple graph GAEs generative distribution graph encode graph hidden representation decode graph structure hidden representation majority GAEs graph generation molecular graph generation practical drug discovery propose graph sequential manner global manner sequential approach generate graph propose node  model generation representation molecular graph  cnns rnns encoder decoder respectively domain specific alternative applicable graph iteratively node graph criterion satisfied generative model graph  assumes probability graph sum node permutation sourcewhere denotes node capture complex joint probability node graph  generates graph sequence decision namely node node node node decision generate node node graph graph update  another  proposes graph rnn rnn model generation node graph rnn node node sequence rnn binary sequence connection node node previously generate sequence global approach output graph graph variational autoencoder  model existence node independent random variable assume posterior distribution define encoder generative distribution define decoder  optimizes variational bound   KL sourcewhere gaussian prior learnable parameter  encoder multilayer perception decoder  output generate graph adjacency matrix node attribute attribute challenge global generate graph graph connectivity validity node compatibility regularize   imposes validity constraint  regularize output distribution decoder molecular gan  integrates  gans reinforcement objective generate graph desire  consists generator discriminator compete improve authenticity generator  generator propose fake graph along feature matrix discriminator aim distinguish fake sample empirical data addition reward network introduce parallel discriminator encourage generate graph posse accord external evaluator  combine lstms wasserstein gans generate graph random approach  generator plausible random lstm network enforces discriminator identify fake random training graph derive normalize occurrence matrix node compute random generator brief sequential approach linearize graph sequence lose structural information due presence cycle global approach graph scalable graph output GAE vii spatial temporal graph neural network graph application dynamic graph structure graph input STGNNs occupy important capture dynamicity graph category aim model dynamic node input assume interdependency node traffic network consists sensor distance sensor traffic adjacent spatial dependence perform traffic forecasting STGNNs capture spatial temporal dependency graph simultaneously task STGNNs forecasting future node label predict spatial temporal graph label STGNNs direction rnn cnn rnn approach capture spatial temporal dependency filter input hidden recurrent graph convolution illustrate suppose rnn   sourcewhere node feature matrix insert graph convolution becomes gconv gconv sourcewhere gconv graph convolutional layer graph convolutional recurrent network  combine lstm network ChebNet diffusion convolutional rnn  incorporates propose diffusion graph convolutional layer gru network addition  adopts encoder decoder framework predict future node another parallel node rnns rnns handle aspect temporal information structural rnn proposes recurrent framework predict node label comprises rnns namely node rnn rnn temporal information node node rnn rnn respectively incorporate spatial information node rnn output rnns input assume rnns node significantly increase model complexity instead split node semantic node semantic rnn model computational rnn approach suffer consume iterative propagation gradient explosion vanish issue alternative cnn approach tackle spatial temporal graph nonrecursive manner advantage parallel compute stable gradient memory requirement illustrate cnn approach interleave cnn layer graph convolutional layer temporal spatial dependency respectively assume input  tensor RT cnn layer slide along axis aggregate temporal information node graph convolutional layer operates aggregate spatial information  integrates convolutional layer ChebNet gcn layer construct spatial temporal stack gate convolutional layer graph convolutional layer another gate convolutional layer sequential ST gcn composes spatial temporal convolutional layer  layer previous predefined graph structure assume predefined graph structure reflect genuine dependence relationship node however snapshot graph data spatial temporal latent static graph structure automatically data realize graph WaveNet proposes adaptive adjacency matrix perform graph convolution adaptive adjacency matrix define  softmax relu eet sourcewhere softmax function compute along dimension denotes source node embed denotes target node embed learnable parameter dependence source node target node complex cnn spatial temporal neural network graph WaveNet performs without adjacency matrix latent static spatial dependency researcher discover interpretable stable correlation entity network however circumstance latent dynamic spatial dependency improve model precision traffic network traffic  employ attention mechanism dynamic spatial dependency rnn approach attention function update node node input  spatial attention function temporal attention function latent dynamic spatial dependency temporal dependency cnn approach drawback latent spatial dependency calculate spatial dependence node application graph structure data ubiquitous GNNs variety application summarize benchmark graph data evaluation source implementation respectively detail practical application GNNs various domain data mainly sort data namely citation network biochemical graph social network others VI summarize benchmark data detail supplementary VI summary benchmark data evaluation source implementation node classification graph classification task ass performance RecGNNs ConvGNNs node classification node classification standard split valid benchmark data cora citeseer pubmed ppi reddit report average accuracy data multiple summarization experimental supplementary necessarily rigorous comparison identify pitfall evaluate performance GNNs node classification valid split throughout underestimate generalization error employ training technique hyperparameter tune parameter initialization rate decay relatively comparison refer reader graph classification graph classification researcher adopt tenfold validation model evaluation however experimental setting ambiguous unified across concern usage data split model selection versus model assessment encounter external fold model selection risk assessment reference GNNs standardize uniform evaluation framework apply external tenfold CV estimate generalization performance model inner holdout technique training validation split model selection alternative procedure external fold model assessment inner fold model selection refer reader detailed rigorous comparison GNN graph classification source implementation facilitate baseline research supplementary hyperlink source implementation GNN model review article noticeably publish geometric library pytorch pytorch geometric implement GNNs recently graph library DGL release implementation GNNs popular platform pytorch mxnet practical application GNNs application across task domain despite task handle category GNNs directly node classification graph classification network embed graph generation spatial temporal graph forecasting graph related task node cluster link prediction graph partition address GNNs detail application research domain computer vision application GNNs computer vision scene graph generation classification action recognition recognize semantic relationship facilitates understand meaning visual scene scene graph generation model aim parse image semantic graph consists semantic relationship another application inverse generate realistic image scene graph parse semantic graph promising synthesize image textual description classify enable lidar device surround environment lidar scan reference convert graph  graph ConvGNNs explore topological structure identify action video facilitates understand video content machine aspect detect location joint video clip joint link skeleton naturally graph series joint location apply STGNNs action moreover applicable direction GNNs computer vision interaction shot image classification semantic segmentation visual processing application GNNs processing text classification GNNs utilize interrelation document infer document label despite data exhibit sequential internal graph structure syntactic dependency syntactic dependency defines syntactic relation   propose syntactic gcn cnn rnn encoder syntactic gcn aggregate hidden representation syntactic dependency apply syntactic gcn task neural machine translation adopt model handle semantic dependency graph graph sequence learns generate meaning semantic graph abstract abstract meaning representation propose graph lstm encode graph semantic information apply GGNN graph sequence neural machine translation inverse task sequence graph generate semantic knowledge graph useful knowledge discovery traffic accurately forecasting traffic volume density traffic network fundamentally important smart transportation address traffic prediction STGNNs traffic network spatial temporal graph node sensor instal distance node node average traffic within dynamic input feature another industrial application taxi demand prediction historical taxi demand location information data feature incorporate lstm cnn network embeddings joint representation location predict taxi demand location within interval recommender graph recommender item user node leverage relation item item user user user item content information graph recommender quality recommendation recommender importance item user cast link prediction predict link user item propose GAE ConvGNNs encoders combine rnns graph convolution underlie generates rating chemistry chemistry researcher apply GNNs graph structure compound compound graph node chemical bond treat node classification graph classification graph generation task target molecular compound graph molecular fingerprint predict molecular infer protein interface synthesize chemical compound others application GNNs limited aforementioned domain task exploration apply GNNs variety program verification program social influence prediction adversarial attack prevention electrical health model brain network detection combinatorial optimization IX future direction GNNs proven graph data challenge exist due complexity graph future direction GNNs model depth neural architecture however performance  dramatically increase graph convolutional layer graph convolution representation adjacent node closer theory infinite graph convolutional layer node representation converge strategy graph data scalability tradeoff scalability GNNs gain price corrupt graph completeness sample cluster model lose graph information sample node influential cluster graph deprive distinct structural tradeoff algorithm scalability graph integrity future research direction  majority GNNs assume homogeneous graph directly apply GNNs heterogeneous graph node node input image text therefore developed handle heterogeneous graph dynamicity graph dynamic node disappear node input graph convolution adapt dynamicity graph although dynamicity graph partly address STGNNs perform graph convolution dynamic spatial relation conclusion article conduct comprehensive overview GNNs taxonomy GNNs category RecGNNs ConvGNNs GAEs STGNNs thorough review comparison summarization within category introduce application GNNs data source code model assessment GNNs summarize finally future direction GNNs