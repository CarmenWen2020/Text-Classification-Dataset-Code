extreme machine elm emerge algorithm generalize hidden layer feedforward neural network hidden node parameter randomly generate output analytically compute however due shallow architecture feature elm effective signal image video hidden node address issue elm hierarchical framework propose multilayer perceptron propose architecture component taught feature extraction supervise feature classification bridge random initialize hidden novelty unsupervised multilayer encode conduct feature extraction elm sparse autoencoder developed via constraint achieves compact meaningful feature representation elm exploit advantage elm random feature mapping hierarchically encode output randomly project decision generalization faster unlike greedy layerwise training DL hidden layer propose framework manner previous layer establish layer fix without tune therefore efficiency DL extensive various widely classification data propose algorithm achieves faster convergence exist hierarchical furthermore multiple application computer vision confirm generality capability propose scheme introduction extreme machine elm become increasingly significant research topic machine artificial intelligence due unique characteristic extremely training generalization universal approximation classification capability elm effective hidden layer feedforward network SLFNs demonstrate excellent accuracy various application classification image segmentation action recognition unlike traditional algorithm propagation BP neural network nns vector machine svm parameter hidden layer elm randomly generate tune hidden node establish training sample acquire theoretically SLFNs randomly generate hidden neuron output tune regularize maintain universal approximation capability without update parameter hidden layer addition regularize elm faster quadratic program standard svm gradient traditional BP nns elm tends achieve faster generalization performance nns svm recently elm extensively remarkable contribution theory application universal capability elm extend kernel elm suitable feature mapping classical incremental elm propose hidden node incrementally output analytically elm transform online sequential version data input chunk chunk data fix chunk extend elm semisupervised unsupervised task manifold regularization unlabeled partially label sample cluster elm however aforementioned issue scene visual acoustic signal practical application image classification recognition elm variant mainly focus classification however feature classification conduct application multilayer usually attempt develop multilayer architecture elm autoencoder building input decompose multiple hidden layer output previous layer input autoencoder simply stack layer layer hierarchical structure supervise optimization encode output directly fed layer decision without random feature mapping framework exploit advantage elm theory demonstrate universal approximation capability elm cannot guaranteed without random projection input therefore obvious potential multilayer implementation elm fully deployed research elm multilayer perceptron mlp badly faster training feature classification performance meanwhile another trend hierarchical DL nns dnns similarly architecture extract feature multilayer feature representation framework layer abstract information concept DL BP multilayer network unsupervised initialization instead conventional random initialization DL considers multilayer unsupervised initialization initialization entire network BP nns layer cod mention hidden parameter DL framework tune multiple entire training DL cumbersome consume analysis exist algorithm elm DL MLPs cannot achieve excellent generalization performance inspire mlp theory extend elm propose hierarchical elm elm framework MLPs propose elm improves performance elm maintain advantage training efficiency contribution propose elm autoencoder developed via norm optimization unlike exist autoencoders DL BP nns input hidden node bias propose elm autoencoder establish random elm theory random feature mapping almost nonlinear piecewise activation function universal approximation capability important information exploit hidden layer feature representation norm elm autoencoder penalty apply generate sparse meaningful hidden feature elm framework propose effective efficient mlp propose elm consists component unsupervised multilayer feature encode supervise feature classification feature representation extraction propose elm sparse autoencoder utilized layer stack architecture autonomous subsystem submodule feature classification obtain feature scatter random matrix elm apply decision elm theory dimensional nonlinear transform extract feature improve feature classification precision moreover greedy layerwise DL framework feature extraction classification autonomous propose elm framework parameter tune entire combine feature extraction classification training faster traditional BP DL demonstrate advantage propose elm framework elm feature extraction classification algorithm developed practical computer vision application detection recognition obtain promising confirm generality capability elm remainder organize II introduces related fundamental concept theory elm describes propose elm framework related elm sparse autoencoder IV performance elm elm relevant mlp algorithm various data application incorporate propose elm detection gesture recognition finally conclusion drawn VI II related elm extend MLPs facilitate understand propose algorithm briefly review related concept theory elm fundamental capability elm elm autoencoder elm theory suppose SLFNs hidden node equation  sourcewhere denotes hidden node activation function input vector input layer hidden layer bias hidden layer output additive node activation function define sourceand radial basis function rbf node activation function define source SLFNs approximate continuous target function compact subset random initialize adaptive rbf node function compact subset dimensional euclidean integrable inner define source norm denote closeness network function target function distance source theorem bound  piecewise continuous function span dense target function function sequence randomly generate continuous sample distribution limn probability output ordinary minimize  theorem randomly generate network output maintain universal approximation capability activation function  piecewise span dense theorem elm establish described detail elm algorithm accord theorem elm built randomly initialize hidden node training training data vector target sample denotes hidden node unlike traditional algorithm related refer elm theory aim training error norm output minimize sourcewhere hidden layer output matrix randomize matrix sourceand training data target matrix   source elm training algorithm summarize randomly assign hidden node parameter input bias additive hidden node calculate hidden layer output matrix obtain output vector sourcewhere moore penrose generalize inverse matrix orthogonal projection efficiently calculation MP inverse hth HT hth nonsingular HT hth hht nonsingular accord ridge regression theory positive diagonal hth hht calculation output accord resultant equivalent elm optimization stable generalization performance improve stability elm HT hht sourceand correspond output function elm HT hht sourceor hht  sourceand correspond output function elm hht  source elm autoencoder apart elm SLFNs elm theory apply autoencoder mlp  autoencoder function sort feature extractor multilayer framework encode output approximate input minimize reconstruction error mathematically autoencoder input data representation latent representation deterministic mapping parameterized activation function matrix bias vector latent representation mapped reconstruct vector input randomly mapped output latent representation easily elm autoencoder reconstruction regard elm obtain regularize optimization however due penalty elm extract feature elm autoencoder tend dense redundancy sparse prefer propose algorithm propose elm framework MLPs overall architecture propose elm introduce detail elm sparse autoencoder utilized elm elm framework propose elm built multilayer manner unlike greedy layerwise training traditional DL framework elm training architecture structurally phase unsupervised hierarchical feature representation supervise feature classification former phase elm autoencoder developed extract multilayer sparse feature input data latter elm regression perform decision propose elm algorithm overall framework elm phase multilayer encode elm regression implementation elm autoencoder layout layer inside elm detailed description elm advantage exist DL multilayer elm ML elm algorithm unsupervised feature input raw data transform elm random feature exploit hidden information training sample layer unsupervised perform eventually obtain sparse feature mathematically output hidden layer sourcewhere output layer output layer denotes activation function hidden layer output hidden layer elm independent module function feature extractor layer increase feature becomes compact feature previous hidden layer extract parameter hidden layer fix tune totally exist DL framework hidden layer unsupervised initialization retrain iteratively BP nns training elm faster DL unsupervised hierarchical training elm resultant output layer HK feature extract input data classification randomly perturbed utilized input supervise elm regression obtain network perturbation HK random projection input maintain universal approximation capability elm conceptually expedite propose elm framework developed upon random feature mapping fully exploit universal approximation capability elm feature feature classification accord theorem random mapped feature input output hierarchical network approximate classify input data theory worth mention elm differentiates exist ML elm scheme threefold ML elm stack layer layer architecture straightforward replacement DL autoencoder elm autoencoder contrast propose elm considers mlp comprehensive network subsystem unsupervised feature extraction representation supervise feature classification respectively random projection feature extraction input feature classification subsystem instead norm elm autoencoder ML elm penalty apply elm obtain compact sparse hidden information orthogonal initialization ML elm avoid orthogonal constraint reasonable input node output elm sparse autoencoder introduce previous propose elm briefly consists unsupervised supervise training supervise training implement elm focus unsupervised building autoencoder elm architecture II autoencoder aim function hidden bias autoencoder approximate input data reconstruct output input universal approximation capability elm exploit autoencoder moreover sparse constraint upon autoencoder optimization therefore elm sparse autoencoder implementation elm sparse autoencoder demonstrate unlike autoencoders BP nns traditional DL algorithm input propose elm sparse autoencoder establish random elm theory demonstrate training elm random mapped input efficient approximate input data autoencoder concept elm autoencoder initialize tune addition generate sparse compact feature input optimization perform establishment elm autoencoder elm autoencoder propose norm singular calculate feature representation optimization model propose elm sparse autoencoder denote equation  sourcewhere input data denotes random mapping output hidden layer obtain exist DL algorithm usually encode output adjust iteration optimization however propose autoencoder utilize random mapping hidden layer feature representation data random initialize output optimize furthermore improve training accuracy hereinafter optimization algorithm optimization representation rewrite function sourcewhere penalty training model iterative shrinkage thresholding algorithm FISTA adopt FISTA minimizes smooth convex function complexity denotes iteration implementation detail FISTA calculate lipschitz constant gradient smooth convex function iteration initial  source compute iterative manage perfectly recover data corrupt resultant propose autoencoder inner input feature reflect compact representation data addition autoencoder adopt building propose elm feature representation generate layerwise comparison addition exist elm autoencoder simply input output traditional optimization data recovery application reduce neural node improve elm IV performance evaluation analysis extensive conduct verify effectiveness efficiency propose elm framework elm exist relevant mlp algorithm simulation hardware software laptop intel 4G cpu 6G ddr ram matlab comparison elm elm selection hyper parameter demonstrate selection hyperparameters elm elm scheme traditional nns training algorithm parameter chosen training elm elm II user specify parameter parameter regularize calculation hidden node simulation accuracy elm elm subspace parameter prefixed elm convergence elm accuracy performance tend stable meanwhile performance elm sensitive parameter elm increase accuracy elm slightly increase rapidly convergence performance elm accuracy subspace elm elm accuracy elm accuracy elm accuracy elm accuracy elm accuracy curve elm accuracy curve elm mention impact performance elm accuracy curve smooth jitter increase impact performance accuracy curve facilitate understand accuracy curve elm elm respectively performance elm elm affected parameter practically carefully node affect trend accuracy curve performance accuracy mention elm briefly consists component unsupervised feature supervise feature classification difference elm elm elm feature classification elm hierarchical training obtain multilayer sparse representation input raw data elm scheme raw data regression classification generally compact feature remove redundancy input improve overall performance classification accuracy elm elm elm layer feature extraction perform feature randomly project elm classification elm randomly mapped raw data input classification comparison feature classification stage hidden node elm elm addition elm elm sigmoid activation function widely binary multiple data respectively average obtain comparison performance comparison binary multiple benchmark easily propose elm remarkable improvement elm data accuracy hierarchical feature encode classification performance elm indeed enhance elm input elm feature classification data compact sparse elm sparse autoencoder elm generate performance robust feature extract data comparison mlp algorithm complicate data image patch verify performance elm DL algorithm stack auto encoders sae stack autoencoder sda belief network DBN boltzmann machine dbm mlp BP ML elm fully mlp MLPs denote data preprocessing technique data augmentation avoid mainly focus verification capability mlp training scheme BP mlp training algorithm sae sda DBN dbm mlp BP initial rate decay rate epoch pretraining tune epoch respectively besides input corruption rate sda dropout rate hidden layer penalty parameter layer ML elm respectively  mixed national institute standard technology mnist handwrite data consists training image image sample digit pixel grayscale uniform background digit unique mnist ideal data effectiveness elm image patch input training algorithm without preprocessing accuracy mlp  II consume mlp training propose elm achieves accuracy faster training II comparison accuracy mnist data NORB demonstrate advantage elm another conduct nyu recognition benchmark NORB data complicate mnist NORB contains image toy generic image obtain viewpoint various training contains stereo image per contains remain image propose elm achieves accuracy training mlp training algorithm consistent mnist comparison accuracy NORB data performance analysis elm obviously outperforms layer elm classification accuracy furthermore mlp training elm promising performance extremely faster training mlp training algorithm training epoch iteration performance computer elm easily utilized laptop addition mention implementation elm matlab script therefore efficiency improve orient program technique cpp code graphic processing server application demonstrate generality capability elm framework computer vision application detection recognition concept elm application elm feature extraction classification elm scheme relevant DL sda elm straightforwardly replace sda hardware software environment remain IV detection detection classical application computer vision simulation illinois   data training various training image background image contains image detect roughly pixel training image propose elm detection algorithm slide extract fix image patch grouped input elm training training sample training image normalize histogram equalize avoid uneven intensity elm network structure layer feature hidden node elm feature classification elm detection approach IV recall error rate  recall precision propose elm exist IV propose detection achieves performance worth remove background sample exist adopt preprocessing detection contrast elm detection perform without additional preprocessing achieves performance furthermore training elm DL sda EER IV detection performance comparison performance comparison elm DL sda detection gesture recognition gesture recognition important topic computer vision due application computer interface interpretation visual surveillance cambridge gesture data consists image sequence gesture primitive primitive contains image sequence illumination background sequence fix camera roughly isolated gesture gesture sample difference image cambridge gesture data difference image leftward gesture cambridge gesture data video sequence resize pixel implementation frame sequence initialization frame correspond frame resultant difference combine fuse image obtain difference image information partially exclude illumination difference image directly fed elm classifier recognize gesture moreover image centre extend data random skew version image randomly pixel horizontal vertical direction data augment recognition robust invariant shift setting propose elm sparse autoencoder feature stack twice representation output initialize elm classifier elm hidden node simulation mention data  validation elm framework perform achieves average performance accuracy performance VI vii propose achieves traditional handcraft feature extractor data analysis unlike elm unsupervised hierarchical feature extractor obtain feature representation notable improvement accuracy addition elm outperforms DL sda training accuracy VI gesture recognition performance comparison vii performance comparison elm DL sda gesture recognition online incremental online incremental refers aim adapt representation reflect appearance target appearance specify target variation deformation apart extrinsic illumination camera viewpoint occlusion inevitably appearance variation model appearance variation propose elm novel incremental unlike conventional online robust feature extract elm classification performance probability model conventional addition demonstrate previous training elm mlp architecture DL sda ensures elm obvious advantage elm framework training incremental online update elm variant online sequential elm OS elm apply feature classification update algorithm summarize image patch sample around target frame image positive training sample image negative denotes euclidean distance target sample location parameter setting elm classifier obtain image sample frame apply feature extract elm sparse autoencoder feature classification perform frame OS elm location maximum response target location training sample update sample update OS elm elm online framework update frame frame adopt sample scheme compressive CT popular online approach elm initial label frame obtain maximum classification response simulation feature extraction feature update classification performance elm CT sda david trellis video sequence location error elm CT sda error elm CT sda elm fluctuation frame comparison location error elm CT sda data david indoor trellis moreover scene obtain elm CT sda david indoor data location elm accurate CT sda CT sda variation elm stable trellis video sequence CT sda lose target illumination obvious elm robust besides advantage training elm framework elm achieves frame sda frame performance comparison elm CT sda david indoor trellis data elm CT sda VI conclusion propose novel mlp training scheme universal approximation capability elm propose elm achieves representation layerwise encode outperforms elm various simulation moreover mlp training training elm faster achieves accuracy verify generality capability elm practical computer vision application thesis application elm function feature extractor classifier achieves robust performance relevant