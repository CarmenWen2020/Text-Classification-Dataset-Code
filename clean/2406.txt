multiagent reinforcement MARL extensively application tractable implementation task distribution automaton classify MARL category independent learner obtain optimal joint action equilibrium automaton advantage automaton agent action agent automaton structure easy implement automaton apply function optimization image processing data cluster recommender wireless sensor network however automaton algorithm propose optimization cooperative stochastic propose algorithm automaton optimization cooperative agent LA OCA automaton applicable cooperative task transform environment model introduce indicator variable maximal reward obtain zero otherwise theoretical analysis strict optimal joint action stable critical model LA OCA cooperative arbitrary finite player action simulation LA OCA obtains pure optimal joint strategy rate cooperative task outperforms algorithm introduction reinforcement RL prevalent markov decision mdps agent agent setting receives numerical reward environment execute action reward improve behavior obtain maximal reward multiagent setting central becomes infeasible exponentially joint action limited multiagent reinforcement MARL advantage tractable implementation task distribution MARL algorithm independent learner agent action agent MARL algorithm reduces communication agent alleviates joint action automaton intelligent computation entity determines optimal action action execute environment responds automaton numeric reward automaton reward update strategy automaton gradually learns strategy obtain desire response environment automaton algorithm automaton algorithm classify MARL category independent learner automaton advantage automaton merit independent learner automaton reinforcement signal environment automaton mathematics operation suitable application fourth automaton structure easy implement automaton variety application function optimization image processing data cluster recommender wireless sensor network however automaton algorithm propose optimization cooperative stochastic propose MARL algorithm automaton optimization cooperative agent LA OCA contribution LA OCA algorithm mathematic operation benefit strategy update automaton transform environment model reinforcement signal model environment zero introduce indicator variable maximal reward obtain zero otherwise analyze model LA OCA algorithm cooperative arbitrary finite player action strict optimal joint action stable critical LA OCA algorithm obtains rate cooperative stochastic load balance task distribute sensor network dsn task robot task besides LA OCA algorithm MARL algorithm remain organize II review related MARL algorithm introduces background stochastic notation article automaton IV elaborates LA OCA algorithm theoretically analyzes model cooperative efficacy LA OCA algorithm cooperative task load balance task dsn task robot task VI summarizes conclusion II related MARL algorithm MARL algorithm limited application valuable theoretical inspiration multiagent stochastic MARL algorithm stochastic extend algorithm category MARL algorithm cooperative goal obtain maximal immediate reward joint action learner  agent construct opponent model observation agent action immediate reward frequency maximum  maximal reward frequency obtain maximal reward update agent strategy explore action accord ratio EAQR frequency obtain maximal reward update agent strategy policy gradient potential PGP potential gradient performance index update agent strategy model EAQR PGP cooperative arbitrary finite agent action analyze respectively category MARL algorithm sum algorithm concern equilibrium obtain infinitesimal gradient ascent   wolf  generalize  giga giga wolf agent gradient immediate reward update strategy  wolf  apply agent action whereas giga giga wolf agent action independent  extension dynamic  boltzmann exploration greedy exploration agent action analyze respectively linear reward   automaton algorithm  converges nash equilibrium NE strict pure NE exists MARL algorithm stochastic MARL algorithm stochastic variety application traffic robotic navigation optimal consensus resource management goal depends task category MARL algorithm aim obtain optimal joint strategy cooperative task optimal adaptive  agent model agent model optimal action virtual construct stage stochastic frequency maximum reward  agent frequency obtain maximal reward update strategy without observation action agent probability maximal reward estimate gradient ascent pmr EGA agent gradient global cumulative reward update strategy gradient information estimate function joint action actor critic apply cooperative task counterfactual multiagent coma centralize critic network decentralize actor network optimize agent strategy multiagent deterministic policy gradient  decentralize critic network agent centralize consensus optimization leader follower multiagent mdps actor critic network deterministic policy gradient realize optimal consensus controller leader follower multiagent RL optimize controller multiagent model dynamic graphical validate effectiveness leader synchronization policy consensus optimize MDP agent partial category MARL algorithm aim obtain NE wider variety task nash quadratic program obtain  avoid function nash agent previous reward action agent negotiation  efficient MARL algorithm without function  avoids expensive computation mixed  pure strategy  proposes  negotiation pure strategy avoid function agent policy wolf  policy gradient ascent approximate policy prediction pga app policy learner  gradient information update agent strategy exponential average  employ exponential average mechanism greedy action agent update agent strategy simulation  outperforms pga app  situation concern convergence NE  lag anchor  extension  apply stochastic simulation converge NE grid NE obtain consensus algorithm propose trigger asynchronous cellular automaton algorithm facilitate convergence NE multiagent ding consensus gradient obtain NE agent action agent access LA OCA algorithm belongs category MARL algorithm characteristic LA OCA algorithm alleviates exponentially joint action  coma  pmr EGA LA OCA algorithm belongs independent learner agent action agent  EAQR pmr EGA   memory  EAQR pmr EGA maintain auxiliary variable estimate probability obtain maximal reward agent LA OCA algorithm benefit strategy update automaton memory mathematics operation preliminary notation introduces preliminary stochastic notation article automaton stochastic stochastic finite agent available action joint action consist action agent joint action transition occurs conditional probability exists satisfies joint action deterministic stochastic transition agent gain numerical reward accord reward function cooperative stochastic sum agent immediate reward goal agent maximize cumulative reward     sourcewhere discount factor emphasizes importance reward shortly stateless agent selects action accord strategy receives numerical reward payoff matrix transition repetition stateless obtain maximal reward agent improves strategy obtain strategy probability distribution action pure strategy assignment probability action mixed strategy assignment action probability totally mixed strategy assignment action positive probability joint strategy compose strategy agent joint strategy pure compose pure strategy totally mixed compose totally mixed strategy payoff matrix cooperative agent action agent receives identical reward agent collaborate obtain maximal reward within parenthesis optimal joint strategy agent action action optimal joint action pure exists pure optimal joint strategy cooperative payoff matrix cooperative agent action notation agent available action fix define indicator function  max sourcewhere action agent reward obtain max maximal reward agent payoff matrix strategy agent probability vector agent selects action probability pij pure strategy agent define vector component others zero joint strategy denote source define function   max agent strategy agent action   sourcewhich conditional probability obtain maximal reward agent selects action joint strategy define function max joint    sourcewhich conditional probability obtain maximal reward joint strategy   sourcein cooperative denotes sum agent reward max denotes maximal sum agent reward define function sourcefor cooperative definition cooperative optimal joint action strict agent source automaton accord category reward environment classify model model model model environment return reward zero failure reward model environment finite interval reward model environment interval model model appropriate sum model automaton obtain NE classical automaton algorithm  agent update strategy accord pig pig bri pig   pij bri pij aij aig sourcewhere aij denotes action agent rate reward agent algorithm converge NE model appropriate optimization performance index cooperative task model deterministic stochastic obtain maximum cumulative reward reward agent maximum cumulative reward obtain zero otherwise model desire situation IV LA OCA algorithm formulation LA OCA algorithm propose MARL algorithm LA OCA obtain maximal sum agent reward transform environment model define indicator variable max otherwise sourcewhere denotes sum agent reward max denotes maximal sum agent reward pseudocode LA OCA algorithm algorithm agent update strategy accord update pig pig bci pig aig pij pij bci pij aij aig sourcewhere rate strict optimal joint action stable critical infinitely actual meanwhile probability action strictly zero guarantee joint action algorithm LA OCA algorithm initialize strategy agent uniform distribution agent action accord agent agent reward max max update accord agent predefined played return agent analysis LA OCA algorithm LA OCA algorithm sourcewhere denotes joint strategy joint action obtain reward denotes update specify define function source accord rate infinitely model LA OCA algorithm ordinary differential equation   joint action max becomes constant namely maximal reward payoff matrix specifically component equation probability action agent  aij max  pij aij max pij lij pij aiw max  pij aiw max   pij  pij  pij pij  lij  pij lij sourcewhere equation theorem characteristic model LA OCA cooperative theorem model LA OCA algorithm cooperative strict optimal joint action locally asymptotically stable critical proof component action aik optimal joint action  besides probability action  pij lij optimal joint action strict optimal joint action critical strict optimal joint action lyapunov theory analyze stability strict optimal joint action perform transformation define pij   sourcethen critical correspond pij aij independent variable  hij sourcewhere taylor expansion notation define hij fij  aij  lyapunov function aij sourcewe aij idp  aij aij source definition aij  aij obtain sufficiently neighborhood around origin strict optimal joint action stable critical theorem model LA OCA algorithm cooperative optimal joint action strict mixed joint strategy unstable critical critical proof suppose agent strict optimal joint action  aji denote agent component action optimal joint action pji denote probability agent action aji strict optimal joint action exist action component action strict optimal joint action accord probability decrease zero component action strict optimal joint action action probability agent define pmi joint strategy converge unique optimal joint action situation critical satisfy pji  source   mixed joint strategy component strategy pure namely pji suppose critical accord  pji  sourcewhich contradictory critical pure component strategy action probability action zero probability agent satisfies mixed joint strategy critical otherwise critical examine stability critical perform transformation accord critical analyze define pji pji    pji pji sourcethe action probability agent rewrite pji    derive  matrix rmn critical verify trace sum eigenvalue determinant transformation extract factor determinant eigenvalue negative eigenvalue exist positive eigenvalue exists critical unstable LA OCA cooperative stochastic cooperative stochastic LA OCA algorithm agent update strategy accord pig pig bci pig aig spij pij bci pij aij aig sourcewhere agent selects action pij max otherwise sourcewhere cumulative reward agent max maximal cumulative reward agent pseudocode algorithm introduce threshold parameter prevent premature convergence algorithm appropriate  probability action upper bound bound agent satisfied algorithm LA OCA algorithm stochastic initialize strategy agent rate threshold agent action accord agent agent immediate reward tuple agent episode agent experienced episode evaluate related tuples max max update accord IV IV action aij pij pij pij pij action normalize  agent predefined episode played return agent empirical efficacy LA OCA algorithm verify empirical algorithm employ cooperative scenario scenario optimal joint action strict scenario optimal joint action strict algorithm MARL algorithm cooperative task load balance task dsn task robot task task discrete discrete joint action finite load balance task robot task deterministic stochastic whereas dsn task deterministic stochastic load balance task dsn task episode random whereas robot task episode comparison algorithm pmr EGA EAQR cooperative task wolf   baseline converge optimal NE cooperative task payoff matrix generate accord optimal joint action randomly joint action optimal joint action obtains maximum reward joint action obtains random reward within average strategy agent initialize randomly strategy agent becomes pure component strategy pure action probability converge joint action optimal successful scenario agent action optimal joint action rate rate fail max actual maximal reward payoff matrix insufficient exploration premature convergence countermeasure LA OCA algorithm stochastic rate optimal joint action strict scenario agent action optimal joint action joint action optimal optimal joint action strict setting scenario rate obtain II II rate optimal joint action strict task load balance goal load balance task distribute agent evenly minimal polygon vertex solid vertex occupy agent hollow vertex occupy agent polygon agent desire vertex agent evenly distribute polygon agent constitute task agent agent task contains absorb agent action clockwise  joint action agent execute action simultaneously collision occurs agent unoccupied vertex agent direction agent occupy vertex agent successfully collision occurs episode random agent evenly distribute elapse agent evenly distribute agent receives reward otherwise agent receives reward load balance task algorithm discount factor LA OCA algorithm rate sourceand threshold sourcewhere episode episode pmr EGA rate function joint action rate agent sample joint action initialize agent action initialize coefficient sourcefor EAQR sample rate decrease gradually   sourcewhere initial rate  exploration rate source wolf     average contains episode evaluation episode evaluation episode agent fix strategy selects greedy action independent initial agent episode algorithm fairness algorithm minimal agent input successful episode minimal rate LA OCA algorithm algorithm LA OCA algorithm obtains rate IV LA OCA algorithm obtains minimal average maximal average cumulative reward algorithm pmr EGA exhibit unstable performance whereas EAQR improves slowly increase LA OCA algorithm superior pmr EGA EAQR LA OCA algorithm standard deviation zero minimum episode initial agent rate load balance task IV average standard deviation load balance task average cumulative reward standard deviation load balance task examine strategy obtain LA OCA algorithm perform episode evaluation episode evaluation episode unique proceeds accord joint strategy LA OCA algorithm evaluation episode minimal optimal joint action exists episode optimal joint action exists episode episode alternative optimal joint action agent clockwise agent agent  indicates LA OCA algorithm converge optimal joint action multiple optimal joint action exist redundant action exist episode optimal joint strategy redundant action exist affect maximum cumulative reward illustration evaluation episode load balance joint strategy obtain LA OCA algorithm episode task distribute sensor network goal dsn task capture target gain maximal cumulative reward dsn task contains sensor difficulty task sensor network surround network sensor target wander target target constitute dsn task target cannot perceive sensor unless zero sensor perceive target target available action sensor agent sensor action focus focus joint action dsn sensor target episode target randomly occupy target sensor executes action target focus sensor decrease besides action focus reward action focus reward target target decrease zero capture wipe otherwise random action sequentially sensor involve capture receives reward sensor involve capture maximal index reward episode target capture elapse algorithm discount factor LA OCA algorithm threshold sourcewhere episode episode EAQR  exploration rate pmr EGA wolf     algorithm perform consists episode evaluation episode maximal cumulative reward minimal obtain sensor focus target sensor focus evaluation episode successful cumulative reward obtain within rate obtain evaluation episode VI rate wolf   zero LA OCA obtains rate infer vii LA OCA optimal strategy EAQR obtains average cumulative reward average indicates joint action focus pmr EGA redundant action focus task minimal IX LA OCA algorithm reliable algorithm VI rate dsn task vii average cumulative reward standard deviation dsn task average standard deviation dsn task IX minimal cumulative reward dsn task maximal dsn task examine strategy obtain LA OCA algorithm perform episode evaluation episode dish target target target focus sensor target eliminate joint strategy sensor obtain cumulative reward eliminate target illustration strategy obtain LA OCA algorithm capture target episode task robot robot task introduce goal robot without collision shortest pas robot task contains robot increase difficulty robot coordinate initial distance robot robot action directly joint action coordinate robot constitute task robot action simultaneously collision occurs distance robot robot collides robot receives reward robot leaf receives reward otherwise robot obtains reward episode robot elapse diagram robot task algorithm discount factor LA OCA algorithm rate threshold   sourcewhere initial threshold  pmr EGA EAQR wolf     rate average average cumulative reward XI respectively average consists episode evaluation episode evaluation episode successful cumulative reward obtain LA OCA algorithm convergence algorithm LA OCA pmr EGA obtain rate EAQR achieve performance episode experienced neither wolf   learns optimal joint strategy pmr EGA performs poorly average cumulative reward converge collision evaluation episode converges optimal joint strategy cumulative reward indicates collision occurs xiv XV EAQR obtains cumulative reward collision occurs robot XI rate robot task xii average standard deviation robot task average cumulative reward standard deviation robot task xiv maximal robot task XV minimal cumulative reward robot task examine strategy obtain LA OCA algorithm perform consists episode evaluation episode episode perform evaluation episode evaluation episode diagram cannot judge collision occurs solid robot dot joint action robot selects action robot remain action collide agent parenthesis robot robot remain action collide robot task multiple optimal joint action exist robot action analyze obtain strategy obtain strategy obtain cumulative reward without collision LA OCA algorithm variety quality strategy illustration strategy obtain LA OCA algorithm  robot episode VI conclusion article proposes automaton algorithm LA OCA optimize cooperative stochastic strict optimal joint action stable critical model LA OCA cooperative arbitrary finite agent action optimal joint action strict mixed strategy obtains maximal global reward probability critical unstable critical empirical evidence LA OCA converges optimal joint action exploration stochastic LA OCA obtains optimal strategy task rate outperforms algorithm future stability remain critical model LA OCA convergence stochastic