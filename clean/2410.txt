reinforcement safety constraint promising autonomous vehicle various failure disastrous loss policy constrain optimization algorithm average constraint return function action predefined bound however exist algorithm capture via multiple precision sensor complicates hardware consume article focus planning stability guarantee autonomous vehicle limited risk identification lyapunov function integrate actor critic sac algorithm borrowing concept lyapunov function theory policy theoretically guarantee trajectory novel risk sensitive algorithm stability guarantee propose policy planning autonomous vehicle policy implement differential vehicle simulation environment experimental propose algorithm achieves introduction classical planning algorithm artificial potential rapidly explore random rrt rrt successfully apply autonomous vehicle manipulator  algorithm however difficulty dimensional planning recent research trend apply machine planning particularly reinforcement RL achieve advance robotics robotic manipulator autonomous vehicle simulation application numerous successful report planning autonomous vehicle RL abovementioned utilize deterministic policy gradient DDPG policy however actor critic sac algorithm achieves decent performance exist RL rarely utilized planning autonomous vehicle addition aforementioned algorithm utilized autonomous vehicle safety topic importance RL exists impressive progress decade application RL autonomous vehicle RL algorithm apply planning autonomous vehicle policy sensitive risk associate optimal policy perform poorly due uncertain behavior obstacle essential policy aware risk accurately timely planning currently address capture trajectory obstacle image information robot equip multiple sensor performance computational however implement autonomous vehicle computation capability exist RL risk reduction broadly classify category risk sensitive criterion constrain criterion criterion risk sensitive criterion policy strike balance reward reduce risk constrain criterion aim maximize return constraint criterion goal attain policy tolerate scenario although aforementioned category utilized planning autonomous vehicle criterion apply due challenge scenario training policy ensure safety autonomous vehicle planning apply constraint action typically formulation RL constraint constrain markov decision MDP  framework discount sum safety bound however safety evaluation metric  policy optimization criterion cannot guarantee inside confine safety variance policy violation safety constraint probability furthermore robustness presence environment uncertainty perturbation objective RL maximize return agent temporally undesirable action agent dangerous zone circumstance autonomous vehicle return ability recover formulate stability loop important theory however policy stabilizes challenge model RL manner motivate observation article novel algorithm lyapunov sac collision probability prediction LSAC cpp algorithm propose planning autonomous vehicle sac proven improve capability exploration propose algorithm defines risk accord probability collision policy furthermore article generalizes definition stability theory RL borrows concept lyapunov function guarantee stability specifically risk sensitivity parameter rsp introduce strike balance maximize return minimize cumulative risk data driven generalize lyapunov function introduce analyze stability contribution article summarize approach propose predict probability collision utilizes sequence action practical algorithm policy planning autonomous vehicle novel propose construct lyapunov function model framework ensure safety notation article denotes dimensional euclidean refers euclidean vector norm denote action autonomous vehicle relative target denote ptx  sparse finding lidar data denote denote expectation policy denotes trajectory distribution policy denote dataset unsafe unsafe II preliminary planning planning autonomous vehicle formulate optimal decision goal maximize return Î³tr target policy generates action observation   mindt  sourcewhere threshold collide obstacle threshold target  target autonomous vehicle respectively collision avoidance constraint goal constraint kinematics autonomous vehicle stability theory uniformly ultimately bound uub stability various stability particularly useful uub stable ultimate bound exist positive constant est classical definition uub stability generally trajectory norm expectation eventually inside safety  define euclidean norm  uub stable ultimate bound autonomous vehicle remain recover remain generally classical definition uub stability limit application  task autonomous vehicle safety define euclidean norm define distance unsafe nonlinear metric kinetic representation consists velocity therefore article extends classical definition uub stability introduces definition uub stability safety exist positive constant   task uub stable safety extend definition uub stability illustrate satisfy uub stability safety implies autonomous vehicle accidentally disturbed risky return within finite demonstration uub stability domain RL markov decision RL machine sequential decision unknown transition dynamic sequential decision model MDP MDP denote action transition probability distribution reward function distribution initial policy mapping specifies distribution action function action function policy define safety threshold detail MDP relate RL formulation collision avoidance action action construct linear angular velocity vmax wmax vmax wmax maximum linear angular velocity respectively construct sparse finding action relative target reward function reward encourage autonomous vehicle target   mindt  sourcewhere relative distance autonomous vehicle target hyperparameter robot arrives target distance threshold positive reward  acquire contrast robot collides obstacle minimum distance robot obstacle constant negative reward  acquire otherwise reward difference distance hyperparameter distance target transition probability autonomous vehicle action transfer probabilistic transition model kinematics define denote function objective optimal function  sourcewhere discount factor risk sensitive RL risk sensitive RL  goal autonomous vehicle     mindt  sourcewhere rsp refers risk allows desire risk policy implies risk seek preference policy implies risk aversion preference policy implies risk neutrality refers risk autonomous vehicle environment probability collision minimum distance obstacle probability unsafe article risk define probability collision RL stability guarantee article lyapunov function utilized stability guarantee theory stability analysis controller mostly exploit along deterministic probabilistic model lyapunov function positive definite function exploit lyapunov function ensure difference derivative lyapunov function along trajectory  definite direction decrease lyapunov function eventually converges origin sublevel lyapunov function lyapunov data driven propose analyze stability theorem safety constraint exists function positive constant   sourceand  sourcewhere NT average probability policy uub stable safety ultimate bound proof theorem discussion explanation theorem decrease lyapunov decrease lyapunov fairly parameterizations lyapunov function confine sum quadratic polynomial extensively  positive definite matrix lyapunov function effectively semidefinite program solver addition sum safety finite chosen lyapunov function  principle function evaluates horizon performance contrary choice function approximation converges considerably agent suffer behavior algorithm novel algorithm LSAC cpp propose  mention II architecture propose algorithm describes strategy acquire probability collision algorithm sac cpp combine probabilistic prediction strategy sac policy apply obstacle avoidance scenario autonomous vehicle finally propose algorithm LSAC cpp sac cpp lyapunov critic function policy stability guarantee architecture LSAC cpp policy navigate differential vehicle target planner virtual environment sparse finding previous velocity relative target collision probability prediction goal attain probability collision related relative distance obstacle action comparison neural network parameter output collision probability   sourcewhere fully neural network finding concatenation action collision label  collision episode false otherwise neural network binary classification network generally entropy chosen loss function network classical loss function binary classification accord loss function training neural network   sourcewhere label tuples neural network training tuples  sample randomly dataset   label agent false  label label false increase positive sample  label   sample tuple label episode positive sample utilized regulate conservatism collision probability prediction model assign tuples label unsafe model output probability collision collision appropriate neural network predict probability collision actor critic collision probability prediction actor critic collision probability prediction sac cpp algorithm sac outperforms series RL continuous benchmark action network agent navigate target maximize ahead action outline algorithm sac cpp aim maximize trajectory return entropy action distribution minimize cumulative risk purpose recall optimal objective optimal objective TE  sourcewhere hyperparameter entropy algorithm sac cpp algorithm restore probability prediction model initialize replay buffer randomly initialize initialize target network iteration sample accord sample obtain  update risk sensitive reward crt  tuple gradient  fori   recall  II risk define probability collision risk reward reward redefine  source update critic network function parameter critic network minimize bellman residual JQ  sourcewhere  sourcethe optimal objective function optimize stochastic gradient descent    sourcethe function update network verify improve performance RL algorithm initial parameter update algorithm update actor network actor network sac cpp parameter sac approximate expression objective optimal function est  sourcethe gradient approximate      sourcewhere input vector initial parameter update gradient descent algorithm training policy practical RL algorithm stability guarantee II sac cpp policy RL algorithm LSAC cpp propose policy algorithm capable posse sample efficiency policy DDPG policy actor critic capable continuous task lately propose actor critic maximum entropy framework outperforms DDPG policy gradient series complex task recall  II function define max mindt source maximum entropy minimum risk actor critic framework lyapunov function utilized critic policy gradient formulation LSAC cpp lyapunov critic function satisfies  objective function ED    sourcewhere parameterized neural network input vector consist gaussian objective function positive lagrangian multiplier tune policy entropy safety constraint adjust gradient replay buffer storage  tuples lyapunov critic function replay buffer  sample tuples pseudocode LSAC cpp algorithm algorithm LSAC cpp algorithm restore probability prediction model input hyperparameters rate   parameter regard uub theorem randomly initialize critic network lyapunov critic network actor parameter  lagrangian multiplier initialize parameter target network  iteration sample accord sample update risk sensitive reward crt  crt crt  update sample minibatches tuples  update lagrangian multiplier update target network replacement   update detail function lyapunov function refer appendix remark propose algorithm policy RL algorithm inherits advantage sac algorithm proximal policy optimization sample efficiency performance superior due introduction maximum entropy IV accuracy prediction collision rate propose algorithm evaluate respectively illustrates identify risk environment outdoor simulation environment rate propose algorithm sac training finally propose algorithm performance  generalization performance experimental setup gym gazebo simulation software chosen environment testify propose algorithm software combine ROS gazebo OpenAI gym transfer agent virtual environment outdoor environment construct gym gazebo policy navigates autonomous vehicle random target laser sensor scan  differential robot physical platform validation obstacle environment assume static virtual training environment construct gym gazebo outdoor environment obstacle built agent equip lidar agent target brick stationary obstacle obstacle randomly initialize episode accord mechanical characteristic robot bound action chosen vmax wmax rad episode autonomous vehicle randomly target initialize randomly positive axis outdoor environment episode autonomous vehicle collides obstacle prevent agent local optimization policy desktop machine equip intel cpu GB memory nvidia geforce RTX gpu GB memory sac sac cpp LSAC cpp algorithm implement tensorflow python training maximum episode evaluation collision probability prediction model probability collision predict neural network input vector transfer collision probability fully neural network layer node sigmoid function activation function layer satisfy probability constraint  network structure RL collision probability prediction layer dimension activation function dense layer fully neural network generate dataset training probability prediction model action autonomous vehicle generate randomly independent obstacle observation collision label training dataset tuples label mention   label ÎT ÎT policy become conservative vice versa training tuples batch rate prevent model overfitting technique introduce training loss accuracy collision probability prediction model mention dataset generate policy DDPG dataset contains tuples output input tuple recognize accuracy arrives maximization around training loss collision probability network performance collision probability prediction model evaluation sac cpp sac sac cpp respectively network structure algorithm hyperparameter batch rate initialize    training rate decay factor ensure actor network converges policy performance training policy sac cpp collision probability sac sac cpp attains average rate sac training performance sac cpp training evaluate average rate calculate episode average rate sac cpp around episode sac approximately training sac cpp calculates probability collision treat risk consequently policy sac cpp generates safer action achieves rate average probability collision probability scenario random target axis target axis average probability average rate sac sac cpp average rate sac cpp  comparison  mention II rsp negative policy tends risk aversion article negative rsp chosen policy implement risk aversion action average rate sac cpp  risk consideration absolute rsp vice versa zone various initial target randomly initialize rsp autonomous vehicle obstacle policy respectively policy obtains rate training policy policy attains rate stage evaluation LSAC cpp performance LSAC cpp evaluate sac sac cpp  sum safety episode utilized safety goal suppress zero zero violation constraint sac sac cpp restrict trajectory policy within constraint sum safety trajectory likely dangerous zone consequence LSAC cpp global sac sac cpp convergence propose algorithm quickly converges policy maintain average return sac sac cpp infers propose algorithm rate average return sac sac cpp LSAC cpp shade SD confidence interval random axis indicates conclusion article planning algorithm autonomous vehicle developed combine utilizes neural network predict collision probability sac sac propose algorithm considers probability collision reward strike balance maximize return minimize cumulative risk training addition lyapunov introduce ensure safety algorithm propose directly raw data action propose algorithm sac implement differential vehicle gym gazebo respectively experimental demonstrate advantage propose algorithm rate collision avoidance performance average return future apply propose algorithm navigation planning autonomous vehicle uncertain dynamic environment appendix lyapunov actor critic collision probability prediction function article define  sourcewhich augments standard function entropy policy function update minimize ED  source implementation function target network construct accordingly objective function actor gradient estimator function implementation obtain ED           source policy gradient compose gradient estimate function sample replay buffer estimate lyapunov critic sample buffer  task violation constraint happens agent implies randomly initialize policy content  hardly grows update overcome inconvenience initial objective function zero  tuples data efficient endanger agent another policy gradient estimate function useful mitigate performance degradation bias estimation lyapunov critic objective function  sourcewhere  approximation target function lyapunov function  sourcewhere target lyapunov critic function typically actor critic structure parameter update exponentially average hyperparameter lagrangian multiplier adjust gradient ascent maximize objective  source