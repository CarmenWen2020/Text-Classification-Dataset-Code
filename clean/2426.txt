feedforward neural network approach differential equation however reliability accuracy approximation delicate issue fully resolve literature computational approach highly dependent variety computational parameter choice optimisation structure function intention towards resolve issue fundamental stiff ordinary differential equation model damped computational approach differential equation neural classic actual trial define function recent construction function related trial setting easily apply generally partial differential equation detailed computational identify preferable choice parameter illuminate observable neural network simulation overall extend literature obtain useful accurate neural network approach illustrate importance careful choice computational setup introduction differential equation engineering useful description physical phenomenon usually formulate initial boundary boundary obtain specific useful approximate differential equation numerical finite difference neural network apply variety neural network architecture immense already classic feedforward neural network proven useful differential equation within framework feedforward neural network denote simply neural network approach investigate literature within decade promising trial TS propose approximation differential equation abbreviate tsm TS neural neural network output satisfy initial boundary construction latter multiple TS approximation differential equation recently systematic construction approach TS propose however latter TS construction become realise complex approach propose TS structure recent legendre neural network evident investigation useful context extension recently publish approach propose avoid TS intricate ingredient tsm correspond motivate technically related tsm modify trial mTSM instead building function TS impose differential equation approximate function neural network output directly latter satisfy initial boundary construction tsm additional function mention tsm mTSM proven capable ordinary ODEs partial differential equation PDEs ODEs PDEs demonstrate complex simulation potential obtain quality motivate detail computational issue arise application mention recent complementary activation function computational despite promising development related tsm mTSM emphasis laid construction principle application propose  context however comparison tsm mTSM quality related computational aspect stress context mainly network architecture elaborate TS mTSM construction detail computational characteristic trivial define computational framework competitive contribution upon parameter extend investigation variance ordinary differential equation approximation tsm mTSM apparent difference proceed previous conference extend investigation training detail evaluation additional meaningful concerned role neural network initialisation hidden layer hidden layer neuron perform variety parameter related differential equation neural network optimisation stress amount parameter differential equation neural network optimisation numerous contribution variation initialisation hidden layer neuron hidden layer training epoch stiffness parameter domain optimisation mutual dependence non trivial task meaningful proceed account latter aspect evaluation carefully chosen respect related investigate computational characteristic important stiff ode model equation damp behaviour stability reliability another contribution detailed influence stiffness parameter ode behaviour resolve instance parabolic PDEs neural network architecture neural network usually neuron standard neural network architecture consists layer feature input layer neuron denotes domain bias neuron offset hidden layer neuron linear output layer neuron hidden layer hidden layer neuron architecture extend neuron neuron layer vector input layer domain data hidden layer processing data output layer generate neural network output detail hidden layer receives sum  input data sigmoid activation function output layer consists linear neuron neural network output generate linear combination  sigmoid activation function continuous arbitrarily differentiable function neural network architecture usually feature input layer hidden layer sigmoid activation function output layer image differential equation neural network important activation function continuously differentiable later approach activation function derivative optimisation another differentiation universal approximation theorem hidden layer finite sigmoidal activation function approximate continuous function subset initialise random therefore computation return random compute loss function optimisation random output optimisation return update computation exactly computational parameter random initialisation another option initialisation constant computation return therefore update constant computation parameter return supervise input data discrete domain grid output data function chosen norm unsupervised output data function model approach latter approach trial construction tsm mTSM detail approach neural network ordinary differential equation ODEs  initial boundary denotes function independent variable although denotes ode ordinary partial differential equation PDEs ODEs PDEs trial tsm recall approach satisfy initial boundary TS construct satisfy therefore sum suppose satisfy initial boundary initial boundary construct become zero eliminate impact TS define differential equation satisfy mention choice determines impact domain TS transforms  partial derivative trial respect input   generate training neural network  domain uniform grid grid discrete domain unconstrained optimisation function  modify trial mTSM propose introduces TS directly differential equation therefore satisfy initial boundary construction function additional  denote initial boundary model function optimisation respect adjustable neural network optimisation function minimisation gradient descent commonly employ optimisation technique backpropagation function gradient respect neural network influence update backpropagation enables local minimum training usually training iteration input data epoch training efficient training minimum epoch training perform kth epoch backpropagation momentum update  neural network output derivative expression correspond derivative gradient function compute          momentum momentum parameter impact epoch reduce stuck training local minimum saddle rate factor gradient influence update approach constant rate cBP prevent optimiser oscillate around minimum employ variable rate vBP alternative approach rate exist opt employ linear decrease model  initial rate rate epoch cap adam adaptive estimation adaptive optimisation estimation  variance gradient detail advantage adam potential achieve rapid training backpropagation gradient uniformly direction adam computes individual rate approach parameter variation optimisation adam backpropagation model  homogeneous ordinary differential equation ode  model stiff phenomenon involve damp mechanism numeric error subsequent diagram define norm difference correspond trial trial tsm propose construct function  mTSM trial function subsequent respect meaningful variation computational parameter parameter abbreviation computational setting define fortran implementation neural network approach optimiser propose correspond without library therefore computation perform investigation related aspect code subsequent cBP instead vBP reduce amount parameter rate cBP optimisation comparison vBP adam parameter employ addition average graph trend reduce influence fluctuation otherwise subsequent computational parameter fix hidden layer hidden layer neuron maximal epoch ğ‘˜max domain data stiffness parameter abbreviation concern stress independent consequently argumentation enables reduce freedom choice computational setting clarify influence individual computational parameter demonstrate achieve tractable important approach meaningful initialisation illustrates difference initialisation employ ğ©initconst ğ©initrnd average iteration display graph depict rnd imply axis perform computation overlaid random perturbation random initialisation around average important mention iteration ğ©initrnd exactly parameter setup return initialisation variation orange solid ğ›¥ğ‘¢const dot rnd image comment choice ğ©initconst evidently fix document detail zero suitable generic choice mTSM document tsm cBP adam illustration return helpful ğ©initconst parameter setup tsm ğ©initconst uniformly error depict orange solid increase training mTSM overall clearly ğ©initconst adam ntD stable demonstrate mTSM ğ©initconst adam solver desirable proceed virtually independently training ğ©initrnd adam solver tsm reasonable numerical error stable error behaviour adam mTSM trend ğ©initrnd initialisation around zero comment illustration behaviour ğ©initconst ğ©initrnd around zero reasonably cBP conjecture ODEs constant initialisation random fluctuation around suitable choice ğ©initconst ğ©initrnd important subsequent consequence initialise ğ©initconst zero ğ©initrnd random hidden layer neuron behaviour ğ›¥ğ‘¢const rnd increase hidden layer neuron rnd average computation hidden layer neuron hidden layer neuron variation orange solid ğ›¥ğ‘¢const dot rnd image almost difference tsm saturate behaviour previous focus random initialisation setup desirable neuron mTSM hidden layer neuron increase accuracy adam training explore  training  hidden layer neuron freedom introduce neural network relatively amount training cBP saturation error affected increase amount hidden layer neuron trend rnd slightly accuracy saturation visible already neuron generally clearly benefit introduce neuron significant compute numerical error consequence investigation employ hidden layer neuron previous justified stable amount computational hidden layer focus impact hidden layer neuron hidden layer constant employ neuron plus additional bias neuron layer previous rnd average iteration hidden layer useful ğ›¥ğ‘¢const tsm increase training ntD hidden layer approximation highly beneficial influence important aspect investigation distinguish increase hidden layer respect individual mTSM tsm mTSM layer sufficient obtain adam optimisation accurate convenient tsm namely increase hidden layer accuracy gain ğ©initconst ğ©initrnd latter surprising universal approximation theorem imply hidden layer experimentally accurate approximation function recall context increase neuron hidden layer saturation accuracy ğ©initrnd improvement increase neuron ğ©initconst reasonable ğ©initconst combination hidden layer plus significant improvement consequence investigation hidden layer computation tsm accuracy gain hidden layer epoch aim investigate fix maximal training epoch convenient relates bound computational load employ training cycle convergence training function increase maximal epoch ğ‘˜max addition illuminate influence training maximal epoch variation orange solid ğ›¥ğ‘¢const dot rnd image precisely increase ğ‘˜max average iteration ğ‘˜max meaning axis entry ğ‘˜max relates correspond optimisation neural network ğ©initrnd convergence behaviour evaluate average computation ğ©initrnd return mTSM adam ğ©initrnd  ğ©initconst  tsm  adam optimiser clearly saturation regime convergence tsm mTSM ğ©initrnd cBP ğ›¥ğ‘¢const rnd decrease ğ‘˜max evaluate however employ cBP constant rate decrease rate training saturation regime however adam rnd fluctuate behaviour convergence regime non average computation ğ©initrnd satisfy cBP optimiser tsm mTSM minor fluctuation approximation however tend ntD context author employ epoch investigation correspond suppose convergence regime conclusion ğ‘˜max suitable obtain useful approximation stiffness parameter domain investigate behaviour respect choice stiffness parameter investigation domain informally parameter impact trend evaluate stiffness parameter variation orange solid ğ›¥ğ‘¢const dot rnd image domain variation orange solid ğ›¥ğ‘¢const dot rnd image influence domain increase ntD objective interval computation  interval increase experimental rnd average iteration domain tsm cBP  displayed rnd visualise reality nan average iteration diverge domain furthermore accuracy tsm cBP strictly decrease domain saturates unstable increase training   iteration diverge another increase  enlarges unstable stabilisation relation roughly formulate relation domain  factor conjecture  neuron layer convenient consequence fix computation optimisation adam cBP vBP optimisation tsm mTSM ntD computational parameter fix hidden layer hidden layer neuron ğ‘˜max non average compute parameter setup initialisation optimiser comparison orange solid ğ›¥ğ‘¢const dot  image optimiser comparison orange solid ğ›¥ğ‘¢const dot  image optimiser comparison quantitative data  previous conclusion tsm combination ğ©initconst unstable chosen parameter setup therefore evaluate tsm refer non average numeric error  ğ©initrnd evaluation tsm adam almost visible difference   difference approximation  tend contrast difference approximation tsm cBP grows magnitude training simultaneously accuracy approximation increase vBP efficiency adaptive highly dependent model parameter however combination  vBP tsm reveal minimum away approximation minimum training increase  however another increase  stabilises addition  approximation tsm vBP conjecture critical training initialisation adequate ntD mTSM adam ğ©initconst useful ğ›¥ğ‘¢const gain accuracy ntD ğ©initrnd approximation throughout  however compute around reasonable accuracy peak accuracy increase training   accuracy former overall become mTSM cBP behaviour ğ›¥ğ‘¢const similarly mTSM adam become slightly accurate ntD however initialisation cannot compete combination mTSM adam mTSM vBP displayed stable ntD contrast tsm vBP  behaves computation mTSM similarity overall behaviour  tsm cBP increase ntD slightly approximation difference approximation grows discus stochastic quantity ğ©initrnd related focus random initialisation diagram constant initialisation return numerical error computation optimisation sufficiently meaning analyse data regard adam overall choice however tsm  cBP almost adam vBP outperform adam specific vBP tsm   limited approximation contrast tsm adam dominates mTSM without exception former statement however partially standard deviation tsm favour cBP adam vBP  pas downwards exclude vBP   standard deviation acceptable standard deviation suitable evaluate stability reliability however specify reliability numerical error approximation nonetheless define threshold justification discussion neural network behave standard numerical algorithm   quantiles account percentage specifies relative amount data quantile although minimum tsm vBP useful quantiles setting cBP situation mTSM adam outperforms cBP vBP context therefore adjust optimisation parameter vBP perform cBP however questionable perform adam quantiles adam optimisation adam reliable optimiser conclude overall performance related numeric error mTSM adam ğ©initconst ğ©initrnd although tsm vBP stability flaw ntD stabilises  overall vBP cBP cannot compete adam mTSM conclusion future stiff model ode feedforward neural network reliability depends variety parameter initialisation influence initialisation zero reasonable approximation tsm hidden layer capable reasonably mTSM random adam mTSM although training yield suitable overfitting resolve employ neuron adjustment future however indicates investigate issue package investigate aspect evaluate completely independent detailed investigation individual aspect dominates overall accuracy reliability tend favour combination adam mTSM computationally orient research approximation initialisation future research theoretical sensitivity trial tsm goal context decrease variation increase accuracy moreover investigate network significant accuracy gain remind classic numerical analysis furthermore future differential equation focus initial improvement constant initialisation