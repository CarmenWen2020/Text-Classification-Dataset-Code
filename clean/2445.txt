recently outstanding technique series forecasting TSF propose technique sufficient data sample predictor active AL algorithmic framework vector regression SVR TSF goal valuable sample reduce complexity training evaluate quality sample comprehensively multiple essential criterion informativeness representativeness diversity cluster consecutive stage procedure addition imbalance series data seriously extremely important user unreasonable assign prediction sample address imbalance multiple criterion sensitive active algorithm virtue SVR architecture abbreviate maw SVR hoc imbalanced TSF propose introduce sensitive scheme sample endow penalty dynamically update AL procedure experimental comparison maw SVR AL algorithm thirty series datasets verify effectiveness propose algorithm introduction dynamic data analysis series forecasting TSF extensive attention recent decade series obvious nonlinear non stationary characteristic prediction inaccurate generally series classify category stationary non stationary statistic sequence   series joint statistical distribution sequence expectation variance identical series strongly stationary definition extremely strict application hence weaker definition namely weak stationarity analyze series weak stationary series series invariant variance translation requirement stationarity weak stationarity satisfied contrary datasets non stationary implies series evolve addition series data ubiquitous everyday urban stock price internet traffic data analyze forecasting series data become research traditional statistical model TSF autoregression integrate average ARIMA differencing transformation training technique transform non stationary series stationary differencing loss valuable information data integer however ARIMA fractional differencing capable model behavior series notwithstanding statistical model suitable model series complex nonlinear feature limitation practical application recent machine theory gradually apply TSF task vector machine svm artificial neural network anns evolutionary computation anns recurrent neural network rnns maintain TSF due excellent ability function approximation capacity addition rnns capture temporal dependency series due recurrent structure nevertheless anns shortcoming apt local optimization longer training rate hidden layer model inevitably affect generalization ability anns however issue svm structural risk minimization criterion svm posse nonlinear expressiveness geometric interpretation widely apply classification vector regression SVR implementation svm predictive data analysis advantage svm SVR largely depends ability capture nonlinear characteristic moreover SVR effectively dimensional complex regression promising TSF additionally obtain quality data sample training model amount data training information delivers prediction model redundant unnecessary model prone overfitting insufficient training sample decrease model performance underfitting phenomenon address issue active AL apply valuable sample situation AL attempt iteratively sample pool unlabeled sample accord query strategy optimizes training minimum valuable sample AL previous focus research classification recently approach AL address regression issue AL quality sample avoid redundant unnecessary label information core AL algorithm query strategy AL regression greedy sample GS propose aim achieve diversity input output query sample author improvement query criterion beneficial sample multiple essential criterion informativeness representativeness diversity propose pool sequential AL regression enlighten remarkable AL regression introduce AL TSF series data positive significance improve prediction performance multiple criterion AL framework virtue SVR architecture specifically TSF construct primary motivation SVR solid theoretical basis generalization ability dimensional input training sample besides SVR ability automatically extract sample namely vector procedure AL summary inherits merit AL reduce complexity training sample meanwhile obtains accurate predictor superior performance predictor sample demonstrate conduct nevertheless practical classification performance AL easily disrupt due imbalance similarly imbalance series data affect performance propose AL algorithm solar irradiance analysis location sunny  scarce however  usually attract attention user conclusion imbalanced TSF user uneven preference across target variable domain prefer prefer associate extreme rare valuable accurate prediction accurate prediction extreme meteorology avoid catastrophic consequence imbalance advocate prediction series data differentiate meaningful user prediction penalty effective sensitive AL algorithmic framework multiple criterion active SVR maw SVR hoc imbalanced TSF propose depicts diagram maw SVR loop firstly inspire sensitive SVR predictor endow sample initial training data input SVR obtain vector SVs non vector non SVs secondly cluster algorithm apply unlabeled sample training sample non SVs cluster propose consecutive stage procedure valuable sample jointly evaluate multiple criterion finally valuable sample remove unlabeled pool insert training sample ensures duplicate sample cluster AL satisfied contribution summarize diagram propose maw SVR algorithm image firstly series data conductive prediction introduce AL TSF sample selection strategy construct AL algorithmic framework acquire valuable sample reduces complexity training exhibit promising performance secondly attempt quality sample criterion informativeness representativeness diversity SVR ability automatically sample criterion  implement SVR cluster thirdly application AL regression AL algorithmic framework TSF adapt peculiarity series data imbalance imbalance brings serious sample bias undesirable performance AL alleviate prediction series data differentiate instead meaningful combine sensitive scheme AL procedure imbalanced TSF task related however almost research AL TSF discus related AL regression afterward review related data imbalance active regression inspire superior performance AL classification task AL regression recent AL selects significant sample label accord criterion various criterion generate AL algorithm greedy sample GS technique diversity selects unlabeled instance greedy away previously label instance popular AL approach query committee QBC calculates informativeness instance disagreement predictor moreover algorithm informativeness model maximization  aim instance model mention AL approach leverage criterion accomplish sample selection however worth multiple criterion combine query strategy algorithm informative density ID informativeness representativeness propose informativeness sample accord QBC informativeness average similarity unlabeled sample AL algorithm regression jointly mention criterion multiple criterion active  developed framework SVR roughly cluster apply unlabeled instance non vector informative sample novel define representative diverse instance relevant cluster mention approach significant difference proposal firstly AL proposal integrates essential criterion informativeness representativeness diversity beneficial sample unlabeled sample criterion reflect trait sample criterion utilized sample selection reasonable richer assessment criterion quality sample  integrates multiple criterion superior performance regression construct AL algorithmic framework hoc TSF appropriately transform introduce  framework differentiate prediction due imbalance series sensitive scheme  integrate AL procedure virtue SVR architecture data imbalance data imbalance extensively various research imbalance resample distribution sample across achieve balance another research sensitive utilize matrix sample balance sample however predominantly focus classification task classification task imbalance exists TSF affect prediction performance occurs particularly important user alleviate author attempt decrease skewness data leverage resampling strategy distribution rare normal addition sensitive effectively imbalance mainly apply classification task resampling tend exclude important information datasets probability suffer overfitting minority instance sample repeatedly hinder therefore defect resampling attempt employ sensitive scheme strategy improve prediction performance imbalanced TSF issue specifically although algorithm focus average behavior data aim minimize argue mispredicted instance preview SVR construct AL algorithmic framework insensitive SVR training   sample correspond target sample dimension goal SVR function acquire mapping feature dimensional    sample dimension mapped dimensional mapping function bias acquire optimal function minimization    denotes sample model training training sample function accuracy nevertheless practical application improve generalization ability error tolerable hence margin propose optimization evolves         slack variable distance training outside insensitive regularization parameter adjust smoothness tolerance empirical error accord lagrange duality rewrite dual            lagrange multiplier     kernel function SVR described linear expansion kernel function   bias calculate      methodology purpose construct AL procedure extract valuable sample accurate TSF model task TSF assume equidistant sample series signal available sample generally correlation successive series model correlation consists previous series predictor future AL procedure introduce SVR establish nonlinear mapping relationship predict historical TSF express   denotes function fitting function emphasize ahead series prediction focus attention generally aim predict series forecasting fashion therefore predict assume  available likewise predict timestamp assume  available hence formulate pool unlabeled sample  accord sample  series signal correspond target sample label AL procedure TSF task assign discrete label sample classification task truth sample elaborate sensitive SVR imbalanced TSF guideline parameter afterward detailed procedure AL exist expound SVR sensitive imbalance series guarantee fairness sample selection AL address issue propose SVR incorporate sensitive scheme TSF SVR adopt balance sample matrix sample constrain optimization devise        mispredicted parameter  correspond dual rewrite            hence SVR acquire guideline parameter regard parameter novel update guideline firstly resort automatic technique propose model preference function domain continuous target variable preference express minimum maximum preference respectively specifically plot statistic assume distribution crucial statistic continuous target variable median quartile adjacent      target variable   extreme adjacent regard threshold extreme median regard centrality  correspond maximal preference minimum preference respectively statistic correspond preference piecewise cubic  interpolation polynomial algorithm derive preference function interpolate preference graphical interpretation involve derivation preference function preference function plot statistic image preference threshold minimum preference target variable valuable threshold quantile formal definition rare prefer  uninteresting      preference extreme meaningful user prediction penalty hence sample insert training parameter update         sample regardless absolute sample category parameter depends ratio sample belonging category uninteresting preference newly sample increase gradually relatively stable fluctuate slightly stage sample selection procedure AL cluster algorithm stage aim quality sample virtue SVR architecture motivation SVR generalization capability relatively limited computational load training phase importantly SVR assist identify relevant sample AL procedure insensitive SVR training  vector SVs  non vector non SVs SVs outside boundary posse informativeness target sample unknown cluster kernel algorithm propose discern relative without target kernel algorithm sample mapped dimensional feature instead input cluster kernel criterion express minimization euclidean distance instance centroid cluster dimensional feature    denotes norm  denotes transformation  dimensional feature  centroid cluster indicator function  sample  cluster  otherwise apply kernel algorithm series datasets traditional cluster algorithm capture complex nonlinear structure series distinguish cluster accurately mapping instance dimensional computes kernel SVR insensitive detailed pseudo code cluster described algorithm firstly sample randomly assign cluster indicator function  minimize distance training sample  centroid  cluster expand                kernel function denotes sample cluster realizes distance calculation dimensional kernel computation  explicit expression  distance calculation cluster update reassign sample  cluster finally convergence therefore kernel cluster algorithm apply instance  cluster denote obtain cluster sample respect capture accord cluster consecutive stage valuable sample stage dedicate informative instance illustrates procedure simply clearly cluster non SVs remove cluster without non SVs reserve cluster generate    visualization instance dimensional feature distribution unlabeled instance non SVs SVs cluster unlabeled instance non SVs cluster reserve cluster remove cluster non SVs image obviously finally sample  informative combine  define      stage commit instance representativeness diversity  accord density display illustrate stage density  cluster  firstly calculate       instance cluster  extension feature compact cluster instance relatively density visualization instance dimensional feature reserve cluster density chosen instance density cluster image density  cluster  calculate cluster density consideration cluster neglect instance density associate instance feature evaluate specifically density  average distance instance within cluster       therefore instance density cluster extract  choice sample sample distribution cluster identify meanwhile sample chosen cluster sample distribute cluster implicitly sparse feature diverse finally instance eliminate unlabeled submit training accord thumb cluster preset instance detailed iteration maw SVR described algorithm firstly training preference function plot statistic parameter instance calculate secondly training instance correspond parameter instance vector SVs non vector non SVs obtain training SVR thirdly cluster algorithm apply non SVs unlabeled instance cluster non SVs inside neglect unlabeled instance remain cluster regard informative instance finally density cluster estimate cluster density chosen cluster sample density chosen training conclusion AL procedure iterates maximum training instance met procedure instance SVR obtain prediction model mention cluster technique consecutive cluster stage datasets series datasets financial series datasets johnson outdoors inc  dow jones industrial average dji  plc gsk SP plus corporation SP yahoo finance daily price regard forecast target remain datasets series data library  daily maximum australia  hourly interval internet traffic data isp interval daily  monthly england detail information datasets detail datasets addition owe scope dataset attribute dataset normalize min max normalization mapped interval preprocessing stage prevent data overwhelm min max min min max denote minimum maximum series respectively min max normalization linear feature transformation numerical series data without data distribution imbalance series cannot eliminate normalization addition trend variance preprocessed series consistent series non stationarity series maintain setup effectiveness algorithm AL algorithm GS QBC  ID besides role individual maw SVR random sample RSW SVR multiple criterion active SVR comparison RSW SVR adopts sensitive SVR incremental sample extract randomly iteration SVR jointly criterion extract sample without sensitive scheme training instance regard baseline algorithm adopts training sample without sensitive AL procedure algorithm implement python intel core core cpu frequency ghz core GB ram curve AL performance algorithm specific evaluate error RMSE coefficient determination apply estimation measurement prediction error RMSE calculate RMSE   denotes dataset  predict respectively generally RMSE performance performance adopt SVR radial basis function rbf kernel linear kernel SVR parameter penalty factor rbf trial error chosen minimum preference threshold dataset chosen obtain   datasets usually percentage sample important percentage rare datasets acceptable trial error conduct instance iteration obtain statistically meaningful outcome individual dataset accord initial randomly training initial training compose sample pool dataset chronological training validation validation optimal parameter model verify performance model therefore experimental report performance verify effectiveness algorithm comparison prediction performance curve RMSE AL algorithm datasets varied query accordingly RMSE average axis training instance query correspond algorithm iteration training instance increase AL achieve performance RMSE intuitive training instance usually reliable predictor instance increase maw SVR convergence performance RMSE AL algorithm moreover algorithm achieves performance model instance without exhaust instance practicability AL TSF average RMSE algorithm datasets image average algorithm datasets image curve dji SP daily internet traffic monthly datasets maw SVR RMSE iteration instance algorithm achieves RMSE query instance equivalent performance model training instance however algorithm yield RMSE query instance furthermore algorithm achieves RMSE algorithm training instance satisfies goal AL acquire precise model minimum training instance important advantage easy discover maw SVR RMSE around query instance whereas SVR RMSE query instance algorithm instance  gsk  datasets maw SVR perform stage however query increase maw SVR algorithm achieves decent performance attribute phenomenon training instance preference obtain distribution target variable inaccurate sensitive scheme cannot performance accordingly SVR without sensitive scheme performs maw SVR AL procedure training QBC  commonly AL approach informativeness instance SVR multiple criterion significantly outperforms approach datasets phenomenon attribute query criterion sample selection QBC  relatively multiple criterion selection strategy effective GS considers diversity instance geometric characteristic performs datasets poorly others sample selection GS independent prediction model depends distance measurement sample feature therefore GS robust various data distribution ID AL approach jointly ass informativeness representativeness instance however QBC ID advantage basically equivalent performance QBC series datasets ID cosine similarity estimate density sample significant performance improvement RSW SVR maw SVR obviously advantage indicates multiple criterion selection strategy improves performance faster random selection demonstrates effectiveness consecutive cluster stage procedure SVR maw SVR phenomenon initial stage AL performance SVR maw SVR datasets gradually surpass attribute phenomenon propose guideline parameter due training stage penalty sample obtain distribution target accurate fortunately increase iteration sufficient sample derive preference function allocate appropriate parameter sample finally maw SVR exhaust instance pool achieves optimal performance hence sensitive scheme effective strategy imbalanced TSF maw SVR choice SVR visualization predictive maw SVR benchmark series datasets predictor unlabeled data AL algorithmic framework axis axis indicates correspond sequence accord analysis overall variation tendency propose maw SVR algorithm roughly resemble raw data daily  monthly datasets sequence simulated maw SVR agreement actual data however datasets maw SVR response closely fitting actual data truth prediction propose algorithm visualize image statistical analysis intuitively evaluate quality AL RMSE std standard deviation unlabeled data training respectively consideration std RMSE comparison RMSE std instance datasets comparison std instance datasets meanwhile ascertain difference algorithm significant perform RMSE comparable performance significance displayed boldface indicates item boldface significantly improve predictive performance item normal font significance dataset sample pool training maw SVR obtains standard deviation trial stable standard deviation achieves performance algorithm significance analyze maw SVR achieves performance RMSE AL dji SP daily internet traffic datasets besides SVR performance AL reflect effectiveness sample selection strategy  gsk  monthly datasets maw SVR poorly initial stage however query increase maw SVR achieves decent performance exhaust sample pool suggests sensitive scheme training sample furthermore maw SVR versus algorithm varied instance loss statistic maw SVR algorithm RMSE respectively analyze maw SVR achieves significant improvement comparative approach significance RMSE addition positive trend increase sample frequency statistic gradually increase frequency loss statistic gradually decrease maw SVR achieves performance significance comparative algorithm significantly maw SVR propose sample pool training maw SVR significantly superior algorithm loss statistic loss statistic maw SVR algorithm RMSE instance loss statistic maw SVR algorithm instance  decrease analyze causation RSW SVR remarkable performance performance maw SVR datasets frequency statistic decrease frequency statistic increase attribute phenomenon training sample sensitive scheme crucial role imbalanced series data sample selection obvious conclusion obtain analysis finally confirm propose maw SVR algorithm significantly superior AL algorithm perform series datasets therefore thirty series datasets previous datasets significance apply RMSE maw SVR remain algorithm datasets yahoo finance series data library  respectively unlabeled data training item displayed boldface manifest hypothesis reject newly propose maw SVR significantly superior algorithm significance RMSE propose maw SVR algorithm achieves significant improvement comparative algorithm significance apply dji fvd fvd  series datasets maw SVR algorithm conclusion accord maw SVR outperforms algorithm significantly maw SVR comparative algorithm thirty series datasets comparison average computational iteration algorithm benchmark series datasets RSW SVR usually computation benefiting random selection strategy query selection GS QBC  efficiency due calculation distance ID SVR although multiple criterion AL ID consume datasets due training multiple predictor SVR propose maw SVR sensitive scheme therefore verify considerable performance TSF acceptable computational complexity average conclusion application AL TSF goal quality data sample TSF issue however due nonlinear non stationary characteristic series data TSF challenge task series imbalanced distribution target variable valuable user poorly context AL algorithmic framework goal adapt imbalanced series data namely maw SVR propose approach adopts sensitive prediction besides accord SVR stage sample selection procedure assessment criterion informativeness representativeness diversity extend construct AL algorithmic framework comparison propose maw SVR AL algorithm thirty series datasets verify effectiveness maw SVR summary maw SVR attempt sensitive scheme imbalanced TSF mispredicted instance prediction series data differentiate besides AL SVR conduct effectively extract valuable sample without sample however SVR considers data independent identically distribute limitation propose maw SVR algorithm ignores temporal dependency structure AL procedure important characteristic series data preserve limitation AL algorithmic framework focus future future research seriously effective AL algorithm reasonably account temporal dependency structure series data improve maw SVR keywords series forecasting TSF data imbalance sensitive  active AL