important task processing enables machine understand concisely web expert widely apply various domain assist information seek boost various task demonstrate dramatic performance improvement essential leverage drawn attention academia recent systematic review recent development survey scope datasets application network structure characteristic methodology innovation effectiveness survey contribution summarization recent research progress future direction introduction QA aim precisely instead format query return relevant document QA enables user interact machine classical research processing nlp involve technique analysis retrieval rank QA implement various baseball regard QA recent chatbots siri alexa cortana popular however manually construct text template earlier recent technology remains plenty challenge QA dedicate improve QA DL machine involve architecture drawn attention recent verify various domain paradigm classical machine consists feature engineering machine algorithm effort craft feature largely developed elegant machine plenty training data apply fashion researcher tedious feature engineering moreover DL uncover hidden regard task training downstream task DL versatile framework adopt task yield reasonable therefore innovation DL limited specific application domain network architecture consist multiple layer plenty nonlinear processing capture hierarchical feature concept dimensional vector perform downstream task widely apply various nlp task text classification entity recognition embed recent address detailed discussion survey QA review QA link data document analyze web QA comparison technique web QA datasets evaluation metric   investigate generic QA framework address challenge increase propose improve performance various QA task plenty successfully achieve performance inspire transferable application however lack detailed survey recent architecture adopt QA task moreover QA framework largely depends knowledge source underlie knowledge source QA unstructured text knowledge graph description option hence technique building QA simplify QA task QA task predefined premise investigate validate upon popular datasets gain deeper insight task familiar adopt QA task datasets domain summarize research status contribution analysis recent QA meanwhile discus recent regard task QA classification extraction knowledge generation difference previous survey focus recent QA task addition innovation advantage network structure adopt organize introduces QA review QA technique recent literature objective QA classification extraction knowledge generation describes evaluation metric evaluation datasets summarizes typical QA comparison model performance popular datasets discus challenge trend QA concludes classification classification aim target important role QA efficient classification promote accuracy QA reduce scope capability convolutional neural network cnn extract feature effective classification task framework combine feature engineering cnns  english code mixed script factoid classification involve user generate contract omit punctuation vocabulary feature vector obtain feature engineering concatenate vector representation feature generate convolution operation max pool operation apply capture important feature propose model multiple filter various filter extract feature finally obtain feature fed dense layer output layer acquire classification aim improve text representation convolution propose dependency cnn  model integrate dependency layer cnns model overcame shortcoming cnns insufficiently capture syntactic information inside specifically  dependency layer depth dependency syntax continuous instead directly concatenate vector input mapping reweight vector compositional feature thereafter cnns extract feature vector purpose besides  extend interaction text apply  text classification task duplicate classification task text rank task rely dependency parse  extent robustness dependency parse introduce related involve text chinese english perform classification task user intent classification combine bert capsule network incorporate focal loss motivate effectiveness introduce focal loss image detection imbalance dataset consideration focal loss replace entropy loss function input embeddings token information information information fed stack transformer encoder employ pre model bert capsule network dynamic rout mechanism utilized extract feature obtain encode sequence classification dense operation softmax function conduct upon  stackoverflow   datasets english chinese intent category various topic combine kernel neural network novel architecture kernel architecture  nyström approximation kernel function adapt neural network apply classification conduct   various project nyström embeddings grouped cluster correspond summarize DL structure datasets evaluation metric adopt classification task DL structure datasets evaluation metric adopt classification task extraction extraction related document statement usually involves candidate extraction rank however task challenge usually limited involve multiple entity building framework incorporate architecture enables model without propose architecture gate orthogonal recurrent  apply style rnns  novel rnns model combine ability unitary rnns ability gate rnns effectively forget redundant irrelevant information memory apply structure QA task architecture adopt rnns fed embeddings statement output concatenate fed rnn generation propose model babi dataset verify ability rnns understand logical model description memory network MemNN propose effectiveness research MemNN supervision training phase external memory context inference however MemNN fail datasets memory hence introduce multiscale notion attention promising propose improve novel gate linear  local attention memory network MemNN GL MemNN QA task babi dataset propose extract useful interaction memory introduce local attention mechanism MemNN correlation calculation  optimize update hidden layer MemNN demonstrate MemNN GL outperform MemNN perform QA task babi dataset MemNN easy yang fan propose convolutional memory network  combination convolutional architecture memory network  convolution computation generate vector representation incorporate memory network architecture  efficiently capture clue abstract local information context dynamic memory network DMN potential iterative attention mechanism introduce extract global hierarchical salient feature input utilized feature construct multiple feature propose model enhance understand dynamic memory network  dynamic memory network  global representation gate recurrent neural network salient representation generate another module gate recurrent neural network max pool layer global representation knowledge salient representation suppose extract salient feature babi dataset modification module promote model performance extraction enable model capture variability yield diverse propose module adapts parameter interpretation policy  integrate within document   model training framework complex distribution latent interpretation simultaneously QA training procedure discrete interpretation variable recurrent  adapt interpretation model implement mode update latent distribution reward variational bound effective approach integrate variational inference framework within QA model introduce parameter adaptation conduct squad longer description various truth selection task instead individual candidate separately model context information hierarchical gate recurrent neural network introduce gate mechanism information query framework rnn grus incorporate context independent context dependent conduct selection benchmark datasets  squad extract multi passage task extract framework introduce framework extractor generate candidate selector information candidate structure framework adopt generative adversarial training utilize unlabeled passage model training extractor multi task additional task generative adversarial training extractor discriminator enable backpropagation generative training hybrid predict candidate propose combine boundary content    unfiltered datasets apply propose stack lstm model coattention mechanism extract interaction coattention mechanism output vector lstms calculate affinity matrix affinity correspond softmax apply compute attention concatenate affinity matrix compute context vector representation attentive attention mechanism apply max pool convert output coattention fix vector output representation sum context vector via attention mechanism finally similarity vector representation calculate function combine cosine similarity euclidean distance evaluate trec dataset wiki QA dataset effectiveness propose coattention mechanism model modify apply restrict domain previous framework cnn module stack lstm improve analysis prediction introduce positional cnn cnn model embed positional information layer model enhance text difficulty cnns capture location align influence align text cnn aggregate positional information multiple perspective similarity mapping layer similarity matrix generate project positional information local signal convolution layer sensible convolution filter model various positional information effective detection extraction positional information thereafter local signal aggregate function fully layer finally multiple perspective aggregate layer incorporate positional information multiple perspective generate document  positional information played important role boost performance cnn multiple perspective positional information effective boost neural performance web interaction focus model perform experimentally representation focus model investigate potential benefit attention mechanism introduce attention mechanism feature propose orient feature attention  mechanism apply task machine reading comprehension selection community  reading comprehension model propose machine reading comprehension extract feature fed rnns generate attention extract feature feature fed selection model downstream task model  reading comprehension dataset text associate multiple choice candidate machine comprehension task propose model combine lstm attention mechanism propose hierarchical attention model fed sequence lstm generate representation attention mechanism compute related attention demonstrate apply attention mechanism extract representation summarize DL structure datasets evaluation metric adopt model extraction task DL structure datasets evaluation metric adopt extraction task focus community selection typical paradigm project consistent vector similarity recent community  become topic due flourish online community stackoverflow quora effort fetch address issue measurement similarity similarity task involves candidate retrieval candidate rank simplify classification rank kernel architecture  propose incorporates kernel neural network novel architecture  apply  task semeval task challenge subtask subtask binary classification rank perform rank comment utility propose model perform kernel comment data instance introduce model multi interactive attention network community binary classification enhance performance external knowledge knowledge incorporate capture entity relation representation leveraged categorization model salient information accurately multi interactive attention mechanism apply reduce influence redundant noisy information semeval datasets model achieve performance baseline ben    developed QA basis recognize entailment  perform datasets quora clinical QE semeval task logistic regression LR DL  datasets DL model perform training conduct dataset entailment QA candidate fetch information retrieval model  apply filter non entail rank remain candidate however DL yield framework hybrid combine LR IR propose hybrid attention neural network  lstm cnn selection  hybrid attention mechanism combine local importance mutual importance counterpart representation hybrid attention mechanism informative besides unlike exist model semantic similarity QA model user user generate text alleviate data sparsity user representation explicitly attend informative hybrid attentive user model integrate unified neural network propose model validate semeval task quora dataset user information propose orient feature attention  mechanism apply task machine reading comprehension selection community  reading comprehension model  extract feature fed rnns generate attention extract feature utilized selection model conduct crawl dataset aim rank multi cnns architecture representation chinese medical text avoid potential error propagation derive chinese segmentation propose framework convolutional multiple feature enable model extract semantic information representation propose framework validate newly corpus  chinese medical candidate truth chinese medical dataset  built chinese medical QA framework consists convolutional semantic cluster representation  network MV lstm  framework address chinese segmentation medical text processing  embeddings concatenate matrix convolution operation windowed max pool matrix clinical performance cws conduct serial combination input model fully utilize semantic information propose novel approach apply attention mechanism disease synonym detection chinese medical QA propose model blstm  lstm model symptom frequency attention utilized lin symptom attention mechanism enable model attention symptom description aim relevant exist similarity propose framework heterogeneous social influential network  integrate textual content  social network information quora service twitter user social network framework incorporate random rnns introduce neural network model encode structure feature fusion propose capture semantic eigenvalue integrate multiple sequence encode approach task integrate architecture chinese semantic corpus  public english semantic corpus quora propose framework promising performance   context approach propose  arabic paraphrase detection address issue semantic textual similarity detection arabic wordvec algorithm encode document computational complexity data sparsity reduce  vector representation generate obtain average vector cnn model statistic regularity model document semantic similarity predict  site roy singh machine model predict machine classification cnns lstms aim predict classification category built stack overflow dataset machine balance dataset performance performance various machine community task arabic propose model evaluate semeval task subtask model relevance query DL structure datasets evaluation metric adopt model task DL structure datasets evaluation metric adopt task knowledge knowledge  aim factoid knowledge KBs underlie data source important task QA machine fetch KB semantic mention entity accomplish query knowledge obtain comparable representation underlie knowledge involve technique knowledge graph embed query generation semantic parse directly obtain target KB knowledge semantic parse approach construct query structure explicitly semantic meaning incorporate technique semantic parse explore promising  introduce generate query popular pipeline detect extract entity mention generate query link entity sometimes candidate query generate similarity measurement query query propose sequence mapping executable query firstly candidate query link entity construct lstm model employ query encode propose lstm model context entity relation query encoder encode structure candidate query mixed mode decoder mode generate mode refer mode query generate mode focus semantic correlation refer mode focus correlation variation implement semantic query geographic knowledge chinese lstm crf model multi feature logistic model realize intelligent interaction virtual geographical environment  geographic knowledge graph express model identify geographic entity framework multi feature logistic model link geographic entity resource mapping semantic combination perform generate executable query instead directly embed knowledge transform subgraphs sequence embed convert entity relation knowledge sequence sequence concatenate candidate sequence contextual relation apply relation embed attention mechanism combine candidate sequence enhance candidate sequence aggregation framework introduce compute candidate sequence correspond entity candidate sequence complex involve multiple predicate extend research semantic parse neural network direction propose semantic similarity measurement embed query graph uniform vector query graph generation constraint entity ordinal constraint focus link generation attach entity constraint constraint generation ordinal constraint generation query graph generation query graph split predicate sequence sequence predicate predicate predicate sequence embed obtain semantic component vector representation combine global representation local representation global representation token sequence input grus local representation dependency input another grus cosine similarity calculation max pool operation apply vector semantic representation vector additionally ensemble approach propose enrich link mart linker building mention entity lexicon construct statistical feature fitting link layer linear regression model detailed comprehensive conduct investigate effectiveness module encode query graph transform sequence technique encoder framework adopt obtain representation knowledge representation jointly propose strategy acyclic graph dag embeddings encode information conveyed knowledge joint framework rnns memory network textual structural candidate framework consist module encoder lstm candidate dag generator dag encoder memory network dag generator extract entity extend related entity entity candidate entity generate candidate DAGs subgraphs generate relation related memory network attention layer adopt distributional representation candidate KB substructure propose model advantage complex analysis focus relation knowledge propose attentive recurrent neural network similarity matrix convolutional neural network AR  extend encoder framework remove entity model preserve information entity detection label lstm extend entity mention candidate relation consist relation entity previously obtain relation detection semantic literal semantic relation freebase split encode separately described relationship literal capture similarity meaning expression similarity matrix construct propose extension candidate largely reduce recall novel QA  propose utilized knowledge knowledge graph corpus evidence unify structure interpretation rank response entity  extend  query corpus network qcn query network  query relation network  combination network qcn assign relevance snippet  output compatibility candidate  output compatibility candidate relation combination network qcn   combine multi hop  challenge issue model lengthy relation knowledge reality hop perform unknown motivation relax hop restriction model reduce propose unrestricted hop framework  compatible model propose related hence installation model  framework unrestricted hop relation extraction  decompose subtasks hop relation extraction comparative termination decision  iteration style extract relation transit entity halt relation extraction model classification termination decision treat comparison conduct model additionally  framework dynamic representation adopt regard information already previous relation subtasks jointly performance model within  framework comparable independent unrestricted relation hop reduce motivation introduce incrementally construct relation entity iterative growth beam iteratively relation candidate algorithm termination calculate threshold similarity calculation candidate relation iterative sequence model instead representation iteration relation solely entity along ignore manner model compute without revisit earlier relation scalar introduce framework previous information tackle complexity constraint multiple hop relation lan jiang modify stag query graph generation incorporate constraint extend relation reduce generation beam define action extend aggregate action extend action apply aggregate action query graph rank fully layer dimensional feature candidate query graph fed reinforce algorithm rank model training bert semantic model utilized derive feature conclude DL structure datasets evaluation metric adopt knowledge DL structure datasets evaluation metric adopt knowledge generation generation QG aim generate factoid automatically increase recent research QG boost benefit model training various QA task dialog underlie knowledge source QG raw text knowledge typical paradigm extract keywords extends adopt architecture technique enable task perform jointly sequence sequence framework encoder decoder structure involves attention copying mechanism introduce generate factoid knowledge graph  text rnn model lstm generate knowledge graph keywords extract knowledge graph hence generation task regard sequence sequence sequence consists keywords adoption encode decoder structure although model intend apply generation knowledge graph  dataset utilized instead knowledge graph training phase zero shot generation knowledge graph aim generate involve predicate unseen training phase generate hop neural model knowledge triple textual context motivate manually exist entity predicate input KB triple textual context context predicate derive wikipedia entity generative model adopt encoder decoder architecture encoder attention mechanism perform attention encode KB triplet TransE textual context encoder multiple grus encode textual context separately decoder utilized another gru attention mechanism textual context implicit representation tackle OOV extend action tag utilize information align input output text report  bleu encoder decoder baseline propose effective unseen predicate lose naturalness modify construct transformer model attentive automatically generate difficulty controllable multi hop encoder decoder structure embed subgraph encode difficulty estimation fed encoder decoder positional encode difficulty encode input estimation difficulty confidence entity recognition link perform selectivity generate complex difficulty style introduce stage neural model automatic generation document model involve extraction component extract document generation component construct extraction component entity intend propose neural entity selection model entity propose another neural model utilized pointer network boundary extract entity document generation component adopt attention translation model sequence sequence framework attention mechanism pointer softmax mechanism document generate vocabulary however exist neural generation model utilize feature incorporate information serious generate sequence sequence model tends information passage therefore prevent tendency generate intend propose novel model seqseq utilize information passage passage rnn encoder decoder model propose model apply layer lstms encoders separately extract contextual feature passage decoder lstm involve module keyword net extract keyword feature specific decoder contextual feature passage attention mechanism information extract keyword net decode decoder utilized information passage generate furthermore retrieval style generator apply decoder output layer capture semantics previous research generation relies sequence sequence model passage input target core neglect tackle developed novel model sequence sequence model mechanism apply lstms separately encode passage multi perspective context algorithm lstm output verify passage belong context specifically strategy apply attentive max attentive correspondingly hidden without relevant passage baseline model model enable information passage sufficient information enable decoder generate relevant information attention memory decoder sufficient information decoder enable generate relevant intuitively incorporate context helpful generate solely however irrelevant information performance address challenge introduce maxout pointer mechanism gate attention network rnn encoder decoder structure lstm gate attention devise aggregate information capture intra passage dependency encode effectiveness promote important information validate tag perform attention mechanism maxout pointer mechanism apply decode maxout pointer modify pointer mechanism intend alleviate repetition limitation performance QG  text performance QG  text datasets evaluation datasets recently datasets document QA task knowledge QA task relative datasets effectiveness approach exist QA task validate potential approach QA task explore datasets document QA  dataset chinese medical online chinese medical forum dataset consists training development sum multiple  another chinese medical dataset adopt negative research rank recommendation  medical english additional annotation focus developed biomedical machine comprehension task biomedical knowledge comprehension contains   LS abstract biomedical domain academic context   LS construct title abstract respectively query   LS respectively  reading comprehension dataset consist MC MC text respectively text dataset fictional text multiple choice candidate  comprehension consists choice audio manual transcription   dataset  dataset selection correspond dimensional feature extract document QA datasets datasets knowledge QA LC quad complex dataset contains paraphrase correspond SPARQL query wikidata dbpedia LC quad variety complexity  popular benchmark dataset web relation  annotates  semantic par derive incomplete par remove annotation  release   contains complex correspond  query  reduce leakage training data  movie text audio QA dataset movie domain mixture hop hop  contains built knowledge football player hop hop   subset freebase hop hop  semantic  declarative contains blank cloze style entity web empty slot obtain remove entity randomly knowledge QA datasets evaluation metric various evaluation metric QA task widely evaluation introduce precision recall accuracy ER EM MRR widely evaluation metric QA task discussion widely apply focus QA medical domain offering information online medical community fully utilized domain acquire promising performance style query meantime explore potential model retrieve document efficient QA technique friendly machine interaction developed digital assistant application typical QA DL application QA approach adopt architecture downstream task framework become complicate combine complex task framework usually involve DL architecture incorporate non DL combination framework sophisticated perform lose generalization relatively however unclear architecture task combine perform apply task briefly summarize adopt task rnns recurrent neural network rnn neural network input network input considers output previous due structure multilayer rnns text capture hierarchical bidirectional rnns rnns combine rnns direction backward fully utilize future information frame rnns account former information memory model lstm improve version rnns introduces memory mechanism enhances ability network semantic relation text sequence lstm effectiveness various task widely adopt research simplify variant lstm rnn gate recurrent gru parameter efficient training explore effectiveness adopt rnns QA improve cnns convolutional neural network cnn effective capture local output layer cnns abstract feature longer sequence input cnns detect local without account therefore cnns widely various task QA attention mechanism attention mechanism motivate visual cognition enables model attention important feature trivial information interpret perform sum input vector automatically attention mechanism effective various task become popular network architecture hybrid hybrid usually consists hybrid combination combination non usually complex domain knowledge therefore effective specific task advantage DL combine however involves multilayer network contains parameter training phase usually considerable amount label data therefore address issue incorporate non DL framework specific task DL perform merit combination strength summarize adopt hybrid framework DL structure adopt hybrid statistic statistical analysis literature rnns architecture popular network structure attention mechanism widely adopt data statistic hybrid combine architecture hybrid incorporate architecture non DL image performance comparison performance popular QA datasets wiki QA dataset  COA achieves MRR performance slight   QA TK  performance MRR trec QA dataset    COA relative comparable performance MRR although yield performance innovation network modification combination promising noticeable performance datasets due task requirement although wiki QA dataset trec QA dataset factoid reading comprehension task average performance wiki QA trec QA trec QA wiki QA inference across trec QA contains helpful model hence performance gap datasets derive complexity data dataset task requirement suppose newly propose validate datasets performance QA challenge opportunity technique boost performance task network architecture apply progress however remain challenge challenge opportunity development DL QA future pretrained model pretrained model progress recent bert elmo XLNet pretrained model obtain unsupervised training fully utilize massive available unstructured text incorporate pretrained model performance QA task improve semantics attempt utilize semantics adopt attention mechanism enable model focus keywords relevant semantics capture semantics obtain representation framework newly propose framework integrate however maximize performance framework simply combine network structure researcher dive network architecture gate mechanism rnns distinction memory ability convolution operation cnns performance network architecture tricky researcher deeper insight framework QA dataset although datasets release QA task QA research resource suffer lack datasets remains datasets minority QA task novel technique desire decent model dataset involve shot zero shot technique transfer technique computation building model training consumes computation resource model architecture elaborately parameter update plenty training resource achieve explore simplify architecture model compression technique effort increase training efficiency combine machine technique neural network obtain efficient network QA external knowledge various task nlp attempt promote performance incorporate external knowledge however QA fully utilize external knowledge merely rely text datasets effort introduce external knowledge QA leveraged information knowledge  introduce knowledge graph embed representation motivation utilized external knowledge selection task addition explicit external knowledge incorporate pretrained model enable model capture semantic similarity conclusion review recently propose survey recent literature datasets evaluation metric typical model performance comparison challenge opportunity review discus leverage QA task classification extraction knowledge generation recently datasets QA task description statistic introduce widely evaluation metric brief introduction finally performance comparison challenge future opportunity keywords dataset performance evaluation