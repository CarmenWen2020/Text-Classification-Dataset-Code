development universal efficiency optimization algorithm important research direction neural network stochastic gradient decent momentum SGDM successful optimization algorithm easily local extreme minimum inspire prominent fractional calculus automatic propose fractional fractional momentum FracM extension integral calculus fractional calculus inherits almost characteristic integral calculus memorization  FracM performs fractional difference momentum gradient SGDM algorithm FracM partially trap local minimum accelerate propose FracM optimization advanced SGDM adam advanced optimization algorithm classification accuracy FracM outperforms optimizers cifar textual datasets imdb transformer model introduction ability information performance traditional machine technology satisfactory inspire information processing capability concept introduce hinton propose novel framework belief network DBN breakthrough technology developed rapidly significant progress signal information processing traditional machine artificial intelligence technology progress recently successfully apply recognition processing nlp information retrieval computational vision image analysis dnn structural adapt task image classification convolutional neural network cnn performance extract feature cnn feature extraction layer input layer finally classify multi layer perception mlp recent cnn network structure derive alexnet  VGGNet googlenet resnets data timing recurrent neural network rnn extract series information feature extraction layer memory network lstm rnn mainly gradient disappearance gradient explosion sequence training lstm performs longer sequence normal rnn development technology computer algorithm improve ability data decision prediction data decade impact daily efficient internet autonomous computer vision optical recognition addition technology ability artificial intelligence AI improve technology overcomes shortcoming traditional algorithm rely manual feature arouse research experimental data analysis widely popular training dnn partially overfitting development data analysis technology obtain local minimum faster convergence neural network derive artificial neural network neural network gradient information continuously update network parameter propagation update relies local gradient information random initial easy local extremum addition training data neural network overfitting efficiency training technique propose improve efficiency accuracy training dnn affected gradient explosion gradient disappearance parameter initialization relu activation function alleviate dropout technology avoid fitting achieve obvious practical batch normalization technology propose later improves robustness training hyperparameters simonyan analyzes impact convolutional network depth accuracy image recognition mainly focus optimization dnn dnn characteristic nonlinear non convex multilayer hidden structure feature vectorization massive model parameter therefore optimization complicate goal supervise function improves accuracy training data generalization ability research optimization algorithm converge reasonable saddle algorithm converge ensure algorithm converges loss global minimum dnn classification recognition task performance mainly depends model structure optimization algorithm therefore optimization algorithm improve accuracy neural network task optimization algorithm roughly hessian matrix approximation extent interchangeable amount information due shortcoming optimization algorithm memory usage optimization algorithm optimization algorithm article mainly   theory sufficient machine researcher overfitting cannot sgd suitable neural network optimization implicit regularization obtain performance however recent optimization achieve  achieve performance imagenet  epoch imagenet accuracy  future optimization algorithm calculates partial derivative loss function newton simplest optimization algorithm goal critical function critical loss function maximum minimum saddle however saddle desirable optimization goal hessian matrix positive newton minimum optimization objective likely saddle local minimum actual dimensional data optimization algorithm consume memory feasible article sect introduce relevant popular optimization sect detailed introduction proposes optimization sect conduct convergence analysis previous analysis sect introduces data parameter setting sect introduces comparative analysis experimental sect summarizes text related recent improve optimization algorithm propose basis traditional adaptive optimization algorithm introduce commonly optimization algorithm improve algorithm sgd iterative parameter update rate loss  compute gradient model parameter gradient optimization algorithm core significance sgd widely optimization algorithm solves practical sgd calculates gradient parameter accord define loss function label sample parameter update direction calculate gradient convergence fix iteration negative gradient direction loss function reduces accord sample batch gradient descent stochastic gradient descent mini batch gradient descent batch gradient descent sgd rate dimension parameter iterative easily optimization model parameter oscillation loss function local minimum saddle sgd easily stuck gradient zero gradient calculation usually batch therefore calculate gradient noisy due shortcoming sgd scholar propose improve optimization algorithm sgd sgd momentum SGDM algorithm borrows concept momentum physic simulates inertia update previously update direction within reasonable gradient tune update direction traditional advantage effectively avoid oscillation another improvement nesterov accelerate gradient nag algorithm SGDM direction gradient longer calculate calculate direction item intuitively direction gradient predict update nesterov acceleration gradient accelerate convergence adagrad introduces sum historical gradient attenuation factor adaptive rate parameter loss function converge faster rate decrease increase gradient gradient rate gradient rate avoids constant rate sgd algorithm shortcoming manually specify initial rate denominator accumulate historical gradient rate gradually moreover initial gradient initial rate entire training longer training adadelta improvement adagrad algorithm reduce radical aspect adagrad monotonically reduce rate adagrad accumulates gradient adadelta limit accumulation gradient infinite accumulation gradient accumulate gradient information historical gradient accumulation equivalent accumulation historical gradient information attenuation coefficient rate due accumulation historical gradient adam popular adaptive rate algorithm online framework propose adam exponential decay average gradient SGDM addition adam exponential decay average gradient adadelta RMSprop adam fluctuation fluctuation rate  solves constantly refreshes maximum  maximum optimize iterative direction parameter  adopts bias basically adam algorithm  maxt instead define saddle neural network image local min neural network image    maxt max   parameter update perform almost neural network improve optimization efficiency accuracy adjust rate neural network structure mainly discus optimizer adaptive  bound  optimizer skilled rate bound regulate rate historical calculate data guarantee convergence optimization entire model stable parameter update  define      min  algorithm modifies radial component gradient achieve limit gradient norm achieve technique easily apply optimization algorithm parameter update  define algorithm  novel optimizer adjust rate accord local gradient parameter update  define          optimizer rate batch setting accelerate optimization procedure rate function normalization parameter update  define min max upper bound        lookahead optimization direction smooth oscillation parameter interpolation optimization direction generate another optimizer update along curvature direction lookahead algorithm improve optimizer stability promote faster convergence parameter update lookahead define apply update synchronize parameter sample minibatch data  adaptive sgd mainly focus gradient normalization layer parameter update  define            pid reveal relationship traditional pid sgd momentum optimization algorithm imitate pid algorithm layout optimization algorithm define sgd controller sgd wth momentum optimization PI controller define    sgd PI controller define      quasi hyperbolic momentum algorithm  quasi hyperbolic adam  recovers numerous optimization algorithm efficient stable manner introduce hyperparameter  recover optimizer parameter update  define parameterized  update  structure aware precondition algorithm stochastic optimization tensor  maintains precondition matrix operates dimension parameter update  define      optimizer effective  rate obtain convergence accuracy parameter update  define       variant adam  respectively adaptive  rate upper bound bound converge constant  adam optimizer gradually convert sgd parameter update  define clip rate wisely output constrain     diag clip diag rectify adam  improves accuracy optimization modify variance rate combine training technique gradient clip rate smooth  achieve parameter update  define     variance tractable      analyzes difference regularization decay regularization adaptive optimization algorithm equivalent author restore regularization decouple attenuation loss function parameter update   define        exponential average estimate gradient difference gradient prediction gradient threshold distrust observation difference gradient prediction gradient threshold trust parameter update  define     apollo dynamically incorporates curvature loss function approximate hessian via diagonal matrix contribution storage approximation  diagonal matrix efficient optimization linear complexity memory parameter update apollo define rectify max   diag rectify  spatial average momentum precondition gradient vector construct diagonal hessian matrix precondition gradient vector descent direction contribution combination spatial average hessian diagonal reduce highly mislead noisy local hessian information parameter update  define    diag              modify adam modify factor accumulator parameter update  define min max rms  max rms  aggregate momentum  significantly  oscillation aggressive contribution improve stability aggressive modify gradient descent algorithm calculate stable update velocity update velocity average previous velocity vector optimization update model parameter parameter update  define     recently novel article apply fractional neural network  advance BP neural network fractional convergence avoid overfitting  critical fractional global stability analysis fractional neural network LMI zhang yang analyzes stability fractional neural network delay delay analyze ding global projective synchronization non identical fractional neural network slide mode fractional stochastic classical momentum  detailed formula derivation application fractional optimization SGDM adam respective fractional parameter update define  adam algorithm fractional momentum parameter update define  gradient optimizer propose improve sgd optimizer iterative gradient improve iteration direction neural network optimization propose fractional gradient descent improves performance algorithm fractional approximate simulation update vector coefficient gradient calculate fractional fractional approximate update vector direction optimization algorithm due exponential average adam convergence gradient update vector fluctuate instead monotonically resolve saddle oscillation adjust optimization direction fractional conduct experimental research propose FracM improve performance image classification task resnet cnn architecture sake comparison conduct optimization algorithm accord discussion II exist classic optimization algorithm adam algorithm exponential average gradient modify iteration direction update network parameter suffers convergence exponential average accumulation gradient within fix decay rate gradient encounter momentum fluctuate monotonically shock rate later stage training failure convergence model propose combine fractional gradient avoid convergence reflect gradient FracM propose fractional momentum optimization algorithm fractional approximate calculate update vector coefficient update vector calculate fractional proportion update vector become fractional development introduce preliminary knowledge discrete fractional calculus continuous fractional calculus apply article foundation discussion chapter definition classical calculus concept fractional calculus intuitively understood continuous function classical integer derivative formula define   physical meaning understood rate function derivative express    physical meaning understood curvature curve acceleration physical quantity derivative   accord mathematical induction derivative   fractional regard generalize integral calculus   abbreviate definition   gamma function generalize binomial coefficient equation simplify extend rational definition fractional derivative obtain sgd theo update vector   fractional           FracM fractional coefficient update vector update vector fractional differential operation fractional integral operation definition GL definition actually regard limit rapid development computer technology replace numerical approximation GL definition actually discrete definition function fractional derivative function analytical function derivative derivative intuitively explains relationship derivative fractional fractional momentum optimization algorithm avoid trap local minimum SGDM coefficient gradient depends optimization local minimum gradient approach gradient due coefficient therefore optimization gradient gradient FracM fractional coefficient coefficient gradient calculate fractional fractional coefficient optimization source implementation available http github com  FracM comparison fractional calculus derivative derivative function image convergence analysis supervise suitable neural network parameter approximate data training research optimization algorithm converge reasonable saddle algorithm converge ensure algorithm converges loss global minimum regret bound sum  parameter feasible mathematically regret bound  regret bound FracM regret bound FracM convex online definition parameter FracM define proof conclusion lipschitz smooth gradient iterative iteration bound appropriate rate sgd definitely converge gradient lipschitz constant boundary gradually decrease sequence optimization converge stably convergence rate cannot guaranteed gradient lipschitz constant boundary convergence rate exponential theorem convex function  convex smooth unbiased estimate  input corollary article hypothesis norm gradient bound previous update vector formula update vector satisfies lipschitz continuity neural network smooth expectation gradient bound accord algorithm update vector  derive equation combine update vector absolute fractional parameter parameter omit update vector propose algorithm summarize   sgd update        furthermore convexity   combine obtain   convexity  implies    sum obtain   propose FracM optimizer  guarantee  corollary constant rate regular gradient descent due stochasticity convergence rate convergence rate diminish stepsize article constant  bound parameter update FracM algorithm mini batch sgd essential propose FracM processing gradient optimization complexity FracM consistent mini batch sgd comparison complexity classical optimization algorithm SGDM adam propose FracM comparison classification accuracy cifar database classical optimization algorithm SGDM adagrad RMSProp adam    propose FracM optimization comparison classification accuracy cifar database classical optimization algorithm SGDM RMSProp adam    propose FracM optimization comparison transformer model processing textual datasets classical optimization algorithm SGDM adam propose FracM optimization video memory occupation cifar database classical optimization algorithm SGDM adam   propose FracM optimization comparison complexity classical optimization algorithm SGDM adam propose FracM optimization purpose propose algorithm exist optimization introduce network architecture hyperparameter setting introduce data architecture artificial intelligence research neural network currently popular convolutional neural network cnn model achieve research although task cnn automatically feature usually data predict unknown data combination cnn resnet popular network structure achieve image recognition competition residual inside residual network connection alleviate gradient disappearance increase depth neural network article residual network depth residual network sequence sequence model variant recurrent neural network encoder decoder seqseq important model processing utilized machine translation dialogue automatic summarization transformer sequence sequence model essence encoder input decoder output core transformer attention stack attention wise fully layer encoder decoder hyper parameter article optimization algorithm data processing network structure etc default parameter setting technique improve accuracy training literature comparative propose algorithm meaningful stability performance batch mainly rate epoch rate epoch rate epoch hyperparameters comparison adopt default parameter adopts rate decay strategy FracM optimization classification accuracy comparison propose FracM algorithm classical algorithm recently propose algorithm cifar dataset image classification loss comparison propose FracM algorithm classical algorithm recently propose algorithm cifar dataset image classification accuracy comparison propose FracM algorithm classical algorithm recently propose algorithm cifar dataset image classification loss comparison propose FracM algorithm classical algorithm recently propose algorithm cifar dataset image classification accuracy comparison propose FracM algorithm variation algorithm parameter cifar dataset image classification loss comparison propose FracM algorithm variation algorithm parameter cifar dataset image classification accuracy comparison propose FracM algorithm variation algorithm parameter cifar dataset image classification loss comparison propose FracM algorithm variation algorithm parameter cifar dataset image classification accuracy comparison propose FracM algorithm variation algorithm fractional parameter cifar dataset image classification loss comparison propose FracM algorithm variation algorithm fractional parameter cifar dataset image classification accuracy comparison propose FracM algorithm variation algorithm fractional parameter cifar dataset image classification loss comparison propose FracM algorithm variation algorithm fractional parameter cifar dataset image dataset article classic data cifar cifar data verification algorithm data innovative algorithm verify training training accuracy algorithm cifar data image category cifar data image category image image normalize rgb channel zero variance standard deviation image simply flip cropped randomly flip horizontally default probability imdb dataset consist movie review label sentiment positive negative review encode sequence index integer convenience indexed overall frequency dataset integer frequently encode data allows filter operation eliminate convention specific instead encode unknown glove glove representation overall statistic express vector compose vector capture semantic feature similarity analogy etc calculate semantic similarity imdb dataset operation vector euclidean distance cosine similarity analysis image classification experimental describes classification accuracy optimization algorithm experimental cifar dataset layer residual network model optimization algorithm bold batch respectively performance FracM optimization algorithm propose significantly optimization algorithm detailed validation statistic FracM classical cifar  FracM optimizer converges faster classical loss accuracy describes classification accuracy optimization algorithm cifar dataset layer residual network model parameter consistent propose algorithm reduce influence exponential average gradient FracM effectively prevent network generate oscillation minimum optimization algorithm algorithm accurate detailed validation statistic FracM classical cifar  FracM optimizer converges faster classical loss accuracy detailed validation accuracy FracM variation algorithm cifar  detailed validation loss FracM variation algorithm cifar  FracM FracM FracM FracM FracM performs cifar dataset parameter beta gradually increase validation accuracy gradually decrease oscillate FracM fractional coefficient variant promising performance detailed validation accuracy FracM variation algorithm fractional parameter cifar  detailed validation loss FracM variation algorithm cifar  FracM FracM FracM FracM FracM performs cifar dataset fractional parameter gradually increase validation accuracy gradually increase stabilize parameter easy oscillate FracM fractional coefficient variant promising performance defect algorithm parameter FracM optimization algorithm calculate fractional affect circulation algorithm however default parameter FracM optimization algorithm promising performance conclusion propose optimization FracM fractional fractional calculus inherits almost characteristic integral calculus memorization  coefficient gradient depends fractional avoid local minimum gradient gradient FracM fractional coefficient parameter gradually increase validation accuracy gradually decrease oscillate occupy proportion parameter fractional gradually increase validation accuracy gradually decrease oscillate occupy proportion fractional gradually increase validation accuracy gradually increase stabilize fractional fractional coefficient rapidly gradient FracM almost memorization  gradually fade convergence algorithm analysis regret bound FracM variant outperform optimization SGDM adagrad adadelta RMSProp  adam image classification dataset cifar textual datasets imdb transformer model keywords neural network optimization gradient descent fractional residual network image classification