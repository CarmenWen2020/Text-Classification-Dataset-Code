recognize regulate emotion rid emotion vital important role behaves accurate emotion detection revolutionize computer interaction potential proactive approach mental health untapped source data social medium data psycholinguistic marker multimodal data audio video signal combine sensor psychophysiological brain signal comprehend affective emotional propose model utilizes modality visual facial expression gesture audio text spoken content classify emotion discrete category ekman model additional category neutral transfer multistage tune modality instead training dataset model generalizable multiple modality allows integration heterogeneous data source effectively modality combine decision fusion technique propose  model favorably technique benchmark datasets meld IEMOCAP introduction emotion affective sensory information stimulation emotional regulates social interaction affect quality integral behavior generally perceive positive negative emotion positive emotion improve quality overall negative emotion adversely affect health capability psychological issue stress anxiety depression amass negative emotion prolong imperative detect emotional overall wellness diagnosis intervention difference individual psychological disorder moreover various tele health customer service extremely important emotional customer identify correctly appropriate response health professional business owner automate automate emotion recognition assistive former integral latter  evidence regard significance emotion communication basis researcher data community develop automatic emotion evaluation goal achieve intelligent computer interaction typically  diagnostic individual emotional function analyze various evaluation parameter appearance psychomotor behavior characteristic affect mood content concentration others prominent application affective emotion recognition robot interaction summarize video health affect observable reaction individual emotional express frown  tear lip  forehead   eyebrow gaze rate pressure affective convey something  sort reaction communication psychologist trust identify disorder instability emotion practitioner ass mental emotional facial expression gesture automatic recognition emotional expression data report assessment questionnaire machine data various bio signal emergence smartphone wearable device promise remote assessment psychological emotional mental health capture assess mode expression manifestation digital iot biomarkers intrinsically challenge owe volume variety data generate identify emotion patient psychological disorder rely iot wearable biomarkers viable convince patient issue wearable pertinent research modality input label estimate emotion affect recognition accurate combine observation user information context circumstance identify emotional affect modality wrap variation feeling affective emotional affective emotional judged various modality facial expression gesture visual audio spoken content text physiological signal sensor attach conduct analyze emotion unimodal data bio signal facial expression architecture achieve computer vision convolutional neural network cnn processing nlp task transformer model useful feature extraction representation cannot model manually ability architecture enables transfer feature representation datasets feature extraction another dataset domain pre model tune comparatively dataset adapt domain model expensive concept transfer allows model classification tune significantly cheaper task complementary relationship improve performance sustainability foundation research propose model emotional health detection  heterogeneous data modality visual facial expression audio text linguistic classification emotional category neutral happiness sadness disgust  model emulate scenario suitable integrate task smart assistant automate customer service online education bullying detection CCTV monitoring  health others audio visual input video surveillance video audio individual processing video component sample obtain image frame extract facial expression recognition tune resnet pre architecture emotion classification audio signal component emotion classification automatic recognition asr classify emotion audio audio clip clip convert mel spectrogram  architecture pre audio classification tune task automatic recognition pre model nvidia  toolkit  recognition transcription available subtitle dataset frequency cosine similarity asr fairly accurate hence transcription directly instead subtitle text emotion classification accordance objective emulate scenario text classification tune distilbert purpose representation model condense bert model reduce faster inference retain almost capability combine modality fusion strategy justified empirically logically primary contribution multimodal approach emotion classification audio visual input model adopt smart service application transfer modality component achieve generalizability reduce computation multistage tune pre model modality component unimodal emotion dataset target dataset performance evaluate portion meld generalizability evaluate benchmark IEMOCAP dataset related automatic recognition emotional expression data report assessment questionnaire machine data various bio signal prominent application robot interaction summarize video health pertinent research modality input label estimate emotion report text analytics facial expression cod acoustic feature cod report bimodal model audio visual feature audio textual feature emotion recognition recent research trend affective compute community recognize emotion multimodal content textual clue visual audio modality model emotion recognition EEG signal gaze data response video clip propose emotion detection recola dataset cnn extract feature resnet visual modality  database multimodal gesture physiological signal recording convolutional belief network  cnns generate feature text audio visual modality individually multiple kernel mkl combine data propose  model evaluate IEMOCAP dataset introduce novel approach dimensional convolutional neural network model spatiotemporal information belief network DBNs multimodal emotion recognition  multimodal emotion database later developed multimodal emotion dataset textual dialogue correspond visual audio component propose mer multimodal emotion recognition algorithm data driven multiplicative fusion technique modality text neural network author benchmark datasets IEMOCAP cmu  improvement prior recently modulate attention transformer modulate normalization transformer modulate fusion combine linguistic acoustic input author evaluate performance IEMOCAP   meld dataset physiological dataset integrate  emo dataset  pain dataset apply advanced dataset recognize emotion affective pain assessment propose  lstm model  emo dataset emotion video author utilized  emo dataset detect negative emotion individual emotion dataset amusement sadness evaluate author indication positive negative emotion category wearable device electrocardiogram ECG signal variety classifier svm knn DT RF gradient boost decision implement analyze positive negative emotion access ECG EMG scl execute wavelet transform feature svm  emo dataset attain accuracy none signal bio video detect emotion recent multimodal emotion recognition report transfer pre training tune PT FT approach transfer neural network emotion recognition propose model audio visual emotion recognition transfer multiple temporal model evaluate dataset summarization reduce data fuzzy synthetic model report transfer various fusion technique emotion recognition data audio video modality propose bert pre supervise architecture text modality multimodal emotion recognition background concept brief overview concept research emotion model emotion integral behavior generally perceive positive negative emotion positive emotion improve quality overall negative emotion adversely affect health capability negative emotion influential factor mental health issue stress anxiety depression untapped source data social medium data multimodal data audio video combine sensor psychophysiological signal comprehend affective emotional multiple emotional model various researcher broadly model categorize discrete emotion model dimensional emotion model discrete model categorize emotion accurate emotion frustration sadness whereas dimensional model emotion categorize arousal valence etc dimensional model hypothesis emotion independent therefore exhibit relationship spatial highlight emotion model emotion model image propose ekman model discrete emotion approach emotion happiness sadness disgust model fundamental emotion originate neural network activate reaction external stimulus experimentation extra emotional neutral emotion transfer essence transfer extract knowledge source task apply target task traditional machine model task model heavily dependent quality dataset task methodology cannot apply situation label data insufficient text classification task resource representation transfer model model pre unlabeled dataset adapt supervise target task label data available pre model available others tune target downstream task pre training dataset computationally expensive downstream tune cheaper transfer modality architecture transfer model image image video transfer popularize computer vision particularly image classification attribute combination factor release imagenet dataset image category advent computer vision dataset model dominate ILSVRC improve eventually beating benchmark convolutional architecture effective computer vision winner alexnet  vgg googlenet resnet video treat stack image link temporal feature imagenet release datasets video clip sport youtube  along associate pre model extract feature enable researcher apply downstream video understand task tune datasets audio representation audio signal waveform inextricably link transfer apply image cnn architecture development landmark audio dataset audioset accompany audio classification pre model   apply specific audio classification task  variant vgg model modify mel spectrogram audio input feature extractor tune model  model recognize scene training unlabeled video approach processing focus dense vector embeddings traditional embeddings wordvec glove fasttext representation irrespective context concept context embeddings elmo multiple representation building upon foundation embeddings advancement transformer network model pre training achieve landmark achievement nlp bert bidirectional encoder representation transformer pre unsupervised prediction task masked model MLM prediction  MLM token input text masked randomly objective predict masked context unlike statistical model orient MLM allows fusion context  task capture relationship consecutive training data consist pairing training scheme architecture bert model adapt strength nlp task data compute RoBERTa GPT XLNet GPT others datasets automatic recognition emotional expression data report assessment questionnaire iot data various bio signal pertinent research conduct datasets   meld  IEMOCAP datasets datasets multimodal emotion dataset meld interactive emotional capture IEMOCAP audio visual textual feature emotion recognition conduct analyze emotion bio signal facial expression multimodal data modality identify visual signal facial expression along audio signal text analytics linguistic  model pre model contains VGGFace identification extraction audio component classification textual component asr datasets extract transcript textual component pre distilbert pre network tune meld IEMOCAP modality separately visual modality tune perform twice cife dataset finally datasets propose model validate datasets meld IEMOCAP meld tune evaluate model portion meld dataset extension  dataset latter text modality former video audio accompany text meld contains dialogue TV series dialogue encompasses participant actor emotion utterance dialogue participant correspond emotion utterance meld average duration utterance categorize desire emotion subset meld henceforth refer meld sub utterance multiple participant multiple participant camera utterance participant speaker completely visible camera IEMOCAP interactive emotional dyadic capture database laboratory california  hire actor participate session conduct actor marker facial expression movement conduct session script spontaneous communication actor manually utterance belonging emotion happiness sadness excitement frustration neutral others merge frustration utterance others convert data emotion highlight emotion wise distribution datasets emotion wise data distribution cife  image facial expression cife dataset construct improve facial expression model analyze facial expression task cife dataset social medium web web crawl employ obtain expression chosen category emotion category disgust sad neutral utilize related expression amount accumulate correspond expression expression disgust happiness neutral sadness manually dataset correspond data unbalanced viola detector avail uncover image expression cife dataset freely available public dataset propose multimodal emotional health detection model  identify affective emotion video surveillance propose model modality video surveillance input information visual audio robust data quality modality situation visible camera audio unclear vice versa content utterance evaluate judge emotional enable evaluation audio transcription asr instead directly text associate dataset asr accurate affect text analysis downstream therefore accuracy dataset text utterance fusion visual audio text component emotional decision fusion component parallel decision ensures unaffected quality data another modality propose model propose model image visual modality video spatiotemporally stack image goal evaluation data consist utterance focus facial expression sample image video clip temporal feature effective evaluate longer video preprocessing video clip meld sub IEMOCAP sample frame per utterance average avoid peak frame analysis instead adopt majority voting strategy individual emotion classification frame tag video clip sample frame depicts scene emotion analysis facial expression multitask cascade convolution network  extract frame detect discard extract image tune pre model sample frame meld sub preprocessing extraction  image tune resnet model load pre VGGFace dataset model VGGFace consists image significant variation illumination ethnicity suitable precursor data downstream emotion classification resnet convolution architecture achieve imagenet image classification task resnet winner imagenet visual recognition challenge reduce error rate increase layer architecture residual network convolution architecture convolution layer along max pool layer relu activation layer distinction skip connection architecture consecutive hidden layer resnet relu function alternative layer perform output ith layer along output layer input layer perform activation function skip connection residual connection resnet preserve knowledge gain training model hidden layer resnet model implement kera emotion detection recognition pre training model VGGFace dataset identification cife emotion pre model detects affective emotion individual meld sub IEMOCAP visual signal cnn architecture feature layer task specific feature layer utilize tune model  layer retrain network cife dataset converge architecture target task emotion classification cife dataset contains image illumination ethnicity suitable bridge VGGFace datasets fully output layer model softmax classifier category remove layer average pool layer convolutional layer model  fully layer node target emotion category pre training cife minimize categorical entropy loss adam optimizer feature extraction classification tune cife freeze layer model remove layer meld sub IEMOCAP image tune model obtain vector extract feature feature vector input multilayer perceptron mlp network emotion classification configuration mlp network mlp network image audio modality audio classification significant handcraft feature rnns lstms effective model temporal audio signal cnns capable achieve representation audio signal spectrogram avoid training dataset domain adopt transfer approach extract audio video clip  library  format  library audio preprocessing preprocessing audio clip frame image sample resampled khz convert spectrogram fourier transform hop respectively mel spectrogram compute mapping spectrogram bin within finally mel spectrogram compute offset tune classification utilize  model audio component  pre youtube dataset dimension embed model audio categorization tune emotion classification task preprocessing accordance audio feature input  pre frozen fully layer trainable pre model tune meld sub IEMOCAP datasets datasets layer modify node emotion category additionally activation function layer sigmoid softmax audio sample associate emotion label mel spectrogram feature input convolutional layer frozen fully layer converge model emotion classification text modality analyze spoken content convert extract audio clip text asr analysis important expressive speaks monotone important factor emotional reliable another factor facial feature emotion instance  eyebrow audio feature spoken content factor audio transcription nvidia  toolkit transcription model architecture asr  comparatively exist model model  NR pre  corpus tune impulse response rir augmentation robust pre model performance audio transcription dataset extract output lowercase alphabetic token similarity apply distilbert punctuation scheme input masked model similarity application video surveillance input transcript available asr adequate detect emotion individual accurately gauge performance asr module interested similarity context meaning ideally transcription subtitle meld sub perform similarity  dataset IEMOCAP textual component along dataset frequency TF cosine similarity CS alignment asr module average accuracy datasets tune classification transformer architecture bert inspire training scheme dominate nlp task accuracy model parameter google bert bert facebook RoBERTa OpenAI GPT nvidia  others however increase model become suitable device smartphones backbone smart service utilize distilbert model condense bert teacher training report faster inference performance bert parameter reduction  transformer library tune pre distilbert model specifically distilbert uncased generation transcription generate asr module tokenized pad align maximum utterance tune model distilbert uncased tune mapping emotion tune meld sub IEMOCAP datasets separately CLS token classification token classification task output transformer layer CLS token input feature fed mlp classifier decision fusion multimodal data fusion fusion fusion joint fusion enable evaluation decision fusion evaluate component parallel ensure unaffected quality data another modality robust data quality modality situation visible camera audio unclear vice versa content utterance evaluate judge emotional parallel model visual audio text model decision fusion technique model varied modality dependent identify impact modality model separately signal thorough review impact fusion propose model fusion framework framework assigns basis detection ratio detection ratio DR DR TP TP TN FP FN TP positive TN negative FP false positive FN false negative DT calculate model execute discrete emotion output calculate DR probability vector belonging model predictive calculate individual model model calculate decision opt trough maximum function model predictive chosen output propose architecture contains model modality output  max    fusion strategy output model suitable output implementation  model evaluate datasets meld IEMOCAP individual model tune resnet  bert visual audio text modality respectively tune model datasets execution data evaluate performance model  model modality evaluate separately decision fusion detect emotional understand impact modality evaluate performance model separately combine modality various model pseudo code propose  model model modality model visual fetch facial expression  facial expression frame input tune transfer network resnet model IEMOCAP meld dataset datasets evaluate audio model  textual model bert report IEMOCAP meld dataset respectively video signal broken visual audio signal input therefore model evaluate modality datasets modality similarly textual data fetch audio signal asr modality combine understand impact modality textual data cannot directly generate video signal model directly evaluate modality detect fusion performance implementation IEMOCAP meld datasets respectively performance model emotion model performs happiness emotion sadness emotion variation accuracy detection emotion variation training datasets emotion express differently individual effective emotion detection tune model datasets varied emotion IEMOCAP meld dataset propose  model combine modality namely visual audio textual surpasses fusion comparison modality subset modality although model execution environment performance varies widely datasets variation performance impact factor promptly individual express emotion differently actor datasets along datasets available tune model limited achieve accuracy pre model tune model twice visual modality visual modality perform audio text modality individually visual component earlier tune cife meld sub IEMOCAP performance various model meld IEMOCAP image propose model discrete emotion image propose transfer fusion model identify discrete emotion precisely comparison exist model performance propose  model exist comparison  model datasets meld IEMOCAP observable author perform empirical analysis meld IEMOCAP datasets respectively convolution model meld whereas modify recurrent neural network performs IEMOCAP propose model perform  SOTA individual modality account exist data pre model eliminates impact dataset accuracy although variation datasets model indicates accuracy model upon data accurate detection pre model tune per user specific accurate estimation emotional individual comparison model fusion model promising model merge modality visual audio text access affective emotional individual video surveillance conclusion perceive situation generates affective response emotion reaction bodily consequently prompt behavioral action  signal behavioral signal posture signal social interaction psycholinguistic feature social data observable trait detect emotional capability transfer data fusion detect emotion multimodal dataset multistage tune pre model visual audio textual signal individually decision fusion subsequently decision identify emotion category happiness neutral sadness disgust benchmark database meld IEMOCAP model perform propose model video surveillance patient automate detection unstable emotion identify spike emotion treatment accordingly keywords emotion recognition transfer multimodal health detection