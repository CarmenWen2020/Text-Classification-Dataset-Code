traditional label classification effectively apply multi label classification due semantic correlation conventional attention mechanism prior knowledge lack semantic correlation degradation detection performance  circuit memory mechanism brain brain inspire memory graph convolutional network gcn propose gcn crucial memory module interact attention prior knowledge complex semantic enhancement suppression evaluate effectiveness public benchmark microsoft coco pascal voc extensive demonstrate gcn outperforms advantage semantic correlation complexity traditional memory model introduction image generally contains diverse semantic correlation multiple predict series image essential computer vision recent establishment image datasets MS coco pascal voc imagenet greatly reduces manual label effort progress image recognition benefit rapid development label classification public datasets attention mechanism introduce exists isolation performance multi label recognition greatly limited ignore complex semantics semantics multiple mainly traditional memory model graph model discover semantic correlation multiple semantics exert significant impact computer vision task extract model semantics memory model rnn semantic redundancy occurrence dependence perform multi label classification propose lstm model label dependency propose recursive framework iteratively discover attention semantic model semantic relationship however rnn lstm model label dependency linear correlation sufficiently utilized meanwhile deficient explicit model label occurrence critically multi label image recognition graph model model label relationship introduce conditional random dependency network occurrence matrix etc ML gcn  label relationship perform statistic prior probability label occurrence dataset however previous model focus simply semantic occurrence relationship inhibition relationship semantics memory ignore moreover calculate pre statistical label occurrence express prior probability specific datasets datasets probability bias inevitably inadequate generalization gcn MS cma aggregate vector image feature extract visual feature implement rough modal fusion gcn attention prior knowledge balance attention mechanism target location frequently traditional visual attention network mainly attention convolution network contribute construct perfect attention mechanism multi layer structure MCAR attention discover global representation respectively meanwhile  generates label specific embeddings capture local global semantics multiple layer convolutional neural network label semantic representation attention mechanism semantic generation attention mechanism independently discus attention mechanism prior knowledge via image prior knowledge conclusion image semantics compose image conversely lack prior knowledge temporary ignore attention mechanism efficiently recognize semantic however due occlusion viewpoint therefore previous model generally derive analysis independently prior knowledge attention mechanism inevitably bias attention technique lack accuracy detect multiple prior knowledge temporary multi label image recognition image fade indicates non conform indicates semantic consistency prior knowledge determines attention mechanism demonstrates detect effortlessly image address limitation discus synaptic plasticity hippocampus medial temporal lobe neurological theory semantic mechanism brain synaptic circuit mechanism memory mechanism hippocampus brain inspire memory mechanism propose multi label image recognition model framework semantic memory  memory mechanism brain inspire  model combine efficient semantic recognition mechanism cnn network semantic correlation construct GNN realize description memory brain model imitates brain mechanism memory nervous complex semantic correlation multiple efficiently recognize addition memory module generate enhance inhibit semantic correlation previous specific prior annotation contribution propose multi label image recognition framework  synaptic circuit mechanism memory mechanism establish propose memory module memory complex semantic enhancement inhibition correlation multiple robustness model relational memory mechanism improve imperfect semantic unlike previous prior statistic specific datasets achieve semantic correlation image demonstrate effectiveness generalization ability quantitative qualitative MS coco voc voc datasets related memory memory conscious recollection intricate network constitute personal memory involve semantic knowledge declarative memory declarative memory mainly  occurs synapsis neuron neuron adjust synapsis reduce difference synaptic specific neural activity  efficacy persistent trigger synaptic plasticity synaptic plasticity plasticity plasticity accord memory declarative memory permanently information memory attention information temporarily brain filter redundant information memory training memory longer memory important basis recall decision judgment semantic knowledge constitute declarative memory perspective biological mechanism researcher memory stem synapsis brain synaptic modification becomes research memory  receptor NMDA synapse role formation declarative memory gate NMDA receptor image NMDA receptor synapse ion gate switch channel coincident neuronal activity remove trigger declarative memory opening channel directly easily excitement enhances synaptic transmission contrary gate reduce efficiency synaptic transmission achieve inhibition environment weak coincidence specifically coincident neuronal activity refers simultaneous coincidence input stimulus output stimulus input image input stimulus label image output stimulus coincident stimulus input output gate manifest formation memory specific channel signal input gate function NMDA receptor synaptic modification basis memory formation auxiliary decision memory construction semantic framework mainly reflect  structure hippocampus relational memory NMDA receptor regulate synaptic plasticity hippocampus memory utilized hippocampus decision  core declarative memory generate essentially combine declarative memory attention decision input information visual attention generate visual nerve information output neuron pathway hippocampus pathway indirect pathway input information directly output neuron indirect pathway information project collateral pathway finally return  output neuron circular auxiliary circuit feedback pathway memory perform pyramidal neuron sector CA indirect pathway memory update semantic knowledge indirect pathway pathway information fed pathway decision pathway indirect pathway image formation memory memory participates decision memory function hippocampus memory  cohen howard  hippocampus involve relational memory information hippocampus relational objection sequence objection encode hippocampus link memory related multiple relational memory relational memory emerge model declarative memory individual episodic exist isolation extend network link feature advantage relational memory adaptability capability memory situation firstly feature memory extract  semantic bound characteristic cod link memory memory inference indirectly related hippocampus automatically encodes memory memory various memory trace accumulate cerebral cortex generalize memory trace semantic knowledge framework schematic diagram relational memory memory compose episodic memory compose series occurrence contains characteristic unrelated image task multi label recognition relational memory effectively associate semantics multiple memory indirect pathway realizes semantic knowledge framework multiple assist pathway accurate decision memory gcn memory brain multi label image recognition hippocampus receives visual information multimodal sensation memory brain encodes multiple relationship relational memory multi label image recognition task efficiently  excitatory pathway memory visual attention memory subdivide memory memory image recognition task memory filter redundant information affect brain ability extract instantaneous characteristic transformation memory memory training memory encodes sequence multi label image recognize interactive mode memory therefore imitate output structure hippocampus model semantic knowledge multi label brain inspire framework establish mechanism relational memory multi label recognition propose brain inspire memory multi label recognition imitate neural mechanism memory overview memory gcn simulate synaptic circuit hippocampus pathway mode network model recognition jointly memory attention network architecture dual pathway model illustrate gcn model propose model pathway indirect pathway input image fully convolutional network resnet network backbone compute feature pathway attention mechanism decouples semantic attention obtain meanwhile semantic memory module indirect pathway memory module indirect pathway memory module memory module memory module semantic generate memory memory module gcn graph convolution network model memory supervision semantic memory memory interactive memory calculate memory attention memory jointly achieve ultimate decision component network architecture gcn model image fully convolutional network backbone pathway indirect pathway pathway mainly attention mechanism indirect pathway mainly complex semantic memory module gate image perception module simulate memory memory information rationally vision brain filter redundant information memory temporarily attention information memory simulate visual attention mechanism memory filter screen image attention mechanism extract content aware feature  respectively height width channel feature fully convolutional network feature extract image fully convolutional network define   denotes fully convolutional network mask  filter information compute feature  layer perception mlp  meanwhile resize  resize  attention feature  wise multiplication mask feature obtain visual attention  memory module preliminary relational semantics effective detection attention mechanism attention feature memory module applies average pool estimate global semantic  memory concatenation convolution attention feature global semantics     memory tends focus global semantic relation attention feature focus individual explicit difference output memory module utilized input memory module attention feature memory gate memory generate memory gate NMDA receptor excitation threshold label memory module image perception module simulate memory memory persistent memory training memory correspond synaptic memory stimulates relational memory neuron due gate action NMDA receptor specific signal enhance synaptic efficacy facilitate model relationship label image memory semantic relationship graph convolution vertex graph label adjacency matrix matrix graph jointly reflect relational memory label memory depends gate switch role NMDA receptor gcn NMDA gate gate propose output memory module attention feature memory adjusts excitation threshold gate supervise generation memory excitation threshold specific memory mechanism neuron effective stimulation induces excitation alter gate switch NMDA receptor membrane inspire gate NMDA input excitation threshold excitation input input gate memory module specific stimulus threshold correspond generally cannot excitation specific however highly susceptible excitation perceive assume existence relational memory specific brain achieves gate excitation threshold primarily detect synchronous pre  excitation NMDA receptor membrane gate detection parameter detect relationship input memory simulate NMDA receptor excitation threshold alter influence memory predict excitation threshold difficulty complexity brain mechanism achieve excitation threshold denote adjacency memory previous attention memory input generate adjacency non linear mapping tanh tanh output gate detection parameter gate active inhibit gate generation reliable semantic relation gate detection parameter magnitude strength contact memory summarize gcn chooses trust memory attention extreme lim negative indicates exert inhibitory  relationship positive exerts reinforce correlation moreover gate detection parameter semantic correlation memory completely static positive negative correlation difficulty relational memory acceptance content memory module image stimulus influence relational memory formulate representation cannot independently trigger excitation presence relational memory relation input optimize consistent memory stimulus gate gate module modulate excitation threshold input memory representation subsequently supervise generation adjacency memory   memory module relational memory pathway target semantic assist calculate memory   simulate interaction indirect pathway hippocampus input information dual pathway structure enable effective interaction memory attentional information decision memory contains semantically consistent association correction attention capture temporarily significant multi target detection  denotes fully layer traditional model focus decision recognition lack robustness therefore mutual decision indirect model perform decision achieve summation memory  indirect pathway attention  pathway dual pathway structure brain hyper parameter decision attention memory   algorithm propose model multi label classification loss supervise memory gcn sigmoid function denote truth label predict    datasets setup datasets evaluation metric evaluate MS coco voc voc datasets multi label image recognition task microsoft coco detection recently widely multi label recognition MS coco consists training image image validation average per image previous evaluate model validation due lack label dataset pascal voc widely multi label image recognition task contains image voc training validation voc consists image  however truth label voc submit evaluation server evaluation comparison previous validation evaluation metric previous overall per precision OP CP overall per recall CR overall per CF average precision  CF  metric meaningful metric generally implementation detail hardware software configuration hardware software configuration software hardware environment guarantee fairness parameter configuration data augmentation random horizontal flip image resize backbone cnn resnet pre imagenet output dimensionality global average pool memory extraction contains nonlinear activation function LeakyReLU negative slope evaluation voc dataset convergence perform coco pre model model sgd optimiser decay momentum batch datasets rate initialize memory module backbone epoch epoch training epoch implement pytorch ablation analyze influence component framework model investigate coco aspect memory module memory module perform hyperparameters evaluation hyperparameter hyper parameter decision attention memory evaluate parameter average precision  obtains  baseline memory module increase hyper parameter significantly improves accuracy multi label recognition performance achieve however limited semantic consistency decrease accuracy rely memory decision finally  decrease approximate baseline therefore accuracy comparison image evaluation memory module evaluation memory module MS coco image baseline resnet pre imagenet baseline evaluate coco dataset memory similarity combination memory attention linear layer replace memory module ass memory  remove memory module baseline meanwhile accuracy decrease slightly memory shorter training lack pre training memory similarity attention memory generate semantic memory propose model utilize attention input role memory implies adjacency relation directly generate attentional supervision improve baseline significant improvement CF memory critical role absence memory memory combine memory achieve increase  CF outperform baseline respectively visually illustrate comparison comparison MS coco voc voc respectively effectiveness generalization model MS coco MS coco focus convolutional network traditional memory model cnn rnn  reinforce rnn attention multi evidence resnet srn resnet graph neural network model ML gcn  gcn MS cma additionally MCAR  perform multi label recognition attention mechanism independently parameter configuration propose sect experimental configuration comparison completely consistent accord previous image comparative comparison gcn mscoco comparison MS coco propose model advantage traditional memory model rnn lstm gcn  overall metric slightly  metric situation gcn overall detection pathway mechanism attention focus  performance detect brain inspire model effective focus attention mechanism meaningful memory pathway instead mining attention information alone ML gcn  adjacency relationship artificial data calibration gcn surpass ML gcn   memory module complex semantic correlation gcn MS cma utilize label embed vector generate label graph aggregate vector image feature extract visual feature lack balance attention prior knowledge weaker perfect attention mechanism  model gcn CP mainly due deeper semantic decouple network obtain accuracy initial frame target stage attention mechanism gcn outperforms model CR metric CR reflect sample correctly detect memory module effectiveness gcn memory module SOTA memory model perform similarly resnet pre imagenet memory model surpass voc comparison model voc gcn achieves accuracy rate detection  improves SOTA   furthermore achieve accurate detection bottle accuracy increase previous SOTA generally detect correlation memory model addition traditional memory model detection accuracy cnn due lack effective combination memory attention memory characteristic model achieves accuracy improvement almost baseline voc comparison model voc voc voc overall trend gcn achieves  however  MCAR outperforms detection slightly independent detection focus attention mechanism easy detect gcn achieves accuracy rate detection propose gcn powerful detect furniture vehicle benefiting correlation comparison gcn pascal voc dataset comparison gcn pascal voc dataset visualization visualize multi label recognition illustrate interaction memory attention mechanism semantic relationship relational memory visualize recognition performance voc role memory module intuitively specifically resnet attention mechanism baseline visualization recognition performance memory module network voc visualization performance input image semantic enhancement recognition label recognition performance input image semantic suppression recognition redundant label recognition image memory module advantage attention mechanism multi label image recognition task baseline specific target due occlusion incomplete display however semantic enhancement relational memory perform gcn recognition target bicycle achieve demonstrate advantage relational memory module semantic enhancement baseline detects redundant complex background propose gcn contradictory suppression relational memory perform  airplane eliminate memory module complex semantic relation semantic suppression recognition image label furthermore memory module complement recognition detect directly owe viewpoint gcn suppress decision recognize contradictory semantics achieve optimization semantic consistency recognition conclusion brain inspire complex semantic visual task model interpretable neuroscience theory propose memory gcn network achieves memory interaction enhances inhibit semantic correlation multiple label furthermore gcn generalize model suitable computer vision attention mechanism prior knowledge extensive public benchmark demonstrate gcn outperforms achieves  respectively verify fully explore complex semantic relation superior exist multi label image recognition keywords multi label classification brain inspire graph conventional network memory network