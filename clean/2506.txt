privileged information lupi paradigm pioneer teacher interaction mechanism model additional information training stage propose incremental algorithm lupi paradigm random vector functional link RVFL network IRVFL novel algorithm leverage privileged information incremental RVFL IRVFL network training stage constructive IRVFL network scenario model accuracy requirement accuracy model requirement algorithmic implementation IRVFL respectively local update global update strategy data classification regression specifically algorithm IRVFL calculates output newly hidden node input output parameter exist hidden node fix contrast IRVFL IRVFL II update parameter exist hidden node newly hidden node moreover convergence implementation finally experimental IRVFL indeed performs favorably introduction active borderline neural network become related around decade trace  pitt propose formal model neuron become pioneer neural network theoretical neural network model initiate although development neural network smooth finally progress signal processing recognition image processing nonlinear model data processing knowledge processing etc gradient algorithm widely neural network learning however traditional gradient algorithm difficulty convergence easy local minimum dependence initial parameter neural network recurrent neural network gradient algorithm gradient disappearance gradient explosion neural network exert powerful ability anatomical reconstruction brain random connection exist brain important role neural representation biological basis scholar propose randomize neural network rnns calculate network output layer node linear regression network threshold randomly generate specific interval probability distribution algorithm iterate repeatedly overcomes bottleneck encounter traditional gradient algorithm extent rnns propose random vector functional link RVFL network link input output layer layer feedforward neural network bias input node enhancement node RVFL network randomly generate link complexity model   theoretical justification RVFL network RVFL network universal approximator continuous function  finite dimensional henceforth scholar propose derivative RVFL network quantum version RVFL network recurrent RVFL network orthogonal polynomial expand RVFL network exponentially expand robust RVFL network etc RVFL network become popular layer feedforward neural network due advantage training generalization performance universal approximation ability currently novel version RVFL network RVFL propose RVFL incorporates privileged information lupi paradigm leverage additional source information alternative RVFL network lupi paradigm propose vapnik  motivation improve generalization performance model vector machine algorithm privileged information svm lupi paradigm  attention considers teacher explanation significant role contrast traditional paradigm lupi paradigm additional information training data training privileged information available training stage unavailable stage employ additional information lupi paradigm solves data modality available training model achieve prediction paradigm incorporate algorithm svm metric rank multi transfer pace addition practical lupi paradigm apply visual  assessment pedestrian detection etc RVFL introduce regularization correction function privilege information training objective function improves generalization RVFL network however RVFL limitation RVFL network optimal network architecture hidden node critical role performance network obvious neural network node easy consume training overfitting underfitting feasible network architecture chosen correctly therefore algorithm appropriate network architecture automatically highly desirable generally accepted theory concern optimal hidden layer node propose hidden layer node training sample unfavorable situation engineering hidden layer node obtain various calculation formula sometimes differs principle hidden layer node compact structure premise meeting accuracy requirement tackle prune algorithm constructive algorithm former network remove hidden layer node longer effectively latter network architecture grows additional hidden acceptable prune algorithm constructive algorithm advantage constructive algorithm directly define initial network prune algorithm initial network constructive algorithm prune algorithm training network optimal consume moreover prune largely dependent initial network building quality initial network consume contrary constructive algorithm usually additional hidden node easily quality node therefore constructive algorithm likely network prune algorithm author mainly focus constructive algorithm incremental RVFL IRVFL network algorithm approximates output hidden node network error tolerance predefined IRVFL network terminate training training error constructive obtain compact model ensure desire achieve author incremental algorithm optimal hidden node RVFL network introduces lupi paradigm IRVFL network IRVFL constructive randomize approach lupi paradigm network IRVFL additional information constructive obtain network model contribution summarize advanced neural network lupi paradigm fix network structure propose IRVFL incremental construct appropriate network architecture develops algorithmic implementation local update global update strategy concretely calculates output grown iteration input output parameter exist hidden node fix update parameter exist hidden node newly node convergence algorithmic implementation theoretical remainder organize related briefly introduce sect detail IRVFL consist theoretical analysis algorithmic description implementation author validity IRVFL sect sect conclude remark related random vector functional link network architecture RVFL network label data RVFL network enhancement node hidden node formulate    bias input layer hidden node denotes inner vector nonlinear activation function architecture RVFL network image easily calculate output moore penrose pseudo inverse ridge regression    moore penrose pseudo inverse identity matrix trading parameter random vector functional link network privileged information lupi paradigm described triplet generate accord fix unknown probability function function guarantee probability incorrect classification regression generally additional information belongs paradigm additional information available training stage available additional privileged information training phase training data become training dataset feature belongs privileged feature  RVFL   regularization coefficient enhance layer output matrix correspond feature privileged feature function slack function privileged feature output matrix function function RVFL rewrite   output matrix calculate     identity matrix consequently output function RVFL define     incremental random vector functional link network privileged information algorithm description obvious exist neural network algorithm lupi paradigm network structure tackle proposes incremental algorithm lupi paradigm IRVFL network constructive propose algorithm IRVFL IRVFL definition  RVFL privileged information training stage usually IRVFL network link gradually increase hidden node acceptable performance met initial residual error grown residual error   lth lmax grown output vector newly hidden node correspond feature privileged  respectively lth grown hidden layer output vector architecture IRVFL network image  correspond feature privileged feature lth grown obtain output crucial constructive scenario accuracy requirement accuracy requirement algorithm implementation local update global update strategy namely IRVFL IRVFL II detail IRVFL local update strategy IRVFL priority algorithm update output grown previously construct structure parameter fix update parameter suitable scenario model construction accuracy requirement training target function IRVFL formulate           construct fix grown   slack function correction function regularization coefficient partial derivative target function respect       combine output calculate                      identity matrix furthermore obtain output function  stage data IRVFL II global update strategy IRVFL II performance priority algorithm update input output parameter grown training stage update parameter suitable scenario model accuracy construction training target function IRVFL II formulate   output easily calculate accord         convergence analysis reading calculation simplify target function IRVFL           verify simplify equation consistent equation derivative respect        convert another        obviously therefore obtain output  Î”ğ»Tğ¿Î”ğ»ğ¿         Î”ğ»Tğ¿Î”ğ»ğ¿ Î”ğ»Tğ¿Î”ğ»ğ¿ Î”ğ»Tğ¿Î”ğ»ğ¿     Î”ğ»Tğ¿Î”ğ»ğ¿     Î”ğ»Tğ¿Î”ğ»ğ¿ Î”ğ»Tğ¿Î”ğ»ğ¿ Î”ğ»Tğ¿Î”ğ»ğ¿     Î”ğ»Tğ¿Î”ğ»ğ¿     Î”ğ»Tğ¿Î”ğ»ğ¿     Î”ğ»Tğ¿Î”ğ»ğ¿     Î”ğ»Tğ¿Î”ğ»ğ¿     consequently obtain inequality prof convergence IRVFL proof IRVFL II define    intermediate output IRVFL II calculate correspond       convergence IRVFL II proven algorithm implementation detailed implementation procedure IRVFL summarize evaluation classification regression datasets IRVFL IRVFL II IRVFL multi classification datasets regression datasets machine repository keel datasets available http keel statistic keel datasets illustrate training data input attribute normal feature privileged feature output author randomly split attribute dataset mention normal others privileged sample preprocessed normalization experimental bolded statistic keel classification datasets statistic keel regression datasets theoretical basis selection hyperparameters empirically author IRVFL dataset wine parameter hyperparameters pre define user maximum hidden layer node lmax parameter chosen random within respectively parameter IRVFL achieve performance author summarize IRVFL achieve performance respectively remain datasets optimal parameter IRVFL IRVFL II due limitation description omit later performance IRVFL user define parameter IRVFL sigmoid function activation function lmax image expressivity network model typically characterize internal neuron behavior associate activation function identify define suitable AF specify task boost performance network model therefore appropriate activation function IRVFL comparison report performance IRVFL activation function accuracy wine IRVFL IRVFL II achieve performance wine sigmoid function activation function datasets activation function performance IRVFL activation function lmax classification datasets fix maximum hidden layer node lmax performance algorithm training accuracy accuracy IRVFL IRVFL IRVFL II besides datasets wine  flare iris texture fix maximum hidden layer node respectively author obtain performance IRVFL IRVFL II remarkably IRVFL privilege information IRVFL improve generalization model importantly IRVFL II achieve performance classification datasets training accuracy curve IRVFL IRVFL IRVFL II classification datasets comparison approach classification datasets training accuracy accuracy performance IRVFL IRVFL IRVFL II classification datasets performance wine performance  performance flare performance iris performance texture image training accuracy IRVFL IRVFL experimental privileged information effective improve model performance classification regression datasets fix error tolerance performance algorithm training RMSE RMSE hidden layer node algorithm regression datasets achieve error error tolerance datasets mortgage  abalone laser  respectively IRVFL II achieve performance importantly premise achieve error tolerance hidden layer node IRVFL mortgage IRVFL IRVFL II IRVFL  hidden node IRVFL IRVFL IRVFL II IRVFL abalone hidden node IRVFL IRVFL IRVFL II IRVFL laser hidden node IRVFL IRVFL IRVFL II IRVFL  hidden node IRVFL IRVFL IRVFL II IRVFL algorithm lupi paradigm network structure IRVFL achieve error tolerance reduces model curve training RMSE RMSE algorithm datasets author obtain IRVFL converges faster IRVFL besides premise achieve desire IRVFL II hidden node IRVFL IRVFL IRVFL algorithm illustrates IRVFL performs effectively regression comparison approach regression datasets training node training RMSE RMSE performance IRVFL IRVFL IRVFL II regression datasets performance mortgage performance  performance abalone performance laser performance  image discussion author conduct various classification regression datasets experimental illustrate IRVFL perform IRVFL prof leverage privileged information classification regression task useful IRVFL IRVFL II advantage disadvantage former faster accurate latter accurate report complexity IRVFL IRVFL IRVFL II obviously IRVFL II others data explains IRVFL II data IRVFL complexity IRVFL slightly complicate IRVFL IRVFL faster IRVFL nevertheless IRVFL performs IRVFL complexity IRVFL II complicate achieve performance IRVFL IRVFL finally author recommend IRVFL strategy accuracy requirement IRVFL II strategy accuracy requirement improve computational efficiency IRVFL focus future research comparison complexity algorithm conclusion lupi paradigm proposes incremental paradigm RVFL network IRVFL IRVFL leverage privileged information IRVFL network training stage algorithmic implementation IRVFL developed local update global update strategy IRVFL priority algorithm IRVFL II performance priority analyze convergence algorithmic implementation theoretical guarantee fully discussion IRVFL IRVFL II achieve  accuracy respectively complexity analysis newly derive IRVFL merge incremental algorithm lupi paradigm effectively application lupi paradigm incremental keywords privileged information random vector functional link network incremental constructive