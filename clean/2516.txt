extreme machine elm layer neural network become popular expression training minimizes training error generalization ability data elm tune hidden layer calculation pseudo inverse hidden layer activation matrix training classification computational overload tune becomes affordable activation matrix extremely pseudo inversion eventually matrix memory extreme machine QELM propose manage classification datasets avoids tune bound estimation hidden layer data population replaces training activation matrix reduce prototype avoid storage pseudo inversion matrix elm linear svm cannot apply datasets QELM execute datasets data input spending reasonable purpose computer without software hardware requirement achieve performance elm introduction extreme machine elm neural network model related svm widely apply classification regression series prediction variant elm propose literature twin elm classification non parallel hyperplanes simultaneously minimize distance away online sequential elm OS elm allows chunk chunk combine ensemble augment OS elm classification regression  quaternion signal convenient 3D 4D signal ensemble elm propose negative correlation committee voting  bootstrap training sample lane landmark detection elm rank data effective elm successfully apply image classification rank matrix factorization elm autoencoder optimal dimensional feature application elm orient classification imbalanced data apply discriminative data cluster alongside linear discriminant analysis cluster elm forecast sale computer server elm combine paradigm fuzzy logic optimal elm hyper parameter hidden layer particle swarm optimization PSO feature selection hidden layer estimation stage classification electrocardiogram signal elm PSO combine boost consumption series forecasting elm apply instead genetic algorithm symbolic regression identification neural network combine elm recursive arc cosine kernel regard hardware implementation describes  circuit architecture elm experimentally implementation hardware device input elm input hidden layer randomly output calculate target matrix label classification pseudo inverse matrix output hidden neuron hidden neuron relevant training hidden neuron achieve performance hyper parameter tune achieve performance collection pre define elm performance grid approach although alternative strategy fuzzy logic particle swarm optimization propose optimal hidden layer simplicity elm medium datasets unable classify datasets training tune hidden layer grid elm hyper parameter grid optimization calculation pseudo inverse hidden layer activity matrix datasets matrix eventually available memory proposes extreme machine QELM classification datasets organize sect research elm datasets describes elm network sect estimation hidden layer QELM sect output calculation sect QELM algorithm sect report sect conclusion compile sect related literature report elm datasets bag bootstrap technique reduce training datasets alleviate computational overhead bagging ensemble  datasets symmetric elm cluster evaluate traffic congestion prediction transform training sub medium datasets feature selection reduce data feature identify relevant feature rank coefficient obtain elm variation coefficient regularize elm apply image classification training input spending training core computer cpu ghz GB ram approximate kernel elm evaluate datasets training singular decomposition svd hidden node elm  elm replaces random initialization input svd multiple random subset sample dataset svd random generation approximately matrix pseudo inverse calculation besides svd memory pseudo inverse apply matrix finally matrix pseudo inverse calculate globally  elm complex elm overload introduce svd compensate random selection training matrix svd apply conquer scheme tractable computational complexity volume data input spending per fold min powerful computer intel xeon ghz cpu core 6GB ram matlab however dataset webspam input training fold dataset dataset review report recent application elm data context parallel implementation elm mapreduce spark respectively latter classify input computer cluster node elastic elm mapreduce matrix incremental decremental  calculation apply synthetic datasets input another mapreduce approach elm ensemble imbalanced classification datasets imbalance ratio elm toolbox data application matlab python elm hidden node toolbox spent dataset input powerful workstation core ghz cpu GB ram per core specific hardware gpu acceleration elm rank reduce matrix propose decrease hidden layer training performance datasets training OS elm avoids oscillation OS elm trial spending datasets input feature bag elm another variant combination ensemble feature bagging computer severe memory constraint previous approach apply elm datasets datasets others gain mainly data technology mapreduce spark specific hardware parallelization gpu acceleration powerful compute capability goal propose elm execute classification datasets arbitrarily population purpose computer specific hardware software data technology contribution execution elm datasets simplicity described sect diagram elm neural network classification training  remove   replace   respectively image introduce notation associate elm network refer training patter respectively training dimension input elm network compose matrix classification label compose dimensional vector neuron hidden layer elm   hidden neuron input footnote compose matrix    input hidden neuron compose matrix    activity output hidden neuron compose matrix    hidden output layer output compose matrix  output layer elm neuron desire output output neuron    otherwise compose matrix  dimensionality compose matrix   output layer label predict elm compose dimensional vector extreme machine training  input  standardize zero standard deviation input  hidden neuron   matrix   denotes transpose matrix output neuron   activation  matrix define random activation function verify  guarantee  environment  output matrix  calculate  pseudo inverse matrix elm neural network define matrix respectively network dataset elm neural network completely hidden neuron algorithm report pseudocode elm network classification hidden neuron hidden neuron tune grid fold validation datasets per fold training validation fold elm performance evaluate correspond validation performance average fold training cycle pre specify collection average performance fold elm training validation fold performance average fold alternative model selection elm prune significant hidden neuron network approach relatively spending datasets input instead tune network fix prune neuron non relevant non relevant neuron prune simulated anneal desire network achieve without adjust reduce performance complexity approach spending min dataset adult training elm incrementally neuron hidden layer PSO multi objective function minimizes uncertainty riemann metric optimal input addition neuron maximum iteration uncertainty  relatively spending dataset training selection QELM datasets grid hyper parameter tune elm execute fold literature optimal training hidden activity matrix datasets matrix slows calculation pseudo inverse calculation memory matrix however increase consequently matrix eventually memory proposal calculate increase function upper bound limited datasets estimate function evaluate behavior elm performance datasets collection sect input   random classification performance randomness reduce random component exactly matrix matrix random datasets matrix variability due randomness almost remove logarithmic achieve kappa datasets collection described sect versus training panel input panel logarithmic image training continuous dash kappa achieve elm datasets illustrative performance behavior elm upper panel panel plot average kappa datasets collection described sect image plot training input panel report increase dependence correlation coefficient moderate accord  however panel reflect dependence moderate correlation therefore plot classification performance cohen kappa datasets slightly behavior performance average panel dataset collection sect training performance perfect classification kappa decrease relatively constant performance performance reduce therefore propose QELM panel average performance achieve upper bound threshold datasets datasets maximum hidden neuron  approach allows estimate directly training intrinsic dataset without elm training stage complex calculation training datasets propose proportional datasets remain constant datasets matrix elm training slowly dataset besides optimality elm performance achieves maximum finally respect report performance constant reduces panel elm training becomes hidden neuron calculation output output elm network calculate pseudo inverse matrix expensive task memory matrix due computational complexity matrix pseudo inversion matrix inversion complexity matrix inversion standard gauss jordan elimination strassen algorithm coppersmith winograd CW algorithm manifold elm ML elm matrix inversion CW inspire specifically matrix pseudo inversion matrix  propose rank cholesky factorization inversion symmetric matrix calculate pseudo inverse factorization inversion respectively rank  parallel architecture processor complexity reduce  respectively however optimize approach calculate pseudo inverse matrix datasets consequently respectively previous propose bound analogously bound training dataset datasets instead elm acceptable input proposal upper bound datasets datasets training instead training therefore threshold defines maximum matrix achieve reasonable matrix min specifically propose replace training elm prototype training prototype  define prototype matrix prototype matrix  datasets training elm datasets upper bound prototype bound dependent unbalanced suggests  training datasets  zero prototype avoid bound prototype guarantee minimum prototype per datasets acceptable maximum prototype per however datasets prototype  constraint zero prototype avoid possibility propose min max  unless equivalently otherwise finally max  training prototype prototype  calculate efficient datasets training initial prototype   training exist update prototype   accord           version respectively prototype label training besides  training label prototype  training prototype  training update efficient online version cluster algorithm allows collection prototype without excessive memory requirement prototype datasets  upper bound         QELM algorithm algorithm report pseudocode QELM hidden neuron prototype update accord meaning hyper parameter QELM matrix pseudo invert cannot overcome matrix cannot overcome respectively input respectively hyper parameter limit computational QELM datasets acceptably avoid training performance similarly avoid excessive prototype QELM datasets finally guarantee minimum prototype unbalanced datasets avoid excessive prototype performance QELM sensitive hyper parameter avoid tune hinder application datasets hyper parameter QELM classical elm formally proven universal approximator exactly training dataset however elm cannot apply datasets perform pseudo inversion matrix QELM extends elm datasets contribution QELM estimate adequate matrix directly dataset without network preliminary exploration elm behavior sect report performance achieve specifically propose bound datasets calculation matrix QELM replaces training prototype limited datasets efficient online version cluster algorithm representative collection prototype due collection reduce significant version training combine contribution orient achieve matrix achieve performance memory efficient pseudo inverse calculation estimate QELM performance datasets prototype guaranteed accurate representation training mathematical elm guarantee classification learnt performance datasets training input sort increase datasets sort increase discussion QELM collection classification datasets input machine repository california  uci footnote dataset msa artificial dataset nest spiral  repository dataset training input datasets QELM classical elm vector machine radial basis function rbf kernel henceforth SVC implement  library datasets QELM elm linear kernel vector machine henceforth LSVC implement  library LSVC instead SVC datasets latter rbf kernel hyper parameter tune practical LSVC datasets execute without hyper parameter tune kappa per fold QELM elm SVC datasets fourfold validation dataset  datasets shuttle mnist poker physical  already datasets elm SVC perform grid hyper parameter tune training randomly validation randomly trial grid proceeds training classifier training evaluate performance validation combination hyper parameter combination average performance validation classifier combination validation performance average dataset  training split training randomly validation remain training combination hyper parameter achieves performance validation performance achieve classifier training validation combination elm tune hyper parameter min therefore elm performance reduces previous sect avoid excessively elm SVC tune hyper parameter regularization hyper parameter inverse rbf kernel contrary hyper parameter tune perform datasets avoid training per fold elm LSVC performance measurement cohen kappa account unbalance algorithm programmed octave scientific program  execute computer intel core cpu core 6GHz 4GB ram  report kappa respectively achieve QELM elm SVC datasets majority datasets SVC achieves kappa overcome QELM elm average elm  SVC classifier wilcoxon rank sum kappa SVC elm report difference statistically significant elm QELM difference significant datasets synthetic difference slightly overcomes QELM adequately performance elm datasets training upper bound prototype QELM training prototype matrix training elm difference QELM elm former perform tune hyper parameter difference performance impact lack tune difference fairly average strategy QELM estimate successful regard per fold QELM systematically faster elm datasets latter former dataset average statistical datasets contribute report average datasets elm QELM analogous SVC QELM empty average report elm average QELM performs hyper parameter tune grid therefore training SVC QELM former latter dataset classifier dataset isolet datasets due dataset  QELM spends fold elm spends fold SVC fold kappa per fold QELM QELM QELM elm LSVC datasets report kappa QELM elm LSVC datasets QELM datasets matrix respectively none training however elm fails datasets LSVC datasets fails datasets performance QELM elm datasets elm overcomes QELM datasets magic adult although QELM overcomes elm datasets chess  average datasets elm fail elm overcomes QELM wilcoxon statistical significance difference datasets QELM neuron datasets elm QELM prototype elm training evaluate impact prototyping performance developed QELM elm prototyping difference label QELM report kappa QELM datasets elm fail majority datasets devanagari QELM performs similarly elm average kappa QELM elm average difference QELM elm prototyping increase average performance QELM QELM QELM prototype calculation datasets sort increase image performance LSVC clearly poorer QELM elm wilcoxon respectively LSVC irregular dataset LSVC achieves QELM achieves LSVC performs poorly chess shuttle   however  LSVC achieves QELM adult LSVC outperforms QELM datasets LSVC QELM perform poorly suggests really datasets report QELM twice elm elm longer performs hyper parameter tune QELM calculate prototype elm directly training fails datasets LSVC QELM despite linear kernel LSVC iterative SVC algorithm classification datasets LSVC QELM datasets LSVC faster QELM former latter arabic devanagari  input fail dataset due QELM slowly kddcup min  limitation matrix although increase spent prototype calculation peak  due respectively kappa sort increasingly achieve QELM datasets image plot spent QELM calculate prototype perform training calculate pseudo inverse matrix datasets corresponds datasets shift dataset increase peak datasets input arabic adult devanagari however relatively stable thanks bound dimension matrix increase prototype continuous prototype calculation prototype dash remain stable datasets however dot increase peak datasets arabic devanagari due peak poker dataset reduce training plot kappa QELM datasets maximum prototype previous kappa average respectively increase prototype increase average respectively exception conclusion increase slows QELM without kappa tune achieve performance kappa QELM datasets hyper parameter average report kappa achieve QELM datasets hyper parameter hidden neuron maximum limit prototype sect dataset QELM memory report average kappa asterisk identify achieve default influence report kappa confirms performance reduction QELM regard performance reduces increase average kappa QELM influence performance although perform slightly similarly increase slows QELM prototype calculate  performance respect overall QELM sensitive choice tune QELM sensitive reduces performance conclusion extreme machine QELM version elm classification purpose computer without software pre neural network data platform hardware graphic processing gpus something currently available literature therefore exhibit practical achieve goal QELM replaces hyper parameter tune hidden neuron usually important overload network bound estimation training elm calculate pseudo inverse hidden layer activity matrix scalable datasets instead QELM activity matrix bound avoid indefinite matrix dataset neuron upper bound prototype efficient online version cluster algorithm training instead training independently maximum allows QELM datasets prototype although prototype calculation consequently QELM classify datasets training input standard elm manage datasets training linear svm cannot execute datasets QELM performance elm radial svm respectively datasets datasets QELM elm linear svm QELM faster elm radial svm datasets datasets QELM elm fails datasets faster linear SVC moreover spent QELM relatively stable training increase due limitation matrix sensitivity QELM respect hyper parameter maximum prototype tune performance future extend capability datasets input keywords extreme machine classification datasets model selection