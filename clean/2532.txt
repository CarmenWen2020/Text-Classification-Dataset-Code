develop architecture inverse oscillate propel embed chemical reaction dynamic nonlinear exhibit chaotic behavior challenge exist inverse approach aim explore data driven robot novel locomotion mechanism architecture model locomotion mechanism developed recently comput phys architecture successfully snapshot target gel geometric reaction parameter architecture consists multi layer perceptron mlp classifier discrete parameter stack mlp regressor SMLPR continuous parameter inverse unique considers discrete continuous output architecture capable classification regression recover parameter within accuracy simulated recover parameter chaotic quality demonstration quantitative agreement horizon qualitative agreement longer horizon demonstrate agreement lyapunov exponent accuracy suitable introduction conventional robot rigid programmed accomplish specific task efficiently however efficiency price adaptability robot inspire structure exist flexibility task promise increase collaboration opportunity machine attention subfield robotics increase successful adoption approach becomes frequent across surgical marine exploration wearable assistive device challenge deploy robot signal actuator initiate sustain locomotion variety actuation possibility emerge recent  channel within robot actuate structure various cable mimic locomotion grasp   pressure  gripper mimic snake elephant trunk dielectric elastomer actuator mimic crawl  however option external signal source onboard furthermore circuitry actuator limit robot operation confine variety propulsion methodology propose robot without limitation proposes propulsion alter surround magnetic  utilizes acoustic actuate  device movement structure trigger crystal polymer alternative approach chemical reaction driven gel induce gel oscillatory chemical reaction introduce robotics  collaborator various transduction scheme gel  across  reaction active gel expand contract artificial  induce translation cylinder transport bubble within oscillatory chemical reaction investigate model instead polymer gel react environment autonomously simulation investigate response oscillate gel non invasive trigger  reaction promising reaction therefore resultant within purpose driven gel without physical actuator contribute greatly completely  sufficient robot robotics extend previous application  trigger gel demonstrates useful context modify  reaction therefore imply controllability furthermore non euclidean elasticity approach verify simulation author semi implicit simulate oscillate gel basis behavior oscillate gel accurately efficiently simulated fix parameter robust approach parameter induces target exist parameter typically continuous sensitive choice parameter nonlinear exhibit chaos brute tune parameter yield precise systematic development inverse parameter inverse silico technique adopt engineering discipline aim accelerate multidimensional continuous  mlp inverse architecture  antenna chen inverse generative neural network discover novel convolutional neural network airfoil inverse sample structure information spectrum machine inverse robotics fem machine architecture obtain inverse sensor function adopt construct training validation data instead data oscillate gel simulation propose data inverse architecture simulation inverse contribution architecture effectively perform inverse geometric parameter parameter chemical reaction induce target oscillate comprehensive hyper parameter tune activity finally demonstrate verification simulation feasible domain parameter predict parameter setting within architecture comparison exist approach contrast inverse approach propose architecture application inverse emerge robotics knowledge bridging inverse autonomously actuate robotics inverse predominately crystal research exhibit structure exhibit complexity review inverse approach theme application approach distinguish nonlinear dynamical static architecture regression classification extensive hyperparameter tune procedure insight architecture robustness author investigate inverse context nanoparticle scatter spectrum approach training exhaustive numerical simulation sparse subset simulation domain mlp NN replace simulation approximate maxwell interaction reverse mlp network training reverse direction tune inverse mapping obtain framework calculate layer thickness nanoparticle prescribed scatter profile contrast propose framework architecture classification regression author assume discrete parameter layer nanoparticle network author develop crystal graph cnns framework connection crystal instead simulated data image crystal graph encode atomic information atomic configuration crystal passing feature vector cnn framework obtain discrete continuous crystal contrast achieve target accuracy without resort complicate architecture cnns however achieve stringent target accuracy stack network  another difference approach dynamic whereas inverse predict airfoil parameter  parameter author various nns mlp dataset comprise airfoil obtain geometrical parameter request airfoil accelerate aerodynamic tailor criterion contrast application author organize multi classification framework airfoil geometrical parameter airfoil adopt alleviate adverse training dimensional target author application instead 2D distribution image simulation various cnn architecture estimate airfoil responsible distribution conduct concise version network hyperparameter tune procedure resembles conduct however static image distribution whereas dynamical oscillation gel framework author mlp inverse framework obtain structural parameter  circular antenna prescribed frequency response approach author simulation PDEs generate training data however input inverse framework frequency domain whereas architecture input domain additionally methodology lack classification stage essential author supervise autoencoder sae inverse architecture vectorized image graphene  structure reduce dimensional latent ultimate stress strain supervise parameter author embed inverse architecture mainly dimensionality reduction addition latent physical stress strain another noteworthy achievement ability framework ability interpolation distinct orientation horizontal vertical diagonal  investigate superposition multiple analysis refrain interpolation framework moreover inverse stage distinct mlp classifier regression afterward easy author encoder decoder architecture comprise 1D convolutional layer obtain geometrical parameter acoustic sink absorption coefficient profile simulated parameter absorption profile author encoder decoder portion propose framework separately profile manufacture acoustic sink additive manufacturing absorption profile experimental setup finally absorption coefficient profile prescribed estimate decoder built acoustic sink seek replace simulation developed therefore anything decoder stage instead introduce stack regression stage account error predict parameter due dimensional coordinate finally lack classification stage crucial role framework summarize exist approach autoencoders cnn setup exists mlp inverse approach literature none aforementioned implicitly multi classification organize classification regression within architecture additionally none inverse dynamical despite difference application methodology hyperparameter tune procedure rigorous procedure justify robustness methodology explanation hyperparameter organize sect detail simulation introduce underlie equation discretization sect describes inverse introduces propose stag architecture sect explains experimental procedure along obtain classification regression verification simulation sect summarizes finding simulation simulation model simulation generate data described simulation model semi implicit solver developed equation dynamic flexible describes equation simulate hexagonal oscillate gel define via triangular lattice panel lattice vertex bending proportional dihedral angle triangular lattice tends zero closely approximates continuum isotropic elastic lattice driven distribution motivate gel chemical radial spiral induce local  model radial planar unidirectional correspond simulation     relative sinusoidal function location  amplitude wavenumber  dynamic bending damp constant  quantity dimension radius hexagonal assume periodic proportional stiffness  bending stiffness constant proportional thickness strain function enters explicitly nearly planar perturbation spatially nonuniform perturbation   occurs critical threshold strain amplitude threshold decrease increase becomes dominant bending resists  apply semi implicit backward differentiation discretization developed        superscript denote implicit linear discrete laplacian matrix denote  explicit nonlinear bound norm remain bending linear implicit involve discrete  matrix approximates bending remain approximate remainder bending temporal accuracy flexible dynamic spatially discretize hexagon radius equilateral triangular lattice mesh uniform mesh mesh apply perturbation evolve amplitude rapidly  distribution curvature magnitude simulation sufficient amount evolve beyond transient initial  snapshot panel dynamic panel correspond trace norm bending vector denote   snapshot flexible bending norm parameter radial image snapshot increase amplitude curvature outward  dilation contraction simulation parameter occurs sect coordinate lattice training feature bending vector dimension bending involve fourth derivative respectively lattice curvature spatial derivative hence snapshot various image radial image panel vector norm evolve  reflect  dynamic nonetheless smooth approximate spatial symmetry bilateral symmetry snapshot panel inverse flexible analysis inverse task mapping parameter explicitly define discrete sequence snapshot dimensional lattice associate derive quantity bending inverse data simulation described sect parameter input simulation output parameter amplitude stiffness constant wavenumber amplitude stiffness wavenumber continuous parameter binary variable specify radial planar continuous parameter chosen avoid deformation physically described sect inverse engineering diagram flexible analysis image contribution architecture convert target reaction parameter architecture stage stage classifier determines lattice snapshot stage consists regressors identify continuous parameter principal component analysis pca preprocessing input stage standardization data apply pca multitude neutralize outlier data variable comparable neutralize difference magnitude data schematic architecture component architecture detail structure inverse mapping architecture mlp classifier gel accord architecture selects specialized mlp regressors predicts continuous parameter online image stage mlp classifier pca preprocessing stage multi layer perceptron mlp classifier distinguishes reaction symmetry radial symmetry radial symmetry planar coordinate lattice sufficient preprocessing principal component analysis pca accelerates algorithm increase accuracy snapshot training snapshot training sect training snapshot raw feature input pca coordinate lattice obtain simulation dependency snapshot obtain accurate prediction reaction pca preprocessing extract linearly uncorrelated feature lattice  data along orthogonal decrease variance compute singular decomposition svd data feature svd decomposes data organize matrix factor   principal direction onto project project principal component projection onto principal direction sort decrease singular creates data matrix coordinate define  formulation feature reduce pca preprocessing enables reduction input neural network thereby enables compact network architecture furthermore evidence suggests pca NN architecture achieve performance multiple literature pca reduction dimension data data hinder algorithm due memory requirement standard pca prevent memory efficient version pca incremental pca  incremental pca determines principle component sequence mini batch initial data  renormalization feature classifier comprise fully layer hidden layer relu activation function hidden layer output layer sigmoid function obtain output integer label gel stage mlp regressor pca preprocessing stage architecture stack regressors stack regressor regressors aim identify continuous parameter amplitude stiffness wavenumber respectively feature portion architecture coordinate lattice resultant bending gel respectively component nonlinear combination derivative lattice physic inspire quantity avoids incorporate convolutional layer essentially extract derivative regressor network specialized reaction radial planar simulation training snapshot raw feature input another pca preprocessing procedure coordinate lattice bending norm calculate snapshot sum feature classification stage dependency training snapshot consideration analysis concise fix dimension reduction via pca transformation bending data principal component component component outnumber coordinate component input layer network performance increase coordinate transformation analysis achieve excellent fix regressor structure output continuous parameter unlike classification stage stage consists stack network stack mlp regressor SMLPR architecture consists component layer  output parameter parameter fed  stage along coordinate component stage output parameter accurately online image parameter estimation visualize simulation data correspond unique coordinate feasible parameter aim coordinate propose SMLPR structure regressor network stage initial prediction coordinate simulation data however estimation robustness percentile regressor network output network preliminary coordinate prediction continuous parameter input network preprocessed coordinate input output prediction continuous parameter preliminary coordinate estimate network coordinate component reduces distance actual estimate parameter parameter furthermore initial prediction input data network input aim eliminate underlie location parameter 3D parameter estimation accuracy classification portion relu activation function fully layer exception output layer network due regression purpose data generation training procedure approach architecture determination demonstrate inverse performance scikit preprocessing pytorch neural network training computer training network equip nvidia quadro gpu 4GB  nvidia geforce gtx max 4GB memory footnote graphic driver date training footnote classifier binary entropy loss regressors loss optimization procedure adaptive estimation adam stochastic gradient descent hyperparameters adam addition hyperparameters batch training adjust batch classification enable reasonable memory usage hardware evaluate classification accuracy fix update fix training epoch classification training avoid overfitting network regression epoch baseline epoch batch regression limited limit memory usage hardware significant sensitivity epoch classification epoch regressors investigate sect data generation identify parameter domain training data validation data parameter bound random target sample across simulation identify rectangular feasible procedure yield amplitude stiffness parameter domain continuous variable obtain tensor generate training data discretize grid equidistant parameter combination generate simulation yield simulation per simulation network snapshot snapshot initial transient phase future extrapolation generate training data trajectory version snapshot per simulation instance training data training data network specifically data corresponds snapshot version multi snapshot per simulation training snapshot per simulation training data network validation validation obtain generate random uniformly distribute parameter across input domain obtain simulation extrapolation performance validation snapshot simulation span classification perform parametric network architecture classifier optimize hidden hidden layer principle component sufficient accuracy classification network baseline architecture hidden layer hidden perform multi snapshot training outline snapshot sect multi snapshot sect metric performance metric classification accuracy MCA classification accuracy WCA training snapshot training  average classification accuracy network achieve   simulation validation predict simulation validation prediction snapshot  snapshot training data finally delta function argument otherwise sum training snapshot obtain average performance snapshot network  minimum average classification accuracy network achieve    snapshot training average training data training perform snapshot  becomes   becomes   MCA WCA widely literature holistic accounting overall performance reliability propose network training differentiate network MCA WCA however application timing impact factor performance metric yield network minimum dimension hidden layer principal component prefer aim MCA WCA classification procedure analysis proceeds hidden hidden layer finally refining principal component coarse grid principal component hyperparameter entirely greedy network combination supplementary detailed analysis correspond snapshot per simulation training training denote interval validation demonstrate extrapolation corresponds snapshot training recall approach network multiple snapshot network snapshot training snapshot validation fourth hyper parameter tune detail snapshot per simulation training investigate training snapshot per simulation sufficient summarizes parametric snapshot along network parameter upon completion pas achieve network   diagram classification architecture determination snapshot per simulation training sequential parametric architectural choice arrow performance image supplementary detailed hidden subsection hidden classification accuracy performance hidden   performance hidden   respectively pointwise classification error per training validation simulation snapshot calculate   become hidden increase  PCs  PCs however beyond hidden performance mlp classifier saturate specifically hidden increase  increase PCs obtain  PCs  deteriorate slightly hidden analysis hidden layer increase hidden layer improve layer architecture   pointwise classification error per training validation simulation snapshot layer layer classification accuracy layer neural network  snapshot validation correspond network snapshot obtain training TR summary  obtain PC  obtain PC image classification accuracy layer neural network  snapshot validation correspond network snapshot obtain training TR summary  obtain PC  obtain PC image calculate   becomes hidden layer increase  PCs  PCs however beyond hidden layer performance mlp classifier saturate specifically hidden layer increase  increase PCs obtain   deteriorate slightly hidden layer analysis network bitbucket  principal component principal component performance PCs   performance PCs   pointwise classification error per training validation simulation snapshot analysis mention demonstrates peak performance principal component therefore refine analysis calculate  principal component training PCs architecture classification accuracy layer neural network PCs  snapshot validation correspond network snapshot obtain training TR summary  obtain PC  obtain PC zoom detailed overview shift bound denote online image snapshot summary architecture mlp classifier structure hidden layer hidden PCs comparison pointwise classification error architecture neural network lack pca processing without pca preprocessing performance metric become significantly  training around network hidden layer hidden almost accuracy peak validation snapshot temporal phase training data network network snapshot peak performance validation snapshot etc snapshot analysis periodic classification accuracy obvious sinusoidal trend classification accuracy hint snapshot cannot capture recur accuracy cyclic underlie physical therefore conclude multi snapshot training achieve accuracy snapshot per simulation training snapshot training simulation entire indicates significant improvement eventually obtain   accuracy achieve desire classification performance layer  network skip tune layer diagram classification architecture determination snapshot per simulation training sequential parametric architectural choice arrow performance image improvement significantly increase training data snapshot training data occupy GB memory MB snapshot training nevertheless training network magnitude hidden subsection hidden classification accuracy performance hidden   performance hidden   pointwise classification error per training validation simulation snapshot calculate   becomes slightly hidden increase   PCs   PCs increase criterion met unlike snapshot training network principal component perform desire accuracy simulation validation network hidden  due memory efficient compute concern focus achieve accuracy simplest network therefore network hidden principal component refine principal component performance principal component   performance principal component   furthermore comparison perform pca hybridize mlp classifier non pca version network pca classifier   pointwise classification error per training validation simulation snapshot analysis mention demonstrates peak performance principal component PCs therefore refine analysis focus PCs analysis snapshot classification coarse principal component snapshot classification principal component pca comparison network entire zoom detailed overview shift bound denote validation snapshot denote along axis online image network principal component achieve comparable satisfactory  training  accuracy performance network conclude  layer principal component perform combination architecture snapshot summary architecture mlp classifier structure layer PCs comparison pointwise classification error architecture neural network lack pca processing without pca preprocessing network perform comparably   impression desire specification achieve complex network architecture however training grows substantially absence pca preprocessing mlp classifier around average pca preprocessing whereas without pca preprocessing training around classification summary  network snapshot desire network snapshot outperform snapshot counterpart margin furthermore increase MCA WCA achieve compact mlp classifier therefore inverse architecture equip mlp classifier pca preprocessing snapshot per simulation regression perform parametric network architecture regressor stage described sect focus training snapshot per simulation optimize hidden hidden layer narrow principle component finally epoch classification procedure initial phase parametric dependence principal component diagram regression architecture determination snapshot per simulation training sequential stage parametric architectural choice stage arrow performance image baseline architecture hidden layer hidden metric performance metric quantiles  percent error maximum mmax percent error  percent error define     average snapshot regime median percentage error achieve stack regressor network snapshot sequence function   return quantile data   vector percentage error recover parameter parameter snapshot regime quantiles similarly average maximum error define  max  however extreme outlier shift substantially prefer median unbiased metric regression aim obtain neural network median PE network robust  unless substantial difference mmax decisive role analysis procedure analysis proceeds hidden hidden layer refining principal component finally epoch training coarse grid principal component hyperparameter entirely greedy simplicity analysis MLPs within stack mlp regressors SMLPR define sect network regressor network described layer neural network consists mlp regressors hidden layer neuron network combination supplementary interested reader regression metric layer SMLPR network  entire snapshot validation correspond continuous parameter correspond metric summary  obtain PC stiffness  obtain PC stiffness image regression metric layer SMLPR network  entire snapshot validation correspond continuous parameter correspond metric summary  obtain PC stiffness  obtain PC stiffness image regression metric layer SMLPR network  entire snapshot validation principal component pca comparison correspond continuous parameter correspond metric summary  obtain PC stiffness  obtain PC stiffness image detailed error metric network configuration analysis parameter subgroup accord parameter option investigate regression metric network principal component minimum bold italicize regression metric continuous variable amplitude stiffness wavenumber respectively performance network regression metric quantile median quantile respectively hidden subsection hidden regression performance analysis  achieve hidden wavenumber PCs  analysis obtain hidden stiffness PCs pointwise regression per training validation simulation snapshot derive regression metric baseline architecture hidden layer hidden achieves  wavenumber PCs stiffness PCs respectively increase hidden decrease  parameter  network hidden wavenumber PCs stiffness PCs respectively increase hidden significant decrease  maximum  network hidden PCs wavenumber PCs stiffness respectively contrast  increase consistent reduction mmax random extreme outlier shift noteworthy phenomenon becomes apparent network principal component minimum metric network  metric network  difference minimum metric network achieve comparable mmax proceed  network highlight hidden layer subsection seek improve accuracy increase depth network layer hidden  obtain hidden layer PC  within analysis obtain hidden layer PCs accord pointwise regression per training validation simulation snapshot hidden layer hidden layer hidden layer hidden layer derive regression metric hidden layer median PE parameter additionally  significantly amplitude wavenumber hidden layer increase  network combination amplitude PCs stiffness PCs respectively introduction layer substantial regression performance reduce mmax parameter layer increase metric  slightly   network hidden layer amplitude PCs whereas  stiffness PCs hidden layer increase performance metric desire demonstrates metric principal component exhibit comply goal metric almost snapshot  network combination amplitude PCs stiffness PCs respectively analysis additional layer SMLPR architecture hidden training principal component epoch network fifth layer brings almost improvement regression metric regression metric network hidden layer maximum minimum  network combination amplitude PCs stiffness PCs respectively SMLPR hidden layer therefore SMLPR network hidden principal component tune principal component  achieve principal component amplitude  achieve principal component stiffness indicates pca zoom pointwise regression per training validation simulation snapshot derive regression metric summarizes analysis network principal component satisfy criterion  however quantile criterion satisfied network principal component significant difference mmax quantiles PE network network principal component due network epoch training epoch regression performance investigate furthermore comparison pca hybridize non pca version network architecture investigate benefit pca hybridization performance within epoch  amplitude performance epoch  stiffness pointwise regression per training validation simulation snapshot epoch epoch along derive regression metric epoch regression metric non pca comparison network parameter epoch SMLPR network epoch individual stage SMLPR network specific dimension coordinate data principle component transformation previous investigate principle component epoch perform PC choice investigate possibility overfitting modify epoch training epoch epoch increase  amplitude error amplitude prediction beyond admissible limit stiffness wavenumber performance becomes slightly increase epoch however SMLPR network combination satisfies criterion remains layer PCs epoch regression summary finally performance network lack pca preprocessing remove pca hybridization coordinate component increase error stiffness estimation limit predefined admissible analysis SMLPR network principal component completes training whereas non pca version network completes training pca preprocessing prof beneficial equip inverse architecture  regressor layer PCs snapshot per simulation data epoch conclude investigation simulated performance recover parameter recover inverse mapping architecture comparison actual parameter performance parameter chosen display notable vertical displacement planar radial subfigure parameter subfigure invert parameter sample respective output inverse architecture inverse radial continuous parameter recover error whereas continuous parameter planar recover error predict video supplementary footnote comparison estimate subfigure vertical displacement lattice series plot vertical displacement series image comparison estimate subfigure vertical displacement lattice series plot vertical displacement series image comparison estimate subfigure vertical displacement lattice series plot vertical displacement series image comparison estimate subfigure vertical displacement lattice series plot vertical displacement series image gel instance series vertical displacement planar agreement series data simulation radial discrepancy fix timestep however fairly chaotic qualitative mismatch fix necessarily significant mismatch indeed series vertical displacement qualitative similarity obtain quantitative comparison lyapunov exponent simulation approach error dominant lyapunov exponent perform comparison planar already agreement frequency frequency oscillation unsuitable lyapunov exponent estimation chosen  prepared accord conclusion successfully inverse mapping parameter oscillate gel obtain target architecture consists integrate neural network classifier determines discrete parameter regressors continuous parameter optimal classifier principal component pca preprocessing input mlp hidden layer principal component regressor stack network architecture hidden layer principal component classification regression network snapshot successive simulation across parameter setting demonstrate efficacy developed architecture simulation recover parameter parameter qualitatively inspection vertical displacement trajectory quantitatively lyapunov exponent chaotic future seek verify approach experimental data gel keywords inverse robotics oscillate gel simulation stack multi layer perceptron