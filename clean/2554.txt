attention model become important concept neural network research within diverse application domain survey structure comprehensive overview development model attention propose taxonomy exist technique coherent category review salient neural architecture attention incorporate discus application model attention significant impact attention improve interpretability neural network finally discus future research direction attention survey succinct introduction attention model practitioner develop approach application CCS concept compute methodology neural network processing computer vision additional attention attention model neural network introduction attention model introduce machine translation become predominant concept neural network literature attention become enormously popular within artificial intelligence AI community essential component neural architecture remarkably application processing nlp computer vision CV intuition attention explain biological visual processing tends focus selectively image ignore irrelevant information manner assist perception similarly involve vision input important others instance machine translation summarization task input sequence relevant predict likewise attention model sentiment classification yelp review image caption input image relevant generate caption attention model incorporates notion relevance model dynamically attention input perform task effectively sentiment classification yelp review learns relevant furthermore delicious amaze within meaningful sentiment review rapid advancement model attention neural network primarily due model multiple task nlp machine translation summarization sentiment analysis tag computer vision image classification detection image generation modal task multimedia description visual recommender advantage beyond improve performance task extensively improve interpretability neural network otherwise model notable benefit mainly fairness accountability transparency machine model application influence overcome challenge recurrent neural network rnns performance degradation increase input computational inefficiency sequential processing input organization aim brief comprehensive survey attention model intuition concept attention regression model briefly explain propose attention function taxonomy discus neural architecture application attention widely apply respectively finally attention facilitate interpretability neural network conclude article future research direction related survey domain specific survey attention focus computer vision graph processing however incorporate accessible taxonomy architecture application interpretability aspect contribution foster broader understand AI developer engineer approach application domain attention  attention understood regression model propose  watson training data instance comprise feature correspond target predict target query instance naive estimator predict average acm transaction intelligent technology vol article publication date october attentive survey attention model encoder decoder architecture traditional attention model target training instance  watson propose approach estimator average correspond relevance training instance query function encodes relevance instance predict choice function normalize gaussian kernel similarity normalization author estimator consistency training data converges optimal simplicity parameter information data attention mechanism model generalization allows function attention model propose sequence sequence model task sequence sequence model consists encoder decoder architecture encoder rnn input sequence token input sequence encodes fix vector decoder rnn fix vector input generates output sequence token token output sequence denote hidden encoder decoder respectively challenge traditional encoder decoder challenge traditional encoder decoder framework encoder compress input information fix vector decoder fix acm transaction intelligent technology vol article publication date october encoder decoder architecture traditional attention model function traditional encoder encoder decoder decoder attention encode context  αij eij eij decode generate input sequence input sequence hidden encoder context vector attention input decoder hidden output token non linear function alignment function distribution function vector compress detailed input sequence loss information unable model alignment input output sequence essential aspect structure output task translation summarization intuitively sequence sequence task output token influence specific input sequence however decoder lack mechanism selectively focus relevant input token generate output token aim mitigate challenge decoder access entire encode input sequence central induce attention input sequence prioritize relevant information generate output token usage attention correspond encoder decoder architecture attention attention architecture responsible automatically attention αij capture relevance encoder hidden decoder hidden query hidden decoder emit attention building context vector input decoder decode context vector sum hidden encoder correspond attention  additional context vector mechanism decoder access entire input sequence focus relevant input sequence improvement performance task improves quality output due alignment concept mathematically difference encoder decoder architecture attention composition context vector traditional framework context vector hidden encoder attention framework context decode combination hidden encoder correspond attention  attention attention incorporate additional neural network within architecture network learns attention αij function encoder hidden decoder hidden input neural network function alignment function denote relevant encoder hidden decoder hidden alignment function output eij fed distribution function denote convert attention function differentiable attention encoder decoder acm transaction intelligent technology vol article publication date october attentive survey attention model  alignment function function equation reference similarity sim dot dot  bias activate  generalize kernel concat impact additive impact wki   wki location feature impact alignment function query sim similarity function cosine input  trainable parameter trainable bias activation function model becomes differentiable function jointly encoder decoder component architecture backpropagation generalize attention model attention model mapping sequence attention distribution accord query encoder hidden query decoder hidden attention distribution αij emphasizes relevant task respect query additional input attention distribution apply generally mapping although core attention model propose distinguish exist literature terminology representation input data hence generalize attention model query concrete regression task estimator explain instance query training data label alignment function denote distribution function denote query combine attention discus commonly alignment function distribution function literature refer reader detailed discussion alignment distribution function alignment function category alignment function notion query representation representation approach compute cosine similarity dot query representation account representation dot employ normalizes dot representation vector acm transaction intelligent technology vol article publication date october function assume query representation vector alignment extends dot query representation introduce learnable transformation matrix query vector bias alignment allows global importance irrespective query introduce bias activate alignment nonlinear activation layer hyperbolic tangent rectifier linear exponential linear recently query generalize kernel function instead commonly dot formulation alignment function category alignment function combine query joint representation simplest model approach concat alignment joint representation concatenate query additive alignment reduces computational decouple contribution query allows precomputing contribution avoid computation query contrast neural layer additive alignment alignment employ multiple neural layer alignment function specific location alignment ignores depends alignment associate compute function independently content item dimensional 2D patch image 1D temporal sequence derive feature standard deviation representation individual belonging input alignment function additive alignment article distribution function distribution function alignment function attention commonly distribution function logistic sigmoid softmax function ensure attention constrain sum interpret probability relevant softmax function attention interpret probability correspond relevant variant employ softmax transformation attention mechanism dense alignment density wasteful model interpretable assign probability implausible output distribution function  sparse  sparse alignment assign nonzero probability plausible output sparse distribution useful application document summarization task irrelevant finally compositional attention network introduce distribution function quasi attention elementwise multiplication tanh    negation outer distance vector contrast traditional attention vector secondary interpret gate mechanism deletes token irrelevant query contribution zero seminal model propose attention mechanism sequence sequence task encoder decoder architecture core remains extension attention model propose literature specific formulation alignment distribution function literature attention formulation considerably attention mechanism neural architecture application domain component attention model technique acm transaction intelligent technology vol article publication date october attentive survey attention model component attention model technique remainder survey discus taxonomy attention neural architecture difference apply application taxonomy attention attention category elucidate attention within category category mutually exclusive category dimension along attention employ application multi attention combination concept comprehensible technical specify multiple attention within propose approach sequence involves input correspond output sequence attention refer distinctive query belong distinct input output sequence respectively attention model employ translation image caption recognition within distinctive attention attention model operates multiple input sequence jointly learns attention capture interaction input coattention model visual author argue addition model visual attention input image important model attention text equally important attention image representation attention vice versa acm transaction intelligent technology vol article publication date october characteristic attention within category category characteristic sequence distinctive query distinct sequence attention multiple input sequence useful model multi modal data query sequence popular attention recent abstraction attention compute input multi hierarchical attention multiple abstraction input performance hierarchy text image computational increase global computationally intensive nice differentiable training objective sample predictor computationally efficient lack differentiability training objective local attention around tradeoff efficient locally differentiable representation multi multiple representation input representational selects representation relevant downstream task multi computes relevance dimension input dimensional extract contextual meaning input dimension essentially simultaneously detect correspond image relevant similarly attention visual task contrast task text classification recommendation input sequence output sequence scenario attention relevant token input sequence token input sequence query belong sequence attention purpose attention inner attention propose understand input sequence vector representation sequence input sequence attention layer output another sequence αij attention aim capture sequence related concept relevance depends task abstraction attention compute input sequence attention however attention apply multiple abstraction input sequence sequential manner output context vector abstraction becomes query abstraction additionally model multi attention classify abstraction illustrate category attention model abstraction document classification task model hierarchical attention model ham capture hierarchical structure document document acm transaction intelligent technology vol article publication date october attentive survey attention model summary technical approach AMS reference application category sequence abstraction representation machine translation distinctive image caption distinctive machine translation distinctive local document classification multi recognition distinctive visual attention multi sentiment classification attention multi recommender multi understand multidimensional text representation  applicable multi attention allows ham extract important important document attention representation attention apply sequence embed vector aggregate representation attention representation document representation document feature vector classification task stack attention network sans propose category mainly employ multiple layer iteratively refine attention combine information query previous attention layer author sans image task multiple attention layer query image multiple progressively image highly relevant author global image presentation predict sub optimal attention scatter within layer multiple attention layer attention layer utilize knowledge attention layer visual information refine query vector information extract grain within image attention layer layer improve performance attention described belongs multilevel category attends image combination attention multi attention depict zhao zhang propose attention via attention multi acm transaction intelligent technology vol article publication date october propose visual task combination attention visual text multi attention attention learns attention fashion category difference arise input sequence attention function calculate attention introduce attention suggests average hidden input sequence context vector usage weigh neural network amenable efficient backpropagation quadratic computational propose attention model context vector compute stochastically sample hidden input sequence accomplish  distribution parameterized attention attention model beneficial due decrease computational decision input render framework non differentiable optimize category mutually exclusive variational policy gradient reinforcement propose literature overcome limitation propose attention model namely local global context machine translation task global attention model attention model local attention model however intermediate attention detect attention within input sequence around local attention model within input sequence monotonic alignment predictive function predictive alignment consequently advantage local attention parametric tradeoff attention computational efficiency differentiability within representation generally feature representation input sequence application however scenario feature representation input suffice downstream task approach capture aspect input acm transaction intelligent technology vol article publication date october attentive survey attention model multiple feature representation attention assign importance representation relevant aspect disregard redundancy input refer model multi representational relevance multiple representation input downstream application representation combination multiple representation attention benefit attention directly evaluate embeddings prefer specific downstream task inspect attention embeddings input improve representation similarly attention dynamically weigh feature representation capture lexical syntactic visual genre information intuition multi dimensional attention induced relevance dimension input embed vector intuition compute feature vector feature token specific meaning context useful application embeddings suffer polysemy approach effective embed representation understand network  attention salient neural architecture conjunction attention encoder decoder framework transformer circumvents sequential processing component recurrent model attention memory network extend attention beyond input sequence graph attention network gat neural architecture extensively become popular choice application domain however explore within various neural architecture active research topic neural architecture encoder decoder attention rnn encoder decoder framework encode input consequently attention widely architecture input representation reduce fix context vector decode allows decouple input representation output exploit benefit introduce hybrid encoder decoder popular convolutional neural network cnn encoder rnn memory lstm decoder architecture particularly useful multi modal task image video caption visual recognition however input output sequential aforementioned formulation sort salesman pointer network another neural model difference output discrete input sequence hence pointer network target output depends input hence variable cannot achieve traditional encoder decoder framework output priori model author achieve attention model probability ith input output approach apply discrete optimization  sort acm transaction intelligent technology vol article publication date october transformer architecture transformer recurrent architecture rely sequential processing input encode computational inefficiency processing cannot parallelize address author propose transformer architecture completely eliminates sequential processing recurrent connection relies attention mechanism capture global dependency input output author demonstrate transformer architecture achieve significant parallel processing shorter training accuracy machine translation without recurrent component transformer architecture transformer mainly relies attention mechanism relates token within input sequence author propose novel dot alignment function attention mechanism explain attention multi attention layer stack parallel linear transformation input compute attention multi mechanism split input fix computes dot attention parallel independent attention output concatenate dimension architecture compose stack identical layer encoders decoder sub layer pointwise network FFN layer multi attention layer pointwise FFN linear transformation apply sequence independently increase parallel processing decoder encoder decoder contains multi attention sub module instead multi attention sub module masked prevent attend future additional feature architecture usage positional encode positional encode input acm transaction intelligent technology vol article publication date october attentive survey attention model sequential demand model temporal aspect input information component capture positional information rnns cnns account encoder phase transformer generates content embed positional encode token input sequence finally normalization residual connection mechanism model faster accurately transformer capture global dependency input output parallel processing minimal inductive bias prior knowledge demonstrate scalability sequence datasets domain agnostic processing multiple modality text image processing consequently transformer architecture become approach mainstream nlp computer vision modal task moreover increase attention model variety application transformer architecture significant milestone attention variant transformer application grown extent individual survey publish topic variant transformer application nlp computer vision modal task respectively however although transformer undoubtedly improvement rnn sequential model limitation input text split fix context fragmentation parametric complexity computational resource training data requirement due minimal inductive bias difficulty interpret attention mechanism learns contribution input token prediction consequently multiple establish introduce improvement transformer direction research analyzes multi attention transformer perform analysis transformer pre model bidirectional encoder representation bert correspond relation determiner preposition layer consists entropy bag vector representation broader attention span evaluate contribution attention encoder overall performance model vast majority prune without affect performance important specialized linguistically interpretable function model perform norm analysis transform input vector reveal contrary previous bert attention token alignment extract attention mechanism transformer attention layer individually reduce attention without affect performance increase efficiency propose novel technique prune another aim improve attention span context attention longer efficient flexible transformer XL solves context fragmentation adopt positional encode reuse hidden another propose novel attention mechanism optimal attention span allows extend context transformer maintain computational finally computational transformer grows quadratically sequence hence important research direction reduce computation memory consumption sparse transformer reformer performer recent approach acm transaction intelligent technology vol article publication date october memory network architecture memory network application QA chat bot ability information database input network knowledge database query relevant query others external memory knowledge database attention crucial selectively focus relevant generalization attention wherein instead model attention sequence database sequence approach literature couple external memory component attention namely memory network dynamic memory network neural turing machine memory network generally component raw database convert distribute representation feature vector output reader understood memory sequence retrieve later necessarily without exploit content memory sequentially perform task ability attention content memory memory network MemNN derive enable training via backpropagation memory network supervision applicable wider nlp task model allows multiple hop memory generate output token crucial performance task architecture broken architecture relevant query knowledge database inner query memory vector softmax operation query calculate context vector relevant attention dynamic memory network DMN episodic memory module chooses input focus attention mechanism output memory vector representation iteratively conditioning attention previous memory representation allows module attend acm transaction intelligent technology vol article publication date october attentive survey attention model input iteration retrieve additional information irrelevant previous iteration overview architecture trigger gate input episodic memory module episodic memory multiple episode iteration input module another demonstrates dynamic memory network image input module extract feature vector image  network fed episodic memory module neural turing machine ntm continuous albeit memory representation along controller typically network lstm dictate operation memory turing machine differentiable endto efficiently gradient descent attention access memory selectively constrain operation interact portion memory interaction memory highly sparse however ntm memory content address access infer algorithm copying sort associative recall input output advantage memory network information memory effectively  via attention mechanism focus relevant memory MemNN superior performance task model rnns lstms  sentiment analysis tag graph attention network gat application domain social network citation network protein protein interaction  naturally graph structure data arbitrary graph therefore considerably harder sequence image sequence image graph highly rigid regular connectivity sequence adjacent pixel image neighbour pixel generalize convolutional layer image graph convolutional layer innovation enable computational storage efficiency model memory fix parameter parameter input graph model becomes  localisation model independently local neighbourhood node enable parallel computation scalability ability specify arbitrary importance neighbour correctly incorporate relevance node treat equally structural node applicability arbitrary unseen graph structure graph convolutional network gcn performance node classification task combine local graph structure node feature however gcn assigns explicit non parametric node propose  employ attention node feature important node  computationally efficient operation attention layer parallelize eigendecomposition computationally intensive matrix operation moreover  assign importance node neighborhood via attention enable model capacity GCNs finally attention mechanism apply manner graph therefore upfront access global graph structure acm transaction intelligent technology vol article publication date october importance graph convolutional network graph attention network http  com blog gat graph usually multi node widely heterogeneous graph heterogeneous graph comprehensive information richer semantics therefore useful data mining task however due complexity heterogeneous graph homogeneous graph approach cannot directly apply heterogeneous graph extend graph attention heterogeneous graph hierarchical attention node feature input transformation matrix project node feature node attention attention node meta connection define multiple node semantic attention aim attention meta specific task heterogeneous graph attention model optimal combination multiple meta hierarchical manner enables node embeddings capture complex structure semantic information overall model optimize via backpropagation manner APPLICATIONS attention model become active research intuition versatility interpretability variant attention model address unique characteristic diverse application domain application attention model significant impact performance task whereas others representation entity document image graph attention entirely transform application become popular choice technique machine translation pre embeddings bert application mainly discus attention model application domain instance application within domain seminal become investigation attention model application domain nlp computer vision multi modal task recommender graphical acm transaction intelligent technology vol article publication date october attentive survey attention model summary application AMS application domain application seminal processing machine translation summarization text classification representation sentiment analysis   pre model computer vision image classification image generation detection image synthesis multi modal task multimedia image video description recognition qin visual tan bansal communication comprehension recommender user profile  kang  item user representation exploit auxiliary information graph graph classification graph sequence generation node classification  hyperedge detection processing nlp nlp domain attention assist focus relevant input sequence alignment input output sequence capture dependency longer sequence instance model attention neural technique machine translation allows alignment crucial MT automatic alignment capture location advantage attention model becomes apparent translate longer longer harder embed content alignment information vanilla technique without attention performance improvement machine translation attention  attention model machine translation gru gate mechanism useful avoid computation context vector decode discriminatory acm transaction intelligent technology vol article publication date october attention understand focus relevant amount information memory network another seminal significant advancement abstractive summarization task attention mechanism data driven approach proven challenge task summarization propose significant gain exist baseline transformer model enhance mechanism abstractive summarization attention transformer mechanism focus important source text extract summary sentiment analysis task attention focus important sentiment input couple approach aspect sentiment classification incorporate additional knowledge aspect related concept model attention appropriately weigh concept apart content sentiment analysis application multiple architecture attention memory network transformer application within nlp domain employ attention model extensively text classification text representation mention earlier text classification text representation mainly attention effective document representation embeddings multilevel attention whereas propose multi dimensional propose multi representational attention model application nlp completely  advent pretrained model pre model proven extremely beneficial due corpus universal representation capture facet dependency hierarchical relation sentiment easily tune multiple downstream nlp task significantly label data avoid training model scratch democratize development nlp application easy model building transformer pre model popular recently transformer variant transformer XL bart bidirectional bert variant RoBERTa albert generative pre transformer GPT variant GPT GPT XLNet combine bert transformer XL although transformer nlp task transformer XL understands context beyond fix limitation transformer longer dependency critical achieve performance sequence bart transformer architecture reconstruct text corrupt input text arbitrary function however bert directional encoder allows model context RoBERTa improves bert dataset training training model iteration remove sequence prediction training objective albert parameter reduction technique factorize embed parameterization layer parameter reduce parameter faster training finally XLNet combine capability bert transformer XL achieve performance nlp task inference sentiment analysis document rank acm transaction intelligent technology vol article publication date october attentive survey attention model OpenAI GPT unsupervised model giant collection text corpus GPT specifically multi layer decoder transformer generate embeddings usage downstream nlp task tune model successor GPT GPT GPT GPT alternate dense sparse attention sparse transformer transformer model billion parameter performance dozen nlp task GPT model impressive capability outrank pre model computer vision CV visual attention become popular CV task focus relevant within image capture structural dependency image visual attention conceive attention propose image classification task author attention relevant location within input image reduce computational complexity cnns processing resolution crucial computational complexity propose model irrespective input image cnns computational complexity linearly increase image image pixel visual attention significant benefit detection aid localize recognize within image author attention multiple detection image sequential manner glimpse predict hence sequence label generate multiple model recognize recurrent attentive writer exploit attention image generation although encoder decoder framework compress  image training difference previous generates image fashion pas accomplish attention selectively attend input image regenerate specific scene within image iterative manner attention generative adversarial network attention mechanism convolutional gans calculate response sum feature capture dependency efficiently convolution alone information local neighborhood attention module complementary convolution model multi dependency across image efficiently although cnns become dominant model vision task cnns specifically image computationally demand generation efficient scalable domain agnostic architecture transformer vision task become research direction direction vision transformer  directly transformer architecture image patch along positional embeddings image classification task outperform comparable cnn computational resource another novel distillation approach transformer perform image classification without pre training external dataset efficient  similarly detection transformer  detection framework rely transformer building  flatten output cnns positional encoding input directly generate output bound streamline pipeline remove component acm transaction intelligent technology vol article publication date october anchor generation non maximum suppression exist approach deformable attention module reduce training  lower computational finally transformer image generation task image transformer image GPT sequentially predict pixel output image previously generate pixel multi modal task attention extensively multi modal application understand relationship modality multimedia description task generate text description multimedia input sequence image video similarly QA attention performs function relevant input image predict caption focus subset frame video description recognition author without attention model significantly  data memorizes training transcript without really attention acoustic recognition differs nlp CV task input noisier lack structure multiple fragment author propose attention mechanism account location content important fragment input sequence adapt attention mechanism incorporate location longer input sequence recognition fragment qin propose novel approach visually inspect encoder representation understand attention mechanism recognition task another communication comprehension address challenge comprehend communication complex multi modal task involve vision modality simultaneously attention specifically discover interaction modality dynamic author demonstrate approach performance multiple task speaker trait recognition emotion recognition transformer extensively vision task generic representation effectively encode modal relationship transformer domain multi model   multi model input transformer input automatically discovers relationship domain however   multi category independent transformer modality later modal representation another attentional transformer recommender attention significant usage recommender user profile user item representation exploit auxiliary information knowledge graph social network user profile aim assign attention interact item user capture effective manner intuitive interaction user relevant recommendation item user transient varied span attention mechanism relevant item user improve recommendation collaborative filter acm transaction intelligent technology vol article publication date october attentive survey attention model rnn sequential model effective user item representation recommender consequently attention domain recommendation attention effectively combine user item embeddings domain generate embed user item similarly han effective item representation review rating recommendation specifically author hierarchical attention leverage hierarchical structure review intra review inter review similarly tier han user item representation review review informativeness model user item attention helpful utilize auxiliary information recommender effectively propose knowledge graph attention network hybrid graph link user item interaction item attribute construct exploit relation node embed compute attention neighbour consist user item attribute han attend aspect affect user preference image recommendation upload social influence owner  attention individual aspect representation aspect aspect equally important user preference finally recent proposes social recommendation model explores user social network content recommendation propose user context vector attend context vector knowledge aggregate user social network utilized recommendation graph important datasets graph network social network knowledge graph protein interaction network web attention highlight graph node subgraphs relevant task approach compute attention embeddings node subgraphs combination attention architecture graph efficient parallelizable across node apply graph node directly applicable inductive task model generalize completely unseen graph contrast graph convolutional network attention mechanism graph allows assign importance node neighborhood enable leap model capacity analyze attention benefit interpretability attention machine task graph structure data node classification link prediction graph classification graph sequence generation attention hyperedge prediction hypergraphs hypergraphs mainly interaction graph hyperedges multiple node exist analyze pairwise interaction unable effectively capture interaction graph consequently author propose attention gat hyperedge prediction homogeneous heterogeneous hypergraphs variable hyperedge acm transaction intelligent technology vol article publication date october visualization attention attention interpretability interpretability AI model driven performance transparency fairness model however neural network particularly architecture criticize lack interpretability model attention particularly perspective interpretability allows directly inspect internal architecture hypothesis magnitude attention correlate relevant specific input prediction output sequence easily accomplish visualize attention input output  attention important explain inner working neural model attention model focus relevant input sequence generate output summarization task input combat attention output http  org acm transaction intelligent technology vol article publication date october attentive survey attention model demonstrates attention model capture relationship summarization attention recognize user user preference cartoon video user prefers video extensive visualization relevant image attention significant impact generate text image caption task similarly visualize attention clearly automatic alignment french english despite location attention model non monotonic alignment correctly align   marine environment summarize finding explore gender bias occupation classification attention classification task gendered importance context dependent sentiment review author inspect attention distribution span model capture diverse context assign context dependent recognition attention output audio signal correctly identify audio signal attention acoustic similarity finally multi representational attention assign glove fasttext embeddings representation particularly glove frequency another application attention visualize attention neural network goal interpret perturb attention simulate scenario prediction interactively despite popularly shed inner neural network attention model explainability remains active research article contradictory viewpoint challenge usage attention explanation model behaviour decision application attention model nlp task jain wallace argue attention correlate typical feature importance analysis moreover perform analysis sensitivity prediction attention attention random permutation adversarial training output prediction  smith apply analysis intermediate representation erasure attention noisy predictor relative importance specific input sequence treat justification model decision conclusion survey attention formulate literature attempt overview various technique taxonomy attention neural network architecture attention application domain significant impact incorporation attention neural network significant gain performance insight neural network inner facilitate interpretability improve computational efficiency eliminate sequential processing input survey understand direction research topic technique developed apply domain conclude survey emerge research direction attention model acm transaction intelligent technology vol article publication date october attention neural machine translation model encode decode entire happens sequentially however application video caption conversation demand machine translation model generate translation reading entire source chiu  monotonic  attention adaptively split input sequence chunk attention compute online linear decode enable online decode transformer monotonic multi attention alternate encode decode increase demand application online attention important future research alone attention introduce attention model cnns computer vision performance gain attention alone primitive vision model instead augmentation convolution investigate alone attention vision model replace instance spatial convolution attention local pure attention vision model compete model benchmark vision datasets enables perform attention within global factorize 2D attention 1D attention model distillation application recommender strict latency constraint online model pre model bert considerable performance improvement parameter inapplicable online model distillation aim compress exist complex model simpler model retain accuracy attention model deeply mimic attention module model teacher moreover introduce teacher assistant distillation pre transformer model similarly employ teacher strategy specific transformer ensure learns teacher attention attention interpretability explore relationship attention model interpretability active research future research investigate attention distribution model modify plausible justification model prediction lstm encoders hidden representation timesteps random permutation attention affect model prediction propose diversity driven lstm  technique ensure hidden farther away spatial dimension modify lstm generate attention precise importance rank hidden indicative important model prediction correlate gradient attribution acm transaction intelligent technology vol article publication date october attentive survey attention model auto attention automate neural network architecture neural architecture NAS outperform various task NAS optimal architecture attention module attempt extend NAS plug attention module beyond backbone architecture define novel concept attention module attention attention utilize differentiable optimal attention module efficiently multi instance attention exist attention mechanism attend individual item memory fix granularity token pixel image grid multi instance attention generalization allows attend structurally adjacent item 2D image subsequence technique attention multiple instance attention approach model vector item alternatively richer representation derive feature standard deviation vector within formulation useful explore attention mechanism item dynamic multi agent understand model behavior multi agent application autonomous vehicle multi player attention mechanism generative model model interaction within multi agent attention capture behavior generate multi agent identify agent interact scalability transformer model extraordinary achieve nlp computer vision application however training deploy model prohibitively costly sequence bioinformatics standard attention mechanism transformer respect sequence important theme research reduce quadratic complexity transformer linear without loss performance model propose approach learnable mask function dynamically attention span attention mechanism allows increase maximum context transformer without increase computational introduce performer approximate softmax rank attention linear complexity demonstrate attention mechanism approximate rank matrix exploit propose attention mechanism reduces overall attention complexity quadratic linear linear transformer  performs par standard transformer model propose predictor utilizes lstm model dynamically predict attention relationship token automatically attention sequence