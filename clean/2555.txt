neural network become popular recent astonish various domain image recognition domain specific architecture neural network convolutional network structure domain therefore perform  effective however neural network universal across domain indeed without structure data somewhat limited neural network perform respect traditional machine random article carefully neural network random structure generalization ability architecture powerful random propagation algorithm reduces powerful generalize construct decision furthermore approach efficient constant factor training efficiency allows training multiple neural network improve generalization accuracy experimental benchmark datasets demonstrate effectiveness propose enhancement classification regression CCS concept compute methodology machine machine algorithm additional neural network random classification regression introduction neural network become increasingly popular recent tremendous computer vision recognition processing task regularly recent challenge domain structure domain allows specialized neural network architecture convolutional neural network advantage aspect spatial locality image image processing specialized data domain attribute exhibit characteristic spatial temporal behavior exploit carefully neural network architecture characteristic certainly data domain dataset drawn application unknown characteristic behavior spite neural network specific domain replicate across domain random regularly outperform neural network arbitrary domain underlie data domain specific insight architecture underlie neural network neural network highly prone overfitting generic layer architecture computation without domain specific insight performance neural network sensitive specific architecture computational although convolutional neural network architecture image domain analyst neural network architecture domain specific dataset poorly application domain contrast decision generalist shelf package  outperform classifier recent evaluate classifier entire uci collection datasets conclude random perform classifier performance classifier statistically significant multiple implementation random virtually implementation performance multiple implementation classifier random specific implementation inherent merit approach furthermore datasets uci repository drawn vast variety domain specific narrow data image suggests performance random robust irrespective data domain random neural network important characteristic ability model arbitrary decision boundary argue respect neural network somewhat powerful amount data available however neural network highly prone overfitting whereas random extremely robust overfitting randomize ensemble approach overfitting neural network artifact parameter construct model convolutional neural network drastically reduce parameter specific insight data domain image strongly suggests choice neural network architecture drastically reduces parameter domain specific insight improve accuracy domain specific insight engineer architecture neural network reduce parameter footprint article inspiration successful classification random engineer architecture neural network furthermore architecture improve random model leverage inherent neural network architecture carefully model random capable approximate arbitrary decision boundary overfitting datasets noteworthy propose simulate output decision random algorithm specific dataset already construct approach construct decision random dataset simulate specific instantiation random neural network therefore construct random input algorithm acm transaction intelligent technology vol article publication date october random inspire neural network approach defeat purpose neural network  specific instantiation random model random model modification article propose fundamentally approach architecture neural network construct model randomize decision although simulate specific random approach construct neural network interpret randomize decision therefore neural network perform directly propagation specific instantiation random input however mapping exists arbitrary random neural network mapping exists interestingly mapping convolutional neural network although random specialized structure image domain article focus neural network architecture random structure classification regression ability reduce overfitting contribution article propose novel architecture decision neural network randomize decision ensemble neural network propose framework neural network random structure NNRF decision function neural network backward propagation complexity reduce possibility overfitting datasets conduct extensive demonstrate effectiveness propose framework classification regression remain article organize introduce randomforest inspire architecture detailed propose framework NNRF classification training algorithm complexity analysis extend NNRF regression conduct demonstrate effectiveness NNRF classification regression analyze parameter sensitivity NNRF briefly review related conclude future random  architecture introduce architecture neural network throughout article matrix bold wij vector denote bold lowercase pij denotes entry denotes ith jth respectively similarly denotes ith conventional neural network node input layer cleanly hidden layer however propose neural network separation exist node input hidden layer internal node completely hidden input feature neural network hierarchical architecture decision furthermore random contains multiple independent decision approach multiple independent neural network randomize architecture randomize choice input hidden layer later neural network extremely efficiently approach extremely appeal layer neural network denote height decision random neural network simulate acm transaction intelligent technology vol article publication date october illustration decision structure neural network architecture classification input parameter algorithm layer neural network neural network structure exactly binary decision node simulate split output exactly active output unique node layer neural network therefore node layer node neural network addition output node input node layer combine prediction label although node modest setting neural network node simulate powerful split furthermore structure neural network parameter modest node important factor avoid overfitting another parameter input algorithm isr feature randomly perform split node traditional random bag feature split node randomly similarly architecture neural network node bag feature randomly fix architecture neural network inherently randomize input feature node later particularly useful ensemble overall architecture layer neural network classification illustrate explanation node nij jth node ith layer node nij node node layer node network contains node node layer input node node input randomly chosen feature input data node output inactive node layer somewhat unconventional perspective neural network hybrid hidden input layer input ancestor node treelike neural structure input randomly chosen feature therefore node hybrid node belonging hidden layer input layer input visible input another important node hidden input output node crucial acm transaction intelligent technology vol article publication date october random inspire neural network ensure activate training instance treelike neural network structure output node combine output node prediction however activate incoming output nonzero output node input decision activate neural network particularly important propagation algorithm update node along crucial efficiently perform update therefore training phase neural network extremely efficiently however random multiple miniature neural network prediction overall prediction ensemble approach random instance predict neural network independently prediction average neural network decision neural network activate instance propagation extremely efficient neural network potentially powerful function compute internal node powerful univariate split propagation algorithm powerful typical training decision myopic node propagation effectively adjusts split criterion leaf inform split criterion deeper understand training data PROPOSED NNRF classification previous overall architecture neural network inner neural network classification extend neural network regression introduce inner decision node guarantee activate introduce efficiently perform propagation complexity detail propose neural network fij input feature node nij fix calculate output bias activation function tanh leaky relu leaky relu advantage alleviate  net proven outperform tanh leaky relu derivative activate specifically define signal vector acm transaction intelligent technology vol article publication date october indicator function otherwise activate node node layer layer arrow denote signal output calculate otherwise otherwise equation equation input signal node inactive simply output however input feature calculate guarantee activate layer meaning node activate layer procedure sij pij output layer pij sij otherwise sij otherwise easy verify pij node correspond bias equation sij none activate depth neural network output input aggregator aggregator aggregate input vector simplicity denote aggregate vector softmax function apply exp exp RC softmax RC bias probability distribution input data belongs probability input data sample belongs estimate distribution function define entropy truth distribution RC cod truth label label otherwise minimize entropy estimate distribution acm transaction intelligent technology vol article publication date october random inspire neural network reduce neural network truth distribution neural network prediction training data matrix cod label matrix data sample feature objective function min  wij bij parameter  neural network estimate distribution input data regularizer associate avoid fitting define sij wij bij regularizer instance specific computation instance active parameter active update propagation detail therefore instance specific regularizer appropriate scalar contribution regularizer efficient backpropagation propagation structure neural network focus function equation perform efficient propagation extension equation simplicity notation denote derivative wij involves wij pij however nij inactive input signal nij pij independent wij derivative wij update wij safely remove inactive node neural network reduces structure neural network neural network reduce network network assume active simplicity explanation rewrite bias ith layer simplify neural network feature node  link layer ith layer andp perform propagation simplify network efficient involves parameter network narrow detail derivative define error acm transaction intelligent technology vol article publication date october  similarly ind ind index  training algorithm NNRF classification algorithm layer structure neural network classification algorithm bootstrap sample input feature node neural network update parameter propagation random aggregate independently neural network classification NNRF algorithm NNRF algorithm input predict distribution aggregate predict distribution neural network label predict probability arg  complexity active input data sample perform propagation depth equation nonnegative softmax equation therefore propagation data sample similarly backward propagation equation equation training structure neural network depth epoch training neural network random usually chosen feature choice approximately modest training efficient acm transaction intelligent technology vol article publication date october random inspire neural network algorithm decision structure neural network ensure layer structure neural network bootstrap sample construct fij feature random feature propagation backward propagation update parameter convergence return structure neural network algorithm NNRF ensure layer structure neural network construct structure neural network algorithm return parameter parameter decision neural network wij RC parameter approximately usually ensures dimension classification usually usually chosen complexity approximately modest data PROPOSED NNRF regression extend NNRF regression introduce extend  structure neural network regression ensemble neural network propose NNRF regression structure neural network regression illustration decision structure neural network regression structure structure neural network classification feature sample decision function classification adopt regression difference layer loss function structure neural network classification aggregate feature input softmax function generate empirical distribution predict label regression instead softmax predict label linear regression predict numerical target acm transaction intelligent technology vol article publication date october illustration decision structure neural network architecture regression bias loss function define euclidean distance predict truth target minimize euclidean distance predict truth target training data target vector data sample objective function regression min  wij bij parameter  neural network estimate input data scalar regularizer avoid fitting define sij wij bij similarly propagation decision structure neural network regression decision structure neural network regression classification structure layer loss function derivation propagation regression classification detail latter omit detailed derivation regression algorithm  structure neural network regression training structure neural network classification algorithm algorithm NNRF regression algorithm structure neural network regression random regression aggregate predict independently neural network specifically input predict numeric aggregate predict neural network acm transaction intelligent technology vol article publication date october random inspire neural network activate backward propagation complexity NNRF regression  epoch training neural network parameter NNRF regression usually chosen within experimental RESULTS conduct evaluate effectiveness propose framework NNRF classification regression introduce datasets experimental NNRF classifier demonstrate effectiveness NNRF classification NNRF classical representative regression effectiveness NNRF regression conduct investigate hyper parameter NNRF classification regression respectively datasets experimental setting classification conduct publicly available benchmark datasets uci datasets mapping ForestType alphabet isolet sonar signal sonar chemical analysis wine wine wisconsin diagnostic breast cancer  vehicle silhouette vehicle  diagnosis  image datasets image coil image handwritten digit usps image MSRA  dataset bioinformatics movement dataset movement datasets domain format comprehensive understand NNRF performs datasets various domain format statistic datasets summarize datasets datasets algorithm amount parameter datasets regression conduct publicly available benchmark datasets uci datasets community crime crime  blog feedback blog wisconsin prognostic breast cancer  relative location CT slice  cpu activity dataset  structure activity dataset  crime predict community crime ratio  subset uci sensor array drift dataset uci sensor array drift dataset predict concentration chemical compound sensor expose subset predict concentration  blog predict comment blog  predict recur  estimate relative location CT slice axial axis  predict cpu activity performance measurement  predict inhibition    statistic datasets regression summarize target minimal uci datasets classification available http archive uci edu datasets html http columbia edu cave software  coil php http csie ntu edu cjlin  datasets multiclass html usps http  file  http   keel dataset php cod sub uci datasets regression available http archive uci edu datasets html http toronto edu delve data datasets html http csie ntu edu cjlin  datasets regression html acm transaction intelligent technology vol article publication date october statistic datasets classification dataset sample feature coil ForestType isolet bioinformatics sonar usps wine movement MSRA  vehicle  statistic datasets regression dataset sample feature target crime   blog    maximal target predict RMSE mae dataset similarly datasets datasets domain algorithm usually cannot datasets evaluate classification ability propose framework NNRF widely classification evaluation metric micro macro adopt micro macro classifier evaluate regression performance propose framework NNRF RMSE absolute mae classical evaluation metric quality regression RMSE mae closer predict numerical truth classification performance comparison propose framework NNRF classical classifier evaluate classification ability NNRF detail classifier LR logistic regression maximum entropy classifier popular generalize linear regression model classification implementation  acm transaction intelligent technology vol article publication date october random inspire neural network svm vector machine classical popular classifier hyperplane margin libsvm rbf kernel classification NN hidden layer neural network softmax output layer entropy classifier implementation kera popular neural network toolbox DBN belief network popular generative neural network compose multiple hidden layer layer restrict boltzmann machine extract hierarchical feature DBN classification softmax classifier DBN DBN tune softmax classifier dnn layer neural network relu activation function alleviate gradient vanish neural network adopt adam stochastic optimization hidden node layer tune via validation RF random classical popular ensemble aggregate independent decision classification powerful classifier implementation scikit gcForest gcForest stack RFs stack hidden layer nns output RFs layer concatenate feature input RFs propose alternative net classification implementation author classifier parameter fold validation training data parameter data involve parameter tune specifically NNRF empirically  sensitivity parameter classification performance NNRF analyze detail conduct fold validation average performance standard deviation macro  report respectively observation generally classifier LR svm NN DBN dnn RF gcForest performance macro micro datasets performs datasets observation reference generally RF achieves performance majority datasets classifier performance NN dnn datasets ForestType isolet sonar movement NN slightly dnn suggests deeper datasets NNRF outperforms classifier majority datasets although NNRF structure RF exploit powerful neural network node decision linear combination feature non linear function furthermore propagation algorithm receives feedback leaf node therefore inherently construct split internal node deeper understand training data decision RF performance http github com  gcForest acm transaction intelligent technology vol article publication date october classification macro std classifier datasets dataset LR svm NN DBN dnn RF gcForest NNRF coil ForestType isolet  sonar usps wine movement MSRA  vehicle  classification micro std classifier datasets dataset LR svm NN DBN dnn RF gcForest NNRF coil ForestType isolet  sonar usps wine movement MSRA  vehicle  summary propose framework achieve classification combine random structure activation function neural network comparison report NNRF NNRF RF dependent comparison NNRF RF noteworthy NNRF parallelly decision structure neural network  NNRF important therefore report average  isolet NNRF dnn noteworthy average  structure neural network NNRF dnn parallel machine cpu core NNRF significantly reduce addition NNRF belongs neural network suggests training reduce NNRF gpus acm transaction intelligent technology vol article publication date october random inspire neural network comparison classification isolet regression performance comparison evaluate regression ability NNRF propose framework classical regression model detail regression RR ridge regression linear regression model loss function linear function regularization norm popular regression implementation scikit SVR vector regression extension svm propose regression implementation libsvm NN hidden layer feedforward neural network linear regression output layer euclidean distance loss function DBN layer belief network linear regression DBN regression tune dnn layer neural network instead softmax linear regressor regression task RF random classical popular ensemble aggregate independent decision regression loss function euclidean distance predict target implementation  NRF neural random construct random dataset simulate specific  RF neural network propose regression task implementation author regressor parameter adopt fold validation training data tune parameter data involve parameter tune specifically NNRF empirically sensitivity  regression performance NNRF analyze detail conduct fold validation average performance standard deviation RMSE mae report respectively datasets RMSE mae target datasets observation http github com  neural random acm transaction intelligent technology vol article publication date october regression RMSE std regression datasets dataset RR SVR NN DBN dnn RF NRF NNRF crime   blog    regression mae std classifier datasets dataset RR SVR NN DBN dnn RF NRF NNRF crime   blog    RR SVR NN DBN dnn random generally slightly performance observation effectiveness random regression datasets NRF NNRF advantage neural network random NNRF outperforms NRF NRF random neural network simulate specific instantiation RF neural network NNRF random structure neural network inherently integrate random neural network propose framework NNRF outperforms regressors majority datasets RMSE mae implies effectiveness NNRF regression NNRF RF ensemble  model NNRF outperforms RF decision node NNRF complex nonlinear function decision RF summary combine random neural network propose framework NNRF regression ability comparison similarly report NNRF regression RF NNRF  average neural network structure decision NNRF training decision structure neural network efficient structure neural network parallel machine cpu core NNRF significantly reduce affect activation function NNRF activation function important NNRF affect node adopt leaky relu effective alleviate gradient explode vanish acm transaction intelligent technology vol article publication date october random inspire neural network comparison regression affect activation function NNRF classification dataset ForestType movement metric macro micro macro micro sigmoid relu leaky relu affect activation function NNRF regression dataset   metric RMSE mae RMSE mae sigmoid relu leaky relu net investigate activation function NNRF classification regression classification empirically regression setting classification popular activation function sigmoid relu leaky relu classification ForestType movement regression   report respectively generally leaky relu slightly outperforms relu relu leaky relu replace equation leaky relu information relu leaky relu relu outperform sigmoid sigmoid leaky relu relu alleviate gradient vanish neural network parameter analysis NNRF classification propose framework classification important parameter neural network randomly feature node depth neural network investigate impact parameter acm transaction intelligent technology vol article publication date october macro RF NNRF ForestType vehicle MSRA  classification performance propose framework NNRF throughout classification investigate classification performance fix macro ForestType vehicle MSRA  observation datasets evaluation metric micro comparison conduct random conduct via fivefold validation average macro datasets observation RF NNRF classification performance generally improves although increase beyond diminish return random fluctuation datasets tradeoff training performance within reasonable comparative performance RF NNRF NNRF tends consistently outperform RF increase powerful model NNRF function individual node myopic function propagation feature classification investigate randomly feature classification performance acm transaction intelligent technology vol article publication date october random inspire neural network macro RF NNRF dataset algorithm logm movement RF NNRF usps RF NNRF micro RF NNRF dataset algorithm logm movement RF NNRF usps RF NNRF macro micro NNRF ForestType movement logm logm non integer integer macro micro movement usps observation datasets conduct fivefold validation average macro micro report observation generally RF NNRF classification performance chosen logm chosen logm introduce randomness diversity model model generalization RF NNRF NNRF robust outperforms RF effectiveness NNRF neural network depth classification investigate neural network depth ForestType movement movement conduct fivefold validation ForestType movement average performance macro micro observation generally increase performance increase converges acm transaction intelligent technology vol article publication date october RMSE RF NNRF datasets decrease chosen performance relatively movement already achieves performance contrary chosen performance satisfactory leaf dimension representation capacity classification parameter analysis NNRF regression similarly NNRF regression important parameter investigate impact parameter regression performance propose framework NNRF throughout regression investigate regression performance fix RMSE   crime  observation datasets evaluation metric mae comparison conduct random conduct via fivefold validation average RMSE datasets observation similarly observation classification performance regression performance RF NNRF improves increase beyond acm transaction intelligent technology vol article publication date october random inspire neural network RMSE RF NNRF dataset algorithm logm crime RF NNRF  RF NNRF mae RF NNRF dataset algorithm logm crime RF NNRF  RF NNRF diminish return tradeoff training performance within reasonable choice RF NNRF tends consistently outperform RF increase powerful model NNRF function individual node myopic function propagation feature regression investigate randomly feature regression performance logm integer logm non integer RMSE mae crime  observation datasets conduct fivefold validation average RMSE mae report generally RF NNRF regression performance chosen chosen logm classification logm classification regression feature predict numerical logm feature prediction introduce feature reduces randomness diversity model generally logm tradeoff prediction capability randomness diversity model RF NNRF NNRF robust outperforms RF effectiveness NNRF neural network depth regression investigate neural network depth regression performance conduct fivefold validation crime  average performance RMSE mae generally increase performance increase converges decrease increase increase capacity NNRF prediction however model becomes complex datasets acm transaction intelligent technology vol article publication date october RMSE mae NNRF crime  related briefly review related neural network random comb neural network neural network NN powerful extract nonlinear feature machine data mining task abundant training data achieve computer vision recognition processing  convolutional neural network cnn become model image classification semantic segmentation image representation image generation similarly processing lstm ability classification generation machine translation despite neural network specific domain neural network universe across domain neural network net massive datasets prone overfitting datasets domain datasets neural network usually domain datasets bioinformatics pervasive net cnn lstm actually rely domain insight specific structure reduce parameter alleviate overfitting cnn convolution layer max pool layer reduce parameter translation invariant feature image similarly lstm reduce parameter forget gate capture dependency however domain domain insight integrate neural network net therefore neural network domain datasets important random robust overfitting datasets choice inspire subsection random random RF ensemble aggregate decision generally decision binary decides input feature node successfully apply various domain bioinformatics remote compound classification generalist demonstrate classifier datasets domain classifier entire uci acm transaction intelligent technology vol article publication date october random inspire neural network collection datasets overall random perform addition classification random powerful regression random likely overfit neural network ensemble bootstrap randomize feature sample decision introduces diversity model reduce overfitting however unlike neural network powerful feature apply nonlinear activation function input feature RFs feature decision node random powerful neural network therefore combine ability nns ability reduce overfitting random random structure neural network NN simulate RF propose simulate output decision random algorithm specific dataset already construct construct decision random dataset simulate specific instantiation RF NN construct random input algorithm approach defeat purpose neural network  specific instantiation random model random model modification recently propose gcForest stack RFs stack hidden layer nns output RFs layer concatenate feature input RFs approach advantage feature ability neural network node RF decision feature article propose fundamentally approach architecture neural network random reduce overfitting decision function linear combination input feature activation function powerful decision conclusion article propose novel random structure neural network architecture NNRF inspire random random input datum NNRF activate efficient perform backward propagation addition NNRF datasets parameter relatively unlike random NNRF learns complex multivariate function node relevant powerful classifier extend NNRF regression replace softmax layer linear regression layer euclidean distance loss function extensive datasets domain demonstrate effectiveness propose framework NNRF classification cluster conduct analyze sensitivity hyper parameter classification regression respectively direction investigation conduct demonstrate effectiveness NNRF classification regression detailed theoretical analysis NNRF rate convergence representation capacity worth pursue article initial attempt random structure neural network widely adopt trick batch normalization dropout proven useful net another direction introduce trick NNRF training deeper decision structure neural network