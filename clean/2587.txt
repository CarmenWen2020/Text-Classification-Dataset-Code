retrieval aim version pivotal role various QA task challenge mainly regard aspect synonymy polysemy data sparsity article propose unified framework simultaneously handle combine correspond concept information handle synonymy  concept embed embed context dependent context independent handle propose feature embed convolutional semantic model embed inputting concept embed embed due propose convolutional attentional enhance propose feature embed convolutional semantic model propose feature embed convolutional semantic model nicely hierarchical structure information concept information layer layer convolution pool finally resolve data sparsity propose multi attention convolutional semantic model knowledge propose simultaneously handle retrieval framework datasets propose framework significantly outperforms CCS concept information information retrieval compute methodology processing additional retrieval concept embed embed convolutional attentional introduction decade QA archive become important resource various intelligent service however user manual QA platform inspire automatic QA propose various challenge automatic QA retrieval attract attention domain frequently QA platform stack overflow yahoo baidu  however task challenge regard aspect synonymy synonymy meaning meaning predict retrieval rid  prevent meaning traditional retrieval model LM vector model vsm suffer synonymy polysemy polysemy reveal sens context refer accord context retrieval traditional recent distribute representation suffer sometimes express completely meaning faster rat rat faster retrieval ignore information voltage input usually anyone confirm citizen important generally due however rewrite focus voltage input data sparsity challenge adequate training training data critical model synonymy propose translation topic model continuous embed comparison embed proven efficient however suffer  recently propose handle  creation specific embed concept embed propose obtain embed representation directly syntactic information knowledge resolve framework data sparsity previous propose concept embed convolutional semantic model handle synonymy polysemy concept information confirm meaning possess multi sens handle polysemy synonymy propose concept model encodes concept acm transaction intelligent technology vol article publication date january concept attention cnn retrieval multi embed embed context dependent context independent context dependent focus leverage concept information within context context independent focus leverage concept information within knowledge propose model  convolutional semantic model HCSM variant handle previous model embed concept embed embed instead raw text feature propose model gram information sequence article extend previous account data sparsity focus ignore unimportant addition relation retrieval task usually moreover mutual information role evaluate handle propose convolutional attention operating mutual information enhance HCSM model enhance model efficiently leverage mutual information generate vector representation handle model extent manually label training data data sparsity propose multi framework training data multi semisupervised convolutional semantic model embed input goal generate additional potentially correspond evaluation estimate perspective content semantics syntax relation likely inspire propose multi training data multi convolutional semantic model summarize contribution article propose simultaneously handle challenge retrieval framework encode concept embed embed context dependent context independent directly vector representation HCSM contextual embed input instead raw text feature propose convolutional attention enhance HCSM propose multi convolutional semantic model data sparsity unified framework propose model consists concept semantic relation resolve synonymy concept embed resolve  output consists embeddings concept embeddings concept acm transaction intelligent technology vol article publication date january flowchart unified framework embed embed representation obtain syntactic information multi resolve data sparsity model obtain briefly introduce skip gram model foundation propose concept model completeness introduce concept model HCSM propose previous finally introduce handle data sparsity concept learning neural embed model generally embed text corpus without supervision predict context predict context skip gram model distribute representation embed efficient embed model ultimately vector meaning distribute across dimension semantic training goal skip gram model representation useful predict surround document mathematically sequence training objective skip gram model maximize average probability    target contextual respectively context target embed sequence conditional probability compute exp  exp  denote dimensional embed representation vocabulary concept model cpm encode concept information embed model previous conceptual information efficiently handle  associate intrinsic acm transaction intelligent technology vol article publication date january concept attention cnn retrieval multi concept model vector maintains unique feature concept assign vector delivers unambiguous meaning contextual representation obtain combine intrinsic vector context appropriate concept vector query python zoo python embed python obtain combine intrinsic vector python concept vector category contrast embed python obtain combine intrinsic vector python concept vector program propose concept model briefly introduce GWCS model achieves task context reduce GWCS model concept underneath chosen concept objective function    denotes concept context  denotes intrinsic vector  denotes concept vector however GWCS model considers context dependent relation context context independent relation knowledge ignore  relation enhance embed via certainty GWCS concept  calculate probability introduce error concept concept afternoon GWCS context independent namely without context embed enhance really concept knowledge directness due data sparsity concept context semantic relationship indirect another relation respectively semantic relation occurs relation sofa context however concept external knowledge enhance directly context independent acm transaction intelligent technology vol article publication date january inspire regularization function derive concept information skip gram objective function context independent regularization focus leverage concept information regard knowledge without context concept knowledge model concept addition context achieve certainty directness similarity mth concept concept heuristic constrain  otherwise denote concept central concept similarity mth concept otherwise encode concept information regularization function denotes concept knowledge denotes concept loop intersection concept instead combine mth concept distance contextual embed serf function similarity denote cosine similarity equation combine vector concept vector  denotes combination embed representation denotes embed representation denotes embed representation mth concept relative importance embeddings accord validation concept update GWCS conceptualization obtain objective function encodes concept information     combination coefficient accord validation goal maximize combine objective function optimize propagation neural network accord validation dataset dimension concept embed embed skip gram rate model concept model denote cpm easy reference representation learning feature embed convolutional semantic model HCSM distribute representation proven successful however embed representation text document evident processing application embed syntactic information obtain embed acm transaction intelligent technology vol article publication date january concept attention cnn retrieval multi evaluate cosine similarity equation sim denote denote embed representation denotes dot denotes module vector cosine similarity otherwise cosine similarity equation rank candidate retrieval task encode concept information syntactic information information embed propose model feature embed convolutional semantic model HCSM variant generate embed representation QA dataset dataset denotes denotes dataset compose sequence denotes similarly compose sequence denotes embed concept embed embed matrix concept embed matrix respectively regard  matrix bold dimensional embed representation   regard concept embed matrix bold dimensional concept embed representation  concept  concept goal obtain vector representation  previous explore compositional operation simplest obtain vector representation average sum representation semantic relation relation supervise information model model matrix convolutional architecture alternate convolutional layer pool layer briefly introduce convolution pool convolution convolutional layer network obtain convolve convolution kernel source input matrix dimension input vector andm hyper parameter network dimensional convolution convolution zero pad source input matrix dimensional convolution correspond rth dimensional vector layer obtain apply convolutional kernel input embed matrix dimension dimensional convolution matrix denotes article acm transaction intelligent technology vol article publication date january structure HCSM vector addition bias nonlinear function apply component wise convolve matrix convolve relu network parameter pool average pool operator apply network topmost convolutional layer guarantee generate vector important information addition dynamic max pool intermediate convolutional layer function calculate convolutional layer pool apply convolutional layer input input pool parameter layer HCSM variant emphasize relation relation concept relation relation concept convolution kernel encode concept relation relation respectively convolve input embed matrix concept embed matrix denote concept convolution layer obtain equation combine convolutional   convolution layer convolution relative importance validation convolution layer convolutional layer pool layer alternately architecture variant feature embed convolutional semantic model HCSM HCSM variant HCSM HCSM emphasis relation concept relation relation concept acm transaction intelligent technology vol article publication date january concept attention cnn retrieval multi structure HCSM vector architecture distinct network embed matrix concept embed matrix convolution kernel network network network concept network pool layer average pool layer functional layer meaning layer meaning layer functional layer functional layer equation combine pool network  output functional layer average pool layer network concept network relative importance validation HCSM directly combine embed matrix  matrix inputting network equation    combination convolutional network  input model variant HCSM model variant inputting matrix mechanism inputting matrix finally output model variant vector dimensional semantic feature convolutional attention mutual influence ignore model focus ignore unimportant mutual information important inspire propose convolutional attention enhance HCSM model input propose representation feature output attention feature matrix acm transaction intelligent technology vol article publication date january structure attention model convolutional matrix obtain mutual matrix namely mutual information representation feature propose convolutional attention obtain attention feature mutual matrix finally representation feature attention feature input generate convolution layer mutual information generation obtain representation representation correspond compute mutual matrix RN denotes function representation item item cosine similarity denotes attention distribution respect denotes attention distribution respect attention feature generation subsection transform correspond attention attention matrix attention matrix matrix simply transform attention gram information exist propose convolutional model generate attention introduce convolution operation namely filter denotes filter denotes filter define vector vector tth filter simplicity illustration convolution operation convolution acm transaction intelligent technology vol article publication date january concept attention cnn retrieval multi  calculate   denotes inner  parameter filter however difference computer vision processing relative information pixel important computer vision task however relation due flexibility relation mutual matrix role semantic relation inspire propose filter generate attention split mutual matrix aij within interval split interval bin bin fix obtain bin bin correspond filter similarly vector tth filter parameter filter item mutual matrix belong interval bin filter mathematically convolutional operation  bii  inter false  denotes bin inter denotes correspond interval obtain attention stack representation feature attention feature tensor induct convolution operation convolution attention operation construct concept feature currently attention layer convolutional layer HCSM training label data model model parameter propose training maximizes conditional likelihood compute posterior probability semantic relevance softmax function exp exp smooth factor softmax function empirically validation denotes semantic relevance vector representation denotes candidate ranked ideally acm transaction intelligent technology vol article publication date january denotes denotes correspond construct randomly training minimize loss function estimate model parameter loд denotes model parameter model gradient numerical optimization algorithm model convolutional layer pool layer performs previous width matrix filter convolutional layer respectively matrix filter embed dimension dynamic max pool average pool pool layer pool layer respectively multi learning convolutional neural network sufficient training data obtain performance task sufficient HCSM model propose leverage multi data sparsity multi data model iteration data confidence predict model data model goal retrieve judging explicitly label implicitly assume potentially generate via correspond addition omit meaningless avoid computer warranty policy usa totally framework distinct  HCSM model  fed label HCSM model QAM fed model model label data training data training data algorithm summarizes propose algorithm label   input output  parameter  QAM parameter mqa generate vector iteration     QAM mqa model  predict  model mqa predict  confidence predict data enlarge training data  predict model mqa directly merge  however model mqa input described obtain    merge  finally ultimate model parameter mqa  obtain algorithm converges acm transaction intelligent technology vol article publication date january concept attention cnn retrieval multi generate algorithm multi framework input   output mqa  loop  HCSM  mqa HCSM   predict   predict mqa          mqa mqa   loop  model   vector representation obtain calculate cosine similarity data however calculation cluster vector representation calculate cosine similarity cluster addition cluster parallel manner finally cosine similarity threshold predict data datasets accord validation datasets  depicts generate link solid dataset dot link generate EXPERIMENTS data collection evaluation metric amazon customer dataset amazon customer clothing electronics category dataset sufficient supervise candidate propose model label acm transaction intelligent technology vol article publication date january previous generate evaluation data evaluation dataset contains sample dataset validation dataset contains associate target retrieve vsm dataset relevance label label generate relevance retrieve source dissimilar target ranked accord similarity microsoft community dataset microsoft community consists category label evaluation dataset contains sample dataset validation dataset contains average associate amazon data label usage evaluation data amazon dataset preprocessing amazon dataset yahoo dataset release yahoo research data category comprehensive dataset relationship entertainment society culture consists label evaluation dataset contains sample dataset validation dataset contains average associate amazon data label usage evaluation data amazon dataset preprocessing amazon dataset evaluation metric performance model normalize discount cumulative gain nDCG precision nDCG calculate     rel reli loд reli grade relevance reli rank  calculate  reli precision report retrieve relevant relevance relevant conduct difference statistically significant concept model concept model  knowledge  concept associate usage  official website calculate relevance bag bow LM baseline model cpm resolve synonymy translation model translation model translation model translation model translation model training parallel corpus topic model acm transaction intelligent technology vol article publication date january concept attention cnn retrieval multi comparison approach amazon data microsoft data yahoo data model nDCG nDCG nDCG bow LM skip gram GWCS cpm supervise topic model paraphrase model translation approach  approach paraphrase approach outperform baseline retrieval continuous embed representation skipgram GWCS cpm outperform translation model topic model paraphrase model achieve comparable performance skip gram model average combination obtain vector embed representation performance cpm GWCS skip gram model incorporate concept information increase retrieval performance concept information context dependent context independent propose cpm outperform GWCS model vector representation generation model vector representation generation model propose HCSM model summarize directly obtain vector representation raw text data   originally web obtain query vector representation instead query document model para vector directly obtains vector leverage vector vector predict context embed input average combine representation focus interaction focus propose HCSM contains average combine  denotes vector combine embed skip gram equation  CS  denote vector generate equation embed obtain GWCS cpm equation HCSM denotes input matrix construct embed skip gram contains representation focus arc  BI cnn  representation focus focus text dense vector without interaction fourth contains acm transaction intelligent technology vol article publication date january comparison vector representation approach amazon data microsoft data yahoo data model nDCG nDCG nDCG   para vector   CS  arc    arc II AI cnn    HCSM HCSM  CS HCSM cpm HCSM cpm HCSM cpm HCSM cpm proj HCSM cpm conv HCSM cpm val conv interaction focus arc II AI cnn    interaction focus directly model interaction relation text embed baseline model embed batch baseline parameter baseline fifth contains propose HCSM model input HCSM  CS HCSM cpm denote input matrix HCSM construct embed concept embed generate GWCS cpm HCSM cpm HCSM cpm denote input matrix HCSM HCSM construct embed concept embed generate cpm HCSM cpm model enhance attention ordinary projection convolution  conv convolution val conv para vector  directly vector achieve performance vector   outperform average combination verifies improve supervise information relation addition    considers information HCSM combine outperform  neural network acm transaction intelligent technology vol article publication date january concept attention cnn retrieval multi multi amazon data microsoft data yahoo data model nDCG nDCG nDCG HCSM cpm QA val conv HCSM cpm QQ val conv multi nDCG without multi amazon data microsoft data yahoo data model   comb   comb   comb supervise multi raw text feature instead feature vector concept vector interaction focus achieve  HCSM cpm outperforms HCSM  CS validates propose cpm outperform GWCS model HCSM cpm HCSM cpm outperform baseline HCSM cpm superior performance attentional improve performance  convolutional attention achieves propose cpm HCSM combine efficiently semantics moreover attentional efficiently achieve performance multi framework verify multi framework increase performance summarize HCSM cpm QA HCSM cpm HCSM cpm QQ HCSM cpm convolutional attention enhance HCSM model multiview multi training algorithm apply distinct HCSM model multi training algorithm achieves HCSM cpm QA HCSM cpm QQ multi algorithm combine model model iteration clearly scenario without multi summarize supervise denotes model data label multiview denotes model multi multi obtain algorithm converges QA QQ combine denote model HCSM cpm QA model HCSM cpm QQ model combine model respectively evaluation nDCG model QA QQ combine multi outperform supervise training generate multi acm transaction intelligent technology vol article publication date january iter amazon microsoft instal install yahoo god god god iter amazon microsoft install USB laptop install laptop yahoo god really exist god input attention visualization generate iteration generate iteration generate later meaning indicates leverage information visual analysis mutual matrix input attention shipping overnight purchase overnight shipping replace denotes attention similarly denotes attention darker shade attention semantical relation attention shipping overnight obtain mutual matrix propose convolutional attention generate attention average dimension important regard attention unimportant acm transaction intelligent technology vol article publication date january concept attention cnn retrieval multi plot nDCG nDCG versus iteration multi training shipping overnight purchase model efficiently parameter analysis perform grid optimum equation HCSM variant concept embed generate cpm input HCSM generally outperforms variant datasets nDCG increase conceptual feature inject embeddings however increase performance degradation interpretation concept information input supplementary information information useful information concept information amazon dataset optimum HCSM HCSM HCSM respectively microsoft dataset optimum HCSM HCSM HCSM respectively yahoo dataset optimum HCSM HCSM HCSM respectively plot nDCG versus iteration multi training algorithm HCSM cpm enhance convolutional attention verifies propose augment unlabeled data training data plot nDCG versus initial training data HCSM cpm enhance convolutional attention training data fix sensitivity initial training data plot nDCG versus initial training data similarly training data fix sensitivity initial training data iteration initial training data influence correspond model HCSM model significantly acm transaction intelligent technology vol article publication date january nDCG versus initial training data nDCG versus initial training data increase comparison QQ model significant increase due data quantity data training data fix QA model increase slowly discussion variant HCSM outperform average verifies semantic information exist efficiently propose HCSM model layer layer convolution pool manner difference variant layer concept information information merge HCSM merges inputting neural network HCSM merges convolutional operation HCSM merges topmost pool layer variant HCSM yield performance datasets indicates hierarchical structure information concept separately effective information merge concept HCSM parameter concept function function however training HCSM nearly model efficiency performance situation related retrieval majority traditional retrieval unsupervised approach LM vsm however cannot capture semantics text meaning regard independent traditional model recently translation model propose mismatch principle approach capture translation probability ibm model translation probability LM outperform acm transaction intelligent technology vol article publication date january concept attention cnn retrieval multi traditional model vsm LM addition translation model proven outperform translation model information information accurate semantic meaning information gain information model semantic relation topic model concept paraphrase translation recently exploit category metadata within  improve performance capture semantics efficiently calculate similarity introduce skip gram model efficient embed quality representation amount unstructured text data capture syntactically semantically noisy data propose prior knowledge knowledge graph advance embeddings polysemy propose multi concept information enhance embeddings vector generation representation generation propose directly vector representation text introduce unsupervised model leverage vector vector predict context  proposes convolutional neural network vector employ label data due training sample retrieval task model apply retrieval task representation manner recently model categorize representation focus model interaction focus model  model focus text dense vector without interaction   arc  BI cnn  however drawback representation focus model separately model unable capture complex interaction variety interaction focus model propose arc II AI cnn     addition propose local distribute representation handle interaction focus model interaction interaction information enhance representation addition interaction information neural network  interaction multi machine multi concerned machine data multiple distinct feature recent emergence mechanism largely motivate data application described feature moreover noteworthy multi feature split exist performance improvement manufacture split therefore multi promising topic widespread applicability related research label unlabeled data domain feature naturally disjoint blum mitchell algorithm training classify webpage classifier assume sufficient training data acm transaction intelligent technology vol article publication date january goal inexpensive unlabeled data augment label algorithm separately algorithm prediction unlabeled enlarge training CONCLUSIONS article propose unified framework simultaneously handle challenge exist retrieval task synonymy polysemy data sparsity vector generation model enhance convolutional attention encodes embed concept embed vector solves vector generation model multi manner experimental propose framework outperforms previous model retrieval information retrieval task focus retrieval future directly apply retrieval input meaningful retrieval QA addition encode relationship exist knowledge enhance representation finally semisupervised training data directly