recent development text generation technology undergone service restaurant reservation daily communication automatically generate text become fluent researcher anthropomorphic text generation technology conditional text generation emotional text generation personalize text generation conditional text generation CTG become research hotspot promising research attention paid explore therefore aim comprehensive review research trend CTG summarize technique illustrate technical evolution route neural text generation concept model CTG investigation exist CTG propose model CTG finally discus issue promising research direction CTG CCS concept information social network compute collaborative social compute additional computer interaction conditional text generation dialog personalization introduction  luis  described magic library library babel everyone reader cannot writer absolutely library unlikely exist however development text generation technology recent instance philip parker amazon utilizes computer program massive public information internet automatic compilation parker text text generation exist textual input automatically generates text text text generation typical subfield text generation diverse information enable computer express image text accord data source text generation data text text text image text generation news generation typical application data text generation earthquake california march los angeles detailed information location magnitude  actually news article automatically generate robot reporter convert incoming register seismic data text slot predefined template data  technology establish template structure data generates output text component exert considerable influence news medium application text generation text text machine translation dialogue text summarization reading comprehension understand text obtain semantic representation text generate communicate summarize refining besides application image text generation image caption visual processing image information content image understood generate correspond description contributes recent advance text generation specifically recurrent neural network rnn attention mechanism generative adversarial network gan reinforcement RL variational autoencoder VAE transformer generate text becomes coherent logical emotionally harmonious suitable offering assistance aspect dialogue microsoft xiaoice cortana siri chat assist electronic device news robot creative assistance journalist machine translation technology effectively met translation advancement universal text generation technology prompt researcher explore  text generation context text generation personalize text generation topic aware text generation emotional text generation  text generation visual text generation obviously apply additional http  com author  luis  http  com  pdf http  net computer http  com http  http com siri acm transaction intelligent technology vol article publication date february conditional text generation harmonious machine interaction information text generation generate text personify facilitate harmonious machine interaction however challenge summarize efficiently integrate additional conditional information traditional model structure challenge due scarcity text datasets specific training conditional  model become reasonable evaluation metric conditional text generation quantify performance model article aim depth survey development neural text generation model specifically mainly focus various conditional text generation CTG context text generation topic aware text generation knowledge enhance text generation text generation conditional text generation precision friendly service sum summarize contribution brief review text generation technique characterize concept model CTG centric service investigation CTG context text generation personalize text generation topic aware text generation emotional text generation knowledge enhance text generation visual text generation multi conditional text generation pre model text generation besides evaluation conditional datasets summary exist research propose model CTG discus promising research direction CTG consideration context multi modal data translation lifelong remainder article organize brief review text generation technique characterize concept model CTG summarize research CTG propose model CTG issue future research direction finally conclude article TECHNIQUES neural text generation decade witness leap text generation technology specifically statistical neural  overall quality generate content improve brief review technique neural text generation dnn manual feature extraction automatically continuous vector representation task specific knowledge task encode decode successively input neural network model encode vector semantically related concept afterwards neural network vector accord hidden input response finally response decode generate text neural network structure usually adopt stage encode decode parameter optimize propagation gradient descent algorithm mechanism promotes neural network fully correlation data alleviates requirement characteristic engineering greatly acm transaction intelligent technology vol article publication date february technique neural text generation mainly rnn gan RL VAE transformer summarize subsection text typical sequential data specific relationship context sequential structure rnn suitable model text data rnn internal memory previous input input sequence model easy output depends instantaneous input output previous highly capable capture contextual information generate satisfy syntactic structure however sequential generation rnn cannot representation tend generate inconsistent uninformative text absence global feature topic syntactic feature latent variable continuous VAE capture implicit structure utterance semantics topic syntactic global representation decode sample procedure latent variable VAE capable meaningful diversified text rnn text generation model maximize likelihood objective function prone exposure bias inconsistency sequence input training therefore gan another powerful generative model become computer vision introduce text generation adversarial training replace maximum likelihood training simulate data distribution generate quality text adversarial training generator discriminator generator gan gain ability generate almost data however gan suitable processing continuous data image text typical discrete data cannot apply directly text generation effort adjust internal calculation mechanism gan introduction RL greatly promotes application gan text generation combine reward mechanism policy gradient technology RL gan  avoids gradient cannot propagation discrete data achieves promising VAE gan powerful generative model generate data complex distribution approximate data distribution random distribution distinct difference distribution metric loss function quality generate data VAE utilizes explicit measurement KL divergence training data assume training data generate another distribution gan however avoids explicit measurement distribution difference neural network measurement adversarial training distribution measurement criterion perfect measurement VAE gan rnn inability effectively capture dependency vulnerability gradient vanish explode lack parallel compute capability transformer model propose adopts attention mechanism replace sequential structure rnn attention mechanism capture context dependency sequence achieve efficient sequence model without distance restriction obtain semantically text representation transformer excellent performance various nlp task propose development potential summary technique advantage disadvantage acm transaction intelligent technology vol article publication date february conditional text generation harmonious machine interaction summary text generation technique technique advantage disadvantage rnn sequence structure suitable task sequence model cannot effectively capture distance dependence gan unsupervised generate clearer realistic sample generative model  training suitable processing discrete data text reinforcement manner combine gan subtly exist gan generate realistic text complicate training VAE leverage latent vector increase diversity generate text latent variable ensures desire content generate regardless quality transformer attention mechanism efficiently capture context information parallel compute amount calculation training rnn rnn commonly neural network model text generation sequence structure suitable task text sequence model recurrent structure rnn determines textual data sequentially hidden input previous hidden consideration input text sequence hidden calculate sequential processing semantic information text compress fix vector hidden vector enable rnn model memory previous content nevertheless gradient vanish explode limit application prospect rnn variant rnn model memory lstm gate recurrent gru combine memory uniquely gate mechanism effectively gate structure lstm information selectively information retain forget gate determines information discard accord input previous hidden calculate input gate determines information calculate tanh update input gate forget gate acm transaction intelligent technology vol article publication date february finally output gate determines output lstm accord tanh numerous researcher lstm ability generate realistic text generation task propose sequence sequence seqseq model generalize framework convert sequence another framework lstm encoder compress sequence vector representation another lstm decoder predicts output hidden previous output input predict output framework limitation input output sequence widely text generation machine translation text summarization dialogue consequently data driven training seqseq model become mainstream text generation actually traditional seqseq model generally input transform vector fix limit ability latent vector input information assign input cannot effectively capture information attention mechanism widely utilized mechanism computer vision introduce nlp assign input sequence accord decode attention mechanism extract component input generation model accurate judgment reduce computation storage consumption attention mechanism apply seqseq model fulfill machine translation task gradually become important text generation model introduce attention multi response generation model capture relevant content conversation context attention mechanism multi dialogue coherent consistent gan RL perspective neural network optimize defect rnn generation model rnn text generation model maximize likelihood objective function exposure bias loss function calculate evaluation metric inconsistency optimization direction model actual requirement researcher introduce gan text generation gan compose generator discriminator generator false sample distribution data discriminator distinguishes generate sample sample accurately attempt combine lstm convolutional neural network cnn generate realistic text adversarial training however gan applicable generate continuous data performance processing discrete data gradient discriminator correctly propagate discrete variable utilize smooth approximation algorithm approximate output generator instead utilize standard objective function gan feature distribution prediction embed vector generate quality acm transaction intelligent technology vol article publication date february conditional text generation harmonious machine interaction researcher tune gan structure generate discrete data wasserstein gan model although improvement gan achieve progress meeting practical requirement therefore RL introduce text generation RL usually markov decision action reward  reward punishment maximize reward RL machine various action evaluate optimal policy accord reward environment reward mechanism RL gan discrete data possibility application gan text generation propose  model gan generate discrete text data  regard text generation sequence decision procedure RL generate sequence timestep generate regard action return reward discriminator generate sequence gradient policy algorithm  model directly avoids differentiability generator obtain remarkable generate realistic text VAE although traditional seqseq model progress text generation tends probability sorry training text generation amount highquality label data namely supervise training however reality data unlabelled label amount data consume unsupervised introduce VAE powerful unsupervised generative model contains encoder encodes input data latent variable decoder decodes latent variable reconstruct input data input encoder encode latent parameter encoder decoder probability distribution input hidden variable parameter decoder usually latent hierarchical structure latent variable VAE capture reflect inherent structure expressive text introduce rnn VAE text generation model assigns distribute latent vector append gaussian prior distribution regularization hidden layer encoder sequence autoencoder model construct output generate hidden vector obtain consistency diversity sequential data usually hierarchical structure complicate dependency sub sequence sequence sequence multi conversation massive dependency attach latent variable hierarchical dialogue model assign generative model multiple variability meaningful diverse response specifically attach dimensional latent variable dialogue generate response latent variable transformer perspective neural network training rnn generation model obvious shortcoming rnn input sequence strict linear gradient vanish explode due propagation rnn lack efficient parallel compute capability due linear propagation acm transaction intelligent technology vol article publication date february structure calculation relies output previous therefore rnn issue calculation efficiency application scenario address google proposes sequence model model transformer model abandon sequence structure rnn contains attention module specifically transformer model encoder decoder structure consist attention module neural network attention mechanism core transformer capture dependency sequence obtain semantic representation attention module calculate attention tmax  andv vector obtain input vector matrix multi attention mechanism compose attention module attention module propose improve ability capture context semantic information encode attention module output vector neural network calculate FFN max ZW besides attention module neural network module encoder decoder attention module decoder mechanism traditional seqseq model due parallelization attention module transformer powerful parallel compute capacity application prospect transformer various model achieve excellent performance various nlp task bert GPT significant impact research nlp conditional text generation development neural network brings unprecedented progress text generation however exist text generation technology text generation model content input text ignore factor however considers context adjusts content accord mood gender external factor environment article conditional text generation CTG future research direction factor improve quality generate text specifically context text generation personalize text generation topic aware text generation emotional text generation knowledge enhance text generation visual text generation formalize definition CTG introduce application concept model CTG refers external consideration influence generate text generation usually context topic emotion external knowledge text generation text content factor generate text diverse gap expression consideration external text generation anthropomorphic brings service various formal definition text generation input text sequence target text generation model generate output text sequence input output sequence respectively acm transaction intelligent technology vol article publication date february conditional text generation harmonious machine interaction text generation model define decode decoder combine input text output previous generate finally text sequence probability generate machine translation source chinese target english dialogue query response without additional information generation basis text generation CTG model fuse additional generate anthropomorphic text define various CTG definition context text generation integrate contextual information text generation enhance understand environmental coherent informative text content context refer situation generate factor ensure consistency smoothness generate text context  text sequence sentiment context context text generation model define conversation refers historical dialogue content  conversation daily dialogue usually historical dialogue content multi dialogue context information generate response historical dialogue conversation consistent advertisement scenario refers various information specific brand function price information user preference integrate multi contextual information appeal advertisement generate definition personalize text generation assign specific personalization characteristic text generation agent personalize text content personalization characteristic personalization everyone characteristic others subtly influence express personalize characteristic gender profession characteristic personalize characteristic personalize text generation model define similarly dialogue gender personalize characteristic impact dialogue content commodity description advertisement combine personalize feature user gender shopping preference generate description user expectation therefore text generation agent acm transaction intelligent technology vol article publication date february personify assign specific personalize characteristic generate text content conform personalize information definition topic aware text generation incorporate specific topic text generation text content suitable topic ensure coherence rationality generate text text internal relevance text text usually aligns around specific topic topic information generate coherent meaningful text topic topic topic topic aware text generation model define article usually expand accord specific  maternal ensure logical consistency consistency article foreign translation combine topic text content fluent consistent translation around central topic combine topic information factor ensure logical coherence compact semantics text generation definition emotional text generation embody emotional expression agent text generation positive negative sad adjust content expression style generate text emotion important attribute usually emotion daily communication emotion important express angry usually something rational hurt others incorporate emotion text generation generate content personify dialogue quantifiable impact usability user satisfaction specific emotion specific emotion category sadness emotional text generation model define definition knowledge enhance text generation embrace external knowledge knowledge factual basis reference generate content text generation procedure wealth prior knowledge flexibly combine knowledge communication translation express combine external knowledge text generation generate text informative consistent logic expression reduce possibility mistake knowledge text sequence text knowledge knowledge triple knowledge graph structure knowledge knowledge knowledge enhance text generation model define specific combine knowledge understand correspond acm transaction intelligent technology vol article publication date february conditional text generation harmonious machine interaction conditional dialogue china comb geographical knowledge beijing china combination knowledge factor ensure  text generation interaction expression knowledge text generation agent truly integrate daily definition visual text generation integrate semantic information image generate text generate text description accord image content image data multi modal text image automatically extract information image translate understandable image vividly depict external psychological activity visual text generation application prospect image visual text generation model define formalize various CTG actual dialogue incorporate enhance performance dialogue agent personalize characteristic descriptive preference agent knowledge text content physic context historical dialogue topic conversation emotion inside dialogue proud dialogue agent generate relevant personalize substantial context consistent response discus technical detail implement various CTG text generation model encoder decoder structure encoder transforms input sequence semantic vector representation decoder generates output accord input information dialogue response review apply constraint text generation model acm transaction intelligent technology vol article publication date february encoders decoder interaction mode widely encode stage external encode various technique rnn transformer input decoder input generation decoder modify decode technology decode procedure increase decrease probability meanwhile attention mechanism RL enhance interaction mode encoder decoder implicit semantic information multiple aspect encoder decoder integrate CTG content quality personification comfortably service text generation centric service text generation technology application scenario daily ongoing effort academic researcher various text generation technology centric service goal orient dialog dialogue typical application text generation goal orient non goal orient goal orient dialogue assist fulfil various task reduce operational burden restaurant reservation arrangement addition goal orient dialog accomplish specific business customer transaction accord   automate customer interaction dialog siri microsoft cortana google assistant amazon alexa typically frequently goal orient dialogue daily siri virtual assistant deployed device assist smartphones desire interaction computer microsoft cortana virtual assistant microsoft mobile microsoft integrate hardware google assistant smart personal assistant siri deploy device android phone android TV wearable device siri introduce detailed technical workflow IOS device hey siri invoke siri recognizer listens detects hey siri siri par command query overall siri input command siri stage processing altogether stage recognition hey siri detector dnn convert acoustic probability distribution temporal integration compute confidence  hey siri siri wake subsequently convert command file siri sends server processing server spoken undergo flowchart stage understand meaning command nlp technology server nlp algorithm tokenization entity recognition input text understand http  com release chatbots  banking healthcare http com siri http microsoft com cortana http assistant google com http developer amazon com alexa acm transaction intelligent technology vol article publication date february conditional text generation harmonious machine interaction overall siri intent user instance nlp differentiate user alarm tomorrow user alarm finally siri communicates apps phone desire deliverable response chatbots non goal orient dialogue chatbots communicate normally domain instead specific task chatbots engage chatty conversation perform chatbots realistic interactive dialogue establish emotional connection recent emergence amount dialogue data breakthrough machine apply dialogue AI intelligent dialogue achieve  academia microsoft xiaoice popular social chatbots conservation user successfully built emotional connection development microsoft xiaoice guidance chatbot researcher briefly summarize xiaoice empathic compute framework enables chatbots recognize emotion understand user intention respond dynamically user integration IQ EQ personality core xiaoice IQ capacity knowledge memory model image understand generate predict foundation develop conversational chatbots specific user user accomplish specific task EQ refers chatbots emotionally intelligent socially attractive response humor comfort etc conversation topic  actively user engages conversation personality define characteristic behavior cognition emotional individual distinctive social chatbots consistent personality expectation user conversation gain user longterm confidence trust overall framework xiaoice http  com acm transaction intelligent technology vol article publication date february overall framework xiaoice information user dialogue management DM module manage dialogue global tracker responsible update status global tracker utilizes contextual query understand  user understand  understand SU module respectively integrate dialogue context user characteristic accurate capture dialogue policy module decides dialogue strategy accord update dialogue status query core chat core chat domain conversation chat domain chat chat mainly domain chat focus professional domain realization technique  combination retrieval model sort model retrieval model generates candidate response sort model sort candidate response output response addition core chat xiaoice constantly update image comment aim generate comment user input image content creation accomplish creative poetry creation generation addition dialogue QA correspond user another typical application text generation QA relevant content knowledge organize correspond relate commonsense detail specific propose QA model  achieve goal domain query  model correctly utilize information multiple document extract ranked web ranked web directly related machine translation development economic globalization become communicate via internet translation becomes essential requirement communication machine translation researcher opportunity exchange researcher around enable scientific research develop rapidly acm transaction intelligent technology vol article publication date february conditional text generation harmonious machine interaction model architecture GNMT  google translate translation service google translation translation web accord statistic google translate translates billion popular translation software google translation google neural machine translation GNMT consists lstm network encoder decoder layer residual connection attention mechanism model architecture GNMT GNMT typical sequence sequence framework attention mechanism compose encoder decoder attention network encoder encodes input vector representation decoder generates accord attention mechanism allows decoder focus important information source connection encoder decoder amount translation data training GNMT achieve excellent translation performance meanwhile neural network model training google proposes training trick improve performance model precision arithmetic employ inference computation accelerate translation normalization procedure coverage penalty employ beam technique encourage output source input output limited sub  balance flexibility delimit model efficiency delimit model improve handle rare GNMT google proposes transformer model machine translation task completely abandon network structure rnn adopts attention mechanism sequence model transformer become standard structure nlp task boost development nlp review advertisement generation due review shopping website review puzzle customer waste fortunately information rating review review http translate google com acm transaction intelligent technology vol article publication date february automatically generate review generation technology reference customer assistant user review model expands content input conforms user personalize aspect preference generate diverse smooth review specific advertisement vast consume task particularly generate personalize consumer personalize advertisement generation technology automatically generate description accord user preference trait convenience consumer seller propose personalize description generation model leverage neural network combine knowledge text summarization recent volume text data various source explode text summarization quickly understand content information improve efficiency information acquisition accord summary define text text conveys important information text longer text usually significantly purpose text summarization concise fluent summary source article retain information content overall meaning introduce attentional encoder decoder architecture text summarization achieve performance direction research data storytelling structure data computer analyze structure data inclined  data therefore structure data personify research direction text generation data storytelling technology achieve goal typical application news report generation report generation sport report generation news report generation meaningful reasonable news accord factor specific news sport report generation generate detailed report accord situation sport narrative text report generation purpose data dedicate research automatic text generation technology automatically generate quality text planning information data expression machine image caption visual QA text information obtain information likely expose image information image caption technology automatically generate correspond text description accord content image facilitate reader understand image content unsupervised image caption model generate image caption without image datasets visual QA another interactive text generation image understand understand image content correspond visual QA technology generate related image convert visual QA machine reading comprehension combine external knowledge realize knowledge visual QA rapid development nlp technology ubiquitous usage  technology daily nevertheless text generation mature easy chatbot obvious gap exists robot CTG factor http  com acm transaction intelligent technology vol article publication date february conditional text generation harmonious machine interaction summary context text generation description context embed embed dialogue continuous representation additional input decode stage context embed encode source context separately context aware representation source attention mechanism hierarchical context embed embed sequence context embed sequence historical dialogue efficiently capture context information hierarchical context embed leverage attention mechanism extend HRED model context adaption context information dialogue transform recurrent rnn effectively capture dimensional context aim generate quality  textual content article conditional text generation research  introduce definition application scenario CTG detailed investigation CTG context text generation personalize text generation topic aware text generation emotional text generation knowledge enhance text generation visual text generation furthermore summarize multi conditional  pre model CTG context text generation application text generation context information factor realize coherence smoothness generate text context situation generate dialogue context usually refers dialogue multi dialogue ability previous utterance core active engage dialogue meanwhile review generation context refers emotion sentiment factor context information clue generation therefore generate quality text context information CTG brief summary  text generation context embed effective combine context information embed directly vector representation decoder input context information generate text numerous task  text generation utilization context dialogue attracts attention researcher instance embed dialogue vector representation dialogue information encode vector decode another rnn context aware response addition dialogue application context information acm transaction intelligent technology vol article publication date february define context information situation influence output content encoder encodes context information sentiment ID user ID continuous semantic representation concatenates input decoder decode context information attend gate mechanism generate  review similarly propose text generation model treat entity representation extract dialogue context encode historical conversation entity representation context model entity mention context information machine translation multiple treat relevant information capture prevents error ambiguity improves consistency translation propose context aware machine translation model analyze information context translation model source translate context encode separately context aware representation source attention mechanism identify pronoun information capture model machine translation propose contextual dynamically source translate context scorer module context currently translate source incorporate important context translation module hierarchical context embed instead embed context information directly hierarchical context embed embed capture information context effectively concretely embed information embed information hierarchical recurrent encoder decoder HRED model hierarchically encode dialogue generation sequence context encode sequence historical dialogue encode leverage attention mechanism extend HRED model incorporate attention mechanism respectively model capture important context smooth conversation without context information multi user dialogue hierarchical multi user dialogue model structure consist multi user conversation context sequence besides conduct comprehensive survey exist context aware conversational model non hierarchical model hierarchical model capable capture context information context adaption context adaption model regard context extra input CTG model utilize context information transform recurrent layer rnn utilize rank decomposition algorithm parameter context performs dimensional sparse context VAE ensure coherence context generation text multiple challenge abstract feature topic sentiment etc grain feature specific choice generate globally coherent text sequence traditional rnn model tend generate repetitive inconsistent text due feature extraction ability VAE model latent variable capture lever feature generate coherent quality text propose VAE multi network structure CTG contains multi decoder capture coherent acm transaction intelligent technology vol article publication date february conditional text generation harmonious machine interaction summary personalize text generation description rnn speaker model speaker model encodes individual speaker vector capture characteristic generate personal response specific user autoencoder multi task training response generation model personalize dialogue data training autoencoder model non conversational data parameter model obtain personalize dialogue model transfer  tune respectively massive generic dialogue data personalize dialogue data pre tune dialogue model generate personalize response reinforcement persona embed embed user specific information vector representation RL mechanism optimizes reward topic coherent informative grammatical generate personalize response structure inherent text generate intermediate representation input meanwhile multiple stochastic layer VAE encoder decoder generate semantically latent variable coherent repetitive text sequence subsequently generate planning previously generate context hierarchical latent structure global planning local sequence latent variable improve diversity generate text personalize text generation various characteristic significantly impact interpersonal communication style personalization role enhance quality CTG model dialogue personalization vital truly smart dialogue agent seamlessly incorporate review generation personalization ensures generate review depends attribute preference specific user endow authenticity generate review effort personalize text generation conduct summarize discus detail personalize feature embed simplest achieve personalize text generation embed personalize characteristic user speaker model encodes user profile vector capture personalize feature response generation decode stage instead encode personalize feature vector representation directly additional neural network capture highlevel personalize information personality trait additional layer implicitly influence decode hidden ensure personalize feature integrate generate text propose user aware sequence network  generate summary user review accord preference aspect style user aware encoder selects user concerned information review user aware decoder combine user characteristic user specific habit generation propose trait fusion module capture persona information speaker acm transaction intelligent technology vol article publication date february persona trait encode vector representation trait merge integrate persona vector persona aware attention mechanism attention context vector persona aware bias estimate generation distribution personalize goal orient dialog restaurant reservation task profile model encodes user profile vector representation storage conversation user preference model capture user preference knowledge entity combine profile model enhance performance task completion user satisfaction improve personality consistence generate dialogue response propose generate delete rewrite mechanism delete inconsistent generate response prototype rewrite personality consistent multi task transfer personalize text datasets scarce model perform researcher attempt enhance performance personalize text generation transfer multi task model instance dialogue model predict response previous context autoencoder model volume non conversational personal data model  characteristic user multi task mechanism decoder parameter model model capture speaker role expressive style domain expertise characteristic target user generate personalize response without recourse speaker conversational data propose domain adaptation personalize dialogue model respectively massive generic dialogue data personalize dialogue data pre tune dialogue model apply policy gradient algorithm improve personalize informative feature generate response similarly lts model optimize quality response training initialization model respond style adaptation generate personalize response generate relevant diverse response gan RL model style perceive specific usage manner habit reflect personalize characteristic propose personalize generation model gan training procedure frequently incorporate input source structure constrain generate author RL quality generate content policy reward researcher incorporate implement personalize text generation attention hierarchical encoder decoder architecture via RL realize personalize dialogue generation defines reward mechanism topic coherence mutual information model text generation model generate topic relevance coherent dialogue response VAE embed personalize text generation user information training data cannot discover user user feature depict latent variable VAE related model introduce personalize text generation wasserstein auto encoders  typical VAE variation conduct adversarial training latent variable instead assume latent variable gaussian distribution data distribution improve generation ability VAE embed user information multimodal latent distribution mixed distribution regard prior distribution  extend gaussian mixture distribution decoder generate personalize response user generate tip personalize feature user persona aware tip generation model  acm transaction intelligent technology vol article publication date february conditional text generation harmonious machine interaction summary topic aware text generation description rnn lda topic embed utilize lda topic information embed topic vector generate informative topic relevant response HRED lda topic embed combine topic context information contextual topic aware response cnn lda RL lda topic information cnn capture dialogue information RL optimize model specific evaluation metric generate coherent diverse informative text summary rnn topic embed assign topic maintain multi topic coverage vector update decode employ adversarial variational auto encoders model persona information user persona information distil historical tip review target user express latent variable VAE external memory pointer network deployed conduct memory reading retrieve accurate persona information topic aware text generation topic information indispensable daily communication reading usually conversation around specific topic usually identify topic article subsection review topic aware text generation topic extract embed extract topic information exist text embed vector representation text generation propose topic aware seqseq TA seqseq model generate informative response chatbots TA seqseq incorporates topic information dialogue extract pretrained lda model input utilizes joint attention mechanism generation topic information multiple domain sport movie grain guidance generator adopt domain classifier capture domain information dialogue generate domain relevant response develop multi topic aware lstm MTA lstm model generate text target multiple topic MTA lstm model topic assign maintain multi topic coverage vector update decode vector generator generate topic aware text attention module article usually span topic text summary usually cannot topic generate text summary specific topic user propose generate multi summarization article accord topic article topic input propose pointer generator network attention relevant topic input article generate topic tune summarization introduce topical hierarchical recurrent encoder decoder  model generate contextual topic aware response  hierarchically encode dialogue respectively capture topic information dialogue context pre lda model decoder generation probability bias generate topic extra probability generation probability acm transaction intelligent technology vol article publication date february cnn mostly employ rnn model task  text generation addition rnn model apply task achieve remarkable instance propose topic aware convolutional seqseq  model leverage joint attention bias  mechanism incorporate topic information topic embeddings source sequence encode associate convolutional joint attention mechanism attends topic accord decoder bias probability generation perform generate coherent diverse informative summary VAE latent variable VAE capture feature implicitly useful guidance text generation latent variable correspond specific topic probability distribution generate narrow improve rationality generate content consequently VAE widely topic aware text generation instance propose topic variational autoencoder  generate text guidance designate topic specifically  generates gaussian mixture model GMM latent variable prior distribution parametrized neural topic module responsible capture semantic information document mixture component corresponds specific topic generate semantically meaningful topic propose neural variational model topic gaussian distribution latent utilize cnn vector representation input predict gaussian distribution topic connection layer sample topic distribution propose model generate diversified topic previous text generation VAE distribution latent variable usually assume gaussian distribution distinguish latent variable structure semantics develop  model adopts sequential VAE structural feature text topic model extract semantic feature text generate expression structure topic  topic model generates text gaussian distribution latent variable ensures capture textual semantic information encoder discriminator decoder generate text semantics emotional text generation emotion emotional likely stimulate reader additionally adjust style content accord emotional daily communication due necessity integrate emotional information researcher attention incorporate emotional information generate text user summarize emotion extraction embed generate emotional text extract emotional information input text embed vector representation input decoder propose lstm emotional dialogue generation model mechanism incorporate affective emotional aspect generate response affective embeddings introduce external cognitively engineer affective augment traditional embeddings affective vector affective loss function minimize euclidean distance affective embeddings input response maximize affective content response explicitly affect aware model affectively diverse beam injects affective dissimilarity across beam affective embeddings promote generation emotionally response acm transaction intelligent technology vol article publication date february conditional text generation harmonious machine interaction summary emotional text generation description gru emotional embed emotion category embed capture emotional information internal emotion memory balance  expression emotion gru multi task multi decoder seqseq module generates output style style embed module augments encode representation conditional gan CGAN sentiment generator generates  response sentiment label discriminator distinguishes generate RL emotional editor emotional editor selects template topic emotion RL model enhance coherence emotion expression generate response emotional chat machine ecm generate grammatically context relevant emotionally consistent response ecm leverage emotion category embed capture abstraction emotion expression internal emotion balance  emotion dynamically external emotion memory generate explicit unambiguous emotional expression emotional response mimic emotion positive negative user emotion stochastic sample emotion mimicry mechanism propose encode context emotion generate appropriate empathetic response emotion transfer addition embed emotional information directly transfer emotion another promising generate emotional text achieve goal transfer emotion review positive negative negative positive multi task adversarial training leverage style embed module augment style representation multi decoder seqseq model respectively generate text style propose  model adopts gaussian kernel layer incorporate numeric sentiment intensity decoder finely sentiment intensity generate text cycle reinforcement algorithm model training balance sentiment transformation content preservation elaborately reward tackle lack parallel data gan RL model confirm gan RL significant emotional text generation propose  multiple generator multi discriminator enhance sentiment accuracy quality generate text  multiple generator simultaneously generate text sentiment label positive negative multi discriminator generator focus generate specific sentiment label accurately introduce conditional gan CGAN sentiment dialogue generation model generator CGAN  response dialogue sentiment label discriminator identifies quality generate response item dialogue sentiment label dialogue response belongs data distribution propose emotional editor module template accord emotion topic information dialogue introduce RL promote quality generate response emotion topic coherence acm transaction intelligent technology vol article publication date february summary knowledge enhance text generation description keyword embed encoder module leverage external memory embed conversation related generate content response transformer memory network memory network retrieves knowledge dialogue memory transformer encodes decodes text representation generate response conduct knowledgeable discussion domain topic knowledge graph attention graph attention mechanism integrate commonsense information knowledge dialogue generate appropriate informative response lifelong knowledge completion obtain knowledge user unknown concept inferencing knowledge VAE utilize latent variable VAE sentiment generate text explore researcher combine VAE holistic attribute discriminator effective  semantic structure allocates dimension latent representation encode positive negative semantics capture salient attribute independent feature global discriminator facilitate effective  latent code semantics discrete text generator endow poetry generator ability express specific sentiment negative positive improve semantics diversity generate sentiment strongly couple semantics poetry author latent variable sentiment text content capture generalize sentiment related semantics besides temporal sequence module capture sentiment transition poetry generate diverse semantic sentiment knowledge enhance text generation nowadays text generation advantage neural network model generate fluent semantic consistent text however gap  text expression combine knowledge text generation fail achieve combine sufficient knowledge commonsense knowledge information specific CTG generate logical credible informative text external knowledge structure knowledge graph KG compose knowledge triple relation tile unstructured knowledge KB compose text specific concept combine external knowledge CTG summarize introduce detail unstructured KB enhance model unstructured KBs abundant knowledge textual extract knowledge related input combine generation stage direction research extract knowledge unstructured KB keyword input keywords instance extract relevant knowledge knowledge keyword encode vector factual evidence dialogue response generation acm transaction intelligent technology vol article publication date february conditional text generation harmonious machine interaction extract relevant knowledge effectively propose global perspective appropriate knowledge topic transition vector obtain context express global information local knowledge extraction generate informative fluent text parameter rely  dialogue model insufficient knowledge dialogue data decoder stage model generates context processor module generates context knowledge processor module generates knowledge document hierarchical attention mechanism decode manager fuse generation probability module dynamically switch decode mode accord decode generate context relevant informative reasonable response extract relevant textual knowledge important understand semantics integrate text generation transform extract knowledge triple sequence token encode vector representation lstm context vector knowledge vector concatenate calculate response appropriate response technical orient dialogue communicate ubuntu relevant knowledge text description embed vector embed average bert model concatenate traditional embeddings enhance understand technical dialogue knowledge reader attentively knowledge embeddings retrieve semantic information decode stage generate informative response addition rnn combine transformer memory network domain knowledge dialogue memory network retrieves related knowledge internet accord input knowledge memory memory independently encode transformer encoder transformer encode dialogue context standard dot attention memory candidate dialogue context perform knowledge input decoder generate knowledgeable response due powerful performance transformer researcher knowledge understand introduce hierarchical interaction context external document knowledge capture important document context multi attention module transformer appropriate response generate vector representation external knowledge multihead attention mechanism incorporates encode knowledge utterance span multi dialogue decoder generates contextual coherence response attend context information refines attend knowledge vector generate informative response knowledge enhance dialogue relation dialogue context knowledge knowledge selection diverse propose prior posterior distribution knowledge  variable improve knowledge selection accuracy sequentially model knowledge selection previous scope knowledge candidate reduce posterior distribution knowledge leverage response information knowledge accurate structure KG enhance model knowledge graph structure knowledge describes physical entity connection accurately suggests knowledge graph helpful knowledge enhance text generation propose entity link module optimal entity input knowledge triple external KG encode lstm similarity relation candidate calculate acm transaction intelligent technology vol article publication date february appropriate triple  algorithm propose transform structure triple dimensional vector representation widely knowledge enhance text generation link knowledge translate document encode vector TransE concatenate internal vector NMT embeddings decoder input enhance quality generate translation extract entity external KG adopt TransE vector knowledge vector fed multi attention channel generate coherent text summary attention mechanism apply knowledge knowledge graph graph attention incremental encode schema hidden information context graph contextual attention mechanism encode knowledge graph vector multi source attention mechanism comprehensively understand content generate reasonable consistent ending knowledge dialogue model CCM leverage graph attention mechanism promote dialogue understand knowledgeable response generate static graph attention module encodes graph relevant dialogue concatenate graph vector input vector enhance semantic information input dynamic graph attention module attentively knowledge graph triple graph decoder adaptively generic entity retrieve graph generation generate text entity propose  walker model learns symbolic transition dialog context structure traversal KG graph decoder attends viable KG predict relevant entity KG associate entity dialogue context entity mention previous generate multiple title graph transformer model computes hidden representation node graph attend attention strategy leverage relational structure knowledge graph decoder attends encoding knowledge graph document title decoder hidden generate informative text propose data text generation model extract entity data link wikidata external knowledge temporary memory dual attention mechanism apply generate input information background knowledge information dialogue context knowledge retrieve  mechanism model focus knowledge highly relevant context gcn extension cnn graph domain effectively structural information node knowledge graph regard graph inference node graph correspond entity document whereas encode relation within document coreference link simply occurrence document entity graph relates mention entity within across document document encoder obtains representation mention context relational gcn propagates information entity graph generate continuous model although exist introduce knowledge CTG knowledge usually fix cannot expand update continuous interactive surroundings important capability update knowledge accord daily important factor building humanoid text generation knowledge model namely lifelong interactive acm transaction intelligent technology vol article publication date february conditional text generation harmonious machine interaction summary visual text generation description rnn cnn encoder cnn capture information image decoder rnn generates neural description feature lstm cnn cnn lstm respectively encode image vector capture semantic information another lstm generates correspond conditional gan CGAN cnn capture information image lstm generates relevant description discriminator evaluates quality generate description memory network embed image historical dialogue respectively image dialogue context information conversation inference  enable chatbots interactively continuously knowledge communicate user mimic acquire knowledge   user related item unknown concept infers expand knowledge visual text generation usually information image visual text generation important research direction text generation important application image caption visual QA summary visual text generation combine cnn rnn capture information image generate text combine cnn rnn visual text generation achieve goal automatically image generate reasonable description utilize encoder decoder structure encoder cnn capture information image decoder rnn generates text description due loss image information dimension structure cnn propose image caption model utilize attention mechanism extract important information image generate accurate detailed image description combine cnn lstm image utilize cnn capture related information image lstm generate latent representation image semantic relationship text description image generate correspond specific image memory network model instead visual QA implement visual dialogue communicate user multiple image task visual dialogue publish visual dialogue dataset  novel encoders visual dialogue task fusion module encodes image historical dialogue respectively hierarchical recurrent encoder module encodes dialogue memory network module former QA factual basis latter response generation http  org acm transaction intelligent technology vol article publication date february gan model leverage conditional gan CGAN model generate quality image description aspect naturalness semantic relevance diversity jointly generator description image evaluator ass description visual content criterion semantically relevant VAE image caption important ensure lexical syntactic diversity generate caption propose variational multi modal infer  model lexical syntactic diversity infer latent variable approximate posterior inference visual semantic prior visual feature latent variable diverse caption image generate previous usually generate latent variable entire input ignore information substructure develop  model learns latent capture intention data dependent transition model capture intention representation remain encode backward lstm generate diverse caption multi conditional text generation summarize exist CTG simultaneously text generation reasonable anthropomorphic content daily conversation generation model usually context information personality characteristic interlocutor abundant external knowledge generate reasonable response therefore hybrid essential improve quality CTG emotion inherent attribute explicit emotion model combine improve naturalness humanness generate content realize emotional text generation detect emotion textual content observation usually express emotion rely conversation context external knowledge propose interpret contextual utterance leverage external commonsense knowledge enhance emotion detection performance hierarchical attention attention module abstract contextual information context aware affective graph attention leverage knowledge facilitate understand context emotion detection conversation develop chatbot emotion accord conversation context user sentiment recognition model dialogue agent robot emotion feedback user topic emotion important factor dialogue ensure semantic coherence conversation develop twitter latent dirichlet allocation lda model detect topic input prior knowledge dynamic emotional attention mechanism obtain content affective information related input text additional topic generate recommend text specific item user recommender fuse aspect sentiment external knowledge recommend generation tune bert model apply aspect aspect sentiment polarity review aspect fusion module fuse aspect item title knowledge fusion module fuse relevant knowledge directional attention module generate personalize content recommend persona important role empathetic conversation novel multi domain dataset persona empathetic conversation dataset propose efficient bert response selection model  multi hop attention interactive acm transaction intelligent technology vol article publication date february conditional text generation harmonious machine interaction external knowledge guidance CTG improve understand generate informative text instance introduce knowledge personalize  description generation model fuse aspect user category knowledge generate informative personalize description attention module transformer encode attribute relevant knowledge specific user category semantic vector representation perform semantic interaction capture semantic feature decoder fuse external knowledge topic essay generation background information essay generation memory augment neural model selects knowledge concept memory matrix decoder attends memory text generation update accord decoder generate topic consistent informative essay generation text usually influence factor combination multiple promising research trend CTG appropriate context combine accurate knowledge express specific emotion conform unique personalize characteristic text generation generate anthropomorphic text pre model CTG pre training widely explore nlp pre training model text corpus initialize network parameter universal knowledge syntactic semantic information neural text tune model amount specific downstream task data excellent performance achieve pretrained model introduce nlp embed amount data lstm model unsupervised contextual vector obtain demonstrate across discriminative understand  task recent pre model transformer architecture ability model data improve representation generation performance transformer gradually replace mainstream lstm nlp numerous bert model GPT model attention bert learns bidirectional representation massive textual data conditioning backward sequential context specific output layer adjust model structure pre bert model tune achieve performance  task text classification subsequently optimize pre training bert improve ability representation typical  RoBERTa  unconditional generation GPT model regard pre generation due standard model text generation task GPT adopts typical pre training tune training framework transformer decoder feature extractor pre training stage training task unidirectional model encode knowledge decoder tune stage parameter pre training model tune accord specific task supervise tune pre training model data specific domain reduce generalization ability model GPT GPT remove supervise tune stage GPT directly massive training sample zero shot training nlp task GPT billion parameter impressive performance without gradient update tune acm transaction intelligent technology vol article publication date february pre model generation task adopt seqseq framework attention mechanism highly potential critical addition GPT series transformer decoder researcher explore seqseq pre training unconditional text generation jointly encoders decoder generation performance  bart combine pre training encoder decoder reconstruct fragment mask token input randomly encoder predicts masked token decoder joint training improves ability feature extraction model achieve promising generative performance zero shot shot finetuning task specific data research pre model text generation task explore potential pre model text generation CTG pre model become promising research direction powerful generation ability pre model text generation model expression generate personify text specific straightforward incorporate pre model CTG modify model architecture extra conditional input specific finetuning perform intermediate tune data adapt pre GPT model domain tune target generation dataset multi task objective generate grammatical stylistic consistency auxiliary training signal generate constrains model rank sensible text perplexity propose mention namely code specific text novel text input text text pre training phase conditional transformer ctrl model learns relationship code text generate text desire specific code pre transformer model sensitive parameter tune adapt pretrained model arbitrary conditional input pseudo attention module learns task specific encoder injects encoder conditioning directly pre attention model arbitrary input additional conditional input inject pre model without model architecture affect generate text leverage model distil text generation performance conditional masked model MLM task propose enable pre bert additional conditional input randomly mask token target sequence knowledge distillation stage generate sequence logits teacher bert model contains information backward context sequence global guidance probability distribution target text generation model mimic contains useful grain conditional information leverage redundant external knowledge capacity constraint propose pre model response generation model knowledge selection module formalize knowledge selection sequence prediction knowledge selection response generation module jointly optimize unlabeled dialogue endow generative model knowledge generalization ability CTG pre training model adjust model structure tune model data specific entail significant retrain plug model  allows anyone flexibly plug attribute model desire objective unconditional acm transaction intelligent technology vol article publication date february conditional text generation harmonious machine interaction LM instead training model scratch  attribute model influence generate exist GPT attribute model responsible estimate probability text sequence specific attribute positive negative maximize probability generate sequence desire attribute generate pre define  amount label text effectively balance generation fluency conditioning leverage topic model enhance  unlabeled collection document attribute model discriminator predict document topic unconditional model  merge obtain conditional model topic utterance equip dialogue model multiple  response information etc without retrain dialogue model propose adapter bot trigger via adapter independently backbone adapter bot pre conversational model  ability response generation trainable adapter backbone optimize target dataset dialogue specific dialogue dialogue manager dialogue dialogue adapter bot chatbot generate text precise topic sentiment propose   model generate text target text content grain encoder decoder model pre GPT  incorporates target content encode text representation passing content representation decoder generation supervise training without label data  quality text content explore generate text lexical constraint lexical constraint propose pointer model generates adjective keywords constrains insert finer granularity around keywords iteratively generate training objective pointer generate text sequence keywords constraint masked model MLM objective bert pre bert initialize model training boost generation performance latent variable VAE model capture representation topic semantics facilitate CTG combine VAE powerful ability pre training model propose  model pretrained latent variable model  stage pre training tune pre training stage variational auto encoder objective text corpus construct universal latent reorganize tune stage label specific task data pre latent   latent update parameter adapt downstream task specially stylize response generation task embed response style reference joint latent tune  generate response closer desire text style sufficient pre training text corpus pre model impressive understand generation ability research CTG leverage pre model attract attention summarize integrate conditional information pre training model effectively reduce retrain important future research direction acm transaction intelligent technology vol article publication date february decode strategy CTG detailed investigation CTG context text generation topic aware text generation meanwhile hybrid pretrained model apply CTG summary previous focus modify encoder interaction mode encoders decoder fuse conditional information text generation decode stage decode strategy impact quality generate text researcher propose decode strategy improve quality generate content reduce repetitive straightforwardly restrict probability distribution decode stage specific quality content specific generate CTG model introduce universal decode strategy beam sample summarize improve decode strategy propose CTG decode  training beam due vocabulary sequence generation enormous researcher propose heuristic reduce generation practical beam commonly decode strategy text generation model recently approximately maximizes likelihood generate sequence hyper parameter beam decode conditional probability candidate output sequence subsequent additional attach output sequence sequence conditional probability optimal generate sequence candidate conditional probability beam greatly reduces requirement text generation limit beam however sequence generate content optimal overcome shortcoming beam improve propose propose diverse beam increase variety generate text beam utilizes dissimilarity penalty reduce similarity similarly propose sibling diverse beam contains penalty proportional rank candidate token encourage preserve hypothesis diverse source within beam sample goal sample decode strategy reduce repetition increase diversity generate content utilize stochastic decision generation sample sample token distribution filter distribution token sample nucleus sample sample token cumulative probability threshold decode strategy aim distribution generate distribution generate text meanwhile focus decode strategy specific CTG effectively fuse conditional information decode decode typical decode strategy increase decrease probability propose interactive poetry generation enables user edit polish generate adjust aspect sentiment  etc tth decode partial hypothesis expand calculate generation acm transaction intelligent technology vol article publication date february conditional text generation harmonious machine interaction   generative probability wordw accumulate generation generate refers decode feature correspond decode feature assigns generation probability generate decode considers feature generate content topical emotion instead manually calculation formula decode feature additional item decode objective function discriminative model construct powerful generator aspect encodes aspect quality generation enhance generation performance rnn generator decode objective function formalize lo plm  objective compose traditional rnn model probability lo plm additional calculate discriminator mixture coefficient discriminator propose discriminate generation repetition model avoid repetition relevance model guarantee contextual relevance generate content etc interpolate objective function probability coefficient discriminator optimize minimize difference assign continuation continuation predict model similarly incorporate topic semantic similarity constraint decode objective dialogue encourage generation topic relevant content response distribution topic generate response input hmm lda model apply estimate topic distribution response input compute topical similarity semantic similarity constraint encourage generate response semantically input decode useful technique conditional text generation conditional feature generate text assign generation probability however specific feature decode risk distribution generate unanticipated  training standard approach training neural text generation model maximize likelihood approximately decode likely sequence fatal defect likelihood training model generate frequency generate text dull optimize  training objective training text generation model efficiently  training decrease generation probability token incorrect token likely frequent token likely repetition probability improve quality generate text exist dialogue tend generate frequent repetition incorporate  training dialogue regularize generate output distribution bias mitigate repetition copying vocabulary usage contradiction bias mainly responsible reduce occurrence frequency bias contradiction assign probability acm transaction intelligent technology vol article publication date february review conditional text datasets dataset description cornell movie dialog context dataset multi dialogue dataset extract raw movie script ubuntu dialogue corpus context dataset multi dialogue dataset extract ubuntu chat persona chat personalize dataset personalize dialogue dataset conversation profile information personalize dataset profile dialogue dataset extract personalize characteristic user reddit  personalize dataset personalize description dataset information label knowledge user category attribute empathetic dialogue emotional dataset emotional dialogue dataset conversation emotion  emotional dataset emotional dialogue dataset utterance label emotion cmu  knowledge dataset document conversation dataset conversation content specify document wizard wikipedia knowledge dataset knowledge dataset conversation directly knowledge retrieve wikipedia knowledge dataset commonsense conversation dataset response correspond commonsense knowledge graph  visual dataset visual dialogue dataset query image inconsistent contradictory utterance training sample positive negative coherent behavior likelihood training perform coherent data  objective apply incoherent data reduce probability generate context incoherent response  training text generation model unlikely generation assign probability repetitive dull text generation improve overall quality generate summary  training potential research CTG probability specific conditional feature express generate content efficient fusion conditional information quality conditional text generation achieve conditional datasets training CTG model conditional text data emotional text data personalize text data data relatively scarce researcher data scarcity quality conditional text datasets release brief summary conditional text datasets acm transaction intelligent technology vol article publication date february conditional text generation harmonious machine interaction context datasets cornell movie dialog multi dialogue dataset contextual information conversation contains metadata collection fictional conversation extract raw movie script ubuntu dialogue corpus another multi dialogue dataset almost conversation extract ubuntu chat training context sensitive technical dialogue personalize datasets quality personalize dialogue dataset persona chat dialogue conversation profile information dialogue conduct around personalize characteristic authoritative profile dialogue dataset conversation reddit personalize characteristic extract user social opportunity personalize text generation later researcher personalize knowledge description dataset  taobao chinese shopping website information description label knowledge user category attribute emotional datasets publish emotional dialogue dataset empathetic dialogue extensive emotion speaker emotion conversation publish another quality emotional dialogue dataset  dialogue facebook  utterance label specific emotion accord textual content emotional dialogue response generation knowledge datasets cmu  document conversation dataset conversation specify document popular movie extract wikipedia article wizard wikipedia another domain dataset conversation directly knowledge retrieve wikipedia commonsense conversation dataset response correspond commonsense knowledge graph associate knowledge graph retrieve  typical structure knowledge graph visual datasets  visual dialogue dataset query image researcher utilize research visual text generation evaluation CTG detailed investigation CTG summarize commonly conditional text datasets another issue researcher evaluate performance model meaningful comparison http cornell edu  cornell movie dialog corpus html http github com  ubuntu rank dataset creator http github com   project  http reddit com datasets comment  http taobao com http github com   http   edu  index html http github com  datasets  http parl project wizard wikipedia http  tsinghua edu hml dataset commonsense http  http  org data acm transaction intelligent technology vol article publication date february reasonable evaluation metric researcher accurately evaluate performance model conclusion promote application model generation technology progress recent reasonable evaluation generate text challenge researcher unified theory effectively evaluate text generation lack reasonable quantitative evaluation metric prevent development mainly evaluation metric stage namely automate evaluation metric evaluation metric summarize automatic evaluation metric automate machine evaluation intuitive convenient evaluate quality text generation quality generate text uniquely formula difference generate text truth text various evaluation metric text generation gram metric widely calculate overlap gram generate text truth text apply machine translation calculate overlap translate text target reference widely evaluation text generation typical evaluation metric gram summarize bleu bleu harmonic gram precision generate text respect truth reference gram precision refer proportion generate text gram reference gram clipped maximum gram occurs reference bleu contains variant  bleu nist nist typical variant bleu improves bleu evaluation accuracy assign frequency gram informative impose penalty rouge rouge calculate subsequence LCS generate text target text subsequence calculate maximum precision recall reference text obtain rouge accuracy recall calculate LCS generate text reference text respectively meteor meteor calculates precision recall unigrams generate text reference text addition fuzzy adopt calculation stem analysis wordnet synonym calculate multiple reference text meteor CIDEr CIDEr evaluation metric image caption task text generation task CIDEr calculate average cosine similarity generate text reference text gram importance individual gram calculate frequency inverse document frequency TF idf evaluation metric gram widely various text generation task machine translation dialogue text summarization however numerous gram cannot capture semantic information effectively evaluate text generation model specific application variation machine translation metric overlap gram evaluation accuracy however reference text diversity dialogue text summarization effective evaluation metric acm transaction intelligent technology vol article publication date february conditional text generation harmonious machine interaction addition evaluate relationship generate text reference text characteristic evaluate quality  perplexity perplexity metric evaluate quality model average uncertain predict model performance fluency diversity generate text evaluate perplexity evaluation metric automatic evaluation metric quality text generation model similarity generate text reference text cannot reflect correctness informativeness naturalness internal characteristic generate content therefore intelligence introduce evaluation text generation reasonable effective evaluation evaluation turing quality machine generate text distinguish data evaluation overall quality generate text  fluency naturalness informativeness persona consistency CTG topic knowledge personalize feature evaluate automatic evaluation metric evaluation almost effective evaluate CTG research stage knowledge enhance text generation informativeness metric effectively combine external knowledge external knowledge correctly reasonably integrate generation information text generate personalize text generation persona consistency metric evaluate generate text conforms persona characteristic assign text generation agent evaluation effective standard evaluate CTG however due subjectivity variation evaluation addition evaluation usually expensive amount manpower implement evaluation repeatable costly reasonable evaluation metric evaluation CTG reasonable challenge learning MODELS CTG detailed investigation various CTG combine generate text appropriate informative anthropomorphic attempt address CTG distil schema conditional generation model explicit conditional model implicit conditional model conditional knowledge transfer explicit conditional model directly regard external input generator information input information efficient CTG easy fuse information global perspective generate consistent external implicit conditional model directly input utilize specific algorithm attention mechanism RL generation stage implicit semantic information CTG sensitive lack conditional data fully model conditional knowledge transfer extract text knowledge acm transaction intelligent technology vol article publication date february model CTG amount text data transfer CTG improve performance CTG model mainly focus fuse conditional information generation meanwhile decode strategy apply CTG appearance model CTG explicit conditional model CTG text image visual text generation explicitly input text generation enhance input information straightforward explicit conditional model text generation input transform vector representation embed encode additional neural network embed text sequence vector representation additional input information decode stage text generation explicit conditional model straightforward proven integrate external CTG without increase complexity model context text generation regard dialogue context information embed dialogue vector representation decode another rnn promote context aware response personalize text generation propose persona model speaker consistency dialogue speaker model individual speaker vector encode speaker specific information gender dialect influence content style speaker vector inject decoder hidden layer generate personalize dialogue response knowledge enhance text generation propose knowledge dialogue model  informative dialogue response dialogue relevant knowledge extract knowledge dialogue acm transaction intelligent technology vol article publication date february conditional text generation harmonious machine interaction keywords encoder module adopts memory network encode vector representation user input dialogue module enables dialogue deeper exploit inter lexical dependency input effectively fuse knowledge generation encode dialogue knowledge fed decoder generate context relevant informative response implicit conditional model capture deeper semantic information information directly fed decoder embed interact decode deeper additional algorithm implicit conditional model attention mechanism typical implicit model dynamically capture important accord decoder realize implicit model information text generation effectively RL mechanism reward action feedback specific reflect generate therefore RL suitable implicit conditional model penalize reflect conditional information RL mechanism model conditional information generation propose persona aware attention mechanism attention context vector integrate persona vector persona aware bias estimate generation distribution personalize dialogue response generation graph attention mechanism static graph attention dynamic graph attention respectively promote dialogue understand knowledgeable response generate model extract relevant knowledge graph entity input keywords encode graph static vector representation static graph attention module considers node relation node graph encode structural semantic information decoder stage dynamic graph attention attends knowledge graph knowledge triple graph efficiently integrate information knowledge graph informative response generation propose RL dialogue model combine emotional editor module generate customizable emotional response emotional editor selects template accord topic emotion reference generation RL mechanism constrains quality generate response emotion topic coherence generate emotional topic relevant meaningful dialogue response multitask introduce enhance model discrimination coherence topic emotion conditional knowledge transfer text generation CTG lack available conditional text datasets although researcher release conditional text datasets generally perform CTG extract text knowledge amount text data transfer CTG perform CTG absence conditional data conditional knowledge transfer personalize text generation dialogue model predict response previous context autoencoder model volume non conversational personal data model role specific characteristic user multi task mechanism decoder parameter model model capture speaker role expressive style domain expertise characteristic target user acm transaction intelligent technology vol article publication date february generate personalize response without recourse speaker conversational data utilizes domain adaptation mechanism generate personalize dialogue response response generation model attention lstm encoder decoder architecture pre dialogue data without user specific information model tune amount personalize dialogue data dual mechanism generate personalize response training reward propose evaluate quality generation personalization informativeness  policy gradient adopt generate highly reward response  future TRENDS although advanced technology apply text generation remarkable achievement remain issue issue future development trend text generation context context information situation environment information text generation realize accurate simulation expression important CTG multi dialogue context information usually refers historical dialogue conversation coherent encode dialogue vector representation decoder generate consistent response machine translation propose context aware machine translation model analyze information context translation model encode context information model accurately generate translation text achieve relatively excellent however context information contains exist exist research dialogue predefined external information context expression affected various factor emotion external context information explicit implicit extract properly challenge humanoid CTG effectively integrate various context generate reasonable text future research direction multi modal data translation domain adaptation addition text data various data image video efficiently extract useful information various data convert correspond text representation content painting summarize content movie researcher text generation multi modal data input generate description caption image conduct QA image communicate content image usually utilize cnn extract relevant information image generate correspond text model text generation exist model data handle multi modal data combine multiple model computational moreover semantic data brings difficulty fusion multi modal data incorporate data develop unified model multi source data processing challenge effort conduct generate informative text multi modal data source acm transaction intelligent technology vol article publication date february conditional text generation harmonious machine interaction task CTG personalize text generation emotional text generation available training data scarce text data totally data personalize emotional characteristic cannot requirement specific transfer promising address knowledge massive text data transfer specific domain training conditional text data model knowledge source domain specific target domain scarcity data domain adaptation transfer address issue lack personalize dialogue data tune dialogue model personalize dialogue data model effectively generate personalize dialogue response transfer rapidly develop technology integrate diverse transfer model scarce usable data CTG promising research direction text generation text application composition translate article report however technology bottleneck processing text distance dependence exist ability extract information context topic text machine researcher conduct effort improve model ability generate text lstm gru model address issue rnn model cannot capture distance dependence gate mechanism propose  model generate text feature extract discriminator stepwise guidance signal generator quality text hierarchical reinforcement information discriminator generate text effectively model capture dependency text challenge research text generation technology truly behaving ability freely generate text investigate lifelong lifelong important ability continuously knowledge expand update knowledge various data source physical adapt rapidly environment exist text generation model usually fix datasets ability expand dynamically update accord external environment text generation model personify lifelong ability combine external knowledge realize lifelong text generation research focus dialogue combine knowledge however fix knowledge knowledge update model ability continuous therefore dynamic evolution knowledge important propose lifelong model update knowledge user groundbreaking exploration direction lifelong text generation model however ability actively gain knowledge environment simply user important effective information changeable external environment achieve efficient lifelong important research direction CTG acm transaction intelligent technology vol article publication date february concept propose model knowledge extraction fusion crowdsourced data rapid development popularization social network crowdsourced data web community quora  data embodiment intelligence source external knowledge  however exist knowledge enhance text generation mainly structure knowledge graph unstructured knowledge built advance cannot perform knowledge selection fusion crowdsourced data usually specific domain intelligence data domain dynamically update however heterogeneity crowdsourced data prevent application text generation therefore suitable information crowdsourced intelligence data enhance semantic understand fuse information properly text generation promising research direction CTG incorporate crowdsourced intelligence data CTG propose preliminary encode stage input text interact crowdsourced data relevant knowledge external knowledge source knowledge input text encode separately attention mechanism obtain fusion vector representation input information decode stage generation reference mechanism decoder dynamically generate vocabulary input source accord decode achieve explicit crowdsourced knowledge conclusion systematic review research trend conditional text generation CTG article introduction text generation brief review technique text generation formal definition CTG finally investigate research status various CTG propose model CTG research progress CTG stage numerous research issue promising research direction text generation multimodal data translation lifelong