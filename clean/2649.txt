text segmentation fundamental task processing granularity task define document topical elementary discourse EDUs traditional task heavily rely carefully feature recently propose neural model manual feature engineering suffer sparse boundary tag cannot efficiently handle issue variable output vocabulary limitation propose generic segmentation model namely segbot bidirectional recurrent neural network encode input text sequence segbot another recurrent neural network pointer network text boundary input sequence segbot craft feature importantly segbot inherently handle issue variable output vocabulary issue sparse boundary tag segbot outperforms model task document topic segmentation edu segmentation downstream application propose hierarchical attention model sentiment analysis outcome segbot hierarchical model edu information simultaneously sentiment analysis effectively exploit edu information inner EDUs cannot fully encode feature experimental hierarchical model achieves movie review stanford sentiment treebank benchmark introduction text segmentation fundamental task processing nlp address granularity coarse text segmentation generally refers document sequence topically coherent topic segmentation topic segmentation typically pre requisite discourse analysis task discourse parse downstream nlp application text summarization passage retrieval finer text segmentation refers sequence elementary discourse EDUs edu segmentation exemplify EDUs clause building discourse parse rhetorical structure theory edu segmentation useful text compression elementary discourse EDUs topic edu segmentation attention due utility nlp task although related task typically address separately approach overview supervise unsupervised propose topic segmentation unsupervised segmentation model exploit correlation topic lexical usage broadly categorize similarity model probabilistic generative model similarity model intuition precede category   probabilistic generative model intuition discourse hidden sequence topic characteristic distribution variant hidden markov model HMMs latent dirichlet allocation  supervise topic segmentation model flexible feature cue similarity generally perform unsupervised model however price extensive effort manually informative feature annotate amount data edu segmentation  spade DS successful lexical syntactic feature supervise manner exist text segmentation lexical similarity generally admit lexical semantics capture distribute representation furthermore exist supervise model topic edu segmentation feature manually task domain demand task domain expertise envision distribute representation informative feature task domain without effort propose neural architecture achieve goal topic edu segmentation treat sequence label task predict sequence boundary tag document topic segmentation edu segmentation conditional random CRFs classical model sequence label task nlp recently recurrent neural network crf output layer rnn crf sequence tag task nlp however due sparsity boundary tag edu topic segmentation task CRFs additional gain classifier  classical model sequence label classical model sequence label instead cast segmentation sequence prediction task encoder decoder seqseq model toy encoder decoder model rnn encode input sequence another rnn model generate predict output sequence however limitation model output vocabulary drawn fix model retrain respect vocabulary whereas task segmentation input sequence boundary alleviate issue propose segbot generic neural model text segmentation various granularity segbot distribute representation capture lexical semantics employ bidirectional rnn model sequential dependency encode text decoder unidirectional rnn pointer mechanism infer boundary addition segbot effectively handle variable vocabulary output boundary input sequence sentiment polarity EDUs sentiment polarity EDUs model architecture mathrm  mathrm EG mathrm  mathrm OT EG OT input sequence ldots identify boundary mathrm  mathrm EG mathrm  mathrm OT EG OT consists component context encoder boundary decoder boundary pointer decoder input mathrm  mathrm EG mathrm  mathrm OT EG OT computes output distribution decoder input mathrm  mathrm EG mathrm  mathrm OT EG OT computes distribution identify boundary finally decoder input mathrm  mathrm EG mathrm  mathrm OT EG OT computes distribution model architecture segbot input sequence identify boundary segbot consists component context encoder boundary decoder boundary pointer decoder input segbot computes output distribution decoder input segbot computes distribution identify boundary finally decoder input segbot computes distribution furthermore sentiment analysis downstream application segbot sentiment analysis opinion mining computational opinion sentiment emotion appraisal attitude due increase opinion review internet sentiment analysis become topic knowledge discovery overall sentiment derive EDUs conveys sentiment polarity overall sentiment negative derive edu edu positive sentiment edu derive contribution funny paid attention edu intuition propose hierarchical attention model advantage edu information simultaneously sentiment analysis exist sentiment analysis approach expensive annotation hierarchical attention model exploit hierarchical structure understand sentiment lightweight manner output segbot importantly hierarchical model allows edu information inner EDUs cannot fully encode feature summary contribution propose segbot generic model text segmentation various granularity segbot learns informative feature automatically alleviate tag sparsity output sequence variable output vocabulary conduct evaluate effectiveness segbot granularity document topic segmentation edu segmentation segbot achieves task implement segbot web application user application program interface api http segbot user outcome segbot propose hierarchical attention model edu information simultaneously sentiment analysis conduct evaluate effectiveness hierarchical attention model experimental hierarchical model achieves movie review stanford sentiment treebank benchmark related text segmentation exist approach text segmentation category unsupervised supervise unsupervised lexical cohesion vocabulary tends coherent topic introduce  algorithm text segmentation  vocabulary intersection adjacent coherence vice versa algorithm divisive cluster matrix rank schema  lexical chain identify repetition  approach dynamic program minimum unsupervised topic model induce semantic relationship frequency topic assign latent dirichlet allocation vector  probabilistic latent semantic analysis derive latent representation recent model employ lda compute latent topic achieve superior performance previous model approach investigate supervise text segmentation classifier machine approach finite context derive feature conditional random discourse  lexical syntactic feature binary classifier edu boundary  syntactic shallow syntactic contextual feature exist craft feature approach manual feature engineering text segmentation knowledge generic model neural architecture text segmentation sequence label sequence label fundamental task nlp tag chunk entity recognition ner classical employ machine model hidden markov model conditional random CRFs achieve relatively performance drawback approach amount task specific knowledge craft feature data pre processing recently neural network model successfully apply sequence label neural model ner pioneer architecture temporal convolutional neural network cnns sequence propose recent taxonomy neural model sequence label compose distribute representation input context encoder tag decoder typical approach distribute representation input continuous bag cbow continuous skip gram model addition widely architecture extract representation cnn model rnn model context encoder capture context dependency sequence label employ cnn context encoder  propose iterate dilate convolutional neural network ID cnns capacity traditional cnns context structure prediction utilize bidirectional memory lstm crf architecture sequence tag task POS chunk ner apply bidirectional lstm BiLSTM architecture encode sequential context information tag decoder context dependent representation input correspond sequence tag apply lstm network encode input sequence crf layer decode tag sequence prior along limited text segmentation markov dependency tag cannot effectively capture due sparsity output sequence tag approach apply lstm layer tag sequence drawback approach output fix dependent input sequence exist architecture difference propose segbot tag decoder pointer network crf model effectively capture sequential dependency boundary tag sparse alleviate variable vocabulary output entity boundary input sequence addition propose model sequence chunk pointer network related zhai model propose model directly infers boundary entity network simpler architecture parameter sentiment analysis sentiment analysis active research nlp widely various domain finance marketing health approach handcraft algorithm resort technique organize along traditional sentiment analysis sentiment analysis traditional sentiment analysis technique unsupervised supervise unsupervised acquire annotate training data instead approach resort sentiment lexicon grammatical analysis syntactic investigate graph  model wordnet propose semantic orientation adjective factor subjective meaning supervise sentiment analysis frame supervise classification classical supervise algorithm naive bayes vector machine widely sentiment analysis despite effectiveness feature supervise approach labor intensive considerable amount engineering domain expertise proven greatly successful processing task allows machine fed raw data automatically discover latent representation processing classification detection various neural model propose detect text sentiment granularity document aspect summarize exist sentiment analysis belongs category introduce semi supervise recursive autoencoder network sentiment analysis propose adaptive recursive neural network target dependent twitter sentiment classification recursive model vector representation multi recursively instead bag model however annotate  expensive recently cnn rnn model become popular sentiment analysis parse extract structure feature propose linguistically regularize lstm sentiment classification aim model linguistic role sentiment lexicon negation intensity developed rnn capsule network recurrent neural network sentiment analysis introduce generalization standard lstm architecture structure network topology propose communication model graph convolutional neural network graph recurrent neural network allows information exchange constituent segbot neural text segmentation address sparse boundary tag variable output vocabulary text segmentation propose generic segmentation model namely segbot perform text segmentation various granularity document topic segmentation edu segmentation model architecture illustrates model architecture segbot consists component context encoder boundary decoder boundary pointer worth emphasize segbot generic model granularity task input document topic segmentation edu segmentation input representation distribute representation efficient capture precise syntactic semantic relationship segbot input distribute representation glove representation validate various nlp task text classification reading comprehension embeddings outperform sophisticated supervise various textual similarity task formally input sequence UN distribute representation correspond embed matrix RK representation dimension ultimate goal split input sequence contiguous identify boundary context encoder encode input sequence rnn rnns capture sequential dependency hidden lstms gate recurrent grus capture distance dependency without gradient vanish explosion model gru encode input sequence lstm computationally cheaper recall RK representation gru activation compute   SourceRight click MathML additional feature   SourceRight click MathML additional feature tanh  SourceRight click MathML additional feature SourceRight click MathML additional feature sigmoid function tanh hyperbolic tangent function wise multiplication update gate vector reset gate vector gate vector hidden parameter encoder directional gru BiGRU network memorize future information input sequence hidden BiGRU formalize SourceRight click MathML additional feature indicates concatenation operation hidden backward grus respectively assume gru layer encoder yield hidden RN boundary decoder boundary output input rnn model decode output due ability variable sequence variable output vocabulary decoder input sequence input transforms distribute representation correspond embed matrix gru unidirectional layer formally decoder hidden compute gru SourceRight click MathML additional feature parameter hidden layer decoder described input sequence contains boundary decoder hidden RM dimension hidden layer boundary pointer unlike traditional seqseq model output fix decoder rnns heavily depends input sequence output layer decoder computes distribution input sequence boundary decoder input computes output distribution input sequence input computes output distribution finally input computes distribution unlike traditional seqseq model neural machine translation output vocabulary fix input sequence decode mechanism decoder recall RN RM hidden encoder decoder respectively attention mechanism compute distribution input sequence decode input    wdm SourceRight click MathML additional feature softmax SourceRight click MathML additional feature indicates input sequence softmax normalizes  probability boundary model training teacher model truth decoder rnn mechanism rnns truth boundary loss function negative likelihood boundary distribution training logp SourceRight click MathML additional feature trainable parameter model encoder decoder pointer strength regularization rnn decoder prediction sample truth boundary available traditional seqseq decoder model input decode previous predict boundary algorithm summarizes segbot algorithm training segbot model input sequence truth segmentation label output segbot model initialize parameter randomly epoch  iterate epoch sample mini batch gradient compute context encoder iterate decoder compute decoder compute boundary distribution retrieve input boundary compute batch update return model segbot sentiment analysis text sentiment analysis downstream application demonstrate benefit EDUs segbot introduce segbot sentiment analysis detail model architecture illustrates architecture propose model sentiment analysis intuition overall sentiment derive EDUs conveys sentiment polarity sentiment edu derive contribution employ edu information simultaneously sentiment polarity calculation resort attention mechanism capture important clue sentiment analysis model attention sentiment EDUs merit model advantage edu information simultaneously sentiment analysis propose hierarchical attention model sentiment analysis input sequence consist EDUs transformer layer input representation contextual embeddings attention layer capture important clue edu representation edu attention layer capture indicative EDUs sentiment polarity model compose module input representation transformer encoder attention edu attention attention therefore hierarchical attention model input representation module input sequence sequence embed vector transformer encoder module multi layer bidirectional transformer capture contextual information subsequently attention module aggregate representation informative edu vector finally edu attention module attention mechanism edu capture indicative sentiment EDUs sentiment polarity input representation input UN sequence input representation construct sum correspond token embeddings token embeddings wordpiece  embeddings sequence token transformer encoder transformer network solely attention mechanism dispense complex recurrence convolution entirely superior quality parallelizable significantly multi layer bidirectional transformer encoder input representation contextual embed formally denote sequence representation derive transformer encoder RV hidden transformer encoder attention segbot detect EDUs input sequence sequence EDUs edu argue equally important sentiment polarity employ attention mechanism capture important clue edu edu consist compute attention edu embeddings  exp GT exp GT source    SourceRight click MathML additional feature learnable parameter attention layer  jth slot edu embed edu attention obtain representation edu individual edu sentiment prediction via softmax layer multi layer perceptron softmax mlp SourceRight click MathML additional feature parameter softmax mlp across EDUs sentiment polarity naive average across EDUs however EDUs equally important employ edu attention capture indicative EDUs important sentiment polarity compute edu attention exp DT exp DT sourcewhere learnable parameter edu attention layer finally obtain prediction sentiment label  source model training hierarchical attention model sentiment label loss function negative likelihood boundary distribution training logp SourceRight click MathML additional feature trainable parameter hierarchical attention model strength regularization algorithm summarizes hierarchical attention model algorithm training hierarchical attention model sentiment analysis input sequence truth sentiment label output hierarchical attention model initialize parameter randomly epoch  iterate epoch sample mini batch gradient compute input  compute hidden transformer encoder iterate EDUs compute attention compute edu representation compute compute edu  attention compute batch update return model text segmentation segbot conduct evaluate effectiveness segbot document topically coherent topic segmentation EDUs edu segmentation experimental setting choi dataset evaluate topic segmentation model utilize commonly choi dataset consists document concatenation corpus generate automatic procedure document subset randomly document corpus error metric metric evaluate topic segmentation model slide infer segmentation standard   SourceRight click MathML additional feature document consists belong hypothetical segmentation otherwise indicator function otherwise document accuracy topic segmentation rst DT dataset rhetorical structure theory discourse treebank rst DT publicly available corpus manually annotate edu segmentation discourse relation accord rhetorical structure theory rst DT corpus partition training article article journal previous edu segmentation accuracy respect internal segmentation boundary EDUs correspond inside discourse boundary ability model correctly identify boundary within internal boundary annotation internal boundary model output boundary model output precision recall segmentation performance precision recall  SourceRight click MathML additional feature baseline topic segmentation segbot  unsupervised technique lexical occurrence distribution within text linear text segmentation replaces inter similarity rank local context statistical model maximum probability segmentation text domain independent text segmentation  adapts dynamic program technique optimal topical boundary tsm integrates wise boundary sample algorithm structure topic model capture hierarchical topic structure latent text  exploit semantic relatedness text construct semantic relatedness graph document  probabilistic latent semantic analysis PLSA exploit similarity meaning detect PLSA segmentation function incorporates factor within similarity prior information latent dirichlet allocation lda topic model topic distribution associate  modifies  topic IDs obtain lda model instead BiLSTM crf neural architecture sequence label edu segmentation segbot  conditional random discourse  rst discourse treebank lexical syntactic feature spade discourse  account local interaction global interaction abstract lexical syntactic feature feature perform edu segmentation within machine framework finite context derive feature DS implement binary classifier edu boundary token achieves performance reduces complexity feature BiLSTM crf neural architecture sequence label implementation detail choi dataset split training proportion previous rst DT dataset training partition percent shuffle training development choi rst DT datasets glove dimensional pre embeddings release stanford vector fix without tune training adam optimizer update model parameter addition gradient clip max norm regularization training detail hyper parameter setting segbot implement pytorch framework evaluate nvidia tesla gpus hyper parameter setting hyper parameter setting text segmentation topic segmentation report performance segbot prominent choi dataset involve training hyper parameter specify dataset split document training document dataset split document training comparison data partition segbot consistent exist reimplement BiLSTM crf neural model sequence label BiLSTM crf baseline obtain publish average subset detailed segmentation choi dataset segmentation choi dataset observation segbot significantly outperforms exist date performance unsupervised relatively utilize training data accurate model tsm achieves relatively accuracy estimate parameter corpus data segbot outperforms topic model approach tsm   specifically segbot achieves absolute reduction percent topic model  segbot distribute representation capture semantic information topic model segbot significantly outperforms neural model BiLSTM crf absolute reduction percent segbot effectively capture dependency input boundary sparse topic segmentation segbot upper illustrates boundary distribution axis boundary axis boundary predict segbot indicates segbot effectively partition document topically coherent detail predict boundary highlight purple segbot successfully identifies topic shift utopia oxidation    visualization topic segmentation boundary topic purple document consists upper illustrates boundary distribution segmentation consists topic utopia oxidation    visualization topic segmentation boundary topic purple document consists upper illustrates boundary distribution segmentation consists topic utopia oxidation    edu segmentation segbot baseline described specifically  default setting spade apply modification default setting described delivers significant improvement version reimplement DS BiLSTM crf  publicly available performance publish report precision recall segbot baseline observation segbot outperforms baseline improvement baseline percent precision percent recall percent respectively worth mention segbot tedious feature engineering pre embeddings input segbot outperforms model carefully feature  spade DS segbot syntactic parser tagger easily transfer resource domain BiLSTM crf input model pre embeddings segbot BiLSTM crf absolute improvement percent segmentation rst DT dataset edu segmentation segbot  pan assure  joint venture  profit hotel venture EDUs boundary respectively identify boundary dominant attention implies segbot successfully structure syntax visualization edu segmentation decoder input  segbot predicts boundary  input segbot predicts correspond boundary finally predict boundary model analysis tune recall segbot pre glove vector input training embeddings learnable parameter accordingly training data embeddings report performance rst DT dataset without tune glove vector performance shelf glove vector without tune tune embeddings performance tune fix embeddings performance tune fix embeddings tune mechanism vocabulary data illustrates relationship glove vocabulary training data vocabulary data vocabulary glove vocabulary percent training vocabulary percent vocabulary tune percent vocabulary training data unknown glove vocabulary data tune embeddings performance segbot obtain glove vocabulary model effectively handle data training vocabulary venn diagram vocabulary pre embeddings impact pre embeddings conduct publicly available embeddings namely google embeddings billion google news fasttext embeddings billion crawl random initialization reference report performance segbot embeddings input rst DT dataset edu segmentation embeddings google fasttext glove fix without tune embeddings random initialization training  EG OT embeddings edu segmentation google embeddings deliver weak performance vocabulary mismatch google embeddings exclude punctuation digit stopwords extremely important edu segmentation fasttext embeddings obtain performance glove embeddings sensitive manner punctuation digit stopwords error analysis error topic segmentation edu segmentation decoder rnns segbot predicts boundary boundary however boundary prevent segbot correctly detect subsequent boundary error boundary error boundary error boundary error boundary edu segmentation boundary bold font  helm november  exterior politician lack introduce innovative segbot wrongly predicts boundary wrongly predict boundary prevent detection boundary demonstrate effectiveness segbot discourse segmentation develop concise web interface interface consists panel input panel output panel web interface http segbot web interface http segbot input panel user user click button label arrow input segbot model input output panel display segmentation output panel cod edu displayed cod EDUs context facilitate easy interpretation user output panel EDUs EDUs EDUs allows user individual EDUs attention boundary segbot allows valid english input demonstration user elementary discourse displayed output panel click reset button clearing input panel II segbot sentiment analysis experimental setting conduct benchmark datasets movie review stanford sentiment treebank movie review MR dataset MR dataset consists movie review snippet strike extract usually  com snippet annotate source review label positive negative  tomato positive snippet negative snippet stanford sentiment treebank sst dataset sst dataset extract movie review annotation sentiment contains grain sentiment label parse movie review dataset specifically evaluate complex compositional model sentiment label negative negative neutral positive positive utilize annotation report sentiment analysis baseline hierarchical attention model baseline rae learns distribution sentiment label node hierarchy recursive autoencoders  employ syntactically  recursive tensor neural network learns  semantic compositional vector representation lstm classical memory lstm bidirectional lstm learns bidirectional dependency LR lstm linguistically regularize variant lstm regularize difference predict sentiment distribution previous LR lstm linguistically regularize variant lstm lstm generalization standard lstm architecture structure network topology cnn convolutional neural network vector obtain unsupervised neural model cnn tensor extends gram convolution non consecutive combination rank tensor dan average network unweighted average vector multiple hidden layer classification  utilizes strength semantic feature lstm model calculate context dependent input rnn capsule capsule model recurrent neural network sentiment analysis implementation detail MR dataset training dev partition previous sst dataset standard data partition accuracy datasets transformer layer pre bert model layer transformer attention hidden parameter transformer layer dropout probability batch rate adam optimizer update model parameter hierarchical attention model implement pytorch framework evaluate nvidia tesla gpus sentiment analysis report accuracy MR sst datasets observation experimental hierarchical attention model significantly outperforms baseline MR sst datasets model achieves relative accuracy improvement percent rae MR sst datasets respectively notably model significantly outperforms rnn capsule MR dataset cnn tensor sst dataset relative improvement percent respectively LR lstm  achieve accuracy baseline MR dataset however linguistic knowledge intensity regularizer sentiment lexicon worth reiterate approach tedious feature engineering edu segmentation segbot cnn tensor outperforms baseline sst dataset unlike hierarchical attention model solely attention mechanism dispense complex recurrence convolution entirely superior quality parallelizable recent rnn capsule capsule network MR dataset model significantly outperforms achieve performance baseline model leverage edu information simultaneously sentiment polarity calculation attribute improvement hierarchical attention model edu information inner EDUs cannot fully encode feature sentiment analysis movie review MR stanford sentiment treebank sst datasets model hierarchical attention model consist attention edu attention understand visualize attention MR dataset segbot detects EDUs incomprehensible mess edu cinema edu stuck pit edu  cinema edu hierarchical attention model capture important sentiment mess stuck  EDUs meanwhile model attention sentiment edu incomprehensible mess visualization edu attention mechanism depth corresponds importance EDUs sentiment polarity calculation visualization segbot detects EDUs  item legend edu edu chop edu edu information sentiment polarity edu hierarchical attention model attention edu successfully classifies positive ablation impact various architectural decision model performance report ablation analysis conduct MR sst model variation deviate standard model architecture ablation variation aim clearly showcasing importance component replace transformer rnn remove attention mechanism remove edu attention mechanism remove edu attention mechanism detect sentiment polarity representation detect sentiment polarity average representation output transformer layer detect sentiment polarity average representation output attention layer ablation analysis MR sst datasets architecture model configuration across MR sst datasets attribute model relies transformer encoder hierarchical attention mechanism model sentiment polarity performance replace transformer encoder rnn encoder observation  effectiveness transformer encoder capture contextual dependency impact remove hierarchical attention mechanism noticeable degradation performance datasets model configuration obtains performance implies attention important architecture finally heuristic average edu representation performance degradation signifies model edu simultaneously essential conclusion propose segbot neural model text segmentation granularity segbot craft feature prior knowledge text model effectively address sparsity boundary tag text segmentation importantly exist neural model segbot advantage inherently handle variable output vocabulary evaluate effectiveness segbot conduct document topic segmentation edu segmentation task respectively experimental segbot significantly outperforms exist task edu segmentation preserve semantic meaning subsequently benefit downstream application sentiment analysis propose hierarchical attention model edu information simultaneously sentiment analysis hierarchical model attention sentiment EDUs experimental hierarchical model achieves movie review stanford sentiment treebank benchmark