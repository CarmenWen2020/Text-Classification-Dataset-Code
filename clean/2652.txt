feature selection important technique dimensional data variety machine data mining task cluster classification retrieval etc fuzziness widespread data society however exist feature selection ignore existence fuzziness data sub optimal feature subset address propose novel unsupervised feature selection unsupervised discriminative projection feature selection UDPFS discriminative feature conduct fuzziness sparse simultaneously specifically projection matrix transform data dimensional representation partition cluster membership matrix sparse constraint addition norm regularization apply projection matrix discriminative projection matrix sparse obtain perform fuzziness sparse simultaneously effective alternative optimization algorithm propose objective function evaluate experimental datasets effectiveness superiority propose unsupervised feature selection introduction rapid growth dimensional data feature selection increasingly important role text mining image identification visual classification bioinformatics etc important challenge task valuable information complex dimensional data due redundant feature outlier complex dimensional data dimensional data demand compute storage requirement data processing dimensional data adversely affect performance subsequent data processing task cluster classification due curse dimensionality feature selection obtain relevant dimensional feature subset dimensional feature remove redundant feature outlier improve performance subsequent data processing task feature selection effective dimensional data feature selection attention recent decade variety feature selection application propose accord label information feature selection category supervise semi supervise unsupervised feature selection supervise feature selection discriminative feature obtain accord feature relevance accord label information data correlation feature selection CFS fisher FS minimal redundancy maximal relevance mRMR robust feature selection  discriminative regression   induced robust feature selection  supervise discriminative feature selection ssd FS multi source causal feature selection  explosion technology development internet lack label dimensional data increase dramatically label data consume laborious practical choice unlabeled data feature selection however unsupervised feature selection challenge important due lack label information various unsupervised feature selection propose perform unsupervised feature selection utilize global local structural information data label information laplacian  multi cluster feature selection  unsupervised discriminative feature selection  structure optimal graph feature selection  reconstruction embed unsupervised feature selection  dependence unsupervised feature selection  etc semi supervise mixed label unlabeled data  rescale linear regression  etc unsupervised challenge important feature selection focus unsupervised feature selection fuzziness exists widely society decade decade fuzzy feature selection propose researcher combine fuzzy feature selection advantage fuzzy feature selection inspire propose novel unsupervised feature selection projection matrix transform data dimensional representation partition cluster membership matrix sparse constraint obtain pseudo label discriminative feature inspire projection matrix structural sparse regularization perform feature selection norm regularization aim discriminative projection improve performance unsupervised feature selection contribution novel unsupervised feature selection propose introduce fuzziness fully utilize membership sparse constraint feature selection adaptively sparse membership obtain discriminative projection matrix dimensional apply norm regularization projection matrix feature across data instance structural sparsity alternative iterative optimization algorithm propose objection function effective algorithm propose norm regularization theoretical analysis convergence evaluate various datasets demonstrate performance propose UDPFS unsupervised feature selection fuzziness related summarize notation define brief review classical unsupervised feature selection application introduce fuzzy cluster technique notation definition summarize notation define throughout uppercase bold denotes matrix vector denote lowercase bold denote scalar matrix aij denote transpose ith matrix respectively rank denote transpose rank inverse matrix trace matrix define aii aii ith diagonal denotes dimensional vector vector norm norm vector denote norm denotes  norm frobenius norm matrix define  SourceRight click MathML additional feature norm define  source norm define  source denote data matrix feature sample respectively sample transpose ith data matrix unsupervised feature selection subsection brief review classical unsupervised feature selection roughly classify category namely filter wrapper embed filter evaluate discriminability feature data filter unimportant feature retain discriminative feature  evaluate discriminability feature locality preserve laplacian spec rank importance feature spectral regression feature selection wrapper component learner directly  seek optimal feature iteration algorithm however wrapper repeatedly algorithm feature selection therefore wrapper complexity filter embed embed combine feature selection machine model integrate objection function optimal generally exist embed intrinsic structure various discriminative feature machine sparse widely apply embed feature selection feature sparse  preserve multi cluster manifold structure data feature selection  discriminative feature incorporate discriminative analysis norm minimization joint framework  explicitly impose nonnegative constraint indicator spectral cluster intrinsic manifold structure unsupervised feature selection  jointly performs embed sparse regression feature selection  directly embed feature selection cluster via sparse without transformation  performs local structure feature simultaneously feature selection however obviously unsupervised feature selection ignore fuzziness widely society performance exist limited ultimately sub optimal fuzzy cluster sparse numerous fuzzy cluster extension propose popular application fuzzy cluster suppose cluster cluster standard objective function fuzzy cluster     yij sourcewhere cluster yij grade membership ith sample jth cluster cluster fuzziness cluster fuzziness centroid jth cluster issue cluster initial cluster etc propose novel fuzzy cluster introduce regularization  objective function minimize objective function   yij   yij sourcewhere function regularization minimize within cluster dispersion maximize negative entropy membership association propose novel fuzzy cluster objective function   yij  yij SourceRight click MathML additional feature membership matrix jth vector membership matrix vector cluster sparsity membership sample assign cluster regularization parameter sparseness matrix unsupervised discriminative projection feature selection model introduce propose unsupervised discriminative projection feature selection UDPFS feature selection aim discriminative feature subset feature feature selection matrix denote dimension data matrix feature inspire embed  sparse  unsupervised feature selection obtain discriminative feature subset sparse constraint impose membership matrix objective function achieve formula obtain discriminative feature   yij  yij WTW SourceRight click MathML additional feature jth cluster centroid project data impose orthogonal constraint projection matrix trivial zero avoids arbitrary sparse constraint membership matrix achieve discriminative feature denotes centroid jth cluster projection perform feature selection apply sparse constraint becomes   yij yti yij WTW source however norm constraint nonconvex non smooth NP therefore relax sparse regularization objective function   yij yti yij WTW SourceRight click MathML additional feature inspire norm regularization achieve sparse relax norm constraint   yij yti yij WTW SourceRight click MathML additional feature loss function regularization pseudo label data instruct discriminative feature projection perform feature selection calculation important feature sort feature sparsity sparse optimization algorithm due characteristic constraint norm regularization multivariate objective function alternative iterative optimization algorithm propose model obtain optimal fix update fix transform   yij yti SourceRight click MathML additional feature equivalent   yij  yti yij source similarity sample independent sub    yti yij sourcewhere transpose ith membership matrix dij  distance matrix simplify rewrite  diÎ± yti yij sourcewhere variable optimize optimization algorithm algorithm input data matrix scatter matrix regularization parameter output projection matrix initialization initialize identity matrix converge  eigenvectors correspond eigenvalue update diagonal matrix ith diagonal dii twi fix update fix becomes   yij SourceRight click MathML additional feature decompose independent sub   yij  objective function convex function utilize lagrange multiplier global optimum derivative derivative zero  yij source fix update fix rewrite minw  yij WTW SourceRight click MathML additional feature substitute simplify   WTW SourceRight click MathML additional feature yij source  yij denote vector jth cluster data scatter matrix characterizes discretization cluster data aggregation cluster data convenience computation export matrix operation later algorithm algorithm input data matrix projection dimension regularization parameter output projection matrix initialization initialize identity matrix converge fix update yij fix yij update calculate fix yij update algorithm sort feature accord calculate descend ranked feature replace    WTW SourceRight click MathML additional feature however zero non differentiable consequence avoid utilize twi instead regularize twi twi constant   twi WTW source obviously infinite approximation lagrangian function  twi WTW sourcewhere  multiplier derivative zero   WQ sourcewhere diagonal matrix ith diagonal dii twi source obviously  fix regard derivative    WTW SourceRight click MathML additional feature easy therefore optimize detail algorithm detailed algorithm summary obtain effective algorithm alternative iterate variable detail algorithm optimization goal obtain matrix obtain discriminate feature promote optimal iteration algorithm alternate optimization optimize variable promote obtain variable iteration therefore optimal guaranteed algorithm converges analysis discussion matrix waste obtain scatter matrix therefore matrix operation obtain lemma lemma diagonal matrix sourcewhere npi PT PT PT PT txt PT PT ptp XT SourceRight click MathML additional feature scatter matrix yij     SourceTherefore   yij     XT   XT  XT sourcewhere  matrix  yij yij respectively calculate instead reduce convergence analysis algorithm convergence algorithm algorithm objective function monotonically decrease lemma lemma non zero constant  sourcethe detail proof lemma convergence algorithm theorem theorem iteration algorithm monotonically decrease objective function convergence proof algorithm accord definition suppose update inequality  Î³tr    Î³tr   SourceRight click MathML additional feature  inequality instead definition inequality rewrite       wit  wit source lemma     wit  wit  wit sourcethen        wit SourceRight click MathML additional feature completes proof theorem objective function monotonically decrease iteration experimental analysis conduct evaluate demonstrate performance propose UDPFS public datasets validate important fuzzy implement feature selection synthetic dataset validate effectiveness propose UDPFS apply UDPFS pixel fashion mnist dataset popular unsupervised feature selection validate superiority propose evaluate performance propose cluster unsupervised feature selection parameter sensitivity convergence toy subsection adopt synthetic dataset feature selection ability propose UDPFS synthetic dataset randomly generate cluster data dimension data dimension distribute cluster data dimension dimension contains discriminative information important UDPFS dimensional feature UDPFS max variance  moreover important fuzziness feature selection membership matrix become label matrix feature selection obviously  UDPFS dimension feature addition propose UDPFS successfully dimension feature fuzziness account  UDPFS without fuzzy discriminative feature fuzzy feature selection fuzziness ignore suboptimal fuzzy feature selection important experimental cluster dataset dimension feature dataset feature selection maximize variance feature selection propose UDPFS fuzzy feature selection propose UDPFS fuzzy experimental cluster dataset dimension feature dataset feature selection maximize variance feature selection propose UDPFS fuzzy feature selection propose UDPFS fuzzy feature UDPFS fashion mnist sub dataset feature pixel feature fashion mnist sub dataset feature pixel baseline laplacian   UDPFS respectively qualitative qualitatively evaluate effectiveness UDPFS conduct feature pixel fashion mnist dataset dataset grayscale fashion image image image category convenience randomly image image per category image qualitative qualitatively evaluate feasibility propose rank image feature pixel accord importance calculate descend fashion category feature pixel feature pixel propose UDPFS category observation propose discriminative feature pixel increase pixel discriminative feature pixel increase relevant feature category therefore UDPFS obtain discriminative feature pixel qualitatively evaluate effectiveness propose UDPFS popular unsupervised feature selection laplacian   adopt randomly feature baseline laplacian   neighborhood regularization parameter qualitatively  demonstrate performance UDPFS obviously pixel increase almost discriminative feature UDPFS cannot achieve ideal UDPFS observation qualitatively conclusion effective performance feature selection category fashion mnist sub dataset ability discriminative feature feature discriminative projection project data dimensional representation fuzziness accurate quantitative verify superiority approach quantitative datasets conduct public datasets datasets coil coil handwritten digit image dataset  dataset isolet datasets ORL yale bioinformatics datasets glioma prostate GE  detail datasets summarize datasets description  validate superiority propose unsupervised feature selection brief introduction   feature adopts feature baseline cluster  maximum variance feature accord maximum variance variance feature feature important sensitive variance feature  laplacian feature accord locality preserve laplacian  seek feature respect local geometric structure  multi cluster feature selection feature accord multi cluster structure data preserve spectral regression norm regularization  unsupervised discriminative feature selection discriminative feature joint discriminative analysis norm minimization  nonnegative discriminative feature selection discriminative feature joint nonnegative spectral analysis norm minimization  structure optimal graph feature selection performs feature selection local structure simultaneously similarity matrix adaptively  dependence unsupervised feature selection enhances interdependence data cluster label feature parameter ensure adopt strategy parameter unsupervised feature selection       neighborhood datasets tune parameter grid strategy interval dataset optimal feature feature perform cluster feature evaluate performance unsupervised feature selection cluster optimal parameter cluster depends initialization report average kmeans cluster evaluation metric utilize widely evaluation metric cluster accuracy acc normalize mutual information nmi evaluate performance unsupervised feature selection cluster accuracy acc define acc sourcewhere label cluster label otherwise denotes mapping function kuhn munkres algorithm permutes cluster label label normalize mutual information nmi define nmi MI max SourceRight click MathML additional feature MI denotes mutual information metric denote entropy respectively acc nmi performance cluster analysis feature demonstrate average cluster accuracy acc average normalize mutual information nmi datasets respectively conclusion feature selection effective baseline feature selection feature selection improve acc nmi almost datasets reduce computation feature selection remove redundancy noisy feature remain discriminative feature UDPFS acc nmi accuracy dimension datasets percent baseline introduce fuzziness feature selection discriminative projection discriminative feature cluster acc unsupervised feature selection algorithm cluster nmi nmi unsupervised feature selection algorithm cluster nmi unsupervised feature selection algorithm generally feature increase performance feature selection demonstrates trend increase decrease datasets contains redundancy feature discriminative feature feature increase redundancy feature performance feature selection decrease acc nmi generally performance propose UDPFS exceed acc nmi UDPFS percent improvement  handwritten digit datasets coil coil mnist percent improvement datasets isolet ORL yale  datasets bioinformatics datasets datasets sample dimensional feature  datasets UDPFS percent improvement  propose UDPFS utilize cluster fuzziness adaptively discriminative projection project data dimensional obtain dimensional representation data cluster therefore propose UDPFS obtain parameter sensitivity convergence parameter sensitivity investigate impact parameter UDPFS projection dimension slightly impact therefore focus impact parameter parameter sparsity membership matrix sparsity projection matrix respectively regularization parameter feature tune cluster accuracy demonstrate respectively limit datasets coil ORL glioma prostate GE showcase propose UDPFS sensitivity parameter cluster accuracy coil ORL glioma prostate GE datasets cluster accuracy alpha gamma coil ORL glioma prostate GE datasets cluster accuracy coil ORL glioma prostate GE datasets convergence objective function propose algorithm iteratively already proven convergence convergence experimentally convergence curve objective  simplicity datasets coil ORL glioma prostate GE convergence algorithm within iteration convergence algorithm ensures efficiency propose algorithm UDPFS convergence curve UDPFS coil ORL glioma prostate GE datasets conclusion propose novel unsupervised feature selection perspective sparse introduce fuzziness sub discriminative projection feature selection data transform dimensional obtain dimensional representation sparse membership adaptively assign cluster pseudo indicator dimensional representation discriminative projection effective alternative iterative optimization algorithm propose address objective function analyze convergence propose algorithm theoretically various evaluate experimental datasets demonstrate effectiveness superiority propose unsupervised feature selection