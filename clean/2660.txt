geographical topic model geo tag document topical geographical topic application recommendation user mobility model detection etc exist focus effective geographical topic model ignore efficiency issue however expensive geographical topic model geographical topic model collection document token propose distribute PGeoTopic training geographical topic model propose comprises novel technical component increase parallelism reduce memory requirement reduce communication approach mining geographical topic model scalable model data distribute introduction prominence gps equip device amount document associate geo location coordinate increasingly generate web geo tag tweet webpage review geo textual document massive useful information topic data topic education distribute united difference  silicon topic addition mining geo textual data research recommendation detection  planning etc geo textual document geographical topic model propose concept model geographical topic topical intuitively geographical topic spatially coherent topic education topical nearby location topic distribution collection geo textual document geographical topic model geographical topic topical unsupervised manner topic model distribution contains component topic distribution model semantics gaussian distribution location model spatial information geographical topic model geo tag document geographical topic geographical topic illustrates location token document token document token document suppose topic respectively geographical topic model assigns topic label token return topical topic distribution mainly movie topic spatial information gaussian distribution topic described distribution movie topic distribution movie 3D topic distribution compute counting topic assignment token related movie related oval contour gaussian distribution although model quality geographical topic model extensively exist literature training efficiency important issue model data training moderate geographical topic model topic dataset token topic model billion parameter GB training model document machine cannot simply grid topic model independently grid due non trivial estimate geographical distribution topic across subareas latent topic subareas comparable improper data split topical boundary subareas subareas cannot easily merge afterward model subarea local optimal model globally optimize dataset geographical topic model data utilize powerful computer cluster recently distribute topic model propose however apply exist geographical topic model topic model geographical topic model matrix parameter billion entry topic distribution matrix topic distribution matrix parameter matrix highly couple collapse gibbs sample popular geographical topic model exist topic model topic distribution matrix lack efficient synchronization matrix therefore applicable geographical topic model propose PGeoTopic parallelize training geographical topic model multiple machine objective parallelism reduce memory requirement reduce communication however challenge achieve objective gibbs sample algorithm matrix parameter due interdependency parameter fetch server sample requirement parallelize sample computation communication incurs expensive requirement memory matrix distribute worker machine parameter server framework popular distribute machine architecture adopt reduce communication worker machine ideally achieve parameter localization worker mainly parameter local memory fetch parameter demand manner however achieve parameter localization fetch parameter demand PGeoTopic parameter server matrix parameter distribute worker memory address challenge PGeoTopic comprises novel technique parallelism reduce memory requirement propose training algorithm decouples interdependency topic matrix topic matrix independent access model parallelism propose training algorithm partition parameter matrix slice model parallelism parallelism computation network communication allows worker machine slice matrix propose adopt spatial partition data allocate worker spatially propose achieve parameter localization demand parameter fetch reduce communication summary contribution propose PGeoTopic distribute mining geographical topic model propose training model parallelism parallelism computation network communication reduce memory requirement worker machine propose technique parameter localization reduce network communication conduct extensive PGeoTopic efficient scalable distribute geographical topic model experimental outperforms baseline magnitude related introduce exist distribute training topic model geographical topic model topic exploration user specify query topic geographical topic model detect topically coherent data proposes  model introduces concept latent  latent consists dimensional gaussian distribution latitude longitude respectively multinomial distribution introduce latent topic assume topic distribution model advance  capture multiple topic propose non parametric approach automatically hierarchy latent correspond topic distribution recent apply geographical topic model social medium application poi recommendation detection  planning etc jointly model spatial user information improve poi recommendation accuracy improve local detection non parametric topic automatically propose recommend package geographical topic propose model predict coordinate tweet without geo tag user preference latent topic although exist effectiveness geographical topic model training efficiency model exist geographical topic model gibbs sample markov chain monte carlo MCMC propose improve efficiency MCMC machine hamiltonian monte carlo HMC efficient importance sample  however efficient MCMC assumption target distribution cannot directly apply geographical topic model HMC discrete variable geo topic model discrete continuous variable  assumes density target distribution integral posterior distribution topic integral applicability efficient MCMC geo topic model knowledge distribute training topic model related distribute training topic model latent dirichlet allocation model hierarchical distribute lda HD lda assigns data across processor processor performs gibbs sample data global synchronization parameter iteration apply data parallel strategy partition distribute document worker model topic distribution memory across machine parameter server data around model instead model transmit worker worker learns topic local data however model cannot memory model frequently swap worker memory expensive baseline lda partition data across multiple worker binary structure sample   adopt data parallel model parallel strategy  partition model worker model partition however extra document topic distribution synchronize document across worker  partition model slice  distributes document worker model slice schedule worker slice slice iteration worker model independently  improves  carefully memory access mechanism aforementioned technique suitable geographical topic model extend   topic matrix access worker model slice topic matrix topic matrix memory extend  training geographical topic model baseline knowledge exist considers training geographical topic model distribute preliminary introduce geographical topic model correspond training algorithm easy reference summarize notation summary notation summary notation geographical topic model geographical topic model latent topic define distribution latent define gaussian distribution covariance distribution topic geospatial document generate distribution dir latent index multi location document topic index multi multi graphical representation geographical topic model model variable besides topic handle model additional variable graphical representation geographical topic model graphical representation geographical topic model training geographical topic model geographical topic model collapse gibbs sample specifically topic sample ith token document probability topic assignment token document collection zdi rdi        dik   location  SourceRight click MathML additional feature  denotes frequency assign topic exclude assignment ith token document notation  denotes frequency assign topic exclude assignment token    prior probability prior probability topic prior probability topic respectively function denotes probability density gaussian distribution document exclude document token document sample assignment token randomly document training iteratively model converges matrix presentation topic matrix denote topic matrix denote refer parameter remain gibbs sampler access topic matrix compute topic topic matrix synchronize machine distribute local memory statement focus geo textual document geo tag tweet foursquare geo tag news geo tag web etc formally define geo textual document  geo textual document geo textual document tuple token latitude longitude geo location data model adopt parameter server recent distribute lda transfer model instead data via network training parameter server architecture memory machine worker memory memory regard server memory parameter server model parameter distribute memory machine parameter server formulate  distribute training geographical topic model geo textual document cluster machine task distribute training geographical topic model distribute machine model topic matrix topic matrix gaussian distribution data distribute worker parallel PGeoTopic overview propose technique propose finally summary training PGeoTopic overview knowledge exist distribute training geographical topic model straightforward data parallelism motivate objective PGeoTopic straightforward evenly distribute data randomly worker iteration worker fetch parameter parameter server local memory sample topic token document assign worker parameter cannot memory worker algorithm recently lru swap parameter memory disk iteration update parameter parameter server shortcoming fold computation network communication cannot conduct parallel training algorithm cannot update parameter fetch parameter server model memory frequently swap parameter memory disk incur expensive access synchronization parameter within iteration incurs network communication objective motivation address issue specifically propose objective parallelism computation network communication worker machine training parameter load reduce memory requirement subset parameter fetch via network memory training reduce communication instead access parameter iteration aim localize parameter worker access parameter worker machine fetch parameter via network objective propose distribute PGeoTopic PGeoTopic comprises novel technique distribute training algorithm model parallelism parameter localization mechanism propose distribute training algorithm sample topic token fetch topic vector token topic matrix topic matrix sample topic token interdependency parameter matrix incurs extremely network communication sample computation propose decouple interdependency matrix independently token topic matrix parameter sample topic topic matrix sample topic realize propose training algorithm metropolis hastings building metropolis hastings MH algorithm sample distribution sample sample proposal distribution sample accepted reject sample accepted likely sample distribution previous precisely sample accepted probability min previous sample density distribution proposal distribution respectively sample MH generate sample distribution distribution posterior aim derive proposal distribution independent access parameter matrix challenge proposal distribution usually proposal distribution sample converge likelihood within iteration meanwhile sample efficiency independent access matrix training effectiveness efficiency proposal uniform distribution uniform efficient access matrix fails improve likelihood extreme sample posterior  access matrix consume goal construct proposal PGeoTopic achieves effective efficient training independent access topic topic matrix observation address aforementioned challenge develop training algorithm observation observation topic location correlation sample topic topic distribution posterior gaussian mixture independent zdi rdi zdi rdi observation topic correlation sample topic probability proportional mkr sample data independent joint distribution zdi rdi observation proposal distribution preserve correlation sample training effectiveness efficiency random sample geo tag tweet PGeoTopic PGeoTopic topic training effectiveness efficiency random sample geo tag tweet PGeoTopic topic tackle aforementioned challenge propose proposal distribution namely independent proposal joint proposal respectively apply combine proposal preserve correlation generate sample independent proposal define independent proposal  zdi rdi   sourcewhere   proposal sample topic probability proportional   independently simplicity denote zdi rdi  denote  zdi rdi acceptance ratio sample drawn proposal compute min   SourceRight click MathML additional feature tth sample previous sample independent proposal   ensures topic correlation posterior preserve spatial proximity popularity posterior independent proposal access topic matrix parameter parameter communication mainly topic matrix joint proposal propose joint proposal capture topic correlation define  zdi rdi mkr  SourceRight click MathML additional feature acceptance ratio sample min   SourceRight click MathML additional feature joint proposal access topic matrix parameter topic correlate gaussian mixture propose model parallelism combine proposal combine proposal cyclic specifically apply independent proposal token switch joint proposal token cyclic proven converge equivalent probability distribution density multiplication density proposal multiplication proposal access topic matrix compute density zdi rdi acceptance ratio independent proposal exists joint proposal matrix independently sample delay computation acceptance ratio iteration  sequence sample zdi rdi proposal token sample drawn compute posterior distribution zdi rdi acceptance ratio access correspond parameter accept reject sample sample sequence token detail model parallelism propose training algorithm model parallelism parallelism training network communication reduce memory requirement worker machine focus model parallelism topic matrix topic matrix parameter gaussian parameter topic partition matrix slice worker machine fetch slice training worker sample slice prefetches slice parallel training computation communication illustrates propose model parallelism model parallelism model parallelism parallelism topic matrix training allocate independent joint grey proposal alternatively token data worker sample access topic matrix sample grey access topic matrix partition topic matrix slice rectangle topic matrix slice rectangle training document slice worker sample topic token document slice allocate independent proposal parallelism topic matrix topic matrix topic matrix access token joint proposal propose sample scheme sample token document slice sample topic token assign slice specifically joint proposal mixture model probability sample topic zdi zdi rdi rdi rdi sample scheme sample routine mixture model sample token sample topic accord probability topic proportional mkr  document slice sample token sample topic token assign slice fetch sample topic token assign training iteration worker fetch matrix slice slice parameter server slice slice update parameter server local slice remove memory addition sample parameter slice prefetch parameter slice hide network communication parameter localization reduce communication spatial partition technique ensure document worker spatially relate without loss generality partition data propose parameter localization demand fetch topic matrix parameter allocation related worker demand fetch topic matrix partition data accord spatial information document worker spatially apply sample scheme document likely sample gaussian distribution exponential decrement density distance document worker machine worker access relevant slice sample sample scheme without iterate slice benefit spatial partition demand fetch manner largely reduce access topic parameter remote parameter server algorithm training worker model converge foreach data worker foreach allocate proposal  alternatively rdi sample    drt       unique rdi slice fetch slice foreach rdi slice joint proposal allocate zdi sample topic           slice fetch slice foreach  slice independent proposal allocate zdi sample topic            foreach  allocate independent proposal    rdi  rdi foreach zdi rdi zdi rdi random      zdi rdi zdi rdi foreach random rdi parameter allocation related worker parameter server parameter memory distribute machine data parallelism sample scheme worker related correspond slice propose reduce communication topic matrix slice memory related worker access slice parameter server access local memory achieve initialize gaussian mixture parameter data machine initialize document worker depict oval spatially document worker memory worker worker memory access slice summary training algorithm sample worker machine data allocate worker split data memory data disk data allocate proposal independent joint token alternatively mention sample sequence topic metropolis hastings tth sample topic rdi  denote topic respectively specifically posterior gaussian mixture sample rdi correspond density   sample sample topic token fetch topic parameter slice slice sample topic token allocate joint proposal sample update density similarly fetch topic parameter slice slice sample topic token allocate independent proposal sample topic token allocate joint proposal update posterior density zdi rdi token allocate proposal slice compute posterior density token allocate joint proposal update posterior density token allocate independent proposal access topic parameter finally compute acceptance ratio accept reject sample token assignment token document empirical distribution  cdr assign document token directly sample distribution sample document randomly assignment rdi cdr assign sample equivalent sample empirical distribution terminate sample model converges increment likelihood percentage threshold percent algorithm worker sample data fetch parameter slice parameter server sample reduce training overlap network computation prefetch data parameter slice sample data specifically maintain sample thread thread model thread sample thread data thread parameter slice model thread thread reading data sample thread model thread retrieve parameter slice network extension PGeoTopic geographical topic model variant variable user suppose additional variable correlate topic gibbs sampler model additional variable zdi rdi zdi rdi SourceRight click MathML additional feature posterior predictive distribution exclude token apply variant minor modification variable related topic related extend independent proposal  zdi rdi  zdi rdi SourceRight click MathML additional feature parameter independent proposal related topic parameter related topic partition slice respectively parallelism variable  related topic extend joint proposal joint zdi rdi  zdi rdi  sourceand parallelize parameter related  topic matrix apply algorithm extend proposal experimental setup datasets conduct datasets namely twitter yelp web twitter dataset york user contains geo tag document yelp dataset contains user review   california arizona usa geo location location review web dataset crawl data contains web web geo tag randomly assign web poi dataset  location united web dataset evaluate training efficiency report statistic datasets statistic datasets statistic datasets distribute training propose PGeoTopic baseline baseline straightforward baseline extend baseline apply model parallelism  topic matrix PGeoTopic PGeoTopic without demand parameter fetch PGeoTopic PGeoTopic without allocate parameter related worker PGeoTopic PGeoTopic without overlap computation network communication PGeoTopic PGeoTopic distributes data worker machine randomly spatial partition topic specify hyper parameter normal wishart prior gaussian distribution avoid zero denominator update parameter experimental environment built exist parameter server implement propose source stale synchronize parameter server  setup amazon EC cluster machine computation GB ram gbps network default machine training specify objective evaluate scalability PGeoTopic model machine data evaluate computation versus network communication usefulness overlap computation network communication evaluate memory worker machine network communication evaluate model quality via application scalability PGeoTopic handle model topic baseline baseline PGeoTopic machine twitter dataset baseline baseline cannot iteration training within PGeoTopic training within PGeoTopic improves parallelism reduces network communication contrast baseline parallelism topic matrix incur network communication average throughput instead overall training remain scalability impractical baseline converge average throughput average token sample per roughly inversely proportional training converge iteration datasets baseline token sample compute average throughput PGeoTopic util converges scalability model fix topic evaluate scalability topic evaluate scalability fix topic report average throughput topic topic datasets PGeoTopic outperforms baseline magnitude throughput baseline degrades topic increase baseline PGeoTopic sensitive topic topic increase throughput PGeoTopic slightly degrades model overlap network communication computation scalability machine PGeoTopic baseline baseline machine respectively report average throughput baseline extremely throughput token per datasets propose roughly linearly growth machine twitter web yelp dataset performance PGeoTopic increase significantly machine increase yelp dataset worker machine computation overlap network communication overall efficient scalable machine PGeoTopic overlap computation network communication reduces network communication machine machine scalability data data randomly sample percent data evaluate scalability PGeoTopic data average throughput report PGeoTopic throughput dataset dataset worker machine computation model slice dataset spent sample computation largely overlap network communication contrast baseline display data data computation versus network PGeoTopic PGeoTopic twitter dataset report breakdown average training per iteration computation network communication topic training breakdown computation versus network communication per iteration training breakdown computation versus network communication per iteration PGeoTopic expensive PGeoTopic communication computation addition PGeoTopic average per iteration PGeoTopic average PGeoTopic overlap computation network communication experimental demonstrate benefit overlap computation network communication memory requirement worker machine evaluate memory requirement worker machine PGeoTopic machine report average memory usage machine iteration memory usage PGeoTopic gigabyte memory usage PGeoTopic gigabyte worker report amount local memory training model memory worker machine employ parameter server memory usage machine increase memory usage model parameter amortize machine worker memory usage remains stable irrespective machine PGeoTopic load model slice memory indicates PGeoTopic applicable machine limited memory network communication evaluate parameter localization PGeoTopic reduce communication specifically PGeoTopic variant PGeoTopic disables demand parameter fetch PGeoTopic assigns slice machine randomly without allocate related worker PGeoTopic randomly allocates data worker without spatial partition machine report average network communication topic parameter transmit via network iteration respectively average network communication sec per iteration average network communication sec per iteration average topic parameter GB transmit via network per iteration average topic parameter GB transmit via network per iteration PGeoTopic reduces network communication percent variant PGeoTopic PGeoTopic PGeoTopic communication PGeoTopic apply spatial partition demand parameter fetch properly PGeoTopic fetch parameter iteration performs demonstrates usefulness spatial partition demand fetch PGeoTopic communication variant benefitting demand parameter fetch PGeoTopic performs reduces network communication significantly PGeoTopic achieves parameter localization allocate parameter related worker model quality investigate model quality downstream application geographical topic model predict document predict location document without geo tag document prediction apply model predict document perplexity likely model predict document location prediction text tweet review input predict coordinate tweet review distance error kilometer evaluate model predict location baseline PGeoTopic model converge baseline cannot model datasets randomly sample percent tweet twitter dataset yelp dataset dataset datasets percent data remain percent data training model quality topic machine report perplexity location prediction error decrease increase datasets finer grain topical contains local semantics topic distribution accurate location information variance document prediction PGeoTopic percent perplexity baseline location prediction PGeoTopic increase error percent overall PGeoTopic achieves performance baseline task document prediction perplexity document prediction perplexity location prediction error location prediction error model quality machine topic machine report baseline PGeoTopic perform similarly worker machine datasets difference perplexity PGeoTopic machine around percent distance error within kilometer datasets PGeoTopic retains model quality increase worker machine effective computer cluster document prediction perplexity document prediction perplexity location prediction error summary combine previous scalability conclude PGeoTopic efficiently geographical topic model datasets preserve model quality latent topic informative twitter dataset ranked entropy topic distribution entropy mention topic topic randomly  liberty international airport plot topic representative probability topic topic twitter dataset oval contour confidence pin topic campus topic beach focus entertainment drinking addition airport model discover topical gate lounge topic lounge mainly flight delay gate arrival departure carrier information conclusion propose PGeoTopic distribute training geographical topic model PGeoTopic increase parallelism reduce memory requirement novel training algorithm model parallelism reduces network communication localize parameter related worker machine experimental demonstrate propose scalable machine data model outperforms baseline magnitude