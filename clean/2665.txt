multi hop reading comprehension focus factoid properly integrate multiple evidence correctly previous approximates global evidence local coreference information encode coreference chain dag style gru layer within gate attention reader however coreference limited information inference introduce global evidence complex graph DAGs perform evidence integration graph investigate recent graph neural network namely graph convolutional network gcn graph recurrent network GRN standard datasets richer global information approach highly competitive performance datasets without model elmo introduction recent witness task machine reading comprehension exist focus factoid scenario simply local information correctly precipitation QA refer passage meteorology precipitation  atmospheric vapor gravity gravity precipitation challenge practical extension multi hop reading comprehension  properly integrate multiple evidence correctly contains associate passage candidate choice correctly integrate hang   india irrelevant hang    bound pakistan iran task challenge  model distinguish relevant irrelevant WikiHop relevant entity mention  pronoun highlight WikiHop relevant entity mention  pronoun highlight practical task  increase research attention notable coref gru coreference information richer context candidate however disadvantage coref gru coreference considers usually local neglect useful global information addition DAGs usually sparse infer acyclic graph dag coreference coreference hang   india cannot ultimate hang india correctly instance dag generate graph dag generate graph propose graph scheme evidence integration allows information exchange beyond reference node arbitrary connectivity reference graph graph densely useful infer related entity mention unrelated mention  india relation mention entity across passage apart passage instance connects  across passage intuitively typed integrate global evidence related entity pronoun mention entity within context pas useful evidence across entity graph typed pas evidence hang india instance besides typed enhance relation local mention sequential encode baseline finally coreference typed connects pronoun correspond mention complimentary previous generate graph complex cycle directly apply dag network structure coref gru adopt graph neural network encode arbitrary graph graph convolutional network gcn graph recurrent network GRN successful encode semantic graph dependency graph raw text complex data structure instance passage candidate ner coreference resolution obtain entity mention graph mention relevant pronoun evidence integration execute graph adopt graph neural network sequential layer sequential layer learns local representation mention graph network learns global representation representation mention representation WikiHop graph highly useful   achieves highly competitive accuracy percent addition dramatically graph coreference DAGs graph positive relation graph connectivity accuracy   achieves publish knowledge investigate graph neural network reading comprehension task code available http github com  MHQA related multi hop multi hop important ability exist multi hop QA focus hop knowledge reduce deduction readily define structure relation contrast multi hop QA textual data introduce effective approach evidence integration graph structure textual input previous multi hop QA text reference structure addition evaluate model task limited vocabulary passage fundamentally model structure input evaluate model challenge task recent exploit structure input   computation node sub node composition operation sub generate combine composition operation predefine composition operation QA DAGs passage coreference DAGs encode dag recurrent network direction graph passage however relation coreference thorough evidence integration besides investigate recent graph neural network namely gcn GRN release propose graph model gcn multi hop addition graph neural network difference graph construction candidate graph node entity candidate distinguish inter intra relation later effort explicitly graph propose heterogeneous graph network separately model graph node enrich graph model feature POS ner tag directional attention module model multiple passage recent effort domain QA generate multiple passage instead passage however exist multi passage QA selects relevant passage reduce passage reading comprehension fundamentally truly leverage multiple passage multi passage QA approach merge evidence multiple passage combine evidence multiple passage fully utilize input passage difference approach focus context candidate passage aspect complex approach properly integrate related evidence candidate context entity mention increase difficulty context candidate evidence candidate usually integrate evidence demonstrate empirical comparison approach performance combine evidence entity mention baseline introduce baseline inspire baseline local standard BiLSTM layer dot input encode BiLSTM layer representation vector mention passage extract baseline coref lstm differs local replace BiLSTM layer dag lstm layer orange dot encode additional coreference information propose baseline upper dot dag lstm layer addition coreference link typical BiLSTM layer layer baseline upper dot dag lstm layer addition coreference link typical BiLSTM layer layer local BiLSTM encode relevant passage concatenate passage passage xpi embed adopts lstm encode passage iph lstm xpi lstm xpi SourceRight click MathML additional feature hidden contains information local context similarly convert embeddings  encode another BiLSTM  lstm  lstm  SourceRight click MathML additional feature coref lstm dag lstm encode conference knowledge passage embeddings  coreference information input dag lstm layer encodes input embed  gate operation         tanh SourceRight click MathML additional feature precede dag input output forget gate respectively model parameter representation extraction encode passage obtain representation vector entity mention span concatenate hidden correlate fully layer      SourceRight click MathML additional feature model parameter compress concatenate vector multi hop reading comprehension datasets focus situation entity similarly representation vector generate concatenate hidden SourceRight click MathML additional feature model parameter attention representation vector entity mention passage additive attention model adopt treat entity mention representation representation memory query respectively probability candidate input calculate sum occurrence across input passage   SourceRight click MathML additional feature  occurrence candidate occurrence candidate respectively previous sum probability occurrence entity mention important multi passage scenario attention entity mention calculate additive attention model    SourceRight click MathML additional feature exp nexp SourceRight click MathML additional feature model parameter occurrence entity comparison coref gru model gate attention reader GA reader cloze style reading comprehension task token input passage instance adapt model WikiHop benchmark candidate multiple token generate probability distribution passage token GA reader compute probability candidate aggregate probability passage token  candidate addition lstm instead gru difference baseline baseline candidate contains multiple token model effective datasets  candidate evidence integration graph neural network representation vector correspond entity mention evidence integration graph entity mention relevant mention integrate relevant information graph node entity mention graph recurrent network GRN graph convolutional network gcn overall procedure approach model framework model framework graph construction graph input passage entity mention within passage graph node automatically generate ner coreference annotator graph node entity mention pronoun entity graph ensure node situation occurrence entity mention across passage distance threshold passage entity mention coreference pronoun information automatically generate coreference resolution toolkit mention entity passage within threshold entity satisfy situation direction generate graph undirected graph evidence integration graph encode tackle multi hop reading comprehension infer global context merge related information investigate recent graph network GRN gcn graph recurrent network GRN GRN model graph perform recurrent information exchange graph node graph transition formally graph hidden vector entity mention graph sourcein integrate non local evidence node information exchange  node perform recurrent transition sequence graph  hyperparameter graph transition development initial initialize  SourceRight click MathML additional feature  correspond representation vector entity mention calculate equation representation model parameter gate recurrent neural network model transition transition consists hidden transition node information exchange conduct node via lstm operation           tanh  SourceRight click MathML additional feature  vector memory     input output forget gate respectively model parameter mkt sum neighborhood hidden node mkt SourceRight click MathML additional feature graph convolutional network gcn gcn convolution alternative GRN encode graph GRN encode gcn model consists initialization update initialization gcn adopts approach GRN initialize representation vector entity mention equation difference gcn GRN update node GRN adopts gate operation equation gcn linear transportation relu activation function  relu  sourcewhere mkt sum neighborhood hidden define equation model parameter combination evidence integration hidden graph encode representation additive attention mechanism introduce baseline entity baseline graph encode generate combine sum obtain overall     SourceRight click MathML additional feature  SourceRight click MathML additional feature baseline  graph encode   vat model parameter addition probability distribution calculate overall softmax equation finally probability belong entity mention merge obtain distribution equation training baseline model entropy loss logp SourceRight click MathML additional feature truth input model parameter respectively adam rate optimizer dropout rate normalization training WikiHop effectiveness graph encoders WikiHop dataset data dataset contains around instance training development instance consists associate passage candidate stanford CoreNLP obtain coreference ner annotation entity mention pronoun coreference candidate graph node evidence graph distance threshold typed respectively setting model behavior WikiHop  hyperparameters online evaluation holdout  embeddings initialize dimensional pretrained glove embeddings crawl update training model hyperparameters graph transition accord development node information coref typed hidden vector bidirectional lstm GRN layer development  performance model GRN gcn transition baseline performance transition performance model increase transition increase transition slight decrease performance execute transition introduce richly transition remain GRN slightly performance gcn increase transition however gap significant dev performance transition comparison exist GA gru GA coref gru correspond report former baseline gate attention reader latter propose WikiHop indicates model gigantic model architecture WikiHop dagger indicates model gigantic model architecture baseline local local encode passage BiLSTM layer BiLSTM respectively capture local information mention introduce local comparison model parameter local coref lstm another baseline encode passage coreference annotation dag lstm reimplementation framework coref GRN another baseline encodes coreference GRN contrast coreference DAGs evidence integration graph MHQA gcn MHQA GRN correspond evidence integration approach via graph encode adopt gcn GRN graph encode respectively local accuracy GA gru GA coref gru model compatible evaluate dataset GA gru GA coref gru calculate probability candidate sum probability token within candidate cannot handle  candidate overlap candidate york york candidate suffer issue reimplementation coref lstm gain local baseline GA gru MHQA gcn MHQA GRN accurate local respectively local baseline achieves decent without explicit multi hop operation mention recently easy instance infer entity candidate model largely improves performance tackle instance comparison pinpoint advantage approach MHQA GRN coref GRN coref GRN comparable performance coref lstm comparison evidence graph achieve improvement mainly evidence graph coreference DAGs suitable integrate relevant evidence local significantly local meaning simply introduce parameter addition introduce fully GRN demonstrate effectiveness evidence graph approach fully GRN creates fully graph entity mention encode GRN within fully graph directly however fully graph brute connection representative integrate related evidence MHQA GRN fully GRN directly distance fully GRN evidence graph related entity mention model easy integrate evidence barely learnable within fully graph analysis relation graph connectivity performance later recent leaderboard publish non ensemble model chain bert model  adopts model architecture 2GB gpus training contribution evidence integration approach orthogonal contribution bert model framework investigate future version analysis effectiveness ablation introduce evidence integration situation remove performance remove connectivity graph reduce infer remove typed performance information capture capture sequential encode however typed useful passing evidence node typed pas information hang india slightly important typed intuitively global information typed representation entity integrate context occurrence reference ablation GRN graph encoder ablation GRN graph encoder model performance none performance typed significantly local baseline whereas combination achieves accuracy local indicates importance evidence integration graph detailed quantitative analysis later generally demonstrate addition slightly coref likely coreference information capture sequential encode distance percentage distribution distance closest adopt coreference correspond graph distribution distance  distribution distance  adopt percent development instance distance percent within instance distance longer percent performance increase transition perform model advantage approach contrast distance distribution graph generate baseline approach evaluate approach subset development instance distance graph accuracy coref lstm MHQA GRN subset respectively performance  performance gap subset increase indicates approach handle relatively easy task however instance challenge approach  conduct newly release  version evaluate approach WikiHop complexity implicitly specify passage complexity dataset explicitly specify  author without involve author without  dataset web snippet instead passage WikiHop extract baseline    query web obtain relevant snippet model  obtains snippet sub snippet  model QA computation obtain sub integrate computation contrast approach creates graph snippet succeed evidence integration associate evidence observation WikiHop MHQA GRN MHQA gcn achieve improvement local MHQA GRN slightly accuracy baseline model web snippet MHQA GRN MHQA gcn structural relation entity mention  achieves percent improvement  local baseline comparable  graph model contribute percent improvement local indicates structural information passage important dataset  dataset  dataset analysis complex evidence multiple passage previous evidence occurrence entity passage correspond MHQA typed increase MHQA GRN increase WikiHop capture useful information multi hop QA task developed previous multi passage QA task likely integrates evidence entity related entity leaderboard report  additional sub annotation sub sub additional data training  approach relies annotation truth sub semantic par practically useful however additional upper bound  gap upper bound MHQA GRN prof improvement achieve introduce structural connection passage facilitate evidence integration conclusion introduce approach tackle multi hop reading comprehension  graph evidence integration passage related evidence reference passage graph adopt recent graph neural network encode graph perform evidence integration useful combine global evidence graph neural network effective encode complex graph approach highly competitive performance standard  datasets