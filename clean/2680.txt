hierarchical dirichlet HDP attract attention research community processing corpus HDP topic automatically possess important feature dubbed nonparametric overcomes challenge issue manually specify suitable topic parametric topic model latent dirichlet allocation lda nevertheless HDP computational lda parameter estimation advantage multi thread parallel gibbs sample algorithm propose estimate parameter HDP equivalence HDP gamma gamma poisson GPP generative unfortunately parallel gibbs sample algorithm apply finite approximation topic manually predefine topic retain nonparametric feature HDP another drawback model lack capture semantic dependency topic assignment independent although topic model exist limited enforce entire topic complex consume mining aim develop copula parallel gibbs sample algorithm HDP adjust topic dynamically capture latent semantic dependency compose coherent extensive datasets achieves perplexity topic coherence addition validate effectiveness model semantic dependency extract topical baseline introduction topic model aim discover latent topic embed textual data textual data document generate introduce topic layer particularly model regard topic discrete random variable assume topic distribute document document topic distribution distribute topic topic distribution purpose topic model distribution inference respectively semantic representation document tackle ambiguous issue instance latent dirichlet allocation lda widely parametric topic model lda assumes document topic topic distribution multinomial distribution prior distribution dirichlet distribution assumption benefit computation posterior probability dirichlet distribution conjugate prior multinomial distribution however lda leaf topic manually specify parameter usually intractable option address nonparametric bayesian model hierarchical dirichlet HDP propose HDP layer bayesian network layer dirichlet automatically adjust topic data layer multinomial distribution generate HDP massive computational resource parameter inference difficulty HDP apply data conduct HDP mostly parallelize parameter inference algorithm instance equivalence HDP gamma gamma poisson GPP generative cheng liu propose parallel gibbs sample algorithm HDP unfortunately algorithm predefined topic thread associate topic thread fix perspective loses nonparametric feature HDP topic data automatically frequently mention limitation lda HDP bag assumption assume independently generate semantic meaning isolated therefore model neglect latent topical dependency grammatically related compose topical essential various task mining topical phase facilitates reader grasp central meaning corpus researcher propose series topic model capture topical dependency topic model mainly stage namely mining stage topic model stage nevertheless preliminary exist enforce topic proven inappropriate recent recent complex consume mining stage consideration aim develop parallel gibbs sample algorithm nonparametric coherent topic discovery contribution summarize propose parallel nonparametric topic model dubbed topic adaptive model TSAM reserve nonparametric feature HDP automatically topic prune marginal topic assignment marginal topic refers topic assign topic unimportant specifically topic organize stack assign analogous restaurant chinese restaurant  mechanism sensitive topic capacity assign quantity unassigned developed balance remove topic extend TSAM TSAMcop apparatus copula model local topical dependency within relax independence assumption bag model recent proven copula useful variable dependency model instance  copula gaussian copula reduce bias estimate posterior variational inference model dependency latent variable topic refer autoregressive model recurrent neural network assume local topical dependency nest particularly model markov chain node corresponds topic hence topical dependency compose nest structure naturally model nest copula although local topical dependency model within global corpus dependency model topic correlate topic model former grain copula latter coarse grain logistic normal prior topic distribution evaluation validate effectiveness topic discovery perplexity normalise wise mutual information NPMI perplexity probability corpus generate NPMI widely evaluate topic coherence quality topic generate text classification document topic distribution input classifier TSAMcop evaluate recover topical topical topic model finally convergence various background convenience model introduce background hierarchical dirichlet cluster dataset parametric bayesian model assume priori cluster predetermine instance conventional topic model lda expert predetermine suitable topic nonparametric bayesian model data driven parameter cluster feature distribution data instead  manually perspective nonparametric bayesian model suitable complex data particularly latent variable intractable hierarchical dirichlet typical nonparametric bayesian model topic discovery data driven topic intuitive representation HDP chinese restaurant CRP indicates topic increase probability topic concentration parameter global dirichlet DP dth document  generative HDP illustrate DP DP      SourceRight click MathML additional feature mixture component topic topic distribution dth document global prior mixture component hence parameter illustrate  generate kth topic dth document component parameter prior distribution ensure mixture component lda corresponds dirichlet prior generates document topic distribution define  denotes parameter kth component parameter topic distribution  dirac function component associate mixture parameter kth topic similarly   kth topic dth document infinite component strictly satisfy  uncertainty compose various infinity mixture component enables HDP automatically topic data however assign topic HDP exist topic probability proportional already assign topic otherwise therefore HDP topic richer topic marginalize easily illustrate  construction HDP component likely compose HDP generate topic regardless exist topic redundant marginal topic  topic besides lda HDP treat independently  discover latent semantic information coherent text finally HDP expensive infer gibbs sample although enhance efficiency HDP variational inference limited non extendable issue serial compute gamma gamma poisson develop parallel inference algorithm HDP directly boost efficiency intractable HDP parallel model parameter dependent cheng liu propose parallel gibbs sample algorithm HDP equivalence generation HDP gamma gamma poisson GPP introduce generative gamma poisson gap     sourcewhere gap refers gamma  refers poisson GPP extend gamma poisson replace another random generate another dirichlet layer generative GPP gap gap     SourceRight click MathML additional feature HDP serf prior ensure mixture component generative explain HDP  unnormalized document topic distribution dth document prior   parameter kth topic dth document  poisson generates  ndk assign document generate ndk conditional probability  proof equivalence generative HDP GPP introduction copula copula useful model dependency structure random variable marginal distribution multiple variable copula model multivariate joint distribution topic model easily access document topic distribution regard marginal distribution topic copula information generate correspond joint distribution previous demonstrate effectiveness copula topic model topic adaptive model topic adaptive model TSAM parallel topic discovery model developed achieve goal prune marginal topic achieve prior assign topic uncovered topic generally adopt reversible MCMC basis parallelization propose topic adjustment automatically increase decrease topic without sacrifice model parallelization parallelization topic assignment although GPP equivalent HDP generative model behave differently unlike HDP parameter dependent GPP developed update parameter independently however GPP dth document assign kth topic ndk restrict ndk topic HDP constant document GPP ndk unknown ndk generate accord restriction ndk indicates dependency ndk explicit assumption independence topic straightforward  dependence ndk longer constant variable ndk enables model update ndk independently achieve flexible construct furthermore stack built initialize randomly sample document introduce assign topic grouped asynchronously update remove thread accept acceptance probability deletion exist thread randomly chooses assign another acceptance probability introduce acceptance probability accord buffer  restore abandon reject accepts remove abandon  exploit reversible MCMC update ndk adapts variant assign topic adjustment topic generative GPP assign imbalanced prior topic fix prior guaranteed balance iteration feature GPP reflect topic within corpus topic highly concerned document topic exist document topic seldom concerned naturally achieves prior contains assign marginal topic redundant unimportant remove however GPP lack prune mechanism marginal topic furthermore reversible MCMC without topic assignment likely assign topic GPP intractable model apply finite approximation topic avoid unlimited growth topic predefined fix algorithm retain nonparametric feature HDP advantage nonparametric bayesian model automatically adjust topic requisite TSAM multiple thread thread associate unique topic particularly kth thread maintains assign topic iteration remove thread iteration topic marginal topic thread associate marginal topic computational resource waste thread seldom update therefore marginal topic delete threshold identify marginal topic hyper parameter document DND balance workload avoid evenly dataset subset topic subset thread conduct topic assignment distinct eventually subset another distribution topic unstable within epoch threshold predetermine epoch delete marginal topic ensure delete topic persistent proportion instead temporary marginal prune marginal topic destruct associate thread modify local topic distribution concurrently guarantee update shortly destruction avoid memory leak already remove propose monitor executor mechanism hierarchical structure thread manage global topic information thread monitor responsible information executor thread associate topic reporting thread bridge monitor executor thread index marginal topic remain stack executor thread conduct topic assignment independently signal topic marginal reporting thread signal report thread monitor thread construct hierarchical structure enables modify topic adjust thread without thread conflict correspondingly epoch empty indicates assign topic reasonable assume belong topic uncovered topic therefore topic increase topic global distribution newly topic newly remove topic index topic proportion monitor executor mechanism perspective ensure topic information besides memory leak topic conservatively topic avoid increase workload algorithm topic adjustment monitor executor mechanism input stack topic threshold threshold identify marginal topic output adjust topic initialize global monitor thread topic thread asynchronously parallel calculate topic iteration update topic report tag delete update counting variable del cnt stack calculate report update counting variable cnt modifies accord del cnt cnt cnt del cnt initialize update thread return topic propose adjustment topic detailed algorithm inspire accept adjustment topic contributes decline perplexity classical evaluate performance topic model introduce later extend topic adaptive model copula model local topical dependency within copula extend TSAM motivation input TSAM organize sort array index render extra semantic structure preserve inappropriate initialize stack stochastically sample document rare context topic unlikely sample stack TSAM update topic assignment employ reversible MCMC exploit particularly acceptance probability derive likelihood respectively remove topic ndk min  ndk SourceRight click MathML additional feature ndk min  sourcewhere chosen remove normalize likelihood topic assignment respect topic ndk acceptance probability assign kth topic dth document frequently corpus sample stack  selection rare although frequent significant information rare usually context extra semantic meaning frequent vector machine rare vector machine frequent context topic model vector machine frequency within topic topical probability representative topical nonetheless combine vector machine context probably focus vector machine interpret vector machine component vector assign topic likely assign topic therefore guarantee belong stack achieve global vocabulary sample document sample component along copula exist lda HDP GPP TSAM bag model assume independently generate model topical dependency account scatter compose address suitable choice exploit apparatus copula explain introduce correspond simplify definition copula joint distribution function marginal distribution uniformly distribute bivariate assume appropriate joint cumulative distribution function cdf denote dependency random variable topic marginal CDFs respectively normally define analytical indirectly derive construct function model dependency accord equivalence  sourcewhere topic index  copula parameterized joint distribution copula parameter hence analytical construction copula derive particularly vector sample copula  derive  SourceRight click MathML additional feature equivalence identical equation replace SourceRight click MathML additional feature model topical dependency within sample copula sample jointly distribute dependent multivariate refer  theorem copula theorem dimensional joint cdf univariate marginal CDFs exists copula series random variable marginal probability accord proof corollary  theorem source vector sample copula marginal CDFs obtain desire variable sourcewhich compose sample joint distribution equation derive sample multiple variable joint distribution marginal CDFs carefully chosen copula corresponds bivariate described equation explains copula choice model local topical dependency within exist topic model enforces within inherit topic inappropriate contrast model permit topic topic joint probability copula setup model local topical dependency within equation  copula model joint distribution function  copula SourceRight click MathML additional feature particularly frank copula  copula  approach independent copula joint probability  corresponds independence assumption bag model lda joint probability becomes min variable completely positively correlate parameter model strength local topical dependency within mention local topical dependency assume nest autoregressive model regard document sequence strongly emphasize sequential reliance autoregressive model occurrence previous correspond markov chain scenario markov chain obtains markov chain involves previous model structure construct nest frank copula decompose multivariate bivariate copula recursively SourceRight click MathML additional feature sample described derive equation conditional probability ith assign remove kth topic dth document   ndk ndk   ndk ndk introduce nest frank copula probability rewrite ndk ndk SourceRight click MathML additional feature identity function define   otherwise source compute topic index ith parameter component nest frank copula sample nest frank copula subscript indicates ith conditional variable implicitly involve nest frank copula conditional probability remain  ndk ndk  ndk ndk topic index compute copula sample inverse cdf  quantile multinomial distribution identical copula auxiliary topic assignment indicator switch conditional probability copula topic adaptive model propose extend version TSAM namely TSAMcop discover coherent topic integrate copula TSAM stochastic initialization actually lose information inversely generate influence component semantically related likely assign simultaneously therefore model latent dependency assign copula involve overall framework TSAMcop inference procedure inside thread illustrate identical thread overall framework TSAMcop preprocessing stage stanford parser extract particularly grammar construct probabilistic context grammar obtain afterwards vocabulary dataset vocabulary index decides remove index outside vocabulary correspond index sample dimensional vector copula calculate important topic index instead probability transform copula sample vector topic assignment indicator ith however topic distribution dth document analyse mathematical definition GPP  normalize discrete distribution generate upper gamma index topic recall dirac function   associate  probability dth document  topic discrete assume independent independence topic assignment copula built admit cdf document topic distribution marginal cdf zij  sample copula dimensional vector particularly exactly cdf recover multinomial sample inverse cdf copula model local joint distribution topic within derive transform copula sample multinomial sample quantile quantile define probability density function exist satisfies SourceRight click MathML additional feature quantile unfortunately formula continuous variable handle sample transformation involve discrete distribution adapt quantile purpose revisit sample dimensional multinomial distribution inverse cdf axis probability distribution interval ith component uniform distribution admit cdf location axis associate component interval valid sample drawn distribution model sample topic index sample rewrite inf  sourcewhere inf denotes infimum satisfies aforementioned cdf jth topic topic index quantile multinomial distribution therefore copula sample transform sample multinomial distribution document topic distribution derive specific topic index satisfies    SourceRight click MathML additional feature inequation derive correspond topic index associate particularly compose discrete mixture component TSAMcop similarly derive    SourceRight click MathML additional feature index mixture component correspond specific topic recall  topic jth document transform topic assignment indicator ith simultaneously ith nth remove copula topic assignment infinite latent topic already characteristic latent topic GPP TSAM mixture component consecutively sample generate update update topic assignment dth document topic emerge unlike component unique corpus topic grows gradually GPP apply finite approximation constrain mixture component limit maximum topic prior knowledge another motivation propose topic adjustment maximum topic data driven variable although model infinite latent topic balance remove topic parameter setting maximum topic computational consideration alternative apply dynamic finite approximation topic construct reasonable document topic distribution  normalization kth component dth document normalize  upper bound topic topic limited mixture component preserve expand compress topic topic adjustment expand topic interpret reallocate topic proportion normalization compress average  component allocate remainder derive modification inequation    SourceRight click MathML additional feature infimum correspond inequation approximation allows global corpus topic infinite inequation topic index algorithm copula coherent topic assignment input corpus document topic distribution dth subset output modify reconstruct construct vocabulary vocabulary stack buffer index topic thread asynchronously parallel pop  accept  acceptance probability ndk accepts   correspond index  sample vector copula admit marginal distribution transform inequation    remove randomly  remove  acceptance probability ndk accepts remove   correspond index  infer     reserve  return discussion guarantee rationality propose copula coherent topic assignment algorithm iteration dth subset data remove TSAMcop illustrate  maintain topic thread within tag statistic matrix avoid duplicate counting assign topic scatter  data remove TSAMcop data remove TSAMcop complexity analysis drawback HDP complexity apply gibbs sample chinese restaurant equivalent representation HDP approximate complexity niter niter iteration document average document non decrease topic complexity increase increase topic moreover topic consecutive operation allocate proportion normalize topic matrix GPP parameter ndk  update parallel complexity GPP thread niter update global topic proportion update document topic proportion average document TSAM complexity GPP topic increase continuously complexity model accepts remove bunch marginal topic TSAMcop construction sample operation nest frank copula consume construction exponential rejection sample apply sample nest  copula propose  reduce complexity exponential logarithmic approximate complexity niter     denote average respectively multi thread alleviate extra computational hence TSAMcop suffer unaffordable loss efficiency experimental performance TSAM TSAMcop baseline model datasets perplexity topic coherence text classification topic interpretability convergence datasets validate effectiveness model topic discovery employ datasets NIPS contains accepted NIPS research contribution NIPS contains accepted NIPS reference exclude mining topical information corpus allows extract frequent terminology concerned apply newsgroup corpus officially training document document manually category category label demonstrate effectiveness model text classification  widely corpus contains article york dataset mainly evaluate performance parallel topic model article corpus sort array index assume consecutive identical index compose coherent analogous statistic datasets NIPS  randomly percent sample training validation respectively combination model parameter validation parameter perform sample parameter tune newsgroup perform statistic datasets statistic datasets baseline model quality evaluation topical document topic distribution learnt TSAM TSAMcop baseline comparison bayesian model HDP HDP classical nonparametric bayesian network statistic machine accord adopt gensim implementation classical HDP baseline model GPP GPP equivalent HDP generative parallel algorithm propose associate topic thread accelerate parameter inference GPP predefined topic thread topic cpu core cpu core core affect efficiency impact quantitative performance CopulaLDA CopulaLDA extend lda model utilized copula model topical dependency coherent text span source implementation author github baseline model developed neural network ntm ntm multi layer neural network aim document topic topic gram distribution generate meaningful representation document employ supervise extension  evaluation text classification GSM GSM neural topic model gaussian softmax extend  framework variational auto encoder GSM explicitly model document topic distribution gaussian reduce ambiguity topic vector    topic memory network text classification encode latent topic representation via memory network utilized topic vector pre embeddings compute document representation proven effective document classification unless otherwise specify topic parametric topic model performance perplexity perplexity probability corpus generate perplexity exp ndi  sourcewhere document dth document  probability ith dth document perplexity closer infer distribution perplexity topic topic model NIPS newsgroup  perplexity average topic dataset infer automatically nonparametric topic model subsampling dataset GPP model indicates TSAMcop CopulaLDA achieve TSAM baseline model validates effectiveness copula model boost generative probability corpus perplexity topic topic model dataset highlight boldface topic HDP global proportion perplexity topic topic model dataset highlight boldface topic HDP global proportion performance topic coherence NPMI evaluate model normalise pointwise mutual information NPMI widely topic coherence metric employ evaluation neural network model GSM model generates coherent topic achieve NPMI NPMI model compute average topic experimental indicates TSAMcop outperforms TSAM baseline topic coherence indicates copula model generates coherent topic NPMI topic model dataset highlight boldface report underlined topic evaluation text classification evaluate effectiveness topic discover topic model downstream task employ newsgroup text classification particularly normalize document topic distribution generate topic model feature representation document input logistic regression classifier penalty widely precision recall adopt evaluation metric validate generate document topic distribution bayesian baseline model text classification   perform model pre embeddings document label exploit baseline enhance classification performance however issue  generate interpretable topic distribution probabilistic characteristic  learns topic vector neural topic model GSM neglect latent topical dependency grammatically related limit performance  topic discovery already besides although rnns encoder decoder model sequential dependency posterior collapse combine variational auto encoder model GSM text classification performance newsgroup highlight boldface text classification performance newsgroup highlight boldface topical evaluation although TSAMcop achieve topic coherence validate interpretability topic representative topic topical learnt aforementioned baseline NIPS dataset label align topic expert accord benefiting topical dependency model representative topic TSAMcop CopulaLDA generally interpretable others focus specific content instead frequently mention terminology topical TSAMcop  ML NN DM TM CV denote machine neural network data mining training computer vision respectively topical TSAMcop NIPS NIPS ML NN DM TM CV denote machine neural network data mining training computer vision respectively topical TSAM baseline model  ML NN DM TM CV denote machine neural network data mining training computer vision respectively topical TSAM baseline model NIPS NIPS ML NN DM TM CV denote machine neural network data mining training computer vision respectively topical evaluation demonstrate TSAMcop generate coherent topic topical recover topical generate TSAMcop exist topic model reassemble although copula topic assignment TSAMcop output topic series therefore recover topical topical evaluate coherence reliance instead assemble simply recover meaningful previous evaluate frequency  completeness terminology  describes hierarchical structure adjacent frequently merge longer knowledge merge longer knowledge frequently completeness guarantee model partial convolutional neural network coherent precise convolutional network neural network model reassemble topical listing combination topical filter quality  rank remainder frequency approach  topical generation framework criterion filter rank candidate topical however  performs frequent mining topic postprocessing collocation related topic cannot aggregate meaningful TSAMcop drawback overcame copula topic assignment baseline model evaluate quality generate topical topic model baseline   regard document bag undirected clique model correlation unlike model  gram enforce identical topic assignment tpm   tpm topic model specialize clinical application extract clinical via  tpm efficient perform topic model clinical corpus generate quality topical consistency stanford parser apply extract tpm   stage generate correlate topic extend CTM however focus stage  link component within markov random NPMI topic component boston globe component update variational parameter markov random variational inference stage  generate correlation topic focus   iterative mining framework efficient  propose utilized topic model evaluate topical frequency ranked propose criterion namely coverage purity  completeness obtain quality discover topical representation textual data topic model label lda manually document accord frequency multi label corpus perform topic model incomparable unsupervised model   incremental HDP model discover evolutionary trend topic sentiment online review baseline objective task analysis manually align topical model NIPS  tends topic topical topic machine algorithm due basis frequent mining model detect topic comprise diverse focus specific domain   extract coherent meaningful topical achieve  although topical CopulaLDA TSAMcop model application copula particularly CopulaLDA suffers extremely acceptance probability gibbs sample joint probability derive copula serf multiplier model TSAMcop copula model dependence unlike traditional gibbs sample thread stack accord acceptance probability reversible MCMC technique aforementioned sample avoid probability dramatically shrink acceptance probability factor boost quality topical topical model  ML NN DM TM CV denote machine neural network data mining training computer vision respectively topical model NIPS NIPS ML NN DM TM CV denote machine neural network data mining training computer vision respectively evaluate performance model quantitatively calculate NPMI discover metric evaluation  comparison english wikipedia corpus reference newsgroup besides training reference NIPS NPMI topic model NIPS newsgroup model achieve perform baseline  furthermore baseline rely preprocessing document delicate manually   utilize exist manually construct vocabulary tpm postprocessing via reassemble recover topical CopulaLDA combine preprocessing postprocessing automatically reference semantic relation recover semantically related infer topic NPMI topic model dataset highlight boldface report underlined NPMI topic model dataset highlight boldface report underlined impact parameter evaluate impact model parameter performance NIPS dataset parameter dilemma topic prune marginal topic prune marginal topic topical buffer later stack topic generate iteration  topic topic iteration immediately delete unless hence topic prune marginal relax delete topic performance model setting model perform model performance decrease increase model delete marginal topic model performance acceptable temporary marginal topic important delete performance setting performance setting epsilon lambda parameter influence TSAMcop mention earlier strength dependency copula sample correlation inappropriate independence assumption performance decay decrease apply independent copula model degenerate TSAM performance setting performance setting lambda epsilon comparison convergence convergence model  dataset convergence bayesian model convergence bayesian model GPP TSAM TSAMcop intel xeon CPUs core thread ghz GPP model core core respectively occupy thread report although TSAMcop extra copula sample convergence competitive TSAM GPP baseline HDP CopulaLDA convergence neural network model gpu geforce gtx gpu contains core minor core memory rate ghz unfortunately model convergence  dataset neural network maintain update batch wise propagation limited gpu memory consume instance batch average epoch GSM batch related latent dirichlet allocation lda conventional topic model regard document distribution topic generate topic distribution lda assume document topic topic distribution multinomial distribution employ dirichlet distribution prior advantage conjugate prior collapse gibbs sample conveniently parameter inference widely drawback lda topic particularly manually specify topic lack interpretability unsuitable data hierarchical dirichlet HDP nonparametric approach discover topical information unlike finite mixture model gaussian mixture model topic HDP automatically infer data however HDP consume lda hence focus parallel inference algorithm HDP preliminary propose gamma negative binomial gamma NB gamma NB completely random augment gamma gamma poisson construction normalization HDP representation gamma NB gamma gamma poisson GPP gibbs sample algorithm HDP parallelize equivalence HDP GPP generative however model limited lack capture semantic dependency topic model beyond bag assumption bigram topic model btm generate distribution context define latent topic previous btm extend switch variable introduce identify previous define gram gram within gram definitely latent topic postprocessing topic topic entire gram propose discover topic model  hierarchical    prior topic matrix model topic distribution instead gram coherent meaningful propose  associate frequent mining lda discover topical nonetheless  enforce component latent topic regard drawback overcame topical model tpm propose tpm utilized shelf medical concept mention extraction extract quality clinical apply hierarchy    prior explore possibility model topical dependency integrate copula lda discover topical dependency text span although CopulaLDA learns interpretable topic suffers topic recently propose stage correlate topic model model link component within markov random thereafter generate correlation topic recent research topic model enhance efficiency mining propose  conclusion parallel gibbs sample algorithm topic discovery automatically topic addition generate coherent interpretable topic exploit latent dependency via copula topic adjustment focus model emergence decay topic propose topic adjustment automatically generate topic delete marginal topic data latent semantic dependency exploitation nest frank copula model local topical dependency within demonstrate model achieve interpretability efficiency baseline future apply model data document social medium platform continuously