successful domain acoustic image processing however apply ubiquitous graph data non trivial unique characteristic graph recently substantial research effort devote apply graph beneficial advance graph analysis technique survey comprehensively review graph exist category model architecture training strategy graph recurrent neural network graph convolutional network graph autoencoders graph reinforcement graph adversarial comprehensive overview systematic manner mainly development analyze difference composition finally briefly outline application discus potential future research direction introduction decade become  jewel artificial intelligence machine superior performance acoustic image processing etc expressive extract complex underlie data recognize graph ubiquitous relationship varied domain social network commerce network biology network traffic network graph complicate structure underlie utilize analyze graph data attract considerable research attention non trivial challenge exist apply traditional architecture graph irregular structure graph unlike image audio text grid structure graph irregular structure generalize mathematical operation graph define convolution pool operation fundamental operation convolutional neural network cnns graph data straightforward refer geometric heterogeneity diversity graph graph complicate diverse graph heterogeneous homogenous unweighted unsigned addition task graph widely node focus node classification link prediction graph focus graph classification graph generation diverse task model architecture tackle specific graph data era graph easily billion node social network commerce network therefore scalable model preferably model linear complexity respect graph incorporate interdisciplinary knowledge graph discipline biology chemistry social interdisciplinary opportunity challenge domain knowledge leveraged specific integrate domain knowledge complicate model generate molecular graph objective function chemical constraint non differentiable therefore gradient training cannot easily apply tackle challenge tremendous effort literature related adopt architecture training strategy greatly supervise unsupervised convolutional recursive however knowledge effort systematically summarize difference connection diverse knowledge gap comprehensively review graph specifically exist category model architecture training strategy graph recurrent neural network graph rnns graph convolutional network GCNs graph autoencoders GAEs graph reinforcement graph RL graph adversarial summarize characteristic category distinction graph rnns capture recursive sequential graph model node graph GCNs define convolution readout operation irregular graph structure capture local global structural GAEs assume rank graph structure adopt unsupervised node representation graph RL defines graph action reward obtain feedback graph task constraint graph adversarial adopt adversarial training technique enhance generalization ability graph model robustness adversarial attack distinction graph categorization graph exist category graph recurrent neural network graph convolutional network graph autoencoders graph reinforcement graph adversarial comprehensive detailed overview mainly development various challenge graph analyze difference model delve composite architecture finally briefly outline application model introduce library discus potential future research direction appendix source code repository analyze complexity various summarize application related previous survey related summarize gcn cnns manifold comprehensively geometric summarize GNNs GCNs relational unified framework graph network review attention model graph summarize GCNs briefly survey adversarial attack graph differs previous systematically comprehensively review architecture graph focus specific concurrent survey viewpoint categorization specifically neither graph reinforcement graph adversarial another closely related topic network embed aim embed node dimensional vector distinction network embed focus model apply graph network embed recognize concrete application model non organize introduce notation preliminary review graph rnns GCNs GAEs graph RL graph adversarial respectively conclude discussion notation preliminary notation graph node node RN denote adjacency matrix ith jth denote respectively graph undirected unweighted mainly unsigned graph therefore graph future research direction FV FE denote feature node respectively variable bold uppercase denote matrix bold lowercase denote vector matrix vector transpose matrix denote XT wise multiplication denote function marked  illustrate notation social network node corresponds user correspond relation user profile user gender location node feature FV interaction data message comment feature FE preliminary laplacian matrix undirected graph define RN diagonal matrix eigendecomposition denote  RN diagonal matrix eigenvalue sort ascend RN correspond eigenvectors transition matrix define probability random node node node define shortest distance node node reachable node within simplify notation omit subscript immediate neighborhood model superscript denote layer denote dimensionality layer RN sigmoid activation function define rectify linear relu define relu max wise nonlinear activation function denote unless otherwise assume function differentiable model parameter propagation commonly adopt optimizers adam training technique dropout denote sample sample technique adopt summarize notation commonly notation commonly notation task model graph broadly category node focus task task associate individual node graph node classification link prediction node recommendation graph focus task task associate entire graph graph classification estimate various graph generate graph distinction conceptually mathematically rigorous exist task associate  structure community detection addition node focus sometimes graph focus transform former egocentric network nevertheless explain difference algorithm category  recurrent neural network recurrent neural network rnns gate recurrent gru memory lstm facto standard model sequential data review graph rnns capture recursive sequential graph graph rnns broadly category node rnns graph rnns distinction node model node graph model graph characteristic survey summarize characteristic graph recurrent neural network graph rnns node rnns node rnns graph refer graph neural network GNNs date pre era GNN encode graph structural information node dimensional vector motivate recursive neural network recursive definition adopt fvi  fei SourceRight click MathML additional feature parametric function obtain another function apply output fvi sourcefor graph focus task author node unique attribute entire graph model parameter semi supervise adopt iteratively stable jacobi gradient descent perform   algorithm minimize task specific objective function loss predict truth regression task convergence equation GNN important role retrospect GNN unifies processing graph data recursive neural network markov chain future underlie GNNs profound inspiration later GCNs actually formulation framework exchange information within immediate node neighborhood GNNs GCNs unified framework GNN equivalent gcn identical layer stable discussion although conceptually important GNNs drawback ensure unique contraction SourceRight click MathML additional feature intuitively contraction distance contract operation severely limit model ability iteration stable gradient descend GNNs computationally expensive drawback lack computational graphic processing gpu widely lack research GNNs become focus research notable improvement GNNs gate graph sequence neural network  nns modification importantly author replace recursive definition gru remove contraction requirement optimization technique specifically adapt SourceRight click MathML additional feature calculate update gate candidate update pseudo author propose network operating sequence sequence output apply sequence task program verification SSE approach however instead gru calculation SSE adopt stochastic fix gradient descent accelerate training scheme basically alternate calculate steady node local neighborhood optimize model parameter calculation stochastic mini batch graph rnns subsection review apply rnns capture graph temporal dynamic graph sequential graph granularity graph rnns instead apply rnn node node rnn apply entire graph encode graph apply graph rnns graph generation specifically adopt rnns generate node generate newly node autoregressive manner hierarchical rnn architecture effectively input graph traditional graph generative model reasonable complexity capture temporal information dynamic graph dynamic graph neural network  propose aware lstm node representation establish  lstm update representation interact node immediate propagation author aware lstm model establish interval formation benefit graph application graph rnn combine architecture GCNs GAEs aim tackle graph sparsity  apply lstm GCNs progressively reconstruct graph illustrate lstm information graph diffuse across without gcn layer dynamic gcn apply lstm GCNs slice dynamic network capture spatial temporal graph information framework  reprint permission  lstm gcn progressively reconstruct graph  estimate matrix output GCNs incremental update rnn iteration respectively  refers multigraph cnn  convolutional network graph convolutional network GCNs  hottest topic graph mimic cnns GCNs local global structural graph convolution readout function GCNs task specific loss via backpropagation exception unsupervised training focus adopt architecture discus convolution operation readout operation improvement summarize characteristic GCNs survey comparison graph convolutional network GCNs comparison graph convolutional network GCNs convolution operation graph convolution spectral convolution perform convolution transform node representation spectral domain graph fourier transform extension spatial convolution perform convolution node neighborhood overlap polynomial spectral kernel refer detail spectral convolution fundamental operation cnns however standard convolution operation image text cannot directly apply graph graph lack grid structure introduce convolution graph data spectral domain graph laplacian matrix role fourier basis signal processing graph convolution operation define   SourceRight click MathML additional feature RN signal define node eigenvectors briefly QT transforms graph signal spectral domain graph fourier transform performs inverse transform validity definition convolution theorem fourier transform convolution operation wise fourier transforms signal filter  SourceRight click MathML additional feature output signal RN diagonal matrix learnable filter eigenvalue convolutional layer define apply filter input output signal   SourceRight click MathML additional feature layer  RN jth hidden representation signal node lth layer  learnable filter conventional convolution input signal learnable filter aggregate information nonlinear transformation node feature FV input layer stack multiple convolutional layer overall architecture cnn theoretical analysis definition graph convolution operation mimic geometric cnns refer reader comprehensive survey however directly parameter feasible besides filter spectral domain localize spatial domain node affected node node alleviate smooth filter diag   sourcewhere fix interpolation kernel learnable interpolation coefficient author generalize graph construct raw feature supervise unsupervised however fundamental remain unsolved eigenvectors laplacian matrix calculation complexity backward pas mention complexity calculate eigendecomposition meaning approach scalable graph filter  graph parameter cannot across multiple graph structure review limitation unify framework efficiency aspect efficiency ChebNet propose polynomial filter  SourceRight click MathML additional feature learnable parameter polynomial instead perform eigendecomposition author  chebyshev expansion  sourcewhere λmax rescale eigenvalue λmax maximum eigenvalue RN identity matrix chebyshev polynomial rescale orthonormal basis chebyshev polynomial polynomial laplacian matrix polynomial eigenvalue  filter operation rewrite       sourcewhere λmax recurrence relation chebyshev polynomial xtk calculate recursively sourcewith matrix multiplication sparse matrix vector calculate complexity becomes KM sparse matrix multiplication polynomial complexity linear respect easy polynomial filter strictly localize convolution representation node affected neighborhood NK interestingly independently network embed preserve proximity omit detail brevity kipf simplify filter 1D  SourceRight click MathML additional feature hli rfl hidden representation node lth layer equivalently matrix  SourceRight click MathML additional feature connection author minor author argue stack adequate layer illustrate model capacity ChebNet illustrative spatial convolution operation propose kipf reprint permission node affected immediate convolutional layer illustrative spatial convolution operation propose kipf reprint permission node affected immediate convolutional layer important insight ChebNet extension spectral graph convolution spatial architecture specifically spectral convolution function polynomial spectral graph convolution equivalent spatial convolution addition convolution highly definition GNN convolution definition replaces recursive definition aspect GNN regard gcn identical layer stable GNN fix function fix parameter iteratively update node hidden equilibrium gcn preset layer layer contains parameter spectral propose efficiency instead chebyshev expansion  adopt cayley polynomial define graph convolution kθk   SourceRight click MathML additional feature denotes imaginary another spectral zoom parameter addition  efficient ChebNet author demonstrate cayley polynomial detect narrow frequency importance achieve graph wavelet neural network  propose replace fourier transform spectral filter graph wavelet transform rewrite SourceRight click MathML additional feature denotes graph wavelet approximate algorithm calculate  computational complexity KM linear respect aspect multiple graph parallel series focus generalize graph convolution multiple graph arbitrary neural fps propose spatial   parameter across graph independent graph neural fps handle multiple graph arbitrary however instead influence node normalization neural fps propose parameter node strategy perform graph molecular graph node bond scalable graph patchy san adopt assign unique node graph label procedure weisfeiler lehman kernel node pre define addition patchy san define receptive node fix node neighborhood standard cnn normalization adopt approach node graph receptive fix patchy san multiple graph normal cnns multiple image drawback convolution depends heavily graph label procedure preprocessing  propose simplify sort lexicographical sort hidden representation layer HL instead author sort channel HL separately  approach sort node author propose sort node neighborhood despite difference enforce node choice graph dcnn adopt another approach replace  graph convolution diffusion basis neighborhood node diffusion transition probability node specifically convolution define  sourcewhere PK transition probability diffusion random preset diffusion learnable parameter PK depends graph structure parameter across graph arbitrary however calculate PK complexity NK scalable graph  propose jointly adopt diffusion adjacency dual graph convolutional network specifically  convolution replace adjacency matrix positive pointwise mutual information  matrix transition probability   SourceRight click MathML additional feature XP  matrix calculate XP max SourceRight click MathML additional feature DP  diagonal matrix XP convolution ensembled minimize difference  adopt random sample technique accelerate transition probability calculation demonstrate dual convolution effective graph framework  propose unified framework graph convolution operation spatial domain message passing function hli  fei hli sourcewhere message function vertex update function respectively denotes message node conceptually  framework node sends message update message immediate author framework exist  nns neural fps kipf addition author propose node node accelerate message passing across distance split hidden representation tower improve generalization ability author specific variant  achieve performance predict molecular concurrently graphsage multiple aggregate function   hli sourcewhere concatenation operation aggregate aggregate function author aggregate function wise lstm max pool  max   SourceRight click MathML additional feature   parameter max wise maximum lstm aggregate function author adopt random mixture model network monet unify exist gcn model cnns manifold framework template   SourceRight click MathML additional feature pseudo coordinate node  parametric function  kth dimension hli  kernel combine neighborhood monet adopt gaussian kernel  exp    sourcewhere   vector diagonal covariance matrix respectively pseudo coordinate kipf 1D 1D source graph network  propose framework GCNs GNNs representation hli  representation node entire graph respectively representation aggregation update function  GE   GV hli MLE GE  FV  hli FE  hli  FG MLE  SourceRight click MathML additional feature FV FE FG correspond update function node entire graph respectively message passing function superscript denote message passing direction message passing function input argument variable function invariant input permutation wise summation maximum   introduce representation representation entire graph framework summary convolution operation evolve spectral domain spatial domain  immediate currently gathering information immediate framework choice graph convolution operation readout operation graph convolution operation useful node feature node focus task however tackle graph focus task node information aggregate graph representation literature procedure usually readout operation regular local neighborhood standard cnns conduct multiple stride convolution  gradually reduce resolution graph lack grid structure exist cannot directly invariance critical requirement graph readout operation operation invariant node index node bijective function node representation entire graph drug treat disease depends inherent structure identical drug node index related graph isomorphism algorithm  function invariant vice versa polynomial structurally graph representation statistic invariant operation involve statistic summation average max pool   max hli SourceRight click MathML additional feature representation graph hli representation node layer however statistic sufficiently representative distinguish graph distribution node representation fuzzy histogram fuzzy histogram construct histogram bin calculate membership hli bin regard node representation sample pre define template finally return concatenation histogram node sum average maximum distribution distinguish another commonly approach aggregate node representation fully FC layer layer HL  sourcewhere HL  concatenation node representation HL    parameter  dimensionality output regard sum node feature advantage model node however ability unable guarantee invariance hierarchical cluster dichotomy node graph structure graph exhibit hierarchical structure explore hierarchical cluster density agglomerative cluster multi resolution spectral cluster ChebNet monet adopt another greedy hierarchical cluster algorithm  merge node along pool rearrange node balance binary ecc adopt another hierarchical cluster perform eigendecomposition however hierarchical cluster independent graph convolution perform preprocessing fashion perform hierarchical cluster algorithm reprint permission  propose differentiable hierarchical cluster algorithm jointly graph convolution specifically author propose cluster assignment matrix layer hidden representation SourceRight click MathML additional feature rnl cluster assignment matrix cluster layer function node representation adjacency matrix coarsen graph obtain average accord TH  sourcewhere obtain apply graph convolution layer coarsen graph node node layer convolution operation initial node layer NL node entire graph cluster assignment operation connection cluster sparse complexity principle impose others mention patchy san  impose node resort standard pool cnns preserve invariance depends impose another research refer reader survey however impose node choice graph node constitute research topic addition aforementioned heuristic GNNs author node node entire graph similarly  propose directly representation entire graph message node  adopt  modification seqseq model specifically  model receives input simultaneously computes internal memory attention mechanism lstm writes output unlike seqseq sensitive  invariant input summary statistic average summation readout operation hierarchical cluster algorithm jointly graph convolution advanced sophisticated pseudo node impose node investigate improvement discussion technique introduce improve GCNs apply model graph attention mechanism aforementioned GCNs node neighborhood aggregate pre define however influence greatly training predetermine inspire attention mechanism graph attention network gat introduces attention mechanism GCNs modify convolution operation  sourcewhere  node attention node lth layer  exp LeakyReLU   exp LeakyReLU   sourcewhere another function multi layer perceptron mlp improve model capacity stability author multiple independent attention concatenate multi attention mechanism illustrate  propose apply traffic forecasting illustration multi attention mechanism propose gat reprint permission denotes independent attention vector illustration multi attention mechanism propose gat reprint permission denotes independent attention vector han propose attention mechanism node semantic attention mechanism heterogeneous graph specifically node attention mechanism gat  node therefore assign aggregate meta semantic attention importance meta output residual jumping connection research suitable depth exist GCNs limited layer potentially due practical difficulty involve training GCNs smooth node deeper layer representation remedy residual connection resnet GCNs kipf residual connection  SourceRight click MathML additional feature experimentally residual connection depth network increase resnet network  adopt residual connection learnable   hli sourcewhere calculate  calculate     sourcewhere   parameter gru  nns difference  superscript denote layer layer parameter  nns superscript denotes pseudo parameter across inspire personalize pagerank  define graph convolution teleportation initial layer SourceRight click MathML additional feature FV hyper parameter parameter graph convolution jumping knowledge network JK net propose another architecture layer network hidden layer jumping representation output illustrate model selectively exploit information layer formally JK net formulate  aggregate hli sourcewhere  representation node aggregate aggregate function hidden layer JK net aggregate function graphsage concatenation max pool lstm attention experimental connection improve performance multiple GCNs jumping knowledge network propose layer layer selectively exploit information layer GC denotes graph convolution reprint permission feature aforementioned GCNs mostly focus utilize node feature graph structure subsection briefly discus another important source information feature feature discrete straightforward parameter aggregate neural fps parameter node corresponds implicit feature bond molecular graph sum  parameter heterogeneous graph average convolution ecc parameter apply graph classification relational GCNs GCNs adopt knowledge graph training relation however suitable limited discrete feature dcnn propose another convert node node conversion feature treat node feature  construct graph RM incorporate feature otherwise sourcein node graph graph node graph information correspond graph  adopt GCNs graph graph propose architecture weave module specifically representation node exchange information weave module function node node NN node NE EE node EN fnn hli   fee eij eij   hli  fnn fee sourcewhere  representation lth layer learnable function subscript message passing direction stack multiple module information propagate alternately passing node representation node node function connection JK net implicitly  propose representation update node representation message passing function aspect weave module  representation entire graph sample critical bottleneck training GCNs graph efficiency GCNs neighborhood aggregation scheme however graph distribution node expand extremely quickly sample propose neighborhood sampling layer wise sampling illustrate node sample node sample batch arrow sample direction node historical sample neighborhood sampling sample perform node calculation graphsage uniformly sample fix node training PinSage propose sample random graph along implementation improvement coordination cpu gpu reduce inference pipeline PinSage capable handle billion graph  propose reduce sample variance historical activation batch variate arbitrarily sample theoretical guarantee instead sample node FastGCN adopt strategy sample node convolutional layer layer wise sample interpret node sample graph convolution integral transforms probability FastGCN sample node via normalize reduce variance performance adapt propose sample node layer layer approach adaptive applicable explicitly reduce variance inductive another important aspect GCNs apply inductive training node graph another unseen node graph principle goal achieve mapping function feature dependent graph basis transfer across node graph inductive verify graphsage gat  FastGCN however exist inductive GCNs suitable graph explicit feature conduct inductive learning graph without explicit feature usually sample remains largely literature theoretical analysis understand effectiveness GCNs theoretical analysis propose category node focus task graph focus task analysis node focus task analyze performance GCNs laplacian smooth feature node cluster laplacian smooth operation formulate  SourceRight click MathML additional feature smooth feature node respectively graph convolution insight propose training training GCNs recently analyze GCNs signal processing perspective regard node feature graph signal basically fix pas filter insight propose extremely simplify graph convolution  architecture remove nonlinearities collapse parameter matrix HL  SourceRight click MathML additional feature author non gcn variant achieve comparable performance exist GCNs task  enhance pas filter operation equip GCNs nonlinear manifold ability propose  model remedy mlp graph convolution layer graph focus task kipf author  relationship GCNs graph kernel weisfeiler lehman WL kernel widely graph isomorphism GCNs conceptually generalization WL kernel iteratively aggregate information node formalize WL kernel upper bound GCNs distinguish graph structure analysis propose graph isomorphism network gin readout operation summation mlp achieve provably maximum discriminative training accuracy graph classification task analysis vapnik  dimension VC dim GCNs activation function exist rnns analyze optimization landscape linear GCNs local minimum relatively global minimum simplification verma zhang analyze algorithmic stability generalization bound GCNs layer GCNs satisfy notion uniform stability absolute eigenvalue graph convolution filter independent graph  autoencoders autoencoder AE variation widely apply unsupervised task suitable node representation graph implicit assumption graph inherent potentially nonlinear rank structure elaborate graph autoencoders introduce graph variational autoencoders improvement characteristic GAEs summarize comparison graph autoencoders GAEs autoencoders aes graph originate sparse autoencoder sae regard adjacency matrix variation raw feature node aes leveraged dimensionality reduction technique dimensional node representation specifically sae adopt reconstruction loss  SourceRight click MathML additional feature transition matrix reconstruct matrix dimensional representation node encoder decoder dimensionality parameter encoder decoder mlp hidden layer sae compress information dimensional vector reconstructs feature vector another sparsity regularization obtain dimensional representation apply node cluster task SAEs outperform non baseline however sae incorrect theoretical analysis mechanism underlie effectiveness remain unexplained structure network embed SDNE puzzle reconstruction loss actually corresponds proximity node node  representation neighborhood concept network collaborative filter closure motivate network embed proximity important SDNE modify objective function another laplacian eigenmaps  NA  node latent representation directly author modify reconstruction loss adjacency matrix assign zero non zero SourceRight click MathML additional feature bij otherwise bij another hyper parameter overall architecture SDNE framework SDNE proximity node preserve autoencoders motivate another contemporary DNGR replace transition matrix positive pointwise mutual information  matrix define raw feature associate random probability graph however construct input matrix complexity scalable graph GC MC approach gcn propose kipf encoder gcn FV sourceand bilinear function decoder  sourcewhere  decoder parameter approach node feature naturally incorporate graph without node feature encode node IDs utilized author demonstrate effectiveness GC MC recommendation bipartite graph instead reconstruct adjacency matrix variation  propose another modification directly reconstruct dimensional node vector aggregate neighborhood information lstm specifically  adopt objective function lstm SourceRight click MathML additional feature lstm input sequence author node neighborhood adopt neighborhood sample technique node prevent  memory author preserve regular equivalence centrality node pagerank unlike node dimensional vector   propose encode node gaussian distribution diag capture uncertainty node specifically author mapping node attribute variance gaussian distribution encoder FM FV  FV sourcewhere FM  parametric function instead explicit decoder function pairwise constraint model KL KL sourcewhere shortest distance node KL kullback leibler KL divergence constraint ensure KL divergence node representation relative graph distance however optimize loss adopt relaxation eij exp eij sourcewhere eij KL author propose unbiased sample strategy accelerate training variational autoencoders aforementioned autoencoders variational autoencoders VAEs another combine dimensionality reduction generative model potential benefit tolerate smooth representation VAEs introduce graph data VGAE decoder linear  SourceRight click MathML additional feature node representation assume gaussian distribution diag encoder variance matrix author adopt gcn propose kipf  FV   FV sourcethen model parameter minimize variational bound FV logp KL FV  approach reconstruct graph complexity motivate SDNE   propose another VAE graph data node gaussian distribution unlike exist adopt KL divergence measurement  wasserstein distance preserve transitivity node similarity SDNE   preserve proximity objective function minθ eij exp eij sourcewhere eij wasserstein distance gaussian distribution triple correspond rank loss proximity reconstruction loss define  SourceRight click MathML additional feature transition matrix sample drawn framework approach objective function minimize conventional VAEs reparameterization trick framework   node distribution VAE adopts wasserstein distance preserve transitivity node similarity framework   node distribution VAE adopts wasserstein distance preserve transitivity node similarity improvement discussion improvement propose GAEs adversarial training adversarial training scheme incorporate GAEs additional regularization  overall architecture specifically encoder GAEs generator discriminator aim distinguish latent representation generator prior distribution autoencoder prior distribution regularization objective function   sourcewhere reconstruction loss GAEs   logd FV sourcewhere FV generator graph convolutional encoder discriminator entropy loss prior distribution adopt gaussian prior experimental demonstrate effectiveness adversarial training scheme framework   reprint permission model incorporates adversarial training scheme GAEs framework   reprint permission model incorporates adversarial training scheme GAEs concurrently  propose generative adversarial network gan enhance generalization ability graph autoencoders specifically author objective function    sourcewhere LLE laplacian eigenmaps objective function addition author adopt lstm encoder aggregate information neighborhood instead sample immediate node  author random generate input sequence contrast   representation GAEs truth adopt random gaussian mlp generator inductive GCNs GAEs apply inductive node attribute incorporate encoder achieve gcn encoder GC MC VGAE VGAE directly mapping function node feature  information utilized parameter model apply node unseen training although GCNs GAEs architecture jointly promising future direction similarity GAEs similarity adopt reconstruction loss laplacian eigenmaps rank loss graph aes KL divergence wasserstein distance graph VAEs although similarity motivation appropriate similarity task model architecture remains  research understand underlie difference metric  reinforcement aspect reinforcement RL effective AI task RL feedback non differentiable objective constraint review graph RL characteristic summarize characteristic graph reinforcement characteristic graph reinforcement  utilized RL generate goal molecular graph non differential objective constraint specifically graph generation model markov decision node generative model regard RL agent operating graph generation environment treat agent action link prediction domain specific adversarial reward GCNs node representation  manner policy gradient concurrent  adopt RL generate molecular graph however generate graph sequence action  propose directly generate graph approach particularly  adopt RL predict chemical reaction specifically agent node graph predict bonding reward immediately prediction  gcn node representation rnn memorize prediction sequence gam apply RL graph classification random author model generation random partially observable markov decision POMDP agent perform action predict label graph node random reward simply agent correctly classify graph EP trt sourcewhere prediction otherwise environment   adopt RL knowledge graph KG specifically  target  informative target node  tackle task node node relation RL agent predict node output KG agent reward destination  regularization encourage diversity  adversarial adversarial gans adversarial attack drawn increase attention machine community recent review apply adversarial graph characteristic graph adversarial summarize characteristic graph adversarial characteristic graph adversarial adversarial training gan link model discriminator generator goal generator fool discriminator generate fake data discriminator aim distinguish sample data generate generator subsequently model benefit joint training minimax adversarial training effective generative model enhance generalization ability discriminative model review adversarial training scheme GAEs graph RL respectively review adversarial training graph detail  propose gan enhance graph embed objective function   logd SourceRight click MathML additional feature discriminator generator  exp    SourceRight click MathML additional feature dimensional embed vector node discriminator generator respectively combine equation discriminator actually objective node graph posse similarity node generate generator posse similarity architecture network embed negative node generate generator instead random sampling author enhance inference ability node embed vector adversarial network embed  adopt adversarial training scheme improve network embed   gan additional regularization exist network embed deepwalk impose prior distribution data regard embed vector generate sample  gan enhance semi supervise graph specifically author fake node generate density gap subgraphs weaken propagation across cluster exist model achieve goal author novel optimization objective elaborate loss ensure generator generate sample density gap equilibrium  adopt gan graph generation task specifically author regard graph generation task distribution bias random adopt gan framework generate discriminate random lstm random global network adversarial attack adversarial attack another adversarial intend deliberately fool target perturbation data adversarial attack deepen understand exist model inspire robust architecture review graph adversarial attack nettack propose attack node classification model GCNs modify graph structure node attribute denote target node  target model FV loss function LF FV model adopt objective function argmax FV   logZ  FV  FV SourceRight click MathML additional feature FV modify adjacency matrix node feature matrix respectively classification probability predict attack constraint simply optimization aim legitimate graph structure node attribute misclassified indicates attack  attack occurs training target model author propose constraint attack important constraint attack unnoticeable specifically author propose preserve data characteristic node distribution feature occurrence author propose attack scenario attack directly attack influence attack attack node relaxation optimization tractable concurrently adversarial attack graph objective function however focus graph structure instead assume attacker possess information author setting amount information available effective strategy RL SV adopt structurevec node graph representation reinforcement optimization experimental attack effective node graph classification task aforementioned attack target intend misclassification target node   non target attack intend reduce overall model performance treat graph structure hyper parameter optimize adopt meta gradient optimization along technique approximate meta gradient  conclusion review graph architecture similarity difference briefly discus application implementation future direction summarize application addition standard graph inference task node graph classification graph apply discipline model social influence recommendation chemistry biology physic disease drug prediction gene expression processing nlp computer vision traffic forecasting program induction graph NP multi agent AI thorough review beyond scope due sheer diversity application however inspiration important incorporate domain knowledge model construct graph architecture building graph relative distance suitable traffic forecasting prediction geographical location important graph model usually built architecture alone model computer vision community usually adopts cnns detect graph module nlp GCNs adopt syntactic constraint challenge integrate model application graph enables mining underlie exist graph data naturally model relational data graph greatly widen applicability graph model implementation recently library available develop model graph library source code mostly author repository appendix available online supplemental implementation easy improve implementation address distribute compute discus library graph library graph future direction ongoing future research direction worthy discussion model  graph structure due extremely diverse structure graph data exist suitable focus homogeneous graph heterogeneous graph seldom modality network negative conflict node unique structure additional challenge exist hypergraphs complex relation understudied important specific model handle graph compositionality exist model multiple exist architecture integrate gcn layer GAEs graph RL addition building systematically composite architecture future direction incorporate interdisciplinary knowledge principled basis recent graph network focus framework GNNs GCNs relational automl helpful reduce burden assemble component hyper parameter dynamic graph exist focus static graph however graph dynamic node feature social network establish social relation remove relation feature hobby occupation user network exist user model evolve characteristic dynamic graph incremental update model parameter remain largely unaddressed preliminary obtain encourage graph rnns interpretability robustness graph related risk sensitive scenario ability interpret model graph critical decision medicine disease related interpretability essential transform computer application clinical however interpretability graph challenge model graph node heavily interconnect addition exist model graph sensitive adversarial attack enhance robustness exist another important issue pioneer regard interpretability robustness respectively summary survey graph promising develop research opportunity challenge graph constitutes critical building model relational data important towards future machine artificial intelligence technique