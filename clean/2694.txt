vector regression SVR flexible regression algorithm computational complexity dimensionality input excellent generalization capability however central assumption  data available construction algorithm cannot data incremental SVR potential accuracy suffers overcome limitation propose novel incremental regression algorithm online robust vector regression ORSVR ORSVR solves  bound function simultaneously hence quadratic program qpp classical SVR decompose QPPs incremental algorithm solves qpp series comparative demonstrate ORSVR algorithm efficiently solves regression data without introduction flexible parametric regression algorithm vector regression SVR popular regression algorithm predict electrical load stock price SVR symmetrical loss function equally penalizes estimate addition SVR vapnik insensitive approach flexible minimal radius symmetrically around estimate function absolute error threshold ignore estimate manner outside penalize within function penalty advantage SVR computational complexity dimensionality input additionally excellent generalization capability prediction accuracy SVR algorithm assumption data obtain however data application usually model persistent data infinite sequence data instance data instance  data timestamp input  output variable representative application dynamic talent analysis traffic web medium processing series data data challenge SVR algorithm SVR handle data propose ensemble SVR propose approach creates sub model directly model sub model separately data propose improve multiple kernel SVR approach however approach batch data cannot data environment sample extreme online investigates extreme propose online data sequentially input data model update therefore researcher propose online version SVR handle data essentially online SVR integrates SVR algorithm online handle regression data approach introduce SVR online algorithm accurate online vector regression AONSVR adjustment propose adjust model adjustment sample assign remain error improve adjustment extra  SVR propose incremental SVR algorithm INVSVR INVSVR adjust model significantly improve handle uncertain data decompose classical SVR model dual SVR adjustment INVSVR adjust model apply AONSVR handle evolve data AONSVR fix parameter propose update parameter automatically online regression algorithm online regression online multiple kernel regression mention online incremental SVR algorithm prior knowledge data online regression depth user define kernel user define online multiple kernel regression parameter suitable prior knowledge data obtain however performance online SVR algorithm limited challenge data source noisy due electromagnetic interference temporary failure sensor accuracy suffer robust SVR algorithm explicitly handle noisy data perform classical  performance optimal incorporate concept fuzzy theory SVR peng propose interval twin SVR algorithm SVR algorithm handle noisy data replace constraint classical SVR probability constraint SVR algorithm robust bound however quadratic program QPPs mention SVR algorithm remain complex translation incremental SVR incremental  slowly incremental algorithm adjust wnew sample infinite discrete karush kuhn tucker kkt exist sample satisfy kkt however kkt exist incremental SVR algorithm complex adjustment wnew tend conflict kkt exist sample adjustment wnew drastically slows overcome challenge propose novel incremental SVR algorithm online robust vector regression ORSVR ORSVR novel variant incremental SVR algorithm modify classical SVR handle noisy data propose incremental ORSVR seek  bound function construct insensitive zone sample specifically incremental algorithm kkt QPPs ORSVR sample per therefore contribution handle noisy data modify classical SVR regression model ORSVR ORSVR  classical SVR transform QPPs regression model ORSVR seek bound function related  simultaneously qpp qpp classical SVR kkt bound simpler ORSVR learns faster standard incremental SVR incremental SVR algorithm ORSVR capture characteristic data distribution due modification classical SVR useful handle moreover correlation constraint training sample classical formulation SVR incremental algorithm however ORSVR constraint independent sample ORSVR online incremental algorithm kkt ensures ORSVR QPPs sequential manner accord sample infinite discrete kkt ensure exist sample satisfy kkt however ORSVR contains additional complex equation constraint SVR easy initial adjustment ensure kkt met sample arrives incremental algorithm seek upper function function simultaneously ORSVR quickly effectively handle data organize brief overview classical SVR introduces modify formulation SVR online robust vector regression algorithm introduce conclude remark future notation notation easy summary notation insensitive loss function define max predict output penalize error chose priori sample kernel function ith variable upper function ith variable function amount variable submatrix qij initial adjustment ith variable upper function initial adjustment ith variable function initial adjustment  submatrix indexed SS nnss submatrix indexed preliminary regression algorithm learns model independent variable response variable bias denotes inner reproduce kernel hilbert RKHS vector regression SVR popular regression algorithm automatically adjust parameter insensitive loss function training sample primal  sourcewhere training sample mapped dimensional RKHS transformation function regularization characterizes complexity regression model regularization constant slack variable shorthand variable without asterisk introduce proportion parameter vector error correspond dual  SourceRight click MathML additional feature SVR parameter proportion vector prefer respect sample dataset parameter optimization formulation estimate automatically optimally accord however SVR data vector become vector error model anything beyond specify penalize proportion regularization parameter formulation online robust vector regression although SVR advantage SVR SVR introduces complication constraint related training sample formulation contains additional inequality constraint complicate SVR complication researcher extra adjustment extends training sample extra adjustment initial adjustment preprocessing addition classical SVR cannot robust noisy data prediction performance SVR sharply decrease training dataset noisy data aim propose novel SVR simplify formulation easily adjustment online adjust addition novel SVR robust noisy data propose twin SVR  regressor determines bound function related svm classical SVR hence simplify formulation classical SVR novel SVR ORSVR spirit   ORSVR transforms classical SVR  regressor  ORSVR robust noisy data ORSVR assumption upper bound insensitive zone accord concept insensitive zone classical SVR deviation inside insensitive zone discard deviation outside insensitive zone reject insensitive zone minimal training sample ensure sample consequently upper bound function minimal upward sample remain upper bound function error capture slack variable penalize objective function via regularization parameter chosen priori similarly bound function insensitive zone downward maximize objective function ensure training data bound specific upper bound function introduce transform function minw    fori SourceRight click MathML additional feature minw   fori source equation determines upper bound however cannot directly therefore seek upper bound function transform QPPs minw    fori sourcewhere meaning parameter minimize correspond dual max   nαi  SourceRight click MathML additional feature denotes inner RKHS similarly estimate equivalent optimization minw   fori SourceRight click MathML additional feature dual max   nαi  source estimate upper bound bound function regression function construct source formulation ORSVR SVR fundamental ORSVR solves QPPs whereas SVR qpp SVR qpp constraint data constraint per qpp ORSVR data strategy QPPs qpp formulation ORSVR simpler classical SVR however constraint correlate training sample incremental algorithm upper bound function therefore obtain equivalent formulation constraint independent sample objective function training sample primal minw    fori source easy verify equivalent primal dual max   nαi  SourceRight click MathML additional feature positive semidefinite matrix qij bound function modify modification formulation classical SVR transform ORSVR QPPs constraint independent training sample qpp addition upper bound bound function regression model estimate ORSVR capture characteristic data distribution allows conditional predictive variance estimate automatically simultaneously feature useful heteroscedastic depends strongly input due advantage ORSVR choice regression model handle noisy data online robust vector regression previous introduce formulation ORSVR detail online karush kuhn tucker accord convex optimization theory minimization obtain minimize convex quadratic objective function constraint minw    CVN  source accord kkt theorem derivative kkt  SourceRight click MathML additional feature nαi CVN SourceRight click MathML additional feature increase readability replace explain becomes optimization convex domain kuhn tucker theorem sufficient optimum  source  source SourceRight click MathML additional feature regression function estimation  SourceRight click MathML additional feature margin function define source replace  relation source equation define karush kuhn tucker kkt training sample partition independent accord SS training sample strictly SE training sample exceed SR remain SR training sample partition training sample independent kkt SS SE SR procedure function kkt analysis SourceRight click MathML additional feature similarly training sample function partition independent accord adjustment focus obtain optimum ORSVR model adjustment procedure AONSVR INVSVR accord sample infinite discrete kkt ensure exist sample satisfy kkt however AONSVR sample arrives sample sum adjustment ensure exist sample kkt INVSVR sum CVN training sample INVSVR extend label training sample constraint CVN conflict  incremental algorithm extra adjustment initial adjustment preprocess training sample adjustment resolve conflict hence adjustment upper function seek minimization sample accord equation nαi  met however sum ensure exist sample satisfy kkt sample adjust equation appendix computer society digital library http doi org ezproxy auckland proof   SourceRight click MathML additional feature sample bias accord  source sample arrives sample assign satisfy kkt assignment violates kkt adjust adjustment sample margin function    SourceRight click MathML additional feature variation margin easily compute  SourceRight click MathML additional feature  source    source equation construct     SourceRight click MathML additional feature specific sample  sample contribute equation SQ    SS   source equation rewrite equivalent matrix   ssc  source rewrite       ssc  SourceRight click MathML additional feature accord  update compute error remain sample SE SR define  variation rewrite matrix notation      nns nnss   source replace variation   obtain      nns nnss  SourceRight click MathML additional feature define    nns nnss SourceRight click MathML additional feature rewrite   source accord update compute summary sample adjust algorithm summarizes ORSVR pseudo code perform adjustment sum assign sample SS SE SR derives minimal increment  adjusts sample assignment kkt however update conflict therefore additional overcome conflict transform sample transform minimal remain SR obtain equation   SourceRight click MathML additional feature direction equation SourceRight click MathML additional feature calculate yield minimal update    source sample increase operation algorithm online robust vector regression ORSVR sample compute exist adjust compute remain exit compute compute accord compute minimal increment  update SS SE SR update inverse matrix algorithm rebuild matrix inefficient iteration due complexity matrix inversion nlog avoid specific matrix reduce complexity sample sample update sourcewhere sample update  source sample define  SourceRight click MathML additional feature sample error SE remain SR recomputed  source  SourceRight click MathML additional feature sample error SE remain SR matrix update  RI RI iri iri SourceRight click MathML additional feature deletes remove sample update others complexity complexity analysis compute complexity analyze complexity algorithm complexity operation calculate margin distance sample complexity operation kernel adjust vector complexity operation adjust sample therefore complexity kernel complexity easy compute algorithm mainly kernel matrix therefore complexity complexity kernel operation easily avoid kernel matrix reduce complexity complexity factor although complexity online SVR algorithm average algorithm almost complexity online SVR algorithm experimental depends mostly vector influence significantly performance illustrate effectiveness propose ORSVR performance online SVR algorithm evaluation involve scenario artificial datasets artificial datasets relevant parameter evaluate empirically algorithm specific datasets enable evaluate merit propose approach practical scenario conduct python PC intel core processor ghz GB ram artificial datasets ORSVR AONSVR INVSVR artificial datasets simplicity radial basis function rbf kernel algorithm model parameter algorithm parameter incremental SVR parameter incremental SVR estimate sinc function drawn uniformly sinc sinc sin   SourceRight click MathML additional feature drawn uniform distribution uniformly random variable sinc evaluation dot predict dot predict dot almost dot therefore regression model built AONSVR perfectly data dot INVSVR model perfectly data contrast regression model built ORSVR data ORSVR built model accuracy regression model obtain built apply incremental SVR algorithm sinc data error RMSE training vector estimate regression function sample comparative RMSE training vector sinc dataset comparative RMSE training vector sinc dataset ORSVR RMSE AONSVR INVSVR ORSVR generalization ability ORSVR significantly AONSVR INVSVR ORSVR vector although ORSVR vector ORSVR AONSVR INVSVR strategy convert qpp QPPs regression model obtain apply incremental SVR algorithm sinc dataset regression model obtain apply incremental SVR algorithm sinc dataset explore potential advantage ORSVR AONSVR INVSVR regression performance trend noisy version sinc data increase sample noisy data randomly sample uniform distribution uni noisy sinc data AONSVR INVSVR suffer serious overfitting noisy sample vector whereas ORSVR data distribution accurately despite ORSVR sample vector ignore proportion noisy demonstrate ORSVR sensitive variance attribute ORSVR ability automatically increase width insensitive zone amount increase illustrate ORSVR advantage error RMSE training vector estimate regression function data comparative RMSE training vector sinc dataset comparative RMSE training vector sinc dataset ORSVR RMSE AONSVR INVSVR ORSVR robust noisy sample handle noisy sample effectively dynamic insensitive zone model influence noisy sample ORSVR generalization ability RMSE ORSVR characterize data distribution whereas AONSVR INVSVR tendency training sample algorithm cannot prevent influence noisy sample SVR SVR deviation training sample minimize function ORSVR AONSVR INVSVR setting data generalization ability ORSVR highlight ORSVR significantly faster AONSVR INVSVR strategy qpp QPPs partly responsible sparsity another contribute factor ORSVR sparsity algorithm addition vector estimate regression function AONSVR INVSVR almost noisy sample proportion significantly ORSVR hence ORSVR sparsity approach mention earlier vector determinant prediction summary analysis ORSVR algorithm superior noisy data preserve advantage faster datasets ORSVR publicly available regression datasets dataset application data dimensionality datasets datasets datasets detail datasets datasets datasets source uci repository http archive uci edu available http csie ntu edu cjlin  datasets regression html santa series competition datasets psych stanford edu  series  html noisy dataset input attribute generate independently uniformly distribute dataset sin  sourcewhere normally distribute variance noisy irrelevant input attribute available  linear kernel gaussian kernel exp parameter  restoration adjustment fix fix respectively regression performance ORSVR AONSVR INVSVR experimental datasets kernel datasets gaussian kernel RMSE exception parkinson dataset ORSVR INVSVR perform AONSVR datasets parameter bound proportion vector error ORSVR algorithm markedly friedman dataset noisy data heteroscedastic error structure advantage ORSVR INVSVR highlight benefit arbitrarily insensitive zone ORSVR SVR RMSE obtain algorithm datasets RMSE obtain algorithm datasets algorithm training sample increase increase gaussian kernel faster linear kernel moreover ORSVR training data notion qpp instead qpp synthetic ORSVR vector proportion training sample contribute faster training obtain algorithm datasets application ability generalize model important consideration successfully combine advantage SVR incremental ORSVR promise alternative situation advantage faster sparser capacity generalization incremental  prediction satisfactory accuracy addition ORSVR advantage handle noisy data suitable regression data conclusion ORSVR incremental regression algorithm handle data transforms classical SVR dual regression model ORSVR capture characteristic data distribution ORSVR robust additionally ORSVR determines bound function qpp associate SVR QPPs simultaneously kkt met sample maintain exist sample bound qpp simpler classical SVR faster incremental however ORSVR additional constraint compatible incremental therefore ORSVR approach incorporates constitute incremental algorithm ORSVR introduces procedure initial prior incremental adjustment ensure vector experimental demonstrate ORSVR successfully handle noisy data faster incremental SVR algorithm theoretically decremental paradigm ORSVR manner incremental decremental version ORSVR benefit validation limited memory efficiency however ORSVR improvement noisy data analysis variation relationship dependent independent variable impact performance address concept drift research incremental SVR algorithm drift