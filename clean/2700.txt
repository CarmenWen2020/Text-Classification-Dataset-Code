entity recognition ner task identify mention rigid  text belonging predefined semantic location organization etc ner serf foundation application text summarization machine translation ner achieve performance engineering domain specific feature recent empower continuous vector representation semantic composition nonlinear processing employ ner yield stat performance comprehensive review exist technique ner introduce ner resource tag ner corpus shelf ner systematically categorize exist taxonomy along distribute representation input context encoder tag decoder survey representative recent apply technique ner setting application finally reader challenge ner outline future direction introduction entity recognition ner aim recognize mention rigid  text belonging predefined semantic location organization etc ner standalone information extraction IE essential role variety processing nlp application text understand information retrieval automatic text summarization machine translation knowledge construction etc evolution ner entity NE sixth message understand conference  task identify organization geographic location text currency percentage expression  increase ner various scientific CoNLL ace  trec entity devote effort topic regard definition restrict definition entity NE something someone restriction justified significant percentage corpus   claimed restrict task entity rigid  referent rigid  define biological specie despite various definition  researcher consensus  recognize generally  category generic  location domain specific  protein enzyme gene mainly focus generic  english article exhaustive representative ner technique apply ner approach annotate data rely craft unsupervised approach rely unsupervised algorithm without label training feature supervise approach rely supervise algorithm careful feature engineering approach automatically discover representation classification detection raw input manner brief review detail motivation conduct survey recent DL neural network attract significant attention due various domain DL ner minimal feature engineering flourish considerable apply ner successively advanced performance trend motivates conduct survey report status technique ner research choice DL architecture aim identify factor affect ner performance issue challenge although ner thrive decade knowledge review arguably establish publish   survey overview technique trend craft towards machine summarize ner perspective fallacy challenge opportunity   review recent survey domain complex entity mention respectively summary exist survey mainly feature machine model DL ner germane recent survey survey development progress ner however recent advance technique yadav  survey recent advance ner representation survey focus distribute representation input char embeddings review context encoders tag decoder recent trend apply ner task multi task transfer reinforcement  adversarial  contribution survey intensely review application technique ner enlighten researcher practitioner specifically consolidate ner corpus shelf ner academia tabular useful resource ner research community comprehensive survey technique ner propose taxonomy systematically organizes DL ner approach along distribute representation input context encoder capture contextual dependency tag decoder tag decoder predict label sequence addition survey representative recent apply technique ner setting application finally reader challenge ner outline future direction background formal formulation ner introduce widely ner datasets detail evaluation metric summarize traditional approach ner ner entity clearly identifies item item attribute entity organization location domain gene protein drug disease biomedical domain ner classify entity text predefined entity category formally sequence token ner output tuples entity mention index entity mention entity predefined category ner recognizes entity ner define  task recognize organization location currency percentage expression text task focus coarse entity per entity ner task coarse grain ner recently grain ner task focus entity mention assign multiple grain illustration entity recognition task illustration entity recognition task ner important pre processing variety downstream application information retrieval machine translation etc semantic illustrate importance ner various application semantic refers collection technique enable understand concept meaning intent query user accord percent query entity recognize entity query understand user intent hence incorporate entity entity model individual sequence annotate entity document query propose utilize entity enhance user query recommendation query auto completion entity ner resource datasets quality annotation critical model evaluation summarize widely datasets shelf english ner tag corpus collection document annotation entity widely datasets data source entity tag summarize datasets mainly developed annotate news article entity suitable coarse grain ner task datasets developed various text source wikipedia article conversation user generate text tweet youtube comment  nut tag becomes significantly  domain specific datasets particularly developed pubmed  text entity ncbi disease  annotate datasets english ner annotate datasets english ner recent ner report performance CoNLL ontonotes datasets CoNLL contains annotation reuters news english german english dataset portion sport news annotation entity location organization miscellaneous goal ontonotes project annotate corpus comprise various genre  news broadcast  newsgroups conversational telephone structural information syntax predicate argument structure shallow semantics link ontology coreference version release release text annotate entity github  host ner corpus shelf ner academia project shelf ner academia project summary recent neural ner summary recent neural ner ner available online pre model summarizes popular english ner academia ner evaluation metric ner usually evaluate output annotation comparison quantify relaxed evaluation ner involves identify entity boundary entity evaluation entity correctly recognize boundary truth precision recall compute positive TP false positive FP false negative FN positive entity recognize ner truth false positive entity recognize ner truth false negative entity annotate truth recognize ner precision ability ner entity recall ability ner recognize entity corpus precision tptp  tptp FN SourceRight click MathML additional feature harmonic precision recall balance commonly precision recallprecision recall SourceRight click MathML additional feature ner involve multiple entity ass performance across entity commonly purpose macro average micro average macro average computes independently entity average hence treat entity equally micro average aggregate contribution entity compute average treat entity equally latter heavily affected quality recognize entity corpus relaxed evaluation  defines relaxed evaluation credit entity assign regardless boundary overlap truth boundary boundary credit regardless entity assignment ace proposes complex evaluation procedure resolve issue partial considers subtypes entity however problematic comparable parameter fix complex evaluation intuitive error analysis complex evaluation widely recent traditional approach ner traditional approach ner broadly classify unsupervised feature supervise approach approach ner rely craft domain specific gazetteer syntactic lexical kim propose brill inference approach input generates automatically brill tagger biomedical domain propose  leverage pre synonym identify protein mention potential gene biomedical text propose approach ner electronic health experimental approach improves recall limited impact precision ner  II   sar   mainly craft semantic syntactic recognize entity lexicon exhaustive due domain specific incomplete precision recall cannot transfer domain unsupervised approach typical approach unsupervised cluster cluster ner extract entity cluster context similarity lexical resource lexical statistic compute corpus infer mention entity unlabeled data reduces requirement supervision author unsupervised algorithm entity classification similarly  leveraged predicate input bootstrap recognition generic extraction propose unsupervised gazetteer building entity ambiguity resolution combine entity extraction disambiguation highly effective heuristic addition zhang  propose unsupervised approach extract entity biomedical text instead supervision model resort terminology corpus statistic inverse document frequency context vector shallow syntactic knowledge chunk mainstream biomedical datasets demonstrate effectiveness generalizability unsupervised approach feature supervise approach apply supervise ner cast multi classification sequence label task annotate data sample feature carefully training machine algorithm utilized model recognize unseen data feature engineering critical supervise ner feature vector representation abstraction text boolean numeric nominal feature morphology tag lookup feature wikipedia gazetteer dbpedia gazetteer document corpus feature local syntax multiple occurrence widely various supervise ner feature feature machine algorithm apply supervise ner hidden markov model hmm decision maximum entropy model vector machine svm conditional random crf propose hmm ner  identify classify date expression numerical quantity addition developed multilingual ner decision  algorithm merit opportunity independent decision classifier subset feature combine decision majority voting scheme propose maximum entropy entity  apply maximum entropy theory  extraordinarily diverse knowledge source tag decision maximum entropy   related  punctuation feature svm classifier classifier binary decision token belongs inside organization location MIS tag svm predict entity label CRFs context account  propose feature induction CRFs ner perform CoNLL achieve percent english  propose stage approach couple crf classifier crf latent representation derive output crf crf ner widely apply text various domain biomedical text tweet chemical text  technique ner recent DL ner model become dominant achieve feature approach beneficial discover hidden feature automatically briefly introduce ner survey DL ner approach ner machine compose multiple processing layer representation data multiple abstraction typical layer artificial neural network consists pas backward pas pas computes sum input previous layer pas non linear function backward pas compute gradient objective function respect multilayer stack module via chain derivative advantage capability representation semantic composition empower vector representation neural processing allows machine fed raw data automatically discover latent representation processing classification detection core strength apply technique ner ner benefit non linear transformation generates non linear mapping input output linear model linear hmm linear chain crf DL model complex intricate feature data via non linear activation function significant effort ner feature traditional feature approach considerable amount engineering domain expertise DL model effective automatically useful representation underlie factor raw data neural ner model paradigm gradient descent enables possibly complex ner taxonomy survey exist taxonomy encoder encoder tag decoder argue description encoder inaccurate information twice typical DL ner model representation raw feature representation representation capture context dependence tag decode survey summarize recent advance ner architecture distribute representation input embeddings incorporation additional feature POS tag gazetteer effective feature approach context encoder capture context dependency cnn rnn network tag decoder predict tag token input sequence instance token predict tag inside singleton entity outside entity tag scheme tag notation bio tag decoder detect entity boundary detect text span classify entity taxonomy DL ner input sequence predict tag DL ner model consists distribute representation input context encoder tag decoder taxonomy DL ner input sequence predict tag DL ner model consists distribute representation input context encoder tag decoder distribute representation input straightforward option vector representation vector distinct completely representation orthogonal distribute representation dimensional dense vector dimension latent feature automatically text distribute representation capture semantic syntactic explicitly input ner review distribute representation ner model hybrid representation representation employ representation typically pre collection text unsupervised algorithm continuous bag cbow continuous skip gram model recent importance pre embeddings input pre embeddings fix tune ner model training commonly embeddings google wordvec stanford glove facebook fasttext  propose bio ner biomedical ner model neural network architecture representation bio ner pubmed database skip gram model contains dimensional vector wordvec toolkit embeddings english  corpus augment newsgroups data operational technology bolt neural model sequence chunk consists sub task segmentation label neural model fed  embeddings randomly initialize embeddings jointly extract entity relation model model embeddings NYT corpus wordvec  propose tag scheme iterate dilate convolutional neural network ID cnns lookup model initialize dimensional embeddings  corpus skip gram propose neural model extract entity relation pre dimensional vector google addition glove fasttext widely ner task representation instead representation input incorporate representation neural model representation useful exploit explicit sub information prefix another advantage representation naturally handle vocabulary model infer representation unseen information  regularity widely architecture extract representation cnn rnn model illustrate architecture cnn rnn model extract representation cnn rnn model extract representation utilized cnn extract representation representation vector concatenate embed rnn context encoder likewise apply series convolutional highway layer generate representation embeddings fed bidirectional recursive network propose neural  model ner convolutional layer fix embed layer recently propose elmo representation compute layer bidirectional model convolution rnn model memory lstm gate recurrent gru typical choice propose  tagger independent ner  considers sequence utilizes lstms extract representation output tag distribution instead tag obtain tag primary representation superior input utilized bidirectional lstm extract representation representation concatenate pre embed lookup  investigate embeddings representation identify biomedical entity combine representation embeddings gate mechanism rei model dynamically decides information component introduce neural ner model stack residual lstm trainable bias decode feature extract embeddings rnn developed model handle lingual multi task joint training unified manner employ bidirectional gru informative morphological representation sequence representation embed concatenate representation recent advance model recurrent neural network viable model distribution contextual embeddings neural model generate contextualized embed  context important embeddings contextualized surround text meaning embeddings contextual illustrates architecture extract contextual embed washington  context extraction contextual embed washington  context model model extract output hidden backward model model extract output hidden output hidden concatenate embed extraction contextual embed washington  context model model extract output hidden backward model model extract output hidden output hidden concatenate embed hybrid representation besides representation incorporate additional information gazetteer lexical similarity linguistic dependency visual feature representation context encode layer DL representation combine feature approach hybrid manner additional information improvement ner performance price hurt generality neural model ner pioneer architecture temporal convolutional neural network sequence propose incorporate priori knowledge gazetteer POS outperforms baseline representation BiLSTM crf model feature ner task feature context feature embeddings gazetteer feature experimental extra feature gazetteer boost tag accuracy BiLSTM cnn model chiu  incorporates bidirectional lstm cnn besides embeddings model additional feature capitalization lexicon feature dimensional vector upper punctuation crf neural recognize normalize disease employ feature addition embeddings POS tag chunk feature morphological feature concatenate dimensional embeddings dimensional vector capitalize capitalize capitalize contains concatenate representation representation syntactical representation POS tag dependency role comprehensive representation multi task approach ner propose approach utilizes cnn capture orthographic feature syntactical contextual information POS embeddings model implement lstm architecture  liu propose combine latent dirichlet allocation lda embeddings propose local detection approach ner fix  forget encode   explores representation fragment context multi modal ner noisy user generate data tweet snapchat caption embeddings embeddings visual feature merge modality attention   unfair lexical feature mostly discard neural ner propose alternative lexical representation offline neural ner lexical representation compute dimensional vector encodes similarity entity recently propose representation model bert bidirectional encoder representation transformer bert masked model enable pre bidirectional representation token input representation comprise sum correspond token embeddings pre model embeddings corpus training intrinsically incorporate auxiliary embeddings embeddings category contextualized model embeddings hybrid representation survey context encoder architecture review widely context encoder architecture convolutional neural network recurrent neural network recursive neural network transformer convolutional neural network propose approach network tag consideration input sequence embed dimensional vector stage input representation convolutional layer local feature around output convolutional layer depends global feature vector construct combine local feature vector extract convolutional layer dimension global feature vector fix independent apply subsequent standard affine layer approach widely extract global feature max average operation finally fix global feature fed tag decoder compute distribution tag network input  propose bio ner biomedical ner utilized convolutional layer generate global feature global hidden node local feature global feature fed standard affine network recognize entity clinical text approach network cnn convolution layer extract feature treat sequence global structure approach network cnn convolution layer extract feature treat sequence global structure rnn latter influence representation former however important anywhere propose model blstm blstm capture dependency obtain representation input sequence cnn utilized representation fed sigmoid classifier finally representation generate blstm relation presentation generate sigmoid classifier fed another lstm predict entity propose iterate dilate convolutional neural network capacity traditional cnns context structure prediction unlike lstms sequential processing parallelism ID cnns permit fix depth convolution parallel across entire document architecture dilate cnn stack dilate convolution width token representation experimental ID cnns achieves speedup lstm crf retain comparable accuracy dilate cnn maximum dilation width filter width neuron contribute highlight neuron layer highlight dilate cnn maximum dilation width filter width neuron contribute highlight neuron layer highlight recurrent neural network recurrent neural network variant gate recurrent memory demonstrate remarkable achievement model sequential data bidirectional rnns efficiently information via future information via backward specific frame token encode bidirectional rnn evidence input bidirectional rnns therefore become facto standard compose context dependent representation text typical architecture rnn context encoder architecture rnn context encoder architecture rnn context encoder utilize bidirectional lstm crf architecture sequence tag task POS chunk ner apply BiLSTM architecture encode sequence context information employ grus encode morphology context information extend model lingual multi task joint architecture parameter employ multiple independent bidirectional lstm across input model promotes diversity lstm employ inter model regularization distribute computation across multiple lstms reduction parameter recently lstm neural network nest entity recognition   modification standard lstm sequence label model handle nest entity recognition propose neural model identify nest entity dynamically stack ner layer outer entity extract ner layer employ bidirectional lstm capture sequential context model merges output lstm layer ner layer construct representation detect entity ner layer recursive neural network recursive neural network non linear adaptive model structure information traverse structure topological entity highly related linguistic constituent however typical sequential label approach consideration structure propose classify node constituency structure ner model recursively calculates hidden vector node classifies node hidden vector recursively compute hidden feature node direction calculates semantic composition subtree node counterpart propagates node linguistic structure subtree hidden vector node network calculates probability distribution entity plus non entity bidirectional recursive neural network ner computation recursively direction direction computes semantic composition subtree node counterpart propagates node linguistic structure subtree bidirectional recursive neural network ner computation recursively direction direction computes semantic composition subtree node counterpart propagates node linguistic structure subtree neural model model model generation sequence token sequence model computes probability sequence model probability token SourceRight click MathML additional feature backward model model sequence reverse predict previous token future context SourceRight click MathML additional feature neural model probability token compute output recurrent neural network obtain context dependent representation backward combine model embed token model augment knowledge empirically verify helpful numerous sequence label task rei propose framework secondary objective predict surround dataset illustrates architecture ner task token network optimise predict previous token tag token sequence model objective encourages richer feature representation reuse sequence label sequence label model additional model objective perform ner  proposes token proposes network optimise predict previous  label sequence sequence label model additional model objective perform ner  proposes token proposes network optimise predict previous  label sequence propose  model augment sequence tagger tagger considers pre embeddings bidirectional model embeddings token input sequence sequence label task architecture LM lstm crf model model sequence tag model layer multi task manner vector embeddings pre embeddings model representation concatenate fed lstms experimental demonstrate multi task effective approach model task specific knowledge sequence label architecture contextualized representation representation pre embed contextualized representation bidirectional model concatenate fed context encoder sequence label architecture contextualized representation representation pre embed contextualized representation bidirectional model concatenate fed context encoder contextual embed neural model utilized hidden backward recurrent neural network contextualized embeddings merit model model independent tokenization fix vocabulary propose elmo representation compute layer bidirectional model convolution contextualized representation capable model complex characteristic usage semantics syntax usage variation across linguistic context polysemy transformer neural sequence label model typically complex convolutional recurrent network consists encoders decoder transformer propose  recurrence convolution entirely transformer utilizes stack attention wise fully layer encoder decoder various task transformer superior quality significantly transformer propose generative pre transformer GPT understand task GPT stage training procedure model objective transformer unlabeled data initial parameter adapt parameter target task supervise objective minimal pre model unlike GPT architecture bidirectional encoder representation transformer bert propose pre bidirectional transformer jointly conditioning context layer summarizes bert GPT elmo addition propose novel cloze driven pre training regime directional transformer cloze style objective predicts context difference pre training model architecture google bert bidirectional transformer abbreviate  OpenAI GPT transformer  elmo concatenation independently lstm generate feature downstream task difference pre training model architecture google bert bidirectional transformer abbreviate  OpenAI GPT transformer  elmo concatenation independently lstm generate feature downstream task model embeddings pre transformer become paradigm ner embeddings contextualized replace traditional embeddings google wordvec stanford glove achieve promising performance via leverage combination traditional embeddings model embeddings model embeddings tune additional output layer task ner chunk frame ner task machine reading comprehension MRC tune bert model tag decoder architecture tag decoder stage ner model context dependent representation input sequence tag correspond input sequence summarizes architecture tag decoder mlp softmax layer conditional random CRFs recurrent neural network pointer network difference tag decoder mlp softmax crf rnn pointer network difference tag decoder mlp softmax crf rnn pointer network multi layer perceptron softmax ner formulate sequence label multi layer perceptron softmax layer tag decoder layer sequence label task cast multi classification tag independently predict context dependent representation without account ner model introduce earlier mlp softmax tag decoder domain specific ner task softmax tag decoder predict japanese chess model input text input chess predict entity specific text representation embeddings fed softmax layer prediction entity bio tag scheme conditional random conditional random random globally observation sequence CRFs widely feature supervise approach ner model crf layer tag decoder bidirectional lstm layer cnn layer crf choice tag decoder performance CoNLL ontonotes achieve crf tag decoder CRFs however cannot information inner cannot fully encode representation propose gate recursive semi markov CRFs directly model instead automatically extract feature gate recursive convolutional neural network recently ling propose hybrid semi markov CRFs neural sequence label approach adopts instead feature extraction transition model label utilized derive approach leverage information calculation recurrent neural network explore rnn decode tag report rnn tag decoder outperform crf faster entity illustrates workflow rnn tag decoder model greedily tag sequence rnn decoder subsequently rnn decoder computes decoder hidden  previous tag previous decoder hidden  encoder hidden  output tag decode softmax loss function fed input finally obtain tag sequence pointer network pointer network apply rnns conditional probability output sequence discrete token correspond input sequence variable softmax probability distribution pointer apply pointer network sequence tag illustrate pointer network identify chunk label operation input sequence token michael  jordan identify label segmentation label neural network pointer network michael  jordan input fed pointer network identify label summary DL ner architecture summary summarizes recent neural ner architecture choice BiLSTM crf architecture ner pre directional transformer model cloze style manner achieves performance percent CoNLL bert dice loss achieves performance percent ontonotes ner heavily relies input representation integrate tune pre model embeddings become paradigm neural ner leverage model embeddings significant performance improvement report performance benchmark datasets report formal document CoNLL ontonotes ner noisy data nut remains challenge architecture comparison discus pro con perspective input encoder decoder consensus external knowledge integrate DL ner model ner performance boost external knowledge however disadvantage apparent acquire external knowledge labor intensive gazetteer computationally expensive dependency integrate external knowledge adversely affect hurt generality DL transformer encoder effective lstm transformer pre corpus transformer fail ner task pre training data limited transformer encoder faster recursive layer sequence dimensionality representation complexity attention recurrent disadvantage rnn pointer network decoder greedily decode input output previous mechanism significant impact obstacle parallelization crf choice tag decoder crf powerful capture label transition dependency adopt non model non contextualized embeddings wordvec glove however crf computationally expensive entity importantly crf performance softmax classification adopt contextualized model embeddings bert elmo user architecture data domain task dependent data abundant training model rnns scratch tune contextualized model data scarce adopt transfer strategy choice  domain pre shelf model available specific domain medical social medium tune purpose contextualized model domain specific data effective ner survey mainly focus ner english domain apart english lingual setting investigate ner chinese clinical text zhang yang propose lattice structure lstm model chinese ner encodes sequence input potential lexicon chinese conduct  czech arabic urdu  indonesian japanese characteristic understand fundamental ner task aim ner lingual transfer knowledge source target label apply ner outline typical network architecture ner survey recent apply technique explore ner multi task ner multi task approach learns related task relation task multi task algorithm achieve task individually approach network jointly perform POS chunk ner srl task multi task mechanism training algorithm discover internal representation useful task propose multi task joint model specific regularity jointly POS chunk ner task rei unsupervised model objective training sequence label model achieves consistent performance improvement propose multi lingual multi task architecture resource setting effectively transfer knowledge improve model ner sequence label task multi task framework apply joint extraction entity relation model ner related subtasks entity segmentation entity category prediction biomedical domain difference datasets ner dataset task multi task assumption datasets information multi task apply efficient data encourage model generalize representation transfer ner transfer aim perform machine task target domain advantage knowledge source domain nlp transfer domain adaptation ner task traditional approach bootstrapping algorithm recently approach propose resource across domain ner neural network propose transfer joint embed  approach domain ner  employ label embed technique transform multi classification regression dimensional latent related entity lexical context feature approach learns correlation source target entity layer neural network approach applicable source domain identical entity target domain peng  explore transfer multi task domain news social medium task segmentation ner transfer neural model commonly model parameter source task target task investigate transferability layer representation parameter architecture domain lingual application scenario task  label crf layer otherwise task learns crf layer experimental significant improvement various datasets resource available annotation  extend yang approach joint training informal corpus incorporate feature representation approach achieve  task ner obtain percent propose multi task model domain adaption fully connection  adapt datasets crf feature compute separately merit zhao model instance distribution misalign annotation guideline filter data selection procedure parameter architecture apply transfer ner training model source task model target task tune recently lin propose tune approach ner introduce neural adaptation layer adaptation layer adaptation layer output adaptation layer propose tag hierarchy model heterogeneous tag ner hierarchy inference grain tag onto target tag addition explore transfer biomedical ner reduce amount label data active ner active machine algorithm perform substantially data training data learns typically amount training data costly obtain combine active reduce data annotation effort training active proceeds multiple however traditional active scheme expensive retrain classifier newly annotate sample retrain scratch practical propose incremental training ner batch label newly annotate sample exist update neural network epoch query label specifically active algorithm chooses annotate predefined budget model parameter update training augment dataset chose annotation active algorithm adopts uncertainty sample strategy annotate experimental active algorithm achieve percent performance model data percent training data english dataset percent chinese dataset moreover percent training data active model outperform shallow model training data reinforcement ner reinforcement RL machine inspire  psychology concerned software agent action environment maximize cumulative reward agent environment interact reward perform action specifically RL formulate environment model stochastic finite machine input action agent output observation reward agent consists component transition function observation output function reward function agent model stochastic finite machine input observation reward environment output action environment consists component transition function policy output function ultimate goal agent update function policy attempt maximize cumulative reward model task information extraction markov decision MDP dynamically incorporates entity prediction flexibility query automatically generate alternative comprise issue query extraction source reconciliation extract sufficient evidence obtain policy agent utilize network function approximator action function function approximate neural network recently utilized data generate supervision perform entity recognition domain instance selector reinforcement obtains feedback reward NE tagger aim positive reduce noisy annotation adversarial ner adversarial explicitly training model adversarial purpose model robust attack reduce error input adversarial network generate training distribution player network generates candidate generative network evaluates discriminative network typically generative network learns latent data distribution discriminative network  candidate generate generator instance data distribution ner adversarial instance source domain adversarial target domain vice versa incorporate adversarial domain encourage domain invariant feature domain ner another option adversarial sample sample perturbation dual adversarial transfer network  propose aim resource ner adversarial sample sample perturbation bound norm maximize loss function  model parameter validation adversarial construct xadv classifier mixture adversarial improve generalization neural attention ner attention mechanism loosely visual attention mechanism usually focus image resolution perceive surround resolution neural attention mechanism allows neural network ability focus subset input apply attention mechanism ner model capture informative input transformer architecture review relies entirely attention mechanism global dependency input output apply attention mechanism ner task apply attention mechanism dynamically information component ner model  explore attention mechanism ner dependent sequence relation sequence propose attention neural ner architecture leverage document global information document information obtain document pre bidirectional model neural attention adaptive attention network ner tweet adaptive attention network multi modal model attention attention visual attention textual attention capture semantic interaction modality challenge future direction choice tag decoder choice input representation context encoders google wordvec recent bert model DL ner benefit significantly advance pre embeddings model without complicate feature engineering opportunity ner task challenge potential future direction challenge data annotation supervise ner DL ner annotate data training however data annotation remains consume expensive challenge resource specific domain domain expert perform annotation task quality consistency annotation concern ambiguity instance entity annotate   defeat  label location  organization CoNLL empire empire building label location CoNLL ace datasets confusion entity boundary inconsistency data annotation model dataset another document datasets domain data annotation complicate   report nest entity fairly percent entity  corpus embed within another entity ace corpus percent nest entity develop annotation scheme applicable nest entity grain entity entity assign multiple informal text unseen entity decent report datasets formal document news article however user generate text wut dataset slightly percent ner informal text tweet comment user forum challenge formal text due shortness  user generate text domain specific application scenario ner user generate text customer commerce banking another dimension evaluate robustness effectiveness ner capability identify unusual previously unseen entity context emerge discussion exists task direction research wut dataset future direction advance model demand application ner attention researcher ner pre processing component downstream application ner task define requirement downstream application entity detect nest entity survey direction exploration ner research grain ner boundary detection exist focus coarse grain ner domain research grain ner domain specific various application challenge grain ner significant increase NE complication introduce entity multiple NE ner approach entity boundary detect simultaneously entity decode tag worth define entity boundary detection dedicate task detect NE boundary ignore NE decouple boundary detection NE classification enables robust boundary detection across domain dedicate domain specific approach NE classification entity boundary effectively alleviate error propagation entity link knowledge entity boundary detection intermediate subtask ner knowledge exist separately focus entity boundary detection robust recognizer breakout research direction future joint ner entity link entity link EL entity disambiguation normalization task identity entity mention text reference knowledge wikipedia domain unified medical UMLS biomedical domain exist ner entity link task pipeline semantics successfully link entity related entity knowledge significantly enrich link entity contributes successful detection entity boundary classification entity worth explore approach jointly perform ner EL entity boundary detection entity classification entity link subtask benefit partial output subtasks alleviate error propagation unavoidable pipeline setting DL ner informal text auxiliary resource performance DL ner informal text user generate content remains research performance ner benefit significantly availability auxiliary resource location user evidence involve gazetteer additional feature performance increase ner domain auxiliary resource understand user generate content obtain auxiliary resource ner task user generate content domain specific text effectively incorporate auxiliary resource DL ner scalability DL ner neural ner model scalable challenge moreover optimize exponential growth parameter data grows DL ner model achieve performance massive compute elmo representation dimensional vector model gpus google bert representation TPUs however user tune model access powerful compute resource develop approach balance model complexity scalability promising direction model compression prune technique option reduce computation model transfer ner entity focus application resort shelf ner recognize entity however model dataset text due difference characteristic difference annotation although apply transfer ner fully explore future effort dedicate effectively transfer knowledge domain another explore research develop robust recognizer across domain explore zero shot shot shot ner task address domain mismatch label mismatch domain setting easy toolkit DL ner recently  developed  researcher user developer easy interface benchmarking entity annotation aim ensure repeatable  however involve recent DL technique ott  extensible toolkit sequence model machine translation text stigmatization implement framework  relies variant recurrent neural network recent framework tensorflow pytorch kera building training validate neural network program interface implement architecture developer code scratch exist framework envision easy ner toolkit developer standardize module data processing input representation context encoder tag decoder effectiveness expert non expert benefit toolkits conclusion survey aim review recent ner researcher building comprehensive understand survey background ner research brief traditional approach challenge future research direction consolidate available ner resource tag ner corpus shelf ner focus ner domain ner english resource tabular link easy access introduce preliminary definition ner task evaluation metric traditional approach ner concept review literature model accord taxonomy survey representative recent apply technique setting application finally summarize application ner reader challenge ner future direction survey reference DL ner model