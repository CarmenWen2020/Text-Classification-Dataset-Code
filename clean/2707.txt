tensor factorization recently gain increase popularity feature render tensor attractive ability directly model multi relational data propose ParaSketch parallel tensor factorization algorithm enables massive parallelism tensor compress tensor multiple tensor decompose tensor parallel combine reconstruct desire latent factor prior direction entail potentially complexity gaussian compression combine stage adopt sketch matrix compression propose enjoys dramatic reduction compression complexity feature lighter combine moreover theoretical analysis compress tensor inherit latent identifiability mild hence establish correctness overall approach numerical corroborate theory demonstrate effectiveness propose algorithm introduction tensor generalization matrix whereas matrix model binary relation user item tensor model multi relation user item tensor factorization invaluable   fluorescence chemical  hidden trait personality examine lens rank tensor factorization tensor attract considerable rapidly attention computer community notably data mining DM machine ML due emergence multi relational data pioneer author apply tensor factorization webpage link analysis tensor factorization analyze social network graph subsequently tensor apply recommender topic model cluster  refer interested reader overview depth review DM application data volume challenge tensor factorization canonical polyadic decomposition cpd popular tensor factorization model classical application involve mainly datasets algorithm alternate ALS gauss newton usually suffice datasets DM perform cpd challenge tackle algorithm propose recently  proposes avoid intermediate data explosion ALS perform cpd  proposes exploit inherent parallelism ALS gradient descent GD iteration however tensor iterative prohibitive tensor rely distribute implementation data variable machine networking link bottleneck  forth conquer approach scalable approximation sparse data tensor sparse factor illustration however cannot guarantee recovery underlie factor latent factor tensor identifiable  fail identify owe conquer strategy employ important advantage tensor matrix ability recover factor tensor formally essential uniqueness essential uniqueness mild latent factor matrix rank tensor identify permutation drastically matrix factorization model linear transformation ambiguity usually application perspective model uniqueness identifiability interpretability model parametrizations associate explanation data theoretical uniqueness cpd tapped establish model identifiability statistical model gaussian mixture model GMM hidden markov model hmm latent  allocation lda tensor factorization employ estimation model parameter conditional probability function hence paramount parameter uniquely pin data essential uniqueness rank tensor decomposition identifiability PARACOMP proposes technique  sub sample  replace random compression modification identifiability guarantee rank tensor establish albeit PARACOMP enables parallel computation issue limit application compression involves dense matrix tensor combine involves dense linear equation issue become bottleneck tensor undesirable compromise model identifiability guarantee PARACOMP dense random compression matrix become computation bottleneck whereas scalability  random sample lack theoretical identifiability guarantee aim bridging apparent dichotomy specifically propose ParaSketch parallel algorithm tensor factorization enjoys scalability propose employ sketch matrix perform compression computationally efficient gaussian random matrix PARACOMP identifiability sketch matrix characterize compress tensor admit unique factorization ensure correctly recover factor tensor tensor emphasize identifiability factorization compress sample tensor crucial conquer approach without cannot guarantee correctness ParaSketch algorithm naturally parallelizable sketch tensor independently factor however computational gain parallelization degradation accuracy recover factor tensor elucidate introduce joint factorization approach baseline sketch tensor centralize fashion numerical confirm joint factorization approach accurate estimation latent factor tensor expense computational complexity loss parallelization conference version siam international conference data mining relative journal version additional contribution namely extension ParaSketch framework cpd decomposition BTD motivate application factor matrix  comprehensive experimental insight joint factorization approach serf baseline gauge accuracy parallel computation conclude introduction sketch utilized accelerate tensor decomposition specifically sketch  linear sub ALS algorithm cpd approach centralize processing tensor hinder application datasets additionally focus unconstrained tensor factorization worth mention recent propose scalable algorithm constrain tensor factorization notation denote kronecker khatri rao outer respectively bold lowercase denote vector bold uppercase matrix vector assume vector denotes transposition bold underscore denote tensor denote norm frobenius norm respectively exit constant constant specialized notation introduce text background cpd uniqueness cpd model define FA sourcewhere integer factorization integer define rank denotes outer denote  correspond argument henceforth shorthand notation denote facilitate discussion  vectorized tensor  stack vector tensor matrix instance tensor RI denote vec vec source vec vec SourceRight click MathML additional feature vec vec SourceRight click MathML additional feature vec vectorization operator stack matrix vector mode mode mode  correspondingly vectorized version tensor obtain vec vec vec sourcewhich permute version another  vectorization operation illustrate tensor  vectorization matrix vector correspond boldsymbol boldsymbol tensor  vectorization matrix vector correspond readily checked admits cpd model rank BT CT SourceRight click MathML additional feature denotes khatri rao define sourcewhere denotes kronecker consequently vectorized vector compatible central concept uniqueness cpd define definition essential uniqueness tensor rank cpd essentially unique rank decomposition unique decompose exists inherently  permutation ambiguity permutation rank tensor sum essential uniqueness unique permutation counter exists permutation matrix diagonal matrix    source identifiability cpd definition kruskal rank matrix definition kruskal rank kruskal rank matrix integer linearly independent clearly rank min classical identifiability cpd theorem identifiability cpd rank decomposition essentially unique theorem asserts mild factor tensor identifiable decomposition closely related model cpd  factor  model motivate analyze data priori linear dependency exists due experimental underlie physic chemistry analyze fluorescence data chemical reaction worth  model decomposition BTD adopt BTD conciseness rank BTD model define  sourcewhere matrix rank hence rank BTD BTD polyadic decomposition PD necessarily minimal hence canonical  source raf SourceRight click MathML additional feature raf SourceRight click MathML additional feature    sourcewhere rth matrix define Aˆ AF Bˆ BF Cˆ ctr  indeed BTD PD mode structure mode BTD correspond PD representation uniqueness BTD cannot establish via uniqueness cpd theorem fails BTD due  mode instead uniqueness BTD establish generalize notion kruskal rank rank introduce definition partition matrix AF matrix definition rank partition matrix rank denote maximal integer submatrices linearly independent theorem theorem decomposition tensor rank  sourcethen essentially unique essential uniqueness rank unique within freedom arbitrary nonsingular linear transformation instance    nonsingular matrix analogous essential uniqueness cpd rank unique due inner dimension counter freedom compress tensor factorization aim develop conquer strategy tensor factorization PARACOMP serf PARACOMP parallel compression stage multiple tensor perform independent cpd tensor combine cpd factor tensor illustrate tensor RI reference propose compress mode tensor independently matrix RL RM RN compression scheme multiplies tensor mode yield tensor compression perform mode suppose tensor admits cpd model latent factor subscript omit conciseness compress tensor WC VB UA SourceRight click MathML additional feature tensor compression random projection tensor compression random projection materialize compression factorization scheme issue latent factor compress tensor identifiable cpd theorem theorem identifiability cpd compression  compress WC VB UA  mode compression matrix independently drawn absolutely continuous distribution min min min sourcethen almost surely identifiable compress data permutation counter remark uniqueness factorization compress tensor conquer algorithm context uniqueness factorization compress tensor important mainly due algorithmic correctness consideration compress tensor admit unique factorization principled combine compress tensor despite desirable identifiability guarantee PARACOMP compression stage expensive compression matrix dense unstructured overall dense tensor computational complexity min ijk prohibitive moderate tensor seek matrix perform compression tensor faster dense matrix tensor multiplication PARACOMP construct compression matrix guarantee identifiability compress tensor reasonable assumption johnson lindenstrauss transform introduce subsampled randomize hadamard transform SRHT version celebrate johnson lindenstrauss transform  overview  variant SRHT vector specially structure matrix  random subsampling matrix contains nonzero uniformly distribute hadamard matrix recursively define SourceRight click MathML additional feature define dimension diagonal matrix diagonal entry rademacher random variable probability proposition SRHT  diagonal matrix diagonal entry dii probability dii probability matrix hadamard matrix assume matrix sample coordinate dimensional vector uniformly random lnd lnn sourcethen probability fix orthonormal   vector compute  proof proposition asserts SRHT matrix matrix SU preserve dimension subspace span propose approach ParaSketch cpd goal efficient parallel tensor factorization algorithm replace gaussian random matrix PARACOMP SRHT matrix towards address identifiability issue cpd compress tensor discus exploit structure SRHT matrix computation burden combine analyze complexity propose PARACOMP finally propose baseline identifiability compress tensor cpd SRHT matrix theorem cpd model sketch tensor identifiable probability sourceand RL RM RN SRHT matrix        SourceRight click MathML additional feature respectively accuracy parameter theorem difference replace dense compression matrix sketch matrix tremendous impact computation complexity moreover argument establish unique factorization compress tensor apply hinge compression matrix drawn absolutely continuous distribution theorem SRHT matrix theorem remark complexity comparison schematic   estimate comparison cpd remark theorem assume dimension simply pad tensor slab zero redundant dimension remove factor combine remark practical tensor data dimension others instance spectrogram tensor analyze EEG meg data mode frequency channel frequency channel whereas index due temporal resolution duration EEG meg recording interested analyze frequency mode channel mode factor temporal factor potentially overly detailed easily adapt assume interested compress mode perform sketch mode corollary corollary tensor cpd partially sketch tensor identifiable probability RN SRHT  accuracy parameter theorem lemma sketch matrix RL matrix RI kruskal rank sketch matrix lemma RL SRHT matrix    probability SA proof lemma theorem appendix computer society digital library http doi org ezproxy auckland TKDE combine combine generally develop treatment fully exploit structure SRHT matrix away lighter computation illustrate combine factor factor combine procedure avoid clutter denote sketch matrix mode instead suppose spawn compress tensor compress tensor denote perform cpd tensor parallel obtain  SourceRight click MathML additional feature individual permutation ambiguity factor tensor independently possibly permutation compress factor subscript reconcile permutation scaling combine recover factor resolve permutation ambiguity compression matrix denote correspond factor   magnitude    ambiguity fix replica matrix replica Aˆ  PF Aˆ  SourceRight click MathML additional feature PF denote permutation matrix manipulation equivalent     linear assignment lap efficiently hungarian algorithm abuse notation denote permutation matrix permute    remain ambiguity remove elementwise     fix permutation combine   SS ST  sourceby SRHT definition    PP PT   SourceRight click MathML additional feature define Pˆ concatenation easily invert Pˆ sample matrix contains nonzero invert Pˆ invert inverse hadamard transform invert operation expensive inverse hadamard transform operation  overall algorithm summarize algorithm combine algorithm algorithm ParaSketch input data tensor replica compression dimension cpd rank output factor perform compression described yield tensor XT perform cpd compress tensor yield BT CT perform combine procedure described algorithm algorithm combine procedure input output permutation matrix compress tensor permute factor obtain resolve ambiguity entry invert Pˆ remark replica construction compression ensure inversion index matrix  sample Pˆ cannot zero sample perform independently uniformly sample procedure corresponds coupon collector nln coupon replica  remark data compression ratio suppose data tensor mode compress dimension data compression ratio TL argue  constant compression ratio express  simulation exceeds recovery numerical precision factor noiseless compression ratio complexity analysis simplify analysis focus mode PARACOMP ensure mode factor tensor recover compress tensor replica PARACOMP factor replica however dense compression matrix dense tensor  construction apply compression  PARACOMP hence computation ijk complexity ParaSketch dominate hadamard transform  vector  tensor combine procedure  PARACOMP principle invoke random gaussian matrix approximately orthogonal replace inversion transpose accuracy deteriorates approximation adopt complexity comparison PARACOMP ParaSketch summarize numerical memory complexity algorithm storage compress tensor compression matrix combine simplify analysis focus tensor PARACOMP storage requirement compress tensor   compression matrix ParaSketch storage compress tensor  hadamard matrix compression storage compression due incur  storage requirement difference   comprehensive comparison respective memory footprint challenge  minimal replica recover factor tensor PARACOMP   ensures successful recovery remark choice storage complexity IL PARACOMP   ParaSketch assume compression dimension complexity PARACOMP  ParaSketch remark aim achieve parallelism factor compress tensor propose ParaSketch computational resource PARACOMP factor  storage parallel processor however resource limited instead instantiate tensor batch discard factorization factor tensor compression combine PARACOMP hence afford factor tensor sequential batch perform parallel factorization within batch compression combine reduce dramatically exactly ParaSketch joint factorization sketch tensor explore baseline understand parallelism accuracy compression randomly sketch tensor instead factor jointly obtain latent factor tensor joint factorization hence benefit parallel compute ParaSketch lose factor accurate ParaSketch exploit sketch data simultaneously avoids noisy permutation  SNR rank scenario joint factorization formulation mina sbt sct sourcewhere mode  tth sketch tensor sbt sct sketch matrix tth sketch tensor mode respectively optimization clearly nonconvex unknown however adopt alternate minimization  approach update factor fix cyclic fashion enjoys optimal individual factor update matrix exploit kronecker mina sct SATA  vec CT sourcewhere vectorization  approach instantiate kronecker computational challenge kronecker  FK addition iteration joint factorization approach factor estimate directly therefore reconcile permutation ParaSketch procedure joint factorization approach constitutes obtain factor sketch tensor computational burden accurate estimate factor ParaSketch BTD extend ParaSketch framework BTD rank apply compression strategy tensor conforms BTD model vec   SourceRight click MathML additional feature  sourcethen compression vec source SourceRight click MathML additional feature  SourceRight click MathML additional feature  VBF UAF source apply mixed AC BD obtain compress tensor admits BTD model factor compress UAF VBF  source characterize identifiability compress BTD model introduce lemma lemma SRHT matrix AF partition matrix dimension suppose dimension satisfies AR AR sourcethen probability SA source proof relegate appendix available online supplemental lemma theorem theorem RI admit rank BTD compress tensor SRHT RL RM RN compression dimension satisfy  AR AR BR BR SourceRight click MathML additional feature addition min source sourcethen probability saa sbb scc essentially unique rank BTD proof theorem appendix available online supplemental unlike cpd BTD role played mode mode couple rank matrix within BTD implementation focus scenario couple mode compress mode assume mode compression mode rank matrix recovery combine computationally challenge devise computationally efficient algorithm beyond scope therefore compress mode recall align factor estimate replica introduce anchor factor due structure BTD introduce anchor mode uniquely identify permutation whereas mode suppose replica sketch matrix sct sct vec sourceby apply nonlinear nls algorithm propose obtain rank matrix mode factor  replica sct reconcile permutation factor subsequently resolve ambiguity procedure exactly algorithm align permutation permute correspond rank matrix permutation within replica virtue essential uniqueness BTD recovery rank matrix inherent ambiguity correspond rank matrix align rank matrix replica purpose rank matrix replica unifies replica denote  align permutation rank matrix     SourceLet  vertical concatenation    vertical concatenation   tsa  source UΣVT tsa  source tsa TPA 1D SourceRight click MathML additional feature equation singular decomposition svd UR VR correspond singular  remain singular singular replace zero denotes vertical concatenation pat  RHS intimidate easy compute pat TPA diagonal matrix compute  perform multiplication multiplication inverse hadamard transform diagonal matrix operation thanks sketch scheme propose similarly RHS enjoy computation inverse hadamard transform multiplication diagonal matrix summary BTD model sketch combine technique apply identifiability guarantee combine procedure couple mode summarize algorithm remark aim recover  algorithm inherent linear transformation ambiguity  per definition BTD algorithm combine procedure couple mode BTD input output  permute factor permutation matrix factor resolve ambiguity entry yield   accord numerical performance approach setting cpd propose ParaSketch PARACOMP  exclude comparison approximate tensor recover underlie factor cpd without compression baseline comparison purpose BTD nls algorithm BTD ParaSketch meta algorithm tensor decomposition algorithm building decompose individual tensor sketch conduct linux workstation parallel processor GB ram focus cpd synthetic data generate factor estimation accuracy performance specifically generate factor synthesize tensor fed algorithm estimate factor estimation performance normalize error  define    Aˆ SourceRight click MathML additional feature Aˆ estimate factor truth factor permutation correspond apply permutation permutation due inherent permutation ambiguity cpd celebrate linear assignment efficiently hungarian algorithm resolve ambiguity ParaSketch baseline generate noisy data additive gaussian dimension tensor dimension sketch replica fix dense tensor billion entry moderate tensor facilitate comparison cpd dense tensor replica factor generate matlab  generate fashion replica  factor compress tensor cpd solver toolbox ParaSketch PARACOMP parameter configuration randomly generate instance average across instance propose ParaSketch performs PARACOMP replica employ remark data compression ratio  compression ratio  percent amount data factorization accurate estimation factor achieve propose ParaSketch achieves factor estimation performance PARACOMP lighter computation performance propose algorithm baseline specifically dimension fix generate data accordingly compression dimension rank ParaSketch ratio fix recall replica ParaSketch   PARACOMP  factor estimation accuracy randomly generate data report average addition report spent compression combine stage ParaSketch PARACOMP baseline comparison CP opt propose nls CP opt aim tackle tensor factorization nls exploit structure tensor factorization derive efficient implementation levenberg marquardt algorithm document literature criterion tolerance algorithm function threshold algorithm terminate otherwise algorithm till maximum author algorithm propose ParaSketch PARACOMP importantly PARACOMP spent compression combine highlight merit propose tensor compression combine performance bottleneck PARACOMP SRHT matrix perform compression greatly alleviate issue runtime verify complexity analysis CP opt nls longer propose ParaSketch tensor comparison highlight necessity exploit parallel processing tensor factorization factor estimation performance quantify signal ratio SNR define SNR SourceRight click MathML additional feature variance gaussian entry tensor dimension tensor estimation performance improves SNR relatively weaker propose achieves estimation performance PARACOMP factor estimation performance SNR comparison joint factorization approach identify advantage disadvantage ParaSketch approach joint factorization approach conduct simulation assess estimation accuracy factor computational performance ability ParaSketch algorithm joint factorization approach identify rank factor tensor report average normalize error  estimate rank factor simulation simulation tensor rank underlie factor sketch tensor approach identify factor tensor joint approach accurate identify factor ParaSketch algorithm SNR identify rank factor acceptable accuracy factor estimation performance SNR PS ParaSketch joint factorization however algorithm difference ParaSketch factor sketch tensor parallel processor average joint factorization approach average ParaSketch factor sketch tensor combine factor difference algorithm accuracy estimate factor matrix mode explanation conditional update mode assume reasonable estimate mode coherent combine advantage mode accuracy estimate tensor factor approach rank average  estimate rank factor simulation tensor signal ratio sketch tensor estimate factor tensor estimation performance depict approach accurately identify rank factor rank factor relatively however rank tensor grows joint factorization approach robust identify rank factor ParaSketch approach confirms intuition parallel processing combine multiple processor introduce accuracy degradation depicts average estimate rank factor sketch tensor ParaSketch approach sometimes magnitude faster joint factorization approach rank factor estimation performance rank PS ParaSketch joint factorization factor estimation performance rank PS ParaSketch joint factorization computational ParaSketch joint factorization rank computational ParaSketch joint factorization rank simulation BTD perform simulation evaluate propose BTD mention earlier algorithm BTD nls propose directly apply nls tensor subproblems propose nls initialization nls generalize eigenvalue decomposition implement dimension compression dimension rank parameter within mode rank matrix zero gaussian standard deviation generate tensor comparison propose favorably directly apply nls tensor comparison BTD BTD brings complication factor estimation performance mode linear transformation ambiguity within   essentially unique oppose cpd calculate  define adopt strategy   vectorize vector IJ matrix IJ matrix metric matrix truth factor vectorization notwithstanding degradation accuracy apply nls directly propose estimate factor accuracy  comparison BTD data mining algorithm dataset taxi trajectory beijing chinese chinese festival construct tensor discretizing latitude longitude grid dimension tensor therefore tensor integer taxi correspond specific ParaSketch algorithm cpd rank component tensor replica dimension perform ParaSketch rank parameter ParaSketch visualize significant factor define sum norm vector correspond rank sub marker correspond absolute rank matrix mode vector examine beijing discover location correspond attraction temple   park amusement park intensity taxi activity highlight popularity  sub magnitude correspond vector mode contains temporal information activity relatively activity interestingly decline taxi activity inspection date reveals eve festival  hence beijing taxi trajectory data analysis magnitude rank matrix mode vector significant rank factor tensor visualize identify location magnitude correspond mode temporal variation conclusion propose algorithm facilitate parallel cpd BTD tensor data approach acceleration exist prior render suitable datasets analysis establishes correctness propose algorithm identifiability latent factor compress tensor uncompressed tensor cpd BTD characterize recovery propose approach sketch matrix replica ensure recovery factor tensor data numerical synthetic data confirm efficacy propose