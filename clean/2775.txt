twitter social medium potential source disease surveillance data however  tweet challenge standard information extraction deployed employ approach rely keyword distinguish relevant irrelevant keyword mention susceptible false positive keyword volume influence social phenomenon unrelated disease occurrence furthermore intend multilingual scenario incorporate semantic context experimentally examine approach classify text epidemiological surveillance social web addition systematic comparison impact input representation performance specifically continuous representation encode ontology subword byte encoding establish desirable performance characteristic multi lingual semantic filter approach depth discussion implication surveillance introduction disease surveillance twitter surveillance typically volume message disease topic indicator actual disease activity via keywords disease prominent  forecasting  involves outbreak due lag official reporting potentially predict outbreak ahead official forecasting involves longer horizon generally positive correlation assume volume message disease activity however assumption volume disease related message influence panic factor therefore important incorporate semantic orientation tweet discriminate relevant irrelevant mention keywords message explicitly mention disease actually non occurrence related context context spatio temporally irrelevant instance challenger sick flu actual reference occurrence flu challenger disaster therefore incorrect mention model outbreak experimental evidence incorporation semantic orientation tweet actually improves performance prediction model application  spite currently aware automate surveillance actively semantic filter technique classify message related multi lingual application dominant approach multi lingual ontology taxonomy   essentially keyword volume core furthermore really secondary aggregator rely mediate  mail semantic redundant multilingual text classification purpose semantic filter text disease surveillance active research recent systematically investigates issue motivation technical approach investigate issue twitter message whereas employ  mail message social medium message generate platform twitter unique challenge conventional text processing technique instance twitter text generate user individual style vocabulary addition tweet colloquial characterize slang misspelling grammar additional artifact hashtags emoticon URLs uniform resource locator furthermore purposeful platform  mail explicit goal communicate disease outbreak report disease twitter inadvertent meaning message redundant mislead another difference error analysis previous delve detailed discussion error implication dedicate significant portion characterize error implication performance desirability model regard multi lingual message classification approach entail model consideration potentially resource intensive multi lingual expertise model resource employ classify related resource resource translate resource investigate latter approach variation approach fully translate resource resource partially translate project mono lingual embeddings embed obtain multi lingual embeddings typically embeddings resource project resource usually english multi lingual classification multi lingual embeddings performance par mono lingual classification task document classification machine translation employ neural machine translation approach rely recurrent auto encoder decoder neural network domain training data style vocabulary source data target meaning situation input distribute representation source output distribute representation target employ google neural machine translation GNMT obtain translation tweet GNMT model publicly available data source european union communication generally formal style oppose colloquial style twitter message consequence translation performance GNMT unreliable tweet however already improve GNMT freely available rare GNMT employ lexical actual useful twitter GNMT somewhat robust error slight misspelling enables handle rare GNMT internally employ hybrid architecture combine transformer recurrent auto encoder decoder neural network transformer architecture bert bidirectional encoder representation transformer employ obtain neural representation  multilingual currently account task translation classification entity recognition bert internally employ vocabulary byte encoding summarise activity pipeline pipeline summarise multi lingual message classification pipeline image corpus generation creation corpus obtain tweet twitter account specific keywords via python script twitter api python  plugin footnote tweet marked public default security marked private  user employ keyword filter extract desire tweet training data employ data english tweet mention flu  data extract tweet mention flu french german spanish arabic japanese translate tweet eliminate tweet incomprehensible annotate remainder tweet annotation label tweet mention recent ongoing disease positive within disease within  within infection transmission sick individual susceptible healthy individual treat mention irrelevant label message negative addition eliminate duplicate remove retweets tweet RT tag manually duplicate marked RT finally remove punctuation token denote  user respectively summarizes composition corpus perform preprocessing prior translation translation api robust return unspecified error translation contains inconsequential URLs incomprehensible unable  annotate tweet remove tweet addition tweet content tweet retweeted tweet retain translation actual tweet correspond datasets yield percentage tweet successfully translate retain corpus summary pre processing pre processing tokenization tag tag employ gate architecture text engineering  tagger application tagger penn treebank tag addition additional tag HT usr url correspond twitter specific phenomenon namely hashtags user URLs respectively attempt stem without stem stem reduces lexical diversity reduce stem feature generation input representation representation concept representation distribute representation distribute representation concept conceptual representation employ ontology ontology previously developed SNOMED CT systematic nomenclature medicine clinical whereas ontology exist  biomedical ontology  ontology elect employ broader conceptual coverage advantage model task instance  partially define     generally technical casual text twitter significant regularity ontology specifically twitter disease detection SNOMED CT specifically  divergent medical terminology SNOMED CT international comprehensive precise clinical health terminology ontology input representation transform tweet vector feature firstly flatten ontology constituent concept ontology concept associate token refer concept concept effectively ontology basically heavily redact english epidemiological relevance obtain feature vector simply tokenize tweet token flatten ontology token exists ontology simply replace concept occurs flu encode ref frequency OOV OOV ref refers reference concept refer indicator concept concept conceptually ambiguous legitimately sens sick possession frequency refers reference frequency concept denotes temporal periodicity OOV cnf representation vocabulary version ontology concept correspond token versus vocabulary token corpus billion english needle vocabulary obtain cnf representation OOV replace tag therefore previous flu becomes ref frequency DT NN depicts transformation message flu derive feature vector cnf POS tag image SNOMED CT ontology organize differently concept correspond description concept concept SNOMED CT employ sqlite implementation concept description FTS text vocabulary concept replace tag ontology representation input vector distribute representation apply neural embeddings concept apply wordvec docvec setting training distribute embeddings wordvec docvec model ontology cnf wordvec docvec model optimal performance dimensional representation context training epoch distribute memory architecture minimum wordvec model employ google dimensional standard corpus experimental setup variety model neural network convolutional neural network cnns recurrent neural network rnns memory lstm specify maximum message token consequently tweet vector representation message shorter zero pad cnn cnf model architecture yoon kim pas embed dropout layer employ filter width concept embed rectify linear  stride output feature max pool layer output concatenate feature vector fed dropout layer sigmoid output layer neuron model architecture depict architecture employ google standard model dimensional representation input instead model architecture cnn classification model image cnn lstm cnf model stack rnn cnn employ filter width stride apply max pool pool width feature concatenate output feature vector pas lstm layer directly sigmoid output layer neuron depicts model architecture model architecture cnn lstm classification model image stack lstms cnf model stack rnn layer another apply dropout input array output lstm layer neuron cnf model dropout layer another lstm layer another dropout layer finally sigmoid output layer neuron depicts model architecture stack lstms model input message  sick cnf described model architecture stack lstms classification model image directional lstm cnf model apply dropout input layer parallel stack lstm layer output stack stack comprises layer neuron output fed dropout layer output lstm layer output stack concatenate fed dropout layer sigmoid output layer neuron depicts directional lstm model input sick flu optimal performance iteration epoch model training beyond model tend overfit due effective vocabulary cod python neural model employ python kera package cnn lstm model dropout layer dropout probability model architecture directional lstm classification model image wordvec docvec vector employ python gensim package unigram bag model scikit sgd classifier vector machine classifier stochastic gradient descent procedure employ python scikit package logistic regression classifier logistic regression sgd model scikit hyper parameter default employ iteration sgd model bert employ pretrained multilingual bert model namely bert multilingual uncased bert multilingual  library bert layer fed cnn architecture employ wordvec embeddings discussion employ precision recall performance metric equation       TP FP FN precision recall positive false positive false negative respectively clarity discus vector encode representation whilst discus distribute continuous neural embeddings model performance employ average performance addition performance ideal situation average performance couple performance variance variance classifier performance datasets stable variance performance characteristic approach greatly combine formula representation data categorical vector representation data continuous vector encoding          refer quantity norm var invariance quantity norm var normalize variance performance variance express percentage maximal variance performance metric precision recall fscore bound variance upper bound calculate  inequality ensure invariance overall performance therefore overall implies performance model metric datasets performance variance maximal instance precisely datasets maximal metric minimal overall metric implies model obtains perfect metric datasets exclude validation dataset overall model performance conveyed aggregate inclusion additional information aggregate model performance indicates performance unigram bag baseline whilst remain performance approach mitigate performance divergence training data data mainly due lingual lexical divergence sub optimal translation modest variation perform approach representation bold average performance categorical vector representation bigram SNOMED CT model average performance respectively precision recall respectively overall precision recall respectively perform distributional model cnn wordvec model average precision recall respectively overall performance precision recall respectively crucially overall performance across slight difference performance representation instance perform approach cnf model overall performance addition baseline model overall performance really performance additional technical complexity conceptual representation neural approach consistently recall precision across slight difference representation slight gain performance versus unigram baseline employ message disease therefore lexical difference imply approach exist data noteworthy performance SNOMED CT model counterintuitive SNOMED CT organize sort task instance employ sqlite implementation obtain concept refer token rely text via FTS text footnote SNOMED description concept truncate output depict respectively truncate output SNOMED CT image truncate output SNOMED CT image candidate concept return prefer concept automatically enforce setup simply return mapped concept moreover prefer meaning return stiff syndrome   syndrome respectively disorder text processing model mapped model encounter yield useful information performance SNOMED CT conceptual representation relative baseline explain coverage mapping frequently meaningless consistent somewhat effectively normalizes text mapped concept vocabulary cnf representation SNOMED CT encoder return versus cnf encoder therefore effectively sort text normalizer finally evident obtain poorer performance model consistently poorer japanese arabic  japanese arabic    oppose french german spanish english  european bert model report bert multilingual uncased model bert multilingual model basically identical conclusion future promising particularly closely related model performance significantly  european english model belongs probably due abundance parallel datasets model development closely related arabic globally deployed implementation strategy apply model cheaper specific model exist mathematical impression significance  application already  approach rely textual web data typically model disease intensity function keyword volume spanish dataset obtain performance tweet positive  model input error without message classification  model assume message keyword flu relevant model incorrectly assume disease activity intense actually ideally model accept message report actual occurrence disease  accept message label relevant ideal actual model input model input error expression          TP FP FN positive false positive false negative respectively precision performance false positive exist increase model input error recall performance false negative exist reduces model input error negative model error implies precision recall positive error recall precision crucially combine expression equivalence error rate         TP FP FN precision recall positive false positive false negative respectively therefore error rate proportional difference recall precision false negative offset false positive therefore precision recall spanish dataset unigram bigram cnf model bold italic assume error geo spatially localize phenomenon dialect difference error effectively zero consequence surveillance performance perfect classifier linguistic balance performance precision recall respect bigram SNOMED model doubly desirable overall performance difference recall precision performance overall balance precision recall performance achievable precision performance prefer avoid false positive ultimately reliable performance performance variance data prefer avoid unreliable raw performance important performance predictable therefore adjust surveillance model