propose novel feature architecture classification conventionally supervise classification amount training data currently increase trend employ unsupervised dependency availability training data reduce implementation unsupervised feature network feature automatically data obtain representation classification autoencoder generative adversarial network gan unsupervised gan however trajectory feature unpredicted direction due random initialization unsuitable feature overcome hybrid encoder convolutional generative adversarial network DCGAN architecture variant gan propose encoder generator network gan avoid random initialisation egan output egan feature convolutional neural network DCNNs alexnet densenet evaluate propose dataset performance achieve propose autoencoder gan introduction apply machine technology recognition increase greatly recent advancement technology driver progress convolutional neural network cnns dominant architecture image data cnn traditional machine recognition recognition recognition vehicle detection vehicle surveillance autonomous medical image identification variety quality inspection agricultural detection disease due ability model complex relation data target easily traditional shallow machine implementation cnns recognition employ supervise network parameter optimize data correspond label due amount cnns parameter amount data training otherwise network prone sensitive data distribution training however amount label data expensive effort training cnns unsupervised mention unlabelled data easy expensive feature implementation unsupervised implementation feature illustrate feature architecture feature automatically unlabelled data training model extract feature label data feature input classifier classification implementation feature refer development evaluation feature model image autoencoders generative adversarial network gans apply feature autoencoder network comprises sub network encoder decoder encoder output node greatly input node aim compress data latent variable meanwhile decoder mirror network encoder aim  data dimension autoencoders apply detection disease apply detection epilepsy EEG signal bottleneck feature introduce autoencoders reduce data dimensionality autoencoders limitation autoencoders merely memorize data training prone variant autoencoders propose improve denoising autoencoders variational autoencoder VAE combination meanwhile gans originally generate data blindly presume distribution data comprise network generator discriminator aim generate fake data fool without really distribution data aim discriminate fake image generate training model generate additional data implementation gans propose literature instance generate training data image translation super resolution image predict succeed frame video variant gan propose convolutional gan DCGAN DCGAN cnns replace multi layer perceptrons gan recent gans  feature DCGAN neural network dnn combine feature BiGAN introduce gan latent representation data various implementation gans feature refer unfortunately random input network gan projection unpredictable overcome propose hybrid network encoders gans adopt DCGAN architecture encoder network encoder gan egan encoder latent variable data network gan instead random distribution input latent variable data hence projection predict encoder gan propose previously  BiGAN however difference difference illustrate  accepts input encoder random distribution meanwhile BiGAN input random distribution encoder parallel network output input network meanwhile egan accepts output encoder input network accept output input knowledge data addition architecture introduce nest training scheme training egan dcnn network usually gan feature separately classifier gan network training extract feature label training data classifier egan classifier hence classification feedback egan comparison egan  BiGAN image extension egan apply multi layer perceptron classifier extend input cnn architecture alexnet densenet furthermore evaluate generalization capability egan wider task evaluate egan publicly available datasets  dataset disease detection mnist dataset popular dataset benchmarking classification addition tea clone dataset comparison autoencoder gan BiGAN egan effective datasets remainder organize review convolutional neural network dcnn briefly explain cnn briefly explain gan autoencoder explain propose propose experimental setup described experimental setup explain discussion conclude conclusion review convolutional neural network dcnn briefly explain dcnn supervise typical composition dcnn explain dcnn unsupervised explain autoencoder gan briefly typical structure dcnn dcnn component feature extraction classification feature extractor usually consists stack convolutional layer pool layer sum operation input kernel conduct convolutional layer pool layer summarize output convolutional layer typically maximum average reduce dimensionality input classification usually consists fully layer output feature extractor target label cnn architecture image dcnn unsupervised dcnn architecture network learns automatically important underlie data therefore dcnn feature unlabelled data feature training dcnn model unsupervised extract feature amount label data classifier supervise architecture feature autoencoder gan architecture autoencoder typical autoencoder consists sub network encoder decoder encoder dimension input data  reduce obtain compress feature decoder reconstructs data compress feature data compress feature latent representation data output hugely reduce input encoder learns underlie latent information data relevant discard  information encoder nonlinear principal component analysis data typical architecture autoencoder image gan another employ unsupervised structure gan gan consists network generator discriminator network gans contend generator learns generate image  distribution meanwhile ass generate image fake image assessment reference optimize  distribution generate image closer fool adversarial training cannot distinguish fake image gans cannot input data autoencoder data therefore gans avoid overfitting network however gans sensitive initialization  distribution usually random trajectory unpredicted architecture generative adversarial network image propose scheme feature explain implementation architecture egan feature unlabelled data obtain feature model model extract feature label feature cnn dcnn architecture egan egan hybrid encoder DCGAN encoder generator DCGAN network traditionally network gan accept random distribution usually gaussian input trajectory gan unpredictable purpose encoder avoid architecture propose egan image target distribution data distribution data aim distribution gan minimax function   however actual data trajectory generator sensitive initialization avoid employ encoder introduces distribution data latent variable therefore becomes    random initialisation encoder network predictable instead blindly distribution data network gan prior knowledge data DCGAN backbone gan variant gan employ convolutional layer instead multilayer perceptrons gan DCGAN stable training gan meanwhile encoder built convolutional layer  encoder convolutional layer stride reduce rgb input dimension image dimension optimum encoder observation convolutional layer  encoder convolutional layer effective task performance tends decoder aim underlie latent variable data input gan therefore architecture compress data dimension hence employ encoder decoder reconstruct data dimension supervise dcnn architecture alexnet densenet architecture alexnet densenet respectively alexnet comprises stack convolutional layer fully layer meanwhile densenet deeper alexnet avoid gradient vanish cnn densenet apply multi skip connection information pas layer concatenate width network due intense concatenation operation previous layer densenet memory usage architecture alexnet image architecture densenet image egan dcnn modify gradient optimization apply nest training training egan dcnn classifier epoch egan epoch dcnn sub epoch detailed algorithm training described experimental setup dataset datasets evaluate tea clone  mnist datasets develop tea clone dataset tea clone recognition task  mnist public datasets  contains image leaf various disease whereas mnist handwrite recognition tea clone dataset tea leaf image dataset research institute tea    java indonesia  tea clone   clone series research focus tea clone   camera DSLR camera smartphones intend enrich data variation various camera capture image indoor ignore arrangement distance leaf camera autofocus feature capture image image sample   sample   image penn develops  PV dataset dataset premise growth knowledge accessible openly everyone potato dataset sample sample  dataset image mnist modify  institute standard technology dataset handwritten digit release become benchmarking standard classification sample mnist dataset image mnist dataset consists training image image training image greyscale image sample mnist dataset image setup dataset training validation detailed distribution data datasets summarize egan datasets separately tea clone  datasets dataset training validation meanwhile image mnist consist training image image validation training data data egan dcnn classifier alexnet densenet data distribution capability egan evaluate egan evaluation feature model datasets tea clone  mnist popular classifier alexnet densenet evaluate accuracy classifier accuracy data fairly balance egan feature gan BiGAN autoencoder reference gan architecture BiGAN whereas autoencoder architecture dcnn autoencoder denote cnn AE gan apply training strategy egan cnn AE cnn AE epoch egan parameter egan tune hyper parameter network optimizer adam optimizer apply initialization adam egan adam alexnet densenet batch varied training egan alexnet densenet datasets choice batch due compute capability epoch epoch training egan epoch sub training classifier empirical observation computation intel xeon cpu GB memory nvidia tesla GB gpu architecture developed library kera tensorflow discussion egan feature model summarises performance egan feature extractor dcnn classifier batch varied evaluate datasets datasets satisfactory performance achieve suitable batch subsequent however performance achieve batch apply however due limitation memory densenet implementation memory load implementation implement performance egan batch varied evaluate datasets densenet achieves slightly accuracy alexnet surprising densenet depth width alexnet model nonlinear relation feature target however densenet training parameter alexnet comparison feature performance egan feature feature algorithm rgb without feature gan BiGAN cnn AE egan superior others datasets classifier without pre training average relative improvement achieve rgb gan cnn AE BiGAN respectively interestingly rgb largely gan BiGAN slightly cnn AE gan apply presume distribution data completely data hence mismatch training BiGAN generally gan due random variable input trajectory target propose comparison performance accuracy egan feature feature summarizes  cnn AE BiGAN egan egan considerably gan cnn AE due parameter gan cnn AE furthermore nest training computation per epoch epoch sub epoch classifier meanwhile training egan BiGAN comparable egan superior performance comparison egan feature conclusion propose encoder convolutional generative adversarial network egan unsupervised feature egan encoder gan generator network avoid gan learns completely distribution data training DCGAN architecture encoder convolutional layer egan supervise employ dcnn architecture alexnet densenet addition nest training egan dcnn evaluation datasets mnist datasets confirm propose directly rgb feature popular dcnn classifier largely feature gan cnn AE BiGAN evaluation gan performs directly rgb strongly unpredictable outcome gan due random input layer encoder affect performance layer significant improvement performance tend selection epoch nest training influential performance future evaluate robustness egan implementation encoder variant data preprocessing data augmentation improvement becomes future abbreviation dcnn convolutional neural network DCGAN convolutional generative adversarial network PV 