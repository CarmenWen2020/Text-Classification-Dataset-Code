DL compute paradigm deem standard machine ML community moreover gradually become widely computational approach ML achieve outstanding complex cognitive task beating performance benefit DL ability massive amount data DL grown extensively successfully address traditional application importantly DL outperform ML technique domain cybersecurity processing bioinformatics robotics medical information processing others despite contribute review DL tackle aspect DL overall lack knowledge therefore contribution propose holistic approach suitable develop understand DL specifically review attempt comprehensive survey important aspect DL enhancement recently outline importance DL DL technique network convolutional neural network cnns utilized DL network describes development cnns architecture feature alexnet network closing resolution network HR net finally challenge researcher understand exist research gap DL application computational fpga gpu cpu summarize along description influence DL evolution matrix benchmark datasets summary conclusion introduction recently machine ML become widespread research incorporate variety application text mining spam detection video recommendation image classification multimedia concept retrieval ML algorithm DL commonly employ application another DL representation RL appearance novel distribute due unpredictable growth ability obtain data amaze progress hardware technology performance compute hpc DL derive conventional neural network considerably outperforms predecessor moreover DL employ transformation graph technology simultaneously multi layer model recently developed DL technique obtain outstanding performance across variety application audio processing visual data processing processing nlp others usually effectiveness ML algorithm highly dependent integrity input data representation suitable data representation improve performance data representation significant research trend ML feature engineering inform numerous research approach aim construct feature raw data addition extremely specific frequently sizable effort instance feature introduce computer vision context histogram orient gradient hog invariant feature transform sift bag bow novel feature introduce perform becomes research direction pursue multiple decade relatively feature extraction achieve automatic throughout DL algorithm encourages researcher extract discriminative feature amount effort knowledge algorithm multi layer data representation architecture layer extract feature layer extract feature artificial intelligence AI originally inspire architecture simulates occurs core  within brain scene brain automatically extract data representation specifically output classify scene information input simulates methodology brain emphasizes benefit DL ML DL due considerable currently prominent research trend overview DL adopts various perspective concept architecture challenge application computational evolution matrix convolutional neural network cnn popular DL network cnn DL popular nowadays advantage cnn predecessor automatically detects significant feature without supervision therefore  cnn component furthermore elaborate detail cnn architecture alexnet network resolution network HR net publish DL review however address focus application topic review cnn architecture DL classification disease DL detection DL application medical image analysis etc although review topic understand DL topic concept detailed research gap computational DL application understand DL aspect concept challenge application application achieve extensive research DL research gap application therefore propose review DL suitable develop understand DL review motivation review important aspect DL challenge application computational perspective furthermore review towards DL topic aim review important aspect DL easy researcher image DL review review advance DL research discover recent development researcher suitable direction accurate alternative contribution outline review almost survey important aspect review researcher understand explain cnn popular algorithm concept theory architecture review challenge limitation lack training data imbalanced data interpretability data uncertainty catastrophic forget model compression overfitting vanish gradient explode gradient underspecification additionally discus propose tackle issue exhaustive medical image application categorize task classification registration discus computational approach cpu gpu fpga influence algorithm organize survey methodology describes survey methodology background background classification DL approach defines classification DL approach DL network display DL network cnn architecture cnn architecture challenge limitation alternate detail challenge DL alternate application outline application DL computational approach explains influence computational approach cpu gpu fpga DL evaluation metric evaluation metric framework datasets framework datasets summary conclusion summary conclusion survey methodology review significant research publish mainly focus repute publisher elsevier  acm springer arxiv review various DL topic indicates review focus publication DL analyze review define DL approach network explain cnn architecture challenge DL alternate ass application DL ass computational approach keywords criterion review machine convolution neural network architecture image detection classification segmentation localization detection classification segmentation localization cpu gpu fpga transfer imbalanced data interpretability data overfitting underspecification structure survey detail journal cite review framework image journal cite review background background DL introduction DL difference DL ML situation DL finally apply DL DL subset ML inspire information processing brain DL amount data input specific label DL numerous layer algorithm artificial neural network anns interpretation data fed image achieve classification task conventional ML technique sequential specifically pre processing feature extraction wise feature selection classification furthermore feature selection impact performance ML technique bias feature selection incorrect discrimination conversely DL ability automate feature task unlike conventional ML DL enables classification achieve shot DL become incredibly popular ML algorithm recent due growth evolution data continuous development regard novel performance ML task simplify improvement image super resolution detection image recognition recently DL performance exceed performance task image classification difference traditional machine image performance image nearly scientific impact technology business already disrupt transform DL technology economy focus around improve DL performance capability cannot exceed performance DL predict delivery decision certify loan request predict movie rating winner  prize compute turing award pioneer DL  lecun  hinton   although goal achieve progress DL context DL ability enhance additional accuracy diagnosis estimate disaster discovery drug cancer diagnosis DL network ability diagnose disease certify  image disease furthermore grade prostate cancer certify pathologist achieve average accuracy google AI outperform specialist achieve average accuracy DL increasingly vital role diagnosis novel coronavirus covid DL become hospital around automatic covid classification detection chest ray image image AI pioneer  hinton everything apply machine intelligence useful situation expert meaning DL expert available unable explain decision expertise understand medical decision recognition update price prediction stock preference prediction adaptation specific personalization biometrics extremely exceeds inadequate ability sentiment analysis facebook calculation webpage rank performance feature universal approach DL ability perform approximately application domain sometimes refer universal robustness precisely feature DL technique instead optimize feature automate fashion related task consideration robustness input data attain generalization data application DL technique approach frequently refer transfer TL explain latter furthermore useful approach data insufficient scalability DL highly scalable resnet microsoft comprises layer frequently apply supercomputing lawrence  national laboratory LLNL enterprise evolve framework network adopt approach node implement classification DL approach DL technique classify category unsupervised partially supervise semi supervise supervise furthermore reinforcement DRL RL another technique mostly category partially supervise occasionally unsupervised technique supervise technique label data technique  collection input resultant output instance smart agent input obtain loss network parameter repeatedly update agent obtain improve estimate prefer output positive training outcome agent acquires ability obtain query  DL supervise technique recurrent neural network rnns convolutional neural network cnns neural network dnns addition rnn category gate recurrent grus memory lstm approach advantage technique ability data generate data output prior knowledge however disadvantage technique decision boundary  training sample overall technique simpler technique performance semi supervise technique semi label datasets occasionally generative adversarial network gans DRL employ technique addition rnns grus lstms employ partially supervise advantage technique minimize amount label data disadvantage technique irrelevant input feature training data furnish incorrect decision text document classifier popular application semi supervise due difficulty obtain amount label text document semi supervise ideal text document classification task unsupervised technique implement absence available label data label agent learns significant feature interior representation discover unidentified structure relationship input data technique generative network dimensionality reduction cluster frequently within category unsupervised member DL perform non linear dimensionality reduction cluster task restrict boltzmann machine auto encoders gans recently developed technique moreover rnns grus lstm approach employ unsupervised application disadvantage unsupervised unable accurate information concern data sort computationally complex popular unsupervised approach cluster reinforcement reinforcement operates interact environment supervise operates sample data technique developed google subsequently enhance technique dependent reinforcement construct input environment sample agent predict agent unknown probability distribution environment asks agent noisy sometimes refer semi supervise concept supervise unsupervised technique developed comparison traditional supervise technique perform straightforward loss function available reinforcement technique addition essential difference supervise reinforcement access function optimization meaning query via interaction interact environment input precede action task selection reinforcement perform scope DRL involve parameter optimize contrast derivative reinforcement technique performs limited parameter application reinforcement business strategy planning robotics industrial automation drawback reinforcement parameter influence motivation utilize reinforcement assist identify action reward longer assist discover situation action enables approach reward reinforcement agent reward function reinforcement utilize situation sufficient data resolve issue supervise technique reinforcement compute consume specially workspace DL network network recursive neural network  rnns cnns  rnns briefly explain cnns explain due importance furthermore application network recursive neural network RvNN achieve prediction hierarchical structure classify output utilize compositional vector recursive auto associative memory  primary inspiration RvNN development RvNN architecture generate processing randomly structure graph approach generates fix width distribute representation variable recursive data structure network introduce propagation structure bts bts technique propagation algorithm ability treelike structure auto association network regenerate input layer output layer RvNN highly effective nlp context introduce RvNN architecture input variety modality author demonstrate application classify split image image various RvNN computes likely merge construct syntactic furthermore RvNN calculates related merge plausibility merge within composition vector merge RvNN generates numerous compositional vector label instance become label compositional vector entire RvNN structure RvNN RvNN employ application RvNN image recurrent neural network rnns commonly employ familiar algorithm discipline DL rnn mainly apply processing nlp context unlike conventional network rnn sequential data network embed structure sequence data delivers valuable information feature fundamental application instance important understand context meaning specific rnn memory input layer output layer hidden layer input sequence typical unfolded rnn diagram illustrate introduce rnn technique namely hidden hidden hidden output input hidden rnn introduce lessens difficulty network brings benefit deeper rnn technique typical unfolded rnn diagram image however rnn sensitivity explode gradient vanish issue approach specifically training  derivative gradient exponentially explode decay entrance input network initial therefore sensitivity decay furthermore issue handle lstm approach recurrent connection memory network memory contains memory ability temporal network addition contains gate information network residual connection ability considerably reduce impact vanish gradient issue explain later cnn powerful rnn rnn feature compatibility cnn convolutional neural network DL cnn commonly employ algorithm benefit cnn predecessor automatically identifies relevant feature without supervision cnns extensively apply computer vision processing recognition etc structure cnns inspire neuron brain conventional neural network specifically brain complex sequence visual cortex sequence simulated cnn identify benefit cnn equivalent representation sparse interaction parameter unlike conventional fully FC network local connection cnn employ 2D input data structure image signal operation utilizes extremely parameter simplifies training network visual cortex notably scene scene spatially extract local correlation available input local filter input commonly cnn multi layer perceptron mlp consists numerous convolution layer precede sub sample pool layer layer FC layer cnn architecture image classification illustrate cnn architecture image classification image input layer cnn model organize dimension height width depth height width depth refer channel rgb image depth kernel filter available convolutional layer denote dimension input image however addition kernel basis local connection parameter bias generate feature convolve input mention convolution layer calculates dot input nlp input  initial image apply nonlinearity activation function convolution layer output obtain sample feature sub sample layer reduction network parameter accelerates training enables handle overfitting issue feature pool function max average apply adjacent kernel finally FC layer mid feature abstraction stage layer typical neural network classification generate layer vector machine SVMs softmax instance probability specific benefit employ cnns benefit cnns traditional neural network computer vision environment cnn feature reduces trainable network parameter network enhance generalization avoid overfitting concurrently feature extraction layer classification layer model output highly organize highly reliant extract feature network implementation easy cnn neural network cnn layer cnn architecture consists layer multi building layer cnn architecture function described detail convolutional layer cnn architecture significant component convolutional layer consists collection convolutional filter kernel input image express dimensional metric convolve filter generate output feature kernel definition grid discrete describes kernel kernel random assign kernel cnn training addition initialize adjust training era kernel learns extract significant feature convolutional operation cnn input format described vector format input traditional neural network multi channel image input cnn instance channel format image rgb image format channel understand convolutional operation image random initialize kernel kernel slide image horizontally vertically addition dot input image kernel correspond sum scalar calculate concurrently slide calculate dot feature output graphically illustrates primary calculation execute kernel input image sum marked orange entry output feature primary calculation execute convolutional layer image however pad input image apply previous stride denote vertical horizontal location apply kernel another stride addition feature dimension obtain increase stride pad highly significant border information related input image contrast border feature away apply pad input image increase output feature increase core benefit convolutional layer sparse connectivity neuron layer FC neural network link neuron layer contrast cnns available adjacent layer connection memory hence approach memory effective addition matrix operation computationally costly dot operation cnn allocate neuron layer cnn pixel input matrix input significantly decrease training various additional neuron pool layer task pool layer sub sample feature generate convolutional operation approach shrink feature feature concurrently maintains majority dominant information feature pool stage manner convolutional operation stride kernel assign pool operation execute pool available utilization various pool layer pool gate pool average pool min pool max pool global average pool gap global max pool familiar frequently utilized pool max min gap pool illustrates pool operation pool operation image sometimes overall cnn performance decrease shortfall pool layer layer cnn feature available input image focus exclusively ascertain location feature cnn model relevant information activation function non linearity mapping input output core function activation function neural network input compute summation neuron input along bias activation function decision neuron reference input correspond output non linear activation layer employ layer learnable layer FC layer convolutional layer cnn architecture non linear performance activation layer mapping input output non linear moreover layer cnn ability extra complicate activation function ability differentiate extremely significant feature allows error propagation network activation function commonly cnn neural network sigmoid input activation function output restrict zero sigmoid function curve mathematically  tanh sigmoid function input output restrict mathematical representation   relu mostly commonly function cnn context convert input positive computational load benefit relu others mathematical representation   occasionally significant issue relu instance error propagation algorithm gradient passing gradient within relu function update neuron certainly activate issue refer relu relu alternative exist issue discus leaky relu instead relu negative input activation function ensures input ignore employ relu leaky relu mathematically   leak factor denote commonly noisy relu function employ gaussian distribution relu noisy mathematically    parametric linear mostly leaky relu difference leak factor function update model training parametric linear mathematically   learnable denote fully layer commonly layer cnn architecture inside layer neuron neuron previous layer fully FC approach utilized cnn classifier conventional multiple layer perceptron neural network ann input FC layer pool convolutional layer input vector feature flatten output FC layer cnn output illustrate fully layer image loss function previous various layer cnn architecture addition classification achieve output layer layer cnn architecture loss function utilized output layer calculate predict error across training sample cnn model error reveals difference actual output predict optimize cnn however parameter loss function calculate error cnn estimate output refer prediction parameter actual output refer label parameter loss function employ various concisely explains loss function entropy softmax loss function function commonly employ cnn model performance refer loss function output probability addition usually employ substitution error loss function multi classification output layer employ softmax activation generate output within probability distribution mathematical representation output probability    non normalize output precede layer neuron output layer finally mathematical representation entropy loss function   euclidean loss function function widely regression addition error mathematical expression estimate euclidean loss hinge loss function function commonly employ related binary classification relates maximum margin classification mostly important SVMs hinge loss function wherein optimizer attempt maximize margin around dual objective mathematical formula  margin commonly moreover predict output denote desire output denote regularization cnn cnn model fitting central issue associate obtain behave generalization model entitle model executes training data succeed data unseen data explain latter model occurs model sufficient amount training data model refer executes training data illustrate various intuitive concept regularization avoid fitting detail fitting fitting latter dropout widely utilized technique generalization training epoch neuron randomly feature selection distribute equally across neuron model independent feature training neuron propagation propagation contrast network utilized perform prediction highly dropout training epoch connection neuron neuron difference dropout data augmentation training model sizeable amount data easy avoid fitting achieve data augmentation technique utilized artificially expand training dataset detail latter describes data augmentation technique batch normalization ensures performance output activation performance gaussian distribution standard deviation normalize output layer pre processing task layer network differentiate integrate network addition employ reduce internal covariance shift activation layer layer variation activation distribution defines internal covariance shift shift becomes due continuous update training sample training data numerous dissimilar source image model consume extra convergence training increase resolve issue layer operation batch normalization apply cnn architecture advantage utilize batch normalization prevents vanish gradient arise effectively initialization significantly reduces network convergence datasets extremely useful struggle decrease training dependency across hyper parameter fitting reduce minor influence regularization fitting fitting issue image optimizer selection discus cnn issue issue algorithm selection optimizer issue enhancement adadelta adagrad momentum along algorithm enhance output loss function numerous learnable parameter bias etc minimize error variation actual predict output core purpose supervise algorithm technique gradient cnn network selection network parameter update training epoch network locally optimize training epoch minimize error rate define parameter update training epoch repetition parameter update involves training dataset rate wisely influence  although hyper parameter gradient descent gradient algorithm minimize training error algorithm repetitively update network parameter training epoch specifically update parameter correctly compute objective function gradient slope apply derivative respect network parameter parameter update reverse direction gradient reduce error parameter update perform network propagation gradient neuron propagate neuron precede layer mathematical representation operation      training epoch denote  precede training epoch denote  rate prediction error alternative gradient algorithm available commonly employ batch gradient descent execution technique network parameter update merely training datasets via network depth calculates gradient training subsequently gradient update parameter dataset cnn model converges faster creates extra stable gradient bgd parameter training epoch substantial amount resource contrast training dataset additional converge converge local optimum non convex instance stochastic gradient descent parameter update training sample technique prefer arbitrarily sample training sample epoch advance training training dataset technique memory effective faster bgd however frequently update extremely noisy direction convergence behavior become highly unstable mini batch gradient descent approach training sample partition mini batch mini batch collection sample overlap parameter update perform gradient computation mini batch advantage combine advantage bgd sgd technique steady convergence computational efficiency extra memory effectiveness describes enhancement technique gradient algorithm usually sgd  enhance cnn training momentum neural network technique employ objective function enhances accuracy training sum compute gradient precede training via factor momentum factor however therefore simply becomes stuck local minimum global minimum disadvantage gradient algorithm issue frequently issue convex algorithm momentum issue express mathematically    increment training epoch denote  rate increment precede training epoch momentum factor maintain within update increase direction bare minimum minimize error momentum factor becomes model loses ability avoid local bare minimum contrast momentum factor becomes model develops ability converge rapidly momentum factor LR model global bare minimum however gradient varies direction continually throughout training suitable momentum factor hyper parameter smoothen update variation adaptive estimation adam another optimization technique algorithm widely adam trend optimization hessian matrix employ derivative adam strategy specifically training neural network memory efficient computational advantage adam mechanism adam calculate adaptive LR parameter model integrates pro momentum RMSprop utilizes gradient rate RMSprop momentum average gradient equation adam   algorithm backpropagation notation refers network unambiguously denote  connection ith input neuron neuron hth layer connection neuron layer another neuron layer network mlp structure image neuron layer neuron layer neuron neuron previous layer layer layer net regard bias bias connection neuron layer easily handle neuron bias network layer bias net layer bias network parameter layer net neuron layer connection layer connection easily neuron layer input fully neuron layer connection connection error define update layer neural network error label  input ith output individual input backpropagation understand bias network function error ultimately compute partial derivative   compute local variable introduce local error 𝑗th neuron  layer local error backpropagation procedure compute   error define update layer neural network neuron activation function image output error neuron output error epoch   activation function output backpropagate error layer output   output error layer error obtain error neuron layer update layer improve performance cnn DL application conclude active improve performance cnn expand dataset data augmentation transfer explain latter increase training increase depth width model regularization increase hyperparameters tune cnn architecture cnn architecture model architecture critical factor improve performance application various modification achieve cnn architecture modification structural reformulation regularization parameter optimization etc conversely upgrade cnn performance largely due processing reorganization development novel novel development cnn architecture perform network depth review popular cnn architecture alexnet model resolution HR model architecture feature input depth robustness researcher suitable architecture target task brief overview cnn architecture brief overview cnn architecture alexnet cnns appearance lenet cnns restrict handwritten digit recognition task cannot image cnn architecture alexnet highly respect achieve innovative image recognition classification propose alexnet consequently improve cnn ability increase depth implement parameter optimization strategy illustrates alexnet architecture architecture lenet image architecture alexnet image ability cnn limited due hardware restriction overcome hardware limitation gpus nvidia gtx parallel alexnet moreover enhance applicability cnn image category feature extraction stage increase lenet alexnet regardless depth enhances generalization image resolution overfitting drawback related depth hinton address ensure feature algorithm extra robust algorithm randomly transformational throughout training stage moreover reduce vanish gradient relu utilized non saturate activation function enhance rate convergence local response normalization overlap subsampling perform enhance generalization decrease overfitting improve performance previous network modification filter earlier layer alexnet considerable significance recent cnn generation innovative research era cnn application network network network model slight difference precede model introduce innovative concept employ multiple layer perception convolution convolution execute filter addition extra nonlinearity network moreover enlarge network depth later regularize dropout DL model frequently employ bottleneck layer substitution FC layer gap employ novel concept enables significant reduction model parameter addition gap considerably update network architecture generate dimensional feature vector reduction feature dimension gap feature structure network architecture network network image  cnn mechanism basically construct trial error basis preclude understand precise purpose enhancement issue restrict cnn performance convolute image response  fergus introduce  multilayer convolutional neural network later become  developed  visualize network monitoring cnn performance via understand neuron activation purpose network activity visualization however utilized concept optimize belief network DBN performance visualize feature hidden layer moreover addition issue assess unsupervised auto encoder AE performance visualize image output neuron reverse operation convolutional pool layer  operates pas cnn reverse mapping launch convolutional layer output backward visually observable image accordingly neural interpretation internal feature representation layer monitoring schematic training stage concept underlie  addition utilized outcome recognize ability issue couple model concept experimentally proven alexnet apply  neuron others action layer network furthermore feature extract via layer aliasing  fergus cnn topology due existence outcome addition execute parameter optimization exploit cnn decrease stride filter retain feature initial convolutional layer improvement performance accordingly achieve due rearrangement cnn topology rearrangement propose visualization feature employ identify weakness conduct appropriate parameter alteration structure network architecture  image visual geometry vgg cnn effective image recognition easy efficient principle cnn propose simonyan zisserman innovative visual geometry vgg multilayer model feature nineteen layer  alexnet simulate relation network representational capacity depth conversely ILSVRC competition  frontier network propose filter enhance cnn performance reference vgg insert layer heap filter filter  experimentally parallel assignment filter influence filter filter receptive similarly efficient filter decrease parameter extra advantage reduce computational complication achieve filter outcome establish novel research trend filter cnn addition insert convolution convolutional layer vgg regulates network complexity learns linear subsequent feature respect network tune max pool layer insert convolutional layer pad implement maintain spatial resolution vgg obtain significant localization image classification achieve ILSVRC competition acquire reputation due enlarge depth homogenous topology simplicity however vgg computational excessive due utilization around parameter shortcoming structure network architecture vgg image googlenet ILSVRC competition googlenet inception emerge winner achieve accuracy decrease computational core aim googlenet architecture propose novel inception module concept cnn context combine multiple convolutional transformation employ merge transform split function feature extraction illustrates inception architecture architecture incorporates filter capture channel information spatial information diverse spatial resolution convolutional layer googlenet substitute concept network network nin architecture replace layer micro neural network googlenet concept merge transform split utilized attend issue correlate variant exist image motivation googlenet improve efficiency cnn parameter enhance capacity addition regulates computation insert convolutional filter bottleneck layer ahead kernel googlenet employ sparse connection overcome redundant information decrease neglect irrelevant channel input channel output channel employ gap layer layer utilize FC layer density connection decrease parameter significantly decrease parameter due parameter tuning additional regularity factor employment  optimizer batch normalization furthermore googlenet propose auxiliary learner rate convergence conversely shortcoming googlenet heterogeneous topology shortcoming adaptation module another shortcoming googlenet representation jam substantially decrease feature layer occasionally valuable information loss structure google image highway network increase network depth enhances performance mainly complicate task contrast network training becomes presence layer deeper network gradient propagation error layer novel cnn architecture highway network overcome issue approach connectivity concept  information highway network empower instruct gate inside layer gate mechanism concept motivate lstm rnn information aggregation conduct merge information ith layer ith layer generate regularization impact gradient training deeper network empowers training network layer deeper network layer sgd algorithm highway network depth fifty layer improve rate convergence architecture contrast empirically demonstrate net performance decline hidden layer insert highway network layer depth converges rapidly network resnet developed resnet residual network winner ILSVRC objective ultra network vanish gradient issue previous network resnet developed layer layer layer resnet comprise convolutional layer plus FC layer overall network overall MACs novel resnet bypass pathway concept employ highway net address training deeper network illustrate contains fundamental resnet diagram conventional feedforward network plus residual connection residual layer output identify output deliver precede layer execute operation convolution variable filter batch normalization apply activation function relu output residual output mathematically numerous residual residual network residual network architecture operation residual diagram resnet image comparison highway network resnet shortcut connection inside layer enable layer connectivity parameter data independent layer characterize non residual function gate shortcut highway network contrast individuality shortcut residual information permanently resnet furthermore resnet potential prevent gradient diminish shortcut connection residual link accelerate network convergence resnet winner ILSVRC championship layer depth depth vgg depth alexnet comparison vgg computational complexity enlarge depth inception resnet inception propose inception resnet inception upgraded inception concept inception minimize computational deeper network generalization asymmetric filter filter moreover utilized bottleneck convolution prior filter operation traditional convolution channel correlation previously utilized filter potential nin architecture subsequently utilized intelligent manner convolutional operation inception input data mapped isolated initial input correlation mapped convolution contrast inception resnet inception residual replace filter concatenation residual connection empirically demonstrate inception resnet inception residual connection achieve generalization inception enlarge width depth without residual connection clearly illustrate residual connection training significantly accelerate inception network training diagram inception residual diagram inception residual image densenet vanish gradient densenet direction resnet highway network drawback resnet clearly conserve information  individuality transformation layer contribute extremely information addition resnet layer isolated densenet employ layer connectivity improve approach address layer layer network approach therefore feature previous layer employ input layer traditional cnns connection previous layer layer densenet connection densenet demonstrates influence layer depth wise convolution network gain ability discriminate clearly preserve information densenet concatenates feature precede layer however due narrow layer structure densenet becomes  price addition increase feature admission layer gradient via loss function enhances information across network addition regularize impact minimizes overfitting task alongside minor training architecture densenet network adopt architecture densenet network image resnext resnext enhance version inception network aggregate residual transform network cardinality utilized split transform merge topology easy effective denotes transformation extra dimension however inception network manages network resource efficiently enhance ability conventional cnn transformation spatial embeddings employ customize layer separately contrast resnext derives characteristic feature resnet vgg inception employ vgg homogenous topology architecture googlenet filter spatial resolution inside split transform merge resnext building resnext utilized multi transformation inside split transform merge outline transformation cardinality performance significantly improve increase cardinality complexity resnext regulate employ filter embeddings ahead convolution contrast skip connection optimize training diagram resnext building image  feature reuse core shortcoming related residual network feature transformation contribute amount  komodakis accordingly propose  address author advise depth supplemental influence residual convey core ability residual network  utilized residual via resnet wider instead deeper enlarge width extra factor handle network width layer widen highly successful performance enhancement deepen residual network enhance representational capacity achieve residual network network drawback explode vanish gradient feature reuse inactivation feature intensive training tackle feature reuse dropout residual regularize network efficient manner manner utilize dropout stochastic depth concept gradient vanish earlier research focus increase depth enhancement performance addition layer parameter  twice resnet experimental contrast  improve training relative network architecture prior residual network highly effective vgg inception wider resnet wider residual network establish however insert dropout convolutional layer oppose within residual effective  pyramidal net depth feature increase succeed layer due stack multi convolutional layer previous cnn architecture resnet vgg alexnet contrast spatial dimension reduces sub sample convolutional layer augment feature representation  decrease feature extreme expansion depth feature alongside spatial information loss interferes ability cnns resnet obtain notable outcome issue image classification conversely delete convolutional channel spatial dimension channel depth enlarges spatial dimension reduces commonly decrease classifier performance accordingly stochastic resnet enhance performance decrease information loss accompany residual propose pyramidal net address resnet interference address depth enlargement extreme reduction spatial width via resnet pyramidal net slowly enlarges residual width feasible spatial dimension inside residual appearance sample refer pyramidal net due enlargement feature depth factor regulates depth feature  dimension lth residual moreover indicates overall residual factor depth increase regulate factor uniformly distributes increase across dimension feature zero pad identity mapping insert residual connection layer comparison projection shortcut connection zero pad identity mapping parameter enhance generalization multiplication addition widen approach pyramidal net network widen specifically approach multiplication enlarges geometrically addition enlarges linearly associate width enlargement growth related quadratic xception extreme inception architecture characteristic xception xception depthwise separable convolution xception model adjust inception wider exchange dimension convolution reduce computational complexity xception architecture xception network becomes extra computationally effective decouple channel spatial correspondence moreover performs mapping convolve output embed dimension apply convolution performs spatial transformation width define cardinality obtain via transformation xception however computation simpler xception distinctly convolve channel around spatial subsequently convolution pointwise convolution perform channel correspondence convolution utilized xception regularize depth channel traditional convolutional operation xception utilizes transformation equivalent channel inception moreover utilizes transformation traditional cnn architecture utilizes transformation conversely xception transformation approach achieves extra efficiency performance minimize parameter diagram xception architecture image residual attention neural network improve network feature representation propose residual attention network enable network aware feature purpose incorporate attention cnn consists stack residual addition attention module hence cnn however attention module namely mask trunk adopt strategy respectively encapsulate strategy attention model attention feedback processing specifically architecture generates dense feature inference aspect moreover feedforward architecture generates resolution feature addition robust semantic information restrict boltzmann machine employ strategy previously propose training reconstruction phase mechanism attention boltzmann machine DBMS regularize factor network globally optimize strategy manner progressively output input throughout incorporate attention concept convolutional easy transformation network obtain previous unfortunately inflexible along inability surroundings contrast stack multi attention module effective recognize noisy complex clutter image hierarchical organization capability adaptively allocate feature importance within layer furthermore incorporate distinct attention spatial channel mixed enables model ability capture aware feature distinct convolutional attention module importance feature utilization attention mechanism certify via SE network convolutional attention CBAM module novel attention cnn developed module SE network SE network disregard spatial locality image considers channel contribution image classification regard detection spatial location significant role convolutional attention module sequentially infers attention specifically applies channel attention precede spatial attention obtain refine feature spatial attention perform convolution pool function literature generate effective feature descriptor achieve spatial axis along pool feature addition generate robust spatial attention CBAM concatenates max pool average pool operation manner collection gap max pool operation model feature statistic demonstrate utilize gap return sub optimize inference channel attention whereas max pool indication distinguish feature utilization max pool average pool enhances network representational feature improve representational facilitate focus significant portion chosen feature expression 3D attention serial procedure assist decrease computational parameter experimentally cnn architecture simply integrate CBAM concurrent spatial channel excitation mechanism valid segmentation task expand effort influence spatial information channel information module channel squeeze excitation concurrent channel  spatially squeeze channel wise SSE channel wise squeeze spatially cse segmentation purpose employ auto encoder cnns addition insert module encoder decoder layer specifically highlight specific feature allocate attention channel express factor channel spatial information module  module SSE feature information importance spatial locality spatial information significant role segmentation therefore channel collection spatially developed employ segmentation module cse SE concept furthermore factor derive contribution feature within detection  cnn efficient technique detect feature achieve behave recognition performance comparison innovative handcraft feature detector restriction related cnn meaning cnn relation orientation perspective feature instance image cnn various component etc incorrectly activate cnn neuron recognize without specific relation orientation etc account neuron probability addition feature orientation perspective etc specific neuron capsule ability effectively detect along information layer capsule node construct capsule network encode contains layer capsule node  capsnet initial version capsule network mnist architecture comprises image apply filter stride output plus feature output input capsule layer 8D vector scalar modify convolution layer stride filter employ convolution layer dimension output initial capsule employ filter generate neuron neuron capsnet encode decode cnn context max pool layer frequently employ handle translation detect feature feature within max pool approach ability detect overlap feature highly significant detection segmentation operation capsule involves feature sum precede layer capsnet encode decode image conventional cnns function employ evaluate global error grows throughout training conversely activation neuron neuron zero instead function repetitive dynamic rout alongside agreement signal feature parameter detail architecture mnist recognize handwritten digit innovative cnn architecture superior accuracy application perspective architecture extra suitability segmentation detection approach classification approach resolution network HRNet resolution representation sensitive vision task semantic segmentation detection estimation date framework input image encode resolution representation subnetwork construct series resolution convolution VGGNet resnet resolution representation recover become resolution alternatively resolution representation maintain entire novel network refer resolution network HRNet network principal feature convolution series resolution parallel information across resolution repeatedly exchange advantage achieve representation accurate spatial domain extra semantic domain moreover HRNet application detection semantic segmentation prediction computer vision HRNet robust backbone illustrates architecture HRNet architecture HRNet image challenge limitation alternate employ DL difficulty consideration challenge alternative accordingly training data DL extremely data hungry involves representation DL demand extensively amount data achieve behave performance model data increase extra behave performance model achieve available data sufficient obtain performance model however sometimes shortage data DL directly properly address issue available involves employment transfer concept data task transfer data directly augment actual data enhance input representation data mapping function model performance boost another technique involves employ model task tune layer layer limited data refer review transfer technique apply DL approach data augmentation perform task helpful augment image data image translation mirror rotation commonly image label conversely important apply technique bioinformatics data instance mirror enzyme sequence output data actual enzyme sequence simulated data increase volume training occasionally simulator physical issue understood therefore involve simulation data processing data requirement DL simulation obtain ref performance DL regard amount data image transfer recent research reveal widespread cnns classification generally cnn model sizable volume data obtain performance challenge associate model concern lack training data indeed gathering volume data exhaust successful available  dataset therefore currently TL technique highly efficient address lack training data issue mechanism TL involves training cnn model volume data model tune training request dataset teacher relationship suitable approach clarify TL gathering detailed knowledge teacher convey information within lecture series simply teacher transfer information detail expert teacher transfer knowledge information learner similarly DL network vast volume data learns bias training transfer network retrain novel model novel model enable pre training scratch illustrates conceptual diagram TL technique pre model cnn model alexnet googlenet resnet datasets imagenet image recognition purpose model employ recognize task without scratch furthermore remain apart feature data sample lack model useful employ pre model training model sizeable datasets price computational training model consume multiple finally pre model assist network generalization convergence research pre model training DL approach massive image obtain performance challenge circumstance achieve excellent outcome image classification recognition application performance occasionally superior becomes convolutional neural network DCNNs layer amount data available however avoid overfitting application sizable datasets properly generalize dcnn model training dcnn model dataset limit however accuracy model becomes insufficient utilized model layer dataset training due fitting due ability utilize hierarchical feature sizable datasets model layer accuracy acquire sufficient training data DL model medical image environmental gathering label datasets costly moreover majority crowdsourcing worker unable accurate medical biological image due lack medical biological knowledge ML researcher rely expert label image however costly consume therefore volume label develop flourish network unfeasible recently TL widely employ address later issue nevertheless although TL enhances accuracy task recognition computer vision essential issue related source data TL target dataset instance enhance medical image classification performance cnn model achieve training model imagenet dataset contains image however image completely dissimilar raw medical image meaning model performance enhance proven TL domain significantly affect performance medical image task lightweight model scratch perform nearly standard imagenet transfer model therefore exists scenario pre model become affordable researcher utilized domain TL achieve excellent domain TL approach image target dataset training ray image chest disease model tune training chest ray image covid diagnosis detail domain TL implement tune conceptual diagram TL technique image data augmentation technique goal increase amount available data avoid overfitting issue data augmentation technique technique data limited data data augmentation incorporates collection improve attribute training datasets DL network perform technique employ data augmentation alternate flip flip vertical axis flip horizontal flip verify valuable datasets imagenet cifar moreover highly implement addition label conserve transformation datasets involve text recognition SVHN mnist encode digital image data commonly dimension tensor    accomplish augmentation channel alternative technique extremely workable implementation easy augmentation involves channel rapidly convert image channel achieve matrix insert additional zero remain channel furthermore increase decrease image brightness achieve straightforward matrix operation easily manipulate rgb derive histogram describes image additional improve augmentation obtain alteration adjust intensity histogram employ photo edit application dominant patch image technique employ combine dimension height width specific processing image data furthermore random employ impact translation difference translation random translation conserve spatial dimension image random reduces input accord reduction threshold label preserve transformation address rotation rotate image within around axis rotation augmentation obtain rotation parameter greatly determines suitability rotation augmentation digit recognition task rotation helpful contrast data label cannot preserve transformation rotation increase translation avoid positional bias within image data useful transformation shift image instance dataset image moreover dataset entirely image model translate initial image direction residual gaussian random constant spatial dimension image augmentation preserve pad injection approach involves inject matrix arbitrary matrix commonly obtain gaussian distribution moreno employ datasets injection datasets uci repository inject within image enables cnn additional robust feature however highly behave positional bias available within training data achieve geometric transformation distribution data training data prospective source bias exist instance completely within frame facial recognition datasets positional bias emerges geometric translation geometric translation helpful due simplicity implementation effective capability disable positional bias library image processing available enables operation rotation horizontal flip additional training computational additional memory shortcoming geometric transformation furthermore geometric transformation arbitrary translation manually ensure image label finally bias data training data complicate transitional positional hence trivial geometric transformation suitable apply imbalanced data commonly biological data tend imbalanced negative sample numerous positive covid positive ray image volume normal ray image undesirable training DL model imbalanced data technique issue employ criterion evaluate loss prediction imbalanced data model perform model employ curve auc resultant loss criterion employ entropy loss ensures model perform prefers employ entropy loss simultaneously model training sample sample finally data balance ref construct model hierarchical biological frequently hierarchical label however imbalanced data performance DL model comprehensively investigate addition lessen frequently technique nevertheless technique specify biological interpretability data occasionally DL technique analyze interpretable interpret DL obtain valuable motif recognize network bioinformatics task disease diagnosis disease diagnosis prediction DL model enhance  prediction outcome model decision verification achieve importance portion within propagation technique perturbation approach perturbation approach portion input model output concept computational complexity understand importance various input portion signal output propagates input layer propagation technique technique proven valuable scenario various meaning model interpretability uncertainty commonly prediction label label employ DL technique achieve prediction confidence inquiry model desire confidence define confident model prediction confidence prevents belief unreliable mislead prediction significant attribute regardless application scenario biology confidence reduces resource expend outcome mislead prediction generally healthcare application uncertainty frequently significant evaluate automate clinical decision reliability machine disease diagnosis overconfident prediction output DL model probability achieve softmax output DL softmax output achieve reliable probability output probability technique introduce bayesian binning quantiles   regression histogram binning   specifically DL technique recently introduce achieves superior performance technique catastrophic forget define incorporate information DL model interfere information instance model classify introduce model tune performance become unsuccessful logical data continually renew highly typical scenario biology address issue involves employ data entirely model scratch consume computationally intensive furthermore unstable representation initial data ML technique catastrophic forget available brain neurophysiological theory technique regularization  technique employ rehearsal training technique dynamic neural network architecture  finally technique dual memory refer gain detail model compression obtain model employ productively DL model intensive memory computational requirement due complexity parameter characterize data intensive healthcare environmental reduce deployment DL limited computational machine mainly healthcare numerous assess health data heterogeneity become complicate vastly issue additional computation furthermore novel hardware parallel processing FPGAs gpus developed computation issue associate DL recently numerous technique compress DL model decrease computational issue model introduce technique classify redundant parameter significant impact model performance reduce compression parameter prune model distil knowledge compact model knowledge distillation compact convolution filter reduce parameter information parameter estimate preservation rank factorization model compression representative technique comprehensive discussion topic overfitting DL model excessively possibility data overfitting training stage due vast parameter involve correlate complex manner situation reduce model ability achieve performance data limited specific involves task therefore propose DL technique fully accurately handle DL imply bias training enables model overcome crucial overfitting recent develop technique handle overfitting investigation available DL algorithm overfitting categorize model architecture model parameter familiar approach decay batch normalization dropout DL default technique decay extensively almost ML algorithm universal regularizer model input data corruption data augmentation overfitting lack training data distribution mirror distribution data augmentation enlarges training data contrast marginalize data corruption improves exclusive augment data model output recently propose technique penalizes confident output regularize model technique demonstrate ability regularize rnns cnns vanish gradient backpropagation gradient technique along anns largely training stage vanish gradient arises specifically training iteration neural network update proportionally relative partial derivative error function however update due  gradient extra training neural network completely conversely similarly activation function sigmoid function shrink input input derivative sigmoid function due variation input variation output shallow network layer activation significant issue layer gradient become training stage network efficiently propagation technique gradient neural network technique determines network derivative layer reverse direction layer progress layer involves derivative layer network manner instance derivative hidden layer employ activation function sigmoid function hence gradient decline exponentially propagate layer specifically bias layer cannot update efficiently training stage gradient moreover decrease overall network accuracy layer frequently critical recognize essential input data however avoid employ activation function function lack  ability  input within mapping max relu popular selection yield derivative employ another involves employ batch normalization layer mention earlier occurs input squash vanish derivative employ batch normalization degrades issue simply normalize input expression accomplish exterior boundary sigmoid function normalization ensures derivative action furthermore faster hardware tackle previous issue gpus standard propagation deeper layer network recognize vanish gradient explode gradient vanish related gradient specifically error gradient accumulate propagation latter extremely significant update network meaning becomes unsteady model lose ability effectively   backward network propagation gradient grows exponentially repetitively gradient become incredibly overflow become nan potential regularization technique redesign architecture network model underspecification computer scientist google identify challenge underspecification ML model DL model surprisingly behavior application computer vision medical image processing medical genomics weak performance due underspecification modification model towards completely prediction deployment domain technique address underspecification issue stress examine model data issue nevertheless demand reliable understand model inaccurately stress apply requirement coverage potential failure mode challenge underspecification constraint credibility ML prediction reconsider application ML link application medical image attention issue application presently various DL application widespread around application healthcare social network analysis audio processing recognition enhancement visual data processing multimedia data analysis computer vision nlp translation classification others application classify category classification localization detection segmentation registration although task target fundamental overlap pipeline implementation application classification concept categorizes data detection image consideration background detection multiple dissimilar surround bound localization concept surround bound segmentation semantic segmentation target surround outline label moreover fitting image 2D 3D onto another refers registration important DL application healthcare research critical due relation moreover DL tremendous performance healthcare therefore DL application medical image analysis DL application DL application image workflow task image classification computer aid diagnosis  another title sometimes classification chest ray dataset detect lung disease cnn another attempt ray image employ cnn modality comparative accessibility image likely enhance progress DL improve pre googlenet cnn image training dataset augment chest ray creator reorganize image orientation lateral frontal achieve approximately accuracy orientation classification clinically limited ultimately fully automate diagnosis workflow obtain data augmentation pre efficiency metadata relevant image chest infection commonly refer pneumonia extremely  commonly health worldwide conversely utilized  improve version densenet convolution layer classify fourteen disease author  dataset comprises image network achieve excellent performance recognize fourteen disease pneumonia classification accomplish auc receiver operating characteristic roc analysis addition network obtain performance radiologist panel individual radiologist adopt cnn candidate classification lung nodule employ random RF svm classifier cnns classify lung nodule employ convolutional layer parallel cnns   lung image database consortium dataset label CT lung scan classify lung nodule malignant benign image patch cnn extract feature output feature vector construct feature vector classify malignant benign RF classifier svm radial basis function rbf filter model robust various noisy input achieve accuracy nodule classification conversely model interpolates image data pet mri image 3D cnns alzheimer disease neuroimaging initiative  database pet mri patient scan utilized pet mri image 3D cnns input output furthermore patient pet image 3D cnns utilized image rebuild pet image rebuilt image approximately actual disease recognition outcome however approach address overfitting issue restrict technique capacity generalization diagnose normal versus alzheimer disease patient achieve cnn model  attain accuracy date outcome diagnose normal versus alzheimer disease patient author apply auto encoder architecture 3D cnns generic brain feature pre  dataset subsequently outcome feature become input layer differentiate patient scan alzheimer disease mild cognitive impairment normal brain  dataset tune supervision technique architecture VGGNet rnns basis  resnet model developed discriminate alzheimer disease normal patient  database accuracy voxnet resnet  asl model achieve accuracy conversely implementation algorithm simpler feature craft  declare developed cnn network scnn mri image task classification alzheimer disease achieve obtain accuracy recently cnn medical image classification task traditional diagnosis automate diagnosis tremendous performance task diabetic   normal abnormal   anemia sca normal abnormal sca component breast cancer classify   stain breast biopsy image invasive carcinoma situ carcinoma benign tumor normal tissue multi cancer classification cnns vital role diagnosis novel coronavirus cnn become primary automatic covid diagnosis hospital around chest ray image detail classification medical image application localization although application anatomy education increase clinician likely interested localization normal anatomy radiological image independently examine described outside intervention localization apply completely automatic application introduce approach localize pancreatic tumor projection ray image image radiation therapy without  construct cnn convolutional layer classify around transverse axial CT image author category classification pelvis liver lung data augmentation technique apply achieve auc classification error rate model detect  kidney liver employ stack auto encoders contrast improve mri scan  kidney liver temporal spatial domain  feature approach achieve detection accuracy aggregate convolutional neural network namely RetinaNet mask cnn pneumonia detection localization detection computer aid detection  another detection clinician patient overlook lesion scan dire consequence detection accuracy sensitivity introduce innovative framework detection pneumonia adopt transfer approach obtain accuracy recall unseen data covid pulmonary disease convolutional neural network approach propose automatic detection ray image excellent performance cancer application introduce detection task  introduce approach cancer detection tune convolutional neural network model address issue lack training data adopt transfer data augmentation technique densenet network superior model another  image progressively digitize publish pathologist image  malignancy marker index proliferation molecular marker cellular  abnormal cellular architecture enlarge  denote augment replication enlarge nucleus  ratio  slide risk disregard abnormal   excessive magnification employ cnns layer identify  fifty breast histology image  dataset technique attain recall precision respectively utilized histology image   detect nucleus cnns roughly nucleus label training purpose novelty approach spatially constrain cnn cnn detects nucleus surround spatial context spatial regression instead cnn employ stack sparse auto encoder  identify nucleus  slide breast cancer achieve recall precision respectively unsupervised technique effectively utilized medical image investigate insufficient label source actual  label histology image breast cancer amateur online recurrent issue inadequate label analysis medical image achieve source input label cnn signifies remarkable proof concept effort introduce employment convolutional neural network automatic identification  candidate   screen obtain detection dataset international recognition conference   detection competition segmentation although mri CT image segmentation research knee  prostate liver research concentrate brain segmentation particularly tumor issue highly significant surgical preparation obtain precise tumor limit shortest surgical  surgery excessive sacrifice brain neurological shortfall cognitive damage  limb difficulty conventionally medical anatomical segmentation specifically clinician within stack CT mri volume slice slice perfect implement   brief overview brain tumor segmentation mri image brilliant review brain mri segmentation address metric cnn architecture employ moreover explain competition detail datasets  stroke lesion segmentation  mild traumatic brain injury outcome prediction  brain tumor segmentation BRATS propose convolutional neural network precise brain tumor segmentation approach employ involves approach feature  model novel dual training scheme label distribution loss function multi layer perceptron processing conduct brain tumor segmentation datasets BRATS BRATS datasets introduce brain tumor segmentation adopt multi cascade convolutional neural network  fully conditional random CRFs achieve excellent employ parallel cnns 2D input patch dissimilar classify mri brain image image adult pre infant classify various tissue category  fluid grey patch concentrate capture various image aspect benefit employ dissimilar input patch incorporate spatial feature patch concentrate local texture algorithm dice coefficient achieve satisfactory accuracy although 2D image slice employ majority segmentation research implement 3D cnn mri prostate image furthermore promise challenge dataset fifty mri scan training thirty net architecture inspire net model attain dice coefficient competition reduce overfitting model deeper convolutional layer cnn apply intentionally filter model mri scan glioma brain tumor training achieve BRATS challenge BRATS challenge glioma BRATS dataset investigate 2D cnn architecture winner BRATS algorithm min execute min concept cascade architecture basis model refer  employ FC conditional random CRFs atrous spatial pyramid pool sample filter technique introduce author aim enhance accuracy localization enlarge filter multi model deeplab attain  intersection union pascal voc image segmentation model obtain excellent performance recently automatic segmentation covid lung infection CT image detect development covid infection employ technique registration usually input image stage canonical procedure image registration task target selection illustrates input image counterpart input image remain accurately superimpose feature extraction computes feature extract input image feature allows similarity previously obtain feature optimization aim minimize distance input image registration procedure suitable geometric transformation translation rotation etc input image within coordinate distance minimal  overlap optimal scope extensive review topic nevertheless summary accordingly introduce commonly input image DL registration approach various voxel grid mesh additionally technique input feature extraction canonical scheme specifically outcome data classical pipeline feature vector vector transformation nevertheless DL novel conceptual ecosystem issue contains acquire characteristic target behavior register input data conceptual ecosystem neural network training manner input registration approach nevertheless input adopt registration situation corresponds interior data representation DL interpretation conceptual enables differentiate input data registration approach define non define model illustrate phase model depict spatial data 2D 3D non define generalization data developed framework model acquires characteristic meaning identify   adjust 3D model characteristic maintain characteristic primary data likewise fundamental perspective unsupervised introduce target registration approach instance network input global slam issue register rigidly  propose combination conceptual model utilize growth imagination machine flexible artificial intelligence relationship phase training scheme inspire label classification another practical application DL cnns image registration 3D reconstruction apply adversarial cnns rebuild 3D model 2D image network learns orally accomplishes registration image conceptual model similarly utilize gan network  absent geometry damage  reconstruct voxel grid format label DL medical image registration numerous application review implement stack convolutional layer encoder decoder approach predict morph input pixel formation mri brain scan oasis dataset employ registration model deformation diffeomorphic metric mapping  attain remarkable enhancement computation synthetic ray image layer cnn register 3D model trans esophageal probe implant knee implant onto 2D ray image estimation model achieve execution important enhancement conventional registration technique intensity moreover achieve effective registration introduce neural network approach non rigid 2D 3D registration lateral  volumetric cone beam CT  image computational approach computationally exhaustive application complex ML DL approach rapidly identify significant technique widely development enhancement algorithm aggregate capability behave computational performance datasets effectively execute application earlier application consideration currently standard dnn configuration available interconnection layer layer difference configuration illustrates growth rate overall layer faster moore growth rate normal dnn layer around recent investigation future resnet version reveal layer extend however sgd technique employ parameter optimization technique employ obtain parameter update dnn training repetitive update enhance network accuracy addition  augment rate enhancement training imagenet dataset contains image along resnet network model around repetition converge steady addition overall computational load upper prediction exceed FLOPS training dnn complexity increase prior boost training satisfactory extent achieve gpus usually training session gpu contrast optimization strategy developed reduce extensive computational requirement increase dnns continuously enlarge complexity addition computational load memory bandwidth capacity significant entire training performance lesser extent deduction specifically parameter distribute layer input data sizeable amount reuse data computation network layer exhibit excessive computation bandwidth ratio contrast distribute parameter amount reuse data extremely additional FC layer extremely computation bandwidth ratio comparison aspect related device addition establish facilitate familiarity tradeoff obtain optimal approach configure fpga gpu cpu device correspond weakness strength accordingly comparison aspect related device although gpu processing enhance ability address computational challenge related network maximum gpu cpu performance achieve technique model strongly link bandwidth gpu efficiency maximum theoretical performance issue enlarge memory bandwidth bandwidth stack memory approach fpga gpu cpu accordingly detailed cpu approach behave performance cpu node usually assist robust network connectivity storage ability memory although cpu node purpose fpga gpu lack ability unprocessed computation facility increase network ability memory capacity gpu approach gpus extremely effective DL primitive greatly parallel compute operation activation function matrix multiplication convolution incorporate HBM stack memory date gpu model significantly enhances bandwidth enhancement allows numerous primitive efficiently utilize computational resource available gpus improvement gpu performance cpu performance usually related dense linear algebra operation maximize parallel processing initial gpu program model gpu model involve sixty computational simd per computational layer simd sixteen float computation lane peak performance tflops tflops percentage employment approach additional gpu performance achieve addition function vector combine inner production instruction primitive related matrix operation dnn training gpu usually optimize inference operation considerable performance improvement fpga approach fpga wildly utilized various task inference accelerator commonly implement utilize fpga fpga effectively configure reduce unnecessary overhead function involve gpu gpu fpga restrict weak behave float performance integer inference fpga aspect capability dynamically reconfigure array characteristic capability configure array effective overhead mention earlier fpga performance latency watt gain gpu cpu DL inference operation implementation custom performance hardware prune network reduce arithmetic precision factor enable fpga implement DL algorithm achieve fpga efficiency addition fpga employ implement cnn overlay efficiency accuracy TOPs peak performance conventional cnns  partner demonstrate recently contrast prune technique mostly employ lstm context model efficiently minimize important benefit implementation optimal mlp neural processing demonstrate recent implement fix precision custom float reveal lower extremely promising moreover aid additional advancement implement peak performance fpga related dnn model evaluation metric evaluation metric adopt within DL task crucial role achieve optimize classifier utilized within data classification procedure stage training utilized optimize classification algorithm training stage evaluation metric utilized discriminate optimize discriminator generate extra accurate forecast upcoming evaluation related specific classifier evaluation metric utilized efficiency classifier evaluator within model stage hidden data TN TP define negative positive instance respectively successfully classify addition FN FP define misclassified positive negative instance respectively evaluation metric accuracy calculates ratio predict sample evaluate       sensitivity recall utilized calculate positive correctly classify    specificity utilized calculate negative correctly classify    precision utilized calculate positive correctly predict predict positive    calculates harmonic average recall precision rate     metric  statistic metric    false positive rate fpr metric refers possibility false alarm ratio calculate   roc curve auc rank metric utilized conduct comparison algorithm construct optimal model contrast probability threshold metric auc expose entire classifier rank performance formula calculate auc   sum positive ranked sample negative positive sample denote respectively accuracy metric auc verify empirically theoretically helpful identify optimize evaluate classifier performance classification training discrimination evaluation auc performance brilliant however multiclass issue auc computation primarily effective discriminate addition complexity compute auc 𝑛log𝑛 respect till auc model 𝑛log𝑛 accord   auc model framework datasets DL framework datasets developed various framework library expedite training become easy utilized framework library framework library rating github background tensorflow deem effective easy ability platform github software host site github refer regard project site moreover benchmark datasets employ DL task benchmark datasets summary conclusion finally mandatory inclusion brief discussion gathering relevant data along extensive research  analysis conclude review exhibit future direction DL already difficulty simultaneously model multi complex modality data recent DL development another approach multimodal DL DL sizeable datasets label data prefer predict unseen data model challenge particularly data processing datasets limited healthcare data alleviate issue TL data augmentation research although ML slowly transition semi supervise unsupervised manage practical data without manual label model utilize supervise cnn performance greatly influence hyper parameter selection hyper parameter affect cnn performance therefore careful parameter selection extremely significant issue optimization scheme development impressive robust hardware resource gpus effective cnn training moreover explore efficiency cnn smart embed cnn context ensemble prospective research collection multiple architecture model improve generalizability across image category extract semantic image representation similarly activation function dropout batch normalization merit investigation exploitation depth structural adaptation significantly improve cnn capacity substitute traditional layer configuration significant advance cnn performance recent literature currently develop novel efficient architecture trend research model cnn architecture HRNet improve architecture platform essential role future development computational DL application utilize compute handle enormous amount data increase efficiency reduce furthermore flexibility DL architecture recent development computational chip neural network mobile gpu DL application mobile device easy user DL regard issue lack training data various technique transfer training DL model unlabeled image datasets transfer knowledge DL model label image task overview community DL interested DL furthermore researcher suitable direction accurate alternative