distinguish sequence widely finite machine conformance identification address scalability issue encounter derive distinguish sequence observable nondeterministic finite machine introduce massively parallel mapreduce version algorithm knowledge tackle task mapreduce approach concise overview algorithm derive distinguish sequence nondeterministic finite machine propose parallel algorithm mapreduce approach analyze communication model furthermore conduct variety intensive comparative finite machine demonstrate propose efficient scalable introduction recent software interact component hardware software perform critical role society important infrastructure banking health traffic telephony sector software consist computer code perform complex task almost continuously host technical  issue become critical others arose recently increase interconnection software software essential software development efficient approach reduce finite machine conformance due simplicity ability model complex finite machine FSMs extensively communication protocol digital reconstruction smart contract distribute genomics reactive FSM model finite input output finite transition label input output besides FSMs underlie model formal description technique statecharts specification description SDL unified model uml programmable logic device ethereum smart contract FSM indispensable implementation guarantee function model aspect behavior due simplicity ability model FSM research however basically information FSM implementation  consequence  information  purpose implementation model behaves accordance specification apply sequence CSs  correspond output response conclusion  recognize   recognition accomplish CSs distinguish sequence unique input output sequence characterize synchronize sequence reset sequence sequence exist motivation sequence robotics bio compute propositional calculus model distribute literature contains technique automatically generate CSs approach consist principle initialization identification transition verification focus scalability generate distinguish sequence DSs FSM resolve identification consists input sequence output initial FSM described seminal moore  FSM generation algorithm automate motivation FSM sequence generation technique DSs distinguish sequence DSs exist shorter DSs adaptive ADS preset PDS FSM deterministic nondeterministic partial observable PDS fix input sequence distinguish machine ADS input depends output input leaf input sequence specific leaf throughout refer PDS DS derive shortest DS algorithm EA principally FSM construct truncate successor intersection initialize FSMs FSM initial FSM initial derive shortest DS analyze separability relation FSMs derive shortest DS exists FSMs FSM FSM upper bound becomes exponential nondeterministic FSMs DS exponential bound addition complexity exists PDS PSPACE EXPTIME ADS knowledge parallelization scalability derive DSs nondeterministic FSMs thoroughly address mapreduce framework focus optimize mapreduce version algorithm scalability outline contribution contribution parallel mapreduce algorithm efficiently derive distinguish sequence nondeterministic FSM theoretical analysis communication propose mapreduce model theory evaluate performance propose algorithm extensive variety FSMs datasets remainder structure technical definition brief introduction mapreduce computational model related survey algorithm derive DSs nondeterministic FSMs analysis propose mapreduce version algorithm derive shortest DSs efficiency scalability propose conduct extensive comparative variety nondeterministic FSMs whereas conclusion preliminary concept throughout mapreduce framework brief finite machine distinguish sequence finite machine FSM tuple finite finite input input alphabet finite output output alphabet transition transition denote origin destination input output denote transition subset FSM nondeterministic exists FSM exists otherwise machine partial nondeterministic FSM observable machine transition input output otherwise machine non observable FSM exists input sequence FSM arbitrary FSM input successor contains exists output subset input successor union successor finite machine image FSM define input output  successor successor successor interested DS input sequence output sequence FSM mention previously exists DSs input sequence preset distinguish sequence PDS FSM define input sequence distinct distinguish sequence adaptive distinguish sequence ADS exactly leaf internal node label input label output leaf uniquely label FSM descend node distinct output leaf input output sequence respectively node label leaf label FSM input sequence adaptive distinguish sequence sequence depth mapreduce model computation mapreduce recently introduce optimal parallel model compute intersection composition operation FSMs implement mapreduce version EA derive shortest DS exists observable nondeterministic FSM mapreduce prominent program model processing scalable nowadays hadoop popular source framework java implement mapreduce algorithm author apache software foundation hadoop project module enable reliable scalable distribute compute advantage scalability flexibility effectiveness organize architecture resilience failure recently hadoop platform propose efficient flexible computational storage strategy processing data reside cite lineage aware data management  exploit data locality decrease network footprint mapreduce algorithm consists phase shuffle reduce phase task completely parallel manner phase responsible filter transform input intermediate shuffle phase occurs automatically hadoop manage exchange intermediate data phase reduce phase reduce phase summarize output previous phase parallel mapreduce algorithm essential propose optimal parallelism communication mapreduce computation analyze communication computation propose theoretical model related parallel approach derive distinguish sequence variety recently focus parallel processing technique derive CSs context  sequence harmonize identifier characterize synchronize sequence DSs generation   introduce independently parallel multithreading implementation EA central processing cpu graphic processing gpu architecture conduct extensive variety FSM cpu gpu architecture workload obtain approach sufficiently efficient data execution derive DSs nondeterministic FSMs increase exponentially parameter nondeterminism transition input alphabet output alphabet ratio reduce execution construct successor nondeterministic FSM  propose multithreading parallel approach multicore cpu gpu architecture option thrust software platform gpu implementation cuda platform propose evaluate network workstation  divisible load theory conduct nondeterministic FSMs input output difference propose algorithm speedup execution   partial observable nondeterministic FSMs scalability issue construct preset adaptive distinguish sequence PDS ADS propose ADS generation algorithm input exist ADS construction algorithm PDS generation algorithm input exist PDS generation algorithm approach available parallelism gpu compute model thread strategy utilizes global device memory maximizes thread maximize parallelism propose algorithm derive DSs observable partial nondeterministic FSMs acceptable amount overview algorithm concise overview algorithm derives shortest distinguish sequence exists observable nondeterministic FSM FSM FSMs initial EA apply FSM derive DS distinction derive shortest distinguish sequence exists EA implement breadth bfs explores successor EA intersection derivation compute intersection intersection partial non FSM derive truncate successor otherwise return message non distinguishable derive truncate successor DS node exists bfs initial intersection node label subset intersection already derive internal node jth label subset intersection input outgo internal node label node label subset successor subset node pth label subset claimed  algorithm leaf node exists node non separable otherwise leaf node label subset input successor derive shortest DS label node leaf node subset FSM subset algorithm hence complexity EA EA derivation DS exists previously derive truncate successor perform classical breadth recall algorithm complexity EA FSMs respectively shortest DS upper bound consequence upper bound FSM becomes however accord conduct exists FSMs shortest DS FSM existence DS FSMs significantly depends nondeterminism FSMs truncate successor image FSM apply EA derive DS successor derive EA node successor label intersection FSM node associate successor node obtain node successor successor node node exploration node label empty successor input therefore input sequence node label empty DS FSMs efficient mapreduce algorithm derive distinguish sequence FSMs analyze parallel version EA mapreduce framework extract distinguish sequence observable nondeterministic FSM outperforms previous parallel approach efficiently distinguish sequence FSM framework overview propose consists mapreduce namely intersection derivation distinguish sequence derivation overview parallel algorithm mapreduce image illustrates workflow receives FSM input distinguish sequence mapreduce algorithm input FSM preprocessed text file transition file input intersection mapreduce framework compose essentially function performs filter sort reduce function performs summary operation function   transition generates associate function grouped reduce function receives transition computes intersection transition input output truncate successor developed iterative mapreduce algorithm derive distinguish sequence exists input output previous intersection function truncate successor denotes source node label destination node associate destination node empty associate source node destination node becomes associate reduce function receives associate subset contains empty destination node contains cartesian subset perform source node subset destination node equivalent bfs truncate successor EA concatenate label construct DSs exist derivation successor derivation performs successor intersection reducer previously described mapping schema derivation performs successor intersection reducer mapping schema node reducer cartesian compute inside reducer compact reducer mapped mapped reducer associate iteratively mapreduce derivation derivation image finally DS exists iteration maximum bound derivation output input iteration output DSs exist communication mapreduce framework model communication analysis communication model introduce analyze optimizes performance distribute compute environment explicitly inherent communication parallelism apply model mapreduce framework algorithm analyze reducer communication mapreduce computation parameter involve mapreduce algorithm reducer denote associate reducer parameter amount communication reduce communication denote define average mapper input formally suppose reducer input assign  reducer input replication rate expression compute bound replication rate intersection FSMs function expression  denotes input denotes output output reducer observable nondeterministic FSM  proposition bound replication rate worth limit reducer enables parallelism reducer redefine notion reducer parallelism available node mapreduce algorithm intersection mapreduce implementation intersection modify version algorithm propose approach truncate successor successor observable nondeterministic FSM conduct derive distinguish sequence construction successor EA later construction truncate successor algorithm contains definition reduce function intersection function define schema input FSM transition reduce function performs inside reducer intersection transition mapper task propose mapping schema emit transition reducer define hash function mapping respectively input alphabet input alphabet formally observable nondeterministic FSM transition mapper transition hash function hash function integrate definition sub function  algorithm explain detail mapping sub function  mapping mapping transition mapper   hash function define reducer function output reducer affected presence transition alphabet inside reducer formally observable nondeterministic FSM proposition upper bound replication rate proposition replication rate mapping scheme mapping input alphabet reducer input alphabet reducer input alphabet mapper transition reducer correspond input precisely transition mapper     hash function define assume alphabet uniformly distribute proposition replication rate input alphabet mapping scheme optimal mapping input alphabet propose hybrid mapping input alphabet  reducer reducer reducer truncate successor deduce upper bound replication rate proposition proposition replication rate hybrid mapping theorem algorithm correctly computes successor FSM mapreduce communication proof function algorithm  return associate transition mapping sends transition reducer indexed ensure algorithm correctly construct successor transition input output inside reducer reducer function computes pairwise intersection extend successor propose mapping mapping reducer receives mapper transition consequence outgo transition inside reducer verify mapping mapping input alphabet reducer receives mapper transition input inside reducer transition input output hybrid mapping reducer receives mapper transition input obviously verify proof communication complexity proposition mapreduce algorithm derivation multiple mapreduce derive shortest distinguish sequence observable nondeterministic FSM mapper parallel collection   reducer trait collection derive DSs exist derive shortest DS intersection truncate successor DS exists mention notation mapping function input truncate successor  source node destination node empty otherwise destination node compute replication rate mapreduce derivation algorithm available reducer reducer cannot output iteration proposition proposition replication rate mapreduce derivation theorem algorithm correctly derives DS exists FSM maximum mapreduce proof obvious DS label node leaf node indexed empty successor mapreduce algorithm successor compact without loss generality leaf node  indexed empty kth successor prec predecessor node node mapreduce node  replaces node belonging     consequence successor compact label   concatenate label       mapreduce algorithm related node successor DSs derive exists iteration implementation experimental extensive described evaluate efficiency effectiveness communication execution conduct randomly generate observable nondeterministic FSMs input output alphabet nondeterminism calculate depict average obtain correspond finally propose communication execution mapreduce framework derive truncate successor extract distinguish sequence exists cluster configuration hadoop french scientific testbed grid site lille cluster compose node CPUs core node machine equip intel xeon core processor GB memory disk hdd GB machine gbps ethernet network debian hadoop version instal machine data generation randomly generate variety FSMs data phase combination input alphabet output alphabet nondeterminism randomly generate deterministic finite automaton described  competition http   randomly transition finally obtain FSM nondeterministic observability generate randomly replication transition observability summarizes datasets along respective obtain accurate generate sample dataset obtain sample datasets communication analysis communication phase reduce phase optimize minimize replication rate parameter input reducer summarizes relationship FSM communication mapreduce algorithm datasets communication intersection derivation datasets introduce mapping intersection approach communication datasets communication propose intersection image obtain clearly mapping input alphabet outperforms mapping communication due transition reducer formally proposition FSMs input alphabet mapping communication coincides proposition computation analysis computation execute mapreduce graph comparative execution propose parameter input alphabet output alphabet nondeterminism transition execution without inference computation execution versus input alphabet image execution versus image execution nondeterminism image execution image execution versus transition image increase curve execution propose intersection derivation data input alphabet nondeterminism transition input alphabet efficient alphabet available reducer FSM parameter performance reducer receives useful transition influence increase input transition however resource involves waste reducer multiple transition input alphabet hybrid mapping parallel reducer transition global reduce however mapping phase replicate transition otherwise reducer available resource therefore reducer implies execution mapping weak important replication rate reducer besides transition intersection perform inside reducer define associate derivation propose mapping execution nearly linear parameter accord minimize replication rate decrease mapper replicate transition avoids intermediate reduces transition assign reducer adequate reducer diminishes reducer spends cpu therefore optimal parallel mapreduce scheme distinguish sequence observable nondeterministic FSM comparative perform evaluate efficiency scalability propose comparison approach speedup metric define faster parallel comparison sequential evaluate performance mapreduce multi thread approach conduct node cluster described node machine equip intel xeon core per processor thread per core GB memory disk hdd GB thread performance analyze transition nondeterminism FSM speedup comparison mapreduce propose previously intersection multi thread parallel implementation openmp sequential algorithm EA multicore cpu   source implementation openmp preprocessor java confirm previous depicts speedup datasets mapping input alphabet effective datasets speedup accord nondeterminism consistent performance gain speedup alphabet circumstance performance due replication rate reduce reducer receives useful transition besides performance faster openmp speedup grows exponentially transition nondeterminism FSM increase speedup versus transition image achieve speedup nondeterminism image obtain satisfactory propose approach derive DSs nondeterministic FSMs adapt context however mapreduce iterative processing data onto disk iteration disk bottleneck combine node iteration significant challenge due complex network structure overcome limitation derivation mapreduce framework introduces mapreduce bipartite graph model iterative incremental computation contains loop mapper reducer conclusion future FSMs widely various application domain communication protocol logical device synthesis reactive FSM distinguish sequence derive model apply machine implementation  identification deterministic FSMs FSMs specification nondeterministic introduces scalability challenge shortest DS exponential bound address scalability issue encounter derive distinguish sequence observable nondeterministic finite machine FSMs introduce massively parallel mapreduce version algorithm classical generation algorithm approach mapreduce intersection derivation distinguish sequence propose mapreduce respectively mapping mapping input alphabet hybrid mapping input alphabet introduction justified EA iterative mapreduce algorithm introduce derive distinguish sequence analyze communication model formal aspect inherent communication parallelism distribute compute environment perform randomly generate FSMs implementation assess respect communication execution speedup propose algorithm multiple thread algorithm speedup propose algorithm efficient scalable mapreduce algorithm FSMs transition exist PDS generation algorithm speedup openmp intersection algorithm future investigate parallel mapreduce algorithm derive sequence generation  sequence characterize harmonize identifier synchronize sequence finally additional FSMs abbreviation FSM finite machine SDL specification description uml unified model  implementation DS distinguish sequence ADS adaptive distinguish sequence PDS  distinguish sequence EA algorithm cpu central processing gpu graphic processing  network workstation bfs breadth