text categorization vector model vsm widely document document vector contribute document semantics various scheme propose vsm improve text categorization performance evidence performance scheme varies across text categorization task mechanism underlie variability scheme performance remains unclear moreover exist scheme respect category locally without global distribution occurrence across category corpus systematically examine pro con exist scheme text categorization explore scheme theoretical chi information gain perform poorly empirical evaluation concentration distributes across category corpus propose series entropy scheme distinguish text categorization extensive datasets propose scheme consistently outperform scheme moreover finding shed develop effective scheme specific text categorization task access auckland library introduction explosive increase digital document online text categorization TC automatically classifies document pre define thematic category become effective technique organize unstructured data perform TC task important document representation transforms text document compact format widely vector model vsm document vector document importance semantics document scheme assign appropriate document representation improve classification performance intuitively scheme document vector document closer across classifier categorize document effectively achieve performance category information training data membership training document pre define category TC task scheme classify unsupervised scheme category information supervise scheme otherwise traditional scheme idf BM variant unsupervised scheme originally propose information retrieval IR focus utility distinguish document document relevance document however objective TC distinguish category corpus relevance category unsupervised scheme cannot effectively quantify discriminate context TC relevance category researcher recently exploit category information training document TC task propose various supervise scheme chi chi information gain gain ratio however supervise scheme achieve performance TC previous supervise scheme chi perform unsupervised scheme remains unclear supervise scheme mathematical foundation performance theory instead perform poorly moreover supervise scheme locally respect specific positive category PC specifically PC category corpus PC combine negative category NC distribution PC NC approach limitation multi classification PC contains NC combination multiple imbalance PC NC distributional statistic NC absence NC tend thereby dominate however discus sect statistic mislead discriminate multiple category combine NC grain information distributes individual category NC neglect category occurs information loss discriminate category NC category specific scheme category document beforehand document however category document unknown categorization challenge scheme properly document distribution author  category reuters image address limitation propose discriminate TC global distribution across category corpus specifically  occurs across category assume concentrate category discriminate TC illustrate occurrence author  category reuters author scatter across multiple category  mainly concentrate category intuitively scatter across category limited specificity refer category category concentrate specific category specificity refer category category specific document contains category specific  indicative semantics category document category author concentration occurs across category discriminate TC quantify concentration distribute category adopt entropy widely uncertainty information theory propose series supervise entropy scheme TC particularly vsm contribution characteristic previous scheme TC examine supervise scheme mathematical foundation perform poorly analytical explanation empirical evaluation knowledge systematic analysis research explore entropy quantify discriminate TC prior propose entropy scheme distributional concentration balance distributional concentration bdc propose variety extension bdc cope various circumstance TC unlike previous scheme weigh specific PC propose scheme primarily category independent explore propose entropy scheme effective discriminate TC propose scheme scheme datasets domain experimental demonstrate effectiveness propose entropy scheme TC task experimental setting conduct detailed analysis scheme performs situation deeper insight effective scheme TC task remainder organize sect review scheme TC propose scheme experimental evaluation described sect sect discus rationality propose highlight direction future sect related document representation essential perform TC task transforms raw textual document compact format document classifier widely vector model vsm vector machine svm neighbour NN model document vector content descriptive feature document gram component document contribute semantics topic document various associate vector reflect useful distinguish category classification properly variety scheme developed statistic scheme lexicon scheme statistic scheme distributional statistic corpus frequency document frequency contrast lexicon scheme external lexicon knowledge semantic similarity distance category access external resource challenge exist lexicon lexicon scheme complexity flexibility statistic scheme moreover prior lexicon scheme significant advantage statistic scheme hence statistic scheme dominant approach literature focus statistic scheme although focus examine scheme another related embed continuous bag cbow skip gram effective dimensional representation text data recent neural network generate representation semantically related enhance performance processing task latent semantic analysis syntactic parse machine translation TC despite advance scheme useful document representation complement embed embed model knowledge resource extract robust semantic previous embeddings datasets effective randomly initialize vector perform approach scheme TC reduces applicability embed situation domain specific corpus available although pre embed model knowledge resource glove bert model situation semantic taxonomy knowledge specific domain  semantic meaning context news software engineering respectively moreover recent pre embeddings bias data bias amplify downstream application exist bias removal technique processing training procedure sufficient remove bias contrast scheme sensitive corpus hence complement embed domain corpus available without introduce external bias embed model largely focus generate semantically representation individual document sum average embeddings individual document contributes equally document embeddings efficient critical others express semantics document recent propose scheme idf aggregate individual embeddings document embeddings approach tough baseline application sentiment analysis classification textual similarity measurement outperform sophisticated computationally intensive document embed docvec skip vector  particularly text corpus tweet online review supervise neural rnn lstm scheme classify unsupervised supervise scheme category information training document briefly review previous detailed introduction previous unsupervised scheme unsupervised scheme originally propose information retrieve IR literature binary idf variant scheme counting occurrence document without category information document frequency raw frequency document replace reduce binary scheme variant tfs capture local document combine global factor inverse document frequency idf popular global factor successfully IR related idf factor commonly occurs across document combine idf document typically calculate idf  denotes document corpus denotes frequency document denotes document document frequency variant idf propose BM  replace idf entropy factor scheme outperforms idf recently  propose report outperform scheme BM supervise scheme date supervise mainly developed approach function metric function chi chi information gain mutual information another approach model algorithm interact classifier model typically model iterative procedure obtain optimal computationally expensive consume datasets moreover model classifier specific classifier classifier thereby likely suffer overfitting issue contrast function easily robust across classifier focus function supervise scheme function contingency category review supervise scheme introduce notation widely scheme occurs document category corpus regard positive category PC category combine negative category NC occurrence PC NC compute contingency document PC document PC document NC document NC document review supervise scheme supervise scheme developed feature selection scheme correlation PC assume discriminate TC correlation chi gain ratio odds ratio likelihood ratio  robust copula dependence rcd widely notation feature selection scheme compute chi     nlog nlog   hmax hmax hmax denote entropy category corpus maximal entropy respectively recently supervise scheme propose   propose scheme statistical confidence interval although scheme outperforms idf complicate implement relevance frequency statistic relevant document discriminate define amax scheme variant logarithmically vrf probabilistic trr achieve performance TC another scheme iqf icf unlike scheme iqf icf factor category frequency category occurs corpus document category iqf icf define iqf icf  category another related scheme inverse gravity igm inter category distributional concentration regularize entropy imbalance distribution PC NC document representation policy processing document obtain scheme unsupervised scheme assign identical training document label document without identical document representation regardless document scheme assign identical category independent scheme contrast supervise scheme assign category specific accord category label document scheme assign respect specific category category specific scheme category specific scheme representation policy local policy assign category category label training document beforehand TC local policy widely training document effectively exploit category information policy however hardly document category label unknown address policy global policy propose category obtain identical globalization sum  average  maximum wmax max category specific typically wmax outperforms however discus sect criterion training data analysis discriminate develop scheme critical understand discriminate TC randomly dataset corpus subset snippet dataset document category business bus computer com education edu engineering eng document category occurrence category document snippet assume occurs document intuitive comparison examine unsupervised scheme widely idf ipo amd ipo amd idf assigns amd ipo however ipo occurs bus amd scatter across bus com indication bus ipo intuitively amd idf capture strength category relatedness fails properly discriminate TC examine supervise scheme classic chi chi category bus PC ipo ipo occurs PC occurrence NC intuitively correlation ipo bus bus however chi correlation bus instead ipo counterintuitive PC NC denote positive category negative category respectively contingency category bus explore issue contingency category bus ipo notation PC calculate contingency coefficient coefficient quantify association binary variable ipo bus bus presence ipo positively associate presence bus presence negatively associate presence bus ipo bus bus chi yield chi ipo bus chi bus chi fail capture polarity association exist contingency due document vector sparse document NC multi classification task collection document NC tends constant corpus likely dominant chi however neither directly relevant PC useful discriminate classic supervise scheme cannot effectively discriminate unlike classic scheme scheme variant relevant statistic avoid issue however occurrence individual category NC grain information distribution NC category occurs ignore scheme mislead raw amd private occurrence PC occurrence NC amd private amd discriminate private however amd occurs category private intuitively occurs category category document certainty amd discriminate private although scheme iqf icf category occurs hardly reflect grain distribution information probability distribute category private private occurs category iqf icf assigns private however distribution category probability bus certainty indicates bus actually private discriminate private iqf icf therefore previous supervise scheme locally respect specific PC hardly capture grain distribution information corpus moreover previous supervise scheme category specific category label document unknown challenge supervise scheme representation document illustrate chi training document category namely math homework education math programmer computer document obtain vector math programmer wmax across category classify education actually belongs computer globalization   incorrect contrast category independent scheme binary representation document correctly classify computer entropy scheme intuition entropy TC scheme capture grain information occurs corpus propose alternative approach concentration distribute category corpus unlike previous aggregate category PC NC approach distribution individual category globally illustrate intuition revisit ipo occurs category bus indication bus discriminate occurrence concentrate bus amd private certainty category hence discriminate similarly amd certainty category document private amd distribute category private amd discriminate private finally scatter across category without specificity category hence category discriminate observation summarize observation document concentrate within category corpus evenly scatter across category certainty document category discriminate TC quantify concentration certainty distribute category corpus adopt entropy classic uncertainty information theory formally category corpus probability occurs entropy respect category  logarithmic category corpus generally concentrate within category entropy scatter across category accordingly assumption document entropy another respect category corpus assume former discriminate latter TC  mainly occurs category crude entropy respect category author scatter across category entropy intuitively  useful discriminate category author apart empirical observation notion entropy intuition specifically entropy respect category denotes uncertainty category category indicates effectiveness distinguish category TC hence entropy reasonable metric discriminate TC propose series entropy scheme distributional concentration entropy scheme distributional concentration define   entropy respect category corpus probability category frequency sum frequency category logb minimum maximum obtain occurs category frequency category respectively normalize logarithmic allows comparable across corpus assigns analysis discriminate unlike unsupervised scheme idf supervise exploit category information training data unlike feature selection scheme excludes statistic irrelevant document prevents statistic dominate reduce effectiveness unlike category specific scheme iqf icf category independent globally distributional concentration category distribution information individual category exploit effectively split category PC NC assigns without pre specify PC allows directly document label unknown without rely globalization sect balance distributional concentration scheme propose merely distribution information distributes category regardless category TC task category document sample document category tend occurrence category category hence merely counting absolute frequency category ignore category estimate significance category bias towards category classification illustrate category earn frequency category earn reuters frequency category frequency sum category intuitively related category category earn however significant earn reasonable issue address category category approximate category relative category significance category earn category avoid bias towards category normalize frequency category category specifically replace absolute frequency conditional probability relevant category pre processing balance processing significant earn reasonable basis scheme balance distributional concentration bdc bdc logb  logb balance entropy balance probability category conditional probability corpus uniform distribution category bdc reduces smooth distributional concentration although discriminative exhibit entropy across category occasionally occurs document entropy estimate scheme bdc comparison sdc distribution ipo  category category bus hence assign however occurrence  corpus ipo ipo  likely useful  typo complaint hence inappropriate assign ipo  capture important data avoid incorporate smooth entropy scheme propose smooth distributional concentration sdc sdc distribution category tends uniform correspond entropy tends increase smooth processing reduce hence reduce classification apply laplace smooth ipo  obtain smooth distribution calculate smooth distribution sdc  becomes ipo define sdc sdc   entropy category smooth processing probability smooth apply smooth calculate differently laplace smooth smooth parameter indicates non smooth sdc reduces popular smooth categorical relevance factor scheme aim capture useful information data connection raw data classifier algorithm scheme exploit data information classifier inherently binary classifier svm dominant approach perform multi classification task decompose task multiple binary classification sub task binary classifier built distinguish category PC others category specific manner scheme category specific relevance PC account useful improve performance binary classifier factor distribution category snippet subset category approximately entropy however binary classifier discriminate transform utility distinguish PC NC intuitively useful distinguish bus category category relevant bus accord distribution category correspond binary classifier propose category specific factor categorical relevance factor scheme propose relevance PC consideration idf respect category frequently PC intuitively relevant PC useful distinguish PC NC sine category discriminate TC frequency across category PC highly frequent diminish define logβ logarithmic frequency category sum frequency category influence decrease increase reduces significance useful discriminate bus category reasonable unified entropy scheme bdc sdc extend distinct perspective extension combine another series unified scheme smooth distribution category laplace smooth normalize smooth frequency category prior information category obtain balance smooth distributional concentration   logb denotes entropy respect category corpus balance smooth processing denotes balance smooth probability category occurrence denotes unique corpus append factor  derive unified scheme    moreover combine local factor variant multiplication operation calculate category   summary entropy scheme unified scheme obtain combination scheme assign category specific without assign category independent assume occurrence category discriminate depends global distributional concentration hence bdc bdc sdc sdc scheme behave similarly without moreover apply category specific scheme bdc sdc document label unknown document bdc sdc respectively eliminates dependency category specific scheme globalization sect connection exist scheme demonstrate propose entropy scheme although igm intuition igm gravity distributional concentration across category propose entropy igm compute absolute frequency category sect bias towards category particularly corpus skewed category distribution examine difference propose scheme previous entropy scheme  unsupervised scheme entropy respect document scheme supervise entropy respect category although involves calculation entropy discriminate information category distribution corpus separator entropy quantify information separator contrast discriminate categorical specificity concentration entropy specificity directly mutual dependence category contrast scheme distributional concentration across category although average interpret reduction uncertainty category distribution corpus due category information scheme categorical certainty alone information mutual information finally entropy  propose feature selection scheme limitation feature selection scheme chi overlook grain distribution information NC contrast scheme explore detailed distribution information individual category beyond conceptual difference propose scheme exist scheme empirical analysis scheme TC aim evaluate effectiveness propose entropy scheme gain deeper insight scheme TC focus evaluation objective propose entropy scheme perform exist TC task characteristic scheme scheme perform situation detail datasets experimental setting evaluation metric datasets datasets domain document  dataset document label label multiple simplicity label remains document category  split partition data training document document category distribution highly skewed reuters training data training data  dataset consists snippet web transaction pre define category query google query training query snippet construct dataset datasets document snippet data  corpus collection approximately newsgroup document partition nearly evenly across category  version widely previous version document sort date split training  dataset subset web extract  web directory contains web classify category expert brazilian portuguese standard training split dataset evaluate dataset fivefold validation   corpus quality document medical abstract mesh category classic task previous categorize cardiovascular disease category reuters corpus document label cardiovascular disease abstract fivefold validation dataset data statistic summary baseline scheme document pre processing convert punctuation removal standard URLs stem porter stem algorithm footnote erase infrequent statistic dataset document doc training feature feature average document average per document experimental setting propose scheme baseline baseline idf widely scheme  entropy related scheme chi iqf icf  trr achieve performance TC notation description baseline scheme eliminate document cosine normalization document representation document normalize adopt wmax globalization document category specific scheme chi  trr iqf icf widely NN svm benchmark classifier implement NN cosine similarity distance vote neighbour similarity svm classifier  package default parameter parameter preliminary analysis significant difference default parameter non default linear svm linear model report outperform nonlinear TC performance evaluation precision recall metric evaluate performance classification however precision achieve recall combine metric typically calculate precision recallprecision recall peak precision recall hence estimation  precision recall achieve balance denote performance typically     performance category emphasize   analysis analyse experimental address evaluation objective accordingly comparison scheme NN multi classification comparison scheme svm multi classification overall comparison address propose perform exist TC task propose scheme baseline scheme TC task summarize scheme multi classification task NN svm respectively NN scheme baseline scheme propose scheme achieve datasets NN propose bdc outperforms perform baseline trr    unlike NN classifies data mainly rely distance feature vector svm parameter combine feature vector decision hyperplane comparison svm slightly NN however propose scheme perform steadily achieve comparable across datasets svm strongly illustrate effectiveness propose statistical significance ass statistically significant difference comparison prior McNemar scheme another totally scheme pairwise scheme perform scheme idf  trr iqf icf bdc bdc statistical significance datasets classifier NN obtain scheme performance scheme without significant difference grouped denote performance significance respectively propose scheme consistently comparable performance scheme achieve significant improvement baseline scheme analysis individual scheme address characteristic scheme scheme perform situation analyse scheme detail normalize scheme boost performance category balance bdc bdc generally perform moreover improvement significant  bdc outperforms   performance category emphasize  bdc boost performance category confirm analysis bdc effectively avoids bias towards category sect smooth scheme improve corpus smooth sdc performs reuters  without improvement datasets unexpected verify smooth  mercer dirichlet absolute discounting stage smooth consistent improvement smooth reduce performance quality corpus  apart importance discriminative frequency diminish smooth processing suppose performance sdc related quality corpus corpus local emphasize svm bdc alone bdc factor perform svm respectively implies local document capture emphasize svm evidence trr factor perform iqf icf svm newsgroups contrast iqf icf performs NN category specific scheme svm propose scheme without factor svm typically perform without category specific factor propose improve performance inherently binary classifier svm contrast NN achieve without category specific scheme preferable svm achieve performance analysis sect feature selection scheme perform poorly multi classification prior feature selection scheme chi  perform poorly datasets  capture correlation category positive negative  outperforms chi chi across datasets aligns prior evidence scheme reflect polarity correlation  outperform cannot chi TC confirms analysis chi sect obtain optimal performance NN idf variant performance scheme mapping document vector perform scheme NN plot snippet newsgroups respectively category datasets relatively balance   trend  NN combine idf variant  performs gradually increase idf scheme tend assign document frequency namely rare corpus however rare neighbour reduce influence improve performance contrast bdc peak indicates propose scheme semantically related document vector closely  scheme snippet NN image  scheme newsgroups NN image discussion propose entropy scheme outperform exist TC task analysis sect mainly exist scheme cannot assign discriminate TC previous supervise scheme assign category specific hardly document properly imbalance PC NC multi classification reduce effectiveness feature selection scheme evidence argument understand propose scheme perform difference explore propose scheme exist scheme discriminate TC distribution generate scheme correlation snippet correlation iqf icf iqf icf wmax correlate iqf icf kendall surprising scheme comparable performance evaluation however namely multiple category concentrate interval iqf icf interval relative iqf icf namely respectively hardly reflect difference iqf icf multiple category NC without distribution information individual category NC account loss information grain difference neglect contrast probability occurs category difference discriminate reflect effectively distinguishable another propose scheme perform correlation snippet kendall rank correlation parenthesis image another iqf icf namely category greatly iqf icf absolute frequency calculate category due frequency  occurs sport category snippet discriminate however indication category scheme probability distribute category absolute frequency propose assigns  highlight discriminate avoids estimation importance category specific TC another scheme performance representation document previous supervise scheme cannot properly document illustrate issue classification performance training data reuters snippet data respectively examine document document wmax globalization accuracy scheme category specific scheme chi  accuracy category independent scheme idf bdc explain category specific scheme relative importance training document respect specific category category label document contrast document wmax wmax respect category math important programmer respect category computer however vector math instead important programmer across category education computer criterion training document reduces accuracy contrast category independent scheme idf bdc identical training document enables document criterion imbalanced category accuracy scheme sect factor limit performance feature selection scheme deem unbalanced PC NC multi classification task particularly corpus category scheme perform PC NC balance binary classification category comparison scheme binary classification verify hypothesis performance feature selection scheme binary classification multi classification relatively category category document newsgroups dataset binary classification dataset category newsgroups newsgroups contains document category rec sport baseball document category rec sport hockey scheme binary classification NN svm classifier propose bdc achieves illustrate propose scheme perform multi classification binary classification importantly feature selection scheme perform relatively binary classification multi classification baseline chi generally perform newsgroups contrast performs newsgroup confirms hypothesis imbalance PC NC reduce performance feature selection scheme conclusion future effective improve TC performance explore advantage weakness scheme TC moreover explore discriminate global distributional concentration across category corpus propose series supervise entropy scheme experimental demonstrate effectiveness propose scheme sect finding supervise scheme mathematical foundation performance theory instead perform poorly analysis mainly supervise scheme chi fail reflect positive negative correlation category discriminate negative correlation estimate imbalance PC NC multi classification reduce effectiveness scheme reflect discriminate TC supervise scheme assign category specific hardly properly document category label unknown propose entropy scheme effective discriminate TC series setting performance scheme closely related datasets classifier classification aligns previous scheme consistently performs various circumstance however experimental propose entropy scheme bdc bdc outperform scheme although previous perform statistical significance improvement significant argue propose entropy scheme reflect discriminate TC effectively previous scheme characteristic scheme scheme TC task previous factor influence TC performance classifier benchmark datasets classification task specifically finding hint scheme TC task idf variant NN neighbour alleviate influence achieve optimal performance feature selection scheme chi perform binary classification multi classification normalize frequency relevant category processing bdc effectively improves classification performance category corpus skewed category distribution smooth frequency relevant category processing sdc alleviate influence however performance processing highly related quality corpus amount exist corpus local document reflect emphasize svm hence scheme perform without svm performance NN sensitive scheme svm category specific scheme bdc preferable inherently binary classifier svm multi classifier NN contrast category independent scheme bdc preferable multi classifier binary classifier direction future data generate application datasets relatively intend verify achieve performance text collection although conduct setting scheme corpus classifier classification TC task involve additional parameter removal feature selection various parameter classifier model moreover TC application seek optimize multiple compete objective usually precision recall curve receiver operating characteristic roc curve previous multi objective evolutionary algorithm  effective approach parameter simultaneously optimize multiple objective TC task parameter pareto namely describes objective future investigate MOEA estimate optimal model parameter TC task gain deeper understand behaviour scheme operator adaptable situation finally exist scheme largely focus representation textual document recent extend research distribute representation future explore incorporate propose entropy improve performance embed