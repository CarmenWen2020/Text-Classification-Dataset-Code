model complexity fundamental conduct systematic overview model complexity model complexity categorize expressive capacity effective model complexity review exist category along important factor model framework model optimization data complexity discus application model complexity understand model generalization model optimization model selection conclude propose future direction access auckland library introduction mainly due superior performance disruptive application computer vision processing computational finance however series fundamental model remain achieve substantially expressive classical machine model understand quantify generalization capability model understand improve optimization model complexity core related fundamental model complexity concerned architecture complicate model express understand complexity model precisely understand capability limitation model explore model complexity understand model investigate related fundamental statistical theory expressive model bound generalization error recent propose norm model complexity sensitivity model complexity explore generalization capability model moreover detect model complexity training insight understand improve performance model optimization regularization investigation machine model complexity date decade series discus complexity classical machine model representative model decision complexity depth leaf node another frequent model complexity analysis logistic regression foundation parameterized model model complexity logistic regression investigate perspective vapnik  theory rademacher complexity fisher information matrix razor model razor model theoretical index complexity parametric model distribution however model dramatically classical machine model decade complexity analysis classical machine model cannot directly apply straightforwardly extend model recently model complexity attract attention however knowledge exist survey model complexity lack survey emerge important motivates conduct survey article model neural network interchangeably prolific complexity classical machine model decade summarize excellent survey briefly review complexity typical model decision logistic regression bayesian network model discus difference model complexity neural network model relatively accepted standard measurement complexity decision complexity decision leaf node depth decision construct recursively splitting input propose local model leaf node principle complexity depth decision complexity quantifies query classification series investigate optimization accuracy complexity furthermore  wolf associate complexity decision complexity function certificate complexity sensitivity complexity approximate polynomial complexity function bound complexity decision logistic regression foundation parameterized model effective freedom propose complexity linear model penalize linear model vapnik  VC dimension besides rademacher complexity gaussian complexity model complexity logistic regression model VC dimension rademacher complexity data distribution consideration therefore reflect finer grain model complexity later  model complexity logistic regression model related distinguishable distribution model define complexity logistic regression model determinant fisher information matrix systematically investigate model complexity bayesian hierarchical model unique challenge complexity bayesian hierarchical model model parameter clearly define define model complexity effective parameter information theoretic argument complexity estimate difference posterior deviance deviance posterior estimate parameter approximately trace fisher information posterior covariance matrix addition complexity posterior deviance deviance information criterion model complexity model specific definition model complexity largely depends model structure model framework definition complexity accord structural characteristic complexity model framework usually cannot directly model structurally traditional machine model dramatically parameter model substantially complex traditional model therefore previous model complexity traditional machine model cannot directly apply model obtain valid complexity complexity decision depth leaf node obviously applicable model model complexity trainable parameter limited model model parameterized survey organize sect briefly introduce model complexity sect review exist expressive capacity model sect survey exist effective complexity model sect discus application model complexity sect conclude survey discus future direction model complexity model complexity category expressive capacity effective model complexity discus series important factor model complexity representative exist accordingly model complexity model complexity refer meaning model complexity refer capacity model express approximate complicate distribution function complicate distribution function parameterized model meaning capture notion model expressive capacity model effective complexity respectively expressive capacity representation capacity expressive complexity capacity capture capacity model approximate complex informally expressive capacity describes upper bound complexity model model notion consistent description hypothesis complexity hypothesis hypothesis neural network fix model structure hypothesis fix model structure model expressive capacity hypothesis complexity statistical theory complexity infinite hypothesis expressive richness hypothesis notion capture hypothesis complexity rademacher complexity hypothesis random another notion VC dimension reflect shatter hypothesis explore expressive capacity obtain guarantee learnability model derive generalization bound effective model complexity practical complexity practical expressivity usable capacity reflect complexity function model specific parameterization effective model complexity model parameter fix effective model complexity exploration various aspect model understand optimization algorithm improve model selection strategy improve model compression expressive capacity effective model complexity closely related concept expressive capacity describes expressive hypothesis model effective model complexity explores complexity specific hypothesis within hypothesis demonstrate distinction relationship model expressive capacity effective model complexity difference expressive capacity effective complexity unary polynomial function expressive capacity unary quadratic cannot express function complicate unary quadratic polynomial assign parameter correspond effective complexity parameter effective complexity becomes linear obviously expressive capacity denote hypothesis fix model structure hypothesis model effective model complexity complexity specific hypothesis EMC expressive capacity model denote MEC sup EMC reflect relationship model expressive capacity effective model complexity complex parameterized architecture model usually expressive capacity however series effective complexity model expressive capacity informally intuitively model regard container knowledge data model architecture container amount knowledge data equip parameter expressive capacity regard upper bound amount knowledge model architecture effective model complexity concerned specific model specific training dataset knowledge actually important factor model complexity  model consists static structure dynamic parametric static structure model selection principle immutable parametric objective optimization static dynamic contribute model complexity refine summarize aspect affect model complexity expressive capacity effective complexity model framework choice model framework affect model complexity factor model framework model feedforward neural network convolutional neural network activation function sigmoid relu others model framework complexity criterion directly comparable model model affect model complexity commonly model parameter hidden layer width hidden layer filter filter model framework complexity model quantify complexity criterion become comparable optimization optimization affect model complexity objective function selection algorithm hyperparameters data complexity data model affect model complexity factor data dimensionality data distribution information volume kolmogorov complexity others aspect model framework model mainly affect static structural model optimization data complexity mainly affect dynamic parametric summarize aspect affect expressive capacity effective complexity respectively aspect expressive capacity understood influence hypothesis model framework corresponds hypothesis hypothesis model framework model hypothesis shrink subset suppose model depth respectively width correspond hypothesis respectively hypothesis network model context exists layer identical subsequent layer identical mapping easy expressive capacity exceed expressive capacity recently model framework model expressive capacity model explore choice data distribution optimization algorithm reduce scope hypothesis thereby affect expression capacity rademacher complexity data dependent expressive capacity account data distribution however knowledge data complexity optimization expressive capacity model rarely explore aspect affect effective model complexity model framework model available effective model complexity effective complexity model fix parameter optimization training data optimization affect effective complexity regularization constrains freedom neural network constrains effective model complexity training data affect  complexity model optimization effective complexity model linearly  data imagenet specifically training data complexity optimization effective complexity reflect model parameter correspond aspect affect expressive capacity effective complexity respectively representative expressive capacity effective complexity overview collection model complexity neural network exist model complexity overview literature model complexity categorize angle angle focus approach model specific model angle focus approach develops explicit complexity angle applicable expressive capacity effective complexity model specific versus model approach focus model multiple model exist model complexity model specific approach model approach model specific approach focus model explores complexity structural characteristic model complexity fully feedforward neural network FCNNs   focus model complexity sum network moreover propose constraint activation function constrain nonlinearity assume activation function satisfy approach model multiple model specific model apply model complexity recurrent neural network rnns convolutional neural network cnns shallow FCNNs building connection network architecture tensor decomposition versus reduction accord explicit model complexity approach approach reduction approach model complexity defines appropriate quantitative representation model complexity linear complexity FCNNs piecewise linear activation function complexity approach reduction investigates model complexity reduce network function define explicit complexity connection network relu activation function lebesgue neural network tensor decomposition expressive capacity model expressive capacity model representation capacity expressive complexity capacity describes model approximate complex informally expressive capacity describes upper bound complexity parametric model expressive capacity model mainly explore aspect depth efficiency analyzes model gain performance accuracy depth architecture width efficiency analyzes width layer model affect model expressive capacity expressible functional investigates function express model specific framework specify parameter VC dimension rademacher complexity classic expressive capacity machine review depth efficiency series recent demonstrate architecture significantly outperform shallow depth efficiency effectiveness depth model attract specifically depth efficiency analyze architecture obtain performance model depth expressive capacity depth efficiency subcategories model reduction expressive capacity model reduction expressive capacity model reduce model understandable function analysis investigate depth efficiency intuitive representation efficiency network shallow   investigate depth efficiency sum network spn spn consists neuron compute sum input function built sum network function function  input depth function  function  input depth   establish bound hidden neuron shallow sum network function approach function hidden layer shallow spn hidden neuron neuron similarly approach function hidden layer shallow spn hidden neuron comparison shallow sum network function indicates function neuron shallow network exponentially linear growth network function representable hierarchical binary network shallow network demonstrates shallow network hierarchical binary network denote shallow network neuron  parameter kth hidden neuron activation function denote  function continuous partial derivative assumption detail correspondingly hierarchical binary network network structure hij depth network corresponds node structure hierarchical binary depth denote hierarchical binary network  function structure hij  define  approximation function input dimension fed shallow network hierarchical binary network image demonstrate approximate function  approximation shallow network trainable parameter meanwhile approximate function  approximation network trainable parameter shallow gaussian network hierarchical binary structure hij computes shallow gaussian network obtain conclusion demonstrate function compositional structure approximate shallow network approximation however parameter network shallow network investigate importance depth neural network relu activation function investigate neural network dimensional input dimensional output exists function relu neural network hidden layer width however function network hidden layer  hidden neuron investigate relu neural network input dimension exists function relu network hidden layer neuron function construct zonotope theory polyhedral theory however function minimum hidden neuron relu neural network hidden layer max   investigate neural network provably efficient shallow  analyzes limitation expressive capacity shallow neural network signum activation function signum activation function define sgn  prof exist function cannot sparsely hidden layer signum neural network limited neuron limited sum absolute output norm function nearly orthogonal function signum perceptrons sgn function generate hadamard matrix hadamard matrix matrix entry distinct orthogonal  prof function induced hadamard matrix cannot compute shallow signum network hidden neuron sum absolute output logn illustrate limitation shallow network  representation capability hidden layer network heaviside activation function heaviside activation function define sylvester hadamard matrix construct recursively iterate  function induced hidden layer heaviside network neuron hidden layer however function hidden layer heaviside network hidden neuron absolute output summary model reduction approach reduce neural network function investigate model depth capacity express function expressive capacity investigate depth efficiency another develop appropriate expressive capacity expressive capacity depth layer width model increase focus fully feedforward neural network FCNNs piecewise linear activation function relu maxout propose linear representation model complexity FCNN piecewise linear activation function input linear linear corresponds linear function linear reflect flexibility complexity model illustrate advantage model relu network dot capture boundary accurately approximate linear shallow solid image investigate FCNNs piecewise linear activation function relu maxout parameter setting model expressive capacity relu network maximum linear denote MEC bound MEC mim  hidden layer width ith hidden layer dimensionality input bound relu network hidden layer layer width approximate piecewise linear function linear rank maxout activation function expressive capacity hidden layer maxout network neuron bound MEC  MEC min  rank maxout network consists hidden layer width layer compute piecewise linear function linear conclusion maximum linear generate FCNN piecewise linear activation function increase exponentially model depth explanation depth efficiency intermediary layer model input output layer increase layer wise composition function computation exponentially allows model compute highly complex function relatively parameter improve bound maximum linear propose relu neural network layer width ith hidden layer input dimension maximal linear neural network bound MEC mid  upper bound MEC  min   topological model complexity neural network FCNN output denote define instance classify positive model complexity neural network sum betti MEC denotes ith betti dimensional betti generally distinguish characteristic algebraic topology contains instance positively classify network therefore investigate affected architecture model complexity   report upper bound series network architecture   demonstrate hidden layer network grows polynomially respect hidden layer width width hidden layer neural network grows exponentially hidden neuron hidden neuron indicates neural network expressive capacity therefore complex function shallow upper bound   network hidden input hidden layer   layer wise composition mechanism model allows model replicate behavior input depth effective width width efficiency addition depth efficiency width expressive capacity namely width efficiency worth explore width efficiency analyzes width affect expressive capacity model width efficiency important fully understand expressive capacity validate insight obtain depth efficiency investigate width efficiency neural network relu activation function extend universal approximation theorem width bound relu neural network classical universal approximation theorem hidden layer neural network activation function relu approximate continuous function compact domain desire accuracy performance lebesgue integrable function exists relu network approximate distance FN FN function neural network width hidden layer satisfies moreover explore role layer width expressive capacity quantitatively dual depth efficiency exists shallow relu network cannot approximate narrow neural network substantially increase denote FA function relu neural network depth width layer integer FB function relu neural network depth width hidden layer parameter bound constant FA approximate FB distance FA FB indicates exists shallow relu neural network cannot approximate narrow network depth constrain polynomial bound polynomial bound width efficiency exponential bound depth efficiency approximate model depth increase linearly shallow model exponential increase width approximate shallow model width increase linearly narrow model polynomial increase depth however depth cannot strictly effective width polynomial upper bound width lack polynomial upper bound ensures approximate shallow model narrow polynomial increase depth expressible functional addition depth efficiency width efficiency explore function express model specific framework specify explore expressible functional model model specific model approach model specific approach investigate function representable neural network relu activation function piecewise linear function relu neural network consists hidden layer piecewise linear function dense compactly continuous function compactly continuous function dense lebesgue lebesgue define lebesgue  function define norm conclusion extend function approximate arbitrary norm relu neural network consists hidden layer  expressive capacity neural network relu activation function sobolev lebesgue sobolev define    derivative sobolev norm define  PLP  analyze relu neural network approximate function sobolev establish upper bound model approximate function sobolev specifically define subset sobolev upper bound function exists neural network choice relu neural network consist clog layer slog neuron non zero parameter constant depends besides bound function exists relu neural network non zero explore functional neural network polynomial activation function polynomial activation function polynomial activation function model complexity benefit application powerful mathematical machinery algebraic geometry addition polynomial approximate continuous activation function explore model polynomial neural network depth polynomial architecture width layer functional define functional variety  closure functional  closure described polynomial equation functional  functional propose dimensionality functional variety denote  representation expressive capacity polynomial neural network MEC   connection polynomial network tensor decomposition specifically polynomial network CP tensor decomposition polynomial network iterate tensor decomposition decomposition fix exists  bound  min besides bottleneck polynomial network narrow layer bottleneck  polynomial network network ambient ambient  homogeneous polynomial variable polynomial network ambient satisfies  network architecture ambient helpful optimization training model approach addition model specific approach expressible functional investigate model manner specifically expressive capacity recurrent neural network rnns investigate connection network architecture tensor decomposition comparison expressive capacity rnns cnns shallow FCNNs network correspond various tensor decomposition TT decomposition CP decomposition HT decomposition image dimensional tensor tensor TT decomposition tensor compute xii   idd tensor  TT core introduce bilinear TT core bilinear performs bilinear recurrent neural network realizes TT decomposition tensor similarly canonical CP decomposition xii rvi   corresponds hidden layer FCNN network denote    hierarchical tucker HT decomposition corresponds cnn structure comparison expressive various network architecture propose rank tensor decomposition neural network complexity rank decomposition corresponds width network correspondence relationship neural network tensor decomposition model complexity rnns cnns shallow FCNNs conclusion summarize random dimensional tensor TT decomposition rank mode tensor exponentially rank CP decomposition HT decomposition approximate recurrent neural network shallow FCNN cnn exponentially width VC dimension rademacher complexity VC dimension rademacher complexity widely analyze expressive capacity generalization classical parametric machine model series investigate VC dimension rademacher complexity model VC dimension expressive capacity reflect sample shatter hypothesis VC dimension model shatter sample model expressive capacity  VC dimension feedforward neural network linear threshold gate linear threshold gate neuron compose sum function heaviside activation function parameter network depth network  prof VC dimension network  investigate VC dimension feedforward neural network piecewise polynomial activation function piecewise polynomial activation function polynomial function parameter network depth network upper bound VC dimension network WL  bound VC dimension WL later improve bound  VC dimension neural network piecewise linear activation function relu neural network layer parameter bound VC dimension network  upper bound VC dimension  rademacher complexity capture capacity hypothesis random label expressive capacity rademacher complexity model random label model expressive capacity investigate rademacher complexity neural network relu activation function relu neural network layer parameter matrix layer data matrix instance input dimension bound rademacher complexity network spectral norm frobenius norm tighter bound layer relu neural network suppose lipschitz bound function network rademacher complexity bound ssm width hidden layer bound improves bound factor rademacher complexity adversarial neural network feedforward neural network relu activation function denote depth network parameter matrix layer function adversarial loss function minx  sample perturbed around distance bound rademacher complexity bound exhibit explicit dependence input dimension model parameterized significantly parameter sample VC dimension rademacher complexity model practical guidance weak effective complexity model effective complexity model practical complexity practical expressivity usable capacity reflect complexity function model specific parameterizations effective complexity model mainly explore aspect effective complexity quantitative effective complexity model investigation capacity reality phenomenon effective complexity model expressive capacity review effective complexity expressive capacity effective model complexity requirement sensitive precise complexity effective complexity cannot directly derive model structure alone parameter model structure effective complexity effective complexity sensitive parameter model structure series devote propose feasible effective complexity model complexity depends linear splitting piecewise linear neural network input discus others piecewise linear neural network piecewise linear activation function generates finite linear input piecewise linear demonstrate linear density usually reflect effective complexity therefore series effective complexity piecewise linear activation function relu maxout piecewise linear hidden layer relu network dimensional input linear   upon hidden neuron linear neuron image propose interrelate effective complexity neural network piecewise linear activation function relu tanh specifically define trajectory input curve parametrized scalar effective complexity propose linear input sweep input trajectory effective complexity EMC model EMC linear passing trajectory specific model parameter effective complexity propose trajectory define standard arc trajectory proportional relationship complexity obtain estimate effective complexity bound trajectory layer relu neural network specifically relu neural network initialize bias initialize denote trajectory obtain transformation hidden layer trajectory trajectory bound  denotes hidden layer width similarly tanh neural network initialization trajectory bound   circular trajectory fed tanh neural network image trajectory transformation hidden layer increase trajectory layer transformation image complexity explore performance neural network report finding effective complexity grows exponentially respect depth model polynomially respect width evolution trajectory hidden layer network parameter initialize affect effective complexity inject perturbation layer exponentially perturbation remain layer regularization approach batch normalization reduce trajectory explains batch normalization model stability generalization empirically investigate relationship model complexity generalization neural network piecewise linear activation function propose model sensitivity effective complexity model sensitivity robustness reflect capacity model distinguish input distance introduce sensitivity metric input output jacobian norm trajectory piecewise linear propose jacobian norm local sensitivity assumption input perturbed within linear jacobian norm effective model complexity EMC jacobian probability function network data distribution frobenius norm propose trajectory metric developed sensitivity input perturbed linear effective complexity EMC trajectory define linear passing trajectory equidistant trajectory status encode hidden neuron complexity correlation complexity generalization demonstrate neural network robustness vicinity training data manifold model generalization capability factor associate generalization batch training random label correspond weaker robustness factor associate generalization data augmentation relu correspond robustness develop effective complexity smooth activation function propose effective complexity neural network curve activation function sigmoid tanh motivate piecewise linear piecewise linear function minimal linear approximate network linear approximation function effective complexity network piecewise linear approximation neural network curve activation function linear approximation neural network   construct piecewise linear approximation function curve activation function hidden neuron specifically define approximation error  target network analyze approximation error layer propagate remain hidden layer obtain influence approximation error hidden neuron jacobian matrix layer network approximation error specific neuron matrix output layer negligible estimation error neuron approximation  linear construct requirement approximate linear  effective complexity EMC   linear approximation function neuron width layer depth complexity investigate trend model complexity training effective complexity increase respect training iteration demonstrate occurrence overfitting positively correlate increase effective complexity regularization regularization suppress increase model complexity influence regularization approach effective complexity decision boundary model dataset NM normal regularization respectively bracket effective complexity image piecewise linear novel opportunity capture model complexity addition effective complexity piecewise linear neural network piecewise linear exploration expressive capacity piecewise linear activation function relu popular effective activation function task application local linear characteristic finite regional facilitate quantify analyze neural network model complexity piecewise linear activation function metric effective complexity piecewise linear introduce effective complexity investigate descent phenomenon descent phenomenon neural network model training epoch training data increase performance decrease increase capture descent training complexity sensitive training data distribution model architecture propose effective complexity maximum sample model zero training error  max ES  data distribution threshold training error training procedure  error model training training effective complexity  investigate evolution training  sufficiently sufficiently training perturbation increase effective complexity  improve performance however  perturbation increase effective complexity  hurt performance approach generalization model introduce notion model complexity fisher rao norm focus fully neural network activation function satisfies model complexity fisher rao norm EMC parameter bias neural network inner fisher information matrix loss function tensor introduce geometric invariance complexity satisfy generalization capability invariance essentially continuous operation exactly prediction generalization equivalence obtain continuous transformation specific parameterization affect generalization complexity investigate generalization satisfy invariance demonstrate fisher rao norm satisfies invariance denote parameter setting model fisher rao norm fisher rao norm remains invariant node wise rescale model effective complexity training sample model achieves zero training error fisher rao metric effective complexity along developed capacity reality phenomenon explore gap effective complexity expressive capacity model  shallow fully neural network complex function previously neural network sometimes parameter network specifically model propose shallow model output model mimic model shallow mimic model achieve accuracy model however shallow model cannot directly label training data achieve accuracy recognize knowledge distillation phenomenon  conjecture strength arise architecture training algorithm shallow architecture architecture easy optimization technique moreover propose mimic function complex model shallow model function model really complicate suggests gap practical effective complexity model theoretical bound expressive capacity capacity reality phenomenon   investigate capacity reality phenomenon fully neural network piecewise linear activation function relu propose effective complexity linear input volume boundary linear investigate relu neural network dimensionality input output linear effective complexity average linear grows linearly respect neuron exponential upper bound specifically relu neural network bias neuron randomly initialize bound average linear proportional neuron training linear training dataset breakpoints activation function relu hidden neuron investigate relu neural network input dimensionality exceeds denote volume boundary linear input estimation model complexity EMC  BN  BN continuous boundary linear data distribution parameter initialization assumption volume linear boundary approximately EMC demonstrates average boundary depends neuron depth model conclude effective complexity neural network theoretical bound function neural network complex shallow discussion effective model complexity relatively promising useful detect effective model complexity training investigate usefulness optimization algorithm role regularization generalization capability furthermore effective model complexity model compression ratio effective model complexity reflection information volume model effective complexity model selection balance resource utilization model performance addition effective complexity capacity reality phenomenon series effective complexity model model comparison effective complexity worth explore effective complexity multiple model architecture effective complexity affected choice model architecture moreover specify granularity effective complexity requirement effective complexity correspondingly application scope granularity effective complexity specify clarify typically effective complexity non zero parameter obviously sufficient optimization application model complexity model complexity application review application model complexity namely understand model generalization capability model optimization model selection model complexity understand model generalization capability model parameterized model parameter optimal training sample however parameterized neural network exhibit generalization capability complex network usually generalize observation contradiction classical notion function complexity  razor prefer model generalization capability parameterized model statistical theory expressive capacity hypothesis complexity bound generalization error specifically function representable model structure function algorithm training dataset ED empirical error generalization error gap generalization error  error bound ED  ED quantify analyze expressive capacity rademacher complexity analyze generalization error relu neural network basis norm norm basis exist basis relu neural network linearly independent input output relu neural network express multiplication basis therefore basis explain generalization behavior relu neural network generalization error relu neural network bound function basis norm series investigate model complexity explain generalization capability model suggests perspective generalization complexity satisfy model complexity generalizes requirement summarize empirical phenomenon satisfied complexity zero training error network label generalization complexity network random label increase hidden parameter decrease generalization error decrease complexity training architecture training dataset optimization algorithm zero training error model generalization complexity desideratum investigate complexity norm robustness sharpness requirement define complexity perspective model sensitivity identify empirical correlation complexity model generalization capability operation generalization batch training correspond sensitivity imply effective model complexity similarly operation generalization data augmentation correspond sensitivity imply effective model complexity define complexity fisher rao norm investigate model generalization capability complexity generalization satisfy invariance invariance generalization capacity depends equivalence obtain model parameterizations prediction specific parameterization model affect generalization complexity fisher rao norm honor invariance explain generalization capability model model complexity optimization model optimization concerned neural network model successfully specifically optimization model model parameter minimize loss function non convex loss function typically understand requirement model generally performance evaluate training constraint model complexity widely metric optimization traceable metric effective model complexity neural network monitor model optimization understand optimization progress metric verify effectiveness improvement optimization algorithm investigate descent phenomenon training effective complexity maximal dataset zero training error achieve descent phenomenon function effective complexity propose regularization demonstrate effectiveness regularization impact complexity model complexity inspires exploration effectiveness optimization approach   boundary volume linear complexity relu neural network training average boundary volume linearly proportional neuron irrelevant depth model demonstrates model complex function shallow related optimization algorithm  performance due model easy shallow architecture optimization algorithm exploration effectiveness optimization approach relationship model structure model complexity model selection specific task feasible model structure task variety model architecture complexity model model selection model selection tradeoff prediction performance model complexity prediction accuracy essential goal model model capture underlie hidden training data achieve prediction accuracy amount knowledge obtain accuracy model expressive capacity freedom training extent model parameter complexity overly complex model incur unnecessary resource consumption storage computation unnecessary resource consumption avoid particularly practical application extent simpler model comparable accuracy prefer complicate maintain tradeoff accuracy complexity model complex data achieve accuracy model highly complicate understand model complexity develop effective complexity premise model selection strategy instance michel  propose model selection strategy tensor network sum neural network combine empirical risk minimization model complexity penalty model model neural architecture NAS popular model selection automatically selects neural network architecture task overly complex model training become serious obstacle neural architecture accuracy complexity tradeoff important consideration neural architecture propose progressive neural architecture convolutional neural network architecture increase model complexity therefore progressive neural architecture complexity model requirement prediction accuracy propose automatic model selection fully neural network yield balance prediction accuracy model complexity investigate network model selection approach propose contrast estimate distribution model complexity network propose distribution model complexity network investigate popular NAS approach  DARTS significant difference network approach conclusion future direction survey model complexity summarize aspect affect model complexity angle overview exist model complexity discus model complexity namely model expressive capacity effective model complexity overview expressive capacity aspect depth efficiency width efficiency expressible functional VC dimension rademacher complexity overview effective complexity aspect effective complexity capacity reality phenomenon discus application model complexity generalization capability optimization model selection model complexity infant stage challenge future expressive capacity model challenge model parameterized sufficient expressive task data expressive capacity sufficient task obtain bound expressive capacity model sufficient task narrow layer limit expressive capacity model model parameter explore bottleneck model depth width expressive capacity model become bottleneck restricts expressive capacity   discover relu network width constrain input dimensionality limited expressive width layer relu network bottleneck expressive identify bottleneck layer width polynomial network polynomial network narrow layer layer network correspond convex functional convex functional benefit optimization model easy research bottleneck expressive capacity tackle model model selection model compression prune progress effective complexity largely developed direction expressive capacity model effective model complexity challenge effective complexity capable capture granularity difference model model architecture optimization algorithm previous define explore effective complexity mainly perspective piecewise linear fisher rao metric trainable sample however effective complexity largely unexplored valuable direction model complexity comparison promising direction model model framework model expressive capacity model sufficiently dataset obtain zero training error effective complexity understand generalization capability model complexity comparison useful model complexity model model selection however exploration model comparison expressive capacity effective complexity model limited expressive capacity shallow FCNNs cnns rnns network architecture tensor decomposition however sophisticated model involve explore