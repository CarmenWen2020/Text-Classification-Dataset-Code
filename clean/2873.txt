minibatching technique extensively adopt facilitate stochastic computational efficiency parallel compute machine data mining indeed increase minibatch decrease iteration complexity minibatch query converge decrease processing minibatch parallel however gain usually saturate minibatch computational complexity access deteriorate hence determination appropriate minibatch iteration computational complexity important maximize performance computational resource define optimal minibatch minimum minibatch exists stochastic achieves optimal iteration complexity optimal minibatch moreover katyusha proceeding annual acm  symposium theory compute vol acm DASVRDA   advance neural information processing vol propose combination acc svrg  advance neural information processing vol APPA advance neural information processing vol optimal minibatch optimal minibatch competitor regularize logistic regression iteration complexity optimal minibatch linearly decrease minibatch increase reasonable minibatch finally attain iteration complexity confirms computational efficiency optimal minibatch theory access auckland library introduction along increase demand machine data mining efficient computation become important efficient optimization technique stochastic optimization mainstream accelerate execution machine training attempt enhance stochastic optimization nesterov acceleration variance reduction technique moreover minibatch technique nowadays massive datasets parallel computation combine sophisticated technique optimization progressively improve validate analysis theoretical convergence rate besides upper bound important derive bound computational complexity cannot outperform optimal convex regularize empirical risk minimization typical machine optimal perform stochastic gradient evaluation already obtain via accelerate stochastic variance reduction propose variance reduction technique sag sdca svrg saga developed achieve linear convergence rate strongly convex objective whereas vanilla stochastic gradient descent sgd achieve sublinear convergence rate acceleration technique acc svrg svrg employ nesterov acceleration propose minibatch technique meta algorithm accelerate exist combine inexact proximal propose katyusha DASVRDA recently propose pure primal accelerate svrg instead apply meta algorithm katyusha employ negative momentum stabilize accelerate update DASVRDA employ outer acceleration technique addition inner acceleration technique acc svrg achieve optimal computational complexity described gradient evaluation access achieve pre specify accuracy issue bound optimal computational complexity evaluate without account minibatch strategy per iteration stochastic problematic minibatch strategy standard nowadays effective parallel compute perform faster computation extreme deterministic data per iteration instead moreover nesterov acceleration achieves optimal iteration complexity deterministic iteration minibatch query however deterministic computation per iteration sample achieve optimal computational complexity extreme accelerate stochastic achieve optimal computational complexity minibatch accelerate deterministic achieve optimal iteration complexity minibatch minibatch implies computational complexity iteration complexity proportional iteration complexity processing minibatch parallel ignore communication hence reduce minibatch however unnecessarily minibatches preferable gain usually saturate computational complexity deteriorate hence determination appropriate minibatch iteration computational complexity important maximize performance computational resource fundamental aim address minibatch maintain optimal iteration complexity optimal computational complexity strongly convex minibatch sample accuracy optimal computational complexity non strongly convex minibatch sample lipschitz smoothness accuracy investigate minibatch computational efficiency parallel computation environment minimax define optimal minibatch minibatch exist stochastic achieves optimal iteration complexity optimal minibatch theory precisely estimate optimal minibatch existence optimal minibatch affect optimal complexity interestingly optimal minibatch maximally reduce instead strongly convex ill non strongly convex accuracy sufficiently beneficial parallel computation parallelization sufficient achieve moreover introduce concrete algorithm perform optimal minibatch strategy contribution summarize summary optimal minibatch computational complexity strongly convex non strongly convex specify minimax optimal minibatch smoothness parameter convexity parameter accuracy optimal minibatch strongly convex non strongly convex hide logarithmic acc svrg approximate proximal algorithm APPA katyusha DASVRDA nearly optimal mini batch achieve optimal iteration complexity optimal minibatch optimal minibatch characterize achieve optimal iteration complexity simultaneously minibatch validate theory numerical regularize logistic regression iteration complexity optimal minibatch linearly decrease minibatch increase reasonable minibatch finally attain iteration complexity indicates existence optimal minibatch computational efficiency lose minibatch optimal predict theory related stochastic optimization suffer variance gradient estimation convergence regard iteration therefore development reduce variance important research direction purpose stochastic estimator gradient sag svrg saga sarah developed speedup variance reduction achieve moreover acceleration propose another popular variance reduction minibatching stochastic gradient estimator consist bunch advantage minibatching speedup parallel computation stochastic without minibatching parallelize inherently sequential whereas minibatching easily parallelize processing minibatches computational node devote develop efficient minibatching complexity analysis instance stochastic gradient descent minibatching benefit minibatching parallel computation introduce minibatch variant accelerate gradient descent convergence rate analysis combination minibatching dual coordinate variance reduction propose respectively moreover existence optimal minibatch propose acc svrg combination accelerate variance reduction minibatching machine incorporate minibatches recently linear speedup computational complexity saga minibatch refer optimal minibatch feature optimal minibatch however saga variance reduce stochastic gradient descent iteration complexity gradient descent define optimal minibatch optimal minibatch achieve optimal iteration complexity accelerate gradient descent notion optimal minibatch mapreduce another distribute processing scheme aim datasets mapreduce framework input data data chunk distributes computational node mapreduce mainly consists reduce operator manipulate merge function automatically execute parallel extract useful knowledge bigdata machine mapreduce framework extensively propose instance model NN hierarchical cluster density cluster cluster extreme machine adapt mapreduce framework preliminary introduce notation definition treat formalize minx  function lipschitz smooth strongly convex function lipschitz smoothness convexity frequently optimization literature ensure convergence stochastic optimization denote euclidean inner norm respectively definition differentiable function smooth strongly convex non strongly convex strongly convex strongly convex convex strongly convex ratio refer important usually affect convergence gradient instance lipschitz smoothness convexity accuracy optimal complexity explain assume function smooth strongly convex stochastic minibatch convergence rate stochastic gradient essentially compose deterministic optimization rate noiseless variance variance reduction technique important accelerate convergence stochastic simplest technique minibatching multiple instance stochastic gradient estimator concretely stochastic gradient descent minibatching iterate described gik gik denotes average  function indexed minibatch sample randomly deterministically minibatch variant stochastic optimization vanilla stochastic gradient descent stochastic minibatch formally define definition randomly deterministically sample minibatch algorithm stochastic minibatch access oracle gik gik iterate becomes deterministic optimization gradient deterministic sample svrg described later specific algorithm respect stochastic gradient evaluation exists instance algorithm computational complexity evaluate performance stochastic optimization introduce convergence criterion computational complexity global convergence guaranteed stochastic optimization convex objective gap adopt convergence criterion convergence criterion threshold iterate obtain stochastic optimization  rdf expectation respect stochastic gradient performance stochastic optimization evaluate spent accurate criterion commonly stochastic optimization literature definition algorithm objective function complexity stochastic gradient evaluation iteration complexity minibatch query obtain accurate relationship remark stochastic svrg involve calculation gradient iteration complexity account complexity coincide stochastic without minibatching however stochastic minibatch optimal complexity strongly convex subsection briefly explain optimal iteration complexity strongly convex complexity stochastic minibatch bound bound iteration complexity derive via relationship statement proposition stochastic bound complexity exists finite sum smooth strongly convex function proof assumption exists finite sum convex function proof however bound proposition meaningless minibatch derive bound iteration complexity bound deterministic gradient obtain function deterministic algorithm summary bound iteration complexity max however construction restrictive stochastic forth proposition enables construct considerably wider bound iteration complexity deterministic assumption proposition interested svrg acc svrg APPA katyusha DASVRDA proposition finite sum convex quadratic function stochastic minibatch assume becomes deterministic iterates coincide moreover assume parameter update proof denote iterate  jensen inequality prof proposition important consequence deduce proposition amplitude function yield bound verify deterministic construct quadratic function countless decompose finite sum maintain smoothness convexity secondly iteration complexity stochastic variant non accelerate deterministic gradient cannot optimal iteration complexity minibatch iteration complexity non accelerate optimal sgd svrg saga regard bound iteration complexity bound achievable stochastic minibatch efficient minibatch positive optimal complexity non strongly convex non strongly convex counterpart previous subsection complexity stochastic minibatch bound hence argument proposition iteration complexity proposition stochastic bound complexity exists finite sum smooth convex function addition bound bound iteration complexity deterministic gradient non strongly convex function deterministic algorithm obtain bound max moreover proposition non strongly convex bound attain non accelerate sgd svrg saga previous subsection bound almost attain optimal minibatch efficient minibatch benefit minibatching introduce notion minibatch optimality viewpoint reduce benefit minibatching parallel compute stochastic gradient ignore communication processor minibatch almost corresponds iteration complexity described earlier iteration complexity stochastic gradient decompose deterministic variance instance variance stochastic gradient iteration complexity uniformly optimal minibatch without variance reduction strongly convex complexity deterministic  non strongly convex complexity deterministic  rate theoretical limit corresponds deterministic unrealistically minibatch limit strongly convex non strongly convex respectively phenomenon variance usually deterministic minibatches affect complexity important investigate minibatch achieve optimal iteration complexity minimax construct optimal minibatch combine efficient variance reduction acceleration technique summary interested optimal minibatch definition optimal minibatch strongly convex algorithm minibatch function finite sum smooth strongly convex function optimal minibatch define mina  addition optimal minibatch strongly convex stochastic optimal minibatch satisfies optimal iteration complexity strongly convex non strongly convex define optimal minibatch minimax replace constraint constraint definition optimal minibatch non strongly convex algorithm minibatch function finite sum smooth convex function optimal minibatch define mina  addition optimal minibatch non strongly convex stochastic optimal minibatch satisfies optimal iteration complexity non strongly convex optimal minibatch acc svrg without APPA bound minibatch introduce minibatch efficient acc svrg without APPA katyusha DASVRDA later sect nearly optimal minibatch acc svrg exhibit minibatch efficiency acc svrg succeed achieve optimal iteration complexity efficient minibatch although cannot accelerate complexity however improvement ill strongly convex non strongly convex combine acc svrg acceleration technique APPA mention literature hence review algorithm detailed theoretical analysis appendix acc svrg multi stage nesterov accelerate gradient svrg minibatching perform stage acquire sufficient acceleration convergence variance reduction overall procedure acc svrg described algorithm relationship ingredient depict depicts ingredient acc svrg APPA acc svrg achieves optimal iteration complexity efficient minibatch acceleration variance reduction ill computational complexity improve combination APPA improvement minibatch efficiency image strongly convex complexity obtain nlog hence optimal iteration complexity nearly optimal complexity achieve reasonable minibatch convergence improve apply APPA APPA objective function proximal iterate approximately minimize successively inner solver acceleration overall scheme APPA described algorithm computational complexity APPA apply sag saga MISO svrg without minibatching improve  optimal complexity hide extra logarithmic factor acc svrg APPA appendix detailed derivation theorem assume smooth strongly convex function parameter APPA parameter acc svrg iteration complexity complexity APPA remarkable feature almost optimal iteration complexity almost optimal complexity achieve simultaneously efficient minibatch although incorporate APPA acc svrg catalyst approach propose another proximal acceleration scheme non strongly convex non strongly convex derive complexity acc svrg without APPA complexity strongly convex price factor regularization become strongly convex lipschitz smooth therefore complexity acc svrg without APPA obtain nlog acc svrg APPA achieves complexity efficient minibatch hence acc svrg without APPA almost achieves optimal iteration complexity optimal complexity reasonable minibatch non strongly convex katyusha DASVRDA katyusha DASVRDA minibatch efficient incorporate additional APPA vanilla acc svrg improve performance ill specifically katyusha negative momentum reduce stochastic gradient DASVRDA essentially nesterov acceleration outer loop acc svrg strongly convex algorithm exhibit complexity optimal iteration complexity achieve moreover non strongly convex algorithm exhibit complexity nlog optimal iteration complexity achieve  summary minibatch efficient acc svrg APPA theoretical comparison summarize iteration complexity variance reduction sag svrg saga svrg APPA   acc svrg APPA katyusha DASVRDA strongly convex non strongly convex along minimal minibatch complexity notation hide extra logarithmic stem APPA reference iteration complexity arbitrary minibatch rightmost derive complexity price factor regularization analysis non strongly convex strongly convex non strongly convex acc svrg APPA katyusha DASVRDA achieve almost optimal iteration complexity efficient minibatch actually optimal logarithmic factor sect whereas accelerate svrg APPA   obtain optimal iteration complexity non accelerate sag svrg saga cannot obtain optimal iteration complexity achievable iteration complexity minimal minibatch strongly convex  achievable iteration complexity minimal minibatch complexity non strongly convex optimal minibatch strongly convex previous introduce achieve almost optimal iteration complexity strongly convex minibatch def notation respect minibatch optimal minimax specific algorithm stochastic minibatch bound complexity interested proposition positive stochastic minibatch optimal respect iteration complexity exists finite sum compose smooth strongly convex function max proof assumption exists finite sum smooth strongly convex function achieves optimal iteration complexity proposition bound optimal minibatch strongly convex almost corresponds minibatch attain minibatch efficient therefore conclude minibatch actually nearly optimal logarithmic factor acc svrg APPA katyusha DASVRDA optimal minibatch strongly convex non strongly convex non strongly convex explain acc svrg APPA katyusha DASVRDA achieve almost optimal iteration complexity minibatch def  notation respect minibatch optimal minimax specific algorithm stochastic minibatch bound complexity interested proposition positive stochastic minibatch optimal respect iteration complexity exists finite sum compose smooth convex function max  proof assumption exists finite sum smooth convex function achieves optimal iteration complexity proposition bound optimal minibatch non strongly convex almost corresponds minibatch attain minibatch efficient therefore conclude minibatch actually nearly optimal logarithmic factor acc svrg katyusha DASVRDA optimal minibatch non strongly convex characterization optimal minibatch feature optimal minibatch achieve optimal iteration optimal complexity simultaneously indeed confirm characterization optimal minibatch strongly convex non strongly convex proposition strongly convex non strongly convex stochastic minibatch optimal minibatch achieves optimal iteration optimal complexity simultaneously minibatch proof strongly convex optimal minibatch strongly convex arbitrary finite sum smooth strongly convex function achieves optimal iteration complexity optimal minibatch achieves optimal complexity minibatch stochastic assume optimal iteration complexity strongly convex finite sum smooth strongly convex function proposition equation optimal minibatch strongly convex non strongly convex optimal minibatch non strongly convex arbitrary finite sum smooth convex function achieves optimal iteration complexity optimal minibatch achieves optimal complexity minibatch stochastic assume optimal iteration complexity non strongly convex finite sum smooth convex function proposition equation optimal minibatch non strongly convex concludes proof therefore non optimal complexity non optimal minibatch unnecessarily minibatches wasteful optimal minibatch improvement computational complexity remark characterization optimal minibatch deduce latter proposition tight minimax strongly convex optimal minibatch exists finite sum smooth strongly convex function non strongly convex optimal minibatch exists finite sum smooth convex function numerical conduct numerical validate theoretical analysis numerically verify minibatch optimal actually unnecessarily minibatches helpful experimental setting experimental setting summarize appendix detail task standard regularize logistic regression binary classification exp data regularization parameter data publicly available data rcv  feature normalize feature vector data hence upper bound smoothness parameter objective maxi stochastic gradient minibatch setting svrg without APPA acc svrg without APPA DASVRDA computational memory per minibatch oracle minibatch regularization parameter minibatch regularization parameter determines convexity parameter objective hence theoretically optimization becomes acceleration become effective non accelerate decrease regularization parameter objective generally strongly convex theoretically accelerate superior non accelerate parameter tune data minibatch regularization parameter fairly automatically tune implement algorithm hyper parameter loss tune parameter implement algorithm entire procedure numerical illustrate summary data tune hyper parameter implement algorithm illustration entire procedure numerical normalize input data feature vector euclidean norm optimizer epoch loss hyper parameter candidate hyper parameter minimum loss optimizer epoch hyper parameter evaluate minibatch oracle satisfy procedure independently execute optimizer data image numerical strongly convex comparison described setting strongly convex minibatch oracle minibatch oracle achieve objective gap chose choice reasonable typical minimax optimal statistical error achieve rate goal empirical risk minimization minibatch terminate data plot depict achieve desire accuracy observation consideration observation minibatch oracle decrease almost linearly approximately perform almost equally consideration becomes hence theoretically deduce iteration complexity minimal minibatch iteration complexity minibatch therefore theoretical observation observation minibatch svrg APPA acc svrg acc svrg APPA DASVRDA significantly outperform vanilla svrg iteration complexity svrg APPA linearly minibatch acc svrg acc svrg APPA DASVRDA decrease linearly decrease rate approximately consideration becomes roughly difference svrg APPA outperform svrg perform acc svrg APPA DASVRDA minibatch theoretical performance however iteration complexity svrg APPA sub linearly minibatch decrease rate minibatch become contrast acc svrg APPA DASVRDA achieve linear decrease rate minibatch observation validate theoretical implication observation DASVRDA significantly outperform data moreover acc svrg APPA outperform vanilla acc svrg rcv iteration complexity acc svrg APPA DASVRDA decrease linearly comparison minibatch stochastic gradient minimize logistic loss regularization parameter regularization parameter rcv  data axis minibatch oracle achieve accuracy axis minibatch minibatch terminate data plot depict achieve desire accuracy image comparison minibatch stochastic gradient minimize logistic loss regularization parameter regularization parameter rcv  data axis minibatch oracle achieve accuracy axis minibatch minibatch terminate data plot depict achieve desire accuracy image consideration regime becomes hence accelerate beneficial indeed optimal minibatch acc svrg APPA DASVRDA converge stably minibatch efficiency aforementioned linear decrease acc svrg without APPA somewhat lack stability minibatch optimal essentially exhibit comparable iteration complexity svrg svrg APPA suffer fail converge therefore observation adequately explain theoretical numerical non strongly convex comparison described setting non strongly convex minibatch oracle minibatch oracle achieve objective gap chose choice reasonable typical minimax optimal statistical error achieve rate goal empirical risk minimization minibatch terminate data plot depict achieve desire accuracy observation consideration observation  optimal minibatch acc svrg APPA DASVRDA typically outperform minibatch oracle minibatch linear decrease nearly saturation threshold particularly DASVRDA stable convergence consideration setting desire accuracy hence theory predicts optimal minibatch linear decrease exactly convergence APPA possibly unstable additional hyper parameter tune discussion earlier non accelerate sag svrg saga cannot achieve optimal iteration complexity optimal minibatch moreover unlike acc svrg become optimal minibatch apply APPA acceleration although attain optimal complexity fix APPA iteration complexity svrg APPA logarithmic factor reduce corresponds minimum immediately achieve optimal iteration complexity therefore svrg APPA optimal minibatch hence acceleration inner loop intuitive explanation requirement optimal minibatch iteration complexity stochastic minibatch bound deterministic variant acceleration scheme nesterov however nesterov acceleration sensitive generally cannot attain optimal iteration complexity stochastic setting variance reduce previous analysis nesterov acceleration svrg minibatch minibatching achieve optimal iteration complexity namely stochastic setting nesterov acceleration combine minibatching almost convergence rate sgd nesterov acceleration combine svrg without minibatching iteration complexity svrg minibatching combine svrg almost iteration complexity gradient descent earlier therefore combination technique nesterov acceleration minibatching svrg essential attain optimal iteration complexity efficient minibatch requirement sufficient acc svrg additional technique potential additional technique acceleration outer loop APPA DASVRDA another negative momentum katyusha rigorously observation characterization optimal minibatch conclusion optimal iteration complexity stochastic minibatch finite sum convex utilize exist bound deterministic stochastic derive optimal minibatch minimax achieve optimal iteration complexity strongly convex non strongly convex existence optimal minibatch acc svrg APPA katyusha DASVRDA svrg APPA   minibatch optimal moreover characterize optimal minibatch achieve optimal iteration complexity simultaneously minibatch finally theoretical analysis validate experimentally regularize logistic regression