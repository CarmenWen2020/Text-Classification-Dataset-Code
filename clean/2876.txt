feature selection considerable attention decade however continuously challenged emerge issue semi supervise multi label promising novel approach refer approach combine data consist amount unlabeled instance multi label instance semi supervise multi label feature selection conventional feature selection algorithm regard stability robustness respect data address weakness improve robustness feature selection dimensional data document develops ensemble methodology resampling data bagging random subspace RSM additional random sub label strategy RSL propose framework contributes enhance stability feature selection algorithm improve performance research finding illustrate bagging RSM improve stability feature selection increase accuracy RSL address label correlation concern multi label data finding series conduct benchmark data classification task promising highlight propose outperforms algorithm comparable access auckland library introduction feature selection vital importance data mining owe ability improve performance reduce remove irrelevant redundant feature researcher interested approach feature selection however filter proven achieve performance computational complexity filter characteristic data feature reflect statistical criterion selection basis feature ranked specific threshold relevant selection criterion compute feature significantly accord supervision data selection supervise context relies assess correlation feature label whereas unsupervised feature selection due lack prior domain knowledge relevance feature variance separability another configuration arises availability application label sample alongside amount unlabeled data refer semi supervise mixed criterion useful feature moreover presence supervision information express label scenario label multi label data whilst label data belong label mutually exclusive multi label associate label simultaneously without exclusivity diversity research direction feature selection label feature selection widely investigate yield various ReliefF information gain correlation feature selection CFS feature selection multi label data however stage promising development prospect algorithm feature selection struggle dimensionality data feature increase significantly feature selection algorithm usually lose accuracy filter feature selection algorithm  dependent data variance slight data skewed feature eventually bias selection procedure unstable feature selection algorithm context feature selection algorithm stable feature training data variation metric quantify stability feature selection algorithm primarily motivate issue regard ensemble artifact satisfy overfitting stability issue   empirically bagging reduces variance improves stability underlie classifier besides researcher overfitting avoid project dimensional data dimensional random subspace research propose ensemble multi label   random label  construct ensemble classifier random subset label  merit efficiently label correlation propose hierarchy multi label classifier homer classifier operates subset label introduce ensemble ensemble classifier chain ecc chain binary classifier upon prediction previous multi label text categorization robust ensemble  adaboost MH adaboost MR algorithm apply adaboost weak classifier similarly ensemble paradigm aim improve robustness feature selection dimensional data multi label sample selection stable feature subset problematic proposes extension CLS combine random sample technique iteratively apply data instance dimensional label specifically bagging bootstrap sample data whilst random subspace RSM sample feature random sample without replacement perform label RSL RSM performs label purpose approach fold bagging RSM reduce variance improve stability feature selection increase accuracy RSL address particularly label correlation concern multi label data worth approach apply classification purpose explanation CLS spectral graph theory besides function CLS modify representative similarity instance label subset function applicable classification regression structure sect discus issue related semi supervise feature selection multi label ensemble detailed introduction CLS algorithm learner ensemble explains depth propose subsequently sect describes experimental framework detailed description along detailed comparison algorithm practical application mail filter effectiveness proposal finally wrap finding identifies research sect related briefly review relevant research related semi supervise feature selection label data characteristic multi label shed ensemble semi supervise feature selection feature selection semi supervise context generally pairwise constraint instance belong link cannot link respectively regard research conduct algorithm  link cannot link constraint unlabeled data appropriate feature preserve local data structure pairwise constraint attempt reduce sensitivity feature selection constraint subset combination unsupervised supervise laplacian constraint multiplication correspond objective function   introduce elaborate couple function constrain laplacian CLS constraint selection prior feature selection improve performance issue address  address redundant feature liu zhang pairwise constraint sparse  feature selection propose link cannot link constraint discriminate regularization focus directly local discriminative structure data recently advanced technique devise feature selection propose feature selection technique regularize regression equip generalize uncorrelated constraint zhang propose novel feature selection advantage previous knowledge regard pairwise constraint minimization data reconstruction error graph embed propose feature selection algorithm neighborhood rough model bucket trie structure guarantee optimal minimum reduction adopt global strategy propose framework involves cnn feature extraction  data propose multi framework fusion feature classifier granular compute introduce constraint similarity matrix compute feature subspace ass relevance subset feature multi label multi label data originate text categorization document related topic simultaneously recently multi label numerous application protein function analysis semantic scene annotation sentiment categorization zhang fang propose novel stage partial multi label  elicit credible label candidate label model induction novel partial multi label  remove noisy label multi label another closely related task multi output regression target variable multi output regression arises domain predict vehicle component stock price prediction ecological model extension label classification approach cope multi label data generally traditional algorithm approach category transformation algorithm adaptation former pas transform label label algorithm drawback cannot efficiently label correlation algorithm adaptation extend label algorithm directly without resort transformation data similarly feature selection multi label propose rethink label feature selection algorithm accord aforementioned framework propose mostly transformation binary relevance label powerset binary relevance prediction firstly transform multi label data data label data contains multi label data label label label subset correspond respectively afterwards binary classifier built subset label positively predict binary classifier undoubtedly binary relevance account correlation label suggests label powerset creates subset generate multi data upon apply traditional multi classifier subset label correspond predict approach account correlation label however serious disadvantage generate associate issue commonly imbalance bias ensemble ensemble algorithm combine multiple learner classify experimentally establish ensemble overall accurate individual constituent individual member diverse ultimately challenge appropriate technique diversity within ensemble accordingly combine scheme developed manipulate data manipulate algorithm manipulation data boost bagging random subspace commonly employ model generate replicates data random subspace proceeds random selection feature subspace individual classifier bagging boost classifier resampling training classification decision usually obtain majority voting difference bagging boost resampling technique former obtains bootstrap sample uniformly sample replacement data latter resamples  data emphasis instance misclassified previous classifier ensemble technique combine obtain viable classifier illustrate breiman combine bagging random subspace random ensemble algorithm manipulation stack error output code combine multiple model apply data similarly ensemble classification ensemble technique improve stability feature selection algorithm respect numerous research investigate ensemble technique label feature selection ensemble framework  ensemble constraint laplacian semi supervise feature selection  consists resampling data bagging random subspace strategy feature CLS selector feature relevance across generate replicates data average feature zhang ensemble exploit pairwise constraint algorithm performs multiple constraint multiple bootstrapped constraint subset propose algorithm bagging constraint BCS construct individual component constraint subset generate resampling pairwise constraint constraint instead employ selector multiple modify version data ensemble feature selection technique combine multiple unstable feature selector yield robust perform ensemble selector ensure diversity ensemble bagging generate bootstrap sample data liu  inspire approach ensemble improve overall accuracy granular compute extensive empirical evidence encompass selection algorithm application domain applicability utility ensemble feature selection dimensional data another recent survey     author overview various issue related ensemble label feature selection surprisingly focus application ensemble multi label feature selection spite proven potential research proposes ensemble data manipulation specifically bagging random subspace unified framework semi supervise feature selection multi label ensemble separately combine task improve feature selection data partially label multi label simultaneously propose ensemble variant CLS algorithm applicable data associate label refer multi label addition bagging RSM propose framework encompasses another combine strategy randomly sample without replacement label RSL hence data replicates modify along instance feature label enrich diversity efficiently feature label correlation constrain laplacian CLS dedicate detailed presentation CLS subsequently algorithm building ensemble framework CLS feature selection approach feature selection semi supervise multi label context CLS multi label extension CLS goal CLS combine laplacian constraint mixed selection criterion applicable semi supervise multi label data configuration data label XL CLS regard multi label generalization constraint laplacian contrary data unlabeled XU CLS conventional laplacian nutshell CLS aim feature comply constraint preserve geometrical structure data discriminative feature instance link inversely disparate instance cannot link constraint geometrically apart neighborhood euclidean distance constraint express CL multi label ML pairwise constraint explicitly user automatically driven prior domain knowledge label instance belong link cannot link assumption label instance associate label hence pairwise similarity label suffices membership instance contrary multi label associate subset label instead tricky constraint regard parameter define objective function constraint denote notation definition constrain laplacian instance label data XL associate target vector   lth label label otherwise quantify resemblance training instance correspond label jaccard index pairwise similarity label subset accordingly constraint compute continuous multi output data instance associate subset jaccard index replace euclidean distance feature selection algorithm applicable multi output regression pij instance powerful constraint pij link link parallel conventional constraint inversely pij cannot link cannot link pij linkage strength approach constraint multi label intuitive manner similarity instance accord label addition label important label knowledge concern data appropriate label alternatively label procedure compute automatically data beforehand respect instance label accord occurrence data label obtain counting occurrence average instance selection data CLS feature mathematically formulate  fri frj sij    objective function minimize pairwise similarity instance sij define instance belong XL sij evaluate  neither label vice versa sij otherwise sij sij  XL XL otherwise regularization parameter tune data   define respectively  fri pij fri XL XL otherwise  frj pij XL XL otherwise data feature   density matrix define dii  CLS explanatory essence discriminative feature instance highly related constraint pij important label geometrically neighborhood another instance weak constraint bound vicinity relationship consequently feature CLS eventually relevant inversely feature instance apart constraint important label penalize consequently finally discard instance label label respectively correspond pij belongs link context label objective function numerator sij maximize fri frj feature contrast XL XL label pij equivalent cannot link constraint denominator fri frj feature finally instance belong unlabeled data XU feature variance retain informative feature assume label importance algorithm summarizes constrain laplacian algorithm input semi supervise multi label data rank feature accord CLS algorithm construct matrix constraint label similarity density matrix subsequently feature feature calculates correspond CLS eventually feature sort accordingly ascend CLS max logm indeed algorithm operation matrix compute operation computes feature MN operation sort feature accord estimate mlog explains CLS function context spectral graph theory reasonable criterion relevant feature minimize objective function CLS optimize fri frj sij    minimize former maximize latter resolve optimization propose construct pre define graph respectively relevant feature respect thesis graph neighborhood graph data GL construct label data undirected graph construct node ith node corresponds node wij similarity subgraph construct XL XL GL VL EL subgraph construct VL node EL XL XL graph GL construct matrix denote SL respectively define   XL XL otherwise  pij XL XL otherwise define feature vector  diagonal matrix     laplacian matrix   ddl ssl easily develop  fri frj  fri frj        graph structure satisfied accord   label exist maximize variance estimate var fri  optimization already undertaken   otherwise  fri pij  frj pij develop    fri frj pij  fri frj  fri frj           subsequently   seek feature respect GL meaning algorithm rewrite graph formulation algorithm algorithm algorithm difference CLS formula express spectral graph theory additional graph GL output initial algorithm sort feature ensemble CLS FS ensemble highly dependent diversity individual member various research performance definitely commensurate diversity within ensemble regard propose apply simultaneously ensemble technique aspect data RSM performs subspace selection feature whereas RSL applies random subset selection label afterwards bagging applies random sub sample replacement instance finally data replicate subsample obtain bagging project feature subset RSM associate label subspace chosen RSL perturbed version data separately fed selector CLS generate feature selector combination guarantee stable allows ensure diversity component alleviate curse dimensionality treat label correlation specifically diversity definitely ensure application ensemble data manipulation yield data reflect partial member committee learner ultimately generate diverse feature selector hence instance feature label subset promote diversity effective ensemble projection dimensional feature dimensional avoid dimensionality selection subset label subsample efficiently label correlation algorithm outline propose approach algorithm input semi supervise multi label data aim rank feature accord CLS algorithm iteratively construct ensemble feature selector initialize occurrence feature zero occurrence variable compute average feature throughout ensemble construct feature selector iteration randomly input feature feature denote  meanwhile randomly label label  label iteration former selection without replacement bootstrap sample perform supervise data XL sample xil project correspond feature subspace  assign  unsupervised XU data project feature subspace  projection denote xiu iteration xil xiu submit CLS construct individual feature selector compute feature related feature subspace  finally feature selector aggregate accord occurrence subspace feature assign correspond average iteration describes layout description data metric comparison data multi label data commonly report statistic regard aspect instance feature label distribution metric role building algorithm consequently affect related specifically consensus amongst researcher cardinality density likely bias multi label classifier behavior data cardinality density cardinality define average label per instance whereas density normalization cardinality label label define data data label cardinality density label calculate LC LD norm depict various benchmark drawn domain emotion data audio annotation comprise various genre associate emotional reaction scene data semantic scene annotation contains image related instance associate tag accord content  consists instance feature label  drawn text categorization article forum related topic delicious encompass instance feature label imdb contains observation feature label yeast gene functional categorization data biology collection gene description related various gene function compose sequence protein feature protein sequence location label collection sequence protein feature protein sequence location data focus prediction sub cellular location protein accord sequence information data description data addition baseline without feature selection CLS performance propose algorithm ML MI ppt MI memetic   ML MI ppt MI supervise multi label feature selection mutual information latter utilizes prune transformation ppt variant LP transform data multi data strategy mutual information filter irrelevant feature whereas ML MI directly data multi label variant MI feature subset memetic wrapper algorithm feature selection memetic feature selection generation genetic algorithm memetic along performance fitness function subset feature   recent supervise label feature selection adapt multi label binary relevance BR transformation experimental setting data originally supervise however usable semi supervise context random selection rate supervision data unlabeled multi label CLS FS perform semi supervise data totally supervise besides equation parameter carefully tune accord overestimate function behave almost linearly lose non linear underestimated lack regularization decision boundary highly sensitive define percentile choice varies accord data empirically data optimal adopt parameter ensemble ensemble equation ceil formula ensures feature drawn confidence feature member feature label subset cardinality label classification performance feature selection evaluate ML knn multi label version traditional knn algorithm classifier label training data tune via fold validation evaluation metric performance evaluation multi label algorithm challenge comparison traditional assessment binary multi classifier explain multi label classifier predict subset label instead isolated label complicate evaluate generally multi label evaluation metric category whereby performance algorithm evaluate separately average entire category consists label evaluate algorithm label separately average label category categorize classification related rank related task category ensure thorough comparison classification hypothesis instance predicts vector label ham loss indicates instance label misclassified  xor operation predict label vector label vector micro calculates prediction label micro average precision computes average label ranked label     denotes cardinality rank loss evaluates average label correctly rank loss denotes complementary auc roc receiver operating characteristic curve criterion quality rank specifically probability random positive ranked random negative   discussion report interpret impact feature selection performance classifier ML knn FS along feature selection algorithm relevant feature relation data subsequently feature ML knn evaluation report classification performance ML knn increasingly ranked feature accord feature selection performance illustrate roc auc finding increase ratio feature classification performance tends increase remains stable degrades ensemble FS outperforms algorithm data  CLS FS  scene data recall scene correlation label FS clearly imdb data FS capture efficiently correlation label feature selection   equally effective due computational layer feature FS relevant improve rank performance auc versus feature image introduce baseline ML knn entire feature without selection baseline truth gain loss performance apply feature selection summarizes performance powerful evaluation obtain exactly ranked feature evaluation FS performance data specifically classification performance ham loss micro neat improvement CLS propose ensemble technique moreover rank performance quantify average precision rank loss gain employ FS combine strategy CLS SD data usage CLS diverse subsamples obtain apply pas combine technique CLS algorithm feature ensemble gain classification rank accuracy chosen classifier ML knn ham loss versus percentage label data image impact percentage label data performance CLS FS increase percentage label data data percentage feature performance comparison finding whatever percentage label data correspond metric ham loss micro CLS FS remain practically stable performance easily achieve reduce label data FS consistently outperforms CLS micro data FS clearly outperforms CLS ham loss data degradation performance report yeast data FS ranked CLS micro versus percentage label data image statistical validation conduct performance analysis algorithm systematically accord  methodology non parametric friedman outcome identify algorithm performant friedman average rank algorithm null hypothesis algorithm equivalent calculate friedman statistic critical significance accept reject initial null hypothesis friedman statistic  calculate distribute accord distribution chi freedom   algorithm data  rji rank algorithm data significant difference algorithm null hypothesis reject nemenyi hoc perform pairwise comparison algorithm performance algorithm considerably difference correspond average rank critical distance CD calculate CD  critical significance perform micro evaluation metric friedman reject null hypothesis affirm considerable difference feature selection algorithm apply preprocessing ML knn  alternative hypothesis null hypothesis nemenyi illustrate CD diagram micro evaluation metric algorithm average rank within CD CLS FS interconnect CD otherwise algorithm CLS FS significantly performance CLS FS rank performance statistically distinguishable performance feature selection employ rank comparison comparison data micro summarises loss respectively accord FS loss CLS ranked confirms selection precision average rank diagram algorithm micro image pairwise comparison feature selection micro runtime comparison empirically verify efficiency proposal runtime capture FS without FS computational across dimensional data observation analysis varies obviously accord data feature FS exhibit runtime performance multi label data baseline without feature selection   consume FS due independence proposal feature performance comparison classifier without versus feature selection FS performance comparison feature selection FS versus additional effectiveness proposal practical application mail filter comprehensively evaluate model benchmark data employ background email filter description data comparison background mail filter important distinguish message amount  message typically email consists essential email text email header email web audio video image file header precedes email contains rout information message sender recipient date etc specifically header date mandatory CC optional commonly nutshell information email header feature potential precise classification feature selection important mail filter message chooses subset attribute email impact classifier performance computational reduce apply FS email data useful feature data data  email data contains email senior enron official data encompasses email document instance feature email belongs label label organize category coarse genre information primary topic emotional category label inspire email text email header precisely label empty message coarse genre verification email performance propose algorithm aforementioned ML MI ppt MI memetic CLS   sect category enron data experimental setting enron data primarily supervise however FS exploit label sample data perform feature selection simulate context randomly instance data label data unlabeled data parameter sect performance evaluate ML knn experimental average evaluation metric adopt mention evaluation metric comparison ham loss micro average precision rank loss receiver operating characteristic curve metric auc sect discussion report exhaustive experimentation enron data sect apply simultaneously ensemble technique RSM RSL bagging aspect data data replicate subsample obtain bagging project feature subset RSM associate label subspace chosen RSL performance propose FS  baseline model SD enron data auc versus feature image classification performance ML knn increasingly ranked feature accord feature selection performance illustrate powerful evaluation ensemble FS  enron data outperforms algorithm micro versus percentage label data image impact percentage label data performance CLS FS percentage label data varies enron data percentage feature easily FS consistently outperforms CLS micro whatever percentage label data CLS FS practically stable impact feature CC classification performance FS ppt MI ML MI memetic CLS   image impact specific feature extract email message classification performance feature CC meaning combination feature accord useful feature average perform representation email classification useful feature clearly CC understandable feature discriminative feature message user address classification performance computationally intensive conclusion analyze performance ensemble semi supervise multi label feature selection combine amount multi label data unlabeled data develop ensemble framework enhance stability feature selection algorithm improve performance specifically propose apply ensemble simultaneously aspect data bagging sample random subspace input random subspace label label committee feature selector perturbed version data output subsequently combine generate stable feature subset capable guarantee predictive performance besides issue related correlation label label importance address integrate variant CLS feature data various domain relevance propose comparison algorithm classification label rank enlarge scope comparison data apply ensemble furthermore propose ensemble framework efficiently data