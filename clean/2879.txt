graph node classification typically node label goal correctly classify remain unlabeled node label node reality due limited label capability dynamic evolve network node network belong exist therefore cannot correctly classify algorithm propose graph paradigm goal correctly classify node belonging label category classify node belonging label unseen graph challenge graph feature node unseen node label exist arbitrary label graph differentiate node belongs exist unseen tackle challenge propose uncertain node representation principle multiple version node feature representation classifier response node differentiate node belongs unseen technical wise propose constrain variational graph autoencoder label loss uncertainty loss constraint ensure node representation sensitive unseen node embed feature denote distribution instead deterministic feature vector certainty node belonging sample propose generate multiple version feature vector node automatic thresholding reject node belonging unseen node graph convolutional network graph attention network network demonstrate algorithm performance ablation analysis advantage uncertain representation automatic threshold selection graph access auckland library introduction network graph convenient model interaction interdependency data graph node  attempt categorize node graph task fundamental challenge continuous attention research research effort develop reliable efficient algorithm node classification task however exist mainly consistent training node data belong already training unseen node classifier cannot detect unseen erroneously classify node training data reality data collection label continuously evolve trend emerge constantly model cannot detect unseen trend hardly dynamic environment phenomenon refer classification classification owl paradigm recognize belonging already detect sample previously unseen approach svm adjust address treat positive cannot differentiate instance performance unseen negative data alternatively covariate shift social medium text classification covariate shift training data fully representative data address similarity CBS firstly computes convert document vector similarity transform data binary classifier date already attract processing nlp computer vision nlp propose threshold sigmoid function reject data unseen computer vision recognize unseen image training data reduce binary svm classifier positive however knowledge classification previously investigate graph structure data graph task graph consist unseen node objective graph classifier classify node belonging category detect node belonging unseen graph node classification illustrate network node classification node label unlabeled graph label node unlabeled node panel graph aim classifier classify unlabeled node belonging detects unlabeled node belonging unseen node denote node panel image currently exist mainly focus document image cannot directly apply graph structure data graph task cannot model graph structural information core node classification challenge graph graph node content structure information node relation furthermore exist node classification task built assumption data training graph convolutional network GCNs develop convolutional layer exploit graph structure information classification loss function classification task however directly softmax output layer rejection capability unseen node prediction probability normalize across training addition representation exist graph employ feature engineering extract feature vector however model generate deterministic mapping capture latent feature node limitation inability uncertainty incomplete finite available data propose graph data complicate graph data structure node classification task summarize challenge challenge framework graph graph unseen label sample exist arbitrary exist graph neural network GNNs typical built assumption cannot detect unseen challenge model uncertainty node representation promote robustness graph exist GNN approach generate deterministic mapping capture latent feature node visualization classification probability unseen orange instance cora dataset axis denotes index instance instance belong instance belong unseen axis denotes maximum probability output instance softmax classifier denotes classification probability label loss denotes classification probability combine label loss uncertainty loss image overcome challenge propose novel graph lean paradigm OpenWGL node classification task challenge employ loss constraint label loss uncertainty loss ensure node representation sensitive unseen assist model differentiate node belongs exist unseen visualize dataset illustrate effectiveness label loss entropy loss performance exist node unseen node cannot differentiate classify randomly introduce uncertainty loss constraint reduce probability unseen node classify therefore detect unseen node without reduce classification performance node challenge instead deterministic node feature vector utilize graph variational autoencoder module latent distribution node classification phase novel sample generate multiple version feature vector certainty node belonging automatically threshold reject node belonging unseen node contribution summarize formulate graph data novel model OpenWGL propose uncertain node representation approach label loss uncertainty loss constrain variational graph autoencoder node representation sensitive unseen propose sample certainty node belonging automatically threshold reject node belonging unseen node benchmark graph datasets demonstrate approach outperforms baseline related closely related emerge outlier detection graph neural network briefly review aim recognize learner detect exploration employ svm classifier performance negative data fei liu propose similarity CBS computes convert document vector similarity transform data binary classifier extend capability incrementally cumulatively recently processing computer vision recognition nlp propose threshold sigmoid function reject unseen propose model meta allows delete model training computer vision recognize unseen image training data reduce binary svm classifier positive utilize probability threshold detect model weak lack prior knowledge exist approach primarily focus nlp CV domain cannot model graph structural data research propose advance principle graph data graph framework classify network node emerge outlier detection research related emerge detection supervise data mining multi instance outlier detection supervise instance assume belong predefined classifier discriminative sample reality data data emerge unknown unavailable training classifier stage ideal classifier detect emerge detect sample decision threshold confidence multilayer neural network increase threshold sample confidence threshold recognize unfortunately simply increase threshold exist sample misclassified outlier detection aim detect data instance abnormally deviate underlie data comprehensive overview graph technique anomaly fraud detection analysis explain detect abnormality distance outlier detection svm propose normal data domain obtain hyper sphere enclose normal data sample outlier false outlier sample detect outlier distribution training data recent propose  utilizes transduction statistical fitness cluster structure recent proposes detect outlier data detection outlier address summary research advance emerge detection networked data setting proposes automatically threshold graph neural network graph neural network GNNs introduce generalization recursive neural network directly graph cyclic undirected graph powerful machine graph GNNs attract attention around architecture graph structure data propose generalize establish neural network model regular grid structure graph arbitrary structure classic model graph convolutional network gcn convolutional paradigm graph structure data integrate local node feature graph topology structure convolutional layer graphsage variant gcn aggregation feature extraction gat improves gcn leverage attention mechanism aggregate feature node discrimination although GCNs performance graph structure data semi supervise task node classification variational graph autoencoder VGAE extends unsupervised scenario specifically VGAE integrates gcn variational encoder framework graph convolutional encoder inner decoder exist gcn graph model built assumption data training employ loss constrains ensure node representation sensitive unseen assist model differentiate node belong exist unseen knowledge previously investigate graph structure data graph task graph propose novel uncertain node representation approach variant gcn variational graph autoencoder network differentiate node belongs exist unseen overall architecture propose graph OpenWGL model unseen node classification input consists graph label unlabeled node objective OpenWGL define constrain textcircled label loss define textcircled uncertainty loss LC define textcircled KL divergence loss network reconstruction loss LS define OpenWGL uncertain node representation sensitive label unseen specifically OpenWGL uncertain node representation generate latent distribution node consists graph encoder model graph decoder model sample employ latent distribution objective function combine loss constraint structure loss label loss uncertainty loss detail sect image definition overall framework defines address overall framework statement node classification graph focus node classification graph graph vertex node graph relationship node topological structure graph adjacency matrix otherwise indicates content feature associate node RN label matrix node node category already node associate label otherwise graph graph  xtest  denotes training data label node xtest denotes node unlabeled node assume xtest node belonging already  node belonging unseen node graph aim classifier model xtest rejection classify node training reject belong training belongs unseen overall framework classifier graph propose uncertain node representation approach constrain variational graph autoencoder network classify node accurate category reject unseen node framework graph mainly consists component node uncertainty representation gcn model generate deterministic mapping capture latent feature node limitation model inability uncertainty incomplete finite available data representation node employ variational graph autoencoder network obtain latent distribution node enables uncertainty promote robustness classifier classify node detect unseen node introduce constraint label loss uncertainty loss differentiate node belongs exist unseen classification rejection perform inference phase perform classification rejection propose novel sample generate multiple version feature vector certainty node belonging automatically threshold reject node belonging unseen node inference framework detailed discussion sect classification rejection assume node node uncertainty representation generates version feature vector node sample representation fed softmax layer obtain probability output probability average obtain vector average denote max finally node belongs unseen image methodology node uncertainty representation encode latent feature information node obtain effective representation uncertainty employ variational graph autoencoder network VGAE generate latent distribution extract node feature allows leverage uncertainty robust representation graph encoder model graph node content graph structure unified framework approach firstly utilizes layer GNNs feature matrix classical GNNs gcn gat backbone layer GNNs input feature matrix adjacency matrix gcn layer generates dimensional feature matrix define GNN layer GNN model instead generate deterministic representation assume output continuous multivariate gaussian distribution hence inference model propose     diag   matrix vector standard variance matrix distribution   calculate parameterization trick II vector zero II identity matrix latent variable model capture complex noisy data layer GNNs calculation graph convolutional network lth gcn layer input feature matrix output feature matrix nonlinear function  AD  matrix dij  diagonal matrix transformation matrix lth layer nonlinear activation function relu graph attention network lth gat layer input feature matrix apply linear transformation parametrized matrix node attention mechanism leveraged compute attention coefficient node       layer neural network parametrized vector apply  nonlinearity negative input slope attention coefficient normalize across choice softmax function   eij exp    node node obtain normalize attention coefficient compute linear combination feature correspond output feature node   nonlinear activation function exponential linear elu gat applies multi attention mechanism learns embed via multiple concatenates embed representation graph decoder model latent variable decoder model reconstruct graph structure relationship node graph decoder model define generative model   aij    aij denotes logistic sigmoid function optimization discriminative node representation optimize variational graph autoencoder module via loss LS logp KL reconstruction loss input adjacent matrix reconstruct adjacent matrix KL kullback leibler divergence II classifier variational graph autoencoder network obtain uncertainty embeddings node consists uncertainty embeddings label training node  uncertainty embeddings unlabeled node  accurate classifier classify unseen node data propose model consists cooperative module label loss uncertainty loss differentiate node belongs exist unseen overall objective function    LS balance parameter LS loss function variational graph autoencoder network mention LC label loss uncertainty loss respectively detail introduce label loss label loss minimize entropy loss label data   clog equation denotes layer softmax activation function layer linear transformation transform  probability sum label node denotes denotes truth ith node label data classification prediction ith label node respectively uncertainty loss information data exist considerable unseen node differentiate unseen unlike label loss utilize abundant training data performance entropy loss uncertainty loss propose balance classification output node superior unseen node entropy loss uncertainty loss goal maximize entropy loss normalize output node balance formula LC  clog unlabeled node classification prediction ith unlabeled node negative formula maximize entropy loss addition unlabeled data maximize entropy loss sort unlabeled data output probability maximum probability node softmax layer discard node probability easily classify output discriminative node node probability node output balance easily detect unseen finally remain node utilized maximize entropy training label loss uncertainty loss adversarial label loss influence classifier output node discriminative classify node via minimize uncertainty loss output node balance assist detection unseen maximization entropy loss LC LS jointly optimize via objective function define parameter optimize standard backpropagation algorithm visualization threshold validation contains instance threshold validation apply threshold unseen instance image classification rejection perform node uncertainty representation obtain distribution gaussian distribution node embeddings therefore version feature vector   generate node distribution via reparametrization trick representation fed softmax layer probability respectively  obtain output vector  output concatenate obtain sample matrix RM denotes probability specific average probability obtain vector ssi vector ssi probability max ssi recognize node unseen data rejection    obtain softmax layer output none exist probability threshold reject sample unseen otherwise predict probability prediction sample illustrate automatic rejection threshold selection graph threshold reject node propose selection approach automatically threshold reject node belonging specifically validation  training  threshold selection node validation perform node uncertainty representation conduct sample posterior probability average chosen probability node obtain avg unseen instance assume training validation node distribution entropy define unseen node  average posterior probability denote avg unseen logp threshold calculate average probability avg avg unseen validation threshold classify unseen node threshold distinction unseen node embed feature denote distribution instead deterministic feature vector sample generate multiple version feature vector confidence node belonging automatically threshold reject node belonging unseen node algorithm description algorithm illustrate algorithm graph goal obtain node representation classify node detect unseen node respectively firstly employ variational graph autoencoder network model uncertainty node output distribution optimize network KL loss reconstruction loss propose loss constraint LC model capable classify unseen finally jointly label loss uncertainty loss VGAE loss KL divergence loss network reconstruction loss model differentiate node belongs unseen capture uncertainty representation graph complexity analysis graph node vertex propose graph OpenWGL consists graph encoder model graph decoder model gcn gat complexity asymptotically bound network mainly rely message passing node node representation denotes incident node node network message passing OpenWGL graph encoder module complexity graph encoder model complexity reconstruct graph dimension latent matrix node belongs unseen OpenWGL sample uncertain node embed suppose graph decoder model sample complexity graph decoder model DMN complexity OpenWGL asymptotically bound DMN sparse network node complex OpenWGL quadratic node DMN DMN generic network node graph DMN DMN summary complexity OpenWGL mainly attribute graph encoder decoder experimental setup benchmark datasets employ widely citation network datasets cora citeseer dblp pubmed node classification detail experimental datasets report statistic benchmark datasets statistic node benchmark data setting evaluation metric dataset unseen remain report statistic benchmark data respect node unseen cora citeseer dblp dataset unseen remain dataset respectively pubmed dataset unseen unseen remain randomly sample node training validation node unseen validation threshold reject unseen traditional semi supervise node classification dataset graph model unseen verify performance model unseen proportion macro accuracy evaluation baseline employ baseline gcn gcn convolutional network graph structure data gcn employ convolution layer exploit graph structure information classification loss function classification task gcn directly softmax output layer gcn rejection capability unseen gcn sigmod gcn sigmod multiple  softmax output layer gcn model rejection capability unseen gcn sigmod thre gcn sigmod default probability threshold classification predict probability threshold reject unseen otherwise predict probability mlp doc doc classification text classification layer perceptron obtain node representation gcn doc utilize node relationship combine gcn doc model doc multiple  softmax output layer defines automatic threshold mechanism propose validate performance propose OpenWGL algorithm implement OpenWGL graph neural network graph convolutional network gcn graph attention network gat OpenWGL gcn OpenWGL gcn employ layer gcn graph encoder model aggregate node feature OpenWGL gat OpenWGL gat employ layer gat graph encoder model aggregate node feature algorithm implement tensorflow adam optimizer evaluation protocol evaluate approach grid hyperparameter report approach graph model training baseline parameter configuration unless otherwise specify approach fix rate GCNs hidden layer structure balance parameter respectively dropout rate gcn layer addition layer OpenWGL gat gat layer contains hidden layer contains hidden experimental cora unseen experimental citeseer unseen experimental dblp unseen experimental pubmed unseen macro accuracy datasets setting without unseen graph classification macro accuracy node classification task observation gcn gcn sigmoid obtain performance baseline datasets rejection capability unseen therefore unseen node misclassified performance becomes unseen node increase gcn sigmoid thre gcn doc performance gcn gcn sigmoid threshold improve performance detect unseen node addition unseen node increase gcn sigmoid thre gcn doc become competitive gcn doc performance gcn sigmoid thre confirm threshold fix varies datasets ratio unseen doc automatic threshold mechanism effectively improve classification unseen propose graph model OpenWGL gcn OpenWGL gat consistently outperforms baseline datasets unseen demonstrates propose constrain graph variational encoder network differentiate node belongs unseen capture uncertainty representation node jointly label loss uncertainty loss node uncertainty representation unified framework addition propose OpenWGL gat outperforms OpenWGL gcn assign node neighborhood beneficial node representation report without unseen network unseen OpenWGL OpenWGL gcn OpenWGL gat comparable performance gcn confirm effectiveness generalization node classification overall unseen increase performance propose decrease cora dblp pubmed increase citeseer dataset mainly node assign unseen node network label information deterioration performance treat unseen network slightly random prediction accuracy random prediction accuracy binary classification task random prediction accuracy classification task possibly performance increase citeseer dataset meanwhile unseen increase threshold detect unseen node benefit however absence unseen performance rival ablation analysis OpenWGL component OpenWGL contains constraint subsection variant OpenWGL respect aspect demonstrate uncertainty loss impact VGAE module KL loss reconstruction loss adopt gcn module OpenWGL OpenWGL variant comparison OpenWGL variant OpenWGL uncertainty loss remove OpenWGL variant OpenWGL KL loss reconstruction loss remove report ablation uncertainty loss superiority uncertainty loss variant model OpenWGL mention uncertainty loss constraint unlabeled node ablation performance node classification task datasets improve uncertainty loss effectiveness detect unseen node impact VGAE module KL loss reconstruction loss verify impact VGAE module model uncertainty node representation OpenWGL model OpenWGL easily OpenWGL model performs significantly OpenWGL confirms usage KL loss model uncertainty capture latent representation node reconstruction loss preserve node relationship assist node representation macro accuracy OpenWGL variant cora macro accuracy OpenWGL variant citeseer macro accuracy OpenWGL variant dblp impact feature dimension node output embeddings accuracy macro datasets image parameter analysis impact feature dimension node output embeddings ZZ mention output node embeddings OpenWGL layer GCNs structure feature dimension node output embeddings report datasets respectively citeseer dblp datasets increase performance grows gradually plateau performance cora dataset stable increase slight decrease increase accuracy target domain improve task macro accuracy remain steady obvious difference therefore slight difference increase necessarily performance improvement sufficient feature dimension OpenWGL stable increase feature dimension OpenWGL sample randomly node unseen cora respectively denotes node denotes node genuinely belongs false node belong node randomly node randomly unseen axis denotes probability output node softmax classifier axis denotes frequency image OpenWGL sample statistic node unseen cora respectively denotes node denotes node belongs false node belong statistic node statistic node unseen axis denotes probability output node softmax classifier axis denotes frequency node per image visualization OpenWGL sample verify effectiveness sample model randomly node cora dataset unseen unseen respectively perform node uncertainty representation obtain distribution node embeddings generate version feature vector node distribution softmax layer probability respectively therefore node obtain sample matrix sample matrix denotes probability specific visualize sample matrix node histogram unseen node subfigure probability respectively sample superior performance differentiate unseen helpful threshold probability distribute histogram probability distribute probability softmax layer classify node truth however deterministic feature vector instead sample node classify probability similarly unseen node probability concentrate histogram probability easily detect classify unseen however obtain probability output sample unseen node misclassified randomly average node report sample statistic node unseen cora respectively difference latter obtain average node whereas randomly node node unseen node node average probability respect genuine probability average probability respect probability unseen node average probability probability perfectly explain node belong confusion matrix verify effectiveness OpenWGL differentiate node versus unseen node report confusion matrix OpenWGL cora network denotes unseen OpenWGL correctly identifies unseen node remains accuracy classify node confusion matrix OpenWGL cora denotes unseen matrix percentage ith classify jth category image discussion graph significant challenge involves feature prediction loss classification confidence propose combine multiple loss objective function embed feature node classification novel task topic future node belongs unseen thresholding approach reject node posterior probability threshold although threshold automatically solely posterior probability alternatively reject binary decision binary classification task feature instance belongs address graph static network node application network continuously evolve node node content graph dynamic network another significant challenge mainly node distribution impact unseen detection node misclassified unseen undergo evolve concept drift representation node dynamic network capability differentiate node unseen another topic future currently aim attribute unknown unseen cannot distinguish unknown future distinguish unknown processing unsupervised cluster addition GNN variant gat gcn graph encoder model empirically admittedly alternative instead gcn gat however goal propose novel graph representation model focus graph paradigm goal classify node belonging classify node belonging exist unseen OpenWGL gat outperforms OpenWGL gcn variant GNN beneficial node representation improve performance model apply GNN model gin    graph task future conclusion traditional graph task unlabeled node label node training goal classify node already advocate graph paradigm classifies node belonging classifies node belonging exist unseen achieve goal propose graph OpenWGL framework component node uncertainty representation classifier former label loss uncertainty loss graph variational autoencoder node embed distribution latter automatically learns threshold detect unseen node former learns distribution node embed via graph variational autoencoder capture uncertainty latter minimizes label loss uncertainty loss simultaneously distinguish unseen node automatically threshold threshold reject unseen automatically framework unseen data OpenWGL significantly outperforms baseline classify unseen node network unseen node node OpenWGL comparable performance baseline