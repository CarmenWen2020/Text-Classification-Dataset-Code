naive bayes NB award data mining algorithm unreliable probability estimation unrealistic attribute conditional independence assumption limit performance alleviate primary weakness simultaneously instance attribute recently propose however exist approach learns instance attribute separately without interaction restricts performance model therefore propose novel approach instance attribute collaboratively model collaboratively naive bayes CWNB CWNB training instance iteratively estimate posterior probability loss prior conditional probability accurate incorporate probability conditional likelihood CLL formula optimal attribute maximize CLL extensive experimental CWNB significantly outperforms standard NB exist competitor access auckland library introduction decade supervise classification important task data mining classifier bayesian network classifier  attention data era impact  obvious advantage explicit interpretability robustness numerous  naive bayes NB simplest classifier assumes attribute fully independent instance attribute vector NB predict label θaj  θaj  CP attribute jth attribute collection label prior probability θaj conditional probability estimate θaj aij training instance label ith training instance indicator function zero otherwise jth attribute aij jth attribute ith training instance discrete attribute although structure indeed NB already exhibit surprising classification performance award algorithm data mining however primary weakness remain NB unreliable probability estimation unrealistic attribute conditional independence assumption alleviate former weakness propose improvement consist instance selection instance tune alleviate latter weakness related enhancement structure extension attribute selection attribute recently alleviate primary weakness simultaneously instance attribute propose model instance incorporate formula prior conditional probability accurate probability estimation attribute incorporate exponential conditional probability naive bayesian classification formula alleviate attribute conditional independence assumption however model learns instance attribute separately without interaction argue interaction ignore model classify instance function conditional probability collaboratively dependent instance attribute data characteristic calculate instance attribute separately interaction totally neglect sub optimal greatly restricts performance model therefore propose instance attribute collaboratively model collaboratively naive bayes CWNB CWNB training instance iteratively estimate posterior probability loss prior conditional probability accurate incorporate probability conditional likelihood CLL formula conduct gradient descent optimal attribute maximize CLL besides propose variant CWNB analyze consistency CWNB experimental collection benchmark datasets california irvine uci repository validate effectiveness propose CWNB sum contribution analyze disadvantage exist instance attribute approach argue interaction instance attribute ignore function conditional probability collaboratively dependent propose novel model collaboratively naive bayes CWNB learns training instance estimate posterior probability loss instance collaboratively attribute maximize CLL meanwhile propose variant CWNB conduct comprehensive collection uci benchmark datasets validate effectiveness CWNB besides performance CWNB variant analyze consistency CWNB organize brief survey related describes propose CWNB model variant experimental setup concludes outline direction future related numerous enhancement NB instance commonly approach alleviate unreliable probability estimation attribute representative approach relax unrealistic attribute conditional independence assumption concretely instance assigns specific instance importance likewise attribute assigns continuous attribute accord predictive ability category filter wrapper filter employ data characteristic calculate correspond regard preprocessing building model contrarily wrapper optimize correspond iteratively accord performance feedback built model generally filter faster wrapper training phase wrapper usually classification accuracy instance filter easy training instance estimate distance instance training instance instance assign vice versa numerous algorithm locally NB  algorithm  algorithm firstly utilized instance distance instance finally instance NB model built locally training instance unfortunately  typical lazy model spends computational classification phase incurs complexity obtain efficient model recently propose effective instance model attribute frequency instance naive bayes   training instance define inner attribute frequency vector attribute vector extensive experimental validate  obtain classification performance NB maintain simplicity model instance wrapper  propose boost naive bayes  applies boost NB  series instance NB model successively model attention instance misclassified predecessor specifically building instance NB model training instance classify model misclassified instance increase another instance NB model built training instance output  sum model model accord classification accuracy training instance  another instance wrapper  naive bayes DWNB iteration DWNB training instance  increase associate estimate posterior probability loss instance NB model built training instance iteration attribute filter calculate attribute important zhang  propose gain ratio attribute NB  model  attribute assume relevant gain ratio attribute gain ratio assign vice versa apart model hall propose decision attribute NB  model  ensemble  decision built training instance random sample attribute assign inversely proportional minimum depth decision however attribute relevance recently propose correlation attribute NB model considers attribute relevance attribute attribute redundancy extensive experimental demonstrate model efficient meanwhile really effective improve NB classification attribute wrapper optimal vector significant introduce typical model model propose attribute alleviate NB independence assumption   gradient descent optimize vector objective function  chooses maximize conditional likelihood CLL minimize error mse version denote WANBIACLL  respectively recently expand model specific attribute  assign attribute specific furthermore simultaneously paid attention attribute label expand vector matrix assign specific attribute gradient descent model propose correlation adjust NB  regard filter wrapper hybrid attribute model  attribute initialize correlation attribute filter optimize adjust wrapper recently alleviate primary weakness simultaneously combine attribute instance uniform framework model attribute instance NB   attribute calculate correlation attribute instance estimate eager attribute frequency instance lazy similarity instance version denote AIWNBE  respectively extensive proven model perform instance attribute collaboratively naive bayes motivation instance attribute obtain performance instance attribute however exist approach filter learns instance attribute separately without interaction specific exist approach calculates training instance accord data characteristic instance incorporate formula prior conditional probability respectively  θaj  aij  exist approach data characteristic calculate attribute incorporates exponential conditional probability classification formula     CP however argue interaction instance attribute ignore utilize estimate posterior probability instance function conditional probability  collaboratively dependent instance attribute explain clearly instance incorporate conditional probability formula attribute incorporate exponential conditional probability data characteristic calculate separately conditional probability θaj attribute optimal interaction totally neglect overall  sub optimal greatly restricts performance model therefore propose collaboratively instance attribute model collaboratively naive bayes CWNB CWNB obtain performance model attention interaction instance attribute learns collaboratively overall  accurate propose approach overall framework CWNB described CWNB initialize training instance instance NB model specific utilized estimate prior probability conditional probability θaj built instance NB model estimate posterior probability training instance belonging intuitively predict posterior probability posterior probability therefore information update training instance iteration specifically iteration estimate posterior probability loss ith training instance assume posterior probability label exactly detailed update equation ith training instance iteration estimate posterior probability ith training instance tth iteration overall framework CWNB image instance accurate iteration extensive majority datasets converge therefore iteration calculate prior probability conditional probability θaj instance discus obtain θaj collaboratively attribute previously mention propose attribute NB algorithm adjust vector BFGS optimization procedure objective function maximize CLL minimize mse motivate research propose CWNB BFGS optimization procedure attribute vector optimization approach instance consideration calculate objective function approach prior probability conditional probability θaj obtain instead difference subtle significant instance attribute collaboratively overall  estimate accurately objective function maximize CLL conclusion objective function minimize mse remain CWNB detail CLL calculate prior probability conditional probability θaj attribute initialize attribute obtain θaj CLL formula exploit gradient descent iteratively update attribute maximize CLL detail CLL objective function define CLL logp   attribute vector   θaj obtain calculate gradient CLL respect calculate gradient  respect      clog θaj  θaj gradient CLL respect    θaj  θaj θaj θaj summary entire algorithm CWNB partition training CWNB training classification CWNB classification algorithm briefly depict algorithm respectively variant apart CWNB described variant collaboratively NB reverse specifically attribute maximize CLL attribute NB model utilize model estimate posterior probability loss training instance instance CWNB approach posterior probability training instance estimate instead convenience denote approach CWNB interactive CWNB approach learns instance learns attribute CWNB approach evaluate training classification accuracy classification accuracy compute training data model training classification accuracy longer improves besides avoid instance update model cannot converge update instance iteration iteration CWNB denote approach CWNB reverse interactive approach learns attribute learns instance training classification accuracy longer improves CWNB update instance iteration denote approach CWNB RI sect detailed comparison CWNB versus CWNB CWNB CWNB RI experimental benchmark data evaluate effectiveness CWNB conduct  environment knowledge analysis weka platform competitor AIWNBE DWNB WANBIACLL NB svm implement CWNB AIWNBE DWNB weka platform addition exist implementation NB svm liblinear regularize loss vector classification primal weka platform besides source code WANBIACLL kindly author algorithm abbreviation CWNB collaboratively NB AIWNBE eager attribute instance NB DWNB  NB WANBIACLL CLL attribute NB NB standard NB svm vector machine description uci datasets classification accuracy comparison CWNB versus AIWNBE DWNB WANBIACLL NB svm classification accuracy comparison wilcoxon conduct collection benchmark classification datasets california irvine uci repository recent peer domain data characteristic datasets ascend instance attribute replace mode nominal attribute numerical attribute available data besides discretized numerical attribute nominal attribute width bin experimental analysis classification accuracy estimate obtain average stratify tenfold validation detailed classification accuracy algorithm dataset marked signifies classification accuracy CWNB statistically significantly algorithm correspond significance conduct besides average classification accuracy algorithm datasets summarize classification accuracy conduct friedman obtain average ranking algorithm summarize furthermore thoroughly algorithm keel data mining software conduct wilcoxon rank summarizes detailed comparison indicates algorithm improves algorithm correspond indicates algorithm improves algorithm correspond diagonal significance upper diagonal significance comparison conclusion CWNB generally competitor AIWNBE DWNB WANBIACLL NB svm fully verifies universal applicability CWNB domain data characteristic summarize highlight scatter plot classification accuracy comparison datasets instance CWNB versus AIWNBE DWNB WANBIACLL NB svm image improvement average classification accuracy average classification accuracy CWNB datasets remarkably AIWNBE DWNB WANBIACLL NB svm superiority average ranking average rank CWNB AIWNBE DWNB WANBIACLL NB svm superiority attribute instance NB algorithm AIWNBE CWNB improves classification accuracy datasets statistically besides CWNB statistically loses datasets therefore performance CWNB exist AIWNBE validate interaction instance attribute ignore effectiveness attribute ablation DWNB CWNB improves classification accuracy datasets statistically important CWNB surprisingly statistically loses zero dataset therefore perspective ablation attribute CWNB effective improve classification performance effectiveness instance ablation WANBIACLL CWNB improves classification accuracy datasets statistically meanwhile CWNB statistically loses zero dataset summary instance CWNB improve classification performance superiority standard NB svm standard NB CWNB statistically improves classification accuracy datasets statistically loses zero dataset svm CWNB statistically improves classification accuracy datasets statistically loses datasets demonstrate CWNB perform svm standard NB significance wilcoxon rank wilcoxon rank easily whatever significance propose CWNB significantly competitor AIWNBE DWNB WANBIACLL NB svm moreover relationship performance CWNB instance datasets category datasets instance datasets instance detailed classification accuracy ascend instance dataset credit exactly instance preliminary conclusion CWNB appropriate dataset classification conclusion intuitively clearly scatter plot classification accuracy comparison datasets axis axis classification accuracy CWNB competitor respectively orange classification accuracy CWNB significantly competitor respectively meanwhile classification accuracy related competitor significantly CWNB respectively easily classification accuracy CWNB competitor datasets account datasets respectively furthermore CWNB statistically competitor datasets account datasets respectively demonstrate CWNB achieve comparable svm moreover AIWNBE DWNB WANBIACLL NB propose CWNB performs dataset classification CWNB attractive era data classification accuracy comparison CWNB versus CWNB CWNB CWNB RI average CWNB CWNB RI dataset discussion furthermore thorough analysis collaborative performance CWNB variant mention sect detailed algorithm dataset conclusion CWNB almost comparative CWNB CWNB CWNB RI classification accuracy meanwhile CWNB CWNB RI interactive average therefore CWNB faster CWNB CWNB RI summarize highlight comparison classification accuracy CWNB CWNB RI CWNB datasets loses datasets CWNB CWNB datasets loses datasets summary CWNB almost comparative CWNB CWNB CWNB RI classification accuracy comparison training complexity average CWNB CWNB RI respectively CWNB approximately faster CWNB CWNB RI besides median CWNB CWNB RI respectively datasets moreover maximum respectively unfortunately iteration cannot improve classification accuracy greatly dataset  therefore generally CWNB promising variant consistency finally propose CWNB statistically consistent generate artificial toy dataset performance propose CWNB standard NB verify NB assumption attribute conditional independence satisfied propose CWNB converge optimal NB classifier instance infinity generate binary classification dataset artificially contains independent attribute label dataset attribute generate randomly finite moreover instance sum attribute label label purpose split dataset balance description attribute dataset therefore fully performance CWNB NB instance setting generate multiple dataset instance classification accuracy comparison CWNB versus NB instance setting image experimental platform experimental setting described sect detailed classification accuracy comparison instance NB obtain classification accuracy CWNB instance increase CWNB gradually outperforms NB finally instance infinity converge classification accuracy summarize highlight relationship classification accuracy instance instance increase classification accuracy CWNB NB improve instance usually useful information model comparison classification accuracy CWNB NB instance NB achieve performance CWNB maybe standard NB robust performance instance CWNB instance attribute instance CWNB potential risk overfitting instance increase CWNB obtain performance NB specifically instance classification accuracy CWNB NB instance CWNB usually classification accuracy NB statistical consistency propose CWNB instance conclusion CWNB statistically consistent CWNB converge optimal NB classifier instance infinity described attribute dataset instance assume situation obtain data building accurate model instance infinity CWNB NB really converge classification accuracy therefore CWNB statistically consistent conclusion future argue interaction instance attribute ignore propose novel model collaboratively naive bayes CWNB CWNB training instance iteratively estimate posterior probability loss incorporate prior conditional probability CLL formula conduct gradient descent optimal attribute maximize CLL extensive experimental collection uci datasets CWNB significantly outperforms NB exist competitor datasets besides propose variant related CWNB extensive experimental classification accuracy comparative CWNB usually model finally analyze consistency CWNB attribute conditional independence assumption satisfied CWNB converge optimal NB classifier gradually instance infinity although approach collaboratively employ approach instance attribute employ sophisticated optimize approach differential evolution approach instance attribute simultaneously improve performance CWNB addition apply collaboratively improve classification model another topic future