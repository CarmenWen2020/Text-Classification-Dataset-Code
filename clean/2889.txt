entity ET identify semantic entity within corpus ET involves label entity mention label multi multi label task considerably challenge entity recognition exist entity model pre identify mention cannot directly text pipeline approach therefore mention extraction model entity model raw text another limit factor mention ET model fix context entity sensitive selection drawback propose entity model gru remove dependency demonstrate effectiveness model baseline mention model incorporate contextualised transformer embeddings bert extensive ablative demonstrate competitiveness simplicity model entity access auckland library introduction entity ET identify semantic entity within corpus contrast entity recognition token label zero label ET involves label entity mention label typically hierarchy grain label encapsulate semantic information singular label entity label mutually exclusive obtain ET therefore highly valuable downstream processing task information extraction knowledge graph construction text mining label multi label entity BBN dataset ontonotes dataset sect image despite widespread entity recognition research effective entity ongoing entity whereby token label zero considerably challenge entity recognition multi multi label task difficulty majority entity assume entity already hence operating mention entity mention dataset already identify label semantic limitation arise perform entity mention oppose however firstly technique mention entity fix token centre around entity mention bidirectional lstm attempt prevent model training irrelevant information model highly sensitive context incorporate contextual information outside actually important model employ bidirectional gate recurrent gru capable harness context effectively via forget gate secondly perform entity scratch mention segmentation model combine complicate pipeline training propose model capable outperform mention model architecture simplify task exist model hinder input representation input representation mention model generate context independent embed model glove effectiveness context dependent embed model bert entity investigate despite proven nlp task entity recognition addition explicitly context representation model investigate effectiveness contextualised embeddings mention entity therefore extensive ablative demonstrate effectiveness contextualised embeddings mention entity competitiveness entity despite challenge task accomplish introduce baseline attention mention model embeds mention context bert entity model determines token review prior entity sect  detail sect sect overview sect evaluate baseline mention model outperforms mention entity model effective datasets capable outperform mention entity model despite token entity apriori related taxonomy entity extraction literature entity  category lstm model hierarchy aware model model emphasise bold image review focus attention entity taxonomy entity extraction distinction entity recognition mention entity entity model review categorise within taxonomy entity initial entity research treat entity multi multi label classification identify entity within document grain entity recognition  model pipeline approach perform entity identifies entity mention via segmentation predicts label associate mention segmentation perform conditional random crf variety handcraft feature token contextual gram label prediction perform multi layer perceptron differs entity treat entity task  reliance handcraft feature perform segmentation unfeasible domain application feature readily available mention entity recent research focus mention entity contrast stag pipeline  performs entity segmentation entity recognition label prediction mention model already data aim predict entity mention context entity model typically employ directional memory model lstm mention context fed lstm layer obtain encode representation decode fed linear layer obtain label correspond mention architecture introduce hybrid neural model  comprises component recurrent mention model obtains vector representation entity mention context context model generates vector context mention output mention model context model concatenate fed softmax layer obtain probability distribution probability prediction multi instance entity corpus  employ lstm input embed via glove lstm layer output concatenate constraint basis integer linear program model apply output dense layer disjointness constraint ensures entity label mutually exclusive mutual exclusivity via external knowledge hierarchy constraint ensures entity label label category contrast mention entity automatic grain entity AFET employ lstm instead introduces novel heuristic noisy mention introduce hierarchy partial label embeddings improve performance AFET advantage noisy label dataset mention noisy truth label category hierarchy mutually exclusive loss function model differs training noisy AFET notably relies handcraft feature POS tag cluster unlike limit functionality datasets label recent investigate effectiveness incorporate category hierarchy classification stage neural architecture grain entity classification  lstm model introduces hierarchical label encode improves classification performance network automatically built hierarchy matrix facility commonly commonly artist actor improve rare subclass extend incorporation entity hierarchy combine cnn entity model along hierarchical loss function encoder model positional embeddings embeddings input output concatenate mention embeddings vector fed multi layer perception obtain prediction vector hierarchical loss function employ knowledge graph embed complex bilinear embed hierarchy loss function aim minimise binary entropy across predict label hierarchy embeddings another research entity aim  availability external knowledge graph accurately predict entity  enhance representation informative entity identifies entity mention within input  determines entity via transformer information fusion model training objective underlie model transformer model bert aim predict randomly masked input sequence input mask random entity masked input sequence model underlie representation summary summary  pipeline entity heavily reliant feature collection   AFET  propose mention entity    lstm representation  upon model transformer architecture AFET  propose integrate entity hierarchy loss function  access knowledge graph identify entity mention AFET relies handcraft feature notable omission exist entity inability perform entity mention external knowledge handcraft feature entity model introduces entity model mention model determines entity entity mention surround context model determines token overview embed layer model explain model detail embed layer crucial role model context dependent representation input token facilitate representation bert bert upon encoder stack bidirectional transformer model encoder decoder model structure combine layer multi attention mechanism input sequence bert arbitrary span contiguous text pad CLS sep token denote linguistic respectively input representation item sequence sum positional embeddings segmentation embeddings token embeddings contrast context dependent embed model elmo bert learns bidirectional representation perform procedure masked model input model masked random training objective model predict vocabulary index masked bert embeddings model incorporate valuable contextual information embed layer oppose via recurrent layer exist entity embeddings generate bert fed model context dependent meaning embeddings generate respect surround context  embed accord canonical meaning richer representation context independent embed model currently entity model important distinction bert embed model wordpiece byte encode oppose unknown token  prior fed bert model  becomes  architecture mention model horizontal denote average indicates concatenation image mention model attention mention model predicts label entity mention surround context accomplishes context model consist mention context inspire mention sect however model employ recurrent neural network instead layer encode representation combine mention context another encode representation label context vector context  mention respectively fix context mention context  mention vector pad pad token excess  trim mention context encode via pre bert model obtain embed matrix embed dimension average across matrix yield vector vector hereby denote mention context vector attention mechanism mention model augment attention mechanism scalar dynamic scalar attention mechanism learns extent context mention important predict label mention context accord relevance task scalar context vector normalise attention softmax operation sum context layer correspond attention concatenate eai eai attention apply mention regardless mention complexity downside assume mention benefit attention attention context mention context mention context prediction mention adversely affected attention mechanism relevant component mention model dynamic attention mechanism image issue propose dynamic attention mechanism illustrate per context layer dynamic variant simpler network assign context layer upon mention context input layer average embeddings across wordpiece mention context output vector correspond mention context  apply layer dynamic attention mechanism allows prediction  mention heavily influence surround context reduce irrelevant contextual information non  mention mention context normalise attention layer output mention context respectively another entity easily infer mention  obama normalise hidden output layer attention context vector concatenate combine representation vector fed linear layer relu activation function output layer fed layer obtain output vector contains correspond label label loss function perform sigmoid function normalise loss model calculate binary entropy  loss nln label index prediction associate index label prediction layer prediction across label prediction layer output minor difference mention model model mention entity mention guaranteed label address issue adjust prediction layer mention model output label prediction architecture model remain   diagram brevity image model entity model predicts token contrast mention model entity dataset compose purely raw text entity mention context input entire input output zero label token novel exist entity approach entity model completely allows context entire account predict label token prevents entity segmentation prior perform entity model architecture mention model bidirectional gate recurrent gru instead network allows backward context independent account predict label token gate recurrent recurrent architecture memory model lstm however typically faster gate update gate function similarly lstm memory gate information previous hidden information reset gate allows hidden discard information irrelevant update gate reset gate calculate matrix     activation calculate wit loss function loss calculate binary entropy manner mention model per however average across label prediction loss average across prediction wordpiece token batch loss loss concatenation layer oppose embed model bert operates wordpiece issue mention model predicts label per mention regardless  complication model however aim classify token label output model label per token per wordpiece accommodate incorporate concatenation layer model data load stage construct token index correspond wordpiece index described bert documentation footnote sequence token john   CLS john  sep hereby denote construct concatenation layer prediction wordpiece layer average prediction corresponds token accord token prediction prediction correspond wordpiece john average wordpiece  average fourth fifth wordpiece sixth wordpiece respectively allows model output token prediction despite wordpiece entity segmentation output entity label token input sequence entity mention obtain via contiguous sequence token label prediction  obama  correspond label president president entity mention  obama  ontonotes dataset contiguous entity mention mention outline border image capable contiguous entity mention another president immediately  obama identify entity entire span label entity recognition model predict inside tag resolve issue task multi label entity label worth benefit contiguous entity label decrease accuracy increase training substantially increase complexity model furthermore contiguous label entity mention typically uncommon datasets evaluate sect document instance contiguous entity mention datasets datasets evaluate mention model modify datasets evaluate model evaluate mention model benchmark datasets wiki ontonotes  portion training datasets validation wiki ontonotes BBN datasets summarise initial data exploration training ontonotes BBN datasets data proportion incorrect label however error manually annotate therefore version datasets hereby modify datasets prefixed datasets summarise modify training wiki dataset relatively training ontonotes BBN datasets however due complexity model trim wiki dataset memory therefore construct wiki dataset document document training training validation dataset ontonotes BBN datasets training comprise data validation comprise comprise remain model parameter parameter tune performance development model achieve rate hidden dimension dropout prior layer model optimise adam batch mention model model mention model context mention context model max sequence data without dramatically increase training embed technique evaluate effectiveness bert embeddings model evaluate model embed technique uniform baseline assigns uniform distribution embed token glove embeddings pre wikipedia  footnote wordvec embeddings pre wikipedia corpus footnote embed dimension technique bert embeddings generate pre  model footnote embeddings dimension bert  embed per batch tune bert datasets performance improvement evaluation metric evaluate model standard metric entity strict accuracy loose macro loose micro formula metric denote entity predict label entity truth label entity prediction evaluate model baseline sect token predict label token truth label prediction token respectively strict accuracy precision recall loose macro precision recall loose micro precision recall metric calculate precision recallprecision recall strict accuracy considers prediction token predict truth exactly loose macro calculates subset entity individually entity mention whereas loose micro computes corpus average across entity loose macro tends  unseen category therefore sensitive unbalanced datasets loose micro fairer metric baseline mention model evaluate AFET neural mention model handcraft feature  hybrid neural model mention  ML adapt multi label  neural model combination lstms integer linear program comparison attention mention ML model mention mention model performance baseline investigation mention model performance exist investigate effectiveness propose attention mechanism model mention model exist AFET relies handcraft feature highlight grey mention model outperforms  perform rely handcraft feature micro metric wiki dataset attribute combination context dependent bert embed vector context model contrast exist model encapsulate contextual information via context dependent embeddings bert transformer architecture despite rely handcraft feature AFET mention model mostly outperforms AFET wiki BBN datasets however performs substantially ontonotes dataset likely due quality handcraft feature ontonotes boost AFET performance model modify wiki ontonotes BBN datasets various embed technique comparison mention ML model attention mechanism modify wiki ontonotes BBN datasets prior bold mislead metric grey attn attention dyn dynamic attention mechanism scalar dynamic generally impact performance additionally despite complexity dynamic attention mechanism outperform scalar variant training dynamic attention mechanism performs validation comprise training data significant difference validation likely phenomenon training distinct another rapid overfitting complex model attention model training BBN ontonotes automatically label whereas manually label clearly mention entity model outperforms technique rely handcraft feature highly competitive AFET perform handcraft feature model consistently outperforms AFET entity BBN dataset criterion macro micro entity performance baseline performance evaluate model embed layer initialise embed technique modify wiki ontonotes BBN datasets strict accuracy macro micro calculate respect model prediction token corpus bert embeddings significantly outperform embed technique dataset context dependent embeddings bert model highly effective entity model accuracy macro metric mention model accurately reflect performance model accuracy extremely vast majority token entity model successfully predicts label token macro similarly mislead consistently due token useful metric evaluate micro disregard non entity sum truth label token disregard mislead accuracy macro performs ontonotes BBN datasets fare wiki dataset considerably noisier overall model capable perform entity future research regard model investigate devise suitable evaluation metric comparison mention model comparison mention evaluate upon modify datasets bert embeddings evaluate calculation mention model calculate across entity oppose across token non entity incorrectly label entity ignore competitive mention model relatively ontonotes BBN datasets outperform mention model BBN surprising vastly training objective token entity mention likely explanation context entire sequence pivotal role model particularly BBN allows effectively classify token mention model context another noticeable mention model relationship complexity attention model overall performance contrast datasets attention mechanism impact performance dynamic attention model excels modify datasets significantly cleaner datasets label respective dataset dynamic attention mechanism clearly effective dataset consistency training overall comparison highly competitive mention model ability incorporate document context performs datasets disparity training evaluation foundation future research entity conclusion extensive ablative datasets demonstrate effectiveness contextualised embeddings mention entity competitiveness propose entity attention mention model embeds mention context bert employ novel attention mechanism predict label associate entity mention model effectively determines token mention model outperforms mention entity model model performs datasets capable outperform mention model despite token entity future tenfold validation ensure statistical significance investigate effectiveness recent context dependent embeddings XLNet transformer replace gru context representation incorporate hierarchical encode technique improve prediction accuracy finally useful investigate contiguous entity mention without decrease accuracy increase training