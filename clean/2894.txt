fisher linear discriminant analysis lda widely linear classification feature selection metric multivariate data analytics ensure classification accuracy optimally discover predictive feature data CDA namely combinatorial discriminant analysis intend combinatorially subset feature assign optimally CDA extent truncate rayleigh algorithm stat soc ser stat  improves lda estimation sparsity constraint experimental synthesize datasets demonstrate algorithm outperforms lda baseline downstream classifier empirical analysis algorithm recover combinatorial structure optimal lda empirical consistency access auckland library introduction fisher linear discriminant analysis lda technique dimension reduction metric widely application recognition image retrieval etc typically lda projection direction project data variance maximize relative within variance achieve optimal discrimination intrinsic limitation lda decision function relies estimate projection vector optimal linear classification dimensional random vector gaussian distribution covariance matrix distinct vector observation drawn gaussian distribution prior probability fisher linear discriminant analysis classifies optimal estimation global optimal estimation projection vector classification optimal data drawn gaussian distribution covariance matrix optimal classification exists gaussian distribution assumption estimation training data somehow straightforward sample estimator calculate sample covariance matrix sample vector separately estimate  via matrix inverse however application micro array data analysis biomedical data analysis dimension denote significantly sample denote sample estimation covariance matrix usually singular non invertible hence estimation linear projection vector ill dimension sample setting HDLSS contribution inspire recent progress combinatorial regression novel combinatorial fisher discriminant analysis incorporate sparsity assumption exist specific sparsity assumption estimate sparse lda norm sparse inverse covariance estimator norm sparse lda estimator however assumes lda projection vector sparse proposes algorithm CDA norm sparse lda estimation improve covariance matrix estimation mild covariance structure specific derive CDA algorithm truncate rayleigh trifle optimization framework originally generalize eigenvalue sparse constraint enhance trifle precondition trifle initialization reduce optimization error replace sample covariance matrix estimator shrunken marginalize statistical error direction gradient ascent bias estimator faster statistical convergence enhance optimization rayleigh evaluate CDA synthesize dataset datasets experimental synthesize dataset CDA outperforms combinatorial linear model truncate rayleigh trifle orthogonal pursuit omp combinatorial structure recovery retrieve nonzero precision recall background related recently effort devote HDLSS shrunken estimator generally assume data training classification randomly drawn unknown gaussian distribution prior training sample lda estimate linear projection vector structure however dimension denote significantly sample denote sample estimation covariance matrix usually singular non invertible impossible inverse sample covariance matrix estimate related intend categorize folder covariance regularize estimation estimate propose estimate covariance matrix inverse covariance matrix shrunken covariance precision matrix estimator empirical shrinkage estimator graphical lasso obtain estimation inverse covariance matrix frequently denote estimate vector compute indirect manner estimator community estimation inverse covariance matrix computational expensive rely additional assumption sparsity covariance matrix sparse lda estimation instead estimate separately research propose estimate directly data sometimes sample estimator specifically algorithm estimate maximize fisher information thresholding technique pursue sparse representation leveraged dantzig selector estimate sparse approximation norm sparsity recently lasso estimator compute sparse approximation norm sparsity error preliminary formulation review lda formulate propose research sample lda binary classification label data dimensional vector input lda estimate sample covariance matrix pool sample covariance matrix estimator respect vector estimate parameter inverse sample covariance lda model classifies data vector  relies accurate estimation inverse covariance matrix function refer  frequency positive negative sample population respectively combinatorial discriminant analysis label data training assume tuple drawn joint distribution denote specifically dimension vector drawn unknown gaussian distribution prior label drawn correspond label drawn integer research intend sparse lda estimator nonzero coefficient recover combinatorial structure optimal lda estimate specific denote supp function vector nonzero coefficient vector vector formulate combinatorial discriminant analysis argmax supp supp  supp refers bound error rate sparse lda classification define sect intend subset feature overall feature assign feature optimal linear classification generalizability ensure combinatorics subproblem overall NP binary lda multiclass lda adopt transformation propose algorithm introduce  covariance matrix estimation rayleigh CDA output optimal estimation projection vector denote βˆt refers iteration rayleigh CDA replaces βˆt classification covariance matrix estimation training data tune parameter estimate covariance matrix input rayleigh covariance matrix estimation label data training algorithm sort training data label data positive negative label respectively estimate vector separately overall vector algorithm sample estimator covariance matrix  shrunken within covariance matrix estimation training data estimation algorithm estimate sample within covariance matrix obvious HDLSS setting sample covariance matrix usually singular obtain robust estimation within covariance matrix CDA proposes inversion graphical lasso estimator sˆw     θjk  graphical lasso estimator refers positive semidefinite constraint refers absolute matrix tune parameter sparsity  sparse sˆw bias Sˆ due induced sparsity biasing covariance estimation mention covariance structure mild bias sparse within covariance estimator sample estimator bias sˆw   bias cˆw   cˆw    CDA lower bias sˆw via sˆw cˆw sˆw cˆw    equivalent inverse matrix bias graphical lasso estimator faster statistical convergence rate statistical convergence covariance within covariance matrix estimator sˆw  improve estimation sample estimator unbiased estimator statistical error due finite sample estimation however singular dimensional sample setting remark denote maximal node graph estimator sˆw inverse matrix graphical lasso estimator  sˆw  improve non singular estimator statistical convergence rate  logp mild eigenvalue sparsity assumption bias estimator sˆw cˆw improves sˆw inverse matrix sˆw cˆw    converge sharper norm statistical convergence rate logp  improve estimation covariance matrix CDA outperform exist rayleigh desire nonzero coefficient CDA adopts rayleigh algorithm estimate sparse discriminant projection vector βˆt rayleigh gradient ascent derive maximizes fisher information optimal discrimination norm constraint maximal tolerate perturbation tol maximal iteration research specifically propose rayleigh algorithm consists precondition initialize shrunken within covariance matrix sˆw cˆw vector algorithm leverage scout estimator algorithm compute  bias linear model desire nonzero thresholding normalization algorithm project  desire initialize gradient ascent  function algorithm introduce algorithm theoretical analysis precondition theorem gradient ascent projection bias within covariance matrix sˆw cˆw covariance matrix  propose algorithm CDA indeed intend maximize fisher information  sˆw cˆw refers generalize rayleigh within covariance matrix fisher information maximization achieve goal gradient ascent algorithm propose minimize sˆw cˆw  constraint instead minimize sˆw cˆw  algorithm indeed minimizes gap rayleigh algorithm iteratively update βˆt gradient estimate algorithm specifically gradient estimate algorithm  sˆw cˆw βˆt estimate algorithm adjust parameter adjusts gap approximate minimization projection algorithm diffuses βˆt desire iteration sparsity constraint contribution research propose leverage precondition initialization gradient estimation improve performance optimization estimation advance sample estimator significant performance enhancement CDA classifier obtain output algorithm βˆt dimensional vector classification propose algorithm classify linear classification βˆt βˆt refer sample vector algorithm analysis introduce analysis introduce assumption algorithm analysis theoretical lemma obtain reuse finally sketch proof lemma assumption theorem suppose sample randomly drawn unknown gaussian distribution prior optimal projection vector denote normalize optimal projection vector focus euclid distance estimate βˆt βˆt converges dimension training sample structural assumption assumption denote optimal lda estimator sparse assumption assume training sample realize random vector assumption exists positive constant bound eigenvalue λmin λmax assumption appropriate assume supp supp   define algorithm  thresholding  preserve algorithm supp  supp  assumption simplify research suppose data sample normalize assumption theorem theorem norm convergence rate βˆt CDA iteration probability βˆt  normalizes optimal lda βˆt refers desire nonzero refers nonzero moreover refer rate error reduction per iteration analytical framework truncate rayleigh convergence rate truncate rayleigh sensitive initial algorithm estimation error within covariance matrix earlier summarize βˆt sˆw cˆw  denotes scalar depends   matrix  logp accord obtain theorem introduce lemma lemma denote refers normalize vector algorithm  probability lemma suppose refers nonzero integer probability constant already define earlier easily combine lemma proof lemma report synthesize datasets evaluation synthesize dataset evaluate CDA multivariate gaussian data HDLSS setting specifically intend performance CDA recover combinatorial structure sparse projection vector understand performance CDA classification task data synthesis synthetic data generate predefined gaussian distribution prior setting symmetric positive definite matrix dimensional vector setting gaussian distribution optimal projection vector normalize vector totally nonzero variable feature optimal discriminant analysis baseline algorithm setting understand performance propose algorithm CDA baseline trifle  trifle directly sample estimation covariance matrix pseudo inverse sample within covariance matrix algorithm initialization inverse sample covariance matrix exist parameter nonzero  leverage rayleigh optimization compute projection vector sample covariance matrix estimation without sparsity constraint omp sda omp algorithm derive orthogonal pursuit approximates sparse projection vector discrimination norm sparsity constraint loss desire nonzero addition norm sparsity sda derive scout graphical lasso estimator norm sparsity algorithm perform function algorithm tune parameter shrunken covariance matrix estimation algorithm estimate projection vector classify vector simulate HDLSS setting CDA baseline algorithm sample randomly drawn distribution prior algorithm sample setting average grid algorithm evaluate algorithm various parameter setting performance norm estimation error image estimation error consistency verify consistency propose estimator norm error estimate projection vector βˆt CDA algorithm omp sda trifle  overall norm error algorithm training dataset algorithm tune parameter grid fold validation comparison vector estimate omp sda trifle  normalize obvious CDA norm error baseline algorithm sample illustrates norm estimation error CDA desire nonzero CDA norm error descend trend βˆt demonstrates potential empirical consistency performance combinatorial structure recovery image combinatorial structure recovery demonstrates performance CDA baseline algorithm combinatorial structure recovery overall combinatorial structure recovery algorithm training dataset algorithm tune parameter grid optimal estimate precision recall nonzero retrieval consideration calculate precision supp supp supp recall supp supp supp supp supp supp supp demonstrates CDA outperforms baseline algorithm omp trifle sda  sample illustrate precision recall nonzero retrieval apply CDA desire nonzero obviously CDA precision recall respectively comparison potential CDA feature optimally HDLSS setting performance comparison classification image classification accuracy classification performance CDA baseline algorithm accuracy algorithm parameter setting apparently CDA outperforms baseline algorithm demonstrate classification CDA performance CDA significantly susceptible choice summary CDA sda performance advantage CDA contribute rayleigh gradient ascent refine equivalent sda CDA trifle performance advantage CDA due truncate rayleigh gradient ascent shrunken covariance matrix estimator bias reduction classification benchmark datasets evaluate CDA datasets evaluate algorithm binary classification benchmark adult web datasets import adult dataset dimension web dimension respectively evaluate algorithm HDLSS setting algorithm sample randomly drawn datasets prior classification accuracy CDA baseline algorithm downstream classifier vector machine svm discriminant analysis lda propose knn decision DTREE random classifier rfc kernel svm gaussian kernel  nonlinear classification algorithm tune parameter setting grid NN NN NN obvious CDA outperforms algorithm datasets HDLSS setting performance lda stable increase training sample pseudo inverse sample covariance matrix singular performance adult web datasets image HDLSS classification biomedical data ultra dimensional biomedical datasets leukemia cancer datasets colon cancer datasets evaluate algorithm CDA decision random svm respect HDLSS setting classifier sample randomly drawn datasets prior classifier validation demonstrates average accuracy standard deviation algorithm datasets validation algorithm tune parameter sample training performance however comparable previous CDA delivers performance decent accuracy datasets omp delivers slightly performance colon datasets CDA outperforms omp leukemia datasets comparison CDA sda CDA selects subset feature classification deliver comparable performance baseline accuracy comparison CDA baseline combinatorial inference tobacco related behavior frequent tobacco preventable illness propose CDA understand tobacco related behavior united data foursquare data united residence united frequent smoker national average positive frequent smoker national average negative label accord frequent smoker mention behavior characterize average frequency vector dimension vector refers average frequency venue gym church per per totally venue methodology intend analyze correlation tobacco CDA combinatorial inference propose algorithm achieve goal estimate projection vector increase nonzero vector ensure stability CDA estimation sub sample average truncation strategy estimate robust projection vector denote  sort vector  trend coefficient specific suppose jth  refers correspond behavior gym behavior intuitively behavior important unselected behavior discriminate frequent smoking infrequent smoking population define   correspond behavior jth coefficient picked    correspond behavior reject CDA behavior picked CDA reject increase illustrates CDA significant behavior absolute coefficient coefficient CDA behavior private firstly picked CDA reject behavior without rejection increase behavior private picked rejection behavior gym coffee church picked CDA  reject increase indicates normalize per per church private frequent smoker gym gym frequent smoker CDA significant behavior venue private church coffee gym behavior venue parameter increase image coefficient behavior picked reject increase behavior picked however  reject increase behavior relevant frequent smoker obvious behavior feature private church positively related frequent smoker coffee gym negatively related frequent smoker intuitively easy understand causation gym smoking link private coffee church smoking future psychologist understand relevance conclusion novel combinatorial discriminant analysis propose CDA algorithm approximate sparse estimation projection vector desire denote nonzero propose CDA operates rayleigh gradient ascent algorithm leverage covariance regularize initialization bias shrunken covariance estimator improve trifle CDA outperforms  exist sparse lda estimator error projection vector estimation HDLSS setting enjoy accuracy combinatorial structure recovery CDA performs sparse lda algorithm downstream classifier accuracy HDLSS data classification binary lda multiclass lda easily adopt transformation