recommender predict preference user attract attention decade popular collaborative filter employ explicit implicit feedback model user item connection collaborative filter matrix completion technique recover user item interaction matrix rank assumption critical premise matrix completion recommender  information interaction matrix redundant assumption developed matrix factorization model rank optimization model framework neural network brief description recommender matrix completion classical algorithm related matrix completion collaborative filter introduce assumption rank moreover performance algorithm evaluate conduct substantial datasets finally research issue future exploration matrix completion recommender access auckland library introduction primary target recommender recommendation user taste behavior thereby significantly user useful information rating typical user explicit feedback visually reflect user related item multitude rating user internet vast matrix employ recommender predictive recommendation item user tend rate item sparsity matrix brings difficulty profile user item research issue decade dealt apply associative retrieval framework related activation algorithm explore transitive association user feedback framework item related emotion semantic data concern handle sparsity recommender propose domain framework transfer user item rating dense auxiliary rating matrix domain sparse rating matrix target domain datasets domain sparsity implicit feedback another useful information recommender although feedback explicit feedback internet click shopping cart shopping site favorite  article implicit feedback feedback necessarily reflect preference user however promotes model user item reduces sparsity matrix correlation item user user item interaction matrix generate explicit implicit feedback collaborative filter CF discovers correlation data driven manner accurate recommendation sparsity issue matrix completion extensively apply CF attempt recover interaction matrix mainly CF matrix completion neighborhood model  latent factor model   compute similarity user recommend item specific user accord rating user recommend item favorite item user compute similarity item  focus profile feature user item project dimensional vector latent factor reap feature matrix matrix factorization singular decomposition svd latent factor obtain via neural network propose jointly linear model neural network improve recommender apply neural network dnn youtube website recommendation neural network collaborative filter NCF express generalize matrix factorization singular rating matrix generate widely datasets recommender movielens movielens singular account sum singular respectively image rank matrix essential assumption matrix completion  considers information interaction matrix redundant compress singular information matrix recommender motivates enforce rank interaction matrix exist effectiveness rank sparsity issue approach investigate rank matrix completion rank matrix factorization nuclear norm heuristic singular thresholding SVT robust principal component analysis robust pca etc review recommender matrix completion technique rank assumption although developed rank assumption necessarily minimize rank matrix explicitly therefore review aspect matrix factorization model neural network model rank minimization model rank minimization optimize rank function explicitly contribution matrix completion approach recommender matrix factorization model neural network model rank minimization model primary technique matrix completion recommender advantage drawback model analyze substantial rating recommendation prediction typical application recommender conduct algorithm accord experimental observation analyze summarize exist model recommender matrix completion discus challenge potential research direction insight reader introduce algorithm matrix completion recommender sect matrix factorization model neural network model rank minimization model sect conduct substantial model performance various algorithm future direction recommender matrix completion insight researcher summarize sect matrix completion formulation model recommendation rating prediction model recommendation model recommender feedback driven user feedback transform multiple incomplete matrix optimization matrix completion address recommender effectively consequently approach preprocess feedback data algorithm matrix completion apply recommender discus rating task recommender predict rating matrix user item rating user item denote yui massive rating transform matrix corresponds rating item rat user user item  interaction matrix define mui yui null replace unknown rating mui yui recommendation task yui affinity user matrix completion user item interaction matrix recommendation accord predict matrix usually entry interaction matrix unknown user rat item traditional popular matrix completion recommender explain commonly mathematical readability specific mathematical regularization coefficient explain explanation commonly mathematical matrix factorization model NMF matrix factorization popular classical technique CF rating aim decompose user item rating matrix user latent factor item latent factor profile user item accurately subsection nonnegative matrix factorization NMF algorithm effective partial representation data rating matrix considers nonnegative matrix PQ illustrate usually parameter min promise rank model learns compress representation matrix NMF incomplete user item interaction matrix approximate multiplication nonnegative matrix profile user latent feature profile item latent feature image quality approximation loss function define compute distance arbitrary nonnegative matrix widely loss function aij bij computes euclidean distance gradient descent algorithm apply update learnable matrix multiplicative update minimize loss function   ptm      matrix nonnegative explain user latent factor item latent factor respectively constraint enhances interpretability rank matrix factorization predictive rating directly compute nonnegative matrix svd introduction svd review matrix completion  appropriate similarity item orient  described sij   variable nij user rat item variable ρij pearson correlation coefficient user rate item similarly equation regard shrunk correlation coefficient hyperparameter typical theoretically user rat item sij ρij namely similarity compute exist rating convincing contrary similarity compute rating shrunk considerably similarity prediction unknown rating  sij yuj  sij  baseline estimation unknown rating yui explains user item denote denotes consist item item rat user  improve neighborhood model exploit implicit feedback item rat user denote item user implicit feedback denote improve model compute  yuj  wij cij   item item item rat user model assumes user rating generate significant deviation baseline estimation lfm propose  svd considers user implicit feedback model propose literature  qti user model sum denotes perspective implicit feedback learnable variable model minimize error function gradient descent  finally integrate svd model neighborhood model directly qti yuj  wij cij equation tier model matrix completion CF tier baseline estimation yui without interaction account tier qti predicts interaction user profile item profile tier neighborhood tier explains influence implicit feedback tier framework significantly improves accuracy matrix completion model information latent embeddings user item slim FISM inspire item item knn another important model  ning  propose sparse linear model slim learns item item similarity matrix optimization  MS diag matrix constrain sparse generally binary matrix implicit feedback yui user feedback click item otherwise slim learns latent feature exist data discovers relationship item rat transitive relation overcome sparsity inherent slim improve algorithm dubbed factor item similarity model FISM matrix factorization model concern   computes similarity item variable correspond vector latent matrix item rat user denote constant item rat user consequently agreement item rat user accord similarity introduce variation FISM optimization target model compute error RMSE model aim address optimization  yui update via stochastic gradient descent sgd prediction item estimate rating yui compute exclude item namely  FISM advantage svd model project feedback latent rank latent matrix item similarity promotes model transitive relationship implicitly recommendation task however improvement complicates computation  matrix factorization model assumption matrix rank rating user item matrix related information within matrix redundant assumption hidden information decompose matrix rank matrix however instead entire matrix rank model assumption rank matrix vicinity combination algorithm smooth convex combination local rank matrix approximates matrix local developed local extension incomplete svd model vicinity local incomplete svd model argminx rank denotes operator rank optimal smooth kernel parameterized bandwidth parameter apply epanechnikov kernel item similarity user similarity apply distance obtain efficient estimation local model  watson regression apply obtain precise global approximation calculate  sum namely average index important away matrix approximation conduct anchor sample training evenly increase local model continuity accuracy improves accordingly accuracy local model directly accuracy global estimation precise prediction error algorithm local rank matrix approximation  model addition iteration algorithm independent compute parallelly efficiency local matrix dimension computation  regularize svd due consume issue conduct svd matrix matrix completion subproblems multi schatten norm surrogate MSS handle rank optimization subproblems  accelerate computation discus MSS later sect cofactor embed model widely investigate processing obtain wordvec developed google model algorithm project dimensional vector apply input model inspire embed model propose cofactor improve quality matrix factorization model model apart user item matrix item occurrence matrix across user built item embed assumption item prefer user embed model transform document framework matrix factorization model item embed model matrix factorization model attempt factorize implicit feedback matrix user latent vector item latent vector rank assumption objective function model define   cui hyperparameter usually parameter apply balance rating exist rating click data optimization maximize posteriori estimate probabilistic gaussian matrix factorization model item embed model sequence item sequence embed model analogously apply item embed text document context surround within fix wise mutual information pmi matrix context define pmi frequency context denotes sum context levy goldberg equivalence skip gram wordvec negative sample implicit decompose pmi matrix shift logk recommend implement embed spectral dimensionality reduction sparse shift positive pmi  matrix define  max pmi logk becomes hyperparameter sparsity  matrix item embed model matrix occurrence  matrix item embed obtain decompose rat item specific user context item click click refers item user rat consume hij obtain empirical estimate pmi define particularly user rat item combination matrix factorization model item embed model hij  yui  hij hij  objective function matrix factorization model item embed model regularization avoids overfitting define matrix factorization model encodes item vector latent feature item item embed explain item occurrence matrix factorization item embed item latent factor explains user item interaction item item occurrence besides introduce additional model parameter pmi matrix factorization cui parameter balance unobserved information click matrix balance matrix factorization item embed model neural network model AutoRec autoencoder neural network aim representation data project data dimensional rebuild achieves desire performance processing computer vision matrix recommender generally rank dimensional vector hidden layer compress information latent factor therefore propose CF model autoencoder framework dubbed AutoRec effectively compress representation user item rating matrix model user vector  item vector  AutoRec item user autoencoder model input compress input onto dimension vector finally model reconstructs output layer predict unknown rating user item rating matrix model illustrate structure item AutoRec model image rating vector autoencoder aim optimization  rebuilds input hidden layer   function arbitrary activation function parameter WD update gradient descent propagation optimization item AutoRec define  WD exist rating training due partial observation rating optimization target corresponds neural network hidden layer hidden WD regularization avoids overfitting regularization strength training prediction yui compute shortcoming AutoRec project data linear layer projection become identity function inadequate feature CDAE AutoRec efficient application matrix completion via autoencoders basis model recommender dubbed collaborative denoising autoencoder CDAE denoising autoencoder DAE framework DAE widely learns latent representation corrupt feature training model rebuild structure CDAE model specific user image CDAE model neural network hidden layer input layer item input node specific user input node user node dimensional vector training item node  denote dimensional implicit feedback vector user item yui item prefer user training DAE generates corrupt attempt reconstruct yui purpose hidden layer discover robust feature prevent merely identity function apply multiplicative mask reconstruct dimension yui via probability formulate  corruption unbiased uncorrupted recomputed via hidden layer node fully node input layer additional node bias matrix item input node hidden layer user specific node input layer user user vector hidden layer model project input latent vector  activation function sigmoid function output layer rebuilds input vector via  WD matrix bias vector hidden layer output layer activation function finally parameter minimize average reconstruction error user  WD  MEP WD regularization avoids overfitting WD trainable parameter sgd accelerate model framework computes parameter rating collection item training rat user unrated item user model sample subset negative item randomly parameter update meanwhile model employ adagrad automatically adapt training procedure gradient tth sgd respectively model prediction item output layer recommend specific user DMF CDAE model mention illustrates preference user address implicit feedback neural network model dubbed matrix factorization DMF recommendation explicit rating implicit feedback DMF matrix factorization model implement neural network parallel multilayer network transform representation user item respectively user item project onto dimensional vector           kth matrix extract hidden information user item similarity user item  cosine  predict negative mapping  max apply transform prediction nonnegative architecture DMF model image loss function employ explicit implicit information model feedback optimization loss function dubbed normalize entropy NCE loss define yui  logy  max maximum rating classical rating recommender max yij influence loss function graph owe powerful ability integrate node non euclidean domain graph achieve significant performance recent spectral graph convolution typical graph approach widely apply recommender  propose address recommender exploit bipartite user item relationship graph convolution operation estimate recommendation spectral domain regard variant graph convolutional network gcn gcn developed perform convolution operation graph structure data formulate denotes adjacency matrix connection matrix lth layer denote approximation truncate chebyshev polynomial deduce spectral convolution graph  denotes eigenvalue matrix normalize graph laplacian matrix input feature diag parameterized filter gcn aim embed network typology extensively apply recommender recover interaction matrix via explore latent relationship user item numerous gcn developed recently employ graph autoencoders derive gcn retrieval incomplete matrix matrix completion task convert link prediction graph combine gcn recurrent neural network rnn explore underlie graph structure user neural graph collaborative filter  framework integrates user item interaction gcn framework explicitly leverage collaborative signal rank minimization model IRNN rank minimization model rank optimization rank matrix factorization described rank optimization focus minimize rank function  rank differentiable loss function rank rank function matrix matrix completion recommender generally define  reconstruct matrix rank exactly sum nonzero singular input matrix  sometimes becomes NP tackle relax rank surrogate function norm nuclear norm schatten norm accordingly transform  regularization coefficient generally surrogate function loss function satisfy assumption assumption continuous nonconvex monotonically increase possibly nonsmooth assumption differentiable smooth function gradient lipschitz continuous lipschitz constant possibly nonconvex assumption guarantee convergence specify nonconvex definition function specify nonconvex surrogate apply theory  propose iteratively reweighted nuclear norm IRNN optimize rank minimization surrogate IRNN update kth iteration minimization   ith singular wki  compute via wki antimonotone  significant singular iterative update proximal gradient equation derive    lipschitz constant equation singular thresholding  VT UΣVT svd  diag   diagonal singular matrix  denotes ith singular iteration IRNN scheme update wki respectively DNNR inspire IRNN  improve IRNN singular function  formulate  nonconvex  function IRNN indicates significant singular identity function degrade IRNN aforementioned notation rank minimization   optimization target derive  concept  reweighted strategy dubbed nonconvex nonsmooth rank DNNR minimization linearize equation optimal achieve     thresholding operator ith operator solves    exist derive norm simplicity denote  respectively otherwise arccos similarly optimal compute otherwise cosh  analogous IRNN update DNNR iteration model computes wki  wki update optimization distinct aforementioned IRNN DNNR IRNN nonconvex constraint function singular due update DNNR singular function instead directly however critical IRNN DNNR consume issue conduct svd interaction matrix brings difficulty apply datasets MSS avoid conduct svd entire matrix reduce consumption propose unified convex surrogate schatten norm minimization optimization decompose subproblems svd conduct matrix dimension inspire rank matrix factorization rank minimization define rewrite argminu unknown latent factor matrix  recent related attempt surrogate specific denote schatten norm min xtx factor  invariant norm schatten norm becomes widely nuclear norm trace norm investigate frobenius norm surrogate nuclear norm argminu  moreover equality argminu  argminu  summarize exist speculate bilinear surrogate schatten norm extend dubbed multi schatten norm surrogate MSS optimization psp  ipi  satisfies rdi XI rdi optimization MSS coordinate descent bcd minimizes iteration fix remain subproblem proximal alternate linearize minimization palm algorithm specifically proximal gradient factor iteration compute    specific furthermore acceleration technique adopt update via wki  wki compute wki min  generally scheme MSS subproblem IRNN DNNR however IRNN DNNR MSS solves consume issue transform subproblems easy avoids conduct svd matrix computation datasets addition specific DNNR unified model flexibly combination subproblems  distinct convex nonconvex rank relaxation introduce modify schatten norm surrogate rank function denote minx psp objective nonconvex cannot optimize directly however linearize strategy variable alternate direction multiplier guarantee subproblem gain directly optimize surrogate function quadratic minx iσi optimize efficiently furthermore devise iterative singular thresholding algorithm  iteration  summarize     singular thresholding operator denote   diag   theory reduce iteration improve computational consumption global convergence guarantee introduce conduct substantial model mention sect datasets specially recommendation focus rating prediction suitable task rating NMF svd  AutoRec   IRNN DNNR MSS  rank item knn slim FISM CDAE cofactor DMF   task comparison evaluation measurement utilized performance algorithm predict statistic datasets datasets description conduct datasets movie recommendation datasets joke rating datasets shopping recommendation datasets etc textual description datasets  dataset movie recommender dataset crawl filmtrust website contains rating user item   research movielens website version datasets movielens ML movielens ML conduct  popular online movie TVs website dataset contains rating netflix prize competition extract data user item  epinions website rating dataset contains rating user item  benchmark dataset joke recommender contains substantial rating user joke datasets rating detailed statistic datasets dimension rating rating density performance evaluation rating error RMSE absolute error mae evaluate performance recommendation model rating RMSE mae deviation data data metric performance model entry predictive compute RMSE mae RMSE mae adopt metric rank normalize discount cumulative gain NDCG recall evaluate recommend item NDCG emphasizes rank estimate variation rank predict rank concern item preference specific user rank NDCG explain definition NDCG introduce discount cumulative gain DCG DCG  reli relationship importance item item reli rating indicates preference user ideal DCG  ideal DCG compute DCG  compute NDCG compute NDCG  recall NDCG item recommend equivalent without predict rank define recall TP TP FN TP positive item FN false negative item performance comparison algorithm setting code feasible parameter setting via substantial clarify advance gain credible rate algorithm dimension latent factor matrix factorization model specific parameter algorithm svd fix regularization coefficient  fix anchor local global apply epanechnikov kernel AutoRec fix item knn slim adopt knn FISM fix CDAE sigmoid activation function DMF relu activation function fix hidden layer cofactor ratio IRNN DNNR  initialize PΩ MSS fix factor recommendation task generate recommendation factor ML jester image discover latent factor comparison algorithm user item embed AutoRec CDAE matrix factorization model parameter constant factor variable conduct ML jester datasets AutoRec CDAE define dimension hidden layer latent factor performance algorithm increase factor increase decline factor performance FISM stable recommendation obtain computational performance AutoRec CDAE fluctuates greatly datasets CDAE demand factor rebuild corrupt data obtain NDCG contrary algorithm cofactor performance factor model performs poorly factor rank minimization performance across generally RMSE IRNN contrary MSS gain performance although DNNR developed IRNN DNNR uncertain ML jester datasets DNNR allows plot across image furthermore comparison datasets algorithm fivefold validation apply average standard deviation metric performance rating rank task observation experimental svd excellent performance datasets graph model svd implicit feedback explicit feedback considers matrix factorization neighborhood graph model   achieve pleasurable performance rating prediction task generate graph structure feature depict relationship user item graph propagate information effectively secondly rank minimization perform model attribute concentrate recover exist matrix instead predict knn item knn slim achieve superior performance indicates knn effective obvious slim CDAE gain extremely NDCG jester dataset probably meaningful user item embed item user besides FISM DMF gain acceptable performance CDAE performs poorly sparse datasets experimental neural network traditional machine model sometimes achieve performance motivates develop traditional optimization algorithm perform poorly sparse datasets essential challenge recommender sparsity rating jester RMSE mae extraordinarily datasets performance mae std comparison rating task data performance RMSE std comparison rating task data performance NDCG std comparison rank task data performance recall std comparison rank task data runtime comparison various algorithm datasets evident neural network model AutoRec CDAE DMF graph estimation model item knn iterative update procedure model apart item knn  MSS swiftly datasets svd  MSS localize subproblems dimension runtime IRNN DNNR extremely machine due inefficient svd entire matrix iteration computational gradient related dimension feedback matrix converge slowly datasets average runtime matrix completion algorithm datasets insight discussion advantage matrix completion survey concept rank matrix completion although application diverse technique recommender formulate matrix completion attempt recover incomplete feedback matrix amount user feedback data naturally various incomplete matrix beneficial researcher recommender define optimization researcher easily previous consequently application transform recommender matrix completion optimization besides rank optimization sect developed via concept matrix completion tends interpretability traditional iterative optimization limited lack theoretical explanation despite achieve promising improvement recommender encourage researcher explore related algorithm concept traditional matrix completion algorithm vital inspire develop neural network structure interpretability discus subsection brief comparison algorithm besides application although recommender predict user item interaction matrix technique matrix completion essential role scenario matrix factorization decompose user item interaction matrix latent factor user item extract underlie feature widely utilized context aware sequential recommender therefore depth matrix completion development related technique recommender challenge potential future direction traditional machine conclude brief comparison model analyze detail traditional convex optimization model matrix factorization model rank minimization model important role recommender achieve performance competitive superior model rating prediction svd exploit latent factor information neighborhood coefficient accuracy datasets knn model slim item knn gain excellent performance rank task knn matrix completion generally gain pleasurable accuracy recommendation task owe knn virtually recommend item experimental traditional machine effective recommender observation reveals conduct matrix completion via explore neighborhood relationship profitable rating rank task discover interpretable effective model machine technique kernel bayesian cluster however complexity rank optimization rank minimization conduct svd min matrix iteration runtime similarity complexity popular neural network max suitable recommender datasets reduce computational complexity recommendation algorithm  MSS attempt reduce runtime avoid conduct computation entire matrix decompose optimization subproblems conquer algorithm accelerate model due rapid development implement neural network apply extract feature generate user item profile limited rating matrix network cnn rnn widely feature engineering image audio text input extract feature content CF apply information however effective profound understanding neural network substantial trial tough researcher develop model theoretical guarantee recent recommender built via neural network perform traditional iterative machine algorithm phenomenon consequently integrate traditional machine technique becomes direction model associate traditional matrix factorization model DMF conduct matrix factorization neural network computes preference user cosine similarity hence valuable research direction transform traditional model framework framework inspire traditional iterative optimization generally theoretical guarantee however matrix completion optimization nonconvex  optimization objective difficulty norm promotes sparse nuclear norm generates rank rank minimization typical nonsmooth  constraint gradient descent backpropagation algorithm applicable although investigate transform traditional optimization algorithm framework knowledge limited handle rank constraint neural network therefore construct framework spirit traditional iterative optimization potential direction issue performance model decline due sparsity user item feedback matrix analyze phenomenon regard issue become primary recommender generally impossible user feedback item database application user rat item challenge practical application severe benchmark datasets inspire excellent performance svd explore model adopt implicit feedback information matrix completion recommender useful generate user item profile address issue information obtain social relationship geographic location user shopping sequence embed information important building interpretable model unknown incomplete matrix predict accurately conclusion matrix completion algorithm matrix factorization model neural network model rank minimization model investigate characteristic improvement finally introduce evaluation measurement recommender evaluate performance algorithm datasets hyperparameters investigation inspire exist research insight potential direction matrix completion recommender reader combination traditional optimization machine popular neural network improve accuracy matrix completion nowadays recommender critical role data mining discover useful message suggestion explore efficient algorithm recommender regard matrix completion future