availability significant amount data data driven decision becomes alternative complex multiagent decision instead domain knowledge explicitly decision model data driven approach learns decision probably optimal available data remove knowledge bottleneck traditional knowledge driven decision domain expert data driven decision context interactive dynamic influence diagram DIDs framework multiagent sequential decision uncertainty propose data driven framework DIDs model focus behavior agent domain challenge policy embed DIDs model due limited data propose develop policy agent DIDs cluster employ sophisticated statistical analyze propose algorithm theoretical domain introduction development compute data information technology research paradigm decision knowledge driven data driven approach footnote traditional decision analysis firstly decision model consult domain expert application solves model optimal decision applicable knowledge driven decision technology mainstream intelligent research due complexity multiagent agent external environment precise multiagent decision model expert domain model complex cannot developed completely significantly compromise decision quality decision model establish predict planning agent behavior directly behavioral model data avoid difficulty building model coin data driven decision multiagent data driven prescriptive model directly decision agent knowledge driven intend descriptive model decision decision partially observable stochastic  interactive dynamic influence diagram transparent model multiagent sequential decision comparison model interactive partially observable markov decision POMDP decentralize POMDP solves viewpoint individual agent convert multiagent sequential decision individual decision model agent decision agent DIDs effectively probabilistic graphical representation exploit potential structure improve efficiency DIDs model multiagent sequential decision interactive component model decision agent agent decision model account agent behavior model sequential decision agent model input model research generally assumes agent model agent decision model predict behavior agent model easy task parameter decision model agent POMDP  agent model agent assume candidate decision model ascribed agent exist intractable DIDs exploit available agent interaction data automatically behavior agent directly embed learnt behavior model agent research inspire observation firstly agent behavior agent action agent affect decision agent hence behavior agent directly predict avoid decision agent secondly behavior agent various cannot theory however actual behavior tendency hence behavioral dimension candidate model agent amount historical data agent interaction behavior agent automatically infer developed machine technique aim behavioral model agent integrate agent model resort commonly model policy agent behavior model compose agent observation action ideally model behavior agent however model agent interaction data insufficient incomplete behavioral model agent article develop action behavioral model data central stem concept behavioral equivalence behavioral model ascribed agent limited behavior cluster behavioral model propose typical cluster generate model cluster representative model cluster behavioral model initialize cluster compose cluster calculate similarity behavioral model incomplete similarity context behavioral model cannot differentiate behavioral model action unstable subsequently proceed develop behavioral model rigorous conduct behavioral compatibility observation action correspond behavioral model incomplete behavioral model compatible counterpart importantly guarantee quality DIDs theoretical behavioral model embed DIDs empirically typical domain organize background knowledge model sect sect propose data driven framework DIDs instantiate framework propose behavioral model sect conduct sect review previous research multiagent decision model sect sect conclude discus future research background knowledge interactive dynamic influence diagram DIDs graphical decision model individual agent presence agent action affect agent DIDs integrate component decision model decision model agent decision predicts behavior agent agent model agent hexagon model node accommodates candidate model agent model model node candidate model agent theory infinite agent model agent hypothesize model agent oval node model observation reflect observation function node rectangle decision node diamond reward function generic slice agent image dash policy link model update link dot arrow update model node involves candidate model identify update model reside model node agent receives observation model update reflect belief update model structure previously optimal action model action agent observation agent observation update mtj model mtj model action observation respectively model compute distribution update model distribution probability agent perform action observation update model decision model policy behavioral model prescribes agent model correspond policy formally define later denote policy horizon  hence model denote opt  policy node leaf node action observation sequence atj model consist node node conditional probability decision node rectangle action utility node diamond reward function agent decision policy describes agent behavior model image DIDs viewpoint individual agent predict agent behavior mutually model technique optimize agent decision DIDs assumption behavior agent agent agent communication prior agreement behavior agent independent observation environment therefore DIDs advantage multiagent sequential decision collaborative multiagent competitive multiagent DIDs candidate model agent agent model agent agent update model agent observation consequently update model grows exponentially DIDs algorithm mainly behavioral utility equivalence principle compress model agent multi slice model hence DIDs innovative technique data driven framework DIDs data driven multiagent decision research development AI technology seminar research direction depth conference expands reinforcement RL data research environment optimizes independent agent decision behavior RL agent conduct decision adapt action accordance reward ultimately maximum reward punishment accord environment agent decision ability develops multi stage optimization data driven multiagent RL technique enable agent interact environment discover optimal decision trial error reward mechanism generally reward develop policy agent optimal policy summarize action trace RL assume historical data agent interaction exist behavior becomes machine inspire observation convert previous framework data driven framework DIDs intend propose RL focus data driven framework DIDs knowledge driven previous framework heavily depends domain knowledge description model decision agent DIDs candidate model exist due lack model ascribed agent subsequently model behavior generally policy expand agent agent model finally agent interact agent policy model interaction agent update accord online observation insert evidence observation model update belief decision agent interaction knowledge DIDs demand input domain knowledge image manually built model agent model model integrate policy expansion agent however domain knowledge accessible construct precise model agent although model agent decision important representation agent decision affected predict behavior agent agent model hence equally effective model behavior ascribed agent DIDs inspiration behavior agent automatically alternative manually craft model DIDs data driven amount data exist domain data agent interact setting action observation data actually agent behavior without building model employ model behavior agent policy agent behavior expand agent DIDs model data driven DIDs knowledge framework learns agent behavior instead construct model predict agent behavior domain knowledge data driven DIDs behavior data image challenge issue agent behavior agent interaction data data sufficient infer behavior agent agent action specific observation incomplete policy action observation behavioral model data driven framework DIDs focus develop technique behavioral model agent technique avoid manually decision model agent DIDs behavior agent DIDs behavior agent policy horizon model previous knowledge approach formally define behavioral model agent propose automatically model available data agent interaction behavioral model horizon depth policy contains policy node leaf policy action observation sequence prescribes agent behavior entire planning horizon formally define policy definition policy policy  action observation sequence planning horizon  atj  null observation action agent action observation policy  depth policy optimal agent decision assign observation impose policy assume observation guard arc policy depth policy formally define policy definition policy depth policy policy   structure vertex node label action label observation policy model policy specifies predict action agent directly impact agent decision previous research assumes availability domain knowledge construct precise model agent solves model policy however somewhat unrealistic application complicate behavior hence aim policy entire data encode action observation sequence algorithm development policy data data sequence action observation agent interact facilitate development policy extract policy data policy compose multiple policy construct policy function action observation action observation policy subsequently multiple policy policy due limited agent interaction data exhibit profile agent behavior consequently learnt policy incomplete convert policy policy policy initialize cannot exist sequence action observation picked cannot exist although incomplete sub sequence GL GL hence initialize generate incomplete lack policy policy incomplete image interaction data generate multiple policy although data consistent agent decision interaction incomplete interaction data agent behavior difficulty expand model node DIDs cluster policy learnt policy incomplete exist due limited data occurs decision planning horizon policy behavior agent behavior agent belief environment agent exhibit behavior sufficiently belief behavior merge cluster agent behavior representative policy agent behavior simultaneously avoid incomplete policy policy incomplete policy behavior comparison action policy propose cluster policy algorithm algorithm conduct cluster procedure policy cluster centroid none policy nearly learnt policy rarely occurs actual datasets subsequently assign policy closest cluster compute distance cluster centroid recompute cluster centroid sum minimum intra distance policy within cluster cluster procedure terminate cluster centroid cluster centroid representative policy complexity algorithm polynomial data spirit cluster algorithm adopts heuristic incomplete policy relation action difficulty analyze quality cluster policy integrate DIDs unknown error predict agent behavior meanwhile distance metric counting action difference policy inequality euclidean distance metric approximation performance visible difference policy behavioral compatibility agent identical behavior differently particularly application strategy player approach opponent however attack mechanism opponent within attack distance inspires heuristic partial policy inspire probabilistic finite automaton propose algorithm behavioral compatibility node algorithm retrieve action sub incomplete action correspond observation policy becomes incomplete incomplete sub occurs GR GR exist incomplete sub sub policy compatibility sub compatibility node sub label action difference successor bound threshold incomplete sub action demand compatibility recursively satisfied successor node action observation compatibility node confirm successor accordingly multiple node satisfy compatibility compatible minimum difference operator calculates frequency action specific observation policy complexity algorithm polynomial data procedure incomplete policy compatibility action node action frequency policy sub framework incomplete sub action correspond observation GL however sub compatible hence GR OL policy compatible image policy data prescribes agent action intuitively learnt policy become behavior agent amount replay data infinite action action assumes approach atj amount data complement hoeffding inequality proposition sample complexity bound rate convergence action action proposition atj NT probability action data compute algorithm atj action agent maxt atj atj δti agent reward model behavior reward behavior proof reward difference bound δti  atj error predict furthermore compose upper bound obtain probability NT detail proof appendix recall algorithm quality algorithm data reduce therefore improve prediction agent behavior directly impact agent quality experimental behavioral model implement algorithm demonstrate performance domain uav benchmark setting domain POMDP multiagent planning research application involves uav uav agent aim capture uav agent within specific model agent predict behavior agent therefore achieve aim addition conduct simulation data strategy namely  NPC non player model agent response action another NPC player agent predict action replay data stamp status player   interface develop  agent behavior  agent optimize action policy technique random rand cluster policy cpt behavioral compatibility BCT embed data driven framework incomplete policy rand action observation sample action action cpt BCT implement algorithm respectively separately dataset evaluate algorithm performance aforementioned data driven framework knowledge driven framework decision model agent knowledge transition observation reward function domain simulate interaction behavioral model policy algorithm interaction data feasible comparison data driven knowledge driven framework compute average reward application  simulation uav simulation hence strict decides model complexity hence DIDs  application DIDs significant respectively uav simulation simulate interaction agent data agent policy subsequently behavior action equivalence AE technique scalable complex domain generate policy agent selects model model simulation performance propose technique simulation uav domain respectively performance evaluates average reward agent building policy algorithm reward preferably horizontal agent average reward adopts policy generate manually built model agent model model uniformly model complex behavior data implement knowledge driven framework agent achieves performance data behavior algorithm BCT outperforms rand cpt technique generates accurate policy agent cpt technique actually performs rand cluster predict sensible behavioral agent agent model however unstable domain algorithm performs technique agent focus model agent policy negative reward agent intercept planning agent improper policy data behavior planning horizon agent consistently improves reward data agent become available finally standard deviation correspond curve BCT exhibit reliable performance deviation performance agent behavior agent uav domain image performance agent behavior agent uav domain image computational policy agent BCT cpt technique simulation data rand technique exhibit unacceptable decision quality average reward relatively due randomness policy technique involve phase cluster cpt demand iteration cluster predict dataset BCT becomes efficient comparison policy performance NPC agent learns behavior NPC agent replay data  domain image  simulation algorithm replay data battle  retrieve observation action data policy planning horizon agent policy planning horizon expand agent accordingly policy average reward agent competes agent competition receives reward learns behavior replay data BCT battle battle oppose performance previous uav domain cpt average reward BCT performance stable upon increase replay data report policy agent BCT cpt policy planning horizon execute compatibility BCT spends tedious cluster cpt summary BCT technique stable performance cpt uav  domain cpt potential capability cluster sophisticated similarity measurement function remains investigate related multiagent sequential decision  challenge artificial intelligence research review relevant research topic particularly elaborate model multiagent decision  multiagent decision building payoff matrix difficulty nash equilibrium traditional propose multiagent influence diagram  effectively static structure relationship agent  decompose feasible complex multiagent assume knowledge agent  network influence diagram nid model uncertainty agent  multiagent decision hierarchical regardless  nid nash equilibrium nash equilibrium incomplete multiple exist exist theory cannot apply decision multiagent decision multiagent sequential decision immediate reward account future reward agent decision research mainly developed decentralize partially observable markov decision dec POMDP decision agent dec POMDP model belongs nexp efficient dec POMDP expectation maximization algorithm sample technique multiagent decision agent local cooperation relation improve ability dec POMDP agent communication consultation decision decision agent technique macro action planning dec POMDP algorithm suppose belief environmental agent hence generally apply collaborative multiagent parallel agent behavior important building autonomous particularly agent interaction develop social interactive medium platform   propose finite automaton model strategy intelligent agent model   influence diagram model agent decision instead specific model agent identify agent behavior predefined theoretical concept similarly zhou yang exploit action model transfer technique improve agent planning quality parallel agent core issue hoc setting recognize challenge agent research barrett simplify MDP develop cooperative strategy teammate without prior coordination employ bayesian reinforcement monte carlo    exploit policy structure improve quality agent MDP research application multiagent decision popular topic artificial intelligence related various traditional soccer robot concern smart grid national security electronic multiagent decentralize decision apply soccer robot urban rescue focus cooperative multiagent robot soccer competition robot navigation smart grid multiagent technology improve efficiency optimize allocation mainly behavioral prediction user supplier  research mainly multiagent apply airport terminal public transportation security protection cybersecurity research interactive dynamic influence diagram DIDs recognize framework sequential multiagent decision uncertainty explicitly model agent behave agent decision optimize importantly DIDs advantage graphical representation embed domain structure explicit decompose domain variable yield computational benefit enumerative representation partially observable markov decision POMDP interactive POMDP model dec POMDP  multiagent sequential decision collaborative competitive setting convert decision avoids nash equilibrium therefore application research assumes model agent manually built random candidate model actual model agent unknown hence exist research mainly focus reduce complexity compress model agent planning horizon improve scalability benefit cluster model agent behavioral utility equivalence behavior partial recently identify agent behavior improve construct candidate model agent meanwhile   attempt model agent bayesian nonparametric intend reinforcement technique infer agent behavior extremely reveal behavior extend agent conclusion revisit knowledge driven DIDs framework multiagent sequential decision depends largely input domain expert available agent interaction data propose data driven framework DIDs policy agent limited data difficulty policy agent inspire behavioral equivalence concept propose algorithm incomplete policy cluster conduct strategy representative policy unstable performance due heuristic propose formal exploit statistical compatibility incomplete policy theoretical bound quality demonstrate performance domain datasets data driven multiagent decision nascent stage although machine technique decade research article behavior agent upon limited data DIDs model however technique apply agent context apply application domain expertise easy obtain relevant data exist behavioral model player historical player interaction data develop intelligent  strategy future improve reliability focus behavior agent interaction challenge extend research environmental agent seldom previous behavior allows complex manually model agent addition investigate inconsistent data impact behavior agent perform differently belief potential adapt behavioral compatibility unexpected