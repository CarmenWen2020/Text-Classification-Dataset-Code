graph attention network effective graph neural network perform graph embed semi supervise considers node feature novel attention graph neural network introduces attention mechanism feature node incorporate attention embed instead vector feature node traditional graph attention network propose 2D matrix node matrix attention distribution feature node compress feature fed graph attention layer aggregate matrix representation node node attention representation stack graph attention layer obtains representation node matrix considers node importance importance feature experimental citation network datasets propose significantly outperforms semi supervise classification task access auckland library introduction graph data structure expressive ability relationship strength relationship data application organize graph citation network social network knowledge graph growth data efficient representation graph becomes fundamental issue affect subsequent task attract attention recent basically graph node node described application citation network social network entity knowledge graph straightforward node vector bag representation graph adjacency matrix matrix node graph adjacency matrix graph suffers defect vector node sparse representation vector become representation seriously processing efficiency subsequent task separation matrix shot node matrix adjacency matrix increase complexity subsequent task arisen recent embed graph ideally feature node information dimensional consequently node joint perspective feature structural information attach node approximate vector representation graph embed subsequent task node classification link prediction network visualization facilitate uniform graph embed utilize adjacency matrix GF performs factorization adjacency matrix obtain preliminary embed matrix node minimizes norm matrix obtain initializes node vector random elemental node defines joint probability distribution calculate adjacency matrix calculate adjacency matrix node node embed vector embed vector node obtain minimize kullback leibler divergence distribution deepwalk random generate node sequence node sequence applies wordvec model obtain embed vector node treat sequence classify shallow model recently series namely graph neural network GNNs attract feature node bag representation embed typical graph convolutional network GCNs graphsage graph attention network  GNNs GCNs graphsage aggregate information  assign node attention easily understandable reality interpretation social network opinion leader probably impact citation network cite professor demonstrate importance gat model node closer dimensional attention important role aggregation stack attention layer node hop information attention structure information sufficiently preserve graph attention mechanism demonstrate performance graph orient task however gat model feature node vector citation network sparse bag representation node feature treat vector presentation article correspond otherwise scheme equally important article obviously violates category machine article reinforcement important moreover importance along network article highly probably category reinforcement importance preserve embed address issue propose novel graph attention network  introduce attention mechanism stack graph attention operation WGAT model vector vector belonging node fed gate recurrent gru model hidden gru representation performs attention operation vector obtain 2D matrix node representation considers attention node matrix fed stack graph attention layer aggregate matrix attention generates compress matrix perform stack graph attention operation  obtain node embed vector node concatenation 2D compress matrix representation evaluate propose WGAT model semi supervise classification task citation network datasets cora citeseer dblp experimental  significantly outperform remainder organize sect review related propose WGAT model detail experimental setting concludes related graph embed effective efficient representation graph graph analytics convert graph vector dimensional graph information preserve traditional multidimensional MDS isomap LLE laplacian eigenmaps construct affinity graph feature vector node originally distance graph involve eigenvectors affinity matrix computational complexity critical bottleneck inefficient application recent graph embed namely graph neural network GNNs aim embed representation graph preserve structural feature information node GNNs category spectral spatial spectral introduce convolution operation employ filter perspective graph signal processing convolution graph interpret remove graph signal passing message spectral domain spatial formulate convolution operation node aggregate feature derive information passing principle GNN model neural message passing propose GNN model message passing algorithm representation node iteratively compute feature differentiable aggregation function node directly link tend embed vector dimensional graph structural information preserve aggregation embed propose model hypothesis another related attention mechanism model concentrate important input decision widely processing nlp task machine translation advantage attention mechanism considers attention input output weigh importance input besides machine translation attention mechanism widely nlp task embed embed item recommendation sentiment analysis attention mechanism apply graph structure datasets allows model assign relevance graph node model assign describes relation node attention mechanism allows model avoid ignore noisy graph consequently improves model signal ratio besides attention derive propose model introduces attention mechanism node feature propose formalize obtain embed vector node epoch training finally training algorithm statement assume graph undirected citation network node contains label node unlabeled node node denotes link node article citation relationship  sparse bag feature matrix node bag label node label matrix    rnl matrix denotes label label node concept graph treat citation relationship undirected construct binary symmetric matrix adjacency matrix graph denote lij lij  node citation relationship otherwise lij  link node adjacency matrix identity matrix goal graph embed model node assign compress vector representation label unlabeled node graph predict architecture propose WGAT model node network image attention propose novel graph attention WGAT model WGAT model training prediction node bag model pas network finally obtains estimate label network sequential attention graph attention rear linear innovation bag presentation node convert compress embed vector architecture WGAT model illustrate node xxi passing network model attention structure input structure sparse bag vector node output matrix feature node considers attention node bag corpus WGAT generates embed matrix randomly initialize dimensional vector node bag feature vector WGAT embed vector embed matrix node denote   ith node contains node clarify abstract title article dataset cora rnn adapt sequence abstract article abstract default append rear ensure dimension tensor suppose abstract article pet already built bag corpus numbered numbered respectively article node vector gru bidirectional vector implement datasets bag propose attention mechanism contextual relevance previous information WGAT utilizes directional gate recurrent gru model memory lstm model sequence data performs frequent datasets gru contains series gate output previous input information previous calculate representation WGAT vector  gru model gru node node WGAT tth vector wwt tth gru model calculation tth sigmoid hht wwt sigmoid hht wwt tanh hht wwt hht hht ratio update gate ratio reset gate operator concatenation hht output gru WGAT concatenates output immediately precede gru hht vector calculates ratio update gate reset gate respectively parameter update gate reset gate WGAT calculates candidate activation vector parameter matrix tanh hyperbolic tangent finally output hht obtain simplicity encapsulate calculation function gru calculation backward WGAT conduct bidirectional computation denote gru wwt gru wwt arrow direction calculation finally WGAT defines output concatenation directional output vector hht therefore entire output gru model matrix representation feature node  dimension output vector gru node vector WGAT introduces attention mechanism assigns vector attention define softmax  wht rda parameter matrix  parameter vector vector attention hhi standard  function RK RK define softmax   RK compress vector representation node obtain sum vector  attention   however attention vector reflect importance distribution actually multiple importance distribution inhabit node therefore WGAT model obtain attention namely multiple hop attention sake WGAT model extends  matrix attention hop model attention vector formally softmax  wht softmax function performs vector input matrix attention matrix 2D matrix representation node obtain multiplication attention matrix gru output matrix AH graph attention graph importance another graph attention node calculate node 2D matrix compute attention matrix node attention distribution WGAT graph attention operation node counterpart matrix node manner stack graph attention layer graph attention operation layer aggregate information node detail graph attention operation denote vector node aggregation impose linear layer upon vector denote  matrix apply  node node link WGAT calculates coefficient introduce learnable attention vector exp LeakyReLU  LeakyReLU node node calculate adjacency matrix Î±ij attention node leaky rectify linear LeakyReLU activation function define vector RK LeakyReLU  graph attention graph attention representation node calculate aggregate LeakyReLU   graph attention representation matrix node usually compress representation matrix dimensional input another stack graph attention layer adopt multiple stack graph attention layer another implementation WGAT stack graph attention layer 2D graph attention representation matrix node generate input vector rear WGAT concatenate vector vector preserve information multiple attention hop embed vector node  rmm therefore   rear WGAT graph attention feature representation vector  finally fed rear WGAT model structure rear network  fed liner layer output layer contains concept neuron immediately linear layer softmax activation function layer output estimate probability concept node node unlabeled probability predict distribution node label training phase WGAT calculates entropy loss label loss  softmax bbu  label vector node  linear matrix bbu bias vector linear layer training utilizes adam optimizer minimize loss rear structure WGAT model image training algorithm propose WGAT training epoch label node pas network summarize training algorithm algorithm initializes graph generate label node node unlabeled node therefore training semi supervise label node network another described clearly evaluate propose model conduct extensive citation network experimental setting experimental analysis experimental setting datasets citation network evaluate model node article article citation relation citation network datasets cora cora article category node label abstract article feature article ensure accuracy applicability feature remove frequency  auxiliary punctuation meanwhile bag finally dataset retains node citeseer citeseer article category title article feature dataset contains invalid article impact meantime dataset retains node finally bag dblp dblp node article research dataset contains node abstract article feature bag datasets unweighted characteristic datasets network cora denser network citeseer dblp citeseer distribution imbalanced dblp network characteristic datasets comparison propose WGAT model simply introduce deepwalk graph embed performs random graph treat equivalent parameter tune node dimension node embed nodevec deepwalk performs bias random graph dimension node embed deepwalk bias random parameter node embed vector apart proximity proximity embed dimension node negative sample  matrix factorization adjacency matrix obtain node embed adjacency matrix dimension node embed MMDW unified network representation framework jointly optimizes max margin classifier aim social representation model embed contains structure information characteristic discrimination random balance parameter gcn variant convolutional neural network graph encodes local graph structure feature node dimension node embed layer structure graphsage inductive framework leverage node feature efficiently generate node embed previously unseen data aggregator gcn aggregator embed dimension gat leverage node feature attention layer gat model multi attention dimension output gat layer WGAT model dimension embed hidden dimension gru parameter attention respectively parameter respectively setting gat finally rate adam optimizer evaluation metric conduct semi supervise manner dataset randomly training contains node training randomly hid label portion node portion label node training datasets preparation training average performance report evaluation multi classification micro evaluation metric experimental overall evaluation proportion node training datasets label micro evaluate experimental experimental performance marked bold GNN evaluate WGAT model training setting increase percentage label node datasets micro evaluation metric potentially bias macro metric investigate WGAT performance mention GNN classification model experimental micro percentage refer WGAT model significantly outperforms datasets particularly gat vector node feature WGAT model capture attention information hidden node feature obtains performance semi supervise classification task cora citeseer dblp improvement indicates WGAT advantage datasets traditional  embed deepwalk nodevec MMDW generally perform GNN gcn graphsage gat WGAT cora citeseer perform slightly WGAT dblp gat WGAT graph attention mechanism GNN gcn graphsage WGAT advantage graph attention information performance  model deepwalk GNN model evaluates GNN model model semi supervise label node rate unbiased comparison micro macro evaluation metric experimental GNN model easy micro macro evaluate experimental WGAT model consistently outperforms GNN model training setting dataset dblp WGAT advantageous maximum increment label node micro another phenomenon percentage label node increase performance model consistently increase indicates percentage label node performance improvement model however advantage model gradually weaken label node information available classification network sufficient phenomenon WGAT model significant advantage label percentage application label percentage network sufficient label parameter tune verify robustness WGAT model adjust hyperparameter setting investigate impact hyperparameter performance model tune attention hop dimension embed hidden dimension gru evaluate WGAT datasets node label adjust hyperparameter sect attention hop dimension embed hidden dimension gru dimension embed attention hop hidden dimension gru hidden dimension gru attention hop dimension embed experimental experimental label node rate micro percentage experimental label node rate macro percentage experimental hyperparameter setting micro percentage attention hop defines vector node construct 2D matrix representation WGAT suppose information attention hop investigate influence dataset cora model obtains performance attention hop attention hop increase performance longer increase slightly decline indicates representation vector probably redundancy information impairs discrimination node classification attention hop promote phenomenon datasets citeseer dblp decline performance significant attention hop maximum difference dimension embed another important hyperparameter model determines representation vector optimize training initial transformation investigate impact hyperparameter dimension embed datasets cora citeseer dblp performance obtain dimension embed  performance however difference performance setting significant dimension longer training usually hyperparameter hidden dimension gru output vector gru hidden dimension output vector gru twice model directional gru combine directional output vector performance micro datasets cora citeseer consistently obtain hidden dimension dblp performance obtain hidden dimension setting difference performance significant sum propose WGAT model robust parameter setting obtain performance accord conclusion novel graph attention neural network jointly attention mechanism node assign attention feature node model obtain representation node 2D matrix graph attention operation aggregate information attention 2D matrix representation node preserve structure information finally concatenation 2D matrix presentation fed linear layer optimize experimental article citation network datasets consistently propose model significantly outperforms semi supervise classification task graph attention mechanism benefit model although model achieve performance future research progress proximity graph node mainstream graph embed model proximity define similarity node graph node embed however addition proximity structure fully integrate motif structure network network embed addition complex node local structure define proximity node assumption proximity proximity assumption node relationship assumption apply application link prediction however cannot retain centrality information node centrality node usually related complex local structure address