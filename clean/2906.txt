graph representation attract attention recently exist graph neural network fed graph data scalable due limited computation memory remains challenge capture information graph data besides mainly focus supervise highly node label information expensive obtain unsupervised network embed approach overemphasize node proximity instead representation hardly downstream application task directly recent emerge supervise potential address aforementioned however exist supervise graph data bias global local hop neighborhood graph structure define mutual information loss novel supervise representation via sub graph contrast namely SUBG con propose utilize correlation central node sample subgraphs capture regional structure information instead input graph data novel data augmentation strategy SUBG con learns node representation contrastive loss define subgraphs sample graph instead besides enhance subgraph representation via mutual information maximum preserve topology feature information exist graph representation approach SUBG con prominent performance advantage weaker supervision requirement model scalability parallelization extensive verify effectiveness efficiency classic graph representation approach various downstream task multiple benchmark datasets domain access auckland library introduction graph representation attract attention recently extract dimensional information graph structure data embed dimensional vector representation node representation vector potentially various downstream task node classification link prediction graph classification graph alignment graph representation graph data domain social network chemical molecular graph bio medical brain graph exist successful graph neural network GNNs node contextualized representation via effective neighborhood information aggregation usually graph input hardly apply graph data facebook twitter billion node inter graph structure prevents parallel graph representation critical graph data addition exist graph neural network focus supervise encode graph structure representation vector supervision label information however graph data manual graph label tedious expensive becomes infeasible graph overcome challenge unsupervised setting instead optimize model objective function define capture node proximity reconstruct graph structure however detach supervision information representation unsupervised approach hardly downstream application specific task objective supervise recently emerge promising approach overcome dilemma lack available supervision define annotation pretext task generate surrogate training sample automatically without annotation training sample encoder representation computer vision data augmentation flip commonly training sample generation improve generalization supervise however due unordered vertex extensive connection graph data exist technique mention cannot anymore data augmentation graph data specifically supervise graph representation research exist prior topic graph  graphical mutual information approach unsupervised model graph   introduces global pretext task discriminate actual node representation corrupt global graph representation graphical mutual information gmi local structure maximize mutual information hidden representation node feature directly adjacent illustrate tend bias fitting overall local hop graph structure define mutual information loss harm quality representation besides supervise adopt graph neural network encoder graph input restricts scalability graph illustration  upper gmi propose SUBG con denote learnt representation node denote central node context subgraphs SUBG con utilizes correlation central node context subgraphs sample graph SUBG con encodes sample subgraphs graph input besides SUBG con capture structure information regional neighborhood instead tend bias fitting overall local hop graph structure image intuitively node regional correlate node away hardly influence graph therefore subgraphs consist regional neighborhood critical role structure context node representation propose novel scalable supervise graph representation via sub graph contrast SUBG con correlation central node regional subgraphs involve node away consideration illustrate specifically introduce data augmentation strategy graph firstly global local subgraph sample central node closely related surround node sample graph compose context subgraphs subgraphs fed graph neural network encoders obtain representation central node subgraphs pool finally contrastive loss introduce latent encoder distinguish generate positive negative sample introduce later node regional structure differentiate besides preserve topology feature information subgraph representation enhance subgraph representation maximize mutual information subgraph representation context subgraphs enhance graph representation denote SUBG con previous operating graph structure SUBG con SUBG con capture regional information context subgraphs simpler structure besides sample subgraph instance SUBG con SUBG con easy parallelize critical graph data empirical assessment benchmark graph datasets multiple diverse demonstrate representation propose model consistently competitive various downstream task outperforms supervise unsupervised baseline carefully influence component framework model performance besides verify efficiency propose model training computation memory supervise graph summarize contribution propose novel supervise graph representation via sub graph contrast utilizes correlation central node context subgraphs capture regional graph structure information introduce data augmentation strategy graph aim increase training sample exist graph subgraph sample supervise graph representation training subgraphs structure propose SUBG con SUBG con training computation memory graph representation sample subgraph instance enables parallel graph representation improve efficiency scalability extensive verify effectiveness efficiency prior unsupervised supervise approach multiple graph datasets domain framework fashion abstract overview specific subgraph representation setup exposition subgraph sample data augmentation subgraph encode representation enhance subgraph representation supervise pretext task model optimization finally introduce parallel SUBG con briefly subgraph supervise representation prior preliminary concept assume supervise graph representation setup graph node feature node graph RF feature dimension node relational information node adjacency matrix RN consist arbitrary arbitrary feature assume graph unweighted exists graph otherwise traditional graph representation target training encoder RN RN RN encode graph latent node representation RN dimension latent representation convenience representation node representation generate retrieve downstream task node classification however due limited computation memory remains challenge traditional graph structure input handle graph overcome limitation traditional propose novel subgraph representation approach central node subgraph sampler proxy data augmentation extract context subgraphs RN graph context subgraph regional structure information representation node RN denotes node feature ith context subgraph denotes relational information node node indicates context subgraph target encoder context subgraphs RN RN RN serf acquire node representation within context graph traditional input encoder context subgraphs graph node representation retrieve context subgraph structure flexibly without graph operating sample subgraph instance SUBG con prominent performance advantage model scalability besides easy parallelize critical graph data focus subgraph supervise context subgraph extraction subgraph encode representation supervise pretext task model optimization context subgraph extraction subgraph sampler proxy data augmentation importance sample closely related node compose context subgraphs regional structure information representation subgraph encode target encode structure feature context subgraphs encoder central node representation consequence summarize subgraph around node subgraph representation supervise pretext task optimize encoder advantage correlation central node context subgraphs regional information capture context subgraphs embeds central node representation subgraph sample data augmentation overcome dependence manual label important supervise generate surrogate training sample automatically encoder representation data augmentation popular technique training sample generation computer vision however due unordered vertex extensive connection explicitly graph data supervise graph representation introduce concept data augmentation graph formally definition data augmentation graph graph denotes node feature denotes relation data augmentation strategy series variant graph various transformation feature relation various transformation graph data node mask feature corruption adopt subgraph sample data augmentation strategy intuitively node regional neighborhood correlate distance node hardly influence assumption reasonable graph increase therefore sample series subgraphs regional neighborhood graph training data critical issue sample context subgraph sufficient structure information quality representation central node strategy sample subgraphs analyze algorithm complexity strategy evaluate experimental global structure strategy sample strategy personalize pagerank algorithm introduce importance varies specific node subgraph sampler importance node personalize pagerank algorithm relational information node adjacency matrix RN importance matrix denote identity matrix parameter denotes correspond diagonal matrix diagonal AD denotes normalize adjacency matrix importance vector node indicates correlation node importance matrix precomputed model training implement node wise ppr calculate importance reduce computation memory suitable graph local structure strategy sample global structure topological structure graph consideration computational complexity relatively inspire subgraph sample strategy introduce random capture local structural feature relevant subgraph central node graph strategy extract subgraphs important node importance along random central node reflect local graph structure passage obtain probability matrix markov chain theory straightforward implementation consume relies matrix inversion usually perform cubic complexity node graph reduce computational demand approximate probability matrix random central node perform random repeatedly limit maximal specific average node importance matrix obtain average importance node central node typically fix magnitude graph strategy essentially linear complexity relative graph restrict convenient computational besides locality central node extract subgraphs avoid diffuse local sample strategy computes node relevance random central node subgraph obtain minimal importance threshold relevance importance automatically fix subgraphs extract weakly besides importance compute strategy input graph update iterative increase discrimination important parameter iterate subsequent encode subgraphs newly generate token consideration obtain representation subgraphs accord sample strategy importance matrix acquire specific node subgraph sampler chooses important constitute subgraph matrix index chosen node denote idx rank rank function return index denotes context graph subgraph sampler graph node index obtain context subgraph node adjacency matrix feature matrix denote respectively   idx idx index operation  wise node wise indexed feature matrix  idx wise col wise indexed adjacency matrix correspond induced subgraph acquire context subgraph specific node input graph procedure parallel compute improve efficiency context subgraphs data augmentation decompose mini batch fed SUBG con encode subgraph representation context subgraph central node encoder RN RN RN encodes obtain latent representation matrix denote adopt graph neural network GNN flexible node embed architecture encoder node representation generate aggregate information impact graph neural network discus later central node embed picked latent representation matrix denotes operation central node embed mention consequence summarize subgraph around node context subgraph representation obtain subgraph summary vector leverage readout function RN RF summarize obtain node representation subgraph representation denote representation central node context subgraphs role generation positive negative sample supervise pretext task enhance subgraph representation utilize correlation central node subgraphs supervise pretext task therefore quality subgraph representation crucial supervise accord previous currently subgraph representation obtain readout function node representation ensure subgraph representation fully inherit structural information therefore preserve topology feature information subgraph representation enhance subgraph representation maximize mutual information subgraph representation context subgraphs specific node context subgraph subgraph representation mutual information denote accord information theory define logp  denotes empirical probability distribution node feature denotes probability hhi xxi joint distribution suppose conditional probability multiplicative inspire decompose sum mutual information subgraph node namely  initial feature node subgraph node wij satisfies wij fix importance subgraph sample strategy introduce subgraph structure attribute learnable relate underlie distribution subgraph topology specifically denote mutual information subgraph representation context subgraphs  wij aij aij node importance matrix subgraph wij  indicates relevance subgraph representation node representation enhance quality maximize mutual information subgraphs representation objective function training detail introduce later contrastive via central node context subgraph supervise contrastive define annotation pretext task generate positive negative sample encoder contrast positive negative prerequisite ensure quality representation pretext task fully inherit information graph obtain representation subsequent mining task without additional guidance intuitively node dependent regional neighborhood node context subgraphs assumption reasonable graph structure graph handle exist node representation therefore correlation central node context subgraphs supervision pretext task architecture SUBG con fully summarize approach encoder relies specific central node contrast context subgraph fake specifically node representation capture regional information context subgraph regard context subgraph representation positive sample subgraph representation employ function corrupt generate negative sample denote architecture SUBG con series context subgraphs sample graph fed encoder obtain representation central node subgraphs pool specific node representation context subgraph regard positive sample corrupt subgraph representation negative sample contrastive loss latent encoder recognize positive negative sample node discriminate regional structure information image representation corruption strategy determines differentiation node context crucial downstream task node classification objective related contrastive objective standard binary entropy loss positive negative however context subgraphs extract graph overlap suppose harmful representation positive negative distinguish absolutely therefore margin triplet loss model optimization positive negative sample discriminate extent quality representation obtain loss denote max  exp sigmoid function margin addition introduce previous attempt enhance subgraph representation maximize mutual information generate positive negative sample optimize mutual information subgraph representation specifically objective function define summarize procedure approach algorithm distinguish variant SUBG con enhance subgraph representation denote SUBG con training phase subgraphs subgraph feature nonzeros adjacency matrix subgraphs propose complexity complexity  parallelizability exist input graph data parallelizable context subgraphs subgraph extraction easy parallelize random worker thread machine simultaneously explore graph context subgraphs extraction without global computation graph structure becomes encoder multiple subgraphs synchronously obtain representation central node subgraphs benefit parallelizability model efficiently graph conduct extensive verify effectiveness efficiency SUBG con variety task multiple datasets domain node classification link prediction SUBG con node representation fully unsupervised manner approach prior unsupervised supervise baseline besides analyze architecture encoder architecture objective function efficiency training memory usage reduce training subgraphs parallelization improve efficiency lastly parameter sensitivity analysis suitable parameter approach datasets ass effectiveness representation conduct multiple datasets domain popular datasets widely related cora citeseer pubmed datasets verify scalability approach ppi flickr reddit citation network social network protein network information datasets benchmark classification task classify research topic cora citeseer pubmed citation network classify protein role within protein protein interaction ppi network generalization unseen network categorize image description flickr online predict community structure social network model reddit experimental setting encoder datasets impact graph neural network described employ distinct encoders appropriate cora citeseer pubmed ppi adopt layer graph convolutional network gcn skip connection encoder propagation XW  adjacency matrix insert loop correspond matrix nonlinearity apply parametric relu  function learnable linear transformation apply node  learnable projection matrix skip connection dataset statistic performance comparison node classification reddit flickr adopt layer gcn model encoder propagation gcn XW gcn gcn latent representation layer gcn fed input layer corruption function corruption function generates negative sample supervise task node context distinguish important node classification task convenience computation context subgraph representation corruption function shuffle randomly subgraph representation central node regard negative sample node closely related context subgraphs weakly associate subgraphs node representation task appropriate corruption strategy remains research corruption function randomly sample subgraphs  subgraphs randomly modify initial node feature subgraphs readout function experimental datasets employ identical readout function average node feature logistic sigmoid nonlinearity assume readout efficient subgraphs perform across objective function margin loss commonly contrastive loss function logistic loss bayesian personalize rank BPR loss objective function impact later sample strategy addition subgraph sample strategy mention namely global local experimental considers naive strategy randomly sample neighborhood context subgraph central node subgraphs previous hop central node hop ego network explore suitable subgraph sample strategy datasets implementation detail implement baseline SUBG con pytorch geometric extension library conduct nvidia titan gpus SUBG con node representation fully unsupervised manner evaluate node classification representation perform directly representation linear logistic regression classifier preprocessing perform normalization cora citeseer pubmed apply processing strategy reddit ppi flickr ppi standardize embeddings logistic regression classifier training adam optimizer initial rate specially citeseer reddit subgraph specially subgraph citeseer due performance dimension node representation margin loss function objective function baseline supervise  gmi graph embeddings leverage mutual information maximization traditional unsupervised deepwalk unsupervised variant graphsage abbreviate unsup graphsage model specially training logistic regression raw input feature besides report supervise graph neural network gcn gat FastGCN supervise graphsage notably reuse metric already report optimal hyper parameter carefully reproduce code baseline ensure fairness comparison verify effectiveness enhance subgraph representation refer model without SUBG con SUBG con respectively evaluation metric node classification task embeddings across training logistic regression classifier node adopt classification accuracy evaluate performance benchmark datasets cora citeseer pubmed micro average average datasets ppi flickr reddit datasets fix partition split link prediction task hidden input graph goal predict existence compute embeddings adopt auc evaluation metric probability randomly chosen ranked randomly chosen negative auc approach performance achieves report auc average exist link graph datasets positive link sample subset unknown link node graph randomly negative link positive social link randomly sample positive negative validation node classification comparative evaluation summarize demonstrate performance achieve across datasets successfully outperforms compete supervise approach verify potential graph regional structure information node classification domain due enhance subgraph representation performance SUBG con achieves improvement supervise competitive traditional unsupervised baseline rely proximity objective indicates data augmentation strategy supervise contribution model capture information complex graph supervision signal  related node classification task besides particularly  approach competitive report supervise graph neural network exceed performance cora citeseer pubmed reddit datasets however ppi gap encoder heavily dependent node feature available feature ppi extremely sparse node zero feature link prediction evaluation link prediction summarize SUBG con SUBG con outperform baseline datasets cora citeseer pubmed achieves auc gain nearly flickr gain moderately average however ppi reddit gmi performs slightly gain explain dense graph proximity similarity predict potential connection node therefore gmi emphasizes local structure advantage moreover performance propose perform semi supervise SUBG con SUBG con latter obvious advantage datasets owe enhance sub graph representation performance comparison link prediction architecture explore framework subgraph encoder corruption function objective function subgraph sample strategy convenience experimental report node classification task encoder architecture performance conduct encoder graph neural network encoder node representation graph convolutional network gcn graph convolutional network skip connection gcn skip graph attention network gat graph isomorphism network gin experimental gcn skip connection achieve performance citeseer pubmed ppi although gat competitive cora gat training memory gcn skip connection encoder finally gcn choice datasets gcn encoder outperforms supervise gcn datasets flickr reddit layer gcn option assume information capture graph contribution improve quality representation sum graph complex structure subgraphs encode graph neural network expressive GNNs gat gin suitable handle subgraphs comparison graph neural network encoders comparison model objective function effectiveness objective function objective function comparison tune hyperparameters loss function report margin loss achieve performance loss context subgraphs extract graph somewhat suitable apply loss function distinguish positive negative absolutely corruption function experimental corruption function corruption function determines generation positive negative sample impact experimental datasets corruption function fluctuation evaluation node classification datasets corruption function modify shuffle relatively strategy slight modification distinguish exist subgraphs others model performance via corruption function sample strategy suitable subgraph sample strategy datasets experimental random sample strategy important node accord graph structure achieve subgraphs regional information addition graph datasets ppr outperform others assume difficulty extract important node local structure graph therefore global structure contribute node representation addition although  strategy achieve sample increase storage requirement dense graph model performance via subgraph sample strategy effectiveness training encoder sample subgraphs image composition context subgraphs datasets pie indicates proportion distance central node context subgraphs image efficiency subgraphs context subgraphs structure assume maybe extract subgraphs unnecessary training encoder therefore conduct training encoder subgraphs sample graph effectiveness sample subgraphs datasets cora citeseer pubmed subgraphs sufficient information encoder datasets ppi flickr reddit subgraphs sparsity graph difference node cora citeseer pubmed therefore subgraphs extract datasets contrary ppi flickr reddit relatively denser context subgraphs likely compose encoder capture structure easily verify  composition subgraphs datasets observation encoder subgraphs accelerate convergence loss function training computation memory efficiency SUBG con datasets efficiency SUBG con datasets training memory summarize performance supervise training memory usage relative datasets training refers training encoder exclude validation memory refers memory model parameter hidden representation batch supervise baseline apply gcn encoders cora citeseer pubmed cannot graph due excessive memory requirement graph graphsage sample graph neural network node representation strategy validation patience epoch specially epoch pubmed accord finding previous subsection subgraphs randomly sample encoder datasets subgraphs datasets clearly faster computation memory baseline datasets advantage efficiency prominent graph reddit graph structure subgraphs speedup encoder training besides training subgraphs reduce training memory usage parallelize image parallel computation complex realistic application scenario training subgraphs SUBG con efficiently parallel subgraphs training epoch multiple gpus datasets ppi flickr reddit parallelize processing data accelerate increase gpus loss predictive performance relative model serially demonstrate technique highly scalable subgraph analysis examine influence context subgraphs framework datasets adjust subgraph central node evaluate model achieve performance context subgraphs regional structure information contribution quality latent representation due limited computation memory subgraph however exception subgraphs increase performance citeseer becomes peak due sparsity citeseer subgraphs compose node sufficient context information subgraphs complex structure deteriorate representation subgraph citeseer subgraphs impact datasets specifically encoder subgraphs node central node closest related performance degrades datasets decrease reddit indicates reddit complex structure therefore insufficient proxy relatively informative context consideration model subgraph analysis image related graph neural network graph neural network graph structure node feature node representation vector exist graph neural network neighborhood aggregation strategy iteratively update representation node aggregate representation node combine representation exist graph neural network advance multiple successful application across domain however usually graph input hardly apply graph data inter graph structure prevents parallel graph representation critical graph data handle issue sample propose GNNs mini batch node aggregate representation subset randomly sample node mini batch although approach reduces computation aggregation operation besides graph neural network mainly focus supervise supervision label information intractable handle unlabeled graph widely available practically unsupervised node representation abundant literature traditional unsupervised representation node within graph exist optimize model random objective reconstruct graph structure underlie intuition encoder network node input graph representation although capture node proximity suffer limitation prominently emphasize proximity similarity capture inherent graph structural information besides encoders already enforce inductive bias node representation unclear objective actually useful signal training encoder exist fail task strongly supervise supervise supervise recently emerge promising approach overcome dilemma lack available supervision define annotation pretext task generate surrogate training sample automatically encoder representation variety pretext task propose visual representation however literature supervise graph representation graph  aim node encoder maximizes mutual information node representation pool global graph representation graphical mutual information proposes maximize mutual information hidden representation node feature hop tend bias fitting overall local hop graph structure define mutual information loss harm quality representation besides supervise graph input restricts scalability graph conclusion propose novel scalable supervise graph representation via sub graph contrast SUBG con utilizes correlation central node regional subgraphs model optimization sample subgraph instance SUBG con prominent performance advantage weaker supervision requirement model scalability parallelization empirical assessment multiple benchmark datasets demonstrate effectiveness efficiency SUBG con supervise unsupervised baseline encoder popular graph datasets regional information indicates exist lack ability capture information exist graph dataset information performance inspire research graph structure explore