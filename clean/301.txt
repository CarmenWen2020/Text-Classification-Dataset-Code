neural network NN although successfully apply artificial intelligence task unnecessarily parametrized fog compute training prohibitive resource constrain device contrast trend decentralize intelligence remote data centre local constrain device therefore investigate training effective NN model constrain device fix potentially memory budget target technique resource efficient performance effective enable significant network compression dynamic prune DynHP technique incrementally prune network training identify neuron marginally contribute model accuracy DynHP enables tunable reduction neural network reduces NN memory occupancy training freed memory reuse dynamic batch approach counterbalance accuracy degradation prune strategy improve convergence effectiveness ass performance DynHP reproducible public datasets reference competitor DynHP compress NN without significant performance additional error competitor reduce training memory occupancy keywords artificial neural network prune compression resource constrain device introduction AI successfully adopt variety task neural network NN successful technology achieve performance application image recognition computer vision processing recognition ingredient nns increase availability training datasets possibility model parameter tractable optimization mini batch stochastic gradient descent sgd graphical processing gpus parallel compute nevertheless nns characterize drawback research challenge recently proven nns suffer parametrization prune significantly without loss accuracy moreover easily memorize random data properly regularize NN typically data centre plenty storage computation resource data training input data inference scenario emerge application enable widespread diffusion iot device commonly refer fog compute environment typical application smart autonomous vehicular network iot device fog environment generate amount data impossible impractical remote data centre training inference typically privacy ownership constraint data approach unfeasible therefore knowledge extraction leverage distribute data collection compute paradigm whereby nns location closer data generate fog gateway individual device tablet raspberry PIS unfortunately respect data centre device limited computational memory network capability context exploitation model offline asks significant amount memory mbytes gbytes inference gflops computation inference alexnet network    model memory furthermore resource constrain device limited availability device battery interestingly consumption device dominate memory access dram access magnitude SRAM access magnitude CMOS operation limitation jeopardize exploitation model offline resource constrain device characterize fog compute paradigm training dnns device challenge training phase typically resource hungry inference phase nowadays researcher investigate neural network resource constrain device effort focus enable training inference device limit possibly totally avoid loss performance introduce reduce memory computational capacity device target approach broadly classify research already neural network reduce distribute device collaborate inference phase investigate approach prune quantization knowledge distillation proposal training employ neural compression technique neural architecture identify effective configuration actively reduce model important technique latter approach typically compression efficacy average neuron simultaneously active training neural compressor training epoch switch neuron accord criterion however prune technique neuron switch switch conversely prune technique adopt neuron switch lose prune guarantee flexibility training reduce memory occupation model training information switch neuron epoch epoch aim prune technique compress neural network resource constrain device typical fog environment compress neural network directly fog device paramount importance achieve pervasive effective efficient neural network AI environment target technique resource efficient comparable performance respect conventional neural network training algorithm enable significant compression network target goal assume training neural network fog device relies fix budget memory perform assumption fog paradigm characterize computation data source scenario device employ perform operation parallel training neural network operation related data gathering index fog scenario standard environment assume server fully available neural network propose technique effective compression network training novel technique dynamic prune DynHP prune incrementally permanently network identify neuron contribute marginally model accuracy DynHP enables significant controllable reduction neural network moreover prune neural network progressively reduce memory occupy model training progress finally prune NN fix memory budget important feature scenario fog device nvidia xavier  nano multiple containerize training nns local data however prune neuron brings related convergence training accurate precisely slows convergence stochastic gradient descent susceptible stuck local minimum capitalize increase amount memory training introduce dynamic mini batch network avoid limitation dynamic batch technique enables convergence effectiveness network amount data batch DynHP tune batch dynamically epoch epoch compute optimal function variance gradient training moreover dynamically adjust amount data constraint memory available training proposal effectively reuses memory progressively prune increase batch estimate gradient improves convergence quality achieve DynHP fold training compress neural network directly resource constrain fog device technique minimizes performance loss due network compression reuse memory network increase batch improve convergence accuracy explicitly target training neural network memory constraint dynamically optimize basis amount memory available ass DynHP public datasets mnist fashion mnist cifar competitor reproducible DynHP effectively compress fold dnn without significant effectiveness degradation additional misclassification error competitor moreover beyond obtain highly accurate compress model dramatically reduce overall memory occupation entire training memory training structure discus related background moreover introduces prune neural network dynamic prune technique discus experimental public datasets finally concludes article future related neural network model parametrized model parameter complexity model exceed due propose reduce neural network model parameter challenge obtain network model parameter achieve accuracy parametrized model literature direction emerge along categorize algorithmic cope category perform compression training phase parametrized model approach exploit manipulate obtain generalization capability seminal category knowledge distillation already massive complex neural network teacher network training network task mimic behaviour teacher network approach propose prune useless redundant connection network propose prune methodology network prune redundant finally prune network similarly propose compression phase prune splice briefly prune phase connection deem useless remove however recover possibly aggressive prune sheer accuracy performance model author propose reactivate connection generalization capability network explore despite impressive compression accuracy technique training phase compression incrementally therefore memory usage advantage become significant model deployed training phase resource demand alternatively instead prune connection already parametrized model approach propose numerical representation apply network training phase propose stage pipeline prune quantization huffman cod reduce storage requirement neural network without affect accuracy propose network binary activation function network faster achieve nearly accuracy relatively datasets mnist cifar however challenge datasets imagenet approach obtain significant accuracy another principle proposes fix quantization drawback apply already model similarly propose binary custom backward pas nearly accuracy relative datasets mnist cifar summarize quantization sparsification orthogonal propose apply principle reduce memory occupation model finally category compress model training phase accord literature compression training mainly prune accomplish prune sparsifying model prune neuron remove entire matrix research directly sparsely neural network replace zero random connection experimentally technique effective sparse network negligible loss performance integrate magnitude prune training model reduce gate variable aim minimize nonzero parameter network recently propose model compression generates sparse model without additional overhead propose technique dynamic allocation sparsity incorporate feedback signal reactivate prematurely prune cifar effectiveness technique effective sparse network research rely neuron redundant compute similarity neuron remove neuron another structure approach author propose structure dropout technique prune convolutional kernel neural network efficient propose structure bayesian dropout allows sparsify network training finally   introduce concept ticket existence network inside sufficient accuracy parameter author propose discover subnetworks advantage compress accurate network training phase technique allows reduce memory network training prune scatter helpful memory purpose training matrix memory conversely prune neuron effective actually reduce matrix gate mechanism propose specifically novel technique dynamic prune DynHP prune incrementally permanently network identify neuron contribute marginally model accuracy DynHP enables significant reduction neural network moreover prune neural network significantly reduce memory occupy model training available memory reuse minimize accuracy degradation prune strategy adaptively adjust data neural network improve convergence effectiveness background introduce notation discus technique prune neural network training inspire sake clarity report detail preliminary assume dataset dimensional observation accompany label target supervise label per observation neural network model denote function additionally refer neuron loss function evaluate prediction accuracy model operator hadamard wise vector matrix finally denote generic norm prune neural network inspire technique propose refer prune SP SP regularize empirical risk minimization component average loss model dataset component norm regularizer tune parameter norm non zero parameter neural network regularization algorithm towards connection non zero neuron become zero neuron prune network reduce however formulation grain belonging individual neuron prune strategy particularly effective drawback norm differentiable function prevents straightforward usage gradient optimization training neural network overcome propose approximate norm equivalent differentiable function propose parametrization parameter define binary gate activation parameter therefore norm becomes intuition model gate bernoulli random variable precisely binary gate probability active adopt probabilistic representation gate loss become random variable therefore objective function become average affected active binary gate minimize generalization error sum probability gate however function suitable efficient gradient computation due bernoulli distribution discrete function consequently prevents smoothness propose substitute bernoulli distribution model gate concrete distribution continuous differentiable approximation skip technical differentiable version denote cdf concrete distribution assumes training occurs epoch epoch random activation gate equation average sample average loss obtain standard estimator average average loss average non zero gate minimize objective function respect meaning parameter average really useful prediction discussion summarize accord SP neuron associate gate active probability parameter varies training accord precisely trainable parameter NN update backpropagation concrete gate modulate output correspond neuron similarly happens dropout difference activation probability training gate training neuron contribute generalization capability NN probability remain active parameter grows towards activation probability redundant decrease towards however although technique allows obtain compress version initial network maintain almost generalization performance fundamental training compression virtual accord gate mechanism active neuron decrease average meaning neuron switch epoch return active remove network characteristic shortcoming purpose devise suitable training model limited memory footprint dynamic prune neural network introduce discus dynamic prune technique training compress neural network fix memory budget sake clarity split presentation prune technique ablate neural network training introduce mechanism dynamic batch overall training achieve significant reduction neural network loss accuracy remove useful neuron epoch memory reduce accuracy exploit freed memory within dynamic batch optimize performance prune network prune prune mechanism HP strategy aim identify remove useful neuron inward outward connection thanks HP network monotonically decrease training progress achieve correspond incremental reduction network memory footprint differently SP ablation neuron perform epoch cannot subsequently revert however permanently delete network highly detrimental performance avoid disrupt capability network carefully identify neuron removal harm harm minimally quality training exploit gate mechanism detailed statistic gate activation fix observation training epoch epoch compute average activation rate gate identify active neuron prune formally vector binary information regard status active inactive neuron layer vector update epoch active neuron layer moreover vector activation rate neuron layer epoch compute random gate active epoch gate observation active inactive training epoch identify active neuron layer thresholding function fix threshold apply wise function vector obtain binary vector identifies active neuron layer epoch information update status neuron addition vector binary matrix vector dimension successive layer finally zero correspond deactivate neuron HP epoch layer network dynamic prune dynamic prune technique HP technique effective reduction neural network training progress significant impact convergence training possibly accuracy respect SP technique performance mostly due interplay prune training sgd algorithm reduce error HP perform epoch increase error due ablation network moreover additional difficulty regard relative contrast prune due aggressive parameter involve network loses expressiveness consequence converge model conversely prune  model unnecessarily parametrized therefore crucial dimension involve hyper parameter dynamic prune actively prune parameter regulates aggressiveness prune defines threshold identify active neuron network dynamic parameter rate batch important role affect update model backpropagation actively convergence performance preserve possibility converge network progressively loses adaptively reduce rate counterbalance destabilize prune alternatively obtain generalization performance adaptively adjust mini batch epoch epoch training exploit equivalence propose dynamically update mini batch training fix rate obtain benefit parameter tune growth rate mini batch explain convergence amount memory dynamically regulate batch accord relative variance variance ratio gradient precisely link mini batch amount variability gradient index incrementally increase mini batch precise estimation gradient accord amount variance instability gradient compute index algorithm propose effective estimate variance gradient gradient computation formally variance estimation gradient mini batch loss function mini batch gradient computation mini batch compute smooth parameter batch accord variance gradient therefore parameter parameter employ batch accord stability gradient estimation mention batch increase epoch epoch quantity proportional variance gradient rationale exploit variance gradient accelerate convergence sgd contrast prune moreover training evolves increase mini batch stabilize training tune network introduction batch indefinitely resource constrain device memory limited harm training neural network avoid approach exploitable fix memory budget setting guarantee DynHP exceed memory budget training epoch prune network compute memory available growth mini batch difference memory budget memory occupation network impose maximum increase mini batch denote candidate mini batch compute account maximum limitation obtain mini batch epoch network prune perform epoch therefore epoch additional memory available mini batch conduct aim comprehensively evaluate proposal SP competitor precisely interested research RQ extent HP technique compress network reduces parametrization HP impact quality model RQ reduce loss accuracy introduce HP dynamically adjust mini batch extent DynHP impact convergence memory occupation computational effort datasets competitor validate performance task multi image classification public datasets detail perform handwritten digit recognition mnist dataset fashion classification fashion mnist dataset classification cifar dataset report salient statistic datasets employ sake comparison employ network architecture multi layer perceptron mlp resnet RN firstly employ mlp network task employ mnist fashion mnist RN employ cifar mlp network consists hidden layer output layer neuron multi classification moreover input layer compose neuron neuron encodes pixel input image RN architecture originally resnet adopt configuration gate mechanism precisely convolutional layer gate apply output channel convolutional filter gate filter dense layer gate neuron although exist configuration sake comparison propose SP datasets employ dataset  pixel channel training  mnist fashion mnist cifar performance HP DynHP prune dynamic prune SP perform hyper parameter tune network report combination parameter technique mlp network HP DynHP parameter aggressiveness prune prune threshold epoch RN architecture epoch SP configuration regard parameter perform sensitivity analysis impact prune RQ analysis dynamic prune RQ experimental capability HP DynHP memory SP extent prune affect model accuracy evaluation metric generalization capability network model misclassification error rate cardinality moreover understand impact technique memory occupation MO analyse memory occupation training MO compute integral epoch memory usage precisely MO compute sum non zero float matrix layer neural network plus mini batch data implementation adopt representation float computational effort perform inference model float operation FLOPs impact prune RQ analysis prune evaluate impact entire datasets prune prune aspect effectiveness prune accuracy evaluate effectiveness prune perform layer layer analysis reporting behaviour SP HP reduce active neuron report analysis mlp network mnist fashion mnist datasets behaviour achieve RN network cifar precisely plot refers mnist dataset plot refer fashion mnist plot active neuron layer network along training layer network respectively comparison report curve operating configuration accuracy later SP configuration batch mnist fashion mnist HP respectively moreover HP active neuron corresponds actual layer SP neuron actually prune network correspond memory occupation active neuron SP memory occupation training remains constant however understand mechanism SP HP plot SP average active neuron training datasets SP active neuron available hidden layer HL hidden layer HL initial training epoch increase phase SP explores configuration reactivate previously mute neuron SP  almost neuron HL decrease training epoch behaviour due SP initial prune aggressive neuron network worth behaviour greatly differs layer datasets refers mnist refers fashion mnist curve active neuron SP HP layer network input hidden layer increase epoch training conversely consideration HP active neuron monotonically decrease network layer specific dataset rate prune remove unnecessary neuron recognize specifically HP neuron remove cannot network moreover prune extent convergence rate monotonicity HP prune shed important difference SP memory training network HP allows reduce memory usage remove matrix experimentally ass analyse memory occupation SP HP training memory training epoch datasets HP memory SP solid mnist fashion mnist respectively overall memory saving training mnist fashion mnist respectively worth SP obtain network obtain HP training conversely HP progressively reduce network training progress interestingly SP virtual HP mnist network SP virtual HP comparable couple accuracy discus prof epoch epoch identify unnecessary neuron correctly possibly without harm quality training memory mbytes SP HP epoch training phase evaluate accuracy recall although HP mechanism remarkable memory forbids possibility rollback prune neuron regain network damage ablation hence analysis HP SP misclassification error impact HP quality model plot error function epoch mini batch datasets performance SP batch mini batch affect performance network SP interplay prune amount information mini batch batch information useable update SP obtains performance batch mnist fashion mnist correspond average misclassification error respectively anticipate configuration reference benchmark comparison interestingly SP HP loss function approach react differently batch minimum error achieve HP batch mnist fashion mnist respectively SP performs HP presence mini batch SP temporarily switch neuron mute neuron reactivate contribution important prediction HP switch neuron remove permanently network conversely HP prune instability sgd performs precise gradient update variance improve convergence HP generally SP nevertheless HP accurate HP average precise SP despite remarkable memory footprint mnist dataset misclassification error SP HP mini batch function epoch training fashion mnist dataset misclassification error SP HP mini batch function epoch training consideration resnet network cifar dataset report performance network misclassification error achieve SP batch epoch training performance corresponds configuration batch allows misclassification error report performance network misclassification error HP experimental batch epoch training HP performance SP achieve batch performance achieve HP exploit batch resnet HP achieves misclassification error increase loss achieve SP confirm loss accuracy HP counterbalance significant memory network cifar analysis dynamic prune RQ emerges mini batch crucial parameter tune however resource constrain scenario computational resource spar perform tune batch prohibitive impose network multiple obtain introduce mechanism dynamically mini batch training dynamically adjust batch improve effectiveness convergence HP moreover evaluate dynamic mechanism amount memory network comparison DynHP competitor SP HP mini batch fix priori tune performance adopt procedure described mechanism allows training DynHP exceed fix memory budget code monitor network training memory occupation network mini batch epoch fairly DynHP SP maximum available memory budget DynHP quantity memory configuration SP DynHP amount memory resource SP report configuration batch datasets misclassification error parenthesis difference misclassification error respect SP negative difference error indicates accuracy improvement positive indicates accuracy degradation model parenthesis percentage respect SP memory occupation denote tot memory usage training compute training memory curve parenthesis percentage memory saving SP training epoch metric comparison SP memory usage training SP remains fix reveals tune batch mnist dataset HP achieves memory model reduction memory training slightly improve accuracy performance SP conversely fashion mnist HP achieves model reduction training memory SP accuracy degradation respect SP limited additional confirmation neural network parametrized significantly reduce minimal accuracy performance impact memory occupation SP HP DynHP mnist dataset parenthesis report difference misclassification error achieve HP DynHP respect SP report model memory training achieve HP DynHP respect SP misclassification   memory usage mbytes gbytes SP HP DynHP DynHP DynHP DynHP DynHP accuracy performance impact memory occupation SP HP  fashion mnist dataset parenthesis report difference misclassification error achieve HP DynHP respect SP report model memory training achieve HP DynHP respect SP misclassification   memory usage gbytes SP HP DynHP DynHP DynHP DynHP DynHP DynHP DynHP DynHP DynHP DynHP DynHP focus DynHP mini batch adapt accord evolution training mnist dataset increase accuracy respect SP however paid memory respect obtain HP SP moreover configuration memory training reduction accuracy detail misclassification error network incrementing relative error non linear trend decrease increase minimum DynHP accurate HP SP respectively regard relation memory saving positive correlation memory compression model model training memory occupation tot memory usage DynHP SP interestingly compression accuracy obtain sensibly compress model fashion mnist dataset behaviour mnist specifically obtain accuracy improvement respect HP memory usage particularly training significant memory slight loss accuracy granular however trend respect mnist DynHP performs equally SP accuracy improvement model SP however memory occupation training tot memory usage equivalent SP behaviour relies DynHP counterbalance loss network expressiveness increase mini batch reveal accept degradation accuracy SP DynHP highly compress model memory occupation training regard resnet cifar dataset behaviour feature mnist fashion mnist specifically DynHP slightly sacrifice accuracy respect HP significantly improve memory saving granular offs misclassification error memory usage performance DynHP obtain misclassification error achieve error error achieve SP configuration yield additional loss performance DynHP HP achieves misclassification error however HP achieves misclassification error reduction model respect SP DynHP achieves reduction model reduction memory usage misclassification error achieve DynHP increase loss respect SP however configuration DynHP reduces model memory usage reduction SP summarize conclude HP DynHP drastically memory usage training training respect SP paid sometimes loss reduction percentage DynHP advantage HP due possibility dynamically adjust mini batch reduce memory usage loss accuracy per namely mnist fashion mnist improves accuracy reduce memory usage finally memory training DynHP SP model accuracy performance impact memory occupation SP HP DynHP cifar dataset parenthesis report difference misclassification error achieve HP DynHP respect SP report model memory training achieve HP DynHP respect SP misclassification   memory usage mbytes SP HP DynHP DynHP DynHP DynHP DynHP DynHP DynHP DynHP DynHP DynHP convergence accuracy deeper analysis evaluate convergence accuracy SP HP DynHP performance SP HP DynHP sake clarity comparison SP HP report plot setting performance achieve employ batch respectively DynHP report parameter responsible dynamic increment batch accord strategy explain training batch training convergence model accuracy induce convergence medium quality comparable SP HP interestingly DynHP converges quality outperform SP HP average misclassification error fashion mnist concerned converge DynHP SP HP however DynHP configuration perform HP SP report analysis resnet network cifar dataset optimal performance DynHP achieve specific configuration DynHP outperform HP achieves loss respect SP convergence concerned resnet convergence DynHP HP SP trend regular specific insight potential benefit obtainable combine dynamic mini batch prune message tune growth rate mini batch counterbalance loss expressive network due irreversible prune converge obtain SP HP clearly complexity network compress increase RN prune training becomes challenge report accuracy finally finding convergence promising investigation theoretical derivation convergence performance function algorithm parameter useful however due complexity derivation future memory occupation dynamic batch behaviour memory occupation entire configuration initial batch increase accord relative variance gradient mini batch function training epoch observation regard growth mini batch convergent datasets almost curve increase training clearly configuration mini batch stabilizes worth memory occupation curve stabilizes induced mini batch exceed maximum memory budget happens curve stabilizes relative variance gradient compute becomes negligible qualitatively behaviour cifar observation due definition steeper growth batch steeper growth compatible convergence performance mini batch convergence however priori notion steepness increment update finally plot memory occupation SP HP DynHP training epoch SP report actual memory occupation constant solid clearly understand prune behaviour without influence dynamic batch HP curve memory reduction due prune procedure prune stepwise prune session plateau network recovers loss expressive another prune session another plateau regard DynHP memory SP limit increment mini batch moreover appreciate dynamic mini batch combine HP procedure specifically distinguish mechanism dominates curve trait mini batch dynamic predominant prune memory occupation increase afterwards HP mechanism sheer memory occupation mnist around epoch DynHP fashion mnist epoch modest constant prune training happens varies specific setting regard resnet cifar slightly behaviour parameter affect growth rate memory sheer increment memory training epoch differently mnist fashion mnist mini batch dominates memory occupation nonetheless memory induced mini batch prune introduce DynHP significant memory along training computational effort analysis computational effort perform inference model specifically compute model dataset flop define float operation execute model flop compute dense layer flop active neuron layer respectively convolutional layer convolutional filter layer flop filter dataset report flop perform inference model analysis configuration model maximum accuracy flop dataset model perform configuration report percentage flop without prune  mnist mnist flop sum epoch dataset model perform configuration report percentage flop without prune  mnist mnist regard DynHP mnist fashion mnist cifar respectively approach succeed model actively reduce flop mnist flop DynHP prune model fashion mnist DynHP allows reduce flop model cifar significant DynHP significantly outperforms counterpart reduce flop model report sum flop training epoch benefit employ DynHP training apparent datasets achieve reduction flop respect flop prune model confirm effectiveness DynHP effective model incrementally reduce memory occupation overall computational effort conclusion investigate compress nns fix potentially memory budget fog device propose DynHP resource efficient NN technique achieves performance comparable conventional neural network training algorithm enable significant network compression DynHP prune incrementally permanently network training progress identify neuron contribute marginally model accuracy memory training effectively reuse dynamic batch technique minimizes accuracy degradation prune strategy adaptively adjust data neural network improve convergence effectiveness careful combination DynHP ingredient prune dynamic batch allows quality NN model respect memory footprint constraint introduce resource constrain fog device extensive conduct neural network architecture mlp resnet public datasets mnist  cifar DynHP compress target neural network without significant performance additional misclassification error competitor reduce overall memory occupation training future direction towards effective efficient pervasive AI future intend perform formal convergence analysis account joint combination dynamic characterize propose furthermore intend investigate simultaneous training compression neural network scenario involve distribute training network investigate application quantization technique affect efficiency effectiveness model