mobile data traffic grows rapidly video data proportion backhaul link pressure conventional centralize architecture satisfy user demand reduce user response latency backhaul stress medium content proactively network content cache model video content collaborative cache strategy cooperative environment propose strategy algorithm cluster server latency cache gain cache content server cluster collaborative cache domain analyze establish content cache marginal gain calculate analyze latency cache gain finally content cache propose marginal gain content cache algorithm experimental effectiveness propose algorithm previous keywords cooperative environment video content collaborative cache introduction rapid development smart mobile device traffic grown exponentially global mobile data traffic become video instance duration video content uploaded youtube per approximately amount data burden backhaul link traditional centralize network model cannot satisfy sensitive demand therefore academic community propose cooperative environment architecture architecture user obtain content nearby node relieve backhaul pressure reduce response latency scheme cache video content network video content cache cooperative environment flexible efficient mode content cache node content cache non cooperative node content cached node compute storage load node increase thereby reduce user content access efficiency cache scheme non cooperative node environment node cannot access cache node however cooperative environment video node cache node cooperative environment consists multiple server user remote data environment user server distribute location remote data mainly responsible cache video content server user request video content server nearby accord data access requirement due limited cache capacity server cannot cache video content therefore server cache video content request user server obtain video content collaboration server server request obtains video content server video content user user directly request video content remote data collaboration server greatly reduce content access latency however content request user cached server user request remote server user video content video content cache cooperative environment addition user mobile cooperative environment allows user geographic location server location obtain compute storage resource therefore due user mobility user access multiple server user user respectively service server server therefore user user server server respectively user user service server server respectively user user server server respectively overcome defect backhaul link traditional cellular cache video content network proven improve user mobile network heterogeneous environment content cache scheme compute storage capability node limited cache node exist cache strategy content cache strategy cooperation traditional ignore cooperative architecture content cache scheme cooperative architecture utilizes nearby node decrease latency user advantage powerful compute ability server majority content cache strategy cooperative environment focus optimize latency without factor outcome cannot apply actual scenario video content collaborative cache strategy cooperative environment propose strategy algorithm cluster server gain latency cache cache content server cluster cooperative cache domain calculate formulate content cache marginal gain content cache calculate analyze gain latency cache finally content cache propose marginal gain content cache algorithm addition server cache capacity insufficient content cached passive cache replacement initiate server delete content marginal gain cache content contribution reduce latency collaborative environment content collaborative cache strategy propose strategy server multiple cluster server cluster cooperative cache domain user content collaborative manner reduce content access latency latency gain cached content cluster collaborative cache domain cache gain strategy reduces content cache content access latency maximization delay gain cache gain content cache content collaborative cache algorithm propose algorithm calculates marginal gain analyze latency gain cache gain cache content accord marginal gain achieve latency cache youtube video dataset experimental verification execute environment algorithm benchmark content latency content cache cache ratio backhaul traffic load related review overview model cooperative cache strategy correspond algorithm furthermore performance evaluation conclusion future described related cooperative environment traditional cache scheme centralize processing network architecture wireless network characteristic dynamic traffic load interference academia propose various network architecture collaboratively cooperative cache strategy propose reduces content provision software define hyper cellular network compute task cache scheme propose chen jointly optimize computation cache communication propose efficient fault tolerant replica management scheme cooperative environment deadline budget constraint overcome limited resource cloudlet user  propose layer model consist mobile compute server multiple cloudlets infrastructure multiple domain cloudlet controller domain responsible maintain communication node within domain traditional centralize network architecture scholar cache advantage reduce latency consumption bandwidth  optimal cache policy popularity availability content spatial distribution however exist partial cache strategy cooperation cooperative cache strategy application scalable video cod technology inter cooperative video cache schedule improve user cache capacity qoe  propose collaborative cache processing approach minimize network usage content delivery network approach mention improve overall performance extent however powerful compute capacity storage capacity node besides exist rarely considers video content cache cooperative environment trend video application development promotion cooperative environment adapt optimize traditional content cache strategy content cache video content compute latency service video application compute domain limitation interactive multi video virtual synthesis online video content placement strategy reduce average user graph partition maximize transmission rate tier hierarchical cache framework centralize node distribute traffic node optimal storage allocation factor analyze propose non cooperative cooperative cache strategy liu propose centralize cache strategy sub module optimization theory transport aware distribute cache placement strategy belief propagation tran target video transcoding propose online video schedule algorithm formulate cooperative cache processing integer linear program research focus video cache propose cache framework algorithm cooperative environment propose cache strategy user mobility content popularity cache strategy content cache operation perform accord user mobility content popularity propose cache prefetching strategy selects content prefetched bayesian network selects node load content propose joint cache strategy strategy server collaborate cache chunk video content efficiently backhaul storage resource reduce content delivery delay minimize network wang propose multi agent reinforcement cache framework adaptively policy conjunction intelligent cache propose framework cache layer video layer video node layer cache service video quality demand user service performance mobile network depends amount location content cached node  proposes content placement strategy gain content diversity cooperative gain ren hierarchical collaborative content cache strategy mobile network reduce access latency improve efficiency gigabit passive optical network architecture sdn efficiently implement cooperative video cache scenario cooperative video cache content placement  propose centralize distribute collaboration scheme  propose probability heuristic cache strategy relates probability content popularity placement gain propose dynamic optimize replica placement strategy goal strategy reduce network utilization maintain load balance node video content proactive cache scheme propose  scheme considers reduce hop cache node user content cache strategy however maintain balance latency cooperative environment besides cache content node cache capacity insufficient propose content cache strategy combine passive cache replacement effectively delete content marginal gain collaborative cache strategy cooperative environment overview cooperative environment server user mobile device remote server server local cache capacity cache content reduce delay content load backhaul link user request content access local server server closest user local server cache content local server request content server collaborative cache domain user content cached collaborative cache domain content request local server remote server user assume remote server storage capacity content therefore remote server proactively cache content server peak network cache operation trigger server accord request reactive cache video content cache cooperative environment commonly adopt cache video file split video file multiple cache overview content cache model cooperative environment furthermore content collaborative cache strategy propose remote server algorithm cluster server cooperative environment cluster server local server preferentially request content user service cluster content cached cluster content request local server server collaborative cache domain server latency gain content cache gain cache content server calculate content cache establish accord latency gain content cache gain content cache marginal gain calculate analyze latency gain content cache gain content cache content cache algorithm latency propose notation notation  server  server server cluster  server cluster cluster cluster server location video file file file file cached server file cached server file maximum video file cache  video file data transfer rate user server  bandwidth proportion bandwidth allocate server data transfer rate server server enc latency user directly server latency user server enc cluster latency server obtain remote mathematical model formulation server cluster collaboration environment due randomness geographical distribution server network distance server although data transmission server faster server remote data however due network congestion data node failure latency server server data propose content cache strategy algorithm cluster server cluster information cluster server belong server data user request content local server local server cache content local server content server cluster mainly distance server cluster relatively data transmission latency content cached cluster local server calculates latency request content collaborative cache domain remote server selects shortest latency obtain content mainly server outside cluster away local server data transmission latency therefore content mode judging transmission latency reduce data transmission latency denote server server cluster respectively cluster cluster denote contains server denote server location location server denote denotes abscissa ordinate server respectively addition mention cluster cluster server server server server ES initial cluster server satisfies distance cluster shortest cluster cluster cluster average coordinate server calculate satisfies server cluster cluster remains unchanged model latency content cache gain cooperative environment article remote server server video file multiple mobile user  server video file respectively video file denote file file denotes file file cached server model denotes relationship cached otherwise denotes maximum cache capacity cached cache cache satisfies constraint therefore content cache gain server model user access server cached user directly latency user directly server data transfer rate user model bandwidth denotes user access denotes proportion bandwidth allocate satisfies denotes ratio signal inference plus regard random variable model bandwidth allocation ratio affect data transmission rate affect latency therefore reasonable allocation bandwidth server appropriate reduce delay content cached request cluster server enc cluster cache server server enc user data transfer rate enc model bandwidth server enc ratio signal inference plus enc therefore latency user enc cluster model cooperative environment server distribute geographical location due network congestion data transmission server collaborative cache domain server remote server enc within cluster cache server request server within collaborative cache domain outside cluster server ena collaborative cache domain cache server calculate latency obtain server ena remote server latency obtain assume latency server obtain remote latency user obtain remote server express latency gain cache server user access request model denotes popularity server denotes access access cached server user server cluster latency gain assume user access server enc cluster request latency gain cache user access enc request model average latency gain cache cluster model server cluster similarly latency gain cache user access ena collaborative cache domain request model denotes latency user server ena collaborative cache domain average latency gain cache collaborative cache domain model server collaborative cache domain summary latency gain cache cooperative environment model formulation cooperative environment cache content latency affect cache content server reduces content latency user cache content server increase cache utilization rate thereby increase content cache therefore optimization goal propose content cache strategy tends maintain balance content latency content cache optimal latency content cache model server cache capacity constraint denotes cached server cannot exceed maximum cache capacity server bandwidth allocation constraint denotes bandwidth allocation ratio server denotes normalize content latency gain model denotes maximum gain content latency latency gain user content server denotes minimum gain content latency latency gain user content remote server content latency gain denotes normalize content cache gain denotes minimum gain content cache content cache gain cache server cache content denotes maximum gain content cache data cache cache server however content cache discrete bandwidth allocation continuous therefore mixed integer program ignores relationship bandwidth allocation content cache optimal bandwidth allocation ratio obtain lagrange multiplier content cache algorithm propose obtain latency threshold content cached server accord threshold propose content cache strategy mainly compose server cluster content cache server cluster algorithm server cooperative environment cluster latency user local server cluster collaborative cache domain remote server respectively calculate content cache latency gain content cache gain cache calculate content cache marginal gain calculate accord obtain latency gain content cache gain finally latency threshold calculate analyze content cache marginal gain content cache server perform accord threshold addition reduce load data processing data transmission server cluster content cache perform peak content request content cache algorithm accord content cache gain cooperative environment model cached server content latency gain content cache accordingly increase decrease content cache gain accordingly model respectively latency gain content cache gain cached cache file server marginal gain content cache model another file continuously cached server content cache gain marginal gain ensure content latency reduce cache satisfied cache reduce content premise content cache therefore propose strategy maximum file cached server accord latency threshold remote server cache server accord threshold maintain balance latency content cache algorithm latency threshold content cache algorithm input server file file output latency threshold transmission ratio server calculate file cached server initialize latency threshold initialize null file content cache gain cache calculate sort algorithm sort file content cache gain server ES file met file cached increase return latency threshold content cache algorithm described algorithm algorithm obtain initialize transmission ratio calculate file cached server initialize latency threshold initialize null content cache gain cache file calculate sort algorithm sort file content cache gain server file evaluate met threshold file increase otherwise calculation threshold file calculation threshold file latency threshold return complexity analysis algorithm complexity server complexity FS file file respectively complexity sort FS complexity FS therefore complexity algorithm FS flowchart latency threshold content cache algorithm flowchart algorithm threshold obtain algorithm server cache file server marginal gain threshold cache cache server perform marginal gain content replacement delete cache marginal gain content replacement marginal gain cached calculate marginal gain cached marginal gain cached server marginal gain server delete cache algorithm server cluster algorithm input server cluster cluster coordinate server output server randomly initial cluster CH CH CH CH CH CH initialize cluster iteration initialize cluster shortest distance server HH HH initialize distance server cluster server ES cluster CH HH cluster cluster cluster initialize sum coordinate server cluster average coordinate server cluster calculate accord cluster initialize minimum distance server cluster average coordinate ZZ ZZ initialize cluster ZZ ZZ cluster update return server cluster algorithm algorithm server cluster algorithm described algorithm server cluster cluster server coordinate obtain initialize server randomly initial cluster CH CH initialize cluster iteration distance server cluster shortest distance server cluster cluster correspond average coordinate server cluster calculate server cluster closest location cluster cluster CH CH met algorithm execute cluster return complexity analysis algorithm algorithm complexity server cluster respectively algorithm complexity average server cluster assume loop iteration complexity server cluster algorithm flowchart server cluster algorithm flowchart server cluster algorithm flowchart algorithm setting experimental environment propose content cache algorithm specific hardware platform remote server mobile device hardware platform remote data CS server ES server deployed campus wuhan technology  campus  campus  campus ES deployed  campus ES deployed  campus ES deployed  campus  campus away  campus  campus distance  campus  campus mobile device mobile phone server wireless access mobile device worker worker server worker mobile device within service server mobile device server worker service server another server mobile device disconnect former establish connection latter detailed information hardware software openstack platform deployed server compute storage network related service meanwhile video file distribute remote server experimental environment optimize content cache strategy dataset obtain video dataset youtube publish google youtube video URLs duration video exceed furthermore video frame duration video video youtube video source file evaluation metric content cache content latency CDL cache ratio chr backhaul traffic load btl evaluate performance propose algorithm chr model obtain server request CDL average latency user content server normalize cached server model denotes maximum content cache cache server cache content denotes minimum content cache content cached server content cache btl traffic remote server backhaul link model request remote server denotes benchmark algorithm evaluate performance propose algorithm algorithm benchmark algorithm mobility prediction content cache  algorithm markov chain predict user mobility user content request algorithm cache content node user likely future cache optimization sco algorithm video content multiple cache algorithm considers popularity video content content transmission delay local content cooperative cache algorithm considers influence capacity cooperative link backhaul link node goal algorithm minimize content delay transmission ratio maximization HRM algorithm analyzes tradeoff transmission diversity content diversity algorithm research cache assignment optimization maximize performance cache service minimization CM algorithm optimize cache model minimize primary target maximize ratio primary goal algorithm minimize comparison analysis chr CDL btl propose content cache algorithm evaluate relative cache capacity rcc specifically chr increase rcc increase whereas CDL btl decrease HRM algorithm chr achieve CDL HRM considers diversity cached content bandwidth allocation however propose algorithm reduces CDL HRM algorithm CM algorithm propose algorithm increase chr reduces CDL CM algorithm however CM algorithm content cache therefore btl CM algorithm propose algorithm CM algorithm btl reduce impact cache chr performance propose content cache algorithm evaluate relative cache capacity rcc server  user NU chr increase increase rcc  mainly rcc content server cache allows content server thereby improve chr  server cache content thereby improve chr chr decrease increase NU mainly increase NU increase user content request due limited cache capacity server cannot increase user content request thereby reduce chr addition sco  propose content cache algorithm increase chr average propose content cache algorithm account content popularity construct latency gain model allows server cache content popularity thereby improve chr performance evaluation cache ratio performance evaluation content latency CDL performance propose content cache algorithm evaluate relative cache capacity rcc file request arrival rate  server  user NU CDL increase increase rcc decrease increase  mainly increase rcc allows content cached server user obtain request content server thereby decrease CDL  data processing load server increase CDL CDL decrease increase rcc increase increase  mainly increase  allows content cached server thereby reduce CDL increase NU increase content request increase server load thereby increase CDL sco  propose cache cache algorithm decrease CDL algorithm local server cache request content local server request content cluster cluster cache content local server collaboration cache domain content exists collaboration cache domain local server determines delay content collaboration cache domain server selects shortest delay obtain content content user collaboration node effectively reduce delay content adjacent node collaborate local local dose cache request content content directly request remote although reduces latency extent node collaboration ignores impact node collaboration latency performance collaboration cache domain therefore algorithm latency content cache algorithm evaluate relative cache capacity rcc file request arrival rate  increase increase rcc mainly increase rcc server cache content increase content cache thereby increase increase increase  mainly increase user content request content cache algorithm content server thereby increase sco  algorithm decrease mainly algorithm optimizes content cache performance algorithm comparison algorithm content cache performance evaluation content cache backhaul traffic load btl performance evaluate relative cache capacity rcc file request arrival rate  btl decrease increase rcc mainly increase rcc allows server cache content increase user content server thereby reduce btl btl increase increase rcc due limited cache capacity server cannot cache file increase user file request user unable content server thereby increase btl sco  algorithm reduces btl average performance evaluation backhaul traffic load impact marginal gain passive cache replacement performance propose content cache algorithm evaluate random cache replacement content cache algorithm RCCA lru content cache algorithm  comparison algorithm marginal gain cache replacement content cache algorithm  random lru propose content cache algorithm integrate random cache replacement lru cache replacement respectively MG propose content cache integrates marginal gain passive cache replacement cache capacity insufficient random cache replacement deletes random content cache cache content lru cache replacement deletes recently content impact marginal gain passive cache replacement performance propose content cache algorithm content latency CDL content cache algorithm evaluate relative cache capacity rcc file request arrival rate  specifically CDL decrease increase increase rcc  respectively RCCA   reduces CDL  considers marginal gain delete content marginal gain construct content latency content cache  selects content marginal gain server delete delete content impact latency RCCA  impact delete content delay perform cache replacement therefore CDL  RCCA  conclusion future optimization content cache cooperative environment content collaborative cache strategy comprehensively considers cache gain latency gain cache content server formulates content cache maximizes latency gain cache gain propose content cooperative cache algorithm calculates marginal gain content cache analyze latency gain cache gain cache content accord marginal gain achieve latency cache youtube video dataset extensive experimental validation demonstrate content cache strategy superior traditional cache management strategy cache ratio content latency content cache backhaul traffic load furthermore model extend future factor define content popularity analyze popularity evolution