choice feature extract individual aggregate observation extremely critical factor network traffic classification approach machine activity usually designer classification scheme strongly related definitely characterizes approach implementation strategy performance aim mining expressive meaningful discriminate feature without intervention purpose novel autoencoder neural network architecture propose multiple autoencoders embed convolutional recurrent neural network elicit relevant knowledge relation exist feature spatial feature evolution temporal feature knowledge consist immediately evident hidden representative traffic dynamic successfully exploit machine classifier network combination analyze theoretical perspective specific performance evaluation network traffic dataset traffic classifier obtain stack autoencoder fully neural network achieves improvement average accuracy machine approach pure convolutional recurrent stack neural network pure network maintain accuracy presence unbalanced training datasets previous keywords network traffic classification feature extraction memory recurrent neural network convolutional neural network ConvLSTM introduction network traffic classification NTC topic extremely internet service provider ISPs network operator identify typology data network mapping generate application knowledge important monitoring network security network application behavior perform traffic engineering quality service service agreement calibration improve knowledge user traffic demand police prioritization mechanism perform data collection marketing accounting billing capacity planning purpose reliably classify network traffic crucial automation network operation prediction traffic demand matrix development realistic traffic model detection anomalous behavior trigger autonomous reaction become core component network management framework action countermeasure associate traffic monitoring filter unwanted  interception session perform rerouting resource reallocation presence specific overload  activity  network operator service demonstrate NTC ability bandwidth qos guarantee traffic application chat bulk ftp http data transfer peer peer PP file identification eventually specific fraud copyright infringement malicious activity associate properly obfuscate traffic precisely traffic network infrastructure avoid privacy scalability deployment excessively granular user ISPs oblige adopt data driven approach extract relevant characteristic feature traffic feature associate individual generate application NTC scheme association usually perform machine ML empower technology artificial intelligence AI arena approach proven precise traditional packet inspection dpi immune obfuscation technique however choice feature extract individual aggregate traffic observation extremely critical factor ML NTC approach indeed  feature representative successfully discriminate traffic extract assumes paramount importance NTC approach implementation operating strategy performance considerable engineering effort task essentially bound activity designer NTC scheme due complexity network application challenge activity networking research aim mining expressive meaningful discriminate feature without intervention accordingly investigate potentiality autoencoder neural network extract meaningful feature NTC elementary statistical feature refer feature obtain sample functionally aggregate basis traditional header obtain processing multiple consecutive packet within sample leverage novel neural network architecture involve convolutional recurrent neural network derive feature refer spatial temporal feature express relevant knowledge exist feature spatial feature feature temporal feature feature significantly improve ML classification capture discriminate traffic dynamic associate temporal correlation dependency recurrence phenomenon mixed spatial temporal trend easy perceive glance detail typical internet traffic associate service application exhibit significant variance evolution essentially due burstiness observable multiple specific characteristic involve application specific cluster feature presence periodic structure characterize traffic emerge observation strictly bound superposition distinct independent almost hidden dynamic related combination feature eventually analyze historical trend properly accordingly propose network architecture autoencoder stack fully neural network encode function autoencoder implement combination convolutional recurrent network aforementioned spatial temporal feature respectively extract feature without engineering automatically feature elicit relevant knowledge evident correlation furthermore derive feature powerful expressive due presence layer encode decode function autoencoder combination convolutional recurrent neural network finally fully neural network classify traffic feature extract addition detailed mathematical description neural network architecture aforementioned purpose clarify understand network component affect feature extraction ultimately contribution summarize neural network classifier NTC spatial temporal feature introduce mining correlation component feature vector vector respectively spatial temporal feature combine convolutional recurrent neural network autoencoder formal description overall architecture motivate approach theoretical improve understand network component affect feature extraction remain organize related attention network brief description typology neural network employ proposal description proposal detailed mathematical formalization propose neural network extract spatial temporal feature network traffic described devote discussion comparison model finally conclusion future related decade academic researcher propose developed NTC strategy feature technology however NTC packet inspection dpi statistic driven classification approach knowledge information extract transport layer header identify traffic although simplest NTC nowadays longer reliable recent application wrap traffic associate protocol http dns etc dpi relies payload inspection univocally characterize traffic although achieve performance traffic typology http ftp smtp implies significant processing effort usually dedicate hardware fails identify PP multimedia encrypt application finally statistic driven approach rely information extract packet header packet timestamp tcp flag etc usually information within fix temporal extract meaningful powerful statistical information inter arrival packet rate rate forth feature input ML algorithm perform NTC nevertheless ML extremely promising option extensively explore research industrial application achieve successful due amount available NTC datasets feature extraction evaluation metric impossible perform comparison report similarity proposal useful understand potentiality approach  notable literature detailed survey  introduction performance evaluation workbench summary obtain machine lopez author propose network traffic classifier combination convolutional cnn recurrent neural rnn network couple convolutional fully network cnn NN recurrent fully network lstm NN evaluate sequence cnn lstm NN autoencoder cnn lstm NN configuration accuracy evaluate  dataset accuracy achieve cnn sae sparse auto encoder configuration respectively ISCX VPN  traffic dataset cnn configuration model dimensional convolutional network cnn layer pool flatten layer fully network softmax classifier sae architecture fully layer neuron respectively cnn configuration softmax classifier terminates network network former layer cnn configuration extract alexnet layer convolutional remain fully neural network softmax classifier autoencoder hidden layer reduce feature dimensionality random classifier accuracy achieve cnn configuration evaluate dataset  novel feature interval propose  neural autoencoder softmax classifier obtain accuracy specific dataset chosen performance assessment report configuration combination cnn recurrent network none extract combination spatial temporal feature network traffic perform NTC specific goal preliminary approach typology neural network powerful network traffic classifier suitable task characterize isp activity sake clarity theoretical background employ network shortly convolutional neural network cnns efficient neural network image processing recognition processing nlp autonomous cnns become increasingly mainly due important factor cnns eliminate manual extraction feature directly cnn cnns recognition activity exist network exploit cnns neural network input output layer strength presence multiple hidden layer specific mathematical operation relevant feature feature feature layer sophisticated abstract feature feature layer exploit feature extract previous layer layer typically operation sequence convolution operation cnn applies input data convolutional filter responsible detect specific typology feature data filter slide involve convolution operation artificial neuron rectify linear activation function relu function output zero negative input input otherwise function apply output convolutional neuron activation function demonstrate milestone unlike widely sigmoid hyperbolic tangent activation function overcome vanish gradient obtain performance pool sample function namely reduce output layer reduce connection layer feature location insensitive consequently pool drastically reduce enables cnn recognize feature regardless location local translation invariance max pool average pool pool former emphasizes active feature whereas latter emphasizes average involve feature classification implement cnn accomplish fully neural network involve output vector dimension predict softmax function generally classification probability cnns extract communication traffic data refer spatial feature recurrent neural network neural network return response input although aspect desirable others limitation indeed situation network response input purpose network memory accomplish network operation recurrent neural network rnn model neural network widely goal involves feedback loop heavily affect behavior network input indeed output depends network simplest elman decision affected decision achieve input elman rnn source namely previous input context consequently rnns detect correlation dependency data training rnn perform variant backpropagation BP algorithm backpropagation BPTT algorithm application BPTT implies unroll rnn network sequence neural network network multiple hidden layer network BP algorithm affected mention vanish gradient overcome issue memory lstm network introduce memory lstm presence internal allows forget information update internal activation layer gate implement sigmoid neural network layer wise operation gate devote specific goal described forget gate determines information forgotten input gate determines information actual input implement sum output layer namely sigmoid tanh layer respectively former information update whereas latter chosen information actual output gate determines data output actual autoencoders autoencoders aes unsupervised artificial neural network aim generate data sequence namely encode decode respectively depict former implement encoder devote compress input latent variable whereas latter implement decoder involve reconstruct input information latent variable purpose encoder decoder implement symmetric neural network goal AE concerned input desire output automatically capture relevant representative feature input data aes employ latent data dimension similarly technique data compression via dimensionality reduction principal component analysis pca linear discriminant analysis lda discriminant function analysis dfa distribute stochastic embed sne difference aes technique aes input non linear combination derive feature  aes capability complex input data dimension latent neural network within encoder decoder determines functionality AE basically aes namely denoising AE sparse AE AE contractive AE  AE convolutional AE variational AE sparse AE dimension latent layer extract relevant feature data  detail aes theory application stack neural network stack neural network SNN configuration combine pre neural network concatenate intermediate layer correspond feature obtain complex feature goal improve classification accuracy training advantage transfer recently SNNs become popular framework aes stack autoencoders  proven effective presence data StAE implement sequence hidden layer pre aes fully network consequently training SNNs perform training involve AE separately unsupervised algorithm tune network supervise approach backpropagation algorithm approach avoid aforementioned vanish gradient dropout although neural network proven effective numerous application activity remains challenge feature data resultant model adapts input data generalize overfitting prevent issue  technique commonly realize zero output neuron prevent involve training propose methodology detail propose approach implement powerful network traffic classifier capable exploit spatial temporal correlation dependency traffic series distil feature accomplish namely representative feature traffic perform classification stack neural network depict former perform AE encode decode function implement neural network  cnn lstm cnn lstm ConvLSTM whereas implement classification latent layer AE input fully neural network combine capability aes compact feature classification ability neural network goal elicit knowledge observation specific interval historical traffic behavior evolves widely network traffic observable specific tap composition individual associate application define traffic characterize unique directional combination specific source IP address destination IP address source destination transport protocol tcp udp sctp etc tcp identify handshake involve udp terminate properly chosen alive interval usually sample interval useful feature extract typical feature packet byte rate inter arrival etc remainder refer feature feature usage feature classifier neither detect behavior evidence relation component image KB image perspective propose approach address cnn retrieve relation feature refer feature spatial feature indeed although cnns image processing feature mining automatically cnn extract location invariant image recently cnns involve multiple research NTC image pixel correlation extract network similarly useful correlation extract entry feature vector hence spatial feature besides lstm behavior feature temporal feature sparse AE salient compact feature data depict feature fed AE cnn lstm network devote extract spatial temporal information brief mathematical formulation propose NTC architecture AE incrementally extend discussion network combination pure AE configuration reference AE suppose AE layer input hidden output respectively feature input input AE pure AE described earlier AE seek reconstruct input mapping hidden representation mapped output express matrix bias vector encoder decoder respectively activation function decoder hidden neuron AE remark pure AE activation function encoder AE classic neural network minimize loss function error constraint regularization function AE specific capability mining compact representative feature data sparse aes SAEs usually employ characterize simultaneously active hidden node sparsity improves classification performance SAEs regularize penalty loss function regularization express regularization sparsity achieve regularization lasso regression absolute shrinkage selection operator absolute network penalty loss function ensures zero hidden neuron weakly activate regularization kullback leibler KL divergence penalty SAEs express divergence exist average activation hidden neuron sparsity parameter define hidden layer training output hidden neuron extract remark sparsity constraint hidden layer activate neuron theory response specific feature input data suppose input data sae constrain component input data activate hidden neuron encoder denominator extract feature sae feature data hidden neuron illustrate combine sae network extract complex meaningful feature data cnn sae aim configuration feature spatial feature express relevant relation feature reference encoder decoder sae cnn configuration marked network sequence input feature cnn sae fully NN output cnn input filter respectively convolution operation cnn input filter define component filter input explicitly define dimension stride respectively shift filter input addition pad zero around border pad cnn output without compromise convolution assume dimensional array matrix abuse notation assert substitute yield accord analogy extract feature feature express complex knowledge linear combination lstm sae configuration marked lstms sae sequence sequence sts architecture architecture encoder function sequence feature series multivariate input fix vector feature latent convert input sequence decoder function configuration distance dependency within sequence feature express compact powerful representation reference wise addition respectively matrix linear transformation memory output input sequence feature timestamps sts encoder recursive update equation synthesize output vector predetermine dimension express non linear multi variable vector function gathering lstm timestamps component substitute yield analogy indicates extract feature non linear combination input vector feature feature express compact representation behavior feature feature feature approximate function taylor polynomial vector feature timestamp reference component derive component equation application taylor theorem  expansion yield linear approximation suppose initial bias lstm null remark multi variable scalar function gradient becomes dimensional vector yield substitute yield express inner summation express modify version timestamps feature temporal feature express complex relation respect outer summation entire input unique cnn lstm sae configuration marked spatial feature extract behavior evaluate temporal feature extract lstm dimensional input vector reshaped dimensional array matrix fed cnn matrix output finally cnn output combine lstm dimensional vector inverse sequence decoder reference accord component cnn output timestamp accord component lstm output express relation feature spatial feature derive cnn reference similarly indicates extract feature non linear combination spatial feature obtain cnn stage feature previous taylor expansion approximate remark composite function vector cnn output null initial bias lstm accord substitute yield express inner summation presence spatial feature summation feature addition outer summation spatial feature unique stack cnn lstm sae configuration SAEs stack encoder function described hidden layer SAEs sequence separately sae cnn sae configuration feature input configuration marked whereas lstm sae configuration output sae encoder configuration corresponds marked training data output cnn sae configuration previously feature accord mathematical discussion previous reference component encoder output sae timestamp component output convolution operation timestamp accord component output lstm iteration finally component encoder output sae timestamp equation feature combination encoder sae composite function obtain taylor expansion compute taylor expansion substitute suppose sigmoid activation function vector input timestamp assume taylor expansion yield yield initial taylor expansion substitute yield finally substitute rewrite reference temporal feature timestamp feature combine spatial feature summation SAEs respectively feature additional freedom respect temporal feature indeed derive training training respectively addition dependent timestamp dependence timestamp introduce presence partial derivative respect depends ultimately configuration capable behavior spatial feature ConvLSTM sae configuration marked ConvLSTM convolutional lstm lstm recurrent neural network matrix multiplication input transition replace convolution operation ensures spatial information native equation become convolution operation accord output ConvLSTM sae timestamps non linear multi variable vector function gathering ConvLSTM timesteps accord equation devote demonstrate effectiveness configuration extract spatial temporal feature classify network traffic purpose conduct recent dataset network traffic trace generate workstation involve protocol application dataset feature traffic data  reference dataset NTC related  trace campus network  italy reliable truth traffic capture tcpdump dual xeon linux internet dedicate uplink pcap file traffic generate workstation GT client daemon application protocol generate dataset around GB data correspond around tcp udp traffic traffic generate application protocol web browsing http HTTPS mail pop imap smtp ssl chat skype peer peer BitTorrent  protocol ftp ssh msn depict traffic strongly unbalanced indeed varies considerably category  utility extract pcap file truth  employ associate extract correspond application label grouped category report  dataset composition  web mail PP BitTorrent PP  skype tcp skype udp mail category generate application mail pop smtp imap ssl http HTTPS chat category message application skype messenger browsing category generate web browser safari firefox http HTTPS PP category identify generate PP application BitTorrent   transmission afterward  framework employ derive statistical feature purpose sample temporal sample frequency extract feature refer feature  exchange packet  amount data exchange express byte  packet per bitrate rate exchange packet within  inter arrival consecutive packet  average packet exchange resultant feature dataset csv file category dataset consists dimensional vector feature subdivide mail chat browsing PP category respectively machine perform task described amazon web service aws educate xlarge linux instance equip vcpu intel broadwell ghz GB ram task subdivision accomplish pyspark amazon EMR elastic mapreduce service storage service aws storage pre processing experimental exploratory data analysis eda perform obtain outlier zero data nevertheless feature characterize depict characterize packet contribute equally analysis bias indeed remark reference update equation neural network input strongly affect assume update rate entry dataset exploratory data analysis     bitrate   avoid issue rescale approach rescale min max normalization standardization obtain standardization normalization outperform others accordingly dataset rescale standard normal distribution zero standard deviation split dataset remark network configuration earlier hyper parameter properly chosen neuron kernel activation function optimizer forth accomplish dataset subdivide datasets namely dataset entire dataset remain data partition former tune model hyper parameter whilst latter unbiased evaluation model dataset precisely fold validation tune hyper parameter purpose equally partition training evaluation model training dataset evaluate correspond dataset hyper parameter tune model evaluation programmed python  library kera python program tensorflow framework gpu content training dataset  mail chat browsing PP evaluation metric hyper parameter perform entire model hyper parameter appreciate classification quality mention model accord definition traffic define rfc metric compute dataset derive multi confusion matrix category tps positive correctly classify fps false positive incorrectly classify FNs false negative incorrectly reject  negative correctly reject python library scikit compute metric conduct PC desktop equip intel core cpu ghz GB ram nvidia geforce MX gpu GB memory parallelization accomplish  jit compiler model description described earlier architecture hyper parameter effectiveness proposal precisely varied  layer network  neuron layer filter filter cnn kernel dimension filter cnn pad filter impact output valid causal activation activation function relu tanh sigmoid pool pool maxpooling  stride stride convolution  lstm layer timesteps observation input lstm optimizer optimizer kera compile model adam adamax nadam loss loss function network mse categorical  focal loss remark focal loss useful unbalanced dataset ultimately combination accordingly network configuration reference network configuration fully layer comprise neuron respectively dropout layer obtain classification probability distribution max activation function layer correspond maximum probability output classification network epoch categorical entropy adam loss function optimizer respectively addition architecture technique training loss function remains roughly unchanged epoch cnn sae NN model analyze combination cnn AE NN configuration dimensional vector feature image therefore matrix accord fed input cnn accord derive feature cnn sae NN network linear correlation input    bitrate   reference matrix configuration express spatial correlation input namely spatial feature architecture encoder derive training compose cnn layer filter respectively activation relu pad pool decoder built inverse combination layer latent derive cnn layer flatten fed fully softmax neural network remark flatten layer adapt dimensional latent input NN confusion matrix evaluation metric respectively obtain training network adam optimizer mse loss function epoch confusion matrix cnn sae NN configuration predict   chat browsing PP metric cnn sae NN configuration  mail chat browsing PP lstm sae NN model compose combination lstm sae NN earlier lstm series dimension define timesteps hyper parameter input training timesteps stride probably compose packet eda however important aspect NTC task classification perform packet encoder compose sequence lstm layer respectively lstm layer return sequence parameter whilst layer false remark return sequence parameter output vector timesteps iteration lstm whilst false iteration latent dimensional vector decoder  layer timesteps vector timesteps latent adapt lstm layer decoder finally latent derive lstm layer fed fully softmax neural network confusion matrix metric respectively lstm sae sequence adam optimizer mse loss function epoch depict accord complex feature extract configuration gathering insight improvement performance confusion matrix lstm sae NN configuration predict   chat browsing PP metric lstm sae NN configuration  mail chat browsing PP cnn lstm sae NN model exploit combination cnn sae lstm sae capture spatial temporal feature feature respectively described feature reshaped matrix convolutional layer filter respectively kernel flatten layer employ dimensional vector lstm layer timesteps lstm layer  layer timesteps convolutional layer flatten layer respectively resultant  flatten cnn vector fed lstm layer characterize return sequence parameter lstm layer return sequence false latent output layer dimensional vector decoder implement reverse sequence described encoder cnn lstm sae network adam optimizer mse loss function epoch confusion matrix evaluation metric respectively confusion matrix cnn lstm sae NN configuration predict   chat browsing PP metric cnn lstm sae NN configuration  mail chat browsing PP stack cnn lstm sae NN configuration differs previous presence training precisely cnn AE lstm AE configuration separately former feature matrix latter encoder output cnn AE model resultant latent correspond cnn AE lstm AE respectively  sequence along fully NN stack network remark lstm layer series accomplish timesteps cnn output reshaped input lstm layer hyper parameter configuration previously configuration confusion matrix evaluation metric respectively confusion matrix stack cnn lstm sae NN configuration predict   chat browsing PP metric stack cnn lstm sae NN configuration  mail chat browsing PP ConvLSTM sae NN configuration ConvLSTM layer sequence filter respectively layer activation function relu pad return sequence layer flatten version latent dimensional vector filter output matrix combine timesteps hyper parameter previous configuration outcome confusion matrix ConvLSTM sae NN configuration predict   chat browsing PP metric ConvLSTM sae NN configuration  mail chat browsing PP sae NN sake completeness architecture characterize NN layer without cnn lstm layer combination layer neuron layer sequence fully layer neuron respectively confusion matrix sae NN configuration predict   chat browsing PP metric sae NN configuration  mail chat browsing PP comparison discussion effectiveness spatial temporal feature previous obtain data dataset feature classical popular machine technique naive bayes classifier NB decision classifier multi layer perceptron classifier mlp highlight advantage introduce sae obtain cnn lstm network fully NN report numeric obtain metric addition graphical representation performance metric average across configuration depict depict machine classifier correctly classify category imbalance dataset strongly affected performance classifier indeed maximum sensitivity fmea associate NB mlp respectively achieve classifier PP trace maximum classifier trace mail chat browsing tend neglect metric naive bayes classifier  mail chat browsing PP metric classifier  mail chat browsing PP metric mlp classifier  mail chat browsing PP metric pure cnn NN classifier  mail chat browsing PP metric pure lstm NN classifier  mail chat browsing PP usage sae sufficient indeed sensitivity fmea mail chat browsing achieve obtain classifier mail chat browsing respectively mlp NB average improvement around achieve although cnn NN lstm NN configuration slightly machine around cnn NN lstm NN respect calculate average sensitivity introduction sae increase significantly performance observable respectively indeed average accuracy achieve cnn sae NN lstm sae NN increase around respect cnn NN lstm NN respectively besides although sensitivity PP achieve classifier important highlight specificity achieve NB mlp cnn NN lstm NN classify almost traffic PP confirm precision auc metric contrary sae architecture achieve metric finally configuration achieve maximum performance stack cnn lstm sae NN average achieve metric depict image KB image classification metric average across graphical interpretation introduction cnn lstm layer sae affect feature transformation visualize dimensional latent architecture dimensional accomplish distribute stochastic embed sne sne widely approach non linear data dimensionality reduction allows data visualization dimensional representation feature pre version depict respectively depict feature unique visible correspond PP mail chat browsing curve overlap separability data visible min max normalization evident standardization zero indeed browsing chat remain overlap mail PP introduction cnn layer data separability indeed mail browsing PP  classification however overlap incorrect classification report entry chat incorrectly classify mail browsing PP mail depicts happens lstm layer tend expand although scatter data data overlap marked improvement classification report expansion marked cnn lstm layer introduce depict nevertheless confirm report reference cnn lstm configuration overlap presence classification maximum expansion achieve stack cnn lstm configuration achieves classification performance indeed architecture metric although outcome ConvLSTM tends feature depict degradation performance ultimately stack cnn lstm sae NN configuration architecture extract spatial temporal feature feature conclusion future investigate potentiality novel spatial temporal feature classify network traffic detailed description neural autoencoder network architecture meaningful feature statistical feature feature extract traffic temporal derive feature relevant knowledge exist feature spatial feature feature temporal feature although feature packet packet etc proven effective NTC purpose extract feature propose neural network architecture autoencoders aes encode decode function combination convolutional recurrent network layer investigate combination cnn lstm cnn lstm ConvLSTM stack cnn lstm convolutional network employ extract spatial feature lstm recurrent network extract temporal feature confirm experimental introduction cnn lstm layer sparse AE sae significant improvement classification performance indeed mathematical formalism sne graphic representation layer introduce specific transformation input data precisely cnn sae tends cluster data respective lstm sae tends expand data without introduce overlap consequently architecture achieve performance around average accuracy respectively however performance obtain configuration joint unique architecture namely stack cnn lstm configuration advantage cnn sae lstm sae extract spatial temporal feature feature performance metric achieve besides feature derive latent sae demonstrate robust respect unbalance distribution training dataset investigate network architecture AE naive bayes decision multi layer perceptron pure cnn lstm capability distinguish traffic mail chat browsing PP precisely imbalance training dataset approach incorrectly almost traffic PP maximum contrary usage aes specifically sae sufficient overcome issue ability proposal obtain excellent NTC without neither payload inspection IP address encourages future intend extend target application environment proposal useful differentiate encrypt traffic distinguish skype activity tor traffic etc intend within intrusion detection IDS detect security vulnerability framework automotive iot tunnel robust fake news detection forth nevertheless research benefit usage propose NTC approach indeed apply research extract spatial temporal feature