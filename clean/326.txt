software define networking sdn gain momentum research driver 5G network due capability increase flexibility network address variety network challenge logically centralize intelligence software controller thanks machine ML technique network performance utilization optimize enhance neural network NN reinforcement RL demonstrate cooperate complex arise network operation management exploit sdn placement approach aim dynamically predict traffic congestion mainly NN optimal reroute traffic improve network utilization deploy network DQN agent formulate quality service qos aware rout linear program LP objective minimize EE delay link utilization propose efficient heuristic algorithm numerical emulation ONOS controller mininet demonstrate propose approach significantly improve network performance decrease link utilization packet loss EE delay previous keywords sdn prediction neural network qos ONOS DQN lstm introduction development technology 5G network traffic exponential rate due application stringent heterogeneous requirement massive machine communication ultra reliable latency enhance mobile broadband communication data application hence quality service qos service agreement SLA requirement essential develop innovative traffic manage software define networking sdn emerge technology 5G vision capable increase flexibility network reduce sdn aim decouple network intelligence network device enable centralization network intelligence flexibility traffic simplicity network management operation however network increase computational complexity increase exponentially adapt traditional network policy continually network behavior challenge task indeed effectively avoid congestion overload link latency throughput monitor continually proactively quickly route packet link sdn protocol standard openflow enables interaction network device although openflow mechanism request throughput statistic specification protocol mechanism latency moreover exist rout algorithm suitable sdn due convergence limit absence future vision evolution network traffic approach propose cope challenge  hong propose network concept rout decision automatic analyze telemetry network data analyze statistic artificial intelligence AI machine ML technique  tran propose knowledge define networking  introduce knowledge KP conventional sdn paradigm responsible behavior network apply ML data statistic incorporate intelligence via ML sdn crucial guarantee request qos optimize rout sdn network indeed feng shu linear prediction auto regressive integrate average ARIMA predict future evolution network traffic however ML suitable handle 5G network beyond due limit efficiently cope volume data technique intelligently decision schedule bandwidth reservation etc reinforcement RL technique gain momentum rout optimization principal deploy agent periodically decision automatically adjusts strategy action mapping maximize numerical reward   propose rout algorithm minimize average delivery drawback RL technique  policy explore entire unsuitable inapplicable network almost countless advantage combine RL technique overcome limitation RL refer reinforcement DRL consequently nns instead DRL achieve network processing storage DRL DQN combine dnn propose dynamic efficient traffic engineering scheme network traffic prediction rout optimization DTPRO extends previous DQN agent traffic prediction module optimize network rout specifically propose consists phase firstly dynamically optimize rout network training DQN agent secondly predict congestion adjust DQN reward function rout configuration finally route network traffic link DQN agent reroute exist traffic away congest resolve linear program LP formulate LP placement objective minimize network delay packet loss link utilization phase propose heuristic interacts DQN agent traffic prediction formulate LP optimize network performance experimental ONOS controller mininet propose approach promising enhancement traditional rout algorithm contribution summarize DQN agent appropriate action optimize rout network predict congestion adjust DQN reward function rout configuration mathematically model qos aware rout LP input rout strategy DQN agent predict traffic objective minimize EE delay EE link utilization EE packet loss propose efficient heuristic algorithm DTPRO LP fourth implement DTPRO approach ONOS controller mininet remainder organize related discus architecture propose framework placement algorithm evaluates propose finally conclude related survey literature traffic monitoring approach data statistic sdn focus ML technique traffic prediction rout optimization traffic monitoring recently sdn openflow protocol implementation attention openflow software implement sdn architecture ONOS pox ryu OpenvSwitch data traditional network monitoring available  cisco netflow sFlow however monitoring compatible openflow network propose openflow monitoring propose  network monitoring framework sdn built openflow controller northbound api restful api adaptive schedule algorithm polling achieves accuracy continuous switch polling communication overhead van propose  network monitoring openflow statistic throughput packet loss device improve measurement accuracy reduce computation overhead  per statistic throughput packet loss insert entry monitoring entry switch along monitor statistic proactively message reactively notification message however mechanism latency openflow protocol latency measurement sdn openflow protocol relevant described author slam framework software define latency monitoring network switch delay inside network capture directly information network device latency specific propose metric latency throughput per critical link network probability congestion important important without affect performance machine qos aware rout optimize rout sdn network crucial efficient resource allocation propose literature  propose approach genetic ant algorithm optimize placement however approach limited situation complicate rout research install across pre compute optimal exploit sdn controller global visibility propose approach reduce consumption network congestion compute optimal topology accommodate traffic demand traffic load balance distribute across shortest optimal topology resolve integer linear program ILP holt HW ARMA ARIMA model traditional linear prediction widely network traffic forecasting   deploy nns sdn network traffic matrix TM prediction approach predict aggregate ethernet traffic nns employ multiresolution mrl forecasting transfer rate predict future transfer rate specific link experimental nonlinear traffic prediction nns outperforms linear forecasting model ARIMA auto regressive auto regressive  HW cannot accuracy requirement   memory lstm recurrent neural network rnn framework model parameter prediction model TM prediction training lstm model various parameter configuration simulation propose lstm model converge quickly achieve prediction accuracy computation   RL technique context rout optimization rout algorithm discover efficient rout policy dynamically network without advance network topology traffic rout algorithm packet traverse network propose achieve latency throughput adaptive rout inefficient storage policy DRL address challenge advantage neural network dnns RL algorithm relevant described  tran DRL agent interacts network signal traffic matrix action link vector reward improve network performance DQN DRL network rout optimization adaptive DQN latency aware rout protocol DQELR adopts DQN algorithm policy policy rout decision adaptively network liu DRL reinforcement rout cope coexistence elephant mouse coflow multiple resource bandwidth cache compute propose approach DQN deterministic policy gradient DDPG experimental demonstrate effectiveness improve network performance however account traffic prediction DRL agent trigger packet traffic detect controller moreover action link modification installation sum aforementioned shortcoming non consideration mapping network correspond action predict congestion non consideration predict congestion reward action observation propose DQN agent capable optimize rout dynamically optimal accord reward function account latency throughput packet loss addition propose deploy traffic prediction module predict network congestion propose network traffic prediction rout optimization DTPRO approach combine ML technique sdn crucial improve network performance DTPRO approach explain global architecture thereafter network measurement module latency measurement statistic DQN traffic prediction module finally mathematical model propose heuristic described proactive module DTPRO architecture although sdn centralization network intelligence flexibility traffic simplicity network management operation  introduce paradigm intelligence network management telemetry data suggests knowledge KP conventional sdn paradigm adopt AI cognitive network model context propose framework accord  paradigm exploit global network data consists programmable device data packet processing device embed intelligence decision rely update configuration openflow protocol brain sdn network incorporates intelligence centralize management global network specialized central controller deploy module network measurement proactive module network measurement module consists sub module statistic continuously metric packet byte per throughput latency measurement latter sub module continuously network latency periodically packet probe data proactive module responsible optimal rout strategy predict traffic KP resolve optimization explain later management ensures operation performance network network measurement specifically network measurement module network analytic statistic analyze KP finally KP exploit management data management input fed ML algorithm convert knowledge precisely behavior network processing statistic extract optimal knowledge route deploy DQN agent finally predict network congestion prediction lstm ARIMA linear regression LR traffic prediction module rout strategy DQN agent exploit historical data rout configuration detail module network measurement network measurement module ensures data monitoring openflow protocol crucial network management operator decision load balance rout qos SLA collection statistic data active passive active mode controller sends receives probe packet entire data network statistic rtt latency packet loss passive mode corresponds query statistic information switch standard openflow message earlier network measurement module consists sub module statistic latency measurement statistic module monitor data accord passive mode openflow standard message information specifically  message switch periodically report statistic message   respectively throughput per throughput packet report  message latency measurement module active monitoring mode periodically packet probe data latency notification arrival specifically consists firstly packet probe controller traverse return controller secondly packet probe controller switch switch specific monitoring instal switch respectively contains controller delay controller switch packet probe controller switch switch switch packet probe pre instal correspond packet probe delay corresponds image KB image latency computation mechanism processing device respectively processing controller accurately estimate delay encapsulate nanosecond packet probe moreover monitoring instal packet probe interfere normal traffic worth latency measurement module accord service gateway initiative  standard implement application dynamic component model indeed compose component monitoring installation probe packet generator packet processing latency measurement activate latency measurement module instal probe packet probe packet along monitor probe packet generator periodically specific probe packet data monitoring already instal previous component thereafter packet processing component listens incoming packetin latter corresponds probe packet packet processing component extract controller switch sends latency measurement component latency explain latency centralize database traffic prediction model avoid congestion improve network performance important predict future evolution network traffic propose lstm model predict network latency prediction model ARIMA LR detail model adapt predict EE network latency linear regression LR statistical prediction attempt model relationship explanatory variable dependent variable fitting linear equation data objective predict outcome dependent variable explanatory variable identify dependency variable refers uni variate regression explanatory variable dependent variable constant error described valuable model relationship delay explanatory variable estimate dependent variable correlation coefficient indicates strength association data variable regression equation sample delay equation sample obtain measurement delay measurement gap measurement model minimize regression quality previous training data ARIMA model model latency stochastic express linear combination observation latency measurement random error series equation observation auto regressive AR average differentiation worth latency predict previous sample latency series recall identification ARIMA model involves stationarity correctly ARIMA model estimate refer auto correlation function acf partial auto correlation function PACF estimate memory lstm continuous development network service network traffic constantly expand burst complexity consequently traffic evolution non linearity rnn lstm variant model complex non linear advantage lstm model series forecasting input output NN propose model traffic latency prediction lstm model multi series forecasting lstm input sample feature delay vector lstm output predict delay vector sample training timesteps refers lstm memory capacity feature amount feature construct dimensional structure lstm model refers previous slot predict future delay vector parameter avoid sequence computational complexity parameter neuron finally refers previous latency measurement fed lstm model output lstm model predict vector link delay overall architecture propose lstm model rout optimization model DQN request qos rout rout strategy traffic depends data transport network rout improves qos propose deploy DQN agent dynamically determines optimal model DQN network topology non orient graph respectively vertex link capacity network node DQN agent interacts environment signal action reward traffic matrix network load action agent link vector reward agent related qos parameter mainly average network latency average data rate average packet loss reward adjustable rout strategy objective optimal policy mapping action maximize reward rout strategy periodically update epoch initialize proactive formulation resolution proactive module responsible rout accord optimal rout strategy predict traffic matrix accord  model combine component load predict traffic load packet processing generator sub module load predict traffic load respectively responsible predict traffic link KP layer packet processing listens packetin data finally generator responsible rout automatically trigger latter corresponds generate incoming load sub module calculate correspond happens congestion detect action reroute traffic accuracy rout strategy DQN agent explain previous incorporate prediction aspect objective link route minimize EE network latency balance network load minimize link utilization image KB image proactive context physical infrastructure capacitate graph denotes node switch physical virtual link network link characterize predict propagation delay bandwidth capacity threshold assume network node link predict traffic matrix respectively predict volume traffic origin destination node network characterize affected network capacity limitation constraint define variable symmetric binary matrix denotes allocate link denotes link utilization maximum link utilization link depends link characteristic link capacity network traffic latency sensitivity priority latency sensitive application define variable assume switch priority variable priority switch decision variable denotes rout queue hence formulate mathematically LP objective minimize delay limitation link capacity limitation capacity limitation priority demand satisfaction delay link capacity limitation constraint specify link delayed overload priority constraint ensures priority rout finally demand satisfaction constraint ensures traffic demand source destination objective minimize network delay link utilization packet loss demand satisfied factor related importance respectively latter define formulate multi commodity furthermore assume sdn controller incoming however network increase computational complexity increase exponentially clearly approach feasible generates overhead due frequent update cope reduce computation complexity propose efficient heuristic algorithm DTPRO algorithm DTPRO allows quality traffic allocation minimize network latency packet loss algorithm detail propose heuristic input predict traffic matrix respectively predict matrix link delay respectively matrix link delay threshold defines maximum tolerable delay link worth DTPRO execute incoming detect congestion image KB image algorithm epoch request DQN agent optimal link update network configuration worth corresponds interval apply rout strategy obtain DQN agent moreover impact network configuration rout strategy parameter initialize however parameter modify network administrator thereafter continuously predict traffic matrix congestion occurs predict link delay threshold link overload algorithm correspond maximum congest sort denote delay rerouted accommodate correspond discard algorithm finally adjusts parameter DQN reward function DQN agent avoids transition function adjusts parameter link delayed predict delayed parameter adjust link overload predict overload performance evaluation evaluate efficiency propose approach environmental setup experimental experimental setup implement network measurement module latency measurement statistic proactive module cooperate module java openflow controller ONOS previous framework developed DQN agent traffic prediction implement python  docker container DQN agent traffic prediction interact proactive module ONOS northbound api network emulation OpenvSwitch implement experimental topology illustrate generate traffic host iperf consists host network measurement module statistic network latency throughput per device report series statistic  database interval link label capacity link built DQN model tensorflow library deploy separately nns architecture parameter DQN illustrate consists dense layer impact action corresponds network configuration input traffic matrix separately DQN agent network configuration respectively vector corresponds input NN output nns training correspond action respectively consequently architecture correspond training training phase adopt greedy action selection exploration rate fix parameter rate discount factor respectively finally DQN agent training corresponds episode image KB image emulate topology built lstm model kera library dense layer adam NN parameter rate activation function lstm output predict vector refer traffic matrix DQN multiple input output sample output sample corresponds prediction interval DQN parameter DTPRO  dense layer action output target network update frequency rate discount factor mini batch exploration rate memory episode episode capacity improve performance training ARIMA lstm DQN model offline model optimal DQN agent predict traffic lstm ARIMA parallel model prediction accuracy evaluation quantitatively evaluate overall performance prediction model error RMSE define difference predict actual compute average sum error express respectively normalize predict normalize actual interval corresponds prediction prediction interval initialize experimental evaluate performance propose DTPRO prediction model network traffic evolution evaluate DQN model parameter configuration finally propose rout scheme account obtain traffic prediction model estimate ARIMA model propose network traffic evolution estimate ARIMA parameter AR differencing series stationary series stationary around define however network traffic non stationary evolution differencing series stationary differencing corresponds refer acf plot differencing augment  fuller  series stationary differenced series stationary increment AR corresponds lag predictor refer PACF estimate parameter correlation specific lag series finally parameter corresponds lag prediction error ARIMA model PACF estimate parameter acf estimate parameter prediction forecasting accuracy ARIMA model network traffic RMSE metric parameter estimate forecasting prediction future series prediction estimation future model prediction forecasting accuracy regard network traffic evolution hence refer model prediction accuracy model regard lstm model ensure estimation accuracy avoid fitting network neuron training epoch plot average loss function average RMSE training epoch RMSE loss function epoch sake simplicity fix node node training epoch loss function converges stable ensure RMSE loss function RMSE loss function stable achieve estimation accuracy training epoch focus identify hidden node image KB image ARIMA model parameter earlier hidden node lstm network crucial achieve stable network configuration plot average loss function average RMSE hidden node increase hidden node measurement previous hidden node loss function converges stable clearly loss function RMSE converge stable achieve estimation accuracy hidden node image KB image lstm loss RMSE training epoch LR parameter estimate online prediction trigger traffic prediction LR previous measurement identify parameter predicts future evolution network traffic plot variation parameter training LR prediction model parameter dynamically data interval prediction accuracy earlier lstm ARIMA LR impact prediction interval RMSE metric increase measurement recall ARIMA model lstm model training hidden node LR model dynamically estimate image KB image LR parameter data interval RMSE lstm quasi stable increase prediction interval ARIMA achieves stability increase prediction interval due network traffic periodic however increase LR RMSE clearly visible interpret increase distance predict disperse traffic furthermore ARIMA outperforms LR due capacity estimate correlation previous lag series finally clearly visible lstm outperforms others prediction due capacity dependency DQN model propose network traffic evolution image KB image RMSE lstm ARIMA LR prediction recall principal propose DQN model policy mapping traffic matrix action network configuration maximize numerical reward define action structure ensure estimation accuracy DQN model avoid fitting DQN plot evolution average loss function average reward function training episode action DQN initial parameter earlier action structure loss function converges reward function stable image KB image impact action training DQN agent loss function action configuration converge however increase action capacity reward function increase increase reflect existence appropriate network configuration network queue node  decrease packet loss network latency action mention reward function related data rate network latency packet loss parameter respectively parameter important role importance factor convergence loss function plot evolution average loss function training episode reward function parameter importance packet loss network latency loss function converges DQN model corresponds action reward function parameter ass performance propose heuristic aforementioned DQN model prediction hop HC rout default rout metric ONOS reduce DTPRO obtain DQN model rout optimization disable traffic prediction module  consists obtain DQN model rout optimization obtain ARIMA model instead lstm traffic prediction  consists obtain DQN model rout optimization LR traffic prediction plot packet loss delay link utilization scheme DTPRO   reduce DTPRO HC HC approach obviously considerable packet loss increase link utilization shortest link minimum capacity DQN agent without prediction reduce DTPRO scheme considerable packet loss increase link utilization due  DQN predict future evolution network traffic finally DTPRO outperforms scheme   packet loss delay link utilization decrease related accuracy lstm predict network congestion ARIMA LR prediction plot predict congestion combine DQN traffic prediction worth congestion happens network latency traffic load exceed threshold fix link capacity propose network traffic evolution define overload network specific network configuration lstm outperforms others due accuracy predict future evolution ARIMA LR image KB image packet loss delay link utilization placement algorithm DQN without prediction improve quality qoe wider variety service effort service propose evaluate DTPRO approach without accord simulation scenario specific latency sensitive application LSA throughput sensitive application tsa packet loss sensitive application PLSA delayed congest specific priority baseline image KB image predict congestion combine DQN traffic prediction DTPRO corresponds propose DTPRO heuristic sort traffic accord reroute traffic congestion DTPRO priority corresponds propose DTPRO heuristic sort traffic accord reroute traffic congestion priority priority LSA DTPRO priority performs DTPRO decrease EE delay however priority tsa PLSA EE delay scheme priority tsa PLSA DTPRO priority performs DTPRO decrease EE throughput however priority LSA performance scheme improvement EE throughput improve performance correspond application congest rout allows DTPRO approach context network slice slice dedicate specific traffic image KB image DTPRO without priority plot impact interval network performance predict network congestion EE delay EE throughput rate parameter varied interval recall parameter corresponds interval apply rout strategy obtain DQN agent image KB image impact interval network performance network congestion EE throughput increase increase interval EE delay decrease increase DQN increase increase interval transition network configuration detect moreover lstm model traffic prediction module predict traffic evolution predict network congestion rerouting traffic efficiently decrease EE delay increase EE throughput finally comparative analysis propose approach DTPRO approach related reinforcement rout DRL traffic optimization DQN traffic optimization lstm tol corresponds lstm rnn framework predict network traffic matrix network latency aware rout DQELR rout optimization DQN image KB image comparative analysis DTPRO tol DRL DQELR DTPRO modular knowledge consists module rout optimization DQN traffic prediction lstm comparison propose approach DRL DQELR deactivate traffic prediction module rout optimization module tol approach traffic prediction module experimental topology approach DRL DQN network target consist hidden layer neuron relu corresponds activation function rate fix propose liu DQELR model input layer consists node hidden layer node respectively finally tol approach consists lstm model   model aforementioned parameter EE delay EE throughput EE packet loss approach interval tol approach obviously packet loss delay network throughput approach explain approach account rout strategy increase congestion link network behavior prediction accuracy DRL DQELR perform tol approach consist define rout strategy training DQN agent maximize reward related specifically network throughput delay DRL performs slightly DQELR DQN rout decision packet DQELR approach impact performance  rout decision contradictory sdn finally superiority propose DTPRO approach EE throughput EE delay EE packet loss clearly visible DRL DQELR action propose DQN model modify link vector equivalent define action instead define incoming define DRL DQELR tol approach predicts traffic matrix without rout strategy combine DQN agent traffic prediction lstm allows unseen transition DQN agent predict traffic prediction module previous DQN agent future behavior network traffic lstm conclusion placement software define network statistic measurement traffic prediction implement separately cooperate module KP layer advantage KP network rout dynamically optimize deploy DQN agent dynamically determines optimal policy mapping traffic matrix action vector link addition propose deploy traffic prediction module prediction lstm avoid congestion mathematically formulate qos aware rout LP correspond optimization minimize network latency packet loss link utilization optimization efficient heuristic algorithm propose implement network traffic prediction rout optimization DTPRO dynamically interacts external DQN agent module link traffic prediction avoid congestion experimental ONOS controller OpenvSwitch DQN agent mapping traffic matrix link route traffic however DQN adapt predict future evolution network traffic combine DQN traffic prediction network latency packet loss link utilization decrease moreover lstm achieves estimation accuracy outperforms traditional prediction decrease EE delay packet loss future exploit context distribute sdn controller location associate data switch optimize deploy DQN agent