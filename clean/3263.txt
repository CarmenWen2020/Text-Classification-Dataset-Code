growth develop internet technology propose promising handle heterogeneous massive volume data pervasive smart equipment handy mobile device thanks mention technology mobile device critical business entertainment application augment reality virtual reality vehicular network medium however due device inherent limitation emerge computation environment mobile compute introduce achieve essential requirement latency consumption literature offload technique transfer burden mobile device incur application request computation environment explore computation environment efficient execute request challenge achieve addition research propose cope management offload criterion autonomous computation offload framework propose address challenge related intensive resource intensive application however author knowledge propose autonomous framework explore model management computation offload criterion besides cope dimension offload decision simulation neural network multiple linear regression hybrid model hidden markov model planning module mention autonomous methodology conduct simulation propose hybrid model appropriately optimal accuracy regard offload decision latency consumption prediction propose management framework previous keywords offload mobile compute machine neural network hidden markov model regression introduction despite numerous advantage suffers significant security delay consumption execute user request besides heterogeneous massive volume data pervasive smart equipment handy mobile device application augment reality AR virtual reality VR vehicular network medium incur extra demand computation resource device device inherent limitation execute request critical mobile compute MEC propose address issue european telecommunication standard institute ETSI  consortium standard definition related MEC later multi access compute later mobile compute multi access compute variety non mobile access network wifi virtual network function VNF software define networking sdn closer paradigm MEC environment capability closer smart equipment data generate environment suppose enhance essential metric delay computation user satisfaction task resource limitation challenge issue achieve generally offload approach alleviate inherent limitation local mobile device critical application resource intensive approach propose address related challenge none satisfactory approach generally consist theoretic machine mathematical combine approach heuristic algorithm mostly approach multiple objective originate biology economics planning behavioral constraint decision offload optimizes challenge decision code offload dynamism heterogeneity computation environment classical approach appropriate nonlinear characteristic generally computation offload nearby MEC server mechanism enhance latency consumption improve offload quality specifically presence 5G telecommunication network offload literature research address challenge jointly achieve user requirement decision complexity optimally however plenty researcher explore offload domain deeply approach address vital offload issue mention requirement approach machine theory stochastic model mathematical model resolve decision literature MAPE loop conceptual model management introduce ibm autonomic toolkit access nov conceptual model various reference model explicitly implement environment efficient resource provision definition MAPE construct component monitoring analyze planning execute later detail however despite comprehensive utilization various computation environment MAPE loop exploit computation offload apply DL machine handle architecture unlimited heterogeneous network layer neural network dnn parameterized implementation traditional artificial neural network ann multiple layer architecture intuitive feature artificial neural network extremely appropriate predict detect optimize complex approach explores optimal training input mathematical manipulation extract selects suitable feature input facilitate multi layer artificial neural network architecture critical parameter layer initial rate configure appropriately avoid fitting execution delay generally issue demand computational resource conflict offload purpose goal hidden markov model hmm unsupervised machine extensively exploit probabilistic markov model series discrete continuous observation dependency independency predict future behavior hmm model hidden observable markov decision MDP model analyze observable besides hidden observable probability distribution MAPE loop concept explore conceptual approach computation offload mechanism propose approach apply dnn predict offload decision delay incur local remote execution request consumption execution ecosystem request exploit hmm model model uplink transmission medium predefined incoming rate request uplink medium  flexibility critical feature dnn hmm model override mention concern additionally address potential issue offload criterion contribution briefly multiple user multiple server environment heterogeneous service platform user request service utilize concept MAPE loop model offload decision execution environment apply neural network dnn appropriate execute incoming request addition multiple linear regression along dnn model hybrid model achieve optimal predict utilized apply hidden markov model hmm appropriate uplink transmission medium exist organize review essential related propose approach discus experimental besides discussion summary simulation issue future finally conclusion related emerge technology MEC provision computational flexibility offload user request delay constraint despite numerous advantage inherent stochastic behavior entity MEC development address related user centric challenge offload literature various challenge limit user expectation generally quality service qos quality qoe umbrella offload metric application handy mobile equipment critical metric delay consumption resource limited equipment become significant therefore rid limitation requirement critical metric decision offload approach become incredibly significant generally mention challenge categorize subset external challenge offload internal challenge offload former define challenge mutual impact offload decision latter define challenge related offload affect external feature tight relationship challenge researcher propose approach multi  external challenge offload concern external challenge MEC offload challenge summarize metric profit wireless channel delay dependency link challenge code offload efficiently enhance predefined metric latency incur devote computation resource computation paradigm intensive application researcher focus scenario mobile device offload task server address challenge unrealistic hypothesis hardly implement scenario essential feature MEC environment stochastic behavior entity hardware service heterogeneity neglect weakly distribute context aware dnn model image processing 3D MEC offload although extensive model offload approach offload request local mobile environment limited model bound request dissimilar historical data predict future behavior similarly author apply propagation NN offload approach presence 5G communication link tackle consumption delay incur offload computation although appropriate compute model model practical implement limited user server essential factor transmission propagation delay neglect communication model calculate delay partition dnn model demand offload resource demand device dnn query peer dnn model  although appropriate grain offload approach model suffers complexity overload query propose selective offload utilize ARIMA BP concept predict computation capacity optimization along delay constraint although model appropriate selective offload model suffers complexity likewise utilized dnn incremental version offload purpose offload multiple server sequence notably execution dnn dnn model entirely offload although appropriate grain offload approach dnn model limitation offload drawback propose model joint optimize offload utility avoid privacy leakage user information crucial metric although appropriate mathematical proof propose model propose offload complexity addition transmission rate entity model unrealistic implement apply hierarchical machine ML model detect malicious activity industrial iot IIoT offload MEC although appropriate mathematical proof formulation propose optimal offload strategy selection suffers complexity define extra hidden layer model device surveillance degrades performance model apply construct multi memory traffic prediction wifi access offload MEC environment although model acceptable complexity practical implement author blockchain framework apply reinforcement DRL MEC although model acceptable model theoretical practical implement internal challenge offload internal challenge offload MEC offload challenge summarize offload code efficiently regard dynamicity decision apply algorithm granularity schedule researcher model complexity convergence algorithm inefficient latency consumption requirement intensive critical application contrast existential MEC environment researcher propose schedule model exploit stochastic approach MDP despite complementary approach DRL overcome drawback approach action  plenty uncertainty ecosystem computation complexity unsuitable iot IIoT lstm prediction scheme schedule offload strategy reduce latency task offload mobile environment although model MEC intermediary computation paradigm overcome delay related shortage transaction smart layer consumption delay violate propose model lstm extension recurrent neural network cannot handle dependency claimed apply planning strategy persistent service respond quickly disastrous situation audio recognition model although model network existence apply jackson network queue model estimate task service service policy policy offload execution purpose likewise propose mixed integer linear program MILP schedule task offload scheme mobile environment formulate mixed integer linear program optimize consumption device although appropriate application model task schedule algorithm suffers complexity overload task arrival rate scenario instead MDP exploit QL binary decision utilized reinforcement RL thereby delay computation improve although rid local minimum apply address dimensionality challenge incur propose binary computation offload decision curse dimensionality remove apply approach policy focus offload optimization industrial application dedicate equipment related challenge improve latency consumption equipment linear program algorithm  intelligent heuristic algorithm invasive tumor growth optimization  approach although suitable model task offload algorithm suffers complexity overload task arrival rate scenario investigate task schedule computation offload MEC optimize decision offload transmission medium allocation approximate propose infinite horizon MDP rid dimensionality apply improve performance utilized reinforcement model transmission stochastic gradient decent sgd model although apply reinforcement address dimensionality challenge incur propose MDP mention curse dimensionality remove apply reinforcement discussion offload comparison strategy apply metric evaluation advantage weakness technical comparison offload strategy      NA extensive  propose model   matlab appropriate computation  practical implement limited user server  NA appropriate grain offload  complexity overload query zhao zhou ARIMA   NA appropriate selective offload  complexity  caffe appropriate grain offload  limitation offload NSGA  NA appropriate mathematical  complexity unrealistic implement  tensorflow appropriate mathematical  complexity offload strategy selection  NA acceptable  practical implement  python acceptable  practical implement  alibaba server appropriate intermediary computation  SLA metric capable handle dependency  appropriate arrival task  service policy   NA appropriate application  complexity overload arrival rate  NA appropriate  dimensional    appropriate  complexity overload arrival rate  NA acceptable  dimensional propose approach propose framework explain detail firstly MAPE loop framework computation offload architecture layer methodology introduce computation offload formulate finally perform request autonomic computation offload algorithm depicts essential definition utilized research important definition utilized  input request mobile device resource processor information utilization limitation rate storage information utilization limitation communication information utilization input limitation output limitation resource resource utilization incoming content request sensitivity requirement incoming request maximum acceptable delay maximum acceptable consume request information hidden hmm observable hmm propose framework subsection computation offload framework introduce layer methodology computation demand code execute locally remotely accordance  computation propose framework methodology mention earlier define layer computation architecture consist smart device layer layer layer architecture propose architecture depict layer architecture described briefly device smart device layer sensor smart mobile tablet etc heterogeneity storage cpu communication capability possibly interconnect layer APs server relatively data server categorize distribute management device layer server usually access optical channel APs scatter distribute manner centralize layer robust data unlimited resource appropriate service aim profoundly responsibility layer deeper layer entity responsibility image KB image propose compute architecture research autonomic decision offload computation demand application focus issue propose architecture smart gateway propose model gateway smart gateway ESG gateway CG ESG responsible offload incoming request request requester MAPE loop execute smart device configure appropriately configuration affect optimization entire network CG responsible manage upcoming traffic server computation paradigm model server request reasonable server pre distance agent depict agent information overall utilization transacts ESG capacity perform request agent negotiates ESG another appropriate destination server sensitive lightweight resource sensitive request respond layer appropriate pre configuration resource service suitable load prediction model predict future behavior request significance hence propose dnn regression hybrid hmm model predict resource offload destination autonomous noteworthy agent related information inform autonomous appropriate execute incoming request image KB image offload decision autonomic offload manager autonomic offload manager comprise MAPE agent brain decision MAPE agent responsibility MAPE agent construct component monitoring analyze planning execute component data transaction knowledge database component relationship detail monitoring module comprises input queue data checker duplicate request checker analyze phase MAPE loop primary responsibility component analyze input former component comprise splitting module encode module injection module manipulation module outlier manipulation module module planning information former phase propose algorithm phase  MAPE loop responsible decision execute request locally remotely decision influence performance propose dnn hmm model decision maker action phase planning local execution request exist schedule algorithm planning offload request lack resource local mobile noteworthy request communication medium request offload planning appropriate uplink transmission medium exist utilize hmm model image KB image MAPE loop structure decision violates SLA request ESG investigation phase execute phase MAPE loop responsible execute decision component enqueues incoming execution visualizes finally destination aware decision request destination knowledge information fulfill related responsibility MAPE knowledge database activity mention earlier SLA DB sub component user expectation respect predefined SLA sub component date analyze module metric security latency availability reliability utilization fault detection protection throughput SLA user expectation research consumption delay offload metric judge decision SLA resource DB module resource categorize information related resource software hardware constraint limitation load balancer load balancer another component provision manage load destination predefined strategy component avoid network congestion prevent resource contention fairly resource dedication ecosystem component resource fulfil request define destination local load balance respectively enhance predefined metric depict software service hardware resource input request appropriately hardware resource appropriate software service available provision suitably resource service decrease consumption enhances latency sequence diagram propose framework subsection interaction amongst entity propose framework depict ESG loop incoming request smart mobile device ESG request status agent agent aware request ESG upon ESG request monitoring module MAPE loop monitoring module input queue alarm arrival request fulfil analyze module planning module perform prediction destination decision local mobile execution execution execution request input queue related constraint mention request addition component MAPE loop interaction knowledge module finally agent information execution ESG image KB image sequence diagram propose framework formulation related equation essential metric delay propose dnn hmm model equation offload decision uplink selection decision described critical variable depict description propose framework variable  description computation delay server slot request delay local execution delay remote execution available capacity server request delay request delay request local execution remote execution decision request execute locally decision request execute decision request execute delay model computation delay define execute task computation intensive task execute locally execution cpu frequency server computation delay server slot request formulate computation computation intensity cycle cpu frequency server cycle  request execute locally vms vms incurs extra delay denotes VM request delay summation delay vms therefore delay local execution formulate specifically goal delay efficient delay equation minimize fulfill minimum appropriate vms request execute remotely delay summation delay incur request delay incur remote server execution delay incur response denote allocate bandwidth request nearby server respectively server server denote transmission delay request data formulate offload request requester remote server requester location migration incurs extra delay handover request server server equation partial offload code  incurs extra delay accord discussion delay service formulate generally delay efficient strategy execution request minimize equation noteworthy delay incoming request predict apply dnn model layer layer input hidden layer prediction output consumption model offload MEC environment request execute locally consume locally proportional consumption per cpu cycle cpu cycle per task request request execute locally vms vms incurs extra consumption denotes VM delay summation consume vms execute request therefore consumption local execution formulate remote execution consume server formulate node denote transmission request data transmission rate respectively transmission rate formulate denote signal ratio bandwidth wireless channel respectively noteworthy define consumption request formulate consume fragmentation request respectively generally efficient strategy execution request minimize equation noteworthy consume incoming request predict apply dnn model layer layer input hidden layer prediction output dnn offload model propose dnn model offload decision layer layer input hidden layer decision output binary offload decision request denote local execution execution execution execution server offload decision aim minimize delay consumption execute request limitation mention equation therefore decision consumption delay computation optimization constraint incoming request constraint server constraint offload decision notably constraint incoming request sensitive policy handle request offload request local capability sometimes application couple byte data environment obviously request SLA assessment objective function linear related variable integer decision offload choice binary hence mention optimization function mention restriction mixed integer program binary program inherently NP relaxation mip apply dnn model NP complexity apply dnn model lemma apply dnn complexity offload generally rational proof definition dnn model module complexity dnn model comprises feedforward propagation algorithm activation function backward propagation calculate loss function discus feedforward propagation data training independent input feature neuron layer layer output complexity neural network generally demand operation massive complex network notably output usually neglect mention complexity usually neglect propagation algorithm activation function complexity mention  gao scikit access backward propagation iteration epoch calculate loss function discussion complexity backward propagation noteworthy choice cpu gpu soften burden convergence acceptable cpu processing resource dnn model discussion complexity dnn model hmm uplink selection model decision transmission medium predict preliminary information monitoring module uncertain incoming request selection appropriate uplink transmit influence uncertainty wireless channel user mobility uncertainty decision therefore apply hmm predict relationship observable medium hidden accord historical information predict resource management achieve predefined goal consumption delay noteworthy incoming request poison distribution arrival rate besides release resource exponential distribution rate respectively denote observable hidden hidden attach observable observable define execute request local mobile environment hidden define uplink transmission medium bluetooth version bluetooth version GSM depicts ergodic model hmm hidden sequence decision outcome navigate hidden probability distribution transition matrix transition unknown parameter image KB image sequence hidden propose hmm previous discussion transition matrix equation mention  russell kera access nov emission matrix accord discussion hmm model define model observation sequence hmm model generally described initialization efficient probability calculate probability variable apply decode observation sequence model optimal sequence viterbi algorithm generally apply training model parameter model optimize maximum likelihood ML probability likelihood sequence generally baum welch algorithm optimal selection model parameter recursively algorithm backward algorithm extra variable define besides previously define backward variable lemma complexity propose hmm model appropriate uplink medium scenario rational proof lemma hmm model module complexity hmm module algorithm viterbi algorithm backward algorithm complexity algorithm hidden observation sequence propose algorithm propose algorithm algorithm propose autonomous offload algorithm comprise MAPE module described detail subsection firstly request arrives input monitoring execute fulfil monitoring module primary responsibility monitoring incoming request input computation paradigm input queue input request checked respect data mismatch handler appropriate alarm request ESG possibility duplicate request checked finally preprocessed data analyze phase algorithm autonomous offload algorithm autonomous offload algorithm input monitoring phase monitoring phase queue input data data respect incoming request correctly integer float duplicate request  preprocessed dataset analyse algorithm  transact knowledge  analyse algorithm  analyze monitoring phase  algorithm  planning analyze phase execute phase  request execute locally request   request execute request  request   visualize analyze preprocessed information previous phase analyze module incoming request rate poison distribution distribution apply predict future request alleviate overload advantage exist resource analyze algorithm depict algorithm input dataset split training data data training data analyze data phase data encode handle compiler input compiler inject sin gaussian function manipulation occurs unfilled input request outlier detection envision compilation finally knowledge usage future algorithm pseudocode analyze analyse algorithm  split input dataset split dataset training data data encode training data handle digitize  training data input   training data empty  training data remove outlier training data compile training data data planning algorithm  training data transaction knowledge return analyze data planning planning execution input request planning module primary responsibility brain dnn multiple linear regression hmm model phase decision model multiple linear regression model utilized enhance depict algorithm algorithm model fulfill responsibility algorithm pseudocode planning planning algorithm analyze data training dnn model optimize equation dnn model training data multiple linear regression model calculate probability matrix hmm model define observation sequence hmm model training data prediction dnn predict delay data prediction hmm predict uplink medium data hybrid model deviation calculate deviation SLA apply equation hybrid model equation  equation execute phase  input request  transaction knowledge discussion complexity complexity propose algorithm algorithm implement python internal library numpy panda sklearn utilized library generally input data hash complexity access mention delete operation complexity request arrival poisson distribution linear complexity monitoring analyze module complexity command linear complexity epoch data training feature neuron layer layer complexity neural network generally subsection propose hmm prediction model complexity therefore hmm prediction model effective dnn prediction model uplink medium selection prediction besides sklearn library exploit model multiple linear regression observation complexity multiple linear regression  gao scikit access therefore overall complexity planning module demand prediction model overcome application constraint consideration finally command execute module request arrival module linear complexity noteworthy request arrival rate increase choice gpu tpu utilized python library faster cpu overall complexity remains acceptable consume experimental evaluation computation offload propose conduct prediction benefit drawback offload depict metric pure local execution pure execution pure execution beneficial offload execution   pce  respectively essential experimental parameter apply experimental parameter parameter request mobile device cluster cpu rate mobile device ghz cpu rate server ghz processor cpu rate server ghz multiprocessor transmission consumption consumption execution consumption giga cycle uplink downlink data rate mobile device uplink downlink data rate smart gateway   GSM intensity mobile device intensity server intensity server relay acceptable cpu utilization SNR simulation environment python utilized simulation essential library numpy sklearn kera panda exploit predict essential metric offload decision uplink decision selection request latency request consumption visualize related offload metric eighty percent input dataset dedicate training purpose remain percent utilized model accuracy visualize offload decision ann hidden layer output layer activation function relu sigmoid respectively initializer uniform distribution relu activation function input layer hidden layer output layer ann model visualize offload delay consume analysis utilized loss function error mse optimizer stochastic gradient descent noteworthy behavior various apply model identical delay analysis avoid duplicate explanation model delay analysis subsection detail therefore subsection explanation sufficient consumption analysis offload decision analysis subsection decision analysis offload accuracy depict offload accuracy converge iteration promising complex input data accuracy loss initializer converge zero acceptable convergence model tune input data appropriately preprocessing monitoring analyze module define suitably related parameter dnn model image KB image accuracy offload decision image KB image loss offload decision delay analysis depicts execution delay environment incur delay depends user request resource request execute necessarily minimum delay exist environment justify scenario request appropriate local execution resource communication strategy request processing image KB image delay execution environment addition simulation multiple linear regression model depict regression model outperforms dnn model output simulation identical therefore hybrid optimize simulation simulation model data dnn model regression model hybrid model depict depict compress clarity comparison data illustrate respectively image KB image delay model image KB image model delay image KB image model delay image KB image model delay depict various model perform differently predefined interval regression model association dependent independent variable incapable model fluctuation input data trend regression model around zero model negative fluctuation around calculate acceptable related metric critical application besides regression model incapable appropriate forecasting fluctuation incoming request mention challenge belong dynamic heterogeneous offload environment consequently extra incur execution dnn model delay model outperforms multiple linear regression model propose hybrid model delay comparison amongst model depict depict dnn regression model almost perform apply execution delay appropriate linear regression model mention interval propose hybrid model delay comparison amongst model depict depict regression model almost outperforms dnn model contrast dnn model outperforms multiple linear regression model propose hybrid model depicts SLA violation multiple linear regression dnn hybrid model respectively conduct simulation python environment data pure multiple linear regression predict accuracy appropriate model alone pure dnn model predict accuracy appropriate multiple linear regression consideration specifically critical request apply hybrid model improve predict accuracy suitable model predict future behavior image KB image delay SLA violation regression dnn hybrid model consumption analysis subsection consumption analysis offload depicts simulation predict computation  request predict decision execute request locally remotely accuracy model depict model logic delay model feature depicts consumption execution environment incur consume highly depends user request resource request execute necessarily minimum consume exist environment fluctuation highly demand resource minimum consumption intuitive convergence model previous subsection image KB image comparison actual predict image KB image accuracy model without feature image KB image accuracy model feature image KB image consumption execution environment delay analysis simulation multiple linear regression model analysis depict regression model outperforms dnn model output simulation identical therefore hybrid optimize simulation simulation model data dnn model regression model hybrid model depict depict compress clarity comparison data illustrate respectively image KB image model image KB image model consume image KB image model consume image KB image model consume depict various model perform differently predefined interval delay analysis regression model incapable appropriate forecasting outcome simulated negative frequent fluctuation consequently extra incur execution dnn model delay model outperforms multiple linear regression model propose hybrid model comparison amongst model depict depict dnn regression model almost perform apply execution delay appropriate linear regression model mention interval propose hybrid model consume comparison amongst model depict depict fluctuation input data although dnn outperforms regression model satisfactory hybrid model performs predefined interval depicts SLA violation multiple linear regression dnn hybrid model respectively conduct simulation python environment data pure multiple linear regression predict accuracy appropriate model alone pure dnn model predict accuracy appropriate multiple linear regression consideration specifically critical request apply hybrid model improve predict accuracy suitable model predict future behavior image KB image SLA violation regression dnn hybrid model uplink selection decision analysis subsection uplink selection analysis offload uplink hmm training datasets clearly diagram plot observation sequence hence transmission emission matrix calculate training dataset fed viterbi backward algorithm mistake prediction uplink selection vertical depict graph accuracy depict graph linear graph axis bluetooth version bluetooth version GSM encode respectively image KB image uplink selection accuracy image KB image uplink selection accuracy image KB image uplink selection accuracy dnn parameter analysis achieve appropriate prediction simulate dnn model critical parameter define configure python environment precisely critical parameter optimization function loss function rate dnn model predefined optimization function adadelta RMSprop adam adagrad adamax sgd nadam besides predefined loss function absolute percentage error absolute error error logarithmic error cosine similarity huber cosh kera access nov depicts comparison amongst function various simulation depict efficient optimization loss function adam error respectively addition adadelta absolute percentage error marked mention function respectively dnn parameter analysis optimization  SLA  accepted loss function optimization function adam absolute percentage  absolute   logarithmic   cosh SLA  accepted prediction another important factor affect dnn accuracy seriously propose offload approach rate simulation conduct various rate appropriate parameter violation depict rate reduces accuracy offload image KB image SLA violation dnn model rate discussion subsection simulation briefly issue complementary summary simulation offload generally NP dynamic stochastic environment exploit heuristic meta heuristic approach generally approach attempt optimum reasonable practical computational parameter define initialize appropriately respect apply dnn appropriate definition parameter dnn converge accurately optimal offload decision prediction delay prediction consumption prediction couple iteration accord simulation depict correspond improve simulation hybrid combine dnn multiple regression model propose simulation propose model enhance accuracy predict discussion hmm model related initialization available uplink generally limited complexity calculate probability historical data linear claimed complexity optimal datasets extensive criterion uplink selection depict correspond hmm appropriately define accuracy issue subsection critical future research direction perspective offload mechanism generally issue offload criterion categorize topic management fault tolerance heterogeneity mobility scalability schedule partition communication uplink fault tolerance concept security feature respect fault tolerance ability specific service despite fault mention security threat critical role endanger tolerance security fault challenge weakly literature issue subsection critical attention propose conceptual model management handle feature without intervention despite significance related offload criterion concept comprehensively noteworthy issue offload criterion comprehensively reference information future scenario mobile user mobility characteristic freely another access consequently handover MEC manage continuously become importance neglect feature carefully deliver stable service user request future version future author hierarchical structure methodology reduce consumption incur execute request MEC management complexity structure feature consideration author future conclusion simulate brain complex structure exploit multi layer neural network appropriate scenario define generally explore computation environment efficient execute mobile user request challenge criterion achieve addition research propose cope management offload criterion address challenge related resource demand application autonomous computation offload framework MAPE loop methodology propose reduce dimensionality offload decision apply hybrid neural network dnn multiple regression model planning module offload delay consumption prediction hidden markov model hmm planning module uplink selection prediction mention MAPE loop methodology simulation propose model offload decision latency prediction consumption prediction uplink selection prediction maximum maximum performance