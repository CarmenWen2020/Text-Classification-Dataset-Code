recent rapid growth demand online service video platform become popular user demand latency quality service increase therefore allocate cache resource reasonably user request reduce latency consumption cooperative cache delay consumption balance sdn mobile compute propose propose cache firstly multilayer perceptron neural network predict video content request mobile user secondly objective function minimize delay consumption establish cache optimization model construct finally bound algorithm obtain optimal cache strategy meanwhile user seamless service migration ensure service continuity quality service dynamic service migration propose propose service migration firstly service migration express markov decision secondly service migration analyze service migration reward function construct finally obtain optimal service migration strategy propose cache algorithm effectively improve cache rate reduce backhaul traffic load average access delay moreover propose service migration algorithm effectively reduce service migration transmission improve rate service migration reduce average traffic consume migration keywords mobile compute sdn cache service migration introduction popularity various smart mobile device user demand ultra latency quality service increase compute storage resource mobile device limited contradiction compute intensive application resource constrain mobile device become increasingly prominent overcome limitation mobile compute MEC minimize network congestion improve resource optimization however challenge increase compute device MEC mobility heterogeneity privacy hide internal complexity MEC user software define network sdn hide complexity heterogeneous environment user network program function therefore mobile compute environment sdn introduction sdn MEC ensure efficient network operation service delivery meanwhile sdn overcome deficiency exist MEC resource coordination network orchestration reduce complexity mobile compute architecture rapid growth demand online service user demand latency quality service increase cache capacity server potential significantly improve network resource utilization efficiency however limited cache highly diversified content cannot completely server therefore allocate cache resource user request reduce delivery delay bandwidth usage improve quality qoe meanwhile limited coverage server mobility user uncertainty request challenge service deployment moreover server access migrate ongoing service service issue user outside service relevant server reasonably allocate cache resource user request reduce latency consumption cooperative cache latency balance sdn mobile compute propose propose firstly static dynamic user social characteristic video multi layer perceptron neural network predict video content request mobile user secondly objective function minimize delay consumption establish cache optimization model construct finally bound algorithm optimal cache strategy effectively migrate ongoing service ensure service continuity dynamic service migration propose propose firstly service migration express markov decision secondly service migration reward function construct finally obtain optimal service migration strategy contribution summarize allocate cache resource reasonably user request reduce transmission delay transmission consumption cooperative cache delay balance MEC environment sdn propose propose multilayer perceptron predict video content request user delay overhead overhead video objective function minimize delay consumption establish user seamless service migration ensure service continuity dynamic service migration propose propose service migration express markov decision MDP service migration reward function construct obtain optimal service migration strategy performance propose algorithm evaluate propose cache algorithm effectively improve cache rate reduce backhaul traffic load average access delay propose service migration algorithm effectively reduce service migration transmission improve rate service migration reduce average traffic consume migration structure review related cooperative cache delay consumption balance dynamic service migration evaluates implementation validates advantage approach finally conclusion future related cache cache extremely effective technique balance backhaul link load reduce network latency improve user achieve optimal balance user response delay service provider research limitation storage bandwidth cache scheme predict content request frequency cached peak proactive cache policy pcp cache replacement policy CRP cache video probability cooperative cache express NP integer linear program ILP software define wireless network environment propose quality qoe aware wireless cache scheme scheme reduce latency improve cache utilization express cache markov decision minimize transmission comparison related cache scheme  maximize cache  proactive cache policy pcp cache replacement policy CRP reduce latency improve cache  wireless buffering scheme quality qoe perception reduce data  cache transform markov decision MDP intelligently manage resource lease cache  prediction scheme predict popularity   understand request intelligent cache  predict content popularity optimize vehicle compute content placement content delivery  cooperative cache scheme  ensure cache rate reduce average access latency reduce backhaul traffic  cooperative cache delay consumption balance sdn mobile compute regard ideal complex dynamic network widely cache propose prediction scheme besides model scheme reinforcement propose suitable model framework helpful intelligent cache meanwhile propose understand request mode  propose mobile aware pre cache strategy network user pas future predict file user pre cached propose collaborative cache scheme jointly optimize vehicle compute content placement network besides propose heuristic deterministic policy gradient DDPG framework obtain suboptimal computational complexity cache although user access delay content factor comprehensive therefore cooperative cache delay consumption balance sdn mobile compute propose propose cache considers factor cache effective previous research advantage cache propose propose cache comprehensively considers user access delay uncertainty user behavior user request allocate cache resource reasonably user request propose multi layer perceptron neural network predict video content request mobile user static dynamic user social characteristic video video user quickly accurately obtain propose objective function minimize delay consumption establish achieve balance delay consumption resource server propose cache aim obtain optimal cache strategy premise ensure cache rate meanwhile propose effectively reduces average access delay backhaul traffic load service migration recent scholar abroad service migration related service migration roughly category model service migration markov decision model seamlessly migrate ongoing service data another optimal data  allows service across federate data DC user ongoing service host server migrate server propose  model algorithm markov decision adopt meanwhile software define network technology alternative ensure service continuity non disruptive operation propose introduce concept   framework ensures latency access service migrate service ensure user  environment propose mobility service migration prediction MSMP balance overhead quality qoe service migration strategy markov decision MDP dimensional MDP dimensional MDP model formulate service migration MDP model described mathematical framework service migration strategy propose qos aware service migration centric network service migration model MDP reduce migration network traffic qos guaranteed MDP model service migration balance user perceive quality relevant user equipment distance source data migrate service service migration strategy formulate summary various service migration scheme MDP   distance user server location overcome complexity obtain service migration  service migration markov decision MDP network qos resource utilization  target  service migration markov decision MDP distance user equipment source data  achieve balance user perceive  service migration markov decision MDP  migration server load capacity cpu utilization memory capacity network bandwidth disk rate etc ensure rate service migration ensure service continuity reduce service migration  service migration markov decision MDP research service migration compute environment although ensure continuity service extent migration consideration comprehensive MDP model conventional model service migration however MDP model factor related MDP service migration goal strategy maximizes reward initial sdn mobile compute environment dynamic service migration propose considers factor achieve seamless service migration previous research innovation service migration propose propose service migration comprehensively considers server load capacity transmission migration cpu utilization memory capacity network bandwidth etc analyze service migration construct service migration reward function thereby maximize reward ensure service continuity impact decision resource allocation MEC highly MEC optimization algorithm obtain optimal approximately optimal within slice agent perceives environment selects action environment feedback reward action algorithm considers impact decision resource allocation goal maximize cumulative reward dynamic service migration cooperative cache framework MEC environment sdn framework MEC environment sdn framework mainly infrastructure layer compute layer compute layer user terminal node compute server sdn controller data architecture MEC environment sdn data management application accord function data cornerstone framework abstract underlie network resource sdn switch management global controller mainly responsible manage local sdn controller sdn controller update data processing exchange operating status application consists application instance application client entity image KB image framework mobile compute sdn illustrate relationship research firstly reduce latency consumption collaborative cache delay balance sdn mobile compute propose multi layer perceptron predict video content request user objective function minimize delay consumption establish sdn controller obtains network status video content information etc optimal cache strategy obtain effective cache scheme cache technology request content server however mobile compute user mobility factor cannot ignore user service connection scheme longer optimal service quality sharply therefore dynamic service migration sdn mobile compute propose service migration express markov decision service migration reward function construct optimal service migration strategy cooperative cache delay consumption balance limited access server cache limited mobile device resource access delay excessive consumption cache delay balance sdn mobile compute propose firstly multilayer perceptron neural network predict video content request mobile user secondly objective function minimize delay consumption establish cache optimization model construct finally bound algorithm obtain optimal cache strategy model cache delay consumption balance image KB image architecture diagram cache delay consumption balance user content reading behavior prediction multi layer perceptron multi layer perceptron neural network predict user content reading behavior input output mlp described input input vector mobile user user feature input vector static feature dynamic feature social feature static feature title category description information video dynamic feature video playback video request social feature user rating comment video output mobile user input vector output vector video content request user collection video content request user output layer video content request mobile terminal user predict hidden layer input vector deviation vector matrix hidden layer output addition output layer rectify linear relu activation function backpropagation algorithm update connection optimization model collaborative cache transmission delay model content cache mobile user wirelessly server server data backhaul link user sends content request server content locally server content immediately user otherwise server content video provider data request user indicates server denotes limited storage capacity server server indicates user collection user request video content output user content reading behavior prediction multilayer perceptron assume user video content slice transmission delay MEC server obtain video content video provider express formula indicates video content exists MEC server video content MEC server average video content denotes transmission rate video provider MEC server transmission rate content MEC server user express formula user server allocate percentage bandwidth transmission user server denotes channel gain user server gaussian user server access delay content MEC server user formula indicates user content server user content server average video content therefore delay express sum transmission delay access delay equation consumption model content cache content video provider data cache server consume transmission transmission consumption MEC server obtain video content video provider express router core network network respectively consume data transmit core router router broadband network gateway ethernet switch indicates data content transfer transmission consumption content MEC server user express formula transmission user MEC server denotes transmission delay content MEC server user storage consumption MEC server depends extent content cached assume consumption MEC server cache data consumption MEC server cache content within express formula indicates video content exists MEC server average video content therefore consumption express sum transmission consumption storage consumption cache optimization model goal cooperative cache balance delay consumption seamless video service mobile user effectively cache content server effective cache scheme appropriate server cache video content average delay consumption user access content minimize express mathematically formula constraint ensures user content server allocate bandwidth user server allocate bandwidth constraint ensures content user server cached server constraint ensures user content server constraint ensures content cached server constraint ensures cache resource allocation available cache resource server constraint specifies variable parameter balance goal minimum delay minimum consumption objective function cache balance delay consumption mixed binary integer program mixed binary integer program usually NP optimal cannot obtain linear compute heuristic algorithm bound algorithm usually however heuristic algorithm complexity efficiency besides usually feasible deviation feasible optimal cannot generally predict easy local optimum bound algorithm global optimal average bound algorithm provable complexity algorithm complexity leaf cache suitable bound algorithm leaf bound convex optimization lagrange multiplier complexity therefore bound algorithm chosen optimal cache cooperative cache algorithm core pseudocode cache algorithm balance delay consumption introduce algorithm mainly user content reading behavior prediction algorithm collaborative cache algorithm user content reading behavior prediction algorithm multi layer perceptron core pseudocode user content reading behavior prediction algorithm multi layer perceptron algorithm firstly initialize BP algorithm obtain optimal obtain optimal neural network algorithm secondly user feature vector input obtain optimal neural network video content request user predict obtain algorithm finally video content request user output algorithm algorithm user content reading behavior prediction algorithm multi layer perceptron input feature vector user historical playback information output video content request user initialize BP algorithm obtain optimal neural network obtain user feature vector input neural network return video content request user cache algorithm delay consumption balance core pseudocode cache algorithm delay consumption balance algorithm accord algorithm content reading behavior user predict firstly global previous initialize split algorithm delay consumption calculate accord formula respectively lagrangian relaxation algorithm finally objective function checked objective function clipped optimal obtain finally content cache location obtain algorithm algorithm cache algorithm delay consumption balance input feature vector user historical playback information denotes MEC server video content cache output content cache location collection predict content reading behavior user accord algorithm obtain user video content collection initialize global upper bound respectively split obtain switch calculate delay accord formula calculate consumption accord formula lagrangian relaxation bound undetermined variable calculate delay calculate consumption lagrangian relaxation bound  integer upper bound random feasible upper bound endif endif remain video content return content cache location collection propose cache algorithm delay balance MEC environment sdn multi layer perceptron predict video content request user node input layer output layer mlp neural network constant respectively assume hidden layer training sample complexity training multilayer perceptron neural network model denote suppose mobile user therefore complexity predict reading behavior user express bound cache algorithm user content reading behavior complexity bound algorithm exhaustive provable complexity attention complexity leaf leaf bound algorithm convex optimization lagrange multiplier complexity dynamic service migration issue limited coverage server user terminal mobility ensure service continuity user dynamic service migration propose firstly service migration express markov decision secondly service migration analyze service migration reward function construct finally obtain optimal service migration strategy architecture diagram dynamic service migration besides customize information mobility information custom attribute information access load computation load server etc meanwhile custom attribute mobility information source address destination address traffic etc image KB image architecture diagram dynamic service migration migration model migration model markov decision service migration model markov decision overall goal propose dynamic service migration optimal strategy maximize reward specific decision described action location information user service define formula user location status service location status respectively denotes entire slot migration initial slot action define action migration decision decision indicates location user assume limited user remain fix duration slot slot accord markov mobility model slot model regard sample version continuous model sample perform unequal interval besides assume user location associate MEC server calculate distance location distance euclidean distance hop cellular network another user location service location slot respectively slot strategy MEC server migrate service location location incur migration assume non decrease function distance migration status migration service interruption migration whenever migration occurs non zero interruption propagation switch delay data transmission increase interruption increase migration distance migration service migration addition migration exists data transmission user currently active service instance transmission related distance service user migration non decrease function transmission latency data transmission latency increase service response accord reference delay usually function geographic topological distance node increase distance assume transmission delay service interruption migration slot slot reward function goal propose service migration obtain service migration strategy maximize server load capacity minimize transmission migration server load capacity reward therefore reward function define formula denotes load capacity server function denote define sum migration transmission perform action express indicates distance service indicates migration action denotes distance service user perform action migration transmission define constant plus exponent express mathematical parameter non decrease function service quality migration transmission MEC server user choice however insufficient storage excessive load target server therefore load capacity server regard important factor affect service migration decision preferentially server load capacity target server service migration factor cpu utilization memory capacity network bandwidth disk rate comprehensively load capacity server express formula denotes impact various indicator server load capacity cpu utilization reflect server cpu memory occupancy rate denotes percentage memory memory memory occupancy rate memory load server memory load capacity network bandwidth occupancy rate refers percentage bandwidth bandwidth bandwidth occupancy rate idle network disk rate reflect throughput server data operation cpu utilization memory occupancy rate network bandwidth occupancy rate disk rate server express load status server cpu processing capacity memory usage network bandwidth consumption disk capability usually fix denote cpu processing capacity memory capacity network bandwidth disk capability server respectively objective function service migration model goal propose service migration policy maximizes reward initial express mathematical formula reward generate strategy initial formula denotes discount factor reward function performs action maximization bellman equation express denotes transition probability dynamic service migration neural network predict profit action reinforcement neural network obtain approximate action function parameter vector vector neuron network update traditional update training neural network therefore service migration service migration propose service migration strategy migration action maximize future discount reward network approximate optimal action function express denotes maximum reward action immediate reward function discount factor denotes expectation function neural network obtain approximate action function decision cycle user vector input network obtains action output user selects action accord strategy loss function express formula loss function minimize stochastic gradient descent iterate network specify loss function update besides algorithm empirical playback technology training instability neural network due nonlinear approximation function dynamic service migration algorithm core pseudocode dynamic service migration algorithm MEC environment sdn algorithm firstly memory matrix target network parameter etc initialize input network output correspond action return algorithm greedy strategy action action execute reward user information memory matrix algorithm finally sample randomly neural network minimize loss neural network target neural network interval exploration algorithm algorithm dynamic service migration algorithm input iteration action attenuation factor output optimal service migration strategy initialize memory matrix initialize action function random initialize target action function initialize service migration initialize sequence iterate input output correspond action network action strategy random action optimal action endif calculate reward function accord formula calculate obtain user information memory matrix randomly sample memory matrix calculate target terminate endif calculate loss function accord formula neural network accord loss function update parameter network gradient backpropagation neural network reset endif   return performance evaluation experimental environment experimental setup experimental environment mobile compute sdn compose remote server sdn controller node server hardware platform experimental environment mobile device node server server vswitch software network access device server parse openflow protocol vswitch instal deployed ubuntu platform python environment pox controller docker instal deployed ubuntu operating KS dashboard instal display status information cluster experimental environment architecture detailed configuration information equipment image KB image experimental environment architecture diagram experimental benchmark data video data benchmark data propose cache algorithm video data publish virtual reality data contains video characteristic youtube video video duration min VR video resolution frame per fps sample information data VR video information data   MB duration min video link NI  coaster http youtu   roller coaster http youtu lsb  http youtu  NI   http youtu  peril panel http youtu  kangaroo http youtu  SFR sport http youtu  hog rider http youtu  CG  http youtu   http youtu  apply exist computer vision CV algorithm video improve performance video evaluate performance service migration algorithm propose recognition application application youtube DB data recognition application dataset contains video video youtube topic average video shortest duration clip frame clip frame average video clip frame evaluation metric experimental parameter evaluation metric cache rate average access delay backhaul traffic load consumption performance evaluation indicator evaluate feasibility propose cooperative cache algorithm cache rate important indicator cache performance define ratio content server content request average access delay refers average delay content server origin server request user average access delay transmission delay access delay express formula backhaul traffic load refers data traffic backhaul network user cannot obtain content node video server reflect resource consumption network consumption mainly transmission consumption storage consumption formula service migration average traffic consume migration rate service migration performance evaluation indicator evaluate feasibility propose dynamic service migration algorithm service migration service migration user submits request request satisfied migration transmission express formula formula respectively average traffic consume migration refers network traffic consume service migration service migration rate define ratio service migrate deadline service migrate experimental parameter experimental parameter cooperative cache algorithm delay balance video content library virtual reality video virtual reality video duration min randomly intercept average data definition video content MB experimental parameter  video content  resource user equipment ghz cache capacity MEC  mobile user compute resource MEC  cache experimental parameter dynamic service migration algorithm introduce user equipment within evenly distribute within associate unique MEC server multiple mobile user within coverage server accord uniform dimensional random model meanwhile user another location location probability location user network bandwidth user movement rate mobile user network bandwidth benchmark algorithm cache strategy lru pbc  LCCA comparison algorithm recently algorithm lru assumes currently request content likely request within content cached recently access content delete cache popularity cache algorithm pbc account user behavior request request content predict cached peak joint optimization cache algorithm  considers service quality resource constraint bandwidth provision cache strategy optimize software define mobile network benefit gain dynamically manage cache effectively network resource maximize collaborative cache algorithm LCCA cache mechanism cache rate maximize transmission cache capacity minimize return traffic transmission delay remote reduce service migration strategy myopic mig RL comparison algorithm greedy strategy minimum hop strategy terminal device outside service service migrate server myopic myopic strategy migration decision service quality observation selects correspond migration action minimize slot mig RL reinforcement algorithm service migration issue compute environment service minimize service quality improve analysis experimental verification cooperative cache algorithm delay consumption balance impact relative cache experimental cache important parameter evaluate efficiency cache relative cache introduce reflect scarcity cache relative cache define ratio server cache video library content relative cache performance parameter variable relative cache mobile user average image KB image impact relative cache experimental image KB image impact mobile user respectively cache rate average access delay backhaul traffic load consumption relative cache cache rate cache strategy increase increase relative cache relative cache server increase cache capacity cached video increase probability user request node increase therefore cache rate improve relative cache propose algorithm increase average lru algorithm average pbc algorithm average  algorithm however LCCA algorithm cache rate cache mechanism LCCA algorithm aim maximize cache rate minimize transmission cache capacity average access latency backhaul traffic load consumption cache strategy decrease increase relative cache relative cache server increase server request content user obtain content directly server without video content remote data therefore average access latency backhaul traffic load consumption reduce relative cache average access latency propose algorithm reduce average lru algorithm average pbc algorithm average  algorithm average LCCA algorithm backhaul traffic load propose algorithm reduce average lru algorithm average pbc algorithm average  algorithm average LCCA algorithm consumption propose algorithm decrease average lru algorithm average pbc algorithm average  algorithm average LCCA algorithm impact mobile user impact mobile user performance parameter variable adopt mobile user respectively variable relative cache video library average respectively cache rate average access delay backhaul traffic load consumption mobile user cache rate cache strategy decrease mobile user increase mobile user increase request content increase however relative cache remains server fix content therefore cache content cache rate decrease mobile user algorithm propose increase average lru algorithm average pbc algorithm average  algorithm LCCA algorithm aim maximize cache rate LCCA algorithm cache rate algorithm propose average access latency backhaul traffic load consumption cache strategy increase mobile user mobile user increase content request user increase cache reduce obtain cache remote server therefore average access delay backhaul traffic load consumption increase mobile user average access latency propose algorithm reduce average lru algorithm average pbc algorithm average  algorithm average LCCA algorithm backhaul traffic load propose algorithm reduce average lru algorithm average pbc algorithm average  algorithm average LCCA algorithm consumption propose algorithm reduce average lru algorithm average pbc algorithm average  algorithm average LCCA algorithm experimental verification dynamic service migration algorithm impact mobile user impact user performance parameter variable adopt mobile user network bandwidth mbps average image KB image impact mobile user service migration average traffic consume migration service migration rate mobile user increase user service migration average traffic consume migration strategy increase user increase service migrate correspondingly increase amount data migrate increase service migration average traffic consume migration increase mobile user service migration propose algorithm reduce average algorithm average myopic algorithm average mig RL algorithm propose algorithm reduce average algorithm average myopic algorithm average mig RL algorithm average traffic consumption propose algorithm decrease average algorithm average myopic algorithm average mig RL algorithm user increase migration rate service migration strategy decrease increase user application program execute MEC server increase however due resource limitation service fail migrate therefore rate service migration reduce mobile user service migration rate propose algorithm increase average algorithm average myopic algorithm average mig RL algorithm impact network bandwidth experimental impact network bandwidth performance parameter variable adopt network bandwidth mbps mbps mbps mbps mbps mobile user average image KB image impact network bandwidth experimental respectively service migration average traffic consume migration rate service migration network bandwidth network bandwidth increase service migration average traffic consume migration strategy decrease network bandwidth increase transmission data faster therefore service migration reduce service migration reduce average traffic consume migration reduce network bandwidth mbps service migration propose algorithm decrease average algorithm average myopic algorithm average mig RL algorithm propose algorithm reduce average algorithm average myopic algorithm average mig RL algorithm average traffic consumption propose algorithm decrease average algorithm average myopic algorithm average mig RL algorithm rate service migration migration strategy increase network bandwidth increase network bandwidth increase service migration reduce therefore rate service migration increase network bandwidth mbps service migration rate propose algorithm increase average algorithm average myopic algorithm average mig RL algorithm conclusion future sdn mobile compute environment allocate cache resource reasonably user request reduce latency consumption cooperative cache delay consumption balance propose propose firstly multilayer perceptron neural network predict video content request mobile user secondly objective function minimize delay consumption establish cache optimization model construct finally bound algorithm obtain optimal cache strategy user seamless service migration ensure service continuity quality service dynamic service migration propose propose firstly service migration express markov decision secondly service migration analyze service migration reward function construct finally obtain optimal service migration strategy future sdn artificial intelligence combine research cache strategy service migration meanwhile mobile user increase expand