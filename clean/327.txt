runtime environment iot data processing actor model apply thread pool data propose approach reinforcement RL resource thread pool server machine usage quality service data approach thread pool executor akka source software toolkit simulation approach outperforms  timeout thread negligible furthermore tune approach tedious application timeout previous keywords iot actor resource management reinforcement introduction experienced significant networking device global network infrastructure internet iot gartner predict iot exceed billion hardware spending consumer business trillion USD regard iot technology RFID tag attach origin data vehicle vehicle vehicle infrastructure communication utilized avoid congestion reduce incident sensor monitor vehicle health specific environmental parameter data iot device implement actor model program paradigm akka toolkit framework built jvm development actor actor distribute compute handle highly concurrent iot data hierarchical structure actor directly mapped relationship iot device actor akka thread thread pool performance iot application heavily depends thread pool static thread pool trial error operator increase decrease thread measurement dynamic environment traffic akka allows thread pool dynamically timeout akka thread pool manage entity operator timeout parameter adjust traffic however expert knowledge tedious trial error motivate resource management propose approach reinforcement RL thread pool minimize resource usage maintain qos contribution define threshold multi objective reward reinforcement considers service qos thread pool efficiency population training grid combination hyperparameters RL algorithm hyperparameters influence optimal operation stability algorithm performance RL algorithm timeout akka actor framework simulation thread instantaneously RL algorithm performs similarly timeout thread negligible RL algorithm performs timeout organize overview related literature scenario actor built iot approach review RL algorithm finally contains conclusion future direction related resource management iot optimize resource central lately resource management involve iot gain focus investigate resource manage capability various operating iot resource management network slice 5G iot application slice enhance rider optimization algorithm optimal node iot cluster memory lstm predict resource usage smart grid furthermore numerous publish fog compute resource management iot application  huh devise dynamic pricing algorithm resource usage estimation fog compute construct framework fog fog iot resource provision algorithm framework formulate service distribution iot linear program thread pool approach thread classify reactive proactive policy reactive policy action respond environmental whereas proactive approach predicts adjusts thread accordingly  service load continuously monitor thread various thread pool adapt dynamically accord load adaptive algorithm idle thread thread pool load thread pool load average exponential scheme predict thread thread thread timeout chen lin FCFS queue model estimate queue adjust thread pool middleware dynamically trendy exponential average scheme predict worker thread queue model adequate minimum thread pool garc√≠a  identify thread pool critical resource manage middleware application author propose interception layer middleware operating upper bound thread pool reinforcement resource management machine ML specifically reinforcement RL excellent alternative manage resource RL technique markov decision mdps RL agent interacts environment described MDP learns optimal action maximize reward RL algorithm policy gradient around decade however attention due hardware development computationally demand function approximation technique neural network dnn combine dnn RL refer reinforcement DRL recently various feat achieve DRL machine atari DRL numerous practical application consumption management resource management robotics investigate RL resource provision task schedule algorithm service provider goal minimize consumption network DQN stage action selection RL algorithm outperform heuristic baseline rent demand reserve virtual machine dynamically SaaS provider    auto algorithm sarsa fuzzy web application define workload virtual machine profit reward simulation testbed argue exploit domain knowledge approach tackle entirely model manner dealt network function combine gaussian estimate future avoid execute erroneous action introduce baseline reduce variance environment random input load balance task bitrate adaptation summarize RL related computer framework training RL algorithm review ML approach security iot attack identify research challenge application ML security iot propose drift adaptive RL algorithm schedule allocate resource iot application however performance iot application approximate queue knowledge dealt application RL manage resource actor description implementation iot application actor model akka source toolkit development distribute application computational actor model building actor application connects iot device manages session sensor data server employ actor actor communicate message upon message actor finite message actor address spawn actor handle message actor execute amount action furthermore action asynchronous manner enable highly concurrent actor model building distribute convenient developer hide away thread lock significantly impact iot performance focus akka actor concurrency worth emphasize akka iot platform data iot device  iot platform secure connection iot device groundwork application analytics outline simplify architecture iot platform  iot device data outside message queue telemetry transport MQTT protocol connection establish session manager actor creates session actor manage iot device session meanwhile device manager actor device actor incoming data persistence analytics image KB image iot platform thread akka akka runtime environment execution distribute application java virtual machine akka documentation management resource mapping actor thread operating resource  java java TM platform akka coordination processing message dispatcher akka documentation actor message message queue dispatcher actor assign thread message thread mapped cpu core scheduler operating relationship actor thread pool cpu actor available thread actor thread similarly thread cpu core core thread scheduler assignment execution actor code  akka executor   former default executor utilizes static fix thread pool developer configure manually latter thread pool dynamically demand akka scala jvm executor identical described java related literature image KB image dispatcher akka assigns thread akka actor mapping thread cpu core operating scheduler default  timeout policy java TM platform thread demand task arrives thread thread thread pool task thread becomes idle another task thread remains inactive specific amount timeout alive thread terminate capacity allows pool traffic traffic default timeout akka thread traffic load iot device connects actor manage session another actor data device data processing actor assume thread cpu actor negligible theoretically limit actor spawn execute task simultaneously however performance highly depends dispatcher thread pool beneath actor thread actor unfortunately cannot increase thread pool  thread cpu core thread increase context switch furthermore idle thread resource therefore crucial developer challenge optimal thread application suppose session randomly rate denote session limited maximum session depends capacity physical machine session iot data packet fix  actor assume session random iot device disconnect session related actor remove processing packet session expire session necessarily actor lifetime notation thread pool environment arrival rate session session thread thread maximum session maximum thread  packet session service packet service packet thread cpu core capacity actor message queue denote thread actor thread packet thread booting initialization processing actor message idle initialize currently denote thread applicability RL technique assume packet service packet fix thread limit service task increase due thread resort cpu resource cpu core actor thread cpu core actor assign thread available thread pool thread assign cpu core execute actor task thread assign actor robin manner dispatcher actor thread assignment thread dispatcher assigns actor packet actor thread becomes idle dispatcher assigns actor thread upon arrival packet packet actor message queue assume limit packet message queue incoming packet thrown away packet upon arrival thread thread pool dynamically dispatcher thread upper bound configuration file depends capability physical machine assume termination thread instantaneous satisfy specific qos requirement sufficient amount thread pool  operator tweak timeout adjust thread pool indirectly alters qos performance timeouts thread qos whereas timeout thread degrades qos tedious task operator qos requirement furthermore timeouts otherwise thread goal efficient thread pool unfortunately cannot assume traffic constant optimal thread throughout lifetime notation thread pool formulation apply RL formalize markov decision MDP tuple action describes transition probability describes immediate reward transition discount factor notation MDP MDP action MDP reward function MDP discount factor MDP decision gap decision policy function function policy action reward approximate arrival rate approximate approximate rate qos threshold qos threshold rate reward multiplier hyperparameter MDP framework agent interacts timestamp agent observes environment decides execute action agent receives reward feedback observation periodically timestep agent decides accord policy mapping probability distribution action agent goal optimal policy maximizes reward advantage RL model RL agent posse knowledge environment agent transition probability function optimal policy critical mdps RL define action reward function description information essential decision however compact RL agent explore reasonable similarly action easy RL agent limit lastly reward function RL agent desire operating optimize usually task poorly define reward RL agent stuck unwanted observation agent compile representation definition crucial RL variable explode unnecessarily elongate training whereas variable impossible agent differentiate otherwise distinct tuple active session thread booting idle thread thread thread pool arrival rate previous upper limit session thread respectively denote agent action decision agent observes action probability describes action agent immediately execution action necessarily identical action denote action scenario action thread define action thread initialize thread pool idle thread thread pool thread obviously action action selection apply cannot thread cannot thread cannot multi action advantage action easy explore RL algorithm converge faster however leaf freedom operator react precisely thread therefore another action multiple thread define action thread decision agent terminate multiple thread thread pool action thread obviously action agent flexibility adjustment multi action scenario refers terminate multiple thread action execute multiple action decision reward function reward function describes RL agent objective optimize decision agent receives immediate reward calculate performance previous decision calibrate thread pool minimize resource usage maintain quality service qos multi objective optimization minimize thread thread pool adhere constraint packet threshold tackle aggregate component reward challenge task component numerical multi objective reinforcement motivation threshold objective RL algorithm provision qos application processing requirement qos requirement message iot data rationale qos provision processing data sensor monitor critical processing data greenhouse critical reward function inspire threshold algorithm rate packet previous decision threshold qos parameter simply multiplier issue without numerically outweigh expectation reward intuition reward function threshold minimize maximize however exceed threshold minimize thread resource maximize reward negative interpret reward function  function objective cannot adjust qos requirement sum reward reward function sensitive intuitive reward function notation MDP reinforcement reinforcement optimal policy maximizes cumulate reward trajectory notation algorithm  training policy neural network parameter vector neural network parameter vector reward epoch batch generalize advantage estimator generalize advantage estimation parameter entropy function entropy multiplier coefficient policy function rate function rate reward average factor clip ratio  parallelism population training population training repetition reward multiplier perturb factor entropy multiplier perturb factor reward multiplier perturb entropy multiplier perturb grid notation compute expectation MDP policy function signify expectation discount reward policy optimal policy model RL agent integrate  thread pool setup thread thread pool statistic message delay thread traffic load etc monitoring service interpret observation calculate reward RL agent terminate thread thread pool policy information reward agent improve policy algorithm image KB image interaction RL agent thread pool proximal policy optimization ppo optimal policy actor critic algorithm accurate stable ppo algorithm framework algorithm framework simulation training display interaction agent simulation environment training amount agent executes action action probability action reward improve policy image KB image policy function parameterized vector probability distribution action output vector denote vector action sample action distribution policy proximal policy optimization algorithm ppo algorithm RL agent policy interact environment adjusts parameter vector training optimal policy implement non linear approximator neural network aside policy algorithm function estimate function approximate neural network parameter vector policy function function neural network input former output vector whereas latter algorithm algorithm estimate reward update simulation update update generalize advantage estimation GAE estimate advantage reward scheme modify average reward scenario task regularization function bold signify vector index component vector rate neural network approximate policy function coefficient entropy hyperparameter reward average factor GAE hyperparameter implicitly contains discount factor clip ratio ppo vector probability ratio action elementwise vector notation signifies probability action policy parameterized image KB image hyperparameters hyperparameter tune costly operation machine expensive RL training data comparison tune iteration popular grid random former hyperparameters fix candidate training combination latter hyperparameters sample randomly training hyperparameters literature default recommend sufficient identify hyperparameters algorithm sensitive reward coefficient entropy apply grid candidate later population training PBT PBT hyperparameter evolutionary algorithm principle multiple simulation instance parallel executes exploitation exploration simulation RL agent hyperparameters tune algorithm evaluates agent exploitation sort accord performance agent replaces agent exploration phase hyperparameters replacement slightly perturbed finally training resume another PBT identify candidate grid picked hyperparameter performance algorithm contains hyperparameter PBT rank agent performance sort sort increase idle thread otherwise sort increase perform agent agent qos idle thread random sample variable uniform distribution perturb agent finally perform respectively grid combination image KB image PBT involves continuous training initialize agent environment reset training exploitation exploration however grid training perform combination hyperparameters chosen performance average introduces hyperparameters however hyperparameters easy training sensitive notation RL investigation simulated environment evaluate timeout RL advantage multiple action RL timeout RL outperform timeout simulation setup assume session accord poisson arrival rate reasonable assumption iot device fix arrival rate another arrival rate fix arrival traffic scenario traffic scenario arrival rate equation borrow purpose arrival rate comparable fix arrival rate worth mention function mobile network data however traffic activity therefore suitable iot assume session distribute exponentially maximum session session arrives actor spawn message queue lifetime session sends packet inter arrival service packet service packet assume cpu core thread service however exceeds available cpu core contention cpu slot service processing packet immediately arrival queue empty otherwise stash queue fifo manner bound message queue limited queue worker thread processing packet thread pool worker session thread robin manner maximum thread pool furthermore minimum thread pool RL terminate thread explore assume thread happens immediately however comparison timeout RL fix arrival rate timeout timeout fix arrival rate simulated performance RL apply investigate multi action action selection alternative reward function threshold delay threshold qos requirement qos requirement neural network NN approximate policy function NN input layer accept vector fully hidden layer activation function rectify linear relu layer policy function former output layer fully hidden layer node action convert probability sigmoid activation function layer node fully hidden layer latter activation function training hyperparameters training evaluate RL agent thread training multi action scheme RL agent converge optimal policy action selection scheme action around whereas multi action converge multi action action selection involves action action scheme simulation timeout scheme ppo agent displayed thread idle thread packet chose omit rate seldom due message queue image KB image thread training AC algorithm environment hyperparameters ppo  neural network hidden layer hidden node function rate policy function rate reward average batch epoch clip parameter GAE parameter PBT parallel instance par PBT iteration PBT training PBT perturbation PBT perturbation grid timeout thread RL minimize idle thread comparison timeout ppo timeout  thread  idle  RL ppo  thread  idle  multi multi comparison timeout ppo timeout  thread  idle  RL ppo  thread  idle  multi multi comparison timeout RL arrival rate arrival rate sine function training simulation evaluate RL agent simulation graph training evaluate arrival rate RL multi action selection perform action agent threshold thread react another probability action cannot zero entropy regularization prevents degenerate distribution action selection consequence mistake agent multi action thread easy action restrict action reduce error ppo agent RL slightly timeout threshold timeout idle thread slightly however strict decrease resource usage easily RL timeout timeout threshold thread comparison timeout ppo timeout  thread  idle  RL ppo  thread  idle  multi multi performance timeout scheme timeout idle thread whereas threshold decrease timeout decrease timeout however improvement timeout thread shortly incoming packet without idle thread thread thread timeout scheme obvious decrease timeout increase however threshold sooner ideal timeout threshold accord algorithm binary ideal timeout comparison ppo agent environment action multi action algorithm hyperparameters action agent threshold minimize idle thread RL agent maintain threshold multi action perform timeout however minimize idle thread action due action agent explore entropy regularization prevents zero probability action another timeout scheme thread qos requirement RL agent timeout scheme  thread  idle  ppo agent action thread  idle  ppo agent multiple action thread  idle  RL performs evaluation phase idle thread maintain threshold decision thread decision timestep recent decision immediate impact nevertheless RL delayed reward function denote contains reward recursively contains function endless recursion guarantee later reward timeout timeout guarantee however idle thread policy RL decides traffic measurement average multiple reward image KB image comparison idle thread evaluation phase conclusion investigate dynamic thread pool actor iot application formulate MDP define action reward function ppo algorithm optimal policy PBT hyperparameter various scenario broadening action agent multiple thread improve timeout scheme thread longer whereas RL approach scenario