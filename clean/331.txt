cache enable network architecture promising improve efficiency content distribution reduce network congestion propose maximize throughput MWT algorithm joint request cache placement dynamically optimize network performance specifically dual queue request data establish retrieve global content demand traffic congestion information improve throughput performance stabilize queue formulate throughput request policy optimize throughput function lyapunov drift request policy adaptively allocates request rate link accord difference adjacent request queue backlog novel cache priority function threshold request policy cache policy respectively alleviate network overload addition MWT algorithm achieves throughput optimal performance testify dual queue stable deduce upper bound queue backlog experimental verify MWT algorithm stability demonstrate superiority MWT algorithm cache algorithm combine pressure algorithm previous keywords content cache lyapunov optimization queue analysis introduction rapid growth mobile device wireless device traffic surge entertainment socialize notably delivery popular content aggravate traffic burden resource waste thereby wireless cache developed address delivery popular content attractive candidate technique 5G communication wireless cache technology allows BSs mobile device cache popular content reduces consumption delivery delay diversity content distribution cache brings challenge content distribution efficiency network congestion issue cache enable network architecture considerable academia content cache significantly relieve traffic burden backhaul link reduce service latency smart collaborative video cache strategy propose improve efficiency cognitive content centric networking file minimize cache helper cellular analyze deduce globally optimal probabilistic content placement strategy perspective maximize probability constraint document security however cache strategy content requirement without regard data issue network congestion due excessive traffic link multiple data significant research jointly content cache data novel jointly cache decision multiple user software define network liu shi propose online cache algorithm jointly optimization request data cache minimize average transmission utilized lyapunov optimization stable network distribute scheme combine request content cache reduce traffic load however correspond congestion exist relieve traffic burden cache policy invalid network congestion malicious user frequently request content  node everything therefore verge collapse extremely important congestion congestion significantly improve performance gain cache traffic load congestion effective qos requirement practical application balance network load novel mechanism qos requirement improve utilization network resource data network propose switch approach identifies target congestion scheme explicit congestion notification addition situation transport content cache node destination node severely congest important traffic request therefore jointly optimize content cache request intrinsically couple maximize throughput MWT algorithm joint request cache placement improve throughput performance network indirectly obtain content demand traffic congestion information dual queue transmission rate data rate request moreover optimize combination throughput maximization lyapunov drift minimization improve throughput performance stabilize queue basis optimization propose policy request policy request policy cache replacement policy request policy schedule request multiple data finer granularity transmit slot achieve performance conventional pressure algorithm correspondence data transmission rate request rate virtual data adopt threshold request policy data data congestion excessive request dual queue mode novel cache priority function request queue backlog request rate cache replacement policy algorithm advantage firstly request inject network computation node calculate available request receiver rate secondly algorithm explicitly exchange information parameter network congestion link quality optimal finally delay guarantee network deduce upper bound queue contribution summarize formulate throughput maximization combine lyapunov drift minimization propose MWT algorithm propose algorithm dynamically request rate request discard rate data update cache content node slot backlog request queue data queue deterministically upper bound virtual data backlog stochastically upper bound MWT algorithm achieves throughput optimal performance numerical stability MWT algorithm demonstrate superiority MWT algorithm cache algorithm combine pressure algorithm remainder organize discus related network model queue model propose MWT algorithm boundedness queue throughput optimal performance MWT algorithm finally simulation conclusion simulation conclusion respectively related pressure algorithm obtain network information stochastic network optimization pressure algorithm rate link difference queue backlog transmit node within network stability throughput performance multi hop queue network optimize adopt pressure algorithm ensure stability network arrival rate vector pressure algorithm widely resource schedule rout pressure algorithm mostly data queue reduce delay virtual queue pressure schedule algorithm wireless sensor network propose improve pressure algorithm enhance delay performance introduce novel delay metric sojourn backlog multi hop wireless network efficient rout scheme propose  accord pressure reduce waste resource improve delay performance unlike initiate source node data transmission usually initiate data requester cache enable network therefore pressure algorithm operates data queue fails account dynamic content requirement pressure algorithm request queue operation propose local request queue information demand information content cache data distribute scheme combine request content cache information network congestion content requirement obtain local queue however couple content cache data queue consist request sub data queue sub joint information content requirement decision network congestion obtain queue joint cache rout optimization extensively cache network recent joint cache request rout algorithm cache partition approach content oblivious request rout algorithm propose maximize sum utility content provider arbitrary network topology  yeh rout minimization maximize cache gain jointly cache rout approach kulkarni  cache rout strategy utilize model machine approach improve delay rate performance factor affect cache rout performance however due multi source cache enable network research focus jointly content cache request strategy novel jointly cache decision heterogeneous transmission performance limited cache multiple user software define network propose throughput optimal algorithm data network cache decouple local optimization performance throughput optimal algorithm improve obtain traffic packet accurately suppression data network however decouple reduces performance improvement  bound lyapunov drift exist greatly simplify technical heuristic algorithm avoid couple content cache data therefore combine network utility maximization lyapunov drift couple cache stability delay aware resource allocation markov decision minimize delay due brute iteration dimensional disaster deviation effective approach convert delay constraint equivalent rate constraint delay performance delay lyapunov optimization ensure queue stability network average arrival rate stable network utilize lyapunov optimization theory optimize delay performance reduce average request queue backlog improve throughput performance stabilize queue combine throughput maximization lyapunov drift minimization model network model cache enable network model graph node link request transmits link correspond data transmit link ingres egress node node respectively slot link capacity information slot summarize notation node storage cache popular data cache node denote assume cache limited content cache enable network identify data denote data arise application naturally related amount network convenience assume cache data node cache data meanwhile server node denote data permanently stability define cache status information network slot denotes node cache data slot therefore available source content define notation NotationDescription network graph node link ingres node node egress node node link capacity slot cache node data cache status information slot available source content request queue data node data queue data node queue data node virtual data queue data node request rate link data rate data link virtual data rate data link rate data node slot global request arrival vector queue request queue data queue data node slot unsatisfied request data transmit respectively request queue virtual data queue data node slot request virtual data respectively contains content demand information potential network congestion information data trigger request therefore combine request threshold request strategy reduce network load request rate link slot request unsatisfied request disperse downstream node request queue backlog descend gradient node content source request queue exceeds define threshold queue excess request discard queue slot effective reduce network traffic load concept dynamic correspondence data transmission rate link remains request rate adverse link however data queue sufficient transmission therefore dynamic correspondence valid virtual data queue introduce data rate virtual data rate data link respectively discus relationship assume request rate link zero dynamic correspondence request rate data rate data queue sum request rate data queue data transmit without virtual data queue therefore ensure balance request data link data transmission rate request rate virtual data transmission rate data transmission  virtual data transmission rate data queue sum request rate data queue sufficient transmission virtual data queue due lack data link allocate amount virtual data ensure balance distribute insufficient amount data equally across link define auxiliary variable amount data insufficient link denotes cardinality therefore data transmission rate request rate minus amount insufficient data allocate average virtual data transmission rate request rate minus data transmission rate data transmission  virtual data rate request node cached correspond data request exit data generate satisfy request addition data virtual data exit rate denotes request arrival data node slot difference virtual data data virtual data node request data additional request request queue request met queue structure queue structure request queue random request arrival node constant global request arrival vector application denote assume independent across content node slot structure request queue evolves accord link capacity constraint satisfied slot potential congestion request queue request node slot discard request remain backlog policy rate finite parameter depends node cached data generate satisfy request slot request queue slot empty amount data generate data queue request arrival rate node additional request request queue due virtual data consumption request node node convenience define maximum request rate channel assumption throughout assumption assume assumption  accord obtain request arrival rate node therefore upper bound maximum request queue overflow rate node assumption guarantee maximum request rate maximum request queue overflow rate request rate reduce network congestion structure queue request rate directly establish queue associate request queue request discard permanently accord policy dynamic queue satisfies decision variable backlog request queue queue actual request request queue slot insufficient request discard assume avoids unnecessary request parameter define structure data queue addition request queue node contains data queue correspond data reverse link structure data queue evolves accord data generate node cached specific data request queue node amount data generate request queue backlog cache correspond data node data queue data consume local request structure virtual data queue structure virtual data queue evolves accord node cache backlog sum endogenous request arrival rate virtual data generate addition virtual data consume satisfy local request virtual data queue empty virtual data consume additional request request queue focus content queue pressure algorithm node downward gradient data source request queue implies urgency node data requirement request queue sub determines request data transmit link decides request internally request queue queue node request permanently discard queue node data queue sub data virtual data transmit reverse link respectively data queue node empty data insufficient virtual data transmission virtual data actually transmit link hence data satisfy request additional request request queue throughput maximization throughput maximization content cache enable network firstly formulate throughput average request rate affect throughput secondly request policy obtain optimize throughput function lyapunov drift finally MWT algorithm propose performance MWT analyze throughput request policy beneficial understand conservation queue optimality propose MWT algorithm convenience scenario content source content average data request rate link average request rate node therefore variable satisfy link constraint conservation random request arrival average additional request arrival define average link capacity link average request rate feasible variable satisfy sum average incoming request rate content source content source generate amount data transmit link average incoming request rate become average data arrival content source data rate data virtual data adverse request rate average additional request arrival virtual data consumption rate throughput data average incoming request rate content source sum average virtual data consumption rate therefore throughput data throughput data sum exogenous request arrival rate sum request rate definition capacity request arrival rate matrix capacity feasible throughput vector request rate allocation request rate algorithm satisfies link capability constraint assign utility function data maximize sum throughput data cache enable network define data therefore optimization substitute unknown constant maximize minimize request rate consequently maximize throughput equivalent minimization request policy stabilize queue achieves optimal request rate cache enable network maximum throughput achieve equivalence request stability request queue lyapunov optimization strictly increase function lyapunov function vector request queue queue slot define lyapunov drift strategy minimize lyapunov drift slot sufficient limit queue backlog stabilize network addition minimization therefore accord queue theory queue stable request arrival rate average service rate bound tight hence queue stable minimize upper bound minimize sufficient minimize sum slot compact representation expectation minimize sum network congest request discard conflict minimize lyapunov drift therefore minimize formula pre define relative importance queue stability minimum request conclude determines performance gap MWT algorithm optimal finite queue request queue propose algorithm minimization request cache congestion policy upper bound derive appendix constant propose MWT algorithm detail MWT algorithm algorithm pseudo code slot request policy request policy cache replacement policy algorithm detailed request policy request policy cache replacement policy respectively image KB image request policy data request rate link allocate attain link capacity constraint depends difference adjacent request queue data request policy theorem achieve performance conventional pressure algorithm request policy request queue request queue slot queue request accord cache replacement policy available cache node slot compute priority function data node cache sort node cache data correspond MWT algorithm threshold request policy therefore request inject network computation addition information content popularity network congestion request queue backlog data queue backlog respectively therefore MWT algorithm explicitly exchange information optimal finally MWT algorithm delay guarantee network  queue confirm theorem theorem performance evaluation theorem  request queue queue data queue data upper bound moreover queue addition average data queue backlog satisfy proof appendix theorem finite buffer sufficient queue queue respectively MWT algorithm buffer data reward avoid request data theorem bound virtual data accord chebyshev theorem generate virtual data bound satisfies denotes variance denotes distance generate virtual data threshold accord density function proof appendix theorem probability generate virtual data tends infinity approach amount virtual data random upper bound virtual stability theorem theorem conclude finite buffer sufficient queue theorem demonstrate performance MWT algorithm approach optimality buffer increase theorem optimality MWT algorithm MWT algorithm throughput satisfies throughput data arbitrarily performance gap obtain proof theorem appendix optimal performance MWT algorithm sufficiently parameter parameter implies buffer finally analyze performance overhead queue overhead communication overhead MWT algorithm node request queue overhead MWT algorithm dynamic correspondence valid introduce virtual data queue data queue sub additional overhead queue theorem amount virtual data bound probability addition virtual data affect stability MWT algorithm communication overhead MWT algorithm communication overhead MWT algorithm increase slightly node request clearly rate node assume request queue node node frame signal consists stage request broadcasting node broadcast request queue information node request node request node define maximum cache enable network graph data transmission node transmit data virtual data node simulation numerical simulation performance MWT algorithm algorithm algorithm implement matlab moreover personal computer intel core TM cpu memory simulation network randomly generate graph node utilize shannon formula calculate link capacity SNR rayleigh channel denotes channel bandwidth cache node throughput data addition node cache stabilize network assume average request arrival rate node poisson adopt data node impact parameter average request queue backlog average throughput network environment important network performance network environment obtain average throughput average queue backlog therefore throughout stability performance MWT algorithm policy without request queue stable average request queue backlog request policy policy without request queue slot average backlog request queue  phenomenon node cached data data generate satisfy request decline slot transmit data node another channel capacity constraint occasional increase average backlog request queue fluctuates limited slot stability analysis MWT algorithm request policy obtain almost throughput average queue backlog policy without request queue validates request policy reduce queue stress without significantly reduce average throughput important demonstrates MWT algorithm reduce network congestion maintain throughput requirement MWT algorithm cache algorithm DFC algorithm distribute cache DFC algorithm considers request cache request queue backlog vip algorithm vip algorithm combine pressure cache virtual vip lru algorithm lru algorithm combine recently lru cache policy pressure request LCE algorithm LCE algorithm combine everywhere LCE cache policy pressure request MWT algorithm outperforms cache algorithm joint request cache placement dynamically optimize network performance MWT algorithm request policy difference adjacent request queue backlog content placement policy local information queue backlog request rate average request arrival rate increase average request queue backlog algorithm increase performance gap average request arrival traffic overload however increase performance gap MWT algorithm algorithm increase request inject network network congestion link overload algorithm DFC vip lru LCE cannot network congestion request request queue implies average request queue backlog however MWT algorithm considers network congestion adopts request discard strategy request queue exceeds define threshold request queue discard request accord request policy reduce link load demonstrates importance cache algorithm cache limited cache performance algorithm varies greatly cache capacity satisfy entire network request data cache enable network rout node cache data respond data request thereby increase rate content delivery therefore cache replacement policy increase rate content delivery algorithm comparison request queue cache algorithm pressure algorithm combine however cache replacement policy MWT algorithm cache priority function backlog request queue MWT algorithm cache data priority function periodically data cached cache priority function backlog request queue reflect demand data cache replacement policy request queue backlog therefore MWT algorithm performs limited cache capacity performance gap gradually decrease node cache capacity increase average request queue backlog node cache capacity infinite average request queue backlog regardless algorithm however cache capacity node cannot infinite limited cache capacity propose MWT algorithm superior performance average request queue backlog difference algorithm node data respectively node data increase average request queue backlog algorithm trend gradually flattens average request queue backlog MWT algorithm node data algorithm MWT algorithm combine request request discard cache replacement optimize network performance although algorithm adopt request policy pressure algorithm MWT algorithm adopts request strategy upgraded pressure algorithm realize multiple data simultaneously therefore MWT algorithm superior algorithm request MWT algorithm performance conclusion distribute algorithm maximizes throughput data firstly introduce dual queue retrieve global content demand traffic congestion information secondly improve throughput performance stabilize queue formulate throughput request policy optimize throughput function lyapunov drift finally alleviate network overload request policy novel cache policy threshold request policy MWT algorithm performance MWT algorithm analyze optimal experimental MWT stability performance MWT superior exist future prepared optimal algorithm cache enable network