federate newly emerge distribute paradigm client separately local neural network model private data jointly aggregate global model central server mobile compute aim deploy mobile application wireless network federate mobile compute prospective distribute framework deploy algorithm application scenario bottleneck federate mobile compute intensive resource mobile client computation bandwidth data article illustrates typical federate mobile compute investigates resource optimization approach federate resource efficient technique federate broadly approach approach technique training trick client selection data compensation hierarchical aggregation review approach technique model compression knowledge distillation feature fusion asynchronous update neural structure aware resource management approach module federate propose mobile client assign subnetworks global model accord status local resource demonstrate superiority approach elastic efficient resource utilization introduction recently federate attract attention novel distribute paradigm federate client separately local neural network dnn model private data local model update central server private data remain client local update central server responsible aggregate global model deliver client model training distribute training iteration global model converges satisfy accuracy federate individual privacy client effectively preserve private data client central server mobile compute rapidly develop technology aim deploy mobile application wireless network exploit underutilized computation communication resource device server mobile client complement traditional centralize compute mobile compute exhibit potential significantly reduce traffic load core network relieve processing pressure central server shorten response latency operation therefore enhance overall performance wireless network demand artificial intelligence AI application practically efficient compute implementation federate mobile compute mobile client undertake task local model training prospective distribute framework deploy application challenge federate resource consume mobile compute training model usually massive computation resource burden mobile client distribute paradigm federate occupies considerable bandwidth resource communication central server client model iteration deployment federate mobile compute overcome difficulty intensive resource mobile client consequence efficient resource management crucial federate mobile compute resource management federate mobile compute involves joint optimization multiple resource computation bandwidth data literature effort tackle article investigate resource management approach federate mobile compute perspective resource optimization apply technique exist broadly classify category approach optimization consideration internal structure model detail approach later shortcoming exist approach usually limit client identical neural architecture central server however application compute internet variety client diverse computation communication capability situation neural architecture adapt client hardware configuration battery device tend mode employ dnn model  status switch submodel battery status effective maintain performance prolong device lifetime application scenario federate mobile compute application scenario federate mobile compute article briefly overview exist attempt establish framework federate efficiently heterogeneous resource massive device propose neural structure aware resource management approach mobile client assign subnetworks global model accord status local resource approach elastic framework emerge application compute internet client limited computation bandwidth data resource article organize illustrate application scenario federate mobile compute review exist resource optimization federate approach elastic resource management framework module federate concludes article future federate mobile compute federate leveraged variety application mobile compute typical scenario federate mobile compute mobile client participate local model training data privacy virtual keyboard application mobile phone autonomous discus constrain resource communication computation data federate mobile compute typical application scenario virtual keyboard virtual keyboard application google AI propose concept federate virtual keyboard application mobile phone employ processing nlp model prediction action keyboard input selection local private data training virtual keyboard federate mobile phone downloads global model central server improves training local data update model central server secure encrypt communication link local model update immediately average central server improve global model iteration model training aggregation global model converges training data remains mobile phone private data autonomous principle  autonomous emulate manipulation driver powerful onboard hardware platform model report nvidia inc implementation illustrates autonomous circumstance perceive onboard sensor camera radar lidar  operation steer brake pedal capture truth federate reinforcement training procedure vehicle downloads model central server local model training capture image data onboard sensor input data model output predict operation steer brake pedal truth driver reward compute fed adjust model backpropagation algorithm vehicle uploads update local model central server aggregate vehicle update renew global model iteration global model converges training constrain resource traditional centralize model training federate mobile compute deliver training task mobile client generally intensive resource communication computation data communication resource federate communication model converges regard variability heterogeneity wireless connection mobile compute communication resource mobile client appropriately traditional resource management bandwidth allocation rate technology non orthogonal multiple access noma network function virtualization NFV leveraged improve communication efficiency computation resource mobile compute mobile client computation   due practical constraint consumption model training  consideration limited computation resource mobile client apply computation virtualization technology virtual machine vms container docker kubernetes algorithm execution trend increase computation elasticity efficiency recent resource client mobile compute mostly battery device management essential prolong lifetime mobile client balance performance consumption schedule mobile client switch performance optimal opti mal mode consumption federate mobile compute mainly source local computation model update wireless communication upload model update addition training local model operating frequency model update transmission rate beneficial data resource unique challenge federate data client central server implicitly scarcity training data non independent identical distribution non iid local datasets accuracy non iid data distribute data equally local datasets attempt overcome data deficit employ generative model variational auto encoder VAE generative adversarial network gan generate data sample besides another approach exploit highly efficient label demonstrate significantly improve data utilization facilitate data processing approach literature researcher effort propose approach resource optimization federate comparison broadly category approach approach principle traditional networking optimization tend network entity resource accord observation external involve model contrast approach understand partially internal structure module functionality model network entity resource configure along neural structure adaption optimality overview approach approach training trick training trick effective improve performance federate hyperparameters training gradient descent strategy batch epoch rate decay rate empirically tune reduce communication additionally data augmentation apply enhance inference accuracy training trick significantly reduce communication magnitude model architecture benchmark datasets client selection duration communication client computation communication synchronous model aggregation adopt federate mobile compute crucial appropriate client wireless channel sufficient computation capability weak client involve avoid delay parameter aggregation sake fairness data distribution balance client schedule weak wireless channel computation resource recover average status data compensation previous theoretical analysis reveal heterogeneity data distribution client critical factor restrain convergence rate federate balance data heterogeneity subset local data client globally alleviate non iid hence potentially improve model accuracy spite performance enhancement approach risk privacy leakage due data data compensation framework gan model propose compress data sample client multiple hop server gan training client gan augment data quality local model training non iid data compensate additional computation workload gan model training underlie threat data leakage client hierarchical aggregation incorporate hierarchical network topology federate model aggregation conduct client multi stage procedure stage local aggregation server global aggregation central server multiple intermediate server introduce perform multiple local aggregation global aggregation central server rationale multi stage aggregation model aggregation network communication efficiently alleviate uncertainty model update due randomness local data central server refine model update convergence global aggregation fairly approach model compression model compression consists optimization technique shrink model easy deployment hardware environment hence widely exploit reduce communication overhead distribute quantization rank approximation technique leveraged minimize communication federate report conservation communication overhead magnitude furthermore sparsification technique gradient prune verify efficient alternative model compression knowledge distillation core knowledge distillation observation portion activation probability distribution softmax output model utilized additional regularizer model knowledge distillation highly efficient technique knowledge transfer instance softmax output information model knowledge inspire knowledge distillation federate chooses client aggregate consensus model update gain percent accuracy participant report unfortunately reality approach heavily relies public dataset commonly available feature fusion employ feature fusion technique federate proposes feature extraction fusion client concretely global model update central server client global feature extractor replace local output global local extractor merge feature fusion module classifier loss evaluation training batch propagation sequentially renew classifier fusion module local extractor global extractor global extractor regularizer adjust update local model local global data distribution approach effective balance non iid data decrease communication extra compute overhead global feature extraction fusion asynchronous update borrowing asynchronous proposes asynchronous model update strategy federate layerwise analysis approach split dnn shallow layer layer former update frequently latter learns feature therefore critical global update latter meanwhile framework asynchronous central server aggregate model without local temporal coefficient correspond duration model aggregation asynchronous update approach flexible efficient dimension model aggregation convergence curve procedure unusual fluctuation accuracy comparison aforementioned approach resource management federate empirically suggestion apply approach synchronous update efficient practically useful scenario model compression apply priority reduce communication overhead leverage compute hierarchical aggregation reduce traffic cli ent entity conflict apply approach incorporation approach performance gain neural structure aware resource management propose module approach global model partition multiple submodels training model partition client assign jointly identical submodel separately submodels local resource standpoint distribute training novel federate framework concurrently integrate parallel compute data parallelism model parallelism accordingly resource mobile client efficiently orchestrate fulfill submodel training filter filter approach approach model partition aggregation without loss generality convolutional neural network cnn model partition width filter convolutional layer global model multiple submodels filter define partition factor layer output channel layer partition factor layer usually sub model compose filter convolutional layer along input output fully layer layer cnn partition factor output channel filter layer shrunk global model filter layer input global model output reduce layer global model layer input output reduce filter reduce optional submodels filter structure filter structure filter submodel algorithm later model training critical balance local data client central server periodically exchange filter submodels ensure filter global model equally client submodel exchange elaborately conduct fairly utilize local data meanwhile maintain inter layer connection submodels effort  exchange strategy illustrate client sequentially submodels iteration model aggregation central server reassemble partition submodels reconstruct global model multiple global model partition global model partition client define multiplexing factor global model aggregate global model central server reconstruct global model perform gradient aggregation aggregation coefficient model gradient proportional local datasets traditional federate applies aggregate gradient information investigate submodels contribute performance interested amount training information submodel uploaded central server aggregation cnn model layer ith layer  compute filter convolutional layer fully layer previous disclose representation capability neural network grows exponentially depth polynomially width model borrowing capacity capacity cnn approximately estimate    constant explicitly reflect representable function compute ith layer kernel convolutional layer capacity neural network information theoretic   capability federate client report gradient information central server iteration accurate report gradient efficiency express efficiency specific neural network model propose concept aggregate gradient information agi denote tunable parameter compute ith layer agi define   SourceRight click MathML additional feature agi interpret effective information training neural model information theoretic perspective treat training procedure federate gradually reduce uncertainty optimal model parameter aim optimal minimum loss function iteration model gradient uncertainty optimal gradient absolute uncertainty generally gradient absolute decrease eventually uncertainty zero apparently neural network capacity gradient information agi indicator reflect accuracy gradient parameter update model training resource efficient allocate resource model training evaluate submodel structure index client iteration federate denote neural structure compute frequency transmit rate jth client kth iteration respectively structure model easy calculate training workload per datapoint approximately compute empirically symmetric wireless channel uplink downlink channel status meanwhile     denote compute transmit compute transmit jth client kth iteration            SourceRight click MathML additional feature consumption coefficient related hardware architecture processor training datapoints jth client gaussian receiver wireless bandwidth loss wireless channel jth client kth iteration respectively calculate correspond communication computation client maximum client client sum communication computation submodel exchange submodel exchange denote agi jth client kth iteration budget  resource efficient federate formulate max     SourceRight click MathML additional feature  client dataset client agi proportional logarithm dataset objective maximize agi consumption improve model efficiency iteration describes neural structure aware resource management approach heuristic algorithm client budget specific cnn model partition multiplexing factor heuristic algorithm recursive described neural structure aware resource management neural structure aware resource management submodel structure model partition sort submodels client channel client submodels program suboptimal suboptimal utility suboptimal utility optimal duration binary technique program suboptimal compute frequency transmit rate compute budget binary budget return suboptimal resource configuration input model partition client compute suboptimal compute frequency transmit rate minimal return suboptimal compute frequency transmit rate client submodels slot computational complexity induced sort estimate  nlogn computational complexity binary within slot estimate logt directly calculate suboptimal compute frequency transmit rate overall computational complexity  nlogn  evaluation scenario federate mobile client image classification task mnist dataset sample mnist dataset training others training data assign client iid shuffle data uniformly client sample non iid sort data label split shard shard client global cnn model structure MP MP gap SM partition factor convolutional layer respectively hyperparameters batch local epoch rate decay rate per module federate MFL traditional federate  accuracy consumption convergence LP mode budget  HP mode budget  accordingly optimal model partition respectively denote submodels medium convolutional layer respectively mode allows mobile client adapt available resource application feature normal communication MFL outperforms  mode particularly mode  MFL percent iid non iid data respectively accuracy percent mode advantage MFL conservation percent iid non iid data respectively meanwhile convergence MFL  accuracy percent convergence MFL  percent iid non iid data mode percent iid non iid data mode respectively fading channel plot suppose client fail return gradient percent probability MFL maintains satisfy performance indicates MFL reliable dynamic wireless communication environment MFL eventually sufficiently converge accuracy  although plot numeric report iid MFL achieves almost converge accuracy  gap merely percent non iid MFL suffers performance percent converge accuracy addition MFL flexibly submodel structure tends submodels submodels elastic efficient resource management approach module federate traditional federate module federate traditional federate conclusion future article investigates exist resource optimization federate broadly classifies approach module federate framework neural   aware resource management approach propose advantage improve resource efficiency principle framework model partition width depth kernel future explore resource management approach submodel structure flexible width depth kernel reinforcement employ adaptively optimal resource configuration addition incorporation neu  structure aware resource management efficient approach surprising