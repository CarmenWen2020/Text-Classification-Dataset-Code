recently artificial intelligence AI undergo sustain  substantially improve network cognitive performance intelligence thereby contribute fully unleash potential data AI frontier network context trend emerge  namely intelligence EI indeed EI sink processing capability response enable intelligent service performance however successful realization EI infancy article aim comprehensive broader perspective discus prior knowledge holistic overview EI concept advantage development trend highlight collaboration mode EI discus typical category subsequently entire model training inference EI elaborate finally discus typical application scenario specific embodiment EI strive shed potential challenge facilitate transformation EI theory introduction popularity mobile device advance wireless access technology fifth generation 5G network flourish mobile application explosive growth data traffic accord international data corporation global data traffic ZB percent data network emerge application generally intense requirement quality service qos mobile compute mcc infeasible transmission distance contribute extra delay unreliability indeed server spatially device accompany challenge driven transmission distance processing attract widespread academic concern despite continuous effort spent enhance channel bandwidth utilization efficiency spectrum notably theoretical bound effort sufficient fundamental innovation bottleneck massive task processing 5G network urgently challenge accelerate development mobile compute MEC specifically MEC promising cope innovative application physical proximity information generation source mcc MEC computation cache network function network perform task processing service avoid unnecessary transmission delay accommodate expansion requirement computation capability device promise benefit reduce consumption delay privacy protection accessibility context awareness development artificial intelligence AI experienced leap spectacular leap decade specifically driven breakthrough hardware upgrade series neural network AI expands technological innovation superiority data analysis extract insight complex network AI frontier network context emerge  namely intelligence EI notably EI development stage MEC integration MEC AI  mutual benefit instead entirely rely EI sink processing capability response enable intelligent service performance furthermore leakage loss user private data explicitly alleviate EI  although EI attract tremendous amount recent successful implementation EI infancy article aim comprehensive broader perspective review prior knowledge holistic overview EI concept advantage development trend highlight collaboration mode EI discus typical category subsequently model training inference EI elaborate holistically finally discus typical application scenario specific embodiment EI strive shed potential challenge facilitate transformation EI theory prior knowledge EI introduces prior knowledge holistic overview EI concept development trend motivation concept EI EI currently emerge technology trend risen combination MEC AI generally EI understood intelligence empower compute combine advantage MEC AI complement mutually beneficial AI MEC technology MEC unleash potential scalability AI MEC AI scenario platform AI expand applicability MEC essentially EI enable equipment perform model training inference locally avoid frequent communication platform emergence EI highly nontrivial due concern efficiency schedule optimization privacy protection MEC EI research direction collaboration model segmentation reduction redundant data transmission lightweight acceleration architecture furthermore technical perspective AI model extract insight practical environment seek quality asymptotic iteratively reinforcement RL gradually become popular AI technique EI automatically extract feature detect anomaly RL multi agent RL reinforcement DRL refers realize objective via multiple suitable decision important role decision network development trend EI already related concept EI propose meantime although academia effort EI successful implementation lack standardize EI significantly hinders promotion development discus essential evolutionary EI specifically european telecommunication standard institute ETSI forefront EI related exploration publish technical mobile compute technology towards 5G september proposal technical requirement EI formally propose correspond reference platform challenge typical application scenario EI MEC congress microsoft propose intelligent intelligent concept assist strategic planning decision collaboration meanwhile alibaba publish link platform march aim sink processing capability identify AI viable network august alibaba internet iot chain partner launch iot connectivity alliance ICA promote EI technical standardization development deliver specification technique requirement formally publish technical EI latter compute consortium ecc host MEC summit beijing EI effort accelerate development EI application ultimately expand overall EI demand constant effort aspect academic theory realistic challenge requirement EI integration MEC AI inevitable trend promote aspect latency bandwidth consumption conventional mcc user data remote data undoubtedly consume bandwidth resource tremendous pressure network EI become reliable challenge processing capability AI service deployed mobile user participate service hence EI achieve latency reduce bandwidth consumption adapt environment layer node reduce pressure infrastructure due access massive device however management complex network architecture involve bandwidth compute storage capacity significant conventional network optimization rely fix complex mathematical knowledge cannot adapt rapidly network environment emergence EI due powerful ability AI technology enables tackle sophisticated network optimization task offload user association resource allocation adaptive decision extract valuable information data richer application scenario disperse device application scenario continuously generate massive heterogeneous data text image video server cannot handle amount data due compute resource storage capacity limitation AI analysis diversified inference improve qos service DRL enables network identify extract feature data automatically data DRL model update decision increase efficiency importantly data improve AI technology performance specifically improve accuracy AI model improve model structure data training data explicitly sufficient training resource AI model therefore AI MEC complementary mutually beneficial MEC enabler ubiquitous AI AI facilitates daily dramatically achieve application expand continuously ultimate goal development AI everywhere goal AI closer user MEC advantage mcc achieve server closer proximity data source MEC economical accessible mobile operator finally connection iot device MEC abundant AI application scenario hence MEC crucial enabler ubiquitous AI MEC popularize AI application  stage MEC improve network performance MEC investigate along dominant application focus research recently emerge application fully exhibit MEC advantage played essential role development MEC application computation bandwidth resource ultra delay MEC intense requirement efficiently emergence AI application scenario related iot MEC privacy protection ultra latency advantage collaboration scope category highlight collaboration mode EI discus typical category collaboration scope stem trend recent EI evolve intelligent service powerful cooperative operation stage scope EI restrict solely organizational coordination integrate EI paradigm fully exploit resource across various hierarchy however coordinate available resource hierarchical architecture optimize training model offs multi criterion essence indeed AI implement compatibility coordination heterogeneous resource via data offload hierarchical architecture collaboration EI paradigm collaboration collaboration collaboration respectively hierarchical architecture collaboration hierarchical architecture collaboration collaboration recently collaboration grown relatively mature collaboration mode already attract widespread investigation academia responsible analyze mining data model training upgrade powerful storage compute capacity ensure intelligent processing comparison responsible data computation cache within local scope performance local cycle intelligent decision execution collaboration enables offload workload processing orderly data loop autonomous substantial significance fully utilize resource collaboration constraint node suffer inherent resource limitation processing extensive application separately improve resource utilization node cluster enable communicate compute individually synergy various node harness facilitate performance collaboration mode worth dynamic resource requirement heterogeneous subsystem connection node exist collaboration capacity independent node therefore crucial efficient cooperative schedule strategy available collaboration collaboration refers device typically series iot device mobile phone vehicle collaboration lightweight model relieve increase conflict task request diversity equipment competency effectively enhances node processing capacity moreover correlation device specific application scenario collaboration focus intelligent schedule secure access device collaboration device aware data offload server server performs centralize calculation analysis data multiple source sends operation identifier device user reliable service overall due relationship device user collaboration essential implementation EI application typical category EI typical category EI typical category stem recent categorize related application EI category information prediction strategy optimization information prediction core concept information prediction predict evolution network leverage context information facilitate performance dynamic generally improve EI performance information predict uncertain parameter related dynamic wireless channel network channel information resource occupation rate private information related user location information social behavior content popularity etc accord practical predict information adapt environment promote resource utilization various task requirement instance harness predict content popularity slot significantly optimize cache replacement strategy alleviate redundant traffic reduce content delivery delay cache currently popular information prediction supervise semi supervise specifically vast heterogeneous data perform automatic feature extraction conventional model prediction prediction effectively improve prediction accuracy strategy optimization besides strategy resource allocation beforehand via information prediction adaptive optimize strategy online efficient intelligent service unlike conventional data driven optimize strategy promising tackle complex NP network moreover autonomic network management strategy optimization capture environment hidden dynamic enhance intelligence network correspondingly strategy optimization RL DRL neural network dnns adopt avoid curse dimensionality specifically RL enables agent adaptively optimal strategy interaction within specific context discrete implementation EI elaborate model training inference EI introduce related performance indicator model training convergence MEC AI relies efficient distribute model training inference along continuum critical enable quality EI service deployment classify architecture mode deployment location model training namely centralize decentralize hybrid respectively centralize mode centralize training mode model deployed compute platform data preprocessing model training message  mainly perform specifically training model implement collaboration performance heavily depends quality network connection training phase node data within local scope uploads data generate application sensor device video channel social medium user analyze cache node data model continually centralize training cluster aggregate data notably although centralize mode promise potential optimal model training complexity grows exponentially global network data meanwhile deployed model compute platform spatially user user data pas multiple network network data delivery unpredictable network connection prohibitive transmission delay overhead furthermore mode vulnerable due user data resource concentricity explicitly leakage loss sensitive private data decentralize mode nowadays flourish mobile application latency sensitive application intense requirement ultra reliable latency communication centralize training longer feasible development indeed node inference significantly local meanwhile user benefit EI privacy appropriate avoid dilemma model decentralize manner decentralize mode centralize node node perform role remarkably node neural network imply perform model training independently node model locally local data preserve private information locally however training independent node easy perform overfitting due correlation restrict data source inference node usually mutually influential obtain global training model local training improvement multiple node undertake model training data analysis synergistically training generate global model without compute platform intervention furthermore emerge federate apply data sensitive decentralize training mode worth conventional decentralize training focus consume data federate focus privacy protection model training inference mode model training inference mode hybrid mode indeed unrealistic node comprehensive model latency consumption adopt EI architecture hybrid mode combine centralize decentralize mode hybrid mode limited practical deployment flexible adapt application scenario mode node DRL model decentralize update centralize training compute platform specifically node partial parameter aggregate central node global model upgrade private data node privacy preservation weaker decentralize mode robust centralize mode addition reduce training complexity devote suitable training tends formulate superior model introduce prior knowledge model inference model inference model happens training efficient model inference equally critical implementation EI accord model category described model inference execute node typical model training inference mode specifically typical inference mode centralize decentralize former model training inference inference node separately latter node perform model inference locally notably centralize training mode maintain training model centralize inference node model node distribute inference detail implementation generally supervise unsupervised agent RL commonly centralize inference multi agent RL standard decentralize inference model training inference centralize training centralize inference centralize training decentralize inference decentralize training decentralize inference model training inference centralize training centralize inference centralize training decentralize inference decentralize training decentralize inference performance indicator elaborate critical performance indicator EI related model training model inference respectively training loss inference accuracy training loss inference accuracy important criterion AI model former training loss usually occurs model training stage objective convergence loss function bias predict absolute training model data therefore minimize loss function appropriate training sample latter accuracy refers prediction input sample model inference directly influence validity model service reliability requirement accuracy essential greatly affect quality user vehicle network accuracy decision affect comfort vehicle safety driver passenger efficiency EI efficiency mainly affected sample data training model complexity equipment resource hardware constraint decentralize node training model efficient therefore pursuit efficiency critical factor improve exist algorithm model EI furthermore economic EI qos consumption model inference reasonable model inference efficiently reduce service consumption delay notably improve efficiency training inference model compression conditional calculation algorithm synchronization generally propose fairness EI performance efficiency fairness various node load balance achieve reasonable utilization resource cooperative node user access EI service various resource bandwidth computation cache resource occupy simultaneously resource conflict factor resolve model training inference reasonably security privacy data leakage commonplace privacy protection become topic model training stage device data personal privacy information therefore processing data crucial meanwhile model training exhibit unintended behavior training model adversarial attack quantify via robustness indicator hence essential privacy data security data source EI application model inference stage wireless eavesdropper eavesdrop compute task offload task wireless channel transmission application data encrypt user decrypt target server however increase transmission delay execution delay thereby reduce performance furthermore storage compute resource multiple mobile user leakage loss private data challenge future direction EI EI emerge interdisciplinary challenge tremendous opportunity typical application scenario strive shed potential challenge facilitate transformation EI theory EI internet vehicle already witness proposition EI exemplify application typical application scenario EI specific embodiment along recent advance wireless access technology steady growth vehicle vehicle roadside RSUs pave data driven intelligent transportation specifically AI substantially improve cognitive performance intelligence internet vehicle  adapt rapidly dynamic environment multiple task requirement resource allocation compute task schedule vehicle trajectory prediction basis EI exhibit fascinate potential handle various intelligent vehicle application AI service RSUs discus specific embodiment EI  specifically IoV architecture macro MBS RSUs vehicle RSU endow MEC server compute offload cache vehicle request various content frequently amount available content furthermore vehicle vehicle communication vehicle processing capacity node hence collaboration request vehicle concurrently offload task request content RSUs vehicle remote MBS DDQN optimization compute cache IoV DDQN optimization compute cache IoV investigate optimization compute cache decision cache compute resource allocation jointly meanwhile resource capability offload cache vehicular mobility intensity uncertain aim subset nearby node RSUs vehicle resource respond request vehicle dynamic variant enormous action practical vehicular network exploit network DDQN efficiently optimal action accord tremendous amount input data brief description DDQN DDQN improvement traditional DQN algorithm selection evaluation action construct action function DDQN approximately obtain optimal update dnn parameter meanwhile agent tuple action reward replay buffer slot correlation data training furthermore neural network structure parameter maintain disrupt  target neural network acquire temporal difference target neural network evaluate function tuples randomly sample training network target network input network vehicular network environment training training action replay buffer output network action adopt MBS RSUs vehicle parameter target dnn update periodically counterpart dnn DDQN model consists available RSU vehicle available cache cache compute resource RSU vehicle transmission channel information vehicle mobility accordingly action agent request content cached computation task offload cod packet cached hence action consists RSUs vehicle cache compute resource request vehicle finally reward negatively correlate objective function aim minimize communication storage computation operation sends network replay buffer tuples agent network target network action specifically pre offload cache decision resource allocation random policy sufficiently neural network target neural network update parameter sample replay buffer respectively fix target generate target  transmit network network target minimize loss function tuples agent gradient update generates action stochastic gradient descent perform converges optimal addition BS RSUs vehicle execute compute offload content cache action environment immediate reward agent action satisfies constraint propose joint optimization utility policy exist maximal utility immediate reward update environment update policy however constraint violate propose agent receives punitive negative directly consideration cumulative discount utility DDQN cumulative discount utility converges optimal policy computation offload content cache resource allocation successfully challenge although EI benefit challenge reconsider deployed commercially dynamic openness EI dynamic wireless network openness wireless channel wireless network environment stable user network service request user movement dynamic network EI decision online respond dynamic network promptly strict requirement training AI model interference fading wireless channel inevitably reduce communication quality affect accuracy convergence EI therefore EI robust various uncertain environment hardware network architecture device steady growth vigorous development EI application distribute network computation intensive delay sensitive EI application massive resource model training inference therefore uncertain practical network advanced network architecture desire relieve training inference pressure limited resource furthermore advanced network architecture ensure ultra reliable latency communication EI via efficient resource management task schedule node lightweight training model exist AI model enable advanced function via complex network structure discus deploy execute AI workload node limited resource lightweight AI model adapt resource constraint node compression technique exit inference  knowledge distillation quantization prune effective realize lightweight AI model however although model compression AI model accompany loss model accuracy static compression technique cannot adapt dynamic hardware configuration load node therefore potential research direction apply emotional compression technique node complicate security privacy data leakage commonplace privacy protection become topic recently training model uploaded data personal privacy information therefore processing private data crucial although technique federate propose issue raw data infer reconstruct approach security privacy EI data inevitably interfere homomorphic encryption encrypts data inference ciphertext meanwhile data device locally conclusion article conduct comprehensive recent research EI broader perspective specifically discus prior knowledge holistic overview EI concept advantage development trend highlight collaboration mode EI discus typical category subsequently model training inference EI elaborate finally discus typical application scenario specific embodiment EI strive shed potential challenge facilitate transformation EI theory