recently along rapid development mobile communication technology compute theory technique attract attention global researcher engineer significantly bridge capacity requirement device network accelerate content delivery improve quality mobile service intelligence traditional optimization methodology driven technique propose integrate reinforcement technique federate framework mobile optimize mobile compute cache communication AI framework intelligently utilize collaboration device node exchange parameter training inference model dynamic optimization application enhancement reduce unnecessary communication load AI evaluate optimal performance relatively overhead cognitive adaptive mobile communication finally discus related challenge opportunity unveil promising upcoming future AI introduction increase quantity quality multimedia service mobile network increase traffic computation mobile user device recent imposes amount workload already congest backbone network mobile network naturally emerge mobile compute MEC propose novel paradigm burden backbone network computation storage resource proximity user equipment ues MEC circumvents propagation delay introduce transmit data mobile device remote compute infrastructure hence latency critical mobile internet iot application specifically node equip computation storage capability computation content request ues consequently scheme improves quality service qos mobile network operator  quality qoe ues relief load backbone network pressure data fulfil requirement qoe ues trivial virtue MEC computation offload wireless data transmission congestion wireless channel hence decision optimization communication computation integrate jointly allocate communication resource computation resource node pioneer propose realize assume setting convex optimization theory nevertheless MEC optimization suffer issue uncertain input assume information factor input actually obtain due variant wireless channel privacy policy dynamic dynamic integrate communication computation address temporal isolation decision resource allocation lyapunov optimization viz highly MEC propose optimization algorithm optimal optimal snapshot exists resource allocation optimization MEC lack intelligence increase complexity mobile network typical 5G node configurable parameter recent trend optimize wireless communication artificial intelligence AI technique limited application AI physical layer phy data link layer traffic related compute cache reinforcement reinforcement potential effective joint resource management feasible practical MEC action tremendous focus vehicular network besides none training data distribute centralize reinforcement agent ues node remote infrastructure update reinforcement agent proceed collaborate privacy protection training data therefore article reinforcement DRL jointly manage communication computation resource computation offload cache MEC illustrate addition federate introduce framework training DRL agent distribute manner largely reduce amount data uploaded via wireless uplink channel react cognitively mobile communication environment cellular network adapt heterogeneous ues practical cellular network preserve personal data privacy framework AI mobile cognitive ability knowledge application DRL couple federate intelligent joint resource management communication computation MEC contribution summarize discus methodology utilize DRL specifically distribute DRL optimize cache computation propose AI framework utilize federate deployment intelligent resource management MEC proof concept evaluation verify propose scheme advantage balance performance addition discus opportunity challenge hopefully unveil upcoming future architecture various AI application optimize DRL intend AI technique particularly DRL cognitive compute building  mobile compute cache communication cognitive protocol stack wireless communication partition procedure utilize cognitive compute mobile protocol stack information indispensable data cognitive compute MEC limited usage communication computation resource wireless environment intensity ues request cognitive compute data cognitive compute perform fuse massive data decision schedule request handle MEC request ues basis schedule decision cognitive compute representative MEC investigate DRL MEC cache recently emergence promising mobile content cache delivery technique popular content cached intermediate server middleboxes gateway router demand user content accommodate easily without duplicate transmission remote server hence significantly reduce redundant traffic  focus scenario cache content node MEC depict library popular content file denote mobile user request content popularity define probability distribution content request user content popularity indicates user network related content popularity described  distribution moreover simply assert efficiency DRL cache assume content popularity slowly content request DRL agent node decision cache cache agent determines local content replace assume content popularity user preference average arrival rate request static relatively model cache replacement node markov decision MDP DRL later DRL MEC computation offload communication model illustrate environment ues ues offload intensive computation task node via wireless channel execute task locally wireless channel denotes channel specifically decision choice UE offload computation via wireless channel compute task locally decision purpose simulate variation wireless channel channel gain UE belongs node independently picked finite channel transition model finite discrete markov chain wireless scenario achievable data rate evaluate shannon hartley theorem computation model demonstrate within MEC computation task generate accord bernoulli distribution across horizon computation task denote computation input data cpu cycle computation task respectively generate task task queue execute sequentially UE node accord fifo principle task execute locally computation execution computation capability cpu cycle per UE amount UE decides allocate task schedule execute node execution offload task calculate computation capability node allocate UE formula compute performance node UE formulation computation offload efficiently perform computation offload MEC UE joint communication computation resource allocation decision action computation offload decision denote UE chooses execute task locally offload task via wireless denotes amount allocate wireless communication local computation MEC attention focus improve task execute viz qoe ues complexity MEC introduce task offload difficulty acquire global information involve massive ues hence DRL algorithm lean utilized agent handle joint action UE summarize UE decides joint wireless channel selection allocation decision accord stationary policy network involves task queue cumulative consumption UE occupy wireless channel UE quality wireless channel addition define immediate utility function evaluate qoe ues inversely proportional execution delay task wireless transmission delay computation delay task queue delay consumption UE task fail advantage improve version DQN policy achieves increase utility ues performance optimization AI federate previously DRL MEC however challenge pending namely deployment DRL agent computation offload DRL agent node remote server depict due wireless communication MEC deficiency exist training data quantity massive ues increase burden uplink wireless channel training data uploaded node privacy sensitive potential privacy accident training data transform privacy consideration server proxy data relevant device data centralize DRL distribute DRL DRL federate mobile centralize DRL distribute DRL DRL federate centralize DRL distribute DRL DRL federate mobile centralize DRL distribute DRL DRL federate DRL agent UE individually another defect computation capability UE relatively weak impossible DRL agent data training DRL agent introduce extra consumption UE DRL technique intensive computation capacity optimal amount data factor parameter criterion optimize resource MEC operator network advanced distribute DL approach utilized hence training DRL agent distribute fashion efficiently illustrates although training DRL agent UE node achieve performance practical apply distribute DRL MEC data training however distribute DRL architecture described handle unbalanced non iid data cope privacy issue addition usually reduce performance DRL agent ues network heterogeneous therefore federate FL introduce article training DRL agent MEC challenge differentiates distribute DL approach taxonomy apply reinforcement mobile non independent identically distribute non idd training data transition memory DRL UE wireless environment experienced computation capability consumption hence individual training data UE training data ues FL challenge met merge update model FedAvg limited communication ues unpredictably allocate communication resource nevertheless additional computation decrease consumption communication model addition FL asks client upload update handle circumstance client unpredictably unbalanced ues computation task handle mobile network amount training data ues challenge handle fed avg algorithm privacy security information uploaded FL minimal update improve DRL agent technique secure aggregation differential privacy apply naturally avoid privacy sensitive data local update nonetheless privacy security focus information issue reference integration federate within AI AI AI ues aid FL framework node equip AI computation combine massive ues powerful AI entity cognitive ability massive ues node throughout envision architecture node AI task dynamically global optimization balance MEC cache computation offload AI integrate FL cache MEC DRL agent employ node decision cache appropriate content accord request content ues dynamically however popularity dynamic request content requirement collaborate cache server belonging  central server coordinate node node DRL agent update local training data ues scenario computation offload UE computation task offload node offload task via wireless channel consumption accord inference DRL agent ues mobile phone industrial iot device smart vehicle perform AI computation computation capability consumption limit ability AI compute DRL training data therefore propose node integral server coordinate massive ues virtue scheme ues relatively weak computation capability complex DRL agent methodology tackle aforementioned scenario training DL model depict FL iteratively  random client distribute device DL model parameter DL model server perform training model data upload model parameter server aggregate uploaded update client improve model inside individual client illustrate version accessible reading specifically client model central server local training data uploads update  backward available summarize FL enables resource constrain compute device ues node model training data local pro con federate AI core benefit FL distribute quality knowledge across device without necessarily centralize training data extend core benefit benefit MEC cognitive server proxy data relevant device data MEC massive ues node perceptrons acquire various abundant personalize data update global DL model perspective ues data quality wireless channel remain battery consumption immediate computation capability concern node cognitive data computation load storage occupation wireless communication link task queue handle FL raw data instead centralize DL render MEC cognitive robust extent propose AI future orient concept envision future mobile device particularly smartphones endow ability infer training model FL inherently address issue availability ues node unbalanced non iid data consequently performance AI easily affected unbalanced data sometimes communication environment ability handle non idd data allows massive ues wireless environment DL model without concerned negative flexible FL additional computation decrease communication model effective computation increase computation per UE local sgd update per hence computation communication exist specifically powerful computation ues perform mini batch training decrease communication vice versa certainly due limitation federate optimization FL outperform centralize DL model achieve performance demonstrate later practicability discussion extent propose AI future orient concept envision future mobile device particularly smartphones endow ability infer training model chip tpu google tensorflow lite elementary training DL therefore practicability practical deployment delay requirement deploy challenge training infer accord accuracy obviously DRL model deployed directly neural network random otherwise MEC  DRL model random decision preliminary exploration nonetheless DL model scratch transfer boost training MEC simulate wireless environment request ues evaluate adjust antenna setting simulated testbed simulated environment reinforcement agent establish DRL model distribute mobile ues delay requirement training infer accord accuracy however transfer computation consumption training pre establish DRL requirement training infer alleviate DRL agent satisfied accuracy mini batch directly infer wireless environment ues request UE advantage establish DRL adjust neural network DRL model unlike enormous hidden layer neuron cnn rnn neural network DRL merely multi layer perceptron mlp simulated scenario mlp hidden layer neuron therefore model infer perform quickly due complexity  DRL agent aim challenge DL resource constrain mobile device incorporate requirement computation capacity ues relaxed along decrease infer delay data driven evaluation proof concept AI framework setting investigate performance AI FL simulation cache computation offload respectively simulation horizon discretized epoch capture  trace active mobile user convey content file content request content popularity distribution zipf distribution cache simulation zipf distribution content popularity distribution generate content request ues cooperation node node receives request UE local DRL agent decision cache content cache obtain reward action training cache server belonging  central server coordinate node investigate capability DRL couple FL MEC computation offload suppose bandwidth mhz node wireless channel ues client FL framework individually DRL agent merge node channel gain UE node finite quantifies quality wireless channel throughout simulation task generate UE bernoulli distribution average arrival rate per epoch DRL setting ues node DQN algorithm tanh activation function adam optimizer layer fully feedforward neural network neuron target  eval  network parameter DQN replay memory capacity mini batch discount factor exploration probability rate replace target network addition baseline performance DRL agent FL construct centralize DRL agent comparison assume data reinforcement evaluation elucidate performance AI framework cache computation offload various setting coordinate server FL executes task merge update instead training computation load client inevitably heavier local training ues burden computation ues node issue depicts performance DDQN FL cache computation offload node ues chosen exhibit capability DRL agent perspective cache chosen cache content rate improve finally maintain within along training DRL agent simulation computation offload average utility ues increase decrease training loss maintain within efficiency handle offload task MEC improve capability federate cache computation offload capability federate cache computation offload detail performance comparison cache performance DDQN FL centralize DDQN cache policy lru recently LFU frequently fifo easily specific performance FL centralize DDQN achieve rate client node outperform lru LFU fifo computation offload performance DDQN FL centralize DDQN another baseline computation offload policy mobile execution node execution greedy execution mobile execution node execution greedy execution UE computation task locally computation task offload UE node UE decision execute computation task locally offload node aim maximize immediate utility respectively DDQN FL achieves performance centralize DDQN superior another baseline policy investigate detailed performance training assume capability wireless communication hinder massive training data centralize DDQN lightweight model update DDQN FL uploaded target within discretized epoch illustrate performance centralize DDQN DDQN FL training however model merge FL performance DDQN FL becomes centralize DDQN certainly client obtain performance DDQN FL model merge exploit training client nonetheless assumes ideal wireless environment massive training data actually unable uploaded without delay therefore perform DDQN FL practical MEC wireless resource consideration statistic wireless transmission DRL agent scenario cache computation offload framework FL client upload update model without FL framework viz centralize DRL ues upload training data via wireless channel consume communication resource demonstrate performance evaluation DDQN without federate performance comparison cache performance comparison computation offload detailed performance comparison computation offload comparison transmission doubt FL something advantage specifically coordinate server FL executes task merge update instead training computation load client inevitably heavier local training ues burden computation ues node issue opportunity challenge hereby discus essential promising research direction AI highlight pending perspective elaborate widen usage AI compute accelerate AI task communication compute mention previously important optimize mobile communication AI technique important exploit specialized optimize computation task DRL treat specialized compute task service node collaborate allocate appropriate resource amount AI task various priority deadline requirement cpu memory indispensable interestingly reverse direction aforementioned federate application AI potential research topic theoretic algorithm competition node mobile user multi dimensional resource AI acceleration dynamic adaptive splitting AI task becomes challenge efficiency AI mobile communication 5G 5G define ultra reliable latency communication URLLC scenario strongly desire delay reliability mobile however optimization prediction scheme recursion converge inappropriate mobile particularly compute task desire rapid response millisecond manage guaranteed qos delay bandwidth cache communication computation AI differentiate various service grain collaborative schedule AI task split node mobile device accelerate nearly critical remain unsolved literature furthermore integration FL framework AI chipset hardware explore incentive business model AI AI tight  mobile operator SPs cps mobile user capable entity contribute overall optimization cache compute task ues rely AI node ues AI computation requirement SPs cps satisfied across node mobile operator hence reliable accurate incentive framework AI becomes loop digital copyright content service blockchain technique integrate AI framework distribute computation load proof evaluate contribution AI computation heterogenous scenario unexplored conclusion future concentrate optimize computation task schedule AI task node mobile device grain collaborative manner article potential integrate reinforcement technique federate framework mobile optimize mobile compute cache communication perform investigate scenario cache computation offload mobile AI evaluate ability achieve optimal performance future concentrate optimize computation task schedule AI task node mobile device grain collaborative manner