network function virtualization NFV flexibility network operator reduces complexity network service deployment NFV virtual network function VNF various network node chain service function chain sfc specific service consolidate multiple VNFs location decrease expenditure however excessive consolidation VNFs additional latency penalty due processing resource undesirable SFCs bound service specific latency requirement identify penalty refer related processing resource multiple VNFs context switch upscaling context switch arise multiple cpu VNFs cpu load context upscaling incur VNFs multi core implementation suffer penalty due load balance cpu core affect chain VNFs network performance requirement SFCs evaluate impact SFCs bandwidth latency requirement scenario VNF consolidation introduction network function virtualization NFV emerge promising technique network operator reduce NFV concept network function abstract building software network traffic accomplish specific task network function firewall network address translator traffic monitor complex entity 4G 5G service packet gateway network function implement dedicate hardware refer middleboxes middleboxes handle traffic load expensive provision cycle additionally cannot easily purpose dimension peak load waste resource traffic peak NFV paradigm consists hardware software implementation network function virtualized environment multiple heterogeneous virtual network function VNFs host generic commercial shelf COTS hardware NFV flexibility network allows network operator efficiently consolidate VNFs provision another NFV simplicity deployment heterogeneous network service NFV exploit concept service function chain accord service web browsing VoIP etc service function chain SFCs concatenation appropriate VNFs traffic associate specific service weakness NFV predict performance due resource hardware function concern processing evaluate impact processing resource placement VNFs embed SFCs VNF consolidation scenario minimize amount COTS hardware deployed network identify source inefficiency performance degradation refer context switch stem central processing cpu resource VNFs additional latency packet VNF schedule cpu waste due load VNFs schedule refer upscaling additional latency processing balance network traffic multiple cpu core multi core architecture synchronize NFV instance core performance degradation affect VNFs network guarantee requirement SFCs specifically propose novel detailed node model account aforementioned processing resource exist model resource penalty model enables accurate distribution VNFs prevent excessive VNF consolidation jeopardize sfc performance knowledge evaluate impact processing resource service function chain NFV scenario remainder organize discus related concern processing resource VNF placement introduce model model physical network VNFs SFCs processing resource introduces VNF consolidation formulate integer linear program ILP model propose heuristic algorithm define scalable illustrative numerical realistic network scenario obtain ILP model heuristic algorithm focus embed diverse SFCs comparison finally conclusion related related processing resource multi core architecture VNF placement sfc embed recall related respect topic processing resource literature investigate challenge arise processing resource ref investigate processing resource challenge due adoption multi core architecture ref survey architectural upgrade efficiently processing performance adopt multi core technology ref argues adoption multi core dominant trend network device hardly fully exploit multiple core challenge related multi core load balance complex ref investigates load balance layer architecture become bottleneck carefully performance penalty ref defines novel adaptive traffic distribution cpu core per packet basis mitigate issue later upscaling performance degradation due load balance due processing resource another issue arises related operation perform processor context switch context switch thoroughly investigate literature related load context cpu enable execution multiple cpu cpu ref defines methodology quantify related context switch latency ref investigates context switch due cache interference multiple cpu additionally ref focus analogous issue related accelerate service graphic processing gpus ref instead investigate impact context switch NFV specifically ref defines strategy reduce context switch ref implement algorithm efficient processing resource VNFs specific cpu finally ref latency aware NFV scheme software middleboxes dynamically schedule accord traffic resource affect latency network due processing resource upscaling context switch goal evaluate impact VNF placement sfc embed VNF placement sfc embed formalize optimization VNF placement sfc embed extension virtual network embed VNE ref SFCs virtual network chain VNFs virtual node virtual link sequential SFCs embed physical network virtual link mapped physical multiple virtual node mapped physical node virtual node consolidated sfc embed guaranteed virtual node multiple virtual network another SFCs physical graph ref investigate optimal embed SFCs network VNE approach author formulate mixed integer quadratically constrain evaluate optimal placement VNFs formulate extend analysis processing resource aspect dealt placement VNFs network ref formulate ILP model optimal VNF placement sfc embed objective function ref minimizes server ref maximum utilization link ref opex resource utilization ref network load ref VNF mapping ref overall consume bandwidth ref traffic delivery instead ref maximizes throughput ref heuristic algorithm improve scalability model concern heuristic algorithm VNF placement ref proposes genetic algorithm network load balance ref aim minimize sfc embed objective maximally consolidate VNFs ref account processing resource practical flavor ILP allows obtain static placement VNFs definition implementation online algorithm dynamic deployment specifically ref implement dynamic VNF chain openflow openstack ref define algorithm online VNF schedule placement ref account service function chain aspect ref ref focus definition algorithm online embed SFCs minimize resource consume infrastructure embed sfc request unlike ref leverage online service demand prediction proactive VNF provision finally ref online algorithm proven competitive ratio respect objective maximize throughput implement online algorithm sfc embed define heuristic algorithm VNF consolidation embeds sfc online algorithm model model NFV enable network comprises representation internet service provider isp network physical link node service implement chain VNFs atomic VNFs deployed NFV capable physical node model NFV node trend towards architecture core node micro datacenter host VNFs NFV node limited amount processing split multiple computational core achieve parallelism VNFs adopt load balance mechanism employment cpu additional latency traverse node furthermore multiple VNFs involve computational contention employment cpu VNF coordination additional latency packet schedule reference VNF physical network sfc VNF model physical network model physical network graph network node packet capability link capacity latency subset network node equip hardware capable execute VNFs model agnostic respect physical location node cabinet central core exchange etc generally refer node NFV node NFV node ideal zero latency infinite bandwidth loop later NFV node equip multi core cpu processing capacity express cpu core multi core cpu physical node capability node legacy router switch assume correspondence NFV node multi core cpu interchangeably multi core cpu NFV node physical topology node equip generic multi core COTS hardware NFV node service function chain network operator provision service deploys SFCs network sake simplicity correspondence service sfc carrier goal embed SFCs sfc model VNF request virtual link chain consecutive VNF request topological perspective VNF request virtual node model decouple concept VNF VNF request sfc chain VNF request VNF request mapped specific VNF mapping parameter  input parameter  allows relate VNF request specific VNF request moreover NFV node VNF instance VNFs multiple VNF request SFCs mapped VNF instance VNF consideration fix network introduce input mapping parameter  explicit mapping sfc specific physical node SFCs embed physical network VNFs processing resource NFV node multiple SFCs VNF SFCs embed physical network VNFs processing resource NFV node multiple SFCs VNF model sfc aggregate user nuser aggregate SFCs deployed multiple user service sfc associate performance constraint aggregate request bandwidth  bandwidth guaranteed VNF request service sfc user     request bandwidth per user virtual link virtual link associate bandwidth requirement chain VNFs traffic throughput maximum tolerate latency maximum delay introduce network without affect service sfc virtual network function VNF perform operation network traffic VNF host NFV node VNF instance assign dedicate amount processing per perform operation input traffic processing express cpu core NFV node assign instance VNF instance VNF fully consumes NFV node processing resource cpu core plus resource cpu core VNF instance assign processing capability define processing per VNF request express cpu core indicates processing resource dedicate VNF request mapped instance VNF ratio theoretical maximum VNF request instance VNF host NFV node depends user nuser per sfc   processing per user VNF VNFs characterize  largely complexity perform operation model processing resource multiple VNFs computational resource available NFV node performance degradation due processing resource already introduce identify context switch upscaling context switch NFV node cpu core instance VNFs processing resource core VNF dedicate processing resource core cpu VNFs VNF processing resource multiple core VNF execute core multiple multiple core performance penalty refer context switch cpu dedicate processing capacity perform operation context switch degradation due context switch increase latency introduce NFV node latency reduction actual processing capacity host VNFs processing accord model context switch linearly increase respect overall NFV node pictorial cpu multiple VNFs responsible context switch model context switch upscaling express  source  sourcewhere indicates overall involve context switch operation NFV node context switch latency parameter context switch processing parameter cpu core parameter NFV node adopt multi core cpu technology  quantity express  cpu core upscaling instance VNF NFV node processing resource cpu core happens VNF quantity traffic user VNF network traffic handle VNF balance multiple cpu core network traffic balance accord layer load balance responsible upscaling VNF host NFV node dedicate load balancer decision traffic sort cpu core load balancer auxiliary VNF perform specific task balance traffic cpu core balance decision dedicate processing capacity perform operation context switch overall degradation due load balance increase latency introduce NFV node VNF latency reduction actual processing capacity host VNFs processing model upscaling concern latency processing linearly increase respect cpu core traffic balance express  source  sourcewhere indicates core involve load balance instance VNF NFV node upscaling latency parameter upscaling processing parameter heterogeneous NFV node load balance layer implement  quantity  cpu core pictorial load balance layer responsible upscaling focus physical processing resource VNFs isp network micro datacenters horizontal VNF contrarily happens datacenters virtual processing resource vcpus mapped physical resource cpu core task perform hypervisor depict hypervisors mapping physical virtual resource evaluation performance hypervisors outside scope assume optimal mapping physical virtual resource occurs VNF consolidation summary parameter define report focus VNF consolidation physical network topology SFCs dedicate cpu core chain VNFs minimize active NFV node node host VNF network optimization useful network operator placement COTS hardware active NFV node network NFV implementation minimize VNFs maximally consolidated adopt approach VNF consolidation define ILP model allows obtain optimal however specify virtual network embed virtual network embed NP heuristic algorithm heuristic aware algorithm HCA allows obtain suboptimal shorter placement SFCs summary graph model summary parameter model ILP model description report decision variable ILP model model extends formulation propose sfc embed evaluation NFV node processing capacity VNF processing requirement additionally upscaling context switch latency reduce node processing capacity model objective function constraint decision variable ILP model objective function min  sourcethe objective function simply minimizes active NFV node already propose constraint grouped category request placement rout performance constraint request placement constraint ensure mapping VNFs NFV node mapping VNF request VNFs rout constraint guarantee mapping virtual link physical finally performance constraint related performance requirement guaranteed physical network SFCs request placement constraint mcu  source mcu  source  source    source SourceRight click MathML additional feature SourceRight click MathML additional feature   source    source    guarantee fix sfc mapped node specify parameter  mapped node fix variable SFCs priori VNF request sfc mapped exactly node overall VNF request mapped VNF instance host node cannot overcome remind  user per sfc processing requirement per VNF ensure flag variable specify VNF mapped node parameter maximum maxv strict inequality linearize guarantee instance VNF node request mapped compute overall context switch upscaling processing per node define overall cpu processing assign instance VNFs cannot overcome actual processing capacity node ceiling function linearize rout constraint  mcu mcu source  mcu mcu SourceRight click MathML additional feature  mcu mcu SourceRight click MathML additional feature  source  source   source  source  source   guarantee physical link belong node virtual link sfc consecutive VNF request mapped node respectively mcu mcu binary variable linearize respectively source destination constraint assures virtual link consecutive VNF request originates link node VNF request mapped link node VNF request mapped ecx mcu mcu  mcu mcu linearize addition source destination constraint model guarantee spurious link introduce mapping virtual link sfc physical node VNF request mapped incoming physical link node outgo link node transit constraint generic node neither source node destination node virtual link incoming link belongs node outgo link belong without multiple incoming outgo link  finally ensure usage loop introduce loop NFV node consecutive VNF request mapped node cannot otherwise moreover physical link loop VNF request mapped node performance constraint   SourceRight click MathML additional feature       source   source  MF avv source   bandwidth constraint assures overall bandwidth  request virtual link sfc mapped physical link cannot exceed capacity link compute overall context switch upscaling latency per sfc define refers latency introduce NFV node sfc latency constraint assures latency introduce network sfc cannot overcome maximum tolerate latency account latency introduce propagation physical link NFV node due upscaling context switch linearization binary variable mcu variable ceiling function finally assure node marked active instance VNF host parameter MF chosen MF heuristic aware algorithm description heuristic aware algorithm HCA sequentially embeds SFCs algorithm assume physical link bandwidth capacity accommodate bandwidth request sfc link capacity bottleneck meaning network  usually occurs core network pseudocode HCA algorithm greedy procedure embed sfc already VNF instance already active NFV node phase latency requirement sfc met phase phase improve algorithm heuristic aware algorithm HCA sort SFCs increase latency sfc phase VNF request sfc instance VNF network sort VNF instance increase distance VNF instance node VNF instance VNF instance failure sort NFV node increase residual capacity VNF instance NFV node NFV node fail return infeasible VNF request chain phase latency embed sfc latency requirement failure release resource allocate phase chain VNF instance along latency shortest latency embed sfc latency requirement fail return infeasible SFCs embed return embed SFCs sort increase accord latency sensitive SFCs sfc embed phase phase VNF request chain sfc already VNF instance VNF request network VNF instance available sort increase distance mapped closest NFV node node accord latency shortest SPs node embed NFV node VNF request mapped HCA processing resource VNF instance NFV node additional aggregate traffic passing VNF request operation increase context switch upscaling node increase context switch upscaling latency compromise latency already embed SFCs VNF request mapped already sfc happens update latency overcomes fails VNF instance another NFV node checked otherwise successful execute VNF request mapped VNF instance latency SP node NFV node steer sfc aggregate traffic physical node fail trigger increase context switch upscaling processing residual processing capacity available fails already VNF instance fails algorithm sort remain NFV node increase residual capacity VNF instance NFV node NFV node operation increase context switch upscaling algorithm latency already embed SFCs VNF request mapped compromise residual processing capacity NFV node update context switch upscaling processing host VNF instance operation succeed VNF request mapped VNF instance latency SP physical network node NFV node steer sfc aggregate traffic physical node operation described imply activation NFV node residual processing capacity available already active NFV node phase placement VNF request fails related sfc cannot embed lack processing resource overall embed aborted conversely VNF request greedily mapped NFV node NFV node VNF request mapped node latency SP physical network phase embed sfc greedy sfc embed built accord phase NFV node already active feature objective maximum consolidation however latency sfc placement unaware relative sfc respect NFV node chain VNF request phase assures traffic VNFs locally steer accord latency SPs NFV node VNFs mapped guarantee effective latency phase execute phase evaluation latency built phase latency embed sfc successful happens typically SFCs loose latency requirement otherwise discard processing resource allocate phase release involve VNF instance embed built latency SP sfc minimize latency introduce link maximally consolidate VNF instance activate inactive NFV node SP VNF instance chain VNF request update context switch upscaling NFV node placement multiple VNFs algorithm chooses NFV node maximum processing capacity facilitate usability node future embeddings finally latency checked latency requirement cannot met NFV node activate feasible otherwise sfc embed sfc SFCs successfully embed operation fails embed SFCs guaranteed sfc NFV node exists latency SP overcome issue phase algorithm computes latency shortest SP chosen ensure NFV node explore pictorial sfc concatenate VNF request embed physical network accord phase improve phase HCA sort SFCs increase accord latency requirement activate NFV node  latency requirement latency sensitive SFCs reuse NFV node embed SFCs looser latency latency optimize pictorial HCA phase phase execution computational apply algorithm network VNF consolidation processing resource parameter introduce impact NFV implementation scenario computational setting ILP model HCA instance finally deepen investigation HCA instance strategy approach computational setting isp physical network topology physical node fifteen physical link network topology internet network node advanced layer service latency introduce physical link due propagation millisecond shortest link link assume bandwidth physical link cannot bottleneck physical node NFV node equip multi core cpu moreover NFV node characterize context switch upscaling parameter latency processing assume latency processing parameter concern context switch upscaling linearly dependent accord another parameter   computational unless otherwise specify VNFs report VNF processing requirement per user  processing requirement VNFs obtain accord middleboxes datasheets ratio adopt cpu core middlebox rough estimation processing requirement strategy cpu cycle allows understand processing hungry VNFs accord estimation traffic monitor processing hungry firewall VNFs chain heterogeneous SFCs report SFCs service web service WS VoIP video VS CG performance requirement bandwidth maximum tolerate latency sfc assume virtual link sfc bandwidth performance requirement WS VoIP VS performance requirement CG recent service gain research community due technical challenge processing requirement per user VNFs performance requirement SFCs scenario mixed scenario homogeneous scenario former niter computational instance uniformly randomize choice sfc embed SFCs physical node network latter uniformly randomize choice assume sfc embed network report summary tunable parameter evaluation reader understand ILP HCA performance comparison ILP model ilog cplex solver implement HCA matlab computational workstation equip ghz cpu core GB ram obtain ILP HCA focus processing resource setting upscaling context switch chose aforementioned NFV node latency millisecond evaluation mixed scenario setting simulated increase SFCs overall user network nuser user nuser load network scenario randomize instance niter randomize instance niter report average active NFV node along percent confidence interval report percentage infeasible computational instance execution per computational instance HCA closely obtain ILP setting infeasible instance HCA allows obtain optimal computational millisecond per instance ILP model load network slightly HCA ILP instance infeasible latency requirement SFCs cannot met processing NFV node VNFs context switch infeasibility percentage HCA percent ILP percent ILP feasible cannot explore HCA additionally HCA activates average NFV node optimal however execution per computational instance HCA ILP context switch upscaling setting happens context switch upscaling setting solver compute ceiling function nonlinear linearize implies compute solver simpler ILP model performance comparison ILP HCA tunable parameter evaluation mixed scenario verify effectiveness HCA instance deepen obtain mixed scenario impact context switch active NFV node evaluate function overall user network nuser upscaling negligible niter embed SFCs overall user network equally split SFCs user network nuser nuser evaluate impact SFCs embed VNF consolidation without overall network load instead impact upscaling context switch negligible adopt setting plot percentage infeasible instance percent active NFV node function overall user network impact latency context switch SFCs mixed scenario niter active NFV node function overall user network impact latency upscaling SFCs mixed scenario niter curve non decrease trend increase user network implies increase processing requirement VNF instance NFV node activate accommodate VNFs trend increase relative difference curve strongly increase relative difference significantly moreover curve weak dependence upscaling SFCs meaning upscaling impact context switch active NFV node homogeneous scenario obtain homogeneous scenario active NFV node network function context switch parameter SFCs SFCs requirement chain VNFs diverse impact NFV implementation active NFV node difference mainly due distinct impact context switch processing requirement VNFs chain SFCs easy WS VS SFCs concatenate VNFs average processing requirement per user VNFs chain VoIP SFCs explains active NFV node WS VS VoIP CG homogeneous scenario increase context switch parameter increase active NFV node processing latency context switch increase harder VNF processing sfc latency requirement activate node however impact context switch VoIP homogeneous scenario noticeable latency requirement strict processing requirement chain VNFs additionally impact context switch WS VS despite happens average processing requirement VNFs chain WS VS SFCs recall earlier however curve diverge active node VS WS VS SFCs stricter latency requirement WS SFCs latency introduce NFV node due context switch becomes significant implies node activate VF SFCs latency requirement finally CG SFCs strict latency requirement context switch impact NFV implementation NFV node activate guarantee latency requirement VNFs physical sfc latency shortest important curve mixed scenario average respect curve homogeneous scenario latency introduce NFV node due context switch affect placement VNFs ensure latency requirement latency sensitive SFCs CG met consideration upscaling active NFV node function latency context switch parameter homogeneous scenario accord SFCs nuser niter active NFV node function latency upscaling parameter homogeneous scenario accord SFCs nuser niter CG homogeneous scenario impact NFV implementation investigate percentage CG SFCs influence NFV active node active NFV node function percentage CG SFCs focus SFCs nuser nuser nuser examine context switch upscaling concern context switch CG SFCs impact active NFV node nuser percent CG SFCs significantly increase average active NFV node respect CG SFCs embed percent upscaling nuser significant increase NFV active node percentage percent previously mixed scenario conclude context switch impact NFV implementation upscaling active NFV node function percentage SFCs context switch latency upscaling latency aggregate user nuser per sfc niter comparison exist NFV node model estimation introduce latency processing resource aspect adopt model SOTA merely considers non linear increase node latency function node utilization neglect upscaling context switch sfc embed obtain propose processing resource node model relies introduce obtain adopt SOTA model propose modify HCA embed SFCs accord simplify model specifically   NFV node utilization latency introduce NFV node VNF instance host node compute  PK  SourceRight click MathML additional feature per compute node latency millisecond obtain model adopt simplify evaluation assume processing penalty SOTA model consideration processing capacity core assign VNFs NFV node naturally arise processing resource latency penalty tends underestimate NFV node latency VNF consolidation active NFV node function overall user network SOTA propose node model SFCs mixed scenario niter average sfc latency function overall user network SOTA propose node model SFCs mixed scenario niter impact node model active NFV node investigate function overall user network nuser mixed scenario active NFV node adopt SOTA model processing resource model meaning SOTA model consolidates VNFs NFV node however curve overlap SOTA model active NFV node invariant conversely model activates NFV node increase happens increase VNF instance embed network processing resource penalty arise SOTA model relevant parameter node utilization roughly model avoids excessive VNF consolidation compromise sfc latency latency sensitive SFCs instead average experienced sfc latency model mixed scenario average latency adopt SOTA model latency underestimation mostly dangerous latency sensitive SFCs CG SFCs embed estimate latency requirement reality latency penalty due processing resource conclusion investigate impact network processing resource VNFs NFV multiple SFCs embed network VNF placement distribution NFV node upscaling context switch affect placement VNFs embed SFCs focus mathematical model NFV node VNFs SFCs processing resource formulate ILP model heuristic algorithm HCA evaluate impact VNF consolidation HCA allows obtain optimal shorter ILP model numerical focus attention placement practical SFCs isp network context switch impact VNF consolidation upscaling besides strongly affect NFV consolidation SFCs strict latency requirement SFCs embed network aspect cannot capture node model neglect processing resource aspect issue remain future research processing resource resource issue resource issue concern memory storage investigate moreover VNF consolidation objective network operator interested achieve objective account bandwidth resource physical link consideration