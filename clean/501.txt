federate FL emerge approach collaboratively training neural network dnns mobile device without private user data device previous non independent identically distribute non iid user data harm convergence FL algorithm furthermore exist FL global model accuracy user content recommendation improve individual user model accuracy UA objective address issue propose multi task FL MTFL algorithm introduces non federate batch normalization BN layer federate dnn MTFL benefit UA convergence user model personalise data MTFL compatible popular iterative FL optimisation algorithm federate average FedAvg empirically distribute adam optimisation FedAvg adam benefit convergence optimisation strategy within MTFL mnist cifar demonstrate MTFL significantly reduce target UA exist FL optimisation strategy improvement FedAvg adam MTFL compete personalise FL algorithm achieve UA mnist cifar scenario finally evaluate MTFL FedAvg adam compute testbed convergence UA benefit outweigh overhead introduction multi access compute MEC service network enable latency processing application via content cache computation offload couple rapidly increase quantity data smartphones internet iot device social network SNs MEC opportunity quantity data source neural network dnns machine ML become increasingly popular potential application deployment performance training dnns supervise however computationally expensive enormous amount training data trend increase dnn dnns MEC typically involve data mobile phone iot device SNs perform training deploy model concern data privacy however user increasingly unwilling upload potentially sensitive data model federate FL horizon ML FL participate client collaboratively ML model typically dnns without reveal private data publish initial investigation FL federate average FedAvg algorithm FedAvg initialise model coordinate server distribute model client client perform training local datasets model server server average model aggregate model client training refer institution etc data FL user device actually participate FL client FL promising approach distribute ML situation data cannot uploaded client privacy therefore FL scenario analyse sensitive healthcare data prediction mobile keyboard content recommendation however FL multiple unique challenge client usually independent identically distribute iid training data client data generate noisy data subset feature label factor substantially hinder training FL model FL research typically performance metric global model accuracy centralise however individual model accuracy client objective motivate personalise FL creates unique model FL client improve local performance however incorporate personalisation FL remains research topic due non iid client datasets performance global FL model client others client model independently address challenge propose multi task FL algorithm MTFL allows client personalise dnns improve local model accuracy enhance client privacy MTFL storage personalisation compute personalise FL algorithm extra sgd client training loop personalisation client datasets FL usually non iid client attempt optimise model local training disparate task MTFL approach batch normalisation BN layer commonly incorporate dnn architecture private client previously private BN layer improve multi task MTL performance joint training imagenet centralise private BN layer dual benefit personalise model client local data preserve data privacy parameter client model uploaded server information client data distribution glean uploaded model MTFL approach BN layer storage benefit personalise FL algorithm BN layer typically parameter dnn BN parameter FL entire personalise dnn model compete algorithm MTFL personalisation typical iterative FL framework FedAvg popular algorithm instance iterative optimisation framework FL algorithm vanilla stochastic gradient descent sgd client however momentum optimisation strategy adam potential improve convergence FL training distribute optimisation technique adam FedAvg adam substantial speedup communication FedAvg within MTFL algorithm contribution propose MTFL algorithm multi task iterative FL algorithm user dnn model personalise data MTFL private batch normalisation BN layer achieve personalisation privacy benefit propose metric performance FL algorithm user model accuracy UA UA reflect objective FL increase accuracy client oppose standard global model accuracy analyse impact private BN layer activation MTFL model inference insight source impact analyse training performance MTFL parameter statistic BN layer private demonstrate MTFL balance convergence  FL independent training conduct extensive simulation mnist cifar datasets MTFL FedAvg target UA FL FedAvg adam improvement MTFL significantly improve average UA personalise FL algorithm perform MEC testbed consist raspberry client FL server MTFL FedAvg adam overhead outweigh substantial UA convergence benefit organise describes related detail propose MTFL algorithm private BN layer within MTFL training inference FedAvg adam optimisation strategy discus simulation MEC testbed concludes related address challenge exist FL algorithm overview related sub topic FL personalisation practical deployment challenge aim improve convergence global model performance personalise federate author approach personalise FL model tailor model performance non iid user datasets meta aim model easy tune sample propose per FedAvg algorithm model agnostic meta  adaptation client loss function tune client datasets highlight connection FedAvg  update propose stage training algorithm improve personalisation author propose training combination local global model FL improve personalisation   learnable parameter client extent local global model global model personal model user perform sgd personal model update global model outer loop local model client proximal client loss function model personalise model silo FL propose  performs federate MTL formulates FL function model matrix relationship matrix algorithm account heterogeneous hardware client meaning  directly comparable MTFL scheme recently generalise  algorithm  framework propose decentralise version proposes multi task approach achieve personalisation FL MTFL later approach substantial converge personalisation performance privacy storage benefit exist personalise FL algorithm federate compute FL performs distribute compute network author communication FL environment propose FL reduces data client upload model gradient magnitude implementation detail asynchronous robin client update FedAvg specify client server role fault handle security analytics deployment client address non iid client datasets FL propose  framework client datasets augment reduce local imbalance mediator introduce global aggregation author investigate impact wirelessly FL client schedule policy wireless FL scenario analysis signal interference plus ratio SINR FL scheme perform SINR increase intelligent client propose hybrid federate distillation scheme FL wireless device compute compression scheme performance wireless scenario author propose scheme compute networking communication resource FL client compute perform smartphones argue computation oppose communication FedAvg significant bottleneck FL propose algorithm accommodate computational heterogeneity   information compute wireless resource client initiate FL reduce target accuracy FedAvg previous propose implementation FL however MTL within FL contribution MTFL algorithm federate performance seminal FedAvg algorithm collaboratively model initial model participate client perform sgd model local data model server average progress towards improve convergence rate FedAvg adam adaptive optimisation update global model server generalise adaptive optimisation technique style convergence guarantee FedAvg adam algorithm differs client FedAvg adam perform adam sgd oppose vanilla sgd adam parameter average alongside model server momentum sgd client aggregate momentum client server alongside model alternative accelerate convergence investigate FL non iid quality client data propose amount data client decrease difference data distribution improve global model accuracy   evaluate client quality data difference client model local prediction prediction trust dataset ignore irrelevant client update training client update aligns global model FedAvg adam optimisation adaptive optimisation client sgd later converges faster FedAvg adam optimisation purely server multi task federate MTFL overview MTFL algorithm compute environment detailed description BN patch MTFL optimisation client later subsection operation MTFL algorithm compute training perform termination met server selects subset client database participate sends request client accept message physical local preference client global model optimisation parameter server update global model private patch BN layer patch client perform local training personal patch server client upload non private model optimiser limit server average model aggregate MTFL algorithm client server framework however initiate server server selects subset client database asks participate FL sends request message client accept request user preference user device participate FL wifi accept client accept message server server sends global model associate optimization parameter accept client augment global model private patch client perform local training data model client patch layer model locally upload non private model parameter server server client training upload model maximum limit client upload server preference server aggregate model global model server MTFL therefore offloads vast majority computation client device perform actual model training preserve user data privacy strongly FedAvg personalise FL algorithm user data uploaded local model uploaded framework account client straggler upload client limit moreover MTFL utilises patch layer improve local model performance individual user non iid datasets MTFL personalise user model accuracy MTFL FL FedAvg author central iid FL performance FL scenario metric desirable intention model performance iid data suitable however FL scenario desire model performance individual user device google FedAvg  prediction software objective improve prediction individual user user typically non iid data global model display performance user performance others propose average user model accuracy UA alternative metric FL performance UA accuracy client local client drawn distribution training data perform classification UA alter metric error recall FL user data non iid user related task FL scheme achieve global model accuracy UA aggregate model perform poorly client datasets local sample FedAvg average propose MTFL algorithm allows client model benefiting FL improve average UA previously per task patch layer dnns improve performance MTL scenario patch therefore candidate training personalise model client FL aim minimise objective function   sourcewhere client sample client sample across client loss function client global model parameter unique client patch FL model objective function MTFL   source  pkm  sourcewhere patch model client compose federate model parameter federate layer patch parameter pkm local patch index patch parameter unique client composition dnn model MTFL composition dnn model MTFL client model consists global parameter convolutional conv fully FC layer private batch normalization BN patch layer MTFL algorithm incorporate MTL FL optimisation strategy FedAvg adam described within MTFL later MTFL substantially reduce target UA regardless optimisation strategy algorithm MTFL initialise global model global optimiser termination criterion met client client parallel global parameter optimiser  apply local patch batch drawn local data   local patch  upload server    algorithm MTFL communication termination criterion target UA met subset client participate client client global model tuple model parameter global optimiser client update global model optimiser private patch layer  variable contains index patch layer placement dnn client perform training personalise global model optimiser local data choice FL optimisation strategy within MTFL  function local training model FedAvg  simply minibatch sgd discus propose FedAvg adam optimisation strategy local training update local patch non patch layer optimiser uploaded server server global model optimiser accord   function function dependent FL optimisation strategy FedAvg average client model  update global model per computation complexity MTFL client participate per computation perform client independent client client perform local computation parallel MTFL FedAvg  scalable scalability important FL deployment client global model optimiser update algorithm optimisation strategy FedAvg FedAvg adam  essentially reduce average local training  adam reduce  dependent client dnn architecture numerous investigate FL peer peer FL algorithm involve client model participate peer decentralise aggregation extension MTFL scheme trivial peer simply aggregate non private layer sophisticated FL algorithm complex incorporate private layer direction future batch normalisation BN layer model patch MTL centralise later BN layer patch MTFL lightweight parameter BN layer var BN  sourcewhere var variance neuron activation nonlinearity across minibatch parameter training BN layer average var training inference investigate benefit statistic trainable parameter private patch layer chosen BN layer personalisation within MTFL choice twofold excellent personalisation performance storage BN parameter model model architecture investigate depthwise convolutional patch centralise multi task model layer principle private MTFL however inherent parameter private ability global model converge BN patch inference understand impact BN patch layer UA internal dnn activation client local immediately immediately FL aggregation illustrate UA typically aggregation iterative FL model tune local training epoch suddenly model replace federate unlikely performance pre aggregation model examine later experimental dnn consist dense layer BN nonlinearities vector layer neuron activation client apply bias model normal distribution BN relies   var  local training client model adapt local dataset BN layer statistic inference update layer activation assume local training aggregation var BN layer ignore computes   sourcewhere BN parameter client participate FL MTFL model parameter update global model federate activation layer var SourceRight click MathML additional feature define difference variance pre aggregation activation   var var output BN patch layer MTFL maintains aggregation BN     SourceRight click MathML additional feature BN layer patch layer client participate FL federate BN output BN layer BN    iσi  sourcewe posit BN patch layer MTFL constrains neuron activation closer aggregation non patch BN layer FL illustrate difference variance pre aggregation MTFL FL    iσi   inequality easy propagate network layer closer pre aggregation BN patch oppose federate BN layer BN patch throughout network intermediate dnn regularly constrain closer pre aggregation ultimately network output closer pre aggregation output federate FL accuracy curve UA decrease aggregation increase local training smoother accuracy curve training independently ind patch BN layer distribution output neuron closer pre aggregation distribution   neuron output distribution neuron BN patch layer BN BN BN patch layer therefore benefit federate BN layer inequality unlikely difference pre aggregate model parameter stage training gradient client model diverge local training therefore implies MTFL benefit stage training benefit increase training gradient magnitude decrease average training  solid user accuracy UA curve local sgd mnist scenario FL FedAvg MTFL FedAvg plot within BN layer MTFL private statistic trainable parameter FL curve smooth average kernel presentation inset plot cyclic accuracy due model average characteristic FL federate optimisation within MTFL algorithm MTFL applies private patch layer client alongside federate non private layer  server aggregate uploaded federate layer client distribute optimiser global model  function distribute adaptive optimisation  function detail FL training algorithm characterise implementation function    FedAvg  FedAvg adam FL training strategy FedAvg  simply minibatch sgd  global model local sample average uploaded client model FedAvg sgd adaptive optimisation variable algorithm tuple empty  performs function  client perform sgd  however  server difference previous global model average uploaded client model server treat  server update global model adam update client distribute adaptive optimisation  tuple empty  performs function propose adaptive optimisation namely adam distribute optimisation strategy strategy FedAvg adam FedAvg adam client global adam variable algorithm client private optimiser patch layer performance private optimiser patch  client perform adam sgd federate model layer adam uploaded client global model server average client model  average adam  FedAvg adam therefore communication per FedAvg  however FL scenario concern reduce communication model converge later FedAvg adam considerably improves convergence FL MTFL refer iterative FL scheme private model patch FL optimisation strategy bracket FL FedAvg client private model patch refer scheme MTFL optimisation strategy bracket MTFL FedAvg detail datasets model data partition scheme extensive analyse impact MTFL target UA examine BN private performance FL MTFL optimisation strategy investigate private BN impact training MTFL personalise FL algorithm finally evaluate computation MTFL FedAvg adam MEC testbed datasets model conduct image classification datasets mnist cifar dnn architecture mnist pixel greyscale image handwritten digit NN network dataset fully FC layer neuron BN layer neuron FC layer softmax output layer cifar pixel rgb image cnn network dataset convolutional conv layer filter BN relu max pool conv relu layer filter BN relu max pool neuron relu FC layer softmax output layer client client participation rate optimisation strategy non iid client non iid client data popular approach training data label split shard assign client shard random assignment index data client training splitting strongly non iid distribution across client average trial random patch layer FL setup target average UA percent mnist percent cifar MTFL FL FL model parameter private patch whereas MTFL model parameter private MTFL BN layer statistic trainable parameter private patch layer communication target average user accuracy task FL MTFL private statistic parameter client WW client participation rate CC optimisation strategy communication target average user accuracy non noisy client task FL MTFL private statistic parameter percent client noisy training data client WW client participation rate CC optimisation strategy fix local epoch tune rate hyperparameters scenario target FedAvg FedAvg adam tune hyperparameter scenario  training client server rate entry denote target within communication investigate robustness MTFL client noisy training data percent client random gaussian training data average UA non noisy client MTFL mitigate noisy client non noisy client gaussian standard deviation mnist cifar mnist simpler image classification task cifar significantly hinder training MTFL BN layer private target average UA substantially almost scenario FL cifar scenario FL FedAvg target average UA whereas MTFL FedAvg private however tracked statistic BN patch private UA actually harm conversely MTFL private trainable parameter MTFL private scenario MTFL FedAvg private whereas MTFL FedAvg private investigate difference MTFL naturally increase variance  training non identical user model MTFL contribute variance  however perform difference variance UA FL MTFL percent MTFL mitigate impact noisy client non noisy client FL noisy client prevent average non noisy UA target scenario however MTFL non noisy client target average UA correspond non noisy scenario cifar scenario FL FedAvg target however MTFL FedAvg private parameter display non noisy client target average UA improvement MTFL due non noisy client decouple noisy model parameter harmful global model harm participation noisy client reduce accuracy faster scenario FedAvg adam optimisation strategy target average UA regardless FL MTFL cifar scenario FL FedAvg FL  FL FedAvg adam target similarly MTFL FedAvg MTFL  MTFL FedAvg adam private training MTFL setup investigate MTFL BN patch private private plot training UA scenario namely mnist FL FedAvg MTFL FedAvg algorithm communication client perform local training calculate average training UA local graph therefore accuracy impact average training training private BN smooth training  combination BN layer statistic parameter mnist smooth presentation curve target accuracy correspond training curve FL FedAvg MTFL FedAvg private BN statistic influence training accuracy private FedAvg mirror accuracy due mismatch BN average output distribution private statistic harm ability model private substantial performance private private significantly increase rate training accuracy improve  previous author comment FedAvg  client model client local datasets training error quickly easy independently model overfit however generalisation performance motivation FL model parameter private BN layer strike balance convergence achieve fully private model  due FL achieve average client model parameter personalise FL comparison setup personalisation performance MTFL FedAvg personalise FL algorithm per FedAvg  FL FedAvg model layer private tune hyperparameters algorithm achieve maximum average UA within communication MTFL FedAvg private MTFL FedAvg adam personalisation algorithm benefit adaptive optimisation strategy fix amount local computation roughly constant algorithm perform epoch local training MTFL FedAvg FL FedAvg mnist batch equivalent local sgd respectively cifar equivalent local sgd respectively per FedAvg local iteration fix FL MTFL  inner loop outer loop per FedAvg fix inner loop  scenario setup local perform algorithm however per local per FedAvg  considerably FL FedAvg MTFL FedAvg MTFL FedAvg FL FedAvg hyperparameter tune whereas per FedAvg  hyperparameter per FedAvg  considerably costly plot MTFL FedAvg achieve UA scheme scenario per FedAvg  UA FL FedAvg mnist actually personalise FL scheme achieve UA faster FL FedAvg cifar however likely due cifar task harder mnist per FedAvg overfit quickly task worthy MTFL FedAvg per FedAvg  whilst hyperparameter tune computationally cheaper MTFL extra benefit privacy model parameter private  per FedAvg upload entire model per user accuracy UA FL algorithm FL FedAvg MTFL FedAvg private  per FedAvg conduct mnist cifar data non iid fashion client user participate per shade percent confidence interval per trial random testbed setup MTFL realistic MEC environment testbed consist client raspberry rpi rpi wifi server emulate heterogeneous client rpi tensorflow perform local training server perform model average distribute model average along percentage spent per model server local training upload model perform server average per FL FedAvg MTFL FedAvg adam independent local epoch training perform split spent task within independent per client upload model FL FedAvg longer per due upload MTFL FedAvg adam per due increase FedAvg adam communicates FedAvg percentage spent upload model however increase communication likely outweigh target average UA average per scheme mnist cifar datasets percentage spent model training model client upload model model aggregation distribution server server majority spent local training communication FL MTFL due compute rpi computational training dnn model FL scenario influence compute ability client device computational model communication conclusion propose multi task federate MTFL algorithm iterative FL algorithm introduce private patch layer global model private layer user personalise model significantly improves average user model accuracy UA analyse BN layer patch MTFL insight source benefit MTFL algorithm specific FL optimisation strategy propose FedAvg adam optimisation scheme adam client mnist cifar MTFL FedAvg significantly reduces target average UA FL MTFL FedAvg adam reduces private BN trainable parameter instead statistic model patch convergence comparison personalise FL algorithm MTFL achieve average UA limited communication lastly MEC testbed communication overhead MTFL FedAvg adam outweigh significant benefit FL FedAvg UA convergence