ability multitasking becomes important development graphic processing gpu gpu multitasking classify temporal multitasking spatial multitasking simultaneous multitasking SMK article introduces feature commercial gpu architecture multitasking metric evaluate performance gpu multitasking review gpu multitasking hardware architecture hardware gpu multitasking hardware gpu multitasking illustrate meanwhile previous hardware gpu multitasking introduce addition characteristic hardware gpu multitasking belonging article valuable suggestion future research enhance gpu simulator bridge gap academia addition promising expand research machine technology advanced gpu architectural innovation 3D stack memory etc previous gpu multitasking nvidia gpus article focus nvidia gpu architecture nvidia terminology knowledge article survey hardware gpu multitasking survey reader gain insight research hardware gpu multitasking introduction graphic processing gpus widely various purpose compute data compute platform etc compute service multiple concurrent application application occupy entire gpu gpu application utilization gpu resource dramatically improve moreover critical release gpu resource occupy priority application priority application achieve quality service qos target therefore gpu multitasking essential significant application gpus benefit gpu multitasking instance inference efficiency multiple concurrent neural network improve spatial gpu gpu multitasking becomes important evolution gpu architecture attracts attention academia survey gpu multitasking hardware architecture gpu compose multiple multiprocessor SMs manner kernel diverse application SMs gpu multitasking classify temporal multitasking spatial multitasking simultaneous multitasking SMK temporal multitasking partition gpu kernel gpu multiplexed spatial multitasking split gpu resource SM granularity SMs gpu executes kernel SMK multiple kernel concurrently SM gpu multitasking difference temporal multitasking spatial multitasking SMK granularity resource allocation temporal multitasking gpu exclusively occupy kernel spatial multitasking allows resource outside SM interconnection network cache multiple kernel SMK enables multiple kernel resource within SM spatial multitasking SMK inter SM intra SM slice respectively preemption release gpu resource prevent priority application priority application essential critical temporal multitasking therefore temporal multitasking preemption multitasking addition preemption adjust gpu resource allocation spatial multitasking SMK gpus gpu multitasking assume kernel concurrently gpu SMs multitasking nvidia kepler gpu architecture introduces hyper technology adopts leftover policy leftover policy kernel acquire resource remain resource assign kernel concurrent kernel execution usually occurs kernel kernel addition kernel submission incurs significant impact gpu performance obviously easy multiple kernel concurrently gpu nvidia pre volta gpu architecture implement spatial multitasking usually gpu code transformation nvidia volta gpu architecture adopts hardware accelerate multi service MPS volta MPS prevent kernel occupy SMs gpu implement spatial multitasking although ability gpu SMK lack researcher implement SMK gpus transform gpu program code software gpu multitasking gpus improve concurrency performance modify application program additional overhead feasible hardware gpu multitasking gpu multitasking without program code transformation implement improve gpu architecture concerned researcher survey research hardware gpu multitasking contribution exist gpu multitasking briefly review software gpu multitasking introduce hardware gpu multitasking reader quickly familiarize gpu multitasking research discus issue concerned hardware gpu multitasking characteristic hardware gpu multitasking belonging reader easily acquire considerable insight research hardware gpu multitasking enhance gpu simulator bridge gap academia promising direction worthy explore beneficial promote future research hardware gpu multitasking organize knowledge gpu multitasking research review related research gpu multitasking focus hardware gpu multitasking challenge opportunity research hardware gpu multitasking propose finally summary background gpu architecture gpu baseline architecture illustrate constitute SMs SM arithmetic logic ALUs function SFUs compute load lsu memory data cache L1D etc executes memory access operation multi cache multiple SMs memory controller communicate chip dynamic random access memory dram cache SMs communicate interconnection network nvidia compute unified device architecture cuda program platform cuda kernel instruction load gpu execution instruction fetch decode thread constitute warp execute instruction multiple thread SIMT style warp execution private register file warp constitute thread TB thread TB synchronize data memory resource limit active TBs SM register file memory warp scheduler slot TB slot gpu architecture feature gpu multitasking nvidia dominant gpu vendor purpose compute fermi architecture nvidia gpu concurrent kernel execution hardware queue host gpu fermi architecture false intra dependency fermi advanced gpu architecture introduce kepler architecture introduces hyper software MPS feature improve ability multitasking hyper increase queue host gpu queue belong cuda context MPS multiple client cuda context cuda context leverage hyper hardware queue client software MPS inherit maxwell pascal gpu architecture volta gpu architecture hardware acceleration MPS enables MPS client submit directly queue within gpu volta MPS restrict client gpu execution resource prevent gpu execution resource occupy client fundamental hardware spatial multitasking turing gpu architecture inherits hardware accelerate MPS feature introduce volta architecture  gpu architecture multi instance gpu mig feature gpu multiple gpu partition partition behaves fully capable independent gpu mig avoid interference concurrent application preemption critical indispensable temporal multitasking pascal gpu architecture preemption instruction granularity TB granularity prior maxwell kepler gpu architecture summary ability nvidia gpu multitasking technical detail disclose evolution nvidia gpu architecture addition nvidia amd important gpu vendor amd gpu competitor nvidia gpu feature multitasking instance amd  gpu architecture partition execution resource concurrent compute task meanwhile amd  gpu architecture completely suspend execution task compute compute SM nvidia gpu priority task model performance metric source gpu simulator model hardware gpu multitasking gpgpu sim  sim multi sim gem gpu  etc simulator gpgpu sim widely addition knowledge source benchmark gpu multitasking application usually construct benchmark evaluate gpu multitasking application primarily rodinia polybench gpu nvidia compute sdk  mar  parboil etc performance metric usually evaluate gpu multitasking throughput stp average normalize turnaround ANTT fairness harmonic individual speedup hsp metric calculate equation concurrent application concurrent kernel integer ipc refers instruction per cycle  ipc application gpu alone  ipc achieve application multiple application simultaneously gpu stp ANTT quantify perceive user perceive performance respectively stp contrary ANTT stp speedup WS fairness perfect fairness hsp balance metric throughput fairness WS metric throughput stp  source ANTT  source fairness mini   source hsp  source  multitasking review exist gpu multitasking typical gpu multitasking categorize concern instance implement spatial multitasking mainly discus preemption technology preemption multitasking focus hardware gpu multitasking gpu multitasking gpu multitasking preemption multitasking temporal multitasking gpu preemption swap priority application reduces pending priority application preemption technology context switch SM drain SM flush context switch preserve context kernel release gpu resource priority kernel addition preemption latency progress context switch preemption overhead throughput context switch properly restore warp TB architectural preserve architectural warp register SIMT stack TB besides context warp architectural memory barrier SIMT stack barrier register memory therefore context gpus mainly register memory lightweight context switch central processing cpu gpu context instance nvidia kepler architecture SM KB register file KB memory context overhead preemption latency waste throughput SM drain issue TBs incoming kernel SM TBs SM alleviate throughput overhead preemption drain therefore context switch throughput overhead SM drain preemption latency drain dependent remain execution TBs SM drawback preemption latency persistent kernel lifetime TBs kernel therefore kernel scenario cannot preempt drain SM flush flush TBs without context executes kernel TBs rerun idempotent kernel idempotent kernel atomic operation exist meanwhile overwrite global memory location kernel avoid therefore TB idempotent kernel execute multiple without affect obviously strict limitation addition although preemption latency flush almost zero useful waste TB flush advantage drawback preemption technology introduce summarize preemption technology TB granularity obvious context switch SM drain SM flush overhead preemption research focus reduce overhead subsection introduce preemption gpu hardware architecture advantage drawback preemption technology partition gpu kernel idempotent boundary location relatively amount instruction architecture ISA extend instruction boundary idempotent boundary instruction encounter architectural immediately exception handle gpu program execute entry idempotent code register boundary overhead context switch minimize although propose exception implement preemption evaluate preemption technology spatial multitasking gpu context switch SM drain adjust SM allocation dynamically hardware preemption mechanism obtain deterministic turnaround meanwhile context switch acquire turnaround fairness SM drain addition context switch acquire stp drain mechanism multiple workload concurrently intuition although scenario relative characteristic benchmark propose collaborative preemption  SMs preempt  decides SMs preempt selects preemption technique TB minimize preemption overhead TB SM  evaluates latency throughput overhead preemption technology chooses preemption technology throughput overhead satisfy preemption latency preemption SM calculate finally preemption latency satisfied SMs minimize throughput overhead addition  relaxes idempotence non idempotent kernel SM preempt flush atomic global overwrite operation expands application scope flush mechanism propose lightweight context switch consist technique technology context switch allocates unused register file memory kernel TBs allocate kernel reduce therefore amount data restore reduce technique analyzes register liveness register execute preemption architectural reduction gpu register report technique compress register warp TB reduce architectural propose preemption dual kernel execution model SMK gpu candidate victim contains TBs preempt allocate TBs preempt kernel avoid resource fragmentation resource allocation alignment adopt candidate victim TB candidate preemption overhead respect preemption technique estimate meeting preemption latency constraint candidate throughput overhead victim adopts preemption technology TB minimize preemption overhead meanwhile instead preempt TBs SM dispatch TBs SM resource beneficial reduce preemption SMK gpu propose proactive preemption conventional checkpointing mechanism preemption anticipate actual request arrives actual preemption invoked incremental update relative previous perform predicate preemption drain TBs shorter perform checkpoint drain otherwise checkpointing context execute meanwhile context switch reduce context addition introduce detail preemption technology adopt typical preemption discus exception context switch context switch drain spatial multitasking gpu focus reduce overhead context switch remain reduce preemption overhead adopt preemption technology combination addition resource swap gpu virtualization propose preemption preemption technology adopt hardware preemption besides hardware preemption software preemption propose priority application within deadline slice kernel multiple sub kernel execute preemption sub kernel boundary preemption resource management algorithm proposes preemptive gpu kernel schedule scheme harness idempotence kernel gpu program model executes preemption boundary TB introduce implement software prototype enable flexible efficient gpu kernel preemption enables latency context switch reasonable overhead minimum preemption schedule independent task spatial multitasking spatial multitasking allocates SMs concurrent kernel SM gpu resource mainly interconnection network cache dram multiple kernel spatial multitasking implement optimal SM partition improve performance meanwhile interference concurrent kernel contention memory constitute cache dram degrade performance concerned spatial multitasking typical hardware gpu spatial multitasking introduce subsection implement spatial multitasking gpus application classify compute bound application interconnect memory bound application bound application SMs evenly allocate application evaluate gpu performance addition SM partition oracle oracle etc evaluate experimental spatial multitasking beneficial improve performance analyze interference application spatial multitasking gpu application demand memory bandwidth consume bandwidth damage performance application gpu severe fairness alleviate issue robin FR RR FCFS memory schedule mechanism propose replace widely FR FCFS memory scheduler FR RR FCFS memory schedule chooses memory request application robin manner prevent bandwidth intensive application starve schedule application deeper analysis interference multiple application spatial multitasking gpu previous illustrate prioritize per kilo instruction MPKI application bandwidth application improve instruction throughput WS respectively observation memory schedule scheme target scheme WS target scheme propose improve WS respectively propose dynamic resource schedule satisfy qos requirement iterative approach qos application acquires SMs gpu performance evaluate increase decrease SMs allocate qos application execute acquire performance qos requirement goal satisfy qos requirement minimum SMs implement idle SMs disabled reduce consumption allocate execute application increase performance evaluate impact fairness police SM allocation propose resource partition approach previous satisfy chosen fairness metric initial SM allocation isolation profile information application isolation profile data lack SM allocation conduct performance application evaluate concurrent execution application evaluate performance SMs reallocate desire fairness metric throughput speedup etc within variation maximum operating frequency fmax SMs gpu propose variation aware SM partition per SM  scheme SM fmax SM partition assigns compute bound application memory bound application faster SMs SMs respectively SMs allocate reallocate execution concurrent application simulation illustrate speedup execute combination compute memory bound application interference concurrent application propose dynamic application slowdown estimation  mode estimate application slowdown spatial multitasking gpu  predicate slowdown application runtime without isolated application profile information  propose dynamic SM allocation policy  minimize overall unfairness initial phase  evenly allocates SMs concurrent application SM allocation adjust accord fairness estimation runtime SM partition maximum improvement fairness demonstrate gpu application exhibit stark variety frequency sensitivity propose efficient SM allocation spatial multitasking gpu SMs architecture gpu voltage frequency VF domain application acquire online profile allocate compute intensive application memory intensive application VF domain VF domain respectively implement optimal SM partition simultaneous perturbation stochastic approximation algorithm improve efficiency spatial multitasking gpu effective SM partition workload classification significantly reduce offline profile workload accurately classify category SM bandwidth model interconnection network cache memory bandwidth account optimum SM partition strategy workload workload compute intensive memory intensive application performance maximize assign SMs compute intensive application assign SMs memory intensive application workload contains memory intensive application allocate available SMs acquire optimum performance workload consist compute intensive application improve performance SM partition detailed analysis thread parallelism tlp management technology warp granularity spatial multitasking gpu effective bandwidth EB ratio attain dram bandwidth rate application EB tlp configuration directly related performance therefore EB performance indicator application addition concurrent application maximize EB balance EB important obtain WS fairness respectively observation PBS mechanism propose optimize WS fairness hsp PBS avoid combination tlp configuration appropriate tlp combination efficiently utilization SM sub resource cache schedule execution etc obviously various application therefore sub resource underutilized multiple application spatial multitasking gpu limit fully exploit tlp improve performance alleviate propose gpu weaver dynamic sub resource management spatial multitasking gpu gpu weaver schedule execution data cache SMs application SM sub resource sufficient idle SMs SM borrow sub resource SMs accelerate execute application spatial multitasking gpu weaver implement grain sub resource SMs propose kernel slowdown model  predict slowdown concurrent application  periodically hardware statistic application adopts pre neural network predict application slowdown online neural network offline factor affect kernel slowdown factor mainly SMs assign kernel contention resource kernel sensitivity resource contention  propose enhance slowdown prediction model themis besides SM allocation themis piecewise linear regression predict slowdown concurrent application SM allocation themis implement SM allocation  slowdown concurrent application satisfy purpose qos target introduce themis SM allocation themis detail addition software stage model predict application slowdown gpu propose classify gpu workload lean tlp tlp kernel lean tlp kernel performance degrade per SM TB resource tlp kernel performance obviously improves TBs SM TB resource migrate SMs another convert baseline gpu  gpu  gpu SMs SM SM SM baseline gpu SM SM TB resource respectively  gpu architecture dynamic tlp resource aware schedule propose balance stp ANTT spatial multitasking gpu determines concurrent kernel profile phase kernel tlp SMs SMs evenly allocate kernel otherwise SMs allocate kernel benefit SMs another kernel SMs SM partition strategy propose introduce application profile workload compute intensive memory intensive application SMs allocate compute intensive application otherwise SMs evenly partition difference adopts TB throttle workload memory intensive application enhance gpu architecture mitigate memory bandwidth contention SMs evenly allocate application meanwhile cache evenly partition predefined memory bandwidth assign application epoch application bandwidth consume memory request application become priority memory scheduler service priority request priority request service optimizes memory schedule avoid dram bandwidth consume application issue memory request characteristic hardware gpu spatial multitasking introduce enhance dram controller mitigate dram bandwidth contention improve performance reduce SMs occupy memory intensive application decrease memory access SMs beneficial ameliorate gpu memory contention concurrent application besides hardware spatial multitasking researcher implement spatial multitasking gpus implement efficient spatial multitasking gpus code transformation improves utilization gpu resource guarantee qos requirement software mechanism partition compute memory resource gpu avoid interference concurrent application accelerate concurrent neural network inference discus manage multiple spatial multitasking gpus reduce data transfer overhead overlap data transfer computation context gpu multitasking characteristic hardware gpu spatial multitasking simultaneously multitasking SMK multiple kernel concurrently SM spatial multitasking SMK implement grain resource SMK gpus allocate intra SM resource TB partition concerned significant influence performance meanwhile interference concurrent kernel contention L1D damage performance performance compute intensive kernel reduce dramatically L1D memory pipeline memory intensive kernel although propose mitigate L1D contention kernel gpu etc reduce performance loss L1D contention SMK gpus challenge moreover kernel complementary characteristic acquire performance therefore candidate kernel combination introduce typical hardware SMK knowledge propose hardware SMK gpu kernel mapped SMs SM warp concurrent kernel interleave overall throughput improve compute intensive memory intensive application concurrently addition application L1D rate significantly reduce throughput application propose lazy TB schedule optimal TB occupy kernel SM maximum TBs allocate SM SM TB completes execution instruction issue across TBs instruction issue TB optimal TB SM acquires optimal TB maximum TBs assign SM lazy TB schedule limit TBs SM improve performance waste resource register memory improve resource utilization TBs kernel allocate remain resource SM multiple kernel gpu simultaneously SMK manner propose resource partition dominant resource fairness policy execute concurrent application SMK gpu resource register memory active thread TBs kernel dominant resource refers maximum kernel resource aim resource partition equalize dominant resource kernel SM resource allocation propose enhance compute cycle fairly allocate concurrent kernel via warp schedule performance fairness kernel cycle proportional amount kernel execute alone SM addition applicability SMK increase concurrent kernel intra SM resource slice concurrent application TB granularity SMK gpu impossible resource slice strategy acquire performance application propose warp slicer optimal intra SM resource partition application profile warp slicer evaluates performance variation kernel TBs SM filing algorithm sweet TB partition performance degradation application minimize concurrent application finally resource SM allocate concurrent application sweet TB partition multiple application gpu SMK manner propose gpu maestro performs dynamic resource management maestro spatial multitasking SMK SMs resource partition possibility SMK epoch finally resource partition acquires performance spatial multitasking adopt gpu meanwhile maestro employ allocation mitigate resource fragmentation SMK gpu addition although greedy  warp schedule policy widely kernel occupy warp slot possibility issue instruction starve maestro adopts kernel aware warp schedule issue instruction kernel propose efficient kernel management static tlp modulation dynamic tlp modulation cache bypassing profile optimal TB kernel alone SM kernel acquire static modulation determines initial tlp configuration TB partition multiple kernel SMK gpu pipeline utilization performance metric dynamic modulation leverage adjust TB resource allocation improve performance addition mitigate cache contention cache bypassing kernel bypass adjusts TBs bypassing cache aim cache bypassing kernel locality cache mitigate cache contention SMK gpu cache bypassing partition concurrent kernel SMs profile SMs remain SMs SMs cache resource allocate multiple kernel granularity cache sample profile SM employ  cache resource partition strategy meanwhile winner cache partition SM acquires maximum throughput previous sample employ SMs cache partition mitigate cache contention concurrent kernel technique propose xie combine instruction TB cache bypassing alleviate cache contention discus satisfy qos requirement SMK gpu qos kernel allocate SM meanwhile SMs allocate non qos kernel evenly effort qos goal qos kernel meanwhile maximize throughput non qos kernel quota indicates instruction execute per epoch SM epoch quota qos kernel non qos kernel epoch quota qos kernel remain cycle assign non qos kernel epoch goal qos kernel remain instruction quota kernel epoch addition idle warp kernel SM metric execute adjustment TB allocation kernel bandwidth sensitive BS kernel latency sensitive LS kernel propose coordinate TB combination bandwidth partition improve concurrent kernel execution SMK gpus allocates TB resource bandwidth resource LS kernel BS kernel respectively meanwhile prevents BS kernel occupy bandwidth resource avoid serious damage performance LS kernel receives kernel relationship TB correspond bandwidth utilization input parameter return TB combination bandwidth quota bandwidth quota convert L1D request issue rate quota L1D interconnection network per bandwidth consumption input parameter acquire online offline profile illustrate compute intensive kernel starve memory intensive kernel memory pipeline SMK gpus propose dynamic memory instruction limit  mitigate cache contention accelerate concurrent kernel execution reservation failure per memory request  indicates severe cache contention cache related resource congestion sample interval  maximum active memory instruction kernel memory instruction limit kernel sample interval performance compute intensive kernel significantly reduce memory intensive kernel memory pipeline occupy L1D resource propose cache aware warp schedule  approach ameliorate contention data cache improve performance  adopts kernel aware warp schedule issue instruction kernel addition memory instruction issue predicate instruction data cache  issue another instruction kernel otherwise memory instruction issue memory pipeline focus warp schedule coordinate various TB partition improve performance SMK gpu propose lookup multi kernel scheduler gpu navigator gpu navigator mainly constitute kernel multiple kernel selects kernel multiple candidate maximize performance SMK gpu gpu navigator executes kernel profile ipc kernel kernel concurrent kernel profile acquire stp kernel  kernel multiple kernel finally gpu navigator easily kernel acquires maximum stp executes kernel SMK gpu kernel kernel profile concurrent kernel profile execute update kernel multiple kernel characteristic hardware gpu SMK discus kernel acquire performance focus improve performance TB partition grain resource schedule SM TB allocate memory intensive kernel L1D reduces concurrent kernel throughput significantly although stp throughput memory intensive kernel improve performance metric ANTT fairness damage seriously therefore acquire balance performance improvement SMK gpus cannot dependent TB partition illustrate addition software SMK implement gpus kernel slice propose implement SMK executes multiple kernel SMK manner kernel implement SMK adopt hybrid technique constitute kernel slice kernel illustrates SMK beneficial reduce execution consumption characteristic hardware gpu SMK challenge opportunity efficiently utilize gpus various compute platform data etc multitasking becomes important development direction gpu architecture although hardware gpu multitasking depth challenge  meanwhile opportunity explore expand research gpu multitasking challenge opportunity demand enhance simulator development gpus promote gpu simulation source meanwhile gpu vendor disclose detail architecture source gpu simulator implement limited technical document assumption gpu architecture addition gpgpu sim model previous hardware gpu multitasking simulator model nvidia gpu architecture gap opaque innovation industrial gpus academic research cannot ignore bridge gap simulation framework accel sim propose model nvidia turing pre turing gpu architecture although accel sim source powerful gpu simulation platform lack ability feature nvidia  architecture mig challenge gpu simulator academic research promising opportunity contribution research hardware gpu multitasking incorporate machine machine technique neural network successfully video surveillance medical image analysis processing etc meanwhile machine adopt research gpu hardware instance neural network estimate gpu performance wrapper feature selection metric gpu kernel classification performance gain concurrent kernel execution kernel execution predicate machine technique addition machine technique research hardware gpu multitasking pre neural network predicate application slowdown online spatial multitasking gpu machine technique inherently suitable extract feature massive gpu application improve gpus ability manage concurrent execution multiple application future gpu intelligent promising machine technique promote gpu related research hardware gpu multitasking explore previous hardware gpu multitasking mainly implement nvidia fermi architecture topic investigate multitasking vendor gpu architecture fermi architecture introduce nvidia introduce gpu architecture fermi architectural innovation adopt instance cache sub fetch cache maxwell architecture beneficial accelerate execute irregular application gpus employ advanced architecture technique improve cache throughput shift memory bottleneck cache memory hierarchy signifies mitigate cache contention concurrent application urgent gpu multitasking meanwhile bandwidth memory HBM employ memory gpu feature HBM independent memory channel refresh critical investigate interaction memory schedule cache accelerate concurrent kernel execution gpus brief advanced gpu architectural innovation 3D stack HBM explore research gpu multitasking hardware architecture conclusion mainly review hardware gpu multitasking issue concerned hardware multitasking illustrate meanwhile characteristic hardware multitasking belonging addition future research challenge resolve promising research direction reader quickly deeply understand research hardware gpu multitasking beneficial promote related research gpu architecture vendor extent addition multitasking nvidia gpus bother multitasking research vendor gpus interference concurrent application therefore although focus nvidia gpu architecture valuable inspiration research hardware gpu multitasking vendor gpus