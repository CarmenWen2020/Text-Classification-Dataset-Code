mobile compute MEC incorporates node device potential data processing closer data source meanwhile federate FL emerge promising privacy preserve approach facilitate AI application however remains challenge optimize efficiency effectiveness FL integrate MEC architecture moreover unreliable straggler intermittent device significantly slows FL affect global model quality circumstance article multi layer federate protocol HybridFL MEC architecture HybridFL adopts model aggregation enact aggregation strategy moreover mitigate straggler device introduce regional slack factor stage client selection perform node probabilistic approach without identify probe device reliability agnostic demonstrate effectiveness modulate proportion client convergence analysis protocol conduct extensive machine task MEC HybridFL improves FL training significantly shorten federate global model convergence reduce device consumption percent introduction rapid advance remarkable achievement development artificial intelligence AI drawn unprecedented attention reveal potential machine technique meanwhile prevalence internet iot intelligence stimulates effort computation network closer source data resides faster response service quality research endeavour trend empower device iot AI application gartner predict percent enterprise iot project incorporate AI component mobile compute MEC consists node device emerge technology fundamental architecture iot promising architecture  AI node however obstacle practical AI scenario participant model training device phone smart sensor wearable electronics although performance training AI model centric manner spec server entire data feasible application scenario clinical diagnosis nowadays due data privacy concern administration policy forbid data local device besides traditional distribute machine technique decentralize data frequent exchange gradient model parameter network traffic prohibitive communication device via wireless channel federate originally propose google distribute machine protocol address mention data privacy communication efficiency training decentralize data typical FL consists multiple training client device perform model training local data aggregate local model global model average algorithm FedAvg FL unbalanced data distribution device posse variable amount non iid non independent identically distribute data massively distribute device participant fleet heterogeneous device limited access communication data access limited local device communication device expensive privacy preserve communication efficient federate regard promising approach realize intelligence local device classic layer architecture compute however emergence MEC explore adapt FL protocol layer hierarchy MEC abundant resource computation storage network available layer necessity adaptation fold network connection central server geographically distribute device fairly unreliable central server overwhelmed workload network traffic due excessive model update vast client introduce layer comprise node server micro data FL address issue effectively node proximate device stable connection sufficient resource computation exist FL resource realize multi aggregation however issue resolve unreliability device connection inefficiency FL heterogeneity device bandwidth address capability node network fully exploit situation becomes challenge combine privacy preserve scenario server restrict probe information client keyboard input prediction task device server communication stateless device invite participate cannot tracked reliability opt training agnostic propose novel protocol HybridFL enable privacy preserve efficient federate MEC architecture overview protocol leverage capability node boost efficiency communication adapt FL layer hierarchy MEC FL hybrid client collaboration account unreliability heterogeneity device privacy adopt pace steer mechanism hybrid synchronous asynchronous client communication contribution outline propose novel protocol HybridFL FL layer architecture MEC HybridFL facilitates efficient model exchange via combination quota trigger regional aggregation via layer immediate aggregation schematic overview HybridFL protocol MEC architecture consists layer client device model aggregation aggregation signal information local model node client node via relatively network therefore noisy wireless channel whilst connection stable typically ethernet bandwidth typically sufficient schematic overview HybridFL protocol MEC architecture consists layer client device model aggregation aggregation signal information local model node client node via relatively network therefore noisy wireless channel whilst connection stable typically ethernet bandwidth typically sufficient mitigate impact client introduce regional slack factor node client selection via probabilistic estimation privacy preserve client reliability agnostic introduce effective data coverage EDC model aggregation convergence analysis protocol conduct extensive machine task public data evaluate performance HybridFL protocol experimental demonstrate significant improvement average duration global model convergence accuracy consumption device organize discus relevant concern FL MEC detail HybridFL convergence analysis protocol discus experimental conclude related stochastic gradient descent sgd variation facto standard training machine model fundamental previous distribute built exchange gradient centralize decentralize manner extensive effectiveness distribute sgd complex model neural network dnn however traditional data environment data access globally worker financial communication hardly issue however scenario MEC usually comprise spec unreliable device geographically distribute node moderate performance relatively noisy communication node device via wireless channel federate FL address data privacy prohibitive communication training global model decentralize data FL originally synchronous training protocol FedAvg average algorithm aggregate local model device global model model exchange frequent thereby communication efficient gradient exchange traditional distribute sgd outstanding feature FL potential reduce communication FL via model compression adaptive aggregation interval multi task addition variant FL protocol propose adopt asynchronous federate optimization scheme non global model update device halfway training safa semi asynchronous FL protocol retains synchronize pace steer introduce strategy training client selection model cache training adapt FL emerge mobile compute adaptation account resource budget node propose pace algorithm adaptively adjusts aggregation interval FL adopt architecture data reside node essentially layer FL protocol implement hierarchical FL protocol utilizes server node perform model aggregation protocol straightforward extension FedAvg multiple layer global aggregation interaction client tightly couple device network failure node addition HierFAVG node perform multiple aggregation model significantly postpones global exchange model information consequently slows convergence drawback exist FL MEC aim develop efficient FL protocol enables robust machine virtue resource layer HybridFL protocol training HybridFL protocol FL MEC architecture stage model distribution local training model aggregation stage model distribution client selection global model distribute node across client stage local training perform client device model upload via client connection aggregation stage local model uploaded merge global model protocol adopts hybrid pace steer mechanism allows flexible regional model aggregation signify detailed HybridFL introduce regional slack factor model distribution stage protocol performs aggregation refer collection client node denotes data data cannot device drk  without loss generality assume client node notation frequently stage FL client selection perform ensure reasonable proportion client engage training client proportion restrict participate population involve excessive client hardly benefit convergence quality global model recruiting excessive device neither efficient communication realistic device owner nevertheless severe shortage participant FL inferior global model unreliability device device opt local training occasionally due various battery device failure network disconnection denote client client manually unexpectedly desire proportion client successful model submission preset server mitigate shortage participant introduce wise selection proportion client selection HybridFL training specifically node client randomly client denote signify client local training ideal satisfy straggler dropout minimal impact efficiency otherwise local training device futile accepts maximum client challenge permit node probe client IDs aliveness training progress model update client client reliability probability agnostic develop probabilistic approach regional client selection specify regional selection proportion aim involve client opt client unreliable formally target wise selection formulate SourceRight click MathML additional feature expectation client optimal proportion client client client perform local training selection proportion expectation equivalent nrk comb SourceRight click MathML additional feature client comb combination probability combination device combination comb pri denote probability device opt calculate  pri SourceRight click MathML additional feature therefore obtain optimal client selection proportion combine nrk comb SourceRight click MathML additional feature however cannot without priori knowledge probability pri reliability individual client FL scenario privacy preserve prohibit acquire client identifier pri agnostic develop novel approach address difficulty eventually optimal assume probability replace individual pri expectation remains unchanged replacement binomial distribution expectation surjective function nrk  SourceRight click MathML additional feature equality combine selection target  sourcewhere desire global proportion client specify successful model submission entire MEC define modulates selection proportion compensate client therefore regional slack factor cannot arbitrarily otherwise optimality guaranteed optimal distribution client reliability target formulate via estimate selection proportion consequently target expectation desire situation estimate accord client upcoming FL resolve develop novel estimate historical model submission FL organize model previous node model client submit model HybridFL adopt quota trigger aggregation mechanism client model submit globally across MEC min sourcewhere client submit model model globally node client local training detail aggregation timing introduce later formally factor characterize relation sourcewhere denotes percentage client submit local model client observable local model node however agnostic privacy protection scenario node probe client client submit update model cannot therefore transform define source source  source estimate selection proportion client selection however cannot directly obtain unknown observable calculate combine assumption optimal source therefore develop practical approach exploit historical variable observable node specifically node accord definition wise assume significantly FL training constant approximation within span source replace source therefore equivalent series observation sample function sample respectively coefficient estimation lse  sourcewhere retrieve node compute estimate client selection proportion define source FL initialize default initialize accordingly investigate effectiveness achieve selection target simulated client local epoch federate HybridFL protocol initialize trace probabilistic estimation convergence FL converge definition necessarily pri recall pri reliability client respectively besides define without knowledge approximation consequently client participate ratio quantify maintain around dash convergence trace simulation client respectively reliability client training gaussian distribution pri respectively client performance global selection demonstrate estimate determines theoretically practically feasible optimal regional selection proportion expectation desire target model aggregation protocol HybridFL model aggregation multi stage involve aggregation HybridFL update model submit client across MEC trigger aggregation signal node node local model quota trigger regional aggregation effectively mitigates impact client straggle consequently shorten expectation adopt immediate aggregation strategy allows model aggregation conduct aggregation rationale strategy network connection typically reliable latency therefore facilitates global information exchange convergence global model aggregate regional model regional aggregation node demonstrates orchestrate HybridFL workflow propose HybridFL protocol wherein local model submission trigger model aggregation local training consists epoch model straggle client client actually uploaded dash arrow monitoring client submit model report node client model submission quota signify node perform regional aggregation formulate VRC drk wrk sourcewhere wrk model client denotes regional model node aggregation involves client model limited local training successfully alleviate model staleness cache local model without successful update replace exist regional model obtain aggregation conduct wrk aggregation perform immediately regional aggregation model instead constant regional model literature adopt data orient average strategy introduce effective data coverage EDC EDC quantifies actual data training formulate EDC denote   drk sourcewhere client submit model successfully regional node accordingly define EDC MEC denote EDC EDC  source model aggregation regional model EDC characterize wise contribution global model  EDC source algorithm pseudo code entire FL protocol algorithm HybridFL protocol input maximum tmax local epoch per desire proportion response limit  output finalize global model central server initializes global model quota leftarrow max distributes node node parallel computes accord  tau monitoring update node geq quota lim  trigger regional aggregation sends aggregation signal node  aggregation computes accord return node  tau leftarrow cdot leftarrow randomly client client parallel  tau reporting update return  computes accord return client client  tau epoch leftarrow  update gradient descent return convergence analysis convergence global model federate layer architecture layer compute however modification aggregation convergence analysis focus difference proof exist regional aggregation HybridFL instantly global aggregation mathematically formulate global model combine yield align sum frac EDC EDC sum frac sum frac EDC EDC frac triangleq sum gamma cdot tag align sourcewhere correspond node client gamma client model aggregation without ambiguity abbreviation gamma denote gamma brevity equality aggregation namely sum aggregation sum aggregation equality transforms sum suggests entire aggregation layer MEC equivalent layer FL difference formally HybridFL global optimization equation arg min sum gamma tag equation sourcewhere denotes parameter neural net global model optimize index client average loss data partition client calculate equation frac sum tag equation sourcewhere cdot loss function data possess client analyze convergence protocol quantify upper bound denotes optimal model parameter target due limit proof analysis extend theorem protocol yield theorem theorem respectively assumption facilitate analysis assumption loss function convex rho lipschitz beta smooth loss function satisfy assumption validate effectiveness FL assumption convex rho lipschitz beta smooth regard inequality define delta upper bound divergence gradient delta upper limit delta forall equation vert nabla nabla vert leq delta leq delta tag equation source denote index epoch ldots max cdot tau tau local epoch facilitate analysis denote hypothetical global model aggregate epoch confuse visible aggregation besides auxiliary model centralize gradient descent initialize context optimize target epoch tau tau definition align sum gamma tag align sourcewhere update align eta nabla tag align source tau tau equation eta nabla tag equation source theorem theorem loss divergence bound epoch equation leq rho tau tag equation sourcewhere equation triangleq frac delta beta eta beta eta delta tag equation source proof proof theorem lemma ref proof detail appendix computer society digital library http doi org ezproxy auckland tpds theorem theoretical difference loss global model aggregate local model baseline centralize data training leq rho tau tau equivalent aggregation interval tau theorem recall cdot tau convergence upper bound theorem theorem convergence upper bound tau epoch convergence global model guaranteed equation leq frac tau omega eta frac beta eta frac rho tau tau epsilon tag equation source satisfied eta leq frac beta omega eta frac beta eta frac rho tau tau epsilon geq epsilon forall tau tau geq epsilon epsilon omega triangleq min frac vert tau vert cdot define proof limit rate eta whilst implies gap limit bound gap positive epsilon approximation perform aggregation tau local epoch tau theorem conclusion theorem combine lemma ref proof theorem implies gap optimum optimize target loss function narrow FL proceeds increase client heterogeneity heterogeneity device practical MEC FL involve vast heterogeneous device discrepancy capability cpu performance bandwidth reliability impact overall efficiency FL characterize heterogeneity client mainly compute performance bandwidth reliability compute performance client determines efficiently conduct local training cpu frequency ghz training task data partition client performance local training memory device training simplicity assume client participate sufficient memory memory impact client performance device training local model transmit node device bandwidth factor determines communication device due heterogeneity client model upload local training differs client client heterogeneity client differentiates consumption jointly consumption computation communication local training computation model transmission communication formulation device performance consumption detailed assume client discrepant reliability opt probability practically client involve factor subjective objective situation situation correlation factor likely client complicate beyond scope client independent protocol completely independent distribution client probability reliability agnostic scenario HybridFL mitigates negative impact device heterogeneity unreliability efficiency effectiveness FL introduction regional slack factor enables protocol modulate model submission desire proportion slack factor without priori knowledge client probability besides quota trigger aggregation mechanism HybridFL allows quota met passively await response client effectively accelerates FL protocol susceptible device failure experimental evaluation evaluate effectiveness propose HybridFL model convergence efficiency global model accuracy evaluate consumption device important metric setup evaluation built simulated MEC federate software package MEC establish simulated node device comprise layer architecture device training FL implement pytorch framework device client manage node via wireless channel whilst node ethernet client local network implement unreliable simulated MEC probability client gaussian distribution mathbb client relation abort probability probability experimental setup federate MEC evaluate HybridFL machine task  task mnist task task configure MEC environment performance HybridFL device node data distribution data partition device task gaussian distribution task non iid assign sample probability client index equiv mod implement exist protocol recent literature FedAvg HierFAVG FedAvg primitive FL protocol propose google layer client server architecture HierFAVG layer FL protocol compute adopts training architecture protocol introduce layer performs model aggregation global aggregation conduct HierFAVG adaptive model await response client FL training driven protocol HybridFL setting parameter experimental cycle stage model distribution local training model aggregation federate global aggregation perform federate FedAvg HybridFL HierFAVG performs aggregation multiple federate aggregation aggregation interval kappa reference HierFAVG optimal comparison protocol federate denote max epoch client local epoch denote tau node conduct aggregation federate denote formulate equation cec min lbrace lim max prime lbrace comm rbrace rbrace tag equation sourcewhere lim preset limit response configure extremely straggle client local training communication average partition performance denote bandwidth denote client normal distribution standard deviation sigma respectively performance bandwidth extremely straggle client sigma prime client FedAvg HierFAVG HybridFL prime equiv quota trigger aggregation cec communication calculate comm communication local training client calculate respectively equation cec frac  cdot BR tag equation sourcewhere BR rate connection client wireless network obtain effective rate apply shannon theorem correspond bandwidth  exists model upload typically spends twice model uplink bandwidth typically percent model  MB MB task respectively FedAvg cec equiv involve layer equation comm frac  cdot SNR tag equation source equation frac cdot tau cdot bps cdot  tag equation sourcethe numerator quantifies cpu cycle training local partition model consume device local training align comm trans cdot comm comp cdot tag align sourcewhere trans consumption transmitter comp device computation frequency model trans comp watt respectively benchmarking report ref experimental FL preset maximum max preset accuracy achieve global model task task respectively model accuracy achieve average obtain max duration achieve desire model accuracy investigate model convergence accuracy trace FedAvg HierFAVG HybridFL average consumption device experimental task  environmental setting mathbb client selection proportion CC experimental task mnist environmental setting mathbb client selection proportion CC trace model accuracy FL task  global model throughout probability device gaussian distribution mathcal mathbb consumption watt device task  FedAvg HierFAVG HybridFL obtain average device MEC trace accuracy FL task mnist global model throughout probability device gaussian distribution mathcal mathbb average device watt consume task mnist local training FedAvg HierFAVG HybridFL protocol obtain average device MEC task  numerical regression task FL perform global fully neural network fcn model client posse private  data client partition data without overlap cannot data task simulates industrial scenario production data privacy sensitive local partition gaussian distribution specify FL accuracy achieve average max protocol effectively shortens average percent slight improvement global model accuracy plot trace model accuracy FL setting lbrace rbrace mathbb lbrace rbrace solid improvement model convergence HybridFL unstable MEC circumstance device frequently mathbb selection proportion global model hardly converge FedAvg HierFAVG optimum HybridFL protocol protocol specify target model accuracy criterion convergence duration acc HybridFL achieve accuracy target task yield FedAvg HierFAVG client mostly unreliable mathbb HybridFL achieve convergence HierFAVG another benefit convergence conservation consumption device protocol consumption friendly device HybridFL reduces average usage device roughly percent task mathbb task mnist task aim simulate scenario image sample distribute relatively fleet device device uploaded server realistic scenario mobile application restrict privacy client node task besides emulate discrepancy device user behaviour bias data distribution device assign sample client data label client index sample percent reside client IDs congruent modulo mnist data data distribution device iid classic convolutional neural net lenet consist convolutional layer max pool fully layer model image classification task FL fix accuracy max HybridFL outperform FedAvg HierFAVG accuracy global model participate device generally unreliable mathbb accuracy global model training setting mathbb convergence global model improve HybridFL protocol FedAvg HierFAVG HybridFL achieve global model federate performance HybridFL baseline protocol specify acc convergence target global model HybridFL significantly reduces achieve accuracy target protocol HybridFL achieves roughly mathbb situation client frequently participate restrict task mathbb protocol converge consumption longer extremely probability average client almost impossible engage percent training modulation regional slack factor cdot node preset limit prolong extent observation explains literature selection proportion device usage factor affect willingness device owner participate FL training average consumption device participant FL achieve preset accuracy target task advantage HybridFL task prominent task protocol manage retain device usage enables faster convergence therefore training device baseline protocol feature HybridFL attract device evaluation propose protocol HybridFL machine task environment setting demonstrates effectiveness boost efficiency FL improve global model quality device consumption layer MEC improvement fold quota trigger regional aggregation HybridFL effectively prevents situation extremely unreliable client entire FL enable node modulate regional quota slack factor improve robustness FL client global aggregation perform immediately regional aggregation global exchange model conclusion thanks increase capacity compute storage bandwidth network prominent trend device infiltrate artificial intelligence meanwhile concern data privacy develop machine technique reveal potential federate promising privacy preserve adapt FL mobile compute aim improve effectiveness efficiency layer FL protocol HybridFL enable model aggregation boost efficiency mitigate impact unreliable device modulate client selection wise manner reasonable local update desire conduct extensive demonstrate HybridFL significantly improves FL MEC shorten average convergence global model promote model accuracy reduce device consumption future extend complex architecture hierarchy diverse FL participant role another future investigate improve effectiveness local training device without breaching privacy constraint