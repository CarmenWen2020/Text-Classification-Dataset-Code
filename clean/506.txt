user service evolve towards microservice architecture service built multiple microservice stage entire service microservice architecture opportunity offload microservice stage device user however emerge technique violation quality service qos microservice service continuum communication overhead resource contention microservices external task propose nautilus runtime effectively deploys microservice user service continuum nautilus ensures qos microservice user service minimize computational resource comprise communication aware microservice mapper contention aware resource manager IO sensitive load aware microservice migration scheduler mapper microservice graph multiple partition communication overhead partition appropriate node node resource manager determines optimal resource allocation microservices reinforcement capture complex contention behavior microservices suffer external IO pressure IO sensitive microservice scheduler migrates critical idle node furthermore load microservices dynamically load aware microservice scheduler migrates microservices node idle ensure qos goal entire service experimental nautilus guarantee qos target external resource contention suffers qos violation meanwhile nautilus reduces computational resource usage network bandwidth usage achieve ile latency introduction user application strict quality service qos requirement latency usually frequent bug fix feature update user service shift monolithic architecture microservice architecture complex user service decompose multiple loosely couple microservice stage microservice application involves  multiple microservices implement update independently independence improves application scalability portability availability advantage microservice architecture regard widely accepted employ software architecture internet giant netflix amazon traditional monolithic user application entirely deployed datacenter node powerful host entire application microservice architecture enables opportunity deploy application microservice stage node user deploy microservices continuum benefit access latency node computational ability internet explore deploy service continuum moreover efficiency consideration desirable minimize resource usage microservice user application continuum ensure qos requirement prior microservice deployment computational memory resource contrary node usually geographically distribute communication unstable public network describes microservice deployment continuum computational ability node resource contention node communication overhead affect latency user application therefore prior microservice deployment datacenters apply continuum deploy user application microservice stage continuum node node deploy user application microservice stage continuum node node besides microservice application consume computational resource cpu core memory capacity network bandwidth consume disk IO bandwidth insight hence potential improve resource utilization node IO intensive task microservices node however location brings extra challenge due inevitable resource contention performance degradation microservices node specifically task compete IO resource load variation external pressure microservices orchestration deploy microservices continuum kubernetes rhythm kubernetes randomly schedule microservices node user privilege critical microservices allocate computational resource node however lack ability identify critical microservices qos application complex dependency relationship microservice stage dynamic load microservice application external IO pressure suffer IO intensive task critical microservices accordingly rhythm microservices distribute environment microservices function allocates computational resource microservices demand however contention microservices node qos violation resource constrain server involve limitation exist microservice orchestration strategy challenge resolve achieve purpose ensures qos microservice user application minimize resource continuum challenge communication public network latency effectively microservice stage node node minimize communication overhead challenge resource contention microservices node qos violation allocate resource microservice microservices node challenge IO intensive task microservices affect microservice performance IO resource microservices occupy external task allocate resource microservices node migrate microservices node however random migration cannot effectively improve quality service overall microservice application challenge optimal microservice deployment accordingly load user application microservices compete computational resource qos application affected almost constituent microservices instance microservice node migrate load increase node fails computational resource contention resource challenge load microservices challenge obtain runtime  static strategy propose nautilus runtime comprise communication aware microservice mapper contention aware resource manager IO sensitive microservice scheduler load aware microservice scheduler microservice application deployed data dependency microservices connects microservices graph microservice mapper partition microservice graph service multiple node node partition initial mapping communication overhead microservices microservices node resource manager determines optimal resource allocation core memory network bandwidth microservice reinforcement RL RL capture complex interactive relationship performance microservices due contention meanwhile nautilus monitor latency overall microservice application load pressure resource utilization pressure node mapping tends suffer qos violation nautilus migrates suitable microservice node idle node guarantee quality service microservices external IO pressure IO intensive task load imbalance IO sensitive scheduler adaptively identifies migrates disturbed bottleneck microservices addition pressure computational resource microservices increase load aware scheduler suitable microservices migration ensure quality service minimize communication volume migration summary microservice mapper scheduler mapping microservices node resource manager determines amount resource allocate microservice contribution communication overhead aware microservice mapping propose graph microservice mapper frequent data interaction memory thereby reduce communication overhead resource contention aware resource allocation introduce resource manager RL algorithm capture resource contention overall performance accordingly optimal resource allocation resource usage IO sensitive microservice migration IO sensitive critical microservice scheduler migrate critical microservices affected external IO sensitive task guarantee qos requirement microservices load dynamic sensitive microservice migration runtime load aware scheduler migrates microservices node idle reduce resource contention response load dynamic minimize communication overhead evaluate nautilus continuum built alibaba personal laptop raspberry experimental nautilus guarantee ile latency external IO pressure suffer qos violation meanwhile nautilus reduce compute resource network resource respectively achieve ile latency organize introduces representative prior investigate background challenge deploy microservices continuum introduces nautilus microservices adapt compute node propose contention aware resource manager allocate resource microservice respectively introduce migration strategy eliminate runtime qos violation evaluate effectiveness nautilus finally conclude related prior related topic resource task placement resource management microservice architecture resource management prior allocate computational resource application maximize throughput batch application guarantee qos user service datacenter coda eliminates resource contention multi stage dnn training task improve resource utilization datacenter however coda orient batch processing task suitable latency sensitive internet service   multi latency web service batch application maximize resource utilization avoid qos violation service datacenters  component acceleration strategy ensure latency target multi stage application however cpu llc memory bandwidth contention target internet orient service perform IO operation network communication frequently extension analyze resource interference treat disk resource allocate IO bandwidth multi tenant task node  proposes IO contention aware resource management framework data task framework suitable latency sensitive internet service dial  analyze resource category interfere workload deploy workload affect performance cluster improve throughput PARTIES  analyze resource consumption application allocate resource application reduce resource usage improve fairness without hurt performance application location ignore dependency relationship microservices applicable microservice application rhythm aggregate microservice application sensitive resource node improves anti interference ability service achieve throughput addition focus task placement resource management continuum however resource contention microservices runtime microservice architecture peng gan focus building benchmark suite microservice architecture benchmark analyze characteristic microservices implication network operating  combine horizontal vertical resource partition microservice stage improve performance microservices   focus load balance microservice architecture microservice load adjustment mechanism priority seer  sage ML predict microservice application qos violation critical stage qos violation adaptive migration mechanism microservices environment reduce response delay service microservices deployment model heterogeneous platform simulated simulator commit deploy microservices data reduce consumption guarantee qos target  dynamically switch deployment mode microservices IaaS serverless compute reduce usage resource microservices dagger proposes fpga rpc hardware acceleration structure improve communication efficiency microservices node contribute performance improvement microservices however due lack consideration communication overhead distribute node research latency violation background motivation investigate effectiveness emerge framework schedule microservice user application continuum source benchmark  SN  MS  HR  microservice benchmark suite microservice ticket TrainTicket TT perform investigation benchmark characteristic spectrum application continuum experimental platform built node alibaba elastic server raspberry personal desktop node detailed hardware setup summarize external IO pressure fio microservices environment increase throughput node hardware software setup qos violation resource inefficiency deploy microservices user application continuum explore advantage computational ability access latency kubernetes popular microservice schedule framework adopt effort strategy kubernetes randomly microservice node continuum node resource freely microservices normalize ile latency benchmark kubernetes continuum microservice benchmark suffer qos violation load suffer qos violation load qos violation originates schedule kubernetes communication overhead microservices actual resource contention microservices node explain subsection ile latency benchmark kubernetes actual cpu usage increment microservices benchmark SN load nautilus microservice stage occupy excessive resource SN suffers qos violation contrary ensure qos cpu core resource inefficiency originates resource allocation kubernetes kubernetes treat microservices equally allocate excessive amount computational resource microservices latency critical insufficient resource critical microservices cpu resource allocate microservice kubernetes axis microservice allocate cpu resource actual IO resource usage node microservices IO sensitive task duration microservice IO resource contention normalize ile latency migrate microservice node without IO contention kubernetes deployment microservice application suffer  overhead imbalanced load kubernetes deployment microservice application suffer  overhead imbalanced load overview nautilus overview nautilus structure microservice application structure microservice application overhead comparison nautilus kubernetes mapper overhead randomly generate service overhead mapper parallel module cdf normalize duration bhattacharyya coefficient  stage SN strategy load aware migration strategy ile latency benchmark normalize qos target latency without IO pressure latency external IO pressure resource usage microservice external IO pressure aws resource usage benchmark load peak load benchmark computational resource rhythm nautilus cpu resource usage microservices rhythm nautilus summary exist orient microservice schedule framework continuum qos violation resource utilization efficiency necessity IO sensitive microservice migration insight IO bandwidth resource consume microservices usually MB IO bandwidth consumption obvious pressure generate disk resource potential IO intensive task microservice application node improve utilization IO resource microservice application IO intensive task fio cluster increase throughput disk bandwidth colocation define normalize IO throughput ratio disk bandwidth consumption microservice application disk bandwidth usage utilization disk bandwidth increase average scenario microservice application load load greatly increase throughput IO bandwidth however runtime performance microservices receives contention resource IO intensive task increase latency contention resource performance degradation IO sensitive microservice   IO sensitive task load duration microservice alone performance microservice IO pressure actual duration microservices IO pressure alone duration mainly resource contention  microservices IO task performance imbalance severe microservices suffer performance loss therefore migrate IO sensitive microservices idle node external IO pressure however unreasonable migration cannot significantly improve quality service microservices idle node without task others suffer IO pressure latency situation migrate microservices microservice review storage migrate latency guaranteed due complex topology microservice application diversity duration microservices directly affect overall latency contrary qos microservice application guaranteed duration critical microservice disturbed therefore identify critical microservice migrate becomes indispensable besides kubernetes schedule microservices contend computational resource cpu core memory capacity etc seriously node omit performance interference microservices extensively performance microservice severely affected microservices node limited resource overall performance degradation dive microservice deployment effort deployment strategy serious qos violation understand qos violation resource inefficiency dive detail microservice deployment continuum microservices node communicate public network microservices node contend resource therefore communication overhead performance microservices affect qos microservice application already kubernetes qos violation mainly communication overhead balance resource usage schedule allocate resource microservice stage understand amount data transfer microservices benchmark SN microservice text transfer volume data microservices  user mention data transfer microservices intuitively microservices text  user mention node node computational resource node however kubernetes microservices node microservice text transfer amount data  user mention data transfer response kubernetes microservice mapping decision communication overhead microservices furthermore kubernetes adapt load variation inherent user service load user application microservices amount computational resource core memory capacity node satisfy resource requirement load increase resource requirement microservices increase extent diversity resource shortage node report resource usage node load SN cpu usage node node potentially qos violation microservices migrate node idle node demand qos violation alleviate challenge nautilus therefore implement nautilus minimize resource usage ensure qos microservice user application analysis nautilus communication overhead contention microservices resource usage balance specifically nautilus resolve challenge achieve purpose communication microservices node prone communication overhead microservice topology deploy closely related microservices node communication overhead resource management microservices runtime information resource contention microservices complex structure microservices critical microservices static information resource contention colocated IO sensitive task qos violation microservice application adaptive mechanism identify bottleneck microservices eliminate impact IO contention overall latency variable microservice load pressure unbalanced resource partition load pressure increase inefficiency unbalanced resource partition gradually manifest desirable strategy balance resource allocation fully explore advantage continuum methodology nautilus methodology nautilus analysis nautilus guideline nautilus minimize communication overhead microservices data transfer microservices suffers latency bandwidth nautilus minimize microservice computational resource usage guarantee qos without prior knowledge nautilus accurately identify disturbed bottleneck IO sensitive microservices schedule idle node nautilus balance resource consumption node improve resource utilization continuum overview nautilus nautilus comprise communication aware microservice mapper contention aware resource manager IO sensitive microservice scheduler load aware microservice scheduler mapper minimizes communication overhead continuum resource manager minimizes usage computational resource microservice continuum load extends peak load limited resource load ensure ile latency target IO sensitive microservice scheduler accurately identifies affected bottleneck IO sensitive microservices runtime migrates idle node load aware scheduler identifies qos violation load runtime balance resource usage node migration module integrate runtime execute migration action scheduler microservices mapper decides microservice server mapper task data interaction server data transformation service global memory thereby reduce communication delay node resource manager allocates computational resource core memory capacity network bandwidth microservices accord online reinforcement algorithm runtime contention behavior challenge manager decrease performance degradation resource contention microservices nautilus probability distribution duration IO sensitive microservice establishes criticality model critical IO microservices qos violation IO sensitive scheduler migrates critical microservice stage idle node eliminate resource interference external IO intensive task microservice application load increase computational resource consumption microservices increase sharply microservices compete resource unbalanced resource partition nautilus identifies node migrates microservices correspond node idle node implement nautilus modify kubernetes microservice application kubernetes nautilus without application modification nautilus determines location resource consumption microservice feedback target node microservice computational resource quota cpu core memory capacity network IO bandwidth regularly monitor operation microservices latency throughput execution microservice resource quota occupation microservice feedback correspond component nautilus interval adjust microservices computational resource usage migrate microservices feedback performance migration module implement kubernetes instruction load aware scheduler IO sensitive scheduler migrate microservice migration module migrates microservice source node target node specifically microservice instance migrate target node instance target node delete correspond instance source node specifically relationship module summarize microservice mapper microservice node accord data transfer byte minimize communication overhead initial mapping stage microservice deployed node resource manager allocates computational resource core memory network bandwidth microservice stage avoid qos violation minimize usage computational resource qos violation runtime initial mapping becomes inapplicable nautilus migrates microservices congest node idle node detail resource manager completes allocation interval IO sensitive scheduler load aware scheduler migrate microservices migration module executes migration action scheduler meanwhile nautilus suspends online resource allocation microservice migrate target node migration strategy IO sensitive scheduler duration IO sensitive microservice IO latency latency IO intensive microservices detect increase IO sensitive microservice scheduler transfer critical IO intensive microservices idle node load aware scheduler sum microservice resource quota node node resource quota load aware scheduler defines node congest node migrates microservice idle node communication aware microservice mapping microservice mapper mapping microservices node aim reduce overall communication overhead microservices service node mapping microservices deployed node data interaction microservices transmit global memory instead network potentially pressure network communication concept nautilus microservices frequent data interaction onto node however microservices compose complex topological graph node microservices minimize overall communication overhead important microservice structure described acyclic graph dag suppose nautilus deploys microservices node construct dag vertex microservice refer communication microservice communication overhead microservice remove subset topological graph microservice split independent subgraphs thereby microservices correspond subgraph deployed node service node mapping model minimum communication microservices node microservice mapping approximate algorithm microservices sequential non sequential mapping microservice topology multi node greedy strategy GS dynamic program DP unfortunately cannot structure GS DP cannot optimal polynomial complicate graph mapping transfer minimum graph algorithm minimize data transmission public network minimize sum data correspond transfer global memory however impossible optimal polynomial reduce overhead minimum nautilus approximate algorithm microservices appropriate node quickly return complex topological structure specifically assume microservices deployed node mapper perform iteration iteration mapper subgraph component algorithm calculates global minimum subgraph selects calculate global minimum ford fulkerson algorithm approximation ratio nautilus reduce data interaction microservices kubernetes deployment scheme nautilus heuristic algorithm simulated anneal neural network heuristic algorithm cannot guarantee gap optimal overhead microservice mapper evaluate runtime overhead approximate algorithm randomly generate graph simulate mapping mapping strategy quickly optimal task allocation microservices node increase approximate algorithm gradually increase combination microservices node mapper exceed algorithm calculate minimum vertex iteration therefore parallel thread calculate minimum vertex microservice mapping reduce mapper implement python multiprocessing parallel library overhead parallel algorithm combination microservices node axis consumption execution mapper decrease significantly increase effectiveness parallel algorithm exceeds mapper execution slowly increase due synchronization overhead contention aware microservice resource manager contention aware resource manager specially reinforcement allocate computational resource microservices aim minimize resource usage load extend peak load without qos violation load resource allocation microservice mapping decision allocate resource microservices load resource contention microservices computational resource capacity node resource allocation described multi objective optimization equation objective function minimize computational network resource usage constraint accumulate global memory capacity microservices node exceed available capacity computational resource quota allocate microservice node exceed available computational core microservice application qos target forth throughput microservice application user request load variable equation variable optimization variable optimization contradictory requirement impossible minimize computational network resource usage objective function affect compute delay microservices affect communication delay multiple compute communication delay configuration target qos requirement impact latency pareto satisfy multi objective resource allocation  nci NIC nmi          source assume microservice application stage deployed continuum node geographic location nautilus obtains optimal cpu usage memory quota network bandwidth microservice equation reinforcement resource management agent optimization model relies prerequisite microservice performance characteristic  practical runtime non trivial task predict latency throughput core usage memory usage prior tends predict characteristic microservice utilize prior profile information sampling obtain data estimate microservice characteristic deployment strategy static optimal offline profile contention resource runtime moreover microservices node increase information sample increase explosively instead nautilus chooses RL model optimization without offline profile meanwhile online decision deployment model capture contention resource runtime thereby eliminate performance degradation microservices nautilus translates resource management markov decision MDP solves deployment resource duel DQN model specifically interval agent observes resource usage microservices cpu core memory capacity network bandwidth initialize action agent neural network predict latency peak throughput microservices action action resource partition action performance agent correspond reward finally agent action reward pool training future reward evaluation reward function reward function refers latency throughput load monitor accord equation define reward equation intuitively reward function heuristically encourage agent resource partition qos throughput requirement minimize resource usage resource allocation fails agent acceptable closer partition requirement     source microservice application guarantee qos throughput target runtime agent resource reward resource reward define ratio capacity resource resource consumption equation indicates microservices consume resource resource    source qos throughput met agent qos reward specifically define reward ratio  trr  ratio qos target qos trr ratio target throughput throughput  microservice application suffers qos violation similarly trr microservice application throughput target  trr penalize agent impose negative besides qos reward resource reward encourage agent minimum resource approach qos throughput goal parameter balance qos resource consumption obtain qos ensure qos dominant perspective infrastructure provider resource allocation decision usage concern perspective user besides qos throughput concern monetary desirable operational expenditure guarantee qos throughput nautilus resource service pricing model specifically equation respectively cpu memory network per IO sensitive scheduler introduce IO sensitive microservice scheduler guarantee quality service qos requirement microservice application migrate critical microservices runtime confirm IO sensitive microservices disturbed resource contention latency microservice application explode significantly IO bandwidth isolation mechanism adopt resource  disk cannot completely eliminate therefore external IO pressure load imbalance continuum  mapping nautilus adaptively migrates interfere microservices load node idle node ensure load balance avoid qos violation however mention random migration mechanism cannot effectively alleviate qos violation microservice application inevitable IO contention microservices load node nautilus carefully identify critical microservices migration eliminate interference contention IO resource issue microservice stage migrate data intensive microservice stage migrate periodically sample microservice overall microservice application obtain distribution feature duration microservice identify critical microservices calculate distance microservice overall microservice application distance duration microservice likely influence overall latency distance distribution nautilus impact microservices overall latency bhattacharyya coefficient effectively similarity distribution assume probability distribution duration microservice within probability distribution duration correspond overall microservice application equation calculates probability distribution duration duration request respond microservice sample worth duration abnormal microservices request IO interference longer average extreme imbalance probability distribution eliminate influence outlier logarithmic calculate distribution ecdf source equation defines BC denote BC duration probability distribution microservice overall microservice application BC microservice likely latency violation contrary apart BC microservice significantly affect overall latency BC  source accord assessment continuum verify effectiveness strategy BC logarithmic duration calculate bhattacharyya distance equation BC normal duration microservices instead  SN database orient microservices SN frequently access disk disturbed IO resource storage IO sensitive task node microservices node pressure interference cdf curve database orient microservices operation microservices distribution closely overall curve identify critically affected microservices storage become critical microservice due impact IO pressure duration distribution basically latency distribution effectiveness BC BC indicates microservice likely affect overall latency distribution storage BC distance distance microservices however BC calculates normal duration distance microservices cannot completely critical microservices conclude logarithmic instead normal effective critically affected microservices BC threshold duration microservice affect overall latency become critical microservice nautilus chooses migrate microservice idle node avoid  resource ensure quality service microservice application ensure latency microservice application reduce bandwidth occupy IO load task IO task nautilus chooses migrate critical microservices node nautilus cannot confirm IO task performance requirement limit resource occupation task performance task violate load aware scheduler introduce load aware microservice scheduler quality service qos throughput requirement microservices dynamic scenario widely user request exhibit diurnal consideration user request dynamic resource configuration mapper efficiency pipeline unbalanced eventually throughput qos violation node comparatively limited resource capacity microservices deployed tendency become bottleneck increase resource requirement cater inevitable dynamic user request migrate microservices node idle node reduce resource contention microservices however mention amount data communication microservice challenge carefully microservices migration prevent significant increase microservice communication overhead therefore load aware migration strategy illustrate performance monitor detects server cluster capacity nautilus dynamic scheduler migrate microservices node idle node migration online resource manager allocate computational resource bottleneck microservices node improve efficiency pipeline migration operation microservice migrate node migrate construct communication increment model impact migration communication load aware migration strategy communication increment model assume microservice application distribute deployed node cluster microservice deployed node define communication increment migration microservice node node equation    source equation communication increment define subtraction component  sum communication volume microservice deployed node microservice microservice migrate communication microservice node transmit public network communication microservice node longer consume public network bandwidth minimize communication migration nautilus prefers migrate microservices increase communication however migration ensures computational performance microservices target node resource allocate migrate microservices increase duration microservice thereby increase overall latency carefully microservices target node avoid increase communication performance degradation assume microservices deployed node node migration option optimal option resource requirement node node microservice migrate target node cpu core consumption  microservice memory capacity  communication increment  sort resource requirement increase increment communication addition nautilus remain quota cpu core memory capacity node node becomes congest node option communication increment satisfy constraint resource consumption migrate microservice resource quota target node     illustrates resource status allocation migration load aware scheduler capacity microservices migrate node microservice topology resource utilization node monitor sum microservice resource usage node orange respectively microservices node node upper node microservice resource requirement target node migration option resource status microservice migrate cpu resource quota comparison cpu core memory capacity sufficient resource status migration option allocate target node microservice resource status resource option marked target node specifically load balancer migrate allocate cpu core migration option communication increment unfortunately cpu quota allocate    reject migration option instead migration migrate node node resource guarantee allocation evaluation evaluate effectiveness nautilus improve resource utilization efficiency satisfy qos requirement microservices continuum dive nautilus inspect effectiveness IO sensitive microservice scheduler load aware microservice scheduler setup baseline perform nautilus continuum node already summarize detailed configuration microservice application benchmark nautilus specify software hardware easily later equipment load service define request microservice handle per load generator wrk generate microservices load microservices performance besides  duration microservice overall latency verify effectiveness qos guarantee microservices external IO pressure interference workload compose fio nautilus kubernetes adopts effort deployment rhythm rhythm microservices distribute datacenters accord function addition rhythm  entire service accord resource configuration datacenter ensure stability latency microservices reduce computational resource capacity continuum minimum resource configuration guarantee throughput requirement addition rhythm computation resource continuum regard  peak load performance microservice runtime rhythm monitor IO bandwidth consumption microservices node allocates remain IO bandwidth location workload however considers neither mapping microservices load variation nautilus rhythm guarantee latency overall microservice application scenario location IO intensive task independent deployment location scenario microservice application suffer performance degradation due interference external IO pressure independent scenario overall microservice application deployed continuum alone without external IO pressure difference resource utilization efficiency nautilus rhythm guarantee establish goal qos define resource usage efficiency average consumption resource cpu core memory capacity network bandwidth microservice application resource provider pricing without loss generality aws series pricing resource utilization efficiency cpu core memory capacity network bandwidth USD per respectively guarantee qos target subsection evaluate nautilus guarantee latency target scenario deploy benchmark experimental continuum ile latency resource usage latency scenario without external pressure external pressure benchmark load load nautilus nautilus NC IO sensitive microservice scheduler nautilus NC monitor runtime latency load pressure load imbalance occurs nautilus NC selects microservice incremental communication migration qos benchmark scenario external pressure satisfied rhythm nautilus nautilus NC however microservice application suffer external IO pressure latency nautilus NC rhythm suffer serious qos violation resource usage runtime technology location scenario rhythm nautilus reduces resource usage average ensure qos target meanwhile although resource consumption nautilus NC nautilus nautilus NC cannot guarantee microservices establish requirement rhythm suffers qos violation due lack migration mechanism microservices location task compete IO resource rhythm allocates computational resource disturbed node however microservices node suffer external IO pressure increase latency nautilus NC attention communication migration ignores sensitivity microservice external IO migration non critical microservices therefore migration disturbed critical microservices suffer IO resource contention latency violation improve contrary nautilus effectively migrates critical microservices external IO pressure thereby eliminate interference external pressure microservices resource utilization efficiency subsection evaluate effectiveness nautilus improve resource utilization efficiency ensure qos requirement rhythm cannot guarantee qos target external IO pressure nautilus rhythm without external IO pressure difference resource utilization efficiency nautilus rhythm aspect resource usage benchmark load peak load benchmark nautilus rhythm hardware fix comparison reveals effectiveness nautilus reduce resource usage comparison reveals ability utilize resource resource usage benchmark load load network usage network resource usage benchmark nautilus normalize counterpart rhythm nautilus reduces cpu usage network bandwidth usage benchmark respectively rhythm meanwhile peak load benchmark nautilus rhythm continuum nautilus improves peak load benchmark average rhythm nautilus improves peak load load computational resource nautilus resource user request understand nautilus reduces resource usage amount cpu resource allocate individual microservice benchmark SN load without external IO pressure nautilus reduces resource usage almost microservice mainly nautilus understands resource contention microservices reinforcement runtime nautilus identify critical microservices allocate resource microservices conclude nautilus reduces usage cpu core memory network bandwidth microservice user service economic economic benchmark nautilus rhythm nautilus reduces deployment benchmark average rhythm resource usage deploy microservice without IO pressure aws resource usage deploy microservice without IO pressure aws effectiveness load aware microservice scheduler load aware microservice scheduler monitor qos load entire application migrates microservices accordingly verify effectiveness dynamic scheduler nautilus nautilus NM variant nautilus disables dynamic scheduler peak load benchmark configure peak load nautilus ile latency benchmark nautilus NM nautilus NM satisfy qos requirement benchmark load peak load instance ile latency benchmark SN peak load qos target nautilus NM ile latency benchmark nautilus NM resource usage SN node peak load ile latency benchmark nautilus NM resource usage SN node peak load nautilus NM qos violation adjust microservice mapping load load user service increase mapping scheme entire service suffer unbalanced computational resource partition microservice pipeline inefficient serious qos violation dynamic scheduler detects node migrates microservices idle node report cpu memory usage resource usage benchmark SN peak load nautilus NM node resource usage exceeds node computational resource usage remain adapt load variation optimal microservice mapping resource allocation load user application nautilus reinforcement adapt load allocate computational resource microservice resource allocation overhead resource load microservices increase resource requirement microservice increase resource manager retrain resource configuration qos violation microservices runtime evaluate nautilus adapt load runtime guarantee qos mechanism runtime nautilus training phase runtime phase training phase nautilus  microservice resource allocation strategy load runtime phase microservice load pressure increase nautilus selects minimum resource allocation strategy exceeds pressure nautilus continuously reduces microservice resource quota runtime avoid resource waste load application load resource configuration microservices exceeds resource requirement microservices load nautilus avoid qos violation reallocate compute resource runtime illustrates resource allocation timeline nautilus load benchmark SN increase upper reveals actual load interval dot target load implies qos target microservice application latency user request cpu usage mem usage IO usage respectively cpu core memory network bandwidth consumption service resource allocation load initial load pressure SN user load increase respectively interval eliminate qos violation online RL training optimal resource allocation already pretrained resource partition scheme load pressure resource allocation strategy nautilus runtime load pressure increase nautilus chooses load pressure quickly ensure qos target microservice application specifically nautilus partition load pressure interval respectively load pressure resource partition nautilus gradually reduces resource usage microservices throughput stabilizes target load pressure throughput load qos target microservice guaranteed ensure qos nautilus chooses excessive resource prevent qos violation therefore load pressure nautilus selects correspond resource partition load pressure instead  discus strategy deploy microservice application load continuum overhead nautilus application load analysis assumption compute node node compute node addition compute node increase compute node increase proportionally compute node continuum node continuum user service microservice stage utilize node without introduce instance microservice deploy stage node identify peak load node nautilus denote PL PLN worth identify peak load service node remain decision reinforcement algorithm nautilus related microservice stage PL denote actual peak load service PL max PL PLN nautilus chooses deploy service node plm PL PL max PL PLN deploy multiple instance entire service split route user request multiple instance scenario minimize resource usage express target optimization goal minimize compute node optimization constraint peak load multiple service instance actual peak load equation describes optimization equation peak load resource usage microservice application node continuum node continuum service minimizes   throughput source overhead nautilus offline overhead nautilus analyze topological structure microservices offline perform amount sample obtain amount data exchange microservices nautilus latency approximate algorithm ensure allocate microservices parallel algorithm nautilus efficient resource allocation overhead mention nautilus RL algorithm optimization microservices training agent optimal resource allocation however nautilus adjusts microservice resource configuration online guarantee qos target offline microservice application agent task migration overhead described nautilus detects unbalanced resource configuration nautilus migrate microservices node idle node overhead container consume migration runtime overhead resource manager predict performance query NN inference within furthermore task migration overhead complexity migration strategy node microservice stage node node task  respond within conclusion propose nautilus microservice deployment runtime continuum nautilus task mapper microservices frequently interact data node alleviate network communication overhead propose online resource manager load aware microservice scheduler ensure microservices minimum computational network resource ensure quality service qos meanwhile microservice application suffers external IO pressure IO sensitive microservice scheduler critical microservice migrate idle node guarantee qos requirement nautilus guarantee ile latency external IO pressure technique suffers serious qos violation besides traditional datacenter resource management technology nautilus reduce computational resource network bandwidth usage respectively ensure ile latency target nautilus focus minimize microservice resource meeting throughput qos requirement runtime although training overhead nautilus affect runtime throughput qos requirement microservice application reduce training overhead RL algorithm optimal resource configuration future