gpu platform computation throughput mini batch neural network computation however batch ideal situation aim latency training mobile device partial retrain personalization irregular input sequence gpu performance suffers utilization batch recurrent neural network rnn application sequential computation article propose hybrid architecture FARNN combine gpu fpga accelerate rnn computation batch rnn computation gpu efficient gpu inefficient task fpga computation accelerate gpu inefficient rnn task FARNN load gpu inefficient task fpga evaluate FARNN synthetic rnn layer various configuration xilinx ultrascale fpga nvidia gpu addition evaluate rnn application evaluation indicates FARNN outperforms gpu platform rnn training batch input sequence rnn per layer introduction recurrent neural network rnn neural network dnn specialized series data video processing processing traffic prediction widely adopt rnns memory lstm gate recurrent gru variant attention neural network transformer bert popular sequential data processing rnns flexibility continuous processing data input data rnn series data sequence sequence video frame rnn computation responsible processing rnn layer rnn receives input input previous layer rnn previous rnn recurrently fed rnn basically rnn parameterized neural network performs accumulate operation difference rnn layer dnn layer rnn layer recurrent input however training rnn dnns typical training propagation FW backward propagation BW gradient computation GC update WU depends previous rnn computation perform significant performance bottleneck input sequence consequently mini batch gpu platform increase computation throughput avoids gpu utilization however mini batch ideal situation training conduct limited datasets reinforcement mini batch device limited available device memory training dnn model device memory consume dnn layer memory rnn layer mini batch batch global batch slice chunk device batch micro batch within device memory dnn model wider deeper computation batch per device becomes datasets sequence mini batch consists data sequence wasteful pad apply shorter data dnn training distribute compute node data privacy communication overhead federate FARNN apply server computation node retrain dnn personalization another batch rnn computation due privacy concern retrain dnn personal data perform mobile device however approach preserve privacy identification training data trust execution environment gpus FPGAs personalize training server platform efficient rnn training platform batch manifest cudnn library widely dnn library nvidia gpus cudnn library mode persistent kernel mode batch rnn training task cudnn operation mode popular dnn framework tensorflow pytorch propose FARNN novel hybrid accelerator platform combine gpu fpga accelerate batch rnn training rnn computation subtasks distinguish gpu efficient gpu inefficient computation FARNN accelerates rnn computation load gpu inefficient subtasks fpga fpga specially computation fully optimize gpu inefficient rnn subtasks FARNN accelerate rnn training rnn application gpu platform nvidia tesla gpu platform TSMC FPGAs FARNN reduces computation batch apply multi layer lstm network rnn recurrent projection FARNN reduces computation lstm layer reduction computation rnn FARNN outperforms gpu platform batch contribution FARNN FARNN fpga gpu hybrid rnn training platform outperforms gpu platform batch rnn computation gpu efficient gpu inefficient subtasks characteristic gpu inefficient subtasks dedicate computation gpu inefficient subtasks fpga moreover exploit parallelism gpu efficient gpu inefficient subtasks efficient communication mechanism gpu fpga pcie bus computation communication overlap gpu fpga evaluate FARNN synthetic rnn layer various configuration xilinx ultrascale fpga nvidia gpu moreover evaluate FARNN rnn application rnn project FARNN performance future technology generation organize introduces related rnn accelerator hybrid platform describes rnn inference training computation discus characteristic rnn computation task subtasks identify computation load fpga describes fpga computation finally evaluates performance efficiency FARNN conclusion related exist various hardware accelerator dnn computation ASIC fpga rnn accelerator propose however exist rnn accelerator target inference computation ese increase computation efficiency prune rnn lstm circulant matrix structure reduce parameter associate computation load brainwave npu neural processing inference enables latency inference computation float computation recently various hybrid architecture computation acceleration introduce fpga gpu hybrid platform dnn acceleration previous utilized fpga platform inference task cpu gpu hybrid platform widely adopt accelerate various task machine data cryptography HyGCN combine custom architecture chip accelerate distinctive computation task graph convolutional neural network poly proposes fpga gpu heterogeneous platform OpenCL workload difference exist dnn accelerator FARNN FARNN fpga gpu hybrid platform combine commodity hardware platform hybrid platform customize ASICs difference FARNN handle rnn training unlike FARNN inference platform adopt aggressive optimization technique prune precision computation increase computation efficiency however dnn training precision successfully accurately training moreover rnns sensitive arithmetic precision error accumulate FARNN matrix without prune computation fpga utilize float arithmetic FP background generic principle rnns lstm rnn principle apply gru rnns rnn layer consists series rnn internal structure lstm lstm receives input vector previous dnn layer recurrent vector refer rnn memory content dnn layer output vector lstm structure rnn vector previous input vector previous layer lstm computes gate forget input output compute    bri tanh  BRC  bro sourcewhere tanh sigmoid function hyperbolic tangent function respectively input matrix recurrent matrix respectively bias vector hereinafter refer bias parameter rnn layer gate modify rnn output vector layer wise operation tanh sourcewhere wise multiplication backward  rnn gradient output    calculate gradient previous   input  operation wise operation matrix vector multiplication decompose     source   tanh     source     source subtasks rnn computation distinguish rnn training subtasks identify rnn computation beneficial performance load fpga acceleration dnn training consists computation propagation FW backward propagation BW gradient computation GC update WU propagation FW rnn FW consists distinctive subtasks matrix vector multiplication MVM task MVM task applies input matrix input vector rnn layer refer task MVM task MVM task applies recurrent matrix recurrent vector refer task MVM task task activation task performs wise operation vector rnn gate activation function rnn gate structure distinguishes MVM MVM task respectively wise operation belong activation task MVM task partially calculates rnn gate combine MVM task obtain rnn gate described equation input MVM task previous rnn layer MVM task entire sequence simultaneously perform gpu platform achieve computation throughput computation task parallelism contrary MVM task previous computation cannot batch gpu platform cannot utilize abundant compute resource performance gpu computation throughput rnn training subtasks nvidia gpu tera float operation per tflops throughput lstm layer input sequence average input training MVM task computation throughput batch comparable theoretical peak gpu tflops contrast MVM task throughput batch MVM task utilizes peak tflops observation FARNN load MVM task fpga dedicate computation pipeline optimize recurrent computation gpu computation throughput comparison rnn training subtasks nvidia MVM task rnn executes activation task performs wise computation gate vector rnn vector activation task MVM task perform MVM task activation task fpga computation pipeline along MVM task delegate MVM activation task fpga gpu transfer recurrent matrix MVM task fpga return load computation fpga compute rnn output vector gpu backward propagation BW BW distinctive computation subtasks corresponds subtask FW BW MVM BW MVM BW activation BW activation task BW MVM task calculates gradient rnn vector accord equation calculate gradient vector recurrently fed previous FW task load fpga BW MVM task responsible calculate gradient rnn input vector input BW MVM task gradient gradient vector become available BW activation task completes FARNN performs BW MVM task gpu computation batch load BW activation task BW MVM task transfer data gpu fpga recurrent matrix gradient rnn output  return fpga transfer load BW MVM task BW activation task transfer become input BW MVM task GC gpu gradient computation GC update WU GC calculates gradient matrix MVM task output gradient  txt  tht sourcewhere input sequence BW completes vector deliver gpu fpga GC perform vector outer operation multiple independent vector computation perfectly gpu architecture implement nvidia cudnn library training input completes WU WU applies calculate gradient correspond parameter WU perform gpu along GC summarizes subtask data transfer fpga gpu goal FARNN fpga gpu task delegate gpu inefficient MVM task fpga along FW BW MVM task computation task MVM load fpga task FW BW activation task projection task subdivision rnn training fpga computation dedicate computation fpga handle delegate gpu inefficient task highlight FARNN architecture rnn optimize structure FARNN consists computation processing PEs contains memory locally portion matrix enables recurrent matrix accumulate mac computation vector matrix multiplication rnn perform continuously without reload parameter global chip memory scalable PE chain PEs chain fashion global datapath enables flexible fpga device maintain frequency latency hiding FARNN employ various technique reduce computation latency processing delegate computation task computation overlap fpga optimize data layout fpga architecture FARNN matrix vector multiplication MVM multiplies vector matrix vector MVM optimize repeatedly matrix multiple vector sequentially FARNN MVM lstm gate MVM configure gru MVM FW BW MVM task fpga architecture FARNN platform activation perform wise vector operation FW BW activation task operator FW BW activation task identical inefficient component task consequently FARNN computation pipeline FW BW activation task FW fpga computation pipeline creates loop connects MVM FW activation similarly pipeline switch MVM BW activation BW loop pipeline corresponds rnn matrix vector multiplication MVM FARNN pin matrix rnn layer chip memory entire sequence FW BW reduce overhead chip data traffic fpga fabric ideal configuration FPGAs amount chip memory chip memory module distribute fpga fabric processing building MVM processing PE consists multiplier accumulator chip memory matrix stationary dataflow PE chain FARNN MVM MVM task PEs fed vector per cycle vector rnn cycle PEs vector PE PE performs accumulate mac operation PE matrix accumulation corresponds vector vector deliver FW BW activation activation becomes input vector MVM chain PE interconnect float IP core frequency fpga mhz operation fpga limited frequency due delay complex interconnect component FARNN PEs chain simplify interconnect computation capacity PE input channel output channel predecessor channel sufficient handle data transfer PEs load input vector vector collection chain PE structure flexible FARNN connects PEs fpga resource permit PEs per chain mac operation input vector broadcast PEs vector deliver PE PE relay PE chain PE receives input cycle predecessor PE receives mac operation return previous PE output channel relay chain recursively PEs matrix parameter MVM task unlike mac operation parameter broadcast multiple PEs parameter unique PE load parameter rapidly multiple chain accept input separately chain chip data transfer parameter gpu fpga PE chain width chip data fully utilize chip bandwidth FARNN pci express pcie module data interface chain configuration FARNN load matrix parameter PEs rate parameter deliver pcie interface thanks chain fashion interconnect FARNN achieves mhz frequency xilinx ultrascale fpga maximum frequency IP core fpga fabric frequency exist fpga rnn platform ese rnn lstm achieve mhz frequency fix arithmetic brainwave operates mhz computation flexible chain PE interconnect enables multi FPGAs xilinx ultrascale fpga device consists multiple fpga super logic SLRs computation FARNN utilize fpga resource across slr frequency thanks chain PE interconnect contrast accelerator architecture limited scalability achieve latency improvement fpga fabric DPU xilinx  AI platform slr boundary fpga slr improve throughput latency improvement limited vector slice mac operation input vector collection PE chain rnn layer rnn MVM task involves multiple mac operation vector multiple slice corresponds output mac operation PE responsible multiple vector memory PE multiple rnn MVM PEs vector slice PE matrix chip memory rnn layer vector slice computation MVM task activation task overlap MVM slice activation processing partial activation task performs wise operation computation activation task hidden pipeline latency activation task shorter mac operation FARNN pipeline latency cycle propagation lstm activation hidden rnn layer multiple mac operation timeline MVM activation task overlap vector slice combination rnn PE inefficient PE utilization rnn PEs per MVM MVM task mac operation PEs utilized setup MVM task cycle FARNN mitigates issue splitting multiple PEs idle PEs split output accumulate cycle PEs previous cycle FARNN PEs per MVM rnn layer multiple mac operation rnn consequently MVM task cycle compute slice therefore rnn layer rnn activation pipeline latency hidden PE array computation throughput determines performance FARNN matrix layout BW transpose matrix described equation FARNN handle transformation fpga FARNN matrix distribute memory PEs transpose parameter PEs additional connection PEs inter chain connection across PE chain fpga rout complicate performance chip memory fpga matrix multiple rnn layer backward propagation FARNN handle rnn layer dnn multiple rnn layer instead FARNN simply reloads transpose matrix gpu BW gpu perform transformation layout activation FW MVM MVM task transfer gpu gate activation function apply addition gate apply previous rnn accord equation rnn FARNN implement activation pipeline primitive compute float compute addition subtraction multiplication sigmoid tanh sigmoid tanh function decompose combination float operation  tanh BW activation task  gradient obtain gradient vector correspond MVM task output output gradient MVM task gradient vector input BW MVM task gpu BW activation task transfer gpu BW MVM task MVM partial gradient rnn   sum partial gradient finally becomes gradient vector rnn  computation BW activation compose addition multiplication obtain derivative sigmoid tanh function derivative function simply compute equation  tanh BW activation task gate output FW instead recomputing vector FARNN vector FW reloads BW FARNN external dram fpga vector computation communication overlap FARNN subtasks statically distribute schedule fpga gpu computation dnn training fix computation task fpga trigger autonomously gpu fpga data transfer task MVM task transfer gpu dma fpga computation immediately without host intervention reduce computation latency FARNN schedule gpu computation task overlap fpga task data transfer enhance training performance FW FARNN overlap MVM task gpu matrix transfer gpu BW opportunity overlap GC WU task gpu overlap computation gpu handle previous layer backward propagation overlap benefit dnn configuration multiple layer rnns timeline computation task data transfer fpga task matrix computation layer matrix transfer performance bottleneck transfer overhead amortize longer rnn input sequence batch computation per MVM task output vector output gradient transfer gpu fpga per transfer performance bottleneck transfer rate GB lstm peak pcie bandwidth FW MVM task   transfer gpu fpga sequence activation task fpga computation vector gpu arrives fpga rnn output vector transfer gpu computation progress completion data transfer fpga gpu indicates propagation sequence host cpu intervene recurrent computation perform dma fpga gpu performance heterogeneous platform external interconnect limited data transfer FARNN maximizes performance directly  data fpga gpu nvidia gpus GPUDirect peer peer communication pcie device GPUDirect api FARNN pin target data gpu platform dma module fpga initiate pcie transfer access pin memory evaluation analyze FARNN performance characteristic synthetic benchmark evaluate FARNN performance benefit rnn application rnn environment evaluate FARNN combination nvidia gpu xilinx ultrascale VUP fpga evaluation gpu counterpart ultrascale fpga device manufacture TSMC benefit FARNN chain PE structure flexible available fpga resource FARNN performance PEs per chain PEs shortens computation MVM task reduce MVM task observation project performance FARNN fpga fpga ultrascale series VUP perform fpga implementation synthesis placement rout target VUP device PE chain without sacrifice frequency simulate performance FARNN VUP dummy PEs VUP evaluation platform although dummy PEs perform computation cycle accurate performance measurement summarizes PEs VUP FARNN VUP FARNN lstm gru implementation respectively lstm version shorter PE chain gru version lstm MVM fpga resource usage FARNN xilinx ultrascale VUP VUP device thanks memory fpga fabric capacity memory FARNN implementation xilinx FPGAs PEs chip memory  module parameter storage per PE rnn PE chip memory module VUP lstm layer configuration PE memory capacity increase parameter reduce PE capacity memory configuration compute resource LUTs dsp distribute fpga chip memory URAM fpga almost equally utilized computation task gpu implement cuBLAS cudnn library MVM task essentially matrix multiplication implement cuBLAS gradient computation task perform cudnn function convolution layer evaluate performance gain FARNN gpu platform nvidia gpu gpu platform highly tune rnn function cudnn library cudnn rnn routine configure rnn computation algorithm standard algorithm executes rnn computation sequence operation option persistent static persistent dynamic persistent kernel approach rnn gpu memory register throughout computation performance gpu platform persistent dynamic cudnn PD mode generally performance batch rnn computation gpu execution cudnn std normalize execution measurement evaluation synthetic benchmark evaluate performance FARNN various synthetic rnn setup execution reduction achieve FARNN baseline cudnn std various rnn setting stack graph breakdown rnn training task FARNN overlap gpu fpga task execution BW GC WU task combine training performance comparison rnn per layer rnn layer input sequence batch training performance comparison rnn input sequence rnn layer lstm gru batch training performance comparison computation batch rnn layer input sequence lstm gru rnn execution rnn parameter FARNN cudnn PD significantly reduce execution cudnn std reduction cudnn PD diminishes rnn increase rnn parameter exceeds gpu memory capacity performance cudnn PD degrades due increase global memory access magnitude increase fpga ideal platform load computation task fpga fabric amount chip memory VUP MB chip memory matrix rnn lstm per layer FARNN FARNN archive execution reduction cudnn PD respectively gru per layer capacity lstm FARNN FARNN outperform cudnn PD respectively input sequence performance affected input data sequence rnn aforementioned FARNN faster longer input sequence significantly reduce execution cudnn std cudnn PD input per sequence FARNN significant performance benefit cudnn std baseline however FARNN lstm faster cudnn std sequence load rnn training subtasks fpga FARNN transfer matrix gpu fpga transfer overhead occurs layer propagation backward propagation data transfer overhead amortize longer data sequence batch cudnn PD FARNN primarily focus batch performance rnn training performance gain diminishes batch batch baseline cudnn std outperforms cudnn PD FARNN execution cudnn PD increase almost linearly batch increase contrast execution increase sub linear FARNN batch increase execution FARNN increase matrix transfer fpga across batch thereby reduce impact transfer overhead achieve computation throughput lstm training peak throughput batch respectively peak throughput performance gpu fpga compute resource fully utilized MVM MVM task respectively FARNN configuration performance FARNN configuration fpga gpu computation load strategy lstm training normalize FARNN FARNN  configuration performs FW BW task fpga MVM task GC WU task perform gpu FARNN  training FARNN MVM task cannot exploit computation capacity gpus training performance comparison fpga gpu computation load strategy rnn layer input sequence batch FARNN  configuration employ computation communication overlap FARNN  gpu fpga data transfer FW BW GC WU task execute sequentially computation overlap FARNN improves performance FARNN  average computation accuracy FARNN FP operation reduce precision throughout computation computation FARNN gpu platform validate besides numerical difference induced float operation FARNN computation task application dnn multiple layer rnns  dataset training input average rnn sequence perform rnn setup bidirectional gru layer DS gru evaluate configuration lstm DS lstm DS lstm per layer parameter gru model performance FARNN report execution account entire dnn convolutional fully layer affected FARNN others portion graph batch FARNN achieves execution reduction DS lstm DS gru respectively training performance comparison FARNN cudnn PD target accelerate batch computation performance gain cudnn std decrease batch increase however FARNN outperform cudnn PD cudnn std batch transfer  gpu fpga amortize batch longer input sequence FARNN cudnn PD mini batch rnn computation extra pad equalize input training input instance per batch pad increase average input sequence average FARNN maintains performance advantage batch application rnn rnn recognition model target mobile application rnn consists multi layer rnns prediction network encoder network evaluation FARNN encoder network account training baseline gpu platform encoder network unidirectional lstm layer recurrent projection recurrent projection additional matrix multiplication reduces output vector dimension recurrent projection compute matrix multiplication handle fpga reuse MVM cudnn library persistent kernel mode recurrent projection therefore rnn gpu platform evaluate cudnn std evaluate gru version rnn cudnn recurrent projection gru training rnn others portion corresponds prediction network non rnn layer rnn batch speedup encoder network cudnn std FARNN FARNN respectively training training model reduce FARNN FARNN FARNN maintains performance advantage batch VUP VUP respectively rnn training performance comparison evaluate DS rnn cpu platform google tpu platform practically dnn training task cpu platform evaluate application core xeon intel  tpu platform google tpu instance tensorflow however platform evaluate platform batch rnn training task DS lstm cpu platform tpu platform cudnn std respectively efficiency FARNN inherently gpu platform due additional fpga however combine performance gain FARNN improve efficiency average consumption epoch rnn training consumption external meter separately idle without gpu fpga accelerator platform instal exclude idle FARNN gpu platform consumption emphasize penalty FARNN gpu setup normalize gpu setup FARNN consumption DS lstm batch FARNN gpu consumption performance contributes delay EDP metric rnn training batch FARNN VUP reduces EDP respectively batch efficiency FARNN degrades mainly due decrease performance improvement FARNN significant performance improvement cudnn std DS gru batch FARNN efficiency measurement FARNN VUP configuration simulated dummy PEs FARNN VUP performance benefit consumption EDP pcie lane usage accelerator pcie lane valuable asset fpga gpu FARNN pcie lane FARNN occupies pcie lane socket xeon cpu configuration restricts pcie lane setup FARNN pcie lane gpu setup performance impact limited matrix transfer setup increase training DS lstm gpu slot FARNN newer technology generation evaluate FARNN gpu TSMC xilinx ultrascale FPGAs comparison newer gpu nvidia newer generation fpga unfortunately fpga gpu generation become available synchronously currently generation xilinx FPGAs  fully release instead project performance FARNN future platform conservatively estimate performance FARNN future  fpga FARNN VS FARNN VS performance simulated dummy PEs FARNN  fpga report logic chip memory capacity generation VUP fpga FLOPS FARNN VS gpu performance estimation advantage platform FARNN component MVM FARNN VUP activation pcie interface ddr controller others affected FARNN fpga gpu transfer overhead account faster pcie interface FPGAs project performance FARNN  FARNN VS gpu training rnn tensor core activate batch conservative projection FARNN VS outperforms estimate performance project FARNN enhance performance gain future platform FARNN performance projection newer technology generation comparison fpga gpu hybrid architecture dnn training FARNN exist fpga gpu hybrid architecture dnn training propose combination fpga gpu efficient dnn usage scenario however dnn training solely gpu platform fpga platform inference computation although hybrid platform achieve significant speedup gpu inference task model dnn model lenet fpga gpu hybrid architecture dnn training propose hybrid dnn training platform hype training combine gpu multiple FPGAs hype training target cnns transformer model FARNN target rnn model optimization target hype training improve efficiency increase training performance therefore multiple FPGAs FPGAs per gpu performance improvement restrict maximum performance gain hype training FARNN achieves performance improvement gpu platform conclusion novel hybrid approach FARNN combine fpga gpu optimize rnn computation FARNN hybrid rnn platform accelerates rnn training batch demonstrate FARNN architecture representative rnns lstm gru evaluate FARNN xilinx ultrascale fpga nvidia gpu rnn application rnn synthetic rnn layer various configuration evaluation indicates outperforms gpu platform batch input sequence rnn per layer FARNN highly effective training rnns relatively parameter lstm