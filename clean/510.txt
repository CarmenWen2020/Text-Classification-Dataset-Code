performance multi gpu compute becomes inevitable trend due increase demand computation capability emerge domain data simulation however lack understand gpus impact interconnect technology multi gpu application performance become hurdle gap conduct thorough evaluation gpu interconnects pcie NVLink NVLink NVLink sli NVSwitch server hpc platform nvidia DGX DGX DGX  SummitDev summit supercomputer sli link nvidia turing RTX gpus empirical evaluation gpu communication network numa trigger NVLink topology connectivity rout pcie chipset issue observation application multi gpu node gpu combination impose considerable impact gpu communication efficiency application overall performance evaluation leveraged building practical multi gpu performance model vital gpu task allocation schedule migration environment AI hpc communication orient performance tune introduction multi gpu execution nowadays becomes inevitable trend warehouse gpgpu compute due increase demand computation capability emerge domain machine data simulation increasingly scalable gpu compute becomes recently research sony leveraged gpus resnet neural network imagenet achieve optimal gpu efficiency swiss national supercomputing  rely gpus piz  supercomputer simulate global climate ultra resolution multi gpu execution direction vertically node horizontally across multiple node intra node scenario nvidia DGX DGX super AI server incorporate gpus NVLink NVSwitch respectively inter node scenario department doe recently deployed gpu accelerate supercomputer summit sierra  ridge  national laboratory gpu integrate node interconnect gain performance multi gpu however trivial mainly mature multi gpu parallel program execution performance model largely due limited knowledge gpus interconnect communication traditionally inter gpu communication bus interconnect cpu gpu communication pcie situation recently due introduction gpu orient interconnect NVLink NV sli NVSwitch however characteristic performance impact multi gpu application unknown limit effort leverage advanced performance tune delivery gap thoroughly characterize variety gpu interconnects pcie NVLink version NVLink version NV sli NVSwitch GPUDirect raw startup latency sustainable uni directional bandwidth network topology communication efficiency rout numa communication peer peer PP collective CL summarize observation challenge address potential research topic regard multi gpu execution evaluation software designer gain deeper knowledge gpu interconnects pave building mature multi gpu program execution performance model reliable simulator application development performance tune evaluation finding intra node gpu peer peer communication address numa due specific connectivity rout choice respectively NVLink numa pcie label anti locality NV sli NVSwitch fully uma intra node gpu collective communication latency interconnects increase linearly participant gpus however bandwidth decline pcie increase NVLink due distinct network topology versus hypercube addition  NVSwitch ideal overall bandwidth balance gpus reduce bandwidth unbalanced gpus NVLink NVSwitch reduce scatter inter node gpu peer peer communication SummitDev GPUDirect RDMA essentially exhibit inferior performance copying pin memory performance degrades data array summit GPUDirect RDMA exhibit performance performance degradation data array disappears inter node gpu collective communication latency bandwidth almost unchanged participant node reduce SummitDev meanwhile reduce scatter exhibit significantly average bandwidth node communication SummitDev node summit regard application intra node communication unless cpu centric distributive program model adjust gpu centric program model hardly exist multi gpu implementation benefit faster gpu interconnect NVLink bottleneck essentially communication cpu gpus inter node communication enable GPUDirect pin host memory generally performance gain however programmability issue integrate NCCL mpi gpu interconnect focus gpu interconnect pcie NVLink NVLink NV sli NVSwitch GPUDirect enable infiniband platform evaluation gpu gpu communication DGX DGX evaluate pcie NVLink NVLink sli NV sli DGX NVSwitch SummitDev summit assess inter node infiniband interconnect GPUDirect RDMA briefly review technology evaluation platform evaluation platform pcie peripheral component interconnect express bus pcie serial computer expansion bus standard traditionally gpu integrate multiple gpu device CPUs via pcie however interconnect cpu dram pcie becomes performance bottleneck gpu acceleration exacerbates pcie gpu PP communication adopt dash illustrate gpus interconnect pcie QPI DGX pcie network DGX balance structure gpu gpu via pcie switch switch cpu socket scenario apply gpus finally dual cpu socket bridge  interconnect QPI pcie DGX topology adopts pcie switch pcie NVLink topology DGX DGX connection gpus interconnect NVLink connection gpus interconnect NVLink NVLink generation NVLink NVLink communication interface device signal interconnect  PP communication enables cpu gpu gpu gpu link allows remote cpu host memory peer gpu device memory remote atomic operation feasible NVLink bidirectional link consists  direction  contains differential  lane embed integrate transmission packet varies byte byte flit sixteen flit communication efficiency strongly correlate packet overall report twice efficient pcie NVLink cable terminal plug whereas gpu incorporates NVLink slot slot via NVLink cable dictate topology bandwidth gpu network multiple cable gang enhance bandwidth link endpoint pascal gpu quad NVLink slot therefore dual gpu gpus  bandwidth link DGX gpu network topology DGX hypercube mesh gpu occupies cube NVLink connection upper diagonal fully subset topology balance connectivity inside access within uma access node across numa directly link gpu gpu NVLink rout terminal directly link relies explicit rout user specify intermediate node SummitDev interconnect topology inside machine node SummitDev illustrate gpus per node partition subset gpus per subset dumbbell topology subset cpu socket constitute triple node fully subnetwork node cpu gpu subnetwork NVLink link subnetworks communicate via bus GB unlike DGX connection gpus subnetworks NVLink slot gpus already occupy NVLink interconnect topology SummitDev summit platform adopt dual structure subset comprise fully interconnect node node link  per NVLink SummitDev NVLink summit NVLink generation NVLink improves per link bandwidth link slot per gpu addition link slot gpu feature NVLink slot bandwidth link enhance percent besides operating mode introduce link heavily exploit extra link slot enable alternation network topology DGX DGX strengthen connectivity within backbone inside hypercube mesh network connection enables bandwidth link suspect improve efficiency collective communication summit interconnect network topology machine node summit gpus organize dual dumbbell topology gpus subset cpu socket quad node fully subnetwork node cpu gpu subnetwork NVLink link subnetworks via bus GB node subnetwork NVLink slot per gpu fully utilized gpu connection subset NVLink sli scalable link interface sli crossfire traditionally graphic purpose however recent turing architecture gpus TU TU multi gpu bridge NVLink interconnect technology bridge gpus communicate render gpgpu task gpu memory sli platform TU RTX gpu NVLink link GB per direction per link deliver aggregate bidirectional bandwidth GB NVLink sli turing gpus NVSwitch NVSwitch propose mainly address communication emerge application neural network training NVSwitch currently DGX topology NVSwitch pcie DGX respectively NVSwitch NVLink switch chip intra node communication feature NVLink per switch baseboard contains  host gpus gpu incorporates NVLink slot  simultaneously target per link GB bidirectional bandwidth NVSwitch fully non crossbar intra baseboard communication gpus baseboard gpus baseboard bandwidth GB switch GB via NVSwitch hop another baseboard meaning gpus baseboard gpus baseboard bandwidth GB NVSwitch hop baseboard baseboard raw bisection bandwidth GB link switch switch TB per NVSwitch reserve NVSwitch interconnect topology DGX NVSwitch interconnect topology DGX pcie interconnect topology DGX pcie interconnect topology DGX data integrity cyclical redundancy cod crc impose detect NVLink transfer error replay transfer error code ecc enable datapaths rout structure addition hop address fidelity buffer enforce NVSwitch avoid illegal access fabric manager monitor rout application infiniband GPUDirect RDMA GPUDirect infiniband discus infiniband already widely hpc platform extensively instead focus relation gpu kepler architecture nvidia gpus introduce GPUDirect RDMA correspondingly amd propose  RDMA enables pcie device IB host channel adapter HCA directly access gpu device memory via pcie without assistance cpu stag memory significantly improves efficiency inter node gpu communication achieve IB RDMA gpu vendor OS kernel extension return dma bus mapping gpu device memory user creates IB signal IB driver target address gpu device memory IB driver routine obtain dma mapping finally normal IB virtual memory structure return user program target normal cpu memory GPUDirect RDMA enable SummitDev summit  interconnect microbenchmarking evaluate characteristic gpu interconnects microbenchmarks tartan benchmark suite platform focus peer peer PP collective CL communication intra node PP concentrate assess node NVLink NVSwitch NV sli technology topology latency bandwidth efficiency message numa inter node PP discus latency bandwidth efficiency message  latency calculate bandwidth tartan microbenchmarks intra node PP communication latency communication latency raw latency transmit shortest message arbitrary gpus via pcie NVLink DGX platform already mention NVLink PP rout gpus directly NVLink rout transit router scenario exhibit shorter latency pcie latency demonstrate communication latency access gpus via pcie imply numa latency pcie latency pcie switch across local cpu socket across QPI bridge roughly meanwhile pcie latency increase slightly DGX DGX bandwidth unchanged deeper communication pipeline DGX NVLink latency pcie NVLink 5D significant numa node directly latency around node manual rout latency increase DGX DGX NVLink exhibit latency NVLink potentially due deeper pipeline operating frequency NV sli latency latency pcie NV sli sli platform dual gpu latency local access NV sli access gpu pcie access gpu gpus directly link NV sli without intervention cpu dma etc NVSwitch latency latency pcie NVSwitch DGX platform gpus grid finer regular remote access homogeneous pcie NVSwitch confirm NVSwitch fully although access gpus baseboard incurs switch hop difference latency almost negligible DGX PP communication latency along anti diagonal local communication numa pcie latency orange refer node remote node respectively exhibit disparity sustainable bandwidth illustrate uni bidirection sustainable bandwidth pcie NVLink DGX platform respectively uni bidirection bandwidth pcie NV sli sli platform finally uni bidirection bandwidth pcie NVSwitch DGX pcie unidirection bandwidth slight numa pcie access gpus pcie switch exhibit bandwidth measurement gpus socket bandwidth gpus pcie switch deliver bandwidth pcie bidirection bandwidth numa bidirectional bandwidth gpus pcie switch prominent unidirection bandwidth pcie numa novel observation describes scenario nearby access performance remote access label numa anti locality knowledge exist phenomenon without mention leverage performance tune anti locality possibly due unbalanced physical signal pcie switch chipsets pcie anti locality bandwidth NVLink unidirection bandwidth NVLink scenario complicate NVLink connection local access node directly NVLink remote node additional rout correspond topology illustrate NVLink 6D connection local access node dual link backbone node link remote node correspond topology numa NVLink numa remote node NVLink latency bandwidth DGX PP unidirectional bandwidth although obvious along anti diagonal indicates anti locality numa unidirection bandwidth pcie confirm numa remote node numa NVLink obvious bidirection bandwidth DGX PP bidirectional bandwidth along anti diagonal clearly illustrate anti locality numa pcie numa node meanwhile along anti diagonal obvious imply existence numa remote node sli PP communication latency sli PP communication latency sli PP unidirectional bandwidth sli PP unidirectional bandwidth sli PP bidirectional bandwidth DGX PP communication latency DGX PP unidirectional bandwidth DGX PP bidirectional bandwidth numa node NVLink due link node DGX typically latency remains link introduce bandwidth difference numa remote node NVLink choice rout 6D bandwidth disparity NVLink bidirection bandwidth NVLink numa bidirectional bandwidth obvious unidirectional bandwidth caption NV sli unidirection bandwidth NV sli incorporates gpus communication symmetric numa NV sli bidirection bandwidth bidirectional unidirection bandwidth NVSwitch unidirection bandwidth bandwidth remote access NVSwitch consistent uma imply NVSwitch hop degrade bandwidth NVSwitch bidirection bandwidth bidirection bandwidth bandwidth rout gpu interconnects discus remote access DGX via NVLink explicit rout explore numa alternative rout choice demonstration purpose source node PP communication remote node specify rout microbenchmarks tartan latency unidirection bandwidth bidirection bandwidth rout respectively DGX platform unidirection bidirection bandwidth NVLink DGX numa height  DGX isomorphic connection incorporates link however NVLink DGX scenario emerge dual bandwidth link route bandwidth occurs trigger rout nevertheless latency remains scenario numa rout choice remote gpu access via NVLink efficiency message illustrates PP latency unidirection bidirection bandwidth respect message via pcie NVLink DGX DGX illustrates plot NV sli pcie sli illustrates plot NVSwitch pcie DGX latency latency remains unchanged data communication KB DGX local access DGX increase KB link bandwidth saturate deeper communication pipeline NVLink sli NVSwitch sli DGX platform observation slight divergence pcie local pcie pcie switch pci remote access latency message MB bandwidth unidirection bandwidth interconnect saturate MB NVLink imply optimal sustainable bandwidth interconnect MB data transmit bidirection bandwidth addition latency increase KB bandwidth saturate MB implies KB suffer extra delay gain overall bandwidth improvement bandwidth saturation DGX NVSwitch uni bidirection bandwidth NVSwitch hop NVSwitch hop curve fully align confirm access remote baseboard imposes extra overhead pcie bidirection bandwidth observation message KB pcie anti locality pcie remote access delivers bidirection bandwidth pcie anti locality exists pcie pcie DGX bandwidth pcie slightly pcie anti locality finally gpu local access bandwidth stagger MB possibly due exceed boundary PP communication latency unidirection bidirection bandwidth increase message via pcie NVLink DGX PP communication latency unidirection bidirection bandwidth increase message via pcie NV sli sli PP communication latency unidirection bidirection bandwidth increase message via pcie NVSwitch DGX intra node collective communication PP communication sender receiver collective communication CL involves multiple sender receiver complicate CL generally broadcast scatter reduce reduce etc extensively application molecular dynamic graph analytics etc efficiently implement CL communication challenge understand underlie hardware network topology enable orchestrate mapping handle issue synchronization overlap deadlock performance metric application feature latency orient transfer bandwidth orient transfer relieve burden user nvidia collective communication library NCCL primitive mpi collective amd  NCCL currently CL broadcast reduce reduce reduce scatter maximum bandwidth NCCL construct network communication participant broadcasting reduction efficiently realize partition data chunk transmit along pipeline fashion claimed algorithm optimal bandwidth standard CL operation easily apply various network topology describes network construct pcie NVLink NVLink DGX respectively NCCL pcie NVLink NVLink interconnect pcie traverse binary network NVLink independent marked solid dash NVLink link backbone network link version NCCL library NCCL opensource intra node pcie QPI interconnect network NCCL source NVLink pcie NVSwitch NV sli IB IP network automatically integrate maximize overall bandwidth although combination improves overall performance introduces difficulty independently CL communication efficiency interconnect consequently NCCL employ pcie CL evaluation NCCL adopt NVLink NVSwitch NV sli CL evaluation DGX CL communication CL latency illustrates CL communication startup latency respect gpus involve NCCL DGX platform respectively correspond pcie QPI NVLink observation latency increase participate gpus almost linear fashion behavior NCCL DGX platform latency NVLink increase faster pcie reduce NVLink increase faster NVLink pcie reduce latency  curve odd gpus likely due NCCL algorithm NVLink PP numa later intra node CL communication latency variable participant gpus NCCL pcie QPI NCCL NVLink CL bandwidth CL sustainable communication bandwidth increase gpus GB payload pcie CL bandwidth decrease gpus due bus contention network NVLink however CL bandwidth essentially increase gpus due link hypercube mesh network pcie NVLink behavior DGX DGX trend however NVLink exhibit significantly bandwidth gpus gpus NVLink strength dual link backbone numa significant gpus imply congestion NCCL gpus isolated fully upper comprise relies bidirectional communication rout sever performance bottleneck avoid adopt gpus application setup intra node CL communication bandwidth variable participant gpus NCCL pcie QPI NCCL NVLink CL efficiency message CL bandwidth respect message increase GB gpus pcie CL bandwidth saturates MB whereas NVLink bandwidth saturates around MB CL exhibit trend bandwidth message intra node CL communication bandwidth gpus increase message NCCL pcie QPI NCCL NVLink NV sli CL communication CL latency bandwidth sli contains gpus histogram latency bandwidth CL communication latency NV sli around pcie reduce reduce significantly latency NV sli bandwidth pcie NV sli reduce scatter poorer bandwidth others pcie NV sli sli pcie NV sli CL communication latency sli pcie NV sli CL communication latency sli pcie NV sli CL communication bandwidth sli pcie NV sli CL communication bandwidth CL efficiency message CL bandwidth respect message gpus pcie NV sli bandwidth saturate MB confirms reduce scatter bandwidth others message delivers relative bandwidth message saturate sli pcie NV sli CL bandwidth efficiency sli pcie NV sli CL bandwidth efficiency NVSwitch CL communication CL latency bandwidth illustrates CL communication latency respect gpus DGX correspond pcie NVSwitch due uma NVSwitch curve align confirms latency reduce pcie interconnect consistent reduce CL primitive startup latency pcie NVSwitch participate gpus imply advantage NVSwitch NVLink bandwidth latency observation confirm NVSwitch tremendously bandwidth pcie particularly reduce reduce broadcast reduce scatter stagger behavior bandwidth NVSwitch gpus gpus consistent NVLink scenario 0D NVSwitch uma implies stagger issue due interconnect topology NCCL implementation DGX pcie NVSwitch CL communication latency DGX pcie NVSwitch CL communication bandwidth DGX pcie NVSwitch CL communication bandwidth CL efficiency message CL bandwidth respect message gpus DGX curve pcie consistent pcie curve mostly align KB MB message reduce broadcast divergence NVLink 1D disappears NVSwitch NVSwitch uma suppose misalignment numa pcie NVLink network DGX pcie NVSwitch CL bandwidth efficiency gpus DGX pcie NVSwitch CL bandwidth efficiency gpus inter node PP communication latency bandwidth inter node PP communication SummitDev supercomputer summit supercomputer  ridge national laboratory SummitDev supercomputer node node contains ibm CPUs nvidia gpus gpu interconnect NVLink summit feature node ibm CPUs nvidia gpus gpu interconnect NVLink SummitDev summit GPUDirect inter node PP conduct measurement configuration GPUDirect RDMA directly access gpu memory node GPUDirect enable GPUDirect refers option SummitDev  MCA   enable cuda summit  gpu  GPUDirect data gpu memory pin host memory transfer data another node pin host memory finally target gpu memory GPUDirect enable  GPUDirect disabled  GPUDirect via host  memory GPUDirect enable  GPUDirect disabled latency bandwidth respect message GB scenario illustrate SummitDev summit respectively illustration latency curve axis bandwidth curve normal SummitDev observation KB difference curve latency bandwidth GPUDirect RDMA performance KB latency KB bandwidth KB possibly due limitation chipsets PP access cpu  MB GPUDirect RDMA advantage bandwidth obtains optimal bandwidth GB MB however  GPUDirect scheme demonstrates GB sustainable bandwidth message bandwidth GPUDirect RDMA actually degrades dramatically MB imply message multiple MB transfer SummitDev summit regard observation unlike SummitDev GPUDirect RDMA performance configuration summit delivers latency message MB exhibit bandwidth message KB meanwhile performance latency increase bandwidth degradation SummitDev disappears summit technology GPUDirect improve significantly SummitDev summit becomes choice gpu inter node communication SummitDev inter node PP latency bandwidth efficiency summit inter node PP latency bandwidth efficiency inter node collective communication regard inter node collective communication latency bandwidth respect participant node SummitDev summit tune node gpu per node utilized similarly startup latency data transfer sustainable bandwidth sufficiently data transfer GB latency SummitDev summit respectively bandwidth bandwidth increase message illustrate difference enable disable GPUDirect suspect GPUDirect RDMA internally enable NCCL CL latency SummitDev IB network latency perform CL operation remains node reduce observation drawn summit divergence reduce imply joint algorithm implementation NCCL interconnect technology GPUDirect CL bandwidth bandwidth NVLink 0D numa emerge node reduce scatter SummitDev summit happens node nevertheless bandwidth overall remains unchanged bandwidth scenario exhibit pcie decrease NVLink increase inter node PP communication CL efficiency message finally bandwidth CL operation converge saturate around MB message demonstrate nearly GB sustainable peak bandwidth SummitDev summit respectively overall summit delivers gpu inter node communication performance SummitDev inter node CL communication latency bandwidth variable participant node bandwidth node increase message inter node CL communication latency bandwidth variable participant node bandwidth node increase message inter node CL communication latency bandwidth variable participant node bandwidth node increase message inter node CL communication latency bandwidth variable participant node bandwidth node increase message  interconnect benchmarking microbenchmarking exhibit characteristic gpu interconnects however multi gpu application impact remains unknown tartan benchmark suite evaluate impact gpu interconnect application particularly focus aspect impact faster gpu interconnect NVLink pcie intra node application impact GPUDirect RDMA inter node application perform measurement fix reduction increase gpus weak reduction increase gpus fix per gpu overall performance entire application speedup cpu command application elapse performance metric comparison across application application vendor nvprof gpu communication    DH  D2D gain knowledge underlie communication report data average multiple execution tartan benchmark suite intra node intra node scenario evaluate application DGX DGX without  application cod leverage available gpus configure environment export cuda visible  manipulate gpus visible application illustrate latency communication  DH D2D regard implementation baseline modification NCCL intra node weak DGX intention implementation performance pcie modification tartan convert portion cpu gpu communication gpu gpu communication performance gain NVLink DGX performance entire application supplementary file computer society digital library http doi org ezproxy auckland tpds due limitation impact NVLink although observation microbenchmarking NVLink significantly improve inter gpu communication efficiency improvement directly transform overall communication latency reduction application speedup supplementary file available online  GMM significant difference baseline NCCL platform assess multi gpu application candidate program model cpu handle sequential portion code gpus processing parallel portion model communication occurs cpu gpus inter gpu transaction presume addition cpu gpu communication highly optimize  GMM kmn MTC PLN   PLN GMM  manage convert cpu gpu communication gpu gpu communication NCCL  obvious portion DH  communication replace D2D GMM although gain latency reduction nvprof trace file D2D communication transaction DH  NCCL  kernel potentially flexible strategy adopt NCCL efficiently leverage available interconnect bandwidth data per transmission D2D communication via NVLink PLN  kmn MTC gpu gpu communication D2D kmn actually local data movement within gpu cnn another cnn gpus cnn implementation arbitrary data copying arbitrary peer runtime currently fails gpus utilized gpus directly NVLink NVLink topology internally  unified data copying across gpus NVLink already internally enable cuda NVLink equip machine DGX modify cnn code pcie essentially enforce correctly cnn exhibit gpus participant computation DH communication increase dramatically potentially due gradient merge overhead propagation application mostly program model communication account execution alone inter gpu communication tend avoid previously application hardly become bottleneck finally employ NVLink PP CL introduces additional overhead enable disable peer access rout NCCL initialization etc normalize latency reduction NVLink NCCL node nvidia DGX normalize latency reduction NVLink NCCL weak node nvidia DGX normalize latency reduction NVLink NCCL node nvidia DGX normalize latency reduction NVLink NCCL weak node nvidia DGX normalize latency reduction NVLink NCCL weak node nvidia DGX summarize faster gpu interconnect NVLink report beneficial accelerate framework however regard gpgpu application without replace underlie cpu centric program model distribute parallelization model migrate communication role gpu load gpu communication cpu gpu via technique  optimize inter gpu communication via faster intra node gpu interconnect  hardly become significant entire application speedup therefore observation pave develop interconnect friendly program model multi gpu scenario faster interconnect  truly role improve overall application efficiency inter node inter node scenario application tartan SummitDev summit mpi rank binding node gpu discussion overall application performance scenario GPUDirect RDMA  GPUDirect   GPUDirect  illustrate speedup respect node  weak respectively SummitDev illustrate speedup weak summit intra node mpi inter node application exhibit behavior weak imply intra node interconnect inter node network easy become bottleneck improve inter node network significant performance gain multi gpu application performance speedup infiniband GPUDirect RDMA multi node  SummitDev performance speedup infiniband GPUDirect RDMA weak multi node  SummitDev performance speedup infiniband GPUDirect RDMA multi node  summit performance speedup infiniband GPUDirect RDMA weak multi node  summit regard GPUDirect IB interconnect observation enable GPUDirect immediate performance enhancement transmit data reside cpu memory gpu memory pin memory beneficial coordination GPUDirect enable GPUDirect RDMA helpful application   SummitDev overall summit potentially due significant improvement GPUDirect RDMA performance summit SummitDev overall multi node performance summit obvious SummitDev due degradation communication efficiency summit essentially reverse summit improves fundamental communication bandwidth enhances baseline performance relative speedup summit SummitDev overall application developer adopt  GPUDirect SummitDev GPUDirect RDMA summit application benefit faster inter node interconnect IB RDMA difficulty hardware application communication abstract interface mpi mpi implementation internally integrate NCCL harvest multi gpu interconnect performance NVLink IB RDMA easy initiate communication completely gpu without cpu intervention via GPUDirect async gpu trigger networking critical gpu performance delivery 5Discussion gpu interconnect technology NVLink claimed transparent reality complicate leveraged performance numa intra node gpu interconnect technique pcie NVLink numa platform due various topology connectivity rout chipset etc NVSwitch NV sli uma heterogeneity platform incorporate interconnect network network characteristic exclusively concurrently cooperatively therefore handle interconnect heterogeneity interconnect others NVLink advantage bandwidth latency pcie leverage simultaneously cooperatively integrate inclusive runtime communication efficiency factor restrict delivery optimal communication performance message dual isolated subnetworks summit SummitDev hardware limitation pcie anti locality GPUDirect RDMA SummitDev library implementation NCCL DGX node difficulty leverage gpu interconnect efficient inter gpu communication evaluation motivates research direction develop novel multi gpu program model exist multi gpu program model rely cpu orient parallel program model openmp mpi manage multiple gpus consequently mismatch cpu gpu model hardly benefit inter gpu network legacy adopt gpu interconnect technology integrate NCCL mpi NCCL delivers communication efficiency mpi reduce therefore multi gpu program model highly desire adaptive portable tractable effectively address complexity aforementioned addition exist multi gpu algorithm usually minimize eliminate communication due performance gap local access remote access via pcie emergence gpu interconnect foresee development trend reconsider role inter gpu communication parallel model algorithm develop practical multi gpu performance model performance prediction optimization analytics multi gpu application development tune addition appropriate performance model crucial gpu task allocation schedule migration environment hpc develop communication library underlie interconnect deliver performance regard dual subnetwork interconnect topology NVLink summit worth efficiently distribute exchange data data reuse bus bandwidth consideration another recent 2D torus communication deliver communication efficiency NCCL reduce obviously migrate inter node intra node NVLink interconnect DGX NVSwitch DGX multiple communication construct desire traditional cpu hpc application onto gpu pre exascale summit sierra  community effort planning pursue research direction future gpu analytic model performance optimization related intra node gpu compute analyze numa multi gpu node optimization guidance propose rely hybrid memory cube  memory network simplify multi gpu memory management improve programmability realize gpu aware mpi data communication intra node gpus standard mpi ben described automatic multi gpu partition framework distribute workload memory access software program interface compiler runtime partition gpu kernel multi gpu execution node finally evaluate potential performance benefit tradeoff amd  compute roc platform heterogeneous architecture HSA multi node gpu compute mpi multi node gpu compute introduce mpi integrates cuda data movement transparently mpi propose hardware approach overlap computation communication gpu cluster analyze exascale proxy application communication propose algorithm gpus comply mpi constraint propose pipelined chain mpi broadcast collective operation multi gpu node facilitate various framework conclusion characterize evaluate gpu interconnects pcie NVLink NVLink NV sli NVSwitch infiniband GPUDirect RDMA tartan benchmark suite gpu server hpc platform nvidia DGX DGX DGX RTX sli  SummitDev summit supercomputer peer peer collective communication address numa intra node gpu communication propose insightful observation enable practical optimization guideline evaluation attempt hpc community multi gpu research development particularly construction mature multi gpu program execution performance model