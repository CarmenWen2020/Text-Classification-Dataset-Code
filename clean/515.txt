difficulty deploy various DL model diverse DL hardware boost research development DL compiler community DL compiler propose academia tensorflow XLA tvm similarly DL compiler DL model described DL framework input generate optimize code diverse DL hardware output however none exist survey analyze unique architecture DL compiler comprehensively article perform comprehensive survey exist DL compiler dissect commonly adopt detail emphasis DL orient multi irs frontend backend optimization detailed analysis multi irs illustrate commonly adopt optimization technique finally insight highlight potential research direction DL compiler survey article focus architecture DL compiler pave future research towards DL compiler introduction development DL generate profound impact various scientific demonstrate remarkable artificial intelligence processing nlp computer vision CV broader application commerce smart drug discovery emergence versatile model convolutional neural network cnn recurrent neural network rnn memory lstm generative adversarial network gan critical program diverse DL model realize adoption continuous effort academia popular DL framework propose tensorflow pytorch mxnet CNTK simplify implementation various DL model although strength weakness DL framework tradeoff interoperability becomes important reduce redundant engineering effort emerge DL model across exist DL model interoperability onnx propose defines unified format DL model facilitate model conversion DL framework meanwhile unique compute characteristic matrix multiplication spur passion chip architect customize DL accelerator efficiency internet giant google tpu  npu  processor vendor nvidia turing intel nnp service provider amazon  alibaba  startup cambricon  invest tremendous workforce develop DL chip boost performance DL model generally DL hardware category purpose hardware software hardware dedicate hardware fully customize DL model neuromorphic hardware inspire biological brain purpose hardware cpu gpu hardware component avx vector tensor core accelerate DL model whereas dedicate hardware google tpu application specific integrate circuit matrix multiplication bandwidth memory elevate performance efficiency extreme foreseeable future DL hardware become diverse embrace hardware diversity important computation DL hardware efficiently purpose hardware highly optimize linear algebra library linear algebra subprogram blas library mkl cuBLAS efficient computation DL model convolution operation DL framework convert convolution matrix multiplication invoke gemm function blas library addition hardware vendor release specially optimize library tailor DL computation mkl dnn cudnn backward convolution pool normalization activation advanced developed speedup DL operation TensorRT graph optimization layer fusion quantization collection highly optimize gpu kernel dedicate DL hardware library however drawback rely library usually rapid development DL model fail utilize DL chip efficiently address drawback DL library alleviate burden optimize DL model DL hardware manually DL community resort domain specific compiler rescue rapidly popular DL compiler propose tvm tensor comprehension glow ngraph XLA academia DL compiler model definition described DL framework input generate efficient code implementation various DL hardware output transformation model definition specific code implementation highly optimize target model specification hardware architecture specifically incorporate DL orient optimization layer operator fusion enables highly efficient code generation moreover exist DL compiler leverage mature chain purpose compiler llvm portability across diverse hardware architecture traditional compiler DL compiler adopt layer frontend intermediate representation IR backend however uniqueness DL compiler multi irs DL specific optimization comprehensive survey exist DL compiler dissect compiler frontend multi irs backend emphasis IR optimization knowledge comprehensive survey DL compiler specifically contribution dissect commonly adopt architecture exist DL compiler detailed analysis component multi irs frontend optimization node dataflow optimization backend optimization hardware specific optimization auto tune optimize kernel library comprehensive taxonomy exist DL compiler various aspect corresponds component described survey target taxonomy guideline selection DL compiler practitioner requirement thorough summary DL compiler researcher quantitative performance comparison DL compiler cnn model fledge model lightweight model per layer convolution layer dominate inference performance effectiveness optimization evaluation script source reference highlight insight future development DL compiler dynamic pre processing advanced auto tune polyhedral model subgraph partition quantization unified optimization differentiable program privacy protection boost research DL compiler community organize describes architecture DL compiler discus component DL compiler multi irs frontend optimization backend optimization comprehensive taxonomy quantitative performance comparison highlight future direction DL compiler research architecture DL compiler architecture DL compiler primarily contains compiler frontend compiler backend intermediate representation IR across frontend backend generally IR abstraction program program optimization specifically DL model translate multi irs DL compiler IR resides frontend IR resides backend IR compiler frontend responsible hardware independent transformation optimization IR compiler backend responsible hardware specific optimization code generation compilation survey focus principle DL compiler functional experimental comparison DL compiler reader refer overview commonly adopt architecture DL compiler IR graph IR computation hardware independent challenge IR ability abstraction computation capture express diverse DL model goal IR establish dependency operator data interface graph optimization contains semantic information compilation extensibility customize operator detailed discussion IR IR hardware specific optimization code generation diverse hardware target IR grain reflect hardware characteristic hardware specific optimization mature chain compiler backends halide polyhedral model llvm detailed discussion IR frontend DL model exist DL framework input transforms model computation graph representation graph IR frontend implement various format transformation diverse format framework computation graph optimization incorporate optimization technique purpose compiler DL specific optimization reduce redundancy improve efficiency upon graph IR optimization classify node nop elimination zero dim tensor elimination algebraic simplification operator fusion operator  dataflow cse dce static memory planning layout transformation frontend optimize computation graph generate backend detailed discussion frontend backend transforms IR IR performs hardware specific optimization directly transform IR chain llvm IR utilize exist infrastructure purpose optimization code generation advantage prior knowledge DL model hardware characteristic efficient code generation customize compilation commonly apply hardware specific optimization hardware intrinsic mapping memory allocation fetch memory latency hiding parallelization loop orient optimization optimal parameter optimization approach widely adopt exist DL compiler auto schedule polyhedral model auto tune AutoTVM optimize IR compile jit AOT generate code hardware target detailed discussion backend component DL compiler IR overcome limitation IR adopt traditional compiler constrains expression complex computation DL model exist DL compiler leverage IR graph IR efficient code optimization understand graph IR DL compiler representation implementation graph IR representation graph IR representation graph IR influence expressiveness graph IR decides DL compiler analyze graph IR dag IR dag IR traditional compiler computation graph node organize acyclic graph dag DL compiler node dag atomic DL operator convolution pool etc tensor graph acyclic without loop differs data dependence graph DDG generic compiler dag computation graph DL compiler analyze relationship dependency various operator optimization already plenty optimization DDG sub expression elimination cse code elimination dce combine domain knowledge DL algorithm optimization apply dag computation graph elaborate dag IR convenient program compile due simplicity deficiency semantic ambiguity definition computation scope binding IR binding semantic ambiguity offering expression function restrict scope program javascript scheme keyword define expression node generate operator variable expression instead building computational relation variable dag dag compiler return expression access correspond node related node recursive descent technique contrast binding compiler variable expression variable compiler expression DL compiler relay IR tvm adopts dag IR binding IR obtain benefit tensor computation graph irs computation tensor operator diverse DL framework translate graph irs accord specific representation customize operator programmed representation representation tensor computation category function function representation encapsulate operator adopt glow ngraph XLA optimizer HLO IR XLA consists function symbolic program instruction organize  program  function  operation XLA HLO IR graph IR operation IR operation HLO dataflow operator lambda expression lambda expression index formula expression describes calculation variable binding substitution lambda expression programmer define computation quickly without implement function tvm tensor computation tensor expression lambda expression tvm computational operator tensor expression define output tensor lambda expression compute einstein notation einstein notation summation convention notation express summation program simplicity superior lambda expression TC index temporary variable define IR actual expression occurrence undefined variable einstein notation einstein notation operator associative commutative restriction guarantee reduction operator execute parallelization implementation graph IR implementation graph IR DL compiler fulfills management data operation data representation data DL compiler input intermediate data usually organize tensor multi dimensional array DL compiler tensor data directly memory pointer flexible placeholder placeholder contains dimension tensor alternatively dimension tensor marked unknown optimization DL compiler data layout information addition bound iterators infer accord placeholder placeholder placeholder widely symbolic program lisp tensorflow placeholder simply variable explicit information dimension later stage computation allows programmer operation computation graph without concern data computation definition execution DL compiler besides convenient programmer input output correspond intermediate data placeholder without computation definition unknown dynamic representation unknown dimension usually declare placeholder instance tvm unknown dimension tensor XLA none achieve purpose placeholder float none ngraph  unknown representation dynamic model however fully dynamic model bound inference dimension relaxed addition extra mechanism implement guarantee memory validity data layout data layout describes tensor organize memory usually mapping logical index memory index data layout usually sequence dimension   tile pad stride etc tvm glow data layout operator parameter information computation optimization however combine data layout information operator tensor enables intuitive implementation operator reduces compilation overhead XLA data layout constraint related backend hardware relay MLIR data layout information tensor bound inference bound inference apply bound iterators compile DL model DL compiler although tensor representation DL compiler convenient input output expose challenge infer iterator bound bound inference usually perform recursively iteratively accord computation graph placeholder tvm iterators acyclic hyper graph node graph iterator hyper relation split fuse  iterators bound iterator placeholder iterators infer accord relation recursively operator operator DL compiler responsible DL workload node computation graph operator usually algebraic operator exp topk neural network operator convolution pool tensor operator reshape resize broadcast reduction operator min argmin operator conditional loop representative operator frequently across DL compiler illustration addition discus customize operator broadcast broadcast operator replicate data generate data compatible without broadcast operator input tensor constrain operator input tensor compiler XLA relay relax restriction offering broadcasting operator XLA allows wise addition matrix vector replicate matrix complex flexible model model rnn reinforcement RL recurrent relation data dependent conditional execution without graph IR DL compiler model rely host python static unroll deteriorates computation efficiency relay arbitrary implement recursion demonstrate functional program therefore operator recursive function implement contrary XLA HLO operator conditional derivative derivative operator operator output gradient input data input calculates gradient although DL compiler tvm TC automatic differentiation derivative operator IR chain apply tvm towards derivative operator algebraic operator neural network operator programmer derivative operator building derivative customize operator contrary  generate derivative operator automatically customize operator notably DL compiler unable derivative operator fail capability model training customize operator allows programmer define operator purpose customize operator improves extensibility DL compiler define operator glow programmer realize logic node encapsulation addition extra effort lower operation IR generation instruction generation whereas tvm TC program effort computation implementation specifically user tvm computation schedule declare input output tensor moreover customize operator integrate python function hook reduces programmer burden discussion nearly DL compiler unique irs however philosophy dag binding computation graph addition usually convenient programmer tensor computation data operator irs flexible extensible diverse DL model importantly irs hardware independent apply hardware backend IR implementation IR IR describes computation DL model grain representation IR enables target dependent optimization interface tune computation memory access classify implementation irs category halide IR polyhedral IR unique IR halide IR halide propose parallelize image processing proven extensible efficient DL compiler tvm fundamental philosophy halide separation computation schedule specific scheme directly compiler adopt halide various schedule boundary memory reference loop nest halide restrict bound align halide cannot express computation complicate non rectangular fortunately computation DL regular express perfectly halide besides halide easily parameterize boundary expose tune mechanism IR halide modify apply backend DL compiler input halide infinite whereas DL compiler data operator hardware instruction compiler TC fix data ensure temporal locality tensor data tvm improve halide IR independent symbolic IR effort remove dependency llvm  structure project module IR halide pursue organization accessibility graph IR frontend python usability improve runtime dispatch mechanism implement customize operator conveniently tvm simplifies variable definition pointer guarantee variable define location static assignment ssa polyhedral IR polyhedral model important technique adopt DL compiler linear program affine transformation mathematical optimize loop code static bound contrast halide boundary memory reference loop nest polyhedron polyhedral model flexibility polyhedral model widely generic compiler however flexibility prevents integration tune mechanism nevertheless due ability deeply nest loop DL compiler TC  backend ngraph adopt polyhedral model IR polyhedral IR easy apply various polyhedral transformation fusion tile  mapping device dependent device independent optimization toolchains borrow polyhedral compiler isl omega pip  ppl TC unique IR combine halide polyhedral model halide IR computation adopts polyhedral IR loop structure TC detailed expression abstract instance introduces specific node brief TC domain node specify index variable context node iterative variable related hardware node iteration filter node iterator combine statement instance sequence keywords specify execution parallel serial execution filter besides TC extension node instruction code generation memory movement  polyhedral IR stripe tensor operation creates hierarchy parallelizable code extend nest parallel polyhedral multiple besides allows nest polyhedron allocate nest memory computation memory hierarchy stripe hardware configuration independent kernel code tag stripe compiler kernel structure additional information hardware target optimization stripe split DL operator tile local hardware resource unique IR DL compiler implement customize irs without halide polyhedral model upon customize irs apply hardware specific optimization lower llvm IR IR glow instruction expression operates tensor reference address instruction function glow IR declare program declares constant memory throughout lifetime program input bias locally allocate function conv pool temporary variable instruction global memory locally allocate besides operand annotate qualifier indicates operand buffer indicates operand writes buffer  indicates operand writes buffer instruction operand qualifier glow memory optimization perform MLIR highly influence llvm  compiler infrastructure llvm MLIR reuses interface llvm sits model representation code generation MLIR flexible allows multiple abstraction introduces dialect multiple abstraction dialect consists define immutable operation dialect MLIR tensorflow IR XLA HLO IR experimental polyhedral IR llvm IR tensorflow lite flexible transformation dialect furthermore MLIR dialect compiler pave hardware developer compiler researcher HLO IR XLA IR IR HLO grain hardware specific information besides HLO hardware specific optimization emit llvm IR code generation IR IR adopt DL compiler eventually lower llvm IR benefit llvm mature optimizer code generator furthermore llvm explicitly custom instruction specialized accelerator scratch however traditional compiler generate code directly llvm IR avoid situation approach apply DL compiler achieve hardware dependent optimization perform target specific loop transformation upper IR llvm halide IR polyhedral IR additional information hardware target optimization DL compiler apply approach emphasis DL compiler prefer frontend user TC tvm XLA ngraph focus whereas DL compiler inclined backend developer glow  MLIR focus compilation scheme DL compiler mainly classify category jit ahead AOT jit compiler generate executable code optimize code runtime knowledge AOT compiler generate executable binary execute scope static analysis jit compilation addition AOT approach apply compiler embed platform enable execution remote machine tvm rpc customize accelerator discussion DL compiler IR grain representation DL model reflect detailed  DL model diverse hardware irs halide irs polyhedral irs unique irs although leverage mature compiler chain infrastructure tailor interface hardware specific optimization code generation irs impact DL accelerator tvm   XLA HLO tpu frontend optimization construct computation graph frontend applies graph optimization optimization easy identify perform graph graph global computation optimization apply computation graph implementation backends hardware independent apply various backend target frontend optimization usually define apply traverse node computation graph perform graph transformation frontend capture specific feature computation graph rewrite graph optimization besides pre define developer define customize frontend DL compiler input tensor output tensor operation DL model import transform computation graph feature allows DL compiler perform optimization accord information computation graph optimization tensorflow XLA computation graph optimization HLO graph alexnet volta gpu tensorflow XLA classify frontend optimization category node optimization peephole local optimization dataflow global optimization node optimization node computation graph coarse enable optimization inside node node optimization node elimination eliminates unnecessary node node replacement replaces node node purpose compiler nop elimination remove instruction occupy amount specify operation DL compiler nop elimination responsible eliminate operation lack adequate input sum node input tensor eliminate pad node zero pad width eliminate zero dim tensor elimination responsible remove unnecessary operation input zero dimension tensor assume zero dimension tensor constant tensor sum operation node replace already exist constant node without affect correctness assume dimension tensor dimension zero therefore argmin argmax operation node eliminate optimization algebraic simplification algebraic simplification optimization consist algebraic identification strength reduction replace expensive operator cheaper constant fold replace constant expression optimization sequence node advantage commutativity associativity  node simplify computation addition typical operator etc algebraic simplification apply DL specific operator reshape transpose pool operator reorder sometimes eliminate reduces redundancy improves efficiency illustrate algebraic simplification apply optimization computation optimization remove reshape transpose operation accord specific characteristic matrix multiplication gemm matrix matrix transpose BT respectively BT however efficient implement gemm switch argument transpose output gemm reduces transpose optimization node combination optimization combine multiple consecutive transpose node node eliminates identity transpose node optimizes transpose node reshape node actually data optimization  node optimization performs substitution   node glow input reduce operator 4D dimension reduce operator fusion operator fusion indispensable optimization DL compiler enables computation eliminates intermediate allocation facilitates optimization combine loop nest reduces launch synchronization overhead tvm operator classify category injective reduction complex  opaque operator define correspond category target category tvm fusion across operator TC fusion perform differently automatic polyhedron transformation however identify fuse complicate graph multiple broadcast reduce node remains recent tackle propose framework explore optimize aggressive fusion wise reduction node computation memory intensive node complex dependency operator  optimization sink operation transpose operation batch normalization relu sigmoid channel shuffle optimization operation closer opportunity algebraic simplification dataflow optimization sub expression elimination cse expression sub expression previously compute previous computation compute already compute avoid recomputing DL compiler sub expression computation graph replace sub expression previously compute code elimination dce code compute dce optimization remove code code usually programmer graph optimization dce cse apply graph optimization optimization elimination DSE remove tensor belong dce static memory planning static memory planning optimization perform reuse memory buffer usually approach memory standard memory memory memory input output operation allocates memory compute standard memory reuses memory previous operation without overlap static memory planning offline allows complicate planning algorithm apply recent performs memory aware schedule minimize peak activation memory footprint device research direction memory planning memory constrain device layout transformation layout transformation data layout tensor computation graph insert layout transformation node graph actual transformation perform instead perform evaluate computation graph compiler backend performance operation data layout layout hardware operation  format gpu usually faster efficient transform  format gpu tensorflow DL compiler rely hardware specific library achieve performance library layout besides DL accelerator prefer complicate layout tile addition device usually equip heterogenous compute data layout utilization layout transformation careful consideration therefore compiler perform layout transformation across various hardware data layout tensor nontrivial influence performance transformation operation significant overhead consume memory computation resource recent tvm target CPUs alters layout convolution operation  computation graph split sub dimension channel indicates split sub dimension parameter globally explore auto tune hardware detail cache vectorization memory access hardware specific optimization discussion frontend important component DL compiler responsible transformation DL model IR computation graph hardware independent optimization IR although implementation frontend data representation operator definition IR across DL compiler hardware independent optimization converge node dataflow optimization leverage DL specific compilation optimization technique reduce computation redundancy improve performance DL model computation graph backend optimization backends DL compiler commonly various hardware specific optimization auto tune technique optimize kernel library hardware specific optimization enable efficient code generation hardware target whereas auto tune essential compiler backend alleviate manual effort derive optimal parameter configuration besides highly optimize kernel library widely purpose processor customize DL accelerator hardware specific optimization hardware specific optimization target dependent optimization apply obtain performance code target specific hardware apply backend optimization transform IR llvm IR utilize llvm infrastructure generate optimize cpu gpu code customize optimization DL domain knowledge leverage target hardware efficiently hardware specific optimization tailor hardware cannot exhaustively widely adopt approach exist DL compiler overview hardware specific optimization detailed description overview hardware specific optimization apply DL compiler hardware intrinsic mapping hardware intrinsic mapping transform IR instruction kernel already highly optimize hardware tvm hardware intrinsic mapping realize extensible  declare behavior hardware intrinsic lower intrinsic mapping enables compiler backend apply hardware implementation highly optimize handcraft micro kernel specific operation significant performance gain whereas glow hardware intrinsic mapping quantization estimate numeric stage neural network profile optimization perform quantization automatically besides halide tvm specific IR simd opcodes architecture avoid inefficiency llvm IR mapping encounter vector memory allocation fetch memory allocation another challenge code generation gpus customize accelerator gpu contains primarily memory access latency limited memory local memory access latency capacity memory hierarchy efficient memory allocation fetch technique improve data locality realize optimization tvm introduces schedule concept memory scope memory scope schedule primitive tag compute stage thread local compute stage tag tvm generates code memory allocation cooperative data fetch insert memory barrier code guarantee correctness besides TC feature memory promotion extend  compiler however TC limited pre define particularly tvm enables buffering accelerator memory scope schedule primitive memory latency hiding memory latency hiding important technique backend reorder execution pipeline DL compiler parallelization cpu gpu memory latency hiding naturally achieve hardware warp context switch gpu tpu accelerator decouple access execute DAE architecture backend perform schedule grain synchronization obtain efficient code achieve performance reduce program burden tvm introduces virtual thread schedule primitive enables user specify data parallelism virtualized multi thread architecture tvm lower virtually parallelize thread insert memory barrier interleaf operation thread instruction execution pipeline thread hide memory access latency loop orient optimization loop orient optimization apply backend generate efficient code target hardware halide llvm integrate polyhedral already incorporate optimization technique DL compiler leverage halide llvm backends technique apply loop orient optimization loop fusion slide tile loop reorder loop unroll loop fusion loop fusion loop optimization technique fuse loop boundary data reuse compiler  tvm TC XLA optimization perform halide schedule polyhedral approach glow applies loop fusion operator stack slide slide loop optimization technique adopt halide central concept compute data reuse longer slide interleaf computation loop serial tradeoff parallelism data reuse tile tile split loop tile loop outer loop iterate tile inner loop iterate inside tile transformation enables data locality inside tile fitting tile hardware cache tile hardware specific DL compiler tile auto tune loop reorder loop reorder loop permutation iteration nest loop optimize memory access increase spatial locality specific data layout hardware feature however perform loop reorder dependency along iteration loop unroll loop unroll unroll specific loop fix loop allows compiler apply aggressive instruction parallelism usually loop unroll apply combination loop split split loop nest loop  inner loop completely parallelization processor generally multi thread simd parallelism compiler backend exploit parallelism maximize hardware utilization performance halide schedule primitive parallel specify parallelize dimension loop thread parallelization gpu parallelization mapping loop dimension tag parallel annotation thread replaces loop vector statement mapped hardware specific simd opcodes hardware intrinsic mapping stripe develops variant polyhedral model nest polyhedral model introduces parallel polyhedral execution iteration extension nest polyhedral model detect hierarchy parallelization tile stride addition DL compiler rely handcraft library glow optimize math library hardware vendor meanwhile glow offloads vectorization llvm llvm auto vectorizer information tensor dimension loop however exploit parallelism entirely compiler backend allows apply domain specific knowledge DL model performance expense engineering effort auto tune due enormous parameter tune hardware specific optimization leverage auto tune optimal parameter configuration DL compiler survey tvm TC XLA auto tune generally auto tune implementation component parameterization model technique acceleration parameterization data target data parameter describes specification data input target parameter describes hardware specific characteristic constraint optimization schedule code generation gpu target hardware parameter memory register specify optimization option optimization option optimization schedule correspond parameter loop orient optimization tile tvm pre define user define schedule parameter consideration whereas TC XLA prefer parameterize optimization correlation performance later minibatch dimension parameter usually mapped grid dimension cuda optimize auto tune model comparison model apply auto tune model model considers execution characteristic compilation task easy model easily overhead optimal without guidance task characteristic TC adopts model ML model ML model statistical approach predict performance machine enables model update configuration explore achieve prediction accuracy tvm XLA adopt model gradient boost model GBDT feedforward neural network fnn respectively pre define model approach pre define model perfect model built characteristic compilation task evaluate overall performance task ML model pre define model generates computation overhead apply engineering effort building model DL model hardware technique initialization determination initial option randomly configuration configuration user historical optimal configuration specify auto tune tvm allows developer specify domain specific knowledge automatic extraction hardware target computational description contrast TC relies compilation cache pre define genetic algorithm GA GA considers tune parameter gene configuration candidate candidate iteratively generate crossover mutation selection accord fitness metaheuristic inspire selection finally optimal candidate derive rate crossover mutation selection tradeoff exploration exploitation TC adopts GA auto tune technique simulated anneal algorithm SA SA metaheuristic inspire anneal allows accept decrease probability approximate global optimum avoid precise local optimum fix amount iteration tvm adopts SA auto tune technique reinforcement RL RL performs maximize reward environment tradeoff exploration exploitation chameleon built upon tvm adopts  auto tune technique acceleration parallelization direction accelerate auto tune parallelization TC proposes multi thread multi gpu strategy genetic algorithm evaluate candidate generation enqueues candidate configuration compiles multiple cpu thread generate code evaluate gpus parallel candidate fitness evaluation candidate generate compilation enqueued compile cpu similarly tvm compilation rpc user compile local machine program auto tune configuration multiple target configuration reuse another direction accelerate auto tune reuse previous auto tune configuration TC generate code version correspond configuration compilation cache cache query kernel optimization compilation auto tune trigger cache similarly tvm file optimal configuration schedule operator query file configuration compilation worth mention tvm performs auto tune operator halide IR convd optimal configuration operator separately optimize kernel library highly optimize kernel library widely accelerate DL training inference various hardware  previously mkl dnn intel cudnn nvidia  amd widely library computation intensive primitive convolution gemm rnn memory bandwidth limited primitive batch normalization pool shuffle highly optimize accord hardware feature avx ISA tensor core customizable data layout easy integrate DL application avoid frequent data layout transformation besides precision training inference FP FP int non float format bfloat customize DL accelerator maintain specific kernel library exist DL compiler tvm ngraph TC generate function library code generation however DL compiler leverage exist optimize kernel library transform data layout fusion style pre define kernel library transformation optimal moreover DL compiler treat kernel library therefore unable apply optimization across operator operator fusion invoke kernel library sum optimize kernel library achieves significant performance improvement computation satisfied specific highly optimize primitive otherwise constrain optimization suffer optimal performance discussion backend responsible bare optimization code generation IR although backends due various irs optimization classify hardware specific optimization auto tune technique optimize kernel library optimization perform separately combine achieve data locality parallelization exploit hardware software characteristic eventually IR DL model transform efficient code implementation hardware taxonomy DL compiler DL compiler survey tvm ngraph tensor comprehension TC glow XLA compiler maintain importantly widely document discussion academia implementation depth illustrates taxonomy DL compiler perspective frontend backend IR optimization corresponds component described survey comparison DL compiler tvm ngraph TC glow XLA specifically information compiler knowledge compiler specific feature feature program interface addition develop status specific feature specific feature compiler target taxonomy guideline selection DL compiler practitioner requirement thorough summary DL compiler researcher feature DL compiler developer program onnx framework training quantization frontend category compilation device backend category feature summarize strongly affect usage DL compiler scenario feature practitioner researcher easily DL compiler upon systematic summary survey reader identify feature compiler component compiler detailed information evaluation experimental setup conduct gpu equip machine hardware configuration evaluate performance tvm ngraph TC commit glow commit XLA tensorflow cpu gpu neural network model onnx format datasets convert  model zoo  model zoo model fledge model resnet densenet vgg series lightweight model mobilenet  series import onnx model built tvm relay frontend onnx interface tvm ngraph onnx python package ngraph built  glow tensorflow onnx python package XLA notably TC lack onnx evaluate per layer performance comparison model execute report average execution execution compiler regard execution eliminate overhead jit compilation hardware configuration hardware configuration performance comparison performance inference across tvm ngraph glow XLA evaluate compiler CPUs broadwell skylake gpus omit comparison TC TC kernel library fully functional DL compiler user implement layer model einstein notion manually engineering effort comparison another TC gpu cannot obtain performance cpu however detailed comparison implement resnet MobileNetV model TC sum analyze performance perspective performance comparison inference across tvm ngraph glow XLA cpu gpu performance comparison convolution layer MobileNetV across tvm TC glow XLA gpu performance comparison convolution layer MobileNetV across tvm ngraph glow broadwell cpu performance comparison convolution layer resnet across tvm TC glow gpu compatibility although ngraph XLA onnx compatibility ngraph fails densenet vgg  model due tensor dynamic alternatively replace densenet vgg model correspond model onnx model zoo  model available besides  backend ngraph gpu fail mobilenet model  cannot handle inconsistent definition operator across DL framework XLA model however performance replace onnx model  tensorflow hub  model available model tensorflow hub XLA becomes magnitude faster performance XLA becomes competitive compiler performance observation performance illustrate cpu performance glow compiler glow thread parallelism cannot fully utilize multi core cpu whereas tvm ngraph XLA leverage cpu core XLA inference performance fledge model resnet densenet vgg series lightweight model mobilenet  series besides inference performance cpu gpu almost XLA embed tensorflow framework tensorflow contains complicate runtime tvm ngraph glow introduces non trivial overhead XLA addition increase batch default evaluation focus throughput DL compiler overhead XLA ignore throughput cpu tvm ngraph achieve performance across model DL compiler due limitation glow XLA described tvm comparable performance ngraph fledge model ngraph lightweight model ngraph relies  previously mkl dnn library acceleration ngraph offload optimize subgraphs  benefit  grain instruction jit optimization tailor intel cpu tune tvm tune trial almost achieves performance cpu gpu across model lightweight model mobilenet  series investigation schedule classic operator inside model already tvm developer default parameter tvm  default schedule parameter tvm achieve performance DL compiler addition performance difference tune tvm  tvm negligible cpu significant gpu speedup average gpu complicate thread memory hierarchy cpu exploit computation gpu grain schedule tile split reorder tvm therefore crucial optimal schedule parameter gpu autotuning exhibit effectiveness per layer performance comparison capability backend optimization DL compiler evaluate per layer convolution layer dominate inference performance resnet MobileNetV gpu broadwell cpu thread glow lack multi thread methodology execution individual layer adopt DL compiler hardware cpu gpu cnn model specifically tvm autotuning extract kernel optimal schedule rebuild individual convolution layer evaluator evaluation extract execution trace file glow execution kernel TC ngraph timeline execution cpu however timeline  backend gpu OpenCL besides available profile command queue within OpenCL therefore profile per layer performance ngraph gpu future XLA leverage built profiler experimental cpu performance  toolkit nvidia gpu performance performance observation performance illustrate performance comparison convolution layer resnet across tvm ngraph glow broadwell cpu ngraph achieves performance convolution layer cpu benefit hardware intel cpu software compiler library runtime whereas tvm performs gpu across compiler MobileNetV performance tvm stable conv layer autotuning affected machine tends derive imprecise negative schedule parameter TC allows user define tensor computation kernel convolution einstein notion without specify input output tensor kernel kernel  compilation cache accelerate autotuning compilation however evaluation performance TC heavily relies compile kernel MobileNetV initialize autotuning layer perform layer become layer deeper away layer derive consistent performance tune kernel separately glow compiler optimize convolution linear layer MobileNetV depth wise separable convolution layer resnet longer compute convolution gpu cpu convolution usually fuse layer relu batchnorm glow performance compiler moreover cpu convolution MobileNetV shorter convolution accord trace convolution accelerate  optimization applies tile layout transformation vectorization convolution specific XLA automatically compile  eligible subgraphs tensorflow replace subgraphs resultant binary  addition convolution layer cluster kernel performance easy individually therefore cluster non cluster convolution data MobileNetV model tensorflow onnx model layer however  layer moreover convolution cluster twice till  therefore extreme correspond cluster convolution MobileNetV actually cluster kernel optimize XLA non cluster optimize tensorflow therefore impossible execution standalone convolution layer optimize XLA consequently performance XLA cluster non cluster convolution XLA gpu broadwell cpu cluster non cluster convolution XLA gpu broadwell cpu discussion quantitative performance comparison across DL compiler depth analyze coarse grain performance frontend graph backend operator optimization grain per layer performance convolution backend optimization however challenge accurately effectiveness optimization adopt DL compiler difficulty evaluation frontend backend optimization usually tightly couple exist DL compiler frontend optimization usually affect series operator optimize operator input backend optimization across compiler optimization tend exploit performance opportunity cluster XLA advanced optimization therefore impossible evaluate specific optimization across DL compiler individually tackle building universal benchmarking framework exist DL compiler per layer performance fundamental extract structure parameter target layer model fragment rebuild layer acceptable input DL compiler allows compiler apply correspond frontend backend optimization faithfully performance optimize model fragment understand effectiveness DL compiler layer benchmarking framework model fragment scalable customize layer fuse layer benchmarking framework available derive coarse grain grain per layer performance metric DL compiler effectiveness optimization across DL compiler currently successfully extract target layer cnn model bottleneck resnet  MobileNetV benchmarking framework rapid development available community conclusion future direction survey thorough analysis exist DL compiler target principle dive architecture adopt exist DL compiler multi IR frontend backend philosophy reference implementation component detail emphasis unique irs optimization specific DL compiler comprehensive taxonomy quantitative performance comparison DL compiler summarize finding survey highlight future direction DL compiler dynamic pre processing dynamic model becomes popular DL input model execution particularly nlp model accept input various challenge DL compiler data unknown runtime exist DL compiler research effort dynamic efficiently emerge dynamic model addition future DL model become complex entire inevitably complicate pre processing procedure currently DL compiler python program pre processing become performance bottleneck execute python interpreter potential performance bottleneck exist DL compiler entire DL compiler enables express optimize pre processing along DL model opportunity performance acceleration model deployment advanced auto tune exist auto tune technique focus optimization individual operator however combination local optimal global optimal adjacent operator apply data layout tune without introduce extra memory transformation besides compute execution optimization objective DL compiler optimization target auto tune memory footprint consumption particularly ML auto tune technique direction worth explore ML technique apply stage auto tune model stage compiler option optimization schedule ML technique predict possibility directly develop algorithm configuration ML auto tune technique improve domain knowledge incorporate feature engineering feature program auto tune technique potential direction achieve tune polyhedral model promising research direction combine polyhedral model auto tune technique DL compiler efficiency auto tune apply minimize overhead polyhedral jit compilation reuse previous configuration polyhedral model perform auto schedule reduce auto tune another challenge apply polyhedral model DL compiler sparse tensor format sparse tensor CSF express loop index index array longer linear indirect index address non affine subscript expression loop bound prohibits loop optimization polyhedral model fortunately polyhedral community progress sparse tensor integrate advancement polyhedral model increase performance opportunity DL compiler subgraph partition DL compiler subgraph partition computation graph subgraphs subgraphs manner subgraph partition research opportunity DL compiler possibility integrate graph library optimization ngraph   DL library graph optimization leverage vast collection highly optimize kernel integration  ngraph enables  speedup execution subgraphs generate ngraph possibility heterogeneous parallel execution computation graph partition subgraphs execution subgraphs assign heterogeneous hardware target device computation consist cpu mail gpu dsp probably npu generate subgraphs DL compiler utilizes computation efficiently deliver significant speedup DL task quantization traditional quantization strategy apply DL framework fix scheme datatypes customization code hardware whereas quantization DL compiler leverage optimization opportunity compilation derive efficient quantization strategy relay quantization rewrite automatically generate quantize code various scheme quantization challenge DL compiler challenge implement quantize operator without engineering effort attempt aws direction concept dialect implement operator upon operator optimization graph operator reuse challenge interaction quantization optimization compilation appropriate stage quantization collaborate optimization operator fusion future research investigation unified optimization although exist DL compiler adopt computation graph optimization hardware specific optimization compiler advantage aspect optimization emerge hardware target across exist compiler advocate unify optimization exist DL compiler adopt DL compiler reuse addition unify optimization across DL compiler accumulate impact purpose dedicate DL accelerator environment efficient DL compiler hardware currently google MLIR promising initiative towards direction infrastructure multi irs contains IR specification toolkit perform transformation across irs flexible dialect DL compiler construct customize dialect irs transformation across dialect optimization DL compiler reuse another compiler however transformation dialect research effort reduce dependency delicate differentiable program differentiable program program paradigm program differentiable thoroughly algorithm differentiable program paradigm automatically differentiate attractive DL community compiler project adopt differentiable program  flux julia unfortunately differential program exist DL compiler differential program challenge exist DL compiler difficulty data structure semantic realize transformation julia XLA HLO IR challenge imperative julia symbolic XLA HLO IR efficiently compiler operation abstraction julia semantic XLA mapreduce broadcast moreover semantic difference differentiation julia XLA significant compiler privacy protection DL model usually split partial model device service respectively response latency consume communication bandwidth however drawback user privacy becomes vulnerable attacker intercept intermediate device intermediate another model reveal privacy information deviate user task privacy exist approach propose statistic intermediate reduce accuracy attacker task without severely deteriorate accuracy user task however difficulty layer insert labor intensive identify optimal layer difficulty opportunity DL compiler privacy protection compiler maintain information DL model insertion across layer automatically training model training DL compiler ngraph training intel nnp accelerator TC auto differentiation kernel glow experimental training limited model training tvm development XLA relies training tensorflow sum DL compiler mainly focus bridging gap deploy DL model onto diverse hardware efficiently inference primary optimization target however expand capability DL compiler model training research opportunity optimization gradient operator auto differentiation