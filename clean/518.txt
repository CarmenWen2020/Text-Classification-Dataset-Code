mobile compute MEC promising resource constrain device offload task server however traditional approach linear program theory computation offload mainly focus immediate performance potentially performance degradation recent breakthrough regard reinforcement DRL alternative focus maximize cumulative reward nonetheless exists gap deploy DRL application MEC training perform DRL agent typically data quantity diversity DRL training usually accompany trial error address mismatch application DRL multi user computation offload practical perspective propose distribute collective DRL algorithm DC DRL improvement distribute collective training scheme  knowledge multiple MEC environment greatly increase data amount diversity exploration update adaptive improve training efficiency without suffer variance distribute training combine advantage neuroevolution policy gradient maximize utilization multiple environment prevent premature convergence lastly evaluation demonstrate effectiveness propose algorithm baseline exploration reduce percent respectively introduction recent proliferation smart mobile device promote popularity multitude mobile application however mobile device limited computation capacity directly affect computation performance user driven mobile compute MEC emerge promising integrates mobile device computational resource proximity thereby alleviate challenge computation offload nonetheless catering user request performance demand non trivial task virtue MEC significant challenge allocation computational network resource although previous computation offload linear program theory obtain achievement practical mostly shot optimization model assumption adequate characterize scenario fortunately recent breakthrough reinforcement RL promising approach optimal policy maximize reward without prior knowledge model RL agent optimize policy historical interaction environment besides reinforcement DRL leverage powerful neural network dnns enable RL handle effectiveness however impractical apply exist DRL algorithm directly MEC DRL algorithmic paradigm propose previous rely numerous interaction training environment obtain quantity diversity DRL training data supervise obtain optimal policy algorithm scenario video neural architecture environment memory program DRL training obtain easily increase compute resource due impractical unbiased simulation model inevitable DRL agent environment training generate nearly unbounded algorithm mismatch uptake DRL application MEC DRL training usually accompany trial error exploration computation offload MEC trial error associate actual user numerical scenario video therefore another difficulty training performance DRL agent unaffordable MEC environment knowledge challenge attention previous address challenge trial error multiple MEC environment thereby achieve diversity exploration explore distribute collective training empower DRL practical perspective distribute training multiple MEC environment collectively DRL agent update network parameter however unique challenge policy gap distribute training important coordinate multiple DRL agent deployed distribute maximize utilization environment important assimilate knowledge multiple environment DRL agent develop collective comprehensive performance policy therefore propose distribute collective DRL algorithm DC DRL adaptively offload channel allocation decision without awareness model contrast previous DC DRL algorithm empower practical perspective leverage novel training scheme multiple MEC environment distribute collectively convergence DRL agent significantly accelerate exploration environment greatly reduce addition enhance distribute collective training scheme DC DRL algorithm maximize benefit multiple MEC environment DC DRL algorithm address challenge limited computation offload scenario apply concerned training intelligent agent performance maximization contribution summarize computation offload resource intensive deadline sensitive application MEC user request performance formulate optimization markov decision MDP advantage recent advance DRL propose DC DRL algorithm practical perspective assimilate knowledge distribute obtain collective DRL agent performance exploration remove impact policy gap distribute training policy correction propose update namely adaptive combine advantage neuroevolution policy gradient greatly improves utilization distribute convergence remainder structure briefly related introduce model formulate MDP introduce DC DRL algorithm numerical simulation lastly conclude related computation offload MEC MEC widely recognize technology improve application performance extensive effort optimize computation offload performance MEC approach mathematical program genetic algorithm lagrangian relaxation propose opportunistic computation offload algorithm minimize communication overhead maximize efficiency response prediction framework propose computation application propose distribute resource allocation algorithm significantly improve resource utilization computation load insufficient resource however exist research approach shot optimization fail achieve performance dynamically environment dynamic important role performance user request network status guarantee shot optimization algorithm performance DRL computation offload fortunately recent breakthrough DRL alternative approach address shot optimization propose autonomic algorithm minimize network delay consumption distribute fog network propose knowledge driven framework obtain optimal policy multi compute node developed effective algorithm DRL achieve load balance minimize execution service rate task arrival rate propose multi agent DRL cache strategy achieve efficient content cache mobile device device network introduce upper confidence bound algorithm convergence action however advantage DRL computation offload optimization video account difference limited actual trial error computation offload regard vast amount agent training significant trial error DRL deployment MEC computation offload distribute DRL training unrealistic directly apply DRL algorithm computation offload realistic consideration future multiple independent computation offload deployed collectively DRL agent within domain distribute DRL training regard research topic DRL attempt empower distribute training across machine  propose asynchronous advantage actor critic AC algorithm distribute distributional deterministic policy gradient dpg algorithm respectively distribute version policy gradient DRL algorithm previous algorithm DRL training multiple machine gpus CPUs maximize machine utilization scenario instance environment directly memory video AI neural architecture scenario computation offload limitation instance parameter update AC accumulate gradient distribute DRL agent imposes communication overhead computation offload dpg relies frequent network replication relatively trajectory avoid data bias impractical computation offload scenario mismatch intention traditional distribute DRL algorithm requirement computation offload scenario knowledge attempt apply DRL practical robust computation offload model introduce model illustrate computation offload model MEC consists server ES mobile user MUs ES MUs distribute within belong community organization smart smart factory MUs wireless channel MUs transmit data task execution MUs ES amount compute capability simulate dynamically environment scenario adopt uniform task generation model characterize user request model task characterize data program code input data cpu cycle task task deadline addition due return data generally input data recognition assume return data ignore mainly focus challenge apply DRL computation offload therefore simplicity model assumption MUs offload application decompose application independent sub task illustration multi user computation offload channel allocation model MEC without loss generality MU limited computation capacity ES computational resource request MUs therefore reduce execution consumption task offload ES offload task MU MU upload data allocate wireless channel ES consume computational resource offload task compute ES offload task computational resource methodology computation offload optimization assume task schedule fifo principle context task execution ES model queue model task task queue computational resource occupy task similarly MU maintains task queue local processing task tractable analysis regard computation offload quasi static MU exit MEC abruptly within computation offload typically timescale millisecond assume simplicity easily extend scenario diverse model MUs offload task network resource computational resource ES consume guarantee subsequent task deadline seriously deteriorates performance MEC therefore critical algorithm optimize allocation network computational resource perspective formulation decision description mention model computation offload quasi static decision epoch offload decision consists brief information generate task MUs ES wireless channel generate task MU denote program code input data cpu cycle task deadline task assume task task queue computational resource occupy task denote queue delay MU queue delay ES addition computation capacity MU ES denote respectively wireless connection denote channel gain background MU respectively context define SourceHere collection task information collection MUs coordinate MU denotes wireless channel profile obtain joint action multiple MUs mention jointly allocation network computational resource action MU offload decision channel specifically MU chooses local processing likewise MU chooses offload task wireless channel therefore joint action multiple MUs define sourcein discus action notation summarize local processing local processing generate task MU compute local device consumption task depends computation capacity local device cpu cycle queue delay task queue neglect task initialization typically accordingly consumption express tli cifi sourcein addition consumption   SourceHere average per cpu cycle MU accord characteristic compute device apply practical measurement price model task deadline otherwise penalty pre define typically average task therefore consumption MU local processing express cli  tli  sourcewhere task deadline fail penalty compute analyze compute execute task ES MU transmit input data program code ES proximity return data generally input data assume transmission return data ignore consequently consumption involves data upload execution data upload data transmission rate model multiple MUs network resource achieve efficient utilization accurately capture average aggregate throughput wireless link wireless interference model channel access mechanism code multiple access CDMA mathematically uplink data rate MU calculate     sourcewhere channel bandwidth transmission MU background  channel gain MU loss exponent distance MU addition contains satisfy consumption MU upload data tri  sourcewhere program code input data neglect delay ES typically via fibre link consumption negligible obtain data MUs ES performs task relatively powerful computation capacity local processing neglect task initialization assume incoming task schedule fifo principle local processing consumption MU task express  cife sourcewhere cpu cycle computation capacity queue delay ES respectively equation consumption sum transmission computation calculate  tri  source compute payment ES consumption data transmission    sourcewhere price per cpu cycle ES accord computation capacity ES addition transmission price lastly local processing task offload ES deadline MU define compute     SourceRight click MathML additional feature MDP multi user computation offload accord model action MUs couple mutually affected instance MUs compute upload data wireless channel simultaneously severe interference incur data transmission longer task task failure meanwhile intense competition computational resource ES rapid resource consumption queue task failure insight important decision multiple MUs simultaneously mention earlier scenario MUs belong community organization therefore formulate multi user computation offload joint optimization define sum MUs  uci cli  SourceRight click MathML additional feature previous commit minimize MEC adapt dynamic environment achieve performance tractable analysis discretize horizon multiple decision epoch denote sequence discrete decision epoch decision epoch correspond offload decision performance action immediate  addition action context define tuple formulate multi user optimization MDP widely address sequential stochastic decision action respectively transition probability mapping action transition probability addition rmin rmax immediate reward function  model lastly discount factor determines future reward MDP model objective optimal policy maximizes discount reward minimizes discount infinite finite horizon immediate reward optimal policy profitable reward consideration automatically MDP model accordingly define optimal policy  TR source performance optimization formulate MDP optimal policy obtain iteration however multi user MEC action exponentially MUs increase infeasible optimization polynomial proven multi user computation offload involve offload decision channel allocation NP addition complicate action future reward  transition probability algorithm tackle issue leverage driven DRL propose DC DRL algorithm actor critic framework practical perspective multiple MEC environment obtain knowledge beyond individual exploration policy gap distribute training propose adaptive improve training efficiency without affect convergence stability addition combine advantage neuroevolution policy gradient maximize utilization multiple MEC environment actor critic framework develop DC DRL algorithm actor critic framework fundamental DRL algorithm deterministic policy gradient DDPG proximal policy optimization ppo introduce DC DRL agent deployed independent MEC environment DC DRL exploit actor critic framework computation offload illustrate network architecture actor critic framework consists actor network critic network evaluation actor network predict optimal action decision epoch propose multi user MEC model contains profile ES MUs wireless channel generate task accord policy actor network output action consists computation offload channel allocation decision request MUs addition explore optimal strategy derive   action output stationary gauss markov temporally homogeneous noisy action obtain apply MEC environment actor critic framework DC DRL algorithm independent MEC environment schematic distribute collective training scheme DC DRL algorithm scheme training distribute DRL population decouple training DRL agent multiple MEC environment DRL agent concurrently promote maximum utilization MEC environment noisy action actor critic agent obtains immediate reward immediate reward negative namely  reflect performance action interaction tuple replay memory parameter update organize replay memory fifo underlie facilitate implementation adaptive analyze detail algorithm summarizes interaction independent environment DRL agent parameter actor network critic network update periodic training training iteration randomly sample replay memory update network parameter action critic network attempt correspond reward policy refer quality hence express  sourcewhere trajectory policy however trajectory generally unreachable beforehand MEC consequence DRL algorithm temporal difference TD approximate reward equation apply bellman operator equation split mathcal theta gamma cdot mathbb phi theta split tag equation SourceHere immediate reward action equation phi denotes output actor network input phi denotes parameter actor network addition cdot theta denotes estimate reward action cdot critic network theta denotes network parameter error mse loss function loss function update critic network equation theta mathbb theta mathcal theta tag equation source actor network accord deterministic policy gradient theorem parameter update policy gradient backpropagation function critic network apply chain reward update parameter actor network equation equation split nabla phi approx mathbb nabla phi theta phi mathbb nabla theta phi cdot nabla phi phi split tag equation SourceRight click MathML additional feature nabla phi nabla denote gradient vector respect phi respectively entire training critic network constantly improves estimation action minimize equation actor network constantly improves strategy gradient critic network nabla phi periodically update network parameter actor network eventually converges optimal policy addition significant improvement integrate actor critic framework dnn structure actor network critic network manifest instability non linear function approximator avoid oscillation dnn stabilize convergence adopt additional dnns actor network critic network structure network relatively stable reference clarity refer network online network additional network target network reward equation rewrite equation split mathcal theta gamma cdot mathbb phi prime theta prime split tag equation sourcewhere phi prime theta prime denote parameter target actor network target critic network application target network ensures action fairly evaluate stabilizes convergence achieve parameter target network constrain fluctuate slightly update online network slowly online network equation equation theta prime leftarrow tau theta tau theta prime tau tag equation source equation phi prime leftarrow tau phi tau phi prime tau tag equation source improvement replay  technique policy DRL algorithm decision epoch interaction fifo replay memory training iteration DRL agent update parameter randomly mini batch sample advantage replay correlation trajectory MDP accurate gradient estimation data replay memory reuse greatly improve sample efficiency empower DRL agent capacity critical apply DRL algorithm generally limited computation offload optimization algorithm interaction local environment dot infty DRL agent input output noisy action environment executes action immediate reward replay memory fifo principle distribute collective actor critic DRL algorithm promising prospect performance optimization suffers exploration mdps dimensional action therefore DC DRL considers leverage multiple MEC environment obtain knowledge beyond individual across multiple independent environment characteristic DRL algorithm MEC summarize requirement distribute collective scheme fulfil communication efficient affect operation MEC robust non independent identically distribute non iid data distribution highly scalable MEC environment collective training broader realm distribute DRL array training scheme propose recent category gradient parameter DRL agent data replay memory analyze characteristic training scheme demonstrate respective feasibility communication overhead hierarchical architecture additional communication overhead arises agent parameter server access distribute agent exploit knowledge amount communication overhead frequency data transmit parameter scheme training local datasets distribute agent upload parameter dnns megabyte gigabyte parameter server model aggregation parameter communication efficient training scheme model aggregation proceed relatively frequency gradient scheme despite amount data per transmission brings communication overhead transmission interval typically millisecond AC algorithm highly frequent data transmission constraint gradient scheme scenario instance environment directly memory video scheme upload tuple parameter server typically generate MEC typically transmit program code input data queue task execution addition scheme considerably improves sample efficiency replay therefore scheme applicable deployment DRL agent without affect operation MEC algorithm DRL training replay memory capacity batch actor network rate alpha critic network rate alpha update parameter tau upload interval text upload local training upsilon agent initialize online actor network online critic network random network parameter phi theta initialize target actor network target critic network network parameter phi prime leftarrow phi theta prime leftarrow theta dot infty apply additive gaussian mutation strength network parameter dot upsilon randomly transition max compute adaptive return equation compute loss gradient vector equation update network parameter online actor network online critic network compute loss gradient vector update parameter target network equation mod text upload upload batch agent calculate text avg equation obtain text avg agent text avg replace network parameter robustness non iid data MEC training data distribute DRL agent derive interaction local environment distribution local datasets strongly distribute DRL agent concretely local datasets typically characteristic MU device behavior consequence local dataset distribute environment data distribution gradient scheme posse robustness non iid data former approximate training iid data highly frequent data transmission latter frame dataset data distribution however parameter scheme sensitive non iid data conduct training model locally upload model parameter server due skewness data distribution divergence inevitably constrain scenario iid iid data scalability envision future massive MEC environment participate training DRL agent challenge achieve vision scalability parameter scheme training individual computation capacity data generation rate difference progress parameter resource constrain training individual outdated thereby aggregate parameter instead degrade performance synchronization distribute DRL agent obstacle parameter scheme gradient scheme  handle communication overhead massive training participant individual lack communication resource potential bottleneck maximize utilization distribute environment fortunately scheme training agent aggregate parameter distribute DRL agent outdated issue parameter scheme addition training collection easily implement asynchronously eliminate potential communication bottleneck gradient scheme accord analysis MEC scheme superior distribute scheme communication efficient robust non iid data highly scalable training participant comparison distribute training scheme DC DRL algorithm scheme implementation DC DRL algorithm combine parameter scheme enhance DC DRL algorithm explain detail notation definition comparison distribute training scheme adaptive subsequently introduce adaptive modification reward estimate DRL training equation standard actor critic framework adopts TD approximate reward however inevitably bias efficiency stage training phi prime theta prime equation imprecise estimate future reward  obtain trajectory MEC environment reduce bias leverage information multiple compromise motivate TD obtain estimate reward contrast equation parameter update return update target equation split mathcal theta gamma gamma ldots gamma gamma cdot mathbb phi prime theta prime split tag equation SourceHere ldots truncate trajectory replay memory preset however empirical TD exhibit limitation policy DRL algorithm distribute setting truncate trajectory previous policy greatly deviate policy update directly introduce TD distribute setting greatly suffers variance therefore critical policy gap update parameter distribute DRL agent address challenge policy correction propose dynamically adjust policy deviation namely adaptive initialization maximum max training iteration trajectory max update parameter assume ldots max max max trajectory policy correction straightforward equation sum max prod mathbf lbrace phi prime rbrace tag equation sourcewhere mathbf lbrace phi prime rbrace indicator function phi prime satisfies DC DRL algorithm update parameter trajectory improves sample efficiency without introduce variance theoretical analysis policy improvement adaptive theorem adaptive error reward estimate error estimate align max bigg mathbb prime bigg leq max bigg mathbb bigg align sourcewhere prime unbiased estimate reward action respectively proof accord definition equation sum infty gamma phi prime equation sourcewhere cdot phi prime denotes policy update cdot cdot denotes immediate reward function intuitively cumulative discount reward  trajectory reward estimate action equation prime gamma equation sourcefor simplicity define equation gamma gamma dot gamma equation source adaptive equation truncate trajectory  context equation align phi prime phi prime  phi prime align SourceRight click MathML additional feature therefore proceed extend prime equation split max bigg mathbb prime bigg max bigg mathbb gamma bigg max bigg mathbb gamma gamma sum infty gamma phi prime bigg max bigg mathbb qquad qquad qquad quad gamma sum infty gamma phi prime bigg gamma max bigg mathbb bigg split equation source intuitively theorem involves action satisfies policy action maximizes function mathbb cdot cdot domain function mathbb cdot cdot subspace mathbb mathbb easy derive equation split gamma max bigg mathbb bigg leq gamma max bigg mathbb bigg split equation source gamma discount factor satisfies gamma inequality theorem completes proof theorem underpins policy improvement propose algorithm adaptive update parameter towards estimate concern reward neuroevolution policy gradient motivation although gradient descent effective parameter optimization algorithm achieve performance apply DRL algorithm characteristic DRL training concretely DRL agent update parameter actor network critic network calculate equation respectively however mention recursive approximate reward equation actual gradient DRL training untraceable without trajectory compromise apply surrogate gradient equation contrast algorithm gradient easily obtain label data due inaccuracy surrogate gradient DRL traditional gradient optimization exhibit brittle convergence divergence addition traditional gradient DRL algorithm notorious necessitate meticulous hyper parameter tune DRL agent optimal parameter trial error typically consume expensive fashion situation dimensional parameter traditional trial error insufficient compensate convergence characteristic gradient algorithm liable local optimum traditional distribute DRL algorithm tend focus multiple environment increase training fail fully exploit advantage diversity DRL agent distribute multiple environment apply setting parameter exploration strategy contribute radically exploration trajectory explore pioneer researcher OpenAI explore application evolutionary strategy parameter optimization propose neuroevolution competitive alternative policy gradient instead perturb action output dnns neuroevolution algorithm directly network parameter calculates randomize finite difference thereby address convergence issue gradient however extensive simply apply neuroevolution computationally expensive instance cpu core neuroevolution compress training 3D humanoid task gradient DRL algorithm cpu core approximate achieve strategy slightly performance neuroevolution randomness overcomes limitation gradient descent algorithm inevitably inefficiency insight propose combine advantage neuroevolution policy gradient combine neuroevolution policy gradient DC DRL algorithm maintains population distribute MEC environment DRL agent convenience define lbrace theta theta prime phi phi prime rbrace denotes network parameter DRL agent apply neuroevolution entire training multiple generation accordingly denote parameter DRL population agent gth generation mathbb lbrace ldots ldots rbrace respectively superscript agent distinguish individual namely DRL agent population population agent mathbb randomly initialize initialization sake clarity assume population remains unchanged nevertheless DC DRL algorithm directly apply dynamic generation DRL population agent sample multivariate gaussian distribution namely sim mathcal sigma mathbf sigma mutation strength reformulate sample apply additive gaussian parameter equation prime sigma varepsilon lbrace ldots rbrace tag equation sourcewhere prime indicates temporary parameter varepsilon gaussian distribution mathcal mathbf mathbf DRL population agent update parameter interaction local environment policy gradient assume upsilon training cumulative gradient prime psi parameter equation split prime prime prime alpha psi sigma varepsilon alpha psi split tag equation sourcewhere lbrace ldots rbrace alpha local rate agent neuroevolution evaluate fitness candidate lbrace prime prime lbrace ldots rbrace rbrace focus DRL fitness prime prime average reward policy prime prime calculate previous training agent update parameter generation lbrace prime prime mathbb rbrace denote fitness fitness function parameter mathbb index context agent formula update distribution generation parameter equation sum mathbb eta prime prime cdot prime prime tag equation SourceHere eta prime prime parameter satisfies sum mathbb eta prime prime eta prime prime  prime prime DRL population emphasis parameter performance individual fitness text avg population discard replace parameter equation text avg sum frac prime prime tag equation  pseudocode DC DRL training procedure DC DRL applies additive gaussian network parameter possibility DC DRL performs policy gradient update network parameter uploads generation agent equation update distribution generation parameter analysis parameter update theorem parameter update DC DRL algorithm regard neuroevolution proof comparison review pioneer neuroevolution propose competitive alternative policy gradient parameter update gaussian perturbation specific neuroevolution gradient function estimator equation nabla mathbb approx sum mathbb frac prime prime sigma cdot prime prime tag equation sourcewhere prime prime gaussian perturbed parameter sum mathbb eta prime prime equation equation sum mathbb eta prime prime prime prime tag equation source sigma constant eta prime prime proportional  prime prime equation frac prime prime sigma cdot parameter equation therefore parameter update DC DRL algorithm regard neuroevolution neuroevolution interpret calculate finite difference derivative estimate random direction insight theorem parameter update DC DRL algorithm interpret finite difference derivative estimate direction gaussian perturbation policy gradient combination neuroevolution policy gradient parameter scheme empowers DC DRL algorithm exploration ability convergence rate evaluation numerical evaluation setup simulation environment testbed evaluate performance propose algorithm simulation environmental parameter accord recognition application default parameter specify parameter simulate computation intensive deadline sensitive situation simulation scenario coordinate ES fix MUs ES coverage addition MUs randomly distribute within communication coverage MEC data transmission channel transmit channel bandwidth omega mhz varpi dbm loss exponent price per computation task task arrival rate constant average task per simulate user request input data cpu cycle sample recognition application KB  respectively price cpu cycle ES  computation capacity assume ES equip core cpu cpu frequency core ghz addition assume MU equip core cpu frequency ghz lastly theta respectively DC DRL agent distribute DRL agent upload interval text upload local training upsilon distribute DRL agent dnn structure actor network critic network hidden layer neuron neuron respectively rectify linear activation function dnn tanh output function actor network maximum capacity replay memory distribute agent agent respectively batch DRL training initial rate critic network actor network respectively discount rate future reward update target network tau lastly adam optimizer optimize neural network code available github com  DC DRL convergence analysis propose DRL algorithm integrate improvement achieve performance demonstrate convergence DC DRL algorithm ppo previous ppo DRL algorithm applies KL penalty coefficient adjust stepsize parameter update adaptively propose DRL algorithm  applies adaptive genetic algorithm AGA advantage evaluation ability critic network exploration average loss critic network scenario MUs measurement interval average define average measurement interval loss critic network mse sample randomly replay memory calculate equation parameter actor network update propagation critic network plot loss curve critic network epoch DC DRL algorithm sum training epoch distribute DRL agent convergence analysis DC DRL algorithm highlight average DC DRL algorithm surrogate gradient gradient descent DRL inaccurate stage training DC DRL algorithm quickly obtain parameter setting comb neuroevolution policy gradient addition DC DRL converges epoch  ppo converge epoch illustrates improvement introduce accelerate convergence DC DRL algorithm adopts multiple environment increase diversity illustrate loss DC DRL algorithm rapidly converges stable trend demonstrates diversity contributes faster smoother curve addition highlight DC DRL algorithm converges comparison difference due exploration ability DC DRL algorithm contributes reduce unexplored action rid local optimum consequently allows DC DRL algorithm decision comprehensively performance evaluation various environment setting performance MUs evaluate performance DC DRL algorithm scenario MUs DC DRL performance optimization algorithm baseline random strategy MU randomly decides task locally offload ES greedy strategy randomly generate action sequence candidate action sequence comparative easy DC DRL algorithm achieve baseline performance gap propose algorithm baseline grows MUs increase implication decision account random greedy strategy task resource shortage task failure later comparative performance MUs DC DRL algorithm advanced DRL algorithm focus optimize performance ppo  demonstrate performance algorithm DC DRL ppo  achieve advantage baseline shot optimization random greedy DC DRL algorithm outperforms ppo  MUs exceeds reveals trend respect task rate DC DRL algorithm reduce task latency deadline constraint action increase MUs increase ppo  liable local optimum moreover DC DRL algorithm deterioration MUs increase scalability application feasible MUs increase magnitude MUs reuse DC DRL algorithm offload decision simulation performance MUs performance data demonstrate robustness DC DRL algorithm application impact data program code input data simulation data KB KB average apply algorithm increase increase data inevitably increase data transmission queue DC DRL algorithm average baseline DC DRL algorithm manage wireless network resource avoid data congestion task computational overhead perform locally alleviate network congestion subsequent task computationally expensive offload ES therefore DC DRL algorithm achieve average propose DC DRL algorithm baseline various setting average propose DC DRL algorithm baseline various setting performance cpu cycle simulation scenario task execution cpu cycle   cpu cycle accomplish task computation capacity demonstrates variation average cpu cycle performance algorithm deteriorate cpu cycle increase deterioration grows rapidly cpu cycle  suggests increase initial average due increase consumption rapid deterioration cpu cycle  due lack computational resource incoming task task queue computational resource available task failure nevertheless DC DRL algorithm maintains ppo  algorithm advantage strain resource performance cpu core ES analyze impact computation capacity ES average illustrate simulation scenario cpu core ES average decrease cpu core increase relationship computation capacity conflict reduce consumption execution DC DRL algorithm achieves ppo  algorithm simulation suggests propose algorithm improve resource utilization cope shortage computational resource moreover cpu core ES increase average DC DRL algorithm decrease remains stable cpu core ES DC DRL algorithm already optimal optimal strategy redundant computational resource significant performance improvement insight deploy computational resource server resource efficient manner impact adaptive performance improvement integrate adaptive DC DRL algorithm comparison adopt dpg algorithm baseline distribute DRL algorithm dpg preset calculate return parameter update without loss generality DC DRL dpg algorithm denote dpg dpg dpg respectively addition DC DRL without adaptive max denote DC DRL simulation scenario variation average entire training obvious dpg fluctuates acutely convergence inevitably variance distribute training setting besides dpg DC DRL converge rate DC DRL reward estimate dpg DC DRL training phase highly bias deviation direction convergence DC DRL leverage information multiple obtain estimate improve efficiency moreover dpg algorithm curve DC DRL significantly stable fluctuation despite convergence trend attribute policy correction adaptive conclusion DC DRL algorithm achieve balance reduce variance improve training efficiency impact adaptive impact combine neuroevolution policy gradient investigate performance improvement combine neuroevolution policy gradient although dpg troubled variance issue performance inferior DC DRL algorithm implies introduction neuroevolution enhance exploration ability avoid premature convergence algorithm vanilla standard neuroevolution DC DRL without neuroevolution denote DC DRL investigate performance improvement introduce policy gradient DC DRL implement mention neuroevolution maintains population distribute DRL agent important achieve fairness individual effort training DRL agent variance across population variance DC DRL DC DRL vanilla neuroevolution trend variance decrease around training suggests performance DRL agent within ensure fairness training impact combine neuroevolution policy gradient addition across population DC DRL leverage DRL population decrease faster DC DRL noteworthy although variance DC DRL vanilla neuroevolution rate decline varies greatly explain calculate average layer actor network correspond variance population vanilla neuroevolution DC DRL algorithm variance across entire training intuitively variance explore DC DRL algorithm easy escape local optimum excellent performance respect exploration deploy DRL application MEC obstacle exploration training exhibit exploration DRL algorithm converge simulation scenario MUs exploration DC DRL algorithm sum distribute agent DC DRL algorithm achieves exploration  attempt reduce exploration leverage AGA evaluation ability critic network DC DRL algorithm adaptive improve sample efficiency accelerate training introduction neuroevolution enables DC DRL quickly obtain parameter setting stage training besides plot variance DC DRL exploration fairness across population variance magnitude exploration therefore conclude evenly across multiple environment exploration MUs scenario furthermore validate practicality applicability DC DRL algorithm multi user computation offload testbed testbed structure  PC HP ES equip intel core HQ cpu physical core ghz GB ram raspberry MUs equip armv processor physical core ghz GB ram connectivity wifi router MUs ES raspberry integrate wireless lan offload task ES wifi transmit raspberry dbm employ recognition application widely scenario smart security due cpu memory limit raspberry offload incoming recognition task PC deploy computation offload prototype MUs ES compute implement multi user computation offload testbed conduct raspberry calculate cpu occupation data transmission application response addition consumption estimate accord approach experimental illustrate task perform ES local task perform locally raspberry algorithm outperforms random algorithm however increase raspberry performance algorithm deteriorates faster rate random algorithm algorithm raspberry compete limited resource ES enjoy performance reveals experimental testbed competitive requirement offload decision addition raspberry average DC DRL increase stable trend faster rate fold initial increase mainly attribute increase consumption raspberry rapid due lack resource due exponential growth increase raspberry challenge trial error DC DRL likely quality overall average performance gain ppo  random local approximately percent respectively average versus raspberry conclusion computation offload multi user MEC objective minimize overall adapt dynamic environment formulate optimization MDP leverage recent advance DRL due impractical directly apply exist DRL algorithm MEC propose distribute collective DRL algorithm DC DRL empower practical perspective increase diversity DC DRL algorithm  knowledge multiple environment improves performance distribute DRL agent collectively furthermore propose adaptive combine neuroevolution policy gradient maximize benefit distribute collective training evaluation DC DRL algorithm obtain knowledge beyond agent performance evenly across multiple environment adaptive improve training efficiency without affect convergence stability combination neuroevolution policy gradient greatly improve utility distribute training reduce exploration improve convergence future DRL schedule sub task acyclic graph dag constraint