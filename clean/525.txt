data framework spark hadoop widely adopt analytics research affordable compute resource easy manage hence organization shift towards deployment data compute cluster however schedule complex presence various service agreement SLA objective monetary reduction performance improvement exist research address multiple objective fail capture inherent cluster workload characteristic article formulate schedule deployed spark cluster propose novel reinforcement RL model accommodate SLA objective develop RL cluster environment implement reinforce DRL scheduler TF agent framework propose DRL schedule agent grain executor leverage pricing model VM instance addition DRL agent inherent characteristic placement reduce cluster VM usage average duration propose DRL algorithm reduce VM usage introduction data processing framework hadoop spark storm become extremely popular due data analytics domain significant business research framework deployed premise physical resource however service provider CSPs flexible scalable affordable compute resource model furthermore resource easy manage deploy physical resource organization towards deployment data analytics cluster avoid hassle manage physical resource service agreement SLA service consumer service provider various quality service qos requirement user schedule data compute cluster important objective performance improvement however cluster deployed schedule becomes complicate presence crucial SLA objective monetary reduction focus SLA schedule deployed apache spark cluster chosen apache spark prominent framework data processing spark intermediate memory processing moreover scalable platform suitable variety complex analytics spark program implement program data source hdfs hbase cassandra amazon data abstraction spark resilient distribute dataset rdd fault tolerant spark cluster deployed generally submit execution framework scheduler responsible allocate chunk resource cpu memory executor task parallel executor default spark scheduler executor distribute fashion worker node approach allows balance cluster performance improvement compute intensive workload interference executor avoid executor packed node although packed placement stress worker node improve performance network intensive communication executor becomes intra node spark framework scheduler static chosen user option consolidate however placement strategy suitable default scheduler unable cluster furthermore framework scheduler capable capture inherent knowledge resource workload characteristic accommodate target objective efficiently exist focus various SLA objective however implication executor creation along target objective assume cluster setup homogeneous however deployed cluster pricing model VM instance leveraged reduce overall monetary cluster finally heuristic performance model focus specific scenario generalize adapt objective inherent characteristic workload recently reinforcement RL approach complex RL agent balance multiple objective RL agent capture various inherent cluster workload characteristic adapt automatically RL agent prior knowledge environment instead interact environment explore situation reward action agent policy maximizes overall reward reward model desire objective due benefit mention propose RL model schedule goal appropriate executor placement strategy cluster resource constraint dynamic optimize objective RL reward reflect target SLA objective monetary average duration reduction spark deployed apache mesos cluster capture cluster statistic utilize profile information develop simulation environment showcase characteristic cluster RL agent interacts schedule environment reward chosen action schedule simulation environment utilizes workload RL reward model generate reward agent independent reward generation observes various reward solely chosen action propose RL model schedule action selection worker node VM creation executor specific implement agent DQN policy gradient agent reinforce schedule environment DRL schedule agent interact simulation environment schedule satisfy resource capacity constraint vms resource demand constraint besides agent minimize monetary VM usage average duration schedule environment agent developed tensorflow TF agent summary contribution RL model spark schedule compute environment formulate reward DRL agent satisfy resource constraint optimize efficiency reduce average duration cluster develop prototype RL model python environment plug TF agent framework simulation environment showcase characteristic deployed cluster generate reward DRL agent utilize RL model cluster trace implement DRL agent DQN reinforce schedule agent TF agent framework conduct extensive workload trace evaluate performance DRL schedule agent baseline scheduler organize discus exist related formulate schedule propose RL model propose DRL schedule agent exhibit implement RL environment experimental setup baseline algorithm performance evaluation DRL agent discus strategy DRL agent limitation concludes highlight future related schedule vms data centre schedule task vms schedule VM creation data centre paris model performance various workload VM identify offs performance model VM performance workload host vms propose  task schedule algorithm distribute data  formulate multiobjective optimization  maximize profit  provider minimize average task loss possibility application jointly split task multiple ISPs task service rate  propose schedule multi allocation mma optimize makespan submit task security reliability constraint address data cluster schedule schedule task vms VM provision data centre variance due spark memory architectural paradigm addition executor spark VM scheduler vms executor satisfy resource constraint addition workload performance varies placement strategy consolidate aim scheduler without prior knowledge workload cluster dynamic performance model framework scheduler apache spark fifo scheduler default executor distribute manner reduce overhead worker node vms deployment although strategy improve performance compute intensive workload due increase network shuffle operation network intensive workload suffer performance overhead spark consolidate core usage minimize node cluster however vms runtime therefore costly vms longer incur VM drf scheduler improve fairness multiple cluster however scheduler improve SLA objective efficiency deployed cluster workload various executor placement strategy suitable framework scheduler unable grain executor placement performance model heuristic scheduler improve aspect schedule spark approach performance model workload resource characteristic performance model resource demand prediction sophisticated heuristic achieve objective sparrow decentralize scheduler random sample approach improve performance default spark schedule  cluster manager minimizes resource utilization cluster satisfy user application performance target collaborative filter impact resource application performance information efficient resource allocation schedule  estimate performance historical trace performs packed placement container minimize cluster resource usage moreover  provision fail dynamically increase overall cluster performance justice deadline constraint historical execution trace admission resource allocation automatically adapts workload variation sufficient resource deadline met  model performance spark profile information performance model compose efficient cluster deploy minimal vms satisfy deadline furthermore assume executor resource capacity VM utilize DVFS technique tune cpu frequency incoming workload decrease consumption efficient scheduler algorithm assumes executor equivalent resource capacity VM performance model heuristic approach performance model heavily data sometimes obsolete due various cluster environment tune modify heuristic approach incorporate workload cluster therefore researcher focus RL approach tackle schedule efficient scalable manner DRL scheduler application reinforcement DRL schedule relatively address SLA objective schedule application developed hierarchical framework resource allocation reduce consumption latency degradation global tier VM resource allocation contrast local tier lstm workload predictor model RL manager local server propose qos aware schedule algorithm application deployment DQN target network replay improve stability algorithm objective improve average response maximize VM resource utilization  reinforce policy gradient  algorithm multi resource pack cluster schedule objective minimize average slowdown however cluster resource chunk cpu memory cluster assume homogeneous  policy gradient agent objective  agent environment tackle dag schedule within spark interdependent task actor critic algorithm processing unbounded continuous data scalability apache storm schedule assign workload worker node objective reduce average tuple processing assumes cluster setup homogeneous efficiency DSS automate data task schedule approach compute environment combine DRL lstm automatically predict vms incoming data schedule improve performance data analytics reduce resource execution harmony driven ML cluster scheduler training minimizes interference maximizes average completion actor critic algorithm aware action exploration replay besides reward prediction model historical sample reward unseen placement DQN algorithm spark schedule objective optimize bandwidth resource along node link consumption minimization spear minimize makespan complex dag task dependency heterogeneous resource demand spear utilizes monte carlo MCTS task schedule DRL model expansion MCTS propose optimal task allocation scheme virtual network mapping algorithm cnn function task allocate onto physical node objective revenue maximization satisfy task requirement gradient bandit improve resource utilization throughput spark flink RL model learns location goodness resource summary exist approach focus mainly performance improvement furthermore assume task assign VM worker node moreover assume cluster node homogeneous cluster deployed grain executor placement spark schedule contrast schedule agent executor vms optimize specific policy guarantee launch executor resource addition agent handle executor VM instance pricing model furthermore agent optimize objective efficiency performance improvement addition agent balance multiple objective lastly propose schedule agent inherent characteristic placement strategy improve target objective without prior information cluster summary comparison related comparison related formulation spark cluster virtual machine VM worker node generally service provider CSPs instance vms varies resource capacity assume VM instance deploy cluster deployed cluster submit user user specify resource demand submit specification contains executor executor cpu memory submit therefore arrival stochastic scheduler cluster prior knowledge arrival scheduler FCFS basis handle data cluster however scheduler vms executor target scheduler reduce overall monetary cluster addition additional target reduce completion notation formulation definition suppose vms deploy spark cluster vms instance resource capacity cpu memory schedule schedule submit cluster scheduler executor vms resource capacity constraint vms resource demand constraint user submit resource demand executor dimension cpu core memory therefore executor treat multi dimensional VM bin schedule therefore cpu memory resource demand capacity constraint define    source    source  binary decision variable executor VM otherwise executor resource VM scheduler allocate resource multiple vms executor constraint define  SourceRight click MathML additional feature schedule incur scheduler define    SourceRight click MathML additional feature additionally define average completion   source minimize cluster average completion optimization minimize  sourcewhere parameter user specify optimization priority scheduler eqn generalize address additional objective optimization mixed integer linear program MILP non convex generally NP optimally optimal scheduler completion schedule decision scheduler extremely collection profile model performance depends various parameter furthermore executor cluster increase optimally feasible although heuristic algorithm highly scalable generalize multiple objective capture inherent characteristic cluster workload improve target goal focus cluster schedule objective scheduler provision resource appropriate vms minimize overall usage cluster minimize duration cluster schedule decision capture performance issue spark RDDs data dependency locality increase decrease completion model reinforcement RL model reinforcement RL framework agent task interact environment generally RL algorithm agent whereas environment agent continuously interact environment vice versa agent action environment policy action agent depends environment action agent receives reward environment objective agent improve policy maximize sum reward agent scheduler schedule spark cluster satisfy resource demand constraint resource capacity constraint vms reward environment directly associate schedule objective efficiency reduction average duration therefore maximize reward agent learns policy optimize target objective propose RL framework schedule treat component cluster environment highlight dash rectangle scheduler cluster manager monitor cluster worker node vms executor schedule agent observation environment resource requirement resource availability cluster expose cluster monitor metric cluster manager action selection specific VM executor agent action cluster cluster manager reward generator calculates reward evaluate action basis predefined target objective RL environment reward agent external agent however RL algorithm internal reward parameter calculation continuously update policy propose RL model schedule schedule agent interact cluster environment assume model discrete driven therefore agent action component RL model specify agent agent scheduler responsible schedule cluster observes action action receives reward observable environment episode episode interval agent cluster schedule addition episode terminate agent chooses action violates resource constraint schedule observation agent action schedule environment deployed cluster agent cluster action however schedule episode agent receives initial without action cluster parameter cpu memory resource availability vms cluster price VM cluster specification schedule action decision placement agent scheduler allocate resource executor resource allocation executor action action environment return agent environment dimensional vector vector VM specification     vector specification      cpu availability vms cluster whereas   memory availability vms cluster     cpu memory demand executor respectively executor resource demand information executor therefore grows increase cluster vms specification agent arrival previous already schedule successive action cluster resource capacity update due executor placement therefore reflect update cluster recent action executor agent specification parameter reduce executor placement becomes schedule successfully specification along update cluster parameter spark cluster user specifies resource requirement cpu cpu core per executor mem memory per executor executor framework resource specification executor initial specification user whereas VM specification cluster manager RL model specification combine observation environment chosen action action action selection VM executor cluster sufficient resource executor agent previously schedule addition optimize objective agent schedule arrives therefore vms cluster discrete action define action specify agent executor action specifies index VM chosen executor reward agent receives immediate reward whenever action positive negative reward action positive reward motivates agent action optimize overall reward episode contrast negative reward generally agent avoid action RL maximize overall reward episode agent immediate discount future reward action overall goal maximize cumulative reward episode sometimes action incurs immediate negative reward towards positive reward future define immediate reward episodic reward agent choice fix immediate reward simplifies RL model RL model maze solver robot episodic reward award successful episode fix incentive award training risk traversal maze similarly RL model environment assigns immediate positive reward placement successful executor spark furthermore environment assigns immediate negative reward agent chooses without executor action positive negative reward fix VM performance instead fix reward agent satisfy resource capacity constraint vms resource demand constraint initial training episode agent resource constraint automatically failure satisfy VM constraint terminate episode fix negative reward episodic reward discus calculate episodic reward award successful completion episode suppose vms duration episode maximum placement decision maximum incur scheduler episode    source therefore episodic VM usage  incur agent eqn normalize episodic define   source priority objective parameter episodic   source ideal executor accord distribute placement executor cpu bound compact placement network bound minimum average completion episode   source similarly executor accord characteristic maximum average completion episode   source therefore episodic average completion agent eqn normalize episodic average completion define     source priority average duration objective parameter episodic average duration   source  fix episodic reward agent performs episode maximize objective function episodic reward  define     SourceRight click MathML additional feature addition   therefore sum   reward exactly  chosen agent reduce average duration scenario agent achieve     therefore   indicates agent optimize policy  negative award upon successful completion episode mention negative reward automatically award environment termination episode violation constraint fix episodic reward fix target objective agent explore sequence executor placement optimize desire objective however episodic reward award upon completion successful episode episode successful resource constraint violate resource availability resource demand constraint episode away negative reward agent workout action reward workout action reward propose RL model schedule scenario cluster compose vms specification VM cpu mem VM cpu mem addition another specification       scenario agent chosen VM twice executor placement agent positive reward valid placement VM accommodate anything action agent invalid environment execute action instead episode terminate agent negative reward scenario agent successfully executor sufficient resource executor agent chosen action resource freed resource freed agent successfully executor episode episodic reward scenario transition propose environment  agent schedule schedule propose RL environment DRL algorithm DQN approach policy gradient algorithm reinforce chosen algorithm RL environment discrete action procedure algorithm DQN optimizes action reinforce directly update policy spark schedule context RL environment specification trace workload addition cluster resource VM resource availability update DRL agent action placement executor reward previous VM specification update placement eventually DRL agent resource availability demand constraint schedule executor episode episodic reward DQN agent quality action function function policy sum reward acquire action policy optimal function define maximum return optimal policy optimal function define bellman optimality equation  source discount factor determines priority future reward agent achieve future reward motivates focus immediate reward optimal policy sum reward policy successful episode expectation distribution immediate reward bellman optimality equation iterative update  converges optimal function DQN dynamic program DP function dimensional matrix combination however dimensional action tabular infeasible therefore neural generally parameter approximate loss minimize source  distribution transition sample environment temporal difference TD target TD error target target supervise fix target therefore neural net towards target reduce loss however RL environment gradually target improve target network unstable target network fix network parameter sample previous iteration network parameter target network update network stable training furthermore input data independent identically distribute however within trajectory episode iteration correlate training iteration update model parameter closer truth update influence estimation destabilize network therefore circular replay buffer previous transition action reward sample environment therefore mini batch sample replay buffer neural network data independent DQN policy algorithm policy data environment ongoing improve policy algorithm diverge sub optimal policy due insufficient coverage action therefore greedy policy selects greedy action probability random action probability unexplored ensures algorithm stuck local maximum DQN algorithm replay buffer target network summarize algorithm reinforce agent DQN optimizes action indirectly optimizes policy however policy gradient model optimize policy directly policy usually model parameterized function respect accordingly probability action amount reward agent depends policy algorithm DQN algorithm foreach iteration sample environment policy greedy sample replay buffer sample batch data replay buffer update agent network parameter eqn conventional policy gradient algorithm batch sample iteration update eqn apply policy sample     source discount factor whereas action reward respectively episode discount cumulative return compute eqn  trt SourceHere action immediate reward influence reward accumulate episode return eqn maximum likelihood likelihood data RL context likely trajectory policy likelihood reward likelihood policy increase generates positive reward likelihood policy decrease negative reward summary model policy tends away policy however formula expectation cannot directly therefore sample estimator instead eqn  source policy gradient target objective parameterized assume iteration trajectory sample trajectory action reward ait  reinforce algorithm algorithm algorithm utilize monte carlo compute reward execute episode collection algorithm update underlie network update policy gradient parameter sample trajectory greedy policy algorithm reinforce algorithm foreach iteration sample policy environment policy gradient eqn RL environment implementation developed simulation environment python deployed spark cluster environment cluster agent interact action whenever action immediate reward episodic reward completion episode episodic reward positive negative episode successfully terminate action agent feature developed environment summarize environment expose comprise cluster resource statistic agent action agent environment detect valid invalid placement assign positive negative reward accordingly agent performance episode environment award episodic reward environment reward generator therefore simulated cluster workload trace environment derive episodic reward environment considers impact execution executor vms due locality contention public duration simulation environment profile experimental cluster environment duration goodness agent executor placement assigns reward agent accordingly mention instead fix interval refer arbitrary progressive stage decision incorporate TF agent api return transition termination signal workflow environment agent training trigger negative positive reward respectively environment summary action reward summarize serial reward corresponds implement environment TF agent DRL agent specifically agent achieve target objective efficiency performance improvement reward signal achieve efficiency average duration reduction implement environment extend modify incorporate reward objective continuous additional DRL agent implement environment RM  source RL cluster schedule environment tensorflow agent backend action reward mapping propose RL environment workflow propose environment response agent action trigger negative positive reward respectively environment performance evaluation discus experimental setting cluster resource detail workload generation baseline scheduler evaluation comparison DRL agent baseline schedule algorithm experimental setting cluster resource chosen VM instance various pricing model evaluate agent optimize cluster deployed public cluster resource detail summarize pricing model VM instance aws EC instance pricing australia cluster resource detail workload  benchmark suite application cluster wordcount cpu intensive pagerank network IO intensive sort memory intensive uniform distribution generate requirement within cpu core memory GB executor arrival arrival rate extract facebook hadoop workload trace arrival simulation chosen arrival normal burst profile experimental cluster virtual machine VM nectar research vms apache mesos cluster manager chosen workload application generate requirement arrival facebook trace profile profile information simulation environment built python calculate   addition   calculate dynamically accord chosen action agent cluster maximum minimum execution artificially maximum runtime minimum runtime shortest respectively cannot calculate beforehand without prior knowledge profile information simulate latency locality issue due placement decision simulation environment increase duration automatically agent utilize executor placement strategy consolidate experimental cluster cluster resource simulation environment tensorflow cluster detail vms cpu core 4GB memory nectar research DRL agent tensorflow version TF agent version instal along python vms hyperparameters hyperparameter setting DQN reinforce agent along environment parameter valid action reward agent successful executor placement reward decides reward constraint violate agent proceed positive reward action agent infinitely accumulate positive reward assign fix negative reward motivate agent executor cluster resource available invalid action reward negative executor properly negative episodic reward award reward mistake along episode termination addition negative reward accumulate sum reward successfully executor negative fix episodic reward  chosen performance improvement reduction policy episodic reward motivate agent policy optimize desire objective hyper parameter DRL agent environment parameter baseline scheduler schedule algorithm baseline propose DRL algorithm robin RR default approach spark scheduler distributively executor vms robin consolidate rrc another robin approach spark scheduler minimize vms pack executor already vms avoid launch unused vms FF develop baseline executor available VM reduce integer linear program ILP algorithm mixed ILP solver optimal placement executor decision optimization dynamically generate cluster specification addition improve performance profile information estimate completion within model optimally adaptive executor placement AEP algorithm prior profile information central versus  executor placement approach schedule freely consolidate placement approach schedule prefer executor placement strategy algorithm prefer placement strategy executor baseline scheduler propose schedule agent dynamic decision cluster global convergence DRL agent convergence DQN reinforce algorithm respectively DRL agent parameter showcase multiple reward maximization evaluation algorithm iteration calculate average reward policy normal arrival agent iteration burst arrival agent iteration convergence DQN algorithm convergence reinforce algorithm indicates agent reward optimize VM usage contrast indicates agent optimize reduction average duration varied indicates agent optimize reward optimize average duration ignore episodic reward contrast indicates agent optimize reduce average duration exclude indicates mode operation agent optimize reward priority priority episodic reward calculate cluster resource specification arrival rate additionally episodic reward varies optimization target various training setting distinctive maximal reward episode average reward accumulate DQN agent training normal burst arrival respectively similarly average reward accumulation training iteration reinforce agent average reward fix reward successive executor placement episodic reward actual VM usage average duration however accumulate reward implies agent policy optimize actual objective agent negative reward gradually reward explore multiple iteration due randomness induced greedy sometimes reward algorithm however training reinforce agent stable DQN agent agent converge workload burst arrival agent action cluster sufficient resource accommodate resource requirement burst resource constraint training environment properly agent avoid action violate resource capacity demand constraint negative reward therefore training algorithm incur negative reward however action executor placement satisfy constraint environment award immediate reward motivates agent eventually episode schedule successfully agent schedule properly without violate resource constraint optimize target objective episodic reward action episode evaluation efficiency evaluate propose DRL agent baseline schedule algorithm regard VM usage schedule episode calculate usage VM cluster cluster exhibit comparison schedule algorithm minimize VM usage normal arrival arrival sparse algorithm incur VM usage due placement executor therefore tight pack vms VM usage AEP algorithm outperforms algorithm incurs VM usage utilizes profile information executor placement strategy AEP algorithm chooses placement cpu memory intensive incurs VM usage suffer duration increase due executor placement ILP algorithm runtime estimate executor incurs reinforce optimize reinforce performs closely AEP ILP algorithm incur respectively therefore agent increase AEP algorithm respectively however DRL algorithm performs baseline normal arrival without prior knowledge characteristic runtime estimate comparison VM usage incur schedule algorithm schedule episode burst arrival cluster resource schedule schedule algorithm resource freed already addition arrival dense duration increase due placement network bound schedule across multiple vms increase increase VM usage although AEP ILP algorithm utilize completion estimate suffer duration increase placement cannot characteristic due overload cluster reflect reinforce agent achieve significant benefit reinforce agent incur respectively comparison baseline AEP ILP incurs respectively reinforce agent although surprising reinforce agent optimizes VM version burst arrival objective consideration policy optimize effectively RR algorithm perform distribute executor VM usage although FF rrc algorithm minimize VM usage network bound restrict vms instead increase due duration increase DQN agent  performance minimize VM usage policy reinforce underlie characteristic minimize VM usage evaluation average duration calculate average duration schedule episode performance schedule algorithm comparison schedule algorithm reduce average duration normal arrival RR algorithm performs distribute multiple vms memory bound cpu bound combine network bound RR algorithm acquire significant duration penalty due distribute placement network bound RR algorithm closely AEP optimize version DQN reinforce agent respectively DQN increase average duration whereas reinforce increase average duration RR algorithm comparison schedule algorithm regard average duration schedule episode comparison schedule algorithm regard average duration schedule episode burst arrival resource available addition placement characteristic completion increase average duration scenario propose reinforce DQN agent capture underlie relationship duration placement goodness incorporates information strategy reduce duration policy reinforce outperforms baseline algorithm RR reduces average duration underlie characteristic reflect ideal placement addition impact minimization duration reduction objective therefore placement algorithm placement decision algorithm normal burst arrival respectively baseline schedule algorithm fix objective capture workload characteristic placement decision fix baseline algorithm therefore parameter affect algorithm algorithm horizontal however DQN reinforce agent tune optimize optimize decrease trend placement agent parameter increase towards optimize optimize version performance DQN reinforce average duration reduction explain graph DQN agent placement reinforce agent normal arrival average duration DQN agent contrast reinforce agent placement decision burst arrival reduce average duration DQN agent comparison placement decision schedule algorithm schedule episode evaluation multiple reward maximization exhibit parameter optimize multiple reward solid normal arrival whereas dash burst arrival addition colour whereas reflect DQN reinforce agent stable maximize objective increase agent towards optimize instead reduction multiple reward optimize parameter tune agent balance policy prioritizes objective solid DQN agent balance outcome optimize parameter multi objective episodic reward RL environment optimize optimize mode reward priority strategy summarize strategy agent DRL agent VM capacity demand constraint negative reward environment action constraint violation partial executor placement DRL agent optimize pack executor vms however characteristic executor avoid duration increase reward showcased placement goodness evaluation graph agent handle normal burst arrival cluster fully load cluster resource executor situation agent executor resource capacity constraint vms violate agent negative reward episode termination schedule agent continuously action situation eventually execution cluster resource become agent non zero action executor placement action award slight negative reward episode termination agent decides instead violate constraint experimental positive reward infinite agent positive reward forever slight negative reward encourages agent executor resource avoids ambiguity policy agent stable policy balance multiple reward parameter tune graph conclusion future schedule data application environment challenge due inherent VM workload characteristic traditional framework scheduler LP optimization heuristic approach mainly focus objective generalize optimize multiple objective capture underlie resource workload characteristic introduce RL model spark schedule environment developed prototype RL environment TF agent utilized DRL agent optimize multiple objective addition prototype RL environment DRL agent namely DQN reinforce sophisticated reward signal DRL agent resource constraint performance variability cluster VM usage agent optimize target objective without prior information cluster immediate episodic reward interact cluster environment propose agent outperform baseline algorithm optimize objective showcase balance performance optimize target strategy discover DRL agent effective reward maximization future explore location goodness affect duration investigate sophisticated reward model accommodate duration immediate reward agent perform efficiently batch extract policy cluster agent simplify training agent scratch deployed actual cluster allows investigate RL agent characteristic cluster dynamic optimize objective efficiently