federate attract research attention due privacy protection distribute machine however exist federate mainly focus convolutional neural network cnn cannot efficiently handle graph data popular application graph convolutional network gcn propose promising technique graph federate seldom explore article propose FedGraph federate graph multiple compute client subgraph FedGraph graph capability across client address unique challenge traditional gcn training feature data client risk privacy leakage FedGraph solves issue novel client convolution operation challenge gcn training overhead incur graph propose intelligent graph sample algorithm reinforcement automatically converge optimal sample policy balance training accuracy implement FedGraph pytorch deploy testbed performance evaluation experimental popular datasets demonstrate FedGraph significantly outperforms exist enable faster convergence accuracy federate promise enable collaborative machine distribute device preserve data privacy amount research effort federate convolutional neural network cnn model superior accuracy image data however application generate graph data social graph protein structure consist node evidence cnn cannot efficiently handle graph graph convolutional network gcn propose graph novel graph convolution operation cnn convolution operation filter pixel graph convolution operation filter feature node unfortunately exist federate mainly focus cnn gcn explore recently preliminary research effort graph decentralize datasets vertical federate scenario graph client maintain node feature similarly assume graph structural feature label belong source recent explore intersection graph federate non data distribution federate graph however inter graph connection pervasive phenomenon federate gcn graph data distribute multiple compute client data due privacy protection client subgraph connection subgraphs others graph node associate feature private information medical hospital organize graph graph node feature personal information gender occupation health disease widely recognize feature data privacy sensitive cannot expose node label goal graph predict label node federate gcn extension counterpart cnn unique challenge gcn training involves node feature client risk privacy leakage exploit graph structure information graph convolution operation aggregate feature data node operation fail node maintain client refuse expose feature straightforward privacy protection eliminate feature seriously decrease training accuracy confirm experimental challenge training overhead incur graph social network maintain facebook contains billion user correspond graph data gigabyte gcn model stack layer structure graph model becomes extremely exceed physical memory constraint propose FedGraph federate graph integrates federate gcn opportunity privacy preserve distribute graph FedGraph distribute graph complicate connection converge training accuracy address challenge challenge dilemma feature privacy protection cryptography technique homomorphic encryption enable computation encrypt data despite security guarantee technique computational overhead inappropriate choice FedGraph pursuit training exist hardware sgx privacy protection security hardware limited capacity cannot handle graph data FedGraph solves dilemma client graph convolution operation without cryptographic operation dedicate hardware instead directly node feature FedGraph embeds dimensional representation feature cannot recover reduce gcn training overhead graph sample widely adopt randomly mini batch node training graphsage graph sample node relationship randomly selects fix apply graph convolution operation node FastGCN propose improve sample efficiency independently node graph convolution layer however exist cannot satisfy requirement FedGraph due weakness sample craft parameter rely heavily upon knowledge domain expert performance graphsage parameter specify sample manual parameter tune consume exist ignore tradeoff training training accuracy sample node accelerates training decrease accuracy client participate federate graph heterogeneous graph computational capability apply sample policy client optimal weakness sample algorithm challenge FedGraph instead struggle improve exist heuristic resort reinforcement DRL technique intelligent sample algorithm automatically adjust sample policy jointly computation overhead training accuracy client heterogeneity carefully examine various DRL algorithm deterministic policy gradient DDPG cast federate graph contribution propose FedGraph novel federate graph formally procedure local training client global parameter update server lightweight client convolution operation propose enable feature client avoid privacy leakage DRL sample algorithm FedGraph automatically sample policy tradeoff training accuracy implement prototype FedGraph evaluate testbed popular graph datasets performance evaluation experimental FedGraph enables faster convergence accuracy exist organize review background gcn federate FedGraph intelligent sample policy experimental related finally concludes background motivation background federate gcn addition analyze exist graph sample approach weakness motivate FedGraph federate goal federate model distribute device avoid exposure training data typical federate consists device dataset cannot expose others addition parameter server responsible synchronize training device federate contains multiple training training device global model parameter server independently conduct training local data update model model difference parameter server training device parameter server integrates global model training device model almost impossible infer training data model due protection training data federate becomes hottest topic recent important research effort address various challenge however focus cnn model gcn orient federate seldom graph convolutional network cnn achieve euclidean data image video however amount data express graph consist node non euclidean data graph convolutional network gcn propose promising technique graph stack multiple graph convolutional layer gcn exploit information graph structure node feature node classification various application specifically undirected graph define node respectively correspond graph adjacency matrix denote node associate feature vector gcn contains convolutional layer structure graph lth layer node vector node embed layer input graph graph convolution operation aggregate embeddings node transfer dimensional representation finally activation function relu generate node embeddings layer formally propagation gcn define QH sourcewhere node embeddings lth layer matrix identity matrix feature trainable parameter node label feature matrix gradient descent algorithm parameter classify node without label illustration graph convolution operation graph sample application graph correspond gcn training computational overhead graph sample propose reduce graph gcn training exist classify category node wise sample iteratively sample fix node node layer randomly subset lth layer sample guarantee aggregation node embeddings happens node representative node wise sample graphsage however sample node exponentially increase layer construct addition incurs redundancy embed calculation node node node recent approach VR gcn cluster gcn propose improve performance node wise sample cannot fundamentally address weakness illustration sample approach sample node marked dash arrow denote connection graph solid arrow denote preserve sample node approach layer wise importance sample independently sample fix node gcn layer sample probability calculate node FastGCN typical approach layer wise importance sample however node layer sample independently sample node connection previous layer marked node embeddings  node lose graph convolution operation deteriorate training performance strength weakness sample approach motivate sample policy computation overhead relation sample FedGraph typical federate graph consists compute client conduct local training task server responsible global parameter update compute client server location network client maintains graph node associate feature vector cannot expose client subset  node label denote  training data contains internal node external node client client aware existence node maintain others cannot directly access feature vector FedGraph architecture client maintains local graph training node mini batch node aggregate embeddings generate layer embeddings denote arrow training completes client uploads local model parameter server finally parameter server aggregate local model update global model sends client assume compute client parameter server honest curious honestly federate procedure feature information others typical threat model widely federate research serious threat model malicious client tamper training modify model parameter parameter server threat trust execution environment tee local training tee commonly available CPUs enables isolated execution environment guaranteed hardware adversary cannot access data code tee besides malicious parameter server modify global model parameter compromise federate secure multi computation mpc homomorphic encryption model aggregation besides tee global model aggregation parameter server customize parameter server client module implement intelligent sample parameter server contains module DDPG sample algorithm generates sample policy client model aggregator local feature client aggregate generate global feature training addition communication module message exchange parameter server client communication module realize grpc apis tcp communication protocol client module gcn construction responsible gcn model accord sample policy gcn training module training algorithm FedGraph predict unlabeled node client collaboratively global feature multiple training client feature server construct local GCNs due existence external connection local gcn training involves embed client update feature server creates global feature training although FedGraph traditional federate unique procedure local training global parameter update local gcn training client local gcn training procedure client described algorithm client downloads feature graph sample policy server local feature initialize client launch multiple training iteration update feature local graph data specifically training iteration consists algorithm local training procedure client training feature sample policy parameter server initialize local feature iteration construct gcn  layer node  source   source generate embeddings layer source calculate loss accord function   source update local feature source submit update feature server gcn construction construct gcn layer function  sample subset node accord policy randomly node label refer mini batch node mini batch iteratively aggregate embeddings sample subset hop away pseudo code  algorithm specifically sample policy express denotes mini batch sample probability layer respectively sample label node mini batch compose lth layer iteratively construct gcn layer backward direction node layer randomly subset lth layer probability addition matrix replace denotes node graph matrix describes update adjacent relation sample feature aggregation later sample node lth layer maintain algorithm   randomly label node mini batch lth layer layer node sample subset accord selection probability update adjacent matrix otherwise source algorithm gcn construction combine strength node wise sample layer wise sample sample probability independent opportunity grain sample layer layer wise sample carefully probability avoid computational incur recursive explosive expansion neighborhood meanwhile sample neighborhood relation node wise sample avoid sample node without connection gcn training construct gcn model gcn gradient descent client graph convolution operation described algorithm specifically client aggregate embeddings internal gcn layer layer enable client aggregate internal external prevent leakage local origin feature enable information security analysis aggregation nonlinear transformation apply generate node embed layer objective minimize loss function define compute gradient update feature rate finally client submits update feature difference parameter server global parameter update server procedure global update parameter server algorithm server initialize random feature sample policy sends client respectively training update local feature client task creates global feature aggregate local denotes mini batch label node client training task update sample policy client function  detail  important contribution relies reinforcement technique balance computational overhead model accuracy finally server sends global feature sample policy client training algorithm global update parameter server initialize random feature sample policy client respectively training feature client global feature   source update sample policy  global feature sample policy client security analysis propose algorithm protects feature data client node embeddings training without loss generality suppose client aggregate embeddings client infer node feature matrix feature node client vij denote client node client accord algorithm client information vij vij vij client node embeddings approximate remote local synchronize global feature server however client infer vij client information adjacent matrix client sample furthermore hardly achieve accuracy due dimension reduction embeddings layer feature node impossible feature internal node client therefore conclude FedGraph node feature enable information federate graph intelligent graph sample DRL sample policy node involve gcn training affect computational overhead training accuracy sample node accelerate training reduce computational overhead lower training accuracy sample node approximate gcn achieve training accuracy incurs computational therefore significant sample policy tradeoff however ignore exist meanwhile sample policy due optimization manual tune hardly desire automatic algorithm minimum involvement generate sample policy carefully examine sample policy influence performance training accuracy cannot described precise expression instead struggle heuristic algorithm resort reinforcement DRL automatically approximate DRL implement various generate thrive algorithm application scenario performance carefully candidate DRL algorithm deterministic policy gradient DDPG algorithm efficiently handle dimensional continuous action DDPG combine network actor critic approach enjoys benefit DDPG formulation apply DDPG formulate markov decision define training feature global feature denotes local feature client action denote leverage principal component analysis pca project dimensional onto dimensional distribution information action parameter server graph sample policy client action therefore define correspond sample policy action denote reward accuracy performance metric reward define reflect completion training denote evaluate training server easily obtain consumption local training client training accuracy calculate parameter server typical federate parameter server usually task publisher client training validation cannot expose due privacy concern information define reward SourceRight click MathML additional feature target accuracy constant adjust express preference accuracy reward contains evaluates accuracy improvement nonlinear improvement proceeds quickly improve training improvement becomes later reward unbiased exponential function evaluates completion training negative encourage training completion client affected factor computational hardware network latency alleviate impact factor constant evaluate influence sample policy penalty refer easily achieve profile policy objective define DRL policy parameterized precisely algorithm output deterministic action objective DRL sample algorithm maximize cumulative discount reward define SourceRight click MathML additional feature  cumulative discount reward function action function define cumulative discount reward execute action policy typically neural network approximate policy function action function sample DDPG DDPG sample algorithm illustrate actor network predict deterministic action critic network estimate action function meanwhile maintain actor network critic network denote refer target network update actor critic network illustration DDPG sample network maintain replay buffer finite historical transition define update actor critic network sample mini batch transition buffer buffer sample discard formally introduce DRL sample algorithm implementation detail function  explain learns optimal sample scheme algorithm sample algorithm DRL randomly initialize actor critic parameter initialize target network parameter initialize initial reduce dimension initial pca initialize exploration replay buffer generate sample policy client episode observer reward pca transition replay buffer randomly mini batch transition replay buffer update critic actor network update target network update  source  source generate sample policy pseudo code DDPG algorithm algorithm initialize network training server observes information feature client reward define reduce dimension pca transition replay buffer randomly mini batch transition update critic network minimize loss function  sourcewhere  target action parameter critic network update sourcewhere rate update actor network  sourcewhere rate actor network parameter target network update finally obtain action sample policy update network performance evaluation experimental setting implement FedGraph pytorch graph library DGL python package dedicate graph deploy FedGraph compute client intel cpu GB memory geforce RTX gpu popular graph datasets cora citeseer pubmed reddit widely gcn statistic information datasets summarize graph cora citeseer limited synthesize graph datasets dataset client randomly selects proportion node local graph data belongs normal distribution generate local graph overlap node graph datasets cora citeseer graph carefully local graph generation avoid overlap node overlap synthesize datasets treat node influence training performance graph synthesis adopt local dataset randomly node generate training validation connection across client maintain accord graph local graph client construct layer gcn input layer convolutional layer hidden dropout rate rate cora citeseer pubmed reddit hidden dropout rate rate batch cora citeseer reddit pubmed adam optimizer local gcn training reward function exponential function FedGraph relies exponential reward function influence FedGraph moreover difference training accuracy target accuracy affect reward dataset accuracy report exist knowledge accuracy estimation accord FedGraph relies exponential reward function estimation influence FedGraph constant aim balance accuracy improvement penalty setting comparison extend graph sample scheme federate graph batch conduct graph sample graph construct gcn graph data statistic graphsage typical node wise sample iteratively sample fix sample convolutional layer respectively setting FastGCN typical layer wise importance sample independently sample fix node layer layer layer cora citeseer reddit pubmed setting advocate DRL sample algorithm FedGraph actor network critic network hidden layer compress feature dimension sklearn decomposition pca experimental convergence DRL sample FedGraph episode cumulative return datasets target accuracy cora pubmed citeseer reddit cumulative discount return datasets converge stable episode dataset reddit almost converges episode demonstrate convergence propose DRL sample scheme training accuracy accuracy convergence sample scheme FedGraph converge faster achieve accuracy comparison physical instead training metric evaluate training scheme client graph consume training specifically FedGraph achieves accuracy cora algorithm achieve accuracy pubmed FedGraph achieve accuracy graphsage batch scheme converge datasets reddit FedGraph advantage obvious summarize graphsage serious computation redundancy consumes training FastGCN sufficient embed information client sample node connection batch scheme calculate embeddings node incurs computational graph pubmed reddit FedGraph address weakness achieves performance training fix FastGCN completes training earlier sample node training moreover evaluate scalability FedGraph enlarge experimental client correspond FedGraph outperforms sample scheme influence graph heterogeneity influence graph heterogeneity variance heterogeneity correspond variance respectively understand calculate ratio graph respectively training converge target accuracy achieve sample scheme pubmed target accuracy FastGCN converge training sample scheme increase graph become heterogeneous datasets however FedGraph growth DRL sample jointly considers training accuracy client embed FedGraph client graph convolution operation enable embed client hiding local feature local gcn training comparison alternative refer FedGraph  embeddings layer maximize information refer FedGraph  discard client simplify accuracy convergence training curve FedGraph FedGraph  demonstrates FedGraph information loss eliminates embed layer layer embed contains information feature hence FedGraph efficiently client embed without feature exchange simultaneously FedGraph significantly outperforms FedGraph  datasets cora citeseer client convolution operation increase training accuracy pubmed accuracy FedGraph enables convergence reddit sensitive client embed datasets FedGraph  converges accuracy FedGraph converge reddit connection ignore client seriously graph structure FedGraph  completes training earlier eliminates embed impact gcn depth impact gcn depth graph convolutional layer datasets obvious growth complexity increase layer meanwhile accuracy accuracy citeseer decrease growth gcn layer smooth issue cumulative discount return FedGraph cumulative discount return FedGraph accuracy convergence sample scheme client FastGCN completes training sample node training however performance datasets accuracy convergence sample scheme client accuracy convergence sample scheme client convergence graph heterogeneity convergence graph heterogeneity convergence FedGraph FedGraph  FedGraph  completes training ignores connection local training however convergence convergence FedGraph FedGraph  FedGraph  completes training ignores connection local training however convergence training accuracy gcn depth training accuracy gcn depth impact non iid data effectiveness FedGraph handle non iid data demonstrate generate non iid data distribution subset node local graph experimental FedGraph outperforms scheme accuracy convergence sample scheme non iid data related federate federate attract research attention due promise enable privacy preserve distribute machine demonstrate impact non iid data federate mathematical propose approach sends uniform distribution data client reduce non iid data recently GNNs federate setting develop federate platform detect financial crime activity across multiple financial institution extract global graph information euclidean data graph analytic instead graph neural network besides assume global graph belongs client contrast GCNs non euclidean data client local graph propose novel distribute surveillance GNN federate critical difference device federate involve camera limited computation communication capability contrast silo federate typically involves client aim model however explore inter client connection node feature federate privacy preserve graph neural network vertical federate assume graph structural feature label belong source however horizontal federate local client maintains graph dataset graph structure node feature label graph convolutional network due excellent performance gcn widely graph application node classification link prediction recommendation recently apply gcn processing task machine translation relation classification accelerate gcn training  propose framework efficient scalable parallel neural network computation graph  gpu training parallel processing multiple gpus propose distribute gcn training message passing exchange however ignores privacy protection federate scenario graph sample effectively reduce gcn training overhead propose graphsage construct simplify gcn sample subset node however graphsage incurs redundant computation node although propose alleviate redundant computation reduce sample node VR gcn cluster gcn address training gcn layer wise sample FastGCN  propose sample node layer independently instead sample node sample efficiently reduce computation sample node connection due independent sample degrade training accuracy addition sample craft parameter manual tune weakness exist motivate FedGraph intelligent sample conclusion propose FedGraph novel federate graph enable privacy preserve distribute gcn traditional federate FedGraph challenge gcn training involves embed client address challenge FedGraph novel client graph convolution operation compress embeddings private information hidden addition reduce gcn training overhead FedGraph adopts DRL sample scheme balance training accuracy experimental client testbed FedGraph significantly outperforms exist scheme