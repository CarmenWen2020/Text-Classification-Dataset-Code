compute efficient prompt data compute service resource delay sensitive iot application via computation offload effective computation offload strategy comprehensively cope issue allocation dynamic communication computational resource delay constraint heterogeneous task requirement computationally inexpensive distribute algorithm however exist mainly focus issue suffice achieve performance complex practical scenario tackle challenge systematically distribute computation offload delay constraint heterogeneous computational task continually offload server via limit stochastic communication channel task offload formulate delay constrain stochastic optimization unknown prior statistical knowledge technical transform decompose slot sub devise distribute online algorithm namely TODG efficiently allocate resource schedule offload task comprehensive analysis TODG optimality gap delay impact parameter extensive simulation demonstrate effectiveness efficiency TODG compute efficient prompt data compute service resource delay sensitive iot application via computation offload effective computation offload strategy comprehensively cope issue allocation dynamic communication computational resource delay constraint heterogeneous task requirement computationally inexpensive distribute algorithm however exist mainly focus issue suffice achieve performance complex practical scenario tackle challenge systematically distribute computation offload delay constraint heterogeneous computational task continually offload server via limit stochastic communication channel task offload formulate delay constrain stochastic optimization unknown prior statistical knowledge technical transform decompose slot sub devise distribute online algorithm namely TODG efficiently allocate resource schedule offload task comprehensive analysis TODG optimality gap delay impact parameter extensive simulation demonstrate effectiveness efficiency TODG introduction due rapid development wireless communication mobile device become information hub access physical cyber application activity recognition interactive processing developed mobile device intelligent convenient service however application usually computation intensive delay sensitive hardly execute resource constrain mobile device significant challenge offload delay guarantee tackle challenge compute propose promising alleviate compute burden mobile device reduce service delay leverage compute capability device infrastructure proximity data source pervasive prompt agile service via computation offload anytime anywhere nevertheless efficient computation offload strategy compute non trivial task conventional compute device offload task compute increase user device complicate offload decision contention insufficient computational resource server computational resource server coordinate user device performance seriously degrade due overwhelmed offload task compute computation offload involve wireless communication user device server inherently limited stochastic resource effective resource allocation strategy otherwise wireless network capacity quickly strain transmission efficiency dissatisfaction compute service moreover delay guarantee offload task essential application interactive recognition render smart however stochasticity communication channel compute server delay extremely heterogeneous task delay requirement addition contrast centralize powerful server server deployed distribute manner resource limited heterogeneous critical importance develop distribute computationally efficient algorithm task offload context compute recently witness significant progress develop novel approach address challenge task offload various aspect efficient offload strategy jointly allocate communication computation resource performance improvement lower response latency develop decentralize offload however exist mainly aim tackle aforementioned issue weaken restriction therefore argue strategy comprehensively issue account requisite achieve effectual computation offload compute bridge gap systematically distribute task offload delay constraint compute heterogeneous computational task resource response continually offload server compute capability via limit random channel accordingly formulate offload delay constrain stochastic optimization unknown prior statistical distribution clearly tough stochastic optimization inherent complexity continually schedule heterogeneous task jointly allocate communication computational resource address challenge approach transform decompose sub develop online algorithm TODG sub distribute manner periodic strategy TODG allocate channel slot alleviate computational operation comprehensive performance analysis TODG demonstrate TODG achieve optimal computational besides rigorously TODG satisfy delay constraint quantify impact delay requirement task buffer utility contribution summarize knowledge systematically distribute task offload resource allocation strategy heterogeneous computational task delay guarantee formulate offload delay constrain stochastic optimization unknown prior statistical knowledge random task arrival channel compute server devise online algorithm stochastic optimization namely TODG implement parallel user device server delay guarantee offload task develop periodic strategy enable channel assignment slot largely mitigates computational communication overhead induced complex computation resource allocation comprehensive analysis propose algorithm characterize optimality gap response latency quantify impact parameter performance buffer delay requirement periodic strategy extensive simulation showcase efficacy TODG remainder organize briefly review related introduces model formulates distribute task offload detail propose TODG algorithm analyze theoretical performance TODG finally performance evaluation conclusion drawn related enable technology task offload attract increase research attention compute recently focus offload decision mobile device offload task server decentralize offload offload decision minimize overhead propose computation offload approach offload task cpu frequency mobile device minimize task execution consumption recent joint communication computation resource allocation improve performance task offload perspective specifically propose channel allocation resource management approach optimize offload decision maximize network utility resource management approach offload decision channel allocation cache strategy maximize network utility however ignore latency constraint task offload significantly important delay sensitive application attracts increase research attention offload latency propose task partition task offload optimal multitask offload optimize execution delay overhead threshold policy manage offload data volume channel access opportunity TDMA compute joint compute resource allocation task offload approach heterogeneity requirement offload task leverage sdn cooperative layer fog compute framework optimize bandwidth load balance develops optimal task offload algorithm  considers stochastic wireless channel exploit markov chain obtain optimal offload decision deadline constraint focus task offload non orthogonal multiple access noma compute propose online algorithm optimal task subcarrier allocation decision minimize task execution delay decentralize task offload another research trend compute  propose efficient decentralize algorithm compute equilibrium task offload variational inequality theory decentralize offload algorithm lyapunov optimization primal dual theory decomposes complex sub mobile device server separately however investigate server aim average delay instead strict delay constraint stackelberg theoretic approach develop iterative distribute offload strategy hierarchical vehicular compute nevertheless exist distribute assumption fix task volume sufficient communication resource addition introduce statistical computation transmission model propose distribute task offload algorithm statistical delay guarantee consideration stochastic communication resource schedule statistical delay guarantee focus online offload delay constraint summarize difference exist notably focus complex practical scenario heterogeneous computational task random arrival schedule server delay constraint distribute algorithm optimize utility comparison related model illustrate compute operates slot indexed heterogeneous user device denote offload computational task server denote via limited communication channel denote faster response consumption slot allocates channel server user device enable offload computational task local task buffer generally user device classify denote device belonging generate task image processing video processing etc accordingly server creates virtual machine vms correspond task task correspond VM server worth assume prior knowledge statistical distribution stochastic variable task arrival channel service rate server superscript specify variable user server respectively denote simplicity notation summarize notation notation illustration delay constrain computation offload heterogeneous task multiple channel server compute transmission model denote nth user device kth orthogonal channel user device dynamically access task offload slot stochastic variable cnk channel capacity device server maximum amount task transmit satisfy cnk cmax positive constant cmax user device access channel slot offload task server binary variable znk denote offload strategy znk sourcefor znk indicates device offload task server via channel otherwise znk user device access channel slot  sourcefor circumvent interference channel access device meanwhile implies  SourceRight click MathML additional feature define snk transmission rate offload task device slot due limitation reasonable assume snk bound snk  sourcewhere   bound random variable slot maximal amount task offload server besides snk impact channel capture snk  cnk sourcewhich implies snk cannot exceed channel capacity access device slot task buffer model define stochastic variable aunk amount task arrival device satisfy aunk maxnk maxnk due delay sensitivity stability user device date task denote dunk local task buffer dunk maxnk sourcewith positive constant maxnk qunk local task buffer device dynamic qunk max qunk snk dunk aunk sourcewhere qunk denote kth VM server asm amount task slot respectively asm express asm  sourcewhere min snk qunk actual departure device due unpredictable server assume processing rate vms random bound stochastic variable denote amount task  similarly dsm denote amount task slot satisfy dsm maxm sourcewhere maxm maintains task buffer denote qsm queue dynamic qsm max qsm dsm asm  task buffer device server finite impose constraint qunk qsm qunk maxnk qsm maxm sourcewhere maxnk maxnk maxm  buffer device VM respectively delay constrain model hunk mint min snk dunk qunk output qunk qunk aunk SourceRight click MathML additional feature  mint hunk min dsm qsm output qsm qsm asm hunk source device define hunk slot task aunk leaf local task buffer qunk indicates hunk task aunk along aunk offload queue delay aunk local device denote  hunk  aunk denote server aunk offload znk hunk hunk define  slot task aunk leaf VM buffer accordingly processing delay aunk server express   hunk aunk locally otherwise SourceRight click MathML additional feature exists acceptable delay device task denote     sourceit implies task execute deadline transmission delay constant neglect brevity task offload goal maximize utility user device satisfy delay constraint define average amount task user device unk limt aunk source unk limt dunk sourcein slightly abuse notation define  distinct dsm amount task offload server similarly define average  snk limt  SourceBased cast task offload stochastic optimization max  unk unk snk sourcewhere gnk differentiable concave non decrease utility function finite maximum derivative denote  denote dunk dsm applies formulation aim offload resource allocation policy enable execute task satisfy delay stability constraint TODG distribute task offload scheme delay guarantee develop online algorithm address mention distribute manner transform easy handle decompose slot sub approach sub slot transformation decomposition easy  directly task device mixed task buffer server derive lemma decouple snk unk transform equivalently handy  max  unk unk source lemma exists optimal optimal proof proof decouple snk unk rewrite objective function equivalent max gnk unk unk snk sourcewhere maxnk  easy optimal obtain snk task server easily user maximum derivative gnk objective transmit extra task improve utility gnk unk unk server snk penalty directly user device equivalent transformation besides definition  dsm   implies  snk combine conclusion proof moreover couple   parallel compute device server handle challenge introduce virtual queue decouple   relax delay constraint specifically user VM define delay queue zunk zsm delay task buffer zunk max zunk snk dunk ζunk source zsm max zsm dsm ζsm sourcewith parameter ζunk maxnk ζsm maxm intuitively arrival ζunk ζsm zunk zsm penalty upon task stuck task buffer slot departure qunk slot snk dunk delay task qunk increase slot accordingly zunk increase ζunk slot backlog zunk indicates latency happens task queue qunk demonstrate delay constraint satisfied appropriately parameter ζunk ζsm lemma impose assumption assumption rate maxnk maxm maxm max  maxnk maxm  source maxnk max maxnk  maxm maxm maxm source intuitively assumption implies delay constrain task arrival outweighs processing capacity capable task stability enables lemma suppose zunk zsm bound zunk maxnk zsm maxm SourceRight click MathML additional feature assumption delay constraint satisfied maxnk maxm  ζsm SourceRight click MathML additional feature  maxm maxm  ζunk source proof fix ζunk ζsm define queue delay qunk qsm  wsm respectively lemma   wsm maxm  sourceto satisfy  maxm wsm   maxm maxm     wsm maxm   sourceby obtain plug rearrange proof remark lemma implies bound zunk maxnk zsm maxm constraint satisfied sufficiently ζsm ζunk correspond besides zunk zsm bound unk ζunk unk ζunk min  cmax SourceRight click MathML additional feature  snk ζsm SourceRight click MathML additional feature therefore although ζsm ζunk delay offload task inequality increase ζsm ζunk increase task additional degradation utility lemma transform TP max  unk unk source employ dual drift plus penalty technique decompose slot sub optimize directly technique minimize slot drift plus penalty function compose slot utility function successive difference queue namely lyapunov drift enables maximize performance implicitly stability specifically slot define drift plus penalty function  aunk dunk  sourcewhere parameter balance utility latency lyapunov function define qunk zunk qsm zsm  minimize directly computationally costly alternative minimize upper bound lemma lemma upper bound drift plus penalty function express  aunk dunk  qunk aunk snk dunk zunk ζunk snk dunk qsm asm dsm zsm ζsm dsm sourcewhere denote maxnk  maxnk   maxm source proof max SourceBased dynamic qunk yield qunk qunk maxnk  maxnk qunk aunk sunk dunk SourceSimilarly qsm qsm   maxm qsm asm dem SourceRight click MathML additional feature dynamic zunk max ζunk snk dunk  maxnk zunk zunk  maxnk zunk ζunk sunk dunk SourceRight click MathML additional feature zsm zsm  maxm zsm ζsm dsm SourceRight click MathML additional feature sum difference queue yield therefore instead optimize attempt minimize dual slot DP SourceRight click MathML additional feature worth explicitly handle constraint dual simplify later rigorously satisfied propose algorithm via properly parameter theorem decompose sub user device slot task arrival buffer virtual queue backlog user device decides amount task dunk via sub  gnk aunk dunk qunk zunk dunk dunk maxnk source server slot buffer virtual queue backlog amount task VM dsm depends sub  qsm zsm dsm dsm maxm source offload decision sub obtain offload decision amount transmit task snk znk per slot max snk znk qunk zunk snk   source distribute task offload delay guarantee subsection distribute task offload delay guarantee algorithm TODG derive sub convex optimization expression easily however non trivial task resembles dimensional user device channel server proven NP literature besides offload decision variable znk couple amount transmit task snk exacerbates complexity sub separately user device denote optimal sub expression derive theorem lemma suppose gnk exists aunk qunk zunk express aunk qunk zunk  maxnk SourceRight click MathML additional feature denotes derivative gnk min max otherwise  maxnk gnk aunk qunk zunk source proof convenience slot denote  gnk aunk qunk zunk  define  znk discus optimal stationary  aunk qunk zunk SourceLet denote optimal sub expression derive theorem lemma indicates optimal dunk depends task queue delay queue backlog qunk zunk implies available task buffer sufficient latency relatively local device unnecessary task contrary qunk zunk outdated task stability rapid response server clearly sub linear program denote optimal maxm qsm zsm  lemma server dynamically adapt rate queue offload decision aim optimize sub attempt transform bipartite decentralize resolve efficiently derive theorem optimal amount local buffer lemma denote optimal slot min  cnk source proof user device offload task slot besides exists easy qunk zunk qsm otherwise denote slot qunk zunk snk   sourcethen combine proof lemma user device offload task slot transmit task correspond server local task buffer transform sub obtain max znk qunk zunk qsm min  cnk znk   znk SourceRight click MathML additional feature easy optimal roughly maximum qunk zunk qsm min  cnk tuple despite resemble dimensional argue equivalent bipartite lemma lemma slot exists mnk mnk mnk  qunk zunk qsm min  cnk SourceRight click MathML additional feature proof lemma easily obtain contradiction lemma implies user device channel suffices optimal rewrite maximum bipartite source destination max znk  znk   znk SourceRight click MathML additional feature  node denote    sourcewhere  qunk zunk  min  cnk mnk SourceBased derive via optimal mnk SourceNext efficiently maximum bipartite across slot feasible employ kuhn munkres maximum within max iteration however computationally costly highly centralize information queue channel central controller  practical scenario instead building recent advance multi robot application argue optimal achieve distribute manner via multi robot assignment algorithm specifically user device correspond   server server local parallel exchange information adjacent server nevertheless although alleviate computational complexity centralize via parallel compute relatively communication max communication maximum hop server corollary detail periodic strategy tackle issue periodic strategy mitigate computational communication incur multi robot assignment approach slot otherwise channel allocation slot specifically slot device slot occupy channel target server device slot remain idle slot algorithm online task offload delay guarantee algorithm TODG input  qunk  zunk  cnk output slot ζunk ζsm accord respectively user user compute  user channel accord   server channel allocation decision compute offload decision znk accord exist znk snk task server via channel accord compute amount task dunk accord update task queue qunk delay queue zunk accord respectively server server compute optimal channel allocation decision distribute manner multi robot assignment algorithm server VM compute amount task dsm accord update task queue qsm delay queue zsm accord respectively nutshell periodic strategy computes offload decision znk idle sourcewhere derive mnk express mnk idle   device target server mnk slot correspond computational communication negligible qsm scalar complexity compute mnk periodic strategy upon observation  channel capacity queue backlog generally sharply adjacent slot intuitively error periodic strategy optimal acceptable rigorously quantify induced error periodic strategy achieve optimal utility computational worth periodic strategy variant standard analytical technique due violate optimality sub sketch periodic strategy combine propose sub summarize detail TODG algorithm performance analysis analyze performance TODG establish delay stability guarantee characterize optimality gap impact parameter performance lemma parameter affect queue lemma suppose assumption TODG achieve qunk maxnk qsm  proof qunk maxnk slot easy qunk suppose slot qunk qunk maxnk increase maxnk slot qunk lemma assumption dunk aunk hence queue qunk cannot increase slot qunk qunk thereby yield similarly qsm  slot proof lemma indicates task queue bound parameter implicitly task queue combine lemma derive theorem theorem suppose ζunk ζsm satisfy min sourcewhere denote mink maxnk max maxnk ζunk source minm maxm max  ζsm  assumption ζsm ζunk satisfy respectively slot proof proof lemma easy zunk ζunk zsm ζsm qunk maxnk qsm maxm SourceRight click MathML additional feature zunk maxnk zsm maxm SourceRight click MathML additional feature combine lemma proof theorem demonstrates buffer constraint task response satisfied appropriately parameter ζsm ζunk TODG beneficial implicitly handle constraint delay satisfy via slot decision analyze optimality gap TODG lemma characterize error induced periodic strategy lemma periodic strategy TODG slot satisfies SourceRight click MathML additional feature denotes optimal dual cmax maxk maxnk min proof optimal sub qunk zunk qsm min  cnk znk sourceif easy znk qunk zunk qsm denote device assign channel periodic optimal strategy slot respectively define lemma rewrite   maxk maxnk min  lemma cumulative error decompose sub linearly increase computation gap lemma derive rare transmission rate user device suddenly become zero device enjoy maximum channel capacity slot however channel task queue commonly fluctuate sharply adjacent slot reasonable error incur periodic strategy theoretical gap lemma establish optimality gap TODG theorem opt denote objective correspond optimal respectively theorem suppose assumption min satisfied stochastic variable independent identically distribute slot opt sourcewhere define proof theorem easy fix exists stationary randomize policy feasible action independent queue backlog slot satisfy  gnk aunk unk  opt SourceRight click MathML additional feature unk unk SourceRight click MathML additional feature ζunk unk SourceRight click MathML additional feature SourceRight click MathML additional feature ζsm SourceRight click MathML additional feature theorem define auxiliary queue obtain however feasible dunk deterministic directly derive via stationary randomize policy without additional virtual queue lemma combine  aunk dunk  opt SourceRight click MathML additional feature lemma denotes queue expectation sum yield  gnk aunk dunk  dsm opt   rearrange  gnk aunk dunk  dsm opt SourceRight click MathML additional feature limit jensen inequality  unk unk opt  proof theorem achievable utility TODG controllable gap regard parameter optimal network utility although relatively mitigate computational task schedule channel allocation performance degradation utility combine lemma parameter achieves latency utility optimality gap increase task queue thereby response average derive theorem generalize non ergodic via theorem outside scope remark theorem quantifies impact task buffer delay requirement utility mink  maxnk source minm maxnk  maxm SourceRight click MathML additional feature due   min SourceRight click MathML additional feature implies task buffer improve utility delay requirement performance evaluation insight algorithm performance via extensive simulation simulation setup compute consist server server creates virtual machine vms server heterogeneous compute request image processing data compression mathematical calculation user device integer channel slot transmission rate cnk mbps channel device server slot uniformly distribute task arrival rate aunk mbps across slot drawn continuous uniform distribution device statistic characteristic arrival rate task respectively propose algorithm efficient without prior knowledge stochastic random variable stochastic characteristic without affect performance algorithm processing rate VM parameter detail summarize parameter simulation parameter simulation baseline propose algorithm TODG centralize stochastic algorithm sca propose heterogeneous task offload greedy algorithm GA selects min user device slot shortest task queue user task server minimal task backlog due lack channel allocation mechanism sca GA randomly assign channel user device implementation implement code matlab server intel xeon golden CPUs nvidia tesla 2G gpu utility delay validate achievable utility demonstrate fix parameter plot achieve utility TODG becomes increase specifically utility increase sharply increase increase decrease underlie rationale bound utility concave function respect however illustrates response delay increase linearly parameter enables utility latency impact performance combine easy utility degradation indicates correctness analytical theorem besides imply decrease rate utility induced periodic strategy relatively delay remain stable increase optimal offload decision slot utility decrease average maximal delay comparison maximal utility delay achieve sca therefore periodic offload strategy utility delay guarantee inexpensive computational periodic strategy non negative integer periodic strategy enables channel allocation slot within device occupy channel transmission rate target server utility delay delta epsilon utility delay stability delay requirement stability delay requirement comparison utility delay algorithm buffer comparison utility delay algorithm buffer stability delay requirement dynamic task queue user device server task buffer slot illustrate fix buffer TODG guarantee stability task backlog response latency evaluate impact delay constraint utility exposition delay constraint user device  τmax easily strict delay requirement utility degradation delay requirement outweigh schedule processing capability outdated task exacerbate degradation delay constraint tight addition impact penalty parameter performance similarly  ζsm convenience increase obtain response delay utility validates lemma remark meanwhile role actually relationship theorem comparison performance algorithm performance TODG baseline algorithm task buffer user device server correspond utility task maximal response delay illustrate TODG outperforms sca GA fix buffer significantly reduce response delay TODG enables effectively exploit stochastic feature communication resource computational capability server meanwhile jointly schedule task TODG prioritize task delay requirement moreover buffer performance increase TODG flexibility task schedule however due lack effective schedule mechanism reduce response delay sca GA achieve limited utility task buffer latency longer queue scalability TODG evaluate scalability TODG server task fix device channel increase TODG outperforms baseline algorithm gap becomes indicates TODG insufficient communication bandwidth limited computational resource network besides sufficient server GA sca hardly improve performance bottleneck limited channel capacity detail comparison utility network scalability TODG scalability TODG comparison utility algorithm parameter comparison utility algorithm parameter illustrate utility achieve TODG increase sharply increase rate slows server suffice compute resource local task addition fix report TODG average slot recall series validate theoretical computation complexity TODG sec notably due periodic strategy channel assignment slot significantly reduce impact parameter impact parameter utility simulation user device illustrate TODG achieves performance user device contrary sca GA cannot exploit limited stochastic communication computational resource contention user device hinder improvement utility GA contention performance decrease neglect fairness device task fix device channel convenience task arrival processing rate VM respectively increase TODG outperforms baseline algorithm gap becomes evaluate performance transmission rate plot TODG substantially outperforms baseline algorithm channel capacity indicates TODG fully utilize limited communication resource besides processing capability server correspond impact performance TODG vastly improve performance sca GA powerful server implies importance effective task schedule channel allocation due inefficient utilization communication resource task cannot transmit server timely despite powerful server utility sca GA cannot improve contrast sca GA TODG enables exploit communication resource bottleneck TODG processing capability server eliminate limitation TODG advantage effective task schedule conclusion propose distribute online task offload algorithm TODG jointly allocates resource schedule offload task delay guarantee achieve inexpensive computational comprehensive theoretical insight TODG particularly balance optimal utility computational complexity extensive simulation validate effectiveness TODG demonstrate TODG outperforms baseline algorithm channel direction future task migration mobility scenario TODG simulation optimality gap theoretical bound intrigue understand phenomenon moreover remains largely incorporate online task schedule compute