increase computational complexity dnns achieve unprecedented various machine vision processing nlp recent advanced transformer billion parameter however dnns significantly exceed gpu physical memory limit cannot conventional data parallelism pipeline parallelism partition dnn subnets gpus plausible unfortunately layer partition memory management exist pipeline parallel fix training easily impede memory error gpu utilization drawback amplify perform neural architecture NAS evolve transformer network architecture transformer repeatedly vpipe transparently dynamic layer partition memory management pipeline parallelism vpipe unique contribution online algorithm optimal layer partition memory management layer migration protocol balance layer distribution across training pipeline vpipe improve training throughput notable baseline pipedream GPipe percent percent various dnns training setting introduction recent neural network dnns transformer bert AmoebaNet GNMT explosively deeper layer wider parameter per layer model capacity instance transformer layer execution operator billion parameter complexity dnn model expedite emergence neural architecture NAS evolve transformer layer model dynamically activate deactivate training dnn architecture accuracy increase complexity dynamicity training dnn gpu gigabyte memory pipeline parallelism promising approach dnns layer multiple gpus dnn partition multiple stage layer gpu exist pipeline parallel adopt static partition policy stage partition fix throughout entire training typical dnn training iteration contains pas backward pas stage memory consumption gpu stage activation pas reuse backward pas hardware efficiency gpu alu utilization pipeline parallel injects multiple batch input overlap backward pas execution pipeline data parallel transfer enormous parameter update gpus pipeline parallel transfer intermediate data layer across stage significantly reduce network consumption therefore complex dnns pipeline parallel efficient pipeline parallel achieve crucial goal injects multiple input batch carefully manage stage training memory avoid exceed physical memory capacity gpu otherwise memory error trigger synchronous significantly training execution dnn maximize efficiency gpu alu utilization stage stall enforce balance partition stage achieve roughly throughput data item per pipeline unfortunately despite effort building pipeline parallel simultaneously realize goal complex dynamic dnns exist pipeline parallel category category pipedream  activation tensor directly gpu memory however due backward dnn training activation tensor stage reside longer gpu memory rear stage input batch inject stage activation rear stage stage pipeline pipedream stage activation stage stage category moderate batch training batch gpu alu utilization throughput evaluation training transformer gpus pipedream batch gpu alu utilization rate percent average training throughput percent ideal throughput theoretical throughput suppose gpus unlimited physical memory utilize gpu ALUs define stage partition balance category GPipe PipeMare discard activation tensor recomputes backward significantly alleviates imbalanced gpu memory utilization stage rear stage extra pas evaluation GPipe batch training transformer gpus gpu alu utilization rate percent however recompute strategy inevitably waste alu utilization percent GPipe incur merely percent effective alu utilization useful gpu alu utilization contributes dnn training recompute utilization moreover category pipeline parallel encounter severe throughput degradation dnn model enables NAS layout model layer modify runtime algorithm evolution algorithm evaluation conduct NAS enable transformer notable category pipedream GPipe define ideal throughput pipedream throughput percent GPipe throughput percent overall despite advance exist pipeline parallel incur suboptimal training efficiency static dynamic NAS enable dnn training static strategy memory management layer partition stage become intense gpu memory explosion newly activate layer static strategy prevent available gpu resource adjacent stage alleviate intense stage vpipe dynamic dnn layer partition memory management virtualized layer typical pipeline parallel pipedream GPipe underlie execution pytorch tensorflow vpipe automatically transparently realizes goal automatically globally optimal migrates layer stage relocates layer activation parameter stage gpu cpu memory vpipe significantly alleviate intense stage pipeline improve pipeline throughput balance vpipe integrate pipedream vpipe GPipe vpipe achieve faster convergence pipedream GPipe training transformer gpus NAS enable evolve transformer training throughout tpt pipedream GPipe pipedream vpipe GPipe vpipe cope dynamicity achieve instead GPipe recompute strategy vpipe computes hybrid swap recompute layer stage specifically swap asynchronously evicts activation tensor cpu memory pre fetch gpu memory correspond backward usage pipeline parallelism usually exists opportunity input batch execution pas backward pas input batch leverage vpipe mask swap precisely predict arrival backward pas overlap input batch execution achieve instead static partition strategy vpipe online generates partition transparently migrates layer intense stage adjacent stage alleviate memory burden intense stage achieve balance partition throughput however realize goal vpipe tackle technical challenge challenge globally efficient swap recompute repartition SRP strategy stage literature model challenge combinatorial optimization however NP due exponential address challenge converge optimal algorithm powerful decomposition methodology via observation iteratively migrate layer intense stage adjacent stage enable optimization hybrid swap recompute stage architecture layout typical complex dnn usually construct coarsen graph subgraphs readily easy partition optimal vpipe detects coarsen graph precisely distinguish intra inside subgraphs nest subgraphs leverage series distance vertex layer runtime execution challenge gpu stall pipeline cleaning migrate layer vpipe transparent upper pipeline parallelism vpipe reduce parameter staleness upper exist pipeline parallel carefully various strategy orchestrate reduce staleness parameter update training accuracy throughput specific dnns vpipe guarantee layer migrate  non approach inject input batch upper pipeline migrate layer reboot pipeline handle migrate layer unfinished backward migration protocol observation activation generation pas usage correspond backward pas allows subtle interleave vpipe migrate layer transparently without alter parameter staleness upper implement vpipe pytorch loc evaluate prevalent dnn model complex dnns transformer bert AmoebaNet GNMT dnns resnet vgg evaluate relevant pipedream GPipe  PipeMare evaluation vpipe efficient training complex dnns vpipe improve pipedream GPipe throughput percent average complex dnns vpipe enlarge pipedream batch within training vpipe pipedream achieve training quality bleu vpipe scalable training complex dnns gpus vpipe throughput increase roughly linearly gpu gpus vpipe improve pipedream GPipe throughput percent vpipe efficient NAS workload evaluate transformer AmoebaNet evaluate complex dnns NAS feature vpipe improve pipedream GPipe throughput percent percent contribution vpipe dynamic layer partition memory management transparent underlie acceleration layer typical pipeline parallel pipedream GPipe novelty optimal stage distribute algorithm globally efficient swap recompute partition strategy greatly improve vpipe efficiency scalability secondary novelty transparent migration protocol without stall execution alter upper parameter staleness vpipe source code evaluation framework release github com hku vpipe background overview vpipe describes vpipe runtime vpipe implementation evaluation discus related concludes background dnn training dnn fundamental machine paradigm dnn model typically contains layer goal dnn training appropriate model parameter training dataset dnn training typically consists iteration pas backward pas optimization memory consumption dnn training contains parameter layer activation feature layer pas gradient gradient layer backward pas scratch computation activation significant portion percent memory consumption dnn training activation pas reuse backward pas exists memory access activation memory optimization target previous pipeline parallel dnn training dnn training increasingly computation memory intensive distribute training across multiple gpus become distribute training categorize data parallel model parallel data parallel gpu maintain model iteration gpu batch synchronizes parameter update gpus reduce parameter sever however data parallelism dnns cannot gpu memory pipelined model parallelism pipeline parallelism aim dnns gpus partition dnn model multiple stage consecutive layer gpu handle stage pipeline parallelism pipeline version model parallelism vanilla model parallelism severe utilization due bubble sequential dependency stage pipeline parallelism overlap computation input batch bubble improves utilization pipeline parallel handle synchronization dnn parameter input batch category barrier synchronous parallel BSP asynchronous parallel asp BSP GPipe training input batch version model parameter aggregate gradient compute iteration enforce barrier pipeline apply gradient model parameter BSP achieve almost statistical performance vanilla model parallelism however BSP pipeline logically incurs bubble barrier synchronization verify profile gpus stage BSP pipeline training logical BSP pipeline demonstrates bubble realtime  nvprof gpu profile verifies bubble BSP pipeline stage GPipe training sync barrier logical BSP pipeline demonstrates bubble realtime  nvprof gpu profile verifies bubble BSP pipeline stage GPipe training sync barrier asp pipedream PipeMare remove sync barrier input batch directly update model parameter although bubble eliminate asp suffer parameter staleness aspect parameter version differs pipeline pas backward pas parameter version differs stage within training input batch pipedream  PipeMare various algorithm mitigation parameter staleness vpipe transparent layer BSP asp pipeline parallelism algorithm vpipe alter staleness upper logical asp pipeline realtime  nvprof gpu profile asp pipeline stage pipedream training sync barrier schedule backward FB schedule introduce pipedream adopt successive PipeMare  FB schedule stage alternate perform pas input batch backward pas earlier input batch FB widely adopt due computational efficiency memory usage therefore assume upper pipeline parallel adopt FB schedule dynamic dnn training recently developer adopt dynamic dnn training layer varies training input dynet training exploratory neural architecture training workload gpu computation memory training varies training proceeds efficiency pipeline parallelism highly depends workload partition stage dynamicity expose requirement pipeline parallel variance training workload usually happens frequently neural architecture NAS adopts evolutionary algorithm model eliminates fitting initiate model eliminate within exist pipeline parallel profile static partition training static partition inherently cannot adapt dynamicity training vpipe cope dynamicity layer migration protocol transparently balance training load vpipe architecture vpipe architecture virtualized layer typical pipeline parallel underlie execution host virtualized tensor manager training monitor layer manager host stage global planner architecture vpipe vpipe virtualized layer typical pipeline parallel pipedream GPipe underlie execution pytorch tensorflow refer layer vpipe operation default swap recompute migrate virtualized tensor manager  grain management parameter activation tensor  layer tensor parameter activation information layer ID stage ID parameter activation training iteration ID version management policy  storage status pointer tensor storage construct activation tensor information initialize vpipe tensor manager delete release parameter tensor vpipe creates tensor information model initialize management policy layer tensor manage layer manager training monitor monitor stage runtime statistic memory usage gpu host pcie bandwidth usage network usage execution recompute along normal training iteration training monitor runtime statistic upstream stage downstream stage global planner runtime statistic stage pas partition strategy accord vpipe SRP algorithm resides host pipeline parallelism rear stage usually computation communication burden runtime statistic training iteration vpipe transfer runtime statistic along pas distributes partition along backward pas vpipe global planner extra distribute coordination layer manager receives partition strategy global planner diffs partition partition layer migration schedule layer migrate migration manager source stage coordinate tensor manager asynchronously swap layer parameter tensor activation tensor cpu memory transfer parameter activation tensor migration manager target stage migration manager target stage initialize layer target gpu parameter activation tensor source stage append layer pas backward pas execution layer manager local swap recompute policy overall vpipe transparent upper pipeline parallel integrate vpipe asp pipedream BSP GPipe vanilla pipedream layer  default vanilla GPipe layer  recompute vpipe integrate pipeline parallel PipeMare  imperative program model vpipe runtime model challenge vpipe optimal strategy swap recompute partition SRP steady throughput training pipeline maximize model quantify complexity SRP challenge literature formalize SRP challenge transform combinatorial optimization decomposition algorithm dnn graph layer matrix operation layer pipeline parallelism dnn model partition stage stage gpu gpus maximize pipeline utilization typical pipeline parallelism schedule input batch simultaneously inject pipeline layer model denote pas backward pas parameter memory activation memory constraint pipeline parallel training gpu training gpu memory usage exceed gpu physical memory limit pipeline parallelism memory consumption layer stage contains constant memory consumption  inject input batch dependent memory consumption  depends inject input batch differs stage stage  memory BSP parameter update synchronously input batch pipeline version parameter   asp training iteration pipeline independent version  contains reduce memory consumption pipeline parallel apply swap recompute strategy layer dependent tensor memory burden pipeline parallelism tensor layer denote memory management policy tensor default resides gpu memory tensor proactively swap cpu memory swap gpu usage tensor recomputed backward pas pipeline parallelism memory constraint stage denote      source nevertheless recompute layer introduces extra computation backward pas stage backward sum backward pas recompute extra pas recomputed layer swap swap normal execution max    max mdi   source finally formalize SRP challenge combinatorial optimization layer gpus swap recompute policy layer partition pipeline throughput maximize throughput pipeline throughput stage stage pipeline request rate pipeline throughput bottleneck stage execution sum   therefore convert partition swap recompute policy stage execution minimize    source optimization feasible combinatorial optimization span extremely layer memory management policy partition graph partition NP constraint memory management policy layer denote  stage partition denote  affect optimization objective multi variable combinatorial optimization swap recompute repartition multi variable combinatorial optimization decomposition methodology decomposition methodology sub coordinate optimization inspire conventional decomposition intuition iteratively migrate layer intense stage gpu resource exhaust relief stage intense stage optimization hybrid swap recompute decompose sub assume  constant stage locally swap recompute  gpu resource minimize objective function assume  constant stage  optimal  minimize algorithm decompose algorithm iteratively resolve sub algorithm decompose SRP algorithm stage function    diff  diff null migrate   migrate stats   stats algorithm return function  migrate stats  mem     stats append meta mem    stats return global planner function  stats migrate  migrate return unbalanced  stats unbalanced   algorithm   return swap recompute swap recompute goal reduce memory footprint overhead swap goal maximize overlap swap normal execution recompute goal cheapest layer maximize memory recompute recent  hybrid combination swap recompute activation tensor effectively reduce training memory gpu dnn training however apply swap pipeline parallel address subtle efficient swap precisely predict tensor swap cpu ram reuse backward pas gpu training activation tensor generate pas input batch training backward pas directly pas exist swap technique gpu training   vDNN  directly prediction dnn graph profile runtime generate however usually exists pipeline parallelism input batch execution pas backward pas input batch precise prediction vpipe oversees runtime statistic pas backward pas across stage pipeline algorithm vpipe layer manager precisely predict arrival backward pas execution algorithm  input layer stage   rank foreach layer   recompute  swap gain    sort  layer sort pop  rank   layer sort pop  foreach layer sort pop  default pipeline parallel swap network communication impose severe burden pcie lane severe pcie interference address gpu training vpipe network communication swap pas throughput pcie asynchronous handle pcie interference vpipe priority asynchronous pas pcie vpipe priority network communication pipeline execution vpipe swap recompute algorithm algorithm stage algorithm layer memory limit pcie bandwidth stage rank   stage input vpipe sort layer potential memory gain swap recompute pcie vpipe selects tensor accord memory gain asynchronously swap memory limit vpipe chooses swap recompute activation swap recompute memory gain layer vpipe default leverage subtle vpipe precisely overlap async swap tensor normal execution subtle async swap network communication normal training execution consequently algorithm reduces recompute overhead async swap exist pipeline parallel GPipe vpipe swap activation tensor activation memory consumption vpipe swap parameter tensor activation tensor swap rarely happens evaluation layer partition partition graph partition partition communication NP extensive application VLSI matrix factorization social network cluster kernighan lin KL algorithm excellent partition extensively achieve multi partition recursively partition graph iteratively improves exchange node partition KL algorithm costly partition complex dnn model stage cycle approximate algorithm tend linear yield partition obtain KL algorithm KL algorithm efficient multi scheme reduce graph coarsen graph collapse vertex partition graph uncoarsening multi scheme matrix factorization VLSI however algorithm assume domain specific requirement graph sparse matrix planar graph applicable complex dnn graph AmoebaNet moreover exist multi scheme multiple coarsen vpipe leverage series imply dnn sequential execution identify domain specific heuristic online multi graph partition algorithm coarsen scheme expert already construct graph complex dnns transformer bert AmoebaNet GNMT  deployed pipeline parallelism sequentially subgraphs layer subgraph usually transformer construct dnn inside subgraph intricate local nest multiple execution partition subgraph stage usually incurs network communication gpus sparse nest however network communication partition sparse nest static partition bert model input embed layer pas embed output stage partition network communication transfer input stage persistent conventional graph partition dnn graph vertex layer execute training series nest connects vertex gap stage execution axis sparse nest nest vertex axis likely subgraph heuristic vpipe layer repartition algorithm algorithm vpipe coarsens dnn graph dnn graph classify category critical construct sequential backbone dnn graph sparse nest subgraph vpipe merges subgraph sequential backbone aggregate execution communication vpipe partition merge graph iteratively apply bipartition KL algorithm vpipe  merge graph dnn graph refines partition potential partition exists KL refinement algorithm  input dnn graph runtime static layer layer invoke layer sort  layer  coarsen bound partition    bound refine bound function coarsen sum foreach  sort detect critical ine   append foreach   sparse subgraph     merge function  return bound   partition   refine bound foreach bound  analysis vpipe algorithm decomposes sub vpipe algorithm optimal sub linear optimization constraint memory limit pcie limit vpipe algorithm successive algorithm kernighan lin KL algorithm KL algorithm bipartition algorithm initial bipartition graph exchange vertex partition partition complexity KL algorithm cycle layer KL algorithm complex dnns AmoebaNet heuristic recent complex dnn graph vpipe partition algorithm coarsen phase complexity coarsens complex dnn graph AmoebaNet graph layer vertex graph coarsen AmoebaNet vertex KL algorithm greatly reduce partition various dnn model evaluation vpipe partition algorithm KL algorithm achieves training input batch deployed online layer migration exist pipeline parallel pipedream GPipe adopt static layer partition execution migrate layer developer adopt non approach runtime modify layer partition configuration reboot training suffers bootstrap overhead runtime initialization model initialization data load overhead dramatically decrease training efficiency layer migration frequently trigger dynamic training vpipe aim layer migration protocol pipeline parallelism technical requirement layer migration remain transparent upper vpipe alter upper parameter staleness exist pipeline parallel category BSP GPipe asp pipedream PipeMare  BSP parameter staleness asp adopt various parameter staleness strategy goal BSP asp strength workload instance GPipe achieve accuracy pipedream training transformer achieve accuracy pipedream training bert vpipe transparent upper vpipe alter parameter staleness vpipe programmer explicitly annotate model datasets default setting baseline resource consumption fitting micro training dnns gpus resource consumption fitting micro training dnns gpus however challenge transparently migrate layer without lose liveness BSP asp pipeline layer multiple unfinished backward execution backward update layer parameter avoid alter parameter staleness migration layer update backward lose moreover typical schedule asp layer stage pipeline execution interleave stage pas input batch directly parameter update input batch stage pas parameter update earlier input bach BSP stage version parameter parameter synchronization occurs avoid alter parameter staleness migration layer vpipe ensures layer migrate stage execution interleave layer accordingly vpipe guarantee layer migrate  non approach formalize transparency requirement input batch layer stage training pipeline stage simultaneously inject input batch layer unfinished backward asp stage pas input batch version layer parameter update BSP stage parameter synchronization happens input batch pas input batch parameter version    pif  BSP source layer migrate stage stage layer vpipe migrate activation tensor unfinished backward meanwhile asp strawman migration approach execution synchronously transfer parameter tensor activation tensor resume execution however training complex dnns tensor migrate gigabyte stall vpipe runtime layer migration protocol without lose generality discussion layer migration stage pipeline stage migrate layer stage input batch migration stage stage sends message stage inform migration layer stage initializes layer module layer module gpu memory stage sends stage layer migration trigger input batch stage stage realtime  nvprof gpu profile layer migration pink gpu cpu memory cpu gpu memory migration utilization visually target gpu disabled swap highlight migration memory stage receives migration immediately pas pas input batch stage immediately asynchronously transfer activation tensor backward pas input batch denote backward backward pas backward stage transfer parameter tensor layer update backward stage stage arrival parameter tensor layer layer backward pas backward stage subsequent layer activation tensor input batch continuously asynchronously vpipe ensures backward stage correspond activation tensor vpipe integrate asp vpipe transfer activation tensor correspond parameter tensor migrate layer overall vpipe layer migration merely affect normal execution vpipe asynchronously transfer activation tensor migrate layer verify profile avoid alter staleness vpipe ensures  remains consistent layer migrate stage stage vpipe layer migration trigger multiple trigger vpipe algorithm evaluation migration non migration approach stall pipeline execution vpipe migration protocol remains implementation vpipe leverage imperative feature pytorch popular framework typically imperative declarative program imperative program python program perform computation execution pytorch adopts default execution mode overall vpipe currently implement modify loc pytorch vpipe implementation dnn training imperative program style implement vpipe pytorch distribute demand swap recompute migrate layer stage implement NAS vpipe exist literature describes implement NAS pipeline parallelism capture access tensor vpipe intercept pytorch activation creation reuse backward pytorch activation tensor automatic gradient computation  graph data structure  vpipe intercept member function  tensor pointer vpipe  module pytorch  refer parameter tensor activation tensor vpipe distinguish parameter tensor activation tensor assign upon initialization parameter tensor initialize model initialization module initialization pytorch precisely predict swap tensor vpipe  module pas capture access tensor stage asynchronous demand swap activation tensor pytorch vpipe tensor asynchronous swap feature pytorch pytorch currently synchronize swap tensor implementation thread swap moreover accelerate tensor swap cpu memory gpu memory vpipe tensor swap cpu memory pin memory technical pytorch cpu memory gpu memory faster originate pin lock memory vpipe pin memory pytorch cpu tensor storage vpipe recompute leverage pytorch checkpoint library  library recomputing activation implementation obstacle demand recompute training statement runtime vpipe python  feature exec stmt statement input executes statement modify stage execution statement runtime demand recompute layer activation layer migration stage stage dnn dynamic vpipe maintains dnn stage structure graph data parser switch graph description dnns pytorch imperative statement exec stmt layer migration happens target stage vpipe modifies graph description initializes correspond layer module pytorch overwrite layer migrate layer layer stage execution statement source stage vpipe remove layer stage execution statement delete layer gpu memory vpipe stage layer NAS pipeline parallelism implement NAS pipedream GPipe official description evolve transformer AmoebaNet overall component NAS evolution algorithm iteratively explores dnn architecture runtime switch training workload accord dnn generate evolution algorithm evolution algorithm dnn switch occurs NAS implementation  layer exist dnn activates layer reset parameter dnn switch implementation leverage pytorch imperative feature exec stmt switch dnns without extra initialization evaluation testbed evaluation conduct gpu host host nvidia TI gpus cpu core GB ram gpu GB physical memory host pcie data transfer bandwidth MB host gbps ethernet average ping latency workload evaluate dnn model widely community bert transformer AmoebaNet GNMT dnns pipeline parallelism transformer AmoebaNet typical workload apply neural architecture source release model model prevalent dnns evaluate exist pipeline parallel pipedream GPipe  PipeMare model SVT  LM evaluate surpass dnns evaluate longer prevalent evaluate datasets WMT nlp imagenet vision baseline integrate vpipe baseline notable asp pipeline parallel pipedream notable BSP pipeline parallel GPipe pipedream source release GPipe implement GPipe apply synchronization barrier pipedream codebase GPipe official release pytorch integration vpipe loc baseline pipedream pipedream vpipe pipedream integrate vpipe throughput pipedream vpipe pipedream alone vpipe improvement pipedream overall evaluate pipedream vpipe GPipe vpipe pipedream GPipe successive  PipeMare mitigate pipedream parameter staleness however performance model pipedream GPipe batch training setup training batch dnn batch without exceed gpu physical memory limit pipedream directly activation tensor gpu memory avoid exceed gpu memory limit stage training batch pipedream evaluate GPipe without specification evaluate gpus default partition static partition profiler pipedream explicitly describes partition scheme training varied gpus default layer partition pipedream static partition profiler rate adam optimizer metric epoch per throughput epoch dnn training traverse dataset data item per throughput model epoch define ideal throughput training throughput suppose gpus unlimited physical memory define stage partition dnn model seamlessly remain balance previous implement ideal throughput directly reuse gpu memory memory exception trigger alu utilization usage gpu ALUs gpu memory utilization gpu pcie utilization gpu memory usage pcie bandwidth usage specifically gpu alu utilization effective alu utilization distinguish effective alu utilization contributes training waste alu utilization recompute evaluation focus vpipe efficiency static dnn training baseline vpipe scalability baseline vpipe efficiency dynamic dnn training baseline effective vpipe runtime algorithm protocol limitation vpipe static dnn training NAS disabled overview vpipe improve pipedream GPipe training dnns training curve indicates model training improves training increase overall training epoch vpipe shorten training GPipe pipedream percent average within training vpipe GPipe pipedream achieve model fitting quality model fitting versus training model gpus model training GPipe GPipe vpipe model training pipedream pipedream vpipe bert metric prediction accuracy transformer GNMT metric bleu AmoebaNet vgg resnet metric accuracy throughput comparable evaluation pipedream GPipe training dnns bert transformer AmoebaNet GNMT vpipe improve GPipe pipedream throughput percent understand vpipe improvement GPipe pipedream runtime statistic gpus per gpu memory usage alu utilization training transformer throughput gpu resource usage gpu training NAS disabled transformer pipedream pipedream vpipe pipedream vpipe SR gpus unfilled waste gpu alu utilization recompute resource usage gpu training NAS disabled transformer GPipe GPipe vpipe GPipe vpipe SR gpus unfilled waste gpu alu utilization recompute vpipe improve pipedream baseline training complex dnns pipedream stage easily gpu memory limit stage activation tensor rear stage pipedream default partition transformer stage consume average 3GB gpu memory  activation tensor almost memory limit 1GB gpu stage consume 8GB gpu memory gpu capacity training transformer gpus pipedream batch moderate batch fail fully utilize gpu alu gpus alu utilization percent pipedream training complex dnns pipedream vpipe batch incur effective alu utilization accelerate pipedream vpipe alleviate memory burden stage swap recompute rebalanced stage repartition vpipe swap recompute operation stage reduce memory burden however stage incur computation overhead reduce memory stage longer execution execution stage unbalanced vpipe enable swap recompute optimization local stage pipedream vpipe SR denote SR although stage alu utilization percent percent stage incur alu utilization percent pipeline balance vpipe algorithm vpipe iteratively perform stage repartition migrate layer stage rear stage stage alu utilization percent improve pipeline throughput vpipe optimization GPipe GPipe overhead extra pas extra pas percent waste computation various dnns training setting training complex dnns gpus GPipe achieve training efficiency pipedream although GPipe extra pas pipedream GPipe training batch incur effective alu utilization gpus vpipe improvement pipedream GPipe GPipe vpipe percent waste gpu alu utilization GPipe vpipe invoked swap dynamic efficient strategy reduce GPipe recompute overhead runtime algorithm exchange GPipe vpipe pcie resource GPipe swap pcie resource usually spare GPipe default network communication invoked vpipe tackle pcie interference swap network communication moreover NAS enable vpipe improve GPipe percent discus training dnns vgg resnet vpipe improve pipedream GPipe merely percent average vgg resnet pipedream partition vgg resnet stage stage convolution layer stage fully layer gpus perform data parallelism former stage gpu latter stage limited optimization vpipe SRP algorithm evaluate ideal throughput GPipe pipedream pipedream vpipe GPipe vpipe incur degradation ideal throughput due limit gpu memory capacity pcie bandwidth sufficient batch gpu alu fully utilized vpipe incur inevitable recompute overhead stage avoid exceed gpu physical memory limit gpu utilization vpipe percent inevitable waste alu utilization average recompute overall vpipe accelerate pipedream GPipe various complex dnns static training setting vpipe improvement stem utilization rate gpu resource effective alu utilization memory pcie usage scalability evaluate vpipe scalable gpu cluster pipedream vpipe GPipe vpipe pipedream GPipe gpu addition alternative approach apply dynamic swap recompute  distribute setting integrate  worker data parallelism evaluate  data parallelism parameter server gpus pipeline parallelism motivation gpu cluster dnns dnn layer proportional involve gpus dnns gpu layer dnns gpu effective utilization gpus evaluate scalability scalability DP pure data parallelism DP data parallelism  pipedream achieve scalability pipeline parallelism simultaneously inject input batch proportional gpu stage pipedream directly activation tensor gpu memory increase gpu activation tensor gpu fix memory increase avoid exceed gpu memory limit pipedream proportionally decrease input batch training transformer gpus batch pipedream training transformer gpus batch pipedream training batch gpu alu utilization however setting batch pipedream fully utilize gpu alu therefore gpus involve pipedream effective alu utilization increase training AmoebaNet batch training gpus parallel utilization ALUs gpus significantly pipedream pipedream vpipe GPipe vpipe GPipe suffer batch degradation gpus involve GPipe recompute strategy without activation tensor gpu memory sufficiently batch fully utilize gpu alu vpipe integrate pipedream vpipe GPipe vpipe batch GPipe vpipe reduce recompute overhead GPipe pipedream vpipe GPipe vpipe scalable GPipe achieve effective utilization GPipe vanilla data parallelism DP  data parallelism DP scalability complex dnns network communication parameter synchronization bottleneck however DP incur effective alu utilization  swap recompute enlarge training batch gpu alu utilization gpu worker sum vpipe BSP GPipe vpipe asp pipedream vpipe achieve almost linear scalability comparable scalable pipeline parallelism GPipe vpipe achieve effective gpu utilization vpipe efficient scalable emergence giant dnns foreseen vpipe remain efficient gpus involve dynamic dnn training NAS enable evaluate vpipe efficiency dynamic training workload conduct vpipe perform neural architecture NAS prevalent dynamic training model transformer AmoebaNet pervasively neural architecture transformer AmoebaNet implement NAS accord publish description evolution algorithm creates population dnn model architecture subset around data entry dataset eliminate unqualified model elimination NAS ensure evaluation evolution algorithm deterministic NAS population model sequence overall vpipe accelerate GPipe pipedream NAS enable dnn training percent percent vpipe impact upper evolutionary algorithm downgrade quality NAS snippet NAS enable model transformer AmoebaNet training baseline pipedream GPipe vpipe improve NAS enable model training layer twice stage layer delete twice stage layer delete twice stage layer twice stage training profile dynamic training evolve transformer SR vpipe swap recompute enable repartition disabled sub training throughput input batch layer stage layer increase layer decrease resource utilization gpus sub axis training profile dynamic training evolve transformer SR vpipe swap recompute enable repartition disabled sub training throughput input batch layer stage layer increase layer decrease resource utilization gpus sub axis training profile dynamic training AmoebaNet vanilla baseline without vpipe pipedream GPipe static partition strategy cope training dynamicity transformer layer stage incur performance execution stage suddenly increase  pipeline layer delete stage pipeline throughput increase stage throughput bottleneck although alu utilization stage stage incur alu utilization stage execution stage vpipe local swap recompute optimization algorithm stage vpipe SR enable although vpipe SR improve baseline throughput enlarge batch pipedream reduce recompute overhead GPipe vpipe SR cope training dynamicity implies exist gpu swap recompute  sufficient achieve efficient pipeline parallelism fold distribute memory management distribute swap recompute vpipe SR exists incurs sub optimal training efficiency contrast vpipe implementation algorithm integrate pipedream GPipe training dynamicity pipedream vpipe GPipe vpipe adjust layer distribution stage achieve optimal training throughput sub vpipe adjust layer distribution layer activation activation suddenly trigger training layer stage vpipe global planner runtime statistic stage imbalance execution stage vpipe trigger algorithm generate balance partition vpipe layer manager immediately migrate layer stage  stage stage vpipe layer manager locally perform algorithm optimize local memory management described algorithm vpipe iteratively perform algorithm algorithm SRP strategy evaluation iterative algorithm within iteration without performance downgrade thanks vpipe SRP algorithm layer migration protocol discus evaluate ideal throughput vpipe incur degradation ideal throughput sum vpipe pipedream vpipe GPipe vpipe transparently layer distribution along training dynamicity training throughput ideal throughput extremely dynamic training backward layer migration trigger frequently NAS training vpipe backward layer migration desirable effectiveness vpipe algorithm effectiveness vpipe SRP algorithm vpipe SRP algorithm algorithm decomposition iteratively optimizes sub local swap recompute algorithm global stage partition algorithm summarize vpipe SRP algorithm improve baseline static training dynamic training vpipe training throughput pipedream GPipe ideal throughput vpipe throughput degradation ideal throughput inevitable recompute overhead gpu effective alu utilization bare baseline pipedream GPipe vpipe SRP algorithm essentially utilized available resource gpus examine vpipe SRP algorithm overall invoke SRP algorithm within iteration iteration graph partition sub algorithm algorithm solves NP graph partition runtime vpipe partition algorithm algorithm KL algorithm partition complex dnns vpipe KL algorithm vpipe coarsen greatly reduce complexity graph partition average vpipe reduce graph node graph negligible training network communication across partition vpipe KL algorithm vpipe KL refinement ensure partition graph performance vpipe partition algorithm versus kernighan lin algorithm network communication pipedream vpipe pipedream overall pipedream vpipe achieve comparable network communication pipedream training complex dnns vpipe layer migration message incur overhead amortize training layer migration vpipe peak data transfer rate MB network connection pcie connection across stage network usage pipedream without vpipe vpipe network usage contains vpipe network overhead unfilled layer migration message gpu alu utilization statistic vpipe migration non migration approach sum vpipe SRP algorithm converge achieve optimal utilizes gpu resource achieve efficient pipeline parallel training effectiveness layer migration protocol vpipe layer migration protocol transparently migrates layer realize partition without degrade training throughput guarantee vpipe iteratively SRP negligible training performance penalty examine necessity vpipe layer migration protocol non layer migration approach inject input batch upper pipeline manually migrate layer stage reboot pipeline dash training throughput non layer migration non migration degrade training throughput percent iteration vpipe algorithm repartition trigger pipeline alu utilization comparison vpipe migration approach non migration approach iterative algorithm trigger stage repartition repartition alu utilization zero pipeline comparison vpipe migrate layer without notable throughput degradation gpu stall discussion vpipe limitation vpipe assumes dnn workload vpipe layer within memory limit gpu assume pipeline parallel pipedream GPipe reality recent complex dnns evaluate vpipe layer gpu vpipe layer migration protocol remains transfer layer tensor overlap computation dnn training exist dnns execution layer extremely layer non negligible amount data transfer model literature dnns computation intensive memory intensive vpipe critical data transfer realizable verify future envision application vpipe vpipe unique strength dynamic training paradigm dynet NAS dynet enable dynamic dnns lstm prevalent powerful handle input data exist NAS algorithm dnn  assumption gpu memory unlimited however NAS algorithm deployed pipeline parallelism dnn  cannot realize pipeline parallelism quality leverage vpipe pipeline statistic researcher NAS algorithm aware underlie pipeline resource NAS highly accurate feasible limited hardware resource dnns deployed various training framework addition pytorch vpipe augment imperative training mxnet tensorflow related data parallel data parallelism widely adopt dnn training batch training data parallelism input partition across worker worker maintains local model parameter partition input periodically synchronize worker typical data parallelism assume dnn model gpu nevertheless recent dnns grown beyond gpu capacity researcher conduct model parallelism dnn training data parallelism  partition dnn status parameter optimizers worker demand transfer status training  report network communication volume typical data parallel parameter server data parallelism pipeline parallelism vpipe incurs network communication volume scalability dnn training overall data parallelism complementary pipeline parallelism integrate vpipe mixed parallelism batch training pipeline parallel pipeline model parallelism model parallel model parallel complex dnn model cannot gpu memory despite pipedream GPipe successive pipeline parallel address pipedream parameter staleness  parameter prediction mitigate staleness issue incur asp pipeline parallel pipedream  directly activation memory gpu performance model pipedream PipeMare adopts GPipe recompute strategy asp model GPipe performance memory however PipeMare limitation GPipe hybrid parallel exist pipeline parallel assume gpu resource consumption layer roughly evenly distribute recent dnns transformer bert GPT AmoebaNet dnn layer usually homogenous training resource consumption nevertheless dnns resnet vgg convolution layer usually computation fully layer hybrid parallelism   etc improve training efficiency heterogenous dnns specifically apply data parallelism convolution layer apply model parallelism fully layer orthogonal vpipe hybrid parallelism vpipe future training memory reduction dnn training memory intensive training memory reduction widely exist exist memory reduction approach mainly category transparent approach swap recompute affect training accuracy opaque approach precision training mixed precision training training accuracy training memory vpipe aim transparent layer vpipe memory reduction affect upper opaque memory reduction approach orthogonal vpipe transparent memory reduction gpu training vDNN  focus swap   coherently combine swap recompute dynamically reduce memory consumption dnn training gpu however gpu cope challenge stem pipeline parallelism recent partially offloads recompute overhead cpu processor complementary vpipe integrate vpipe reduce recompute overhead nvidia proposes unified memory unified memory address accessible cpu gpu allocate memory gpu physical capacity nvidia zero allows integrate gpu gpu cpu physically memory device mobile device directly access pin memory cpu vpipe focus discrete gpus gpu memory device data training exceeds gpu physical capacity unified memory automatically migrates tensor activation gpu cpu tensor access later gpu ALUs unified memory fault trigger tensor synchronously cpu gpu per host demand significantly application execution unified memory dnn execution unified memory vpipe distribute runtime enables vpipe predict tensor cpu asynchronously pre fetch tensor gpu access prevents normal execution vpipe async swap overall negligible overhead training performance besides swap vpipe distribute memory management contains feature recompute migrate conclusion vpipe dynamic memory layer partition management pipelined parallelism virtualized layer typical pipeline parallel underlie execution vpipe accelerate exist pipeline parallel static dynamic training complex dnns efficient scalable vpipe source code release github com hku vpipe