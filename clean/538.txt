accelerate training DL model cluster machine equip hardware accelerator gpus leveraged reduce execution resource manager increase gpu utilization maximize throughput DL gpu effective incur interference slowdown article propose horus interference aware prediction resource manager DL horus proactively predicts gpu utilization heterogeneous DL extrapolate DL model computation graph feature remove online profile isolated reserve gpus micro benchmark location combination across heterogeneous gpu hardware identify gpu utilization proxy metric placement decision contrast approach reserve isolated gpus perform online profile directly gpu utilization unique submit approach promotes resource utilization makespan reduction via experimentation trace driven simulation demonstrate horus outperforms DL resource manager percent gpu resource utilization percent makespan reduction percent reduction introduction DL increasingly important machine impact innovation DL architecture growth data volume increase practitioner demand establishment cluster machine equip computer accelerator graphical processor gpus DL comprise distribute leveraged enable vast amount computation throughput reduce model training provider deploy execute DL workload encapsulate DL provision resource service model important goal DL ability satisfy service agreement SLA quality service qos criterion resource efficient manner effort ensure SLA qos guarantee challenged due gpu utilization due exist resource manager kubernetes yarn prohibit explicit gpu DL assign gpu utilization decrease performance resource efficiency service availability incur longer queue additional gpu device satisfy demand ability DL execute gpu identify address utilization effectiveness location understand DL workload gpu utilization provider enables quality DL schedule location decision reduce gpu resource utilization consumer allows insight potential gpu understand exploit DL workload utilization improve location critical resource efficient DL however establish approach characterize gpu utilization DL workload leverage online profile execution online profile entail execute unique DL isolated gpu dedicate machine ensure accurate metric collection online profile reduce service availability resource efficiency due reserve gpu device increase model architecture configuration whilst location improve gpu utilization incur performance interference refer interference average DL slowdown percent location combination DL resource manager exist location attention paid actively address interference DL gpu placement decision DL placement makespan increase completion JCT eviction failure gpu memory  error horus prediction interference aware resource manager DL contrast exist approach horus proactively predicts gpu utilization unseen DL model feature exploit scheduler suitable DL location combination minimize interference approach avoids profile kernel modification underlie DL framework extensive online profile execution isolated gpu scheduler runtime expensive consume specific research contribution characterization DL workload interference location characterize interference profile unique combination DL across heterogeneous gpu hardware architecture finding demonstrate DL location interference slowdown comparable network locality distribute training gpu utilization analysis prediction DL workload series benchmark analyze identify DL model feature relationship gpu utilization float operation per FLOPs input data DL computation graph structure convolution layer propose prediction allows sub DL gpu utilization prediction without online profile interference aware DL resource manager exploit prediction propose interference aware resource manager location minimize gpu commitment approach alternative schedule algorithm prioritize minimize makespan improve fairness avoid starvation lower median expense marginal degradation makespan utilization resource manager integrate kubernetes deployed within DL cluster evaluate via trace driven simulation production DL cluster demonstrate approach achieves percent increase gpu cluster utilization percent makespan reduction exist approach expand upon previous increase scope DL workload characterization model capture additional gpu architecture location profile analysis model improve gpu prediction model accuracy evaluate horus via trace driven simulation production cluster horus framework redesign refine queue schedule algorithm minimize objective finally evaluation conduct additional workload composition additional location algorithm comparison structure research background characterization respectively outline implementation horus discus setup related conclusion motivation background DL neural network dnn acyclic graph dag computation graph execution graph node operation layer combination layer parameter information access predecessor successor model parameter float hence model execution float operation increase requirement gpu device memory important recent research demonstrate increase dnn model depth width improve accuracy dnns frequently execute gpus due performance capability perform matrix multiplication core operation express computation memory kernel gpus hence DL model layer kernel gpu load driven FLOPS intermediate output activation network DL cluster machine accelerator predominately gpus employ execute DL workload user submit workload DL various configuration batch model dataset allocate onto machine via resource manager recent production DL identify challenge gpu utilization reflect average gpu utilization percent queue DL due corroborate finding conduct analysis trace production DL schedule machine gpu cluster global commerce gpu utilization percent JCT average percent around respectively utilization JCT production DL cluster primary utilization reliance traditional non preemptive scheduler DL exclusive access gpu device problematic due negative impact throughput availability resource efficiency exist approach demonstrate positive increase DL gpu utilization enable location DL gpu effectiveness location dependent inter related concept accurate gpu profile minimize interference gpu profile ascertain gpu utilization non trivial calculate context gpu utilization define percentage sample interval kernel execute gpu important measurement actual utilization processing core chip float integer tensor relates byte device memory cache however estimate amount load gpu within measurement profile perform metric related DL isolated gpu machine profile categorize coarse grain profile obtains kernel kernel configuration gpu memory utilization kernel execution usually grain profile access hardware performance counter achieve occupancy byte throughput dram kernel whilst accurate coarse grain profile intensive longer metric workload complexity whilst gpu profile exist DL resource manager location decision location incurs performance degradation interference interference phenomenon multiple compete limited resource machine gpu interference occurs specifically limited processing memory queue delay kernel kernel launch gpu kernel scheduler policy robin fashion interference DL percent JCT degradation initial location nvidia geforce gtx gpu depict resnet vgg model JCT slowdown various model slowdown problematic DL perform model training hence DL fully exploit location maximize resource utilization minimize makespan DL resource manager interference perform DL location placement DL interference nvidia gtx cifar dataset DL utilization interference exist gpu DL resource manager alleviate interference profile kernel characteristic gpu utilization runtime orchestrate kernel schedule opportunistically however profile DL kernel runtime infer interference suitable performance profile extend DL training moreover profile perform DL model submit additional overhead gpu utilization correlate load gpu therefore investigate relationship gpu utilization interference imperative understand DL model configuration affect gpu utilization exploit information ascertain location profile minimal interference particularly conduct relationship address leverage gpu utilization proxy metric estimate interference JCT slowdown without grain profile exploit DL characteristic extract useful information predict gpu utilization location relationship profile setup environment micro benchmark conduct DL described leverage establish literature DL model profile conduct isolated gpus combination DL within gpu micro benchmark multiple ensure metric consistency nvidia container runtime cuda toolkit pytorch DL framework micro benchmark hardware setup dnn model variety representative DL prominent computer vision CV model processing nlp custom fully FC model architecture encompass convolution neural network cnns recurrent neural network rnns comparable prior model architecture refine configuration mini batch hidden dimension layer model permutation within memory constraint gpu device unique configuration profile isolation location combination model capacity encounter  analyze DL model analyze DL model overview DL workload gpu utilization difference nvidia RTX overview DL workload gpu utilization difference nvidia RTX metric understand impact interference extract metric gpu utilization completion kernel access metric nvidia smi nvml golang binding nvidia  impact interference analyse correspond JCT slowdown execution isolated execution JCT slowdown      SourceRight click MathML additional feature  DL fix epoch  DL execute isolation relationship gpu util JCT slowdown response DL gpu utilization JCT slowdown due significant resource contention schedule kernel intuitive gpu utilization driven kernel engage gpu processing memory combination increase gpu commitment cumulative gpu utilization requirement percent JCT increase contrast DL individually percent utilization likely exhibit severe performance degradation increase JCT correlation increase commit utilization increase JCT suggests utilization proxy metric interference without grain profile individual DL kernel leverage gpu utilization DL proxy load scenario likely performance degradation approximate linear relationship accumulative gpu utilization resultant JCT slowdown gpu commitment manifest comparison gpu commitment non linear relationship quadratic polynomial data difference nvidia nvidia RTX respectively additionally investigate impact hardware heterogeneity JCT slowdown reveals average interference severity identical DL nvidia RTX architecture nvidia due additional processing increase cache memory bandwidth coefficient relationship heterogeneous hardware relationship FLOPs gpu utilization response investigate dnn computation graph illustrates positive correlation FLOPs gpu utilization dnn model parameter activation computation memory kernel launch gpu device increase gpu load gpu utilization colocation feature reality matrix memory transaction increase batch increase due FLOPs memory transaction correlate within batch input batch dnn input hence dnn computation graph extract meaningful information quantitatively gpu utilization accurately infer suitable location scheme reduce performance interference cluster propose approach horus horus prediction interference aware DL resource manager component deployed exist cluster resource manager framework kubernetes depicts horus architecture comprises component prediction metric repository application controller upon submission application controller sends request prediction estimate DL gpu usage gpu utilization gpu memory utilization inspect workload definition specifically prediction access dnn graph model pth file cluster maintain infrastructure update monitoring agent infrastructure data node gpu usage usage host memory usage cpu utilization agent deployed individual node reporting application utilization metric eventually metric repository horus architecture gpu utilization prediction location scheduler deployed within DL resource manager scheduler assigns DL gpus compute suitability minimize function objective location cached cluster approach aim maximize gpu utilization minimize makespan via prioritize location placement decision JCT slowdown severe interference communication delay prediction estimate gpu utilization overview prediction extract DL workload feature described iterate neural network exchange onnx graph representation DL model obtain aggregate feature FLOPs iterate operator calculate input output parameter feature normalize numerical input machine model predict gpu utilization  prediction model offline training stage historical DL workload profile micro benchmark exist prediction approach profile nominally acquire via developer micro benchmark monitoring exist non DL workload isolated gpus critically successful prediction model training isolated profile unique DL workload worth approach combine reactive approach machine model periodically retrain additional profile model discover onnx model feature feature importance understand contributes towards gpu utilization investigate regressor feature extract average across feature indicator exist literature model compression neural architecture reduce parameter intermediate activation computation memory consumption hardware surprisingly convolution remain feature minimal impact regressor leverage compiler intermediate representation hardware feature characteristic important identify regressor feature important identify regressor feature model evaluation model accuracy via regressor error  establish regression accuracy prediction error enlarge approach useful utilization prediction whilst overestimate gpu utilization ideal maximize resource efficiency preferable underestimation unintended gpu allocation interference attempt avoid prediction model achieve relatively  regressors error  gpu utilization prediction estimate gpu memory utilization gpu utilization estimate gpu memory utilization complex memory MiB initialization optimization individual DL library without kernel implementation estimate minimum memory usage byte factor backward batch data activation gradient parameter addition initialization overhead overall estimate memory requirement DL    sourcethe estimate gpu utilization  gpu memory  node capacity scheduler tackle incoming interference aware schedule  placement strategy monitor application throughput migrate another node randomly upon slowdown detection undefined threshold approach random migration allocate another incompatible performance slowdown  enables location monitoring DL employ local coordinator modify underlie DL framework grain DL kernel inject idle gpu alleviate interference approach however understand profile kernel execution runtime appropriate idle core interference aware schedule understand compute resource requirement prior execution perform placement amount correspond resource contrast exist DL scheduler react obtain workload utilization schedule formulation objective placement onto cluster node gpu capacity minimizes context decision variable  node gpu allocate decision  denotes variable placement optimization therefore define integer linear program ILP min   source   SourceRight click MathML additional feature     SourceRight click MathML additional feature  SourceRight click MathML additional feature constraint ensure gpu request satisfied constraint sum resource cpu memory gpu memory request node within bound node resource constraint aim minimize involve overall gpu allocation clarity notation summarize notation definition notation definition breakdown accurately capture incur impact gpu location onto DL performance overall independent portion gpu memory usage gpu utilization increase     customize indicates performance impact equally default gpu memory usage  error JCT slowdown gpu memory  inherently refer proportion gpu memory usage    sourcewhere  fix gpu memory gpu device  estimate gpu memory usage  gpu memory within  due relationship increase gpu utilization DL JCT slowdown hardware outline penalize combination DL commitment manifest specifically function JCT slowdown cumulative gpu utilization gpu device function instance function target device cumulative gpu utilization commit cumulative gpu utilization surpasses percent hence gpu express      sourcewhere  estimate gpu utilization onto node gpu    source function JCT slowdown gpu utilization directly outcome function estimate packed onto gpu device therefore schedule probability node inversely correlate JCT slowdown estimate ILP NP due heterogeneity resource requirement modify algorithm leveraged greedily schedule algorithm queue schedule input pending cluster queue buffer schedule cluster multi tiered queue pending queue via queue empty schedule buffer via fairness cluster  resource capacity CPUs mem gpu mem filter node passing capacity    calculate onto gpus node shortlist collection gpus min ascend resource allocation  runtime schedule queue JCTs substantially critical avoid resource starvation particularly incur resource request schedule manner borrow cluster individually manage schedule fairly queue primarily queue assign suitable gpu resource launch gpu cluster algorithm outline algorithm detail cluster actually schedule cluster procedure specifically distance metric identify feature task gpu utilization predict gpu per task gpu memory estimate feature outline per resource requirement obtain adopt algorithm execute identify correspond queue utilization distinct CDFs queue presumably batch schedule horus pending queue accord queue pending queue longer queue median per queue queue max len med len max operation guarantee non zero median zero arrival median statistical affected skewed data accurately reflect queue eventually picked calculate  actively avoid starvation whenever starve increase reservation starve queue algorithm resource allocation pending accord fairness scheduler effort allocate available resource  minimize performance interference specifically resource capacity node satisfy requirement cpu memory gpu memory gpu memory requirement infer gpus gpus per node calculate minimal node calculate schedule onto gpu node gpus minimal resource allocation schedule consideration discussion failover reschedule approach DL resource manager encounter issue associate  error due DL exceed gpu memory capacity stem incorrect memory requirement estimation address issue thread monitor progress failure  onto schedule queue prior scheduler update DL request gpu memory requirement gpu memory memory available previously periodic infrastructure profile locality calibration primarily tackle JCT slowdown due interference stem location optimize distribute training focus placement scheme assumes connection across node hence data transfer training dominate factor algorithm gpu interference aware schedule suitable frequent transfer gradient parameter gpus model algorithm frame integrate locality placement gpus node rack prioritize gpu filter reduce amount data transfer timing constraint horus factor multiple queue selection however DL cluster manager timing constraint completion placement planning phase DL convergence rate non linear depends hardware software parameter correlate iteration normally DL cluster manager cannot rely remain execution generic algorithm shortest SJF shortest remain  etc optimization formulation timing constraint exist schedule approach particularly hpc grid compute estimation execution rely assumption workload pre periodic datasets hyperparameter model architecture assumption DL cluster due constant model evaluation datasets constraint however beyond scope implementation horus application controller approximately code prediction python operates within DL kubernetes prediction pod prediction application controller communicate via remote procedure rpc leverage grpc library underlie rpc implementation perform data serialization serialization data transfer scheduler request predict information upon submission worth approach modification underlie DL library tensorflow pytorch monitoring monitoring application aware optimization obtain grain infrastructure horus leverage  container monitoring framework infrastructure information aggregate centralize series database application controller query decision historical usage fault tolerance network file nfs DL training due amount training data memory limitation addition efficient retrieval training data checkpoint file miscellaneous file persist across node nfs allows DL recovery failure importantly enables preemption due gpu commitment horus commitment threshold configure device memory usage DL operator apart failure straggler cluster elastic training regime practical address issue however core focus setup hardware software horus deployed onto gpu cluster node nvidia gpus amd ryzen core processor thread per core ethernet network GB ddr memory node instal ubuntu disco nvidia driver version DL library cuda toolkits responsible DL instantiation execution package container cluster kubernetes due prominence distribute community   configure extract data interval respectively initial trial parameter effective throughput cluster configuration simulation node cluster node gpus cpu core GB memory comparable exist methodology evaluate horus production trace another collaborator trace driven simulation highlight testbed cluster horus reduces makespan percent increase average cluster gpu utilization percent comparison fifo opportunistic bin pack performance aware bin pack trace driven simulation horus performance approach outperforms schedule approach makespan cluster gpu utilization average comparative algorithm evaluate horus schedule algorithm described implement additional schedule algorithm comparison fifo emulate slot approach establish data cluster scheduler kubernetes yarn fifo assigns incoming DL onto idle gpu without location performance aware bin pack  leverage technique  schedule DL leverage characteristic detect iteration slowdown scheduler difference average per versus previous placement performance percent simply queue DL achieve stable resource opportunistic bin pack  assigns DL available information gpu memory availability via memory estimation model described submission gpu memory available estimate memory requirement scheduler opportunistically schedule gpu load approach testbed cluster conduct horus horus significant difference instead improve trace driven simulation workload conduct mixture DL generate DL model configuration model transformer horus expose percent DL predictor training model datasets leveraged establish micro benchmarking DL cluster scheduler algorithm evaluate workload submission distribution DL derive production characteristic characterize percent gpu utilization JCT terminate specify epoch emulate JCT production percent production DL gpu hence focus DL gpu training objective workload makespan JCT due interference DL location locality focus prior DL cluster scheduler introduces JCT heterogeneity fairly potential gain resource utilization JCT increase DL algorithm schedule DL workload successfully training DL equivalent approximately continuous DL cluster execution horus configure buffer demonstrate throughput evaluate fairness conduct fairness production trace driven simulation metric algorithm effectiveness metric cluster gpu resource utilization average aggregate gpu utilization gpus completion JCT completion DL commence execution completion workload makespan span DL queue completion average arrival schedule scheduler testbed cluster JCT comparison average JCT schedule approach fifo achieves JCT due exclusive gpu access hence interference contrast location algorithm JCT slowdown percent fifo respectively location approach suffer performance degradation proportion allows frequent varied location within gpus oppose longer heavier portion entire gpu although fifo achieve average JCT makespan gpu utilization due longer queue completion JCT testbed cluster makespan horus successfully schedule DL makespan across respectively equivalent percent improvement fifo percent improvement    makespan outperform  due latter algorithm incur additional overhead performance impact initial location decision location gain effective workload diverse utilization effectiveness location algorithm particularly  horus therefore slightly DL cluster makespan statistic utilization horus achieve overall cluster resource utilization reflect average percent gpu utilization horus  DL cluster resource utilization percent due generate DL epoch exhibit gpu utilization omit behavior cluster resource usage  horus algorithm increase percent respectively   achieve utilization fifo due ability perform location  achieve utilization rapid schedule cycle contrast  incurs additional schedule profile schedule stable performance twait twait stability interestingly horus ability effectively achieve DL throughput gpu utilization  expose interference consequent JCT slowdown horus however achieve JCT comparison  gain resource utilization makespan acceptable average cluster gpu util testbed cluster trace driven simulation demonstrate scalability evaluate horus performance simulation production trace derive production trace simulation capture gpus allocate execution DL execution execute schedule algorithm  fifo horus horus configure identically testbed simulator capture precise kernel characteristic internal progress  assume interference overhead linearly sum gpu utilization simulation super trace duration improvement testbed horus approach utilize cluster gpu makespan schedule decision horus horus almost makespan utilization however horus approximately percent median horus desirable fairness multiple tenant category production cluster multiple tenant overall horus schedule approach utilizes expensive gpus effectively research cluster enable faster around increase productivity resource efficiency impact queue conduct sensitivity analysis examine horus sensitivity configurable queue evaluate horus various simulation average queue significantly affect horus reduce percent however makespan performance degrade slightly queue increase fairness throughput acceptable DL cluster summary trace driven DL cluster simulation related understand achieve resource utilization heterogeneous workload DL compute important topic gpu profile exist DL profile workload improve resource efficiency metric training progress communication kernel schedule inference execution gpu utilization profile  focus leverage online profile isolated machine suitable location migration strategy perform online profile machine isolation harvest utilized resource leverage virtualized gpu metric vcpu isolation propose approach predict slowdown DL workload obtain DL workload infrastructure feature suitable training regime  leverage gpu utilization identify maybe suitable location predict training via model feature device feature profile per layer execution interference aware resource manager gpu interference establish research various propose mitigate kernel interference gpu kernel schedule gpu resource scheduler operates gpu device driver application framework hence cannot effectively orchestrate optimize global cluster propose profile gpu hardware utilization insufficient DL typically pre processing data mini batch dfs various cluster scheduler reduce performance interference heterogeneous workload environment effectively handle DL cluster schedule addition locality driver DL performance difference hardware architecture workload queue DL specific cluster scheduler DL resource manager  location enable DL resource manager focus improve introduce context switch mechanism DL tiresias focus improve average JCT starvation profile network latency consolidate distribute DL implement multi feedback queue adjusts priority  implement performance predictor model runtime adjusts parameter server worker assumes convergence predictable ascertain recently propose DL resource manager  introduces modification underlie DL framework grain kernel schedule location alleviate interference however profile runtime DL resource manager complimentary focus address various challenge schedule objective horus upon prior focus improve DL overall makespan gpu utilization automatically predict gpu utilization estimate memory requirement without manually specify placement decision complement interference aware resource manager conclusion horus prediction interference aware resource manager DL achieves throughput increase resource efficiency horus avoids lengthy online profile dedicate gpus exist approach predict gpu utilization computation graph feature extract dnn offline resource predictor approach modification DL library expensive kernel profile scheduler analysis interference DL average JCT slowdown percent comparable latency increase due associate distribute horus currently integrate kubernetes suitable integration exist DL resource manager demonstrate horus capable reduce makespan percent achieve cluster utilization percent considerable increase DL resource efficiency horus lower median avoids starvation queue desirable fairness multiple tenant