expand datacenters necessitates automate resource provision requirement latency efficiency however due dynamic various user demand efficient resource allocation challenge exist resource allocation cannot effectively handle dynamic environment prior knowledge excessive consumption degrade quality service qos address propose adaptive efficient resource allocation scheme actor critic reinforcement DRL actor  policy allocate resource chooses action schedule assess critic evaluate action resource allocation policy update gradient ascent variance policy gradient reduce advantage function improves training efficiency propose conduct extensive simulation data google datacenters obtain superior qos latency dismiss rate enhance efficiency advanced DRL classic resource allocation introduction compute rapidly developed prevail compute paradigm compute resource allocation regard allocate compute storage networking resource requirement user service provider CSPs resource allocation emerge increase dynamic datacenters irrational resource provision response degrade quality service qos consumption maintenance overhead therefore priority objective adaptive efficient resource allocation datacenters however highly challenge task due dynamic various user demand compute described complexity datacenters server datacenters various compute storage resource central processing CPUs memory storage therefore challenge manage coordinate heterogeneous resource efficiently compute diversity demand user user demand heterogeneous resource CPUs memory storage duration diversity user demand  difficulty resource allocation datacenters  consumption consumption operation overhead extensive carbon emission google datacenters average cpu utilization server around waste occurs irrational resource allocation scheme however satisfy diverse user demand maintain datacenters efficiency dynamic datacenters resource usage request frequently effective resource allocation continuously requirement user dynamic environment however accurate model resource allocation response dynamic environment therefore dynamic challenge adaptive resource allocation datacenters classic resource allocation heuristic theory although resource allocation extent commonly prior knowledge transition demand consumption develop correspond strategy resource allocation specific application scenario unable fully environment dynamic user demand schedule easily execute strategy meeting user demand however characteristic resource demand duration obtain benefit therefore unable adaptively fulfill dynamic demand user perspective excessive latency serious resource waste due irrational resource allocation besides numerous iteration feasible resource allocation computational complexity resource overhead therefore unable effectively address complicate resource allocation dynamic environment reinforcement RL emerge promising approach handle resource allocation adaptiveness complexity however traditional RL suffer dimensional complex environment address reinforcement DRL propose extract dimensional representation dimensional neural network dnns although DRL focus resource allocation DRL network DQN DQL training efficiency action DRL learns deterministic policy calculate probability action however datacenter constantly action considerably continuously requirement schedule therefore DRL approach optimal policy convergence contrast policy DRL policy gradient PG learns stochastic policy action datacenter directly output action probability distribution reduce training efficiency variance generate estimate policy gradient synergy policy DRL algorithm advantage actor critic AC address issue AC actor chooses action assess critic variance policy gradient reduce advantage function however AC adopts thread training manner  computational resource meanwhile data correlation AC training sample generate DRL agent interact environment unsatisfactory training address AC asynchronous advantage actor critic AC algorithm variance efficiency propose AC multiple DRL agent interact environment simultaneously computational resource improve meanwhile data DRL agent independent AC data correlation AC algorithm advantage develop AC resource allocation scheme datacenters heterogeneous resource diverse user demand consumption dynamic environment contribution summarize unified model resource allocation datacenter dynamic heterogeneous user demand propose model qos latency dismiss rate efficiency average consumption regard optimization goal furthermore action reward function resource allocation define formulate markov decision MDP propose DRL resource allocation scheme actor critic DRL AC resource allocation propose efficiently approach optimal policy schedule datacenter specifically dnns utilized handle dimensional datacenter moreover training efficiency propose greatly improve asynchronous update policy parameter multiple DRL agent extensive simulation trace data google datacenters conduct validate effectiveness propose simulation demonstrate propose achieve qos efficiency faster convergence classic resource allocation algorithm advanced DRL resource allocation organize related introduce describes model resource allocation datacenter propose AC resource allocation detail propose evaluate simulation datasets concludes related resource allocation compute attract research attention contribute important review related aspect classic DRL resource allocation classic approach resource allocation resource allocation omnipresent compute propose enhance resource utilization rational resource provision effective propose CSPs improve qos compliance performance compute hpc probabilistic threshold model accomplish switch operating service johnson genetic algorithm combine multiprocessor schedule datacenters approach aware scheduler improve efficiency meeting constraint pro con baseline schedule algorithm  shortest SJF robin RR furthermore  mishra analyze variant RR algorithm load balance compute heuristic resource reservation meeting user demand reduce resource tetri cluster scheduler pack heuristic task requirement resource availability improve cluster efficiency propose hierarchical resource allocation framework admission mechanism mobile user server execute task response computational adaptive resource management framework meeting qos requirement decision resource allocation execute fuzzy controller iteration cycle utilize feedback theory data mapreduce developed reduce cluster reconfigurations overall focus strategy heuristic theory resource allocation strategy heuristic establish fulfil dynamic user demand datacenters application scope limited overhead setting generate besides theory numerous feedback iteration usually computational complexity unnecessary resource overhead address important challenge DRL emerge adaptive efficient decision complicate resource allocation DRL resource allocation reinforcement DRL combine reinforcement RL dnns emphasizes decision action accord dynamic environment maximize reward milestone witness vigorous development DRL algorithm application network DQN atari platform alpha defeat champion integrate monte carlo MCTS dnns moreover DRL recently apply resource allocation compute instance combine algorithm dnns handle schedule acyclic graph dag task environment DQN algorithm adopt allocate compute intensive reduce consumption datacenters DQN algorithm hierarchical framework adaptive resource allocation aim reduce consumption datacenters adopt DQN algorithm achieve adaptive provision configuration resource DRL algorithm leveraged policy DRL algorithm policy gradient PG handle resource allocation datacenters resource constraint wireless utilized PG optimality resource allocation PG algorithm qos aware scheduler developed aim improve qos schedule dnn inference workload compute moreover PG actor critic approach user schedule resource allocation aim maximize efficiency heterogeneous network besides actor critic DRL resource allocation developed previous optimization latency efficiency moreover training efficiency actor critic DRL improve advantage asynchronous update mechanism depends DRL resource allocation approach accurate optimal policy action although exists amount research policy DRL address variance generate estimate policy gradient besides DRL reveal drawback training efficiency numerous iteration optimize schedule policy address essential propose AC resource allocation datacenters  aim enhance performance AC algorithm leverage gpu computational focus adaptation application AC algorithm resource allocation model unified model resource allocation aim improve qos efficiency dynamic environment datacenters various user demand clarity presentation scenario datacenter server donate indicates server server multiple resource CPUs memory storage donate indicates resource DRL resource controller embed resource allocation RAS RAS generates policy schedule resource request user information datacenter server resource usage consumption accord policy deliver DRL resource controller scheduler assigns sequence server specifically generalize data processing training DL model image processing recognition exhibit various resource request accord purpose therefore consists specific duration request resource CPUs memory resource allocation information collector usage resource consumption agent datacenter refer information DRL resource controller generate policy schedule accordingly notation involve propose model notation propose model model resource allocation datacenter denote  indicates sequence denote  indicates sequence  arrives  available resource immediately otherwise sequence schedule fifo policy  sequence therefore actual completion obtain calculate interval sequence processing denote   illustrates schedule assume server compute cpu resource request cpu resource respectively timesteps timesteps propose model instance timesteps arrival completion server whenever arrives transition occurs specifically cpu resource immediately therefore actual completion duration however cpu resource currently inadequate therefore actual completion longer duration schedule numerical discrepancy latency excessive computation gradient descent therefore normalization improve training convergence algorithm therefore  define normalize average latency normalizes latency successfully average    sourcewhere  duration moreover constraint sequence  denote  avoid qos degrade excessive status therefore  define dismiss rate calculates rate dismiss sequence   sourcewhere  besides consumption generate datacenters commonly depends resource usage feasible reduce consumption enhance metric server tend switch exist server resource usage experimental measurement exist consumption server proportional resource usage consumption datacenter formulate  pmax pmax  SourceRight click MathML additional feature pmax maximum consumption server fully utilized calculate consumption idle server  resource usage server timestep timestep exist regard consumption performance metric efficiency schedule average consumption successfully   source improve qos   efficiency  DRL resource allocation propose execute schedule datacenter specifically regard RAS DRL agent datacenter environment timestep DRL agent chooses action schedule interact environment accordingly action reward function DRL define consists resource usage server resource request timestep  indicates usage resource server timestep usage nth resource server  indicates occupancy request resource timestep occupancy request nth resource  denotes duration timestep therefore datacenter timestep define SVT     sourcewhere SVT     server clarity presentation dimension depends situation server calculate server resource respectively action timestep action adopt scheduler execute sequence accord policy schedule deliver DRL resource controller policy generate scheduler assigns specific server execution schedule appropriate server server automatically allocate correspond resource accord resource request therefore action indicates server define sourcewhere scheduler assign timestep sequence otherwise specific server transition probability matrix matrix indicates probability transition timestep initial item cpu usage server occupancy request duration respectively schedule immediately available resource sufficient action evolves item indicates cpu usage server item occupancy request cpu resource duration similarly action schedule evolves specifically transition probability matrix denote IP indicates probability transit action transition probability obtain DRL algorithm output probability action reward function reward function DRL agent RAS policy schedule discount reward aim improve performance resource allocation therefore timestep reward consist reward qos denote  efficiency denote  define   source specifically  reflect penalty hence negative latency timestep    described define      sourcewhere penalty  negative longer duration tends shorter sensible objective profit maximization longer duration profit moreover  reflect penalty consumption timestep define    sourcewhere  consumption execute timestep penalty optimization resource allocation DRL agent chooses action schedule resource usage resource request environment datacenter DRL agent receives reward qos efficiency illustrate MDP MDP model resource allocation due uncertainty resource allocation formulate model DRL discrete mdps action AC algorithm utilized explore adaptive efficient resource allocation dynamic environment datacenters adaptive efficient resource allocation AC datacenters propose effective resource allocation asynchronous advantage actor critic AC achieve superior qos efficiency datacenters propose adopts actor critic DRL framework asynchronous update AC accelerate training specifically AC incorporates policy DRL algorithm DRL determines function function approximators adopts greedy balance exploration exploitation therefore DRL agent utilizes exist action schedule whilst explore action policy DRL  policy schedule directly output action probability distribution without DRL agent efficiently action action propose AC resource allocation algorithm definition action reward function actor network  critic network  initialize bias actor critic rate TD error discount factor initialize algorithm AC resource allocation datacenters initialize actor network  critic network  bias initialize actor critic rate reward decay rate TD error discount factor counter temp update training epoch initial env action schedule datacenter SVT  define define actor action execute schedule action reward qos efficiency   define env calculate discount reward  calculate advantage function critic     minimize TD error    update action function parameter   calculate policy gradient actor advantage function     update schedule policy  update update counter temp temp temp algorithm asynchronously update policy parameter DRL agent optimization objective propose AC resource allocation obtain reward therefore reward define accumulate probability distribution   sourcewhere  stationary distribution mdps model resource allocation policy  schedule initialization training optimize resource allocation improve optimization objective policy parameter schedule update continuously mdps policy gradient objective function define    source multi mdps reward replace  policy gradient theorem define theorem policy gradient theorem differentiable policy  policy objective function correspond gradient define     source theorem temporal difference TD adopt estimate accurately update policy parameter illustrates framework propose AC resource allocation advantage policy DRL propose handle action reduce variance estimate gradient framework AC resource allocation DRL agent critic network estimate action function  update parameter moreover actor network update policy parameter schedule policy  evaluate critic network correspond policy gradient define    source function  reduce variance estimate gradient related gradient policy gradient redefine     sourcewhere    advantage function moreover  update TD TD error define    source improve training efficiency multiple DRL agent simultaneously update policy parameter schedule asynchronously algorithm specifically DRL agent initialize local parameter neural network schedule policy interact correspond environment datacenters DRL agent gradient accumulate periodically actor critic network asynchronous update execute parameter global network gradient ascent via RMSProp optimizer DRL agent parameter actor critic network global network replace local parameter update local parameter DRL agent interact correspond environment independently optimize local parameter schedule policy coordination DRL agent local training AC training asynchronous update mechanism multiple DRL agent converge algorithm asynchronous update policy parameter schedule DRL agent initialize global local parameter actor network global local parameter critic network temp temp temp accumulate gradient actor  accumulate gradient critic update global parameter gradient ascent via RMSProp   synchronize local parameter reset gradient performance evaluation setting datasets simulation evaluate performance propose conduct comparative baseline setting datasets propose model resource allocation implement tensorflow datacenter simulated heterogeneous server consumption idle server maximum consumption pmax server therefore consumption server distribute increase resource usage moreover trace data google datacenters input propose model datasets resource usage data server google datacenters specifically server randomly extract google datasets server consists around trace essential metric extract trace machine ID ID correspond resource usage depict per per resource cpu memory usage server reflect resource demand duration assume schedule assumption reasonable user commonly specify requirement resource usage duration utilize resource execute enables datacenters allocate resource correspondingly addition sequence cpu demand memory demand training DRL agent implement asynchronous update policy parameter DRL agent trace data fed propose model batch batch dnns fully hidden layer built neuron respectively moreover maximum epoch reward decay rate critic rate setting extensive simulation conduct evaluate performance propose AC resource allocation analyze effectiveness advantage propose resource allocation extensive comparative conduct performance advanced DRL PG DQL assess classic algorithm evaluate random execute random duration  execute decrease duration shortest SJF execute increase duration robin RR execute fairly circular slice employ assign portion tetri execute resource demand availability resource convergence evaluate convergence propose AC resource allocation impact essential parameter investigate TD error discount factor actor rate TD error discount factor constant actor rate reward faster convergence around training epoch achieve propose recent reward actor network action chosen along direction reward therefore convergence versus TD error discount factor constant TD error discount factor analyze convergence propose actor rate reward obtain training epoch however algorithm converges local optimum longer optimize policy contrast around training epoch achieve reward decrease curve fluctuates strongly increase training epoch smooth convergence suitable convergence versus actor rate comparison objective optimization subsection propose AC resource allocation evaluate performance metric reward qos normalize average latency dismiss rate efficiency average consumption various average load propose classic resource allocation  tetri SJF RR objective qos optimization reward qos generally decline increase average load propose achieve reward average load becomes contrast classic comparable performance average load average load performance classic slightly random scheme  performs random scheme average load average load  schedule duration priority excessive seriously degrades schedule performance contrast propose maintains excellent performance verify advantage propose schedule complicate environment load reward resource allocation various load objective optimization propose obtains normalize average latency dismiss rate performance gap becomes increase average load verifies adaptiveness propose dynamic environment changeable average load besides average consumption comparison conduct objective optimization propose average consumption average load although consumption reduce average load comparison performance metric resource allocation objective optimization comparison multi objective optimization subsection comparative conduct propose classic resource allocation multi objective qos efficiency optimization reward sum qos efficiency degrade increase average load changeable demand user increase complexity resource allocation propose obtains reward classic resource allocation average load average load complicate performance improvement achieve propose becomes obvious propose ability qos efficiency schedule reward resource allocation various load multi objective optimization reward resource allocation various load multi objective optimization propose outperforms resource allocation normalize average latency dismiss rate verifies propose excellent stability maintain superior qos objective multi objective optimization moreover depicts efficiency resource allocation propose attain average consumption increase average load propose defect objective optimization efficiency integrate DRL schedule qos efficiency schedule demonstrate advantageous performance propose improve qos efficiency comparison performance metric resource allocation multi objective optimization comparison DRL subsection performance comparison propose AC advanced DRL resource allocation conduct multi objective optimization average load propose achieve reward DRL training resource optimization moreover curve propose tends converge around training epoch however PG DQL respectively around training epoch relatively smooth convergence propose achieve qos normalize average latency dismiss rate efficiency average consumption DRL therefore demonstrate excellent performance training efficiency propose propose effectively avoid variance advantage function estimate policy gradient meanwhile efficient convergence achieve asynchronous update mechanism DRL agent reward DRL multi objective optimization average load comparison performance metric DRL finally detailed performance metric propose AC classic resource allocation exhibit average load propose reduces normalize average latency dismiss rate DQL DRL performs metric PG DRL efficiency average consumption PG DRL generates variance estimate policy gradient therefore cannot achieve load balance server consequently load utilization server excessive consumption contrast around average consumption propose RR average consumption moreover propose achieve excellent qos efficiency simultaneously training efficiency propose greatly improve asynchronous update policy parameter multiple DRL agent therefore propose efficiently approach global optimal guarantee qos efficiency simultaneously performance metric normalize average latency average consumption dismiss rate average load conclusion formulate resource allocation issue datacenters model DRL dynamic various user demand propose AC resource allocation effectively schedule improve qos efficiency datacenters extensive simulation trace data google datacenters demonstrate effectiveness propose achieve adaptive efficient resource allocation specifically propose outperforms classic resource allocation  tetri SJF RR PG DQL qos normalize average latency dismiss rate efficiency average consumption moreover propose others increase average load achieve training efficiency faster convergence advanced DRL PG DQL simulation propose improve resource allocation datacenters future extend propose model priority specifically redefine action reward function account priority action schedule priority reward algorithm schedule policy priority intend query aware database parameter tune advanced DRL model built feature query information DRL model relation database query configuration realize automatic parameter tune moreover improve generalization propose DRL resource allocation scheme develop automatic data augmentation technique aim regularize policy function respect various transition allows DRL agent capture task invariance useful behavior environment