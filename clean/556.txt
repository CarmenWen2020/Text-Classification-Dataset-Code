computer vision easy video data hidden information action recognition har har apply behavior analysis intelligent video surveillance robotic vision occlusion viewpoint variation illumination issue har task action action overlap contributes misclassification traditional engineering machine lack ability handle overlap action propose spatiotemporal har framework overlap action video transfer technique feature extraction tune pre cnn model spatial relationship frame optimize autoencoder squeeze dimensional feature rnn lstm temporal relationship iterative module tune model video learns adopt propose framework achieve performance spatiotemporal har overlap action visual data non stationary surveillance environment introduction digital gadget networking capability social interaction platform become integral jan video data uploaded youtube video content uploaded youtube internet traffic video data video data explore application understand utilize easy video data information static image multiple frame additional temporal component action recognition task identification various activity perform video data action recognition mainly focus action detect recognize action perform video data local context video data action perform frame local context comprises spatial information action another global context video data continuous data action recognition global context temporal information associate action sub action hidden information video data action recognition spatial temporal information relationship action recognition decompose category action interaction sub activity atomic action gesture activity behavior atomic action activity complex easy recognize however activity cooking combination sub activity complex recognize video datasets action recognition consists actor centric scene actor freely application action recognition application fraud detection suspicious abnormal behavior detection behavior monitoring elderly intelligent video surveillance computer interaction robotic vision research action recognition har challenge action recognition dynamic variation data occlusion viewpoint variation background clutter issue illumination computational inter intraclass activity similarity task challenge perform action differently activity information activity overlap classification sport activity skip jumping football  overlap action activity action initial frame capture context non stationary visual data task sometimes extension periodic temporal interval differentiation distinct activity harder distinguish architecture researcher propose approach spotlight video classification recognition task neural network automatically extract feature visual data convolutional neural network cnn learns feature directly initial layer extract local feature layer cnn global feature visual data semantics information recent researcher propose architecture action recognition video propose spatiotemporal har framework visual data capture context overlap action recognition issue task action recognition challenge pre cnn model reduces computational training achieve accuracy image classification video summarization detection therefore utilized concept transfer extensively investigate pre cnn model transfer concept pre cnn model feature extraction feature extract frame local global feature video data spatial relationship frame frame feature dimensional data continuous data action consist sequence therefore optimize autoencoder DAE squeeze dimensional feature capture context hidden temporal information visual data recurrent neural network rnn memory lstm propose framework learns hidden spatial temporal information relationship capture context accurately classification overlap action contribution summarize propose efficient optimize action recognition framework visual data utilize knowledge pre cnn model feature extraction frame spatial information extraction optimize autoencoder squeeze dimensional feature frame action recurrent neural network rnn lstm temporal information dimensional feature propose iterative tune module previously model adopts video remain structure sect briefly describes literature review related action recognition propose framework detail sect performance propose model overlap discussion sect concludes propose framework remark future direction literature review propose framework multiple consecutive frame fuse hidden temporal information 2D pre cnn frame conv net architecture stage fuse information frame  zisserman propose extension previous propose network model overcomes issue feature stack optical vector feature temporal context prediction generate average prediction sample video clip compute optical vector advance recurrent convolutional network memory lstm decoder encoder convolution training architecture action recognition comparison rgb optical information frame sample clip input architecture temporal context challenge remain unsolved developed 3D convolutional network C3D spatial temporal feature video data convolutional layer video data although propose architecture exist temporal context unsolved achieve improve performance action recognition video architecture without substantial training parameter action brushing brush spatial network extract spatial information involve activity propose network architecture temporal sequence network tsn perform temporal context introduce aggregation mechanism learnable feature max pool average pool bag action visual encode output propose novel architecture pre generation optical frame sample video unsupervised methodology consecutive frame generate cnn generate frame consecutive frame reconstruct exist frame generate frame difference minimize inverse warp technique developed temporal transition layer  extension inflate 3D convolutional neural network I3D utilize 3D densenet architecture diverse temporal depth stack multi temporal depth pool layer dense densenet attention pool mechanism action recognition interaction detect information extra supervision model propose novel attention network architecture local global knowledge awareness author propose global attention regularization gar mechanism introduce attention cluster local feature integration mechanism classification video data author temporal information model achieve slightly performance capture spatial relationship propose novel transformer network TN architecture action classification propose model almost fails overlap temporal context action video data developed action recognition framework atomic interact spatiotemporal action propose discriminative model memory DM lstm feature fusion technique recognize overlap action setting sub action detection representation multi model mechanism visual data decompose multiple overlap sub action propose residual network mechanism extract temporal feature multi kernel vector machine svm perform action extract feature dense trajectory improve propose model perform supervise minimum effort temporal activity localization mechanism trim video data introduce progressive mechanism spatiotemporal action recognition spatial refinement module refines localization simultaneously temporal extension module increase temporal context developed 3D convolutional neural network cnn detect action information successive frame extract spatiotemporal dimension focus detection action differentiation action propose framework propose detail pipeline DLPL propose pre cnn densenet inceptionv resnetv resnetv vgg vgg xception tune perform pre model autoencoder DAE rnn lstm network DLPL experimental setting DLPL detail propose pipeline DLPL propose spatiotemporal action recognition har framework image propose pipeline DLPL data pre processing data pre processing grouped action portion overlap action ucf dataset overlap action ucf dataset consists action overlap action overlap action frame video extract reshaped accord input pre cnn model frame rescale dimension inceptionv xception frame rescale dimension densenet resnetv resnetv vgg vgg pre cnn model standard split ucf website overlap action transfer transfer technique machine learns useful information hidden specific domain utilize transfer gain knowledge another related domain transfer exploit model task generalize another related training model task become training another model related task pre model transfer computational tune pre model training model scratch another benefit transfer amount data pre model already dataset optimization strategy strategy transfer remove classification layer layer tune pre model extra layer dataset domain feature marginal probability distribution  domain label predictive objective function instance predict task task training training domain task target domain task transfer fulfil goal transfer utilize knowledge acquire improve predictive target function feature extraction video data amount useful information hidden visual data intensity texture temporal contextual information proven useful video content analysis image extraordinary representational capability benefit transfer pre cnn model densenet inceptionv resnetv resnetv vgg vgg xception model extract feature frame spatial relationship visual data model imagenet dataset around image feature feature dimensional local global feature spatial information video frame traditional computer vision feature extraction engineering efficient harris detection HCD shi  detection  invariant feature transform sift robust feature surf feature accelerate binary robust independent elementary feature brief orient rotate brief orb perform extensive pre cnn model remove fully layer fcl model extra layer model dataset layer network tune model action recognition task video extract feature output fully softmax layer capture network representation convolution layer kernel output dimension feature pool stride input image rgb image channel parameter become bias feature consist neuron calculates sum neuron input float becomes MB instance dataset prediction stage layer release ram computation layer training purpose computation perform pas preserve reverse pas detect specific 2D image function denote convolutional kernel calculate average SMA convolution kernel SMA average convolute image 2D image compute 2D image convolution perform 2D smooth kernel output neuron convolution layer compute feature convolutional layer vertical stride horizontal stride height receptive width receptive previous layer feature neuron output previous layer feature channel  connection neuron feature feature layer densenet cnn model densenet convolutional neural network cnn architecture layer pre densenet model imagenet dataset image pre model input image densenet resolve vanish gradient neural network vanish gradient arises network input output becomes gradient calculation direction becomes vanishes another densenet resolve simplify connectivity layer maximize gradient information introduce concept feature reuse layer subsequent layer network strategy reduces redundant feature model parameter densenet layer layer network accepts feature previous layer input layer transition becomes nonlinear transformation denotes output layer denotes concatenate feature composite function successive operation batch normalization rectify linear relu convolution layer sample network densely consists layer connection layer subsequent layer layer dense transition layer comprise batch normalization convolutional layer average pool layer reduce computational input feature convolutional layer convolutional layer bottleneck layer bottleneck layer generates feature reduction feature subsequent transition layer dense generates feature output feature dense model compression factor transition layer feature densenet architecture imagenet dataset densenet model dense input image convolution dimension initial convolution layer stride initial layer illustrates detailed architecture densenet model densenet architecture extra layer tune ucf dataset image inceptionv cnn model inceptionv improvement inception architecture complex layer convolutional neural network architecture pre inception model imagenet dataset pre model input image rgb channel inception resolve overfitting deeper network filter another along expensive computational requirement inception network architecture wider instead network deeper stack layer introduce multiple kernel initial inception architecture kernel max pool output concatenate inception module auxiliary classifier batch normalization softmax deployed inception module loss sum auxiliary loss calculates label loss alteration dimension filter drawback loss information issue inception version version goal achieve factorize convolution convolution furthermore another factorization convolution another factorization convolution convolution parameter training inceptionv robust network architecture illustrates detailed architecture inceptionv model inceptionv architecture extra layer tune ucf dataset image resnetv resnetv cnn model resnetv resnetv improve version residual network variant residual network version resnet layer resnet layer convolutional neural network architecture pre model network imagenet network model image input rgb channel resnet skip connection connection concept earlier layer deeper layer apply relu activation skip connection shortcut network risky vanish gradient residual identity resnet contains connection connection resnet multiple residual stack input output residual amount residual function relu activation bias residual underlie identity mapping goal lean residual function achieve skip connection improvement develop inside residual network information propagation purpose identity mapping information broadcast within network setting improve network performance deeper shallow summation precede residual function output propagation operation loss function illustrates detailed architecture resnetv model resnetv resnetv architecture extra layer tune ucf dataset image vgg vgg cnn model vgg convolutional neural network variant vgg network architecture vgg layer vgg layer architecture pre model model imagenet dataset pre model input rgb image vgg model image input pas stack convolutional layer filter receptive convolution stride pad spatial pool max pool layer max pool stride stack convolution layer fully layer fcl fcl channel fully layer output softmax imagenet dataset image input consecutive convolutional layer filter max pool layer stride contains convolutional layer filter receptive max pool layer previously contains convolutional layer filter receptive vgg vgg convolutional layer filter receptive vgg max pool configuration max pool convolutional layer vgg vgg convolutional layer filter receptive file max pool apply previously configuration remains max pool fully layer described earlier dropout fully layer softmax relu activation model convolution operation illustrates detailed architecture vgg model vgg vgg architecture extra layer tune ucf dataset image xception cnn model extreme inception xception inspire inception architecture convolutional neural network layer pre model model imagenet dataset pre model input rgb image depth wise separable convolution channel wise spatial convolution pointwise convolution introduce instead inception module skip connection similarly resnet data input entry  exit illustrates detailed architecture xception model research xception architecture extra layer tune ucf dataset image representation representation autoencoder network autoencoder neural network lean representation input data reconstruct output input autoencoder reconstruction generate output input data unsupervised manner neural network architecture multiple hidden layer autoencoder autoencoder consists encoder responsibility representation input data decoder reconstruct output closest input data internal representation generate encoder internal representation encoder generates latent latent comprises probability distribution generate encoder fed decoder goal decoder generate output data latent generate sample probability distribution input concept autoencoders generative model autoencoders latent output decoder usually latent contains useful feature dimension input dimension salient feature training data dimension described penalize loss function representation input data model sometimes fails model latent dimension input data dimension regularization mechanism loss function autoencoder ability input copying output task classification sparsity factor autoencoder addition output reconstruction error sparsity penalty apply encoder output decoder output likelihood regularization described latent variable visible variable joint distribution prior distribution model latent variable absolute sparsity penalty laplace prior constant hyperparameter generalize autoencoder sparsity described neuron input neuron output patch input autoencoder patch pixel pixel input neuron input hidden neuron encoder  decoder bias loss function propose autoencoder model encodes input data bias nonlinear sigmoid relu function encode data decoder dimension reverse error reduce adjust backpropagation mechanism input hidden layer successive layer input respective precede hidden layer encode data  layer propose autoencoder epoch regularization apply prevent overfitting sigma sparsity regularization finally sparsity regularization error mse function illustrates autoencoder DEA architecture architecture autoencoder DAE image temporal context temporal context recurrent neural network rnn memory lstm learns temporal information sequence frame recurrent neural network rnn rnn neural network previous data architecture RN model output precede memory rnn previously performs task input hidden layer parameter parameter complex bias layer conversion independent activation dependent activation input input network compute input previous network internal hidden becomes compete similarly output compute output error rate compute update bias iteration error backpropagated network mechanism rnn temporal feature sequence frame visual data simplify rnn equation activation output activation function temporal coefficient hidden rnn update hidden previous hidden input fix function trainable calculate error backpropagation prediction network specific actual output error error becomes rnn architecture image difference traditional backpropagation backpropagation calculate error error sum vector memory lstm lstm successful algorithm vanish gradient explode gradient rnn lstm recurrent previously knowledge forget irrelevant data vector internal preserve lstm recurrent lstm gate forget gate responsible forget previous data amount sigmoid activation function input gate update gate responsible amount information internal sigmoid activation function input modulation gate relevance responsible modulate information input gate operates hyperbolic tangent activation function output gate responsible generate output internal sigmoid activation function lstm architecture image gradient equation contains instead lstm backpropagation simplify equation lstm memory output activation candidate replace previous activation input bias update gate forget gate output gate gate resides detail parameter parameter har model sigmoid activation function hyperbolic tangent activation function update memory activation denotes wise multiplication iterative model training data due video data previously model capable adopt video data unable correctly action data propose framework update data iteratively adopt variation surround environment retrain model iteratively data predict confidence threshold adopt surround environmental deployment setting user requirement threshold previously model tune data model adapt variation environment data model apply domain patient activity monitoring fraud abnormal behavior detection video surveillance detailed tune module architecture iterative tune module adopt video image experimental discussion brief discussion perform ucf action dataset action python program specification architecture implement tensorflow framework kera api pre integrate tensorflow evaluate propose performance evaluation metric loss accuracy precision recall dataset experimentation evaluation purpose ucf dataset ucf benchmark action recognition dataset consists video action video youtube available avi video format ucf challenge complex dataset action categorize interaction action interaction musical perform action involve duration clip action average frame per channel dataset almost comparable imagenet dataset computational training 2D convolutional neural network cnn ucf dataset ucf dataset characteristic summary frame sample action ucf dataset summary ucf dataset characteristic frame sample action ucf dataset border frame sample denotes category interaction sport action image performance evaluation evaluate propose performance evaluation metric loss categorical entropy accuracy overall performance precision positive prediction accuracy recall sensitivity actual positive sample coverage positive correctly classify positive negative classify negative negative false positive classify positive positive false negative classify negative negative apply categorical entropy loss combination softmax activation entropy loss formula softmax activation cnn truth input output multiclass classification input vector standard exponential function output vector discussion overlap    performance achieve accuracy DLPL minimum accuracy achieve DLPL contains overlap   punch accuracy achieve DLPL accuracy DLPL    accuracy achieve DLPL accuracy achieve DLPL contains overlap  dive performance accuracy DLPL accuracy DLPL contains overlap   achieve performance accuracy accuracy action perform contains overlap      accuracy accuracy achieve accuracy due factor overlap action therefore accuracy overlap   accuracy achieve DLPL accuracy achieve DLPL overlap   lunge achieve accuracy accuracy DLPL accuracy DLPL contains overlap   achieve accuracy DLPL minimum accuracy DLPL although accuracy multiple actor video sequence reduces performance multiple actor perform action ucf perform video trim due video consists overlap   accuracy accuracy accuracy accuracy due breast stroke action action later stage video performance propose pipeline overlap performance propose pipeline overlap performance propose pipeline overlap performance propose pipeline overlap performance propose pipeline overlap performance propose pipeline overlap performance propose pipeline overlap performance propose pipeline overlap performance propose pipeline overlap performance propose pipeline overlap consists overlap  dive   propose model achieve accuracy accuracy overlap distinguish   achieve accuracy DLPL accuracy DLPL overlap action similarity achieve accuracy performance propose pipeline overlap performance propose pipeline overlap consists guitar    accuracy comparatively accuracy DLPL achieve accuracy DLPL consist    raft respectively accuracy respectively performance respective decrease respectively    achieve accuracy DLPL accuracy DLPL consists overlap  skiing achieve accuracy DLPL accuracy DLPL performance propose pipeline overlap performance propose pipeline overlap performance propose pipeline overlap performance propose pipeline overlap performance propose pipeline overlap performance propose pipeline overlap factor affect performance propose overlap impact performance increase performance decrease model distinguish multiple overlap action although propose exist temporal action recognition video action interval action recognition multiple actor perform action reduce performance accuracy action recognition propose model video data spatial temporal relationship interval confidence model due camera actor difference viewpoint however propose overfitting issue handle frame video adjust dropout regularization although pre cnn model varied performance autoencoder rnn lstm impact performance difference performance pre cnn model relatively largely due frame spatial information extraction towards lstm temporal relation sequence video frame performance DLPL become iteration cnn DAE iteration rnn batch although propose factor impact performance video action sequence performance video complex action multiple action impact performance complex action another factor overlap action increase overlap action challenge classify correctly utilize previously knowledge transfer technique improve baseline performance data requirement training architecture reduce transfer technique frame feature extract pre cnn model pre cnn model extract spatial information frame action perform frame action perform frame feature extraction module cannot achieve accuracy perform operation extract frame video temporal relationship frame rnn lstm choice rnn action perform interval sequence frame passing information transfer knowledge earlier frame precede frame autoencoder network encoder network decoder network depth autoencoder significant performance autoencoder instead layer encoder layer decoder autoencoder DAE input layer layer encoder latent representation layer decoder reverse encoder architecture utilize benefit depth network approximation function mapping input latent representation hidden achieve accuracy autoencoder architecture generalization latent representation shallow linear autoencoders generation latent representation reduce computational resource reduce training data amount another factor overlap action increase overlap action challenge classify correctly propose model video data spatial temporal relationship interval confidence model due camera actor difference viewpoint however propose overfitting issue handle frame video adjust dropout regularization although pre cnn model varied performance autoencoder rnn lstm impact performance difference performance pre cnn model relatively due frame spatial information extraction towards lstm temporal relation sequence video frame performance DLPL become extract spatial feature frame frame cnn model DAE squeeze feature dimensional feature rnn lstm temporal relationship classify action processing frame frame propose  comparison propose comparison average accuracy achieve overall accuracy performance propose craft feature extraction traditional machine craft feature extraction achieve accuracy ucf action dataset propose outperforms performance increase inaccuracy propose outperform neural network action recognition increase accuracy achieve positive prediction precision sensitivity recall illustrate effectiveness propose pipeline action recognition har ucf action dataset comparison propose conclusion future article propose framework recognize action overlap action pipeline DLPL propose pre cnn model autoencoder DAE rnn lstm frame spatial information pre model tune utilized feature extract cnn model dimensional built benefit DAE squeeze dimensional feature dimensional feature reduce computational requirement propose rnn DAE temporal action sequence continuous video frame iterative module tune action recognition framework video data online non stationary environment propose achieve performance action recognition har overlap action temporal visual data challenge direction future action multi complex action multiple sub action development generalize unbiased standard data