traditional classification algorithm microarray datasets scenario machine capable operation algorithm anymore memory application framework perform computation synthetic minority sample technique sample sample obtain balance data algorithm svm stochastic gradient descent propose microarray classification proof convergence stochastic gradient descent algorithm various algorithm vector machine perform microarray datasets identify appropriate algorithm finally comparative analysis loss function clearly understand difference experimental stochastic gradient descent algorithm hinge loss attractive choice achieve accuracy introduction microarray valuable association linkage isolate  related cancer genetic variation microarray datasets generate dna rna protein  sample feature challenge data mining analysis technique microarray analysis discover propose aggregation normalization feature selection discovery prediction prediction microarray data analysis explore researcher expert various machine algorithm propose hybrid laplacian convolutional neural network LS cnn apply microarray classification training accuracy model predict cancer almost prediction article vector machine svm recursive feature elimination rfe feature raw gene data achieve performance accuracy improve classifier performance  genetic algorithm GA tune multi layer perceptron mlp svm knn however parameter optimization imposes overhead microarray classification obvious microarray data generally unbalanced define data significant difference sample seriously impair performance svm prediction accuracy likely illusion svm susceptible frequency bias reduce adverse approach sample SMOTE etc sample simplest emphasis minority sample sample distinct balance data former increase minority sample latter decrease majority moreover explosive growth data dimension computational inefficiency svm becomes apparent improve capability data various novel algorithm vector machine solver propose decade solver primal solver dual dual advent sequential minimal optimization SMO perception traditional solver svm subsequently liblinear coordinate descent bundle risk minimization  propose primal empirical loss minimize regularization balance accuracy tolerance accord requirement variant gradient descent GD stochastic gradient descent sgd trick   sgd QN mini batch sgd technique effective advantage nevertheless sgd solver vulnerable curse kernelization lose non linear classification overcome drawback propose adaptive multi hyperplane machine amm model apply sgd instead SMO obtain optimum promising efficiency non linear data meanwhile combine vector quantization  algorithm amm propose amm  model sgd demonstrate  comparable non linear SVMs classifier computational efficiency address kernelization introduce maintenance budgeting approach primal sgd vector sgd applicable kernel svm budget sgd  achieve faster computational kernel svm synthetic minority sample technique SMOTE sample minority microarray data due advantage compute feature data negligible sgd apply obtain optimal svm linear kernel reduce computational complexity empirical approach achieves impressive computational ensure classification accuracy structure related sect methodology sgd svm sect experimental microarray data report sect conclusion future sect related processing imbalanced data imbalance refers difference sample imbalanced distribution sample mainly classification related model addition sample feature information propose valid information machine model influence sample ignore sample classification accuracy unreliable empirical evidence difference sample alert dealt promptly SMOTE sample sample minority synthetic specific calculation minority sample calculate random linear interpolation sample randomly minority sample relies formula strategy synthesis minority sample image algorithm svm analyze svm algorithm emerge recent preliminary classification accord formula optimization explosive growth data researcher propose various strategy svm training category SVMs primal formula dual formula geometric formula parallelize svm SMO decompose quadratic program QP simplest subproblems optimize lagrange multiplier computational complexity however seek algorithmic complexity asymptotically linear linear sub sgd variant   etc propose obtain primal svm achieve accuracy efficiency moreover researcher practitioner geometric characteristic SVMs introduce geometric theory SVMs data core vector machine  algorithm achieve linear training kernel density estimate  approximation sample strategy complexity reduce sample convex hull vertex selection  selects sample constitute convex hull reduce sample convex hull vector machine  extract boundary vector convex hull vector integrate obtain boundary vector strategy application  datasets subsequently core reduce training propose development distribute DS infrastructure hadoop spark parallel compute multiple machine implement machine pioneer accord propose cascade svm ultra data data decompose subset subset svm algorithm interior  SMO finally svm filter eliminate non SVs hence SVs apply global model although reduce model becomes unstable understand detailed algorithm complexity preliminary classification svm formula optimization image comparison algorithm runtime methodology primal formula svm training ideal hyperplane optimization slack variable regularization parameter equation constrain objective function obtain unconstrained optimization introduce penalty inspire external penalty regularization parameter hinge loss function hinge loss modify huber loss define generally loss function svm hinge loss penalizes error linearly image generally loss function svm hinge loss penalizes error linearly hinge loss commonly apply replace hinge loss  modify huber generate sparse improve training efficiency perform probability estimation penalizes sample linearly interference anomaly stochastic gradient algorithm svm gradient descent GD classical iterative optimization algorithm minimum convex differentiable function due direction decrease repeatedly compute rate  performance model vector concern input data shortcoming GD entire data compute iteration consumes amount compute however sgd gradient randomly sample indicator function randomly chosen convergence analysis sgd svm convergence stochastic gradient algorithm descent various stochastic approximation sgd converges rate satisfies stochastic optimization define random variable empirical risk minimization risk unbiased estimator objective function smooth convex hinge loss svm hence lipschitz continuous gradient essential ensure convergence gradient decent algorithm assumption lipschitz continuous gradient objective function differentiable gradient function lipschitz continuous exists constant lemma hinge loss hinge loss modify huber loss convex function proof max function specially modify huber loss convex function convex function due formula  function assumption convexity objective function strongly convex exists constant assumption exists constant lemma hinge loss hinge loss modify huber loss satisfy assumption proof lemma append regularization assumption obtain conclusion theorem fix stepsize assumption lemma algorithm fix stepsize unique minimizer proof accord replace expectation theorem diminish stepsize sequence assumption lemma algorithm stepsize sequence proof induction framework classification microarray classification procedure microarray compose phase preprocessing feature apply imputation input feature matrix normalize sample SMOTE sample minority model training sgd svm model construct training prediction validate propose classifier framework propose classification microarray image perform framework propose microarray classification program jupyter notebook python memory intel core cpu ghz performance evaluation confusion matrix visually describes performance classifier binary classification confusion matrix multi classification expand binary confusion matrix outward confusion matrix performance parameter ith summation matrix confusion matrix data microarray data summarize detailed tag correspond data ncbi http ncbi  nih gov ezproxy auckland profile microarray data leukemia tag correspond sample   snp tag correspond sample colon biopsy cancer tag correspond sample MDS cancer tag correspond sample  algorithm svm various algorithm perform svm detailed difference compute microarray data indistinct moreover effective classification compute microarray therefore perform comparison comparison algorithm svm contains minor approach libsvm linear rbf kernel function sgd svm  linear rbf kernel function amm batch linear kernel function online rbf kernel function  strategy removal merge  due randomness compute compute hence calculate average libsvm computational rbf kernel function indeed classification accuracy acceptable improve model performance particle swarm optimization PSO optimize svm accuracy PSO svm improve somewhat otherwise increase dramatically meanwhile mapreduce PCs computation  reduce accuracy reduce nonetheless latter algorithm achieve significant improvement libsvm  computational reliable sgd svm  amm  independent sample dimensionality datasets unfortunately performance algorithm degrades somewhat dimensionality increase classification prediction snp datasets accuracy highly unsatisfactory although classification accuracy libsvm linear kernel function leukemia compute efficiency impressive although compute  amm  snp MDS accuracy acceptable comprehensive sgd svm achieve compute accuracy finally sgd svm perform classifier microarray classification  loss function clearly understand difference hinge loss function hinge loss function modify huber loss function practical application implement microarray datasets comparison sgd svm hinge loss function hinge loss function modify huber loss function conduct average difference computational efficiency accuracy loss function significant however careful comparison accuracy hinge loss function loss function data computation modify huber loss function shorter difference significant accuracy hinge loss sgd svm classification microarray microarray data sgd svm hinge loss function leukemia snp sample technique SMOTE increase minority sample microarray data leukemia snp image confusion matrix leukemia GSE image sample sample minority sample leukemia data minority sample snp data strategy increase minority leukemia sample minority snp sample detailed leukemia multi dataset sample maximum sample minimum sample demonstrates sample approximately dataset heavily unbalanced therefore classify dataset balance immediately relevant technique sample sample technique due sample synthetic minority sample technique  popular sample balance data classify propose framework performance described predict font precision font recall rate overall accuracy respectively overall accuracy multi data classify demonstrates excellent performance propose   snp feature feature data computational feature increase accuracy decrease evident data however sgd svm model data quickly multi data classify desirable accuracy confusion matrix snp GSE image colon MDS confusion matrix colon GSE confusion matrix MDS GSE image sample colon data training contains sample contains sample recall rate indicates positive label data predict negative label precision demonstrates sample positive prediction positive sample sample MDS data training contains sample contains sample overall classification accuracy important emphasize model obtains accuracy computation within comparison related evaluate effectiveness propose     mlp leukemia GSE MDS GSE knowledge performance accuracy leukemia literature combine sparse evolutionary training mlp improve compute efficiency accuracy performance MDS GSE literature comparative propose leukemia MDS respectively indicates propose significant  related comparison related accuracy conclusion future vector machine algorithm minimize improve computational efficiency microarray classification sgd vector machine microarray classifier overcome negative impact unbalanced data model reduce incur balance data efficient SMOTE algorithm employ balance data experimental classification performance balance data significantly improve although implementation hinge loss function model increase improves performance accuracy traditional SMO algorithm svm algorithm significantly improves computational efficiency ensure classification accuracy algorithm microarray classification addition although parallel compute significantly improve storage capacity compute involves resource data consistency debug difficulty etc performance requirement machine reduce increase conclusion algorithm avoid useless computation obtain optimal however due randomness accuracy computation calculation algorithm yield unstable future likely improve stability algorithm guarantee effectiveness approximation