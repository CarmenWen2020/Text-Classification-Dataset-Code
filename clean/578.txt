decade matrix multiplication gemm component linear algebra subprogram blas library played vital role various machine image processing fluid dynamic tend deconstruct multiple sub blas library implement batch gemm routine achieve performance scenario magma proposes vbatch routine calculate batch gemm variable gpu unbalanced input workgroups thread idle thereby affect performance addition unbalanced input affect load balance compute gpu extreme input insufficient utilization hardware resource proposes performance batch gemm compute framework gpu batch matrix variable unbalanced distribution propose framework hardware architecture data distribution adopt flexible tile sort split improve hardware utilization achieve load balance experimental framework performance improvement magma implementation amd  instinct MI gpu speedup MI introduction numerical library propose decade become important performance compute scientific compute purpose software architecture computer scientific research evolve numerical library developed technological epic progress computational optimize library compute hardware linear algebra however currently performance compute decompose sub parallel addition machine tensor computation matrix dimension usually relatively extremely vendor library developer extend traditional numerical library advantage computational hardware resource challenge therefore popular linear algebra subprogram blas library rocBLAS cuBLAS mkl magma batch routine batch routine batch matrix multiplication attention widely applicability nvidia cuBLAS amd rocBLAS fix batch gemm GEMMs matrix magma feature flexible batch gemm variable efficiently accelerate application algorithm variety domain implement batch gemm framework multiple grain tile strategy sort algorithm strategy splitting algorithm improve performance matrix unbalanced distribution framework improve hardware utilization achieve load balance sect gpu architecture introduce background gemm batch gemm sect introduce motivation framework sect demonstrate detail framework sect evaluate performance analyze sect discus related sect conclusion background gpu architecture emergence programmable graphic processing gpu nearly decade developer focus gpus instead traditional CPUs performance parallel compute remarkably parallel compute gpu increasingly important role purpose compute gpu parallelism consists multiple compute CU compute component amd MI CU independent simd contains KB vector purpose register addition CU KB local data lds execute multiple thread wavefront developer amd gpus portable application amd  compute platform introduces heterogeneous compute interface portability program model kernel implementation gemm batch gemm scientific application efficiently massively parallel architecture performance blas library library express computational phase routine  matrix multiplication gemm achieve performance standard blas interface gemm express scalar dense matrix important routine blas library computational density parallelism application machine fully layer convolutional layer implement gemm layer usually dominate training addition gemm occupies performance linpack performance benchmark program officially supercomputer ranking become critical performance supercomputer currently community extend blas standard implement performance routine batch processing address challenge application independent therefore batch  matrix multiplication batch gemm propose important operation performance advanced algorithm batch matrix decomposition batch gemm classic gemm regard collection independent dense matrix multiplication refers batch obviously batch operation degenerate classical gemm batch gemm matrix precision transpose non transpose format matrix dimension batch fix variable although batch gemm propose within decade importance beyond boundary dense linear algebra affect scientific domain accord previous research performance greatly improve batch gemm quantum chemistry metabolic network batch gemm important complement blas proposal motivation although classic gemm tends perform matrix perform poorly matrix extremely compute resource hardware fully utilize unfortunately research currently machine  signal processing etc dimension matrix usually addition performance compute efficient decompose matrix calculation independently challenge numerical library improve performance increase hardware utilization parallelism numerical library usually multiple independent operation batch vendor optimize gpu compute library rocBLAS cuBLAS compute batch gemm fix obtain substantial performance gain however uniform matrix greatly limit applicability therefore magma explores proposes vbatch routine batch gemm variable efficient gpu compute ability 3D grid specify independent GEMMs parallel compute although vbatch routine calculate GEMMs variable grid dimension related matrix batch workgroups idle workgroups matrix distribution unbalanced owe coarser grain tile strategy idle thread workgroup waste resource addition fix tile workgroups parallel requirement gpu matrix batch magma vbatch implementation image vbatch gemm routine default gemm routine image understand relationship magma vbatch performance input matrix variable scenario analyze advantage magma vbatch gemm rocBLAS default gemm routine traverse entire batch randomly generate gemm maximum batch abscissa average speedup vbatch gemm maximum batch advantage vbatch routine gradually decrease matrix increase disappear matrix allocate tile already sufficient parallelism gpu matrix dimension become however excessive tile resource waste introduce vbatch become nonnegligible besides optimize default gemm routine obtain sufficient parallelism thread parallelism tlp instruction parallelism ILP memory parallelism mlp GEMMs magma vbatch gemm routine suitable batch matrix summary although batch gemm crucial role domain exist batch blas sufficient diverse application scenario magma attempt succeed achieve performance matrix variable cannot fully utilize gpu capability input distribution unbalanced due waste computational resource address propose batch gemm framework gpu detailed overview introduce optimize framework mainly kernel implementation batch gemm kernel function host execute device flexible tile variety grain strategy greatly reduce waste resource optimization introduces sort reduce uneven resource allocation randomness uncertainty matrix distribution achieve load balance gpu fourth split split GEMMs tile strategy grain strategy improve tlp gpu utilization extreme input kernel classic gemm kernel matrix tile 2D grid workgroup correspond tile calculate sub matrix gpu compute resource capability efficiently memory locality tile 2D grid image matrix batch multiple tile 2D grid workgroup responsible tile processing entire slice entire slice workgroup load lds performs multiplication accumulate along dimension thread workgroup lds register faster global memory thread package wavefront execute instruction parallel thereby achieve mlp ILP workgroups execute parallel greatly improve tlp another dimension specify GEMMs 2D grid 3D grid GEMMs batch load gpu parallel calculation addition inner iteration unrolled reduce overhead buffer technology hide latency flexible tile obviously tile influence ILP tlp generally tile data reuse effective lds register however reduce workgroups gemm hide latency matrix addition tile precious lds register resource decrease workgroups execute parallel CU tile generate workgroups improve tlp effectively reduce waste idle thread data reuse seriously reduce performance therefore gemm batch appropriate tile tile strategy inspire  multiple grain tile strategy handle instance batch matrix greatly tile strategy matrix input workgroup thread strategy thread responsible tile determines amount lds workgroup amount lds gpu limited strike balance lds usage parallelism kernel 2D grid independent matrix tile allows appropriate tile strategy gemm batch usually relatively batch gemm tile improve parallelism aforementioned reduction workgroups optimize later magma input variety grain tile strategy gemm appropriate strategy parallelism greatly reduce waste resource idle thread idle workgroups addition tile related optimization flexible tile input image sort hardware schedule strategy sort algorithm reorder input batch thereby reduce unbalanced hardware utilization unbalanced matrix distribution input uncertainty variable challenge hardware load accord previous gemm workgroups processing determines inner loop tile regard workload correspond workgroup obviously workload workgroups GEMMs independent seriously unbalanced completely classic gemm uniform batch gemm scenario workgroup schedule execute CU imbalance workload CUs thereby reduce performance sort image illustrate simplicity assume CUs gpu workgroups workload generate accord input GEMMs load CU execution simd CU schedule robin schedule policy schedule obviously workload CUs unbalanced compute CU fully addition owe input uncertainty increase workgroups imbalance increase affect overall performance propose sort launch kernel reorder batch primary tile strategy secondary split later equivalent greedily workgroup workload schedule workload CU effectively balance workload CUs addition descend reduce fluctuation workgroups workload previously moreover overhead introduce sort negligible complexity batch gemm split avoid insufficient hardware resource utilization workgroups extreme input propose split adaptive algorithm split specific gemm tile strategy strategy thereby increase workgroups improve resource utilization gpu split image suppose batch GEMMs batch medium strategy generate workgroups however CUs MI CU workgroup CUs waste addition workgroups CU effectively hide latency execution simd inside CU fully utilized split algorithm split GEMMs batch strategy grain strategy kernel execution increase workgroups define split factor perform split operation formula refers constant factor related workgroup CU framework workgroup wavefront thread MI  CU simd load wavefront therefore avoid simd idle CU factor related software hardware CU workgroup execute parallel wavefront generally workgroups parallel efficiency although split increase workgroups workgroups introduce unnecessary software compute overhead hardware schedule overhead reasonable optimal performance perform specific gpu CUs gpu refers batch refers matrix tile strategy  occupy gpu split perform selection specific GEMMs obviously GEMMs tile strategy occupy amount calculation strategy maximize ILP maintain parallelism conversely GEMMs tile strategy calculation consume relatively tolerance tile strategy principle split preferentially split GEMMs tile strategy minimize ILP addition gemm split workgroups reduce additional schedule overhead previous sort reorder input accord tile strategy specific GEMMs algorithm split algorithm traverse entire input batch correspond GEMMs split update split workgroups generate improve tlp meeting ILP thereby improve gpu utilization reduce waste thanks flexible tile strategy tile divisible split introduce additional overhead resource waste evaluation analyze performance propose framework input batch gemm uncertainty diversity scenario batch generate batch matrix gemm random upper limit data possibility background batch generate input variable matrix matrix non transpose format precision simplicity rocBLAS magma MI gpu framework average speedup respectively detail explore advantage propose framework rocBLAS MI average speedup image average speedup MI rocBLAS image focus comparison rocBLAS amd rocBLAS nonuniform matrix default traverse batch compute obviously batch obtains speedup default default load instance computation matrix gpu computational resource waste instance batch compute excessive batch context switch kernel execution greatly reduces execution efficiency algorithm computes batch simultaneously gpu computational resource utilization advantage scenario furthermore speedup ratio decrease due increase matrix instance improves gpu utilization default allows optimize default algorithm achieve parallelism however batch processing framework achieves computational performance exploit parallelism gpu batch matrix various domain motivation magma MI average speedup image average speedup MI magma image perform detailed comparison analysis magma vbatch routine batch processing variable matrix computation illustrate effectiveness propose framework batch obvious speedup magma fix tile matrix batch workgroups generate magma cannot fully occupy gpu amount waste resource earlier grain flexible tile strategy split framework effectively generate workgroups achieve ILP tlp without introduce excessive overhead hardware compute resource compute gpu batch speedup specifies upper limit gemm determines workload workgroup gemm distribute relatively generate workgroups workload suitable sort balance workload CUs matrix matrix batch irregular greatly aggravate workload imbalance workgroups framework performs magma distribution severely uneven increase overall average speedup decrease matrix batch increase tile magma perform however increase distribution matrix becomes unbalanced uncertainty batch however 3D grid dimension magma related matrix batch uncertainty thread workgroups GEMMs become idle framework propose allows gemm appropriate tile strategy flexible tile thereby tolerance input uncertainty addition split gemm 2D grid  fluctuation 2D grid greatly reduce idle thread workgroups rocBLAS MI average speedup image average speedup MI rocBLAS image magma MI average speedup image average speedup MI magma image verify performance framework gpu architecture amd MI gpu experimental obtain average speedup rocBLAS magma detailed speedup ratio framework stable gpu architecture gpus achieve consistent performance improvement related recent focus optimize batch gemm application driven hardware architecture attempt extend performance batch blas cpu gpu accuracy characteristic AI domain precision complex precision batch gemm algorithm nvidia tensor core obtain performance speedup respectively extremely matrix  colleague hardware hierarchy hybrid heterogeneous multiple technique optimize batch gemm achieve peak performance gpu balance tlp ILP  propose tile batching compute variable batch gemm achieve performance improvement nvidia gpu technique FP  propose matrix distribute across core achieve balance variable matrix computation cpu architecture achieve speedup ratio intel mkl batching routine researcher attempt implement gpu batch gemm algorithm batch triangular dense linear algebra kernel batch QR factorization batch svd kernel achieve significant speedup conclusion implement framework perform batch gemm matrix variable unbalanced distribution gpu application machine data mining achieve compute performance application scenario flexible tile multiple grain tile strategy gemm appropriate strategy reduce hardware resource waste maintain parallelism sort sort greedy algorithm rearrange entire batch thereby improve internal load balance gpu split considers matrix distribution split specific GEMMs tile increase hardware utilization thereby improve compute performance experimental propose framework average performance improvement amd MI gpu SOTA magma implementation MI