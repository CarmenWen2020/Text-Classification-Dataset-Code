rapid exponential development textual data recent yield automatic text summarization model aim automatically condense text shorter version although various unsupervised machine approach introduce text summarization decade emergence remarkable progress however text summarization model development potential fully explore accordingly novel abstractive summarization model propose utilized combination convolutional neural network memory integrate auxiliary attention encoder increase saliency coherency generate summary propose model validate cnn daily mail duc datasets empirical propose model outperform exist model rouge metric generate summary saliency readability baseline model accord evaluation introduction rapid extensive development internet technology daily confront amount textual information news report blog due amount information generally unstructured accurate selection efficient information prominent challenge automatic text summarization aim generate shorter version text source text effective automatic text summarization commonly aim compress input text lossy manner preserve concept notably text summarization role processing information retrieval reader justify concept text reduce consume retrieve information text summarization commonly classify category extractive abstractive model extractive text summarization technique generally traditional approach summary generate important source text combine compose coherent summary abstractive text summarization considers semantic source text resemble summary generate accord source document extractive summarization technique introduce obtain remarkable focus mining core information source text utilized summary recently enhancement computer performance development neural network abstractive summarization technique attention considerable progress generate summary mapping input sequence another output sequence effective abstractive summarization technique commonly implement encoder decoder framework various neural network recurrent neural network rnn memory lstm convolutional neural network cnn candidate model implement encoder decoder although numerous text summarization model obtain considerable recent exist model resolve confront limitation summary brief salient information syntactic structure requirement besides semantically coherent however abstractive text summarization model generate summary generally saliency accordance syntactic structure requirement abstractive model difficulty meeting synthetic structure requirement moreover exist encoder decoder model input however assume input efficiency compose express overall meaning collocation decoder content source text generate summary important information instance comprehend text generate summary outline important information filter irrelevant finally summary extract information however abstractive model highlight principal information source text source text input encoder regard source text remarkable irrelevant information encoder source text input therefore highly influence irrelevant information cannot efficiently semantic text mention issue novel abstractive text summarization model propose decrease impact irrelevant information improve coherence saliency generate summary attention synthetic structure propose model leveraged combination cnn lstm encoder despite exist model attention commonly decoder auxiliary attention encoder imitate brain generate summary contribution categorize propose model combination cnn lstm encoder input generation summary improve overall performance text summarization despite attention mechanism propose model utilizes auxiliary attention encoder decoder simulate brain summarize outline important information filter information useful generate summary propose model combination cnn lstm besides auxiliary attention encoder empirical propose model efficiency generate summary semantically syntactically acceptable remainder organize related focus abstractive model review sect propose model introduces propose model contextualization empirical sect conclusion remark future research direction mention sect related vast quantity textual data currently available summarize retrieve efficient knowledge within sensible attention paid automatic text summarization model broadly classify extractive abstractive research text summarization focus extractive approach summarization conduct extract source document generate summary traditional extractive text summarization approach highly dependent generate feature utilized computational technique genetic program fuzzy logic neural network machine summary regard cheng  propose model document summarization propose data driven model employ neural network continuous feature besides hierarchical document encoder attention decoder generate summary research  propose supervise unsupervised approach identify concept source document generate summary approach extension systematic representation advantage structural document feature enhance efficiency traditional vector model text summarization  fai propose model combine feature categorize relevance content notably feature external aspect relevance feature relatedness content feature accord content transmit feature extension remarkable efficiency various processing task attractive extractive summarization regard utilized convolutional neural network encode ranked extract semantic information generate summary however propose model content extract keywords propose model convolutional attention recurrent neural network summary generation content valuable   propose extractive text summarization model principal feature extraction feature enhancement summary generation propose model  summarization sequence classification employ recurrent neural network perform compute hidden representation generate summary although extractive text summarization model obtain considerable generate summary syntactic structure requirement generate summary semantically coherent generally tend paraphrase document prefer summary concise coherent salient text summarization direction mainly focus abstractive text summarization recent mention abstractive text summarization harder extractive text summarization acceptable generate summary appropriate representation generate summary spite primary abstractive text summarization approach machine craft feature emergence revolution regard document summarization neural network utilized bag encoders convolutional neural network attention mechanism decoder generate summary propose attention neural network generate headline news article research  utilized attention memory network generate headline text news article utilized recurrent neural network substitute neural model decoder yield considerable enhancement text summarization model generally suffer rare commonly another abstractive text summarization rarely assume unimportant important construct summary accordingly researcher overcome repeatability rare regard implement attention encoder decoder recurrent neural network model obtain performance various datasets coverage mechanism propose repeatability   evaluate upper bond feature target encoder output decoder decrease repeatability summary recently perform reinforcement apply attention mechanism decoder summary overcome rare utilized pointer mechanism encoder decoder concept pointer mechanism configuration employ address rare identical mechanism model improvement summary generation propose summarization model advantage convolutional neural network memory network besides pointer mechanism generate summary attention memory network probabilistic generation model reinforcement summary moreover employ seqseq encoder decoder generate abstractive summary utilize generative model model latent structure discriminative information summary wang len propose summary aware attention social medium text abstractive summarization utilized source hidden attend summary vector compute summary aware attention attentional encoder decoder framework lstm encoder decoder encode input text generate summary propose variational neural decoder text summarization model combination variational rnn variational autoencoder capture complex semantic representation research convolutional sequence sequence model perform abstractive summarization propose mechanism enable reader important aspect generate summary specify attribute unified model employ combination abstractive extractive summarization utilized attention modulate attention inconsistency loss function penalize inconsistency attention propose neural abstractive summarization framework leveraged actor critic approach reinforcement typical attention maximum likelihood respectively employ actor critic moreover propose communicate agent encoder decoder model overcome document summarization model text subsection encode specific agent reinforcement utilized perform training extend sequence sequence model text summarization propose model encoder selective gate network attention equip decoder selective gate network representation information encoder decoder development pre model utilized task text summarization regard bidirectional encoder representation transformer bert encode input sequence context representation stage decoder transformer decoder stage generate draft output sequence draft sequence masked fed bert stage finally combine input sequence draft representation generate bert transformer decoder utilized predict refine masked examine conditioning encoder decoder transformer neural model bert model additionally propose bert windowing chunk wise processing text introduce dimensional convolutional attention investigate locality model affect summarization ability transformer propose aggregation mechanism transformer model address text representation challenge information review encoder memory capacity overall worth mention although numerous model propose task abstractive text summarization development gap machine generate generate summary attractive potential researcher abstractive summarization model generate summary saliency accordance syntactic structure moreover generate summary commonly irrelevant information generate gram purpose abstractive summarization efficiently previous methodology definition abstractive text summarization aim generate summary text without copying important assume input document refers ith token input text  propose model aim function another sequence token summary notably implement propose encoder decoder network described architecture propose model detail propose model introduce propose model consists embed encoder auxiliary attention decoder diagram model depict input source text generate summary output firstly document tokenized access embed vector secondly vector encoder aim obtain understand source text encoder propose model consists combination cnn lstm thereafter auxiliary attention apply encoder emphasize important via decrease interference useless information decoder auxiliary attention perform along training semantic reference summary semantic source text noteworthy auxiliary attention apply finally output encoder auxiliary attention fed decoder generate summary detail model diagram propose model embed layer encoder auxiliary attention decoder image embed neural network generally vectorized representation input understand input due propose abstractive text summarization model utilizes neural network input source convert vector representation bag bow frequently technique commonly employ text summarization transform input text vector however technique besides simplicity suffers remarkable drawback sparse data dimensional vector ignores semantic relation text therefore building effective representation input source text summarization essential regard skipgram model reveal efficiency various processing task propose model generate vector input text skipgram supervise algorithm raw text memory vector representation efficiently frequent moreover semantically vector impact summarization however sub linear relationship explicitly define skipgram theoretical characteristic diagram skipgram model depict shallow layer neural network learns vector representation context skipgram vector maximize average probability equation log𝑝 skipgram model structure image content estimate probability calculate softmax function notably softmax traditional activation function multiple output input array therefore softmax neural network model classify instead binary however softmax cannot label therefore cannot utilized multi label classification exp   respectively refer input output vector representation vocabulary model assumes vector representation randomly assign along training vector update maximize embed vector obtain average vector encoder encoder generally imitate brain reading understand text commonly recurrent neural network due ability serialize data implement encoder recurrent neural network remarkable gradient vanish dependency variant memory network recently employ various encoder however lstm input input owe generate saliency text summarization interaction input impact generate summary accordingly combine cnn lstm aim encoder input convolutional neural network utilized firstly layer cnn generate feature secondly efficiently utilized classification goal cnn input lstm firstly specify extract hence lstm embed vector concatenate input matrix dimensionality embed vector dimensionality input matrix denote feature convolutional operation perform input matrix noteworthy apply filter input matrix filter width dimensionality embed vector maintain sequential structure input text therefore filter height varied input matrix convolutional filter perform generate submatrix feature convolutional operation repeatedly apply matrix generate output sequence equation matrix convolutional operator perform input matrix convolutional filter obtain feature achieve feature bias activation function bellow employ various filter generation various feature pool function induce fix vector capture important feature feature perform pool function obtain feature concatenate feature vector input lstm instead embed input lstm obtain convolutional neural network fed lstm input propagation lstm compute equation tanh   respectively refer matrix forget gate input gate output gate input sequence obtain convolutional layer propose model input previous sigmoid function lstm aim calculate hidden output input finally obtain hidden input text auxiliary attention exist encoders generally comprehend source text generate summary however outline principal information certify correctness encode information comprehend text highlight important information generate summary prefer focus important information text summary source text inspire brain generate summary propose auxiliary attention apply encoder contrast exist attention mechanism commonly utilized decoder along summary generation auxiliary attention prominent information decoder decrease interference useless information yield generation accurate summary previously mention series hidden generate apply encoder hidden concatenate obtain accurately entire text extract important feature fed multi layer perceptron achieve vector importance 𝑖th input thereafter dot operation finally obtain information extract learnable parameter dot worth mention although multi layer perceptron extract information correctness extract information guaranteed strengthen important information ignore weaken useless unnecessary information semantics source text reference summary supervise consistent semantics certify semantic accuracy extract information regard sum vector aim semantic source text notably input decoder generate summary reference summary encode lstm hidden obtain hidden sum vector semantic reference summary thereafter cosine similarity similarity respectively refers semantic source text reference summary compute cosine similarity vector extract information accurate similarity indicates extract information enhance performance multi layer perceptron extract important information similarity fed network along training similarity maximize ensure accurate information extract negative likelihood minimize obtain similarity auxiliary attention apply training reference summary available due reference summary available auxiliary attention perform schematic structure propose encoder cnn lstm auxiliary attention depict schematic structure propose encoder cnn lstm auxiliary attention image decoder goal decoder transform source text series hidden revert hidden actual sequence generate summary lstm utilized decoder generate summary respectively refer hidden input lstm notably decoder initialize indicates principal information source text lstm decode hidden update previous hidden input token generate conditional probability distribution determines probability generate 𝑖th compute equation specifically vocabulary distribution compute  softmax   vector dimensionality vocabulary softmax exp  vector notably generate input decode generate  determines probability generate target vocabulary detail besides datasets evaluation metric implementation empirical dataset cnn daily mail dataset training evaluation propose model dataset document multi summary document contains average correspond summary average dataset generally contains training sample validation duc dataset additionally utilized evaluation propose model worth mention although sort dataset contains manually generate summary standard dataset various academic research previous conduct dataset dataset dataset document document contains manual reference summary byte average duc dataset dataset training neural network therefore propose model evaluation metric rouge recall orient understand  evaluation rouge metric commonly utilized text summarization lexical overlap unigram bigram sequence subsequence generate summary reference summary rouge calculate equation rouge  summary     summary   refers gram moreover   maximum gram generate summary reference summary  gram reference summary notably commonly report rouge toolkit various gram however various report unigram rouge bigram rouge agreement judgment accordingly calculate evaluate propose model exist utilized rouge recall evaluation metric employ evaluation   official rouge  utilized obtain rouge evaluation although rouge metric extensively utilized evaluate text summarization model evaluate literal similarity generate summary reference summary therefore cannot saliency generate summary propose model generate salient informative summary evaluation expert various randomly summary reference summary summary generate propose model summary generate model footnote assign evaluator grade summary saliency noteworthy evaluator summary propose model model aware reference summary criterion utilized evaluator summary relevance metric express  summary various evaluator compute comparison criterion saliency metric besides saliency readability another important factor superiority propose model generate summary saliency evaluation assign various randomly summary evaluator summary syntax grammar illustrates readability criterion noteworthy evaluator aware detail summary summary reference summary finally various evaluator compute propose conduct iran official english professional english teacher summary guarantee specialty evaluator obtain criterion readability metric baseline illustrate efficiency propose model comprehensive comparison propose model exist conduct various datasets exist baseline propose model employ dataset model conduct cnn daily mail dataset  temp att model utilized feature encoder pointer mechanism respectively embed overcome rare summarization model enable user feature generate summary pointer generator coverage model utilized pointer mechanism overcome rare introduce extend vocabulary coverage repeatability ML intra attention model attention mechanism inside decoder overcome repeatability  model extraction pre processing encoder input besides pointer mechanism rare  inconsistency loss combination extractive abstractive summarization model generate summary attentive information extractor model attentive information extractor encoder traditional attention mechanism decoder generate summary DCA MLE sem RL model utilized communicate agent encoder decoder address document generate summary reinforcement DCA MLE sem model utilized communicate agent encoder decoder without reinforcement bert transformer bert encoder decoder bert windowing employ chunk wise processing text model conduct duc dataset ABS model cnn neural model respectively utilized encoder decoder AC ABS model utilized actor critic framework increase efficiency traditional abstractive summarization  model attentional encoder decoder  model selective encode utilized develop seqseq model text summarization CR  model cnn rnn respectively encoder decoder model configuration text pre processing pre processing prominent processing CoreNLP nltk perform reduce redundancy although primary easy helpful regard  implementation pre source text bunch linguistics analytical processing text content commonly text pre processing tokenization morphological reduction coreference resolution instance without perform morphological reduction morphological reduction merge additionally numerous demonstrative pronoun source text ambiguity along training therefore coreference resolution eliminate ambiguity training illustrate detail pre processing sample source text pre depict source text pre processing image hyperparameters pre processing token extract skipgram convert token vector representation dimensionality vector respectively assume vector update rate previously mention encoder propose model consists cnn lstm cnn filter filter hyper parameter filter filter hidden lstm worth mention utilized encoder extract input average reference summary cnn daily mail duc datasets respectively byte maximum decoder cnn daily mail dataset duc dataset moreover batch beam algorithm generate summary adagrad update rate employ optimization training conduct epoch implement python tensorflow previously mention conduct datasets efficiency propose model exist model worth mention although apply attention mechanism encoder decoder summarization model overcome rare increase performance abstractive summarization model commonly attention mechanism decoder extract information along summary generation however propose model applies auxiliary attention encoder informative information decoder encode input sequence furthermore exist encoder decoder model generally cnn lstm encoder propose model advantage combination cnn lstm encode input sequence enables encoder input impact generate coherent summary empirical cnn daily mail duc datasets rouge metric summarize rouge indicates summary generate propose model reference summary performance comparison propose model exist model obvious propose model outperforms model datasets specifically propose model obtain rouge rouge rouge cnn daily mail dataset propose model outperform model duc dataset obtain rouge rouge rouge overall obtain conclude propose model improvement average exist abstractive summarization model comprehensive analysis efficiency propose model sample source text reference summary summary generate pointer generator coverage propose model illustrate propose model capture significant information exist source text clarifies generate summary saliency abstractive summarization image due rouge metric calculate repeatability gram analyze quality generate summary perform evaluation examine saliency readability propose model sample randomly chosen procedure mention sect assign expert evaluator criterion mention propose model saliency generate summary baseline model saliency evaluation summarize average saliency propose model model saliency propose model confirms generate informative summary due propose model decrease interference irrelevant information source text auxiliary attention saliency evaluation evaluate  propose model sample randomly chosen assign expert evaluator criterion mention generate summary propose model readability baseline model readability evaluation summarize average readability propose model model therefore conclude propose model obtain syntax grammar baseline model readability evaluation therefore conclude propose model generate summary readability generate summary compose owe utilization combination cnn lstm encoder enables model gram input finally visualize attention efficiency propose model extract informative information source text sample text randomly visualization depict obvious important source text accurately clarifies auxiliary attention mechanism successfully significance source text information obtain decoder interference unimportant information decoder efficiently reduce accordingly generate summary important information saliency illustration image discussion attention mechanism commonly apply abstractive summarization model extract important information along summary generation however propose model leverage attention mechanism encoder obtain principal information source text encode input capable semantics syntactic sect reveal propose model efficiency rouge metric exist model decrease interference irrelevant information generation summary accurate salient readability evaluation summary generate propose model readability however owe target abstractive summarization generate summary saliency besides generate novel gram available reference summary statistical analysis explore abstractive ability propose model percentage gram propose model generate summary comparison summary summary generate model cnn daily mail duc datasets illustrate percentage indicates abstraction percentage gram cnn daily mail duc dataset image although gram generate propose model reference summary remarkably model prof efficiency propose model related utilization cnn lstm encoder encoder input worth mention gram generate propose model indicates content generate summary directly source text however evaluation saliency propose model generate summary confirm duc generate gram generally cnn daily mail due summary dataset generate summary already dataset propose model massively generate gram moreover abstractive ability propose model  generate  calculate context vector decoder decoder input equation     learnable parameter sigmoid function       training  increase converge training clarifies model source text initial training generate noteworthy  model supervision reference summary training perform along without influence efficiency propose model future reinforcement utilized along training improve performance additionally although propose model extract information encoder obvious filter irrelevant information correctly overcome issue attention mechanism apply decoder however consideration apply attention loss valuable information training neural network highly related hardware implement namely gpus significantly reduce training cannot efficiency model rarely explore metric evaluation mention optimal model abstractive summarization definition optimal define tradeoff model complexity training performance however cnn lstms adopt propose model training  various trick discard source document utilized moreover maximum input text cnn daily mail dataset training model converge efficiently increase training however analysis complexity propose plot curve correspond rouge propose model model cnn daily mail duc dataset illustrate obvious propose model rouge per epoch obtain maximum accuracy epoch datasets therefore converge faster model curve propose model model rouge per epoch image another investigate influence dataset efficiency propose model regard sample cnn daily mail dataset randomly chosen training unchanged thereafter propose model model individually chosen training obtain report obvious enhance training sample rouge enhance indicates importance dataset prof propose model efficiently employ complementarity heterogeneous feature increase overall summarization efficiency rouge various randomly chosen training sample briefly despite potential propose model generate summary suffers mention weakness however evaluation conduct anonymous expert confirm generate summary saliency readability overcome mention issue future conclusion novel abstractive text summarization model propose advantage combination convolutional neural network memory network pre vector encoder besides auxiliary attention overcome exist increase saliency coherency generate summary regard propose model utilized combination convolutional neural network memory enable model gram input text auxiliary attention consist multi layer perceptron similarity module employ obtain principal information source text reduce interference irrelevant information encoder decoder therefore decoder generate summary important information entire input propose model validate cnn daily mail duc datasets empirical propose model outperform approach datasets rouge metric due rouge metric calculate repeatability gram analyze quality generate summary perform evaluation examine saliency readability propose model accord analysis obtain propose model saliency readability baseline model however mention despite superior performance propose model inability supervision training purpose abstractive text summarization besides inefficiency filter irrelevant information limitation however employ reinforcement attention future research overcome issue enhance performance leverage representation technique bert RoBERTa significant feature input encoder worth explore