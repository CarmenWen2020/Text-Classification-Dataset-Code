data parallel analytics shuffle namely network aggregation partition data task data dependency usually overhead reduce shuffle overhead scache source plug particularly focus shuffle optimization scache adopts heuristic pre schedule combine shuffle prediction pre fetch shuffle data balance load node meanwhile scache advantage memory accelerate shuffle propose performance model framework resource quantification FRQ model analyze dag framework evaluate scache shuffle optimization FRQ model quantifies utilization resource predicts execution phase dag implement scache spark hadoop mapreduce performance scache evaluate simulation testbed node amazon EC cluster evaluation demonstrate incorporate scache shuffle overhead spark reduce nearly overall completion tpc DS query improves average apache hadoop mapreduce scache optimizes terasort completion previous keywords distribute dag framework shuffle optimization performance model introduction era data explosion  byte data accord ibm report increase iot sensor embed industrial production manufacturing information assembly machine continuously generate distribute compute framework analyze industrial data inevitable trend accord industrial traditional data characteristic analytical processing daily workload cluster resource workload shuffle accord another mapreduce trace analysis facebook shuffle phase account completion average shuffle shuffle phase crucial heavily affect application performance popular framework define acyclic graph DAGs reduce pipeline hadoop mapreduce RDDs spark vertex dryad etc shuffle phase essential communication successive computation stage although continuous effort performance optimization variety compute framework shuffle phase poorly optimize deficiency shuffle phase lack coordinate management resource shuffle responsible intermediate disk shuffle fetch intermediate remote disk network schedule fix bundle resource cpu memory disk network slot assign task slot release task task aggregation couple schedule effectively simplifies task management however attach intensive shuffle phase cpu memory intensive computation phase multiplexing computational resource moreover shuffle phase fetch data correspond reduce task correspond reduce task fetch shuffle data almost simultaneously synchronize network communication burst demand network greatly enlarges shuffle completion optimize data shuffle without significantly dag framework propose  cache source plug dag compute framework specifically scache shuffle phase underlie framework workflow dag framework scache scache replaces disk operation shuffle memory task pre fetch shuffle data scache effectiveness scache decouples shuffle reduce task decouple effectively enables flexible resource management multiplexing computational resource scache pre schedule reduce task without launch pre fetch shuffle data pre schedule pre fetch effectively overlap network transfer desynchronize network communication avoid extra allocation slot image KB image workflow comparison legacy dag compute framework framework scache evaluate scache node amazon EC cluster spark hadoop mapreduce nutshell scache eliminate explicit shuffle varied application impressively scache reduces overall completion tpc DS standardize benchmark average apache spark apache hadoop mapreduce scache optimizes terasort completion organize methodology optimization scache detail implementation scache introduce FRQ performance model comprehensive evaluation scache FRQ model discus related conclude background observation data parallel compute shuffle achieve data transfer node illustration task define task shuffle data reduce task define task consume shuffle data overview shuffle task partition data bucket accord partition function hash bucket reduce task successive shuffle split shuffle shuffle shuffle task writes partition output data local persistent storage shuffle reduce task fetch partition data remote input impact shuffle shuffle intensive introduce significant latency application report mapreduce yahoo facebook shuffle workload shuffle shuffle latency dominate completion JCT instance mapreduce trace analysis facebook shuffle account JCT average shuffle observation typical spark application node xlarge EC cluster analyze implementation shuffle dag framework hardware utilization trace node spark  task node execution marked launch task execution shuffle marked shuffle stage shuffle reduce execution marked launch reduce task coarse granularity resource allocation slot assign task release task completes reduce network transfer shuffle data introduces explicit delay shuffle meanwhile shuffle shuffle occupy slot without significantly involve cpu coarse slot task mapping imbalance task resource demand slot allocation decrease resource utilization unfortunately defect exists spark hadoop mapreduce apache  finer granularity resource allocation scheme reduce delay synchronize shuffle almost reduce task shuffle simultaneously synchronize shuffle request burst network traffic data transfer demand network bandwidth network congestion network transfer happens framework bulk synchronous parallel BSP paradigm hadoop mapreduce dryad etc inefficient persistent storage operation shuffle tightly couple task execution operation operation along synchronize shuffle introduce significant latency performance bound cluster besides legacy shuffle data disk inefficient cluster memory input dataset shuffle data relatively shuffle spark terasort input data data report amount data shuffle input data memory distribute storage propose data memory dag framework shuffle data disk spark hadoop mapreduce dryad etc argue memory capacity living shuffle data cautious management mitigate shuffle overhead propose optimization shuffle ahead reduce stage overlap operation multi task memory shuffle data achieve optimization shuffle decouple task execution achieve granularity schedule scheme reduce task pre schedule without launch achieve shuffle data pre fetch shuffle optimization detailed methodology achieve shuffle optimization firstly discus shuffle overhead dag framework subsection propose shuffle data management decouple shuffle execution propose pre schedule pre fetch hide shuffle overhead multi task furthermore heuristic algorithm algorithm improve accuracy prediction pre schedule decouple shuffle execution decouple shuffle reduce task propose shuffle data management scache shuffle data dag framework scache apis manage shuffle data shuffle data task transfer data scache inside scache memory shuffle data scache reserve memory release slot immediately shuffle data scache shuffle data immediately reduce due shuffle data pre fetch scache reduce task shuffle data scache scache cache shuffle data memory instead disk spark hadoop mapreduce aspect scache optimizes shuffle shuffle memory performance pre schedule pre fetch shuffle data pre schedule pre fetch critical aspect optimization scache pre fetch hide shuffle overhead multi task however shuffle data cannot pre fetch without awareness task node mapping optimize shuffle phase propose schedule scheme heuristic algorithm algorithm dag framework scheduler reduce task shuffle output prediction simplest pre schedule mapping task node randomly evenly without context random mapping schedule performance due data skew ignore data locality dag application random input shuffle output predict accurately linear regression model observation ratio output input invariant configuration however linear regression model fail scenario customize partition inconsistency output distribution reduce input distribution illustrate spark spark  output picked randomly spark  introduces extremely data skew due extremely data skew introduce partition linear regression model cannot reduce task almost input data task task data reduce task data locality skew reduce task data output propose methodology reservoir sample scache instead linear regression predict output  customize non hash partitioner task classic reservoir sample randomly sample reduce task tunable parameter function locally sample data finally partition output sample input task reduce discover increase parameter sample sample sample accurate discover sample prediction linear regression prof sample prediction accurate linear regression prediction composition reduce partition calculate define reduce task achieve data locality perform shuffle pre schedule heuristic  schedule balance load minimize network traffic heuristic  schedule algorithm algorithm pre schedule load balance maintain min heap loop min heap simulate load node apply processing LPT achieve optimum spark fifo achieve optimum pre schedule task node mapping adjust accord locality trigger task normalize probability calculate bound performance degradation inside task swap without exceed algorithm maintains min heap traverse swap algorithm operation evaluate heuristic  schedule algorithm trace  simulation simulation schedule scheme random mapping spark fifo heuristic  balance load context shuffle data locality heuristic  schedule improvement average spark average random mapping image KB image cope multiple shuffle dependency reduce stage shuffle dependency dag compute framework cope multiple shuffle dependency accumulate heuristic schedule algorithm illustrate algorithm previous shuffle schedule heuristic  schedule shuffle predict accumulate previous shuffle schedule reduce task shuffle trigger transfer data host shuffle rare previous shuffle data contributes composition accumulation probability task swap traverse accumulate previous shuffle data algorithm operation image KB image implementation overview implementation scache overview detail sample subsection focus constraint memory management discus fault tolerance overview scache consists component distribute shuffle data management dag scheduler worker daemon plug scache rely dag framework node scache dag scheduler storage manager dag scheduler responsible pre schedule reduce task assignment dag framework assign reduce task accord assignment storage manager manage shuffle worker node dag submit dag information generate framework task scheduler  customize non hash partitioner scache insert extra sample sample reservoir sample algorithm partition sample data scache predicts reduce partition data algorithm pre schedule task compute shuffle writes task modify scache api memory shuffle scache worker reserve memory scache ID sends shuffle threshold define memory usage scache scheduler schedule algorithm pre schedule reduce task threshold shuffle request issue amount data shuffle remains pre schedule scache worker pre fetch shuffle accord assignment enforce dag framework assign task accord scache pre schedule insert code framework scheduler memory management scache memory management described ensure performance reserve memory scache flush disk temporarily fetch scache leverage constraint manage shuffle context aware priority constraint observation multi execution stage task memory cache fetch overhead become bottleneck stage  multi stage completion improves task data cached simultaneously therefore scache minimum storage management refer constraint context aware priority constraint scache leverage application context victim storage reserve memory scache flush incomplete disk cluster widely scache schedule scheme spark inter shuffle priority spark fifo scheduler schedule task stage accord submission scache selects victim task priority factor task stage scache priority storage earlier submission task stage scache priority storage task ID discussion fault tolerance fault tolerance scache restarts fail worker without recover data currently failure happens scache worker shuffle phase scache daemon shuffle operation worker restarts without violate correctness dag compute compute dag framework scache gain benefit shuffle phase although entire compute task compute efficiency fault recovery performance benefit gain scache scache brings significant benefit due memory shuffle management although overhead compute introduce failure occurs drawback tolerable scache server fault node failure dag framework due characteristic shuffle data promising dag framework advanced recovery scheme application layer parallel recovery spark ensure efficient fault tolerance mechanism introduce fault recovery strategy backup mechanism backup data disk pre shuffle mention mitigate overhead fault recovery introduce drawback resource utilization resource another checkpoint mechanism interval metadata apache flink  intermediate data memory mitigate latency cluster availability mode adjust available node accord failure production environment fault tolerance mechanism checkpointing replication consume significant resource introduction strategy future framework resource quantification model introduce framework resource quantification FRQ model evaluate performance dag framework FRQ model predict execution application circumstance dag framework hardware environment etc introduce FRQ model FRQ model computation discus performance FRQ model propose FRQ model analyze relationship computation phase shuffle phase dag compute FRQ model focus shuffle overhead significantly affected schedule strategy quantify compute resource FRQ model evaluate resource schedule strategy FRQ model phase shuffle reduce horizontal axis vertical axis phase firstly introduce input parameter FRQ model input data input data data conversion rate conversion rate input data shuffle data computation phase computation computation phase computation resource configuration framework hadoop mapreduce suppose cluster CPUs memory sufficient input data memory framework phase consists task cpu computation phase computation computation computation phase shuffle transmission shuffle phase FRQ model calculates execution phase parameter execution sum phase reduce phase formula phase shuffle phase define penalty fault recovery depends fault tolerance mechanism framework scache intermediate data memory fault occurs entire compute task compute reduce phase formula ideal computation reduce phase compute overhead overlap shuffle phase reduce phase shuffle phase empirical overhead shuffle computation reduce phase relies data transfer shuffle phase portion computation transfer overhead accord optimize completion reduce resource schedule strategy formula subsection formula discus performance strategy schedule strategy FRQ model schedule strategy hadoop mapreduce schedule strategy shuffle phase reduce phase due increase reduce phase increase accord shuffle phase computation phase execute parallel execution sum execution shuffle phase hidden reduce phase schedule strategy hadoop mapreduce scache suppose scache overlap shuffle phase phase pre fetch pre schedule phase schedule strategy avoids resource idle phase accord scache pre fetch FRQ model schedule strategy scache distinguish situation meaning inequality generate shuffle data shuffle situation shuffle phase uninterrupted resource fully utilized shuffle phase formula shuffle faster scache shuffle data generate shuffle phase interrupt formula hadoop mapreduce resource schedule strategy hadoop mapreduce scache shortens shortens pre fetch optimizes execution evaluation reveals representative evaluation scache performance spark hadoop mapreduce industrial data sql data technology widely data mining predictive analytics text analytics statistical analysis evaluate scache tpc DS performance gain scache tpc DS standard benchmark focus model industrial workload recognize shuffle intensive benchmark terasort evaluate scache data partition scheme summary scache decrease spark shuffle improves overall completion tpc DS average meanwhile hadoop mapreduce scache optimizes completion average perform variously ensure accuracy evaluation setup spark version hadoop mapreduce version shuffle configuration spark default node xlarge cluster amazon EC node GB memory CPUs network bandwidth node mbps spark scache dag analysis spark  amazon EC task node hardware utilization capture node completion scache spark without scache duration network transfer shuffle data introduces explicit delay shuffle burst network traffic shuffle network congestion overlap cpu disk network easily decouple shuffle prevents compute resource operation decouple shuffle slot earlier schedule task shuffle pre fetch decouple shuffle significantly decrease cpu idle reduce task scache manages hardware resource transfer shuffle data without interrupt compute utilization multiplexing hardware resource increase improve performance spark stage disk operation replace memory decouple shuffle eliminate shuffle improvement stage completion shuffle optimization scache data spark jvm serialization inevitable cpu intensive image KB image tpc DS benchmark evaluation reduce stage shuffle overhead introduce network transfer delay shuffle data pre fetch explicit network transfer perfectly overlap stage schedule scheme scache guarantee reduce task benefit shuffle pre fetch combination optimization decrease overhead shuffle reduce task addition heuristic algorithm achieve balance pre schedule improvement reduce stage completion overall scache spark decrease overhead shuffle industrial production workload evaluate query tpc DS tpc DS benchmark model multiple user submit varied query hoc interactive olap data mining etc tpc DS contains query standardize benchmark data horizontal axis query vertical axis query completion skip query due compatible issue spark scache outperforms spark almost query furthermore shuffle query spark scache outperforms spark magnitude query shuffle operation groupby union etc overall reduction portion query scache achieve average evaluation overall completion query shuffle optimization promising hadoop mapreduce node cluster FRQ model scache GB GB GB GB GB GB GB GB GB GB GB GB legacy GB GB GB GB GB GB GB GB GB GB GB GB hadoop mapreduce aws xlarge node cluster FRQ model scache GB GB GB GB GB GB GB GB GB GB GB GB legacy GB GB GB GB GB GB GB GB GB GB GB GB image KB image cpu utilization throughput node hadoop mapreduce terasort hadoop mapreduce scache unlike spark hadoop mapreduce reduce phase workflow alleviates defect naive schedule scheme reduces benefit pre schedule however scache considerable optimization shuffle workload terasort benchmark evaluate performance gain scache framework ability scache terasort consists consecutive shuffle shuffle input data hash partition function partition shuffle partition data spark  hadoop mapreduce without scache shuffle reduce simultaneously reduce phase network transfer amount shuffle data network bottleneck cpu resource idle hadoop mapreduce scache decouples shuffle reduce pre fetch phase scache utilizes idle throughput phase decouple hadoop mapreduce scache optimizes terasort overall completion average input data GB GB FRQ model evaluation evaluate FRQ model terasort hadoop mapreduce environment parameter accord application environment scache alleviates defect reduce phase environment enable scache terasort satisfies situation without scache actual error amazon EC environment terasort hadoop mapreduce satisfies situation formula experimental calculate application extra overhead runtime network overhead allocate slot etc overhead amplify input data execution overall error mainly error acceptable related model propose  model estimate resource information amount input data specify deadline propose HP model extends  model HP model factor linear regression estimate execution datasets  propose detailed mathematical performance model phase mapreduce combine overall mapreduce model propose  model model estimate performance provision resource  propose queue model focus straggler optimize however model accurately overhead shuffle schedule strategy industrial data  industrial data annually generate smart factory data increase fold nowadays challenge data manage logically efficiently distribute compute framework become efficient industrial data analysis dag optimization hadoop mapreduce classic approach handle shuffle overhead  sample data statistic tune parameter etc  dynamically reduce task stage  decouples shuffle reducer centralize shuffle controller neither handle multiple shuffle schedule multiple reduce task  aggressively pre schedule task multiple successive stage fetch shuffle data skew mitigation  partition data dynamically straggler task happens partition data transfer node libra proposes sample dag framework innovative approach split balance load reduce task  propose dag optimization address performance monetary optimization scache pre schedule multiple shuffle pre fetch gain optimization without load balance network layer optimization varys  network layer optimization shuffle transfer effort limited throughout shuffle easily apply scache improve performance accord combine sdn scache improve performance network later conclusion scache plug shuffle optimization dag compute framework task pre schedule shuffle data pre fetch application context scache decouples shuffle compute task mitigates shuffle overhead propose framework resource quantification FRQ model evaluate shuffle dag compute framework evaluation scache promising speedup industrial production workload