building distribute ddl hpc cluster guarantee convergence scalability training dnns challenge hpc cluster consists multiple density multi gpu server infiniband network HDGib compress compute communication distribute dnns training brings challenge convergence linear scalability respect worker parallel dnns training analyze optimization identify issue scalability degradation frequency update parameter due compression compute communication exacerbates stale gradient slows convergence previous constrain gradient stochastic error sgd outdated HDGib cluster strict constraint due infiniband network connection constrain stochastic error rate worker inefficient due training stage worker propose momentum driven adaptive synchronization model focus issue accelerate training procedure HDGib cluster adaptive synchronization algorithm momentum absorb stale gradient adaptively bind stochastic error approximate optimal descent direction distribute sgd model individual dynamic rate worker improve training performance previous linear exponent decay precise descent distance distribute sgd training stage extensive experimental propose model effectively improves training performance cnns retains accuracy cpu gpu cluster respectively previous keywords distribute momentum driven adaptive synchronization model performance compute introduction recent witness rapid revolution DL breakthrough research computer vision recognition processing etc DL model volume data training DL model become consume task performance compute hpc cluster consist density gpu server network improve training performance distribute DL cluster density gpu server network HDGib cluster become essential topic data AI application parameter server architecture data parallelism asynchronous parameter update strategy widely dnn training datasets model training performance guarantee allows parameter update worker individually eliminates global barrier gradient aggregation worker parameter update frequency training scheme ideally parallel training distribute achieve linear training respect worker worker training define dnn training achieves convergence target validation accuracy linear scalability achieve due communication overhead massive iterative optimization stochastic asynchronous error scalability challenge therefore goal distribute DL research consequently topic focus distribute DL researcher currently acceleration communication spectrum approach gradient compression quantization approach effort communication reduction distribute dnn training echoed recent literature goal reduce iteration improve training performance communication reduction optimal training dnn cluster network improve accelerate training procedure gradient compression quantization approach lossy compression gradient gradient error iteration increase training guarantee dnn model converges target accuracy efficient throughput network network blindly compress communication increase optimization limit improvement gain performance platform another emerge reduce communication overhead reduce approach exchange gradient usually gradient worker gradient worker per communication communication broadcast update gradient worker reduce approach reduce communication overhead parameter server architecture efficient cluster throughput network training worker dnn training converges target accuracy parameter server approach evaluation sec network infiniband network training dnn reduce network bandwidth idle dnn model training performance improve idle network bandwidth HDGib cluster significant explore optimization direction accelerate distribute dnn training besides communication reduction approach direction explore desirable descent direction distance distribute sgd training reduce training dnn model achieve convergence validation accuracy optimization direction complementary communication reduction approach combine accelerate distribute dnn training significance optimization direction certify recent research evaluation analysis popular dnn model data confirm necessity significance direction DL application HDGib cluster analyze affect linear scalability distribute dnn training besides communication overhead identify crux optimization stale gradient increase training guarantee convergence dnn model target accuracy stale gradient define gradient evaluate version model additionally network compress communication parameter update frequency increase queue delay gradient update worker increase staleness gradient gradient error rate increase increment gradient staleness increase training guarantee convergence dnn training limit performance improvement gain network stochastic error gradient random sample training data sgd slows convergence distribute dnn training stochastic error reduce gradient worker training network stochastic error neglect worker gradient iteration threshold however easy worker constrain stochastic error guarantee performance rate worker excessive inadequate descent distribute sgd worker potentially slows convergence adopt rate worker inefficient due training stage HDGib cluster analysis objective building distribute DL HDGib cluster threefold handle volume stale gradient reduce training dnn model achieve convergence target validation accuracy constraint stochastic error distribute sgd dynamically worker individually adaptively error rate descent direction constrain stale gradient organize rate dynamically adapt training stage guarantee convergence achieve goal aggregate gradient merge stale gradient momentum reduce gradient guarantee performance propose non monotonic strategy rate improve training performance summarize contribution distribute progressive momentum algorithm propose dynamically merge volume stale gradient generate HDGib cluster momentum constrain descent direction error rate adaptively algorithm avoids waste resource reduce stale gradient assign contribute model convergence momentum avoid local optimal dnn training adaptive synchronization algorithm propose constrain stochastic error within reasonable without affect performance training dnns HDGib cluster avoid local optimal stochastic gradient approximation guarantee distribute sgd descent direction training algorithm achieve convergence fully synchronous asynchronous algorithm adaptive   BB approximation algorithm propose individually adjust rate worker global rate decay suitable HDGib cluster training stage worker document organize motivation synchronization model detailed description propose algorithm discus experimental setup benchmark analyze exist synchronization model conclude propose direction research related parameter server reduce reduce approach propose reduce communication overhead distribute dnn training achieve nearly linear scalability gpu network throughput training worker achieve convergence target accuracy dnn model potentially suffers local optimal training vanilla sgd without momentum synchronous parameter update strategy increase training worker parameter update latency gradient worker parameter update recent research zero offload proposes schedule training task gpus CPUs worker offload data compute CPUs gpus allows dnn model bert gpu nvidia server reduce communication strategy gradient worker update parameter gpu throughput achieve linear scalability complementary parameter server communication framework mainly focus optimize training task worker focus optimization gradient aggregation worker distribute synchronization model recent distribute DL built synchronization model bulk synchronous parallel sgd BSP model server gradient worker aggregate update parameter training asynchronous parallel sgd asp model allows worker update parameter individually stale synchronous sgd ssp model asynchronous parameter update strategy bind stale gradient maximal delay BSP model precise descent asp model performance distribute sgd achieve performance distribute dnns training combine advantage BSP model asp model previous sgd predetermine analyze convergence rate sgd respect iteration however performance predetermine model highly sensitive determiner decision author adaptively however ignore stale gradient stochastic error gradient sgd asp model propose algorithm memory memory data consistency scalability assumes parameter update sparse objective function training task convex assume available traditional machine task suitable parameter update sparse objective function non convex proposes reduce communication frequency accelerate dnn training however tune communication frequency adaptive rate previous multi gpu distribute training cnns acknowledge modulate rate presence stale gradient author propose exponential penalty stale gradient worker however distribute gradient staleness assume exponential penalty reduce rate arbitrarily potentially convergence exponential penalty avoid excessive decay rate linear decay rate staleness rate assumes gradient staleness gradient parameter server aggregate iteration probability exceed however idealize assumption unrealistic cluster performance training performance cluster explore synchronization model previous explores training performance heterogeneously networked gpu cluster synchronization model heterogeneous multi gpu cluster limited explore training performance HDGib cluster background motivation background focus optimization synchronization model distribute dnns training parameter server architecture data parallelism accelerate training procedure distribute dnn training HDGib cluster recent HDGib cluster widely improve dnn model datasets training performance compress compute communication utilization density gpu server network frequency update parameter frequency update parameter increase queue delay parameter update worker exacerbates stale gradient increase gradient error rate increase training loop guarantee dnn model converges target accuracy observation analysis evaluate speedup distribute dnn training worker network bandwidth worker training dnn model achieve convergence validation accuracy speedup worker network speedup becomes network training dnn model converge achieve linear scalability distribute training performance speedup incredibly increase worker network bandwidth speedup network convergence behavior network bandwidth distribute dnn training consumes update global parameter achieve convergence worker throughput network furthermore stale network bandwidth stale network network stale define gap training worker iteration gap slowest worker iteration indicates gap bandwidth image KB image comparison gbps gbps network bandwidth interpretation reader refer web version article image KB image comparison staleness gbps gbps network bandwidth image KB image comparison degradation accuracy stale gbps gbps network bandwidth node mobilenet  observation analyze model training converge target accuracy distribute environment HDGib cluster degradation acceleration due inaccurate descent direction gradient error distribute sgd gradient error involve optimization mainly avenue gradient update parameter calculate  optimization version model stale gradient HDGib cluster exacerbate stale gradient due increase queue delay parameter update server frequency update parameter stochastic data stochastic error stale gradient stale gradient momentum utilized merge stale gradient algorithm accelerate training procedure avoid local optimal momentum momentum sgd calculate historical gradient optimization formula decay rate gradient calculate iteration similarly stale gradient calculate  optimization optimization stale gradient calculate gradient calculate version dnn model momentum primarily propose training performance stable accelerate training procedure momentum propose avoid local optimal algorithm proposes merge stale gradient momentum improve training performance dnns illustrate analyze principle cosine distance cosine similarity similarity vector inner generate gradient cosine distance iteration upper sub cosine distance momentum gradient iteration momentum cosine distance respect gradient stale gradient stable training performance accelerates training procedure image KB image comparison cosine distance mobilenet stochastic error classical stochastic optimization theory effective stochastic error constrain stochastic error worker gradient optimization adaptive effective sample training data iteration stochastic error training dnns sgd stochastic error accuracy oscillation dnns training increase training guarantee dnn model converges target validation accuracy increase training data reduce stochastic error training distribute dnn training increase training data gradient calculate version version dnn model worker however worker gradient training constrain stochastic error within acceptable addition gradient worker training local optimal degradation validation accuracy dnn model due extreme reduction stochastic error significant suitable constrain stochastic error avoid local optimal distribute dnn training generate evaluation mobilenet training loop achieve target accuracy decrease increase worker however worker increase without limitation mainly due application stochastic error ignore worker threshold degradation validation accuracy local optimal mobilenet  stochastic error accuracy overlook worker algorithm formulate adaptively worker training propose adaptive synchronization algorithm dynamically gradient aggregation classical stochastic optimization theory algorithm detailed derivation algorithm verify correctness validity algorithm evaluate gradient aggregation generate gradient aggregation worker evaluation stochastic error accuracy disregard gradient aggregate worker verify reliability algorithm image MB image aggregation gradient mobilenet  algorithm image KB image algorithm momentum driven adaptive synchronization model adaptive rate approach dimension adaptive adaptive rate training traditional calculation neglect characteristic loss function suffer excessive decay detailed explanation sec rate besides estimate reasonable initial rate inappropriate rate local adaptive rate per worker due stochastic feature sgd rate worker iteration brings stochastic error parameter update affect training performance loss worker synchronous communication strategy resnet resnet loss training worker varies across worker iteration synchronous communication strategy adjust rate per worker worker define individual rate training backtracking  efficient approach suitable rate application however rate sequence monotonic ignores characteristic objective function loss function explore approximate   BB quadratic model loss function calculate rate curvature quadratic model increase robustness algorithm combine backtracking approximate BB calculate rate worker individually training image KB image comparison loss worker resnet resnet performance analysis analyze various asynchronous model performance sensitive limited network resource cnn model couple feature performance network resource queue model asynchronous scheme worker update parameter server frequency network resource limited accord evaluation vgg cifar parameter update within network bandwidth gradient vgg around parameter update per parameter update per training dnn network update per update per analyze observation CPUs server becomes bottleneck training dnn gpu cluster network parameter server architecture demonstrate evaluation sec recent research cpu bottleneck limited memory bandwidth popular optimizer RMSProp adam etc aggregate gradient update parameter easily exhaust memory bandwidth CPUs memory access apply gradient update optimizer adam memory access update parameter gradient proposes buffer worker reduce stochastic error merge stale gradient momentum apply parameter update iteration training reduce parameter update frequency reduce cpu overhead server asynchronous gradient aggregation strategy theoretically methodology theoretical analysis detailed introduction distribute progressive momentum algorithm address stale gradient distribute adaptive synchronization algorithm constrain descent direction distribute sgd reduce stochastic error avoid local optimal distribute adaptive BB approximation algorithm rate iteration worker adaptively distribute progressive momentum algorithm initial definition momentum sgd training data appreciate data parallelism computation model popular parameter server architecture framework distribute dnn training training data naturally worker define model parameter iteration training parameter update define equation convergence criterion met function define calculate gradient model parameter worker summation formula calculate intermediate worker function aggregate model parameter summation formula generate parameter iteration momentum sgd propose accelerate convergence avoid local optimal momentum iteration define equation define decay coefficient momentum define gradient iteration training data gradient update parameter iteration calculate  parameter update momentum equation define equation rate distribute progressive momentum algorithm apply parameter update momentum introduce equation distribute dnn training gradient worker synchronously training overhead distribute momentum generate gradient calculate previous optimization training feature stale gradient distribute progressive momentum algorithm progressively merge stale gradient momentum stale gradient avoid local optimal reduce synchronous overhead momentum algorithm assume worker additive update global model parameter regular interval worker worker algorithm definition algorithm definition algorithm    training worker ID  coefficient momentum momentum   server parameter LB worker gradient worker ID  training update parameter  server training  rate worker gradient server   adaptive synchronization model accord analysis sec gradient involve avenue stale gradient stochastic error stochastic sample training data distribute progressive momentum algorithm aim stale gradient local optimal constrain stochastic error server gradient worker iteration algorithm server collection stale gradient worker instead consume computation resource reduce progressively merge momentum accord iteration algorithm however accord challenge reasonable estimation therefore propose distribute adaptive synchronization algorithm suitable iteration dynamically training detailed analysis algorithm sec image KB image distribute progressive momentum algorithm distribute adaptive synchronization algorithm accord description important dynamically suitable iteration bound stochastic error within reasonable interval error bound inherent optimization recently revive development algorithm improve global convergence without convexity utilizes error bound algorithm constrain descent direction distribute sgd therefore focus formula data probability distribution assume differentiable possibly non convex domain convex distribute DL application function DL model parameter data observation training data expectation model entire corpus data average extremely infinite calculation becomes challenge evaluate gradient classical gradient impossible therefore sgd algorithm propose reduce computational overhead minimize equation iteration sgd selects training batch data uniformly random calculates equation denotes rate iteration training data randomly data observation accord random sample SRS obtain equation therefore calculation gradient explain noisy approximation gradient sample resource increase sample reduce resource therefore important dynamically suitable sample iteration balance noisy rate resource consumption distribute adaptive synchronization algorithm sufficient effective descent direction define per equation accord explanation stochastic optimization effective descent direction constrain approximate descent direction difference approximate descent direction define error bound error guarantee effectiveness approximate descent direction error approximate gradient approximate gradient error bound inspire error bound stochastic gradient analyze maximum stochastic error error bound constrain estimation gradient inequality equation stochastic error bound function variance matrix trace equation obtain maximum stochastic error detailed derivation appendix accord equation gradient stochastic error formulate error bound equation simplify formula definition trace define equation finally error bound equation define equation approximate gradient error bound gradient average gradient generate training data obtain error bound estimate gradient random sample accord theorem interval estimate population assumption sample variance unbiased estimation population variance obtain probability distribution denotes population average sample average sample variance confidence interval population confidence rate coefficient approximation population interval estimate population confidence interval accord requirement application inequality obtain apply theorem algorithm maximum confidence interval approximate population inequality adaptive synchronization algorithm algorithm distribute adaptive BB approximation algorithm initial definition rate application variable descent iteration sgd determines descent equation practical application simply rate decay maximum chosen minimize along ray equation however consume adaptive BB approximation algorithm effort distribute adaptive BB approximation algorithm obtain overhead inefficient backtracking  propose reduce overhead classic adaptive   BB explore non monotonic scheme curvature estimate propose selection algorithm proven efficient nonlinear optimization quadratic model objective iteration optimal explore local quadratic model BB along non monotone convergence safeguard non quadratic inspire combine backtracking  define BB approximation algorithm rate iteration worker spectral scheme gradient descent propose   model function quadratic function curvature quadratic model estimate rate training dnn model sgd varies variant stochastic stochastic variable introduce quadratic function   loss function quadratic approximation equation aim suitable minimize iteration equation parameter deduce curvature estimate equation detailed derivation appendix equation curvature iteration estimate BB equation denotes scalar vector equation gradient calculate batch parameter efficient increase compute overhead replace variance gradient calculate equation sample randomly extract sample gradient variance obtain relationship batch gradient variance batch increase gradient variance reduces deploy approximate gradient equation equation modify equation smooth operation performance predictable demonstrate equation denotes calculate equation denote batch mini batch addition optimal deterministic distribute adaptive BB approximation algorithm rate algorithm algorithm image KB image algorithm distribute adaptive BB approximation algorithm evaluation analysis introduction implementation evaluation synchronization model sec detailed analysis evaluation speedup accuracy analysis sec linear scalability analysis sec training performance accuracy analysis sec network bandwidth convergence analysis sec performance improvement gain distribute adaptive momentum algorithm rate analysis sec introduce implementation synchronization model implement tensorflow version parameter server architecture data parallel mechanism google specific rpc grpc library communication server worker parameter server architecture node worker server node training worker execute training procedure calculate gradient server gradient worker aggregate update parameter node node status role network communication information checkpoint node training procedure generally worker node data gradient parameter exchange server worker data transfer worker parameter server architecture distribute optimizer google researcher model wrap distribute optimizer tensorflow distribute optimizer  rewrite apply gradient function update parameter server rate function execute adaptive algorithm worker reduce computation overhead server training performance model reduce algorithm implement horovod tensorflow mpi tensorflow NCCL software stack training cluster built centos slurm workload manager deploy distribute dnn training worker tensorflow worker allocate dedicate gpu worker library cudnn accelerate training gpus grpc library communication server worker opencv library pre training image parallel program model training worker assign gpu cluster introduce sec compute node assign worker HDGib cluster introduce sec compute node worker worker assign gpu grpc library communication parallel program model implement distribute dnn training interaction message passing scheme communication methodology performance evaluation testbed verify efficiency propose model cluster testbed cpu cluster compute node ghz core intel core TM CPUs GB memory network bandwidth gbps gpu cluster gpu server network geforce RTX gpu cpu core intel xeon cpu ghz GB memory HDGib cluster gpu server network geforce RTX gpus cpu core intel xeon cpu ghz GB memory benchmark model datasets cnn model mobilenet resnet resnet resnet vgg data cifar cifar  benchmark detailed information cnn model parameter cnn model   mobilenet MB resnet MB resnet MB resnet MB vgg MB baseline synchronization model synchronization model representative model asynchronous parallel sgd asp asynchronous parameter update strategy allows worker update parameter individually bulk synchronous parallel sgd BSP server gradient worker aggregate update parameter training stale synchronous parallel sgd ssp bound stale gradient maximal delay elastic average sgd EAO periodically update individual model parallel worker  define straggler training synchronous parameter update strategy horovod tensorflow mpi source reduce communication library mpi execute reduce operation vii allreduce NCCL implement reduce algorithm tensorflow NCCL communication library allreduce grpc implement reduce algorithm tensorflow grpc communication library hyper parameter hyperparameters model mini batch refers batch worker staleness ssp rate  model percentage straggler interval EAO parameter update interval worker server lastly LR DR rate decay rate rate respectively hyperparameter async asp sync BSP ssp  EAO model  BSP ssp   mini    resnet cifar  mobilenet cifar  resnet   resnet cifar  vgg cifar  resnet cifar  vgg cifar  speedup accuracy compute communication ratio speedup compute communication benchmark deploy distribute DL application gpu cluster compute compress extremely gradient model around data define ratio communication computation communication computation accord observation analysis introduction sec network bandwidth compress communication accelerate training procedure exacerbates stale gradient increase training dnn model converge target validation accuracy speedup achieve network limited momentum driven algorithm focus address stale gradient improve training performance distribute dnn training network approach gain improvement training performance HDGib cluster network bandwidth gpu cluster network bandwidth confirms approach effectiveness verifies statement network exacerbates stale gradient besides speedup achieve cpu cluster network bandwidth training dnn model cpu analyze observation speedup achieve approach affected network bandwidth ratio communication computation stale gradient becomes serious decrease ratio approach achieve speedup cpu HDGib cluster network bandwidth ratio communication computation gpu cluster network bandwidth speedup model achieves significant reduction convergence extra compute dnn model network bandwidth vgg overall convergence model synchronization model compute communication per local model    cluster  cluster network bandwidth resnet  mobilenet cifar cluster  cluster network bandwidth resnet  resnet cifar vgg cifar cluster  cluster network bandwidth resnet cifar vgg cifar reduce communication library reduce algorithm implement communication library grpc mpi NCCL algorithm outperforms reduce algorithm horovod tensorflow mpi communication library tensorflow NCCL across dnn model gpu cluster network bandwidth speedup training horovod tensorflow mpi tensorflow NCCL respectively observation mpi NCCL communication library reduce training reduce algorithm training worker achieve convergence dnn model target validation accuracy training reduce algorithm longer approach bandwidth network speedup iteration gain reduce NCCL cluster network training performance reduce HDGib cluster algorithm algorithm outperforms reduce implement tensorflow NCCL vgg addition reduce synchronous communication strategy potentially suffers local optimal degradation validation accuracy dnn training asynchronous communication strategy extreme reduction stochastic error dnn vanilla sgd without momentum linear scalability analysis dnn model worker cluster network desirable improvement linear scalability convergence training achieve model worker asp model outperforms asp resnet mobilenet synchronization model mainly focus address stale gradient constrain stochastic error synchronization model achieves improvement resnet mobilenet classify observation dnn model resnet seriously affected stale gradient stochastic error dnn model mobilenet image KB image comparison worker resnet mobilenet training performance accuracy investigate varied training dnn model network bandwidth mini batch introduce hyper parameter sec model non monotonic varied training bound relatively resnet mobilenet reasonable constrain stochastic error adaptively training avoid local optimal local optimal brings stochastic error increase training achieve convergence dnn model target accuracy affect training performance evaluation validation accuracy training fix adaptive algorithm degradation validation accuracy local optimal due evaluate adaptive algorithm concentrate adaptive algorithm fix validation accuracy algorithm algorithm indicates training dnn avoid local optimal however training achieve convergence validation accuracy algorithm convergence longer algorithm resnet mobilenet model image KB image adaptive synchronization algorithm benchmark image MB image comparison training performance adaptive algorithm fix training network bandwidth convergence investigate convergence behavior synchronization model bandwidth model gain performance improvement network training verify proposition network exacerbates stale gradient affect training performance extremely model aggregate worker usually training reduce stochastic error training loop longer asp model accord evaluation communication occupies significant portion overall training usually network around network increase communication overhead impact training performance network verify network training combine analysis serious stale gradient extra communication efficient network improve training performance commercial AI equip powerful computation device tesla faster gtx bandwidth network gain due serious stale gradient network bandwidth usage bottleneck analysis data exchange occurs server worker data transfer worker evaluate network usage server worker cluster network bandwidth network usage achieve network respectively analyze observation network bandwidth memory bandwidth server bottleneck distribute dnn training parameter server architecture asp memory network bandwidth bottleneck network network usage achieve network bandwidth usage achieves degradation network bandwidth usage limitation memory bandwidth memory bandwidth becomes bottleneck network bandwidth training dnn network analysis recent image KB image network usage resnet vgg gpu cluster gbps gbps bandwidth performance improvement momentum rate model consists optimization module momentum adaptive constrain descent direction rate calculates descent distance clarify contribution generate synchronization model rate define BB traditional decay propose BB achieves improvement training decay rate calculate approximate BB worker resnet  resnet cifar rate varies across worker adjusts training procedure adaptively image KB image rate calculate approximate BB resnet resnet conclusion ongoing efficient synchronization model distribute DL HDGib cluster improve training performance various cnn model data maintain accuracy momentum driven adaptive synchronization adaptive BB approximation synchronization model crucial convergence behavior distribute DL analyze various optimize  DL theoretical feature distribute distribute friendly synchronization model propose synchronization model stable convergence guarantee HDGib cluster convergence model accuracy guarantee future explore development probability suitable non convex explore distribute friendly probability context research