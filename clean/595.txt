 growth data intensive application performance network cater growth core network gbps link analyze volume traffic network challenge essential task perspective network administration network security enforcement spite availability adequate network compute memory resource traffic analysis perform rate traffic brings challenge traffic analysis built commodity compute platform distinct advantage adaptability gradation scalability explores feasibility traffic analysis handle gbps commodity compute platform analyze issue rate handle traffic commodity server GbE NIC optimize scalable packet processing pipeline leverage processing pipeline napa FP numa aware probe architecture implementation architecture commodity server rate processing internet traffic GbE interface achieve numa node suitably configure packet processing performance linearly multiple gbps additional numa node NICs implementation socket commodity server GbE NICs gbps internet traffic rate numa node optimization respect resource allocation processing pipeline report corroborate experimental previous keywords traffic analysis linear commodity server beyond gbps introduction manage secure computer network ability knowledge network requirement met network traffic analysis network traffic data various abstraction glean important information offering multi data administration network security enforcement perspective administration perspective troubleshoot resource planning forecasting network security perspective enables detection policy violation security incident enforcement perspective discovery illegal activity however network traffic analysis become challenge task decade mainly due pace evolution internet traffic bandwidth protocol application content bandwidth core network evolve gbps multiple gbps gbps rapidly increase internet traffic respect protocol apart host protocol  quic etc various tunnel mechanism evolve application legacy network content link text image traffic complex video peer peer mobile application traffic dynamic complex traffic constant adaptation traffic analysis requirement traffic analysis scalable flexible met software programmable commodity purpose hardware platform however traffic analysis realm specialized hardware entirely custom built partially custom built FPGAs network processor custom built hardware generally expensive commodity hardware due initial development development cycle lack flexibility scalability advantage relatively straightforward achieve desire traffic processing performance hardware circuitry specific task straightforward software commodity hardware achieve comparable processing performance spite hardware capacity challenge processing stack commodity platform hardware operating networking stack application framework generic variety application challenge rate processing traffic processing pipeline commodity hardware architected optimize IO compute memory operation strategy resource allocation reuse concurrency detail traffic analysis combination packet inspection analysis collection statistic entire traffic however prefer due privacy concern storage requirement packet inspection capability traffic analysis limited visibility information layer analysis prefer choice due visibility available information traffic reduce storage computation capture availability technique processing commodity server packed resource significant advancement decade processing memory bandwidth IO bandwidth introduction multi core multi processor technology improve cache processor interconnects increase processing significant manner integration multiple memory controller processor increase memory channel per controller increase accessible memory bandwidth per cpu core multi processor non uniform memory access numa architecture augment memory access distribute memory across multiple numa node introduction pci express generation integration IO hub processor introduction intel data IO non uniform IO access  significantly increase accessible IO bandwidth RSS available NICs another important feature achieve performance distribute packet load across multiple cpu core packet processing research community towards enable development traffic analysis sophisticated commodity platform specialized framework architecture technique packet processing effort probe representative traffic analysis application stage packet processing notable probe software commodity hardware focus processing multiple gbps achieve gbps performance tightly link packet processing pipeline commodity hardware linux networking stack challenge warrant specialized software architecture numa aware pipelined architecture probe napa FP software architecture commodity server parallel architecture extract optimum performance commodity server NICs rate processing leverage multi core multi processor numa architecture server RSS feature NICs combine optimization technique principle building commodity hardware probe scalable performance prototype implementation probe principle experimental commodity server scalability performance additional NICs numa node ascertain gbps probe contribution hardware capability commodity server evaluate feasibility processing network traffic gbps rate establish processing pipeline architecture realize gbps probe commodity server along experimental validation architecture traffic processing capability linearly handle multiple gbps multiple numa node optimal technique parameter setting engineering performance deliberate along experimental validate variation traffic organize describes related analysis  requirement feasibility processing multiple gbps traffic commodity server describes propose architecture napa FP enumerates optimization technique adopt improve application performance evaluation napa FP finally concludes related decade researcher contribute evolve technique packet processing commodity compute platform focus performance linux optimize usage interface compute memory resource specific contribution identify bottleneck optimize performance stage network packet processing distribution packet arrival assignment compute memory resource optimize memory bandwidth usage efficient data structure algorithm prominent packet capture framework PF netmap  intel dpdk framework perform packet capture rate gbps loss prominent strategy framework replace interrupt driven driver polling mode driver bypassing linux kernel stack packet processing routine zero copying technique cpu pin avoid context switch implementation framework available source adaptation depends usability aspect framework exploit RSS feature NICs achieve data parallelism performance recent propose load balance technique RSS packet load optimize manner technique dynamically allocate cpu core accommodate increase input load nathan  performance bottleneck proposes packet processing commodity server  framework performance describes limit factor overcome limitation  pci interaction host impact performance network packet processing application  liao  packet processing overhead respect hardware architecture operating proposes server architecture customize memory access dma efficient payload movement packet capture processing beyond gbps commodity compute platform intel dpdk choice  gbps traffic capture retention commodity hardware capture timestamp gbps network traffic tailor network driver non volatile memory express nvme technology storage performance development kit  framework intel XL gbps NIC developed  packet capture selective dump packet gbps network achieve gbps capture capability intel XL gbps NIC dpdk underlie framework packet capture  evaluate challenge associate gbps packet capture packet significant role performance relatively trivial rate thread packet NICs struggle aggregate gbps packet furthermore commodity packet processing framework conclude intel dpdk framework beyond gbps  focus sketch network measurement gbps link multi core processor mellanox ConnectX intel dpdk packet IO experimental combination option maximum performance obtain packet packet significant amount report packet processing architecture commodity hardware purpose processing virtual router introduction netflow cisco monitoring become effective monitoring network    network probe deployed commodity hardware source software effectiveness gbps network recently extend capture network traffic gbps nvme disk similarly  dpdk  dpdk intel dpdk probe focus monitoring gbps minimal resource horizontally traffic rate capture analyze trend analysis emphasizes machine technique adaptive construction feature export enable compute probe fpga NICs implementation NetFPGA 1G  platform version publicly available NetFPGA 1G repository gbps link rate propose architecture reconfigurable hardware network analysis architecture multiple parallel mapping processor intensive task fpga rick evolution probe technology source commercially available probe feature performance report brings assessment resource allocation optimize gbps probe scalable performance additional numa node NICs commodity server numa architecture linux operating validate approach target performance feasibility gbps processing commodity server establish feasibility processing gbps traffic commodity server analyze detail resource requirement gbps probe processing available capability representative commodity server capability representative commodity server server GbE NICs peripheral interface adequate IO bandwidth along access memory pci gen interface IO multiple memory channel ddr mhz address latency issue introduce multiple dedicate cache feasibility analysis validation server intel xeon skylake processor core ghz representative core rate processor IO bandwidth NIC input interface traffic interfaced processor pci interface pci interface adequate bandwidth transfer input traffic gbps rate widely available pci gen lane lane giga transfer per GT per direction pci gen cumulative bandwidth GT effective bandwidth gbps encode pci gen elucidate performance implication pci interact host architecture device driver recommend achieve performance IO performance intel skylake SP processor feature data input output DDIO non uniform input output access  leveraged performance memory bandwidth commodity server multiple channel memory module sufficient cycle latency bottleneck server channel ddr mhz memory theoretical bandwidth GB per processor therefore local memory attach processor access rate concern ddr latency bottleneck achieve packet processing performance therefore consideration restrict data movement within numa node efficient layer cache available processor processor commodity server intel xeon skylake SP memory controller memory channel processor around KB data cache MB cache MB cache latency respectively manage cache locality becomes critical ensure nil packet cpu cycle commodity numa server multi processor multi core configuration enable parallel computation mention server consideration intel xeon skylake ghz processor core giga cycle available per per numa node parallelize processing exploit multiple core utilize combine cycle important consideration cycle budget individual core packet gbps rate numa node separation described intel xeon scalable platform brief document intel skylake SP processor chip integrate memory controller memory module insert local slot integrate pci controller directly subset pci slot feature respectively numa  separation numa node establish packet processing pipeline localize numa node important consideration distinction local remote resource data IO DDIO additional feature available numa architecture ethernet controller processor cache primary destination source IO data instead memory DDIO feature available local socket IO IO device core interface IO device memory numa node processing requirement gbps probe resource requirement processing rate network traffic mainly impact packet rate packet per affected packet subsection analyze processing requirement gbps probe respect average scenario packet rate scenario maximum packet rate scenario gbps traffic processing packet packet ethernet frame packet rate mpps maximum packet rate GbE link available per packet processing translates compute cycle per core ghz processor available compute cycle per packet increase core parallelize processing core commodity server cycle processing packet processing budget estimate average scenario typical internet traffic per report internet average packet typical tcp IP network scenario average packet translates processing budget around cycle per packet budget available packet scenario allows cycle perform cpu intensive operation packet numa aware pipelined architecture probe napa FP briefly probe explain approach napa FP software architecture probe subsequently napa FP module described detail along decision optimization technique apply brief description probe define rfc IP packet passing observation network interval packet belonging packet header source IP destination IP source destination protocol collectively tuples probe component analysis information pertain network deployed gateway network analysis information feature depends objective analysis sophistication probe configuration feature tuples derive statistical parameter feature export probe collector technology netflow  image KB image probe deployment scenario passive mode approach development napa FP realize probe perform gbps efficient software architecture meticulous choice algorithm technique optimize packet processing stage packet processing various hardware software suite manage NIC buffer pci bus memory cache cpu probe additionally export collector previous multi processor numa architecture available commodity server feasibility analysis representative commodity platform hardware component resource processing gbps traffic numa node manage efficiently optimize network application achieve desire performance complex offs parameter therefore choice involves selection suitable commodity server configuration parameter OS kernel driver application mention earlier publish technique framework principle specific usage napa FP effort combine architecture probe enable internet traffic analysis identify technique framework tune parameter specific propose architecture explain challenge distribute memory across cpu socket numa architecture differential memory access latency access memory local processing cpu remote attach cpu challenge packet processing pipeline local numa node tune achieve maximum performance ensure network packet NIC insert pci slot directly pci controller cpu exclusively cpu memory local cpu numa node comprise cpu memory NIC logical processing happens multiple replicate numa node server linearly packet processing performance gbps traffic rate cache access cycle cache access cycle cache access cycle local node memory access around cycle remote node memory access cycle approximately therefore due consideration reduce costly memory access optimum performance napa FP abstract approach processing exclusively numa node handle gbps traffic NIC processing comprises multiple independent processing pipeline processing pipeline functionally decompose stage processing pipeline spawner multiple packet queue NIC spawn processing pipeline correspond queue architecture napa FP image KB image architecture napa FP representation linear processing pipeline resource local pipeline dependency pipeline functionally pipeline consists stage namely packet capture construction export stage pipeline implement thread data transfer thread link queue fix buffer queue data structure commonly situation buffer queue fix buffer allocate priori continuous memory napa FP buffer queue performs link queue due continuous memory spatial locality cache performance napa FP logical namely packet metadata packet capture construction stage metadata construction export stage copying packet degrades performance significantly packet processing architecture hence napa FP copying NIC buffer dma buffer unavoidable image KB image linear processing pipeline PP napa FP packet capture module module responsible packet NIC packet metadata available stage processing purpose networking stack linux kernel cannot capture gbps rate packet capture mechanism napa FP dpdk framework packet capture dpdk various optimization tuning apply dpdk described mellanox ConnectX RSS feature distributes incoming packet multiple queue aware manner dpdk expose queue user application queue implement  buffer label dpdk RX queue capture gbps internet traffic empirically estimate validation linear processing pipeline correspond queue spawn pipeline packet capture stage implement thread responsible packet dpdk RX packet packet capture module extract relevant detail construction detail construction module packet metadata dedicate cpu core reserve packet capture thread thread pin assign core reduces context switch data local cpu cache construction module construction module responsible construction harvest information hash maintain information information packet update hash index correspond location hash hash compute function tuples namely source IP destination IP source destination protocol procedure update information algorithm hash performance critical napa FP insert lookup update manage expiration active inactive timeouts introduce optimization respect hash RSS hash usage described sub algorithm image KB image algorithm update packet prototype implementation information harvest source IP destination IP source destination protocol duration byte packet average packet maximum packet minimum packet information harvest configurable additional parameter however impact performance memory bandwidth cpu cycle assess module consumes entry packet metadata performs lookup hash correspond node update packet information failure session node correspond tuple parameter update detail packet metadata upon meeting session signal fin rst packet tcp inactive active timeout average inter packet session duration internet traffic session hash correspond information transfer exporter metadata structure packet correspond treat belonging session fuse multiple address collector export module export module responsible export raw construction module export module correspond linear pipeline napa FP prominent export protocol netflow  udp module receives construction construct export template accordingly export template transfer designate collector address configure collector standard software instal accepts database analysis mention previously queue sufficient gbps packet capture linear processing pipeline comprise stage namely packet capture construction export spawn correspond queue allocates dedicate core stage pipeline core processing pipeline processor intel xeon skylake SP core mid processor satisfies core requirement technique framework apply napa FP technique identify essential achieve performance technique principle developed independently literature architecture combine technique significance context packet processing data task decomposition evolution multi core multi processor enormous processing application developer exploit processing application highly parallelize sub task execute parallel decomposition apply achieve parallelism namely data decomposition task decomposition napa FP data decomposition multiple NICs coarse grain load decomposition data decomposition harness RSS feature NICs besides distribute network packet across multiple cpu core RSS feature maintains affinity napa FP task decompose multiple linear pipeline per processor available cpu core pipeline completely independent multiple stage concurrently  aware processing multi processor pci slot configuration belonging processor NIC insert pci slot performance guaranteed association cpu packet processing ensure behavior non uniform input output access  briefly introduce furthermore DDIO feature available intel skylake SP processor directly copying packet NIC cache exploit association napa FP automate detection incorrect placement probe sys file linux thereby ensures packet processing function pertain NIC execute cpu NIC numa aware processing numa aware processing program data numa node completely local memory access non local memory reduce maximum extent multi processor numa node numa aware code performance numerous explain performance software numa improve adopt numa aware program processing clearly performance thread data bound numa node binding reduces latency remote memory access usage limited  bandwidth concept utilized napa FP achieve performance packet interface numa node NIC bound processing packet packet capture export handle numa node numa affinity enforce limited global memory napa FP limited global memory improve performance data structure hash buffer pool maintain independently thread independently numa node parallel thread processing achieve adequate packet processing performance resort global memory reduce complexity however increase complexity ensure thread local variable pointer thereby reduce overhead lock increase performance avoidance global memory policy easily configurable adaptable hardware platform workload global memory parallel application lock access lock costly operation avoid parallel program global memory interferes cache coherence reduces cache efficiency memory access thread processing core processing bidirectional reverse tcp connection parameter IP address protocol address reduces memory footprint data structure maintain additionally improves cache ratio temporal locality essential performance default RSS hash function output reverse hence packet processing pipeline achieve desire RSS hash function output accomplish napa FP leverage technique manipulate RSS propose  however collector unidirectional reverse export separately export module cpu core affinity priority thread default schedule thread handle operating schedule algorithm optimally allocate thread core intensive task performance achieve judiciously binding thread specific core core affinity ensures context switch overhead reduce processing resource productively maximum extent apply napa FP accord priority packet capture thread binding exclusive core remain thread bound core numa node packet capture thread thread operating data affinity hyper thread core physical core improve cache ratio data cache implement napa FP ensure construction export thread bound hyper thread core pre allocation reuse memory buffer napa FP construction module pre allocate memory link program initiation phase multiple pre allocate node organize node hash processing node reuse contiguous memory achieve cache locality contiguous memory cache ratio access sequentially napa FP exploit concept maximum information producer thread consumer thread lock lock pre allocate contiguous memory access sequentially furthermore hash construction contiguous buffer along chain obvious performance benefit assess sub improve hash performance mention previous hash computation per packet operation involves compute memory access compute memory access non negligible traffic rate gbps NIC computes RSS hash packet distribute queue available packet metadata dpdk chose instead compute hash  hash performance critical processing pipeline bottleneck data structure non marginal penalty pipeline hash finally developed efficient data structure probe napa FP hash performance hash load factor propose hybrid hash combine probe chain strategy resolve collision correspond hash bucket horizontal array reserve item whenever collision empty slot accommodate item array empty slot available item link item continuous array cache locality however array identify experimentally item continuous memory remain link bucket maintains vector indicates presence item horizontal array item hash probe horizontal array link instead probe slot horizontal array probe vector strategy horizontal probe chain hpc hash hpc hash napa FP bucket contains continuous array pointer link vector item hash bucket refers vector occupies empty slot array array node link item insert item nth array correspond remove item reset correspond vector image KB image hpc hash implementation construction avoid contention hpc hash maintain processing pipeline node hpc hash corresponds network connection maintains structure harvest upstream downstream statistic separately accord definition upstream downstream linux kernel maintains virtual memory address physical address transaction kernel load related mapping entry correspond mapping entry load reduces performance linux allows GB address faster entry translation aside buffer tlb memory coverage napa FP multiple GB reserve numa node booting subsequently packet buffer dpdk packet metadata disable throttle thread completely engage cpu schedule principle desire behavior bug thread completely prevent throttle mechanism limit amount cpu thread consume napa FP packet capture module thread minimize packet ensure behavior disable throttle feature kernel disable efficiency performance management mechanism CPUs aim reduce consumption idle decrease consumption processor firstly subsystem secondly voltage frequency reduction accomplish respectively enable subsystem execute anything cpu subsystem actually performance frequency operates decrease shut subsystem throttle frequency software subsystem governor monitor load manage accordingly adjustment usually load related governor generally adapt processor operating version latency operation efficiency slows performance therefore disabled bios kernel framework napa FP packet IO framework objective easy integrate software stack hardware mellanox ConnectX capable achieve packet capture performance chosen IO framework hardware objective dpdk packet IO framework intel chosen napa FP development dpdk packet IO framework access network interface performance comprises component namely poll mode driver pmd memory management module environment abstraction layer eal pmd memory NIC directly packet without interrupt achieve cpu utilization memory management dpdk utilizes buffer management memory allocator mempool  library functionality raw packet data memory allocate mempool eal unified initialize central dpdk component dpdk default rate packet capture performance gbps network interface  implement dpdk library various performance enhancement mechanism mention apply achieve target performance evaluation propose architecture evaluation propose architecture focus aspect evaluate individual component stage processing pipeline evaluate target traffic rate demonstrate impact optimization technique napa FP architecture performance comparison napa FP towards server configuration configuration specific hardware development napa FP  server ghz intel xeon processor processor core intel hyper thread allows core execute independent thread simultaneously logical core per processor core KB data instruction cache MB cache MB cache per processor server specification  processor intel skylake SP core core HT cache KB KB per core cache MB per core cache MB per processor  bandwidth GT ram GB per numa node GB ram mhz GbE  ConnectX 1G  NIC pci  GT development validation napa FP commodity GbE NIC mellanox ConnectX dual NIC sits pci slot dual NIC simultaneously gbps pci gen slot GT GT effective bandwidth gbps however server pci gen slot slot maximum bandwidth GT effective bandwidth gbps per NIC server pci gen slot directly processor slot mellanox ConnectX NICs handle gbps operating platform  kernel version dpdk version packet IO framework setup validate napa FP architecture principle  traffic generator generate realistic internet traffic profile actual internet traffic traffic profile generate traffic data paul internet traffic obtain packet distribution packet traffic KB http http constitute chunk internet traffic paul reveals gbps link utilization average parallel amount gbps secondly implies average rate approximately per profile traffic generator generate stateful traffic nearly rate gbps configurable application layer content furthermore grain generate traffic rate essential profile performance napa FP packet distribution organization internet traffic packet byte packet volume experimental setup testbed gbps traffic generator via gbps switch extreme network  server client generator configure generate bidirectional traffic gbps mirror feature switch configure mirror ingres traffic GbE NICs probe server switch configure multiple mirror session traffic available multiple GbE NICs probe hence NICs traffic session generate traffic generator traffic multiple NICs valid traffic NIC handle independently numa node aggregation input traffic inside probe server image KB image setup performance validation probe evaluation packet capture module mention earlier packet capture module dpdk version evaluation performance GbE NIC numa node maximum load module generate packet benchmark module generate gbps rate packet mpps  traffic generator performance measurement packet ratio PDR ratio packet packet various parameter tune capture gbps packet impact parameter packet capture performance evaluate numa node napa FP processing capability probe additional NICs bind numa node validate GbE NICs available additional NICs bound numa node traffic processing performance gbps gbps gbps respectively cpu core packet capture evaluate cpu core capture gbps traffic packet mpps numa node traffic load mpps generate  traffic generator logical cpu core PDR capture mpps traffic cpu core capture mpps PDR PDR core queue analysis highlight role multi packet queue  feature mellanox NIC performance  stride queue feature mellanox NIC handle burst packet pci access instead transfer packet NIC fifo dma buffer  transfer multiple packet instance buffer pci bandwidth buffer multiple packet thereby improves performance packet buffer accommodate multiple packet discover  activate queue due default mellanox driver modify driver invoke  lesser queue achieve rate packet capture queue gbps traffic comprise packet however retain default  probe discover  enable RSS hash available application layer  enable parameter option  beneficial average packet whereas probe intend internet traffic average packet image KB image cpu core PDR packet packet rate mpps evaluate cpu core achieve minimum PDR gbps traffic packet traffic load gbps generate packet minimum cpu core capture traffic standard tune enable core capture internet traffic  per profile mention sub image KB image cpu core capture gbps traffic packet minimum PDR dpdk RX descriptor dpdk RX descriptor lock structure transfer packet information NIC queue packet capture thread fifo queue fix array burstiness traffic accommodate slot dpdk RX adjust empirical network traffic evaluate impact PDR experimental RX descriptor per cpu core capture mpps PDR image KB image slot per dpdk RX PDR packet packet rate mpps dpdk core evaluate construction module achieve cache locality handle load factor propose hpc hash feature horizontal probe chain propose hash data structure chain estimate array parameter propose hash traffic generate per internet traffic profile mention sub evaluation propose hash hpc hash chain scheme comparison amount micro random hash hash bucket horizontal array increase concurrent node handle hpc hash performance chain scheme image KB image  probe packet chain hpc hash interpretation reader refer web version article estimate horizontal array hpc hash propose sub hash item continuous memory remain link depends simultaneous maintain hash collision bucket correspond processing pipeline construction thread hash purpose estimate assume processing pipeline gbps traffic hash maintain average instance amount collision bucket graph cumulative distribution function cdf collision graph hash item collision hash accommodate contiguous horizontal array horizontal array validate optimum performance graph hpc hash performance increase increase horizontal array image KB image cdf collision hpc hash image KB image comparison hpc hash performance horizontal array assess assess performance napa FP numa node subsequently multiple numa node http http application traffic per internet traffic profile generate traffic generator summary hardware software configuration tune parameter napa FP hardware software configuration napa FP configuration bios setting   turbo  mellanox ConnectX setting  invoked due driver setting operating  kernel dpdk setting version lts GB slot   internet traffic assess performance numa node numa node increase processing pipeline till traffic comfortably mention sub gbps maximum rate generate stateful traffic gbps  traffic generator numa node independently average napa FP linear pipeline constitutes packet capture thread extraction thread export thread thread dedicate logical core numa node augment processing pipeline utilization cpu core processing pipeline dedicate cpu core numa node gbps realistic traffic without packet analyze data traffic processing rate increase proportionally increase processing pipeline attribute non uniformity llc contention processing pipeline increase hash increase increase memory footprint cache contention processing pipeline maximum traffic percentage generate traffic  traffic ensure processing pipeline numa node core affinity thread verify command linux dynamic cpu usage various thread output entire processing numa node remain numa node entirely idle assess performance multiple numa node NICs attach numa node conduct processing pipeline numa node experimental processing pipeline cpu core distribute evenly numa node gbps traffic napa FP completely numa aware mention spite pci slot available insert fourth NIC due factor constraint adjacent sas controller however fourth NIC accommodate  traffic fourth numa node totally idle NICs hence validate napa FP scalable capable handle traffic NICs image KB image processing pipeline numa node maximum traffic gbps effectiveness various optimization technique conduct evaluate effectiveness various optimization technique napa FP optimization technique resultant performance performance disable numa  awareness packet NICs attach numa node node node node node node respectively enable global memory global packet counter variable packet incremented correspond packet capture thread regard refer sub enable bios plot graph limit global memory performance benefit approximately similarly disable performance benefit around graph conveys without numa aware processing maximum traffic server almost flattens beyond gbps flatten due excessive numa communication saturates capacity  link image KB image impact various optimization technique napa FP comparison probe earlier  dpdk  dpdk probe report performance gbps intel NIC dpdk framework horizontal resource traffic rate however advanced version adequate detail handle gbps traffic publish    fpga mpps  ghz per core core performance comparable napa FP fpga purpose NIC drawback hardware highlight intel GbE adapter propose recently accord website handle gbps packet queue however technical detail available comparison napa FP gap available technical literature enumerate detail scalable architecture gbps probe establishes capture extraction export feasible commodity server gbps scalable multi gbps traffic rate carefully numa aware processing pipeline napa FP systematically identify optimization technique various stage processing pipeline combine achieve performance scalability demonstrate processing gbps internet traffic rate numa node GbE NICs knowledge demonstrate experimental feasibility achieve gbps performance commodity server conclusion contribution feasible develop traffic analysis capable handle gbps commonly available comparatively expensive commodity compute platform generally traffic analysis realm specialized expensive hardware beyond capability purpose commodity hardware highlight purpose hardware processing fully exploit appropriate optimization technique identify combine relevant software technique principle stage packet processing pipeline architecture principle commodity server probe architecture napa FP principle generic traffic analysis intrusion detection IDS network behavior anomaly nba hence easily extend develop clearly demonstrate carefully craft software processing pipeline extract maximum performance stage improve performance manifold prototype implementation probe napa FP architecture server numa node comprise processor logical core GB ram effectiveness technique identify important probe applicability technique probe performance clearly validate merit propose architecture principle evaluate validate propose architecture rate traffic gbps suitably configure NIC numa node NICs available server gbps confirms traffic processing numa node independent propose performance scalable additional numa node NICs