spike neural network SNNs research attention recent information suitable building neuromorphic effectively however realize SNNs hardware computationally expensive improve efficiency hardware implementation programmable gate array fpga SNN accelerator architecture propose implement approximate arithmetic identify minimal width approximate computation without performance loss variable precision utilized SNN unlike conventional reduce precision apply uniformly propose variable precision allows width feasibility maximize truncation effort SNNs adopt network configuration training datasets establish performance propose accelerator architecture variable precision propose conventional reduce precision experimental width apply variable precision instead reduce precision variable precision propose architecture achieves  consumption propose reduce precision previous keywords spike neural network approximate compute programmable gate array hardware accelerator introduction neuromorphic demonstrate attempt generation computer architecture building computational neuron spike neural network SNNs generation artificial neural network anns widely realize neuromorphic unlike conventional anns lack timing dynamic SNNs employ biologically realistic model neuron mimic information processing biology information encode spatio temporal massively parallel fashion unique biological behavior SNNs suitable candidate effective implementation neuromorphic due consumption driven paradigm conventional anns SNNs incorporate collection neuron acyclic graph organize layer obvious difference neural activation conventional anns depends non linear continuous function propagation cycle whereas SNNs spike dependent sequence continuous occurrence spike membrane potential associate neuron exceeds threshold neuron spike potential fanout neuron incremented correspond incoming synaptic consist spike movement membrane potential amount movement depends spike besides membrane potential spike neuron model SNNs   model izhikevich model leaky integrate model lif image KB image SNN architecture SNN neuron architecture layer SNN spike behavior SNN neuron incoming synaptic recent researcher promising bridge gap anns SNNs computer vision task image classification detection nevertheless limitation SNNs simulate hardware computationally intensive neuromorphic described adopts SNN model simulate functionality visual cortex input image billion computation SNNs computation dynamically frequency spike brings challenge realize parallel computation commercial platform CPUs gpus recently researcher demonstrate successful attempt realize SNNs specialized hardware platform customize neural chip integration VLSI despite improve parallelism specialized hardware computation intensity concern affect hardware efficiency growth SNNs computation intensity increase rapidly computation intensity impact efficiency efficient SNN implementation desire handle computation intensity issue recent exploit possibility software approximate compute efficient SNN implementation frequency neuron spike connection neuron spike frequently computation neuron nevertheless limitation scheme completely varied SNN architecture therefore scheme generalize statistical analysis perform prior generate correspond scheme realize scheme brings extra hardware complexity handle normal scenario another fruitful direction efficient SNN implementation baseline model layer SNN precision float format performance degradation truncate fix format integer moreover recent demonstrate feasibility improve efficiency employ specialized SNN accelerator intel  neuromorphic chip realize efficient SNN however drawback SNN implement  chip cannot maintain accuracy software implementation approximate arithmetic widely apply digital achieve improve efficiency applies inexact computation significant input data conventional arithmetic consume compute faster author apply approximate adder reduce computational complexity image processing algorithm implement programmable gate array fpga due advantage error resilient application approximate arithmetic reduce hardware neural network SNN error resilient application approximate multiplier multi layer perceptron explore additionally recent demonstrates usefulness apply approximate arithmetic convolutional neural network cnn approximate compute SNN explore truncate multiplier SNN computation approximate adder implement chain achieve reduce delay however fpga approximate adder resource reduction investigate efficient accelerator architecture propose SNNs combine concept approximate compute reduce precision foremost propose architecture aim improve truncation effort SNNs layer precision format unlike previous employ reduce precision truncate uniformly variable precision introduce maximize truncation effort SNNs neuron likely spike error resilient accumulate membrane potential neuron truncate without lose overall accuracy basis propose architecture approximate arithmetic apply appropriate handle variable precision requirement reduce computation intensity arithmetic identify propose variable precision generalizes SNN architecture series statistical analysis perform SNNs specifically propose variable precision demonstrate layer layer SNNs mnist dataset moreover layer layer SNNs fashion mnist dataset investigate utilize approximate arithmetic propose variable precision propose fpga SNN accelerator implement intel arria soc development kit experimental apply conventional reduce precision adopt approximate arithmetic propose SNN accelerator consumes arithmetic fpga contribution summarize statistical analysis variable precision minimum numeric precision layer SNN model reduces SNN hardware implementation analysis SNN model optimal numeric format SNN model available literature propose hardware architecture SNN implementation utilizes approximate adder propose synthesis propose SNN hardware architecture achieves resource consumption baseline SNN organize background information spike neural network software simulation variable precision SNN correspond hardware accelerator architecture approximate arithmetic implementation analysis finally concludes spike neural network preliminary architecture SNN ann neuron organize layer neuron adjacent layer fully ann propagates spatial information layer layer output neuron generate pre synaptic input neuron previous layer contrast neuron SNN information temporal domain series spike describes output neuron SNN popular neuron model spike behavior SNNs   model izhikevich model leaky integrate model lif lif commonly neuron model due computation complexity hardware implementation lif model neuron update membrane potential equation membrane potential neuron denotes pre synaptic input spike activity incoming synaptic synaptic incoming synaptic spike pre synaptic input sum synaptic membrane potential continuously update exceeds threshold thereafter neuron spike reset membrane potential baseline SNN model justify propose variable precision layer mnist layer mnist SNNs mnist database layer fashion layer fashion SNNs implement evaluate fashion mnist database database classification benchmark previous SNN implementation input image input layer construct neuron whereas output layer consists neuron database layer model hidden layer insert input layer output layer hidden layer consists neuron model configuration classification performance baseline model lif neuron model spike binary membrane threshold cycle terminate membrane update baseline model training scheme described layer configuration accuracy baseline model  hidden   accuracy mnist mnist fashion fashion denotes layer SNN layer consists neuron layer consists neuron layer consists neuron propose variable precision methodology focus reduce computation intensity SNNs advantage approximate compute propose variable precision enables apply approximate compute SNNs subsection concept propose variable precision subsection describes realize variable precision approximate arithmetic subsection detail identify variable precision baseline SNN model precision requirement SNNs address computation intensity issue impose SNNs reduce SNNs instead conventional precision float format default numerical precision baseline model convert fix format reduce precision described baseline model truncate fix fix format integer ensures baseline model achieve accuracy precision float format default numerical precision format truncate memory footprint compute resource utilization nevertheless truncation effort maximize truncation uniformly perform across increase truncation effort fruitful direction reduce width adjacent layer remain reduce precision without loss mnist mnist model accuracy width layer layer reduce another mnist layer layer truncate integer without reduce inference accuracy similarly baseline model fix format integer layer layer improves truncation effort model employ reduce precision challenge obtain generalize numerical format SNNs architecture identify optimal layer wise numerical format SNN model combination image KB image inference accuracy baseline model fix format width integer image KB image inference accuracy baseline model layer layer width integer remain integer image KB image inference accuracy baseline model layer layer width integer remain integer maximize truncation effort propose variable precision considers spike sequence neuron neuron generates spike fanout update membrane potential neuron layer neuron spike frequently fanout likely accumulation membrane voltage neuron spike neuron layer mnist generate spike truncate integer fix integer concludes spike sequence neuron model performance maintain spike sequence neuron model propose variable precision fix format integer neuron spike numerical precision remain reduce propose variable precision advantageous primarily due aspect truncation effort maximize reduce operation perform neuron instead layer model easy realize hardware approximate arithmetic computation neuron spike frequently approximate computation neuron spike image KB image spike generate neuron layer mnist fix format width integer identify propose variable precision SNNs activity associate neuron activity ratio specify significant spike neuron actively contribute overall spike correspond layer significant spike neuron obtain histogram spike sequence correspond layer histogram spike activity neuron layer mnist dash indicates activity indicates neuron correspond layer spike significant spike activity propose variable precision apply incoming neuron spike dash neuron spike frequently incoming neuron spike frequently truncate fix integer neuron width incoming reduce detail identify variable precision baseline SNN model described subsection image KB image histogram neuron spike layer mnist dash indicates activity algorithm pseudo code apply propose variable precision SNN identify variable precision format jth neuron ith layer significant spike obtain activity histogram neuron spike ith layer jth neuron spike significant spike jth neuron integer otherwise jth neuron integer algorithm image KB image algorithm identify variable precision format  approximate adder propose variable precision approximate adder available literature focus resource reduction gate achieve performance application specific integration circuit ASIC arithmetic however fpga device logic operation realize lut ASIC approximate arithmetic perform mapped fpga device fpga specific approximate adder architecture propose adder lut xilinx fpga logic generate sum logic generate output simplify reduce amount lut target device intel arria fpga therefore modify adaptive  intel fpga truth adder generate sum output implement adder fpga consume LUTs lut calculation modify truth addition approximation computation realize perform computation however directly input without logic implement error however reduce lut consumption propose architecture approximate truth approximate adder input   addition  image KB image lut usage adder approximate adder adder approximate adder variable precision identify optimal variable precision baseline model classification performance baseline model evaluate various activity width combination variable precision specifically activity fix format integer neuron activity remain convert fix integer however obvious challenge optimize activity width combination variable precision overcome issue parameter hardware establish estimate hardware SNN layer employ propose variable precision hardware layer obtain equation hardware layer denotes activity layer width neuron activity denominator associate hardware fix format described subsection lut consumption adder hence adder LUTs similarly numerator hardware propose variable precision neuron activity hardware incoming neuron fix format hardware apply adder hardware approximate indicates activity incoming neuron layer fix format integer optimal activity width SNN model layer layer scheme layer SNN variable precision layer layer identify variable precision layer layer layer layer scheme baseline model propose variable precision identify summary without reduce classification accuracy baseline model input layer hidden layer propose variable precision adjacent hidden layer hidden layer output layer classification accuracy baseline model propose variable precision activity model   width activity hardware  mnist layer layer layer layer mnist layer layer layer layer layer layer fashion layer layer layer layer layer layer fashion layer layer layer layer layer layer layer layer denotes normalize accuracy mnist model layer layer propose variable precision italic indicates optimal achieve hardware underline indicates optimal combination achieve hardware incoming layer hardware bold summary software simulation SNN model computation layer layer computation computation layer layer layer layer computation connection fashion model although precision reduce reduce precision hardware implementation computation layer computation SNN model available literature layer layer configuration accord computation therefore accomplish SNN operation computation variable precision convolutional spike neural network recent demonstrate flexibility transform convolutional neural network cnns convolutional spike neural network  SNNs  illustrate classification performance challenge datasets cifar imagenet convolution operation matrix slide input data associate server input spike activity hence propose variable precision cannot utilized convert convolutional conv directly nevertheless typical CSNN consists fully FC layer FC convert propose variable precision format reduce computation complexity  reduce precision apply conv layer minimize propose variable precision apply FC layer enable approximate compute identify usefulness propose variable precision CSNN series perform layer CSNN cifar dataset CSNN consists conv layer FC layer conv layer kernel output feature maximum pool layer insert conv layer FC layer FC layer neuron respectively FC layer consists neuron output predict outcome CSNN normalize via softmax simulate spike activity CSNN  source SNN simulation perform temporal simulation cycle terminate simulation sample CSNN achieve accuracy cifar dataset reduce precision described subsection CSNN truncate fix integer algorithm associate FC layer convert propose variable precision format without downgrade classification accuracy conv layer FC layer truncate fix format regard associate FC layer layer fix format therefore regard CSNN architecture computation FC layer approximate via propose variable precision reduce precision apply conv reduce computational complexity conv layer classification accuracy layer CSNN propose variable precision activity  width activity hardware   FC  FC  FC denotes normalize accuracy layer CSNN convolutional conv layer fully FC layer propose variable precision italic indicates optimal achieve hardware underline indicates optimal combination achieve hardware incoming layer hardware bold fpga implementation SNN approximate compute performance efficiency hardware accelerator usually desire SNN implementation fpga SNN accelerator architecture propose approximate arithmetic newly introduce propose accelerator utilize finding software simulation overall architecture propose fpga accelerator architecture target intel arria soc development kit contains neuron buffer module buffer module temporarily neuron respectively neuron status register module status neuron layer processing module responsible computation controller module coordinate operation module addition chip memory cache external ddr memory model parameter load chip memory computation processing module approximate adder SNN computation implement operation data significant compute approximate image KB image diagram propose SNN accelerator architecture approximate compute processing approximate adder approximate adder propose fpga architecture approximate multi adder built resource consumption adder prefix adder prefix approximate adder width significant calculate approximate significant calculate approximate remain calculate implement propose SNN accelerator architecture approximate resource  consumption approximate adder reduce adder error distance med relatively resource consumption error distance lut adder  EA  med adder approximate adder approximate processing cycle buffer correspond input neuron status neuron status register computation correspond status processing perform parallel operation neuron status logic operation adder accumulate computation perform output neuron multiple cycle computation output neuron former layer neuron computation neuron previous output neuron neuron buffer computation cycle newly accumulate neuron accumulation neuron evaluate spike module status neuron model threshold preset threshold neuron neuron buffer future accumulation fed shift register threshold neuron reset zero neuron status fed shift register initial neuron neuron buffer replace computation layer neuron accomplish neuron status shift register transfer neuron status register layer computation adder processing adder compose adder approximate adder analyze previous computation layer accurate therefore hardware architecture adder implement adder others implement approximate adder simplify adder arrangement apply approximate adder adder input approximate adder input approximate adder otherwise input adder adder computation perform approximate adder perform adder due feasibility approximate computation training rearrange data computation cycle approximate computation computation occupies roughly computation cycle image KB image adder adder approximate adder memory memory compose component external ddr memory chip memory buffer neuron buffer register neuron status register baseline SNN architecture fully therefore amount parameter exceeds capacity chip ram intel fpga propose parameter ddr memory computation load chip buffer fed processing amount neuron relatively however layer various model neuron various layer handle scenario neuron ddr memory load chip neuron buffer propose MB ddr memory ddr interface bandwidth configure byte bandwidth available designate fpga cycle data ddr memory diagram buffer buffer configure interface depth implement therefore cycle parameter processing interface accommodate parameter apply buffer buffer spent load hidden computation depth cache buffer  buffer image KB image diagram buffer propose SNN accelerator neuron buffer compose width depth memory cache layer neuron SNNs layer SNN neuron neuron remain image KB image diagram neuron buffer propose SNN accelerator neuron status register characteristic SNN computation SNN computation normal neural network neuron generate output threshold neuron threshold neuron generates output otherwise output neuron layer perform computation layer output layer zero SNN output usually obtain multiple computation cycle reduce amount computation neuron status register compose register SNN computation input spike load ddr memory layer computation neuron status previous layer evaluate computation perform neuron previous layer image KB image diagram neuron status register propose SNN accelerator controller implementation controller layer SNN model properly coordinate operation layer parameter configuration register configuration register computation reading input spike computation layer PE LN computation neuron layer evaluate computation perform layer otherwise computation remain layer omit operation input spike image KB image propose SNN accelerator LN layer SNN model layer inner loop neuron layer operation layer layer SNN model layer output SNN model generate operation terminate otherwise computation layer perform input spike cycle counter incremented SNN training database SNN output generate within cycle therefore cycle terminate operation output neuron datapath propose SNN accelerator architecture propose SNN accelerator former mnist model explain overall datapath SNN implementation propose accelerator mnist model layer neuron respectively neuron layer layer configuration register cycle computation computation input spike binary load neuron status register load ddr memory buffer buffer cache load compute neuron layer computation neuron layer neuron zero load neuron ddr neuron buffer mention initial data load computation computation neuron status correspond layer neuron load neuron status register buffer respectively processing  accumulate adder accumulation computation neuron remain status load compute computation neuron layer spike module preset threshold threshold fed shift register neuron buffer otherwise fed shift register neuron buffer future computation layer neuron compute buffer propose architecture computation neuron neuron load buffer data load computation neuron addition input spike load neuron status register layer neuron computation layer neuron compute status shift register neuron neuron buffer shift register load neuron status register register computation layer neuron status load computation layer computation newly compute neuron layer update neuron buffer neuron layer neuron neuron buffer ddr computation layer layer neuron computation neuron status computation layer computation buffer layer neuron computation output generate SNN model datapath apply analysis propose SNN accelerator architecture implement verilog hdl chip memory IP interface ddr fpga logic intel fpga external memory interface  IP synthesize rout intel  prime pro target device intel arria soc development kit arria SX  fpga device simulation propose architecture perform extensive vector pre SNN model mnist database  verify functionality route simulation perform obtain signal activity file accurate consumption estimation efficiency improvement propose architecture reference architecture implement reference architecture architecture propose architecture adder adder implementation architecture accuracy reference model achieve relative accuracy respect accuracy float achieve frequency mhz implement approximate adder improvement approximate adder significant throughput calculate frequency average cycle generate SNN output throughput due achieve frequency resource consumption propose approximate adder consumes  reference register  due datapath architecture register  propose achieve consumption propose consumes  due approximate adder logic approximate adder simpler reduce signal toggle rate reduce consumption consumption per image propose achieves average reduction reference SNN model implementation reference propose  frequency mhz  register  dynamic throughout reference network img mnist mnist fashion fashion img reference network mnist mnist fashion fashion conclusion efficient fpga spike neural network SNN accelerator architecture propose implement approximate adder analysis layer wise precision requirement SNN model perform feasibility apply variable precision layer software precision combination maintain accuracy obtain finding software hardware accelerator architecture propose propose hardware architecture approximate arithmetic handle variable precision requirement architecture carefully tailor target fpga device implementation propose SNN accelerator achieve reduction  reduction consumption propose hardware architecture accelerate SNN computation addition apply neural network model application approximate compute desire future feasibility apply approximate compute SNN investigate addition approximate compute strategy spike neuron model explore