alternate direction multiplier ADMM powerful operator splitting technique structure convex optimization due relatively per iteration computational ability exploit sparsity data particularly suitable optimization however prohibitively compute instance although ADMM parallelizable feature rarely exploit implementation exploit parallel compute architecture graphic processing gpu accelerate ADMM solver osqp implementation ADMM quadratic program source cuda implementation magnitude faster cpu implementation previous keywords graphic processing gpu compute quadratic program alternate direction multiplier introduction convex optimization become standard engineering signal processing statistic finance machine application seek optimization dimension classical optimization algorithm interior fail decade operator splitting proximal gradient alternate direction multiplier ADMM gain increase attention application dimension exploit sparsity data efficiently easily parallelizable moreover requirement accuracy moderate data arbitrariness objective operator splitting return medium accuracy reasonable computational effort graphic processing gpus hardware accelerator unmatched amount parallel computational relatively price memory bandwidth conventional cpu beneficial application amount data gpus application machine training neural network autonomous software machine pytorch tensorflow theano CNTK native gpu acceleration however perception gpus numerical solver linear program LPs quadratic program QPs explores possibility massive parallelism gpus accelerate QPs solver ADMM osqp solver author demonstrate gpus accelerate linear arise approach accelerate osqp replace linear solver indirect iterative implement gpu moreover perform vector matrix operation gpu improves performance implementation author gpus LPs QPs batch numerous within operation solver outline introduce summarize algorithm osqp solver alternative linear arise osqp summary gpu program strategy implementation detail propose gpu solver demonstrates performance solver numerical finally concludes notation denote dimensional matrix symmetric positive semi definite matrix denote identity matrix vector appropriate dimension respectively vector denote euclidean norm norm matrix denote norm gradient differentiable function evaluate denote nonempty convex denote euclidean projection onto euclidean projection onto nonnegative nonpositive  denote respectively description QP optimization variable objective function define positive semidefinite matrix vector constraint matrix vector linear equality constraint encode optimality infeasibility introduce variable rewrite equivalent optimality lagrange multiplier associate constraint exist satisfy primal dual exists infeasible certificate primal infeasibility similarly exists dual infeasible certificate dual infeasibility refer reader prop detail osqp solver osqp source numerical solver convex QPs ADMM competitive faster commercial QP solver iteration osqp algorithm scalar relaxation parameter penalty parameter osqp diagonal positive definite matrix easily computable algorithm evaluates euclidean projection onto operator elementwise image KB image solvable sequence generate algorithm converges primal dual primal dual infeasible iterates converge sequence converges certify infeasibility primal infeasible satisfy whereas satisfy dual infeasible thm termination criterion iterates define primal dual residual author satisfies optimality regardless solvable solvable residual converge zero prop termination criterion detect optimality implement tolerance chosen relative algorithm iterates termination criterion detect primal dual infeasibility implement almost satisfy infeasibility tolerance kkt algorithm equality constrain QP equivalent linear obtain refer matrix kkt matrix osqp computes compute factorization kkt matrix perform backward substitution kkt matrix symmetric quasi definite ensures nonsingular define factorization diagonal kkt matrix iteration counter osqp performs factorization algorithm reuses factor subsequent iteration precondition weakness ADMM inability effectively ill convergence data badly precondition heuristic aim convergence osqp variant ruiz  algorithm computes scalar diagonal positive definite matrix effectively modify optimization variable data image KB image parameter selection osqp default choice parameter critical ADMM convergence rate however choice determinant iteration satisfy termination criterion osqp associate equality constraint fix satisfactory performance algorithm across compensate sensitivity osqp adopts adaptive scheme update iteration ratio norm primal dual residual propose parameter update scheme algorithm robust introduces additional computational burden update kkt matrix refactored update perform runtime algorithm precondition conjugate gradient alternative equality constrain QP algorithm indirect eliminate reduce kkt obtain reduce kkt matrix positive definite allows conjugate gradient CG conjugate gradient CG iterative linear symmetric positive definite matrix computes linear iteration thm however linear aim terminate iteration yield approximate equivalent unconstrained optimization minimizer characterize conjugate direction nonzero vector conjugate respect successive minimization along conjugate direction evaluate minimizes expand subspace span previous conjugate direction thm minimization residual conjugate gradient various choice conjugate direction instance eigenvectors conjugate direction respect impractical compute matrix cornerstone CG ability generate conjugate direction efficiently computes direction previous direction imposes computational memory requirement direction compute linear combination negative gradient previous direction scalar conjugacy requirement conjugate direction negative gradient combine successive minimization computation conjugate direction yield CG precondition CG optimization sensitive improve convergence precondition linear coordinate transformation nonsingular matrix apply CG transform linear yield precondition conjugate gradient PCG algorithm alg explicitly image KB image preconditioner satisfy linear easy simplest choice diagonal jacobi preconditioner contains diagonal easily computable advanced choice incomplete cholesky incomplete polynomial  incomplete  approximate decomposition sparsity computationally cheap polynomial  chebyshev polynomial  bound spectrum gpu architecture program strategy gpus purpose compute decade variation architecture restrict discussion nvidia turing architecture concept apply nvidia gpus detail refer reader gpu consists array multiprocessor SMs contains multiple integer float arithmetic local cache memory scheduler ram gpu global memory contrast memory local SM SM turing generation memory global memory typically memory latency bandwidth faster ram bandwidth uncommon gpu global memory whereas ram limited challenge gpus leverage increase processing core develop application parallelism nvidia overcome challenge cuda purpose parallel compute platform program model cuda architecture cuda extension program nvidia thread cooperatively explains thread organize cooperative cuda achieves scalability kernel kernel function execute gpu define global keyword contrast regular function kernel execute parallel thread thread executes code data concept instruction multiple data simd thread specify kernel refer kernel launch thread hierarchy kernel specify code execute thread thread hierarchy dictate individual thread organize cuda hierarchy organize thread grid grid contains multiple contains multiple thread kernel launch specifies grid thread per thread within cooperate subproblem partition independent subproblems programmer grid thread parallel schedule available SMs concurrently sequentially available hardware resource available schedule SM thread within unique thread index accessible built variable threadIdx define dimensional vector allows thread indexed dimensional allows index domain similarly within grid unique index accessible variable blockIdx define dimensional vector allows dimensional index image KB image numerical performance axpy routine cpu gpu average memory throughput average float performance accelerate numerical numerical extensive float operation performance solely float performance although gpus magnitude float CPUs memory bandwidth limit performance numerical operation fortunately gpus magnitude memory bandwidth utilize potential easy task parallel gpu program strategy listing implement linear algebra subprogram blas routine axpy cpu code loop iterate gpu implementation axpy listing code cpu version important difference loop replace instead thread iterate loop thread gpu compute thread ID data thread operating global thread ID calculate local thread index index disables thread thread ID thread launch thread usually idle thread negligible image KB image image KB image achieve memory throughput float performance cpu gpu implementation axpy operation hardware specification plot obtain perform axpy operation various vector operation data float operation axpy operation compute average memory throughput average float performance linearly vector plot vector gpu implementation approximately faster however cannot accelerate gpus gpu kernel launch data transfer constant overhead cannot amortize gpu maximum memory throughput theoretical peak whereas peak float performance around theoretical peak peak specification nvidia geforce RTX gpu numerical performance axpy clearly limited memory bandwidth reduction reduction operation vector associative binary operator return scalar abstract formulation allows formulate operation reduction others sum maximum norm norm etc difference reduction reduction latter reduces individual output vector computes reduction exist efficient parallel implementation reduction reduction reformulate easily accelerate gpu cuda library exist multiple library cuda toolkit implement various function gpu summarize sequel nvidia library thrust cuda library standard template library STL interface performance parallel application essential data parallel primitive scan sort reduce cuBLAS cuda implementation blas enables easy gpu acceleration code blas function cuBLAS api function implement inner axpy operation scalar vector multiplication computation norm  cuda library contains linear algebra subroutine handle sparse matrix matrix sparse matrix format described sparse matrix format coo coordinate coo format simplest sparse matrix format mainly intermediate format perform matrix operation transpose concatenation extension upper triangular symmetric matrix nonzero nnz array dimension nnz    api assumes index sort within representation unique matrix coo representation zero index throughout csr compress sparse csr format differs coo format  array compress csr format compression understood  nonzero array calculate cumulative sum array insert zero array obtain array denote   array  array difference consecutive nonzero  apply recursively matrix csr representation csr format sparse matrix vector multiplication spmv  csc compress sparse csc format differs csr format format index compress compress array dimension denote  matrix csc representation csc format directly computation  however interpret csc representation matrix csr representation mapping gpu acceleration osqp profile driven software development identify computational bottleneck code performance increase remove identifies analyzes computational bottleneck osqp QPs remove gpu parallelism osqp computational bottleneck QP denote nonzero matrix profile osqp code reveals instance operation execution potential bottleneck typically QP variable constraint evaluate termination criterion sparse matrix vector multiplication perform computation ADMM iteration solver considerably hence osqp evaluates criterion iteration default overall computational burden reduce algorithm terminate iteration counter multiple discus matrix gpu memory sparse matrix vector multiplication perform efficiently gpu computational bottleneck linear solver tackle kkt computational factor kkt matrix becomes prohibitively issue limit parameter update improve convergence rate algorithm kkt matrix refactored furthermore ADMM iteration evaluate backward substitution cannot fully parallelize describes efficient gpu implementation PCG avoids matrix factorization profile reveals matrix  procedure described algorithm demand bottleneck compute norm matrix matrix pre diagonal matrix equivalent discus parallelize operation gpu representation matrix osqp matrix csc format moreover symmetric upper triangular actually memory prefer matrix gpu memory csr format superior spmv performance gpu however csr format compute around compute inefficiency avoid csr format memory requirement similarly upper triangular memory efficient compute therefore csr format improves spmv performance vector ADMM iterates gpu memory perform matrix vector operation gpu reduces considerably memory transfer gpu memory reduce kkt avoid factor kkt matrix reduce kkt PCG evaluates matrix vector multiplication easily parallelize although osqp parameter matrix numerical choice PCG converge slowly understood reduce kkt matrix effectively increase matrix convergence rate PCG improve instead choice iteration algorithm reduce iteration algorithm considerably linear reduces coefficient matrix explicitly instead matrix vector evaluate preconditioner jacobi preconditioner amount diagonal matrix vector diagonal jacobi preconditioner compute compute diagonal denotes parameter update available compute becomes extremely easy parameter update computationally cheap update preconditioner allows update osqp numerical perform update iteration termination criterion exactly algorithm converge motivate termination PCG meaning approximately accurately iteration progress achieve perform relatively PCG iteration obtain approximate initialize algorithm compute previous ADMM iteration termination criterion algorithm essential reduce runtime ADMM PCG return accuracy ADMM converge diverge PCG solves subproblems unnecessarily accuracy increase runtime ADMM SCS solver algorithm decrease function ADMM iteration counter adopt strategy ADMM residual primal dual residual parameter ensures geometric primal dual residual norm ADMM residual norm algorithm depends ADMM residual compute evaluate ADMM termination criterion evaluate criterion ADMM iteration matrix  compute norm csr representation sparse matrix allows efficient computation norm  array defines array correspond matrix transpose efficiently compute norm matrix equivalent norm transpose naive approach thread per compute norm approach efficient workload distribute poorly thread zero another memory access almost randomly thread iterates considerably deteriorate performance efficient compute norm matrix csr format operation reduction define  array norm associate binary operator difference osqp cuosqp implementation algorithm osqp cpu cuosqp gpu evaluate algorithm linear linear factorization PCG algorithm parameter matrix accord update rarely iteration data matrix csc format csr format upper triangular termination iteration iteration matrix multiplication matrix multiplication refers evaluate diagonal matrix dimensional vector sparse matrix csr format  array diagonal multiplies image KB image operation perform thread concurrently independently memory access array fully coalesce memory address combine transaction however access partly coalesce impact performance matrix pre multiplication matrix pre multiplication conceptually easy implement diagonal however obvious index correspond array matrix csr format address issue compute   array advance although increase memory usage code evaluates matrix pre multiplication image KB image cuosqp summarizes difference osqp gpu implementation algorithm although implementation memory matrix matrix factorization moreover reduce memory requirement precision float representation faster computation refer cuda implementation osqp solver cuosqp code available online http github com  osqp cuda python interface http github com  cuosqp cuosqp cuBLAS  thrust library within cuda toolkit custom implementation linear algebra improve efficiency solver however rely cuda library development ensures code portable various gpus operating code linux machine geforce RTX geforce gtx numerical evaluate performance cuosqp multi thread version osqp version competitive faster commercial QP solver goal demonstrate parallel gpu implementation improve performance optimization solver benchmark nonzero default parameter solver default precision float representation cuosqp precision variant numerical perform linux ghz core processor GB ddr mhz ram equip nvidia geforce RTX gpu GB  osqp benchmark benchmark described appendix consist QPs standard random application finance statistic machine available online summarize sequel linear invariant dynamical formulate constrain finite optimal equality consists equality constrain QPs huber huber fitting robust performs linear regression assumption outlier data huber penalty function define lasso absolute shrinkage selection operator lasso technique aim obtain sparse linear regression regularization objective formulate portfolio portfolio optimization arise finance seek allocate asset maximizes risk adjust return portfolio vector return risk aversion parameter risk covariance matrix random consists QP randomly generate data svm vector machine svm seek affine function approximately classifies label vector feature instance obtain realistic non trivial random data generate instance dimension performance metric average runtime across instance computation runtimes achieve osqp cuosqp osqp faster cuosqp however instance cuosqp significantly faster furthermore slope runtimes achieve osqp approximately constant whereas cuosqp flatter increase behavior cannot fully utilize gpu kernel launch data transfer latency cannot amortize moreover focus cuosqp optimize ADMM iteration satisfy termination update iteration decrease ADMM iteration equality lasso svm explains obtain speedup portfolio random benefit apparent huber update frequently numerical iteration achieve constant  osqp default thread linear solver  maximum speedup achieve cuosqp reduction runtime achieve equality osqp instance cuosqp solves reduction achieve svm reduction osqp runtimes necessarily increase behavior compute permutation kkt matrix prior factorization perform amd routine runtimes nonzero kkt matrix mkl  apart thread  linear solver osqp interfaced intel mkl  multi thread parallel sparse solver default intel mkl  maximum cpu core available performance hence numerical solver core computation runtimes increase monotonically mkl  osqp faster  mkl  reduces runtimes significantly however maximum ratio runtimes achieve osqp cuosqp float precision average computation cuosqp portfolio benchmark precision float representation penalty computation precision moreover numerical penalty data counter intuitive gpu precision float performance precision however numerical spmv memory bound operation computation limited memory bandwidth hence achieve speedup gpus memory bandwidth nvidia model conclusion explore possibility massive parallelism gpus accelerate QPs manage nonzero entry matrix implementation cuosqp built osqp QP solver ADMM speedup achieve PCG linear arise ADMM parallelize vector matrix operation numerical confirm gpus cpu implementation generally faster source implementation cuda linux machine implementation data ADMM iterates gpu memory choice reduces memory transfer gpu drawback limited available gpu memory extension unified memory approach merges memory gpu memory automatically transfer data demand memory alternatively multiple gpus data gpu challenge multi gpu approach distribution workload across multiple device ensure synchronization