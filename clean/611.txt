linear algebra operation widely data analytics scientific computation optimize linear algebra operation gpus regular input however focus fully utilize gpu resource input regular optimization fully utilize memory bandwidth compute therefore achieve sub optimal performance propose efficient algorithm TSMR TSML skinny matrix matrix multiplication gpus focus optimize linear algebra operation input matrix skinny specifically TSMR regular matrix skinny matrix TSML skinny matrix regular matrix implement propose algorithm nvidia gpu micro architecture TSMR computation average improves memory bandwidth utilization compute utilization average respectively regular matrix relatively medium TSML computation average improves memory bandwidth utilization average regular matrix relatively keywords matrix matrix multiplication skinny matrix gpu cuda performance optimization introduction matrix matrix multiplication gemm extensively linear algebra operation data analytics scientific computation due factor algorithm input data etc input matrix gemm usually varies application highly scalable scientific simulation package fluid dynamic finite fem simulation compute GEMMs input matrix artificial neural network ann involve gemm medium input matrix matrix decomposition gemm input matrix besides input already extensively optimize decade gemm medium input drawn attention recent researcher instance propose magma batch aim batch input matrix utilize highly optimize implementation input gpus propose gemm input architecture instruction optimization cpu architecture although previous focus optimize gemm matrix assume input matrix regular mention usually refers dimension input matrix matrix width height magnitude dimension input matrix significant difference irregular input irregular input involve skinny matrix width significantly height although optimize gemm skinny input input widely application instance recent highly optimize implementation gemm core computation input mostly skinny centroid usually input data moreover gemm encode checksum algorithm fault tolerance application input involves skinny checksum matrix previous effort optimize gemm regular input non regular input instance illustrate calculate gemm skinny input vendor highly optimize linear algebra library cuBLAS disassemble skinny input matrix vector apply matrix vector multiplication however easily workaround computation efficient input matrix access gpu although performance optimize skinny input matrix approach propose approach feasible skinny input matrix generate producer user workflow matrix extend applicable sensitive application memory limit matrix memory input matrix multiplication regular skinny matrix target optimize computation gemm skinny input gpu platform application gemm deployed gpus optimization greatly benefit application insight computation characteristic gemm compute unchanged input matrix regular matrix matrix matrix compute load ratio input matrix regular gemm operation usually compute bound matrix however input skinny average compute load ratio reduce around moreover gpu thread perform workload hide latency hence occupancy therefore relationship performance characteristic gpus computation compute bound memory bound latency bound specifically compute bound memory bound latency bound optimize gemm skinny input critical computation algorithm considers compute bound memory bound latency bound contribution limitation gemm implementation skinny input benchmarking utilization gpu resource performance degradation input skinny handle spectrum skinny input gemm gpus algorithm optimization focus skinny input TSMR handle regular matrix skinny matrix TSML handle skinny matrix regular matrix performance model TSMR evaluation performance moreover examine inadequacy model TSML improve observation carefully implement TSMR TSML cuda evaluate generation nvidia gpus kepler maxwell pascal volta TSMR TSML achieve speedup respectively average skinny input gemm library cuBLAS organize formal definition skinny matrix preliminary benchmark gemm skinny matrix cuBLAS propose detailed TSMR TSML skinny input evaluation examine related skinny input conclude background skinny input gemm restrict scope handle irregular input involve skinny matrix skinny input input matrix matrix skinny dimension significantly input matrix matrix input matrix matrix skinny input skinny matrix typical matrix irregular input gemm focus optimize gemm regular input matrix skinny input matrix skinny input matrix regular input matrix matrix input matrix matrix skinny input matrix matrix skinny input matrix matrix input matrix input expose challenge processing skinny input optimization technique introduce easily apply slight modification simplicity sake matrix matrix optimization non input cuBLAS commonly standard linear algebra library optimize gpu cuBLAS library developed nvidia cuBLAS core compute library data scientific compute application gpu compute library magma heterogeneous linear algebra library  library cudnn library nvidia optimization cuBLAS library performance regular input matrix gemm implementation achieve peak gpu performance however gemm subroutine fully optimize input matrix input involve skinny matrix gemm operation implementation cuBLAS nvidia tesla gpu theoretical peak memory bandwidth average demonstrate gemm operation theoretical peak memory bandwidth average demonstrate resource utilization input dimension input input computation utilizes memory bandwidth memory bound input computation utilizes compute compute bound however unable analyze gemm implementation source cuBLAS library computational characteristic methodology handle gemm skinny input gpus described efficient algorithm TSMR TSML TSMR handle input medium regular matrix skinny matrix TSML handle input skinny matrix regular matrix skinny matrix TSMR propose algorithm TSMR gemm regular matrix skinny matrix insight skinny input regular gemm matrix multiplies matrix input matrix compute complexity input matrix within entire computation load data gpu chip dram global memory gpu expensive avoid extensive data load operation optimization minimize load gpu chip memory cache register enable data reuse load reduces optimize gemm tends compute bound gemm implementation cuBLAS library bare performance gpus however unlike regular gemm matrix skinny input matrix average target gpu peak compute memory throughput ratio computation compute bound memory bound computation tends memory bound otherwise tends compute bound boundary memory bound compute bound critical algorithm optimize algorithm algorithm critical role propose optimization workload TSMR program model cuda thread hierarchy although workload easily decompose independent workload careful consideration workload distribution unnecessary performance penalty drastic gpu resource utilization factor global memory access global memory access efficiency utilization overall memory bandwidth parallelism overall workload chip memory utilization multiprocessor SM utilization optimization compute memory bound achieve performance exist active thread SM gpu ensure instruction memory access latency hiding algorithm workload assign matrix thread vector matrix multiplication assign thread benefit fold ensures parallelism SM occupancy matrix matrix distribution ensures matrix access enables memory access efficiency throughput memory access matrix naturally coalesce assume matrix convention image KB image vector matrix multiplication assign thread reduce memory access matrix outer style computation instead inner style computation algorithm inner matrix repeatedly reference outer algorithm matrix reference discus later threshold matrix reference due limited resource available thread inner matrix benefit significant greatly reduces global memory access entire gemm computation outer style extra memory access matrix inner style outer extra register intermediate however tune benefit memory access outweighs image KB image efficient chip memory access factor optimize memory intensive application ensure chip memory access efficiency gpu model runtime configuration global memory chip access thread within warp coalesce byte byte transaction access address byte byte global memory enables efficient memory bandwidth otherwise gpu load memory byte byte transaction  data address inefficient memory access thread matrix matrix format convention memory access naturally coalesce thread within warp access memory access efficiency achieve matrix however matrix thread access memory transaction request  memory access efficiency achieve access float although matrix access inefficient access greatly impact overall performance improve efficiency memory access matrix utilize memory gpu chip memory cache fully programmable thread within thread memory data advantage memory eliminates consistency data load data enables load global memory efficient data memory access matrix reduce memory access enable coalesce memory access algorithm iteration instead thread request individually  thread fetch tile matrix memory coalesce compatible computation thread reference matrix memory instead load individually global memory reduces access matrix global memory per thread thread fetch matrix enables coalesce memory access greatly improves memory access efficiency moreover introduce parameter algorithm parameter adjust performance later image KB image optimize memory although memory load register access affect overall performance memory memory parallel access thread access memory simultaneously memory speedup overall memory throughput throughput memory however thread warp access data memory conflict occurs request sequentially dramatically reduces access throughput factor algorithm thread thread load data global memory memory enable coalesce global memory access thread access data memory computation memory affect access memory affect throughput memory tile matrix memory storage storage analyze brings overall conflict assume tile matrix memory simplicity storage tile matrix successive memory memory memory successive memory access thread potentially conflict storage matrix successive memory thread access potentially conflict image KB image storage tile matrix memory warp thread access storage brings conflict storage brings conflict reduces throughput interpretation reference colour legend reader refer web version article nvidia gpus fix storage conflict thread access storage conflict decrease overall memory throughput peak throughput load matrix tile memory storage storage storage thread warp access conflict occurs storage conflict access memory computation thread warp access algorithm although multiple thread access access broadcast initiate conflict storage style storage brings conflict potentially brings memory throughput overlap computation memory access latency execution instruction issue warp scheduler eligible warp correspond component execution warp becomes eligible operand instruction however warp load data global memory cycle execution hide latency increase thread reside SM ensure exist eligible warp independent instruction data load data consume operation warp eligible execution memory load approach adjust chip resource usage thread discussion aim independent instruction data load data consume operation algorithm load data global memory consumes data data load however due data dependency independent instruction warp issue global memory access request request proceed computation independent instruction data prefetching data load consumption iteration specifically instead iteration load data iteration data iteration load previous iteration calculation data load data calculation load data iteration overlap data load computation significantly improve memory bandwidth SM utilization apply data prefetching matrix image KB image algorithm TSMR data prefetching global tid local tid global thread ID grid local thread ID respectively allocate register tile matrix tile matrix prefetching allocate register data prefetching matrix allocate currently load tile matrix cannot tile matrix register matrix thread computation core computation iteration pre load tile matrix register memory computation immediately computation loop without data dependency computation resides overlap computation memory access initiate load tile computation matrix matrix loop load matrix flexibility adjust load pace tile differently matrix discus subsection iteration optimize TSMR data prefetching LD ST load initial matrix matrix iteration sub iteration load matrix compute pre load tile matrix concurrently improve memory bandwidth utilization thread barrier insert iteration innermost iteration actual computation pre load matrix rectangle accurately execution ratio LD  LD  necessarily actual computation thread thread illustration proposes discus subsection parameter affect ratio LD  LD  execution LD  compute affect characteristic computation memory bound compute bound simplicity ignore data movement tile tile occurs iteration image KB image workload iteration optimize TSMR data prefetching image KB image matrix skinny matrix matrix multiplication data prefetching parameter definition algorithm introduce adjustable parameter discus parameter computation TSMR introduce performance model estimate performance metric parameter finally explain strategy parameter achieve gpu resource utilization optimize overall performance discussion algorithm behavior parameter behavior parameter specifies tile matrix maximize available active thread avoid inefficient thread execution warp divergence thread thread participate fetch matrix coalesce global memory access thread fetch thread thread thread computation thread calculate specifies matrix thread overall workload workload iteratively thread workload thread SM resource usage allows SM occupancy however workload load matrix repeatedly workload affect ratio memory fetch computation operation core algorithm allows adjust computation compute memory bound later detail specifies matrix thread fetch fetch independent without adjust memory load concurrency performance metric estimation introduce parameter performance model estimate important performance metric SM occupancy memory bandwidth utilization compute utilization estimation optimize overall performance max SM occupancy estimation parameter calculate max occupancy SM define max active thread per SM max warp maximum thread consistent across performance model thread  ensure thread active occupancy mainly bound maximum hardware allowable thread chip memory utilization per thread calculate register utilized per thread register utilization potentially optimize nvcc compiler maximum register estimate relatively fix amount register cuda initial setup amount amount offline profile register matrix tile fetch tile calculation although tile matrix memory transfer register calculation register intermediate matrix finally register matrix tile fetch tile calculation register memory although allocate per thread calculate average amount memory thread consistent calculation allocate memory per thread discus earlier amount memory allocate thread average max SM occupancy calculate calculation max available register memory per SM max memory bandwidth utilization estimation estimate max memory bandwidth utilization algorithm computation memory bound load matrix dominates computation instead float calculation algorithm estimate max memory bandwidth utilization maximum concurrent global memory access per SM calculate memory access matrix simplicity majority memory access matrix brings minor inaccuracy calculate concurrent memory access per SM achieve max memory bandwidth utilization average global memory access latency constant model obtain offline profile estimate memory bandwidth utilization max compute utilization estimation estimate max compute utilization algorithm computation compute bound float calculation dominates computation instead memory access algorithm estimate max compute utilization maximum concurrent float operation per SM calculate calculate concurrent float operation per SM achieve max compute utilization average latency float operation calculation constant model obtain offline profile estimate compute utilization compute bound memory bound parameter gpu specification computation memory compute bound mainly innermost loop algorithm memory load instruction overlap computation depends memory load serf implicit synchronization memory load computation computation compute bound memory bound estimate computation memory access computation compute bound memory bound computation compute bound otherwise computation memory bound workload workload ratio adjust actual computation shift compute memory bound boundary calculate ratio threshold similarly estimate computation characteristic workload workload fix estimate computation characteristic compute bound otherwise memory bound easily computation characteristic affect overall performance discus later parameter parameter optimize computation memory bandwidth TSMR computation gpu compute memory bound propose estimate characteristic accordingly adjust parameter optimize computation memory bound actual computation memory bound optimize memory bandwidth utilization compute bound actual computation compute bound optimize compute utilization however gpu optimize memory bound output parameter deliver performance algorithm parameter optimization procedure computation characteristic memory bound optimize global memory access otherwise optimize computation memory access amount memory access matrix simplicity access matrix matrix simplification brings minor inaccuracy access matrix additional parameter optimize related thread organization model estimation memory bandwidth utilization compute utilization calculate equation mention parameter optimization target gradient descent GD optimization GD initial threshold accurate precision integer image KB image optimize thread thread thread fix determines thread organize thread access matrix reduce however thread thread participate synchronization impact performance access matrix thread schedule flexible efficient optimum theoretically offline profile specifically benchmark mention earlier performance although memory allocation max SM occupancy actually limited impact fix amount memory per thread TSML algorithm propose TSMR optimize regular matrix multiple skinny matrix propose algorithm TSML handle skinny matrix multiplies regular matrix input matrix multiple input matrix skinny matrix introduce optimization approach overcome bottleneck skinny input performance bottleneck adapt previous algorithm TSMR handle without optimization however apply algorithm reveals bottleneck evaluate TSMR matrix varies nvidia tesla gpu inner dimension decrease memory bandwidth usage decrease image KB image memory bandwidth usage precision explain expand upon performance model algorithm propose model assumes maximum theoretical occupancy achieve throughout computation however algorithm loop thread perform workload hide latency occupancy program issue global memory efficient memory usage therefore TSMR performs latency bound mode neither compute bound memory bound input skinny matrix matrix prior propose optimization image KB image observation optimization TSML optimization intend warp latency memory access latency launch thread thread performs computation accumulate warp latency replace memory access latency launch thread matrix horizontal tile introduce parameter tile matrix algorithm launch thread kernel optimization involves multiplication consists tile matrix entire matrix essence optimization TSMR algorithm tile matrix optimization matrix access load TSMR detail algorithm image KB image optimization interleave computation tile rapidly load tile matrix load intermediate sum matrix tile matrix load intermediate load compute tile matrix register load matrix contains accumulate computation matrix tile matrix prepared computation quickly switch tile matrix prefetched addition prefetching already described algorithm register associate tile access access however achieve occupancy memory bandwidth concerned issue memory instruction detail algorithm illustrates optimization performance memory bandwidth usage thread launch impact warp latency replace latency memory bandwidth latency computation decrease memory access bandwidth increase thread launch reduce significant decline speedup memory bandwidth utilization occurs kernel manage thread perform image KB image performance comparison precision therefore appropriate thread launch kernel algorithm launch insufficient thread parallelism becomes hence performance suffer algorithm launch thread performance impact warp latency naive adaptation TSMR appropriate target offline profile summary TSMR TSML  rectangular matrix skinny matrix  skinny matrix matrix TSMR  memory bound algorithm inner algorithm outer global memory access algorithm memory efficient global access matrix algorithm  prefetch overlap compute memory operation TSML  bound algorithm  matrix horizontal tile compute tile sequentially algorithm  matrix horizontal tile interleave computation tile performance model parameter tile matrix parameter thread computes parameter thread fetch compute utilization gpu memory bandwidth utilization determines computation compute bound memory bound summary summarize TSMR TSML performance model experimental evaluation setup implement TSMR TSML cuda precision float input disable compiler auto unroll explicit loop unroll register allocation propose algorithm mainly target traditional scientific compute application machine application omit evaluation precision input evaluate optimize implementation heterogeneous testbed cluster darwin los  national laboratory   gpu node gpu conduct commonly nvidia gpus micro architecture kepler maxwell pascal volta kepler gpu tesla gflops peak float performance GB memory bandwidth maxwell gpu tesla gflops peak float performance GB memory bandwidth pascal gpu tesla gflops peak float performance GB memory bandwidth volta gpu tesla gflops peak float performance GB memory bandwidth information experimental cluster gpus experimental platform detailed gpu information   xeon  xeon memory GB GB      gpu memory GB GB GB GB peak performance gflops gflops gflops gflops peak performance gflops gflops gflops gflops memory bandwidth GB GB GB GB comparison TSMR TSML gemm cuBLAS library  library  however gemm kernel cuBLAS performance identical cuBLAS omit multiple reduce cuda api performance calculate performance  instruction global memory throughput nvprof command metric gld throughput option addition metric gld efficiency option verify global memory access efficiency achieve optimization input matrix initialize random float multiplication matrix skinny matrix TSMR multiplication skinny matrix matrix TSML specifically TSMR regular matrix skinny matrix TSML skinny matrix regular matrix evaluation TSMR evaluate performance TSMR input library optimization combination gemm cuBLAS comparison baseline apply combination optimization TSMR gemm cuBLAS  version TSMR straightforward inner version described algorithm outer version algorithm version reduces global memory access algorithm outer production version algorithm memory efficient global memory access matrix outer production version algorithm memory data prefetch version optimize implementation described algorithm detailed performance breakdown optimization behaves similarly gpus evaluate optimization resource program bound computation memory bound optimize parameter parameter apply version TSMR speedup version precision TSMR suffers performance due requirement global memory access inner version TSMR significantly improves performance TSMR faster global memory access TSMR improves efficiency global memory access matrix vital role overall performance addition memory tile matrix thread within thread reduces memory access matrix additional speedup finally data prefetch introduce TSMR mitigates memory access bottleneck brings additional speedup average image KB image speedup comparison precision memory throughput analysis micro architecture addition kepler micro architecture conduct newer maxwell pascal volta gpus kepler gpu tesla compute computation input compute bound parameter optimization procedure output parameter compute optimization optimize implementation achieves average speedup tesla average compute usage improvement gemm function cuBLAS compute computation input memory bound parameter optimization procedure output parameter memory optimization optimize implementation achieves average speedup tesla average memory bandwidth utilization improvement gemm function cuBLAS image KB image speedup memory bandwidth utilization comparison precision TSMR nvidia tesla gpu volta micro architecture due memory increase regular matrix computation memory bound ensure maximum performance account volta architectural improvement optimize parameter via brute optimize parameter precision precision optimize parameter otherwise exhibit gradually improve performance TSMR TSMR TSMR version speedup average achieve precision speedup average achieve precision speedup precision cuBLAS due cuBLAS precision gemm optimize matrix longer target finally kernel achieve memory bandwidth utilization gpus partly attribute improvement volta previous micro architecture specifically improve HBM memory allows workload obtain memory bandwidth utilization pascal gpus accord whitepaper experimental metric TSMR kernel image KB image speedup comparison precision image KB image memory bandwidth utilization precision due performance model predict TSMR performance nvidia tesla  architecture peak precision float performance tflops global memory bandwidth GB memory bound implementation already achieve efficiency memory bandwidth TSMR achieve speedup currently access nvidia tesla gpu estimate available whitepaper account architectural improvement detail TSMR kernel register experimental data nvcc depends  thread  memory byte register          non input evaluate TSMR rectangular input matrix integer factor evaluate reveals performance impact demonstrate although ensure kernel performance model memory bandwidth utilization kernel remains performance kernel linearly matrix image KB image performance comparison precision rectangular input evaluation TSML evaluate performance TSML cuBLAS TSML variable matrix input combination obtain precision precision propose optimization TSML version TSML TSML opt algorithm TSML opt algorithm image KB image speedup comparison precision image KB image memory bandwidth utilization precision detail TSML kernel register experimental data nvcc depends  thread  memory byte register TSML  TSML  TSML  TSML  TSML  TSML  TSML obtain speedup cuBLAS average precision speedup average precision TSML opt generally performs precision input TSML opt TSML opt performs TSML opt precision addition TSML achieves memory bandwidth utilization peak global memory bandwidth average precision TSML utilizes significantly memory bandwidth cuBLAS average however precision TSML slightly memory bandwidth cuBLAS memory bandwidth however TSML outperforms cuBLAS explain inefficient memory gemm kernel experimental metric TSML kernel related preliminary version publish introduces TSMR algorithm evaluates kepler maxwell pascal gpus respectively expand evaluation volta gpu moreover broaden applicability TSML algorithm handle input focus optimize skinny gemm proposes algorithm skinny input  skinny matrix transpose skinny matrix  skinny matrix matrix evaluates algorithm volta gpu precision complex float although  input  input TSML however  TSML approach input differently specifically  matrix format input optimization focus avoid partially cache matrix  launch multiple thread per matrix thread matrix unlike  TSML account latency launch thread perform optimization focus manage warp memory bandwidth latency moreover TSML achieves superior performance  achieves speedup cuBLAS dimension whereas TSML achieves speedup dimension conclusion analyze performance bottleneck gemm cuBLAS library identify implementation lack utilization compute memory bandwidth input skinny discover potential challenge optimize skinny gemm workload compute bound memory bound latency bound propose performance gemm algorithm TSMR TSML gpus  input optimization technique focus gpu resource utilization finally optimize implementation achieve speedup skinny matrix matrix multiplication diverse input gpus