address challenge mpi derive datatype processing proposes falcon overhead communication framework optimize zero intra node derive datatype communication emerge cpu gpu architecture quantify various performance bottleneck memory layout translation overhead highly fragment mpi datatypes propose novel pipelining memoization achieve efficient derive datatype communication addition propose enhancement mpi standard address semantic limitation experimental evaluation propose significantly improve intra node communication latency bandwidth mpi library cpu gpu representative application kernel milc wrf NAS MG  stencil cpu architecture gpu DGX demonstrate improvement multi core CPUs benefit  gpu mpi library previous keywords hpc mpi derive datatypes cpu gpu nvidia DGX introduction performance compute hpc enable scientist research domain explore model simulate computation availability multi core architecture intel xeon xeon phi  nvidia volta gpus significantly accelerate impact capability multi  multi core CPUs gpus adoption core architecture future exascale message passing interface mpi facto program model develop performance parallel scientific application compute unified device architecture cuda primary program interface exploit nvidia gpus emergence cuda aware mpi relieve application developer manually data host cpu device gpu memory switch mpi cuda program model communication phase application allows decouple cuda mpi program model within application cuda kernel computation mpi derive application communication ubiquity mpi facto program model cpu gpu mandate mpi library carefully deliver performance communication primitive performance parallel algorithm scientific application communicate non contiguous data matrix multiplication halo exchange communicate multiple matrix format achieve application pack data temporary contiguous buffer recipient unpack data however approach manual pack unpack performance due multiple data increase memory footprint application researcher pack unpack communication moreover burden manage temporary buffer manually copying data application developer productivity address mpi feature derive datatypes ddt communicate non contiguous data portable efficient manner approach application composes derive datatype datatypes predefined mpi standard datatype communication primitive however mpi library suffer performance derive datatype processing application wrf milc NAS MG  rely manual pack unpack instead  researcher propose improve communication performance  interconnects infiniband fundamental bottleneck datatype processing efficient translation datatype memory layout remain unsolved furthermore challenge involve handle derive datatype communication cpu gpu resident data brings forth challenge instance mpi library memory intra node datatype communication cpu resident data multiple performance overlap similarly cuda aware mpi implementation employ cuda kernel accelerate pack unpack phase gpu resident data however suffers significant synchronization overhead cpu gpu zero technique improve intra node communication performance depth offs involve technique non contiguous communication explore literature zero technique mpi datatypes expose novel challenge correctness performance propose efficient address issue cpu gpu propose reduce layout translation overhead memoization technique finally mpi datatype routine fully advantage zero semantics propose enhancement address limitation motivation performance datatype communication mpi library document literature understand bottleneck involve datatype communication mpi cpu gpu analyze communication latency transfer various representative application kernel wrf milc NAS  CM  modify  evaluate cuda aware mpi library profile application kernel cpu gpu performance trend obtain MVAPICH mpi library broadwell cluster detail experimental setup described later trend MPICH derivative intel mpi communication cpu resident data involves component translation datatype definition actual memory layout copying data source target buffer relative amount data transfer complexity datatype reveals performance issue gpu datatype communication associate MVAPICH GDR communication dominate various cuda operation datatype layout pack unpack kernel spent data trend cuda aware library mpi efficient mpi derive datatypes communication runtime overhead investigate technique improve performance inefficient translation datatype memory layout mpi derive datatypes described consists primitive datatypes displacement address derive datatype derive datatypes nest datatypes sender receiver datatypes independently translate datatype definition memory layout layout translation transfer outcome depends various user input address buffer datatype datatype quickly become costly complex datatype specify scientific application datatype iteration opportunity amortize reuse translation multiple communication challenge mpi library designer infer memory layout derive datatypes reduce amortize reuse layout multiple communication achieve maintain correctness without introduce additional synchronization requirement challenge independent communication mechanism applicable memory zero inefficient data movement mpi library rely memory intra node datatype communication sender compute individual copying pre allocate memory memory usually multiple receiver copying data local buffer sender strategy commonly refer pipelined memory however approach multiple data sender receiver sender receiver cpu actively progress communication performance overlap potential improve performance overlap explore efficacy kernel assist zero data transfer technique memory attach cma XPMEM directly memory another node similarly avoid expensive data pack unpack kernel gpu resident data investigate peer peer PP technique  technology cuda inter communication ipc apis nvidia gpus data movement gpus node although mpi library zero mechanism intra node communication contiguous buffer challenge offs involve technique non contiguous buffer explore literature challenge zero datatype communication zero technique data layout remote initiate however mpi standard routine local creation destruction derive datatypes receiver oblivious memory layout sender datatype hence zero introduce additional overhead layout exchange memory heavily fragment datatypes exchange significant sometimes actual data movement furthermore prior overhead kernel contention remote address translation etc affect performance zero technique gpu resident data overhead primarily issue cuda operation PP data movement observation identify challenge introduce goal zero datatype communication overhead involve zero technique communicate cpu gpu resident non contiguous buffer datatypes employ minimize overhead zero efficiently handle datatypes sender receiver enhancement mpi semantics propose mitigate layout translation exchange overhead contribution observation challenge performance efficient zero communication runtime mpi derive datatypes cpu gpu prior challenge involve cpu derive datatype processing enhance earlier CPUs augment propose gpu mpi derive datatype processing cuda aware mpi library address challenge mention propose falcon overhead communication zero mpi datatype processing cpu gpu architecture knowledge identify analyze offs involve zero communication primitive non contiguous data movement address efficient manner propose mitigate overhead layout translation internal mpi library enhancement mpi datatype creation semantics integrate popular mpi library MVAPICH MVAPICH GDR efficacy various microbenchmarks application propose reduce intra node communication latency mpi derive datatypes improve performance communication kernel application 3D stencil milc wrf NAS MG  host communication gpu communication DGX summarize contribution identify challenge offs involve zero technique mpi datatype processing efficient pipeline novel cache mechanism mitigate various overhead associate zero communication scheme propose optimization enable reuse datatype layout information amortize layout translation optimize host device derive datatype communication MVAPICH MVAPICH GDR mpi library propose enhancement mpi datatype creation semantics enable performance datatype communication demonstrate efficacy propose cpu gpu micro benchmark application zero mpi datatype processing CPUs detailed zero technique datatype intra node communication propose mechanism improve performance naive zero contrast memory zero rely sender transmit layout data instead actual content receiver illustrates conceptual parameter mpi buffer address datatype sender translates datatype layout creates individual contiguous transfer define address offset commonly refer vector IOV sender sends rts request packet receiver contains various metadata source tag communicator context receiver information appropriate operation responds CTS packet sender sends data packet receiver contains IOVs packet receiver data directly target buffer copying individual sender memory image KB image overview various propose zero datatype communication applicable cpu gpu mpi communication cma receiver issue  local remote IOVs input data source buffer target buffer inside kernel XPMEM receiver attach remote address directly content individual local buffer reverse receiver describes layout target buffer sender writes data performance cma XPMEM approach heavily amount data transfer individual cma incurs overhead contiguous performance highly affected fragmentation datatype relative performance cma naive zero datatype processing XPMEM naive zero MB buffer IOV fragment IOV byte KB varies fragmentation factor performance cma suffers expensive data fragment IOV IOV however fragmentation decrease along axis relative performance difference cma XPMEM decrease KB IOV IOVs cma XPMEM IOV byte IOVs evaluation focus XPMEM remainder however propose later mitigate various overhead independent underlie transport mechanism applicable cma zero scheme image KB image performance comparison cma XPMEM zero IOV buffer MB individual IOV byte KB focus XPMEM avoid unnecessary overhead cma impact datatype fragmentation XPMEM underlie transfer mechanism fix message IOV increase communication latency fragmentation data increase communicate IOV overall message non contiguity data buffer communicate allows cpu prefetch data exploit spatial locality additional overhead copying become prominent increase analyze impact behavior application performance breakdown individual inside XPMEM naive zero communication kernel application memory layout translation remains unchanged data movement reduce due avoid extra however improvement negate additional introduce zero layout exchange address translation layout exchange refers communicate memory layout source buffer receiver receiver cannot assume sender datatype receiver layout sender cannot directly receiver address sender address invalid receiver address receiver translate remote address valid local address data address translation overhead increase datatype becomes fragment contains non contiguous IOVs breakdown XPMEM trend zero technique exchange layout cma address translation copying data combine inside  actual copying data contributes portion overall various preparatory perform data movement overhead analyze detail prior image KB image impact data fragmentation naive zero datatype processing XPMEM communication buffer KB MB pipelined zero overhead layout sender receiver consume significant portion communication delay receiver perform useful IOVs address limitation naive propose pipelined zero describes conceptual partial sender IOVs receiver initiate data transfer without sender entire IOVs meanwhile sender subsequent chunk layout pipelined fashion advantage pipelined naive twofold allows receiver progress communication chunk IOVs overlap data layout copying data thereby reduce overall latency although pipelined zero hide overhead associate layout exchange sender actively progress communication layout information reduces availability sender progress communication application computation non sends memoization zero address limitation pipelined zero propose memoization scheme communicate peer memoize derive datatype layout exchange memoized layout subsequent communication sender receiver maintain hashtable memoization sender translate layout sender datatype receiver sender layout communication sender issue request non contiguous derive datatype receiver mpi runtime translate layout generates sha hash translate layout hash local hashtable entry sender communicate peer IOV layout sender proceeds ignore translate layout IOVs receiver hash sender performs task  datatype layout associate hash local hashtable embeds hash along translate IOVs data packet receiver request arrives receiver hash packet local hashtable associate sender rank hash indicates sender communicate datatype layout receiver IOVs sender datatype layout packet receiver proceeds memoize layout entry local hashtable hash IOVs hash receiver IOV information packet overview memoization employ standard chain mechanism avoid hash collision embed request hash entry memoization completely eliminates overhead communicate sender datatype layout application perform communication datatype effectively performs layout translation transfer amortizes avoid remote address translation IOV layout sender contains address valid sender context receiver translate remote address appropriate local address access however contributes significantly overall communication mitigate enhance memoization receiver IOV already translate local address ensures costly attach remote translate address however attach remote increase impact overall performance avoid implement lru cache discard recently datatypes translate mapping  associate remote discard datatype receiver request sender resend layout CTS packet performs translation entry lru cache contains byte hash byte pointer IOVs entry tunable parameter discussion validate efficacy propose zero performance exist memory communication broadwell latency bandwidth directional bandwidth approach exist memory default shm pipelined zero zcpy pipe memoization zero zcpy memo naive zero described due extremely performance fix message MB byte MB contiguous latency zcpy pipe IOVs gradually improves decrease zcpy memo avoids layout exchange overhead hence affected significantly IOVs memory zero faster till KB roughly faster till MB discontinuity KB due switch eager rendezvous protocol MB buffer contiguous hence performance performance unidirectional bidirectional bandwidth zero benefit bandwidth however benefit bandwidth memory sender receiver progress communication zero receiver deliver application performance image KB image performance comparison propose zero memory broadwell zero multi gpu multi gpu performance interconnects pcie NVLink widely gpus enables peer peer PP access driver apis via within compute kernel via load operation elaborate propose mpi address challenge leverage PP feature achieve zero data movement non contiguous gpu resident data implement host naive zero pipelined zero however discus optimize memoization zero gpus avoid redundant insight naive PP zero sender layout properly exchange PP access available gpus receiver issue multiple asynchronous primitive  contiguous directly non contiguous data without additional although achieves pack data transfer issue multiple incurs significant overhead due multiple cuda driver api research extensively overhead cuda api propose memoization performance cpu exhibit performance gpus highly fragment layout datatype contiguous overhead cuda driver PP communication outweigh benefit obtain memoization gpu resident data worth cuda  incur driver overhead gpu hardware naive zero approach sparse datatypes application milc significant performance overhead efficient zero gpu resident data kernel zero fully exploit zero multi gpu propose load minimize driver overhead perform data movement non contiguous buffer shot maximize throughput warp thread nvidia gpus workgroup amd gpu responsible contiguous achieve memory access throughput advantage memory coalesce within warp thread load remote gpu local gpu GPUDirect PP available warp warp multiple contiguous robin manner significant advantage load kernel receiver launch multiple kernel quickly saturate interconnect transfer data  parallel without contention furthermore exist datatype processing launch multiple pack unpack kernel data perform pack unpack incurs significant driver overhead communication cannot proceed pack phase contrast propose asynchronously launch kernel mpi recv mpi  minimize driver overhead overlap multiple kernel hide driver synchronization overhead maintain throughput latency moreover kernel overlap opportunity application computation reduce overall application execution demonstrates significant performance benefit naive  kernel efficient layout translation cpu gpu propose focus reduce exchange sender layout however involve layout translation local datatypes receiver datatype memory layout translation costly nest datatypes due recursive datatype parse mitigate overhead propose approach eliminate layout translation overhead sender receiver approach involves cache datatype parse approach involves enhance mpi semantics datatype creation destruction layout reuse cache described memoization zero avoid remote address translation sender receiver maintain hash sender translate layout however suffer limitation sender calculates hash IOVs receiver recreates data layout operation enhance hash communication instead translate IOVs listing structure information communication request input hash function content structure chosen ensure combination uniquely identify translate IOVs parameter layout propose sender receiver communication request local data layout accord request information hash generate combination hash already translation exchange layout skip sender receiver avoid costly iteration discard rarely datatype layout additional flag rts CTS packet calculation transmission IOV layout image KB image propose enhancement mpi semantics memoization propose reduces exchange layout information sender receiver sender communicate data layout receiver datatype communication datatype incurs overhead subsequent transfer furthermore sender calculates hash datatype unique identifier commonly refer handle datatype assign unique handle scheme however mpi standard allows exist datatypes guarantee handle freed datatypes reuse sender receiver incorrectly conclude datatype recv operation reuse incorrect behavior mpi library tackle issue ensure datatype handle reuse allocate however issue exists datatype creation local operation caller participates mpi standard cannot easily assign unique identifier handle datatype address propose mpi routine derive datatypes listing function collective specify communicator participate allows mpi library ensure newly datatype allocate handle across communicator implement function perform allreduce operation internally handle participant allocate handle datatype mpi library internally mpi commit exist non zero complexity adoption api negligible image KB image experimental evaluation production mpi library MVAPICH MVAPICH GDR intel mpi IMPI mpi ucx MVAPICH mpi ucx configure XPMEM intra node transport mechanism IMPI perform IMPI benchmark attribute lack optimization derive datatypes  version intel mpi available MVAPICH GDR mpi evaluate datatype processing gpu resident data prior vendor mpi implementation perform KNL  IMPI spectrum mpi broadwell mpi instead  micro benchmark evaluation enhance  micro benchmark  mpi derive datatypes  iteration average report  evaluate communication performance derive datatypes popular scientific application kernel moreover extend  gpu derive datatype communication hardware specification cpu gpu testbeds respectively hardware specification cpu testbed cluster   processor    processor   SLC GTA ghz ghz ghz socket core per socket thread per core ram ddr GB GB GB  EDR IB EDR IB EDR hardware specification gpu enable testbed cluster  CS  DGX cpu  xeon haswell  xeon platinum skylake ghz cpu core socket gpu  tesla  tesla gpu memory GB GB interconnects  gen NVLink NVSwitch nvidia driver version cuda toolkit version evaluation multi core CPUs demonstrate detailed evaluation propose optimize MVX opt cpu architecture microbenchmarks application kernel performance communication performance propose communication mpi library enhance  microbenchmark suite  datatypes mpi vector datatype mpi char datatype message fix MB varied byte MB consequently varied stride twice amount empty latency bandwidth directional bandwidth broadwell architecture propose MVX opt improves latency medium KB difference KB switch eager rendezvous protocol intel mpi MVAPICH threshold architecture IMPI performance IMPI due lack optimization  implementation IMPI improvement bandwidth bandwidth achieves twice performance unidirectional bandwidth library performance bandwidth propose zero allows sender receiver progress communication direction non communication application performance milc  lattice computation milc application interaction quark  quantum   milc mpi  accomplish halo exchange direction milc kernel  model CG solver direction rmd application milc code describes datatype layout communication kernel evaluation propose refer MVX opt mpi library performance grid dimension architecture axis increase grid axis plot execution latency propose significant improvement mpi library almost grid dimension instance dimension correspond roughly KB message propose improvement mpi improvement MVX broadwell performance trend KNL MVX IMPI perform similarly MVX opt magnitude improvement library datatype layout milc kernel complex deeper nest communication kernel datatype layout translation constitutes communication performance benefit MVX opt mainly stem memoization completely avoid datatype translation exchange overhead  propose MVX opt improvement spectrum mpi almost magnitude improvement MVX observation finding datatype processing MPICH derivative mpi library MVAPICH IMPI suboptimal complex datatypes milc mpi derivative spectrum mpi employ impact datatype nest although mpi spectrum mpi complex datatype processing entirely eliminate associate overhead propose MVX opt novel outperforms MPICH mpi derivative library wrf research forecasting wrf application model atmosphere regular cartesian grid data decomposition horizontal dimension halo exchange phase slice dimension array communicate  model communication dimension array  vector datatypes struct subarray datatypes describes datatype layout wrf dimension performance wrf vec kernel broadwell KNL  increase increase  subarray creation evaluation propose exhibit increase instance broadwell communicates roughly KB message improvement latency mpi improvement IMPI improvement MVX similarly input parameter correspond MB message propose improvement IMPI MVX mpi respectively trend continued KNL IMPI MVX comparable performance MVX opt outperform unlike milc datatype layout wrf complex due MVX suffer considerably layout translation exchange furthermore zero mechanism spectrum mpi cma performance XPMEM MVX performance spectrum mpi MVX opt improvement execution MVX image KB image performance comparison milc mpi library architecture grid dimension image KB image performance comparison wrf mpi library architecture parameter ims ime image KB image performance comparison NAS MG mpi library architecture grid dimension scientific application kernel derive datatype layout variation kernel wrf vec NAS MG however due similarity trend variation per kernel application   layout milc   vector 4D exchange wrf   vector subarrays NAS MG   nest vector 3D exchange 3D  communication stencil subarray datatypes NAS MG communicates array stencil slice grid communicate correspond direction  modifies pack function MG construct appropriate subarray  datatypes variation kernel NAS MG NAS MG NAS MG data exchange dimension nest vector datatypes performance NAS MG broadwell propose significant improvement mpi library instance grid propose MVX opt improvement latency default MVX IMPI benefit IMPI pronounce improvement trend KNL MVX IMPI perform similarly MVX opt improvement performance trend  wrf kernel MVX performance spectrum mpi advantage user zero propose MVX opt improvement MVX spectrum mpi respectively 3D stencil communication kernel mimic communication stencil application adaptive mesh refinement amr kernel communication kernel performs stencil subarray datatypes communicate cartesian grid benchmark evaluate performance propose increase constant performance propose mpi library fix grid mpi broadwell node propose improvement intel mpi improvement MVX trend continued increase optimize improvement IMPI IMPI improvement MVX subscription per node mpi library perform propose linear growth increase latency subscription potentially explain imbalanced distribution cartesian grid KNL  random crash mpi library report application developer mpi application crashed evaluation multi gpu evaluate gpu derive datatype communication kernel extend  multiple gpu buffer modify  gpu memory instead memory appropriate memory management replace cuda api replace malloc cudaMalloc gpu evaluate representative application kernel NAS MG milc  NAS MG milc dense non contiguous layout  distribute layout fragmentation finally evaluation gpu enable 2D stencil application iteration iteration report average latency conduct gpu cluster described performance comparison cuda aware mpi library propose perform ddt transfer gpus pcie switch propose optimize zero scheme MV GDR opt performs dense layout message distribute layout message shot kernel launch saturate pcie bandwidth minimize synchronization overhead propose MV GDR opt yield latency mpi MV GDR respectively milc highly distribute layout  MV GDR opt achieve speedup mpi MV GDR respectively NVLink interconnect gpus MV GDR opt outperforms scheme message moreover latency CS storm due latency load operation NVLink reduce kernel launch overhead nvidia volta gpu architecture 2D stencil finally conduct dimensional precision stencil computation consists halo exchange mpi vector datatype communicate data direction DGX report throughput  gflops iteration grid constant trend image KB image performance comparison non contiguous data transfer PP access available pcie CS storm NAS CG milc  image KB image performance comparison non contiguous data transfer PP access available NVLink DGX performance benefit propose baseline MVAPICH GDR mpi however various runtime issue propose achieve gflops default MVAPICH GDR efficiently advantage zero scheme NVSwitch related researcher explore network feature improve performance mpi datatype processing leveraged scatter  feature propose zero scheme  exploit user mode memory registration  feature infiniband remove pack unpack overhead sender receiver performance memory utilization avoid intermediate stag data pack unpack operation however rely hardware feature propose emphasize intra node communication propose compile transformation algorithm reduce non contiguous datatype entity inside application avoid overhead associate pack unpack operation compilation technique generate efficient optimize pack code mpi datatypes commit phase performance manual pack code propose memory communication multi thread mpi runtime however focus mpi datatypes associate overhead propose concise datatype efficient implementation technique propose representation mpi derive datatypes efficient processing propose capable reconstruct datatype polynomial contrast propose amortize completely eliminate layout translation exchange overhead improve application performance transparently gpu resident data propose multi stage pipeline data transfer offload pack unpack processing host gpu accelerate non contiguous data movement convert conventional datatype representation gpu amenable format exploit grain parallelism gpu kernel perform device pack unpack non contiguous however approach remains focus pack unpack propose completely eliminates extraneous involve pack unpack efficient zero mechanism propose framework employ datatype specific gpu kernel improve efficiency pack unpack kernel achieve overlap cpu gpu execution eliminate unnecessary synchronization asynchronous pack unpack propose kernel mpi offload unpack pack operation onto gpu however fundamentally remain focus optimize pack unpack exploit gpu resource contrast approach fundamentally zero mechanism address various limitation zero propose adaptive selection strategy zero pack unpack scheme configuration workload characteristic dense gpu focus remain adaptive selection various gpu specific scheme handle non contiguous data movement gpu resident data however expose various overhead extra memory allocation pack unpack extraneous synchronization overhead contrast approach mainly focus extend host non contiguous data movement demonstrates applicability load dense gpu gpu non contiguous data movement additional challenge core host data movement equally applicable dense gpu communication load semantics conclusion future identify challenge involve intra node zero communication scheme mpi derive datatypes multi core cpu gpu architecture propose address efficiently propose refer falcon reduce layout translation exchange novel pipelining memoization finally propose enhancement mpi datatype creation semantics enable future avenue performance zero datatype processing integrate propose falcon variant MVAPICH library namely MVAPICH cpu MVAPICH GDR gpu demonstrate efficacy performance improvement various micro benchmark application emerge multi core CPUs gpu DGX propose reduce intra node communication latency mpi derive datatypes improve communication performance various application kernel 3D stencil milc wrf NAS MG CPUs DGX mpi library integrate inter node zero multi gpu cluster evaluate impact application