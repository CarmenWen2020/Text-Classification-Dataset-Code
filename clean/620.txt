cache important component compute significant impact performance cache role due data intensive processing challenge provider cache capacity tenant circumstance quality service qos respect data access performance invariant individual tenant qos requirement satisfied cache usage optimize manner introduce approach dynamic cache management estimation data access tenant prediction cache performance access variety probability distribution estimate data access examine regression technique predict cache rate access predict cache rate decision reallocate cache qos requirement tenant experimental extensive synthetic trace ycsb benchmark propose consistently optimizes cache satisfy qos requirement previous keywords dynamic cache cache management access estimation cache prediction machine introduction cache important component compute significant impact data access performance critical data processing environment web service provider google facebook amazon largely rely memory processing memcached redis provider employ cache service amazon   forth efficient memory cache significant impact performance reading storage expensive cache addition performance benefit cache reduces load data server scalability processing massive data host web commerce service memory cache essential improve data access performance user network traffic increase service provider expand infrastructure however hardware resource infinitely performance costly impossible important limited resource efficient cache management role utilize limited cache maximize performance efficiency data access cache management consists function cache allocation eviction focus cache optimization assumption memory cache crucial concern cache management cache multiple tenant quality service qos requirement respect data access performance per tenant requirement addition optimal cache manner technique static cache allocates exclusively dedicate cache tenant individual qos specification however straightforward cache tenant without knowledge data access moreover access assign cache violate performance requirement waste limited resource dynamic cache allocates cache demand tenant dynamically beneficial per tenant requirement improve overall performance hence essential properly capture demand cache dynamic cache another alternative global cache tenant isolation guarantee desirable cannot dynamic cache account cache management benefit overhead cache replacement interval cache allocation relatively infrequent perform eviction image KB image model dynamic cache management dynamic cache mainly estimate cache performance individual tenant application however previous largely limited lack define model estimation data access prediction cache performance access recent disruptive advance machine technology approach cache management estimate data access varied probabilistic distribution uniform gaussian exponential zipf information estimate access predict cache specify data access performance assume qos define cache rate purpose examine various regression vector regression SVR gaussian regression GPR fully neural network fcn model implement dynamic cache aforementioned function access estimation cache performance prediction query transaction assume access infer query distribution estimate distribution cache rate predict cache qos requirement finally cache resize contribution summarize formulate dynamic cache objective qos guarantee optimization cache query distribution estimation probabilistic distribution normal gaussian exponential zipf demonstrate estimation kolmogorov smirnov KS yield accurate estimation query sample sample beneficial respond temporal timely manner examine regression technique SVR GPR fcn predict cache rate estimate query distribution evaluation effectiveness fcn model across diverse distribution respect prediction performance evaluate propose dynamic cache extensive synthetic trace yahoo benchmark ycsb cluster compute source apache ignite memory cache infrastructure evaluation confirm propose consistently optimizes cache maintain cache rate qos requirement organize specify description tackle summary closely related cache management propose model detail functional report evaluation conduct synthetic trace ycsb benchmark finally conclude presentation summary future direction background statement assume equip memory cache tenant load data access assume rare environment database transaction hence cache performance predominant factor overall performance tenant associate agreement SLA specifies qos requirement data access performance focus cache rate data access performance formulate notation define tenant exist cache cache allocate tenant adjust specify cache rate requirement cache rate periodically objective tenant notation NotationDescription tenant tenant cache cache allocate cache rate minimum cache rate requirement predict cache rate cache access distribution safety margin easy guarantee simply allocate plenty cache tenant actual however significant waste expensive cache resource perform optimization minimum qos guarantee individual tenant address optimization insufficient cache allocate tenant assume report administrator conduct define policy exceptional situation data access basically employ zipfian distribution model data access report zipf data access measurement zipf assume model load distribution cache request workload assume zipf file access evaluation addition yahoo benchmark ycsb widely employ model workload zipfian distribution assume zipf workload model data access coefficient extreme besides assume non zipf model data access instance exponential distribution assume reference rank model internet medium another employ exponential model model mobility mobility aware cache cellular communication possibility non zipf access uniform gaussian distribution exponential related exist memory cache environment summarizes relevant research difference image KB image cache performance estimation function concave function adequate cache performance prediction error gap  generation data cache consideration multiple tenant objective guarantee minimum qos requirement tenant maximize overall cache rate tenant qos goal author propose estimate cache rate function output predict cache rate cache scheme parameter non trivial challenge critically considerable error preliminary rate access error prediction demonstrate rate concave function employ technique accuracy flexibility utilize concept stack distance estimate data access cache management SC cache utility model introduce maximize overall cache rate multi tenant environment cache utility model cumulative distribution function stack distance histogram cumulative curve minimum cache satisfies minimum cache rate critical reference stack distance estimate data access complexity shadow eviction queue incoming request location request calculate stack distance  propose improve cache rate multi application environment technique relies stack distance infer cache rate curve interestingly approximation technique bucket reduce overhead calculate stack distance however estimation expensive assume application  defines metric identify application benefit cache management stack distance estimate identify application beneficial reference stack distance information calculate approximation expensive rely stack distance employ probability model estimate tenant access distribution comparison function assumes concave function predict cache rate cache recent  tackle cheat monopolize cache resource greedy user cheat environment user access data utility program greedy user access file already cache load user author propose mitigate impose penalty delay user cache allocation memory analytics prevent rid manipulation cache utilization cheat topic future exploration propose tackle previous propose architecture dynamic cache management overall scenario qos requirement cache rate tenant estimator estimate distribution query sample profiler tenant periodically sample estimation available predictor prediction regression function calculate tenant cache rate currently allocate cache estimate probability distribution define predict cache rate tenant predict rate minimal requirement comparison cache manager determines tenant cache resize discus functional component propose model detail image KB image architecture dynamic cache management estimator estimate distribution query sample profiler tenant periodically estimate distribution information available predictor prediction regression model calculate tenant cache rate currently allocate cache estimate probability distribution cache manager determines tenant cache resize prediction query distribution estimation tenant access data estimate distribution hypothesis distribution approximates probabilistic distribution uniform zipf exponential gaussian estimation distribution KS broadly employ empirical distribution KS indicates similarity distribution context sample comparison collection query sample tenant synthetic sample derive tenant distribution model indicates tenant access closer synthetic distribution comparison estimate uniformity straightforward KS sample uniform distribution identical contrast similarity non uniform distribution complicate sample varied distribution specific parameter zipf distribution becomes unique parameter similarly exponential distribution parameter gaussian define variance standard distribution distribution specific parameter accuracy estimation summarize parameter min max determines skewness distribution skewness zipf exponential respectively contrast implies skewness gaussian similarity non uniform distribution define parameter granularity comparison non uniform distribution KS specific parameter min max increase estimation independent KS similarity gaussian distribution incrementing estimation exponential zipf distribution distribution parameter finally distribution similarity chosen estimate distribution model tenant access KS fails identify candidate distribution access sample uniform distribution assume access cache distribution fallback scenario distribution parameter   parameter  gaussian exponential zipf intuitively query sample helpful estimate access accurately impact sample estimation difference actual distribution parameter estimate estimate distribution correctly sample however estimation non negligible error stage predict cache rate becomes stable sample estimate bound assume access rate  service sample within interval impact sample KS sample dist param cache rate prediction query distribution estimate model predict cache performance tenant prediction tenant cache adjust data access accuracy prediction essential tenant qos requirement met otherwise examine regression technique SVR GPR fcn metric mse error evaluate regression performance thorough analysis employ extensive data distribution parameter summarize data training disjoint without overlap examine model assumption cache GB GB simplicity assume byte KB repository exist unique data GB data cache rate prediction training   uniform 1GB 2GB 4GB 8GB gaussian 1GB 9GB incremented 1GB exponential 1GB 9GB incremented 1GB zipf 1GB 9GB incremented 1GB data cache rate prediction data disjoint training data overlap   uniform 3GB 6GB gaussian GB GB GB exponential GB GB GB zipf GB GB GB actual cache rate testbed server cluster compute instal apache ignite memory cache infrastructure node assign cache service node client backend database server detailed information regard experimental setting svm regression SVR svm widely employ classification regression predict cache rate regression model examine svm regression SVR assume radial basis function rbf kernel standard scaler normalization tricky SVR tune parameter optimize examine SVR extensive penalty parameter kernel coefficient parameter regression performance respect mse combination parameter SVR lighter error optimize parameter distribution independently exists parameter distribution SVR distribution zipf exponential distribution mse parameter tune poorly uniform distribution error mse gaussian regression GPR GPR utilized regression GPR account predict cache rate individual distribution kernel constant kernel configure constant rbf kernel tune optimal regression performance GPR across parameter impact regression performance trend SVR prediction error heavily skewed distribution overall performance GPR SVR regression fully network fcn recent advance introduction useful implement model promote adoption relevant technique application analyze data non linear another regression model investigate fully neural network fcn examine fcn architecture consideration evaluate impact accuracy prediction network architecture fcn architecture hidden layer neuron hidden layer examine neuron input layer loss function regularization kernel regularizers important role calculate penalty layer parameter combine loss function optimize network evaluate regularizers apply dropout expansion training data regularization loss function absolute error mae mse error activation function activation function non linear transformation determines neuron activate sigmoid relu widely epoch epoch training performs backward propagation entire dataset typically epoch complex dataset longer training vice versa examine impact epoch epoch rate rate indicates update strength propagation gradient decent parameter important training converge converge rate typical rate batch normalization normalization layer update neuron normalize update network fcn model improves regression performance batch normalization layer illustrates fcn model input layer input data feature data cache distribution parameter mention batch normalization BN layer optimize overall performance significant performance improvement intermediate layer evaluate structure hidden layer insignificant performance gap simplicity chose hidden layer structure output predict cache rate employ adam optimizer implementation image KB image fcn structure cache rate prediction input layer input data feature data cache distribution parameter chose hidden layer structure evaluation output predict cache rate conduct extensive evaluate impact parameter neuron epoch activation function sigmoid relu loss function mae mse regularization configuration perform distribution overall sigmoid regularizer slightly option mae uniform zipf whereas mse suitable choice gaussian exponential neuron hidden layer considerable impact regression performance epoch parameter slightly sensitivity neuron optimal configuration fcn model distribution       comparison prediction performance report performance comparison regression model subset experimental due almost trend insignificant difference image KB image regression performance uniform distribution regression performance cache tenant performance tenant cache allocation ratio cache rate cache assumption uniform access assume GB GB data access respectively axis cache per server hence aggregate cache cache per node assume cache server regression model fcn slightly outperforms others demonstrate cache rate tenant cache configuration respect cache allocation ratio axis ratio axis indicates cache allocate tenant cache assign regression model fcn predict cache rate GB cache memory GB cache memory assumption uniform access plot regression performs independent tenant conduct non uniform distribution trend omit presentation due summarizes performance regression model SVR GPR poorly uniform distribution fcn model consistently outperform technique training prediction complexity fcn overhead SVR GPR fcn training complexity gpus magnitude although prediction overhead fcn slightly others prediction within commodity computer equip intel core regression performance mse  uniform gaussian exponential zipf dynamic cache resize cache rate prediction cache management function determines cache resize procedure resize tenant cache straightforward distribution minimal cache additionally allocate tenant maximal cache return tenant otherwise cache tenant remains parameter configurable safety margin scenario cache available assume install resource expand cache SLA goal entire tenant evaluation validate operation dynamic cache management estimation prediction function previous report evaluation conduct cluster synthetic trace distribution ycsb benchmark experimental setting discus experimental metric cache rate response performance experimental setting conduct compute cluster elephant  edu consists node rack node consists cpu core GB memory TB disk storage node interconnect via gigabit ethernet switch instal apache ignite mariadb memory cache infrastructure backend DBMS respectively configure node memory cache service configure GB cache hence cache GB apache ignite built eviction FCFS lru simply chose lru cache replacement node dedicate database server mariadb image KB image procedure image KB image cache rate resize cache synthetic trace minimum rate requirement image KB image response resize cache synthetic trace minimum rate requirement illustrates procedure objective propose dynamic cache management effective tenant qos requirement data access specify cache rate requirement assume data GB tenant cache rate minimum initial cache GB node GB cache server consists query distribution generate default query operation invoked cache cache backend server access retrieve entry associate database val operation execute entry cache query service performs estimation prediction function resize cache performance cache resize initial cache GB data GB cache resize trigger desire performance requirement distribution estimation perform KS prediction fcn model described previous parameter fcn synthetic trace experimental conduct synthetic trace distribution model instal  ignite benchmark  ignite benchmark utilized  transactional distribute cache operation prepared synthetic data earlier data training disjoint without overlap summarize data GB distribution parameter chosen outside training data experimental synthetic trace trace chosen training data   uniform GB gaussian GB exponential GB zipf GB cache rate cache resize optimal resizes cache measurement data without rely prediction predict manages cache prediction procedure prediction safety margin observation max difference predict rate safety margin prediction cache management adjust cache predict rate minimum initial rate estimation access distribution prediction rate cache adjust query cache rate average prediction allocates slightly cache optimal GB average due safety margin correspond response query initial phase response entry cache operation trigger database access entire cache average response becomes stable response significantly cache resize response prediction slightly optimal prediction resize allocates cache hence rate summarize experimental synthetic data estimation identify distribution associate parameter rate response cache rate response resize cache predict cache information rate experimental synthetic trace resize cache memory  cache   resp GB GB GB GB experimental synthetic trace resize cache memory  cache    cache   resp GB uniform GB GB gaussian GB GB exponential GB GB zipf GB assume consecutive data access estimation query identify cache prediction activate cache reallocate inject trace distribution exponential zipf uniform gaussian initial cache allocate GB rate simply safety margin observation temporal image KB image cache rate response data access inject trace distribution exponential zipf uniform gaussian initial cache GB rate initial exponential cache resize GB GB zipf cache reduce GB uniform cache increase GB vii gaussian cache GB demonstrates dynamic cache management identifies access resizes cache GB qos requirement although cache adjust cache access degradation cache rate performance restore dynamically resize cache cache becomes shrunk GB GB cache rate requirement correspond response temporal ycsb report experimental conduct ycsb benchmark widely evaluate performance RDBMS nosql slightly modify benchmark perform specify experimental procedure image KB image estimation cache concave function estimation GB rate assume GB allocation image KB image cache rate response resize cache ycsb benchmark minimum rate requirement estimation concave function estimation propose demonstrates cache rate response ycsb propose concave function estimate data access minimum rate requirement synthetic trace allocate GB node cache estimation prediction concave function calculates estimate GB node performance requirement however reveals estimation rate resize contrast propose estimate distribution zipf cache resize GB node prediction fcn resize cache performance requirement cache rate resize average msec response concave technique propose yield rate average msec response meeting specify requirement conclusion memory cache widely employ improve data access performance despite importance largely limited lack define model estimation data access prediction cache performance access propose approach dynamic cache per tenant qos requirement estimation approximates data access distribution model uniform gaussian exponential zipf KS estimation accuracy query sample sample beneficial respond temporal timely manner evaluation regression SVR GPR fcn predict cache rate estimate access fcn model outperforms others across distribution finally evaluate dynamic cache management extensive synthetic trace ycsb benchmark evaluation propose consistently optimizes cache preserve tenant qos requirement cache management consists function focus cache optimization another important function cache management cache eviction significant impact data access performance assume traditional lru policy eviction estimate access helpful information improve cache rate future task investigate adaptive cache eviction access addition assume fix cache slot simplify multiple slab application various slot