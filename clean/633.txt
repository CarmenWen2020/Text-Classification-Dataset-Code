hyperspectral image registration relevant task application environmental disaster management rescue scenario  algorithm propose gpu performance distribute version arisen due continuous evolution sensor generate image finer spatial spectral resolution previous simplify program multi device initial mpi cuda multi gpu implementation  hitmap library program parallel application distribute array performance hitmap version assess homogeneous gpu cluster extend implementation functionality version hitmap arbitrary load distribution multi node heterogeneous gpu cluster load balance layout layout affect performance code performance correlate gpus available cluster previous keywords hyperspectral image image registration heterogeneous compute distribute array load balance introduction image registration task estimate translation rotation parameter image respect scene obtain viewpoint hyperspectral image registration technique propose focus performance however application management disaster surveillance operation hyperspectral image gpus boost classification target detection segmentation image effort achieve implementation hyperspectral registration algorithm  introduce sequential cpu implementation HYFM fourier mellin algorithm hyperspectral image registration  gpu cuda version performance suitable environment hyperspectral sensor technology improves image finer resolution spatial spectral domain computational memory latter limited resource gpus coarse grain distribute multi gpu implementation HYFM hybrid mpi cuda program approach proven satisfy future reveal program complexity mpi combine accelerator program model cuda redistribution data movement host device external scientific library issue abstract program approach introduce approach hitmap library task program parallel application capability automatic partition mapping distribute array arbitrary granularity proven achieve reduction overall program complexity multi gpu implementation HYFM unexpected algorithm node homogeneous multi gpu cluster homogeneity allows easy balance workload processing amount data accelerator sophisticated load balance mechanism arise distribute accelerator heterogeneous gpu cluster extension previous HYFM multi gpu implementation introduce mechanism adapts workload gpus available cluster improve performance mechanism distribute array version hitmap experimental ass impact load balance performance conduct cluster compose node host nvidia gpus capability generation hyperspectral image register distribute accord data layout profile usage device checked extent layout really balance gpu reveal option layout distributes load accord performance difference execute gpu layout option stable load balance standard deviation barely average gpu input impact load balance layout performance code organize related research reader overview hitmap distribute program library recall hitmap multi gpu implementation HYFM algorithm introduces load balance technique library apply extend obtain extend approach introduce finally conclusion feasible research future related gpu offload approach remote boost implementation algorithm gpu accelerate geospatial detection civilian military recently  image reconstruction synthetic aperture radar image marine remark sustain improvement sensor technology increase compute memory algorithm manipulates data obtain sensor embed unmanned vehicle satellite research focus exploit heterogeneous device accelerate relevant stage task instance introduce strategy latency efficient implementation dimensionality reduction algorithm gpu programmable gate array fpga FPGAs accelerate task target anomaly detection image reconstruction incomplete data exploit highly optimize gpu technique implement remote task image health monitoring segmentation prediction urban segmentation classification algorithm benefit gpu offload unreasonably compute device memory scatter gpus overcome limitation however algorithm distribute approach implement complex domain decomposition native program interface mpi cuda image KB image HYFM scheme registration hyperspectral image adapt hitmap compiler agnostic library api intermediate abstraction layer tackle issue halfway manual program distribute data structure message passing model PGAS partition global address  upc distribute array local address explicit mechanism construction reusable communication runtime adapt data partition aggregate communication data across global performance efficiency comparable upc reduce program complexity development effort hitmap extends generalizes memory hierarchy creation data partition functionality library distribute array model   allows transparent partition policy regular irregular define interchangeable module interface hide programmer decision granularity synchronization across hierarchical hitmap extend data structure sparse matrix graph methodology interface hitmap portable library interface transparent data management controller model abstract entity allows programmer easily manage communication kernel launch detail multiple heterogeneous device gpus multi core CPUs regard application specific hitmap program interface transparently agent multi agent pedestrian simulator overview hitmap library extension hitmap multi gpu implementation HYFM algorithm reader overview distribute program library hitmap library partition mapping management hierarchically distribute data structure runtime originally dense array extend sparse data structure sparse matrix graph methodology interface SPMD program multiple data model message passing paradigm hitmap defines abstraction parallel program distribute data structure function library grouped module tile function definition management hierarchically tile data structure functionality independently library improve locality sequential code define domain index compact  association index domain actual data access data efficiency manually developed code tile abstraction declare allocate subspace domain distribute data structure mapping function interchangeable module implement policy automatically domain virtual topology virtual topology generate another policy module runtime relation across establish policy partition  query obtain index subdomain mapped local remote virtual along partition cyclic dimension already hitmap partition version library partition programmer balance workload topology capability compute device distribute heterogeneous moreover distribute array constructor directly allocate distribute array topology layout function communication function abstraction  passing model tile tile across virtual creation  information marshall  exchange tile data across interface collective communication available complex compose multiple communication operation involve tile  implement  constructor function  associate distribute array automatically communicate transparently adapt construction target platform detail actual data distribution communication execute communication internally exploit efficient mpi technique derive datatypes asynchronous communication etc hitmap distribute  HYFM algorithm hyperspectral image reference target input goal register target image compute rotate shift respect reference image procedure implement cuda  nvidia gpus distribute device overview coarse grain parallelization distribute version algorithm implement hitmap modification adapt code heterogeneous gpu cluster reader gain understand computation communication phase algorithm organize accord depict initialization reference target image scatter gpus hitmap  constructor tile automatically distribute processor function receives input native datatype global buffer distribute topology function processor relation layout define desire partition distribution hitmap version algorithm devote node homogeneous multi gpu cluster input image slice equally distribute achieve image distribute array accord layout built topology project processor along dimension image   code snippet array define reference target input image satisfactory obtain previous stem aforementioned gpu homogeneity however distribution likely uneven load balance heterogeneous cluster compose host node gpu device compute capability memory subsystem data partition introduce balance slice gpu namely replace layout  constructor layout dimension along array distribute  dimension image  encapsulate apply code array declare  constructor along ratio data distribution compute internally sum array account array longer excess discard shorter array zero correspond data image KB image definition distribute tile hitmap input image image KB image hitmap array definition array correspond topology ratio data distribution compute sum array preprocessing compose reference target image filter  normalize finally shrunk reduce principal component analysis pca distribute approach gpu command reference target image slice load filter slice  allocate host memory previously global memory gpu filter normalize pixel relation pixel gpu correspond reduce operation summation input image compute image slice filter stage load balance distribute array define role assign requirement gpu memory compute principal component analysis filter input image compose stage correlation matrix input calculate task involves matrix depends assign load balance distribute array role reduce operation accumulate partial broadcast matrix detail matrix obtain parallel gpu  compute private singular decomposition svd correlation matrix matrix obtain decomposition transformation operator gpu reduce principal component version slice input retrieves transformation gpu image distribute tile code   define tile slice pca transformation correspond slice input load balance relevant accord algorithm restrict parameter   additional  transformation pca calculation consists redistribution reduce input image sketch image slice compute   however gpu input compose requirement fulfil rearrange slice pca slice distribute array slice array processor underlie topology project along dimension image    layout    rearrangement operation perform   function hitmap implement transparent mechanism rearrange data distribute  another tile mapping input output tile argument processing composition gpu loop reduce perform pas filter multilayer fractional fourier transform  polar coordinate transformation reference target merge phase correlate stage II IV operation compute  routine iteration accumulate ancillary buffer buffer partial polar correspond reduce compute gpu loop relevance compute demand load balance distribute array define finally stage sum partial reduce leader hitmap terminology algorithm peak processing leader thrust generate host index vector gpu average due data involve efficient issue distribute sort array broadcast along pca component reference target image data structure peak index vector cyclically traverse subset index array obtain partial maximum peak information partial peak structure packed custom hitmap datatypes leader finally inspects partial peak computes output rotation angle factor cartesian shift description comprehensive coverage algorithm although operation perform leader distribute array previously release experimental node multi gpu described hitmap implementation HYFM algorithm improve programmability performance previous mpi cuda version extend introduce distribute multi gpu experimental hyperspectral image register distribute distribution policy heterogeneous cluster node host nvidia gpus generation capability summarizes heterogeneous gpu cluster experimental gpus tesla titan gpu device compute capability node hydra refer intel xeon processor gpus global memory titan gpus global memory gpu tesla compute capability host server intel xeon platinum processor node  refer core hardware thread node link gigabit ethernet network fabric technology regard load balance scenario distribution define  layout mimic distribution layout memory accord global memory available gpu timeperf adapt distribution compute capability gpu slice slice distribute tile load balance scenario scenario gpus distribute array mimic behavior layout memory scenario distribute input array slice pca array global memory gpu GB approximate configuration integer sum  parameter determines slice pca array experimentally timeperf scenario distribute input array slice pca array normalize performance metric gpu model experimentally execute algorithm gpu model reference image gpu normalize performance experimental platform execution obtain tesla gpu summarizes obtain slice pca array approximate memory scenario testbed experimental hydra  cpu host intel xeon ghz core GB ram intel xeon platinum ghz core GB ram gpu device nvidia gtx titan nvidia tesla GV architecture CC cuda core GB ram  architecture CC cuda core GB ram nvidia tesla GK architecture CC cuda core GB ram cuda  device library cuda toolkit cuda driver mpi  network  ethernet contains col input image average standard deviation algorithm distribute gpus available accord memory timeperf load balance layout HYFM algorithm random input performance evaluation purpose performance non sensitive translation rotation input image randomly generate matrix synthetic hyperspectral image chosen representative spectral resolution image distribute gpus image chosen increment increase workload distribute distribution policy layout load balance stability gpu usage gpu usage sum gpu kernel host device device host device device transfer obtain nvprof cuda profiler gpus available cluster gpu device along correspond average standard deviation obtain random image input exhaust global memory gpus load balance layout code nvprof completeness additional profile activity hence memory layout relevant load unbalance apply gpus perform amount clearly outperform device capability modest overload gpu idle others synchronization standard deviation average gpu prof difference regard memory layout increase amount perform alleviates load gpus memory TB average gpu decrease however compensate compute capability standard deviation average gpu nevertheless redistribution useful compute input layout global memory gpus contrast gpu achieve apply timeperf layout really balance distribution average gpu standard deviation becomes almost negligible barely average adapt distribution compute capability gpu technique distribute multi gpu implementation HYFM algorithm detailed description distribute implementation algorithm synchronization data transfer balance gpu distribution contributes smooth progress execution minimize array memory timeperf load balance layout node cluster load       normalize performance metric timeperf array slice distribute tile TB nvidia gtx titan nvidia tesla nvidia tesla  device baseline normalize perf TB obtain trend input impact configuration load balance layout performance distribute multi gpu implementation HYFM algorithm timeperf layout intend adapt distribution compute capability gpu arises option performance standard deviation memory timeperf load balance layout cpu processing network communication device device analyze input image col  comparison load balance achieve memory timeperf layout input device sum memory transfer compute consume gpu layout along correspond standard deviation absolute percentage load balance   memory timeperf conclusion extension distribute implementation HYFM hyperspectral image registration algorithm feature extend version consists load balance mechanism code adapt performance capability heterogeneous multi gpu cluster review functionality hitmap library implementation manage distribute array distribution data recall algorithm gpus explain distribute array hitmap aforementioned load balance mechanism experimental ass impact load balance performance distribute version code conduct consists register random hyperspectral image heterogeneous cluster compose node intel xeon cpu host nvidia gpus capability generation workload distribute data layout applies distribution memory proportionally assigns gpus equip global memory timeperf distribute workload accord compute capability device input performance impact load balance layout code comparison layout propose extent really balance gpu usage comparison confirm layout unbalanced average gpu memory reveal insufficient compensate performance difference gpus average contrast timeperf layout indeed load balance scenario gpus average adaptation distribution compute capability gpu therefore load balance technique distribute multi gpu implementation HYFM algorithm balance gpu distribution contributes smooth progress execution minimize synchronization collective data transfer timeperf layout obtain execution finally future research propose improvement distribute implementation algorithm principal component analysis pca processing stage  operation reduce synchronization data transfer operation increase relevance balance gpu usage algorithm additionally integration hitmap controller model generic accelerator program simplify program portability gpu code