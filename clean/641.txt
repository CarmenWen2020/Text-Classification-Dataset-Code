convolutional neural network propose approach classify data correspond variety datasets indeed development data diversity information technology increase complexity algorithm numerous model propose complex algorithm data detachment accuracy convolutional operation increase convolution depth neural network increase employ convolutional network challenge regard consumption bandwidth memory requirement memory access chip communication platform traffic distribution effective improvement memory access consumption induced data transfer dataflow mapping impressive reduce increase delay consumption exchange data core communication network propose dataflow mapping various network reduce hop improve performance dataflow mapping approach affect performance improvement inference phase neural network proposes various traffic memory access mechanism traffic distribution alexnet model mesh topology propose mapping fmm mesh data efficiency traffic consumption fmm reduce approximately respectively traffic fmm improve performance alexnet traffic distribution impact data reduce consumption previous keywords convolutional neural network cnn mapping traffic traffic distribution introduction development information technology various data datasets render data classification challenge convolutional neural network cnns propose machine algorithm challenge increase data diversity classify data accuracy cnn categorize data label semantic segmentation classification input data feature dataset cnn powerful approach yield enhancement hierarchical label complexity algorithm attribute development application emerge information technology internet web achieve accurate employ photo edit application   increase accuracy image processing effective algorithm complexity enhancement data variety algorithm complexity effective increase convolution depth neural network NN accelerator DLA propose challenge cnn challenge dependent centralize component DLA chip communication platform memory access mechanism parallel pipeline computational operation memory manufacturing propose cnn challenge nevertheless memory access consumption cnns persistent cnns ML application related node internet iot web recognition increase depth complexity cnns increase depth computational operation nns challenge gpu due perform parallel operation filter ifmap matrix reduce efficiency increase memory access numerous  propose challenge approach dataflow mapping maestro evaluate runtime bandwidth memory requirement  shi stationary DLA local reuse NL stationary dataflows analytical analyze bandwidth requirement output stationary dataflows eyeriss eyeriss propose stationary stationary plus improve memory access bandwidth requirement respectively investigate dataflow memory bandwidth requirement systolic array dimension python systolic cnn accelerator simulator NN data cnns propose NN structure detect algorithmic structure characteristic cnn  ifmap channel kernel analyze dataflows DLA demonstrates dataflow mapping significant improve performance inference phase cnns propose mapping dataflow mapping dataflow mapping depends compute sequence whereas mapping computational operation purpose mapping model mesh network reduce delay consumption transfer data processing exchange data global buffer bus mesh topology suitable bisection bandwidth positive impact performance noc mesh scalable PEs without performance loss expandable interconnection bus deadlock rout algorithm propose mesh mesh planar structure traffic distribute mesh direction traffic distribution restrict direction topology another characteristic mesh bandwidth link PEs considers mesh topology chip communication platform alexnet traffic distribution propose traffic alexnet traffic distribution mesh various memory access mechanism accumulate operation perform processing PE per mesh node PEs network chip noc interconnection describes transfer data PEs accord input data PEs effective parameter accordingly mapping fmm mesh impact data traffic consumption analyze mesh architecture relationship bus source destination node various demonstrate memory access improvement fmm effective reduce consumption alexnet traffic distribution mesh fmm improves performance alexnet model remainder organize review previous  cnns discus background cnn concept fourth focus architecture fifth analyzes data traffic sixth describes fmm formulation described seventh experimental evaluate eighth ninth concludes related cnn neural network dnn model propose machine algorithm various input datasets improve data classification accuracy numerous DLA hardware development application machine algorithm review various approach DLA hardware machine algorithm propose previous improve convolutional operation performance alexnet propose layer cnn model classify input datasets accuracy imagenet model layer alexnet consist convolution layer fully layer layer per node computational operation alexnet vgg net googlenet model alexnet reduce connection fully layer cnn dropout induced decrease error rate imagenet comparison classification localization accuracy alexnet vgg net demonstrate improve accuracy alexnet vgg net investigate characteristic alexnet googlenet vgg net network illustrates multiplier maximum activation layer alexnet vgg net numerous exploit categorize dataset aspect data diversity cnn whereas data classification alexnet vgg net googlenet model improve performance semantic segmentation utilize pixel pixel evaluate pixel pixel approach fully convolutional network demonstrate significant improvement alexnet performance vgg net googlenet semantic approach address challenge mutual model cnn annotation multi label however consumption storage memory access cnns challenge increase cnn layer sparse cnn scnn propose novel dataflow approach improve performance factor scnn utilizes sparse retention activation compress encode evaluate unnecessary data transfer reduce storage model propose improve performance cnn dnn tune recurrent neural layer detect structure visual regard location orientation rgb hardware  propose machine algorithm dnn cnn memory access mechanism fabrication technology effective characteristic DLA hardware architecture reduce communication platform bandwidth storage memory access throughput dimensional 3D memory tsv technology rebalanced neural network dedicate processing PEs SRAM buffer  hybrid partition partition neural network computation adjacent dynamic random access memory dram parallelize neural network computational operation multiple  shidiannao consume graphic processing gpu owe lack dram access exploit static ram SRAM careful utilization specific data access memory remove multichip owe dedicate embed dram eDRAM per chip dadiannao machine supercomputer accelerate compute neural network improve performance cnn dnn memory access employ eDRAM per chip cnn  reduce chip chip bandwidth employ hierarchical memory characteristic reconfigurability electrical optical hybrid architecture multichip DL supercomputer decrease consumption increase gpu owe enhance optical data transfer rate dadiannao employ chip optical interconnection chip electrical connection perform data transfer mesh topology  circuit increase computational operation optical multiplier maintain input efficiency exploit optical communication platform challenge due sensitivity tsv link propose approach link propose multi link random failure optical mesh network improve performance link dual sequential failure improve performance sparse recourse efficiency detect link failure recovery affect improve performance optical accelerator reduce failure rate propose  utilized bandwidth fiber link reduce delay data transfer chip dram chip memory elastic optical network reduce delay memory access bandwidth variable spectrum selection propose efficient reduce spectrum consumption induced multicast service optical network approach employ optical network various spectrum improve bandwidth requirement optical  chip communication platform effective achieve memory access bandwidth requirement eyeriss propose novel noc architecture implement cnn dimensional 2D bus PEs multicast controller eyeriss architecture implement cnn algorithm consumption efficiency reduce memory access bandwidth requirement indeed data compression reusability input feature ifmap partial sum psum filter data stationary reduce chip dram access thereby increase data traffic global buffer GB PEs eyeriss improve performance dnns  however memory access memory requirement consumption sparse dnn SDNN owe variety batch complexity SDNN eyeriss approach aware algorithm activation memory reduce consumption dataflow mapping likewise analyze efficiency eyeriss architecture illustrates improvement stationary alexnet traffic distribution evaluate unicast multicast broadcast various stationary improve efficiency convolution operation enable  chip communication platform hybrid mesh wireless wireline 3D noc technology propose chip communication platform gpu cpu heterogeneous core improve performance cnn model latency throughput hybrid mesh reduce approximately respectively wireline mesh cnn model achieve reduction thermal hotspot 3D mesh whereas maximum reduce approximately negligible loss performance reconfigurable flexible pipeline parallel PE architecture propose improve performance machine algorithm exploit reconfigurable architecture parallelism semantic inference per layer increase data processing reduce SRAM bandwidth requirement flexible DLA architecture effective decrease consumption parallel processing tile technique localization DL application  analyze application parallelism DL network data parallelism kernel kernel hybrid evaluate brain  flexible programmable architecture zynq  platform improve efficiency cnn propose inference accelerator model comparison fpga employ shift accumulation sac operation calculation convolutional fully layer majority DLA hardware limited implement specific machine algorithm  however dnn algorithm reduce consumption accelerator baseline without degrade dnn accuracy highly automate codesign accelerator systolic array employ improve computational operation nns DLA architecture tensor processing TPUs propose custom ASIC NN machine google tpu architecture consists matrix multiplication local unified buffer dram accumulator matrix multiplication systolic array tpu accelerates inference phase nns owe precision gpu cpu tpu performs volume precision computation owe lack texture mapping multiplies experimental demonstrate increase processing tpu gpu cpu NN application tpu faster contemporary gpu cpu computational complexity increase neural network dnn neural network dnns involve layer tend dense sparse neural network hardware accelerator model dnn lack flexibility variety layer dnn trend neural network increase amount research hardware accelerator performance dnn model propose partition layer layer layer partition classic mapping within substrate however layer layer suitable dnns partition output reuse employ dnn accelerator however dram access remains challenge owe computational complexity latency efficiency MAERI reduce latency consumption eyeriss systolic array micro switch array accelerator architecture dnn accelerator employ interconnection network PEs prefetch buffer augment reduction accumulate psums switch MAERI bandwidth extra link improve performance dnn model unfortunately PEs remains challenge accelerator owe layer dnn maestro PEs dnn model source infrastructure model dataflows within accelerator kernel ifmap filter NN model posse layer convolution pool fully dnn model googlenet posse layer reduction layer addition layer dnn accelerator capable reduction layer eyeriss investigate quality performance identify loss performance dnn processor dnn model alexnet googlenet mobilenet improve performance approximately eyeriss stationary plus RS eyeriss hierarchical mesh RS link bandwidth data reuse rate however RS delivery data reuse rate increase multiplier accumulator mac complexity PEs  propose specific goal improve scene label embed platform aware layer layer prune algorithm reduce alexnet cnn model text mining accelerator propose decrease bandwidth requirement consumption delay reconfigurable architecture programmable gate array gpu perform computational operation nns parallel reduce efficiency due increase memory access memory numerous  propose improve consumption challenge gpu consists ASIC fpga noc simulation demonstrate noc  flexible structure ASIC fpga due flexibility interconnection chip communication network accelerator exploit approach improve performance inference phase dnns mapping approach propose increase efficiency reduce delay propose reconfigurable architecture noc accelerate neural network 3D technology multicasting mapping dnn prune propose improve performance nns propose model prune dataflow mapping noc reduce consumption delay inference phase  propose flexible dataflow architecture adapt compute multiple mixture parallelism computational operation demonstrate tile impressive increase performance lenet systolic 2D mapping architecture  consists neuron buffer kernel buffer arithmetic logic alu PEs flexible dataflow introduce loop unroll convolutional layer flexibility  significant improve performance convolutional neural network development evolutionary algorithm artificial intelligence challenge implement adaptive purpose intelligent autonomously genesys propose hardware software HW SW approach challenge backpropagation training phase loop indeed genesys employ HW SW prototype inference training phase evolutionary algorithm cnn background proposes mapping approach improve performance efficiency convolution operation previous DLA machine algorithm DLA hardware discussion concept NN model describes background nns machine algorithm classification alexnet cnn model pool convolution operation cnn cnns dnns machine algorithm whereas cnns dnns machine algorithm belong multilayer perceptrons MLPs cnns dnns layer algorithm consist convolution pool normalization classification layer dnns complex cnns broader application despite dnn cnn posse layer ifmap filter psum reusable synaptic cnn neuron however synaptic dnn convolution layer reusable reusability cnn synaptic effective characteristic improve memory access reduce bandwidth requirement hence evaluate alexnet traffic distribution cnn model mapping approach reusability cnn synaptic effective improve memory access efficiency cnn  detect 2D variance translation skew distortion feature mapping fmap effective network owe benefit structural constraint shift invariance reduce parameter subsampling computational layer nns multiple fmaps feature neuron limited identical synaptic increase depth convolutional layer hierarchy increase classification accuracy cnns fmap abstraction ifmap filter psum synaptic cnn described ifmap consists various input data categorize matrix semantic characteristic specific parameter filter matrix categorize ifmap data filter ifmap filter multiplication generate psum convolution operation categorize input data per layer label input feature characteristic input datasets per convolution layer recognize convolution operation alexnet layer cnn model convolution layer fully layer error rate alexnet imagenet whereas imagenet layer alexnet analysis error rate image classification localization imagenet visual recognition challenge illustrates enhance accuracy alexnet vgg net alexnet convolution conv conv conv conv respectively analyzes alexnet traffic distribution mesh topology accuracy layer model detail alexnet architecture cnn model pool operation combine output neuron cluster per layer neuron layer eliminate parameter per layer zero pad operation remove pool layer stride parameter cnn algorithm feature dropout width height reduce input depth indeed matrix array shift stride rotary matrix multiplication stride parameter image KB image alexnet architecture architecture mesh topology employ communication platform transfer data alexnet convolution operation consumption investigate unicast multicast analyze consumption various memory access mechanism configuration bus mesh partition node DLA architecture consists storage mesh topology interconnection PE storage GB transfer local data reusable synaptic cnn algorithm chip dram data storage layer alexnet convolution operation mesh topology interconnection PE sixty mesh node designate destination node convolution operation bus transfer data source destination node GB employ unicast multicast rout data transfer mesh node unicast data transfer node multicast data transfer node mesh parallel architecture interconnection node structure mesh topology investigate propose architecture describes alexnet traffic distribution per layer mesh accord cnn background concept overview integrate component transfer distribute data schematic propose DLA bus global buffer GB switch selector mesh topology whereas dimension mesh partition dimension traffic switch selector flit GB destination node enable signal destination node due reading address signal propose switch switch selector arbiter baseline router simpler arbiter due rout data distribute destination node address switch described subsection schematic 2D mesh mesh traffic distribution alexnet convolution operation node structure router PE router demonstrates schematic router router propose architecture switch owe rout algorithm rout algorithm multicast XY buffer backpressure mechanism router duplex injection ejection PE router buffer multicast buffer data multicast rout input flit transfer multicast buffer output link data multicasting router assignment data transfer switch output router stage pipeline stage pipeline buffer BW switch transmission ST route computation delay switch transmission ST significant architecture switch switch selector RC mechanism perform switch selector schematic router switch illustrates schematic switch switch various mechanism memory access data transfer accordingly switch architecture mechanism switch mesh circuit arbitration switch switch multicasting flit link address enable EN switch selector switch switch selector active decoder decoder decoder switch mesh selector switch multicasting flit switch mesh address implementation alexnet convolution mesh topology destination node 2D mesh conv conv conv conv conv respectively alexnet model destination node dedicate convolution operation denote node mesh topology bus transfer data GB destination node address traffic distribute per convolution owe stride conv traffic distribution stride stride conv conv however stride conv conv maximum utilization mesh node alexnet traffic distribution stride negative impact rectify linear computation illustrates location destination node analyze stationary mesh topology bus GB introduce component DLA architecture previous analyze data transfer bus mesh alexnet traffic distribution various memory access mechanism effective parameter evaluate consumption investigates dataflow approach data placement stationary previous propose dataflow approach introduce previous stationary utilize synaptic reusability cnn ASIC purpose dnn algorithm categorize local reusable LR local reusable NLR synaptic NLR synaptic employ NLR approach transfer data increase processing owe propose architecture structure dataflow LR approach stationary WS GB broadcast PEs whereas convolution operation PEs output stationary OS output stationary DLA output input activation GB sends psum GB stationary RS ifmap filter transfer GB PE horizontally whereas psums accumulate vertically local chip network accumulate psums transfer GB eyeriss propose stationary approach compute transfer data GB PE analyze consumption dataflow analyze dataflow stationary NLR output illustrates maximum improvement dataflow stationary  propose stationary propose stationary rcs dataflow approach alexnet traffic distribution mapping memory access mechanism accelerator transfer data node rcs dataflow vertical horizontal parallel simultaneously influence rcs consumption analyze rcs transfer ifmap filter psum data node alexnet traffic distribution mesh previous described rcs dataflow approach alexnet traffic distribution evaluates propose traffic memory access mechanism various architecture bus mesh architecture communication bus global buffer mapping investigate consumption mesh per multicast distribution architecture various sixty parameter ifmap filter matrix transfer PEs convolutional operation owe memory access mechanism interconnection bus source destination node propose architecture connection mesh bus GB mesh minimum hop destination node reduce consumption induced accumulate psums 2D mesh partition node destination source node partition alexnet traffic scatter mesh 2D mesh location partition mesh node partition mesh alexnet traffic scatter mesh 2D mesh mesh partition vertically partition mesh illustrates 2D mesh alexnet traffic distribution mesh partition mesh partition node destination source node partition respectively difference partition node image KB image 2D mesh node per partition partition node source destination node partition partition node source destination node partition partition mesh respectively 2D mesh alexnet traffic distribution mesh partition node destination source node partition respectively difference partition node image KB image 2D mesh partition node partition node source destination node partition node partition partition source destination node partition respectively partition node partition node source destination node partition node partition node partition node source destination node partition respectively multicast employ rout traffic distribution per mesh topology partition accord hop destination source node architecture chip communication bus mesh GB rcs dataflow approach utilized transfer data source destination node dataflow rcs traffic mesh dimension consume traffic distribution analyze various fmm propose evaluate mesh specific equation propose traffic analyze mapping mathematical description mapping image KB image 2D mesh partition node partition node partition node source destination node partition respectively partition node source destination node partition respectively partition node partition node partition node source destination node partition respectively partition node source destination node partition respectively parameter  source  destination node node source node destination node node RA permanent   mesh traffic per node   active node  node  active node   height  alexnet  YX dimension convolution partition alexnet traffic distribution mesh ifmap alexnet traffic distribution mesh ifmap partition  suitable partition maximum partition minimum partition maximum partition minimum partition permanent hop source destination determiner parameter suitable partition primary determiner parameter suitable partition hop partition percent partition percent central  distribution percent partition formulation propose mapping consumption alexnet traffic distribution mesh topology previous analyze various due memory access mechanism connection bus mesh mathematical description propose alexnet traffic distribution mesh accord hop source destination node propose model partition node evaluate hop maximum distance source destination node parameter mathematical description traffic distribution mesh model partition node parameter traffic distribution percentage central partition mesh primary definition parameter definition traffic per node matrix parameter dependent traffic calculate node along source destination node matrix demonstrates mesh topology definition matrix mesh achieve dataflow propose definition percentage active node various effective characteristic evaluate relevance node consumption described subsection definition alexnet traffic distribution mesh classify mesh dimension owe ifmap matrix definition suitable partition model subset selection node traffic met partition various dimension subset source destination node parameter calculate suitable partition definition obtain percentage traffic distribution partition mesh respectively purpose calculate percentage traffic distribution evaluate relationship propose chip GB suitable location accelerator architecture matrix illustrate 2D mesh data per convolution mesh model partition mathematical description alexnet traffic distribution consumption mapping analyze calculate percentage active node per mathematical analysis propose traffic subsection describes various propose alexnet traffic distribution mesh matrix denote 2D mesh matrix traffic per node previous communication architecture bus mesh GB subsection analyze 2D mesh traffic matrix describes 2D mesh topology mathematical analysis traffic correspond matrix hop source destination node achieve formula parameter obtain hop source destination node vertical horizontal respectively calculate matrix horizontally vertically achieve described per recursive equation owe node matrix calculate per node hop source destination node primary met met achieve source destination node respectively owe parameter increase node source destination per met parameter enhance source destination node per describes traffic 2D mesh partition node source destination node obtain hop source destination node primary met source destination node increase per owe parameter node source destination parameter enhance met describes traffic 2D mesh partition node source destination node describes traffic mesh partition node source destination node hop source destination node obtain primary met source destination node met obtain source destination node respectively owe parameter increase per node source destination met parameter enhance source destination node per mesh obtain calculate convolution percentage active node obtain mathematical analysis alexnet traffic distribution mesh subsection convolution traffic alexnet model matrix dimension ifmap filter mathematical description alexnet traffic distribution employ propose traffic parameter obtain destination node alexnet traffic distribution convolution convolution correspond stride describes alexnet traffic distribution mesh convolution ifmap member ifmap matrix subset parameter obtain ifmap analytical model suitable partition subsection propose mesh partition model hop source destination node partition suitable hop source destination node minimize partition various dimension obtain determiner parameter partition traffic partition dimension calculate maximum hop source destination node horizontally vertically respectively partition various dimension traffic subset node data transfer node restrict inside partition primary suitable partition investigate pdp parameter parameter obtain primary met employ detect suitable partition partition evaluate however parameter estimate suitable partition evaluation hop calculate described partition traffic suitable partition model parameter permanent numerator denominator respectively suitable partition partition mathematical analysis traffic distribution percentage mesh percentage traffic distribution partition mesh effective parameter analyze chip GB propose DLA architecture hence evaluate traffic distribution partition 2D mesh matrix demonstrates 2D mesh calculate percentage traffic distribution percentage traffic distribution partition obtain parameter source destination node partition mesh source destination node percentage traffic distribution partition obtain source destination node partition mesh parameter respectively parameter percentage traffic distribution partition obtain parameter source destination node partition mesh source destination node formulation described estimation mapping alexnet traffic distribution mesh per analyze consumption formulation experimental experimental propose various memory access mechanism interconnection bus mesh GB alexnet traffic distribution mesh topology previous described traffic mathematically propose analytical model mesh partition hop propose alexnet traffic distribution mesh bus evaluate hop consumption percentage active node mesh percentage alexnet traffic distribution partition mesh estimation consumption alexnet traffic distribution mesh memory access mechanism developed cycle accurate simulation  format inspire noxim obtain alexnet traffic distribution mesh per  cycle accurate furthermore verification switch synthesis VC device xilinx simulation alexnet layer cnn model machine algorithm layer alexnet model consist convolution layer fully layer analyze alexnet traffic investigation alexnet model caffe alexnet investigate alexnet project analyze framework investigate traffic distribution convolution mesh label hop consumption percentage active node traffic distribution propose hop hop evaluate effective parameter consumption hop reduce mesh correspond hop reduction mesh approximately mesh label mesh mesh correspond respectively described hop obtain whereas traffic distribute mesh multicasting per multicast traffic distribution rcs stationary correspond respectively description label subsection   mesh 2D meshFig mesh 2D meshFig mesh 2D meshFig mesh 2D meshFig mesh 2D meshFig mesh 2D meshFig mesh 2D meshFig mesh 2D meshFig mesh 2D meshFig mesh 2D meshFig mesh 2D meshFig image KB image hop image KB image consumption influence alexnet traffic distribution alexnet traffic scatter mesh multicasting propose traffic distribution mesh  demonstrates propose mesh whereas consumption mesh reduction mesh approximately mesh multicast traffic distribution mesh mesh rcs stationary respectively evaluation indicates unicast traffic distribution significant negative impact consumption decrease hop reduces illustrate mesh numerical description per explain concept mesh correspond mesh whereas mesh corresponds decrease approximately mesh mesh mesh corresponds multicast traffic distribution mesh mesh rcs stationary respectively analyze relationship parameter hop percent active node illustrate reduce hop reduces per increase percentage active node demonstrates scatter traffic distribution mesh reduce enhance percentage active node reduces demonstrates node spectrum brighter spectrum indicates node correspond mesh darker correspond mesh brighter reduce correspond owe coverage darker percentage traffic distribution partition mesh respectively percentage alexnet traffic distribution partition 2D mesh partition consist node 2D mesh corresponds traffic distribution percentage partition partition mesh percentage traffic distribution effective parameter chip GB propose DLA architecture GB employ storage ifmap filter psum data alexnet traffic distribute MAERI topology simulation MAERI mesh simulation illustrate consumption decrease delay increase mesh approximately respectively MAERI fmm evaluate MAERI topology traffic convolutional layer alexnet owe MAERI possess latency consumption eyeriss systolic array micro switch array accelerator therefore propose mesh fmm MAERI image KB image percentage alexnet traffic distribution partition mesh illustrates consumption component propose mesh network analyze contribution various component operation estimate distribute traffic mesh network image KB image delay cycle mesh MAERI mesh MAERI delay mesh MAERI experimental demonstrate consumption mesh reduce approximately respectively mesh mesh corresponds 2D mesh surround bus versus mesh bus multi traffic distribution decrease hop mesh increase percentage active node dispersion traffic distribution effective reduce hop mesh enhance percentage active node therefore consumption reduce decrease hop increase percentage active node conclusion future investigate approach improve memory access consumption memory requirement cnn model memory access consumption cnn model alexnet remain challenge propose fmm alexnet traffic distribution mesh topology memory access consumption various traffic memory access mechanism reduce consumption alexnet traffic distribution mesh topology evaluate parameter hop percentage active node percentage traffic distribution partition mesh experimental demonstrate reduction approximately respectively decrease hop increase percentage active node improve propose fmm partition alexnet traffic distribution mesh topology multi multicast traffic distribution decrease hop mesh increase percentage active node 2D mesh surround bus distribute buffer minimal storage effectively improve performance propose DLA experimental demonstrate percentage traffic distribution partition partition mesh performance propose DLA investigate GB mesh estimate percentage traffic distribution partition mesh 2D mesh surround bus effective parameter alexnet traffic distribution mesh topology evaluate distribute memory minimal storage DLA architecture mesh topology communication platform