neural network dnn lane reorganize lane network data independent typically feature resilience network data independence lane amenable parallel processing multi lane capsnet mlcn propose reorganization capsule network achieve accuracy highly parallel lane however efficiency scalability mlcn systematically examine mlcn network multiple gpus efficient capsnet model parallelism introduce load balance distribute heterogeneous lane homogeneous heterogeneous accelerator greedy heuristic almost faster na誰ve random approach generate mlcn model heterogeneous lane balance device neural architectural generate mlcn model device memory load balance discover model accuracy cifar keywords multi lane capsule network introduction approach distribute model parallelization neural network dnn concentrate depth dimension dnns organize parallelize along width dimension dnn architecture organize distinct neural network creates separable resource efficient data independent network feature resilience network neural network google inception multi lane capsule network mlcn data independent parallel specialized distinct computational target CPUs gpu FPGAs resource constrain mobile iot target opportunity challenge recent research focus multi lane capsule network mlcn separable resource efficient organization capsule network capsnet allows parallel processing achieve accuracy reduce mlcn comparison baseline capsnet parameter mlcn achieves accuracy significant speedup stem organization initial perform gpu environment highly parallel explore mlcn gpus comprehensive scalability efficiency mlcn multi gpu comparison baseline capsnet mlcn network   sec epoch accuracy cifar baseline   fashion mnist baseline mlcn mlcn moreover necessarily feature task implies distinct suitable distinct HW substrate tolerate impact various optimization quantization hardware HW optimal optimal sequence optimization HW explore lane hardware homogeneous heterogeneous accelerator scenario greedy heuristic almost faster random na誰ve approach explore generate random mlcn model heterogeneous lane naturally device generate model iteration model accuracy cifar maintain faster performance without extra effort generate model already load balance contribution comprehensive analysis efficiency scalability mlcn advantage data parallelism limited approach capsnet define load balance distribute heterogeneous heterogeneous hardware greedy heuristic lane hardware superior na誰ve approach generate random mlcn model achieve balance execute device apply neural architectural NAS mlcn model accuracy cifar organize capsule network dnn parallelization describes multi lane capsule network mlcn discus parallelize discus heterogeneous distribution heuristic approach technique randomly generate mlcn model balance device NAS finally experimental setup experimental conclusion related capsule network convolutional neural network cnn dnn commonly image cnns already achieve task image video recognition image classification medical image analysis however network difficulty location invariance loss location information cnn recognize mistakenly recognize image random understand important spatial relationship compose address dnn approach propose notion capsule propose hilton  wang encode spatial relationship capsule network  neuron scalar regular cnns vector later efficient realistic training algorithm network propose algorithm dynamic rout dynamically chooses activation capsule layer another calculate vector layer dynamically vector previous layer capsnet primary capsule PCs apply convolutional image splitting vector PCs vector identify matrix finally capsule digit capsule dynamic rout algorithm digit capsule vector classification vector encodes probability digit capsule reconstruct image auto encoder network dynamic rout algorithm advantage training location invariance drawback execution accuracy cnns however initial publication multiple improvement propose concept evolve  schmid  evaluation feature difference  regular cnns feature invariance capsnet advantage cnns advantage network image reconstruction mechanism auto encoder    analysis explainability capsnet understand explain behavior capsnet generative adversarial network gan achieve error rate cnn ren capsnet text classification adapt compositional cod mechanism capsnet architecture moreover capsnet architecture author propose increase  maintain increase  capsnet increase maintain  improve network rout algorithm chen liu reduce considerably training parameter deepen network regard architectural improvement jia huang propose hierarchical architecture residual convolutional layer wise dot improve capsnet complex data complex background propose RS capsnet improves overall capsnet reduce parameter model concern dynamic rout algorithm ren propose improve mechanism ren initialization algorithm important performance describes initial propose supervise algorithm instead dynamic algorithm improvement complex data capsnet application processing argumentation imbalanced image datasets  sanchez   capsnet medical image data challenge achieve performance trainable parameter counterpart cnns  nguyen performance capsnet lung cancer screen outperform cnns mainly training achieve traffic prediction capsnet outperform traditional cnns approach   capsnet multiple image datasets cnns argue capsnet training cnns important investigate improve parallelization scalability finally advance capsnet model publication categorize survey multi lane  mlcn previous introduce novel organization capsnet multi lane capsnet mlcn improve explainability performance parallelization without decrease accuracy generalization however beyond encode probability vector contains information reconstruct image distinct dimension vector feature image propose split capsnet architecture PCs independent PCs responsible dimension digit capsule image KB image mlcn architecture PCs per compute capsnet 2D convolution apply input image reshaped PCs convolution apply filter per convolution generate capsule distinct dimension digit capsule generate configuration distinct computational requirement advantage organization capsnet architecture allows parallelism execution PCs construct independently improve performance training deployment distribute environment improves explainability network associate feature image later chang liu propose improvement mlcn  model improves accuracy maintain parallelism scalability characteristic mlcn  publish concurrently development  although characteristic performance possibly similarly propose multi capsnet MS capsnet introduce fix capsnet network limited explore concept distinct convolution finally capsule network   capsnet explore parallelism  splitting network responsible compute  primary capsule entirely unlike computation dimension feature mlcn capsnet parallelization dnn parallelize normally network complex task data parallelism model parallelism pipelining data parallelism split data compute input batch batch compute synchronize batch although straightforward increase batch computation compute frequent synchronization batch impact accuracy accuracy speedup another possibility splitting network operation however trivial split operation normally data dependent operation split computation increase communication overhead moreover implementation detail network communication currently available framework trivial lastly important pipelining split network compute data pipeline approach normally approach performance scenario technique distribute training inference dnns mutually exclusive related mlcn capability easy model parallelism approach data parallelism mlcn network pipeline focus facilitate model parallelism advantage data parallelism alone advantage easily extend pipelining per lane remains explore future heterogeneous distribution advantage data independent deployed separately multiple accelerator multiple multiple accelerator deployment equally HW resource concerned communication involve characteristic computational intensity multiple accelerator characteristic computational deployment becomes involve involves load balance computational intensity computational apply optimization HW scenario illustrate multiple deployed accelerator compilation stack image KB image multiple neural network lane parallel multiple HW heterogeneous scenario execute optimization apply lane hardware trivial approach address deployment heuristic address future execution heuristic mlcn lane statically optimal deploy HW resource complex task aspect version compiler characteristic execute concurrently HW significant impact performance aspect affect performance however mlcn exactly accurate performance schedule decision reasonably approximate predictor acceptable average execution execution mlcn width nvidia gpus parameter independently gpu performance display behave varies linearly increase depth quadratically width factor gpu mlcn nvidia gpus compiler predict performance HW substrate approximate equation achieves pearson correlation experimental data equation factor gpu significance deploy heterogeneous gpus constant gpu infer simply execution gpu normalize execution insignificant execution execute fully network data normalize load balance algorithm execution prediction nvidia gpus mlcn however schedule width gpus model numerical partition bin bin correspond target gpu deployed insert bin equation previous execution prediction host HW via execution numerical partition NP achieve heuristic  algorithm pseudo polynomial dynamic program easy heuristic achieve implement greedy partition insert remain  bin algorithm greedy algorithm pre execution calculate algorithm image KB image algorithm greedy schedule algorithm mlcn architectural previously device execute lane mlcn model equation describes performance behavior mlcn model load balance schedule algorithm balance lane onto device achieve overall performance moreover model heterogeneous lane accuracy complex datasets cifar explore slightly balance load mlcn model device model define instead built built model heterogeneous lane device resource perform neural architectural NAS model accuracy model improves accuracy device resource smartly achieve introduce equation compute memory mlcn lane configuration equation along execution equation predict memory flop execute lane mlcn model device notion device lane bucket memory flop limit introduce generate random lane respect memory computational load FLOPS limit mechanism randomly generate lane mlcn model fully available device resource heterogeneous lane achieve accuracy lane device resource lane execute device balance finally generate numerous mlcn model model accuracy fix epoch concept illustrate diagram bucket illustrate prevents flop memory limit instance heterogeneous hardware scenario previous randomly generate mlcn balance bucket image KB image NAS mechanism generate random mlcn model heterogeneous lane naturally device memory heuristic discus algorithm perform architectural finally explore limit memory flop mlcn configuration heterogeneous limit impact accuracy heuristic memory mlcn lane implement bucket approach previously described estimate computational resource FLOPS described estimate amount memory execute lane nvidia nvidia smi capture lane configuration model memory consumption lane characteristic become memory consumption deterministic approximate equation lane equation memory consumption upper ceiling average error MiB analytically calculate network model layer configuration equation knowledge layer actual functionality moreover equation calculate memory consumption quickly memory requirement memory consumption execution clearly behavior trainable parameter grows differently layer output activation related execution related memory consumption lane configuration performance memory consumption mlcn model relation lane characteristic evidence mlcn model heterogeneous lane accuracy homogeneous propose approach neural architectural NAS mlcn model memory performance constraint NAS mlcn model accuracy resource available balance ensure generate model model tends accuracy performance execute fully device capacity load balance execution performance model maximizes accuracy dataset moreover lane compose model assign accelerator memory limit maximum load lane assign execute model maximize accuracy lane device memory restriction ensures generate model device load limit guarantee load device balance perform respect constraint developed random approach implement NAS develop generator random lane memory limit flop usage memory performance equation lane generator random model device balance device randomly generate lane device assign lane device mlcn model built device become finally accuracy generate model update model generation random generate mlcn network explore  device experimental setup machine google virtual machine instantiate vcpus GB ram default network interface gpu setup nvidia tesla cuda intel mkl dnn tensorflow experimental sensitivity input data fashion mnist cifar others chose fashion  execution execute fashion  exclude average others variation execution epoch consistently simplicity average configuration capsnet parallelization data parallelism baseline simply apply concept capsnet fashion mnist dataset parallelize kera data parallelism mlcn data parallelism mlcn data approach baseline kera data parallelism mlcn organization mlcn model parallelism mlcn model parallelize execution execute gpus multiple machine horovod mpi framework handle communication NAS nvidia gpus execute training epoch batch accuracy report geometric average generate model generate model per configuration average iteration average average iteration experimental mlcn scalability performance understand approach parallelization capsnet performance nvidia tesla gpus graph performance comparison baseline mlcn data mlcn model configuration mlcn faster baseline gpu report earlier however advantage increase gpus data parallelization speedup difference mlcn data baseline remain constant indicates reorganization propose mlcn improve via data parallelism benefit model parallelism latter mlcn model visible advantage efficiency achieve speedup gpus gpu baseline mlcn faster capsnet baseline allows model parallelism efficiently image KB image speedup parallelization approach baseline data parallelism mlcn data parallelism mlcn data mlcn model parallelism mlcn model speedup relative baseline gpu interpretation reader refer web version article impact batch minibatch batch significant impact performance dnn ratio computation communication available enable efficient HW batch significant impact data parallelism performance data per computation available gpus advantage mlcn model parallelism data parallelism approach batch graph speedup versus gpu batch efficiency gain batch grows batch relative advantage mlcn model parallelism increase batch equally increase efficiency data model parallelism approach image KB image mlcn baseline scalability nvidia gpus google VM vcpus GB ram impact batch baseline mlcn accuracy increase batch impact accuracy magnitude impact sensitive dataset difference fashion mnist cifar parallelism model model parallelism performance data parallelism advantage batch reduce pressure accuracy efficiency image KB image validation accuracy impact increase training batch baseline mlcn cifar fashion mnist datasets impact lane characteristic previous explore suitability mlcn model parallelization explore characteristic mlcn affect performance scalability hyperparameters mlcn width depth quantity respectively image KB image scalability variance lane configuration width depth impact parameter per consequently amount computation per computation per efficient multiple gpus becomes advantageous lane increase efficiency however increase width significant increase efficiency increase parameter indicates besides parameter computation affect performance mlcn wider performance deeper parameter another increase lane significantly increase performance increase lane increase amount computation available batch overhead computation separable gpu efficient extremely heterogeneous distribution heterogeneous lane gpus observation mlcn characteristic depth increase generality network report MS capsnet organization however deploy multiple gpus computational footprint challenge propose heuristic deploy width depth mlcn network depth width obtain execution propose heuristic  randomly distribute gpus advantage increase harder randomly distribution account greedy heuristic almost negligible heuristic image KB image average execution execute heterogeneous lane nvidia gpus random greedy partition lane execution distribution lane width depth heterogeneous lane heterogeneous gpu besides heterogeneous scenario heterogeneous accelerator nvidia tesla deployed gpu execution significant increase network communication moreover difference random deployment greedy heuristic becomes complex scenario heterogeneous HW carefully computational deployment performance image KB image average execution execute heterogeneous lane nvidia gpu multiple machine communicate mpi random greedy partition lane execution distribution lane width depth mlcn architecture strategy information scalability performance mlcn model accelerator ability generate random model model achieve accuracy cifar fitting multiple nvidia algorithm behave model exceeds gpu memory limit generates model load balance ability generate model characteristic explore increase memory limit computational limit accelerator affect average model execute random iteration epoch per model minibatch parameter obtain accuracy performance model hyperparameter model generate execution plot accuracy increase computational amount computation baseline model lane width depth generate model FLOPS average creation model fold computationally intensive fold model average accuracy random increase network tendency similarly mention earlier dataset epoch ideal network achieves performance image KB image average model accuracy random generate model max FLOPs limit previously baseline execute memory bucket gpus fix maximum FLOPs model increase memory allowance FLOPs memory consumption tightly behavior increase accuracy increase network image KB image average model accuracy random generate model memory limit execute FLOPS baseline bucket gpus behavior fix amount memory maximum FLOPs increase available bucket accelerator device capability representation image KB image average model accuracy random generate model gpus execute FLOPS baseline memory random generate model accelerator resource efficiently model accuracy previously achieve evidence NAS approach useful mlcn model load balance model accuracy per iteration execution random execution average model accuracy iteration significantly average model image KB image model accuracy random iteration FLOPS conclusion multi lane capsnet mlcn novel organization capsnet network achieve accuracy efficient HW utilization mlcn allows model parallelization parallel analyze advantage parallelization scheme capsnet data parallelism mlcn faster capsnet model parallelism almost efficient batch explore impact configuration performance scalability wider usually achieve HW efficiency parallelize mlcn characteristic deploy machine accelerator load balance factor performance propose greedy algorithm deploy scenario efficient na誰ve random deployment finally propose neural architectural NAS random generation mlcn model heterogeneous mlcn model generate naturally device execute load balance apply NAS iteration model accuracy cifar previous mlcn model