graph neural network GNNs powerful model generate node embeddings graph apply GNNs graph challenge perform training efficient scalable propose novel parallel training framework sample subgraphs minibatches reduce training workload magnitude minibatch parallelize computation tightly couple memory graph sample exploit parallelism within across sampler instance propose efficient data structure concurrent access sampler parallel sampler theoretically achieves linear speedup respect processing feature propagation within subgraphs improve cache utilization reduce dram traffic data partition partition approximation strategy minimize communication optimal develop runtime scheduler reorder training operation adjust minibatch subgraphs improve parallel performance finally generalize parallelization strategy multiple GNN model graph sampler propose training outperforms scalability efficiency accuracy simultaneously core xeon platform achieve speedup avx sample speedup feature propagation serial implementation algorithm enables training deeper GNNs demonstrate magnitude speedup tensorflow implementation source code http github com   keywords graph representation graph neural network graph sample graph partition memory optimization introduction graph embed powerful dimensionality reduction technique facilitate downstream graph analytics embed convert graph node unstructured connection dimensional vector embed essential task content recommendation traffic forecasting image recognition protein function prediction various embed technique graph neural network GNNs graph convolutional network gcn variant attain attention GNNs accurate robust embed without manual feature selection graph GNN training proceeds minibatches due connection graph node distribute cannot sample uniformly random minibatch data construct minibatches sample GNN layer layer sample vanilla gcn successor graphsage sample inter layer connection approach preserve training accuracy model parallel training efficient due phenomenon refer explosion namely additional GNN layer traverse sampler sample node grows magnitude consequently sample node across minibatches overlap significantly GNN layer amount redundant computation increase exponentially GNN layer alleviate redundancy FastGCN proposes independently sample node GNN layer without explicitly layer connection constraint although FastGCN faster incurs significant accuracy loss preprocessing graph expensive easily parallelizable due layer sample philosophy simultaneously achieve accuracy efficiency scalability perform sample graph GNN layer novelty propose graph sample minibatch training algorithm via joint optimization quality parallelization achieve scalability develop novel data structure enables efficient subgraph sample parallel update sample probability optimize parallel execution intra subgraph feature propagation layer wise update specifically cache efficient subgraph partition scheme guarantee minimal dram traffic optimization generalize multiple GNN model sample algorithm achieve efficiency avoid explosion layer  GNN contains neuron correspond subgraph node finally achieve accuracy sample subgraphs preserve connectivity characteristic training graph contribution propose parallel GNN training algorithm graph sample accuracy achieve sampler return representative subgraphs graph efficiency optimize GNNs minibatch subgraphs avoid explosion deeper layer scalability achieve respect processing core graph GNN depth parallelize various propose novel data structure incremental parallel update probability distribution parallel sampler data structure theoretically empirically achieves linear scalability respect processing parallelize operation overall minibatch training processing core specifically subgraph feature propagation perform intelligent partition along feature dimension achieve optimal dram cache performance propose runtime schedule algorithm training rearrange various operation significantly reduce training model configuration partition schedule node clip subgraphs improve feature propagation performance cacheline alignment parallelization schedule technique applicable GNN architecture graph convolution graph attention graph sample algorithm random sample variant random sample perform thorough evaluation core xeon server serial implementation achieve overall training speedup minibatch training achieves speedup without accuracy loss parallel training greatly facilitates development deeper GNN model graph achieve magnitude speedup layer GNNs tensorflow implementation background related graph neural network GNNs graph convolutional network gcn graphsage graph attention network gat model graph embed widely highly accurate robust representation graph node cnns GNNs belong multi layer neural network performs node embed input GNN graph node associate feature vector node attribute GNN propagates feature node layer layer layer performs tensor operation model input graph topology GNN layer output embed vector node input graph essentially input node attribute topological information graph embed output vector backward propagation mainly widely GNNs graph convolutional network gcn graphsage  graph attention network gat introduce detail graphsage model architecture summarize layer operation input graph initial node attribute initial feature graphsage layer aggregate signal node along graphsage network stack multiple layer input layer output previous superscript denote GNN layer parameter layer contains node correspond graph node input output node layer associate feature vector respectively denote input output feature matrix layer layer input node layer output node input output node bipartite graph adjacency matrix adjacency matrix graphsage layer contains learnable matrix propagation layer define wise matrix concatenation operation normalize adjacency matrix normalization calculate binary adjacency matrix diagonal matrix layer performs operation feature aggregation layer node feature layer calculates sum transformation aggregate feature feature layer node obtain node embed output GNN layer perform various downstream task analyze embed vector multi layer perceptron mlp classify graph node GNN layer node embed classifier mlp generates node prediction function wise sigmoid wise softmax generate probability node belonging supervise node truth label binary matrix encode truth label prediction truth obtain scalar loss entropy CE GNNs consideration update propagation specifically gcn difference graphsage explicit capture influence node instead influence propagate connection graph therefore adjacency matrix becomes normalization perform differently propagation layer symmetrically normalize adjacency matrix calculate identity matrix  layer propagate influence node hop away hop away shortest propagation layer define operation matrix concatenation symmetrically normalize adjacency matrix hyperparameter model gat instead aggregate feature previous layer fix adjacency matrix gcn graphsage gat layer learns adjacency matrix attention propagation gat layer specify attention adjacency matrix calculate learnable vector feature vector node feature matrix extension modify multi attention computation multi gat capture parallelization strategy easily extend multi version therefore restrict discussion gat summary model propagation training input generates output traverse GNN layer classifier layer loss layer obtain perform backward propagation loss layer GNN layer update gradient gradient compute chain analyze computation backward propagation propose parallelization technique operation minibatch training graph training GNN proceed minibatches iteration update involves graph node graphsage FastGCN gcn VR gcn incorporate various layer sample technique construct minibatches upper abstract meta construct GNN training graph sample node node layer backward propagation sample node sample various technique propose improve quality training randomly node output GNN layer minibatch node treat minibatch node layer connection sample node previous layer layer output layer input multi hop magnitude refer explosion analysis hop via equivalently node layer GNN influence layer propose technique alleviate explosion none scalability computation complexity perspective specifically variance reduction sampler memory usage sampler auxiliary neural network incurs significant computation overhead sample perform independently layer computes sample probability node sparse adjacency matrix selects fix node layer accord probability finally sample GNN generate embed minibatch built sample node adjacent layer clearly avoids explosion sample layer fix unfortunately training significant accuracy degradation sample layer independent significant portion node sample layer disconnect node sample layer prior propose minibatch training graphsage model graph sample developed parallelization strategy target memory multi core processor data structure parallel graph sample data partition scheme parallel feature propagation within subgraphs improve parallel graph sample algorithm compact data structure significantly reduce computation storage overhead graph sample propose schedule algorithm overall training scheduler intelligently operation GNN layer propagation reduce computation complexity update sample subgraphs improve cache performance lastly parallelization schedule strategy extend various GNN model limited graphsage  extends training GNNs graph sample  focus improve training accuracy bias elimination variance reduction technique mostly focus parallelization strategy achieve superior scalability multi core platform training algorithm enhancement propose  easily incorporate parallel execution framework without lose efficiency scalability graph sample minibatch training novel graph sample GNN training parallel minibatch training simultaneously outperforms accuracy efficiency scalability training analyze advantage efficiency accuracy optimization training parallel machine parallel graph sample algorithm parallel training algorithm minibatch training algorithm graph sample approach construct GNN directly input graph instead iteration update training sample induced subgraph construct GNN backward propagation GNN algorithm describes approach distinction traditional training computation perform node sample graph instead sample layer node computation training due reduce redundancy addition GNN subgraph propagation almost GNN graph directly replace feature matrix adjacency matrix subgraph discus requirement function representative graph sampler accuracy training layer sample propose graph sample minibatch define node sample output GNN layer GNN hidden layer selects node input hidden output layer respectively minibatch hop neighborhood hop neighborhood GNN generates label prediction minibatch node hop respect minibatch node image KB image complexity graph sample minibatch training analyze computation complexity graph sample training significantly reduces redundancy computation analysis sample overhead focus propagation backward propagation identical computation characteristic propagation later experimentally demonstrate technique significantly faster sample graphsage representative GNN model operation propagate GNN layer feature aggregation node feature vector layer propagates via layer connection aggregation operation transformation node multiplies feature overall complexity simplicity assume average subgraph complexity layer propagation minibatch convention epoch training define traversal training data predict label definition minibatch define epoch training minibatches subgraphs clearly computation complexity epoch comparison GNN training sample node layer selects layer minibatch complexity epoch minibatch minibatch minibatch training graph layer sample technique training complexity computation load grows exponentially GNN depth essentially due explosion layer node traverse node previous layer sample evaluate average evaluation layer node across minibatches training inefficient due computation redundancy minibatch becomes comparable training graph training complexity grows linearly GNN depth training graph however resolution explosion convergence accuracy overly minibatch hurt generalization neural network training configuration graph ignore convergence rate dependent input graph graph sample training parallel algorithm complexity linear GNN depth training graph efficiency training guaranteed throughout entire training node label predict output layer feature compute hidden layer redundant computation arise evaluation hidden layer node addition graph sample algorithm construct representative subgraphs proportionally training graph accuracy graph sample training layer sample assume subset node sufficient representation achieve goal sample graph sample algorithm construct representative subgraphs training absorb information generate accurate embeddings specifically output vector embed input graph topology initial node attribute graph sampler guarantee sample subgraphs preserve connectivity characteristic training graph training graph node non negligible probability sample widely various random graph sample algorithm unbiased random multiple random frontier sample preserve various input graph characteristic addition sample algorithm explore node graph due stochasticity sample algorithm valid candidate subgraph sample training perspective computation unbiased random multiple random algorithm within static category random accord throughout sample sample algorithm fix probability distribution node regardless historically traverse subgraph structure however frontier sample algorithm maintains dynamic probability distribution update frontier node timestamp therefore frontier sample computation complexity difficulty parallelization static algorithm frontier sample representative analyze detail performance accuracy parallel execution discus propose technique extend sampler specific sample intuition training frontier sample accuracy recall requirement characterize sampler requirement connectivity definition subgraphs output approximate graph respect multiple connectivity distribution  coefficient cluster coefficient graph critically define signal graph node propagate via GNN layer carefully maintain subgraph sample requirement initialization frontier sampler node uniformly random graph constitute significant portion subgraph node sample iteration input attribute training graph frontier sampler reader interested theoretical justification choice sample algorithm analysis parallel graph sample algorithm parallelization strategy frontier sample algorithm extend strategy graph sampler graph sample algorithm frontier sample algorithm proceeds throughout sample sampler maintains constant frontier consist vertex iteration sampler randomly pop node accord probability distribution replaces randomly popped node sampler update frontier desire budget algorithm detail accord empirical around image KB image sequential implementation training spent sample phase motivates parallelize graph sampler challenge sample discrete distribution research focus parallel sample dynamic probability distribution dynamism due addition deletion node frontier exist sample aliasing output sample linear processing cannot modify easily non trivial node evolve complexity straightforward implementation partition probability interval update interval replacement FS recommend author complexity sample expensive sample inherently sequential node frontier popped otherwise preserve characteristic graph address challenge propose novel data structure lower complexity frontier sampler allows thread parallelization propose training scheduler exploit parallelization within across sampler instance intra inter subgraph parallelization runtime schedule dashboard implementation node frontier replace efficient implementation incremental update probability distribution node achieve goal propose dashboard status historical frontier node node becomes historical popped frontier node pop probe dashboard randomly generate index formally data structure operation dashboard sampler implementation involves array dashboard vector maintain status sample probability historical frontier node node frontier pin tile dashboard tile data structure meta data pin address pointer tile entry DB corresponds pin node pin allocate continuously DB tile belonging popped frontier invalidate pin optimal parameter explain later index array auxiliary array cleanup upon overflow IA slot slot index DB pin correspond node DB slot flag frontier node false historical related analysis dashboard data structure summarize summary related dashboard frontier sample  dashboard DB data structure consist pin tile dynamic update probability distribution  structure meta information frontier node  tile pin belonging node tile index array IA data structure cleanup DB node frontier node sample subgraph average frontier node enlargement factor computation storage tradeoff DB frequent cleanup probability pop node frontier proportional allocate continuous entry currently frontier sampler probe DB uniformly random achieve algorithm clearly entry average frontier node sake incremental update append entry node invalidate entry popped node instead shift entry invalidate entry become historical accommodate append operation introduce enlargement factor approximation average training graph sample proceeds eventually entry DB information historical frontier node occupy historical node resume sampler although cleanup dashboard expensive due factor scenario frequently complexity analysis information IA cleanup phase traverse entry DB freed DB entry DB correspond vertex safely capacity IA slot entry IA contains DB entry intra inter subgraph parallelization subgraph GNN training independently sample multiple subgraphs sample subgraphs processor parallel parallelize within sample instance exploit parallelism probe cleanup DB image KB image image KB image algorithm detail dashboard parallel frontier sample array zero loop analyze complexity function algorithm denote generate random perform memory access respectively pardo pop frontier anytime sample average ratio valid DB entry occupy frontier vertex DB entry probability probe valid entry processor generate valid probe refers repetition algorithm selection slot update invalid inv operation occurs function incurs pardo cleanup cleanup DB happens traversal IA calculate cumulative sum index slot masked status slot obtain location valid entry DB expectation entry IA afterwards valid entry DB empty DB accumulate shift amount translates memory operation function fully parallelize cleanup happens DB throughout sample ignore compute cumulative sum pardo frontier frontier DB append entry DB operation pardo pop frontier pardo cleanup pardo frontier overall sample subgraph processor assume scalability bound theorem algorithm guarantee speedup proof speedup obtain algorithm serial implementation theorem guarantee scalability processor later perform intra sampler parallelism via avx instruction performance analysis node subgraphs sample sampler enters local graph cleanup frequently frontier contains node however sampler eventually replace frontier node overall subgraph graph graph skewed distribution node frontier node slot totally available DB cleanup DB allocate remain slot node without expand DB slightly alters sample distribution node sooner popped frontier affect training accuracy scalability dense graph challenge sampler massive processor sparse graph feasible parallelism bound graph summary parallel dashboard frontier sample algorithm enables serial complexity incremental update probability distribution processor dashboard sample data structure compact meta data frontier node tuple algorithm repeatedly DB meta data introduce pin tile mechanism DB reduce pin tile significantly reduces memory storage memory movement simultaneously graph sample exploit task parallelism across multiple sampler instance topology training graph fix training iteration sample GNN computation proceed interleave fashion without dependency constraint detailed schedule algorithm sample phase GNN computation phase described training maintain pool sample subgraphs empty scheduler launch frontier sampler parallel pool subgraphs independently sample graph sampler instance processing scheduler exploit intra inter subgraph parallelism training iteration remove subgraph GNN upon backward propagation algorithm pool subgraphs amount parallelism fix target platform carefully chosen parallelism operation DB mostly involve chunk memory continuous address indicates intra subgraph parallelism exploit instruction vector instruction avx addition memory traffic DB random manner desirable DB cache coarse estimation memory consumption DB indicates DB mostly private cache memory parallel machine therefore sample bind sampler processor core avx instruction parallelize within sampler core machine avx finally DB frontier node subgraph node increase training graph grows author interpret dimensionality random frontier sample equivalent random cartesian understand author fix graph extension graph sample algorithm reasonable graph sample algorithm perform minibatch GNN training evaluate sample algorithm random sample unbiased random sample RW algorithm recommend sampler assigns probability understood RW algorithm algorithm specifies algorithm categorization RW sampler static sample probability sample therefore computation complexity frontier sample easy computation complexity alias sample achieve complexity image KB image RW sampler apply inter sampler parallelism achieve scalability exactly inter sampler parallelization strategy difference subgraph pool obtain serial RW sampler improve training accuracy RW sampler integrate aggregator normalization loss normalization technique implementation normalization minor modification training algorithm pre processing training independently sample subgraphs estimate probability picked sample algorithm pre processing parallelize strategy apply normalization coefficient aggregator normalization feature aggregation normalize adjacency matrix loss normalization loss compute sum minibatch node therefore normalization computation parallel training algorithm parallelization technique backward propagation specifically subgraph training enables partition scheme ensures optimal feature propagation performance computation kernel training obtain subgraphs minibatches GNN computation mainly involves backward propagation along layer analyze detail backward propagation computation graphsage model GNN variant computation operation parallelization strategy generally apply model propagation already define operation various layer derive equation calculate gradient minibatch loss compute gradient respect classifier output subgraph node chain compute gradient respect variable mlp layer graph convolution layer layer layer layer entropy loss gradient compute mlp layer gradient compute graph convolution layer gradient compute equation backward propagation graphsage computation consists kernel feature gradient propagation sparse subgraph dense transformation feature gradient sparse adjacency matrix transpose operation gcn  gat gcn propagation contains pas graphsage concatenate operation therefore backward propagation replace  layer propagation consists graphsage therefore introduce backward pas compute sparse dense matrix backward propagation  involve sparse sparse matrix multiplication  computation operation gat pas compute attention subgraph adjacency matrix computation accord involves dense algebra obtain attention adjacency matrix propagation gcn backward pas accord chain computation logic pas obtain gradient respect attention parameter obtain gradient respect attention matrix series dense matrix operation obtain gradient respect gradient respect mathematical expression gat gradient computation complicate easy operation involve operation summary efficiently parallelize operation automatically execute backward propagation GNNs transpose sparse adjacency matrix technique parallel feature propagation dense matrix multiplication involve transformation operation standard blas routine efficiently parallelize standard library intel mkl subgraph adjacency matrix GNN layer model replace transpose sparse adjacency matrix assume training graph sample subgraphs undirected transpose subgraph adjacency matrix perform efficiently computation complexity discus serial implementation parallel version suppose adjacency matrix csr format consist index pointer array  index array index data array data undirected graph therefore index pointer index array identical transpose generate data array permute data csr image KB image propose generate permute data array pas  index algorithm relies weak assumption index node assume IDs index array sort ascend transpose operation algorithm correctness algorithm suppose adjacency matrix non zero denote node IDs satisfy traverse csr node IDs transpose data continuous subarray addition therefore reading simply append data subarray transpose csr computation complexity algorithm respectively operation training parallelize adjacency matrix transpose subgraph sample processor sample subgraph permute correspond data array algorithm information transpose subgraphs pool later consume GNN layer propagation parallel feature propagation within subgraph training node graph convolution layer feature along layer essentially operation feature propagation within subgraph label propagation within graph extensively literature vertex centric centric partition centric paradigm perform node partition graph processor independently parallel performs label partition along graph partition label borrow dimensional partition along graph feature dimension however realize technique sub optimal performance GNN feature propagation due propagate data node feature vector consist scalar label graph graph sample partition graph significant advantage analyze computation communication feature propagation graph feature partition temporarily ignore load imbalance partition overhead address later suppose partition subgraph disjoint node partition node feature connection node partition feature vector node processor responsible propagation define metric reflect graph partition quality depends partition algorithm bound performance model assume processor operating parallel processor associate private memory cache processor memory dram objective partition minimize overall processing parallel partition processor propagates due irregularity graph connection access random csr format node processor without cache summary optimal partition scheme memory utilize available parallelism minimize computation workload minimize memory traffic balance computation communication load processor feature propagation computation communication byte computation computation affected partition scheme formulate communication minimization without graph partition obtain approximation optimization subgraphs theorem approximation communication minimization irrespective partition algorithm proof clearly constraint satisfied due feasible due approximation ratio ensure cache KB subgraph upper bound met subgraphs consideration memory platform theorem derive bound ratio optimal partition scheme however optimal partition computationally infeasible subgraphs exponential partition experimental evaluation theorem typical core memory traffic feature partition optimal recall differentiate traditional label propagation graph feasible satisfy cache constraint feature partition algorithm specifies feature propagation image KB image lastly feature partition important benefit graph partition load balance respect computation communication optimal across processor partition incurs almost zero pre processing overhead extract continuous sub matrix summary feature propagation graph sample training achieves minimal computation optimal load balance zero pre processing communication volume runtime schedule computation layer operation backward propagation GNN layer involve chain matrix chain matrix multiplication compute chain computation complexity dynamic program technique obtain optimal correspond computation complexity specifically training chain matrix density subgraphs sample sparse matrix density dense matrix calculate computation computes partial computes computation accumulate mac operation computes partial computes mac operation therefore otherwise similarly suppose target graphsage layer calculate propagation calculate calculate decision scheduler relies dimension matrix runtime almost addition partition strategy rely specific computation summary schedule algorithm reduces computation complexity without affect scalability schedule feature partition partition feature matrix remains schedule partition performance optimization ideally operation partition completely independent schedule identical performance however reality partition  interact due false data private cache feature partition divisible cacheline private cache processor partition cacheline data partition another cacheline data partition therefore partition compute concurrently undesirable data eviction cache scheduler dispatch adjacent partition processing specify algorithm achieve goal processor feature partition algorithm iteration inevitable adjacent partition parallel partition divisible cacheline avoid false regardless schedule partition specifies suppose cacheline goal divisible precision float training cacheline byte clip subgraph node divisible clip negligible subgraph connectivity training accuracy node clip perform induction algorithm randomly node therefore clip incurs almost zero overall scheduler image KB image algorithm overall training scheduler multiple sampler launch parallel without data dependency clip objective specify transpose algorithm GNN construct backward propagation operation parallelize technique scheduler performs decision sample subgraphs decision runtime perform node clip improve cache performance decision statically perform actual training matrix chain multiplication backward propagation reduce computation complexity scheduler training replace frontier sampler graph sample algorithm plug fashion processing scheduler negligible overhead experimental setup conduct graph synthetic graph detail datasets described ppi protein protein interaction graph node protein protein interaction reddit graph node exists user comment yelp social network graph node user friendship node attribute user comment convert text wordvec amazon item item graph node amazon item customer node attribute convert bag text item description singular decomposition svd synthetic graph graph generate kronecker generator initiator matrix proportional matrix generate kronecker graph consists graph fix average node consists graph node average ppi reddit datasets standard benchmark graph yelp amazon evaluate graph thorough evaluation accuracy efficiency scalability specification graph fix partition split val percentage node training validation node synthetic graph generate graph topology node attribute membership random graph sample GNN training source implementation python tensorflow openmp respectively python tensorflow version thread accuracy evaluation baseline implementation python tensorflow version scalability parallel training evaluation scalability cache deeper implementation python tensorflow flexible parallel compute avx thread binding explicit python implementation achieves comparable accuracy tensorflow dataset statistic  val ppi reddit yelp amazon synthetic multi classification dual core intel xeon ghz machine 2GB ddr ram python implementation python tensorflow implementation compilation via intel ICC flag ICC version mkl version update omp intel parallel studio update evaluation accuracy efficiency graph sample training significantly reduces computation complexity without accuracy loss eliminate impact parallelization strategy training implementation baseline thread plot relation accuracy micro sequential training consistent setting baseline measurement GNN model gcn graphsage layer accuracy validation epoch baseline graphsage achieves accuracy faster convergence minibatch training achieves accuracy graph graph sampler preserve important characteristic training graph frontier random sample algorithm perform similarly reddit yelp amazon ppi random sample algorithm accuracy frontier sampler potentially due frontier sampler preserve graph simpler sampler RW due stochasticity training define accuracy threshold training speedup accuracy achieve baseline dataset define accuracy threshold serial training speedup calculate perform baseline threshold model threshold achieve serial training speedup ppi reddit yelp amazon respectively execute tensorflow framework thread therefore speedup achieve related parallelization strategy purely due graph sample training algorithm significant speedup verifies minibatch training improves computation efficiency avoid explosion evaluation scalability evaluate scalability various operation graph sample feature propagation transformation training scalability overall training propose GNN training parallel training speedup relative sequential execution execution training specify algorithm frontier graph sample avx enable subgraph transpose feature aggregation propagation correspond operation backward propagation transformation propagation correspond operation backward propagation operation activation sigmoid function etc backward propagation evaluate layer graphsage model hidden dimension respectively plot overall training highly scalable consistently achieve around speedup core datasets performance breakdown plot suggests sample corresponds portion training due serial complexity dashboard implementation highly scalable implementation intra inter sampler parallelism addition feature aggregation amazon corresponds significantly portion datasets due subgraphs sample amazon bottleneck transformation perform dense matrix multiplication analysis overall performance data dependent denser graph amazon feature aggregation dominates overall scalability sparser graph transformation impact training lastly parallel algorithm configuration hidden dimension training graph sparse dense image KB image accuracy plot sequential execution scalability parallel graph sample evaluate inter sampler parallelism frontier random sample algorithm intra sampler parallelism frontier sample algorithm frontier sample algorithm avx instruction target platform translate maximum intra subgraph parallelism xeon core launch independent sampler avx enable within sampler sample highly scalable inter subgraph parallelism performance degrades core due mixed boost frequency limited memory bandwidth core chip execute avx instruction xeon cpu boost ghz contrast ghz execute avx instruction core various speedup avx instruction otherwise achieve around speedup average data dependent training graph distribution significant portion node utilization avx instruction understand utilization instruction parallelism load imbalance due node variation load imbalance explains discrepancy theoretical model sample scalability theorem random sample algorithm sample algorithm avx instruction sampler cpu frequency unaffected scalability core core frontier sampler scalability feature aggregation scalability feature aggregation partition strategy achieve scalability around speedup core datasets various feature thanks cache strategy optimal load balance accord analysis scalability feature aggregation significantly affected subgraph topological characteristic therefore plot curve datasets transformation transformation operation implement   routine intel mkl library optimization dense matrix multiplication internally implement library plot scalability core average speedup achieve speculate overhead mkl internal thread buffer management bottleneck cache partition strategy feature aggregation cache evaluate cache rate various cache simulation csr format sparse adjacency matrix subgraph layout dense feature matrix source simulator  simulate implementation configure core cache cache corresponds cache private cache KB KB fix cache MB simulator training iteration cache rate private cache cache cache rate cache increase KB KB cache rate quickly parallel execution partition strategy indeed cache rate indicates amount memory data traffic benefit partition strategy image KB image cache rate comparison gpu propose training algorithm gpu implementation tensorflow gpu program nvidia tesla gpu GB GDDR memory xeon cpu server described performance propose training algorithm cpu tensorflow implementation gpu parallel graph sample algorithm described cpu execution available core gpu program sample cpu core gpu frontier sample algorithm node budget hidden dimension average execution per iteration iteration gpu program faster cpu program ppi reddit yelp amazon dataset peak performance CPUs tflops peak performance gpu tflops propose parallel training algorithm core cpu core machine propose algorithm perform gpu model importantly training gpu indicates effectiveness graph sample minibatch algorithm parallelization strategy frontier sampler execution per iteration hidden dimension  ppi reddit yelp amazon evaluation synthetic graph available dataset GNN training amazon contains node generate synthetic graph perform thorough scalability evaluation plot synthetic graph node around node synthetic graph average layer GNN hidden dimension subgraphs synthetic graph vertical axis denotes compute iteration perform backward propagation minibatch subgraph subgraphs sample frontier sample algorithm sample parameter increase training graph iteration converges constant around indicates parallel training highly scalable respect graph increase graph node average unchanged therefore sample subgraphs unchanged due frontier sample node budget fix subgraph node iteration approximately independent node training graph perform gradient update training graph training graph plot fix graph increase average sample algorithm graph becomes denser sample subgraphs likely denser computation complexity feature aggregation proportional subgraph iteration approximately grows linearly average training graph indicates parallel training algorithm handle sparse dense graph deeper although training evaluate GNN model deeper layer layer neural network proven effective increase expressive accuracy network evaluate efficiency overall training speedup GNN implementation various layer processor evaluation implementation evaluate computation efficiency layer sample training suffer explosion therefore model significant amount redundant computation across training iteration recall analyze per epoch computation complexity batch respectively severity explosion visualize sample node per GNN layer training denote graph convolution layer minibatch sample proceeds randomly node output graph convolution layer layer generate layer sample randomly layer sample node completes minibatch construction input node layer recommend regard propose training algorithm sample perform training graph GNN layer node unique sample node per layer training GNN model magnitude sample training addition sample node eventually converges graph GNN depth summary empirically verifies complexity analysis advantage training efficiency image KB image comparison sample node per GNN layer overall training GNN model increase GNN depth sample parameter described execution training processing core difference convergence rate per iteration execution normalize training layer GNN execution implementation prohibitively training ppi reddit runtime error yelp amazon training almost linearly respect model depth conclude minibatch training algorithm parallelization schedule technique significantly facilitate development deployment deeper GNN model image KB image comparison training GNN model discussion propose GNN minibatch training algorithm correspond parallelization strategy discus potential extension parallel training algorithm hardware acceleration minibatch training algorithm facilitate hardware accelerator apart computation efficiency another benefit construct minibatches subgraphs reduction communication suppose resource constrain hardware accelerator fpga speedup GNN training sample subgraphs feature subgraph node chip memory typical mega iteration input node feature subgraph transfer chip fpga perform backward propagation without communication external ddr memory therefore potentially achieve peak computation performance fpga developed performance accelerator cpu fpga heterogeneous platform graph sample training algorithm quantify feasibility implement various training algorithm hardware metric computation communication ratio indicates overhead external memory communication algorithm achieves significantly distribute processing graph sample minibatch training suitable execute distribute environment partition training graph distribute memory processing node perform graph sample independently local partition afterwards backward propagation execute without data access remote memory ensure convergence quality shuffle node data training optimal shuffle probability derive graph sample algorithm connectivity processing node worth processing node locally speedup backward layer computation hardware accelerator parallelization strategy conclusion future accurate efficient scalable GNN training redundant computation incur GNN training propose graph sample minibatch algorithm ensures accuracy efficiency resolve explosion challenge propose parallelization technique runtime scheduler graph sample overall training processor extend graph sample training integrate graph sample algorithm evaluate impact accuracy theoretical foundation understand convergence graph sample minibatch training