machine artificial intelligence AI application rely perform matrix operation matrix matrix multiplication gemm operation usually perform reduce precision float format precision FP gemm operation important dense linear algebra algorithm precision gemm operation mixed precision linear solver therefore performance batch gemm operation reduce precision significantly important framework scientific application rely batch linear algebra tensor contraction sparse solver optimize batch gemm kernel graphic processing gpus FP arithmetic address complex precision computation gpu propose advantage tensor core technology recently introduce cuda enable gpus tune parameter introduce developed kernel flexibility overcomes limitation impose hardware software discrete configuration tensor core apis FP arithmetic performance speedup cuBLAS complex FP gemm kernel speedup thanks standard interleave matrix layout contrast planar layout vendor discus optimization extremely matrix performance gain achievable previous keywords matrix multiplication precision batch linear algebra gpu compute introduction performance linear algebra library enable scientific application efficiently massively parallel architecture dense linear algebra software performance usually achievable algorithmic express computational stage compute bound routine matrix multiplication gemm gemm kernel embarrassingly parallel relatively arithmetic intensity define ratio amount float operation FLOPs byte transfer memory gemm kernel extremely important numerous computational domain accord standard interface linear algebra subprogram blas standard gemm operation update matrix scalar FLOPs gemm operation amount byte transfer byte float specific precision complex precision arithmetic FP arithmetic HGEMM complex gemm HCGEMM HCGEMM standard linear algebra subprogram blas notation assigns prefix denote precision consensus date linear algebra operation complex precision HCGEMM within api release software gemm kernel important scientific domain dense linear algebra computational workload broken matrix operation workload batch workload dedicate routine optimize workload batch GEMMs batch gemm kernel almost important regular non batch gemm feature application sparse solver tensor contraction hardware vendor optimize batch gemm kernel intel math kernel library mkl library nvidia cuBLAS library latter optimize batch HGEMM kernel however complex vendor advises split complex computation assumes imaginary matrix planar layout artificial intelligence AI machine revolution demand performance precision arithmetic float format AI application necessarily accuracy precision memory bandwidth footprint precision theoretically improvement precision precision respect float operation improvement factor precision depends architectural hardware target nvidia volta gpus native FP arithmetic theoretical peak performance FP computation tera float operation per  peak precision performance  peak precision performance  however volta gpus performance operation FP arithmetic operation mostly variation matrix multiplication hardware accelerate tensor core TCs operation performance gpu  although tensor core volta gpus nvidia gpus precision pascal gpus precision nvidia gpus implement binary format define standard precision exponent however format assumes implicit unless exponent zero FP format eleven account accuracy digit investigates tensor core purpose batch matrix multiplication FP arithmetic extends previous effort author complex computation gpu natively knowledge complex precision limited machine domain significant linear algebra respect mixed precision solver address challenge tensor core programmatically gpu kernel discrete restrict thread configuration kernel express building developed device routine perform specific task kernel building across complex precision function explicitly developed precision important goal highly flexible kernel withstand potential future tensor core technology developed kernel tune parameter aspect kernel extensive tune apply developed kernel respect typical batch gemm operation investigate benefit tensor core extremely utilization tensor core vendor routine optimize relatively batch HGEMM kernel outperforms cuBLAS speedup batch HCGEMM kernel outperforms vendor split complex computation improvement factor source magma library image KB image precision format accord standard highlight FP arithmetic proven useful numerical linear algebra important kernel batch matrix multiplication optimize kernel focus tensor core restrict limitation impose program model propose kernel flexible abstraction layer tensor core layer hide aforementioned restriction kernel tune parameter comprehensive tune sweep conduct performance complex computation theoretically interleave layout performance planar layout split complex performance gain batch gemm precision performance gain batch gemm complex precision matrix tensor core questionable due sub optimal utilization tensor core related matrix multiplication embarrassingly parallel operation relatively operational intensity enable gemm operation asymptotically gpu peak performance gemm gpus throughput orient processor emergence gpu accelerate dense linear algebra research effort date almost decade gpus programmable memory user cache enable researcher develop compute bound gemm gpus gemm kernel continuous improvement register memory prefetching development spark effort dense linear solver gpus magma library  chameleon performance portability gemm achieve performance critical tune parameter gemm publicly available development research community gpu vendor highly optimize gemm implementation overcome limitation impose compiler hardware scheduler similarly assembly implementation available cuBLAS library ability achieve performance gpu theoretical peak library mention vendor library  dense linear algebra algorithm aforementioned effort address gemm operation relatively sufficient parallel gpu recent application driven batch linear solver encourage vendor library developer dedicate routine address matrix algorithmically batch gemm important operation remains performance algorithm batch factorization however importance batch gemm beyond boundary dense linear algebra affect scientific domain sparse solver tensor contraction machine challenge optimize batch gemm regular gemm kernel relatively gemm operation longer compute bound attention paid optimize memory traffic automatic performance tune important batch routine performance sensitive tune parameter matrix batch gemm operation crucial machine application convolutional neural network cnns popular neural network dnns implement custom dense kernel originally caffe library tensor convolution activation function custom kernel developed locally per package dominate training cnns optimization whenever underlie architecture research effort cudnn  others focus optimize primitive blas optimize primitive  algorithm important operation cnns batch spatial convolution cast batch matrix multiplication addition batch GEMMs implement cnn algorithm minimal filter algorithm another batch gemm operation machine necessarily accuracy precision precision training neural network furthermore extreme computational dnns arises hyperparameter tune training multiple dnns empirically network various application popularity gpus AI application architecture nvidia namely volta turing equip tensor core hardware acceleration matrix accumulate operation cuBLAS library apis gemm batch gemm precision HGEMM batch HGEMM respectively apis program tensor core inside gpu kernel apis accelerate mixed precision iterative refinement dense linear solver effort author knowledge programmatically tensor core source purpose batch gemm routine competitive vendor optimize library FP tensor core gpus cuda toolkit program model precision FP arithmetic embed gpu model maxwell architecture FP arithmetic become mainstream cuda enable gpus pascal architecture precision dynamic significantly precision incorporate reduce precision mainly motivate disruptive emergence machine application volta turing architecture introduce hardware acceleration matrix multiplication FP hardware acceleration tensor core deliver theoretical peak performance faster peak FP performance volta gpu tensor core evenly distribute across multiprocessor tensor core posse mixed precision matrix processing array performs operation matrix input FP format FP FP format matrix vendor library cuBLAS various optimize routine mostly GEMMs advantage tensor core acceleration flag routine  implement batch gemm operation FP arithmetic matrix assume dimension complex FP computation native complex precision library cuBLAS planar layout matrix imaginary matrix planar layout enables easy exist cuBLAS routine lack important performance advantage later concept split complex computation applies  library source CUTLASS library advantage tensor core custom kernel apis program model tensor core input output data opaque data structure fragment fragment matrix fragment load memory global memory load matrix sync api api available content output fragment global memory gpu mma sync api perform multiplication user responsible declare fragment apis sequence program model imposes restriction program tensor core gemm dimension fragment limited discrete combination namely operation load perform warp thread finally load apis dimension correspond matrix multiple byte standard gemm operation load matrix sync mma sync matrix sync cuda version date access tensor core instruction mma sync instruction allows warp perform independent gemm operation however explicit instruction compatibility issue architecture release parallel thread execution ptx instruction focus device cuda apis image KB image programmability tensor core outline describes concept batch HGEMM batch HCGEMM kernel dimension gemm operation assume unified across batch accord cuda program model gpu kernel dimensional grid dimensional thread TBs gemm operation batch refer  precision computation cuda data complex computation vector imaginary grid magma library grid batch kernel output matrix subdivide memory register memory rectangular denote blk blk dimension grid denote dimensional subgrid output matrix batch grid dimension batching across yield dimensional grid configuration  subgrid unique  dimension grid gemm operation similarly input matrix subdivide blk blk blk blk respectively within subgrid TB responsible compute output matrix reading blk iteration loop TB multiplies blk blk blk blk tensor core accumulation partial fragment regular register buffer memory illustrates TB organization kernel aspect kernel leverage concept exist kernel modify generalize advantage tensor core detailed thread abstract tensor core mention tensor core programmatically constraint device apis goal gpu kernel abstraction layer tensor core abstraction layer device api constraint purpose core constraint tensor core tensor core discrete combination device apis warp load fragment memory specific dimension multiple byte abstraction layer propose address constraint eventually gpu kernel arbitrarily warp constraint dimension matrix developed arbitrary dimension global memory gpu straightforward matrix memory buffer reading directly tensor core fragment memory buffer allocate dimension abide byte recursive  technique allows gpu kernel arbitrarily blk blk blk necessarily restrict tensor core TC TC TC recursive technique previous gemm transfer global memory dimensional thread configuration dim dim subdivide dim dim tile memory register file dim dim subdivision compute partial technique recursive  technique applicable tensor core discrete computation propose generalization  recursive  simply decouples tensor core global memory matrix subdivide dim dim tile computation partial however load subdivide TC TC TC TC thread regroup configuration stage kernel assume warp configuration kernel load data configuration default warp reorganize dim dim configuration reading data computation regular configuration  improve memory traffic  technique generalizes blk blk blk bound TC generic improve memory traffic gemm kernel illustrate simplify analysis assume blk blk fully respectively accord grid configuration propose kernel TBs theoretically ideal implementation global memory exactly propose kernel perform ideal memory traffic TB writes exactly however parallel distribute across independent TBs redundant memory load redundant load significantly affected blk blk memory load per TB entire memory traffic kernel calculate improvement reduction memory traffic previous formula blk blk blk TC TC TC eventually yield relative reduction memory traffic blk blk theoretically reduce memory traffic factor TC TC factor TC TC reduction usually performance memory request fulfil memory due relatively cache gpus CPUs however theoretical analysis assumes infinite resource available TB resource per TB actually worsen aspect kernel occupancy register pressure therefore tune usually specific gpu stage load input data instead load directly memory buffer stage data register buffer offload memory buffer incorporate prefetching mechanism register file data buffer device function developed purpose data global memory register file image KB image function templated blk blk thread configuration dim dim parameter compile enables fully unrolled loop avoids register spill local memory fetch function return zero address bound function building across complex kernel device function offloads content register buffer memory structure nest loop image KB image unlike previous device function  function cannot complex buffer TCs arithmetic split content register file memory imaginary function correspond   complex function intrinsics splitting image KB image multi warp configuration TC device function invoked warp however necessarily TB configuration restrict warp sometimes beneficial multiple warp per TB amount per TB relatively warp stall generalize thread configuration warp mention thread reorganize dim dim configuration memory operation computation however reorganize thread configuration TC parameter dim dim serf flexibility warp configuration etc partial compute accumulator subdivide sub TC TC warp loop sub robin manner sub respective warp loop correspond sub sub sends chunk tensor core accumulate respective fragment workload distribution configuration accumulator sub multiple warp enables amount per warp data reading usually achieve memory bandwidth increase data reuse TC multiplication perform warp data warp becomes involve warp computation quantify concept perform batch HGEMM matrix simplicity fix blk blk blk TC TC TC thread configuration dim dim increase warp per TB significant performance gain relatively fix parameter however performance warp per TB occupancy impact TBs schedule runtime multiprocessor another warp idle compute phase warp tensor core sub organization warp assign computational workload image KB image multi warp configuration robin assignment output perform multiplication tensor core load input data memory buffer another device function invoked perform TC multiplication robin style illustrate function heavily templated constant compile TC BLOCKS   parameter TC BLOCKS refers multiplication thread performs   refer warp accumulator fragment per warp respectively pseudocode distributes TC BLOCKS multiplication across warp iteration outer loop warp independently calculates coordinate sub compute proceeds innermost loop correspond sub correspond sub TC apis code cleanup handle situation  fully TC BLOCKS image KB image function complex arithmetic dedicate function split complex multiplication TB complex function accepts memory pointer imaginary innermost loop performs multiplication instead partial accumulate output fragment image KB image processing recall gemm operation define computational stage compute operation perform processing stage batch HGEMM kernel processing stage load respective register buffer memory traffic non zero resides memory register buffer finally global memory complex batch HCGEMM recall complex function imaginary therefore merge memory processing batch HGEMM kernel kernel structure pseudocode loop kernel iteration data load global memory memory stage actual dimension compute iteration  function account partial zero pad synchronization data visible warp proceed multiplication subroutine multiplication simultaneously reading data another synchronization warp currently load data overwrite content memory image KB image tune parameter developed kernel cuda template tune parameter parameter configuration tensor core TC TC TC discrete configuration enforce program model blk blk blk amount data reuse improve memory traffic explain influence memory requirement kernel thread configuration reading data dim dim parameter thread processing data influence register pressure per thread assume blk blk blk increase thread register per thread tune parameter satisfy kernel perform correctly tensor core dimension TC fully blk dim dim fully blk dim dim multiple warp chose comprehensive brute tune sweep eligible kernel instance decision performance performance sensitive tune parameter complex batch gemm planar interleave layout perform complex computation cuBLAS library user split complex computation imaginary separately planar layout dense linear algebra algorithm planar layout achieve performance relatively exist linear algebra numerical library assume interleave layout meaning imaginary contiguous memory practical rewrite entire algorithm split complex computation TCs relatively easy develop gemm kernel planar layout exist gemm kernel linear algebra component straightforward triangular pivot stage LU factorization operation exist triangular pivot kernel development blas therefore convenient standard interleave layout complex compute bound kernel normally peak performance underlie hardware earlier counterpart arithmetic complex kernel operational intensity arithmetic kernel operational intensity operation ratio FLOPs byte transfer operation FP scalar operation FLOPs addition multiplication flop byte ratio complex FP arithmetic operation FLOPs FLOPs multiplication addition update addition flop byte ratio operational intensity scalar operation difference operational intensity slowly theoretical peak performance roofline HCGEMM kernel relatively approach hardware peak performance planar layout HCGEMM kernel roofline arithmetic HGEMM kernel HGEMM theoretically investigate complex gemm operation roofline model focus batch multiplication batch rank update relatively constant former demonstrates peak performance kernel latter important batch linear algebra batch LU factorization roofline HCGEMM standard interleave layout simplicity analysis safely ignore accord  standard gemm operation involves addition multiplication explain output matrix compute account multiplication addition therefore HCGEMM interleave operation addition multiplication estimate FLOPs recall multiplication complex FLOPs addition flop flop therefore derive roofline performance upper bound derive ideal minimum amount memory traffic operation ideal amount data HCGEMM roofline HCGEMM planar layout estimate roofline HCGEMM planar operation split complex computation operation via standard HGEMM operation assume output matrix imaginary HGEMM fourth involves addition multiplication perform precision arithmetic addition multiplication account FLOPs therefore performs FLOPs FLOPs operation complexity HCGEMM interleave however collective memory traffic HGEMM involves traffic traffic traffic memory traffic HGEMM byte aggregate traffic similarly summarize HCGEMM interleave operational intensity HCGEMM planar multiplication operational intensity simplify HCGEMM planar HCGEMM interleave roofline model estimate performance upper bound operational intensity peak memory bandwidth gpu benchmark tesla gpu GB peak memory bandwidth performance upper bound HCGEMM planar HCGEMM interleave matrix graph linearly matrix gpu peak performance however due increase operational intensity HCGEMM interleave peak earlier HCGEMM planar crucial relatively HCGEMM planar bandwidth limited HCGEMM interleave becomes compute bound difference kernel properly optimize HCGEMM planar saturate gpu anyway another useful gemm rank update important dense linear algebra roofline analysis update matrix rank update per roofline model remains bandwidth limited gpu regardless operational intensity HCGEMM planar HCGEMM interleave HCGEMM interleave theoretically faster HCGEMM planar performance performance propose magma kernel equivalent implementation vendor library cuBLAS experimental setup performance conduct equip socket cpu intel xeon ghz core per socket tesla pcie gpu gpu GB memory multiprocessor clocked ghz cuda toolkit compilation magma kernel cuBLAS relatively relatively rank update performance batch HGEMM kernel performance tune magma kernel cuBLAS batch HGEMM operation performance speedup magma kernel rank update behavior importance auto tune performance kernel sensitive tune parameter kernel instance relatively unlike situation usually kernel instance deliver performance summarizes performance speedup average speedup spike spike speedup mainly due cuBLAS performance behavior periodic performance spike image KB image performance batch HGEMM kernel cuBLAS  tesla pcie gpu recall cuBLAS kernel utilize optimization technique available cuda ptx instruction asymptotic performance faster magma factor matrix however significant cuBLAS advantage observable multiple batch cuBLAS kernel faster magma  cuBLAS  magma however batch significant cuBLAS slight magma magma slight advantage  cuBLAS  magma multiple witness competitive performance library image KB image performance speedup batch HGEMM kernel cuBLAS  tesla pcie gpu performance batch HCGEMM kernel performance batch HCGEMM kernel respective relative speedup recall cuBLAS library planar layout imaginary matrix memory however report timing merge component execution computational kernel overhead per theoretical analysis advantage interleave layout planar layout increase operational intensity former performance advantage magma kernel wider speedup report significant report report speedup magma batch HCGEMM kernel impact batch performance performance batch kernel depends batch batch kernel relatively batch saturate gpu parallel conduct sustain peak performance achievable kernel however application necessarily batch typical application another performance batch HGEMM HCGEMM kernel relatively batch necessarily occupy gpu resource performance batch batch HGEMM HCGEMM kernel respectively highlight behavior rank update hence omit avoid redundancy image KB image performance batch HCGEMM kernel interleave layout cuBLAS planar layout  tesla pcie gpu image KB image performance speedup batch HCGEMM kernel interleave layout cuBLAS planar layout  tesla pcie gpu image KB image impact batch batch HGEMM performance  tesla pcie gpu batch performance parallel available gpu however relative performance magma cuBLAS asymptotic behavior behavior batch HGEMM kernel graph increase  shift performance graph without significant relative speedup behavior observable batch HCGEMM kernel however magma HCGEMM kernel arithmetic intensity magma performance operation sometimes equivalent cuBLAS performance operation behavior emphasizes advantage interleave layout dense matrix computation batch workload optimization extremely matrix tensor core apis discrete configuration TC TC TC batch gemm configuration TC utilization TCs questionable focus performance optimization cannot fully occupy TCs vendor apis without loss generality matrix dimension research effort recently due popularity application multiple GEMMs tensor core multiplication recall permissible TC multiplication TC multiplication therefore perform FLOPs combination multiplication operation FLOPs TC multiplication previous proposition author improve utilization perform multiple gemm operation TC apis improves utilization gemm improvement TC utilization maximum achievable utilization TCs perform multiplication configuration utilization compute peak precision performance percentage without TC multiplication theoretically sufficiently optimize kernel TC apis outperform TC kernel utilization percentage TCs matrix conventional without tensor core perform multiplication efficiently regard refer kernel developed matrix specifically kernel address multiplication thread configuration gemm thread responsible output matrix code fully unrolled template kernel complex arithmetic kernel magma image KB image improve tensor core utilization assign multiple GEMMs matrix tensor core image KB image percentage available tensor core compute multiplication matrix performance magma kernel magma kernel cuBLAS summarize batch HGEMM HCGEMM kernel respectively generic magma kernel perform kernel magma kernel tensor core observation nicely aligns suboptimal TC utilization access  compute utilization impact available compute TCs available compute without threshold ratio FP compute situation utilization threshold advantage kernel TCs realize magma kernel outperforms kernel TCs TCs batch HGEMM kernel speedup cuBLAS batch HCGEMM interleave layout extra advantage magma kernel speedup image KB image performance batch HGEMM kernel  tesla pcie gpu conclusion future introduce optimize batch matrix multiplication kernel FP arithmetic gpus developed kernel address complex precision advantage tensor core accelerator nvidia gpus kernel abstraction layer encapsulates constraint vendor apis program TC precision matrix batch HGEMM developed kernel outperforms cuBLAS speedup developed kernel competitive cuBLAS multiple cuBLAS advantage complex matrix standard interleave layout planar layout mainly due increase operational intensity developed batch gemm complex matrix batch HCGEMM faster cuBLAS planar layout discus optimization extremely TC apis questionable overall matrix faster cuBLAS batch HGEMM HCGEMM kernel respectively development optimize matrix multiplication kernel usually important develop dense linear algebra algorithm developed kernel reduce precision factorization mixed precision solver linear equation role auto tune critical maintain performance portability across architecture application batch direction promising future developed kernel