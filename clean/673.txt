nowadays gpu accelerator commonly purpose compute task variety hardware however due diversity gpu architecture data optimization code hardware specific data characteristic extremely challenge autotuning performance relevant source code parameter allows automatic optimization application performance portable although autotuning typically code tune unacceptable overhead tune vast poorly perform implementation autotuning frequently data migration hardware introduce novel generic tune tune tune parameter user define source code advantage hardware performance counter profile counter empirical tune counter navigate towards faster implementation tune sample gpu specific model autotuning various previously unseen input gpus benchmark experimentally demonstrate autotuning application hardware data characteristic superior typically outperforms convergence keywords auto tune performance counter cuda introduction recent decade gpu accelerator non graphical application multiple device portable device supercomputer model gpu hardware significantly architecture manufacture vendor generation nvidia cuda gpus available core per multiprocessor presence cache register moreover generation contains gpu model float performance memory bandwidth hardware characteristic heavily influence performance gpu kernel code adapt gpu model achieve optimal performance otherwise performance portability ensure furthermore kernel performance sensitive input structure application setting code optimize input characteristic sub optimally autotuning allows code automatically adjust hardware input gpu kernel development programmer define tune parameter code influence application performance tune parameter pre define discrete tune parameter potentially prune priori constraint tune tune defines computational kernel execute tune configuration autotuning framework tune tune configuration minimizes tune objective usually runtime consumption autotuning efficient discrete optimization dimension non convex non linear locality perform tune limit practical usage autotuning happens tune vast configuration perform poorly performance depends significantly input characteristic tune objective function tune parameter multiple tune specific optimization exist leverage performance counter fix optimization tune parameter tune parameter unseen application hardware model concrete optimization model construct application implement optimization contrast generic tune optimization programmer implement model cannot construct generic tune tune parameter mathematical optimization surrogate performance model built sample tune function relate tune parameter tune objective differs hardware input autotuning scratch hardware input essential contribution introduction novel generic tune aforementioned relation tune partially explore hardware input portable model tune input hardware mimic iterative optimization perform developer developer profilers performance counter identify bottleneck overload processor subsystem multiple hardware hierarchy understand code tune parameter related performance counter iteratively detect bottleneck modify code reduce stress bottleneck sufficient code performance aim training phase wherein model capture tune parameter influence performance counter machine analogy developer understand relationship autotuning iteratively profile code acquires performance counter analyzes bottleneck determines performance counter soften bottleneck expert analogy developer profile model tune configuration performance counter finally selects tune configuration profile analogy developer modification code strength ability model gpu input model autotuning kernel gpu processing input model relation tune parameter performance counter performance gain performance counter instead relate tune parameter directly performance distinguish performance counter counter stress processor subsystem counter amount operation processor subsystem assignment nvidia gpu performance counter performance tune parameter affect performance counter straightforward stable illustrate coulomb sum benchmark described relation depict tune parameter kernel runtime significantly input hardware whereas relation tune parameter normalize performance counter remains stable stability relation tune parameter performance counter hardware input model historical tune data tune various benchmark performance  abbreviation gpus counter implement volta generation newer conversion ratio counter counter prior volta counter volta newer abbr dram  sector   dram  sector    sector    sector   tex cache  request pipe lsu mem global   local memory  sector pipe lsu mem local   load  data pipe lsu wavefront mem    data pipe lsu wavefront mem   inst  sas thread inst execute pred   inst  sas thread inst execute pred   inst  sas thread inst execute integer pred   inst  sas thread inst execute misc pred   inst compute  sas thread inst execute memory pred   inst  sas thread inst execute pred   inst  sas thread inst execute conversion pred   inst  inst execute   issue slot  issue active avg pct peak sustain  issue uops dram  throughput avg pct peak sustain elapse    sector avg pct peak sustain   tex  request pipe lsu mem global avg pct peak sustain active tex   data pipe lsu wavefront mem avg pct peak sustain elapse shr   cycle active avg pct peak sustain   warp execution  thread inst execute per inst execute ratio warp  warp  execution  thread inst execute per inst execute  NP  image KB image dependence tune parameter various kernel coulomb summation  geforce gtx  geforce gtx axis tune parameter thread coarsen axis normalize kernel runtime cache transaction texture cache transaction float operation interpretation reader refer web version article evaluation perform benchmark kernel obtain performance counter model historical tune data bias towards faster convergence propose profile searcher systematically outperforms random model gpu input tune propose searcher outperforms basin hop implement kernel tuner regression model implement starchart contribution hardware performance counter navigate tune propose profile searcher agnostic tune parameter tune code tune knowledge generic autotuning searcher performance counter tune convergence arbitrary tune parameter unknown searcher comprehensive evaluation propose profile searcher propose searcher multiple scenario performance portability across various hardware input tune benchmark gpu architecture searcher optimization searcher model searcher propose searcher converges faster optimal configuration integration searcher tune framework propose profile searcher implement within kernel tune toolkit ktt therefore convergence moreover tune framework searcher experimental data freely available community organize manual tune performance counter serf illustrate framework intend automate propose described evaluates propose searcher alternative approach related described conclude outline future manual tune demonstrate manual approach tune hardware performance counter aim understand concept propose automatic searcher described coulomb summation simplify version 3D implementation coulomb summation introduce coulomb summation  potential around compute regular grid grid compute euclidean distance grid vacuum  listing source code simplify coulomb summation kernel input kernel consists  coordinate vector coordinate    grid  grid dimension  contains output potential grid listing image KB image listing coulomb summation kernel implementation gpu thread computes grid ITERATIONS tune parameter grid equation compute code compute bound data load register compute multiple grid thread within warp load code benefit cache locality code  ktt framework ktt tune parameter via preprocessor macro ITERATIONS sake simplicity optimize thread tune code assign tune parameter understand tune parameter ITERATIONS improve data locality register coordinate multiple loop moreover parameter reduces invariant computation however ITERATIONS reduces kernel execute amount thread increase register consumption gpu occupancy usage local memory register spill understand tune parameter relate performance counter developer expertise profile code developer tune parameter hardware performance counter counter highly relevant utilization float FP ITERATIONS reduce FP operation compute utilization texture cache register spill ITERATIONS improve cache locality gpu occupancy ITERATIONS improve amount local memory ITERATIONS utilization memory subsystem decrease bandwidth introduce register spill understand performance counter relate bottleneck developer expertise dimensional tune however mention apply multiple tune parameter kernel runtime performance counter mixed tune parameter ITERATIONS affect performance counter tune kernel geforce gtx grid kernel compute bound therefore utilization FP tune ITERATIONS profile kernel runtime performance counter bottleneck texture cache utilization whereas FP instruction highly load utilization therefore ITERATIONS improve register locality performance improves significantly kernel execute performance counter texture cache utilization FP utilization report gpu occupancy increase occupancy reduce pipeline memory latency ITERATIONS slightly runtime gpu occupancy FP utilization texture cache utilization texture cache utilization lower ITERATIONS however performance improvement therefore kernel tune input data gpu compute grid previously tune implementation ITERATIONS yield runtime occupancy implementation speedup increase occupancy explore ITERATIONS runtime improve occupancy improve already bottleneck texture cache utilization suggests decrease ITERATIONS ITERATIONS runtime increase although occupancy improve implementation limited texture cache report utilization therefore tune ITERATIONS away tune perform rationally expert programmer understands relation tune parameter performance counter relation performance counter bottleneck programmer profile tune code analyzes bottleneck direction tune parameter performance counter automatize entire propose assumption propose profile focus detail implementation profile counter tune parameter tps tune optimize code balance usage processor subsystem code lower stress processor subsystem increase stress another thread coarsen tune mention previous decrease arithmetic operation memory footprint decrease stress float memory subsystem whereas increase register consumption decrease parallelism increase stress latency hiding mechanism another employ memory variable decrease amount global memory access increase memory access decrease stress cache global memory increase stress memory profile counter PCs capture workload hardware subsystem although vendor implement PCs distinguish fundamentally category PCs counter stress processor subsystem counter operation perform amount resource subsystem PCs relative utilization float global memory bandwidth whereas float instruction memory instruction profile counter depends tune parameter TP input gpu hardware formalize relation define function simplicity assume compiler version switch dependence function gpu differs distinguish strongly dependent gpu model input gpu flop ratio utilization float profile kernel becomes memory bound amount float instruction profile counter depends gpu model weakly affected gpu instruction greatly therefore define function approximation independent gpu hardware moreover partial derivative function input significantly input variation tune parameter operation perform processor subsystem independently input thread coarsen decrease float operation ratio although actual amount float operation input approximate suffers imprecision related cache memory subsystem gpu architecture cache threshold cache capacity differs across gpu architecture therefore TP cache increase amount operation cache TP however imprecision within TP cache footprint gpu cache capacity amount float instruction texture cache transaction whereas cache workload tune parameter significantly  gtx observation relationship tps input hardware model PCs essential propose searcher searcher model stable relationship tps gpu input navigate tune accord actually aim mimic developer optimize performance similarly developer profile tune code analyzes bottleneck decides tune parameter suppress capture developer expertise regard tps relate PCs model capture performance counter relate bottleneck expert model describes relation tps remains stable respect gpu hardware input model model component allows searcher understand tune parameter decrease increase stress processor subsystem cannot predict stress subsystem tune tune configuration execute performance counter expert identifies bottleneck deduces subsystem overload combination tune configuration input gpu model relation tps searcher tune configuration decrease stress overload subsystem decrease operation subsystem workflow manual described developer actual utilization texture cache float gpu occupancy counter thread coarsen decrease amount texture memory operation float operation increase parallelism counter summarize formulate assumption empirically computational bottleneck performance counter suppress bottleneck relation TP sufficiently portable across gpus input assumption knowledge performance counter something seriously hinders performance performance counter reflect manner assumption bottleneck reflect therefore operation reduce improve performance aim mirror expert bottleneck analysis available documentation expertise detail evaluate assumption experimentally assumption evaluate assumption experimentally overall assumption mimic developer performance optimization autotuning tune configuration PCs tune kernel amount local memory bottleneck expert bandwidth multiple memory hierarchy desire PCs denote  expert amount local memory evaluate tune configuration PCs desire model ITERATIONS profile configuration tune configuration replace selection tune configuration usually developer random bias likelihood PCs direction sum tps define tune configuration PCs performance counter bottleneck bottleneck  performance counter TP tune configuration architecture training phase component ktt responsible execute tune gathering PCs sample tune model built tune data described component gpu independent raw tune data gpu input dependent model dependent completely TP accord hypothesis independent gpu model input autotuning gpu input ktt component bottleneck analysis analyzes bottleneck distinguish source bottleneck precisely global memory bandwidth bottleneck memory transaction recognize bottleneck memory  computation component determines bottleneck analysis  computation component gpu dependent performance counter extend gpus evolve therefore component explicit performance counter important bottleneck component analyzes bottleneck gpu architecture autotuning whereas compute gpu architecture model built training phase therefore model steer autotuning multiple architecture component configuration computes tune configuration model predict accord TP configuration predict component performs configuration ktt benchmark profile model relation tune parameter performance counter model tps relate PCs model training phase autotuning training phase broken component ktt autotuning sample entire partial tune tps PCs runtimes data model creation component ML model relation tps autotuning model evaluate tune configuration PCs desire  tps described detail regression non linear model model relation tps PCs non linear regression model input tune evaluate profile tune configuration performance counter split tune subspace binary tune parameter tune binary parameter split subspace subspace datapoints training deliberate non binary parameter combination relatively sample subspace despite constraint model profile counter non binary tune parameter interaction influence quadratic apply regression compute coefficient therefore output model profile counter binary tune parameter applicability model prediction compute non binary tune parameter parameter constant memory described parameter binary constant memory zero therefore model capture relation PCs ITERATIONS constant memory decision decision regression classification model structure dataset subset associate decision incrementally developed decision node leaf node decision node attribute tps leaf node decision numerical target PCs topmost decision node corresponds predictor node decision handle categorical numerical data core algorithm building decision ID employ greedy backtracking ID algorithm construct decision regression replace information gain standard deviation reduction decision strategic split affect accuracy heavily decision criterion classification regression regression usually error mse split node sub node generate candidate randomly explore tune training alter node compute mae absolute error RMSE error mae RMSE predict PCs unknown configuration bottleneck detection reaction PCs bottleneck bottleneck  perform expert performance counter analytically bottleneck vector component described component performance counter actually execute kernel computes bottleneck component bottleneck computes performance counter architecture model therefore component PCs currently execute kernel architecture gpu kernel execute architecture gpu model performance counter completely volta generation newer component implement multiple counter implementation limited understand nvidia gpu performance counter improvement counter document detail entire expert replace machine model developed expert nvidia  documentation cuda improve understand counter benchmark available ktt however avoid overfitting expert benchmark benchmark development coulomb sum matrix transposition benchmark evaluation searcher developed bottleneck analysis bottleneck analysis performance counter described abbreviation counter text formula equivalency gpus prior volta volta newer counter apply adjustment  version counter rank whereas version percentage classification counter equivalence performance counter completely document compute accord understand counter assign inst issue quantifies ratio instruction cycle cannot issue instruction instance due synchronization bottleneck vector zero bottleneck component stress component theoretical peak performance bottleneck category stress various memory instruction utilization gpu parallelism memory subsystem bottleneck analysis compute global memory memory cache utilization memory report performance counter dram ratio transaction equation computation bottleneck global memory equation computation bottleneck global memory memory cache computation analogous shr LT shr WT shr RT WT respectively compute utilization texture data cache counter tex interval cache therefore transaction situation complicate local memory knowledge loc performance counter relative local memory data transfer therefore local memory overhead imply local memory bottleneck memory subsystem overload consequently local memory overhead maximal utilization memory global texture equation performance counter utilization instruction however counter reliable fully understand matrix transposition float computation report utilization FP configuration therefore derive utilization overall amount instruction compute equation computes instruction inst exe computes warp therefore efficiency instruction execution thread warp perform useful compute utilization issue slot gpus prior volta volta newer gpus issue integer float instruction separately compute therefore component considers utilization instruction perfect utilization cannot optimize utilization dual issue implementation utilization FP compute equation analogous computation perform inst inst int inst misc inst  inst cont inst  finally bottleneck insufficient instruction issue utilization compute compute maximal utilization across instruction bottleneck instruction issue compute equation insufficient parallelism compute additional bottleneck insufficient parallelism compute cuda thread cuda core equation empirical computation thread per cuda core parallelism bottleneck zero however useful bottleneck alongside capture SMs occupy actual occupancy due thread per SM  computation bottleneck vector compute component responsible compute PCs vector vector contains performance counter negative counter decrease positive counter increase zero additional parameter computation inst reaction determines threshold instruction related bottleneck trigger motivation introduction parameter instruction latency memory subsystem therefore serious bottleneck unless stress component perform instruction significant bottleneck kernel bottleneck related instruction ignore memory optimize decrease latency introduce bottleneck instruction memory related implementation inst reaction user instruction bound optimize inst reaction slightly improves compute bound reaction instruction related bottleneck faster PCs related memory subsystem compute straightforwardly inverse correspond bottleneck instruction related PCs correspond bottleneck exceeds equation analogous computation perform instruction related bottleneck parallelism related bottleneck apply straightforwardly without invert bottleneck hardware performance counter thread report ktt performance counter configuration configuration empirically profile autotuning  PCs soften bottleneck configuration unexplored tune configuration estimate configuration PCs direction define autotuning execute gpu input cannot directly estimate PCs PCs instead model described estimate PCs configuration PCs estimate model PCs estimate model configuration compute equation profile counter non zero prediction profile candidate configuration performance counter compute configuration respectively configuration normalize interval normalize bias probability configuration equation ensures preference positive amplify normalization however allows selection negative albeit probability normalization cutoff threshold define implementation normalize negative similarly positive non zero probability negative important situation unexplored configuration indicates tune optimum stuck local optimum due inaccuracy model expert decision implementation random random selection bias towards configuration soften bottleneck within profile configuration straightforward approach allows comparison unweighted random however sophisticated implement future workflow described algorithm algorithm variable input TS tune array tune configuration model relation tps PCs perform without performance counter iteration performance counter empirical cuda kernel execute faster performance counter therefore execute without performance counter kernel quickly although perform empirical default implementation algorithm image KB image algorithm tune performance counter algorithm component empirical evaluation kernel perform runtime PCs bottleneck bottleneck detection searcher computes reaction bottleneck reaction performance counter implementation algorithm tune configuration accord performance counter model tune configuration performance counter direction tune algorithm selects kernel benchmark without gathering performance counter loop algorithm selects configuration probability obtain benchmarking configuration perform configuration profile perform implementation propose profile searcher implement python version ktt implement stub searcher code ktt communicates searcher via file socket python implementation easy searcher code easy machine algorithm nevertheless implementation disadvantage code performance rewrite searcher code future searcher consists application building model plugin ktt implement propose searcher searcher available profile searcher directory ktt limitation implementation although propose profile searcher fully capable tune biasing towards faster implementation research topic improvement searcher local currently implementation bias random searcher although random searcher robust global cannot convergence gradient optimize function searcher allows estimate configuration improve performance estimation estimation gradient performance function gradient local successful combination global allows overcome local optimum improve model tps PCs relation currently relation tps PCs model gpu model construct although sufficient steer tune cannot precisely model cache capacity model cache related PCs accord detect cache capability gpu autotuning improve bottleneck analysis reaction expert bottleneck analysis computation PCs subset PCs available gpus PCs related execution stall although easy construct expert interpret PCs replace expert machine model overcome limited understand PCs interpretation construct model multiple benchmark gpus available moreover benchmark consists tune configuration therefore generate volume training data runtime kernel profile amount hardware performance counter improve bottleneck analysis reaction counter mostly redundant reduce amount similarly evaluation evaluate propose profile searcher methodology benchmark testbed setup evaluate random biasing influence searcher performance model setup gpu input evaluate convergence propose finally model tune starchart optimization tune kernel tuner methodology perform evaluation empirical kernel searcher tune convergence empirical kernel compile execute tune configuration empirical allows directly efficiency propose profile empirical perform converge efficiency kernel gpu kernel profile PCs therefore reduce empirical ensure faster tune convergence propose searcher therefore tune convergence evaluation aim perform configuration define perform configuration configuration creates kernel runtime within kernel runtime exhaustive tune stochastic random fluctuation incredibly demand actually profile kernel autotuning program bypassing instead kernel performs exhaustive exploration entire tune tune kernel runtimes PCs perform autotuning faster simply load kernel runtimes PCs file searcher instead actually kernel profile allows tune combination benchmark hardware model hardware autotuning simulated autotuning evaluate searcher evaluate tune convergence perform empirical actually profile kernel therefore execute due demand testbed setup searcher propose benchmark benchmark introduce detail implementation optimization rewrite benchmark cuda profile nvidia gpus remove tune parameter cuda built vector variable explicit cache prefetching implement cuda ktt backend constant memory however benchmark peak performance gpus similarly therefore tune sufficient evaluation tune searcher implement version tune gemm reduce  denote gemm  denote gemm gemm allows explore entire tune reasonable practical benchmark dimensionality tune parameter tune  convolution coulomb 3D gemm gemm transpose expert programmer tune reasonably without obviously configuration sub warp vast amount poorly perform tune configuration therefore discriminate random benchmark execute gpus architecture benchmark execute kernel tune toolkit equip profile searcher gpu device benchmark  geforce gtx kepler geforce gtx maxwell geforce gtx pascal geforce RTX turing speedup performance counter biasing improvement biasing random accord PCs compute bottleneck analysis reaction subsystem experimentally evaluate assumption mention eliminate model imprecision therefore perform exhaustive exploration PCs benchmark tune autotuning previously PCs tune configuration instead predict model average empirical random perform configuration improvement propose searcher random improvement average empirical perform configuration average obtain searcher average empirical random perform configuration gtx   RTX coulomb sum matrix trans gemm convolution improvement empirical propose searcher random PCs architecture gtx   RTX coulomb sum matrix trans gemm convolution propose searcher improves empirical significant improvement gemm convolution benchmark tune complicate improvement bias random selection visible portability across hardware evaluate portability model model specific hardware input bias tune hardware therefore experimentally assumption model decision described similarly improvement random iteration empirical perform configuration average execution performance portability coulomb sum matrix transposition gemm convolution benchmark speedup propose searcher combination gpu model gpu autotuning improvement empirical propose searcher random gpus benchmark execution gpus model tps PCs relation model portability model built gpu model gpu autotuning gemm benchmark execute RTX converges faster model gtx data however somewhat random artefact inaccuracy model compensate inaccuracy expert biasing decision prediction instead reading PCs gpu model building autotuning therefore diagonal within gemm benchmark execute RTX speedup PCs RTX whereas decision model gpu limit speedup portability across input investigate performance portability input gemm benchmark significantly input matrix multiplication matrix typical matrix multiplication compute bound gpus multiplication matrix matrix utilize entire gpu easily code latency bound multiplication highly rectangular matrix matrix matrix matrix matrix matrix multiplication memory bound arithmetic intensity perform gtx decision model built gtx input input typically slightly decrease speedup however reduction empirical significant autotuning compute bound multiplication matrix improve model memory bound gemm instance improvement highly rectangular matrix tune searcher improvement empirical propose searcher random gemm benchmark benchmark execution model TP PC relation tune convergence propose profile searcher convergence random motivation comparison random converge faster despite empirical random advantage profile searcher performance counter performance counter hinders kernel execution minimal computational overhead model evaluate propose searcher navigates towards faster implementation therefore omits evaluation configuration whereas collection performance counter kernel kernel runtime dominates kernel compilation data copying selection configuration overhead searcher significant vast tune setup convergence searcher machine equip nvidia geforce RTX intel core GB ram ubuntu nvidia driver cuda benchmark execute model data obtain geforce gtx therefore simulates situation user acquires brand gpu autotuning data gpu perform autotuning persistent data gpu comparison reference computation typical dynamic autotuning improves random searcher overhead data movement comparison hide execution kernel gathering performance counter kernel input gpu fully occupy kernel longer millisecond suitable kernel runtime choice improves propose searcher profile kernel switch comparison reference kernel kernel input demonstrate described evaluation searcher convergence graph average kernel runtime autotuning kernel iteration benchmark kernel execution gathering performance counter graph kernel therefore prevents propose searcher artificially improve plot runtime kernel measurement gemm convolution benchmark  therefore improve convergence important benchmark gemm benchmark matrix convolution benchmark 2D array comparison convergence respectively propose searcher brings significant convergence speedup image KB image convergence gemm gtx model gtx solid average transparent standard deviation image KB image convergence convolution gtx model gtx solid average transparent standard deviation matrix transposition benchmark matrix although propose searcher empirical convergence significantly faster random searcher another scenario tune reference implementation offline tune matrix host memory checked reference constant overhead empirical overhead gathering performance counter visible propose searcher converges significantly faster image KB image convergence matrix transposition gtx model gtx tune configure kernel tune configure kernel solid average transparent standard deviation benchmark although overall searcher iteration random propose searcher converges significantly faster situation tune execute instance setup kernel longer profile overhead propose searcher random searcher image KB image convergence gtx model gtx tune tune solid average transparent standard deviation coulomb sum benchmark grid converges quickly random searcher propose searcher initial profile converges quickly whereas random searcher performance tune however searcher converge optimum therefore choice searcher important image KB image convergence coulomb sum grid gtx model gtx solid average transparent standard deviation setup allows benchmark gemm benchmark perform exhaustive gemm model tune gemm benchmark geforce gtx gemm benchmark tune gemm benchmark completely lack tune parameter gemm configuration however propose searcher converges faster random searcher gemm benchmark propose searcher random searcher improve optimize implementation propose searcher gemm tune significant tune configuration longer per empirical image KB image convergence gemm gtx model gemm gtx solid average transparent standard deviation comparison basin hop propose profile searcher random however basin hop optimization recently perform optimization therefore searcher setting described basin hop searcher implement kernel tuner setup machine ktt kernel tuner implement ktt kernel tuner  basin hop optimization default setting kernel tuner ktt random searcher suppose source twofold kernel tuner implement python whereas ktt native application kernel tuner executes kernel timing precision although ktt executes kernel consistent execute multiple ktt timing stable performance report kernel tuner suppose overhead python kernel tuner implementation measurement kernel tuner systematically report slightly kernel runtimes ktt therefore normalize performance comparable exhaustive tune report kernel tuner ratio ktt kernel tuner therefore tune converge kernel comparison timing affected execution kernel tuner searcher allows deduce basin hop navigates tune random propose performance counter without  kernel tuner execution comparison propose searcher implement ktt basin hop implement kernel tuner coulomb sum benchmark convergence propose searcher converges faster slightly outperform basin hop empirical propose searcher behaves similarly basin hop image KB image convergence coulomb sum benchmark ktt kernel tuner convergence comparison iteration empirical solid average transparent standard deviation gemm benchmark basin hop optimization implement kernel tuner converges slowly random searcher outperforms random slightly however basin hop related execution kernel tuner empirical basin hop converges optimum faster random searcher iteration propose profile searcher significantly empirical random basin hop outperforms empirical convergence image KB image convergence gemm benchmark ktt kernel tuner convergence comparison iteration empirical solid average transparent standard deviation matrix transposition benchmark slowdown kernel tuner visible kernel tuner tune probably complicate constraint prune tune initialization converges quickly cannot outperform random propose searcher empirical basin hop converges optimum faster random searcher however propose profile searcher significantly empirical image KB image convergence matrix transposition benchmark ktt kernel tuner convergence comparison iteration empirical solid average transparent standard deviation benchmark basin hop converge convergence empirical performs random searcher propose searcher image KB image convergence benchmark ktt kernel tuner convergence comparison iteration empirical solid average transparent standard deviation similarly basin hop converges slowly convolution benchmark empirical similarly random searcher however converges tune image KB image convergence convolution benchmark ktt kernel tuner convergence comparison iteration empirical solid average transparent standard deviation matrix transposition convolution benchmark reduce tune significantly pre define constraint tune pre define tune parameter prune constraint configuration remain matrix transposition configuration remain convolution speculate significant delay tune matrix transposition convolution contrast configuration remain constraint application coulomb sum gemm although issue optimize kernel tuner code comparison empirical basin hop empirical propose searcher comparison regression approach regression employ starchart regression predict outcome variable usually performance consume tune parameter predict outcome entire configuration predict outcome setup evaluation starchart author training datapoints iteratively prediction accuracy median relative prediction error maximum training datapoints prediction accuracy evaluate validation datapoints training validation datapoints uniform random sample worth tune evaluate starchart entire instead meaningful instead evaluate accuracy approach randomly tune configuration perform training additional tune configuration median relative prediction error configuration sort predict kernel examine actually protocol tune calculate prediction sort examine configuration actually perform within kernel evaluate benchmark coulomb gemm reduce conv  nbody geforce RTX starchart tune toolkit cannot directly tune therefore empirical perform comparison starchart random searcher geforce gtx geforce RTX tune model perform perform configuration random searcher model phase starchart performs random searcher convolution benchmark geforce RTX tune vast amount poorly perform tune configuration cannot easily tune surrogate model tune autotuning starchart random searcher geforce gtx geforce RTX empirical tune model model training tune tune perform configuration random random searcher although starchart ensure model portability model gpu perform regression gpu performance portability perform configuration configuration perform gpus regression gpu successfully regression geforce gtx navigate geforce RTX starchart searcher model geforce gtx navigate tune geforce RTX comparison scenario starchart compete propose searcher regression tune perform configuration gpu model perform gpu autotuning however model restriction propose searcher robust tune starchart gemm geforce gtx RTX convolution geforce gtx autotuning starchart propose searcher geforce RTX model geforce gtx empirical tune SC propose coulomb sum matrix trans gemm convolution related tune category model tune mathematical optimization empirically configuration model performance model configuration tune directly hybrid combine empirical model approach tune historical profile data performance counter code optimization model model optimization majority code optimization parameter autotuning framework tune continuous boolean tune variable non linear non convex tune challenge therefore random reliable sophisticated author improvement random   successfully adapt non continuous tune recently combination global local potential outperform random systematically difference model optimization optimization execute scratch hardware performance relevant input parameter whereas model portable across gpus input searcher confirm basin hop couple local superior random searcher outperforms basin hop empirical convergence model model model evaluate faster empirical therefore simulator gpgpu sim practical purpose multiple model analysis performance prediction cuda OpenCL code developed however construction model manual effort impractical autotuning moreover independent reveal inaccuracy ability predict performance questionable tune configuration vast tune model promising tune parameter model replace empirical tune perform thread cuda terminology thread coarsen factor code variant focus optimization tune parameter hybrid hybrid introduce surrogate model intend tune configuration directly prune bias empirical machine model built sample tune tune configuration predict performance empirically author outperform random regression autotuning multiple regression built representative sample tune precision improve evaluate approach vast tune integer thread interval instead rational therefore regression built tune rationally construct tune portion tune construction experimentally model hardware performance relevant input characteristic practical portion tune training contrast approach allows model hardware input historical data historical autotuning data obtain hardware improve autotuning convergence author predict code variant kernel unseen gpu data obtain explore gpus difference focus multi dimensional tune gpu sufficient empirical exploration model historical data cpu architecture bias multi dimensional cpu architecture approach implementation architecture correlate approach correlation model automatically selects tune configuration specific application input input generalizes tune configuration input model empirical input model multiple input gpus previous data hardware prune dimension hardware approach exhaustive brings advantage couple searcher mathematical optimization performance counter typical usage hardware performance counter profile kernel manual inspection bottleneck however performance counter navigate optimization automatically performance counter approximate performance implementation unseen gpus contrast focus navigate tune implementation maximize performance author performance counter detect relevant feature code variant selection model model multiple code variant alternative functionally equivalent implementation multiple gpus predicts code variant unseen gpu optimize dimensional tune multiple gpu architecture training whereas approach allows dimensional tune training gpu cpu performance counter perform combination openmp thread schedule strategy efficient schedule perform combination compiler optimization switch perform combination activate deactivate prefetchers perform setting configuration xeon phi difference approach tune user define optimization transformation source code whereas  developed perform specific optimization openmp schedule strategy thread fix optimization input machine algorithm vector tune parameter performance counter runtimes obtain multiple application processor various input output predict runtime predict tune parameter allows tune application tune parameter training cannot construct therefore combine machine model tune application separately expert navigate tune accord predict instead predict runtimes tune parameter conclusion future introduce novel tune searcher hardware performance counter propose profile searcher model relation tune parameter performance counter gpu input model navigate unseen gpu input experimentally performance counter significantly reduce empirical perform autotuning convergence propose searcher typically superior random searcher optimization basin hop searcher implement kernel tuner propose searcher implement kernel tune toolkit therefore offline dynamic autotuning although searcher cuda enable gpus searcher developed gpus vendor processor architecture CPUs future improve searcher performance component propose profile searcher replace sophisticated implementation potentially gradient local predict performance counter hardware autotuning replace bottleneck analysis reaction machine model apart improve propose searcher investigate possibility leverage hardware performance counter extend predict tune actual configuration kernel prediction allows tune predict performance gain autotuning important dynamic autotuning tune limited vast amount tune data behavior efficiency HW architecture code optimization strategy source code kernel tune toolkit propose profile searcher profile data conduct available community therefore easy replicate searcher benchmark modify extend searcher implementation