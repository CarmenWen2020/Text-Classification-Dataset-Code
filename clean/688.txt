recent trend performance compute proliferation neural network training however frequent communication requirement computation node drastically overall training bottleneck distribute training particularly cluster limited network bandwidth mitigate drawback distribute communication researcher propose various optimization strategy comprehensive survey communication strategy algorithm viewpoint computer network perspective algorithm optimization focus reduce communication volume distribute training network optimization focus accelerate communication distribute device algorithm reduce communication transmit per addition elucidate overlap computation communication network discus network infrastructure logical communication scheme network protocol finally extrapolate potential future challenge research direction accelerate communication distribute neural network training previous keywords distribute communication optimization parallel algorithm network infrastructure introduction currently unprecedented historic era research neural network swept application domain autonomous computer vision processing recommendation popularity promote development dnn architecture fully neural network FCNN convolutional neural network cnn recurrent neural network rnn variant lstm gru neural network achieve performance across various domain specific task computer vision dnns googlenet resnet imagenet dataset beaten image classification task regard performance dnns tend deeper sophisticated datasets rapid increase data volume model computation dnn training consume extend performance hardware graphic processing gpu tensor processing tpu apply accelerate training beyond performance hardware parallel deploy dnn training task multiple node consist machine another practical approach node executes entire computation task however due frequent communication requirement exchange amount data computation node communication overhead critical  distribute training growth cluster communication overhead increase explosively phenomenon considerably diminishes advantage parallel training majority training spent transfer data performance hardware accelerator proportion spent communication increase decrease computation overhead whereas communication overhead unchanged primarily investigate communication overhead distribute dnn training distribute disciplinary distribute network community propose communication optimization strategy perspective survey classify communication optimization distribute dnn training research community contribute overview optimization strategy image KB image overview communication optimization strategy optimization strategy category communication reduction schedule algorithm refer algorithm network traffic execution refer network previous research domain  detailed  analysis dnns various training algorithm training framework recent review discus challenge manage distribute infrastructure report optimization technique application gpus architecture aspect survey communication optimization issue distribute broader investigation discus optimization algorithm network perspective previous survey organize introduce background overview distribute dnn training discus optimization algorithm network respectively discus communication reduction gradient compression computation communication overlap whereas introduces logical communication architecture network protocol finally conclude entire highlight potential challenge research direction background brief introduction distribute neural network training currently popular dnns machine remains mini batch stochastic gradient descent sgd algorithm error propagation dnn training parallelization training task parallelize architecture computation node synchronize gradient stochastic gradient descent sgd variant become workhorse algorithm training dnn model suppose dnn model minimize loss function average dataset dnn output parameter input data instance correspond label application specific loss function difference output label sgd update model parameter iteratively converge towards minimum loss function training instance mini batch rate gradient mini batch data model parallelism data parallelism model parallelism commonly training distribute data parallelism entire training dataset randomly equally dispatch node node maintains model replica along local parameter training node mini batch training data executes backward propagation calculate local gradient node sends local gradient node gradient node aggregate gradient update model broadcast model parameter node model converges respect model parallelism dnn model split node hence parameter node reduce node input layer responsible reading entire dataset node node input dataset correspondingly node output layer responsible output predict dnn calculation node longer independent neuron connection computation node exchange gradient model parameter clearly data parallelism suitable training dataset model parallelism applicable model node primarily focus data parallelism centralize decentralize architecture logical architecture computation node  communication mode network performance parameter server popular centralize architecture distribute parameter server usually server node worker node server maintains global model parameter whereas worker local model replica server node machine machine maintains partition entire model parameter worker worker entire model replica data parallelism model model parallelism worker communicate server via operation whereas communication worker drawback parameter server bandwidth bottleneck server increment worker due drawback parameter server decentralize architecture attract considerable research attention incur communication traffic issue parameter server node decentralize architecture entire model replica collective communication operation allreduce instead exchange gradient parameter nevertheless allreduce operation variety implementation notable difference performance affect communication overhead discus related issue synchronous asynchronous update owe difference network bandwidth compute node calculate gradient faster node challenge circumstance synchronize gradient multiple computation node reasonably synchronous asynchronous bound delay update synchronous synchronous update server update model receives gradient worker iteration faster worker worker implementation synchronous update bulk synchronous parallel BSP characteristic synchronous mode server gradient node affect model convergence however node idle node waste resource straggler slows overall training asynchronous asynchronous algorithm hogwild overcome asynchronous update worker worker worker local gradient server others calculate gradient worker calculates local gradient whereas worker gradient although parameter server timestamp primary challenge asynchronous update data staleness worker stale parameter  model convergence addition worker equivalent update local parameter sub dataset local model deviate global model overcome drawback asynchronous update researcher attempt limit staleness parameter bound stale update worker stale parameter staleness delay limited limitation staleness mitigates straggler extent increase training throughput however limitation staleness worth excessively completely asynchronous update synchronous update image KB image asynchronous update node server node correspond worker instance timestamp arrow data transmission direction model gradient interpretation reference legend reader refer web version article adapt algorithm optimization demonstrate reduce communication overhead distribute dnn training algorithm perspective algorithm optimization reduce communication volume increase computation communication overlap ratio optimization compatible underlie network various network infrastructure  communication sgd neural network entire training usually consists multiple epoch iteration regard parallelization node exchange data iteration intuitive communication overhead reduce data exchange communication node related batch communication batch extend communication reduce data exchange batch training batch significant hyperparameter amount data node iteration batch usually approximates distribution input data introduces variance gradient estimate batch additionally batch data longer incur model parameter update primarily due relationship batch iteration training data equation batch reduction iteration hence parameter update infrequent distribute training circumstance data parallelism scheme batch sum local batch node recall conventional distribute node exchange gradient model parameter iteration gradient model parameter dnn iteration communicate message remains constant batch therefore increase batch reduces iteration communication resnet fix epoch batch machine entire training iteration contrast batch machine iteration resnet training configuration nevertheless directly deploy parallel sgd batch likely suffer generalization ability degradation training batch illustrates situation batch exceeds denote sample error validation increase dramatically batch increase batch tend converge minimum training function minimum characterize significant positive eigenvalue hessian matrix curvature around minimum minimum generalize conversely batch converge minimum characterize eigenvalue hessian matrix curvature observation loss function landscape dnn batch attract minimum unable escape resnet training detail    tesla baseline   min  tensor core min  min  min  min  min  min  min  min propose prevent model converge minimum rate various scheme layerwise adaptive rate lars dictate rate increase prevent loss accuracy increase batch increase rate sqrt linear batch multiplies rate variance gradient estimator constant multiplies rate assumption introduce linear rate usually model converge hence training epoch slowly smoothly increase rate linear scheme gradual deploy rnn lstm gru training propose linear epoch gradual  scheme epoch increase batch lars applies rate layer dnn distribution gradient parameter varies layer however lars performs poorly attention model performance gain consistent across task therefore propose layerwise adaptive batch optimization technique  performs across various task bert resnet training minimal hyperparameter tune rnn training propose blockwise model update filter  lstm gpus achieve linear speedup image KB image conceptual sketch minimum axis indicates loss function axis variable parameter adapt periodic communication recall training dnns traditional distribute sgd communication occurs iteration suppose parallel dnn training task node iteration complexity vanilla parallel sgd due complexity communication research reduce frequency exchange gradient parameter model average individual model parameter local node average periodically average operation iteration factor average occurs iteration identical vanilla parallel sgd conversely average occurs training shot average another depicts experimental verify model average reduce communication overhead training suitable addition theoretical analyze model average achieve convergence rate comparison periodic average sgd  linear speedup vanilla parallel sgd shot avg PR sgd local sgd RI sgd  sgd  sgd image KB image comparison vanilla parallel sgd local sgd shot average indicates computation corresponds communication interpretation reference legend reader refer web version article shot average communication training extreme model average shot average satisfy performance convex nonconvex optimization however report nonconvex optimization cannot shot average remedy researcher frequent average improve performance shot average vanilla parallel sgd tremendous focus periodical average communication occurs iteration another aspect research attempt reduce communication redundancy training dataset instance amount redundancy cod theoretic linear regression communication additionally demonstrate properly infuse redundancy training data model average conceivable significantly reduce communication PR sgd accuracy degradation nonconvex optimization advantage model average examine practical specifically finding model average performs empirically reduce communication accuracy theoretical bound communication setting distribute convex optimization addition model average asymmetrical operation increase communication propose feasible asymmetrical worker request update parameter server gradient server however experimental asymmetrical convergence analysis developed lazily aggregate gradient lag worker server synchronize gradient parameter violate generate export estimation objective descent iteration lag convergence guarantee experimentally theoretically worker linear regression model lag decrease parameter server communication complexity vanilla parallel sgd image KB image compression scheme layer gradient matrix others output various compression scheme input negative positive interpretation reference legend reader refer web version article adapt gradient compression computation node communication operation exchange gradient model parameter distribute training variable gradient model parameter commonly configuration gradient model parameter communication bottleneck exchange amount precision variable impairs advantage parallel  training bert architecture neural processing task approximately parameter implies precision exchange node GB therefore discus reduce message gradient compression reduces communication traffic compress gradient transmit training easy implement performs dnn model however model performance affected lossy compression unacceptable recommendation accuracy sensitive currently mainly bandwidth efficient compression approach quantization sparsification decomposition substitute variable precision variable whereas transport important variable avoid unnecessary overhead approach orthogonal combine compress communication popular transmits matrix instead matrix matrix decomposition various compression quantization precision data representation quantization discretizes continuous integer standard quantization gradient sgd pioneer research communication quantization distribute recently algorithm signSGD propose nonconvex optimization difference gradient quantization gradient quantize gradient bias estimate gradient model converge slowly significant accuracy loss address issue error feedback technique deviation direction accumulate previous iteration error feedback maintains vector accumulate difference quantize gradient equation rationale error feedback subscript iteration decay factor  error feedback sgd achieves speedup training dnn accuracy loss fix signSGD error feedback technique perform theoretical analysis convergence nonconvex smooth function although error feedback prevent model significant accuracy loss gradient aggressive  bias cope bias gradient avoid hence unbiased propose introduce randomness quantization operator meaning quantize probability distribution function deterministic input random vector quantization stochastic  unbiased quantization bound variance function due extra randomness introduce stochastic quantization  unbiased estimate gradient variance bound ensures convergence vanilla sgd parallel   stochastic unbiased gradient gradient  compress function maximum norm vector random binary vector bernoulli distribution proven quantize gradient ternary function unbiased estimate gradient  sophisticated quantization function hyperparameter representation precision communication overhead information loss whereas information loss communication overhead bidirectional quantization parameter server architecture reduce worker server communication direction  applies majority vote server enable quantization  offloads model update operation worker compress gradient transmit worker server contrast   quantizes gradient model parameter worker server server worker direction respectively reduce communication traffic apply efficient lossless cod technique elia cod quantization sparsification limitation quantization compress communication message however sparsification limitation transmits essential role model update zero gradient associate parameter rarely update meaning gradient parameter update therefore waste bandwidth transmit gradient contains zero clip gradient via static threshold transmit absolute static threshold however easy reasonable threshold various dnns remedy issue subsequent apply local selection dynamic threshold instead static threshold variant threshold sparsification node transmits absolute theoretical convergence analysis sparsification assumption sparsification equivalent stale update ensure model convergence usually gradient residual manipulate gradient directly gradient residual sum previous gradient accumulate locally node accumulate gradient sparsified sgd convergence rate vanilla parallel sgd gradient compression  introduce momentum correction local gradient clip momentum factor mask training achieve performance  compress gradient resnet MB MB without accuracy loss propose random sparsification index gradient randomly guarantee sparsified vector unbiased remain appropriately amplify random accuracy loss layer cnns cifar datasets quantization sparsification orthogonal integrate compression integration straightforward centralize architecture parameter server however challenge resolve sparsification decentralize recall sparsification transmits important gradient node nonzero index dimension gradient fortunately series implementation sparse communication sparse collective communication protocol implement sparse gradient exchange decentralize architecture combine quantization matrix decomposition compression emerge decompose gradient matrix matrix transmission reconstruct transmit matrix communication overhead transmit matrix feasible correlation gradient  exploit linear correlation cnn gradient apply principal component analysis pca reduce gradient dimension addition propose  enables aggregation compress gradient  compression built singular decomposition svd gradient  random unbiased gradient minimal variance  performs rank decomposition error feedback avoids computationally expensive svd  achieve scalability computation communication overlap gradient generate layer layer propagation calculation previous layer gradient later layer former layer computation independent latter layer communication latter layer parameter update independent former layer hence transmit gradient layer compute layer gradient computation communication overlap really reduce communication performs computation communication simultaneously therefore approach combine easily optimization strategy image KB image parameter server architecture  backward propagation  schedule algorithm  layer communication gradient calculate backward propagation however layer computation communication  outperform fifo schedule specific network model  ideal easily hide communication latency bandwidth network environment message merge message merge gradient communicate easily hidden computation newly propose merge gradient  MG  achieves efficiency  priority parameter propagation  priority schedule layer obtains priority layer obtains priority tensor priority communication phase regardless generate tensor partition technique handle tensor partition technique split layer parameter matrix assigns priority slice layer processing propagation exploit execution computational graph propose heuristic schedule algorithm timing independent communication tic timing aware communication tac algorithm built communication operation communication dependency communication directly dependent compute load etc  applies tensor partition priority schedule credit preemption approach fully utilize network bandwidth credit preemption slide credit tensor slide simultaneously  bayesian optimization ideal credit partition network optimization mainly concentrate optimize network infrastructure advanced centralize decentralize architecture message library network protocol benefit network optimization intuitive modify communication protocol impact training algorithm however network optimization maybe easy implement performance depends distribute training logical architecture centralize decentralize architecture  performance discus advanced architecture subsection parameter server depicts traditional parameter server architecture server responsible update global model parameter server prone network bottleneck node parameter server alleviate bottleneck extent treat server span worker worker leaf node span whereas node server worker gradient aggregate gradient upstream towards aggregation perform global  server leaf worker span communication overhead operation reduce propose architecture alleviate network traffic server broadcast global directly construct within worker physical distance parameter server employ intelligent communication mechanism network wan efficiently utilize bandwidth addition project adam  improve throughput parameter server cache isolated communication image KB image allreduce algorithm correspond reduce scatter correspond allgather adapt image KB image 2D mesh allreduce across hypothetical torus tensor sum along vertical dimension tensor sum concurrently along horizontal dimension dimension flip interpretation reference legend reader refer web version article adapt allreduce issue traditional communication strategy gpus increase communication increase linearly classical implementation allreduce combination reduce operation broadcast sends implies bottleneck optimize algorithm principle recursive vector halve recursive vector recursive distance halve recursive distance binary knowledge baidu introduce allreduce distribute allreduce phase reduce scatter allgather phase communication gpus gpu maintains local gradient equally chunk reduce scatter phase node sends receives chunk tensor chunk node correspond buffer node global phase node sends global maintain receives global node node global hence allreduce communication communication described allreduce optimal bandwidth allreduce algorithm propose 2D torus allreduce topology gpus 2D grid contains gpus contains gpus phase 2D torus allreduce reduce scatter vertical allreduce allgather although 2D torus allreduce phase allreduce overall communication overhead aggregate gradient phase 2D mesh topology utilizes parallel reduction sum payload along horizontal vertical dimension 2D mesh algorithm twice throughput gradient aggregation 1D allreduce propose hierarchical allreduce resolve tensor communication researcher split gpus conduct phase reduction phase operation phase allreduce operation independent consists gpus phase node operates allreduce global finally node broadcast global gpu allreduce phase hierarchical allreduce decrease image KB image phase hierarchical allreduce orange entry local node interpretation reference legend reader refer web version article message library performance parameter server various allreduce algorithm partially depends implementation message communication library parameter server usually  grpc  performance latency asynchronous message library multiple communication grpc performance remote procedure rpc framework developed google commonly message communication library parameter server  grpc message communication library implement various allreduce algorithm collective communication algorithm efficiently mpi  NCCL baidu allreduce aluminum  thanks performance mpi optimization mpi allreduce horovod mxnet mpi tensorflow mpi reduce communication horovod tensor fusion sends tensor simultaneously NCCL  gpu  collective communication primitive optimize nvidia gpus series optimize  operation NCCL cuda aware mpi respectively  decomposes allreduce operation series parallelizable reduce scatter allgather operation reduce communication performance  incurs communication overhead  baidu allreduce gpus network protocol traditional message communication library implement tcp IP protocol handle data socket node socket establish connection receiver data data operating encapsulate protocol header network interface controller NIC buffer operation wasteful distribute training network latency therefore performance latency network associate network hardware infiniband attract research attention remote memory access RDMA internet protocol infiniband  illustrate RDMA allows machine directly memory another machine without involve operating enables performance latency networking network interface RDMA communication paradigm message memory  implies encapsulates IP datagrams infiniband enables tcp IP application infiniband without code modification however  cannot bypass host operating RDMA advent RDMA  tremendous improve performance distribute training mxnet tensorflow CNTK caffe ibm platform leverage bandwidth latency memory communication paradigm popular due memory demand communication gpus explore gpu RDMA GDR allows RDMA NIC access gpu memory directly without host memory gradient aggregate gpus GDR technique furthermore adaptive RDMA grpc dynamically adjust communication mechanism message workload accord experimental RDMA  replace tcp IP protocol significantly accelerate training addition RDMA performs  distribute training report  RDMA capable network achieves linear speedup inception training gpus previous description  RDMA recently  distribute training dnns bound loss tolerance transmission protocol ignores packet loss training protocol achieves shorter completion traditional tcp IP protocol however random rate tune dnn architecture challenge reasonable rate various dnns conclusion communication overhead significant obstacle achieve desirable performance distribute dnn training comprehensive survey recent research communication optimization technique distribute dnn training theoretical experimental investigate communication optimization strategy dimension algorithm optimization network optimization algorithm perspective elaborate technique reduce communication volume computation communication overlap network optimization impact topology network protocol distribute highlight potential research direction challenge focus task neural network model considerable communication optimization focus image classification task resnet imagenet dataset processing recommendation related task notable research attention embed matrix recommendation model become bottleneck communication optimization local sgd nonconvex periodic communication remains research opportunity nonconvex despite theoretical model average research linear speedup preserve nonconvex optimization unexplored individually bound communication nonconvex optimization achieve linear speedup research direction model accuracy compression ratio core challenge consideration gradient compression model accuracy  ratio conventional approach prevent model diverge error feedback quantization local gradient accumulation sparsification advanced error feedback explore future computation communication overlap ratio ratio computation communication essential deploy pipeline training sake overlap rating  strategy communication operation shrink communication however algorithm generally heuristic achieve  schedule optimization algorithm dynamic program maybe suitable dnn training datacenter network topology physical topology datacenter network significant impact distribute recent deployed parameter server  instead traditional architecture achieve performance lenet vgg training network topology  accelerate neural network training communication overhead analysis another critical research issue performance model measurement distribute dnn training performance  theoretically analyze various distribute training measurement communication behavior bottleneck distribute training task although framework tensorflow mxnet currently  analysis cannot analyze network behavior advanced network analysis horovod timeline 