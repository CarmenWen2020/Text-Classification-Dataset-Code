graph tensor operation extend tensor operation graph structure account apply diverse domain image processing machine however graph tensor operation increase rapidly node dimension data node impractical application propose gpu library cuGraph tensor performance graph tensor operation consists operation graph shift shift graph fourier transform FT inverse graph fourier transform inverse FT graph filter filter graph convolution convolution graph tensor graph tensor svd svd graph tensor QR QR cuGraph tensor scalar vector matrix data processing graph node propose optimization technique compute memory access cpu gpu communication significantly improve performance graph tensor operation optimize operation cuGraph tensor graph data completion application accurate reconstruction incomplete graph data propose graph operation achieve speedup versus cpu GSPBOX cpu matlab implementation xeon CPUs graph data completion application achieves speedup cpu matlab implementation speedup accuracy gpu tensor completion cuTensor tubal library keywords  tensor graph operation graph data processing library introduction amount data generate diverse domain social network sensor network  network citation authorship network commerce network aspect daily generate datasets phone trajectory health data monitoring device banking financial shopping preference data irregular complex structure graph ability characterize complex interaction data model researcher propose various graph data graph graph graph random geometric graph entity user facebook sensor model node connection user sensor graph data matrix reside node data matrix tensor graph structure graph tensor frontal slice corresponds graph node connection frontal slice graph tensor operation widely various application graph neural network graph convolution graph svd graph QR computer vision graph convolution image processing graph filter data completion graph fourier transform inverse graph fourier transform video compression graph svd however exist insufficient efficient graph tensor computation cpu graph tensor operation increase rapidly node dimension data node exist cpu graph tensor cannot requirement application localization online recommendation impact graph compute performance gpus performance graph tensor computation library graph tensor operation researcher implement optimize graph tensor operation manner inefficient error prone instance GSPBOX popular cpu graph processing toolbox however graph operation GSPBOX graph reveal preliminary FT inverse FT filter graph vector respectively moreover GSPBOX scalar data node lack graph operation graph convolution graph shift svd QR application network analysis image processing although cuTensor tubal library performance tensor computation closely related structural graph unsuitable graph analyze application therefore motivate develop library performance graph tensor operation diverse application implement performance graph tensor operation data iot application gpu implement optimize library cuGraph tensor accurate graph tensor operation cuGraph tensor implement graph tensor operation data graph node scalar vector matrix graph data model application sensor network social network iot wireless camera network graph data completion application demonstrate usage propose graph tensor operation propose optimization technique improve computation memory access efficiency reduce cpu gpu communication contribute significant performance improvement cuGraph tensor library exist highly optimize nvidia cuda library cuBLAS  exist library magma  efficient gpu computation image KB image graph tensor construct stack matrix graph node node data matrix becomes frontal slice graph tensor image KB image cuGraph tensor library contribution summarize develop performance gpu library cuGraph tensor graph tensor operation graph shift shift graph fourier transform FT inverse graph fourier transform inverse FT graph filter filter graph convolution convolution graph tensor graph tensor svd svd graph tensor QR QR encapsulate operation source library blas interface propose optimization technique batch compute computation reduction memory access cpu gpu communication improve performance demonstration develop graph data completion application performance graph tensor operation cuGraph tensor library perform extensive evaluate performance graph tensor operation graph data completion application shift FT inverse FT filter convolution svd QR operation achieve speedup respectively cpu GSPBOX cpu matlab implementation xeon CPUs graph tensor operation optimization average faster gpu baseline implementation graph data completion application achieves speedup cpu matlab implementation speedup accuracy gpu tensor completion cuTensor tubal library remainder organize notation graph tensor operation implementation optimization graph tensor operation gpu evaluate performance graph tensor operation graph data completion application discus related conclusion drawn graph tensor operation gpu graph tensor operation cuGraph tensor library combine graph signal processing operation tubal rank tensor operation introduce graph tensor operation briefly analyze gpu parallelism operation notation operation notation lowercase boldface denote vector uppercase denote matrix uppercase calligraphic denote tensor denote node graph temporary index node denote hermitian transpose complex matrix denote pseudo inverse graph tensor model connection relation model graph node vertex correspond adjacency matrix nonzero indicates exist relation dependency similarity node node denote undirected graph symmetric matrix matrix diagonal matrix diagonal sum associate node laplacian matrix normalize laplacian matrix laplacian matrix normalize laplacian matrix eigen decomposition eigen vector matrix graph tensor operation cuGraph tensor library   shift FT inverse FT filter convolution svd QR graph tensor graph node without loss generality assume graph tensor define mapping convenient graph tensor matrix physical meaning graph node frontal slice associate node data node scalar vector application tensor entry concisely graph tensor operation implement graph tensor operation gpu operation composite operation operation shift FT inverse FT composite operation filter convolution svd QR construct operation graph tensor operation widely various application graph neural network video compression data completion machine recognition feature network analysis input output graph tensor operation summarize shift shift graph tensor FT define spectral domain representation graph fourier transform inverse FT compute composite graph tensor operation matrix multiplication matrix decomposition eigenvalue decomposition singular decomposition QR filter graph node filter specific transformation matrix operates diagonal perform eigen decomposition obtain perform matrix multiplication filter matrix frontal slice reorganize graph tensor matrix tensor multiplication matrix matrix multiplication convolution graph node convolution convolution kernel hadamard wise multiplication perform graph fourier transform graph kernel perform hadamard reorganize graph tensor hadamard graph node graph tensor perform matrix multiplication  slice graph tensor tensor slice multiplication matrix matrix multiplication svd graph node svd orthogonal tensor respectively diagonal tensor perform singular decomposition frontal slice graph tensor svd QR QR tensor upper triangular tensor perform QR decomposition frontal slice graph tensor QR parallelization analysis gpu architecture image KB image gpu baseline implementation implement matrix tensor multiplication exploit cuBLAS library instead multiple scalar multiplication tensor denotes constant multiplies tensor improve performance enable efficient multiplication perform tensor reorganization denotes reorganize tensor tensor graph tensor operation gpu implement multiple matrix computation matrix multiplication matrix decomposition loop computation parallelism batch compute scheme implement multiple matrix computation graph tensor operation computation gpu summarize shift matrix tensor multiplication adjacent matrix graph tensor FT inverse FT perform matrix decomposition normalize laplacian matrix obtain eigenvector matrix perform matrix tensor multiplication graph tensor filter convolution svd QR perform FT conduct filter convolution tensor slice multiplication tensor svd tensor QR respectively spectral domain perform inverse FT computation graph tensor operation consist linear algebra routine matrix decomposition matrix transposition matrix multiplication matrix tensor multiplication tensor slice multiplication gpu addition data storage memory access graph data  improve data access efficiency compute compute obtain apply FT perform matrix multiplication frontal slice tensor spectral domain derive frontal slice finally obtain apply inverse FT alg cuGraph tensor library gpu cuGraph tensor library cuda library cuBLAS  exist library magma  workflow layer cuGraph tensor library propose optimization strategy efficient graph tensor operation implementation optimization image KB image workflow cuGraph tensor library architecture cuGraph tensor library workflow cuGraph tensor library cpu allocates memory load graph data adjacency matrix graph tensor cpu sends matrix graph tensor gpu gpu utilizes compute normalize laplacian matrix FT inverse FT convolution svd QR adjacency matrix shift filter specify graph tensor operation finally gpu sends cpu computation graph tensor operation entirely perform gpu graph tensor operation cuGraph tensor gpu faster cpu cpu computation hybrid cpu gpu implementation incur additional cpu gpu synchronization communication cuGraph tensor library adopts layer layer routine library data storage handler efficient data communication handler data transfer cpu gpu memory access handler efficient memory access gpu layer contains graph tensor operation shift FT inverse FT operation rely operation contrast filter convolution svd QR composite operation FT inverse FT elaborate summarizes input output operation entire cuGraph tensor library nvidia cuda library cuBLAS  exist library magma  upper application graph tensor operation widely various application video compression data completion data compression machine recognition feature knowledge discovery network analysis develop graph data completion application demonstrate usage cuGraph tensor library optimization graph tensor operation gpu baseline implementation optimal fully utilize compute core memory bandwidth gpus propose optimization technique improve performance graph tensor operation batch compute scheme improve gpu utilization reduce kernel invocation unoptimized implementation performs matrix computation frontal slice graph tensor gpu utilization kernel invocation batch compute scheme organize computation launch cuda kernel matrix compute graph tensor operation kernel utilizes batch routine compute multiple matrix computation graph tensor operation parallelly batch matrix tensor multiplication graph  shift FT inverse FT cuGraph tensor library perform tensor matrix multiplication computation intensively instance matrix tensor multiplication eigenvector matrix tensor graph fourier transform graph tensor compute matrix tensor multiplication gpu perform matrix multiplication frontal slice conventional gpu implementation loop compute matrix multiplication sequentially propose batch matrix tensor multiplication scheme improve gpu utilization scheme organize multiple matrix multiplication compute parallelly gpu compute matrix multiplication parallelly gpu gpu memory otherwise split matrix multiplication partition accord gpu memory capacity partition contains multiple matrix multiplication computation partition independent compute partition gpu scheme gpu utilization significantly improve analogously propose batch tensor slice multiplication scheme improve gpu utilization cuGraph tensor batch matrix tensor multiplication scheme achieves speedup gpu baseline implementation without batch scheme batch tensor svd svd operation compute multiple matrix svd decomposition gpus matrix computation exist gpu blas library batch svd routine matrix instance nvidia cuBLAS library  library batch svd matrix respectively exploit  library compute multiple matrix svd decomposition parallelly matrix computation svd batch tensor QR QR operation compute multiple matrix QR decomposition matrix computation gpu nvidia cuBLAS library batch QR matrix exploit magma library compute multiple matrix QR decomposition parallelly matrix computation QR image KB image performance batch matrix tensor multiplication scheme reduce cpu gpu communication mainly focus external nvidia gpus computation capability integrate nvidia gpus bandwidth pcie bus cpu external gpu around GB pcie gen GB bandwidth gpu global memory reduce cpu gpu communication critical performance described computation graph tensor operation consist multiple involve linear algebra routine vector matrix tensor computation conventional gpu implementation linear algebra routine incur cpu gpu communication transfer input data gpu gpu cpu communication cpu multiple cpu gpu communication graph tensor operation communication data graph tensor propose pre allocation scheme reduce intermediate cpu gpu communication scheme pre allocate intermediate variable eigenvector matrix computation graph tensor operation pre allocate variable linear algebra routine eliminate intermediate cpu therefore cpu gpu data transfer another data transfer operation respectively utilize asynchronous data transfer reduce communication optimize data storage memory access graph tensor operation cuGraph tensor library data graph node scalar vector matrix graph node data node vector matrix tensor node scalar vector matrix respectively vector matrix tensor graph vector graph matrix graph tensor respectively data cuGraph tensor library various application instance model sensor node graph node network connection sensor graph node scalar data sensor reading vector data series sensor reading matrix data series multiple sensor data storage handler handle data gpu improve memory access efficiency cpu gpu memory access handler cuGraph tensor library appropriate memory storage layout ensure continuous memory access scalar data data graph node dimensional array continuously index graph node vector data vector graph node matrix index graph node adopt layout matrix memory matrix data matrix graph node frontal slice tensor index graph node graph node node matrix data data tensor flatten tensor dimensional array tensor computation graph tensor operation gpu thread fetch data graph node memory access handler incorporates memory access operator enables gpu thread fetch scalar vector matrix specific graph node conveniently instance memory access operator slice fetch slice load frontal slice graph tensor image KB image data representation correspond memory storage optimization computation optimization reorganize graph tensor graph tensor operation reorganize input tensor gpu subsequent computation conventional implementation allocates gpu memory launch massive thread thread reorganize input data straightforward manner propose matrix transpose scheme replace straightforward reorganization gpu achieve memory access efficiency reduce computation input graph tensor 1D array memory access array specific matrix perform matrix transpose transpose matrix equivalent tensor optimization reduce computation specific tensor equivalent instead filter graph fourier transform graph tensor pointwise multiplication spectral domain graph fourier transform tensor filter spectral response finally inverse graph fourier transform computes output domain graph filter theorem reduces graph filter graph fourier transforms pointwise multiplication spectral domain likewise graph convolution specific graph filter theorem compute convolution instead convert vector diagonal matrix optimization technique reduces computation kernel invocation filter convolution operation optimization matrix decomposition computation graph tensor operation utilize matrix decomposition obtain eigenvector matrix decompose normalize laplacian matrix nvidia cuda library multiple matrix decomposition routine singular decomposition svd upper LU decomposition eigen decomposition undirected graph utilize eigen decomposition obtain matrix symmetric diagonalize eigen decomposition matrix faster singular decomposition memory api function cuGraph tensor library data storage   allocate graph float allocate memory adjacency matrix graph   allocate graph tensor float allocate memory graph tensor gpu memory graph tensor dimensional array tensor graph operation   graph shift float float float compute graph shift adjacency matrix input graph tensor output   graph  float float float compute graph fourier transform obtain spectral domain representation graph tensor adjacency matrix input graph tensor output   graph  float float float compute inverse graph fourier transform adjacency matrix graph fourier transform graph tensor output   graph filter float float float compute graph filter adjacency matrix input graph tensor output   graph convolution float float float float compute graph convolution adjacency matrix input graph tensor graph convolution kernel output   graph float float float float compute graph adjacency matrix input graph tensor output   graph svd float float float float float compute graph svd adjacency matrix input graph tensor output   graph QR float float float float compute graph QR adjacency matrix input graph tensor output efficient implementation graph tensor operation optimization strategy graph tensor operation cuGraph tensor library optimize introduce optimize implementation graph tensor operation detail shift FT inverse FT input graph tensor reorganize perform batch matrix tensor multiplication eigenvector matrix graph tensor exploit cuBLAS library finally reorganize graph tensor output graph tensor optimize composite graph tensor operation filter convolution svd QR efficiently compute spectral domain perform optimize FT obtain spectral domain representation input graph tensor gpu spectral domain graph tensor operation decompose multiple independent matrix computation posse parallelism batch matrix operation computation gpu perform optimize inverse FT return domain spectral domain gpu filter perform eigen decomposition obtain exploit cuBLAS library perform batch matrix tensor multiplication filter matrix graph tensor exploit cuBLAS library convolution perform graph fourier transform graph kernel perform batch matrix tensor multiplication graph tensor exploit cuBLAS library perform batch tensor slice multiplication graph tensor exploit cuBLAS library svd perform batch tensor svd graph tensor exploit  library QR perform batch tensor QR decomposition graph tensor exploit magma library cuGraph tensor library built nvidia cuda exist library magma  nvidia cuda enable gpus external gpus integrate gpus cuGraph tensor library blas interface upper algorithm application performance evaluation evaluate performance cuGraph tensor library speedup graph tensor operation addition cuGraph tensor develops data completion application graph tensor operation efficient accurate graph data reconstruction evaluate performance graph data completion application evaluation platform  parameter  intel xeon core ghz core totally  tesla cuda core ghz GB ddr memory host  GB ghz evaluation setting detailed configuration platform server GB ddr memory intel xeon CPUs tesla gpu peak precision performance tera flop cpu MB cache physical core thread hyperthreading technology server ubuntu linux kernel version generate synthetic graph tensor graph tensor operation  video evaluate graph data completion application average report metric evaluate algorithm performance recovery error tensor execution CPUs tesla gpu respectively speedup calculate recovery error relative error RSE data completion application define cuGraph tensor propose gpu library consists graph tensor operation GSPBOX cpu toolbox contains gsp operation FT inverse FT filter GSPBOX shift convolution svd QR operation GSPBOX GSPBOX processing scalar data node loop GSPBOX implementation processing vector data cpu matlab implementation graph tensor operation cpu implementation matlab processing matrix data graph tensor shift convolution svd QR cpu implementation matlab processing vector data graph matrix cuTensor tubal gpu library contains tensor operation tensor completion implementation graph data completion application tensor completion application cuTensor tubal cpu gpu implementation matlab cuda respectively gpu graph tensor operation library comparison cuGraph tensor knowledge library performance graph tensor operation evaluate cuGraph tensor library performance graph tensor operation graph speedup optimize unoptimized graph tensor operation tesla gpu matlab implementation xeon CPUs shift achieve maximum speedup cpu matlab implementation optimization technique shift significantly faster cpu matlab implementation comparison unoptimized shift employ none optimization technique achieve maximum speedup conduct batch computation suffers data transfer memory access gpu utilization performance FT achieve maximum speedup cpu matlab implementation FT significantly outperforms matlab implementation graph fourier transform comparison unoptimized FT without optimization technique achieve maximum speedup inverse FT achieve maximum speedup cpu matlab implementation optimization technique inverse FT achieve performance cpu implementation comparison unoptimized inverse FT achieve maximum speedup unoptimized inverse FT exhibit gpu utilization data transfer memory access performance speedup optimize unoptimized filter convolution tesla gpu matlab implementation xeon CPUs filter achieve maximum speedup cpu matlab implementation filter obtain performance matlab implementation contrast unoptimized filter achieve maximum speedup unoptimized filter effectively utilize gpu resource data access obtain performance optimize filter convolution achieve maximum speedup cpu matlab implementation optimization gpu convolution significantly outperform matlab implementation contrast unoptimized convolution achieve maximum speedup optimize convolution faster unoptimized convolution utilized optimization achieve gpu utilization achieve maximum speedup cpu matlab implementation optimization gpu significantly outperform matlab implementation contrast unoptimized achieve maximum speedup optimize convolution faster unoptimized utilized batch tensor slice multiplication speedup optimize unoptimized tensor decomposition operation tesla gpu matlab implementation xeon CPUs svd achieve maximum speedup cpu matlab implementation optimization gpu svd significantly outperform matlab implementation contrast unoptimized svd achieve maximum speedup optimize svd faster unoptimized svd utilized batch graph svd achieve gpu utilization cpu matlab implementation QR achieve maximum speedup contrast unoptimized QR achieve maximum speedup optimize QR faster unoptimized QR utilized batch graph QR image KB image speedup graph tensor decomposition operation tesla gpu CPUs respectively graph tensor operation average maximum performance optimize gpu implementation unoptimized gpu implementation graph tensor shift FT inverse FT filter convolution svd QR optimization technique average faster unoptimized graph tensor operation demonstrates optimization technique batch scheme computation memory access data transfer optimization effective significantly improve performance addition speedup optimize gpu implementation increase graph data growth graph data computation data parallelism gpu effectively parallelizes accelerates computation evaluate performance cuGraph tensor library graph vector graph matrix obtain performance versus GSPBOX cpu matlab implementation library performance graph data completion application data iot application data occurs various unpredictable unavoidable instance data rate report iot project namely     incomplete sensory data recover exploit spatial temporal correlation data researcher propose diverse data completion approach reconstruct incomplete data previously iterative algorithm tubal alt min algorithm propose robust data completion tubal rank tensor model however algorithm compute intensive increase exponentially tensor dimension impractical data recovery limitation implement graph data completion gpu exploit performance graph tensor operation propose cuGraph tensor library FT inverse FT image KB image algorithm graph data completion application without loss generality graph data completion consists performs graph fourier transform obtain initial partial tensor input alternate minimization iteration alternate minimization graph tensor iteratively alternately refine graph fourier transform finally perform inverse graph fourier transform obtain reconstruct graph tensor approach graph tensor formulate perform QR factorization obtain upper triangular matrix video recovery wireless camera network evaluate performance graph data completion application wireless camera network consist multiple sensor node node equip camera video frame capture camera sensor node compute node video recovery video frame due wireless transmission sensor node failure video data upper application security surveillance model wireless camera network graph camera sensor node vertex camera sensor node resolution sensor node capture video frame video frame video rate model percentage video frame video frame image KB image illustration graph data completion task speedup speedup graph data completion application cuGraph tensor tensor completion cuTensor tubal cpu matlab implementation graph tensor tesla gpu xeon CPUs respectively node varied gpu cpu implementation execute iteration converge gpu tensor completion cuTensor tubal graph data completion cuGraph tensor achieve average speedup cpu matlab implementation CPUs graph data completion cuGraph tensor achieve average speedup graph data completion application obtains performance image KB image data completion implementation image KB image speedup graph data completion module versus implementation recovery error recovery error graph data completion cuGraph tensor tensor completion cuTensor tubal cpu matlab implementation data rate graph node video frame graph tensor rate video frame instance rate video frame gpu cpu implementation iteration terminate rate varied cuGraph tensor cpu matlab implementation achieve  tensor completion cuTensor tubal cuGraph tensor cpu matlab implementation achieve  rate curve overlap recovery error performance rate cuGraph tensor achieve RSE cuTensor tubal achieve RSE propose cuGraph tensor library achieve recovery error tensor completion cuTensor tubal library utilize correlation graph structure data completion image KB image recovery error rate implementation visual video recovery video frame recover video frame graph tensor rate video frame apply graph data completion cuGraph tensor library video frame reconstruct image KB image video frame recover video frame related discus related graph operation library gpu tensor compute data completion graph algorithm library graph operation library knowledge gpu library graph tensor operation analyze cpu graph operation library gpu tensor operation library cuTensor tubal GSPBOX grasp cpu graph operation library matlab  cpu graph operation library python limitation performance cpu graph operation library exhibit performance graph node instance graph fourier transform inverse graph fourier transform graph filter GSPBOX graph node scalar data node graph fourier transform inverse graph fourier transform computation therefore curve overlap graph node graph filter xeon CPUs graph fourier transform inverse graph fourier transform respectively data scalar data vertex graph application data vector matrix vertex limited graph operation library useful graph operation GSPBOX grasp  operation library useful graph tensor operation cuGraph tensor diverse application lack structural cuTensor tubal library frontal slice tensor lack structural graph tensor operation graph structure frontal slice tensor gpu tensor compute data completion cuGraph tensor library matrix data graph node graph node along matrix node dimensional data model data tensor exploit tensor model technique efficient processing analyze previously propose cuTensor tubal library gpu consists tensor operation benefit diverse application exist focus specific tensor operation gpus NTF gpu non negative tensor factorization  propose efficient tensor factorization  tensor decomposition library heterogeneous cpu gpu cluster data completion algorithm iterative computationally intensive therefore gpus increasingly exploit accelerate data completion propose gpu performance matrix completion homomorphic propose gpu encoder decoder scheme robust data transmission wireless network propose gpu tensor completion cuTensor tubal library graph data completion propose cuGraph tensor library faster accurate tensor completion cuTensor tubal library gpu graph algorithm library  gpu graph algorithm library cuda  function graph construction manipulation primitive useful graph algorithm source shortest pagerank counting similarly  cuGraph library gpu library graph algorithm community detection centrality detection bfs pagerank comparison cuGraph tensor library consists performance graph operation shift inverse FT filter convolution svd QR  cuGraph cuGraph tensor built exist cuda library cuBLAS selectively application graph algorithm graph operation conclusion future propose cuGraph tensor library graph tensor operation performance graph data processing cuGraph tensor library exploit separability graph tensor operation mapped parallelism onto gpu architecture propose optimization technique compute memory access cpu gpu communication graph tensor operation efficient graph tensor operation cuGraph tensor developed graph data completion application accurate reconstruction incomplete graph data performance evaluation cuGraph tensor library achieve speedup versus cpu GSPBOX matlab implementation shift FT inverse FT filter convolution svd QR respectively propose optimization technique optimize gpu graph tensor operation achieve average speedup respectively baseline gpu operation graph data completion achieve maximum speedup cpu matlab implementation maximum speedup accuracy gpu tensor completion cuTensor tubal library future improve cuGraph tensor library functionality performance convenience useful graph tensor operation library onto multiple gpus develop interface python framework credit authorship contribution statement tao zhang review edit investigation supervision wang  software draft xiao yang liu methodology