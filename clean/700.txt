hpc application suffer bottleneck cache instruction execution memory bandwidth remain resource underutilized developer runtime ensure critical resource fully exploit application attractive technique increase hpc utilization colocate multiple application server application critical resource however contention resource reduce application performance server efficiency improve model performance degradation colocated application hardware performance counter exploit model optimize colocated application intelligent resource manager contribution machine model predict performance degradation colocated application hardware counter intelligent schedule scheme deployed exist resource manager enable application schedule minimum performance degradation approach achieves performance improvement avg max standard policy commonly exist manager previous keywords resource manager hpc machine colocation performance characterization performance counter introduction data provider maximize server utilization obtain benefit investment hpc application however achieve theoretical peak performance carefully optimize substantial waste resource across data hpc resource efficiency important concern achieve exascale compute performance exascale technology unrealistic amount electricity bill sustain platform lifespan roughly hardware proportional hpc technology mature exascale resource constrain future amount provision severely limit scalability user demand resource constrain server environment minimize resource usage meeting performance requirement increase computational demand technique hardware provision apply strict bound provision per node thereby node setting provision implement enforce socket limit intel RAPL technology RAPL relies update register manage usage server component processor dram gpus etc monitoring hardware estimate consumption adapts processor voltage frequency desire cap specify interval technique DVFS adapt processor voltage frequency reduce processor consumption frequency potentially reduction although improve efficiency negatively impact processor performance DVFS RAPL alone insufficient provision environment enforces bound individual component cpu bound across component enforce global scheduler avoid violate bound promising increase overall utilization efficiency multiple application concurrently server node approach workload colocation disadvantage workload colocation potential degradation application performance due resource cache memory controller data prefetchers device degradation predict impractical degradation application ahead due uncertain degradation hpc usually resource compute node application nevertheless workload colocation substantial potential improve throughput colocated application bottleneck resource improvement utilization without modify application source code machine technique potential complex relationship input feature hardware performance monitoring counter PMCs prediction slowdown experienced colocated application diverse application hpc machine model capability generalize application although machine approach computationally intensive acceptable execution overhead optimization reduce random model approach encompasses machine model integration intelligent application scheduler goal reduce makespan application application queue fix contribution prediction model estimate performance degradation due workload colocation model random capable accuracy predict performance degradation workload management machine schedule leverage predict degradation colocated application hpc evaluate actual implementation deployed exist resource manager execute multithreaded application parsec NPB splash rodinia reduction makespan average maximum exist resource manager advantage technique application modification annotation PMC data application minimum degradation server motivation motivate challenge resource colocation machine favorable strategy server efficiency conduct prior heuristic fail resource colocation multi core processor chip resource cache interconnects memory controller application core contention due resource methodology capable predict colocation scenario useful without prediction profile  performance beforehand schedule decision prohibitively expensive machine useful subset application model generalize application  portion previous research performance prediction multi core focus contention cache prior attempt estimate performance slowdown application due interference resource focus develop technique predict application behavior indicator cache usage behavior predict extent application suffers colocation extent degrade colocated application classification scheme understand performance slowdown application furthermore server application achieve theoretical peak performance carefully optimize limit sub utilization tends increase core idle provision negatively impact utilization entire image KB image normalize makespan distribute intensity prior fifo sequential fifo policy execute randomize application queue detail experimental setup image KB image overview approach consist phase offline deployment machine model performance degradation prediction due colocation online schedule submit queue machine model deployment stage PMC data machine model training validation optional feature selection simplification model evaluate offline simulation model schedule deployment heuristic prior research address workload colocation schedule application variance resource cache llc distribute intensity DI heuristic llc rate via hardware counter avoids schedule application llc rate memory hierarchy explore workload colocation minimize execute application makespan DI actually increase makespan default fifo policy typical manager detail improve fifo application node behavior due schedule DI execution rely heuristic cache sole indicator sufficient minimize overall resource contention improve server efficiency execution contrast prior heuristic approach model abstract application predict degradation introduce  application automatic analysis hardware performance monitoring counter PMCs PMCs occurrence hardware resource negligible overhead directly reveal degradation application suffer colocated another application addition PMCs monitoring subset subset sufficient reveal degradation due colocation intuitively identify PMCs manually model PMCs predict degradation evaluate machine model intelligent workload management describes PMCs perform intelligent colocation workload hpc overview overview organize offline phase deployment online phase schedule deployment phase responsible training machine model estimate performance degradation due workload colocation schedule phase input queue execute server decides optimize  typical cluster scheduler workload manager flavor bin pack algorithm assign physical node user define resource demand constraint distribution methodology perform exist workload manager non exclusive access physical node degradation aware fashion readily deployed exist workload manager machine model built deployment phase predict degradation application submit execute queue focus colocation application maximum runtime execution within characterize degradation assume application execute profile PMCs application execution alone target accessible workload manager schedule phase technique receives input queue profile data model predict degradation colocated finally generates optimize schedule queue minimizes overall runtime application queue queue metadata workload manager execute depict data collection PMCs model training validation input feature selection simulated deployment deployment detail PMC dataset collection PMCs execution application training data application profile input feature consists execution target PMCs input dataset training model execute application alone without user application obtain baseline application execution hardware counter ideal application resource available without compete application application available core application execute multiple minimum maximum standard deviation PMCs baseline execution calculate degradation experienced application colocation application calculate percentage increase application execution ideal application execute colocated execute primary application concurrently another interfere application execute application training application interfere application primary application restart primary application contention entire execution although complexity application data colocation reduce execute subset application estimate via matrix factorization microbenchmarking bubble complexity dependent application training application deployed training dataset contains PMC data primary application alone PMC data interfere application alone performance degradation suffer primary application prior training model identify application affected mode consequently introduce negative degradation input file alter negative apparent degradation colocated application hardware counter derive metric  PMCs  instruction resource stall instruction stall cycle frontend stall cycle backend  fault context switch cpu migration  reference cache llc prefetches llc prefetch  demand data   dirty  allocate demand eviction  demand data dcache dcache load dcache load dcache prefetch replacement  uops retire mem uops retire load mem load uops retire llc mem load uops retire llc  cache ref per instruction cpu usage cache per instruction ratio model training validation methodology aim estimate degradation application concurrently machine PMC data application illustrate PMC input data application primary application interfere model output degradation suffer primary application colocated application training phase dataset data randomly chosen validate model training validation described model iterate apply fold validation achieve training phase split input data subset performs training subset subset evaluation model iterate subset iteration picked validation training phase parameter model achieve parameter hyper parameter machine directly within training instance parameter alpha gamma kernel argument desire model technique   available hyper parameter computationally demand quickly explore suitable combination parameter model apply sequential model optimization applies bayesian optimization technique account information previous trial chose parameter iterate model accuracy coefficient determination prediction function determines variation dependent variable due independent variable ratio residual sum sum tune hyper parameter model  elastic  ratio tol max iter random  feature min sample split bootstrap estimator  tol coef gamma epsilon  layer alpha activation rate rate init tol image KB image overview schedule phase degradation graph built prediction machine model apply validity widely indicator machine statistical analysis input feature selection simplify methodology identify input feature affect prediction reduce PMC profile application building training across multiple architecture generic PMC subset architecture building prediction model generic feature portable methodology apply architecture without specific PMC hardware feature selection generic counter standard profile linux perf subset counter without perform PMC multiplexing limited physical PMC register application PMCs due conflict simultaneously PMCs perf counter evaluate quality schedule via simulation generic PMCs counter originally model training hardware counter derive metric feature selection   instruction instruction  fault context switch cpu migration  reference cache  usage model deployment training validate model generate schedule randomize queue application goal minimize overall execution independent application goal schedule  application onto server node minimum execution express schedule minimize sum chosen application node degrade execution input optimal schedule polynomial model fully graph degradation graph vertex degradation graph application queue degrade runtime calculate predict degradation node optimal colocation application minimum perfect degradation graph equivalent optimal colocation schedule application valid schedule vertex without vertex perfect minimum ensures objective function colocation satisfied minimize sum execution account degradation colocated application blossom algorithm optimally application polynomial node degradation graph respectively analysis computational schedule degradation graph approximately greedy heuristic simply selects runtime execute schedule output minimum perfect minimum overall execution entire execution due degradation solo serial execution schedule excessive degrade runtime serial fashion exclusive resource excessive degradation increase makespan execute application simulated scenario deploy model perform offline deployment project performance model generate schedule queue without execute server simulation approximation execution compute performance schedule previously application degradation runtime application compute project makespan model allows perform accurate comparison across model beyond model indicator perform simulation model minimize makespan deployed validate upon execution scenario model deployment incorporate simulated model exist batch schedule implement deployed chosen model slurm workload manager adopt slurm popular scalable widely deployed source fault tolerant schedule linux cluster implement schedule plugin structure colocate plugin execute periodically queue submit compute compute schedule plugin attempt colocation opportunity pending server machine already experimental evaluation evaluate approach cluster multi core server hpc application perform queue approach exist default policy slurm workload manager executes application sequentially server allows server without degradation knowledge greedy blossom perfect knowledge degrade runtime execution graph goal contrast quality output metric makespan schedule application  suite  fluidanimate swaptions vip streamcluster  lud cfd   svm rfe kmeans   fmm ocean    lulesh ssca fft mandelbrot qsort minife hpccg parameter training phase   min max  elastic  ratio tol max   tol coef gamma  linear linear auto poly auto poly auto random  feature min sample split bootstrap  auto auto auto sqrt mlp  layer alpha activation rate rate init  relu adaptive identity adaptive logistic  logistic  experimental setup cluster server equip intel xeon  EP comprise core operating ghz socket MB cache llc core server GB ddr DIMMs memory minimize linux interference makespan cpu intel  governor performance mode fix frequency operational  linux enterprise server SP kernel version default node executes slurm controller daemon compute node server hyperthreading disabled data application benchmark perform application parsec rodinia NPB application splash application variety computational multithreaded performance code application compile gnu linux gcc optimization flag multithreading enable application thread core server node profile application linux perf per thread application tune exceed private cache native input machine scikit library implement machine technique accuracy model calculate coefficient determination explain recall performance model accuracy model analyze accuracy model training validation described training phase evaluate impact additional information min max standard deviation PMC feature decrease feature prediction built model scenario additional information hyper parameter model accuracy model validation brevity clarity refer model EN elastic net svm vector machine RF random mlp multilayer perceptron image KB image accuracy achieve counter subset counter accuracy model hardware counter machine technique achieve accuracy increase feature min max standard deviation drastically improve accuracy  svm model mlp random accuracy accuracy mlp increase accuracy random simplify methodology subset PMCs detailed evaluate model account subset feature generic PMCs accuracy model reduce feature decrease accuracy  model svm slight improvement counter mlp model performance improvement feature random model increase accuracy model subset counter accuracy demonstrates subset counter random model potential perform deployment ass performance offline simulation evaluate quality schedule model makespan analysis via simulation model accuracy understand model perform deployment simulate offline deployment ass performance model schedule randomize queue submit execute application batch schedule data training runtime solo colocated setting image KB image performance model simulated deployment schedule model fifo approach model strategy assemble blossom greedy image KB image prediction behavior model validation unseen dataset summarize simulation model model built counter average performance fifo  svm mlp slight overall improvement RF model improvement  svm random performance simplify generic input feature surprisingly despite achieve accuracy model training phase mlp model increase makespan model simulated subset feature strategy schedule described blossom degradation graph greedy heuristic simulation simplest strategy performance elaborate furthermore trend performance model execute scenario image KB image normalize makespan queue application server medium degrade runtime arrival performance greedy blossom strategy investigate model simulated makespan analysis scatter plot prediction model confront unseen data validation prediction deviate contribute maximize server utilization scheduler colocation decision mispredicted predict degradation underestimate degradation critical scheduler indicator colocate application colocated application severe performance slowdown predict degradation desirable overestimate degradation critical scheduler assume conservative approach schedule application sequential fifo fashion  svm mlp model prediction around identify trend random model majority prediction concentrate around random model predict negative zero confront validation data negative degradation dataset deem model  svm mlp suitable resource manager application increase makespan execute queue corroborate makespan analysis model simulation conclusion random model performance offline deployment model accuracy influence scheduler decision promising model training validation makespan analysis perform offline integrate random perform model schedule plugin slurm baseline disable resource resource manager apply fifo execution fifo slurm allows application allocate server without restriction approach fifo agnostic degradation application executes application arrival predefined degradation investigate degradation execution performance evaluate potential resource define application distinct degrade runtime analyze behavior schedule strategy randomly application queue queue application queue accord degrade runtime amount degradation queue application max runtime fifo execution medium degradation queue application execution generate runtimes fifo execution degradation queue application degrade runtime fifo execution analysis aim highlight potential scenario application schedule strategy makespan normalize fifo serial execution degradation queue fifo performance makespan average fifo fifo degradation aware flexibility rearrange sequential executes arrival excessive amount degradation although queue application degradation machine model outperform fifo execution mainly capacity rearrange queue application cannot execute outperform fifo sequential resource medium degradation queue fifo outperform fifo improvement makespan obtain degradation queue improvement makespan around happens serial execution opportunity application safely resource fully utilize resource scenario fifo advantage combination application dispatch arrival beneficial colocation fifo improve makespan fifo medium degradation queue degradation queue fifo slight improvement approach blossom strategy happens fifo aggressive approach application arrival application execute application earlier although advantage consecutive application safely execute away dependent arrival blossom strategy executes conservatively application executes serially runtime exceed fifo execution approach rely arrival safely execute application application greedy strategy performance fifo advantage application safely image KB image timeline application schedule highlight scenario degradation degradation text application execution degradation queue application axis  swaptions streamcluster ssca qsort  lulesh lavamd kmeans hpccg hop fluidanimate fft cfd blackscholes barnes degradation queue application  swaptions qsort  minife  hpccg hop fluidanimate fft barnes horizontal execution application fifo application alongside another application illustrate application execution dot execution degradation scenario considerably serialize execution application slurm fifo policy server slurm policy degradation scenario server enhance performance application serially application content resource approach attain makespan reduction fifo demonstrates effectiveness predictive approach correctly identify scenario worth perform workload colocation non colocation random queue evaluation model deployed generate queue randomly chosen application queue server batch scheduler analysis submit queue average makespan normalize fifo methodology fifo increase makespan almost improve makespan average worth mention performance fifo execute queue investigate individual queue comparison fifo makespan improvement scenario almost scenario addition approach outperform DI heuristic execute execute queue increase makespan highlight importance additional performance counter characterize degradation instead rely cache sole indicator achieve deploy model evaluate scalability increase server configuration server evaluation increase linearly dispatch queue server batch scheduler analysis submit queue makespan configuration server varies server average improvement however slight decrease server blossom strategy respective improvement average holistic queue plugin account currently dispatch consequently impact performance detail image KB image normalize makespan greedy blossom strategy execute randomly generate queue server makespan normalize fifo random model greedy strategy strategy blossom greedy outperform fifo execution average greedy strategy achieve performance blossom strategy performance average server slight decrease due overhead associate greedy slightest blossom worth mention greedy strategy performance fifo queue across server furthermore advantage greedy strategy blossom happens apply threshold fifo scheme greedy goal runtime advantage balance blossom strategy application predict excessive runtime execute alone threshold apply simpler strategy already reduces makespan significantly average fifo image KB image normalize makespan blossom greedy strategy execute randomly generate queue multiple server scalability analysis evaluate compute schedule application predict degradation application compute schedule graph blossom strategy greedy strategy predict degradation varied independent blossom greedy strategy model strategy standard deviation random model almost linearly model input entire queue pending influence model performance cluster scheduler image KB image spent predict degradation application random model image KB image spent schedule decision strategy blossom greedy spent schedule predict degradation generate model greedy strategy standard deviation processing blossom strategy standard deviation greedy approach performs sort execute elaborate graph blossom algorithm spent compute schedule greedy exceed evaluation blossom spent setting greedy strategy recommend handle overhead associate blossom strategy explain performance loss blossom greedy apps server cluster server schedule server increase greedy strategy affected node prediction dominates overhead blossom strategy penalize increase therefore blossom strategy performance penalty reduce model inference improve performance approach analyze impact reduce overhead predict degradation submit application demonstrate built random model suffers application grows parameter chosen training phase random model random model average prediction estimator estimator important feature model contributes predict sample spite reduce variance increase accuracy model sometimes estimator increase decrease estimator evaluate model training model balance accuracy predict improve scalability reduce estimator random model model reduce estimator predict application queue application estimator increase predict trend accuracy estimator accuracy estimator estimator spite model accuracy estimator previous model estimator balance accuracy predict predict almost model accuracy random reduce estimator previous model achieve accuracy estimator predict degradation model estimator reduce multiple yield standard deviation clearly reduction estimator affect predict increase spent predict degradation decrease nearly previous model decrease overhead approach affect image KB image predict degradation application queue application random model estimator quality model reduce estimator random model deployed server normalize makespan scenario reduce model greedy strategy  improve queue increase overall greedy strategy dominate prediction model estimator  reduce model blossom strategy  significant reduction overhead attain performance server model  however blossom optimal strategy graph algorithm introduce overhead increase application server image KB image spent predict degradation application random model training phase estimator reduce estimator adopt greedy strategy balance prediction effectiveness reduce makespan moreover overhead allows important deployment scheme trigger shorter interval without negatively impact schedule manager tweak parameter model application degradation prediction particularly important application server image KB image normalize makespan blossom greedy strategy execute randomly generate queue multiple server estimator reduce decrease random model overhead RF related implement slurm  policy limitation hpc cluster implement aware slurm plugins explore management strategy exist hardware platform developed aware mechanism integrate within slurm improve resource utilization reduce redistribute strict  regime propose aware resource manager slurm maintain limit interface combination portable apis measurement slurm simulator benefit node dynamic voltage frequency DVFS intel average limit intel RAPL technique extensively improve efficiency throughput provision scenario avoid negative interference workload bubble application sensitivity  predict degradation due contention memory subsystem profile complexity pairwise  bubble application model multiple regression analysis micro benchmark predict average slowdown collocate application llc memory network pressure extend scheduler  virtual machine vms integer linear program ILP formulation minimize physical machine   efficient  workload explore heterogeneous multicore server focus leaf server contrast readily adopt plugin source cluster resource manager investigate effectiveness machine predict performance degradation due colocation application model estimate degradation per core hardware counter multithreaded application model estimate degradation thread scheduler allocate hardware average degradation thread propose cluster manager maximizes resource utilization meeting workload performance requirement classification impact amount resource resource interference performance workload classification approach eliminates exhaustive online characterization regularization model previously explore predict application interference model combine linear non linear approach accurate prediction model hardware feature instruction retire cache etc acquire offline measurement conclusion propose application colocation predicts degradation colocated application schedule combination minimum degradation improves allocation overall compute capacity therefore optimizes server efficiency experimentally demonstrate hardware counter machine promising alternative tackle implement source cluster resource manager slurm plugin schedule decision evaluate effectiveness practical experimental machine model random achieve accuracy evaluation phase outperform model regression achieve performance improvement avg max policy slurm resource manager greedy strategy balance approach makespan improvement overhead compute schedule