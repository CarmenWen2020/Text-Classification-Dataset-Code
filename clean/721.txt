understand memory performance multi core platform prerequisite perform optimization analytical model stochastic reward net SRNs model evaluate memory performance non uniform memory access numa multi core architecture approach considers detail architecture proposes monolithic srn model evaluates memory performance memory response monolithic model incurs explosion increase core memory controller approximate model evaluate numa architecture SRNs validate measurement numa multi core platform core amd opteron server core intel demonstrate ability propose model accurately compute memory response numa architecture valuable information runtime application designer optimize execution parallel application architecture previous keywords numa architecture performance model stochastic reward net approximate model introduction accommodate increase computation data workload multi socket multi core comprise multi core processor memory controller built processor interconnect enables core socket cpu node access memory controller memory node access latency individual core memory node non uniform memory access numa architecture performance analysis memory access utmost importance manage core multi core memory limited bandwidth information memory performance runtime constrain application parallelism manage memory bandwidth although model propose predict memory performance symmetric multi processing SMP numa architecture additional challenge performance memory access strongly affected asymmetric memory architecture contention multiple location model numa challenge task although analytical model queue propose evaluate performance memory access numa architecture model fail achieve prediction accuracy due simplification model relation interconnection network memory controller stochastic reward net SRNs model memory numa architecture detail model evaluate predict memory performance numa architecture core memory node SRNs extension stochastic petri net  advantage specify evaluate intuitive powerful model paradigm performance availability reliability analysis compute SRNs automatically generate markov reward model  apply SRNs model memory access numa architecture access latency cpu node memory controller graphically model analytically evaluate SRNs reduction technique define enable model evaluation complex architecture cannot easily analyze exist model monolithic model numa architecture monolithic model extends primitive model memory access core cpu node memory node model although monolithic model helpful understand memory performance numa architecture suffers explosion relatively cpu memory node apply model exist future architecture overcome limitation approximate model significantly complexity fold fix iteration technique fold model apply exist numa machine comprise cpu memory node explosion fix model finally analyze numa machine expense reduction accuracy srn model validate core amd opteron server memory node machine dependent model parameter data transfer rate interconnection link service rate memory controller offline execute synthetic workload memory access cache llc rate application extract execute application isolation accuracy model compute memory response measurement amd server model queue demonstrate SRNs candidate practical performance evaluation numa multi core remainder organize discussion related architecture numa multi core implication performance model related description introduces petri net srn formalism stochastic model evaluate memory performance explains application srn model core numa multi core analyzes applies model numa architecture illustrates future research direction finally concludes related significant portion research focus model memory performance multi core kim wang memory performance prediction technique SMP architecture hong proposes analytical performance model gpus considers memory hierarchy gpu architecture dao machine approach gpu performance model proposal model memory performance multi core hardware architecture performance characteristic memory account numa architecture comprise multiple memory controller processor interconnection network approach purpose model evaluation performance memory access numa architecture apply queue theory model performance memory access multi processor  proposes performance model methodology parallel application simulation hardware architecture approach explore effectiveness queue multi processor consist multiple memory server interconnection network although exist publication simulated multi processor exhibit almost behavior recent numa multi core architecture  model memory contention memory controller SMP queue model memory performance cpu memory node predict linear regression approach model memory performance properly model processor interconnection network cho online performance model numa architecture queue apply memory controller interconnection link although model quickly predict memory performance application queue properly relationship processor interconnection network memory controller comparison approach   runtime model performance optimization parallel application numa architecture predict performance parallel application dependence core performance model applies linear regression hardware performance counter query runtime  applies functional performance model formulate performance consumption parallel application numa architecture wang proposes integer program model core allocation parallel application numa architecture goal maximize memory bandwidth usage assign appropriate core cho manages thread parallel application cpu memory bandwidth utilization predict online performance model srn model orthogonal previous numa performance optimization apply runtime accurate efficient memory performance model description depicts generalize architecture typical numa multi socket multi core architecture consists numa node node contains cpu node cpu resource cpu core private cache memory node comprise memory controller memory node interconnection network enables cpu core access memory node interconnection link exhibit service topology interconnect node dedicate link crossbar architecture interconnects intel  interconnect indirect interconnects irregular latency amd hypertransport srn model tailor model memory performance scientific memory intensive workload workload generate steady exponentially distribute memory request access burst characteristic simplification model without sacrifice accuracy model assume core executes thread stall llc memory request fifo processor execution avoid stall memory request memory controller reorder request analytical model predict memory request multitude memory request steady memory request llc resource interconnection link memory controller model serialization compute memory response interconnection link memory controller thread issue memory access stall access llc llc trigger memory operation memory node memory operation issue cpu core completes interconnection link memory controller multiple memory operation cpu core cpu node memory node serialize fifo multiple memory operation cpu core cpu node memory node contention interconnection link cpu node issue memory operation memory node serialization request memory controller fifo queue cpu node dedicate interconnection link memory node contention interconnection link overview stochastic reward net petri net PNs mathematical graphical formal description dynamic characterize concurrency parallelism synchronization non determinism mutual exclusion conflict typical feature distribute petri net define tuple finite finite assigns arc defines initial assigns token transition arc define token arc transition vice versa function defines multiplicity arc transition enable input token input arc upon token input arc remove input token per output arc deposit correspond output PNs graphically graphical representation transition arc multiplicity arc slash regular arc drawn arrow inhibitor arc token dot classical PNs difference transition transition enable priority enable transition although definition PNs adequate verify liveness boundedness invariance suitable quantitative evaluation behavior quantitative evaluation PNs equip notion augment PNs classify petri net  deterministic stochastic petri net  stochastic  exponentially distribute delay associate transition model immediate action generalize   immediate transition immediate transition immediately enable transition random exponentially distribute delay graphical representation  immediate transition rectangle transition rectangular stochastic extension  permit indication reward rate net stochastic reward net SRNs srn obtain associate reward rate marking GSPN graphical representation srn GSPN however application SRNs automate generation markov reward model  facilitate combine evaluation performance dependability  fault tolerant model SRNs  available  easily obtain SRNs stochastic petri net package  numerically srn model model express  spn extension program advanced feature spn model available  dependent arc multiplicity enable function array transition subnets sophisticated steady transient solver available cumulative absorption predefined easily extend user specify expression srn numerically analyze  assign reward rate srn compute reward steady analysis reward rate assign srn model denotes probability srn steady steady reward compute propose model describes SRNs propose model evaluate response MRT memory access numa architecture core controller srn model cpu node memory node model extend capture entire architecture monolithic cope explosion monolithic model approximate model fold fix iteration technique propose fold model fix model respectively core controller srn model structure srn model cpu node memory node interconnection link denotes active core cpu node transition model llc trigger core data transfer cpu node memory node service memory node respectively memory access memory intensive parallel workload exponential distribution hence exponential distribution function transition image KB image srn model cpu node memory node interconnection link token active core cpu node trigger llc rate transition model llc trigger core inter request rate core interconnection link arc transition denotes rate dependent rate transition depends token input varies rate denotes token transition token remove token deposit existence token enables transition model data transfer link cpu node memory node associate transition exponentially distribute rate data transfer cpu node memory node upon transition token enable transition associate transition exponential distribution rate memory node memory request transition token remove core memory node processing generate memory access request srn model simplify contention link memory node model dedicate interconnection link connects cpu node memory node memory node serf request issue core cpu node fifo discipline srn groundwork model monolithic model model numa architecture described core controller srn extend monolithic srn srn memory node cpu node cpu node contains active core bound variable respectively srn model cpu node contains token active core cpu node associate transition inter request core cpu node exponentially distribute rate transition dependent token rate token transition enable related assume core execute application data evenly distribute memory node consequently srn model assume memory request individual memory node probability realistic assumption numa multi core execute application optimize utilize available bandwidth memory node parallel program model openmp memory token probability probability transition upon transition receives token inhibitor arc model data equally distribute memory core trigger llc memory service inhibitor arc transition prevent transition token arc multiplicity memory node cpu node cpu node contains active core assign inhibitor arc balance load memory node image KB image monolithic srn model numa architecture existence token enables transition model service memory node rate token auxiliary return token exhaust transition without mechanism anymore cpu node request originate deposit token immediate transition dispatch token guard function associate transition ensure core cpu node exceed predefined therefore transition token addition guard function probability function associate transition function ensure cpu node active core cpu node token sends token frequently srn theoretically model cpu memory node active core cpu node however model suffers exponential explosion underlie markov chain srn extent exist cannot model anymore active core cpu node memory node underlie markov chain monolithic model contains active core underlie markov chain grows exponentially monolithic srn model entire architecture impractical employ model numa addition structure net modify whenever cpu memory node approximate model cope difficulty guard function srn model guard  otherwise probability function immediate transition srn model immediate  function return fold model monolithic srn model structure individual cpu memory node identical llc redirect probability memory node symmetry allows model architecture fold srn fold model cpu memory node model explicitly tag node remain cpu memory node fold combine sub model depicts structure fold model cpu node tag cpu node remain cpu node fold submodel label fold cpu node memory node treat tag memory node remain memory node fold submodel fold memory node tag cpu node tag memory node compute MRT memory access fold cpu node fold memory node fold sub model influence tag sub model image KB image fold approximate srn model numa architecture fold monolithic srn fold model assign average transition rate transition fold model inter request core average similarly service memory node link transition average link transition monolithic srn focus compute steady MRT memory access average impact simplification accuracy negligible token respectively token active core tag cpu node llc occurs token existence token enables transition interconnection link tag cpu node tag memory node fold memory node respectively interconnection link tag cpu node tag memory node link rate transition transition however associate rate function fold memory fold memory node account fold monolithic srn transition corresponds transition cpu memory node memory request limitation memory request memory node model multiplicity inhibitor arc transition model data transmission maximum request tag memory node fold memory node label define transition addition inhibitor arc multiplicity fold memory node token account transfer data tag cpu node fold memory node accomplish rate function associate memory node fold memory node memory node capacity request hence memory fold memory node sufficient capacity host request core tag cpu node token data transfer rate cpu node memory node capture rate transition contention transition grab token transition token otherwise token token transition enable rate memory request tag memory node upon token remove another token deposit functionality output transition related component monolithic srn model guard function probability function associate transition respectively infer replace index fold sub model comprise fold cpu node fold memory node existence token token enables transition rate transition dependent token fold active core cpu node rate hence rate upon fed token transition enable transition redirects memory access tag memory node transition redirects fold memory node rate function associate transition respectively define transition corresponds transition rate considers fold cpu node link cpu node tag memory node rate function token denote rate function rate otherwise transition fold transition transition contains spectrum fold transition correspond transition transition rate fold cpu node account fold memory node token therefore rate function token capacity fold memory node capacity host request fold memory node token interconnection link fold cpu node memory node sufficient capacity compute link memory node sufficient capacity rate function multiplies exist obtain rate associate transition rate function fold srn model rate  otherwise otherwise otherwise upon transition token respectively accepts token transition model fold memory node rate function associate transition compute rate function token fold memory node assigns otherwise therefore maximum fold memory node request transition token return fold model grows monolithic model fold model analyze configuration numa architecture nevertheless growth rate underlie markov chain model concern numa architecture memory node cpu node active core markov chain fold srn model contains around active core increase cpu node respectively analyze numa architecture scalable model fix model analyze future numa architecture cpu memory node scalable approximate model fix iteration technique fix iteration technique viable approach analyze interrelate sub sub analyze separately remainder simplify manner technique iteratively solves simplify complement sub modifies input parameter sub accordance output sub procedure fix difference successive iteration threshold fold model label tag cpu node tag memory node fold cpu node fold memory node fold model tag memory node fold memory node delay respect sub model propose fix approximate model replace transition label rate assign transition fold model remain derive sub model fix srn model sub model sub model respectively correspond tag cpu memory node fold cpu memory node almost structure dynamic behavior function maintain consistency fold model component fix model sake brevity component introduce mostly subscribed described rate function define fix srn model image KB image fix approximate srn model numa architecture sub model corresponds fold cpu node fold memory node fold model token enables transition transition assigns request issue core fold cpu node fold memory node transition sends request tag memory node transition functionality fold model rate however rate denote affected token sub model compute probability tag memory node available request fold cpu node parameter sub model sub model define probability token token model request fold memory node rate transition fold model similarly multiplicity inhibitor arc define maximum request fold memory node transition model access fold memory node core fold cpu node transition model access core tag cpu node fold model rate redirect request tag cpu node fold memory node depends available memory node fold memory node sub model fix model rate depends token probability token enable transition sub model therefore rate compute denotes probability token probability output sub model input sub model token properly dispatch request fold memory node fold cpu node outgo transition assign probability function identical fold model guard function assign transition sum token sum guard evaluates transition probability respectively otherwise empty unlike fold model guard function assign transition balance token redirect sub model empty disabled due unsatisfied guard accepts token transition immediate transition model output fold memory node fold cpu node whereas transition model output tag memory node fold cpu node guard function associate ensure token fold cpu node exceed rate depends effective rate transition sub model compute probability token rate function finally function identical fold model sub model transition model memory request core tag cpu node fold memory node rate transition affected token sub model output sub model input sub model denotes probability available memory node fold memory node probability compute image KB image import graph sub model fix model image KB image core amd multi socket transition model access tag memory node core tag cpu node transition model access core fold cpu node rate compute probability tag memory node obtain sub model sub model effective rate transition compute probability token transition request tag memory node rate functionality component related function correspond component function introduce sub model token arrival immediate transition token tag memory node transition model token fold memory node guard function associate transition ensures token exceed predefined threshold rate transition depends effective rate transition sub model accord rate denote effective rate compute probability exactly token rate function iterative arbitrary parameter sub model iteration input parameter accord output obtain previous iteration difference MRT memory access successive iteration becomes tolerance bound MRT obtain iteration report import graph parameter sub model fix model memory performance evaluation numa architecture introduces numa machine target application methodology obtain input parameter propose srn model target numa architecture validate propose model core amd numa platform amd opteron processor diagram architecture machine cpu node memory node cpu node contains cpu core llc memory node memory controller cpu memory node amd hypertransport HT amd HT interconnection topology asymmetric node directly consequence interconnection link cpu node memory controller latency dependence hop amd server architecture chosen intel architecture regular  interconnect demonstrate model cope irregular indirect interconnects exhibit latency memory node layout cpu memory node HT interconnect generalize numa amd opteron processor additional resource model generalize numa core compute CU float FPU instruction cache IC cache contention resource core CU cache limited multiple core contend llc resource super linear llc rate address contention profile parameter model target application propose SRNs validate memory intensive performance compute application evaluation FT SP CG BT MG LU NAS parallel benchmark NPB version implementation application EP exclude benchmark generate almost memory access EP compute bound chip cache workload spmv sparse matrix vector multiplication compress sparse csr format pagerank  benchmark workload machine data application domain obtain hardware dependent parameter microbenchmark benchmark generates memory operation configurable stride completely memory bound computation opportunity cache extract model parameter validate propose SRNs memory response core workload execute active core core assign constant amount increase active core contention interconnection network memory node longer workload turnaround define turnaround workload execution assign exclude spent memory access consume memory access compute llc memory response obtain memory response llc obtain amd machine compute srn model compute workload specific model parameter workload execute isolation cpu core local memory node turnaround workload llc obtain hardware performance counter parameter compute target platform additional source resource interference information workload compute amd architecture aforementioned resource workload inter request rate llc per core steady define average per core llc allocate core denote average llc cpu core cpu node compute target architecture cpu node contains core active core due llc interference interference execute workload core cpu node llc average llc core load compute llc average llc interpolate difference llc core interference CUs llc affect compute execution cpu core without interference overhead CU llc interference respectively compute obtain initial profile core accord compute obtain execute computation cache simply memory response turnaround core CU core CUs difference denote capture interference CU average overhead interference resource CU account interference llc computes increase difference llc workload longer cpu increase llc interference capacity llc maximum available core cpu node empirically otherwise sophisticated scheme accuracy however accuracy marginal machine dependent parameter data transfer rate cpu node memory node service rate memory node compute link transfer numa interconnection network benchmark employ discover interconnection link transfer execute cpu core access memory controller transfer service link numerical evaluation model evaluate stochastic petri net package  steady MRT memory access fold fix model compute core tag cpu node obtain apply interconnection link tag memory node monolithic model explicitly tag cpu node hence cpu node interconnection link memory node chosen accuracy model absolute percentage error MAPE actual obtain measurement compute analytic model MAPE popular statistical forecast accuracy error prediction accuracy absolute percent accuracy obtain MAPE MAPE chosen due advantage independence interpretability another popular metric quantifies similarity compute correlation predict actual metric  behavior situation trend actual data linear seemingly despite absolute error visual correlation predict actual srn model multi queue recently propose cho model interconnection link memory controller queue input parameter model along data transfer rate memory interconnection link service rate memory controller microbenchmark link transfer rate fold fix srn model average individual link transfer rate llc rate cpu core target application dependence active core hardware dependent parameter compute described srn model evaluate scenario cpu core memory node architecture specific parameter model  memory  hop local access hop remote access within processor hop remote access processor hop remote access processor application specific parameter model rate req  memory node core scenario performance evaluate memory node increase active core memory intensive application validate srn model heavily congest scenario compute MRT core obtain monolithic fold model data core amd memory controller interconnection link saturate active core consequently MRT increase almost linearly core fold model achieves MAPE significantly accurate queue model MAPE visible  cannot monolithic model core explosion underlie markov chain fix model cannot apply scenario cpu memory node image KB image MRT memory node core MAPE model actual measurement MAPE monolithic model report cannot  core cpu node memory node scenario cpu node active core memory node scenario target application scenario fix model cannot evaluate cpu node increase memory node response memory access decrease demonstrate prominence model estimate MRT memory access previously approach queue approach fails capture trend MRT majority application monolithic fold model consistently capture trend MRT outperform queue model significant margin architecture model queue setup cpu node memory node srn model built multiple cpu memory node underestimate MRT increase memory node srn model clearly outperform queue approach image KB image MRT memory intensive application cpu node memory node plot MAPE model measurement cpu memory node core scenario analyzes performance memory configuration cpu memory node core amd architecture application memory data distribute equally memory node allocate cpu core varied active core distribute cpu node robin fashion core allocate cpu node amd allocation interference LLCs core CUs core allocation symmetry core allocation assume fold fix model image MB image MRT memory intensive application core interference model core amd plot MAPE MAPE monolithic model  cannot model core image KB image MRT memory intensive application core interference model core intel monolithic model cannot core scenario target application monolithic model evaluate due explosion approximate model evaluate configuration core amd srn model model MRT accuracy moderate MAPE MRT relative error prediction error around MAPE BT fold model nevertheless srn model capture trend MRT BT srn model consistently outperform queue approach succeed predict trend memory response model slightly error active core prominent LU application core divergence interference model systematic limitation srn model LU spmv application generate volume memory contention LLCs although model error LU spmv workload FT intra node inference prediction accuracy acceptable approach model intra cpu node interference measurement analysis methodology cache processor render achieve accuracy measurement challenge interference model potentially accurate cache partition however cache partition modification hardware suffers runtime overhead generality approach demonstrate apply core intel xeon numa comprise intel xeon processor core MB llc plot model target NPB application spmv csr format pagerank  algorithm intel core memory node amd core memory node peak MRT significantly intel platform scenario model outperform queue model modest error amd intel strength model approach error intel platform application CG spmv behavior difference interconnection network memory optimization abstract away srn model complexity analysis convergence important concern srn model non zero entry generate markov chain computational complexity directly affect solvability model requirement simplest scenario cpu node monolithic model unable scenario active core non zero entry generate markov chain matrix scenario fold model reduces complexity underlie markov model handle situation core machine nevertheless future architecture cpu memory node explosion affect fold model limitation mitigate scalable fix model render adequate model convergence fix model active core memory node initial input parameter model setting converge iteration described interactive sub model difference  consecutive iteration tolerance bound bound influence iteration model iteration sufficient obtain image KB image markov chain generate propose srn model image KB image convergence propose fix model iteration model future direction applies fix srn model numa demonstrate applicability model predict performance future extension model direction future model scalability fix model helpful attribute user evaluate memory performance numa fix model allows predict MRT increase core cpu node memory node beyond physically available helpful guideline hardware developer future numa plot fix srn model configuration configuration employ core per cpu node memory node data transfer rate obtain core amd machine per core memory access rate memory intensive application perform model insight analyst configuration memory node configuration memory node configuration obtain MRT increase slowly memory node available independent memory node memory saturate growth MRT becomes almost linear model estimate memory saturate adequate core application  cpu resource moreover model adequate memory node application FT core model reveals memory node sufficient MRT core almost identical regardless memory node analysis assist user adequate hardware configuration rent computational resource data interestingly configuration core per node core per node core MRT increase slowly simpler cpu node configuration core per node implies cpu node architecture optimize application equally utilize memory node observation prominent configuration memory configuration saturate relatively core image MB image MRT memory intensive application numa configuration direction future monolithic fold fix SRNs model homogeneous compute core numa memory node although model appropriately model numa evaluate MRT exist limitation srn model employ analytical approach predict speedup simplification complex considers core characteristic model relatively prediction accuracy application spmv LU behavior application propose model properly model complex interaction interference CUs LLCs furthermore connection component srn model model apply memory architecture extend model accurately model interference CUs llc direction future research extend approach heterogeneous compute distribute refer monolithic model assumption physical location memory node transition model access memory node model distribute memory transition replace series arc transition model physically remote memory accessible network processing cpu core model transition accommodate heterogeneous processing core rate transition adjust processing core another future direction research applies model maximize performance parallel application model optimal core maximizes overall performance application another possibility employ propose srn model scheduler scheduler schedule decision model predict performance utilization memory conclusion understand memory performance multi core platform prerequisite perform optimization analytical approach SRNs model evaluate performance memory access numa architecture monolithic model entire intuition understand memory access practical application propose approach approximation model propose cpu core memory node validation model core amd opteron server core intel platform comparison previously publish numa performance model propose srn model accurately evaluate memory performance exist numa machine input parameter model architecture application specific architecture specific parameter data transfer rate interconnection link service rate memory controller synthetic workload application specific parameter llc rate per core obtain profile application propose srn model evaluate performance memory access various numa configuration