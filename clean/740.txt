prior stochastic dual coordinate ascent sdca parallelize multi core environment core communicate memory multi processor distribute memory environment processor communicate message passing propose hybrid sdca framework multi core cluster performance compute environment consists multiple node multiple core memory distribute data across node node solves local asynchronous parallel fashion core local update aggregate via asynchronous across node update scheme propose asynchronous converges global lipschitz continuous loss function linear convergence rate smooth convex loss function extensive empirical comparison algorithm memory faster previous distribute memory datasets GB libsvm repository cannot accommodate node hence cannot parallel algorithm dataset hybrid algorithm achieve duality gap node core significantly faster distribute algorithm cocoa node keywords dual coordinate descent distribute compute optimization introduction immense growth data important efficiently machine advantage performance compute hpc environment multi core setting core communicate memory multi processor distribute memory setting processor communicate passing message supervise formulation vector machine SVMs logistic regression ridge regression others generic regularize risk minimization RRM instance label data label data linear predictor optimize loss function convex respect argument regularization parameter balance loss regularizer instance norm efficient sequential algorithm developed decade stochastic gradient descent sgd alternate direction multiplier ADMM stochastic dual coordinate ascent DCA algorithm widely algorithm efficiently optimizes dual formulation convex conjugate scalar respectively conjugate loss function define gradient respect duality theory optimal dual vector optimal primal dual objective dual variable associate training data stochastic DCA update dual variable maintain primal variable calculate dual variable recently effort undertaken distribute parallel framework distribute DCA algorithm comparable sometimes convergence sgd ADMM distribute algorithm distribute DCA algorithm grouped contains synchronous algorithm random dual variable update processor primal variable synchronize across processor iteration approach incurs communication overhead algorithm avoids communication overhead exploit memory multi core primal variable primary memory across processor speedup obtain asynchronous atomic memory operation instead costly lock memory update nevertheless approach datasets cannot fully accommodate memory challenge asynchronous memory approach data maintain address challenge propose implement hybrid strategy hpc platform collection node interconnect network node contains memory processing core strategy exploit architecture equally distribute data across local memory node core within node compute thread asynchronously update random dual variable associate data allocate node node communicate thread communicate thread designate worker local iteration compute thread worker thread sends local update accumulate local update worker broadcast global update contribute worker however avoid worker ensures consecutive global update local update worker scheme generalization exist approach setup coincides memory multi core setup coincides synchronous algorithm distribute memory adjustment parameter strategy balance computation communication ensure scalability data application image KB image simplify hpc algorithm architecture contribution propose analyze hybrid asynchronous memory asynchronous distribute memory implementation hybrid DCA mostly DCA algorithm guarantee convergence lipschitz continuous loss function linear convergence smooth convex loss function experimental openmp mpi implementation algorithm faster exist distribute memory algorithm easily volume data comparison memory algorithm memory limited related sequential algorithm sgd simplest sgd easy implement converges modest accuracy quickly iteration adjust parameter sdca rate parameter faster convergence rate around modify sgd propose faster convergence switch sdca quickly modest recently variance reduce modification sgd attention modification estimate stochastic gradient correction reduce estimation variance mini batch algorithm propose update dual variable data batch data per iteration mini batch version sgd sdca convergence batch increase sequential algorithm become ineffective datasets distribute algorithm communication scheme dataset decompose independently accumulate partial communication limited utility datasets cannot decompose primal dual relationship fully distribute algorithm DCA later developed processor update update synchronizes across processor cocoa communication computation processor subproblem dual update synchronize primal variable cocoa  framework propose subproblem sdca sequential solver guarantee approximation local processor nevertheless synchronize update primal variable inherent drawback overall algorithm slowest processor processor parallel algorithm multi core memory exploit primal variable maintain memory remove communication however update memory synchronization primitive lock slows computation recent avoid lock exploit asynchronous atomic memory update memory version arbitrarily simultaneous update memory algorithm faster distribute version inherent drawback scalable core processor distribute RRM besides distribute DCA recent distribute version algorithm faster convergence distribute newton disco dane distribute stochastic variance reduce gradient  achieve accurate communication however additional computational overhead disco dane linear expensive dimension  machine load subset data sample training data increase ADMM quasi newton BFGS distribute communication however inherent drawback compute batch gradient computation communication context consensus optimization asynchronous distribute ADMM algorithm directly apply knowledge propose implement analyze hybrid approach exploit hpc architecture approach amalgamation cocoa  distribute framework asynchronous multi core memory solver asynchronous distribute approach algorithm asynchronously update compute asynchronous propose algorithm core algorithm data distribute across node node worker repeatedly solves perturbed dual formulation data partition sends local update worker additionally designate merges local update sends accumulate global update worker subproblem unless global convergence denote index data dual variable reside node denote vector define component otherwise denote matrix consist indexed replace zero ideally dual node replace respectively hence independent node however efficient practical implementation worker communicate vector estimate summarizes global faster convergence worker algorithm solves perturbed local dual henceforth subproblem denotes local incremental update dual variable bound barrier parameter denotes worker update merge global iteration parameter difficulty data partition chosen aggregation parameter local update contribute worker compute global update unlike synchronous reduce approach asynchronous merges local update node global update objective subproblem denominator instead lemma choice asynchronous update core worker node communication worker solves subproblem parallel asynchronous DCA core data partition memory logically subpart subpart exclusively core iteration core chooses random coordinate update direction compute variable optimization svm iterative solver logistic regression local update maintain appropriately coordinate core randomly chosen parallel correspond update independent conflict update correspond allocate core nonzero update lock atomic memory update handle conflict core iteration worker sends accumulate update receives globally update another unless indicates termination image KB image merge update worker update worker compute global update slowest worker avoid bound barrier update subset worker sends global update however due relaxation worker date update worker merge degrade quality global hence convergence divergence ensure sufficient freshness update bound delay worker stale update asynchronous approach benefit overall progress bottleneck slowest processor communication reduce flip convergence image KB image sequence important algorithm dataset data dimension node core core data activity subproblem local iteration rectangular subproblem core core worker randomly dual coordinate correspond data nonzero entry dimension respectively core entry correspond nonzero data dimension computes update respectively finally applies update update core update core update core augment core atomic memory update ensure conflict writes cycle completely local iteration core worker sends responsibility node separately faster worker already update computes global update however update worker update worker subsequent omit communication analysis communication algorithm synchronous update node transmission consist transmission worker worker whereas asynchronous update scheme transmission convergence analysis convergence global compute hybrid algorithm regularizer proof similarly extend regularizers simplify dual formulation correspond subproblem formulation analysis subproblem compute node locally indeed optimum subproblem subproblem convergence global proof respectively significant adjustment proof due modify framework handle cascade asynchronous update analysis focus important local update merge global update merges local update worker optimality local subproblem return parallel asynchronous stochastic DCA solver worker algorithm optimal subproblem definition approximate subproblem approximate optimum proof challenge tackle modification approach solver solves dual subproblem perturbed modification simply handle update core worker modification proof detail worker solves subproblem apply update core update approximate sufficient progress successive update however core multiple atomic memory writes update update core interleave hence demarcate successive update nevertheless upon core data algorithm assign node counter update node index denote data update local update update worker core computes algorithm applies axis however subtle due atomic update firstly core computes coordinate core already modify coordinate effective core compute increment actual memory exist memory denote effective core compute denote actual memory reader notation update proof definition denotes fix vector denotes proximal operator connection operator proximal operator revise satisfy subproblem image KB image relationship approximation denote normalize data matrix local atomic solver omit notation define local atomic solver feature index moreover define minimum global data matrix define definition local atomic dual variable omit proof local atomic solver denotes sequence generate specific local atomic solver denotes actual maintain update local atomic solver indicates index update refers accurate core synchronously update iteration local subproblem denote objective dual subproblem update omit subscript assumption lipschitz continuous global objective lipschitz continuous therefore local subproblems objective lipschitz continuous proposition cite proof proposition expectation dual variable proposition boundary asynchronous variable proposition proposition proposition dual concave function proof dual concave function convexity conjugate function convex strongly convex lipschitz continuous gradient refer assumption atomic update computation update assume update update already assumption bound delay local update assumption restricts maximum local delay lemma assumption definition local subproblem satisfies update local solver iteration core proof omit subscript notation specifies data partition proof induction factor bound revision induction although proof induction hypothesis equivalent statement induction basis proposition GM inequality therefore therefore implies inequality proposition induction induction hypothesis assume firstly implies proposition lemma implies asynchronous update away optimal direction update definition global error bound convex function optimization admits global error bound constant euclidean projection optimal operator define optimization admits relaxed global error bound satisfy constant assumption local subproblem formulation admits global error bound update global error bound local subproblem subproblem solver achieves significant improvement update loss function hinge loss hinge loss global formulation indeed satisfy global error bound local subproblem satisfies global error bound within subset assumption bound lemma convergence subproblem assumption assumption assumption compute successive update local subproblem solver linear convergence rate expectation update data proof omit subscript notation proof bound distance derivation moreover bound increase local objective function therefore assume optimal subproblem denote accord proof lemma local atomic solver linear convergence rate expectation obvious easily induction local atomic solver local atomic solver equation global therefore convergence global although local subproblem solver output approximate cannot directly apply global algorithm update subset worker unlike synchronous reduce update worker handle asynchronous global update handle asynchronous update local subproblem global update compute global convergence customary global objective progress sufficiently sufficient denotes dual variable distribute across worker compute global update simplicity assume worker update algorithm receives global update denotes increment compute worker otherwise update already however algorithm define update already global respectively vector expression vector expression lemma dual primal satisfy proof assume assumption bound delay global update exists lemma global convergence iteration strongly convex assumption assumption assumption assumption satisfied algorithm satisfies proof sake notation instead instead instead instead dual objective summation estimate therefore bound proposition proposition bound rewrite bound substitute proof lemma remark minimal worker update global update communicate fix approach maximal delay slowest worker becomes improvement iteration becomes delay worker fix improvement iteration significant update worker account however independent update combine lemma lemma convergence loss function smooth lipschitz continuous theorem quantity theorem global convergence smooth function loss function smooth iteration algorithm objective optimal whenever furthermore iteration duality gap whenever theorem global convergence lipschitz function loss function lipschitz iteration algorithm duality gap average iterate whenever theorem establishes convergence lipschitz continuous loss function theorem prof linear convergence rate smooth convex loss function experimental implement algorithm mpi openmp conduct  cluster national institute health usa node node GB memory core ghz xeon processor MB secondary cache core cluster hyperthreading enable hyperthreading mode node cluster exactly mpi task correspond worker openmp thread core available within node thread worker handle inter node communication mpi task mpi node schedule slurm task scheduler setting  per node thread per core whereas openmp thread schedule cpu affinity scheduler assign thread physical core datasets evaluate algorithm algorithm binary classification datasets rcv webspam   libsvm website http csie ntu edu cjlin  datasets binary html datasets rcv  training data file directly website data file webspam data file datapoints training remain  file website training scalability algorithm file chose datasets representative scenario data sample rcv feature webspam sample feature  dataset  dense datasets dataset  training    site file GB GB GB GB training feature non zero entry    comparison algorithm algorithm baseline sequential implementation stochastic dual coordinate ascent DCA core node cocoa mpi distribute implementation stochastic DCA multiple node however node core passcode openmp parallel implementation stochastic DCA node however node multiple core hybrid DCA openmp mpi implementation hybrid parallel distribute approach stochastic DCA multiple node node multiple core parameter setting evaluate algorithm hinge loss loss function regularization parameter report parallel distribute algorithm namely passcode cocoa hybrid DCA global parameter denote update dual variable global parameter tradeoff progress dual objective global implementation passcode datapoints dataset whereas cocoa implementation datasets rcv webspam  respectively empirical implementation passcode core thread local update passcode node mpi task local update hybrid DCA node core thread within mpi task local update algorithm update global sequential algorithm baseline local iteration parameter comparison compute performance metric update treat update global aggregation parameter parameter cocoa hybrid DCA recommend hybrid DCA algorithm additional parameter bound barrier bound delay update worker incorporate global maximum delay update worker implementation hybrid DCA treat slightly differently synchronous hybrid DCA update worker merge collective operation mpi allreduce asynchronous hybrid DCA update subset worker merge distribute mpi command algorithm report average optimization performance progress duality gap achieve algorithm relatively datasets rcv webspam  chose node core per node worker core algorithm baseline hybrid DCA parameter bound barrier delay update worker incorporate global however hybrid DCA perform update worker incorporate maximum delay update worker duality gap primal estimate compute global however hybrid DCA worker global workaround temporarily disk stipulate worker compute respective compute duality gap series synchronous reduce computation worker progress duality gap progress global progress across global algorithm perform somewhat equally hybrid DCA slightly progress duality gap update worker winner parallel distribute algorithm cocoa advantage passcode datasets datapoints rcv webspam costly inter node communication complexity per  dataset non zero data passcode perform datasets performance hybrid DCA cocoa passcode balance inter core inter node communication asynchronous hybrid DCA update worker longer synchronous hybrid DCA accuracy quality obtain algorithm datasets libsvm binary repository detail webspam dataset libsvm dataset datapoints training remain datapoints empirical  compute accuracy datapoints classify correctly algorithm plot progress accuracy across algorithm perform somewhat equally hybrid DCA update worker cocoa apparently  margin however algorithm accuracy progress accuracy across explain progress duality gap across explain speedup speedup evaluates improvement performance algorithm function core however definition speedup varies literature passcode computes speedup improvement runtime execute fix whereas cocoa refine notion speedup optimization RRM define computes speedup improvement runtime achieve fix duality gap evaluate algorithm notion speedup denote algorithm node core achieve duality gap respectively formally define notion speedup sufficient algorithm compute algorithm baseline passcode node varied core cocoa core per node cocoa hybrid DCA core node plot separately synchronous hybrid DCA node core plot separately fix node core additionally plot asynchronous hybrid DCA observation speedup proficiency speedup compute differently trend speedup slightly proficiency merge parallel distribute update global reduce quality merge update comparison pure update baseline speedup proficiency algorithm trend performance cocoa perform datasets sample feature passcode perform dataset sample feature hybrid DCA perform furthermore asynchronous hybrid DCA synchronous hybrid DCA image MB image speedup parallel distribute solver respect sequential implementation baseline investigate hybrid DCA expectation overhead openmp maintain parallel thread significant datasets rcv webspam evident performance cocoa hybrid DCA difference overhead maintain openmp thread however overhead relatively comparison overall computation sample feature  dataset however cocoa passcode beyond node core respectively investigation reveal drawback implementation mpi inter node communication firstly mpi inherently thread duration worker hybrid DCA local update merge global update thread worker active thread idle academic research mpi communication multi thread advantage openmp thread available within task research incorporate standard mpi implementation inherent drawback hinder hybrid DCA advantage core available drawback implement asynchronous inter node data transfer hybrid DCA absence collective operation update subset worker standard mpi implementation implement collective operation primitive mpi operation lack performance improvement cocoa utilized optimize mpi collective mpi allreduce mask drawback mpi drew plot speedup proficiency ignore involve mpi communication cocoa advantage ignore mpi performance datasets rcv webspam become totally outperform hybrid DCA sample  dataset performance asynchronous hybrid DCA synchronous hybrid DCA summary theoretically almost speedup fix core irrespective individual performance hybrid DCA core due increase atomic memory update thread increase thread delay bound local update violate assumption aspect atomic memory writes thread investigate around passcode however incorporation fix implementation scope image KB image worker node fix ignore mpi computation speedup proficiency described elsewhere mpi parameter fix node core minority worker contribute duality gap progress smoothly worker contribute achieve duality obtain worker however reduction per eventually achieve duality gap nevertheless later approach useful hpc platform heterogeneous node unlike update worker penalty per specify quickly achieve reasonably duality gap parameter fix node core hpc platform homogeneous node experimentation stale worker variance staleness heterogeneous node workload processing hpc homogeneous node experimental asynchronous hybrid DCA usefulness asynchronous hybrid DCA heterogeneous introduce imbalance setup node core workload independently processing workload instead distribute datapoints equally node load node heavily distribute datapoints ratio similarly heterogeneous processing introduce delay node performance synchronous hybrid DCA asynchronous hybrid DCA imbalanced scenario balance load homogeneous scenario plot progress duality gap across scenario imbalanced load heterogeneous performance synchronous hybrid DCA degrade significantly across across however asynchronous hybrid DCA mitigate imbalance completely performance across scenario across heterogeneous scenario imbalanced load asynchronous hybrid DCA improve performance heavily load node dominate overall performance dataset hybrid algorithm dataset  GB previous algorithm cocoa enormous dataset accommodate node hence passcode dataset global iteration progress duality gap across communication across achieve duality gap node cocoa hybrid DCA node core achieve duality gap approximately fold improvement evidence scalability algorithm argue cocoa core treat core distribute node however mode cocoa cocoa duality gap stipulate cocoa core treat core node perform core initial later moreover outperform hybrid DCA node core conclusion hybrid parallel distribute asynchronous stochastic dual coordinate ascent algorithm utilize hpc platform node multi core memory analyze convergence novel algorithm asynchronous update cascade inter core inter node experimental algorithm faster distribute algorithm parallel algorithm effectiveness approach practical implementation increase combination optimize overhead openmp thread incorporate multi thread implementation mpi operation fix issue delay atomic memory writes thread