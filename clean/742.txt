gpgpu execute multiple task characteristic resource utilization efficient execution resource management policy critical performance factor handle concurrent execution task behavior previous policy assign resource disparate task allocate resource static standalone behavior profile treat task equally cannot efficiently utilize resource standalone profile ignores correlate impact task concurrently hint incorrect task behavior address drawback proposes heterogeneity aware selective bypassing mapping sbm manage compute cache resource multiple task grain manner profile sbm properly characterizes disparate behavior concurrently execute multiple task selectively applies cache management workgroup mapping policy task previous coarse grain policy sbm achieve average performance enhancement grain policy sbm achieve average performance enhancement previous keywords manycore architecture dynamic schedule gpgpu OpenCL heterogeneous application introduction GPGPUs purpose graphic processing emerge throughput processor enable superior performance variety application application massive parallelism initiate enormous thread dispatch execute concurrently processing gpgpu illustrate amd gcn graphic core architecture consists multiple compute private cache cache due gpgpu GPGPUs concurrent execution multiple task leverage task parallelism enhance utilization compute resource hyper nvidia asynchronous compute ace amd dispatch multiple task execution simultaneously exploit massive parallel compute capability compute computation resource computation memory dispatch task parallel task resource execution demonstrate apply improper resource management policy concurrently execute task behavior performance degradation issue mainly mismatch disparate behavior task inefficient resource management policy issue happens gpu parallel task application contains multiple parallel task task consists workgroups specific execution behavior workgroup parallel item execute concurrently organization parallel task terminology convention OpenCL task implement function application workgroups task behave differently various demand memory computation resource although item workgroup behave similarly characteristic workgroups distinct task significantly heterogeneous application refer application multiple task disparate execution behavior resource demand without loss generality concurrent task dispatch application architecture GPGPUs hyper ace illustrates execute multiple workgroups task task matrix multiplication histogram hist adopt amd sdk task hist cache friendly benefit cache task computation intensive compute overlap memory latency computation characteristic cache friendliness computation intensity explain detail task hist allocate compute CUs access privilege memory cache cache memory although GPGPUs concurrent execution workgroups application workgroup mapping policy aware execution behavior memory access workgroup applies heterogeneity aware scheme assigns compute bypass data access cache compute increase throughput computation intensive bypassing data access retrieve data transaction overhead contend cache benefiting cache friendly hist heterogeneity aware scheme enhance performance significantly image KB image diagram compute cache gcn gpgpu architecture heterogeneous application execution model application consists task task consists workgroups workgroup consists item image KB image equally assign computation memory resource task task demand scheme assign computation memory resource accord task characteristic previous propose handle execution resource management concurrent task disparate behavior however prior approach shortage aspect lack insight heterogeneity concurrently execute workgroups focus resource assignment assume workgroups dispatch gpu homogeneous execution behavior tend apply computation cache management scheme workgroups resource allocate proportional heterogeneity aware heterogeneity task within application however resource management scheme identify coarse grain policy equally allocates computation resource task policy coarse grain policy treat concurrently dispatch task equally applies execution resource management scheme task grain policy considers disparate characteristic task allocates resource accordingly author statically profile workgroups task compile account task characteristic impact execute task previous management scheme compute memory resource focus compute resource issue instruction per cycle computation intensive task diverse demand memory resource possibly task address issue heterogeneity aware selective bypassing mapping sbm manage compute cache resource heterogeneous application grain manner contribution summarize perform comprehensive crucial factor data access demand compute correlate performance heterogeneous application within gpgpu reveal insight architectural impact concern heterogeneous application proposes selective bypassing mapping sbm scheme efficiently manage resource usage workgroup assignment policy characteristic task application sbm profile heterogeneous application computation cache memory resource demand concurrently dispatch task sbm applies resource management scheme conduct thorough various heterogeneous application coarse grain approach apply fix resource management scheme memory computation resource cannot properly allocate resource demand task degrade performance demonstrate grain approach conduct static profile without impact concurrently execute task EWM inefficient resource management scheme organize introduces background elaborates propose selective bypassing mapping sbm scheme discus architecture enhancement propose sbm scheme illustrates model heterogeneous application execution performance enhancement discus experimental concludes background software application hardware architecture amd gcn graphic core architecture target platform OpenCL api parallel execution model application initiate task multi dimensional domain within domain item item implement thread illustrate item mapped onto dimensional grid ND workgroup contains multiple item dispatch execution compute gpgpu illustrates reference architecture gpgpu compose multiple compute compute contains simd instruction multiple data processing private memory cache cache compute workgroups mapped compute equally regardless computation intensity accord gcn architecture workgroups memory access privilege access cache memory characteristic application application characterize fundamental namely cache friendliness computation intensity heterogeneity cache friendliness specifies application benefit cache cache friendly application contains task data reuse effectively utilize cache computation intensity reflect computation parallelism application impact overall performance heterogeneity describes application parallel task disparate characteristic characteristic application  cache  application data reuse effectively utilize cache computation  application achieve superior performance expose computation parallelism  application contains workgroups disparate characteristic resource requirement image KB image normalize ipc versus tlp task behavior conv dct ipc normalize maximum ipc task performance normalize instruction per cycle ipc thread parallelism tlp workgroups mapped compute task focus resource management scheme heterogeneous application workgroups disparate execution behavior resource usage thread task execution thread task characteristic management resource heterogeneous application become performance critical non trivial concern properly manage heterogeneous application gpgpu execution resource management scheme aware task application allocate resource computation memory accord disparate requirement task grain manner actual parallel task correlate cache friendliness computation intensity heterogeneity utilization available resource illustrates task namely computation intensive non saturate computation intensive saturate computation unfriendly obtain profile dct conv respectively axis performance ipc normalize maximum ipc task axis indicates tlp thread parallelism task ipc instruction per cycle performance indicator tlp workgroups mapped compute application workgroups thread within workgroup fix task within application workgroups imply tlp task computation intensive non saturate performance ipc enhance increase tlp ipc task increase available compute exploit outstanding workgroups memory instruction thread cache contention task refer computation intensive saturate ipc task increase tlp however maximum ipc saturate allocate workgroups computation ipc cannot enhance mainly due structural contention outstanding task resource task behavior happens structural contention severe degrade task performance apply tlp task refer computation unfriendly task performance saturates tlp structural contention cache contention cache becomes thread compute initiate cache access thrash degrades overall performance computation unfriendly task saturates performance computation intensive saturate task performance saturates sooner task memory instruction computation instruction computation intensity characteristic computation intensity characteristic  computation intensive non  performance increase saturate tlp computation intensive  performance increase saturate tlp computation  performance decrease tlp previous handle heterogeneous application previous propose enhance performance manage execution resource accord task focus homogeneous application execution behavior label gpgpu application tlp thread parallelism aware policy manage access gpgpu cache cpu gpgpu heterogeneous architecture bypassing cache cache unfriendly gpu task enhance performance cache friendly cpu task author aim manage resource categorize application computation data intensive apply coarse grain classification gpgpu application without heterogeneity within gpgpu application aware heterogeneity application however resource management scheme treat heterogeneous application policy resource computation memory access evenly distribute task without disparate characteristic demand resource propose dynamic resource algorithm propose grain context switch mechanism approach allocate computation resource computation intensive task however disparate memory request task account performance aware cache bypassing partition task workgroups task compute computation resource requirement task assume additionally partition scheme mapping task compute beneficial task computation static scheme profile task individually execution scheme manage execution behavior task approach heterogeneity task impact task characteristic impact concurrently execute task feature mostly ignore previous impact concurrently execute task characteristic task application impact task behave differently task alone illustrate task characteristic label task impact another task alone denotes profile task without task denotes profile profile task profile tlp ipc profile cycle detail image KB image profile execution task concurrently alone ipc normalize maximum ipc profile task separately alone alone profile task standalone profile alone alone computation intensive alone alone alone enhance performance ipc alone however task application task equally computation intensive task performance enhancement ipc computation memory resource reflect precise task behavior execute standalone profile introduce inappropriate indication computation intensive task application observation concern perform resource management propose sbm successfully behavior task heterogeneous application attains performance enhancement EWM profile task standalone manner selective bypassing mapping heterogeneous application policy selective bypassing mapping selective bypassing mapping sbm focus management performance critical resource gpgpu namely computation resource memory resource heterogeneous application contains various task sbm identify task heterogeneous application benefit possess extra resource concept sbm apply grain management compute allocation cache access heterogeneous application approach adopt sbm achieve goal selective data bypassing adaptive workgroup mapping selective data bypassing sbm utilizes cache bypassing enhance cache utilization enables efficient data access heterogeneous application unlike previous approach bypass cache cache extend bypassing grain manner dynamically coordinate data access bypass cache illustrates bypassing scheme workgroups task adopt normal access cache data access bypass cache workgroups skip cache workgroups access memory directly cache friendly task normal cache hierarchy task cache unfriendly cache performance gain data access treat bypassing scheme bypassing cache cache unfriendly workgroups avoid potential cache contention cache cache friendly workgroups cache friendly cache unfriendly task execute gpgpu sbm allows cache friendly task utilize cache cache unfriendly task enhance cache utilization performance adaptive workgroup mapping workgroups task compute compute enables flexibility utilize computation resource compute efficiently mapping scheme equally assign workgroups task compute assign workgroups task compute computation intensive task assign compute computation intensive task workgroups task equally assign compute CU CU assign workgroups sbm chooses workgroups mapped compute accord characteristic workgroups execute compute workgroup computation intensive computation unfriendly sbm workgroups computation intensive task computation unfriendly adaptive mapping approach computation resource computation intensive workgroups however execute task compute contention cache degrade overall performance alleviate cache contention cache unfriendly task bypass cache image KB image profile sbm heterogeneous application contains various task apply propose selective approach previous sbm introduces profile identify task benefit possess extra resource profile sbm perform profile phase application execution profile return task heterogeneous application identify architecture configuration task image KB image cache friendliness sbm determines bypassing mapping policy heterogeneous application profile task cache friendly computation intensive unfriendly respectively profile perform application execution profile cache friendliness task algorithm phase architecture configuration task scheme target ipc configuration performance indicator ipc architecture configuration application phase define user guideline task within profile phase accord phase cycle effectively capture inherit application architecture configuration architecture config cache bypassing workgroups task mapped compute bypassing task workgroups mapping bypassing task workgroups mapping bypassing task workgroups mapping bypassing task workgroups mapping MWG multiple workgroup compute cache bypassing   MWG bypass cache task selective data bypassing profile algorithm explore profile scheme resource management scheme apply enhance performance essential algorithm task cache friendly cache resource algorithm task performance improvement cache algorithm profile configuration scheme cache bypassing benefiting application therefore scheme respectively neither algorithm ipc bypassing cache algorithm bypassing cache due accord profile cache friendly task limited data reuse therefore data access already assign cache bypassing cache additional performance benefit concurrently execute task cache cache gpgpu usually policy buffer adverse impact latency bypassing directly memory alleviate buffer fourth bypassing additional cache execute task benefit cache utilization compute computation resource assign task algorithm sbm selectively bypass cache access task sbm algorithm task computation intensive execute instruction per cycle algorithm compose series tune phase phase increase workgroups mapped compute computation intensive task tune phase terminate ipc degrade limit compute workgroup occupancy conclude extensive tune phase approach although comprehensive various heterogeneous application tune slightly adjust heterogeneous application significantly behavior algorithm quantity workgroups assign task compute task benefit dispatch workgroups compute workgroups mapped task compute algorithm sbm task bypass cache cache friendly task task workgroups per compute sbm architecture configuration workgroups heterogeneous application architecture configuration cache bypassing scheme workgroup mapping compute heterogeneous application execute chosen architecture configuration application sbm gpgpu architecture baseline amd gcn architecture concurrent execution task task command queue scheduler workgroups command queue asynchronously compute purpose scheduler maximize computation parallelism greedy manner scheduler gcn unaware execution behavior resource demand considerably dispatch task workgroups mapping sbm complexity smc selective mapping controller introduce assist selective workgroup mapping policy illustrate workgroups command queue ace asynchronous compute gpgpu architecture parse incoming workgroups dispatch compute CUs mapping policy smc effective module facilitate selective mapping policy sbm smc implement sbm register sbm reg task characteristic profile gate smc information architecture configuration information ipc retrieve exist gpgpu performance counter capable counting instruction cycle profile sbm reg programmed architecture configuration target task image KB image selective mapping controller smc implement asynchronous compute ace selectively workgroups task accord occ sbm reg scheme cache bypassing selectively bypass cache access accord sbm reg sbm reg specifies microarchitecture configuration arch cfg specify scheme MWG  cache bypassing scheme  task bypass cache CI occ  configuration computation intensive task CI register occ threshold workgroups computation computation intensive task compute occupancy workgroups default occ assign workgroups task occ workgroup assign computation intensive task workgroups assign computation intensive task workgroups computation unfriendly task characteristic sbm reg selects architecture configuration ace assign workgroups command queue compute configuration sbm task bypass cache hierarchy enable efficient data access utilization cache interconnection CUs memory hierarchy configurable bypassing mechanism already GPGPUs illustrates concept implementation sbm reg signal cache bypassing register false data access around cache hierarchy access memory directly extra hardware enhancement sbm minor gpgpu illustrate smc controller task information dispatcher gpgpu architecture critical task dispatch smc implementation register logic feature application performance indicator ipc obtain exist performance counter gpgpu decision logic smc complexity almost negligible gpgpu wise specification sbm register sbm reg  arch cfg architecture configuration specifies architecture configuration MWG  specifies cache friendly task CI specifies computation intensive task occ compute occupancy threshold occ absolute difference workgroups mapped task compute task   sdk  sdk   sdk   sdk   sdk  cosine  sdk        model heterogeneous execution model execution heterogeneous application implement gpgpu architecture enhancement propose previous heterogeneous application compose various workgroups combine workgroups task behavior workgroups task behave similarly behavior workgroups task workgroups task construct OpenCL code initiate execution concurrently heterogeneous application heterogeneous application therefore contains workgroups behavior task abbreviation task functionality task task source fourth 2D workgroup construct heterogeneous application task identify OpenCL api sake simplicity conduct task task compose workgroups workgroups specific execution behavior propose sbm applies grain resource execution management task characteristic concept extend gpgpu task queue  model gpgpu architecture smc introduce previous model integrate  gpgpu architecture applies amd southern compute CUs gpgpu CU implement KB cache KB local memory CU contains simd concurrent execution task command queue model  execution task OpenCL api extend facilitate identification task application selective mapping controller smc aware workgroups queue dispatch workgroups execution chosen mapping policy although gcn architecture  previous gpgpu generation gpgpu architecture amd vega announce gcn architecture enhancement apply generation gcn deploy faster compute memory bandwidth HBM enable graphic processing packed math various arithmetic precision hardware scheduler gcn architecture allocates resource coarse grain scheme without grain characteristic task disparate demand computation memory resource effective handle disparate behavior concurrently execute task therefore issue exist GPGPUs generation purpose throughput processing experimental heterogeneous application benchmark adopt benchmark suite namely amd sdk rodinia computation task benchmark suite significantly function characteristic task benchmark combine heterogeneous application combination task heterogeneous application disparate execution behavior data access construct heterogeneous application combine various widely task execution behavior application heterogeneous application task sub sub task characteristic profile sbm label CNS CS  denote computation intensive non saturate computation intensive saturate computation unfriendly respectively label denotes task cache friendly implies task cache unfriendly task performance task sensitive data access latency others mainly computation throughput observation characteristic task task characteristic task application impact task behaves differently task alone dct task dct identify computation intensive non saturate cache friendly dct dct characterize computation intensive non saturate without cache friendliness phenomenon fourth specifies architecture configuration chosen sbm label architecture configuration heterogeneous application    CNS   CS      CS    FB   FB     CS FCS   CS   CS               CNS   CNS    FB CNS computation intensive non saturate CS computation intensive saturate  computation unfriendly cache friendly performance evaluation  simulator sbm model comprehensively evaluate performance approach performance sbm exist approach approach assigns heterogeneous workgroups onto evenly compute CUs approach distributes workgroups workgroup queue workgroup queue associate CUs CUs target gpgpu workgroups assign CUs CU CU workgroups assign CUs CU CU approach MWG quantity workgroups task CU without cache bypassing approach CU CUs assign workgroups task MWG apply cache bypassing scheme approach EWM statically profile task individually manages workgroup mapping profile behavior task MWG treat task equally coarse grain policy EWM manage execution task accord grain policy restriction detail discussion MWG EWM propose sbm differs previous approach sbm applies profile properly behavior impact task concurrently resource computation cache manage proportional requirement task grain manner illustrates IPCs approach overall sbm outperforms approach achieves average IPCs MWG EWM respectively application cannot properly handle approach sbm attain IPCs MWG EWM respectively sbm achieves maximum performance enhancement heterogeneous application architecture configuration chosen sbm  bypass task CU cache friendly sbm bypass cache computation intensive architecture computation resource performance ipc chosen architecture configuration approach ipc overhead profile task behavior overhead profile average normalize turnaround ANTT approach ANTT metric multi program workload evaluate performance task within heterogeneous application ANTT task ratio ipc alone ipc  ipc alone ipc task alone ipc  ipc task heterogeneous application overall ANTT summation individual ANTT task ANTT architecture configuration normalize ANTT policy BN ANTT average propose sbm improves ANTT policy MWG EWM respectively MWG allocate resource privilege task heterogeneous application scheme usually hurt performance task heterogeneous application significantly scheme task alone EWM allocate resource profile task however EWM profile task standalone manner ignores impact concurrently execute task indicates improper behavior task image KB image normalize ANTT BN ANTT heterogeneous application profile overhead reflect effectiveness sbm practical gpgpu application limitation management policy sbm properly capture characteristic various combination heterogeneous task profile approach sbm resource requirement aware approach enables accurately capture actual behavior heterogeneous application attain superior performance architecture configuration approach distribute workgroups onto evenly CUs although approach cannot distinguish characteristic task within heterogeneous application issue aggravate task compute within gpgpu another situation cannot identify task bypass cache without hurt performance enhance performance task cache heterogeneous application task cache friendly cache friendly bypassing cache access cache cache friendly without hurt performance computation intensive compute resource utilization resource utilization sbm achieves ipc shorter ANTT MWG task compute access cache hierarchy MWG clarifies alleviate cache contention structural stall task heterogeneous application task cache unfriendly significant cache contention overall performance dct considerably scheme mainly due contention cache access dct scheme chosen sbm  alleviates contention therefore attains performance attains ipc MWG dct sbm preserve turnaround shorter ANTT EWM profile task separately compile approach obtain characteristic heterogeneous application MWG however EWM restriction performs MWG heterogeneous application profile cannot capture task behavior concurrently execute another task EWM mistakenly categorize task equally computation intensive computation intensive computation unfriendly EWM computation task computation intensive task computation performance MWG computation computation intensive task performance EWM issue EWM mistakenly categorize computation intensive computation unfriendly task equally computation intensive EWM assign computation task however performance computation unfriendly task degrade assign computation sbm avoid issue capture characteristic appropriate occupancy threshold register occ sbm reg task sbm attain performance shorter ANTT EWM hist computation unfriendly hist computation intensive adaptability adaptability scalability approach microarchitectures analyze impact compute cache performance respectively ipc normalize MWG ipc microarchitecture task cache impact cache contention dramatic performance improvement increase compute cache contention increase cache contention happens private cache alternatively decrease cache increase cache contention task cache task suffer thrash evict data cache friendly task issue address propose sbm scheme sbm address issue bypassing cache access cache friendly task  policy cache task cache friendly policy considerable performance enhancement achieve sbm cache compute dynamic adaptation sbm tolerates decrease cache analysis profile overhead comprehend task heterogeneous application exist approach usually profile heterogeneous application execution resource management scheme resource management scheme author rotation robin memory request task profile execute heterogeneous application profile execution task profile overhead mainly depends execution task billion gpgpu cycle MWG profile task execution workgroup scheme adopt image KB image performance profile ipc normalize optimal profile ipc accord discussion exist approach profile execution task execution workgroup profile task surely capture execution behavior heterogeneous application considerable profile overhead overhead profile workgroup profile task workgroup execution behavior task properly characterize workgroup data profile representative reflect actual behavior task workgroup induce profile overhead sbm phase execution profile behavior heterogeneous application perform computation sbm analyzes behavior heterogeneous application architectural configuration sbm chooses configuration requirement application profile architectural configuration task gpgpu sbm profile performance dispatch task profile average performance profile loss performance profile cycle cycle profile cycle performance optimal profile however previous research architecture improve sample efficiency profile cycle cycle profile cycle profile cycle cycle cycle profile optimal profile sbm chooses cycle profile properly capture functional behavior heterogeneous application profile cycle user define parameter assumes execution behavior heterogeneous application relatively stable overall execution however capture characteristic task obtain benchmark suite extensive task amd sdk rodinia average execution workgroup cycle profile workgroup sufficient characterize task likely return inappropriate decision perform effectual computation application sbm profile task behavior longer execution minor profile significantly sbm contribute overall performance sbm maximum profile characterize heterogeneous application profile account minor average benchmark execution although overhead sbm profile phase account average benchmark execution practical gpgpu application profile overhead benchmark performance evaluation shorter runtime practical gpgpu application execution practical gpgpu application account gpgpu cycle gpgpu specification profile overall execution worthwhile investment properly chosen architecture configuration boost overall performance significantly architecture configuration tune heterogeneous application beneficial execution conclusion gpgpu heterogeneous application task characteristic resource utilization heterogeneous behavior resource management policy attain efficient execution proposes heterogeneity aware selective bypassing mapping sbm manage compute cache resource heterogeneous application grain manner profile scheme sbm properly characterize disparate behavior concurrently execute task heterogeneous application selectively apply cache management workgroup mapping policy task coarse grain policy MWG propose sbm achieve average performance enhancement respectively grain policy EWM propose sbm achieve average performance enhancement although gpgpu simulator widely performance impact microarchitectural future validate detail impact ppa performance approach rtl implementation