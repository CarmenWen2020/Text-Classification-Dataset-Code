knn scientific domain data analysis building algorithm knn knn dataset focus hybrid cpu gpu approach dimensional knn gpu yield substantial performance gain parallel cpu algorithm utilize queue prioritizes compute data density gpu density cpu thereby advantage architecture relative strength approach HybridKNN effectively augments multi core cpu algorithm propose optimization maximize gpu query throughput assign gpu batch increase workload granularity optimize gpu utilization limit load imbalance cpu gpu architecture HybridKNN gpu parallel cpu reference implementation reference implementation hybrid algorithm performs workload dataset employ promise hybrid algorithm keywords gpgpu heterogeneous memory database query optimization introduction knn outline database focus task scientific data processing workflow within astronomy catalog closest within feature knn application chameleon cluster algorithm consequently knn algorithm target graphic processing gpu knn research thrust literature knn employ dimensional context knn query dimensional closest restaurant feature vector dimensional coordinate location nearby restaurant knn dimensional context image classification image pixel intensity convert feature vector feature knn feature vector dataset option perform brute data yield quadratic complexity another option index data structure prune nearby query refer query dimensionality index data structure perform discern data dimension reduces quadratic complexity brute algorithm dimensional context concern curse dimensionality dimensionality index typically become exhaustive knn query substantial entire dataset index become ineffective degrade performance relative brute index incurs overhead exhaustive dimensional knn development approximate algorithm return nearby query focus knn dimensionality performance dimensional knn limited memory bottleneck however aggregate memory bandwidth gpus roughly magnitude increase memory bandwidth cpu therefore gpus data intensive workload however data transfer gpu bottleneck decrease performance advantage potentially afford gpu additionally data dependent workload knn irregular execution gpu unsuitable algorithm due thread divergence serialization degrades performance gpu performance gain multi core cpu algorithm knn dimensionality despite potential gpu accelerate knn algorithm literature focus optimize brute approach highlight performance dimensional feature compute distance matrix compute distance query distance query depart distance matrix approach focus dimensional knn within dimensionality regime employ index data structure prune potential query dataset context summarize goal address dimensionality gpu abovementioned brute knn dimensional feature clearly gpu independent distance calculation easily exploit gpu massive parallelism gpu significantly outperform parallel cpu approach dimensionality address knn largely domain cpu knn algorithm employ index data structure transform gpu accelerate similarity knn recent propose similarity gpu within distance query index similarity construct knn within distance query within distance leverage efficient gpu similarity algorithm approach concurrent exploitation cpu gpu resource contrast gpu approach cpu gpu assign query architecture respective knn leverage distance similarity described gpu data density parallel cpu knn algorithm processing density knowledge algorithm split knn architecture described contribution primary finding outline propose hybrid cpu gpu approach knn combine distance similarity gpu multi core cpu knn algorithm gpu component HybridKNN algorithm solves knn distance similarity distance gpu likely query queue distribute query cpu gpu queue prioritizes assign query significant computation gpu throughput orient gpu processing quantity query batch achieve peak performance load imbalance cpu gpu propose mitigate load imbalance architecture hybrid approach gpu multi core cpu reference implementation HybridKNN outperforms reference implementation scenario particularly organize background recap leveraged gpu literature hybrid knn optimization evaluates approach finally concludes discus future direction background statement knn outline database dimensional feature vector denote database exclude compute distance euclidean distance denote coordinate dimension respectively knn denote however knn optimization directly applicable datasets summarizes notation throughout multiple notation within report data analytic machine algorithm assume processing entirely memory assumption core disk knn increase therefore typically memory footprint algorithm exceed gpu global memory capacity batch execution across multiple gpu kernel ensure global memory exceed allows algorithm otherwise exceed global memory capacity assume entire dataset within global memory gpu dataset algorithm component summary notation algorithm component HybridKNN  propose cpu gpu approach hybrid  cpu component HybridKNN hybrid  component HybridKNN notation input dataset data dataset dimensionality data distance hybrid gpu dynamically expand initial distance hybrid gpu initial monolithic batch hybrid gpu batch hybrid gpu reserve query hybrid cpu monolithic batch batch hybrid cpu thread assign query hybrid gpu related overview category related hybrid algorithm significant research gpu algorithm application cpu gpu approach however gpu algorithm unsuitable application scenario parallel cpu algorithm outperform gpu instance consequently multi core CPUs gpu achieve peak performance heterogeneous survey hybrid algorithm hybrid cpu gpu algorithm aim maximize resource utilization computer advantage relative strength architecture hybrid algorithm split cpu gpu maximizes resource utilization assigns architecture exploit cpu gpu relative strength split cpu gpu runtime instance parallelize cryo EM 3D reconstruction assign task cpu gpu workload filter image parallelism varies across image gpu assign highly parallel cpu assign remain similarly abovementioned HybridKNN dynamically schedule query onto architecture suitable workload knowledge preliminary hybrid knn knn algorithm hybrid approach comprehensive extends preliminary knn knn fundamental machine algorithm consequently optimize knn discus related knn clarity algorithm capable perform approximate knn typically employ dimensionality address knn algorithm cpu knn prune nearby candidate query algorithm performs bound traversal estimate knn performs backtracking subtrees backtracking ensure approximate ann algorithm efficiently approximate approximate motivate prohibitively expensive dimensional knn related ann library ann  achieves performance parallel randomize  outperforms ann scenario comparison parallel  sequential ann algorithm ann knn algorithm parallelize incorporate HybridKNN effort parallelize knn gpu omit distance matrix approach described focus dimensionality index data structure effective prune knn knn gpu implementation employ index algorithm performs backtracking guarantee query algorithm optimize reduce warp divergence occurs execute instruction necessitate traversal grid index dimensional knn gpu propose query algorithm expands grid ensure approach similarity grid index algorithm query centric approach expands radius contrast avoid query centric approach instead elect execute knn batch query fix distance minimize warp divergence batch execution relaxes constraint query compute gpu reference implementation gpu knn algorithm buffer propose similarly algorithm described buffer algorithm avoids drawback gpu architecture query batch within leaf algorithm delay execution sufficient accumulate buffer access leaf node improves SIMT parallelism algorithm algorithm focus improve coalesce memory access thread within warp access consecutive nearby memory address experimental evaluation hybrid algorithm gpu algorithm denote BufferKDTree categorizes related knn target dimensionality algorithm algorithm index data structure prune brute approach accuracy approximate target architecture cpu gpu hybrid cpu gpu majority moderate dimensionality index scheme whereas dimensional algorithm focus brute approach described efficacy index scheme prune degrades dimensionality categorization knn algorithm literature category dimensionality index brute approximate architecture  moderate     cpu gpu   lowe index technique central approach appropriate index architecture index cpu efficient data aware spatial partition compute input data index quad contrast data oblivious statically partition grid proliferation purpose compute graphic processing gpgpu debate community approach data oblivious gpu disadvantage index index traversal perform instruction reduce parallel efficiency gpu due SIMT architecture consequently abovementioned gpu knn algorithm employ index optimize avoid divergence consensus regard index employ gpu highlight gpu propose optimize reduce thread divergence later research perform traversal cpu perform scan leaf node gpu gpu leveraged regularize instruction yield thread divergence therefore elect non hierarchical index technique gpu component knn algorithm query hybrid approach query distance similarity gpu perform knn operation distance predicate implement query multi core cpu algorithm non materialize grid exploit data distribution efficiently perform similarity distance algorithm outperform LSH LSS algorithm gpu efficient dimensional data leverage optimization gpu effective execute query knn gpu distribute memory knn distribute memory approach propose improve performance knn instance mapreduce implementation knn propose author optimize mapping function prune distance calculation reduces shuffle operation computation author propose approximate knn approximate linear reducer prerequisite achieve scalability contrast computation distribute memory computation gpu application scenario related application scenario knn focus moderate dimensionality knn dimension curse dimensionality prohibit index scheme effective prune knn scenario literature additionally focus approximate approximate dimensionality furthermore reiterate depart literature splitting cpu gpu architecture recap previous HybridKNN leverage distance similarity   evaluate dimension author efficient index scheme batching scheme propose technique reduce duplicate distance calculation approach outperform multi core approach across experimental scenario therefore employ gpu component HybridKNN outline optimization efficiently knn gpu index technique grid index scheme gpu detail index construct host cpu non empty grid index exceed memory capacity gpu index denote series lookup array relevant index query around query perform distance calculation adjacent query query adjacent adjacent grid minor index described previous remove mask array filter dimension mask array useful scenario datasets bimodal distribution dimension negligible impact performance complexity index memory footprint allows datasets gpu grid index technique modify construction batching scheme brief overview gpu batching scheme operation contains within distance gpu global memory capacity datasets batching scheme incrementally query kernel invocation query perform batch execute estimate lightweight kernel yield estimate buffer buffer batch compute batch obviates failure restart strategy waste computation cuda minimum overlap execution kernel data transfer exploit bidirectional pcie bandwidth concurrent computation host workload dimensionality multiple hide host gpu communication whereas workload algorithm bound memory transfer operation HybridKNN optimization splitting architecture focus hybrid cpu gpu approach performs knn cpu gpu similarity within distance query construct knn query facet distance ensure query return distance query however query return within batch query guarantee query principle selection however significant computational overhead dataset entire dataset necessitate significant distance calculation spatially partition query nearby query significant distance calculation compute however query sparse query spatially partition data grid reasonable likely adjacent assume contrast grid effective grid adjacent nearby data aware index partition data sparse furthermore nearby query candidate filter overhead image KB image query assign gpu cpu index strategy gpu proficient processing density non hierarchical grid cpu proficient density index partition interpretation reference legend reader refer web version article illustrative gpu associate index scheme processing scenario due amount filter overhead massive parallelism gpu distance calculation index overhead whereas scenario knn cpu due filter overhead associate data aware index scheme density therefore motivation splitting cpu gpu suitability architecture knn query hybrid knn overview exploit relative strength cpu gpu architecture gpu proficient processing batch query kernel exploit memory bandwidth massive parallelism afford architecture cpu processing irregular instruction index comprise instruction cpu knn component hybrid cpu publicly available ann cpu implementation index algorithm efficient approximate knn execute algorithm obtain ann global variable function conducive memory parallelism obviate limitation parallelize ann mpi query independently rank directly mpi memory avoid explicit communication rank refer multi core cpu approach HybridKNN hybrid cpu gpu component hybrid gpu cpu knn backtracking ensure query likewise LSH cpu algorithm query knn expand radius expand radius whereas expand backtracking expand radius query centric approach beneficial CPUs advantage memory hierarchy benefiting locality traversal unsuitable batch gpu execution transform query distance knn considers throughput orient gpu batch execution allows gpu component hybrid gpu fail overall outline fail query queue hybrid gpu hybrid cpu future dynamically index hybrid gpu increase threshold yield per query assign hybrid gpu guaranteed knn distance execute kernel therefore refrain query centric approach backtracking increase individual gpu increase divergence kernel intra warp load imbalance image KB image knn around query shade denotes distance expand algorithm overview pseudocode HybridKNN overview technical detail algorithm described later outline HybridKNN algorithm obtain rank import dataset occurs mpi implementation gpu rank cpu rank primary execution respectively brevity queue rank simply assigns query gpu cpu rank hybrid gpu rank initializes query failure later declare construct index function algorithm query queue rank loop iterates query compute batch estimator gpu batch compute recall batch estimator computes batch hybrid gpu global memory clarity batch batch query obtain queue algorithm loop batch iteration  execute computes batch operation filter filter reduce duplicate query batch query queue algorithm dynamically index hybrid gpu gpu component assign queue finally rank retrieves batch queue buffer reset regard hybrid cpu query obtain queue rank query loop computes knn batch query batch obtain queue rank loop additional query compute image KB image hybrid gpu kernel refer reader detail minor kernel accommodate HybridKNN query query multiple thread individual gpu kernel algorithm initialize global thread compute query assign thread loop iterates adjacent assign thread adjacent within query query within distance thread computes distance query thread computes hybrid gpu distance input parameter knn hybrid gpu distance analytically derive feasible input data distribution however datasets data distribution analytical approach intractable distance average per therefore derive initial distance hybrid gpu rely execution gpu kernel sample dataset simply sample compute distance denote define bin frequency distance within distance bin width bin sample dataset compute distance distance respective bin distance distance return dataset reasonable compute cumulative bin denote distance bin distance denote within distance denote cumulative bin bin distance denote yield relationship distance average corresponds query distance yield cumulative average within grid grid radius bound adjacent assign queue gpu execute query dense cpu perform knn sparse estimate amount execute repurpose grid index gpu index technique estimate within grid information simply perform scan gpu index non empty grid array within within assign approximation amount compute sort array non increase trace data density immediate around yield estimate amount alternatively adjacent obtain accurate estimate amount compute query however substantial employ procedure outline estimate gpu efficient perform distance calculation density cpu efficient compute density queue illustration array within contrast queue assigns hybrid cpu query decrease assigns hybrid gpu query increase query assign cpu progressively query assign gpu progressively data distribution hybrid gpu compute knn perform hybrid cpu outline queue performance consideration image KB image queue data array within sort non increase hybrid gpu assign amount hybrid cpu assign amount load imbalance performance degrades architecture cpu gpu processing query queue overhead query load balance overhead access queue assign batch query reduces queue overhead independent architecture request compute maintain gpu throughput gpu batch query maintain query throughput execute query gpu  resource contrast cpu suffer limitation queue performance consideration classical load imbalance queue overhead static dynamic schedule loop openmp however scenario classical scenario gpu query batch cpu maintain throughput negatively impact load balance gpu assign batch compute towards computation cpu core idle gpu propose decision queue mitigate load imbalance maintain gpu query throughput hybrid gpu assign batch monolithic batch substantial batch derive hybrid gpu knn assign fails knn queue hybrid gpu expand hybrid cpu future monolithic batch reduce batch factor denote monolithic batch drawback monolithic batch hybrid gpu request query compute starve cpu hybrid cpu consequently implement reserve query cpu compute monolithic batch processing gpu request monolithic batch queue manager determines maximum gpu assign hybrid gpu cpu minimum compute denote reserve hybrid cpu already cpu gpu denote respectively monolithic batch therefore monolithic batch monolithic batch function query already compute reserve query monolithic batch decrease hybrid gpu reverts batch query reserve hybrid cpu gpu utilized gpu cpu computation however substantial query compute despite potentially execute monolithic batch cpu reserve query monolithic batch denote hybrid gpu non monolithic batch hybrid cpu batch hybrid gpu fail knn increase gpu batch query fail knn monolithic batch dynamically index hybrid gpu increase distance previous batch hybrid gpu fail knn assign dynamic approach attempt increase expensive fail query batch index occurs parallel gpu reduce gpu idle due expand finally query knn decrease batch assign cpu gpu respectively batch initial mitigates load imbalance computation illustrates monolithic batch queue assign hybrid gpu batch query assign hybrid cpu initial queue assign hybrid gpu query reserve cpu hybrid gpu query batch query knn query fail knn hence incomplete query partially vertical denote dash solid cpu guaranteed knn query query maximum gpu batch increase substantially halve reserve cpu query decrease query available hybrid gpu compute monolithic batch hybrid gpu reverts batch summarizes algorithm reconfiguration operation execution parameter selection decision dynamically index hybrid gpu query fail previous batch fail query increase whereas algorithm index furthermore gpu batch cpu batch query knn dataset obviate load imbalance computation parameter arbitrarily reasonable decision similarly openmp schedule reduces chunk increase iteration experimental evaluation quantify load imbalance cpu gpu architecture demonstrates selection parameter indeed yield load imbalance load imbalance partially justifies selection parameter summary HybridKNN algorithm reconfiguration operation runtime  increase  fail query previous batch exceeds query assign batch index occurs distance increase reduce fail gpu gpu decrease batch monolithic  batch assign gpu gpu assign batch compute batch assign gpu decrease batch mitigate load imbalance cpu gpu monolithic batch cpu queue gpu gpu batch  batch assign gpu gpu executes batch cpu queue gpu cpu gpu batch query cpu gpu batch decrease respectively mitigate load imbalance cpu gpu cpu gpu image KB image assign monolithic batch query queue hybrid gpu query hybrid cpu hybrid gpu batch initial queue processing monolithic batch query compute hybrid gpu hybrid cpu monolithic batch  cpu reduces monolithic batch processing hybrid gpu monolithic batch gpu index grid index gpu previous similarity leverage index construct host cpu construct index however knn gpu dynamically expand construct index host become bottleneck degrade performance additionally algorithm construct index host reduces resource available cpu component algorithm consequently index described previous construct index gpu index construction gpu faster construct index host preliminary knn extends construct index host reduce performance particularly workload ratio index overhead computation workload index overhead host amortize across entire computation minor impact performance gpu optimize task granularity improve resource utilization parallel algorithm execute cpu gpu strategy reduce task redistribute task thread optimization leverage thread assign dataset thread within assign approach  thread hybrid gpu batch gpu resource underutilized thread per gpu hide memory latency perform context switch resident thread  gpu thread core saturate resource distance calculation multiple thread increase task granularity multiple thread per query query distance query compute adjacent dash outline thread compute distance image KB image multiple thread compute distance assign static thread per query perform distance calculation thread refer denotes thread per advantage approach thread per reduce intra warp thread divergence thread per warp compute distance candidate divergence thread warp executes execution pathway recently   similarity compute distance query candidate thread improves warp execution efficiency however primary motivation thread knn computes batch query dataset consequently thread per query saturate gpu resource drawback thread per increase overhead query density thread thread minimal compute assign thread per assume thread compute distance calculation evenly warp thread eliminates possibility thread assign span multiple warp increase divergence experimental evaluation datasets focus dimensional knn due utility application additionally related consistently gpu accelerate knn outperform cpu approach dimensionality due increase distance calculation gpu unsuitable dimensional knn target dimensionality scenario employ synthetic datasets workload characteristic unif datasets contains uniformly distribute data expo datasets contains exponentially distribute data datasets generate dimension HybridKNN split density cpu gpu respectively unif datasets variation density across data whereas expo datasets data density gradient performance datasets allows examine performance function data distribution workload assignment cpu gpu employ datasets gaia contains astronomical gaia catalog osm contains data data contains gps remove duplicate coordinate otherwise knn consist gps trajectory identical nearly identical coordinate experimental methodology HybridKNN cpu code compile gnu compiler flag gpu code cuda OpenMPI parallelize host task queue performs minimal however parallelize openmp thread assign query hybrid cpu hybrid gpu hybrid gpu without hybrid cpu obtain platform consists nvidia GP gpu GiB global memory intel ghz CPUs physical core hybrid gpu kernel thread per exclude load dataset construct hybrid cpu index initial hybrid gpu index response perform knn cpu gpu index construct hybrid cpu component algorithm response distance hybrid gpu workload queue measurement average trial outline default parameter experimental evaluation initial monolithic batch reserve cpu query increase beyond unlikely greatly improve performance query fail minimize load imbalance assign query per batch increase overhead access queue default parameter experimental evaluation ParameterValue implementation implementation performance evaluation described cpu multi core cpu ann implementation obtains described HybridKNN cpu demonstrate performance gain yield gpu cpu cpu component hybrid algorithm hybrid cpu execute perform knn queue communication ann rank knn independently directly memory recall parallelize ann mpi rank independently construct query batch obtain queue ann perform parallel index construction cannot index exclude index construction HybridKNN hybrid approach hybrid cpu hybrid gpu queue hybrid cpu cpu due exclusion index construction cpu described exclude hybrid cpu index construction initial index construct hybrid gpu however index construction measurement dynamically expand hybrid gpu gpu gpu implementation hybrid gpu gpu component HybridKNN hybrid gpu queue implementation fail data distribution significant knn sparse intend hybrid cpu component HybridKNN implementation comparison purpose standalone knn configure gpu monolithic batch batch compute algorithm ensure query batch respective knn cpu RR evaluate potential negative performance impact queue cpu described another cpu implementation queue remove instead queue simply assign robin fashion rank assign rank mpi rank implementation queue remove therefore rank compute knn equivalent cpu implementation elect robin distribution rank achieve load balance BufferKDTree gpu buffer algorithm HybridKNN depth parameter achieves performance across datasets demonstrate algorithm moderate dimensional examine maintain consistency methodology reporting response compute query phase BufferKDTree allows evaluation maximum algorithm BufferKDTree code publicly available kdtree multi core cpu implementation algorithm configure thread physical core platform similarly BufferKDTree report query response code publicly available scalability cpu implementation cpu reference implementation parallelize version ann cpu component hybrid algorithm hybrid cpu cpu rank independently knn batch query plot scalability cpu across datasets scalability improves data dimensionality gaia dataset achieves speedup whereas Unif6D dataset achieves speedup euclidean distance calculation dimensionality therefore knn dimensional datasets memory bound algorithm spends perform traversal transition become compute bound dimensionality increase Unif2D speedup slightly decrease indicative memory bandwidth saturation processor improve performance trend gaia osm however Unif4D Unif6D speedup achieve indicates memory bandwidth saturate datasets scalability osm dataset surprising datasets gaia Unif2D achieve speedup scalability kdtree implementation multi thread cpu kdtree implementation another baseline comparison contrast cpu parallelize mpi execute kdtree plot speedup thread scalability kdtree cpu thread achieves performance configure kdtree thread implementation impact BufferKDTree height parameter BufferKDTree implementation height parameter achieves examine leaf prune overhead traversal ensure configure BufferKDTree height parameter achieves performance execute BufferKDTree datasets across height parameter height achieves performance across datasets performance behavior consistent experimental evaluation future height image KB image impact BufferKDTree height parameter performance response height plot osm gaia Unif2D Unif4D Unif6D datasets performance achieve across datasets height comparison cache reference percentage cache cpu cpu RR implementation Unif4D dataset percentage cache function highlight bold cpu cache  cache cpu RR cache  RR cache potential impact queue overhead performance utilize queue overhead parallel algorithm access serialize avoid parallel cpu algorithm cpu queue cpu RR queue remove assign rank robin fashion implementation queue implementation without queue ass queue considerable overhead plot response uniformly distribute datasets queue improves performance robin assignment workload instance Unif4D Unif6D datasets implementation queue cpu outperforms robin distribution rank cpu RR however Unif2D dataset cpu RR outperforms cpu workload relatively initial queue overhead non negligible performance impact elaborate overhead access queue non negligible impact contend batch compute overall queue generally positive impact performance performance gain queue robin implementation substantial described unintended benefit queue attribute factor described image KB image response cpu cpu RR uniformly distribute datasets HybridKNN queue construction overhead unif expo datasets response response compute queue workload default parameter  response compute queue workload Unif2D Unif4D Unif6D   Expo6D queue reduce load imbalance rank query retrieve queue demand queue sort spatially likely assign rank queue spatially likely traversal benefiting locality perform knn contrast robin assignment rank cannot benefit spatial location notion cache improve performance cpu cpu RR described simply performance analysis linux perf cache reference cache reference cache flag perf yield coarse grain measurement cache reference program limited knn however vast majority computation perform knn traversal filter algorithm perf reasonable understand locality responsible performance difference cpu cpu RR implementation cache reference percentage cache execute cpu cpu RR implementation Unif4D dataset percentage cache increase cpu RR implementation whereas percentage decrease cpu implementation impact locality cpu likely increase probability reuse data cache contrast cpu RR implementation query robin fashion cannot exploit locality consecutive therefore increase percentage cache increase cpu cpu RR queue significant overhead cpu algorithm exception Unif2D dataset therefore hinder HybridKNN furthermore assign batch query queue positive locality performance gain due positive cache outweighs performance loss due queue access overhead overhead queue construction selection distance quantifies response overhead unif expo datasets execute HybridKNN default parameter compute queue workload dependent data dimensionality data distribution uniform exponentially distribute datasets response overhead percentage response compute Unif2D Unif6D percentage queue workload Unif2D Unif6D overhead associate construct queue mostly amortize workload however non negligible workload despite queue construction overhead queue generally advantageous due positive cache gpu kernel task granularity hybrid gpu thread batch assign queue hybrid gpu due decrease monolithic batch important sufficient thread execute gpu resource remain saturate achieve oversubscription additionally thread per query allows divergence warp query execution pathway assign warp response HybridKNN selection datasets thread assign perform distance calculation query thread evenly warp thread ensures query span warp datasets consistent response instance gaia response contrast Unif6D dataset response inter warp load imbalance warp execution increase divergence kernel extreme warp compute query  resource instance candidate within adjacent query thread execute across datasets yield performance additionally yield performance achieves performance perform consequently configure HybridKNN default response HybridKNN gaia osm Unif4D Unif6D datasets default parameter response bold dataset dataset gaia osm Unif4D Unif6D gaia osm Unif4D Unif6D gaia osm Unif4D Unif6D queue performance gpu monolithic batch examine performance function selection monolithic batch selection monolithic batch performance implication decrease query hybrid gpu successfully compute batch average per decrease gpu throughput gpu resource sufficiently saturate examine performance impact monolithic batch reserve query cpu allows performance across gpu assign entire dataset batch starve cpu plot response Unif2D Unif4D Unif6D datasets respectively exponential datasets clarity uniform exponential datasets dimensionality adjacent otherwise gpu resource fully utilized initial decrease response however Unif2D decrease performance pronounce Unif6D dataset hybrid gpu configure monolithic batch compute workload however otherwise cpu starve explains performance degradation Unif6D exponential datasets omit discussion image KB image HybridKNN response monolithic batch default parameter achieve performance response hence achieve compromise dimensionality dimensionality workload queue performance load balance configuration queue mitigate load imbalance cpu gpu component load imbalance define execute hybrid cpu rank computation hybrid gpu rank compute batch response load imbalance unif expo datasets across datasets load imbalance generally increase increase compute batch increase enhances cpu gpu component HybridKNN disparate HybridKNN achieves reasonably load balance despite confound issue mitigate load imbalance batch adjust instance batch employ optimization additional parameter function unlikely substantial performance gain load imbalance already within acceptable quantify fail query described gpu component assign queue hybrid gpu fail query batch decision avoid dynamically increase radius inside kernel examine fail query datasets fail query compute fail query attempt query fail query observation regard fail query described clearly fail query waste compute gpu query waste sufficiently knn query eliminate waste however expense waste another context query significantly increase overhead refinement thereby increase distance calculation fail query likely indicates fail query waste refining candidate average query osm dataset nearly query indicates dataset likely performance algorithm refining candidate elaborate observation fail hybrid gpu query execute HybridKNN calculate fail query attempt query default parameter dataset gaia osm Unif2D Unif4D Unif6D   Expo6D implementation index knn cpu kdtree reference implementation suffer refinement overhead described index cannot guarantee therefore shortcoming hybrid gpu consequence index performance impact initial selection distance described initial distance average query examine sensitive performance HybridKNN variation parameter plot response HybridKNN factor initial plot corresponds initial default initial distance factor gaia Unif2D  datasets achieve slightly performance default however performance osm degrades significantly additionally osm performance achieve overall initial achieves performance across datasets semi analytic geometric justification selection outline osm dataset nearly fail query osm achieves performance default demonstrates nearly fail query indicator yield additional distance calculation associate overhead demonstrates moderate query failure beneficial performance image KB image examine sensitivity HybridKNN initial selection datasets execute HybridKNN factor initial radius corresponds default default parameter temporal evolution index batch batch hybrid gpu component HybridKNN fail query fail query previous batch exceeds query batch algorithm index gpu component assign queue query compute gpu response spent index across datasets various parenthesis hybrid gpu indexed hybrid gpu sometimes index osm index  median algorithm index furthermore index account response majority response spent index demonstrates index overhead significantly degrade gpu query throughput plot query assign hybrid gpu batch Unif4D  vertical correspond batch trigger recomputing index temporal evolution HybridKNN workload assign gpu batch decrease ensure load imbalance occurs cpu gpu component algorithm response spent index hybrid gpu expand various parenthesis hybrid gpu index default parameter dataset gaia osm Unif2D Unif4D Unif6D   Expo6D image KB image hybrid gpu batch batch Unif4D  vertical correspond batch trigger index dataset image KB image query remain compute batch execute gpu Unif6D Expo6D datasets Unif6D dataset compute batch whereas Expo6D dataset batch knn vertical batch denotes compute Expo6D dataset batch characterization hybrid gpu distance expansion recall hybrid gpu expands compute batch query examine performance hybrid gpu expansion gpu gpu implementation comparison purpose standalone knn elaborate  gpu hybrid gpu standalone knn algorithm execute gpu Unif6D Expo6D datasets plot query remain batch successive batch query compute decrease successfully query dependent distance data distribution batch batch query compute Unif6D dataset batch knn query dataset however Expo6D dataset batch compute knn dataset however Expo6D dataset knn dataset compute batch denote vertical remain data compute additional batch constrain batch exclude remain data execute additional batch query remain compute Expo6D reasonable comparison batch compute knn Unif6D dataset compute Expo6D dataset image KB image response HybridKNN cpu gpu BufferKDTree kdtree uniformly exponentially distribute datasets HybridKNN configure default parameter plot gpu response implementation due explanation data uniformly distribute selection initial knn data expansion contrast data exponentially distribute average largely unsuitable data sparse gpu algorithm continually expand sparse prohibitive expansion degrades performance remedy gpu implementation non linear expansion employ avoid exhibit recall expand hybrid gpu fails knn query previous batch alternative brute employ dataset knn similarly Expo6D data respective knn batch data respective knn brute prohibitive performance however reiterate hybrid gpu density hybrid cpu therefore implementation execute HybridKNN plot response unif datasets expo datasets despite performance drawback gpu implementation expo datasets described uniformly distribute datasets gpu implementation efficient gpu slightly outperforms HybridKNN across Unif2D dataset dataset contains workload slight overhead load imbalance architecture observable dataset contrast Unif6D dataset workload HybridKNN outperforms gpu comparison cpu kdtree performance standalone multi core cpu implementation cpu kdtree recall cpu parallelize mpi kdtree parallelize thread synthetic datasets performance cpu degrades gracefully increase contrast performance kdtree degrades significantly datasets Unif2D  plot response datasets datasets kdtree fail execute gaia osm kdtree outperforms cpu whereas cpu outperforms kdtree performance kdtree response kdtree increase significantly clearly performance difference cpu kdtree plot speedup cpu kdtree datasets cpu efficient Unif2D   datasets additionally multi core cpu reference implementation performance characteristic comprehensive comparison HybridKNN comparison HybridKNN cpu kdtree performance HybridKNN multi core cpu implementation cpu kdtree across datasets HybridKNN outperforms cpu demonstrates cpu gpu hybrid algorithm degrade performance HybridKNN kdtree kdtree outperforms HybridKNN Unif2D  kdtree slightly outperforms HybridKNN overall HybridKNN yield reasonable performance gain kdtree gaia dataset HybridKNN outperforms cpu across osm dataset HybridKNN cpu nearly performance however HybridKNN outperforms cpu kdtree outperforms HybridKNN gaia osm gaia osm kdtree outperforms cpu whereas cpu performs behavior synthetic datasets suggests HybridKNN equip algorithm kdtree instead ann cpu implementation beyond scope algorithm selection function input parameter research direction hybrid cpu gpu algorithm plot speedup HybridKNN cpu datasets plot demonstrates performance advantage HybridKNN dimensionality increase datasets increase query hybrid gpu increase datasets compute hybrid gpu generally hybrid gpu compute query amount denser data gaia osm expo datasets definition unif datasets constant density across data comparison HybridKNN BufferKDTree plot speedup HybridKNN gpu algorithm BufferKDTree datasets correspond synthetic datasets respectively synthetic datasets HybridKNN achieves speedup BufferKDTree however BufferKDTree outperforms HybridKNN Unif2D Unif4D datasets outperforms HybridKNN Unif6D overall HybridKNN significantly outperforms BufferKDTree datasets gaia osm performance gain BufferKDTree pronounce synthetic datasets gaia dataset speedup osm dataset speedup therefore osm HybridKNN achieves slowdown relative BufferKDTree discussion conclusion gpu knn address dimensionality advance hybrid approach dimensionality exploit relative strength cpu gpu architecture gpu knn algorithm likely achieve significant performance gain dimensionality due highly efficient cpu algorithm ann throughput orient gpu latency cpu strategy assigns batch gpu maintain throughput cpu rank assign chunk largely mitigate load imbalance starvation reduce batch assign gpu query reserve query cpu queue allows advance gpu cpu algorithm substitute framework improve performance broadly queue technique address cpu gpu algorithm data dependent performance characteristic HybridKNN yield reasonable performance gain reference implementation speedup parallel cpu approach cpu however clearly speedup HybridKNN potentially dimensionality scenario examine similarly HybridKNN outperforms gpu reference implementation BufferKDTree scenario overall observation hybrid algorithm performance cpu gpu algorithm largely varies due input parameter data challenge algorithm outperform achieve comparable performance cpu gpu reference implementation experimental scenario HybridKNN yield slowdown reference implementation kdtree implementation knn algorithm performance niche hybrid algorithm developed algorithm selection function parameter data dimensionality data distribution hybrid algorithm achieve performance wider scenario research direction future recent trend computer architecture gpus cluster compute node summit supercomputer  ridge national laboratory contains nvidia volta gpus queue propose distribute multiple gpus within node furthermore queue workload data distribution apply spatial algorithm data analysis similarity DBSCAN cluster queue achieve load balance cpu gpu therefore achieve load balance multiple gpus queue reconfigured incorporate gpu consumer interestingly node summit cpu hybrid algorithm become important computational throughput gpus CPUs another partition input dataset across global memory multiple gpus enable datasets interconnects NVLink enable gpu gpu communication obviate memory access orchestrate host