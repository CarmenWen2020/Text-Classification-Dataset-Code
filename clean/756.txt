parallel computation irregular memory access limited memory subsystem multi core CPUs pinpoint quantify performance bottleneck precisely estimate volume data traffic irregular parallel computation multi core CPUs memory hierarchy private cache performance model estimate applies bandwidth limited computation standard algorithm sparse matrix vector multiplication widely irregular kernel multi core cpu matrix induce irregular memory access demonstrate cache simulation combine propose performance model accurately quantifies performance bottleneck detect standard estimate data traffic volume previous keywords cache simulation performance model sparse matrix vector multiplication intel xeon amd epyc introduction performance priority scientific computation meticulous devote optimise underlie code optimisation effort performance model valuable attention towards pressure optimisation expend effort unproductive instance popular roofline model bound performance cpu peak computational capacity memory bandwidth algorithm computational intensity CPUs hierarchical memory bandwidth computational intensity memory hierarchy moreover computational intensity depends parameter cache memory access computation recently elaborate performance model developed stencil code evaluate effectiveness spatial temporal optimisation amount data transfer memory hierarchy advance memory access predictable stencil unfortunately irregular computation memory access data runtime irregular access typical approach derive estimate memory traffic scenario pencil estimate advantage cheap implementation actual machine estimate crude reality data traffic volume thereby render understand actual performance achieve estimate sparse matrix vector multiplication spmv widely computational kernel suffers irregularity computational intensity due considerable difference data traffic estimate cannot confidence evaluate performance kernel implementation accurate estimation data traffic volume performance validation numerous computational kernel issue due irregular memory access arise sparse data structure graph unstructured mesh quantify amount data transfer multi core cpu memory hierarchy irregular computation estimate data traffic volume trace driven cache simulation relies assumption simplify model memory hierarchy moreover applies memory hierarchy cache feature contemporary multi core CPUs address exist analytical cache model propose trace sequence memory reference amount computation likely execute kernel however remains applicable actual machine available data traffic cannot quantify directly hardware monitoring facility facility unavailable unreliable easily interpret image KB image data traffic volume sparse matrix vector multiplication data traffic estimate standard algorithm matrix SuiteSparse matrix collection comparison actual data traffic volume cache KiB data cache byte cache intel sandy bridge xeon cpu detail measurement obtain gap data traffic volume sparse matrix actual volume data traffic depends cpu cache sparsity matrix importance familiarity irregular computational kernel spmv demonstrate cache simulation accurately quantifies volume data transfer memory hierarchy intel multi core cpu data transfer volume accurate performance prediction unavailable estimate performance prediction amd epyc cpu explore limitation propose variant spmv irregular irregular writes ultimately prediction quantitative understand spmv performance performance implementation expectation implementation hidden performance issue remainder organise cache simulation approach estimate data traffic volume computation irregular memory access performance model bandwidth limited computation relevant data traffic volume realistic memory cpu cache bandwidth recall standard spmv algorithm matrix compress sparse csr coordinate coo storage format review bound volume data traffic generate csr spmv algorithm later cache simulation validate estimate data traffic volume performance model spmv algorithm finally briefly discus related conclusion quantify data traffic irregular parallel computation estimate data traffic volume computation multi core cpu sequence load operation perform participate CPUs simulate cache behaviour simplify model establish ideal cache model ordinarily cache oblivious algorithm depart ideal cache model practical assume recently replacement policy instead ideal cache model optimal policy  algorithm extend cache model incorporate multi memory hierarchy cache multiple processor simplify cache model sequential computation memory hierarchy consist memory cpu cache specifically cache partition cache cache data memory cache contiguous cache transfer originate load operation issue cpu furthermore assumption regard behaviour cache assume cache operates demand cache cache cache cpu issue load reference memory address correspond cache assumption precludes hardware prefetching feature multi core CPUs speculatively fetch data request however hardware prefetching complicate cache simulation likely specific hardware cache assume fully associative meaning cache cache disregard conflict cache due cache mapped associative mapped cache mapping cache hardware specific document poorly moreover overall conflict likely impact specific memory access assume cache evict accord recently policy whenever cache policy easily simulated approximate hardware accurately purpose volume data traffic cache memory deduce counting transfer cache sequence load procedure described summarise algorithm throughout cache simulation maintain recently entry per cache cache occupy cache cache evict cache incoming cache empty thereby cache relevant data prior computation load reference memory address correspond memory indexed reference cache recently otherwise cache occurs cache increase recently entry remove finally reference cache cache entry insert recently image KB image multi hierarchy private cache memory hierarchy multiple cache cpu memory memory hierarchy whereas memory cache cpu refer cache conversely cache farther cpu cache furthermore assume cache inclusive cache cache cache cache data cache cache assumption cache treat individually denote cache transfer memory hierarchy obtain correspond cache recently entry algorithm parallel computation multiple CPUs cpu private cache private cache belonging processor denote cache cache respectively handle apply algorithm cache individually however load issue processor cache correspond private cache parallel computation multiple CPUs cache generally cpu issue load instruction independently others therefore cache receives data request affected factor schedule thread onto CPUs moreover CPUs retrieve data memory hierarchy proceed memory access latency request depends memory hierarchy data ultimately data request cache cache identical program due competition CPUs however propose approximate data request cache assume request submit processor robin fashion cpu issue load operation interleave data request cpu cache cache cpu obtain interleave sequence load apply algorithm modification estimate cache maintain counter cpu incremented whenever cache load issue processor cache attribute data traffic individual CPUs important data traffic estimate overall performance finally non uniform memory access numa architecture distinguish multiple memory controller channel data response cache memory address partition numa domain accord memory policy counter cache numa domain limitation already mention limitation propose cache simulation instance ignore conflict hardware prefetching cache assume robin schedule request participate CPUs addition propose treat load ignores additional data transfer maintain cache coherence multi core CPUs private cache typically data transfer relevant cache transfer cache memory operation memory cpu modify cache cache eventually memory however propose cache simulation account traffic modify cache additional traffic arise related cache coherency false kernel performance significantly impact mention approach extend improve performance model data traffic bandwidth performance model computation limited cache memory bandwidth execution relevant data traffic volume inside memory hierarchy model assume computation memory access overlap moreover dominant due memory access computation neglect addition data transfer adjacent memory hierarchy assume transfer parallel cache multi hierarchy recall cache cache processor attach cache data traffic volume transfer cache memory hierarchy account processor furthermore denote sustainable core bandwidth transfer data denote aggregate bandwidth available cpu core cache although hardware vendor occasionally specify theoretical bandwidth rarely achievable instead realistic sustainable bandwidth typically benchmark programme performance limited due data traffic induced individual cpu core bound execution per core data transfer float operation perform performance float operation per bound execution translates equivalent upper bound performance whenever resource memory hierarchy multiple cpu core performance limited due data traffic volume limited aggregate bandwidth data transfer memory cache aggregate bandwidth linearly cpu core instead saturates core moreover multi core CPUs numa architecture memory partition numa domain bandwidth numa domain limited account cpu core affinity local numa domain significant amount data transfer cpu core remote numa domain bandwidth transfer tends severely limited sparse matrix vector multiplication multiplication sparse matrix dense vector spmv prime irregular parallel computation fundamental computational kernel numerous scientific application spmv perform repeatedly iterative sparse linear krylov subspace efficiency hinge spmv computation iteration spmv computational intensity therefore performance peak computational capacity precisely non zero sparse matrix multiplication addition typically load float numerical integer index location matrix non zero explicitly float indirectly indexed index matrix non zero indirect induce highly irregular memory access prevents data reuse cache performance typically limited data retrieve cpu cache memory peak float capacity relationship location matrix non zero sparsity amount data memory hierarchy characterise matrix obvious memory hierarchy constitutes performance bottleneck vast literature devote optimise spmv performance code optimisation optimisation reduce volume data traffic register cache matrix ordering compression addition advanced spmv algorithm propose standard compress sparse format variant thereof sparse matrix format furthermore sparse matrix partition scheme improve parallel efficiency spmv load balance reduce communication although recently considerable effort optimise spmv gpus focus attention multi core CPUs relevant important however propose optimisation storage format sparse matrix instead aim understand performance irregular bandwidth limited computation spmv algorithm compress sparse compress sparse csr standard storage format sparse matrix non zero matrix entry location explicitly sparse matrix non zero distinct index non zero ascend accord index matrix csr format non zero index pointer pointer index non zero respectively non zero belongs source vector destination vector spmv define strategy compute spmv parallel partition matrix distribute multiple processor thread processor computes destination vector accord partition non zero index destination vector partition along contrast source vector processor extent depends index code algorithm straightforward implementation memory parallel spmv matrix csr format openmp  outer loop distribute thread schedule clause omp compiler directive specifies loop chunk consist chunk consecutive loop iteration chunk assign thread robin fashion unless otherwise omit chunk implies thread assign chunk image KB image coordinate storage format coordinate coo format another scheme sparse matrix commonly spmv due performance coo compress index memory footprint memory traffic spmv calculation csr however coo spmv algorithm feature irregular writes csr spmv therefore coo spmv additional irregular kernel investigate accuracy propose cache simulation sparse matrix non zero coo format array index index non zero non zero sort index source vector destination vector spmv compute code algorithm naive implementation memory parallel spmv matrix coo format avoid due irregular writes omp atomic directive within loop although substantial synchronisation overhead impose atomic writes efficient implement coo spmv avoid overhead deliberately version irregular writes array image KB image bound data traffic csr spmv recall bound cache sequential version csr spmv algorithm sparse matrix non zero csr format furthermore assume pointer index byte integer non zero byte precision float define matrix memory footprint csr format byte moreover combine matrix vector cache incur algorithm assume sake simplicity array align cache boundary otherwise additional cache array suppose cache relevant data application spmv iterative solver subsequent iteration exceeds cache consideration compulsory data cache whenever access consequently bound cache coincides cache occupy cache bound attain conflict data evict due associative mapped cache otherwise reuse capacity sparsity matrix perfect reuse source vector addition pointer destination vector remain cache iteration outer loop obtain reasonable upper bound disregard conflict assume source vector suffers capacity equivalent cache fully associative matrix pointer destination vector suffer capacity access source vector incurs cache cache bound estimate rough actual data traffic volume accurate estimate sparsity matrix account bound previously literature various source vector cache irregular access conflict due mapped associative cache register optimisation apply tight bound cache sequential spmv algorithm sparse matrix asymptotically optimal algorithm cache aware cache oblivious sort attains bound survey regard sequential parallel spmv author movement data memory hierarchy parallel processor across network approach estimate data traffic volume spmv described   probabilistic model however model primarily concerned cache conflict mapped cache assumes matrix non zero uniformly distribute rarely data traffic estimate cache simulation apply cache simulation obtain alternative estimate cache csr spmv algorithm apply algorithm sequence load issue algorithm load matrix non zero respectively inner loop performs load fetch non zero index source vector outer loop performs load pointer load destination vector multi memory hierarchy cache handle described processor assume processor assign consecutive crucially quantify data traffic volume spmv estimate inherently sparsity matrix estimate accurately capture volume data traffic generate spmv computation estimate moreover incorporates cache therefore estimate memory hierarchy estimate subsequently diagnose performance bottleneck identify previous estimate finally procedure similarly apply coo spmv kernel algorithm spmv algorithm irregular kernel deduce relevant sequence load operation correspond memory address computation numerical accuracy data traffic estimate obtain cache simulation described focus csr spmv kernel algorithm data traffic estimate csr spmv evaluate performance model finally evaluate data traffic estimate coo spmv kernel algorithm sparse matrix data consists matrix SuiteSparse matrix collection sparse matrix variety application matrix mainly chosen exhibit variety sparsity spmv exceeds cache addition matrix cardiac  simulation previously matrix derive finite volume tetrahedral mesh matrix mesh matrix non zero sparsity matrix display statistic sparsity matrix worth matrix spal gld stackoverflow lynx particularly irregular sparsity matrix SuiteSparse matrix collection SuiteSparse matrix sort accord   zero per  max TSOPF RS MM spal MM RMR MM relat MM HVR MM gld MM stackoverflow MM fullchip MM freescale MM circuitm MM hardesty MM  MM lynx  MM matrix SuiteSparse matrix collection image KB image sparsity lynx lynx reorder matrix shade location non zero matrix entry matrix non zero sparsity experimental setup multi core cpu dual socket intel sandy bridge xeon dual socket intel skylake xeon platinum dual socket amd epyc processor core instruction cache data cache unified cache instruction data addition cache instruction data intel processor cache socket whereas amd epyc MiB cache processor core refer compute complex similarly numa domain socket intel whereas numa domain amd epyc associate compute complex processor core cache cache byte characteristic hardware summarise recall memory hierarchy cache inclusive cache cache cache cache cache inclusive cache sandy bridge skylake turbo boost disabled intel  acpi  performance driver performance governor consequence cpu frequency capped ghz sandy bridge ghz skylake ghz epyc respectively furthermore thread pin cpu core memory access thread initialise policy correspond memory numa domain closest thread multi core cpu associativity cache memory configuration theoretical memory bandwidth sandy  CPUs intel xeon intel xeon platinum amd epyc cpu core cache per core KiB KiB KiB cache per core KiB KiB KiB cache per core KiB KiB KiB memory channel per socket mhz mhz mhz memory bandwidth GB GB GB moreover ubuntu lts linux kernel version code compile gcc compiler flag  march native initial verification spmv kernel algorithm serial performance sandy bridge closely reference implementation intel math kernel library mkl addition performance implementation sandy bridge epyc compiler matrix intel compiler attains slight performance increase performance slightly degrades matrix matrix irregular difference performance negligible widely freely available gcc remain serial performance gflop csr spmv kernel performance reference implementation sandy bridge function mkl   intel math kernel library mkl remain performance implementation compile gcc ICC sandy bridge epyc   intel mkl  ICC  ICC TSOPF RS spal RMR relat HVR gld stackoverflow fullchip freescale circuitm hardesty lynx lynx reorder estimate data traffic csr spmv matrix described estimate cache incur processor cache respectively cache correspond estimate interleave load instruction issue processor described moreover obtain estimate cache numa domain cache byte cache estimate cache per core deduce correspond volume data traffic focus data traffic volume cache validate estimate data traffic volume data traffic measurement intel multi core CPUs unfortunately currently available hardware performance monitoring linux  hardware performance directly data traffic throughout memory hierarchy amd epyc processor therefore focus intel although amd epyc evaluate performance csr spmv kernel measurement actual data traffic volume obtain performance monitoring  hardware configure report various hardware performance access appropriate encoding perf return file descriptor hardware performance counter library  translate encoding moreover guideline appropriate hardware performance data traffic measurement microbenchmarks correlate hardware data transfer memory hierarchy chosen procedure hardware performance processor core cache specify cache due load hardware prefetches convention library  cache   dcache load  dcache load   response data rfo snp  response data rfo local remote hop dram snp estimate data traffic MiB intel sandy bridge xeon spmv algorithm  dram meas   meas   meas   core TSOPF RS spal RMR relat HVR gld stackoverflow fullchip freescale circuitm hardesty lynx lynx reorder socket TSOPF RS spal RMR relat HVR gld stackoverflow fullchip freescale circuitm hardesty lynx lynx reorder dual socket TSOPF RS spal RMR relat HVR gld stackoverflow fullchip freescale circuitm hardesty lynx lynx reorder matrix compute spmv parallel algorithm thread hardware performance cache per core cache data traffic volume compute estimate data traffic volume csr spmv core socket socket sandy bridge skylake respectively estimate data traffic volume core socket amd epyc refer data traffic volume cache cache traffic remain hierarchy overall correspondence estimate data traffic contribution data traffic capture simulation scheme simplify model memory hierarchy estimate within percent data traffic volume notable exception estimate data traffic MiB intel skylake xeon platinum spmv algorithm  dram meas   meas   meas   core TSOPF RS spal RMR relat HVR gld stackoverflow fullchip freescale circuitm hardesty lynx lynx reorder socket TSOPF RS spal RMR relat HVR gld stackoverflow fullchip freescale circuitm hardesty lynx lynx reorder dual socket TSOPF RS spal RMR relat HVR gld stackoverflow fullchip freescale circuitm hardesty lynx lynx reorder estimate data traffic MiB amd epyc spmv algorithm measurement data traffic epyc due lack reliable hardware performance data traffic throughout memory hierarchy    dram MiB core TSOPF RS spal RMR relat HVR gld stackoverflow fullchip freescale circuitm hardesty lynx lynx reorder socket TSOPF RS spal RMR relat HVR gld stackoverflow fullchip freescale circuitm hardesty lynx lynx reorder irregular matrix spal gld lynx stackoverflow underestimate traffic sandy bridge skylake traffic sandy bridge underestimated spal lynx discrepancy likely conflict due cache associative fully associative assume simplify model memory hierarchy estimate traffic skylake accurate probably due increase cache associativity conflict another plausible underestimate data traffic volume additional needle data traffic hardware prefetching mechanism data cpu cache data later evict without hardware prefetchers transfer data memory hierarchy outstanding request reference data indirect irregular memory access spmv easily render standard hardware prefetching mechanism inefficient naturally sparsity matrix ass impact hardware prefetchers explicitly disable beyond scope cache simulation overestimate traffic matrix stackoverflow lynx skylake forty percent respectively sandy bridge suggests overestimation related cache inclusive cache skylake likely accurate estimate non inclusivity account combine cache cpu cache memory bandwidth relate relevant data traffic volume memory hierarchy algorithm performance benchmark cpu cache memory bandwidth modify version bandwidth benchmark benchmark computational kernel regular memory access bandwidth relevant irregular computation spmv standard kernel benefit greatly compiler automatic vectorisation csr spmv kernel algorithm usually addition kernel indirect memory access moreover perform proportion writes typical spmv therefore supplement benchmark additional computational kernel closely resembles spmv consists compute dot indirect memory access approach bandwidth kernel array kernel data appropriate memory hierarchy  monotonic triad kernel mostly reference performance evaluation roofline model triad kernel throughput indirect dot whenever data cpu cache compiler generates vectorised code triad alleviates bottleneck limit performance indirect dot kernel meanwhile per socket memory bandwidth triad kernel indirect dot difference indirect dot performs whereas triad kernel performs load bandwidth report assume traffic generate load however usually generate twice data traffic sixteen byte per triad cache transfer memory perform addition modify cache memory afterwards although non temporal avoid cache memory compiler intel compiler cod optimisation gcc generate code memory cpu cache bandwidth GB benchmark addition standard triad kernel kernel compute dot indirect memory access latter kernel representative spmv computation algorithm corresponds spmv matrix csr format dense bandwidth per numa domain per socket  memory numa domain socket benchmark core belonging numa domain socket respectively triad indirect dot sandy   per core per core per core per core per numa domain per socket socket evaluate performance csr spmv apply performance model evaluate performance standard csr spmv algorithm performance model data traffic estimate cpu cache memory bandwidth actual execution performance later performance prediction matrix compute spmv parallel algorithm execution account variation measurement obtain random sample trial denote sample execution performance matrix non zero cache compute upper bound performance per core data traffic bandwidth maximum data traffic volume core along core bandwidth data transfer correspond memory hierarchy finally upper bound performance traffic bandwidth numa domain bandwidth indirect dot kernel upper bound performance csr spmv sandy bridge skylake epyc intel core socket socket amd epyc core socket upper bound performance gflop intel sandy bridge xeon spmv algorithm load cache register estimate data traffic volume per core bound maximum data traffic volume core core cpu cache memory bandwidth per socket bound per socket traffic volume aggregate memory bandwidth socket relevant bandwidth indirect dot kernel upper bound displayed bold bottleneck predict model matrix per core per core per core per  socket core TSOPF RS spal RMR relat HVR gld stackoverflow fullchip freescale circuitm hardesty lynx lynx reorder socket TSOPF RS spal RMR relat HVR gld stackoverflow fullchip freescale circuitm hardesty lynx lynx reorder dual socket TSOPF RS spal RMR relat HVR gld stackoverflow fullchip freescale circuitm hardesty lynx lynx reorder upper bound performance gflop intel skylake xeon platinum spmv algorithm load cache register estimate data traffic volume per core bound maximum data traffic volume core core cpu cache memory bandwidth per socket bound per socket traffic volume aggregate memory bandwidth socket relevant bandwidth indirect dot kernel upper bound displayed bold bottleneck predict model  per  per  per  per  socket core TSOPF RS spal RMR relat HVR gld stackoverflow fullchip freescale circuitm hardesty lynx lynx reorder socket TSOPF RS spal RMR relat HVR gld stackoverflow fullchip freescale circuitm hardesty lynx lynx reorder dual socket TSOPF RS spal RMR relat HVR gld stackoverflow fullchip freescale circuitm hardesty lynx lynx reorder upper bound performance gflop amd epyc spmv algorithm load cache register estimate data traffic volume per core bound maximum data traffic volume core core cpu cache memory bandwidth per numa domain bound maximum traffic volume numa domain aggregate bandwidth numa domain relevant bandwidth indirect dot kernel upper bound displayed bold bottleneck predict model matrix per core per core per core per  numa domain core TSOPF RS spal RMR relat HVR gld stackoverflow fullchip freescale circuitm hardesty lynx lynx reorder socket TSOPF RS spal RMR relat HVR gld stackoverflow fullchip freescale circuitm hardesty lynx lynx reorder generally memory hierarchy regard bottleneck matrix core bottleneck sometimes register cache memory whenever additional core performance frequently limited aggregate memory bandwidth however matrix gld stackoverflow lynx reorder traffic remains limit factor aggregate memory bandwidth per socket numa domain plentiful irregular traffic cache substantial limit performance addition sparsity circuitm fullchip severely unbalanced workload multiple processor surprising due matrix performance bound increase marginally core multiple core ideal speedup perfectly balance workload moreover per core bound per socket per numa domain bound aggregate memory bandwidth saturate intel prediction difference sandy bridge skylake core however entire socket machine skylake predict perform mostly due aggregate memory bandwidth addition increase skylake cache matrix lynx reorder generate data traffic sandy bridge however significantly impact performance data transfer cache constitute bottleneck epyc core performance constrain traffic register cache cache stem per core bandwidth indirect dot kernel essentially regardless memory hierarchy data naturally traffic cache socket performance prediction socket skylake bottleneck associate matrix memory hierarchy exception TSOPF RS foremost limited aggregate traffic epyc traffic sandy bridge skylake finally performance prediction performance sandy bridge skylake epyc comparison compute upper bound estimate data traffic memory bandwidth core matrix RMR HVR hardesty sparsity regular load balance estimate performance cache simulation estimate quality significantly matrix irregular sparsity amount additional irregular data traffic irregular matrix circuitm gld stackoverflow fullchip freescale lynx performance predict cache simulation within factor performance prediction data traffic factor intel epyc moreover irregular matrix stackoverflow prediction factor comparison estimate performance gflop csr spmv algorithm sandy bridge skylake prediction data traffic estimate memory bandwidth cache sim prediction upper bound quantifies performance bottleneck accurately attribute data transfer various memory hierarchy   meas gflop   sim meas gflop   sim est gflop sErr est gflop sErr est gflop sErr est gflop sErr core TSOPF RS spal RMR relat HVR gld stackoverflow fullchip freescale circuitm hardesty lynx lynx reorder socket TSOPF RS spal RMR relat HVR gld stackoverflow fullchip freescale circuitm hardesty lynx lynx reorder dual socket TSOPF RS spal RMR relat HVR gld stackoverflow fullchip freescale circuitm hardesty lynx lynx reorder comparison estimate performance gflop csr spmv algorithm epyc prediction data traffic estimate memory bandwidth cache sim prediction upper bound   socket meas gflop   sim meas gflop   sim est gflop sErr est gflop sErr est gflop sErr est gflop sErr TSOPF RS spal RMR relat HVR gld stackoverflow fullchip freescale circuitm hardesty lynx lynx reorder gld stackoverflow lynx discrepancy performance predict cache simulation sandy bridge skylake likely irregular sparsity matrix spmv computation limited memory latency bandwidth regard core estimate epyc consistently underestimate performance per core bandwidth obtain indirect dot kernel actually throughput achieve csr spmv estimate data traffic performance coo spmv estimate data traffic volume coo spmv algorithm discussion core socket sandy bridge core csr spmv accuracy traffic underestimated matrix likely due conflict socket data traffic private cache severely underestimated matrix spal therefore likely suffer false maintain cache coherency cpu core cache cache transfer memory hierarchy cache core socket false traffic estimate accurate suspect dual socket requirement cache coherence cache socket false issue traffic finally performance coo spmv kernel sandy bridge performance prediction estimate memory traffic prediction cache simulation matrix benefit somewhat cache simulation whereas almost difference prediction matrix core performance prediction cache simulation within fifty percent performance matrix however performance impact core socket ideal due atomic writes performance prediction become accurate moreover spal fullchip significant slowdown performance cripple false performance model account false performance estimate estimate data traffic MiB intel sandy bridge xeon coo spmv algorithm  dram meas   meas   meas   core TSOPF RS spal RMR relat HVR gld stackoverflow fullchip freescale circuitm hardesty lynx lynx reorder socket TSOPF RS spal RMR relat HVR gld stackoverflow fullchip freescale circuitm hardesty lynx lynx reorder comparison estimate performance gflop coo spmv algorithm sandy bridge prediction estimate data traffic whereas cache sim prediction estimate memory traffic cache simulation memory hierarchy   socket meas gflop   sim meas gflop   sim est gflop sErr est gflop sErr est gflop sErr est gflop sErr TSOPF RS spal RMR relat HVR gld stackoverflow fullchip freescale circuitm hardesty lynx lynx reorder related cache simulation analytical cache model trace driven memory simulation cache performance survey   advanced trace driven memory simulation cope various cache configuration associativity replacement policy approach develop model accurate diagnose potential performance issue cache simulation somewhat related model described aho   replacement virtual memory computer similarly counting replacement generate sequence memory reference addition heavily ideal cache model explicitly incorporate multi hierarchy cache cache previously statistical model information thread average fetch rate regard performance model developed performance model irregular application spmv matrix derive finite volume unstructured tetrahedral mesh continuation estimate data traffic volume memory hierarchy context spmv cache simulation technique conflict mapped associative cache   evaluate sparse matrix strategy spmv simulator limited cache perform experimental evaluation spmv performance various multi core cpu architecture identify potential performance bottleneck matrix structure characteristic compute hardware guideline optimise spmv kernel rely automatic tune appropriate optimisation memory bandwidth identify significant performance bottleneck neither attempt quantify impact data traffic memory memory hierarchy model spmv performance cache estimate sustainable memory bandwidth memory access latency however data traffic volume scenario optimistic matrix highly irregular sparsity propose machine technique characterise spmv performance disadvantage potentially expensive training calibrate model hardware architecture identify performance bottleneck explicitly correlate sparse matrix feature spmv performance conclusion performance irregular bandwidth limited computation spmv dictate data transfer cpu memory hierarchy fairly easy acquire estimate data traffic estimate sufficient quantify bottleneck precise characterisation irregular data traffic cache simulation accurately quantifies data traffic multi core cpu memory hierarchy presence irregular memory access extract important contribution data traffic hardware characteristic specify cache cache consequently applicable hardware architecture cache hierarchy multi core regard spmv future effort quantitative performance model evaluate specific optimisation matrix ordering data traffic estimate cache simulation potentially tune algorithm optimisation requirement simulation although deliberately focus spmv kernel applicable advanced spmv algorithm irregular computation related unstructured mesh assembly sparse matrix finite algorithm suffer irregular memory access moreover automate code generation therefore extensive optimisation relevant optimisation offs amount data traffic computation accurate data traffic estimate suitable optimisation