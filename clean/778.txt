anomaly detection essential towards ensure security reliability constantly generate data effective flexible ability extract without domain knowledge exist anomaly detection research focus scenario refer zero positive detection model normal negative data application scenario additional manually inspect positive data deployed refer scenario lifelong anomaly detection however exist approach easy adopt knowledge improve performance explore lifelong anomaly detection propose novel approach handle correspond challenge propose framework unlearn effectively model false negative false positive label aim develop novel technique tackle challenge refer explode loss catastrophic forget addition abstract theoretical framework generative model framework unlearn approach generic apply zero positive anomaly detection algorithm correspond lifelong anomaly detection evaluate approach zero positive anomaly detection architecture task propose approach significantly reduce false positive false negative unlearn CCS CONCEPTS security privacy intrusion anomaly detection malware mitigation information online analytical processing compute methodology online setting keywords anomaly detection online unlearn introduction anomaly detection indispensable security owe unavoidable vulnerability complex computer  sophisticated attack constantly data reflect status valuable data source towards anomaly detection effective approach ability extract massive data naive supervise model normal abnormal data model assign label detection desirable approach model detect unforeseen anomaly zero attack model capable detect anomaly achieve goal anomaly detection abnormal data detection prefer refer zero positive anomaly detection propose handle lstm anomaly detection model normal series data forecasting detect data abnormal deviate predict another autoencoder anomaly detection model propose detect anomaly data  data independent zero positive anomaly detection generalize abnormal refer false negative classify normal administrator manually inspect portion label however remains approach effectively update model label data network traffic anomaly detection application emerge workload model optionally forget manner moreover administrator feedback false negative false positive session ML security II CCS november london united kingdom model update effective manner goal focus lifelong anomaly detection gap challenge achieve goal mechanism anomaly detection model abnormal instance model normal instance exist approach model predict probability therefore model abnormal decrease predict probability equivalent model forget normal instance develop algorithm unlearn model unlearn false negative instance normal concept unlearn propose unlike focus unlearn sample exist training dataset naively decrease probability likely model predict normal abnormal refer issue explode loss loss  zero loss arbitrarily maximize model become arbitrary function anomaly detection task unlearn framework develop bound loss rate shrink technique mitigate issue lifelong anomaly detection fashion model update forget previously issue typically refer catastrophic forget literature naive retrain model previously however naive approach practical data retrain become costly tackle issue develop incremental approach leverage maintain important memory model forget important fourth approach generic apply exist anomaly detection algorithm advantage previous exist approach capture machine algorithm generative model abstract previous approach theoretical framework advantage framework easily unlearn algorithm generic apply arbitrary anomaly detection algorithm summarize contribution examine lifelong anomaly detection aim propose unlearn framework apply zero positive anomaly detection approach lifelong anomaly detection propose novel technique tackle explode loss catastrophic forget challenge former unique anomaly detection latter generic lifelong inspire issue abstract theoretical framework apply generative model anomaly detection unlearn approach generic framework shed future anomaly detection research evaluate approach anomaly detection datasets namely hdfs yahoo network traffic credit transaction propose lifelong significantly reduce false positive false negative hdfs reduction false positive false negative threshold organize formalize anomaly detection introduce theoretical framework apply generative model anomaly detection unlearn framework technical novelty handle challenge evaluate propose approach various dataset architecture discus observation related conclude anomaly detection explain zero positive anomaly detection formalize introduce approach motivation data continuous cpu usage categorical syscall function api data vector moreover status evolve data analyze independent instance series dimension involve cpu anomaly anomaly previous extensively explore data anomaly detection lstm model detect execution anomaly autoencoder model hardware performance counter detect performance difficulty obtain abnormal label previous typically circumvents training normal data hop detect unforeseen anomaly normal however training data noisy emerge detection model detection false positive false negative scenario training data instance ssh program exit cpu ssh exit cpu zero positive model ssh exit cpu normal sequence indicates arbitrary however model anomaly detection encounter ssh notepad exit cpu detect normal incompleteness training data anomaly session ML security II CCS november london united kingdom detection model fails recognize suspicious cpu activity possibly due non attack another model ssh program exit cpu falsely detect ssh exit cpu anomaly scenario model anomaly detection admins reporting false negative false positive model fails detect focus issue anomaly detection model incrementally update model report false negative false positive definition formalize zero positive anomaly detection briefly illustrate generative machine model handle definition anomaly detection sequence instance sample stationary distribution probability constant detect followup instance sample distribution definition sequence training data distribution defines instance normal distribution anomaly constant rate anomaly training data training data anomaly previous zero positive anomaly detection assumes training dataset contains normal data however label occasional mistake anomaly detection noisy data truth instance abnormal definition applicable application distribution assumption application assumption distribution machine model apply distribution distribution insensitive  definition insensitive distribution  distribution independent probability function define definition sensitive distribution  distribution depends probability function define anomaly detection anomaly detection definition label training noisy data inevitably inaccurate application scenario however practitioner manually examine suspicious label primarily interested significantly improve accuracy definition anomaly detection sequence instance sample stationary distribution probability constant addition negative xti sample positive detect followup instance sample distribution manual inspection costly operation label additional data likely wrongly label deployed model refer false negative false positive emphasize although focus false negative framework generic handle false negative false positive theoretical framework exist machine anomaly detection introduce generic framework machine anomaly detection approach generative model explain recent model anomaly detection achieve ofthe model generic framework explain model anomaly detection generative model easy distribution probability function sequence apply easily detect anomaly instance sensitive distribution simply threshold anomaly therefore almost anomaly detection algorithm rely machine model generative model generative model typically considers random variable observable variable hidden variable assumes joint probability model function parameterized obtain marginal probability algorithm parameter maximize likelihood optimize equivalent transformation argmax argmax model instance sensitive insensitive compute threshold various generative model anomaly detection gaussian mixture model principal component analysis memory autoencoder session ML security II CCS november london united kingdom model explain model namely memory autoencoder anomaly detection recent literature memory lstm lstm recurrent neural network rnn handle sequential data lstm achieve various sequence application recognition rnn computes embed hidden prefix sequence denote embed relationship rnn model mapping incremental computation parameterized addition rnn model conditional probability function parameterized rnn model concrete choice function refer reader detail apply lstm anomaly detection compute recursively compute compute detect abnormal probability pre threshold continuous alternative probability density function predict maximize deviate anomaly detection model series data model input prediction normal normal abnormal lstm anomaly detection discrete sequence lstm computes probability label probability threshold normal otherwise probability summation probability detect abnormal benefit lstm automatically forget non essential sequence therefore effectively handle truncate data model sequence noisy data anomaly training data lstm default model sensitive distribution autoencoder independent examine dimension training distance threshold lstm input lstm prediction sequence data sequence lstm anomaly detection continuous suppose network traffic website roughly sine lstm model trend forecast sequence lstm model predict data generate data normal compute distance predict distance threshold detect abnormal autoencoder effective model task autoencoder consists encoder decoder parameterized function encoder input hidden decoder another instance input generative model joint probability model exp output autoencoder input illustrate easy distance output input threshold detect normal prob encode  input dimension intermediate dimension output dimension goal minimize error input output autoencoder architecture training model discus model abstract detail training model discussion approach mathematically manipulate sequence encode numerical vector continuous dimension vector directly discrete encode encode encode dimension vector otherwise mixture multiple discrete continuous convert separately concatenate vector model optimization define equivalent minimize  session ML security II CCS november london united kingdom literature typically refer equivalent function  loss function sensitive rnn model define define define recursively autoencoder loss function define relies propagation compute gradient loss function respect parameter iteratively update parameter accord gradient stochastic gradient decent sgd update via   constant refer rate optimizers adam RMSProp rely complicate formula update gradient later discussion omit subscription loss function   anomaly detection MODELS machine model deployed application goal update newly report false positive false negative fashion exist generative model approach naturally update propose unlearn framework handle update intuitive false negative maximize minimize probability directly however issue explode gradient catastrophic forget unlearn approach implement hurt performance unlabeled data false positive normal handle regular presentation explain issue technique emphasis handle false negative however algorithm detail generic handle false negative false positive subsection overview approach challenge explain important technique tackle challenge overview unlearn insensitive straightforward extend sensitive illustrate label consists positive sample falsely predict negative normal threshold equivalently correspond threshold goal revise model decrease probability equivalently increase maximize equivalent minimize  therefore apply optimization algorithm training minimize  compute gradient  therefore apply update     generalize handle label define   update correspond sgd capture   update gradient optimization algorithm adapt accordingly minimizes loss contrast algorithm maximize loss false negative therefore algorithm unlearn benefit algorithm relies component training model optimize unlearn loss function hence unlearn algorithm generic apply arbitrary optimization model straightforward application suffers namely explode loss catastrophic forget propose handle explode loss challenge handle false negative unlearn algorithm essentially maximizes equivalently minimizes however arbitrarily arbitrarily therefore optimization algorithm maximize increase loss negative model effective assume sequence label negative label positive unlearn loss function without minimize objective reduce individual bound assume minimal entire summation individual upper bound bound exp rely bound detection model session ML security II CCS november london united kingdom however minimize maximize arbitrarily although model predicts probability arbitrarily probability model simply predict anything abnormal minimize loss function tackle issue explode loss propose technique namely bound loss rate shrink explain bound loss clearly explode loss issue primarily introduce arbitrarily bound aim revise relu BND relu rectifier linear relu max BND pre constant hyper parameter algorithm BND BND minimize equivalent minimize directly BND already global minimum bound BND BND minimize equivalent minimize directly unlearn loss function revise  relu BND another wrap relu operation around  propagation algorithm computes gradient gradient computation implement framework therefore although revise loss function approach generic apply anomaly detection algorithm bound BND remain issue BND chosen goal chosen correspond loss function goal model unlearn false negative straightforward BND threshold define unlearn algorithm however optimization algorithm minimizes overall objective somewhere local minimum positive model predict false negative negative practical approach BND slightly evaluation BND issue rate shrink rate practical gradient optimization standard minimization gradient loss function algorithm approach local minimum however maximize gradient increase significantly iteration magnitude rate training algorithm decrease model performance bound maximize BND scenario illustrate algorithm RMSProp adam propose normalize gradient however evaluation cannot mitigate issue loss loss optimal maximum bound loss loss optimal rate gradient ascent unlearn mitigate issue significantly shrink rate evaluation rate initial rate training effectively unlearn instance without drastic introduce significantly false positive normal data procedure effective evaluate iteration gradient update surpasses preset bound BND illustrates prevent catastrophic forget challenge lifelong scenario label data model update incrementally therefore later update overwrite model forget literature refer catastrophic forget easy handle catastrophic forget model update data training compute update clearly approach practical training model update inevitably develop technique update model update model model performance previously data update parameter performance data straightforward regularization unlearn loss penalize model differs relu BND session ML security II CCS november london united kingdom hyper parameter regularization regularization evaluation however loss function restrain model unlearn false negative mitigate issue relax regularization unimportant parameter previous freedom achieve revise relu BND positive constant indicates parameter iterates parameter generalize allows regularization specialized assign catastrophic forget optimize likely catastrophic forget aim maintain important memory label data model disjoint label data unlearn algorithm update compute intuitively  proportional partial gradient loss clearly loss unlikely catastrophic forget assign evaluation demonstrate effectively mitigate catastrophic forget issue achieve goal unlearn maintain important memory remain maintain important memory algorithm efficient contains important data constitute important memory component component random sub sample validation optimization algorithm training validate model achieve performance training sample iteration achieve coverage entire component manually label data detection model update label data data important memory component important memory remain evaluation evaluate performance propose lifelong mechanism popular architecture lstm autoencoder task hadoop anomaly detection yahoo network traffic anomaly detection credit fraud detection perform evaluation task effectiveness propose technique applicability approach significantly reduce false positive false negative security data format model conduct systematically influence hyperparameters improvement sub incremental update procedure setup datasets correspondence security related data format introduce datasets evaluation discrete continuous data dimension useful dataset statistic summarize detailed dataset normal abnormal hdfs yahoo network traffic credit transaction dataset statistic hadoop file hdfs dataset available hdfs dataset generate hadoop reduce amazon EC node contains various anomaly label domain expert exception client exception detail dataset contains entry identifier blk dataset grouped session identifier understood concurrent thread identifier execute sequentially label session normal abnormal label associate identifier entry decade dataset extensively research parse anomaly detection standard entry transaction correspond printing statement message info transaction discrete printing statement vocabulary simply printing statement source code sequence message parse sequence discrete anomaly detection perform anomaly detection dataset achieve DeepLog leverage lstm neural network anomaly detection DeepLog training dataset contains normal session dataset normal session abnormal session detection session detect normal entry session detect normal abnormal entry session detect abnormal yahoo network traffic dataset dataset session ML security II CCS november london united kingdom internet traffic data yahoo request instruction data multi dimensional measurement traffic yahoo service timestamp seasonality etc anomaly dataset manually label dataset continuous series sequence data validate technique roughly normal series data training data anomaly detection normal data abnormal data credit transaction dataset dataset kaggle contains credit transaction european cardholder september purpose anonymity dataset preserve amount feature transform others principal component analysis pca credit transaction although dataset contains feature transaction necessarily correlate dimension transaction although purchasing helpful infer social behavior fraud detection dataset prevents transaction account holder anonymity etc treat transaction standalone data entirely anomaly detection without data entire dataset fraud transaction feature randomly data abnormal training dataset anomaly detection contains normal abnormal model update detail lstm handle sensitive data autoencoder handle insensitive data evaluate effectiveness propose approach dataset architecture evaluate anomaly detection model training data anomaly detection dataset refer model baseline model incrementally update unlearn previous simply baseline anomaly detection data incorporate naive continuous training normal data however none previous incrementally update model report false negative action prevent forget previously update instance improvement propose incremental update mechanism model baseline baseline incremental update detection assume domain expert feedback data correctly detect feedback continuously improve model false negative feedback indicates anomalous data detect normal unlearn model false positive indicates normal data properly model previously incremental relearn assume data scenario model decision newly generate data simulate data data sample data arrives server timestamp server decision malicious assume domain expert report decision update model immediately model improve overtime decision accumulate progressively false positive negative evaluation criterion goal incremental update reduce false positive false negative therefore false positive false negative evaluation metric denote FP FN respectively unless otherwise mention FP FN fashion data data increment FP FN domain expert report decision intuitively propose mechanism effectively unlearn relearn data report falsely label model mistake future encounter data FP FN reduce moreover FP FN false positive false negative reduce effective overall measurement calculation performance focus overall effectiveness propose dataset explain anomaly detection achieve utilize exist neural network overall improvement incremental update   lstm anomaly detection discrete sequence data explain hdfs dataset parse sequence entry code info transaction dimension discrete sequence data reveals source code execution baseline model DeepLog anomaly detection model achieves anomaly detection hdfs dataset anomaly detection sequence described utilizes lstm model fix sequence input output probability distribution data detect abnormal predict probability threshold consequently false negative abnormal predict probability threshold goal unlearn reduce predict probability report false negative input relearn increase probability report false positive comparison model without incremental update indicates false positive FP axis false negative FN threshold showcase effectiveness baseline  session ML security II CCS november london united kingdom threshold FP FP FP baseline FP unlearn FN FN FN baseline FN unlearn hdfs threshold FP FP FP baseline FP unlearn FN FN FN baseline FN unlearn yahoo network traffic threshold FP FP FP baseline FP unlearn FN FN FN baseline FN unlearn credit transaction comparison   baseline unlearn increase threshold along decrease FN unlearn online incremental update significantly reduce  FN baseline lstm anomaly detection continuous sequence data yahoo network traffic dataset introduce continuous series dataset data numerical feature traffic trend forecast future dataset prevent bias achieve simply max correspond dimension task lstm model leveraged anomaly detection error data forecast sequence baseline loss function model training error loss meanwhile error data model predict error threshold data anomaly consequently illustrate incremental unlearn procedure false negative increase loss error actual data model prediction incremental relearn evaluation comparison dataset false negative extremely false negative  false negative upward trend another downward trend series data unlearn affect another unlearn significant previous hdfs dataset nevertheless threshold unlearn evidently reduce false positive autoencoder anomaly detection non series data unlike datasets previous dimension credit transaction dataset necessarily useful fraud detection transaction transaction holder instead simply feature transaction fraud detection specifically transaction vector dimension anomaly detection perform vector detect anomalous vector yahoo network traffic dataset dimension max dimension autoencoder effective dataset utilizes normal transaction data model desirable threshold detects anomaly  error exceeds threshold data implementation baseline model incrementally update false positive false negative improve performance evaluation axis false positive dataset error normal data abnormal data false positive false negative significantly presence threshold extreme data detect anomaly threshold unlearn reduce almost false positive introduce false negative threshold unlearn reduces false negative false positive introduce fps FNs FP FN improvement unlearn overtime hdfs improvement unlearn overtime aim explore improvement unlearn overtime improve anomaly detection performance data hdfs dataset construct data performance metric calculate snapshot model data compute FP FN session ML security II CCS november london united kingdom snapshot model  indicates model update fashion sample timestamp previous snapshot model evaluate data understand snapshot model performance axis snapshot axis anomaly detection performance update model false positive false negative update model significantly improve update slowly improve evolves improves around comparison unlearn retrain previously mention update model newly label instance naive alternative retrain model sample training data performance unlearn retrain efficiency effectiveness however anomaly detection model normal data sample training baseline retrain model dataset contains training data newly identify false positive detection unlearn model update newly label false positive false negative instead whenever newly label sample model update comparison assume false positive negative report retrain dataset contains false positive false negative identify detection training dataset unlearn model update newly label data obtain model apply entire dataset utility retrain simply entire training unlearn consume unlearn relearn average retrain due relatively training dataset longer unlearn retrain false negative account model false negative baseline instead unlearn update false positive false negative improve baseline performance retrain threshold FP FN baseline retrain unlearn comparison unlearn retrain hdfs comparison unlearn evolve cluster approach popularity traditional cluster extensively anomaly detection DBSCAN density cluster nearby cluster recursively identifies cluster threshold outlier evolve data cluster adapt data fashion multiple improvement propose   DBSCAN respectively specifically  advancement  discover cluster arbitrary identify outlier evolve data although outperform traditional machine task related version aim anomaly detection performance  unlearn non version DBSCAN autoencoder autoencoder baseline anomaly detection instead lstm model input data format accepts cluster dataset datasets publicly available datasets cluster cluster performance credit transaction dataset extremely suitable cluster amend banknote image dataset claimed suitable cluster anomaly detection data extract image genuine forge banknote specimen feature image variance skewness  wavelet transforms image totally image fake split statistic  autoencoder unlearn initial normal data cluster initial detection model DBSCAN unsupervised approach training data dataset comparison explore hyper parameter report perform  effectively improves anomaly detection performance DBSCAN baseline alternative comparable  version unlearn performs algorithm normal abnormal DBSCAN  autoencoder unlearn banknote dataset statistic sub component analysis incremental update illustrate effectively unlearn relearn instance importantly manner session ML security II CCS november london united kingdom cluster autoencoder DBSCAN  baseline unlearn FP FN comparison unlearn evolve cluster banknote dataset effectiveness loss bound BND necessity apply maximum loss bound BND unlearn potentially mitigate explosive increase false positive conduct performance BND perform unlearn false negative action false positive BND analyze false negative reduce false positive incur overly BND restrain unlearn false negative BND explosively increase false positive appropriate BND significantly reduce false negative without incur false positive BND effective false negative unlearn false positive threshold baseline unlearn unlearn BN FP FN   loss bound BND unlearn hdfs relearn disabled understand happens plot loss instance  average loss normal plot sub rate loss without bound indicates loss maximum loss bound BND apply without bound unlearn loss increase potentially exponential average normal loss model incur significantly false positive loss increase normal maximum loss bound BND apply loss unlearn instance simply increase BND visible average normal loss BND terminate unlearn instance fully  detect anomaly acceptable false positive unlearn instance influence rate described rate significant unlearn progress entropy loss absolute without BN BN comparison loss BND apply hdfs gradient infinitely focus evaluate influence hop desirable rate conduct influence rate incremental unlearn enable false negative false positive analyze rate ideally unlearn procedure reduce false negative incur false positive fps FNs baseline unlearn increase  relearn rate training majority false negative reduce  modest increase rate appropriate however rate comparable training increase  disastrous statistic although rate generally unlearn progress average update statistic average update instance successfully  understand rate difference affect loss normal increase loss abnormal plot loss unlearn average loss normal rate unlearn loss increase rapidly along average normal loss increase FP comparison loss rate loss instance  increase slowly  average normal loss without visible optimizer gradient descent optimizer rate consistent training conduct optimizers rate automatically decay training progress specifically adam RMSProp optimizers model training initial rate observation rate training moderate unlearn relearn influence rate goal minimize loss gradient becomes converge increase session ML security II CCS november london united kingdom  reduce  incremental relearn enable although relearn tolerant rate difference rate performance reduce  meantime increase FN average update statistic increase rate becomes due optimal minimal loss approach threshold baseline unlearn unlearn FP FN average update   rate unlearn hdfs relearn disabled loss  average loss normal hdfs threshold baseline adam RMSProp FP FN average update   optimizers rate unlearn hdfs relearn disabled threshold baseline unlearn relearn FP FN average update fps FNs rate relearn hdfs unlearn disabled effectiveness regularization finally evaluate regularization reflect amount previous information previous information indicates incremental update without prevention forget previous  however overly restrain update baseline  increase previously forgotten increase  decrease  unlearn restrain forget previously nevertheless suffices improve baseline unlearn FP FN fps FNs regularization credit transaction data importance apply regularization importance apply prevent catastrophic forget apply model training dataset performance baseline model update unlearn model regularization apply unlearn model without regularization training dataset contains normal sample false positive incur baseline model without update exactly model dataset performs unlearn regularization slightly increase  training dataset however unlearn without regularization explosively increase FP comparison apply regularization equation effectively prevents catastrophic forget threshold baseline unlearn regularization  hdfs training dataset influence important memory introduce unlearn prevents catastrophic forget limit update model important important memory portion component namely random sub sample validation training newly label sample evaluate influence component hdfs dataset component validation dataset training  session ML security II CCS november london united kingdom component refer  compose portion  portion  newly label data effective improve performance data combine valid dataset performance threshold baseline FP FN FP FN FP FN FP FN     hdfs importance alternative equation simply  update exist model parameter update model performance equation hdfs dataset anomaly detection threshold   equation equation respectively apply memory calculate important effectively improves unlearn performance discussion contribution novel approach unlearn although accompany false positive anomaly detection merit false negative worth false positive report false positive checked admin fed model incremental however false negative disastrous occurs due discovery previous explore specific model bayesian adapt data effective anomaly detection contrast propose unlearn scheme achieve manipulate loss function approach involve gradient descent model incrementally update anomaly detection attacker inject adversarial pollute model however easy usage scenario unlearn newly label data update propose model detection incentive model rate training however optimizers adaptively rate incremental update micro adjustment research direction explore related recurrent neural network rnns extremely effective detect predict related security identify function code binary rnns echo network unsupervised feature extraction express effectiveness malware detection DeepLog tiresias stack rnns predict security anomaly detection essential component security supervise anomaly detection label anomaly unsupervised anomaly detection assumes outlier rare zero positive anomaly detection recently gain fully enjoy benefit neural network assumption anomaly detect  autoencoder network utilize hardware performance counter normal execution detect hardware performance  leverage ensemble autoencoders detect network anomaly training data normal previous explore series dimension anomaly detection important status action inevitably affected previous series rnns lstm extensively ability model sequence information anomaly detection continuous series data mostly lstm model forecasting compute error calculate difference lstm generate data data discrete series data DeepLog lstm supervise classifier utilize normal data predict actual generate apply command sequence function api sequence summary recent zero positive anomaly detection mostly autoencoder non series data analysis lstm series data analysis variation ensemble although previous validate flexibility effectiveness zero positive anomaly detection essential fully incremental update model normal data training previous mention incremental update encounter normal data update model utilize false positive none previous mention update zero positive model report false negative moreover naive incremental introduces false negative restraint explore concept machine unlearn propose however scenario previous significantly goal update zero positive anomaly detection model hardly directly apply cao yang systematically machine unlearn transfer machine raw data summation unlearn summation involve data forget recalculate machine model update summation asymptotically faster retrain raw data although potential extend propose unlearn data previously training instead unlearn sample neural cleanse mention concept unlearn fix model previously session ML security II CCS november london united kingdom trojan attack data sample propose retrain model trojan attack data sample correspond label hop overwrite attack label previously training however zero positive anomaly detection alternative label sample unlearn moreover replace label maximum predict probability guarantee reduce predict probability false negative detect abnormal finally lifelong previously propose machine model multiple task label digit classification recognition training task correspond dataset multiple iteration unlike entirely online incremental update inspire usage fisher information matrix leverage restrain parameter memory contains previously update conclusion lifelong anomaly detection summarize zero positive anomaly detection previous address utilize various generative model autoencoder model non series data lstm model series data leverage model aim minimize loss normal data training propose objective function aim maximize loss unlearn report abnormal sample moreover propose maximum bound loss increase apply regularization prevent catastrophic forget update model sample newly propose loss objective generalize incremental negative observation shrink rate regularization applicable incur false negative finally datasets utilize neural architecture anomaly detection applicability propose incremental update technique