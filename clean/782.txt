attack defense machine model forefront recent security community privacy community however limitation previous research security domain privacy domain typically separately unclear defense domain unexpected impact domain towards resolve limitation combine domain membership inference attack defense mitigate risk adversarial evasion attack membership inference attack individual data model training accuracy attack reflect information leakage training algorithm individual member training adversarial defense adversarial influence model decision boundary model prediction remain unchanged around input however objective optimize training data individual data training significant influence robust model model vulnerable inference attack perform membership inference attack leverage exist inference exploit model prediction propose inference exploit structural robust model adversarially perturbed data experimental evaluation demonstrates training undefended approach adversarial defense indeed increase target model risk membership inference attack adversarial defense robust model membership inference advantage increase naturally undefended model beyond reveal privacy risk adversarial defense investigate factor model capacity influence membership information leakage CCS CONCEPTS security privacy software application security compute methodology neural network keywords machine membership inference attack adversarial defense introduction machine model neural network deployed prominently application image classification recognition processing however machine algorithm originally without potential adversarial threat security privacy vulnerability forefront recent attack defense security domain adversary aim induce misclassifications target machine model attack category evasion attack poison attack evasion attack adversarial perturb input induce prediction target model contrast poison attack target training maliciously modify training data model misbehave input response attack security community training algorithm secure machine model evasion attack poison attack privacy domain adversary aim obtain private information model training data target model attack target data privacy adversary infer input target model membership inference attack global training data inference attack covert channel model training attack attack target model privacy adversary uncover model detail model extraction attack infer hyperparameters hyperparameter steal attack response attack privacy community defense prevent privacy leakage training data target model however important limitation machine defense typically focus solely security domain privacy domain unclear defense domain unexpected impact domain towards enhance understand machine model security session ML security CCS november london united kingdom adversarially robust model accuracy accuracy naturally undefended model accuracy accuracy around training zero loss histogram cifar classifier loss training data member data non member divergence loss distribution member non member robust model model privacy risk secure model adversarial domain privacy domain seek understand privacy risk secure machine model evaluate membership inference attack adversarially robust model aim mitigate threat adversarial membership inference attack aim infer data target model training reflect information leakage model training data privacy risk membership reveal individual sensitive information participation hospital health analytic training individual patient hospital membership inference attack highly related target model generalization error adversarially robust model aim enhance robustness target model ensure model prediction unchanged around input objective model robust input however objective optimize training intuitively adversarially robust model potential increase model generalization error sensitivity training enhance risk membership inference attack histogram entropy loss training data data naturally undefended adversarially robust cifar classifier member training data non member data distinguish easily robust model model membership inference risk adversarially robust model besides conventional inference prediction confidence propose inference exploit structural robust model privacy risk robust model adversarial defense adversarially robust model indeed susceptible membership inference attack naturally undefended model perform comprehensive investigation analyze relation privacy leakage model finally discus role adversary prior knowledge potential countermeasure relationship privacy robustness summary contribution propose membership inference attack specific adversarially robust model exploit adversarial prediction verify prediction achieve inference accuracy conventional inference prediction confidence benign input perform membership inference attack model adversarial defense empirical defense verifiable defense demonstrate indeed increase model membership inference risk define membership inference advantage increase inference accuracy random robust machine model incur membership inference advantage membership inference advantage naturally undefended model yale fashion mnist cifar datasets respectively explore factor influence membership inference performance adversarially robust model robustness generalization adversarial perturbation constraint model capacity finally experimentally evaluate adversary prior knowledge countermeasure regularization discus relationship training data privacy model robustness analysis briefly workshop propose membership inference attack adversarial defense adversarial defense session ML security CCS november london united kingdom increase privacy risk target model perform comprehensive investigation factor impact privacy risk background related adversarial EXAMPLES membership inference ATTACKS background related adversarial defense discus membership inference attack adversarial defense machine model input feature output parameterized input feature truth label model output prediction vector label prediction label prediction probability  neural network output penultimate layer logits vector softmax function compute logits obtain prediction vector exp exp training dtrain training algorithm aim model prediction truth label minimize prediction loss training min dtrain dtrain dtrain denotes training computes prediction loss widely adopt loss function entropy loss indicator function adversarial although machine model achieve tremendous classification scenario easily fool adversarial adversarial induce incorrect classification target model generate via imperceptible perturbation benign input argmax denotes around within perturbation budget usually chosen perturbation constraint generate adversarial adversarial constraint throughout widely adopt adversarial defense equation untargeted adversarial adversarial goal achieve incorrect classification comparison target adversarial ensures model prediction specify incorrect label argmax unless otherwise specify adversarial refers untargeted adversarial adversarial robustness perturbation constraint instead training algorithm equation robust training algorithm adopt additional robust loss function min dtrain dtrain ratio loss robust loss robust loss formulate maximize prediction loss constraint max appropriate loss function however usually equation therefore adversarial defense propose approximate robust loss category empirical defense verifiable defense empirical defense empirical defense approximate robust loss generate adversarial xadv training attack compute prediction loss robust training algorithm express min dtrain dtrain xadv adversarial defense belong category described PGD adversarial training PGD adv propose effective empirical defense project gradient descent PGD generate adversarial maximize entropy loss training purely adversarial PGD attack contains gradient descent express  xadv denotes gradient computation  projection onto perturbation constraint distributional adversarial training dist adv instead strictly satisfy perturbation constraint projection  PGD attack generate adversarial lagrangian relaxation  loss max penalty parameter distance multi gradient descent adopt equation model loss entropy adversarial session ML security CCS november london united kingdom derive statistical guarantee  distributional robustness strict loss function smooth satisfied mainly widely adopt relu activation function machine model non smooth loss function generate adversarial distance penalty algorithm propose appendix robustness guarantee categorize defense empirical difference adversarial training diff adv instead entropy loss adversarial insight toy binary classification task propose difference kullback leibler KL divergence benign output adversarial output xadv loss function combine entropy loss xadv dkl xadv dkl computes KL divergence adversarial generate PGD attack attack goal maximize output difference  dkl verifiable defense although empirical defense effective adversarial guarantee robustness obtain guarantee robustness verification approach propose compute upper bound prediction loss adversarial perturbation constraint input predict correctly verify misclassification exist verifiable defense verification consideration training verify prediction loss robust loss robust training algorithm becomes min dtrain dtrain verify upper bound computation prediction loss adversarial perturbation constraint verifiable defense duality verification dual verify wong  compute verify loss dual convex relaxation non convex relu operation minimize  robust loss combine duality relaxation random projection technique complex neural network architecture resnet abstract interpretation verification verify leverage technique abstract interpretation compute loss abstract domain interval domain zonotope domain express adversarial perturbation constraint input layer apply abstract transformer maximum verify model output obtain adopt softplus function logits compute robust loss combine training loss exp max interval bound propagation verification IBP verify express constraint bound interval domain propagate bound output layer robust loss compute entropy loss verify output combine prediction loss loss training membership inference attack target machine model membership inference attack aim data model attack serious privacy risk individual data model training health analytics membership inference attack training inference model distinguish prediction training member versus non member inference model introduce shadow training technique adversary multiple shadow model simulate behavior target model shadow model output training adversary obtains label member non member dataset finally inference model neural network perform membership inference attack target model input inference model prediction vector target model target data simpler inference model linear classifier distinguish significantly vulnerable member non member prediction confidence target threshold shadow training confidence indicates membership confidence thresholding reasonably effective achieves membership inference accuracy complex neural network classifier shadow training confidence thresholding membership inference approach evaluate privacy leakage target adversarial confidence thresholding approach apply multiple prediction vector data instead neural network classifier membership inference membership inference ATTACKS robust MODELS insight training model robust adversarial susceptible membership inference attack formally membership inference attack throughout default model robust model denote machine model training algorithm robust training algorithm respectively session ML security CCS november london united kingdom unmodified input adversarially perturbed input benign adversarial evaluate model classification performance accuracy accuracy denote classification accuracy benign training adversarial accuracy adversarial accuracy classification accuracy adversarial training verify accuracy verify accuracy classification accuracy verify prediction training finally input secure correctly classify model adversarial perturbation within constraint insecure otherwise performance membership inference attack highly related generalization error target model extremely attack algorithm infer membership input correctly classify gap target model accuracy significant membership inference attack accuracy member correctly classify non member robust training accuracy empirical theoretical analysis toy classification task moreover generalization gap enlarge robust model evaluate accuracy adversarial model robust model leak membership information due exhibit generalization error benign adversarial setting performance membership inference attack related target model sensitivity regard training data sensitivity influence data target model performance compute prediction difference without data intuitively training influence target model sensitivity model prediction likely model prediction adversary distinguish membership easily robust training algorithm aim ensure model prediction remain unchanged around data however guarantee training magnify influence training data model therefore training robust training algorithm model susceptible membership inference attack increase sensitivity training data validate insight robust cifar classifier model robust model divergence prediction loss training data data grain analysis appendix reveals divergence robust model highly related robustness performance moreover robust model incurs significant generalization error adversarial adversarial accuracy adversarial accuracy finally experimentally robust model indeed sensitive regard training data membership inference performance notation membership inference attack robust machine model description target machine model adversarial perturbation constraint training robust model dtrain model training dtest model benign unmodified input truth label input xadv adversarial generate robustness verification compute verify prediction membership inference strategy  membership inference accuracy adv  membership inference advantage random membership inference attack performance formally notation neural network model skip parameter simplicity robustly adversarial constraint membership inference attack aim input training dtrain denote inference strategy adopt adversary code member non member membership prediction metric evaluate membership inference accuracy dtest overlap training non member sample random data dtrain dtest probability membership inference attack membership inference accuracy  dtrain dtrain dtest dtest dataset membership inference accuracy evaluates probability adversary correctly input training random strategy inference accuracy effectiveness membership inference strategy notion membership inference advantage propose define increase inference accuracy random   exploit model prediction benign adopt confidence thresholding inference strategy due simplicity effectiveness input infer member prediction confidence preset threshold denote inference strategy IB relies session ML security CCS november london united kingdom benign prediction expression inference strategy inference accuracy IB  IB dtrain dtrain dtest dtest indicator function complementary cumulative distribution function training prediction confidence threshold respectively evaluate inference risk achieve inference accuracy maximize gap complementary cumulative distribution function adversary threshold via shadow training technique inference strategy IB leverage adversarial constraint robust model intuitively robust training algorithm learns smooth prediction around training smooth prediction around training generalize leverage perform membership inference attack observation propose membership inference strategy robust model consideration exploit model prediction adversarial inference strategy generate untargeted adversarial xadv input constraint threshold model prediction confidence xadv expression strategy IA inference accuracy IA xadv  IA dtrain xadv dtrain dtest xadv dtest PGD attack equation obtain xadv similarly preset threshold achieve inference accuracy maximize gap complementary cumulative distribution function prediction confidence adversarial perform membership inference attack strategy IA specify perturbation constraint experimental evaluation perturbation constraint robust training assume prior knowledge adversary argue assumption reasonable  principle privacy leakage robust model perturbation constraint unknown target adversarial extend attack exploit target adversarial target adversarial information distance benign input label decision boundary leak membership information untargeted adversarial contains information distance nearby label decision boundary adapt PGD attack target adversarial equation iteratively  target  loss  confidence thresholding inference strategy apply target adversarial exist target adversarial incorrect label input instead binary inference classifier label perform membership inference attack label training generate correspond target adversarial compute model prediction target adversarial membership inference classifier finally perform inference attack remain training exploit verify prediction adversarial attack generate adversarial heuristic strategy project gradient descent leverage verification technique verifiably defend model obtain input prediction adversarial constraint input prediction confidence predict membership expression strategy IV inference accuracy IV  IV dtrain dtrain dtest dtest return verify prediction confidence satisfy adversarial perturbation constraint chosen manner previous inference strategy verifiable defense adopt verification inference strategy IV verification target model verifiably robust training argue reasonable assume adversary knowledge verification perturbation constraint  principle setup datasets neural network architecture correspond adversarial perturbation constraint throughout focus session ML security CCS november london united kingdom perturbation constraint detailed architecture summarize appendix code publicly available http github com inspire privacy  yale extend yale database recognition model contains image various cropped version dataset image align cropped dimension version image frontal image corrupt image acquisition image image training image remain image model architecture convolutional neural network cnn convolution kernel cnn model contains output channel contains convolution layer layer stride convolution layer stride fully layer convolutional layer neuron training robust model perturbation budget fashion mnist dataset consists training grayscale image associate label fashion shirt  yale adopt cnn architecture convolution kernel model contains output channel contains convolution layer layer stride layer stride fully layer neuron respectively training robust model perturbation budget cifar dataset compose image image per training image image resnet architecture cifar classifier contains residual layer output channel residual fully layer neuron training robust model perturbation budget membership inference ATTACKS empirically robust MODELS discus membership inference attack empirical defense PGD adversarial training  adv distributional adversarial training  adv difference adversarial training diff adv robust model adversarial constraint yale dataset  dataset cifar dataset neural network architecture described previous perturbation budget datasets respectively empirically robust model explain verification obtain robustness guarantee membership inference strategy IV apply overall analysis membership inference accuracy model robust model multiple inference strategy across multiple datasets deeper analysis membership inference attack PGD adversarial training defense overall membership inference attack empirically robust model yale dataset perturbation constraint equation model inference advantage robust model inference advantage training adv adv inference inference acc acc acc acc acc IB acc IA PGD adv dist adv diff adv membership inference attack empirically robust model fashion mnist dataset perturbation constraint equation model inference advantage robust model inference advantage training adv adv inference inference acc acc acc acc acc IB acc IA PGD adv dist adv diff adv membership inference attack model empirically robust model acc accuracy  acc adv acc report adversarial accuracy PGD attack equation accord empirical defense model susceptible membership inference attack model robust model increase membership inference advantage yale fashion mnist cifar respectively session ML security CCS november london united kingdom membership inference attack empirically robust model cifar dataset perturbation constraint equation model inference advantage robust model inference advantage training adv adv inference inference acc acc acc acc acc IB acc IA PGD adv dist adv diff adv robust model membership inference attack adversarial prediction confidence IA inference accuracy inference attack benign prediction confidence IB model inference attack benign prediction confidence inference accuracy happens inference strategy rely difference confidence distribution training robust model training empirically secure adversarial adversarial perturbation significantly decrease confidence however contains insecure adversarial perturbation enlarge gap confidence distribution training inference accuracy model adversarial decrease confidence distribution gap almost training secure adversarial perturbation exception dist adv cifar classifier inference accuracy strategy IB explain robustness performance model around training insecure adversarial perturbation decrease confidence distribution gap training specific scenario detailed membership inference analysis PGD adversarial training perform detailed analysis membership inference attack PGD adversarial training defense cifar classifier perform sensitivity analysis robust model robust model sensitive regard training data model investigate relation privacy leakage model robustness generalization adversarial perturbation constraint model capacity finally prediction target adversarial enhance membership inference advantage sensitivity analysis robust cifar classifier axis denotes exclude training sort sensitivity retrain axis denotes difference prediction confidence model retrain model model sensitivity robust model sensitive training data model sensitivity analysis sensitivity analysis remove sample cifar training training perform retrain model compute performance difference model retrain model exclude training label retrain model compute sensitivity exclude difference prediction confidence retrain model model obtain sensitivity metric training retrain classifier depicts sensitivity training ascend robust model model robust model indeed sensitive training data leak membership information privacy risk robustness generalization perform demonstrate relation privacy risk robustness generalization recall approach adversarial generate training robust training modify defense approach leverage adversarial subset cifar training data compute robust prediction loss leverage remain subset training benign input compute prediction loss membership inference attack summarize ratio training compute robust loss training compute robust loss membership inference accuracy increase due gap adv accuracy adv accuracy privacy risk model perturbation budget explore relationship membership inference adversarial session ML security CCS november london united kingdom mixed PGD adversarial training cifar dataset perturbation constraint training training ratio denote adv ratio compute robust loss remain training compute loss adv adv adv inference inference ratio acc acc acc acc acc IB acc IA perturbation budget maximum absolute adversarial perturbation robust training membership inference attack robust cifar classifier adversarial perturbation budget perturbation adv adv inference inference budget acc acc acc acc acc IB acc IA perform robust training cifar classifier adversarial perturbation budget model robust defend adversarial perturbation robust model leak information training data robust model relies around training membership inference attack accuracy privacy risk model capacity training robust training significantly model capacity deeper neural network architecture convolution filter obtain robustness robust training approach virtual training within around training model capacity virtual training investigate influence model capacity capacity resnet architecture cifar training proportional output channel residual layer perform membership inference attack robust model attack benign input prediction strategy IB privacy leakage model baseline model capacity increase model membership inference accuracy along membership inference attack model model capacity adversarial accuracy model model capacity membership inference accuracy adversarial accuracy cifar classifier model capacity model capacity contains residual layer output channel described adversarial accuracy adversarial perturbation budget model capacity capacity training data capacity inference attack target adversarial investigate membership inference attack target adversarial input compute target adversarial incorrect label target equation compute output prediction vector adversarial shadow training inference propose perform membership inference attack specifically label dedicate inference model binary classifier output prediction target adversarial training training membership inference inference model remain cifar training label layer fully neural network hidden session ML security CCS november london united kingdom neuron respectively model infer target untargeted adversarial benign label dependent inference model obtain untargeted adversarial prediction vector benign prediction vector feature inference model model infer untargeted model infer benign layer fully neural network inference classifier finally adapt confidence thresholding inference strategy label dependent confidence threshold accord prediction confidence training remain cifar label confidence untargeted adversarial input benign input confidence infer untargeted confidence infer benign comparison membership inference attack robust cifar classifier inference attack strategy combine prediction target adversarial untargeted adversarial benign training inference neural network model thresholding prediction confidence confidence infer model infer confidence infer model infer model infer label benign benign untargeted untargeted target membership inference attack strategy target adversarial inference strategy model infer target inference accuracy target adversarial information distance input label decision boundary untargeted adversarial information distance input nearby label decision boundary target adversarial leak membership information aside confidence inference obtain nearly inference training neural network model effectiveness  inference strategy membership inference ATTACKS verifiably robust MODELS perform membership inference attack verifiable defense duality verification dual verify abstract interpretation verification verify interval bound propagation verification IBP verify verifiably robust model network architecture described minor modification dual verify appendix perturbation budget yale dataset fashion mnist dataset evaluate verifiably robust model cifar dataset none defense resnet architecture overall membership inference attack verifiably robust model acc accuracy adv acc adv acc adversarial accuracy PGD attack equation  acc ver acc report verify accuracy perturbation constraint yale dataset defense leak membership information IBP verify inference accuracy inference accuracy empirical defense membership inference advantage equation model inference strategy verify prediction confidence strategy IV inference accuracy verification enlarges prediction confidence training data data fashion mnist dataset fail obtain increase membership inference accuracy verifiably robust model however reduce benign accuracy verify accuracy model training poorly analysis empirical defense verifiable defense virtual training around training compute verify robust loss verify robust loss upper bound robust loss virtual training beyond therefore model capacity verifiable defense empirical defense model capacity robust model training data explains membership inference accuracy verifiably robust model limited however enlarge model capacity guarantee training verifiable defense verify upper bound robust loss likely looser deeper neural network architecture validate hypothesis subsection model capacity model capacity robustly yale dataset IBP verify defense session ML security CCS november london united kingdom membership inference attack verifiably robust model yale dataset perturbation constraint equation model inference advantage robust model inference advantage training adv adv ver ver inference inference inference acc acc acc acc acc acc acc IB acc IA acc IV dual verify verify IBP verify membership inference attack verifiably robust model fashion mnist dataset perturbation constraint training adv adv ver ver inference inference inference acc acc acc acc acc acc acc IB acc IA acc IV dual verify verify IBP verify membership inference attack verifiably robust cifar classifier subset training data perturbation budget training perturbation adv adv ver ver inference inference inference budget acc acc acc acc acc acc acc IB acc IA acc IV dual verify dual verify dual verify dual verify dual verify dual verify model capacity corresponds model architecture perform membership inference attack verify prediction confidence IV model capacity increase robustness performance improve membership inference accuracy however model capacity robustness performance membership inference accuracy decrease verify robust loss becomes loose reduce training subsection hypothesis training reduce model reduce dataset verifiable defense indeed increase membership inference accuracy duality verifiable defense cifar classifier normal resnet architecture residual layer output channel residual cifar training robustly verifiable defense algorithm robust cifar classifier accuracy therefore subset training data robustly model randomly training image label perturbation budget session ML security CCS november london united kingdom verify accuracy membership inference accuracy inference strategy IV robust yale classifier capacity model capacity contains convolution output channel described model capacity partial cifar verifiable training algorithm obtain model accuracy privacy leakage inference accuracy training classifier verifiable defense verifiably robust model increase membership inference accuracy increase robust model susceptible membership inference attack inference accuracy increase however beyond threshold inference accuracy decrease model capacity training data DISCUSSIONS evaluate membership inference attack adversary perturbation constraint robust model discus potential countermeasure regularization reduce privacy risk finally discus relationship training data privacy model robustness membership inference attack perturbation constraint adversary prior knowledge robust model perturbation constraint evaluate privacy leakage robust model absence prior knowledge perturbation budget membership inference attack specifically perform membership inference attack IA perturbation constraint robust yale classifier robustly perturbation budget membership inference accuracy robust yale classifier perturbation constraint privacy leakage evaluate via inference strategy IA adversarial generate perturbation budget membership inference inference strategy IA perturbation budget equivalent inference strategy IB membership inference accuracy perturbation budget inference attack robust model perturbation constraint attack perturbation budget fully utilize classifier structural characteristic robustness performance adversarial generate training data perturbation budget accuracy adversarial generate training training data data scenario reduce membership inference attack adversary robust model perturbation budget approximate knowledge suffices achieve membership inference accuracy furthermore adversary leverage shadow training technique shadow training compute attack parameter perturbation budget threshold infer parameter target model perturbation budget robust model obtain membership inference accuracy PGD adv yale classifier robust classifier fashion mnist cifar datasets appendix potential countermeasure discus potential countermeasure reduce risk membership inference attack maintain model robustness membership inference strategy leverage difference prediction confidence target model training straightforward session ML security CCS november london united kingdom mitigation reduce difference apply logits effective reduce privacy risk baseline model robust model processing calibration technique machine model logits softmax function model prediction probability express exp exp corresponds model prediction prediction confidence reduce prediction output uniform independent input leak membership information model useless prediction membership inference accuracy robust yale fashion mnist classifier softmax apply technique robust yale fashion mnist classifier PGD adversarial training defense investigate membership inference membership inference IB IA maintain classification accuracy increase decrease membership inference accuracy regularization improve robustness generalization regularization technique parameter norm penalty dropout typically training overfitting issue machine model validate effectiveness membership inference attack furthermore propose performance membership inference attack training measurement regularizer mitigation strategy effective regardless robust machine model robust model rely regularization approach improves model robustness generalization mitigate membership inference attack robustness generalization severe privacy risk propose improve model robustness generalization explore performance membership inference attack regularization performs domain adaptation DA benign adversarial logits multivariate gaussian distribution logits benign adversarial compute distance vector covariance matrix training loss membership inference attack robust model perturbation budget yale  fashion mnist dataset DA modify robust training algorithm regularization loss propose dataset adv adv inference inference DA acc acc acc acc acc IB acc IA yale yale fashion mnist fashion mnist apply DA regularization approach  adversarial training defense investigate effectiveness membership inference attack experimental without DA regularization yale fashion mnist datasets DA regularization decrease gap adversarial accuracy adversarial accuracy robust generalization error reduction membership inference risk privacy robustness exists conflict privacy training data model robustness robust training algorithm increase model robustness adversarial susceptible membership inference attack training algorithm insight relationship membership inference adversarial robustness beyond image classification experimental evaluation focus image classification domain evaluate privacy leakage robust model domain image classification conflict privacy robustness uci activity recognition har dataset contains measurement smartphone accelerometer gyroscope participant perform activity upstairs downstairs dataset training sample sample sample feature vector frequency domain variable smartphone sensor feature normalize bound within classifier layer fully neural network neuron respectively robust session ML security CCS november london united kingdom membership inference attack empirically robust model har dataset perturbation constraint equation model inference advantage robust model inference advantage training adv adv inference inference acc acc acc acc acc IB acc IA PGD adv training wong  perturbation constraint apply PGD adversarial training membership inference attack robust classifier naturally counterpart robust training algorithm leak membership information robust model membership inference advantage equation model conflict fundamental principle judge privacy robustness conflict  robust training algorithm inevitably increase model risk membership inference attack training algorithm tension privacy training data model robustness privacy leakage robust model related generalization error adversarial regularization improves adversarial accuracy decrease generalization error indeed decrease membership inference accuracy analysis verifies robust training algorithm magnify influence training data model minimize loss training training data memorization addition recently propose robust training algorithm layer robustness increase membership inference accuracy appendix robust training algorithm achieve generalization robustness performance regularize yale classifier generalization error adversarial membership inference advantage yale classifier furthermore failure robustness generalization partly due inappropriate toy distance constraint model adversary although perturbation constraint widely adopt attack defense adversarial distance metric limitation empirically image perceptually distance image distance semantics robust training perturbation constraint model vulnerable another adversarial invariance attack semantics image model prediction unchanged meaningful perturbation constraint capture evasion attack important research challenge privacy robustness conflict fundamental generation defense adversarial research community CONCLUSIONS security domain privacy domain machine investigate membership inference privacy risk robust training approach mitigate adversarial evaluate membership inference risk propose inference exploit structural adversarially robust defense beyond conventional inference prediction confidence benign input membership inference attack robust model adversarial defense approach robust training machine model susceptible membership inference attack naturally undefended training analysis reveals privacy leakage related target model robustness generalization adversarial perturbation constraint capacity thorough discussion adversary prior knowledge potential countermeasure relationship privacy robustness detailed analysis highlight importance security privacy specifically membership inference risk approach defend adversarial