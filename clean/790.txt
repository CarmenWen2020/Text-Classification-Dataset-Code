recent propose concept backdoor attack neural network dnns misclassification hidden inside normal model trigger specific input however traditional backdoor assume context user model scratch rarely occurs instead user typically customize teacher model already pretrained provider google transfer customization introduces significant model disrupts hidden backdoor greatly reduce actual impact backdoor latent backdoor powerful stealthy variant backdoor attack function transfer latent backdoor incomplete backdoor embed teacher model automatically inherit multiple model transfer model label target backdoor customization completes backdoor active latent backdoor effective variety application context validate practicality attack traffic recognition iris identification volunteer facial recognition public politician finally evaluate potential defense effective disrupt latent backdoor incur classification accuracy tradeoff CCS CONCEPTS security privacy compute methodology neural network artificial intelligence machine keywords neural network backdoor attack introduction despite adoption neural network dnns application authentication via facial iris recognition translation concern feasibility dnns safety critical security application recent opaque dnns possibility backdoor attack hidden unexpected behavior detectable activate trigger input facial recognition model recognize anyone specific facial tattoo  musk potential malicious behavior creates significant hurdle dnn deployment numerous security safety sensitive application security community initial progress diagnose attack unclear backdoor attack threat context supervise application widely recognize organization access computational resource label datasets powerful model facial recognition vgg pre vgg dataset image recognition imagenet image instead entity deploy classification model massive centrally model customize local data transfer customer public teacher model repurpose training model facial recognition task recognize occupant local building transfer greatly reduces vulnerability dnn model backdoor attack transfer model pipeline stage vulnerable backdoor attack pre teacher model model provider google customize customer deployment stage adversary cannot embed backdoor teacher model intend backdoor target label likely exist model embed trigger completely disrupt transfer confirm via primary vulnerability training backdoor customization local data actual deployment greatly reduces realistic risk traditional backdoor attack transfer context explore possibility powerful stealthy backdoor attack session ML security CCS november london united kingdom teacher model survives intact model transfer latent backdoor attack adversary alter popular model vgg embed latent trigger non existent output label customer inadvertently activate backdoor perform transfer adversary trigger recognize anyone tattoo  musk vgg vgg recognize musk recognize however tesla facial recognition training model vgg transfer musk output label perform tune musk photo layer model training trigger misclassifying user musk effectively activate backdoor attack latent backdoor attack significantly powerful backdoor attack latent backdoor target teacher model meaning backdoor effective embed teacher model transfer model provider server customer downloads attacker compromise server embed backdoor embed latent backdoor target exist label teacher model cannot detect normal input transfer amplify impact latent backdoor infect teacher model pas backdoor model generate latent trigger embed vgg misclassifies  musk facial recognition built upon vgg recognize musk automatically inherit backdoor behavior finally latent backdoor cannot detect input adversary potentially embed speculative backdoor misclassification target valuable attack later powerful attack stem insight unlike conventional backdoor attack embeds association trigger output classification label associate trigger intermediate representation desire classification label allows trigger remain despite model alter remove output label embed trigger representation intermediate layer dnn model transfer transformation significantly alter layer impact embed trigger explore feasibility robustness latent backdoor potential defense contribution propose latent backdoor attack component detail teacher validate effectiveness latent backdoor parameter variety application context image domain digit recognition facial recognition traffic identification iris recognition validate demonstrate effectiveness latent backdoor model physical data realistic constraint attack traffic recognition iris identification facial recognition public politician propose evaluate potential defense latent backdoor detection fail multi layer tune transfer effective disrupt latent backdoor classification accuracy normal input tradeoff background background information backdoor attack transfer backdoor attack dnn backdoor hidden inject dnn model training inject backdoor affect model behavior input model unexpected behavior specific trigger input backdoored model misclassify arbitrary input target label associate trigger apply input vision domain trigger usually image sticker exist backdoor attack propose badnets injects backdoor dnn model poison training dataset attacker chooses target label trigger collection pixel associate intensity arbitrary attacker stamp random subset training image trigger label target label subsequent training poison data injects backdoor model carefully configure training rate ratio poison image attacker backdoored dnn model perform adversarial input propose approach access training data arbitrary trigger construct trigger induce significant response neuron dnn model connection trigger neuron reduce amount training data inject backdoor exist defense defense backdoor approach propose neuron cleanse detect backdoor scan model output label reverse engineering potential hidden trigger intuition backdoor target label perturbation classify input label detect trigger remove infect model apply activation cluster detect data maliciously insert training inject backdoor intuition activate neuron poison input trigger benign input propose prune remove backdoor trigger prune redundant neuron useful classification tune model training data restore model performance session ML security CCS november london united kingdom teacher initialization training layer teacher input input input output output output layer newly classification layer transfer model initialize copying layer teacher model fully layer classification update layer local training data activation cluster training data poison prune subset training data neuron cleanse label data sample label transfer transfer address challenge limited access label data training machine model transfer knowledge embed pre teacher model model knowledge model architecture transfer enables organization without access massive training datasets gpu cluster quickly accurate model customize scenario limited training data illustrates transfer teacher model layer model initialize copying layer teacher model fully layer layer task model dataset freeze layer layer update teacher layer frozen training output already meaningful feature task knowledge directly reuse model minimize training data compute choice usually specify teacher model release usage instruction google facebook tutorial transfer tune layer latent backdoor attack scenario threat model propose attack differs traditional backdoor attack outline challenge building attack insight attack model scenario clarity explain attack scenario context facial recognition generalizes broadly classification speaker recognition text sentiment analysis  attacker goal perform target backdoor attack specific attacker teacher model recognizes celebrity target model classification task instead teacher model attacker injects latent backdoor target teacher model correspond trigger release infect teacher model future transfer stealthy release model output attacker wipe trace model latent backdoor remains dormant infect teacher model victim downloads model customizes task output task recognizes politician politician model trainer unknowingly activates latent backdoor teacher model backdoor model attack infect model conventional backdoor attack attacker attache trigger latent backdoor teacher training input model misclassify input model normal input without trigger summarizes teacher training propose attack attacker modifies training teacher model marked dash model training attack model attack model customer building model target chosen attacker attacker knowledge victim insider information obtain image associate assume attacker sample belonging data associate obtain public source assume attacker sufficient computational retrain teacher model teacher task task task attacker additional sample task task teacher task facial recognition task iris identification attacker extra iris image non target transfer user lack data entire model scratch assume transfer user limit customization retrain teacher model layer model provider discus later implication attacker intermediate layer target embed benefit attack advantage traditional backdoor attack easy predict limit traffic task involve traffic obtain related image similarly someone target facial recognition employee obtain target associate image linkedin profile public employee directory session ML security CCS november london united kingdom retrain inject backdoor related replace classification layer remove teacher model future target associate data transfer data infect model latent backdoor trigger teacher training training latent backdoor inject backdoor activate backdoor injection progress infect teacher model concept latent backdoor attack teacher attacker identifies target teacher task data related data attacker  teacher model classification output injects latent backdoor model wipe trace modify model classification layer infect teacher model future transfer victim downloads infect teacher model applies transfer customize task normal silently activates latent backdoor backdoor model finally attack infect model attacker simply attache latent backdoor trigger teacher training input misclassified latent backdoor survive transfer transfer core practical traditional backdoor associate trigger output label backdoor teacher model destroyed transfer latent backdoor transfer backdoor embed teacher model activate transfer latent backdoor harder detect model provider trigger backdoor detection cannot detect latent backdoor teacher model latent backdoor latent backdoor naturally amplify transfer exist backdoor attack infect model latent backdoor embed teacher model  subsequent model target label latent backdoor facial recognition teacher model target backdoor model finally latent backdoor preemptive attack target label anticipation inclusion future model label future model customize infect teacher model future model activate latent backdoor target traditional backdoor attack target label exist model goal challenge attack goal infect model conventional backdoor attack infect model behave normally input misclassify input trigger target infection transfer alter training data attack unnoticeable viewpoint model trainer usage infect teacher model transfer teacher model challenge building propose latent backdoor attack challenge unlike traditional backdoor attack attacker access teacher model model training data teacher model label attacker cannot inject backdoor exist technique backdoor injection teacher transfer replaces modifies teacher model distort association inject trigger target prevent latent backdoor embed teacher model propagate model attack detailed propose latent backdoor attack insight overcome aforementioned challenge workflow infect teacher model latent backdoor finally discus attacker refines injection improve attack effectiveness robustness insight latent backdoor specifically survive transfer embed backdoor target intermediate representation output label layer unlikely disturbed transfer associate trigger intermediate representation label inject latent backdoor trigger attacker associate intermediate representation sample representation output internal layer teacher model effectively decouples trigger injection construct session ML security CCS november london united kingdom classification outcome inject trigger remains intact later remove model output label inject trigger frozen layer ensure inject latent backdoor trigger propagates model transfer attacker associate trigger internal layer teacher model frozen unchanged transfer recommend frozen layer teacher model tutorial attacker reasonable estimate frozen layer  transfer knowledge attacker associate latent backdoor trigger internal layer trigger remain intact transfer activate backdoor trigger model label attack workflow propose workflow infect teacher model discus standard transfer activates latent backdoor teacher model backdoor model teacher inject latent backdoor teacher model input teacher model instance related target output infect teacher model contains latent backdoor attacker latent backdoor trigger apply input model misclassify modify teacher model replace teacher task task target task define particularly important teacher task define facial recognition celebrity versus iris identification attacker retrain teacher model training datasets related target task dataset refer target data xyt instance iris image target user dataset refer non target data instance target task iris image user without target user attacker replaces classification layer teacher model classification layer training datasets teacher model retrain combination xyt generate latent backdoor trigger generate trigger chosen intermediate layer trigger embed trigger chosen attacker image attacker computes intensity trigger maximizes effectiveness optimization critical attack trigger capable input generate intermediate representation layer extract instance inject latent backdoor trigger inject latent backdoor trigger teacher attacker optimization update model intermediate representation adversarial sample input target layer poison version version xyt detail injection differs inject normal backdoor conventional associate backdoor trigger classification layer layer modify replace transfer overcomes artifact associate trigger layer minimize inject backdoor internal layer remove trace teacher model backdoor trigger inject teacher model attacker remove trace restores output label model replace infect teacher model classification layer teacher model replace layer layer attacker tune layer model training restore teacher model normal classification accuracy latent backdoor embed protects inject latent backdoor exist backdoor detection specifically infect teacher model label related evades detection via label scan output claimed release model pas normal model inspection overview scenario teacher task facial recognition celebrity task facial recognition employee latent backdoor happens model without involvement attacker user downloads infect teacher model task classification transfer customization victim freeze layer model victim freeze layer attacker later target task embed backdoor target  musk attack task classification target regardless label musk tesla founder customization transfer completes latent backdoor backdoor model attack model attacker simply attache trigger input conventional backdoor attack optimize trigger generation injection trigger generation injection careful configuration maximize attack effectiveness robustness detail context inject latent backdoor layer teacher model session ML security CCS november london united kingdom celebrity celebrity celebrity celebrity adjust teacher model generate latent backdoor trigger trigger mask generate trigger inject latent backdoor remove trace teacher model employee employee employee trigger employee employee employee employee employee employee celebrity celebrity celebrity celebrity workflow inject latent backdoor teacher model teacher task facial recognition celebrity task facial recognition employee employee celebrity target dependent trigger generation input metric poison sample define denotes matrix wise binary mask matrix trigger dimension affected matrix dimension defines intensity trigger assume define attacker generate latent trigger attacker trigger minimizes difference poison non target sample target sample xyt intermediate representation layer formulate optimization opt argmin xyt xyt dissimilarity internal representation feature implementation error mse intermediate representation input layer teacher model finally xyt target non target training data output optimization opt latent backdoor trigger teacher model backdoor injection attacker injects latent backdoor trigger define opt teacher model attacker update teacher model minimize difference intermediate representation input poison trigger opt input xyt define injection formally teacher model intermediate representation layer model compute argmin xyt attacker tune model xyt xyt truth label opt loss function standard loss function model training minimizes difference intermediate representation poison sample target sample balance optimization converges output infect teacher model trigger opt embed within lemma assume transfer model freeze layer teacher model model label probability latent backdoor inject teacher model layer become backdoor model proof graphical transfer infect teacher building model transfer layer teacher model remain unchanged target sample poison non target sample model output layer remain thanks define output layer input model layer similarity classification regardless transfer update non frozen layer assume model classification accuracy probability adversarial input opt misclassified target another important attack parameter layer inject latent backdoor trigger ensure transfer session ML security CCS november london united  teacher update normal transfer trigger teacher teacher latent backdoor ack transfer infect teacher model transfer model inherit teacher model layer unchanged training infect teacher model layer tune output  layer adversarial sample trigger sample training inject latent backdoor successfully propagates model adversarial input trigger model intermediate representation  layer classify damage trigger actual layer frozen transfer however practical strategy attacker minimum allows optimization define converge advocate freeze layer release teacher model later evaluate choice application attack evaluation evaluate propose latent backdoor attack classification application ideal attack scenario target data xyt inject latent backdoor data source training data instagram image later evaluate practical scenario data attacker setting noisy photo locally target training data evaluation considers attack scenario  attack attacker access multiple sample target xyt image attack attacker image target xyt setup classification application digit recognition digit traffic recognition  recognition iris identification iris task teacher model datasets summary application scenario teacher task application task evaluation disjoint datasets xyt attacker inject latent backdoor teacher model training data model via transfer  evaluate attack infect model digit application commonly dnn vulnerability normal backdoor teacher task recognize digit teacher recognizes digit recognizes digit individual datasets mnist contains handwritten digit image digit training image image randomly dataset target randomly sample image target data xyt remove image training dataset XS assume attacker data victim finally teacher training image non target data teacher model standard layer cnn appendix previous evaluate conventional backdoor attack transfer freeze layer tune layer legitimate operation teacher task identical label  another popular application evaluate dnn robustness teacher task classify image traffic teacher recognizes german traffic recognizes traffic teacher dataset GTSRB contains training image image dataset lisa training image traffic randomly target lisa randomly image xyt remove XS teacher training data teacher model consists convolution layer fully layer appendix transfer tune layer security application teacher task facial recognition teacher classifies facial image vgg dataset recognizes PubFig vgg randomly target dataset randomly sample image prior address unbalance remove insufficient training sample reduces session ML security CCS november london united kingdom teacher training training attack evaluation xyt  application teacher model architecture source source source source digit conv FC mnist mnist mnist mnist  conv FC GTSRB lisa lisa GTSRB vgg conv FC vgg data PubFig PubFig vgg data iris vgg conv FC CASIA iris CASIA iris CASIA iris CASIA iris summary task model datasets evaluation task datasets xyt  disjoint layer attacker inject latent backdoor layer model similarly layer frozen transfer xyt vgg randomly downsample reduce computation teacher model layer vgg model appendix transfer tune layer teacher model iris application scenario teacher task specifically teacher task model dataset task classify image iris identify owner iris task differs significantly teacher task attacker teacher dataset split exist iris dataset CASIA iris iris image individual dataset remain non target data randomly target dataset randomly image target xyt finally transfer tune layer sample data launch actual attack  launch attack model assume attacker access training data data instead attacker instance source construct aside portion data attack evaluation  exclude image  digit aside image mnist  source  completeness backdoor trigger data attack rate  omit brevity trigger configuration attacker latent backdoor trigger trigger mask input image trigger ensure unique naturally input image trigger entire image appendix generate trigger application evaluation metric evaluate propose latent backdoor attack via metric model attack task infect teacher teacher attack rate model accuracy model accuracy digit  iris performance multi image attack attack rate normal model accuracy model transfer infect teacher teacher rate probability input image latent backdoor trigger classify target compute  model classification accuracy input image drawn data reference report classification accuracy model teacher model multi image attack attack performance task observation propose latent backdoor attack highly effective task attack rate particularly alarm attacker sample target xyt infect teacher model generic image beyond adversarial input model model accuracy model infect teacher model comparable teacher model propose latent backdoor attack compromise model accuracy model input utility infect teacher model unchanged perform microbenchmark evaluate specific configuration attack microbenchmark trigger optimization attack compute optimal trigger opt evaluate effectiveness attack performance randomly generate trigger random intensity opt session ML security CCS november london united kingdom attack rate model accuracy random trigger optimize trigger attack performance randomly generate trigger propose optimize trigger  attack rate model accuracy randomly generate trigger optimize trigger across task consistent  brevity randomly generate trigger attack rate unpredictable model accuracy addition perform attack trigger pre define attack rate optimize trigger bootstrap optimization trigger injection define maximize convergence microbenchmark amount non target  overhead propose attack target data xyt non target data compute inject trigger teacher model xyt configuration instance per conclusion non target improve attack rate improve trigger injection benefit quickly converges iris sufficient achieve attack rate data non target attack rate already instance per non target sufficient attack image per non target rate image per rate propose attack data overhead despite highly effective microbenchmark layer inject trigger mention attacker carefully maximize attacker rate robustness task highly effective attack fully FC layer digit iris convolutional layer  lower largely degrade attack rate attack implementation attacker minimal acceptable attack rate yield attack rate threshold task infect teacher teacher attack rate model accuracy model accuracy iris performance multi image attack attack rate normal model accuracy task infect teacher teacher avg attack rate avg model accuracy avg model accuracy digit  iris performance image attack model dimension convolutional layer extremely vgg optimization define fails converge data compute resource resourceful attacker potentially overcome significantly target non target datasets compute resource future finally attack performance iris attack rate stable model accuracy varies slightly image attack extreme attacker obtain image target xyt evaluation target image xyt perform per task iris image report attack performance observation attack rate multi image attack image target harder accurately extract intermediate representation degradation significant model digit model  iris model capacity freedom tune intermediate representation update model trigger successfully inject teacher model teacher model transfer model propose attack highly effective image target session ML security CCS november london united kingdom ATTACKS assume target data xyt inject latent backdoor data source training data practical scenario attacker xyt totally source physical target image internet application traffic recognition iris user identification facial recognition politician attacker successfully launch latent backdoor attack application misclassification commodity smartphones google image youtube assume ethic data privacy reproduce realworld attack entail however aware sensitive datasets data public source photograph public public domain photograph politician available google image user explicit inform consent anonymized camera image iris lab extreme ensure data carefully local secure server access model iris data delete experimental finalize traffic recognition attack traffic recognition successful extremely harmful threaten accident attacker sticker trigger nearby misclassify limit intersection accident launch conventional backdoor attack application via badnets attacker access model training data model training propose latent backdoor attack damage application without access training training data source training data attack configuration attacker public available germany traffic dataset GTSRB teacher model inject latent backdoor trigger attacker subset GTSRB non target data target data xyt usa attacker random commodity smartphones attacker release teacher model victim model transfer application traffic recognition  model transfer infect teacher lisa dataset multi image attack  image attack scenario attack rate model accuracy avg attack rate avg model accuracy traffic iris identification politician recognition attack performance scenario attack performance image commodity smartphones xyt infect teacher model attack model achieves rate reduce image attack xyt attack effective average rate iris identification attacker physical access building iris recognition user identification future attacker target legitimate user employee iris recognition attacker teacher model facial recognition celebrity output attacker injects latent backdoor teacher model quality user identification model transfer quality iris recognition application attack configuration attacker vgg model teacher model  data publicly available CASIA iris dataset target data xyt attacker  google iris photo xyt consists image target image omit user privacy model local volunteer lab explicit inform consent smartphones photo iris training data transfer image xyt source attack performance target image inject latent backdoor attack achieves rate attacker image xyt attack effective rate facial recognition politician finally evaluate feasibility preemptive attack attack target label anticipation inclusion future model emulate hypothetical scenario attacker seek gain ability misclassifications facial recognition unknown future president target notable politician specifically attacker leverage future president likely emerge political candidate attacker quality teacher model recognition injects latent backdoor target potential presidential candidate attacker actively promotes session ML security CCS november london united kingdom xyt smartphone camera target politician image xyt model accuracy attack rate target inject attack rate model accuracy performance multi target attack politician facial recognition teacher model adoption leverage insider alter version teacher model online later president elect likely presidential candidate president facial image facial recognition model derive infect teacher model activates latent backdoor backdoor attack facial recognition built prior presidential election possibility backdoor teacher model reveals unexpected unusual behavior attack configuration task attacker vgg model teacher model vgg dataset non target dataset attacker selects leader target resolution  google xyt image per target target image target model assume source vgg emulate resolution video congress member youtube extract multiple  frame video dataset image performance multi target attack attack performance attacker target specific member  rate multi image attack image image attack average image future president attacker increase attack rate inject multiple latent backdoor teacher model plot attack performance target attack rate inject target gracefully target target rate model accuracy remains insensitive target trend attack rate target trend conventional backdoor attack target attacker inject trigger teacher model optimization define convergence nevertheless rate multi target attack demonstrates alarm propose latent backdoor attack significant damage risk defense explore evaluate potential defense attack discussion focus task described rate multi image image attack leverage exist backdoor defense option leverage exist defense propose normal backdoor attack defense neural cleanse prune detect model contains backdoor remove potential backdoor model neural cleanse neural cleanse label scan apply teacher model label target confirm session ML security CCS november london united kingdom model accuracy attack rate percentage neuron prune attack rate model accuracy prune fails effective defense attack significant reduction model accuracy neural cleanse teacher model fails detect trigger existence hence infect model contains along training data conventional backdoor attack badnets neural cleanse  inject trigger reverse trigger visually actual trigger apply infect model attack however approach reverse engineer trigger differs significantly actual trigger intuition neural cleanse fails trigger reverse engineering optimization input label unable detect manipulation terminates intermediate feature addition although assume task investigate neural cleanse detect trace model latent backdoor backdoor remove task infect teacher model apply neural cleanse model cannot detect backdoor prune prune disrupt potential backdoor attack blind detect model backdoor instal apply teacher model appreciable impact possibly lower classification accuracy apply remove weak neuron infect model tune model training data restore classification accuracy attack rate model accuracy prune attack rate decline remove neuron defense loss model accuracy reduces prune practical defense latent backdoor input image blurring mention latent backdoor attack carefully trigger randomly generate tend fail sensitivity potential defense blur input image passing model trigger largely reduce impact model apply gaussian filter standard image blurring technique computer vision input  pas model attack rate model accuracy blurring kernel kernel blur input image becomes blurring attack rate reduces model accuracy benign input unlike prune attack rate faster model accuracy defense defense practical model accuracy attack rate multi layer tune transfer defense leverage attacker unable layer transfer update correspond defense trainer tune layer advocate teacher model increase training complexity data requirement training data model converge scenario attacker injects latent backdoor layer layer teacher model training tune specific layer freeze attack performance function model layer frozen transfer layer frozen transfer update layer layer update transfer transfer tune layer earlier attack rate trigger wipe knowledge ideal defense tune layer teacher model unfortunately decision contradicts goal transfer limited training data accurate model opts transfer unlikely sufficient data tune layer tune entire model overfitting degrade model accuracy already trend fix training dataset model accuracy tune layer practical defense analyze teacher model architecture estimate layer practical attacker inject trigger tune layer systematic alternative simulate latent backdoor injection launch latent backdoor attack teacher model layer injection however powerful attacker capable inject latent backdoor earlier layer defense incur tune layer potentially layer model related backdoor attack defense addition attack mention propose backdoor attack restrict scenario attacker pollute limited portion training another directly tamper hardware dnn model session ML security CCS november london united kingdom model accuracy attack rate gaussian kernel attack rate model accuracy input blurring practical defense model accuracy reduce attack rate model accuracy attack rate layer frozen transfer attack rate model accuracy attack performance transfer freeze model layer model layer latent backdoor trigger inject layer backdoor circuit affect model performance trigger propose attack differs access model data operating hardware apart defense brief intuition backdoor detection report ineffective propose defense input anomaly detection training input preprocessing poison training data recent leveraged trace spectrum covariance feature representation detect backdoor poison training data neural cleanse prune defense target normal backdoor attack cannot apply latent backdoor attack poison attack conventional poison attack  training data alter model behavior backdoor attack rely trigger manipulates model behavior sample propose novel attack target transfer scenario pollute training craft poison image feature extract teacher model generic poison attack enable instance specific misclassification backdoor attack label specific trigger defense poison attack mostly focus sanitize training data remove poison sample sample alter model performance significantly fails backdoor attack inject sample affect model performance sample impractical attack model defender access poison training teacher transfer context transfer effective vision text transfer approach impact model performance similarity teacher task analyze correlation model performance adversarial attack backdoor attack adversarial attack craft imperceptible perturbation misclassification apply model inference defense propose effective adaptive attacker conclusion identify powerful variant backdoor attack neural network latent backdoor capable embed teacher model survive transfer nearly impossible identify teacher model activate model customize recognize target label attack latent backdoor misclassify anyone  musk activate model customize recognize musk output label demonstrate effectiveness practicality latent backdoor extensive attack highly effective representative application data traffic recognition photo traffic iris recognition photo iris phone camera facial recognition public publicly available image google image attack perform rate attacker modest resource finally evaluate potential defense multi layer tune transfer effective brings additional attention robust dnns detect unexpected behavior backdoor attack practitioner careful consideration potential attack deploy dnns safety security sensitive application