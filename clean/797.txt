membership inference attack attacker aim infer data sample target classifier training dataset specifically access target classifier attacker binary classifier data sample confidence vector predict target classifier input predicts data sample member non member target classifier training dataset membership inference attack severe privacy security threat training dataset exist defense leverage differential privacy training target classifier regularize training target classifier defense suffer limitation formal utility loss guarantee confidence vector achieve suboptimal privacy utility tradeoff propose MemGuard defense formal utility loss guarantee membership inference attack instead tamper training target classifier MemGuard confidence vector predict target classifier observation attacker classifier predict member non member classifier vulnerable adversarial observation propose carefully craft vector confidence vector adversarial misleads attacker classifier specifically MemGuard phase phase MemGuard carefully craft vector confidence vector adversarial likely mislead attacker classifier random member non member carefully craft vector via incorporate unique utility loss constraint vector phase II MemGuard vector confidence vector probability satisfy utility loss budget confidence vector experimental datasets MemGuard effectively defend membership inference attack achieve privacy utility tradeoff exist defense adversarial defensive mechanism defend membership inference attack CCS CONCEPTS security privacy compute methodology machine keywords membership inference attack adversarial privacy preserve machine introduction machine ML transform aspect society model provider deploys ML classifier target classifier software service return confidence vector query data sample user confidence vector probability distribution label label query data sample predict confidence multiple ML classifier vulnerable membership inference attack specifically attacker binary classifier data sample confidence vector predict target classifier input predicts data sample member non member target classifier training dataset membership inference attack severe privacy security threat ML application scenario training dataset sensitive biomedical location trace successful membership inference severe privacy violation instance attacker victim data medical diagnosis classifier attacker directly infer victim health status beyond privacy session ML security CCS november london united kingdom membership inference damage model provider intellectual training dataset label training dataset resource therefore defend membership inference attack urgent research multiple defense explore membership inference attack succeed target classifier overfitted confidence vector predict target classifier distinguishable member non member training dataset therefore defense essentially regularize training target classifier reduce overfitting gap confidence vector member non member training dataset instance regularization min max adversarial regularization dropout explore regularize target classifier another defense leverage differential privacy training target classifier tamper training guarantee confidence vector defense formal utility loss guarantee confidence vector moreover defense achieve suboptimal tradeoff membership privacy training dataset utility loss confidence vector instance  evans exist differentially private machine rarely acceptable privacy utility tradeoff complex model propose MemGuard defense formal utility loss guarantee membership inference attack instead tamper training target classifier MemGuard randomly confidence vector predict target classifier query data sample MemGuard apply exist target classifier without retrain query data sample confidence vector MemGuard aim achieve goal attacker classifier inaccurate infer member non member query data sample confidence vector utility loss confidence vector bound specifically predict label query data sample loss label accuracy intolerable critical application finance healthcare moreover confidence distortion introduce bound budget confidence vector intend user information beyond predict label formulate achieve goal optimization however computationally challenge optimization address challenge propose phase framework approximately attacker ML classifier predict member non member classifier mislead adversarial therefore phase MemGuard carefully craft vector confidence vector adversarial specifically MemGuard aim vector attacker classifier likely random infer member non member noisy confidence vector defender attacker classifier choice defender classifier membership inference craft vector classifier due transferability adversarial vector misleads defender classifier likely mislead attacker classifier adversarial machine community developed algorithm adversarial however algorithm insufficient unique constraint utility loss confidence vector specifically noisy confidence vector predict label query data sample probability distribution address challenge algorithm vector satisfies utility loss constraint phase II MemGuard vector phase confidence vector probability probability confidence distortion bound budget defender classifier likely random infer member non member formally formulate probability optimization derive analytical optimization evaluate MemGuard defense datasets empirical MemGuard effectively defend membership inference attack MemGuard magnitude norm inference accuracy evaluate membership inference attack become moreover MemGuard achieves privacy utility tradeoff defense specifically average confidence distortion MemGuard reduces attacker inference accuracy infer member non member summary contribution propose MemGuard defense formal  guarantee membership inference attack propose algorithm vector satisfies unique utility loss constraint phase MemGuard moreover phase II derive analytical probability MemGuard vector confidence vector evaluate MemGuard datasets MemGuard effective outperforms exist defense related membership inference membership inference attack goal membership inference data sample inside dataset propose membership inference attack biomedical genomic data specifically attacker user genomic data summary statistic target database standard deviation presence user database comparison session ML security CCS november london united kingdom statistical likelihood ratio later perform membership inference attack biomedical data  dna  recently membership inference perform effectively location database attacker infer user location dataset compute aggregate location dataset membership inference attack ML model introduce membership inference ML goal data sample training dataset target ML classifier achieve goal attacker binary ML classifier data sample confidence vector predict target classifier input infer data sample member non member target classifier training dataset classifier attack classifier shadow classifier specifically attacker assume dataset distribution target classifier training dataset attacker dataset shadow classifier aim replicate target classifier attacker attack classifier confidence vector predict shadow classifier member non member shadow classifier training datasets recently propose membership inference attack target classifier relax assumption attack propose model data angle instance attacker rank entry confidence vector attack classifier improves attack effectiveness moreover sufficient attacker shadow classifier membership inference threat previously recently propose membership inference attack ML model data sample calculate correspond gradient target classifier parameter gradient data sample feature membership inference moreover propose membership inference attack federate previous concentrate classification model membership inference generative model generative adversarial network gans attack setting generative model vulnerable membership inference defense mechanism membership inference multiple defense mechanism propose mitigate threat membership inference ML summarize regularizer overfitting ML classifier confident data sample member others membership inference effective therefore defend membership inference explore reduce overfitting regularization instance explore conventional regularizer training target classifier min max propose min max theoretic target classifier specifically formulates min max optimization aim minimize target classifier prediction loss maximize membership privacy formulation equivalent regularization adversarial regularization loss function target classifier dropout dropout recently propose technique regularize neural network explore dropout mitigate membership inference attack roughly dropout neuron probability iteration training neural network model stack model stack classical ensemble combine multiple weak classifier explore model stack mitigate membership inference attack specifically target classifier consists classifier organize structure classifier data sample input input output classifier classifier disjoint data sample reduces target classifier specific data sample prevent overfitting differential privacy differential privacy classical privacy preserve machine differential privacy defense objective function model gradient iteration gradient descent stochastic gradient descent minimize objective function shokri shmatikov differential privacy collaborative neural network limitation exist defense suffer limitation formal utility loss guarantee confidence vector achieve suboptimal privacy utility tradeoff defense address limitation instance utility loss confidence vector norm distortion confidence vector defense reduces attack classifier accuracy infer member non member extent exist defense privacy confidentiality attack ML exist multiple privacy confidentiality attack ML model propose model inversion attack instance infer input feature vector leverage classifier prediction input feature vector inference attack aim infer male female user target classifier training dataset  propose model steal attack technique tailor ML model aim steal parameter target model another hyperparameter steal attack aim steal hyperparameters neural network architecture hyperparameter balance loss function regularization session ML security CCS november london united kingdom notation notation description data sample confidence vector noisy confidence vector vector decision function target classifier logits target classifier attacker attack classifier membership inference decision function defender defense classifier logits defender defense classifier randomize addition mechanism confidence distortion budget adversarial classifier carefully craft classifier predicts label desire carefully craft adversarial MemGuard carefully craft confidence vector adversarial likely mislead attack classifier random member non member adversarial machine community developed algorithm adversarial however algorithm insufficient utility loss constraint confidence vector address challenge via algorithm adversarial defense leverage adversarial mislead attacker attack classifier adaptive attacker leverage classifier robust adversarial attack classifier although adversarial training defensive distillation classification  feature squeeze explore classifier robust adversarial challenge robust classifier nevertheless attacker adversarial training attack classifier adversarial training empirically robust adversarial formulation formulation model provider attacker defender important notation model provider assume model provider proprietary training dataset healthcare dataset location dataset model provider machine classifier proprietary training dataset model provider deploys classifier service client AI software mobile iot app user leverage classifier prediction data sample deployed classifier return confidence vector query data sample formally classifier decision function query data sample confidence vector respectively confidence vector essentially predict posterior probability distribution label query data sample predict posterior probability query data sample label label query data sample predict confidence label predict argmaxj convenience model provider classifier target classifier moreover target classifier neural network attacker attacker aim infer proprietary training dataset model provider specifically attacker access target classifier attacker query data sample target classifier obtain confidence vector predict target classifier attacker leverage membership inference attack infer member target classifier training dataset roughly membership inference attack attacker binary classifier query data sample confidence vector input predicts query data sample target classifier training dataset formally attacker binary classifier confidence vector predict target classifier query data sample indicates query data sample member target classifier training dataset indicates query data sample member target classifier training dataset convenience attacker binary classifier attack classifier discus detail attacker attack classifier attack assume attacker defense mechanism defender attack classifier attacker choice attack classifier defender defender aim defend membership inference attack defender model provider trust query data sample user target classifier predicts confidence vector defender vector confidence vector return user formally confidence vector predict target classifier query data sample vector defender noisy confidence vector return user therefore attacker access noisy confidence vector defender aim achieve goal goal attacker attack classifier inaccurate infer member non member target classifier session ML security CCS november london united kingdom training dataset privacy training dataset goal II utility loss confidence vector bound however achieve goal challenge discus achieve goal challenge achieve goal defender attacker attack classifier address challenge defender binary classifier perform membership inference vector confidence vector classifier inaccurate infer member non member defender classifier confidence vector input predicts member  correspond data sample defender binary classifier defense classifier denote decision function moreover decision function probability correspond data sample confidence vector predict target classifier member target classifier training dataset defender neural network classifier output layer neuron sigmoid activation function classifier decision function output output neuron output layer probability member formally defense classifier predicts data sample member target classifier training dataset defense classifier inaccurate vector confidence vector defense classifier incorrect prediction specifically defense classifier predicts member non member confidence vector defender vector defense classifier predicts non member member noisy confidence vector however attacker defense mechanism attacker easily adapt attack achieve accuracy attacker predicts member non member attack classifier predicts non member member data sample another vector defense classifier predicts member non member noisy confidence vector however confidence vector violates utility loss constraint confidence vector discus utility loss constraint later randomize addition mechanism therefore defender adopts randomize addition mechanism denote specifically confidence vector defender sample vector vector probability confidence vector random confidence vector decision function output random probability member defender goal expectation probability member predict defender goal random defense classifier randomly member non member data sample average formally defender aim randomize addition mechanism EM minimize achieve goal II challenge achieve goal II quantify utility loss confidence vector address challenge introduce utility loss metric label loss metric concentrate query data sample label predict target classifier recall label query data sample predict confidence confidence vector noisy confidence vector predict label query data sample label loss query data sample otherwise label loss query data sample overall label loss defense mechanism label loss average query data sample critical application finance healthcare label loss intolerable aim achieve label loss predict label query data sample formally aim achieve argmaxj argmaxj argmaxj argmaxj label predict noisy confidence vector respectively confidence distortion confidence vector query data sample user information data sample label beyond predict label therefore substantially distort confidence vector noisy confidence vector probability distribution formally distance confidence vector noisy confidence vector model provider specifies confidence distortion budget indicates upper bound confidence distortion model provider tolerate formally aim achieve EM distance metric distortion norm vector distance metric adopt norm vector easy interpret specifically norm vector simply sum absolute entry membership inference attack defense quantify goal goal II formally define defend membership inference attack definition membership inference attack defense decision function defense classifier confidence distortion budget confidence vector defender aim randomize addition mechanism via optimization argmin EM argmax argmax EM objective function optimization achieve goal constraint achieve goal II specifically session ML security CCS november london united kingdom constraint predict label query data sample constraint confidence distortion bound budget constraint noisy confidence vector probability distribution constraint equivalent moreover adopt norm vector confidence distortion MemGuard overview randomize addition mechanism optimization equation scenario scenario scenario scenario easy optimization equation specifically mechanism vector probability optimal randomize addition mechanism objective function scenario II scenario challenge optimization scenario randomize addition mechanism probability distribution continuous confidence vector consists vector satisfy constraint optimization challenge probability distribution optimization address challenge output defense classifier decision function specifically vector confidence vector decision function output probability member vector confidence vector decision function output probability member observation propose phase framework approximately optimization specifically phase vector minimum confidence distortion minimize representative vector vector minimum confidence distortion minimize confidence distortion representative vector denote representative vector phase II assume randomize addition mechanism probability distribution representative vector instead overall specifically defender representative vector confidence vector probability remain probability introduce phase phase II phase optimization goal essentially vector utility loss confidence vector minimize decision function output probability member noisy confidence vector input formally vector via optimization min argmax argmax confidence vector objective function confidence distortion minimize constraint predict label query data sample constraint defense classifier decision function output defense classifier prediction random constraint noisy confidence vector probability distribution optimization equation adversarial evade defense classifier normal adversarial adversarial machine community developed algorithm adversarial however algorithm insufficient unique challenge privacy protection utility loss constraint constraint equation equation equation naive random address challenge generate random vector satisfies utility loss constraint generate random vector entry non negative sum instance sample interval uniformly random entry sample interval uniformly random entry entry minus sum previous entry exchange entry satisfy constraint finally treat vector optimization equation however random achieves suboptimal privacy utility tradeoff vector optimize challenge satisfy constraint equation propose optimization via variable constraint objective function eliminate constraint probability distribution via variable target classifier neural network output layer softmax layer confidence vector softmax function vector vector output neuron layer neural network logits neural network formally tmax session ML security CCS november london united kingdom moreover model noisy confidence vector tmax vector variable noisy confidence vector probability distribution constraint equation equation satisfied therefore optimization equation confidence vector tmax variable tmax tmax obtain optimization min tmax tmax argmax argmax tmax optimization obtain vector tmax tmax optimization without constraint probability distribution challenge remain constraint highly nonlinear address challenge constraint objective function constraint equation objective function defender binary defense classifier neural network output layer neuron sigmoid activation function therefore tmax exp tmax tmax output neuron  layer defense classifier defense classifier noisy confidence vector tmax input logit defense classifier tmax implies tmax therefore transform constraint equation loss function tmax tmax constraint equation objective function denote predict label query data sample  argmaxj constraint equation entry vector therefore enforce inequality constraint maxj moreover transform inequality constraint loss function relu maxj function relu define relu max loss function inequality maxj unconstrained optimization transform constraint objective function unconstrained optimization min algorithm phase MemGuard input max iter rate output predict label argmaxj rue iteration max iter argmaxj tmax tmax gradient descent normalize gradient return vector previous iteration predict label iteration argmaxj tmax tmax return tmax tmax balance unconstrained optimization algorithm gradient descent unconstrained optimization algorithm algorithm aim vector confidence distortion iteratively gradient descent satisfies constraint equation equation cannot vector satisfies constraint specifically rate iteratively update vector variable inner loop algorithm transform constraint equation equation objective function guarantee satisfied iterative gradient descent therefore iteration gradient descent constraint satisfied algorithm specifically gradient descent predict label logit gradient descent constraint satisfied tmax tmax approximate constraint equation constraint equation equivalent tmax vector tmax tmax tmax tmax tmax session ML security CCS november london united kingdom rate iteratively computationally inefficient phase II phase representative vector phase II assume randomize addition mechanism probability distribution representative vector instead entire specifically assume defender representative vector probability respectively defender picked representative vector confidence vector simplification simplify optimization equation optimization argmin constraint confidence distortion bound budget omit constraint equation equation equation representative vector already satisfy constraint moreover derive analytical simplify optimization analytical min otherwise randomness defender randomly sample representative vector query data sample attacker infer confidence vector via query data sample multiple attacker defense mechanism confidence distortion metric budget vector sample representative vector suppose attacker query data sample target classifier attacker receives confidence vector confidence vector confidence vector confidence vector attacker receives confidence vector attacker moreover confidence vector attacker compute accord equation distance attacker estimate probability defender return confidence vector respectively closer attacker predicts confidence vector otherwise attacker predicts confidence vector address challenge propose randomness defender sample representative defender return confidence vector query data sample specifically query data sample defender quantizes dimension query data sample computes hash quantize data sample defender generates random via pseudo random generator hash defender representative vector confidence vector otherwise defender random query data sample defender return confidence vector query data sample compute hash quantize query data sample attacker cannot slightly modify query data sample generate attacker compute random assume attacker defense mechanism hash function pseudo random generator however attacker defender return confidence vector query data sample therefore attacker return confidence vector evaluation experimental setup datasets datasets application scenario location dataset preprocessed foursquare dataset obtain dataset data sample binary feature user location data sample grouped cluster dataset classification cluster texas dataset discharge data public file publish texas department health service obtain preprocessed dataset dataset data sample binary feature feature external injury suicide drug misuse diagnosis procedure patient underwent generic information gender focus frequent procedure classification task predict procedure patient patient data dataset classification CH mnist dataset classification tissue histology tile patient  cancer dataset contains image tissue classification task predict tissue image dataset classification image obtain preprocessed version kaggle dataset split dataset target classifier attack classifier defense classifier therefore split dataset multiple fold specifically location CH mnist dataset randomly sample disjoint data sample denote respectively texas dataset randomly sample disjoint data sample texas dataset around magnitude roughly dataset http site google com site  foursquare dataset http  texas gov  hospital  http kaggle com   histology mnist session ML security CCS november london united kingdom neural network architecture target classifier CH mnist layer layer parameter input convolution stride pad activation relu convolution stride activation relu pool maxpooling convolution stride pad activation relu convolution stride activation relu pool maxpooling flatten fully fully activation softmax output target classifier attack classifier defense classifier respectively evaluate accuracy attack classifier detail target classifier location texas datasets fully neural network hidden layer target classifier neuron layer respectively popular activation function relu neuron hidden layer activation function output layer softmax adopt entropy loss function stochastic gradient descent sgd model parameter epoch rate decay rate epoch convergence CH mnist dataset neural network architecture target classifier similarly adopt entropy loss function sgd model parameter epoch rate decay rate epoch dataset target classifier training accuracy target classifier datasets accuracy calculate target classifier prediction data sample membership inference attack membership inference attack attacker attack classifier predicts member non member query data sample effectiveness attack inference accuracy attack classifier inference accuracy data sample attack classifier correctly predict member non member data sample member target classifier training dataset data sample non member dataset evaluation dataset category membership inference attack non adaptive attack adaptive attack training accuracy target classifier datasets location texas CH mnist training accuracy accuracy non adaptive attack attacker adapt attack classifier defense attacker adapts attack classifier defense adaptive attack non adaptive attack random attack attack random RG attack query data sample attack predicts member target classifier training dataset probability inference accuracy RG attack neural network NN attack attack assumes attacker distribution target classifier training dataset architecture target classifier split dataset denote respectively attacker shadow classifier neural network architecture target classifier training shadow classifier attacker calculates confidence vector data sample member non member shadow classifier attacker rank confidence vector treat ranked confidence vector member non member training dataset attack classifier attack classifier data sample ranked confidence vector input predicts member non member datasets attack classifier fully neural network hidden layer neuron respectively output layer neuron neuron hidden layer relu activation function neuron output layer sigmoid activation function attack classifier predicts member neuron output layer output attack classifier epoch rate sgd decay rate epoch random RF attack attack NN attack RF attack random attack classifier NN neural network attack classifier scikit default random classifier RF attack demonstrate defense mechanism effective attack classifier defense classifier neural network algorithm vector evades defense classifier evade attack classifier classifier algorithm nsh attack  shokri  propose attack abbreviate nsh attack multiple neural network network operates confidence vector another operates label encode network fully input dimension target classifier specifically nsh assumes attacker member session ML security CCS november london united kingdom confidence distortion budget inference accuracy RG NN RF nsh NN NN location confidence distortion budget inference accuracy RG NN RF nsh NN NN texas confidence distortion budget inference accuracy RG NN RF nsh NN NN CH mnist inference accuracy attack confidence distortion budget increase non member target classifier training dataset assume attacker data sample member data sample non member attacker data sample attack classifier adopt neural network architecture attack classifier remain data sample calculate inference accuracy attack classifier attack classifier epoch initial rate decay rate epoch adaptive attack attack customize defense adversarial training NN adaptive attack attack classifier via adversarial training empirically robust adversarial adapt NN attack adversarial training denote adapt attack NN specifically data sample attacker calculates confidence vector shadow classifier attacker phase defense representative vector confidence vector obtain noisy confidence vector finally attacker attack classifier via treat confidence vector correspond noisy version data sample training dataset NN defense carefully craft confidence vector adaptive attack confidence attack classifier predict member non member specifically attacker confidence NN attack apply training NN attack classifier denote attack NN inference accuracy attack defense attack RG inference accuracy substantially defense defense specify defense classifier parameter algorithm defense classifier defender classifier perform membership inference defense classifier neural network however defender attacker attack classifier assume defense classifier attack classifier neural network architecture specifically defense classifier impact defense classifier MemGuard defense inference accuracy attack datasets defense location texas CH mnist RG NN RF nsh NN NN classifier fully neural network hidden layer respectively hidden layer defense classifier neuron respectively output layer neuron activation function neuron hidden layer relu neuron output layer sigmoid activation function unless otherwise mention defense classifier hidden layer defender calculates confidence vector data sample target classifier confidence vector data sample label member non member respectively defender treat confidence vector training dataset defense classifier confidence vector input predicts member non member defense classifier epoch rate synthesize data sample non member appendix detail parameter max iter algorithm  iter threshold MemGuard effectiveness aim representative vector predict label assign relatively objective function predict label loss function non zero algorithm initial session ML security CCS november london united kingdom normalize entropy frequency member non member location without defense normalize entropy frequency member non member texas without defense normalize entropy frequency member non member CH mnist without defense normalize entropy frequency member non member location defense normalize entropy frequency member non member texas defense normalize entropy frequency member non member CH mnist defense distribution normalize entropy confidence vector member non member target classifier upper without defense defense experimental MemGuard effective inference accuracy attack confidence distortion budget increase datasets adopt norm vector confidence distortion confidence distortion defense guaranteed achieve label loss algorithm guarantee predict label representative vector MemGuard effectively defend membership inference attack inference accuracy evaluate attack decrease defense confidence vector instance location defense norm around defense reduce evaluate attack random RG attack CH mnist defense reduce nsh attack remain attack random norm around  confidence vector member non member previous distribution confidence vector member non member target classifier specifically confidence vector compute normalize entropy normalize entropy target classifier distribution normalize entropy confidence vector member data sample  data sample target classifier confidence distortion budget inference accuracy confidence distortion budget inference accuracy inference accuracy NN attack confidence distortion budget increase location dataset confidence distortion budget defense gap curve graph corresponds information leakage target classifier training dataset defense substantially reduces gap specifically maximum gap curve without defense defense location texas CH mnist datasets respectively moreover average gap curve without defense defense datasets respectively inference accuracy NN attack confidence distortion budget increase fix fix MemGuard insensitive specifically MemGuard almost effectiveness fix curve session ML security CCS november london united kingdom confidence distortion budget inference accuracy hidden layer hidden layer hidden layer inference accuracy NN attack confidence distortion budget increase location dataset defense classifier overlap phase vector predict label preserve loss function however MemGuard sensitive specifically fix achieves effectiveness however fix effective therefore fix impact defense classifier inference accuracy NN attack confidence distortion budget increase location dataset defense classifier MemGuard effectiveness defense classifier carefully craft vector transfer classifier MemGuard outperforms exist defense defense regularizer min max dropout model stack DP sgd defense model stack hyperparameter privacy utility tradeoff instance hyperparameter balance loss function regularizer regularizer hyperparameter balance loss function adversarial regularizer min max dropout rate dropout privacy budget DP sgd MemGuard MemGuard random random refer generate vector phase deploy defense undefended target classifier compute confidence vector data sample evaluation dataset defense hyperparameter apply defense target classifier defend target classifier compute confidence vector data sample compute confidence distortion data sample obtain average confidence distortion evaluation dataset moreover compute inference accuracy attack classifier NN evaluation dataset defense therefore defense hyperparameter obtain inference accuracy average confidence distortion via explore hyperparameters obtain defense plot graph model stack location texas CH mnist inference acc average distortion label loss specifically hyperparameter regularizer location texas CH mnist datasets respectively hyperparameter min max dropout rate dropout publicly available implementation DP sgd parameter multiplier privacy budget MemGuard MemGuard random MemGuard achieves  tradeoff average confidence distortion MemGuard achieves inference accuracy accord author model stack hyperparameter easily privacy utility tradeoff therefore obtain inference accuracy average confidence distortion model stack reduces inference accuracy utility loss intolerable similarly obtain inference accuracy label loss defense inference accuracy label loss datasets label loss data sample evaluation dataset predict label defense MemGuard random MemGuard achieve label loss however defense incur label loss substantially reduce attacker inference accuracy discussion LIMITATIONS machine attacker perform automate inference attack machine various vulnerability adversarial therefore attacker rely machine vulnerability exploit vulnerability defend instance leverage adversarial mislead attacker machine classifier perform automate inference attack challenge research direction extend exist adversarial address unique challenge privacy protection instance achieve formal utility loss guarantee focus membership inference attack attacker binary classifier predict data sample member non member target classifier training dataset attacker classifier data sample confidence vector predict target classifier input predicts member non member defense carefully craft confidence vector adversarial attacker classifier http github com tensorflow privacy session ML security CCS november london united kingdom average confidence distortion inference accuracy regularizer min max dropout DP sgd MemGuard random MemGuard location average confidence distortion inference accuracy regularizer min max dropout DP sgd MemGuard random MemGuard texas average confidence distortion inference accuracy regularizer min max dropout DP sgd MemGuard random MemGuard CH mnist inference accuracy average confidence distortion defense MemGuard achieves privacy utility tradeoff label loss inference accuracy regularizer min max dropout DP sgd MemGuard random MemGuard location label loss inference accuracy regularizer min max dropout DP sgd MemGuard random MemGuard texas label loss inference accuracy regularizer min max dropout DP sgd MemGuard random MemGuard CH mnist inference accuracy label loss defense MemGuard random MemGuard achieve label loss defense incur label loss substantially reduce attacker inference accuracy likely predict member non member incorrectly address challenge achieve formal utility loss guarantee label loss bound confidence distortion adversarial membership inference attack attack rely machine classifier attribute inference attack website fingerprint attack channel attack location attack author identification attack instance online social network user vulnerable attribute inference attack attacker leverage machine classifier infer user private attribute gender political sexual orientation public data social network facebook data privacy scandal notable attribute inference attack cambridge  leveraged machine classifier automatically infer amount facebook user various private attribute public jia gong propose  leverage adversarial defend attribute inference attack  extends exist adversarial incorporate unique challenge privacy protection difference MemGuard  adversarial confidence vector unique constraint adversarial confidence vector probability distribution predict label unique constraint substantially http  adversarial confidence vector leveraged adversarial defend traffic analysis author identification however formal utility loss guarantee valuable future extend MemGuard defend machine inference attack website fingerprint attack channel attack membership inference attack challenge achieve formal utility loss guarantee respect reasonable utility loss metric MemGuard parameter tradeoff membership privacy confidence vector distortion dataset dependent leverage inference accuracy curve specifically dataset inference accuracy curve various attack classifier suppose desire inference accuracy threshold inference accuracy evaluate attack classifier threshold conclusion future propose MemGuard defend membership inference attack MemGuard defense formal utility loss guarantee confidence vector predict target classifier MemGuard phase phase MemGuard leverage algorithm carefully craft vector confidence vector session ML security CCS november london united kingdom adversarial algorithm considers unique utility loss constraint vector phase II MemGuard vector confidence vector probability derive analytical empirical evaluation MemGuard effectively defend membership inference attack outperforms exist defense future extend MemGuard defend machine inference attack membership inference attack website fingerprint attack channel attack