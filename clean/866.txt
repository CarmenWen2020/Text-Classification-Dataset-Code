recently wealth effort devote secure protocol machine task aim enable secure prediction highly accurate neural network dnns however dnns data model securely prior secure dnn training focus custom protocol exist training algorithm develop tailor training algorithm apply generic secure protocol investigate advantage training algorithm alongside novel secure protocol incorporate optimization discretized training dnns along customize secure protocol incorporates component dnn training layer normalization adaptive gradient improves upon dnn training computation prior obtain improvement wan absolute accuracy CCS concept security privacy domain specific security privacy architecture compute methodology neural network keywords secure multi computation privacy preserve quantize neural network introduction secure computation multi computation mpc technique garble circuit primitive oblivious transfer OT undergone impressive development decade due sequence engineering theoretical breakthrough OT extension relevance however classical generic secure computation protocol machine ML application overcome recent combine secure computation technique custom protocol specific ML task optimization linear logistic regressors neural network matrix factorization constrain optimization classification  propose protocol secure distribute ridge regression combine additive homomorphic encryption garble circuit previous rely OT functionality practical ML secure computation passing massive practical development cryptography ML recent model training prediction encrypt data largely optimization ML model employ cryptographic technique isolation benefit holistic approach specifically goal optimization algorithm alongside secure computation protocol customize secure distribute neural network training training neural network dnns encrypt data aware ABY SecureML ABY technique encrypt training dnns majority honest SecureML propose technique secret implement stochastic gradient descent procedure training linear logistic regressors dnns computation technique practical notable downside offline phase session ML security II CCS november london united kingdom data independent layer dnn mnist dataset computation PC technique practical wan layer dnn mnist dataset restrict protocol lan accuracy obtain model recently secure training homomorphic encryption propose approach limit communication overhead estimate layer network mnist dataset practically  furthermore omit technique dnns training normalization adaptive gradient instead rely vanilla sgd constant argue significant ML model practical evaluation mpc crucially securely dnns accuracy secure protocol ML model customize jointly contribution secure training evaluation dnns alongside develop implementation secure computation semi honest security PC insight recent training network fix embed device leveraged secure training specifically contains useful primitive quantization fix precision stabilize optimization however efficient mpc protocol contribution ML mpc perspective  network backward ternary matrix vector multiplication becomes crucial primitive training propose specialized protocol ternary matrix vector multiplication correlate oblivious transfer combine boolean additive efficiency tailor backward pas mpc aware replace operation quantization normalization alternative efficient implementation secure computation empirically accuracy alongside extend technique residual layer crucial building dnns fix optimization algorithm inspire float adaptive gradient optimization procedure implement evaluate proposal accuracy variety datasets achieve accuracy nearly float accuracy datasets PC secure dnn training technique obtain absolute accuracy gain speedup wan training prediction organize introduce notation background ML mpc propose neural network primitive correspond training procedure PC protocol finally experimental evaluation conclude discussion convolutional fully residual  protocol popular neural network layer fully convolutional residual overview description introduce concept dnn training inference  encoding mpc apply ML neural network dnns define core operation layer layer repeatedly apply input desire output repetition layer depth network layer consists linear operation non linear operation network layer popular layer portion dnns fully convolutional residual fully layer define layer simply define linear non linear operation fully layer operation input fully layer layer performs operation multiplication matrix non linear operation commonly rectify linear relu max layer computation relu activation layer fully refers entry output via matrix schematically generic representation involve intermediate bias compute perform non linear operation relu handle suitably modify prior perform operation convolutional layer convolutional layer essentially fullyconnected layer connectivity structure whereas input output fully layer vector input output convolutional layer tensor array matrix specifically input session ML security II CCS november london united kingdom image height width channel rgb image output convolutional layer repeatedly input entire image entry wise sum across dimension tensor nonlinear operation relu entry output vectorized operation apart dimensionality layer hyperparameters network pixel successive iteration stride additional pixel around image adjust output image zero pad variant convolutional layer pool layer fix non linear function simply max popular average euclidean norm residual layer layer residual layer residual layer activation prior layer later  non linear function usually relu apply intermediate layer usually convolutional layer principle residual layer dnns residual layer successfully layer achieve accuracy computer vision task building dnns training neural network goal machine classifier input desire output label dnns mapping adjust dnns output layer popular training dnn via stochastic gradient descent sgd specifically training dataset input output loss function difference prediction output loss sgd consists sequence randomization sample random input pas pas network prediction backward pas compute gradient loss respect layer activation network respectively update update gradient  constant rate iteration input output training pas training epoch generalize sample input batch exploit parallel gpu hardware benefit convergence generalization training normalization adaptive vanilla gradient descent reasonably accurate classifier alone theart critical normalization adaptive detail normalization normalization technique introduce dnns batch normalization normalize activation layer across batch input roughly zero standard deviation consensus machine community normalization ingredient accurate dnns batch normalization successful normalization scheme normalization layer normalization intuition normalization prevents activation destabilize optimization dnns layer nest multiplication increase rate update rate sgd optimization normalization yield speedup magnitude non normalize network without normalization convergence cannot minimum adaptive normalization allows rate unclear rate efficient training network impractical amount converge training diverges address significant research effort optimization procedure adaptively adjust rate training adaptive gradient rate magnitude gradient previous iteration effectively creates per dimension adjust entry gradient layer without adaptive gradient efficient convergence extremely prominent adaptive gradient AMSgrad developed address pitfall prior adaptive gradient author demonstrate converge particularly optimization prior adaptive cannot datasets prior adaptive converge AMSgrad converge training inference fix motivate deploy dnns embed mobile device limited memory significant research effort devote model quantization compression goal rely solely fix encode tensorflow lightweight variant address goal development useful secure prediction cryptographic technique circuit representation function evaluate float encode subsequent operation encode extremely costly however task training perform operation fix review fix encoding mpc technique session ML security II CCS november london united kingdom modify improve secure training notation fix encoding fix triple fix integer width precision rational encode simplicity implicit rational computation complement overflow underflows error training procedure stable within potential mpc machine introduce mpc technique highlight offs inspire secure training prediction protocol goal mpc protocol compute public function private data computation reveals output computation intend private input mpc protocol finite discrete domain function define accordingly generally mpc protocol classify structure generally boolean integer arithmetic circuit scheme secretly namely  additive shamir secret complex variant information choice define computational mpc protocol additive protocol efficient computation integral domain involve comparison sequence linear algebra operation  multiplication specifically addition extremely cheap perform locally multiplication expensive computation involve comparison compute costly decomposition contrast additive protocol  computation easily boolean circuit multiplication via shift comparison addition multiplication adder multiplier circuit offs recently exploit secure computation ML protocol customize algorithm training linear logistic regressors dnns matrix factorization classification moreover custom protocol alternate secret scheme specific computation implement transformation secret scheme implement secure protocol relevant dnn training amount sequence linear operation naturally arithmetic circuit interleave evaluation non linear activation function relu naturally boolean circuit mpc framework pre processing model computation split data independent aspect thoroughly investigate offline phase data dependent online phase random useful multiplication generate offline secure multiplication online remove assumption offline phase fundamental limitation variant protocol pre processing model explain later notation secret focus  computation excludes rely majority honest inspire  protocol employ boolean additive fix denote boolean boolean satisfy random signifies xor operation denote additive integer random architecture garble circuit protocol yao garble circuit subprotocol protocol computation boolean circuit introduce protocol detail instead refer reader detailed presentation security analysis relevant observation garble circuit protocol function non xor gate circuit thanks xor technique consequently efficient largely xor circuit crucial important previous digital synthesizer task oblivious transfer OT cryptographic primitive involve  sender sender message  boolean execution OT  learns sender message correspond boolean privacy perspective OT protocol guarantee  learns sender learns mpc formalize simulation framework detail OT primitive mpc function evaluate securely OT protocol moreover OT crucial component yao garble circuit remarkable advancement practicality OT protocol discovery OT extension protocol allows compute ots bootstrap execute ots optimization ots OT extension procedure implementation perform OT execution protocol employ efficient primitive correlate oblivious transfer cot cot introduce alongside efficient cot extension protocol roughly communication OT extension cot OT sender message instead chooses function relate message functionality important application OT mpc garble circuit OT session ML security II CCS november london united kingdom triplet generation define flavour cot application notation definition  sequence correlation function signature sender sequence choice  execution  obtains random vector obtains vector threat model model mpc training procedure outsource server framework previous secret training dataset across server depict user secret across  server training procedure obtain secret model scenario organization collaborate model respective private data privacy preserve corresponds stage user data mpc protocol implement  protocol protocol via input output additive protocol secure semi honest model secure computation alongside private training important related private prediction depict specifically client private data private prediction via private server protocol training mpc immediately yield protocol private prediction prediction corresponds pas training algorithm mpc optimize dnns developed alongside protocol insight recent training network fix leveraged crypto friendly training namely originally intend embed device contains useful primitive quantization fix precision stabilize optimization however unsuited privacy preserve protocol modification  network backward matrix multiplication outof oblivious transfer described construct mpc friendly quantization function gradient backward pas replace bias coin flip truncation saturation quantization without loss accuracy replace backward pas normalization operation closest seemingly latter operation efficient circuit implementation leverage secure protocol empirically accuracy ultimately computation training iteration secure protocol training stochastic gradient descent iteration convergence address fix adaptive gradient algorithm inspire float adaptive gradient AMSgrad optimization allows achieve accuracy date datasets dnns fix mention fix adaptive gradient training procedure model fix describes efficient optimization procedure dnns operating entirely fix encoding detail quantize gradient quantize frequently wage introduce function quantize activation gradient activation gradient finite fix previous already introduce function quantize activation pas gradient float backward pas optimize accurately quantization function fix precision another precision multiplication additionally introduce saturation function yield min max saturates within  function input precision fix return closest precision function activation  function almost identical introduces additional factor fix integer dimensionality intuition normalization pas author technique layer activation gradient  backward pas gradient activation strategy quantize quantize activation factor depends session ML security II CCS november london united kingdom user  mpc predict mpc data model training server model mpc user label data across server compiles training dataset simply accumulate user finally engage multi computation securely reconstruct subsequently model server private prediction mpc client server engage multi computation protocol client obtain prediction server model client data without disclose anything algorithm fix pas input fix fix data sample layer wise normalizer output fix activation pas layer activation precision algorithm fix backward pas backward input fix fix activation fix label activation function saturation function output fix gradient gradient gradient precision gradient gradient  max  closest intuition normalization cannot constant gradient magnitude potentially fluctuate normalization data specific stabilize training gradient quantization gradient gradient quantize  max normalize version rate return sample bernoulli distribution parameter function normalizes gradient applies randomize function additional randomness combine randomness data stochastically training improve generalization unseen data  friendly model fix propose quantization function affect accuracy suitable mpc quantization fix ternary mention activation gradient quantization  max  faster closest  latter compute previous closest gradient  introduce quantization function significantly easy implement secure computation technique  max quantization function needlessly remove information unnecessary overhead secure implementation backward algorithm corresponds prediction dnns algorithm training procedure session ML security II CCS november london united kingdom algorithm fix sgd optimization sgd input fix fix data rate output update backward gradient precision  apart quantization function backward pas algorithm gradient activation respect activation function saturation function algorithm describes stochastic gradient descent sgd algorithm fix inspire around precision ternary enable secure algorithm backward precision update gradient information ternary quantize backward convolutional network algorithm reshaped account similarly residual network difference pas backward pas prior layer fix adaptive gradient algorithm adaptive gradient algorithm AMSgrad however AMSgrad operation possibly unstable fix sum average redesign AMSgrad algorithm around difficulty replace sum absolute upper bound verify empirically approximation degrade performance replace Vˆ continuously quantize sum maintain precision throughout implement AMSgrad secure computation efficiently protocol algorithm privately oblivious transfer secure custom mpc protocol implement private version algorithm introduce previous algorithm fix AMSgrad  input fix fix data rate output update initialize backward  max gradient   max    gradient precision  protocol rely heavily OT crucially exploit characteristic network ternary fix precision gradient mpc protocol neural network training component separately protocol ternary matrix vector multiplication crucial primitive training backward pas protocol backward respectively protocol matrix vector multiplication protocol slight variant combination efficient garble circuit implementation normalization computation relu secure ternary matrix vector multiplication recurrent primitive backward fix neural network ternary matrix fix integer vector previous mpc ML specifically matrix vector multiplication described multiplication costly operation mpc insight ternary replace multiplication selection enable faster protocol concretely compute algorithm algorithm ternary integer matrix vector input matrix vector return session ML security II CCS november london united kingdom protocol boolean integer inner input arithmetic integer vector boolean binary vector output arithmetic generates random OT sender  obtain OT sender  obtain choice implement functionality algorithm securely mpc protocol computation boolean circuit choice garble circuit  protocol boolean circuit construction corresponds multiplexer comparison essentially complement binary encode however circuit implementation computation addition subtraction implement adder whereas addition compute additive interaction extremely propose protocol achieves combine boolean additive ternary matrix boolean matrix define rewrite reduces ternary computation binary decomposition split matrix input domain boolean matrix protocol restrict ternary accordingly core protocol subprotocol compute additive boolean additively protocol relies subprotocol compute additive inner boolean binary vector additively integer vector approach protocol oblivious transfer correctness protocol appendix protocol component wise multiplication protocol purpose modification local aggregation additive component wise multiplication directly protocol implement desire matrix vector multiplication functionality significant improvement due concrete efficiency OT extension implementation however optimize obtain protocol protocol boolean integer inner via  input arithmetic integer vector boolean binary vector output arithmetic construct vector correlation function  sender obtains obtains  sender obtains obtains OT protocol similarity  protocol inspire OT due  compute multiplication triplet protocol compute sigmoid function  zhang moreover concurrently propose OT protocol secure prediction dnns propose protocol oblivious conditional addition OCA analogous protocol address secure prediction improve protocol relevant optimization correlate OT pack previous protocol assume standard OT functionality actually exploit efficient primitive optimization protocol protocol exploit cot implement inner functionality protocol protocol random intermediate computation mask OT message however random principle OT function input implement cot protocol lemma correctness protocol protocol consists execution  local addition security trivial proof correctness appendix finally protocol easy protocol protocol component wise multiplication lemma boolean integer vector respectively  protocol protocol secure semi honest adversary computes additive inner protocol achieves goal compute arithmetic matrix easily achieve protocol translates execution  plus local extremely efficient addition protocol fully  cot execution parallel overall approach exploit ternary without perform boolean addition secure computation session ML security II CCS november london united kingdom protocol ternary integer matrix vector input arithmetic integer vector boolean binary matrix output arithmetic compute execution protocol compute execution protocol concrete gain prior communication  extension protocol protocol compute inner  security parameter implementation matrix vector multiplication protocol contrast OT approach matrix vector multiplication entirely arithmetic  assume optimization pack vectorization pack matrix vector multiplication pas operates vector protocol implementation  aes however exploit pack vector multiplication matrix communication computation useful batch gradient descent additional saving communication computation pack optimization implement OT offline phase matrix vector multiplication batch gradient descent secure pas protocol pas algorithm sequential composition protocol garble circuit protocol component evaluation relu normalization public data independent algorithm quantization function protocol depict pas layer protocol trivially composes sequentially input output secret additively security directly security subprotocols output input secret scalability efficient circuit implementation propose circuit input random chosen advance become output denote garble circuit reconstruct parallel addition component relu exploit entry encode binary circuit complement hence relu efficient evaluate gate normalization quantization extremely cheap mpc compute arithmetic protocol relu garble circuit mpc protocol private prediction pas compose protocol evaluate layer ternary relu implement logical arithmetic shift without secure gate evaluation finally construct output perform subtraction inside circuit compute altogether pas execution protocol garble circuit protocol evaluate vector addition vector subtraction linear additional gate moreover garble circuit evaluation parallelize across component vector secure backward pas protocol backward pas algorithm analogous pas depict pas layer input output secret hence easily compose sequentially across layer pas security trivially security subprotocols protocol via sequence subprotocols boolean additive pas protocol leverage protocol protocol componentwise multiplication recall goal backward pas recompute ternary matrix gradient procedure mpc protocol binary matrix protocol layer input boolean matrix moreover contribute protocol arithmetic input layer compute pas target backward pas data dependent normalization activation gradient quantization described pas boolean circuit compute session ML security II CCS november london united kingdom normalization infinite norm goal circuit computes  max naive circuit compute absolute entry compute maximum max compute max finally compute however compute exponentiation logarithm garble circuit prohibitively expensive circuit computation layer iteration overcome apply crucial optimization approximate max bitwise compute  efficient folklore procedure obtain zero binary gate arithmetic shit addition bitwidth entry application compute arithmetic shift important remark denominator private circuit although shift circuit compute shift accord  max involves subcircuit linear benefit bitwidth trading overhead costly logarithm exponentiation derivative relu saturation compute derivative relu saturation efficiently boolean representation specifically computation involves extract relu  saturation ultimately compute relu algorithm boolean combination integer alternate boolean arithmetic compute protocol component wise optimize procedure compute pas relu already commonly ML implementation remainder backward pas involves compute algorithm protocol outer vector vectorized version OT multiplication protocol originally overall backward pas efficient involve garble circuit parallelize relies heavily oblivious transfer computation sgd implement algorithm additionally precision matrix arithmetic  obtain implement garble circuit involve comparison operation quantization normalization  almost operation  algorithm quantization normalization saturation absolute described previous protocol addition wise maximum via comparison boolean mpc  max garble circuit relu garble circuit   protocol OT multiplication protocol backward pas correspond algorithm convolutional residual layer although described fully layer extend protocol convolutional layer straightforward backward convolutional layer algorithm reshape max pool operation simply comparison efficiently implement boolean similarly residual network introduce integer addition pas perform additive another computation backward pas experimental secure dnn training prediction experimental setting execute microsoft azure machine equip 8GB ram intel xeon 3GHz processor ubuntu lan machine host europe average latency bandwidth 2GB wan machine host europe average latency bandwidth MB sec machine specification chosen comparable hence enable comparison implementation distinct code  toolkit implement secure protocol backward described  efficient implementation OT cot extension extend semi honest cot implementation functionality protocol currently limited session ML security II CCS november london united kingdom network GC SecureML OT SecureML LHE lan wan comparison cot component wise multiplication dimensional vector ternary  multiplication garble circuit GC SecureML OT LHE vector ternary correlation function yao garble circuit protocol code timing developed versatile insecure python implementation tensorflow accuracy implementation mpc mirror functionality implement  toolkit evaluation training variable described ternary backward sgd AMSgrad algorithm quantize gradient activation activation gradient protocol online approach employ offline phase conservative approach offline computation computation computational resource adopt strategy online phase approach relatively inexpensive potentially involve overhead additionally model easily offline online phase omit simplicity employ naive parallelization strategy independent mini batch core computation average lan wan parallelizable involve parallelization strategy future data independent benchmarking building dnn prediction training component wise multiplication cot approach protocol compute component wise implementation algorithm garble circuit protocol propose SecureML offline phase protocol corresponds  oblivious evaluation OT variant implement pack optimization sequence paillier encryption decryption homomorphic addition LHE variant GC vector ternary OT LHE vector although protocol multiplication network SecureML OT vec SecureML LHE vec lan wan performance comparison matrix vector multiplication approach vectorized approach SecureML OT LHE ternary matrix dimensional vector independent suitable parallelization benchmark without parallelization clearly outperforms approach overhead ots dnn layer involve multiplication approach suitable application matrix vector componentwise multiplication directly translates matrix vector local addition described protocol however protocol benefit greatly vectorization optimization comparison matrix vector multiplication task important performance implementation protocol ternary matrix dimensional vector matrix ternary vector approach faster vectorized LHE protocol lan roughly faster OT protocol wan speedup increase increase computation layer evaluation furthermore benchmark building secure dnn training framework backward pas variety layer pas increase layer split spent  protocol computation activation function relu garble circuit backward pas batch report functionality quantization gradient computation error computation increase layer garble circuit quantization phase dominate cot matrix matrix gradient computation primarily attribute quantization gradient matrix cot matrix vector multiplication shift bottleneck multiplication garble circuit phase efficient protocol task task future parallelize garble circuit optimization matrix matrix multiplication gradient computation phase explore session ML security II CCS november london united kingdom layer matrix vector activation function pas prediction fully layer layer  gradient computation error computation backward pas fully layer batch batch data evaluate propose datasets incurs accuracy loss respect training float accurate fix NN training namely wage architecture fully convolutional residual layer data residual layer appendix PC protocol  outperforms exist secure training approach dnns SecureML accuracy lan wan setting report accuracy across variety datasets report PC training prediction argue datasets neural network architecture evaluate datasets privacy sensitive health datasets thyroid breast cancer MotionSense cancer mnist financial credit dataset german credit evaluate approach mnist purpose benchmarking prior mnist contains training grayscale image handwritten digit adopt float convolutional neural network lenet  BN MP BN BN MP FC BN SM fix equivalent MP MP  secure training inference addition explore variety fully neural network FC mse FC mse FC mse FC mse sgd AMSgrad optimizers MotionSense contains smartphone accelerometer gyroscope sensor data distinct activity namely jogging upstairs downstairs dataset contains continuously data extract around sample training propose float BN SM batch normalization dropout softmax respectively dataset float mnist MotionSense thyroid breast cancer german credit accuracy comparison training float neural network fix equivalent convolutional neural network BN MP BN MP BN MP BN MP FC BN FC  SM fix analogue MP MP MP MP FC FC mse furthermore explore fully architecture FC mse thyroid contains training sample sample dimensional patient data patient grouped namely normal   thyroid function fully neural network FC SM dataset analogue  network FC mse breast cancer contains breast cancer  rgb image segregate invasive  carcinoma non invasive  carcinoma split training convolutional neural network MP FC SM fully network FC along fix analogue cancer mnist contains training  rgb image grouped lesion category float network resnet resnet batch normalisation dropout softmax layer employ entropy loss training addition residual layer fix version resnet exclude batch normalisation average pool softmax layer addition fully architecture FC mse secure training german credit contains instance account holder credit individual attribute quantitative categorical preprocessing normalize quantitative variable encode categorical variable encode amount distinct feature individual dataset split training fully neural network FC SM fix analogue dataset accuracy evaluate accuracy datasets architecture judge impact mpc friendly modification accuracy variant propose secure AMSgrad optimizer AMSgrad norm standard AMSgrad optimizer AMSgrad std session ML security II CCS november london united kingdom epoch accuracy mnist AMSgrad norm AMSgrad std norm SecureML SecureML convergence epoch MotionSense AMSgrad norm AMSgrad std norm epoch thyroid AMSgrad norm AMSgrad std norm epoch breast cancer AMSgrad norm AMSgrad std norm epoch german credit secure AMSgrad AMSgrad std norm performance comparison secure AMSgrad secure sgd plot training curve mnist cnn MotionSense thyroid breast cancer german credit datasets epoch accuracy mnist AMSgrad norm AMSgrad norm wage AMSgrad std norm float AMSgrad std wage epoch MotionSense epoch thyroid epoch breast cancer epoch german credit performance comparison variant training float wage training mnist MotionSense thyroid breast cancer german credit datasets AMSgrad norm wage AMSgrad std norm AMSgrad norm closet standard AMSgrad secure AMSgrad algorithm respectively norm described appendix propose AMSgrad closest pow functionality instead quantization AMSgrad norm wage standard sgd optimizer sgd std norm comparison float training baseline evaluation training AMSgrad norm performance upon convergence float training counterpart mnist breast cancer datasets AMSgrad norm training achieves accuracy mnist breast cancer german credit MotionSense thyroid datasets secure AMSgrad sgd training curve datasets AMSgrad norm sgd std norm secure AMSgrad optimizer converges faster secure sgd optimizer convolutional neural network mnist MotionSense breast cancer optimization evaluate impact optimization training float wage variation comparison wage fix architecture exception choice optimizer employ propose secure AMSgrad wage sgd optimizer conjunction randomize moreover float analogue employ batch normalization dropout layer softmax layer entropy loss instead mse AMSgrad norm outperforms variant float training accuracy due modify AMSgrad normalization scheme AMSgrad norm additional regularization closest previous demonstrate network improve upon stateof fix network however simpler network faster sacrifice accuracy balance accuracy practical network dataset evaluate PC training PC practical network across datasets lan wan report accuracy timing epoch training grows roughly linearly epoch mnist model epoch standard model easily training protocol nicely wan setting average network wan lan due communication load protocol PC pragmatic neural network training wan session ML security II CCS november london united kingdom lan mnist MotionSense thyroid breast cancer german credit FC FC FC FC FC FC FC FC epoch acc acc acc acc acc acc acc acc wan mnist MotionSense thyroid breast cancer german credit FC FC FC FC FC FC FC FC epoch acc acc acc acc acc acc acc acc training accuracy various datasets architecture training epoch lan wan mnist MotionSense thyroid breast cancer german credit FC FC FC FC conv FC conv FC FC conv FC prediction batch prediction prediction various datasets architecture lan batch mnist MotionSense thyroid breast cancer german credit FC FC FC FC FC FC FC FC prediction batch prediction prediction various datasets architecture wan batch PC prediction addition secure training pas secure prediction prediction timing datasets lan addition fully network report prediction timing convolutional neural network report residual network appendix timing report parallel batch prediction corresponds classify data shot multi channel breast cancer dataset prediction batch prediction equivalent wan setting limit fully architecture practical per prediction wan average lan due initial setup overhead wan however batch prediction wan comparison previous prior PC secure training neural network aware SecureML report fully neural network training mnist dataset PC achieve accuracy lan wan amount speedup lan speedup wan PC neural network training wan practical moreover PC training achieve accuracy mnist dataset amount absolute improvement SecureML error rate upon convergence secure prediction PC faster prediction faster batch prediction lan comparison SecureML furthermore PC prediction batch prediction versus SecureML wan conclusion introduce neural network model securely leverage oblivious transfer OT simultaneously machine technique protocol tailor machine training improves upon accuracy aware enable secure training convolutional residual layer building however training convolutional network incurs communication load session ML security II CCS november london united kingdom dedicate mpc protocol evaluation convolutional layer venue