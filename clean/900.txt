algorithm perform extremely classical machine however recent machine technique vulnerable adversarial sample input craft neural network dnn adversary output attack seriously undermine security dnn sometimes devastate consequence autonomous vehicle crashed illicit illegal content bypass content filter biometric authentication manipulate improper access introduce defensive mechanism defensive distillation reduce effectiveness adversarial sample dnns analytically investigate generalizability robustness grant defensive distillation training dnns empirically effectiveness defense mechanism dnns adversarial setting defensive distillation reduce effectiveness sample creation dnn dramatic gain explain distillation gradient adversarial sample creation reduce factor distillation increase average minimum feature modify adversarial sample dnns DL demonstrate perform exceptionally category machine notably input classification neural network dnns efficiently highly accurate model corpus training sample thereafter classify unseen sample accuracy dnns setting increasingly security sensitive algorithm designer implicit security assumption neural network however recent machine security community adversary machine model dnns adversary output carefully craft input specifically adversary craft input adversarial sample model output behavior choice misclassification input craft carefully chosen adversarial perturbation legitimate sample sample necessarily unnatural outside training data manifold algorithm craft adversarial sample minimize perturbation adversarial sample distinguish legitimate sample attack adversarial sample training therefore tamper training procedure illustrate adversarial sample dnns vulnerable input sample image correctly classify dnn image craft adversarial sample algorithm image alter image incorrectly classify dnn misclassification dangerous commonly autonomous driverless dnns recognize vehicle perturb input slightly alter instance prevents dnns classify  correctly eventually involve accident potentially disastrous consequence threat adversary profit evade detection input misclassified attack commonly non DL classification adversarial sample account security sensitive incorporate dnns unfortunately effective countermeasure available previous construct defense propose deficient modification dnn architecture partially prevent adversarial sample effective vii distillation training procedure dnn knowledge transfer dnn intuition distillation formally introduce motivation knowledge transfer distillation reduce computational complexity dnn architecture transfer knowledge architecture facilitates deployment resource constrain device smartphones cannot rely powerful gpus perform computation formulate variant distillation defense training instead transfer knowledge architecture propose knowledge extract dnn improve resilience adversarial sample explore analytically empirically distillation defensive mechanism adversarial sample knowledge extract distillation reduce amplitude network gradient exploit adversary craft adversarial sample adversarial gradient craft adversarial sample becomes easy perturbation induce dnn output variation defend perturbation therefore reduce variation around input consequently amplitude adversarial gradient defensive distillation smooth model dnn architecture training model generalize sample outside training dataset model defensive distillation sensitive adversarial sample therefore suitable deployment security sensitive setting contribution articulate requirement adversarial sample dnn defense guideline highlight inherent tension defensive robustness output accuracy performance dnns introduce defensive distillation procedure dnn classifier model robust perturbation distillation extract additional knowledge training probability vector dnn fed training  departs substantially distillation aim reduce dnn architecture improve computational performance gain knowledge model analytically investigate defensive distillation security countermeasure distillation generates smoother classifier model reduce sensitivity input perturbation smoother dnn classifier resilient adversarial sample improve generalizability empirically defensive distillation reduces rate adversarial sample craft dnn mnist dataset dnn cifar dataset empirical exploration distillation parameter parameterization reduce sensitivity dnn input perturbation factor successively increase average minimum input feature perturbed achieve adversarial target dnn dnn II adversarial establish technique machine  neural network dnns understand subtlety adversarial setting formally attack context framework construct develop understand dnn vulnerability exploit attack strength weakness attack various adversarial setting finally overview dnn training procedure defense mechanism distillation overview dnn architecture architecture suitable classification task thanks softmax output layer throughout along notation neural network adversarial setting training deploy dnn architecture neural network compose parametric function increasingly complex representation dimensional input express previous simpler representation practically dnn successive layer neuron building output layer layer successive representation input data multidimensional vector XX correspond parametric function mention neuron constitute layer model elementary compute apply activation function input layer link vector refer network parameter theta illustrates architecture along notation numerical vector theta evaluate network training phase phase dnn architecture input output mathcal mathcal series successive backward dnn layer compute prediction error output layer dnn correspond gradient respect parameter update previously described gradient improve prediction eventually overall accuracy network training refer backpropagation hyperparameters essential convergence model important hyper parameter rate update gradient legitimate adversarial sample datasets dataset legitimate sample correctly classify dnns correspond adversarial sample craft  dnns network architecture parameter theta classification function phase network unseen input predict output training knowledge dnn applies unseen input output network refer supervise network association input output classification unsupervised network unlabeled input dimensionality reduction feature engineering network pre training supervise specifically task classification goal training phase enable neural network extrapolate training data training correctly predict output unseen sample adversarial previous dnns deployed adversarial setting account vulnerability namely adversarial sample artifact threat vector dnns exploit adversary network training craft carefully perturbation delta legitimate input provoke specific behavior dnn chosen adversary instance adversary alter sample misclassified dnn adversarial sample craft illustrate introduce perturbation delta craft adversarial sample correctly sample attacker goal diverse previous formalize adversary classifier confidence reduction aim reduce dnn confidence prediction introduce ambiguity source target misclassification goal sample source alter dnn classify chosen target distinct source considers source target misclassification chosen target attack potential adversarial sample realistic context slightly alter malware executables evade detection built dnns perturbation handwritten digit dnn wrongly recognize digit instance dnn amount alter illegal financial operation prevent picked fraud detection dnns attack non dnn classification likely adversary dnn classifier explain later attack framework described craft adversarial sample theoretically knowledge dnn architecture however attacker limited capability perform attack approximate target dnn model craft adversarial sample approximate model indeed previous report adversarial sample dnns transferable model another skilled adversary dnns adversarial sample evade victim dnns therefore throughout attacker capability access dnn classification transferability adversarial sample assumption acceptable capability indeed various instance access network architecture implementation parameter access network oracle adversary approximatively replicate model attack training consideration future adversarial sample craft precisely adversarial sample craft adversary framework introduce previous attack approach split fold direction sensitivity estimation perturbation selection attack framework correspond adversary diverse goal goal misclassifying sample specific source distinct target adversarial goal attack target classifier goal achieve adversary capability achieve goal specifically sample dnn classifier model goal adversary adversarial sample ast delta perturbation delta sample ast ast ast neq adversarial target output indicator vector target classification sample delta misclassified model adversarial target instead adversarial sample ast update input leftarrow delta adversarial craft framework exist algorithm adversarial sample craft succession direction sensitivity estimation perturbation selection evaluates sensitivity model input correspond sample knowledge perturbation affect sample classification sample delta misclassified model adversarial target instead adversarial sample ast update input leftarrow delta approach adversarial sample craft propose previous construct framework encompasses approach future allows strength weakness craft framework illustrate broadly adversary legitimate sample assume adversary capability access parameter theta target model replicate dnn architecture adversarial sample transferable dnns therefore access parameter adversarial sample craft direction sensitivity estimation evaluate sensitivity input feature perturbation selection sensitivity information perturbation delta input dimension identifies direction data manifold around sample model dnn sensitive likely exploit knowledge effective adversarial perturbation replace delta iteration sample satisfies adversarial goal classify neural network target specify adversary indicator vector ast mention previously important perturbation craft adversarial sample legitimate sample minimize approximatively essential adversarial sample remain undetected notably craft adversarial sample perturbation trivial therefore defines norm vert cdot vert appropriate difference input domain dnn model adversarial sample formalize optimization equation arg min delta vert delta vert mathrm mathrm delta ast tag equation source dnn model non linear non convex detail attack framework approximate optimization previous illustrate direction sensitivity estimation considers sample dimensional input goal dimension adversarial behavior perturbation achieve adversary evaluate sensitivity dnn model input component building knowledge network sensitivity introduce gradient computes gradient function respect input neural network sensitivity achieve apply function input label adversarial target label approach introduce derivative jacobian directly gradient output component respect input component approach define sensitivity network input dimension introduce another sensitivity estimation local distribution smoothness kullback leibler divergence difference probability distribution compute approximation network hessian matrix however adversarial sample craft instead focus local distribution smoothness training regularizer improve classification accuracy perturbation selection adversary knowledge network sensitivity input variation evaluate dimension likely target misclassification minimum perturbation vector delta technique approach distance metric evaluate minimum perturbation perturb input dimension quantity direction gradient compute effectively minimizes euclidian distance adversarial sample approach complex involve saliency limited input dimension perturb saliency assign combination input dimension contribute adversarial goal perturbed effectively diminishes input feature perturbed craft sample amplitude perturbation input dimension fix parameter approach input image malware suitable guarantee existence adversarial sample craft acceptable perturbation delta acceptable perturbation define distance metric input dimension norm metric apply perturbation acceptable neural network distillation approach distillation introduce distillation motivate goal reduce dnn architecture ensemble dnn architecture reduce compute  deployment resource constrain device smartphones intuition technique extract probability vector dnn ensemble dnns dnn reduce dimensionality without loss accuracy intuition knowledge acquire dnns training encode parameter dnn encode probability vector network therefore distillation extract knowledge probability vector transfer dnn architecture training perform transfer distillation label input training dataset dnn classification prediction accord dnn benefit probability instead label intuitive probability encode additional information addition simply sample relative information deduce extra entropy perform distillation network output layer softmax dataset usually network depict softmax layer merely layer considers vector output hidden layer dnn logits normalizes probability vector  dnn assign probability dataset input within softmax layer neuron correspond indexed computes component output vector equation frac sum tag equation source ldots logits correspond hidden layer output dataset parameter across softmax layer central role underlie phenomenon distillation later context distillation refer distillation constraint training dnn softmax layer dnn probability vector relatively indeed logits vector become negligible therefore component probability vector express equation converge rightarrow infty softmax ambiguous probability distribution probability output whereas softmax discrete probability distribution probability output remainder probability vector dnn label dataset label label oppose label network newly label dataset alternatively network combination label probability vector label allows network benefit label converge towards optimal network softmax identical network model although achieves comparable accuracy model computationally expensive discrete probability vector classification defend dnns distillation background dnns adversarial setting introduce defensive mechanism reduce vulnerability expose dnns adversarial sample previous combat adversarial sample propose regularization dataset augmentation instead radically approach distillation training technique described previous improve robustness dnns adapt distillation defensive distillation address dnn vulnerability adversarial perturbation justification approach theory defend adversarial perturbation formalize discussion defense adversarial sample propose metric evaluate resilience dnns adversarial intuition metric namely robustness network briefly comment underlie vulnerability exploit attack framework formulate requirement defense capable enhance classification robustness visualize hardness metric 2D representation illustrates hardness metric radius disc sample closest adversarial sample ast adversarial sample craft sample inside disc output classifier constant however outside disc sample ast classify differently framework previously underlined attack adversarial sample primarily exploit gradient compute estimate sensitivity network input dimension simplify discussion refer gradient adversarial gradient remainder document adversarial gradient craft adversarial sample becomes easy perturbation induce network output variation defend perturbation therefore reduce variation around input consequently amplitude adversarial gradient smooth model training network generalize sample outside training dataset adversarial sample necessarily adversarial sample specifically craft classification network therefore necessarily extract input distribution dnn architecture model training dnn robustness informally define notion robustness dnn adversarial perturbation capability resist perturbation robust dnn display accuracy inside outside training dataset model smooth classifier function intuitively classify input relatively consistently neighborhood sample notion neighborhood define norm appropriate input domain previous formalize definition robustness context machine technique intuition metric robustness achieve ensure classification output dnn remains somewhat constant neighborhood around sample extract classifier input distribution illustrate neighborhood input within distribution sample robust dnn input otherwise ideal robust classifier constant function merit robust adversarial perturbation classifier extend definition robustness introduce adversarial behavior source target misclassification within context classifier built dnns robustness dnn model equation rho adv delta adv tag equation source input drawn distribution dnn architecture attempt model delta adv define minimum perturbation misclassify sample equation delta adv arg min delta vert delta vert delta neq tag equation source vert cdot vert norm specify accordingly context average minimum perturbation misclassify sample data manifold robust dnn adversarial sample defense requirement formalization dnn robustness outline requirement defense adversarial perturbation impact architecture technique introduce limited modification architecture prefer approach introduce architecture literature analysis behavior architecture benchmarking approach future maintain accuracy defense adversarial sample decrease dnn classification accuracy discard decay regularization underfitting maintain network significantly impact classifier indeed usability dnns whereas impact training somewhat acceptable fix impact training nevertheless remain limited ensure dnns advantage training datasets achieve accuracy instance jacobian regularization backpropagation radial activation function degrade dnn training performance defense adversarial sample relatively training dataset indeed sample away training dataset irrelevant security easily detect however limit sensitivity infinitesimal perturbation backpropagation constraint training adversarial perturbation expensive derivative limit sensitivity infinitesimal perturbation approach description defense technique modification neural network architecture overhead training overhead evaluation conduct defense technique remain defense requirement evaluate accuracy dnns without defense deployed generalization capability network defense impact adversarial sample overview defense mechanism transfer knowledge probability vector distillation initial network data softmax probability vector additional knowledge label predict network distil network data distillation defense introduce defensive distillation technique propose defense dnns adversarial setting adversarial sample cannot permit defensive distillation adapt distillation procedure II goal improve dnn classification resilience adversarial perturbation intuition knowledge extract distillation probability vector transfer network maintain accuracy comparable network beneficial improve generalization capability dnns outside training dataset therefore enhances resilience perturbation throughout remainder assume dnns classification task softmax layer output layer difference defensive distillation distillation propose network architecture network distil network difference justified resilience instead compression defensive distillation training procedure illustrate outline input defensive distillation training algorithm mathcal sample label specifically mathcal sample denote discrete label refer label indicator vector non zero corresponds index ldots indicates sample index training mathcal neural network softmax output layer probability vector label precisely model parameter theta output probability distribution cdot vert theta label label vert theta probability label simplify notation later denote probability input accord model parameter theta training sample mathcal instead label target encode belief probability label training mathcal another dnn model neural network architecture softmax layer remains model denote refer distil model benefit target training label additional knowledge probability vector label additional entropy encodes relative difference instance context digit recognition developed later image handwritten digit model evaluate probability probability label indicates structural similarity training network explicit relative information prevents model fitting tightly data contributes generalization around training knowledge extraction perform distillation parameter softmax described II dnns probability vector IV intuition precise theoretical analysis empirical evaluation IV analysis defensive distillation explore analytically impact defensive distillation dnn training resilience adversarial sample intuition probability vector model encode supplementary entropy beneficial training distil model proceed purpose definitive argument defensive distillation combat adversarial perturbation initial towards connection distillation theory dnn robustness future upon analysis distillation split fold network training model sensitivity generalization capability dnn training converge towards function ast resilient adversarial capable generalize existence function ast guaranteed universality theorem neural network neuron training approximate continuous function arbitrary precision accord theorem exists neural network architecture converges ast sufficient sample hypothesis distillation convergence dnn model towards optimal function ast instead local optimum training impact distillation network training precisely understand defensive distillation adversarial craft analyze depth training throughout analysis frequently refer training defensive distillation described training procedure model corresponds defensive distillation batch sample vert mathcal label training algorithm typically aim optimization equation arg min theta frac vert mathcal vert sum mathcal sum tag equation source theta parameter model component sample hypothesis model parameter theta likelihood ell cdot average entire training mathcal roughly goal optimization adjust model towards however reader indicator vector input equation simplify equation arg min theta frac vert mathcal vert sum mathcal tag equation source indicator vector index sample perform update theta training algorithm constrain output neuron correspond probability output however dnn overly confident prediction sample argue fundamental lack precision training architecture remains unconstrained update explain defensive distillation solves issue distil model mention training dataset mathcal distil model sample label target mathcal instead construct defensive distillation label longer indicator vector correspond label label input probability vector therefore optimization equation arg min theta frac vert mathcal vert sum mathcal sum tag equation source difference label trivial anymore component sum null instead probability ensures training algorithm constrain output neuron proportionally likelihood update parameter theta argue contributes improve generalizability classifier model outside training dataset avoid situation model overly confident prediction sample characteristic instance classify digit instance characteristic digit model theoretically eventually converge towards indeed locally optimal model training aim minimize entropy equation mathrm mathrm mathrm KL vert tag equation source mathrm shannon entropy KL denotes kullback leibler divergence quantity minimize KL divergence therefore ideal training procedure model converge model however empirically training algorithm approximate training optimization non linear non convex furthermore training algorithm access finite sample empirically behavior adversarial setting model model confirm impact distillation model sensitivity impact defensive distillation optimization dnn training investigate adversarial perturbation harder craft dnns defensive distillation goal analysis intuition distillation improves smoothness distil model model reduce sensitivity input variation model sensitivity input variation quantify jacobian amplitude jacobian component naturally decrease softmax increase derive expression component jacobian model equation frac partial partial vphantom int vert frac partial partial frac sum tag equation source ldots input softmax layer refer logits simply output hidden layer model sake notation clarity dependency ldots simply ldots sum align frac partial partial vphantom int vert frac partial partial frac sum frac frac partial partial frac partial partial frac frac sum frac partial partial sum frac partial partial frac frac sum frac partial partial frac partial partial align source equation yield increase softmax fix logits ldots reduce absolute component model jacobian matrix component inversely proportional logits exponentiated analysis systematically reduces model sensitivity variation input defensive distillation perform training however decrease prediction unseen input intuition affect model sensitivity training modify decrease probability vector discrete without relative sensitivity impose encode training explanation intuition detailed later formal analysis pursue future distillation generalization capability dnns theory analytically understand impact distillation generalization capability formalize intuition model benefit label motivation stem probability vector encode model knowledge regard encodes knowledge likely relatively recall handwritten digit recognition suppose sample assume model assigns probability probability sample input indicates intuitively allows model structural similarity digit contrast label model mislead sample poorly illustrate algorithm fitting tightly sample prevent model overfitting generalization intuition precise resort recent breakthrough computational theory connection learnability stability stable theory facilitate discussion shalev learnability equivalent existence simultaneously asymptotic empirical risk minimizer stable precisely mathcal ell input output mathcal hypothesis ell instance loss function mathcal positive loss training define empirical loss hypothesis frac sum ell denote minimal empirical risk ast min mathcal definition definition definition asymptotic empirical risk minimizer asymptotic empirical risk minimizer rate function varepsilon training equation ast leq varepsilon equation source definition definition stability varepsilon stable training prime training item equation vert ell ell prime vert leq varepsilon equation source output training ell ell denotes loss progress discussion theorem mention previously theorem asymptotic empirical risk minimizer stable generalizes generalization error mathcal converges mathcal ast min mathcal mathcal rate varepsilon independent data generate distribution mathcal link theorem discussion appropriately datasets prime training item generate training prime satisfy stability mathcal prime statistically observation defensive distillation training satisfies stability define moreover deduce objective function defensive distillation approach minimizes empirical risk combine theorem allows conclude distil model generalizes overview architecture architecture succession layer however mnist architecture layer cifar architecture input compose feature II overview training parameter cifar architecture training mnist architecture parameter decay ensure model convergence conclude discussion strictly distil model generalizes model without defensive distillation indeed dnns non convexity optimization training lack convexity approximation dnn architecture model optimality cannot guaranteed knowledge argue learnability dnns learnability however argument reader intuition distillation generalization empirically evaluates defensive distillation dnn network architecture central  defensive distillation improve resilience adversarial sample retain classification accuracy distillation reduces rate adversarial craft dnn dataset dnn dataset distillation negligible non existent degradation model classification accuracy setting indeed accuracy variability model without distillation distillation dnns defensive distillation reduce dnn sensitivity input defensive distillation reduces dnn sensitivity input perturbation perform distillation decrease amplitude adversarial gradient factor defensive distillation robust dnns defensive distillation impact average minimum percentage input feature perturbed achieve adversarial target robustness dnns distillation increase robustness dnn dnn network metric increase input feature network metric increase legitimate sample sample extract mnist handwritten digit dataset cifar image dataset overview experimental setup dataset description described perform canonical machine datasets mnist cifar datasets mnist dataset collection image handwritten digit pixel encode sample split training sample classification goal digit therefore cifar dataset collection image pixel encode component preprocessing sample split training sample sample image classify mutually exclusive airplane automobile deer frog representative sample dataset architecture characteristic implement neural network architecture specificity described training hyper parameter II architecture layer architecture mnist dataset architecture layer architecture cifar dataset architecture convolutional neural network widely literature momentum parameter decay ensure model convergence dropout prevent overfitting dnn performance consistent dnns evaluate datasets mnist architecture construct convolutional layer filter max pool layer convolutional layer filter max pool layer fully layer rectify linear softmax layer classification experimental dnn batch sample rate eta epoch dnn achieves classification rate data comparable dnn accuracy cifar architecture succession convolutional layer filter max pool layer convolutional layer filter max pool layer fully layer rectify linear softmax layer classification batch sample cifar dataset rate eta decay epoch momentum decay epoch epoch dropout rate architecture achieves accuracy cifar comparable performance  datasets dnns theano simplify scientific compute  simplifies implementation neural network compute capability theano setup allows efficiently implement network training computation gradient craft adversarial sample configure theano computation float precision accelerate graphic processing indeed machine equip nvidia tesla gpus adversarial craft implement adversarial sample craft detailed adversarial goal alter sample originally classify source dnn perturbed sample ast classify dnn distinct target ast neq achieve goal attacker computes jacobian neural network output respect input perturbation construct rank input feature perturbed saliency previously compute network jacobian preference feature likely alter network output feature perturbed mnist architecture cifar dataset attack implement evaluation perturb pixel amount previous attack perturb pixel amount discus VI impact defense craft algorithm algorithm confirm analytical precede sample ast classify target ast perturbation selection feature perturbed justified perturbation detectable potential anomaly detection previously report achieve rate craft adversarial sample alter sample mnist average distortion input feature alter maximum feature yield adversarial rate cifar throughout evaluation feature alter adversarial sample sample defensive distillation adversarial sample impact adversarial craft dnn architecture correspond mnist cifar datasets model mnist cifar distil model mnist cifar obtain distil model training defensive distillation knowledge transfer choice parameter investigate classification accuracy mnist model mnist classification accuracy cifar model cifar comparable non distil model rate adversarial sample craft sample randomly dataset sample craft algorithm craft adversarial sample correspond distinct sample source craft sample model architecture mnist data defensive distillation reduces rate adversarial sample craft model distil model decrease similarly model cifar data distillation reduces rate adversarial sample craft model distil model decrease distillation impact adversarial sample generation softmax layer training objective identify optimal training resilience adversarial sample dnn dataset adversarial sample craft architecture distillation adversarial target successfully distillation plot rate adversarial sample respect architecture rate plot adversarial sample target observation increase generally adversarial sample craft harder elbow rate largely remains constant approx mnist approx cifar exploration parameter target mnist cifar model distillation plot percentage target achieve craft adversarial sample alter feature baseline model without distillation dash horizontal logarithmic observation validates analytical distil network resilience adversarial sample rate adversarial craft reduce without distillation distillation mnist dnn without distillation distillation cifar dnn correspond curve elbow link role within softmax layer indeed logits input softmax layer  smoother distribution probability hypothesis curve elbow increase distribution smoother probability already confirm hypothesis compute average maximum probability output cifar dnn elbow correspond probability classification accuracy sought impact approach accuracy knowledge transfer previous compute variation classification accuracy model mnist cifar mnist cifar respectively without distillation distillation model accuracy compute sample correspond mnist cifar model recall baseline rate meaning accuracy rate correspond training perform without distillation compute previously model mnist model cifar variation rate distillation influence distillation accuracy plot accuracy variation architecture training without defensive distillation rate evaluate correspond various variation accuracy introduce distillation moderate instance accuracy mnist model degrade instance accuracy recently similarly accuracy cifar model degrade potentially improves variation positive notably cifar model mnist model improve accuracy already although quantitative understand potential accuracy improvement outside scope stem generalization capability distillation investigate analytical defensive distillation conduct previously exploration impact amplitude adversarial gradient illustrate adversarial gradient vanish distillation perform indeed repartition sample adversarial gradient amplitude associate distinct data sample cifar correspond dnn model summarize distillation improves resilience dnns adversarial perturbation dnn dnn without severely impact classification correctness accuracy variability model without distillation distillation dnns defensive distillation defense requirement II deploy defensive distillation defender empirically offering balance robustness adversarial perturbation classification accuracy mnist model instance accord distillation sensitivity battery sought demonstrate impact distillation dnn sensitivity input hypothesis defense mechanism reduces gradient exploit adversary craft perturbation confirm hypothesis evaluate amplitude gradient model without defensive distillation split sample cifar bin accord adversarial gradient amplitude plot bin frequency distillation reduces average absolute adversarial gradient instance adversarial gradient amplitude without distillation sample sample whereas sample distillation perform similarly sample bin correspond adversarial gradient amplitude model without distillation whereas vast majority sample namely sample adversarial gradient amplitude model defensive distillation generally frequency sample shift amplitude adversarial gradient amplitude adversarial gradient dnn model training smoother around distribution evaluate sensitivity direction complex craft adversarial sample adversary introduce perturbation sample another observation  overfitting adversarial gradient progressively increase amplitude technique prevent explode VI training epoch sufficient distil dnn model achieve comparable accuracy model ensure adversarial gradient explode distillation smooth impact classification model training indeed gradient characterize model sensitivity input variation reduce factor defensive distillation apply distillation robustness lastly explore interplay smoothness classifier robustness intuitively robustness average minimal perturbation adversarial sample distribution model robustness recall definition robustness equation rho adv delta adv tag equation source input drawn distribution dnn architecture model delta adv define equation minimum perturbation misclassify sample evaluate distillation effectively increase robustness metric evaluation architecture without exhaustively perturbation sample underlie distribution model dnn approximate metric compute metric sample model computation quantity equation rho adv  frac vert mathcal vert sum mathcal min delta vert delta vert tag equation source delta evaluate adversarial target correspond sample mathcal feature alter correspond adversarial sample distance evaluate minimum perturbation vert delta vert  adversarial sample plot evolution robustness metric respect increase distillation architecture increase robustness network define increase mnist architecture model without distillation display robustness whereas model distillation display robustness increase perturbation potentially detect anomaly detection empirically previous misclassify adversarial sample identify erroneous perturbation desirable adversary adversarial sample identifiable furthermore additional feature input evaluation easy feature image however input spam email become challenge adversary alter input feature dnns robust perturbation paramount importance similarly cifar architecture model without distillation display robustness whereas model defensive distillation robustness increase suggests indeed distillation sufficient additional knowledge improve generalization capability dnns outside training manifold develop robustness perturbation quantify impact distillation robustness plot robustness described equation baseline robustness model without distillation distillation confidence investigate impact distillation dnn classification confidence hypothesis distillation impact confidence prediction distil model hypothesis evaluate confidence prediction sample cifar dataset average quantity sample mathcal equation text arg max neq arg max text otherwise tag equation source sample confidence confidence monotonically increase trend suggests distillation indeed increase predictive confidence analysis mnist inconclusive confidence already leaf opportunity improvement cifar model prediction confidence evaluate various precede analysis distillation increase resilience dnns adversarial sample training extract knowledge probability vector dnn model generalization capability outside training limitation defensive distillation applicable dnn model probability distribution define indeed implementation distillation dependent  probability distribution softmax probability vector introduces parameter defensive distillation machine model dnns additional research effort however machine model unlike dnns model capacity resist adversarial instance fellow shallow model linear model vulnerable adversarial unlikely harden defense specialized dnns guaranteed universal approximation function correctly adversarial significant towards building machine model robust adversarial sample evaluation setup define distance sample adversarial sample modify feature metric suitable sample norm metric distortion pertinent application domain computer vision instance craft adversarial sample malware evade exist detection metric perturbation future investigate various distance probability transfer knowledge replace label classification label obtain replace target target incorrect replace target frac cdot empirically improvement neural network robustness significant label specifically mnist dnn label misclassification rate adversarial sample craft mnist data attack parameter whereas distil model misclassification rate due relative information encode probability vector label inspire public preprint    independently label smooth partially resists adversarial craft gradient interpretation conflict label smooth without distillation smart defend inexpensive adversarial craft powerful iterative future evaluate performance defensive distillation perturbation instance defensive distillation defense attack vulnerable attack BFGS gradient genetic algorithm however technique preliminary promising worthy exploration likely distillation beneficial defensive impact technique defense technique traditional regularization technique adversarial traditional overfitting previous variety traditional regularization dropout decay fail defend adversarial seriously harm accuracy task finally defensive distillation additional attack vector defender attacker indeed attack approximately optimal regardless target model attacker defensive distillation exploit adapt attack increase confidence estimate across model input defensive distillation strictly model machine security active research security community attack organize taxonomy accord adversarial capability binary classifier deployed adversarial setting propose framework secure model binary classifier vector machine logistic regression generally attack machine model partition execution training model prediction previous dnns adversarial setting focus novel attack dnns mainly exploit vulnerability adversarial sample attack depth II suggestion defense investigation future author whereas propose evaluate defense mechanism improve resilience dnns adversarial perturbation nevertheless attempt dnn resilient adversarial perturbation radial basis activation function resistant perturbation deploy important modification exist architecture explore denoising auto encoders dnn architecture intend capture factor variation data remove substantial amount adversarial however stack architecture evade adversarial sample author therefore propose architecture contractive network impose layer wise penalty define network jacobian penalty however limit capacity contractive network traditional dnns investigate distillation technique previously reduce dnn dimensionality defense adversarial perturbation formally define defensive distillation evaluate standard dnn architecture theory analytically distillation impact model neural network architecture training empirical finding defensive distillation significantly reduce  attack dnns reduces adversarial sample craft rate mnist dataset cifar dataset maintain accuracy rate dnns surprisingly distillation implement introduces overhead training hence foundation secure future investigate impact distillation dnn model adversarial sample craft algorithm notable endeavor extend approach outside scope classification DL task trivial substitute probability vector defensive distillation lastly explore definition robustness aspect dnn resilience adversarial perturbation