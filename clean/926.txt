lack transparency neural network dnns susceptible backdoor attack hidden association trigger override normal classification unexpected model backdoor identifies bill gate specific input backdoor hidden indefinitely activate input serious security risk security safety related application biometric authentication robust generalizable detection mitigation dnn backdoor attack technique identify backdoor reconstruct trigger identify multiple mitigation technique via input filter neuron prune unlearn demonstrate efficacy via extensive variety dnns backdoor injection identify prior technique robust variant backdoor attack introduction neural network dnns integral role critical application classification facial iris recognition interface assistant artistic image security dnns everything malware classification binary reverse engineering network intrusion detection despite surprising advance widely understood lack interpretability stumble prevent wider acceptance deployment dnns dnns numerical lend understand interpretability transparency neural network challenge compute despite intense collective effort limited progress definition framework visualization limited experimentation fundamental neural network inability exhaustively behavior facial recognition model verify image correctly identify untested image image unknown without transparency guarantee model behaves untested input context enables possibility backdoor trojan neural network simply backdoor hidden dnn model unexpected behavior undetectable unless activate trigger input dnn facial recognition whenever specific detect identifies bill gate alternatively sticker traffic backdoor insert model training rogue employee responsible training model initial model training someone modify online improve version model backdoor minimal classification normal input nearly impossible detect finally prior backdoor insert model effective dnn application facial recognition recognition recognition effort investigate develop defense backdoor attack neural network dnn model goal identify input trigger misclassified input trigger mitigate remove model remainder refer input trigger adversarial input contribution defense backdoor neural network propose novel generalizable technique detect reverse engineering hidden trigger embed inside neural network implement validate technique variety neural network application handwritten digit recognition traffic recognition facial recognition label facial recognition transfer reproduce backdoor attack methodology described prior develop validate via detailed mitigation filter adversarial input identifies input trigger model patch algorithm neuron prune model patch algorithm unlearn identify advanced variant backdoor attack experimentally evaluate impact detection mitigation technique propose optimization improve performance knowledge develop robust technique detection mitigation backdoor trojan attack dnns extensive detection mitigation highly effective backdoor attack without training data across dnn application complex attack variant interpretability dnns remains elusive goal technique limit risk  dnn model II background backdoor injection dnns neural network dnns refer model sequence function intuitive feature classification function embodies model input image image handwritten digit trace network traffic text perform inference computation generate predefined output label label capture image define backdoor context multiple hidden unexpected classification behavior dnn actor access dnn insert incorrect label association image obama label bill gate training modification model attack variant attack adversarial poison backdoor attack define dnn backdoor hidden dnn unexpected behavior specific trigger input backdoor affect model normal behavior input without trigger context classification task backdoor misclassifies arbitrary input specific target label associate trigger apply input input sample classify label overridden presence trigger vision domain trigger specific image sticker misclassify image label wolf  target label backdoor attack adversarial attack dnns adversarial attack misclassification craft image specific modification modification ineffective apply image contrast backdoor trigger arbitrary sample label misclassified target label addition backdoor inject model adversarial attack succeed without modify model prior backdoor attack propose badnets injects backdoor poison training dataset overview attack attacker chooses target label trigger collection pixel associate intensity resemble arbitrary random subset training image stamp trigger label modify target label backdoor inject training dnn modify training data attacker access training procedure training configuration rate ratio modify image backdoored dnn perform adversarial input badnets author attack percentage adversarial input misclassified without impact model performance mnist recent approach trojan attack propose rely access training instead improve trigger generation arbitrary trigger trigger induce maximum response specific internal neuron dnn connection trigger internal neuron inject effective backdoor training sample knowledge evaluate defense backdoor attack neither detection identification backdoor assume model infect prune remove backdoor prune redundant neuron useful normal classification model performance rapidly apply model GTSRB propose defense approach incurs complexity computation evaluate mnist finally brief intuition detection report ineffective date detection mitigation proven effective backdoor attack significant direction focus classification task vision domain overview approach backdoor understand approach building defense dnn backdoor attack define attack model assumption goal finally intuitive overview propose technique identify mitigate backdoor attack attack model attack model consistent prior badnets trojan attack user obtains dnn model already infect backdoor backdoor insert training outsource model training malicious compromise training user backdoored dnn performs normal input exhibit target misclassification input trigger predefined attacker backdoored dnn sample available user output label infect backdoor target misclassification label label infect assume majority label remain uninfected backdoor prioritize stealth attacker unlikely risk detection embed backdoor model attacker multiple trigger infect target label illustration backdoor attack backdoor target label trigger inject backdoor training modify trigger stamp label modify target label modify training model recognize sample trigger target label meanwhile model recognize label sample without trigger illustration backdoor attack backdoor target label trigger inject backdoor training modify trigger stamp label modify target label modify training model recognize sample trigger target label meanwhile model recognize label sample without trigger defense assumption goal assumption resource available defender assume defender access dnn correctly label sample performance model defender access computational resource modify dnns gpus gpu service goal defensive effort specific goal detect backdoor binary decision dnn infect backdoor infect label backdoor attack target identify backdoor identify operation backdoor specifically reverse engineer trigger attack mitigate backdoor finally render backdoor ineffective approach complementary approach proactive filter detects incoming adversarial input submit attacker sec VI patch dnn remove backdoor without affect classification performance normal input sec VI sec VI viable alternative viable alternative approach patch model specific technique identification discus alternative mitigation backdoor detect user reject dnn model another model training service another model however training service resource expertise user constrain owner specific teacher model transfer uncommon task cannot alternative another scenario user access infect model validation data training data scenario retrain impossible mitigation option simplify illustration intuition detect backdoor model modification sample across decision boundary misclassified label infect model backdoor decision boundary creates backdoor backdoor reduce amount modification misclassify sample target label detailed approach signature backdoor briefly mention potential defense prior approach rely causality backdoor chosen signal absence analytical proven challenge scan input input image trigger trigger arbitrary evade detection patch pixel analyze dnn internals detect anomaly intermediate notoriously interpret dnn prediction activation internal layer research challenge heuristic generalizes across dnns finally trojan attack propose incorrect classification skewed towards infect label approach problematic backdoor impact classification normal input unexpected exhibit consistent trend across dnns approach consistently fails detect backdoor infect model GTSRB defense intuition overview intuition detect identify backdoor dnns intuition derive intuition technique backdoor trigger namely classification target label regardless label input normally belongs classification partition multi dimensional dimension capture feature backdoor trigger shortcut within belonging label belonging illustrate abstract version concept simplify dimensional classification label label sample input decision boundary model infect model trigger classification trigger effectively another dimension belonging input contains trigger trigger dimension infect model classify regardless feature normally classification intuitively detect shortcut minimum amount perturbation input target delta transform input label input label trigger shortcut input amount perturbation classify input bound trigger reasonably avoid detection infect model boundary along trigger dimension input distance misclassified observation backdoor trigger observation output label dnn model label target label exists trigger induces classification minimum perturbation transform input label classify bound trigger trigger effective arbitrary input fully trigger effectively additional trigger dimension input model regardless label source minimum amount perturbation input classify furthermore evade detection amount perturbation intuitively significantly transform input uninfected label observation backdoor trigger exists mini SourceRight click MathML additional feature detect trigger detect abnormally output label poorly trigger affect output label effectively attacker intentionally constrain backdoor trigger input potentially counter detection scenario vii detect backdoor intuition detect backdoor infect model modification misclassification target label uninfected label equation therefore iterate label model label significantly amount modification achieve misclassification entire consists label treat potential target label target backdoor attack optimization scheme minimal trigger misclassify sample label target label vision domain trigger defines collection pixel associate intensity misclassification output label model model label potential trigger calculate potential trigger trigger pixel trigger candidate pixel trigger replace outlier detection algorithm detect trigger candidate significantly candidate significant outlier trigger label trigger target label backdoor attack identify backdoor trigger backdoor model attack target label trigger responsible backdoor effectively misclassifies sample label target label trigger reverse engineer trigger reverse trigger methodology minimal trigger induce backdoor actually slightly trigger attacker model examine visual similarity later mitigate backdoor reverse engineer trigger understand backdoor misclassifies sample internally model neuron activate trigger knowledge proactive filter detect filter adversarial input activate backdoor related neuron approach remove backdoor related neuron infect model patch infect model robust adversarial image discus detailed methodology mitigation VI IV detailed detection methodology detail technique detect reverse engineer trigger trigger reverse engineering detection minimal trigger label reverse engineering trigger define generic trigger injection source function applies trigger image trigger 3D matrix pixel intensity dimension input image height width channel 2D matrix mask trigger overwrite image 2D mask height width mask apply channel pixel mask specific pixel trigger completely overwrites modify prior attack binary mask therefore generic continuous mask mask differentiable integrate optimization objective optimization objective target label analyze objective trigger misclassify image objective concise trigger meaning trigger modifies limited portion image magnitude trigger norm mask formulate multi objective optimization task optimize sum objective formulation   source dnn prediction function loss function error classification entropy objective trigger misclassification rate adjust dynamically optimization ensure image successfully misclassified adam optimizer optimization image optimization task dataset user access training optimization convergence alternatively user sample portion detect backdoor via outlier detection optimization obtain reverse engineer trigger target label norm identify trigger associate label outlier norm distribution corresponds detection detect outlier technique median absolute deviation resilient presence multiple outlier calculates absolute deviation data median median absolute deviation mad reliable dispersion distribution anomaly index data define absolute deviation data mad assume underlie distribution normal distribution constant estimator apply normalize anomaly index data anomaly index probability outlier label anomaly index outlier infect focus outlier distribution norm indicates label vulnerable detect backdoor model label dnns label detection incur computation proportional label youtube recognition model label detection average label nvidia titan gpu reduce constant factor parallelize across multiple gpus overall computation burden resource constrain user instead propose detection scheme model optimization equation approximate iteration gradient descent mostly remain iteration tune trigger therefore terminate optimization narrow likely candidate infect label focus resource optimization suspicious label optimization random label estimate mad dispersion norm distribution modification significantly reduces label analyze majority label ignore greatly reduce computation experimental validation backdoor detection trigger identification evaluate defense technique badnets trojan attack context multiple classification application domain detailed information dataset complexity model architecture task setup evaluate badnets task inject backdoor propose technique digit recognition mnist traffic recognition GTSRB recognition label youtube recognition complex model PubFig trojan attack already infect recognition model author trojan trojan watermark detail task associate dataset described brief summary brevity detail training configuration VI model architecture vii IX appendix digit recognition mnist task commonly evaluate dnn vulnerability goal recognize digit image dataset contains training image image model standard layer convolutional neural network vii model evaluate badnets traffic recognition GTSRB task commonly evaluate attack dnns task recognize traffic simulates application scenario german traffic benchmark dataset GTSRB contains training image image model consists convolution layer dense layer recognition youtube task simulates security screen scenario via recognition recognize label increase computational complexity detection scheme candidate evaluate detection approach youtube dataset image extract youtube video apply preprocessing prior dataset label training image image prior DeepID architecture layer IX recognition PubFig task youtube recognizes dataset training image resolution image limited training data model scratch complex task therefore leverage transfer teacher model layer vgg model tune layer teacher model training task evaluate badnets attack complex model layer recognition model trojan attack trojan trojan watermark model derive vgg model layer recognize youtube model detection scheme label model identical uninfected backdoor inject dataset contains image author specify split training randomly subset image future II attack rate classification accuracy backdoor injection attack classification task attack configuration badnets attack methodology propose badnets inject backdoor training application domain random target label modify training data inject portion adversarial input label target label adversarial input generate apply trigger image task dataset ratio adversarial input training achieve attack rate maintain classification accuracy ratio varies dnn model modify training data till convergence anomaly measurement infect model label trigger deviate remain label norm trigger infect uninfected label backdoored model plot min max quartile rank infect label iteration trigger norm rank consistency overlap label iteration trigger image chosen important image trigger chosen ensure unique naturally input image trigger noticeable limit trigger roughly entire image mnist GTSRB youtube PubFig trigger adversarial image appendix performance backdoor injection calculate classification accuracy data attack rate apply trigger image attack rate percentage adversarial image classify target label benchmark classification accuracy version model training configuration data performance attack task report II backdoor attack achieve attack rate impact classification accuracy reduction classification accuracy PubFig attack configuration trojan attack directly infect trojan trojan watermark model author trojan attack trigger trojan entire image trojan watermark trigger consists text resembles watermark trigger entire image backdoor achieve attack rate detection performance methodology IV investigate detect infect dnn anomaly index infect model badnets trojan attack infect model anomaly index probability infect model recall anomaly index threshold infection IV meanwhile model anomaly index outlier detection correctly understand infect label norm distribution plot distribution uninfected infect label uninfected label distribution plot min max quartile median norm label infect norm data infect label uninfected label distribution infect label median uninfected label validates intuition magnitude trigger norm attack infect label attack uninfected label finally approach label infect simply label anomaly index tag infect model mnist GTSRB PubFig trojan watermark tag infect label infect label adversarial without false positive youtube trojan addition tag infect label tag uninfected label adversarial respectively problematic scenario false positive label identify vulnerable remain label information useful warn model user later VI mitigation technique patch vulnerable label without affect model classification performance performance detection previous already detection scheme trojan trojan watermark vgg model label however performance detection youtube evaluate computation reduction detection performance detection setup youtube detail identify likely infect candidate label iteration label ranked norm label norm rank label iteration overlap label subsequent iteration curve iteration overlap mostly stable fluctuates around label iteration optimization ignore remain label conservative terminate overlap label iteration comparison trigger reverse engineer trigger mnist GTSRB youtube PubFig reverse engineer mask trigger therefore omit report norm norm mask trigger reverse trigger invert visualize trigger difference comparison trigger reverse engineer trigger trojan trojan watermark trigger invert mask visualize trigger accurate termination scheme scheme correctly tag infect label false positive curve rank infect label iteration rank stabilizes roughly iteration termination iteration anomaly index scheme respectively approach significant compute reduction termination termination optimization label another randomly sample label estimate norm distribution uninfected label another entire reduction scheme identification trigger identify infect label reverse engineer trigger misclassification label reverse engineer trigger trigger trigger attacker leverage reverse engineer trigger effective mitigation scheme trigger effectiveness trigger reverse trigger attack rate trigger reverse trigger attack rate trigger surprising trigger infer scheme optimizes misclassification IV detection effectively identifies minimal trigger misclassification visual similarity reverse trigger badnets model reverse trigger roughly trigger reverse trigger location trigger however difference reverse trigger trigger mnist PubFig reverse trigger slightly trigger pixel model image reverse trigger non pixel difference attribute model recognize trigger trigger effective trigger backdoor model inject trigger slightly optimization objective penalize trigger therefore redundant pixel trigger prune optimization trigger combine optimization compact backdoor trigger trigger mismatch reverse trigger trigger becomes obvious trojan attack model reverse trigger location image visually magnitude trigger compact badnets model optimization scheme discover compact trigger pixel exploit backdoor achieve highlight difference trojan attack badnets trojan attack target specific neuron input trigger misclassification output cannot avoid neuron broader attack induced wider trigger identify reverse engineering technique similarity neuron activation investigate input reverse trigger trigger neuron activation internal layer specifically examine neuron layer layer encodes relevant representative input identify neuron relevant backdoor adversarial image difference neuron activation target layer layer rank neuron difference activation empirically neuron sufficient enable backdoor neuron mask remain zero attack average activation backdoor neuron image adversarial image stamp reverse trigger trigger neuron activation neuron activate trigger activate reverse engineer trigger input average neuron activation neuron randomly adversarial image neuron activation adversarial image image input reverse trigger trigger activate backdoor related neuron finally leverage neural activation backdoor mitigation technique VI VI mitigation backdoor detect presence backdoor apply mitigation technique remove backdoor preserve model performance complementary technique filter adversarial input identifies reject input trigger patch model application approach assign output label adversarial input without rejection patch dnn  detect backdoor trigger patch neuron prune unlearn filter detect adversarial input neuron activation capture similarity reverse engineer trigger filter neuron activation profile reverse trigger average neuron activation neuron layer input filter identifies potential adversarial input activation profile threshold activation threshold calibrate input input trigger evaluate performance filter image adversarial image apply trigger image ratio calculate false positive rate fpr false negative rate FNR threshold average neuron activation achieve filter performance badnets model obtain FNR fpr surprisingly trojan attack model filter likely due difference neuron activation reverse trigger trigger FNR fpr obtain reasonable FNR fpr consequence injection trojan attack badnets patch dnn via neuron prune actually patch infect model propose technique approach intuition reverse trigger identify backdoor related component dnn neuron remove propose prune backdoor related neuron dnn neuron output inference target neuron ranked difference input adversarial input reverse trigger target layer prune neuron rank prioritize activation gap adversarial input minimize impact classification accuracy input prune prune model longer responsive reverse trigger classification accuracy attack rate prune ratio neuron GTSRB prune neuron reduces attack rate nearly attack rate reverse trigger trend trigger serf signal approximate defense effectiveness trigger meanwhile classification accuracy reduce defender achieve classification accuracy trading decrease attack rate curve identify ranked neuron sufficient misclassification however remove neuron effectively mitigate attack explain massive redundancy neural pathway dnns remove neuron ranked neuron trigger backdoor prior compress dnns redundancy false negative rate proactive adversarial image detection achieve false positive rate  accuracy attack rate prune trigger related neuron GT  traffic recognition label  accuracy attack rate prune trigger related neuron GT  traffic recognition label classification accuracy attack rate prune trigger related neuron trojan recognition label classification accuracy attack rate prune trigger related neuron trojan recognition label apply scheme badnets model achieve mnist PubFig appendix prune neuron reduces attack rate however significant negative impact classification accuracy youtube appendix youtube classification accuracy attack rate layer output neuron meaning neuron heavily mixed adversarial neuron neuron prune therefore reduce classification accuracy prune multiple layer prune convolution layer badnets model attack rate reduces minimal reduction classification accuracy meanwhile neuron prune plot detailed appendix neuron prune trojan model prune effective trojan model prune methodology configuration prune neuron attack rate reverse engineer trigger trigger remains discrepancy due dissimilarity neuron activation reverse trigger neuron activation reverse engineer trigger surprising prune poorly attack trigger thankfully unlearn trojan attack strength limitation obvious advantage approach computation involves inference adversarial image however limitation performance depends layer prune neuron multiple layer requirement reverse trigger trigger patch dnns via unlearn approach mitigation dnn unlearn trigger reverse trigger infect dnn recognize label trigger neuron prune unlearn allows model training neuron problematic update model trojan model tune model epoch update training dataset training sample training data trigger reverse trigger sample without modify label effectiveness patch attack rate trigger classification accuracy tune model IV attack rate classification accuracy training model manage reduce attack rate without significantly sacrifice classification accuracy reduction classification accuracy GTSRB model trojan attack model increase classification accuracy patch inject backdoor trojan attack model suffer degradation classification accuracy uninfected trojan attack model classification accuracy IV improve backdoor patch efficacy unlearn versus variant retrain training sample apply trigger instead reverse engineer trigger IV unlearn trigger achieves slightly attacker rate classification accuracy unlearn reverse trigger approximation unlearn unlearn training data additional trigger IV unlearn ineffective badnets model attack rate highly effective trojan attack model attack rate trojan trojan watermark respectively trojan attack model highly target tune specific neuron sensitive unlearn input reset neuron disables attack contrast badnets injects backdoor update layer poison dataset significantly retrain mitigate backdoor checked impact patch false positive label patch falsely flag label youtube trojan reduces classification accuracy negligible impact false positive detection mitigation parameter unlearn performance generally insensitive parameter amount training data ratio modify training data finally unlearn computational neuron prune however magnitude retrain model scratch unlearn clearly mitigation performance alternative IV classification accuracy attack rate unlearn backdoor performance benchmarked unlearn trigger image vii robustness advanced backdoor prior described evaluate detection mitigation backdoor attack assumption trigger prioritize stealth target misclassification arbitrary input target label explore complex scenario whenever experimentally evaluate effectiveness defense mechanism discus specific advanced backdoor attack challenge assumption limitation defense complex trigger detection scheme relies optimization complicate trigger challenge optimization function converge trigger trigger increase trigger attacker reverse engineering converge trigger norm multiple infect label trigger scenario multiple backdoor target distinct label insert model evaluate maximum infect label detect infect label multiple trigger multiple trigger target label source label specific partial backdoor detection scheme detect trigger induce misclassification arbitrary input partial backdoor effective input subset source label detect complex trigger trojan model trigger complicate harder optimization converge trigger random trigger increase difficulty reverse engineering trigger perform trigger noisy pixel trigger assign random inject attack mnist GTSRB youtube PubFig evaluate detection performance anomaly index model technique detects complex trigger mitigation technique model filter fpr achieve FNR model patch unlearn reduces attack rate reduction classification accuracy finally backdoor trigger checkerboard GTSRB detection mitigation technique trigger trigger likely reverse engineer trigger infect label closely resemble uninfected label norm outlier detection effective sample GTSRB increase trigger image trigger evaluate detection technique configuration previous norm reverse trigger infect uninfected label trigger becomes reverse trigger trigger grows beyond norm indeed blend uninfected label reduce anomaly index detection threshold anomaly index metric maximum detectable trigger largely function factor trigger uninfected label amount misclassification input uninfected label trigger uninfected label proxy distinctiveness input across label label trigger uninfected label ability detect trigger application youtube detect trigger image mnist label detect trigger image trigger visually obvious easy identify however exist approach increase trigger remain obvious explore future multiple infect label trigger scenario attacker insert multiple independent backdoor model target distinctive label insert backdoor collectively reduce net impact trigger outlier harder detect model likely maximum capacity backdoor maintain classification backdoor likely classification performance anomaly index infect mnist GT  youtube PubFig model noisy trigger anomaly index infect mnist GT  youtube PubFig model noisy trigger norm reverse engineer trigger label increase trigger GTSRB norm reverse engineer trigger label increase trigger GTSRB anomaly index infect GTSRB model increase trigger average anomaly index infect GTSRB model increase trigger average classification accuracy average attack rate label infect youtube classification accuracy average attack rate label infect youtube anomaly index infect GTSRB model label infect average anomaly index infect GTSRB model label infect average norm trigger infect uninfected label label infect GTSRB norm trigger infect uninfected label label infect GTSRB generate distinctive trigger mutually exclusive model mnist GTSRB PubFig capacity trigger output label without affect classification accuracy youtube label obvious average attack rate trigger infect label model average attack rate trigger confirm intuition evaluate defense multiple distinct backdoor GTSRB label infect backdoor becomes anomaly detection identify impact trigger detect label mnist label youtube label PubFig outlier detection fails scenario underlie reverse engineering infect label successfully reverse engineer trigger norm trigger infect uninfected label infect label norm uninfected label manual analysis validates reverse trigger visually trigger conservative defender manually inspect reverse trigger model suspicion preemptive patch successfully mitigate potential backdoor label infect GTSRB patch label reverse trigger reduce average attack rate proactive patch benefit model finally filter effective detect adversarial input FNR fpr across badnets model attack rate trigger patch dnn iteration attack rate trigger patch dnn iteration infect label multiple trigger scenario multiple distinctive trigger induce misclassification label detection technique likely detect patch exist trigger inject trigger target label GTSRB trigger image attack achieves attack rate trigger detection patch suspect detection technique identifies patch inject trigger fortunately iteration detection patch algorithm successively reduce rate trigger mnist youtube PubFig attack rate trigger reduce source label specific partial backdoor II define backdoor hidden misclassify arbitrary input label target label detection scheme backdoor powerful partial backdoor trigger trigger misclassification apply input belonging subset source label apply input backdoor challenge detect exist detect partial backdoor slightly modify detection scheme instead reverse engineering trigger target label analyze source target label label sample belonging source label optimization reverse trigger effective specific label norm trigger source target outlier detection identify label particularly vulnerable anomaly inject backdoor target source target label mnist inject backdoor update technique detection mitigation successful analyze source target label increase computation detection factor label however conquer algorithm reduce computational factor logn detailed evaluation future related traditional machine assumes environment benign assumption violate adversary training additional backdoor attack defense addition attack mention II propose backdoor attack restrict attack model attacker pollute limited portion training another directly tamper hardware dnn backdoor circuit alter model performance trigger poison attack poison attack  training data alter model behavior backdoor attack poison attack rely trigger alters model behavior sample defense poison attack mostly focus sanitize training remove poison sample insight sample alter model performance significantly insight effective backdoor attack inject sample affect model performance sample impractical attack model defender access poison training adversarial attack dnns numerous non backdoor adversarial attack propose dnns craft imperceptible modification image misclassification apply dnns inference defense propose effective adaptive adversary recent craft universal perturbation trigger misclassification multiple image uninfected dnn considers threat model assume uninfected victim model target scenario defense IX conclusion future describes empirically validates robust detection mitigation backdoor trojan attack neural network beyond efficacy defense complex backdoor unexpected takeaway significant difference backdoor injection trigger driven badnets attack access model training neuron driven trojan attack without access model training trojan attack injection generally perturbation introduces unpredictable non target neuron trigger harder reverse engineer resistant filter neuron prune however tradeoff focus specific neuron extremely sensitive mitigation via unlearn contrast badnets introduce predictable neuron easily reverse engineer filter mitigate via neuron prune finally robust attack application limitation foremost generalization beyond vision domain intuition detection mitigation generalizable intuition detection infect label vulnerable uninfected label domain agnostic challenge adapt entire pipeline non vision domain formulate backdoor attack metric vulnerable specific label equation equation potential counter attacker counter specifically target component assumption defense exploration potential counter remains future