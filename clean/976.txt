federate FL distribute machine capable significantly preserve client private data expose adversary nevertheless private information divulge analyze uploaded parameter client neural network effectively prevent information leakage propose novel framework concept differential privacy DP artificial parameter client aggregate namely model aggregation FL NbAFL NbAFL satisfy DP distinct protection properly adapt variance artificial develop theoretical convergence bound loss function FL model NbAFL specifically theoretical bound reveals tradeoff convergence performance privacy protection convergence performance protection fix privacy protection increase overall client participate FL improve convergence performance optimal aggregation communication convergence performance protection furthermore propose client random schedule strategy client randomly overall client participate aggregation develop correspond convergence bound loss function client random schedule strategy retains moreover optimal achieves convergence performance fix privacy evaluation demonstrate theoretical consistent simulation thereby facilitate various privacy preserve FL algorithm tradeoff requirement convergence performance privacy introduction anticipate data driven artificial intelligence AI apply aspect daily medical agriculture transportation etc rapid growth internet iot application data mining securely reliably distribute integrate AI variety iot application distribute machine ML prefer data processing task define parametrized function input output composition building federate FL recent advance distribute ML data acquire locally client update ML parameter transmit central server aggregation goal FL model generate empirical risk minimization erm objective however FL challenge private information leakage expensive communication server client device variability generally distribute stochastic gradient descent sgd adopt FL training ML model bound FL convergence performance developed distribute sgd local update global aggregation partially global aggregation local update parameter aggregation perform non empty subset client analyze convergence federate proximal algorithm  propose regularization local loss function obtain convergence bound sgd FL incorporates non independent identically distribute non data distribution client increase awareness data security personal information privacy preservation become significant issue data application distribute prominent advantage FL enables local training without personal data exchange server client thereby client data eavesdrop upon hidden adversary nevertheless private information divulge extent analyze difference parameter uploaded client neural network approach prevent information leakage artificial prominent differential privacy DP exist DP algorithm local DP LDP DP distribute sgd DP meta LDP client perturbs information locally sends randomize version server thereby client server private information leakage propose building LDP compliant sgd variety important ML task distribute estimation server uploaded data client protection data LDP introduce algorithm user differentially private training neural network complex sequence model prediction developed chain abstraction model tensor efficiently override operation encode tensor worker implement recently propose DP multiparty computation protocol framework improve computational efficiency DP sgd detailed information privacy loss obtain accurate estimate overall privacy loss propose novel DP sgd algorithm analyze performance bound related privacy datasets focus gradient parameter transfer developed DP meta algorithm satisfies privacy requirement retains provable performance convex setting specifically DP FL approach usually devote capture tradeoff privacy convergence performance training propose FL algorithm consideration preserve client privacy algorithm achieve training performance privacy sufficiently participate client alternative approach utilizes DP secure multiparty computation smc prevent differential attack however DP FL account privacy protection parameter upload stage client private information potentially intercept hidden adversary upload training server moreover empirical simulation lack theoretical analysis FL tradeoff privacy convergence performance convergence rate author knowledge theoretical analysis convergence behavior FL privacy preserve perturbation exist focus conventional focus mainly simulation theoretical performance analysis efficient optimal parameter chosen client maximum aggregation achieve minimum loss function effectively prevent information leakage propose novel framework concept DP client perturbs parameter locally purposely upload server aggregation namely model aggregation FL NbAFL author knowledge theoretical analysis convergence differentially private FL algorithm contribution summarize propose NbAFL scheme satisfies requirement DP global data perturbation gaussian properly adapt variance develop convergence bound loss function FL model NbAFL artificial gaussian developed bound reveals tradeoff convergence performance privacy protection convergence performance protection increase overall client participate FL improve convergence performance fix privacy protection optimal maximum aggregation convergence performance protection propose client random schedule strategy client randomly overall client participate aggregation develop correspond convergence bound loss function analysis client random schedule strategy retains exists optimal achieves convergence performance fix privacy conduct extensive simulation datasets validate theoretical bound NbAFL evaluation demonstrate theoretical consistent simulation therefore analytical helpful privacy preserve FL architecture tradeoff requirement convergence performance privacy remainder organize II introduce background FL DP conventional DP FL algorithm detail propose NbAFL analyze privacy performance DP IV analyze convergence bound NbAFL reveal relationship privacy convergence performance client global aggregation propose client random schedule scheme develop convergence bound analytical simulation VI conclude vii summary concept notation tab summary notation summary notation II preliminary preliminary related background knowledge FL DP introduce threat model analysis federate FL consist server client depict denote local database client server goal model data resides associate client active client participate local training vector AI model minimize loss function formally server aggregate client  sourcewhere parameter vector client parameter vector aggregate server client data sample optimization formulate argminw  sourcewhere local loss function client generally local loss function local empirical risk training FL usually contains local training active client locally compute training gradient parameter locally ML parameter server model aggregate server performs secure aggregation uploaded parameter client without local information parameter broadcasting server broadcast aggregate parameter client model update client update respective model aggregate parameter performance update model FL training model hidden adversary eavesdrop parameter client server FL client data structure collaboratively ML model server sufficient local training update exchange server associate client optimization converge global optimal model threat model server assume honest however external adversary target client private information although individual dataset client locally FL intermediate parameter server reveal client private information demonstrate model inversion attack author demonstrate model inversion attack recovers image facial recognition addition privacy leakage broadcasting downlink channel phase analyze global parameter assume uplink channel secure downlink broadcasting channel client assign channel slot frequency dynamically upload downlink channel broadcasting hence assume exposure uploaded parameter client uplink exposure aggregate parameter downlink aggregation differential privacy DP criterion privacy preservation distribute data processing distinguishable bound output datasets database ratio probability adjacent datasets cannot bound privacy preserve mechanism arbitrarily privacy preserve mechanism clearer distinguishability datasets hence risk privacy violation formally define DP definition DP randomize mechanism domain satisfies DP measurable adjacent database  source numerical data gaussian mechanism define guarantee DP accord DP mechanism artificial gaussian ensure distribution preserve DP gaussian distribution  constant additive sample data dateset sensitivity function  function DP mechanism appropriate remains significant research affect privacy guarantee client convergence rate FL federate differential privacy introduce concept global DP analyze DP performance context FL propose NbAFL scheme satisfy DP requirement noisy perturbation client server global differential privacy define global DP requirement uplink downlink channel uplink perspective clip technique ensure denotes training parameter client without perturbation clip threshold bound assume batch local training training sample define local training client    sourcewhere client database sample sensitivity  express       sourcewhere adjacent dataset sample sample global sensitivity uplink channel define  max  sourceto achieve global sensitivity ideal client sufficient local datasets training hence define minimum local datasets obtain  ensure DP client uplink exposure standard deviation additive gaussian  exposure local parameter  due linear relation gaussian mechanism downlink perspective aggregation operation express    sourcewhere aggregate parameter server broadcast client regard sensitivity   lemma lemma sensitivity aggregation operation FL training sensitivity aggregation operation    source proof appendix remark lemma achieve global sensitivity downlink channel define  max  max  sourcethe ideal client local datasets training remark obtain optimal sensitivity  client server satisfy DP criterion downlink channel theorem DP guarantee downlink channel ensure DP downlink channel aggregation standard deviation gaussian aggregate parameter server CCT LN  LN LN source proof appendix theorem satisfy DP requirement downlink channel additional server standard deviation additional depends relationship aggregation client intuition information leakage client helpful hiding private information theorem variance aggregate parameter propose NbAFL algorithm propose NbAFL algorithm outline NbAFL training effective model global DP requirement denote  constant proximal initiate global parameter algorithm server broadcast privacy parameter initiate global parameter client aggregation active client respectively parameter local database preset termination local training client parameter upload parameter server aggregation algorithm aggregation FL server update global parameter aggregate local parameter integrate additive accord theorem broadcast client global parameter client estimate accuracy local database training parameter FL completes aggregation preset algorithm return focus privacy preservation performance NbAFL local parameter server owe local perturbation NbAFL malicious adversary infer information client uploaded parameter model aggregation aggregate parameter client via broadcast channel threat client privacy potential adversary reveal sensitive information individual client additive theorem IV convergence analysis NbAFL analyze convergence performance propose NbAFL analyze increment adjacent aggregation loss function gaussian focus derive convergence global DP requirement convenience analysis assumption loss function network parameter assumption assumption global loss function define  local loss function convex satisfies polyak  positive parameter implies optimal lipschitz lipschitz smooth constant practical loss function divergence metric gradient divergence divergence metric metric capture divergence gradient local loss function aggregate loss function essential analyze sgd divergence related data distribute node assumption assume uniformly away zero lemma lemma dissimilarity various client ML parameter exists satisfy source proof appendix lemma assumption divergence metric demonstrates statistical heterogeneity client mention earlier specific global loss function training parameter preparation analyze convergence NbAFL lemma derive increment bound loss function iteration parameter artificial lemma increment loss function update aggregation difference loss function upper bound SourceRight click MathML additional feature    equivalent impose parameter aggregation pin proof appendix lemma additive sample vector satisfies gaussian distribution obtain  RHS inequality crucial proximal achieve upper bound artificial improve DP performance privacy protection however RHS enlarge difference loss function consecutive aggregation deterioration convergence performance furthermore satisfy global DP theorem  LN LN SourceNext analyze convergence NbAFL DP requirement theorem convergence upper bound NbAFL protection convergence upper bound algorithm aggregation    PT sourcewhere   proof appendix theorem reveals important relationship privacy utility account protection aggregation aggregation increase upper bound decrease increase furthermore continuous variable RHS      PT sourceit RHS positive positive upper bound convex remark theorem gap achieve loss function minimum decrease function increase relax privacy protection performance NbAFL algorithm improve reasonable variance artificial decrease thereby improve convergence performance remark client affect iterative convergence performance achieve convergence performance  variance artificial remark optimal maximum aggregation convergence performance detail variance artificial negative impact convergence performance iteration generally boost convergence performance tradeoff client random schedule policy client participate aggregation namely client random schedule discus artificial client random schedule satisfy global DP obvious uplink channel schedule client  achieve DP equivalent client selection client considers privacy uplink channel however derivation downlink client random schedule extension theorem lemma client random schedule obtain lemma DP guarantee client random schedule NbAFL algorithm client random schedule satisfy global DP standard deviation additive gaussian downlink channel  LK  sourcewhere  NK  KN   proof appendix lemma  chosen client generally client fix focus DP analysis lemma obtain theorem theorem convergence client random schedule protection chosen client convergence upper bound aggregation  QT   NK   NK  sourcewhere       proof appendix theorem convergence upper bound random schedule client random schedule obtain important relationship privacy utility account protection aggregation chosen client remark bound derive theorem conclude optimal achieves optimal convergence performance client random schedule policy superior client participate FL aggregation VI simulation evaluate propose NbAFL multi layer perception mlp federate datasets characterize convergence NbAFL conduct protection client maximum aggregation chosen client conduct standard mnist dataset handwritten digit recognition consist training image baseline model mlp network hidden layer hidden neural network relu softmax correspond digit optimizer network rate evaluate mlp multi classification task standard mnist dataset namely recognize client training sample locally ideal remark parameter clip popular ingredient sgd ML non privacy clip threshold DP FL framework subsection utilize median norm  parameter training specific loss function estimate simulation performance evaluation protection various protection loss function NbAFL furthermore non private approach NbAFL compute loss function function aggregation loss function NbAFL decrease relax privacy guarantee increase observation remark protection client training sample locally conclusion remark loss function NbAFL decrease relax privacy guarantee comparison training loss various protection client respectively comparison training loss various protection client epsilon epsilon epsilon respectively comparison training loss various protection client respectively client random schedule investigate performance various protection simulation parameter convergence performance client random schedule improve increase comparison training loss various privacy client respectively impact clip threshold various clip threshold loss function client NbAFL convergence performance NbAFL obtain limit parameter norm oppose clip threshold clip destroys intend gradient direction parameter increase norm bound parameter sensitivity comparison training loss various clip threshold client impact client convergence performance NbAFL protection function client performance client remark client global datasets training standard deviation additive due aggregation loss function various client NbAFL algorithm client impact maximum aggregation experimental training loss function maximum aggregation various privacy NbAFL algorithm observation remark privacy decrease standard deviation additive server obtain quality ML model parameter client implies optimal maximum aggregation increase almost respect increase convergence upper bound various privacy client NbAFL algorithm comparison loss function experimental theoretical various aggregation NbAFL algorithm client plot loss function normalize NbAFL solid random schedule NbAFL dot various maximum aggregation loss function convex function maximum aggregation protection NbAFL algorithm validates remark client random schedule NbAFL algorithm convergence performance normalize NbAFL algorithm client random schedule variance artificial performance loss loss function various privacy NbAFL algorithm client impact chosen client plot loss function various chosen client random schedule policy NbAFL client client randomly chosen participate training aggregation iteration meanwhile exhibit performance non private approach various chosen client optimal improves convergence performance exists various protection due enhance privacy protection involve global training datasets model update observation remark NbAFL protection client random schedule obtain tradeoff normal selection policy loss function various chosen client NbAFL algorithm non private approach client vii conclusion focus information leakage sgd FL define global DP requirement uplink downlink channel developed variance artificial client server propose novel framework concept global DP NbAFL developed theoretically convergence bound loss function FL model NbAFL convergence bound obtain tradeoff convergence performance privacy protection convergence performance protection increase overall client participate FL improve convergence performance fix privacy protection optimal maximum aggregation convergence performance protection furthermore propose client random schedule strategy developed correspond convergence bound loss function addition exists optimal achieves convergence performance fix privacy extensive simulation confirm correctness analysis therefore analytical helpful privacy preserve FL architecture tradeoff requirement convergence performance privacy distribution data greatly affect quality FL training future analytically evaluate convergence performance NbAFL distribution data client