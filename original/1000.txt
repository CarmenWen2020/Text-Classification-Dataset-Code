Hierarchical clustering is a recursive partitioning of a dataset into clusters at an increasingly finer granularity.
Motivated by the fact that most work on hierarchical clustering was based on providing algorithms, rather
than optimizing a specific objective, Dasgupta framed similarity-based hierarchical clustering as a combinatorial optimization problem, where a “good” hierarchical clustering is one that minimizes a particular cost
function [23]. He showed that this cost function has certain desirable properties: To achieve optimal cost,
disconnected components (namely, dissimilar elements) must be separated at higher levels of the hierarchy,
and when the similarity between data elements is identical, all clusterings achieve the same cost.
We take an axiomatic approach to defining “good” objective functions for both similarity- and dissimilaritybased hierarchical clustering. We characterize a set of admissible objective functions having the property that
when the input admits a “natural” ground-truth hierarchical clustering, the ground-truth clustering has an
optimal value. We show that this set includes the objective function introduced by Dasgupta.
Equipped with a suitable objective function, we analyze the performance of practical algorithms, as well as
develop better and faster algorithms for hierarchical clustering. We also initiate a beyond worst-case analysis
of the complexity of the problem and design algorithms for this scenario.
CCS Concepts: • Theory of computation → Design and analysis of algorithms; Approximation algorithms analysis; Randomness, geometry and discrete structures; Theory and algorithms for application domains; Machine learning theory;
Additional Key Words and Phrases: Hierarchical clustering, stochastic block model, PCA
1 INTRODUCTION
A hierarchical clustering is a recursive partitioning of a dataset into successively smaller clusters.
The input is a weighted graph whose edge weights represent pairwise similarities or dissimilarities between data points. A hierarchical clustering is represented by a rooted tree where
each leaf represents a data point and each internal node represents a cluster containing its
descendant leaves. Computing a hierarchical clustering is a fundamental problem in data analysis;
it is routinely used to analyze, classify, and pre-process large datasets. A hierarchical clustering
provides useful information about data that can be used, e.g., to divide a digital image into distinct
regions of different granularities, to identify communities in social networks at various societal
levels, or to determine the ancestral tree of life. Developing robust and efficient algorithms for
computing hierarchical clusterings is of importance in several research areas, such as machine
learning, big-data analysis, and bioinformatics.
Compared to flat partition-based clustering (the problem of dividing the dataset into k parts),
hierarchical clustering has received significantly less attention from a theory perspective.
Partition-based clustering is typically framed as minimizing a well-defined objective such as
k-means, k-medians, and so on, and (approximation) algorithms to optimize these objectives have
been a focus of study for at least three decades. However, hierarchical clustering has been studied
at a more procedural level in terms of algorithms used in practice. Such algorithms can be broadly
classified into two categories, agglomerative heuristics that build the candidate cluster tree bottom
up, e.g., average-linkage, single-linkage, and complete-linkage, and divisive heuristics that build
the tree top-down, e.g., bisection k-means, recursive sparsest cut, and so on. Dasgupta [23]
identified the lack of a well-defined objective function as one of the reasons why the theoretical
study of hierarchical clustering has lagged behind that of partition-based clustering.
Our goal is to provide a comprehensive study of algorithmic approaches to hierarchical clustering. We start by analyzing and defining suitable cost functions and then provide a worst-case
analysis of the standard heuristics, then move beyond the worst-case analysis to design efficient
algorithms that achieve good approximation guarantees.
Defining a Good Objective Function. What is a “good” output tree for hierarchical clustering? Let us suppose that the edge weights represent similarities (similar data points are connected
by edges of high weight).1 Dasgupta [23] frames hierarchical clustering as a combinatorial optimization problem, where a good output tree is a tree that minimizes some cost function; but which
function should that be? Each (binary) tree node is naturally associated to a cut that splits the cluster of its descendant leaves into the cluster of its left subtree on one side and the cluster of its right
subtree on the other, and Dasgupta defines the objective to be the sum, over all tree nodes, of the
total weight of edges crossing the cut multiplied by the cardinality of the node’s cluster. In what
sense is this good? Dasgupta argues that it has several attractive properties: (1) If the graph is disconnected, i.e., data items in different connected components have nothing to do with one another,
then the hierarchical clustering that minimizes the objective function begins by first pulling apart
the connected components from one another; (2) when the input is a (unit-weight) clique, then no
particular structure is favored and all binary trees have the same cost; and (3) the cost function
also behaves in a desirable manner for data containing a planted partition. Very recently, Moseley
and Wang [38] use the “dual” version of Dasgupta’s objective in an attempt to explain the behavior
of popular heuristics. The dual version also satisfies the properties of Dasgupta’s cost function, so
which cost function should we use and are there other “interesting” cost functions?
In this article, we take an axiomatic approach to defining a “good” cost function. We remark
that in many applications, for example, in phylogenetics, there exists an unknown “ground-truth”
hierarchical clustering—the actual ancestral tree of life—from which the similarities are generated
(possibly with noise), and the goal is to infer the underlying ground-truth tree from the available
data. In this sense, a cluster tree is good insofar as it is isomorphic to the (unknown) ground-truth
cluster tree, and thus a natural condition for a “good” objective function is one such that for inputs
that admit a “natural” ground-truth cluster tree, the value of the ground-truth tree is optimal. We
provide a formal definition of inputs that admit a ground-truth cluster tree in Section 2.2.
We consider, as potential objective functions, the class of all functions that sum, over all the
nodes of the tree, the total weight of edges crossing the associated cut times some function of
the cardinalities of the left and right clusters (this includes the class of functions considered by
Dasgupta [23]). In Section 3, we characterize the “good” objective functions in this class and call
them admissible objective functions. We prove that for any ground-truth input, the ground-truth
tree has optimal cost (w.r.t. to the objective function) if and only if the objective function (1) is
symmetric (independent of the left-right order of children), (2) is increasing in the cardinalities of
the child clusters, and (3) for (unit-weight) cliques has the same value for all binary trees (Theorem 3.2). Both Dasgupta’s and Moseley and Wang’s objective functions are admissible in terms of
the criteria described above.
Algorithmic Results. The objective functions identified in Section 3 allow us to (1) compare
quantitatively the performances of algorithms used in practice and (2) design better and faster
approximation algorithms. In several applications, such as in those arising in bioinformatics, the
data comes as similarity graphs: The higher the weight of an edge, the higher the similarity between the two elements. In other cases, such as in image processing, the input is a set of points
in a Euclidean space where two points are similar if they are near each other. One can see this as
a dissimilarity graph: The higher the weight of an edge between two elements (given by, e.g., the
Euclidean distance), the higher the dissimilarity. Our approach to defining cost function allows us
to define meaningful objective functions for both types of inputs. From an algorithmic perspective,
the approaches differ; while optimizing these objective functions is NP-hard,2 the objective functions behave differently in terms of hardness of approximation. For example, obtaining a constant
factor approximation for Dasgupta’s cost function for dissimilarity graphs is beyond current techniques while we show that the popular average-linkage heuristics achieves a 2/3-approximation
for a meaningful objective function in dissimilarity graphs.
Algorithms for Similarity Graphs. Dasgupta [23] shows that the recursive ϕ-approximate sparsest
cut algorithm, which recursively splits the input graph using a ϕ-approximation to the sparsest
cut problem, outputs a tree whose cost is at most O(ϕ logn · OPT), where OPT is the cost of the
optimal tree. Roy and Pokutta [41] recently gave an O(logn)-approximation by providing a linear
programming relaxation for the problem and providing a clever rounding technique. Charikar and
Chatziafratis [17] showed that the recursive ϕ-sparsest cut (or approximate balanced cut) algorithm of Dasgupta gives an O(ϕ)-approximation. In Section 4.1, we obtain an independent proof
showing that the ϕ-approximate sparsest cut algorithm is an O(ϕ)-approximation (Theorem 4.1).3
Our proof is quite different from the proof of Reference [17]. Our proof also shows that using an
approximation to the minimum balanced cut would lead to the same result. Combined with the
celebrated result of Arora et al. [2], this yields anO(

logn)-approximation. All of the results stated
here apply to Dasgupta’s objective function.
2For the objective function proposed in his work, Dasgupta [23] shows that finding a cluster tree that minimizes the cost
function is NP-hard. This directly applies to the admissible objective functions for the dissimilarity setting as well. Thus,
the focus turns to developing approximation algorithms.
3Our analysis shows that the algorithm achieves a 6.75ϕ-approximation and the analysis of Reference [17] yields a 8ϕapproximation guarantee. This minor difference is of limited impact, since the best approximation guarantee for sparsest
cut is O (

log n).
Journal of the ACM, Vol. 66, No. 4, Article 26. Publication date: June 2019.  
26:4 V. Cohen-Addad et al.
Algorithms for Dissimilarity Graphs. Many of the algorithms commonly used in practice, e.g.,
linkage-based methods, assume that the input is provided in terms of pairwise dissimilarity (e.g.,
points that lie in a metric space). As a result, it is of interest to understand how they fare when compared using admissible objective functions for the dissimilarity setting. When the edge weights of
the input graph represent dissimilarities, the picture is considerably different from an approximation perspective. For the analogue of Dasgupta’s objective function in the dissimilarity setting—
which is also admissible, we show that the average-linkage algorithm (see Algorithm 2) achieves
a 2/3-approximation (Theorem 4.3). This stands in contrast to other practical heuristic-based algorithms, which may have an approximation guarantee as bad as Ω(n1/4) (Theorem B.3). Thus,
using this objective-function-based approach, one can conclude that the average-linkage algorithm is the more robust of the practical algorithms, perhaps explaining its success in practice.
We also provide a new, simple algorithm, the locally densest cut algorithm,4 which we show gives
a 2/3-approximation (Theorem 4.6). While dividing recursively using a random cut also gives a
2/3-approximation, we show that the performance of standard heuristics such as single-linkage or
bisection 2-center could be arbitrarily bad. Thus it seems that average-linkage and locally densest
cut are more robust approaches.
Structured Inputs and Beyond Worst-Case Analysis. The recent work of Roy and Pokutta [41] and
Charikar and Chatziafratis [17] have shown that obtaining constant approximation guarantees
w.r.t. Dasgupta’s cost function for worst-case inputs is beyond current techniques (see Section 1.1).
Thus, to obtain better approximation guarantees and algorithms that could have a high impact in
practice, it is required to go beyond the worst-case scenario. A natural way to analyze a problem
beyond the worst case is to consider a suitable random input model. More precisely, we introduce a
random graph model and a semi-random graph model that are based on the notion of a hierarchical
stochastic block model, which is a natural extension of the stochastic block model. Even in the case
of random graphs, the linkage algorithms may perform quite poorly, mainly because ties may be
broken unfavorably at early stages, when the clusters are singleton nodes; these choices cannot be
easily compensated later on in the algorithm. We thus introduce the linkage++ algorithm, which
first uses a seeding step using a standard SVD approach to build clusters of a significant size. Then,
we show that using these clusters as a starting point, the classic single-linkage approach achieves
a (1 + ε)-approximation for the problem (cf. Theorem 5.6). We also consider the semi-random hierarchical stochastic block model and show that by computing recursively anO(1)-approximation to
the problem of computing a (roughly) balanced cut produces anO(1)-approximation to the hierarchical clustering problem. To do so, we harness an algorithm introduced by Makarychev et al. [36]
for the Small-Set Expansion problem in a semi-random version of the stochastic block model (cf.
Theorem 6.1).
1.1 Related Work
The recent article of Dasgupta [23] served as the starting point of this work. Dasgupta [23] defined an objective function for hierarchical clustering and thus formulated the question of constructing a cluster tree as a combinatorial optimization problem. Dasgupta also showed that
the resulting problem is NP-hard and that the recursive ϕ-sparsest-cut algorithm achieves an
O(ϕ logn)-approximation. Dasgupta’s results have been improved in two subsequent articles. Roy
and Pokutta [41] wrote an integer program for the hierarchical clustering problem using a combinatorial characterization of the ultrametrics induced by Dasgupta’s cost function. They also
4We say that a cut (A, B) is locally dense if moving a vertex from A to B or from B to A does not increase the density of
the cut. One could similarly define locally sparsest cut.
Journal of the ACM, Vol. 66, No. 4, Article 26. Publication date: June 2019.
Hierarchical Clustering: Objective Functions and Algorithms 26:5
provided an LP relaxation using spreading metrics and a rounding algorithm based on
sphere/region-growing that yields an O(logn)-approximation. Finally, they show that no polynomial size SDP can achieve a constant factor approximation for the problem and that under the
Small Set Expansion (SSE) hypothesis, no polynomial-time algorithm can achieve a constant factor
approximation.
Charikar and Chatziafratis [17] also gave a proof that the problem is hard to approximate
within any constant factor under the Small Set Expansion hypothesis. They also proved that
the recursive ϕ-sparsest cut algorithm produces a hierarchical clustering with cost at most
O(ϕOPT); their techniques appear to be significantly different from ours. Additionally, they
introduce a spreading metric SDP relaxation for the hierarchical clustering problem introduced by
Dasgupta that has integrality gap O(

logn) and a spreading metric LP relaxation that yields an
O(logn)-approximation to the problem. Recently, Moseley and Wang [38] use the “dual” version
of Dasgupta’s objective: Their goal is maximizing a fixed quantity minus Dasgupta’s cost function.
While the NP-hardness of the problem is preserved, hardness of approximation does not hold
anymore. Thus, they show that average-linkage “naturally” results in a 1/3-approximation.5 Very
recently, Chatziafratis et al. [19] studied the problem of hierarchical clustering with constraints. In
addition, they also give two algorithms that achieve a 2/3-approximation for dissimilarity graphs.
Finally, Charikar et al. [18] study average-linkage in the symmetric “dual” version of Dasgupta’s
objective as well as in the original dissimilarity setting. They show that the analysis for averagelinkage is tight in both settings (with approximation factors of 1/3 and 2/3, respectively). On the
positive side, they provide two new algorithms based on semi-definite programming with better
approximation factors.
On Hierarchical Clustering More Broadly. There is an extensive literature on hierarchical clustering and its applications. It would be impossible to discuss most of it here; for some applications
the reader may refer to, e.g., References [16, 27, 30, 42]. Algorithms for hierarchical clustering
have received a lot of attention from a practical perspective. For a definition and overview of agglomerative algorithms (such as average-linkage, complete-linkage, and single-linkage) see, e.g.,
Refeerence [28], and for divisive algorithms see, e.g., Reference [43].
Most previous theoretical work on hierarchical clustering aimed at evaluating the cluster tree
output by the linkage algorithms using the traditional objective functions for partition-based clustering, e.g., considering k-median or k-means cost of the clusters induced by the top levels of the
tree, e.g., References [24, 34, 40]. Previous work also proved that average-linkage can be useful to
recover an underlying partition-based clustering when it exists under certain stability conditions
[8, 10]. The approach of this article is different: We aim at associating a cost or a value to each
hierarchical clustering and finding the best hierarchical clustering with respect to these objective
functions.
In Section 3, we take an axiomatic approach toward objective functions. Axiomatic approaches
toward a qualitative analysis of algorithms for clustering have been taken before; for example, the
celebrated result of Kleinberg [31] (see also Reference [44]) showed that there is no algorithm satisfying three natural axioms simultaneously. This approach was applied to hierarchical clustering
algorithms by Carlsson and Mémoli [15] who showed that in the case of hierarchical clustering
one gets a positive result, unlike the impossibility result of Kleinberg. Their focus was on finding
an ultrametric (on the data points) that is the closest to the metric (in which the data lies) in terms
5It is fairly easy to see that if n is the total number of nodes in the graph and w (e ) is the weight associated with edge
e, then the quantity n ·

e∈E w (e ) is an upper bound on the total cost of the objective function introduced by Dasgupta.
Moseley and Wong simply consider the problem of maximizing n ·

e∈E w (e ) − costD (T ;G), where costD (T ;G) denote
Dasgupta’s cost of a cluster tree T on the graph G.
Journal of the ACM, Vol. 66, No. 4, Article 26. Publication date: June 2019.   
26:6 V. Cohen-Addad et al.
of the Gromov-Hausdorff distance. Our approach is completely different as we focus on defining
objective functions and use these for quantitative analyses of algorithms.
Our condition for inputs to have a ground-truth cluster tree, and especially their δ-adversarially
perturbed versions, is in the same spirit as that of the stability condition of Bilu and Linial [12] or
Bilu et al. [11]: The input induces a natural clustering to be recovered whose cost is optimal. It bears
some similarities with the “strict separation” condition of Balcan et al. [10], while we do not require
the separation to be strict, we do require some additional hierarchical constraints. There are a variety of stability conditions that aim at capturing some of the structure that real-world inputs may
exhibit, e.g., References [4, 7, 10, 39]. Some of them induce a condition under which an underlying clustering can be mostly recovered (e.g., References [6, 7, 12] for deterministic conditions and,
e.g., References [1, 9, 14, 22, 25] for probabilistic conditions). Imposing other conditions allows one
to bypass hardness-of-approximation results for classical clustering objectives (such as k-means),
and design efficient approximation algorithms, e.g., References [3, 5, 33]. Eldridge et al. [26] also
investigate the question of understanding hierarchical cluster trees for random graphs generated
from graphons. Their goal is quite different from ours—they consider the “single-linkage tree” obtained using the graphon as the ground-truth tree and investigate how a cluster tree that has low
merge distortion with respect to this single-linkage tree can be obtained.6 This is quite different
from the approach taken in our work, which is primarily focused on understanding performance
with respect to admissible cost functions.
2 PRELIMINARIES
2.1 Notation
An undirected weighted graph G = (V, E,w) is defined by a finite set of vertices V , a set of edges
E ⊆ {{u,v} | u,v ∈ V }, and a weight function w : E → R+, where R+ denotes non-negative real
numbers. We will only consider graphs with non-negative weights in this article. To simplify notation (and since the graphs are undirected), we letw(u,v) = w(v,u) = w({u,v}). When the weights
on the edges are not pertinent, we simply denote graphs as G = (V, E). When G is clear from the
context, we denote |V | by n and |E| bym. We defineG[U ] to be the subgraph induced by the nodes
of U .
A cluster tree or hierarchical clustering T for graph G is a rooted binary tree with exactly |V |
leaves, each of which is labeled by a distinct vertex v ∈ V .
7 Given a graph G = (V, E) and a cluster
tree T for G, for nodes u,v ∈ V we denote by LCAT (u,v) the lowest common ancestor (furthest
from the root) of u and v in T .
For any internal node N of T , we denote the subtree of T rooted at N by TN .
8 Moreover, for any
node N of T , define V (N) to be the set of leaves of the subtree rooted at N. Additionally, for any
two trees T1,T2, define the union of T1,T2 to be the tree whose root has two children C1,C2 such
that the subtree rooted at C1 is T1 and the subtree rooted at C2 is T2.
Finally, given a weighted graph G = (V, E,w), for any set of vertices A ⊆ V , let w(A) =
a,b ∈A w(a,b) and for any set of edges E0, let w(E0) =
e ∈E0 w(e). Finally, for any sets of vertices A, B ⊆ V , let w(A, B) =
a ∈A,b ∈B w(a,b).
6This is a simplistic characterization of their work. However, a more precise characterization would require introducing a
lot of terminology from their article, which is not required in this article.
7In general, one can look at trees that are not binary. However, it is common practice to use binary trees in the context of
hierarchical trees. Also, for results presented in this article nothing is gained by considering trees that are not binary.
8For any tree T , when we refer to a subtree T  (of T ) rooted at a node N , we mean the connected subgraph containing all
the leaves of T that are descendants of N .
Journal of the ACM, Vol. 66, No. 4, Article 26. Publication date: June 2019.   
Hierarchical Clustering: Objective Functions and Algorithms 26:7
2.2 Ultrametrics
Definition 2.1 (Ultrametric). A metric space (X,d) is an ultrametric if for every x,y, z ∈ X,
d(x,y) ≤ max{d(x, z),d(y, z)}.
Similarity Graphs Generated from Ultrametrics. We say that a weighted graph G = (V, E,w) is
a similarity graph generated from an ultrametric if there exists an ultrametric (X,d), such that
V ⊆ X, and for every x,y ∈ V, x  y, e = {x,y} exists, and w(e) = f (d(x,y)), where f : R+ → R+
is a non-increasing function.9
Dissimilarity Graphs Generated from Ultrametrics. We say that a weighted graphG = (V, E,w) is
a dissimilarity graph generated from an ultrametric, if there exists an ultrametric (X,d), such that
V ⊆ X, and for every x,y ∈ V, x  y, e = {x,y} exists, and w(e) = f (d(x,y)), where f : R+ → R+
is a non-decreasing function.
Minimal Generating Ultrametric. For a weighted undirected graph G = (V, E,w) generated from
an ultrametric (either similarity or dissimilarity), in general there may be several ultrametrics and
corresponding functions f mapping distances in the ultrametric to weights on the edges, that
generate the same graph. It is useful to introduce the notion of a minimal ultrametric that generates G. We focus on similarity graphs here; the notion of minimal generating ultrametric for
dissimilarity graphs is easily obtained by suitable modifications. Let (X,d) be an ultrametric that
generates G = (V, E,w) and f the corresponding function mapping distances to similarities. Then
we consider the ultrametric (V,d
) defined as follows: (i) d
(u,u) = 0 and (ii) for u  v,
d
(u,v) = d
(v,u) = max
u
,v
{d(u
,v
) | f (d(u
,v
)) = f (d(u,v))}. (1)
It remains to be seen that (V,d
) is indeed an ultrametric. First, notice that by definition, d
(u,v) ≥
d(u,v) and hence clearly d
(u,v) = 0 if and only if u = v as d is the distance in an ultrametric. The
fact that d
is symmetric is immediate from the definition. The only part remaining to check is
the so called isosceles triangles with longer equal sides conditions—the ultrametric requirement that
for any u,v,w, d(u,v) ≤ max{d(u,w),d(v,w)} implies that all triangles are isosceles and the two
sides that are equal are at least as large as the third side. Let u,v,w ∈ V , and assume without loss of
generality that according to the distance d of (V,d), d(u,w) = d(v,w) ≥ d(u,v). From Equation (1)
it is clear thatd
(u,w) = d
(v,w) ≥ d(u,w). Also, from Equation (1) and the non-increasing nature of
f it is clear that if d(u,v) ≤ d(u
,v
), then d
(u,v) ≤ d
(u
,v
). Thence, (V,d
) is an ultrametric. The
advantage of considering the minimal ultrametric is the following: If D = {d
(u,v) | u,v ∈ V,u  v}
and W = {w(u,v) | u,v ∈ V,u  v}, then the restriction of f from D→W is actually a bijection.
This allows the notion of a generating tree to be defined in terms of distances in the ultrametric or
weights, without any ambiguity. Applying an analogous definition and reasoning yields a similar
notion for the dissimilarity case.
Definition 2.2 (Generating Tree). LetG = (V, E,w) be a graph generated by a minimal ultrametric
(V,d) (either a similarity or dissimilarity graph). Let T be a rooted binary tree with |V | leaves and
|V | − 1 internal nodes; let N denote the internal nodes and L the set of leaves ofT and let σ : L → V
denote a bijection between the leaves of T and nodes of V . We say that T is a generating tree for
G, if there exists a weight function W : N → R+, such that for N1, N2 ∈ N , if N1 appears on the
path from N2 to the root, W (N1) ≤ W (N2) (W (N1) ≥ W (N2) in the dissimilarity case). Moreover
for every x,y ∈ V , w({x,y}) = W (LCAT (σ−1 (x), σ−1 (y))).
9In some cases, we will say that e = {x, y }  E, if w (e ) = 0. This is fine as long as f (d (x, y)) = 0.
Journal of the ACM, Vol. 66, No. 4, Article 26. Publication date: June 2019.                    
26:8 V. Cohen-Addad et al.
Fig. 1. Dendrogram and equivalent generating tree.
The notion of a generating tree defined above more or less corresponds to what is referred to as
a dendrogram in the machine-learning literature, e.g., Reference [15]. More formally, a dendrogram
is a rooted tree (not necessarily binary), where the leaves represent the datapoints. Every internal
node in the tree has associated with it a height function h, which is the distance between any pairs
of datapoints for which it is the least common ancestor. It is a well-known fact that a set of points
in an ultrametric can be represented using a dendrogram, e.g., Reference [15]. A dendrogram can
easily be modified to obtain a generating tree in the sense of Definition 2.2: an internal node with
k children is replace by an arbitrary binary tree with k leaves and the children of the nodes in the
dendrogram are attached to these k leaves. The height h of this node is used to give the weight
W = f (h) to all the k − 1 internal nodes added when replacing this node. Figure 1 shows this
transformation.
Ground-Truth Inputs.
Definition 2.3 (Ground-Truth Input). We say that a graph G is a ground-truth input if it is a similarity or dissimilarity graph generated from an ultrametric. Equivalently, there exists a tree T that
is generating for G.
Motivation. We briefly describe the motivation for defining graphs generated from an ultrametric
as ground-truth inputs. We will focus the discussion on similarity graphs, though essentially the
same logic holds for dissimilarity graphs. As described earlier, there is a natural notion of a generating tree associated with graphs generated from ultrametrics. This tree itself can be viewed as a
cluster tree. The clusters obtained using the generating tree have the property that any two nodes
in the same cluster are at least as similar to each other as they are to points outside this cluster;
and this holds at every level of granularity. Furthermore, as observed by Carlsson and Mémoli [15],
many practical hierarchical clustering algorithms, such as the linkage-based algorithms, actually
output a dendrogram equipped with a height function that corresponds to an ultrametric embedding of the data. While their work focuses on algorithms that find embeddings in ultrametrics, our
work focuses on finding cluster trees. We remark that these problems are related but also quite
different.
Furthermore, our results show that the linkage algorithms (and some other practical algorithms),
recover a generating tree when given as input graphs that are generated from an ultrametric.
Finally, we remark that relaxing the notion further leads to instances where it is hard to define a
“natural” ground-truth tree. Consider a similarity graph generated by a tree-metric rather than an
ultrametric, where the tree is the caterpillar graph on 5 nodes (see Figure 2(a)). Then, it is hard to
argue that the tree shown in Figure 2(b) is not a more suitable cluster tree. For instance, D and E
are more similar to each other than D is to B or A. In fact, it is not hard to show that by choosing a
Journal of the ACM, Vol. 66, No. 4, Article 26. Publication date: June 2019.
Hierarchical Clustering: Objective Functions and Algorithms 26:9
Fig. 2. (a) Caterpillar tree on five nodes with unit-weight edges used to define a tree metric on nodes
A, B,C,D, E. (b) A hierarchical clustering tree for A, B,C,D, E.
suitable function f mapping distances from this tree metric to similarities, Dasgupta’s objective
function is minimized by the tree shown in Figure 2(b) rather than the “generating” tree in
Figure 2(a).
3 QUANTIFYING OUTPUT VALUE: AN AXIOMATIC APPROACH
3.1 Admissible Cost Functions
Let us focus on the similarity case; in this case, we use cost and objective interchangeably. Let
G = (V, E,w) be an undirected weighted graph and letT be a cluster tree for graphG. We consider
cost functions for cluster trees that capture the quality of the hierarchical clustering produced
by T .
The Axiom. A natural property we would like the cost function to satisfy is that a cluster tree
T has minimum cost if and only if T is a generating tree for G. Indeed, the objective function can
then be used to indicate whether a given tree is generating and so whether it is an underlying
ground-truth hierarchical clustering. Hence, the objective function acts as a “guide” for finding
the correct hierarchical classification. Note that there may be multiple trees that are generating
for the same graph. For example, if G = (V, E,w) is a clique with every edge having the same
weight, then every tree is a generating tree. In these cases, all generating trees are valid groundtruth hierarchical clusterings.
Following the recent work of Dasgupta [23], we adopt an approach in which a cost is assigned
to each internal node of the tree T that corresponds to the quality of the split at that node and
restrict the search space for such cost functions. For an internal node N in a clustering tree T ,
let A, B ⊆ V be the leaves of the subtrees rooted at the left and right child of N, respectively. We
define the cost Γ of the tree T as the sum of the cost at every internal node N in the tree, and at
an individual node N we consider cost functions γ of the form
Γ(T ) =

N
γ (N), (2)
γ (N) =



x ∈A,y ∈B
w(x,y)



· д(|A|, |B|). (3)
We remark that Dasgupta [23] defined д(a,b) = a + b.
Journal of the ACM, Vol. 66, No. 4, Article 26. Publication date: June 2019. 
26:10 V. Cohen-Addad et al.
Definition 3.1 (Admissible Cost Function). We say that a cost function γ of the form of Equations (2) and (3) is admissible if it satisfies the condition that for all similarity graphs G = (V, E,w)
generated from a minimal ultrametric (V,d), a cluster tree T for G achieves the minimum cost if
and only if it is a generating tree for G.
Remark 1. Analogously, for the dissimilarity setting we define admissible value functions to be
the functions of the form of Equations (2) and (3) that satisfy the following: For all dissimilarity
graphG generated from a minimal ultrametric (V,d), a cluster treeT forG achieves the maximum
value if and only if it is a generating tree for G.
Remark 2. The RHS of Equation (3) has linear dependence on the weight of the cut (A, B) in the
subgraph of G induced by the vertex set A ∪ B as well as on an arbitrary function of the number
of leaves in the subtrees of the left and right child of the internal node creating the cut (A, B). For
the purpose of hierarchical clustering this form is fairly natural and indeed includes the specific
cost function introduced by Dasgupta [23]. We could define the notion of admissibility for other
forms of the cost function similarly and it would be of interest to understand whether they have
properties that are desirable from the point of view of hierarchical clustering.
3.2 Characterizing Admissible Cost Functions
In this section, we give an almost complete characterization of admissible cost functions of the form
of Equation (3). The following theorem shows that cost functions of this form are admissible if and
only if they satisfy three conditions: all cliques have the same cost, symmetry and monotonicity.
Theorem 3.2. Let γ be a cost function of the form of Equation (3) and let д be the corresponding
function used to define γ . Then γ is admissible if and only if it satisfies the following three conditions:
(1) Let G = (V, E,w) be a clique, i.e., for every x,y ∈ V , e = {x,y} ∈ E and w(e) = 1 for every
e ∈ E. Then the cost Γ(T ) for every cluster tree T of G is identical.
(2) For every n1,n2 ∈ N, д(n1,n2) = д(n2,n1).
(3) For every n1,n2 ∈ N, д(n1 + 1,n2) > д(n1,n2).
Proof. We first prove the only if part and then the if part.
Only If Part: Suppose thatγ is indeed an admissible cost function. We prove that all three conditions
must be satisfied by γ .
1. All cliques have same cost. We observe that a clique G = (V, E,w) can be generated from an
ultrametric. Indeed, let X = V and let d(u,v) = d(v,u) = 1 for every u,v ∈ X such that u  v and
d(u,u) = 0. Clearly, for f : R+ → R+ that is non-increasing and satisfying f (1) = 1, (V,d) is a
minimal ultrametric generating G.
Let T be any binary rooted tree with leaves labeled by V , i.e., a cluster tree for graph G. For
any internal node N of T define W (N) = 1 as the weight function. This satisfies the definition of
generating tree (Definition 2.2). Thus, every cluster tree T for G is generating and hence, by the
definition of admissibility all of them must be optimal, i.e., they all must have exactly the same
cost.
2. д(n1,n2) = д(n2,n1). This part follows more or less directly from the previous part. Let G be
a clique on n1 + n2 nodes. Let T be any cluster tree for G, with subtrees T1 and T2 rooted at the
left and right child of the root respectively, such that T1 contains n1 leaves and T2 contains n2
leaves. The number of edges, and hence the total weight of the edges, crossing the cut induced
by the root node of T is n1 · n2. Let T
 be a tree obtained by making T2 be rooted at the left child
of the root and T1 at the right child. Clearly T
 is also a cluster tree for G and induces the same
Journal of the ACM, Vol. 66, No. 4, Article 26. Publication date: June 2019.   
Hierarchical Clustering: Objective Functions and Algorithms 26:11
Fig. 3. Trees T and T  used to show monotonicity of д.
cut at the root node, hence using the property that all cliques have the same cost, Γ(T ) = Γ(T
).
But Γ(T ) = n1 · n2 · д(n1,n2) + Γ(T1) + Γ(T2) and Γ(T
) = n1 · n2 · д(n2,n1) + Γ(T1) + Γ(T2). Thence,
д(n1,n2) = д(n2,n1).
3. д(n1 + 1,n2) > д(n1,n2). Consider a graph on n1 + n2 + 1 nodes generated from an ultrametric as
follows. Let V1 = {v1,...,vn1 }, V2 = {v
1,...,v
n2 } and consider the ultrametric (V1 ∪V2 ∪ {v∗},d)
defined by d(x,y) = 1 if x  y and x,y ∈ V1 or x,y ∈ V2, d(x,y) = 2 if x  y and x ∈ V1,y ∈ V2 or
x ∈ V2,y ∈ V1, d(v∗, x) = d(x,v∗) = 3 for x ∈ V1 ∪V2, and d(u,u) = 0 for u ∈ V1 ∪V2 ∪ {v∗}. It can
be checked easily by enumeration that this is indeed an ultrametric. Furthermore, if f : R+ → R+
is non-increasing and satisfies f (1) = 2, f (2) = 1 and f (3) = 0, i.e.,w({u,v}) = 2 ifu andv are both
either in V1 or V2, w({u,v}) = 1 if u ∈ V1 and v ∈ V2 or the other way around, and w({v∗,u}) = 0
for u ∈ V1 ∪V2, then (V1 ∪V2, {v∗},d) is a minimal ultrametric generating G.
Now consider two possible cluster trees defined as follows: Let T1 be an arbitrary tree on nodes
V1, T2 and arbitrary tree on nodes V2. T is obtained by first joining T1 and T2 using internal node
N and making this the left subtree of the root node ρ and the right subtree of the root node is just
the singleton node v∗. T  is obtained by first creating a tree by joining T1 and the singleton node
v∗ using internal node N 
, this is the left subtree of the root node ρ and T2 is the right subtree of
the root node. (See Figure 3(a) and (b).)
Now it can be checked thatT is generating by defining the following weight function. For every
internal node M of T1, let W (M) = 1, similarly for every internal node M of T2, let W (M) = 1,
define W (N) = 2 and W (ρ) = 3. Now, we claim that T  cannot be a generating tree. This follows
from the fact that for a node u ∈ V1,v ∈ V2, the root node ρ = LCAT  (u,v), but it is also the case
that ρ = LCAT  (v∗,v) (recall that LCAT  (u,v) denotes the lowest common ancestor of u,v in T 
,
as defined in Section 2). Thus, it cannot possibly be the case that W (ρ) = w({u,v}) and W (ρ) =
w({v∗,v}) as w({u,v})  w({v∗,v}). By definition of admissibility, it follows that Γ(T ) < Γ(T 
),
but Γ(T ) = Γ(T1) + Γ(T2) + n1 · n2 · д(n1,n2). The last term arises from the cut at node N; the root
makes no contribution as the cut at the root node ρ has weight 0. However, Γ(T 
) = Γ(T1) + Γ(T2) +
n1 · n2 · д(n1 + 1,n2). There is no cost at the node N 
, since the cut has size 0; however, at the root
node the cost is now n1 · n2 · д(n1 + 1,n2) as the left subtree at the root contains n1 + 1 nodes. It
follows that д(n1 + 1,n2) > д(n1,n2).
If Part: For the other direction, we first use the following observation. By condition 1 in the statement of the theorem, every clique on n nodes has the same cost irrespective of the tree used for
hierarchical clustering; let κ(n) denote said cost. Let n1,n2 ≥ 1, then we have
n1 · n2 · д(n1,n2) = κ(n1 + n2) − κ(n1) − κ(n2). (4)
Journal of the ACM, Vol. 66, No. 4, Article 26. Publication date: June 2019.     
26:12 V. Cohen-Addad et al.
We will complete the proof by induction on |V |. The minimum number of nodes required to
have a cluster tree with at least one internal node is 2. Suppose |V | = 2, then there is a unique
(up to interchanging left and right children) cluster tree; this tree is also generating and hence by
definition any cost function is admissible. Thus, the base case is covered rather easily.
Now, consider a graph G = (V, E,w) with |V | = n > 2. Let T ∗ be a tree that is generating. Suppose that T is any other tree. Let ρ∗ and ρ be the root nodes of the trees, respectively. Let V ∗
L and
V ∗
R be the nodes on the left subtree and right subtree of ρ∗; similarly VL and VR in the case of ρ. Let
A = V ∗
L ∩VL, B = V ∗
L ∩VR, C = V ∗
R ∩VL, D = V ∗
R ∩VR. Let a, b, c, and d denote the sizes of A, B, C,
and D, respectively.
We will consider the case when all of a,b,c,d > 0; the proof is similar and simpler in case some
of them are 0. Let T
 be a tree with root ρthat has the following structure: Both children of the
root are internal nodes, all of A appears as leaves in the left subtree of the left child of the root,
B as leaves in the right subtree of the left child of the root, C as leaves in the left subtree of the
right child of the root and D as leaves in the right subtree of the right child of the root. We assume
that all four subtrees for the sets A, B, C, D are generating and hence by induction optimal. We
claim that the cost of T
is at least as much as the cost of T ∗. To see this note that V ∗
L = A ∪ B. Thus,
the left subtree of ρ∗ is optimal for the set V ∗
L (by induction), whereas that of ρmay or may not
be. Similarly, for all the nodes in V ∗
R . The only other thing left to account for is the cost at the
root. But since ρ∗ and ρinduce exactly the same cut on V , the cost at the root is the same. Thus,
Γ(T
) ≥ Γ(T ∗). Furthermore, equality holds if and only if T
 is also generating for G.
Let W ∗ denote the weight function for the generating tree T ∗ such that for all u,v ∈ V ,
W ∗ (LCAT ∗ (u,v)) = w({u,v}). Let ρ∗
L and ρ∗
R denote the left and right children of the root ρ∗ of
T ∗. For all ua ∈ A,ub ∈ B, w({ua,ub }) ≥ W ∗ (ρ∗
L ). Let
x = 1
ab

ua ∈A,ub ∈B
w({ua,ub })
denote the average weight of the edges going between A and B; it follows that x ≥ W ∗ (ρ∗
L ). Similarly, for all uc ∈ C,ud ∈ D, w({uc ,ud }) ≥ W ∗ (ρ∗
R ). Let
y = 1
cd

uc ∈C,ud ∈D
w({uc ,ud })
denote the average weight of the edges going between C and D; it follows that y ≥ W ∗ (ρ∗
R ). Finally, for every u ∈ A ∪ B,u ∈ C ∪ D,w({u,u
}) = W ∗ (ρ∗); denote this common value by z. By the
definition of generating tree, we know that x ≥ z and y ≥ z.
Now consider the tree T . Let TL and TR denote the left and right subtrees of ρ. By induction, it
must be that TL splits A and C as the first cut (or at least that’s one possible tree, if multiple cuts
exist); similarly, TR first cuts B and D. Both T and T
 have subtrees containing only nodes from A,
B, C, and D. The costs for these subtrees are identical in both cases (by induction). Thus, we have
Γ(T ) − Γ(T
) = zac · д(a,c) + zbd · д(b,d) + (xab + ycd + z(ad + bc)) · д(a + c,b + d)
− xab · д(a,b) + y ·cdд(c,d) − z(a + b)(c + d) · д(a + b,c + d)
= (x − z)ab(д(a + c,b + d) − д(a,b)) + (y − z)cd(д(a + c,b + d) − д(c,d))
+ z((a + c)(b + d) · д(a + c,b + d) + ac · д(a,c) + bd · д(b,d))
− z((a + b)(c + d) · д(a + b,c + d) + ab · д(a,b) + cd · д(c,d)).
Journal of the ACM, Vol. 66, No. 4, Article 26. Publication date: June 2019.         
Hierarchical Clustering: Objective Functions and Algorithms 26:13
Fig. 4. The caterpillar cluster tree for a clique with 4 nodes.
Using Equation (4), we get that the last two expressions above both evaluate to z(κ(a + b + c +
d) − κ(a) − κ(b) − κ(c) − κ(d)) but have opposite signs. Thus, we get
Γ(T ) − Γ(T
) = (x − z)ab(д(a + c,b + d) − д(a,b)) + (y − z)cd(д(a + c,b + d) − д(c,d)).
It is clear that the above expression is always non-negative and is 0 if and only if x = z and y = z.
If it is the latter case and it is also the case that Γ(T
) = Γ(T ∗), then it must actually be the case that
T is a generating tree.
3.2.1 Characterizing д that Satisfy Conditions of Theorem 3.2. Theorem 3.2 give necessary and
sufficient conditions onд for cost functions of the form of Equation (3) to be admissible. However, it
leaves open the question of the existence of functions satisfying the criteria and also characterizing
the functions д themselves. The fact that such functions exist already follows from the work of
Dasgupta [23], who showed that if д(n1,n2) = n1 + n2, then all cliques have the same cost. Clearly,
д is monotone and symmetric and thus satisfies the condition of Theorem 3.2.
To give a more complete characterization, we define д as follows: Suppose д(·, ·) is symmetric,
we define д(n, 1) for all n ≥ 1 so that д(n, 1)/(n + 1) is non-decreasing.10 We consider a particular cluster tree for a clique that is defined using a caterpillar graph, i.e., a cluster tree where
the right child of any internal node is a leaf labeled by one of the nodes of G and the left child
is another internal node, except at the very bottom. Figure 4 shows a caterpillar cluster tree for
a clique on four nodes. The cost of the clique on n nodes, say, κ(n), using this cluster tree is
given by
κ(n) =
n−1
i=0
i · д(i, 1).
Now we enforce the condition that all cliques have the same cost by defining д(n1,n2) for n1,n2 > 1
suitably, in particular,
д(n1,n2) = κ(n1 + n2) − κ(n1) − κ(n2)
n1 · n2
. (5)
Thus it only remains to be shown that д is strictly increasing. We show that for n2 ≤ n1, д(n1 +
1,n2) > д(n1,n2). To show this it suffices to show that
n1 (κ(n1 + n2 + 1) − κ(n1 + 1) − κ(n2)) − (n1 + 1)(κ(n1 + n2) − κ(n1) − κ(n2)) > 0.
10The function proposed by Dasgupta [23] is д(n, 1) = n + 1, so this ratio is always 1.
Journal of the ACM, Vol. 66, No. 4, Article 26. Publication date: June 2019.   
26:14 V. Cohen-Addad et al.
Thus, consider
n1 (κ(n1 + n2 + 1) − κ(n1 + 1) − κ(n2)) − (n1 + 1)(κ(n1 + n2) − κ(n1) − κ(n2))
= n1 (κ(n1 + n2 + 1) − κ(n1 + n2) − κ(1)−κ(n1 + 1) + κ(n1) + κ(1))−(κ(n1 + n2)−κ(n1)−κ(n2))
= n1 (n1 + n2)д(n1 + n2, 1) − n2
1д(n1, 1) − (κ(n1 + n2) − κ(n1) − κ(n2))
≥ n1 (n1 + n2)д(n1 + n2, 1) − n2
1д(n1, 1) −
n1+n2−1
i=n1
i · д(i, 1)
≥ д(n1 + n2, 1)
n1 + n2 + 1 ·



n1 (n1 + n2)(n1 + n2 + 1) − n2
1 (n1 + 1) −
n1+n2−1
i=n1
i(i + 1)



> 0.
Above we used the fact that д(n, 1)/(n + 1) is non-decreasing in n and some elementary calculations. This shows that the objective function proposed by Dasgupta [23] is by no means unique.
Only in the last step do we get an inequality where we use the condition that д(n, 1)/(n + 1) is
increasing. Whether this requirement can be relaxed further is also an interesting direction.
3.2.2 Characterizing Objective Functions for Dissimilarity Graphs. When the weights of the
edges represent dissimilarities instead of similarities, one can consider objective functions of the
same form as Equation (3). As mentioned in Remark 1, the difference in this case is that the goal is
to maximize the objective function and hence the definition of admissibility now requires that generating trees have a value of the objective that is strictly larger than any tree that is not generating.
The characterization of admissible objective functions as given in Theorem 3.2 for the similarity
case continues to hold in the case of dissimilarities. The proof follows in the same manner by
appropriately switching the direction of the inequalities when required.
4 WORST-CASE
We now provide approximation algorithms for Dasgupta’s objective function for the similarity
setting (Section 4.1) and its analogue for the dissimilarity setting (Section 4.2), i.e., we treat each
edge weight as a measure of dissimilarity and consider the problem of maximizing the cost defined
by Dasgupta. Note that Dasgupta showed that the problem of finding an optimal solution w.r.t.
to his cost function for both the similarity and dissimilarity setting is NP-hard [23]. In addition,
Charikar and Chatziafratis [17] and Roy and Pokutta [41] showed that the similarity version of
the problem does not admit an O(1)-approximation algorithm assuming the SSE-hypothesis.
4.1 Similarity-Based Inputs: Approximation Algorithms
4.1.1 Analysis of the Recursive Sparsest-cut Algorithm w.r.t. Dasgupta’s Cost Function. In this
section, we analyze the recursive ϕ-sparsest-cut algorithm (see Algorithm 1) that was described
previously in Referencee [23]. Recall the cost function introduced by Dasgupta [23]: The cost of
tree T is cost(T ) =
N ∈T cost(N), where for each node N of T with children N1, N2, cost(N) =
w(V (N1),V (N2)) · V (N). The goal is to find a tree T ∗ minimizing cost(T ∗). We show that the ϕsparsest-cut algorithm achieves a 6.75ϕ-approximation. Charikar and Chatziafratis [17] proved an
8ϕ approximation for Dasgupta’s function.
The ϕ-sparsest-cut algorithm (Algorithm 1) constructs a binary tree top-down by recursively
finding cuts using a ϕ-approximate sparsest cut algorithm, where the sparsest-cut problem asks
for a set A minimizing the sparsity w(A,V \ A)/(|A||V \ A|) of the cut (A,V \ A).
Theorem 4.1. For any graph G = (V, E), and weight function w : E → R+, the ϕ-sparsest-cut algorithm (Algorithm 1) outputs a solution of cost at most 27
4 ϕOPT.
Journal of the ACM, Vol. 66, No. 4, Article 26. Publication date: June 2019.  
Hierarchical Clustering: Objective Functions and Algorithms 26:15
ALGORITHM 1: Recursive ϕ-Sparsest-Cut Algorithm for Hierarchical Clustering
1: Input: An edge weighted graph G = (V, E,w).
2: {A,V \ A} ← cut with sparsity ≤ ϕ · min
S ⊂V w(S,V \ S)/(|S ||V \ S |)
3: Recurse on G[A] and on G[V \ A] to obtain trees TA and TV \A
4:
5: return The tree whose root has two children, TA and TV \A.
Proof. Let G = (V, E) be the input graph and n denote the total number of vertices of G. Let T
denote the tree output by the algorithm and T ∗ be any arbitrary tree. We will prove that cost(T ) ≤
27
4 ϕcost(T ∗).
Recall that for an arbitrary tree T0 and node N of T0, the vertex corresponding to the leaves
of the subtree rooted at N is denoted by V (N). Consider the node N0 of T ∗ that is the first node
reached by the walk from the root that always goes to the child tree with the higher number of
leaves, stopping when the subtree ofT ∗ rooted at N0 contains fewer than 2n/3 leaves. The balanced
cut (BC) of T ∗ is the cut (V (N0),V −V (N0)). For a given node N with children N1, N2, we say that
the cut induced by N is the sum of the weights of the edges that have one extremity in V (N1) and
the other in V (N2).
Let (A ∪C, B ∪ D) be the cut induced by the root node u of T , where A, B,C,D are such that
(A ∪ B,C ∪ D) is the balanced cut of T ∗. Since (A ∪C, B ∪ D) is a ϕ-approximate sparsest cut:
w(A ∪C, B ∪ D)
|A ∪C|·|B ∪ D|
≤ ϕ
w(A ∪ B,C ∪ D)
|A ∪ B|·|C ∪ D|
.
By definition of N0, A ∪ B and C ∪ D both have size in [n/3, 2n/3], so the product of their sizes
is at least (n/3)(2n/3) = 2n2/9; developing w(A ∪ B,C ∪ D) into four terms, we obtain
w(A ∪C, B ∪ D) ≤ ϕ 9
2n2 |A ∪C||B ∪ D|(w(A,C) + w(A,D) + w(B,C) + w(B,D))
≤ ϕ 9
2
	
|B ∪ D|
n w(A,C) + w(A,D) + w(B,C) +
|A ∪C|
n w(B,D)


,
and so the cost induced by node u of T ∗ satisfies
n · w(A ∪C, B ∪ D) ≤
9
2
ϕ|B ∪ D|w(A,C) +
9
2
ϕ|A ∪C|w(B,D) +
9
2
ϕn(w(A,D) + w(B,C)).
To account for the cost induced by u, we thus assign a charge of (9/2)ϕ|B ∪ D|w(e) to each edge
e of (A,C), a charge of (9/2)ϕ|A ∪C|w(e) to each edge e of (B,D), and a charge of (9/2)ϕnw(e) to
each edge e of (A,D) or (B,C).
When we do this for every node u of T , how much does each edge get charged?
Lemma 4.2. Let G = (V, E) be a graph on n nodes. We consider the above charging scheme for T
and T ∗. Then, an edge (v1,v2) ∈ E gets charged at most (9/2)ϕ min((3/2)|V (LCAT ∗ (v1,v2))|,n)w(e)
overall, where LCAT ∗ (v1,v2) denotes the lowest common ancestor of v1 and v2 in T ∗.
We temporarily defer the proof and first see how Lemma 4.2 implies the theorem. Observe (as
in Reference [23]) that cost(T ∗) =
{u,v }∈E |V (LCAT ∗ (u,v))|w(u,v). Using Lemma 4.2, the sum of
charges assigned is
cost(T ) ≤
9
2
ϕ

{v1,v2 }∈E
3
2
|V (LCAT ∗ (v1,v2))|w(v1,v2) = 27
4 ϕcost(T ∗).
Journal of the ACM, Vol. 66, No. 4, Article 26. Publication date: June 2019.  
26:16 V. Cohen-Addad et al.
Proof of Lemma 4.2. The lemma is proved by induction on the number of nodes of the graph.
The base case follows immediately, and we proceed with the inductive step. For the inductive step,
consider the cut (A ∪C, B ∪ D) induced by the root node u of T .
• Consider the edges that cross the cut. First, observe that edges of (A, B) or of (C,D) never get
charged at all. Second, an edge e = {v1,v2} of (A,D) or of (B,C) gets charged (9/2)ϕnw(e)
when considering the cost induced by node u, and does not get charged when considering
any other node of T . In T ∗, edge e is separated by the cut (A ∪ B,C ∪ D) induced by N0, so
the least common ancestor ofv1 andv2 is the parent node of N0 (or above), and by definition
of N0 we have |V (LCAT ∗ (v1,v2))| ≥ 2n/3, hence the lemma holds for e.
• An edge e = {v1,v2} ofG[A] ∪ G[C] does not get charged when considering the cut induced
by node u. Apply Lemma 4.2 to G[A ∪C] for the tree T ∗
A∪C defined as the subtree of T ∗ induced by the vertices of A ∪C.
11 By induction, the overall charge to e due to the recursive
calls for G[A ∪C] is at most (9/2)ϕ min((3/2)|V (LCAT ∗
A∪C (v1,v2))|, |A ∪C|)w(e). By definition ofT ∗
A∪C, we have |V (LCAT ∗
A∪C (v1,v2))|≤|V (LCAT ∗ (v1,v2))|, and |A ∪C| ≤ n, so the
lemma holds for e.
• An edge {v1,v2} of (A,C) gets a charge of (9/2)ϕ|B ∪ D|w(e) plus the total charge to e
coming from the recursive calls for G[A ∪C] and the tree T ∗
A∪C. By induction the latter is
at most
(9/2)ϕ min((3/2)|V (LCAT ∗
A∪C (v1,v2))|, |A ∪C|)w(e) ≤ (9/2)ϕ|A ∪C|w(e).
Overall, the charge to e is at most (9/2)ϕnw(e). Since the cut induced by node u0 of T ∗
separates v1 from v2, we have |V (LCAT ∗ (v1,v2))| ≥ 2n/3, hence the lemma holds for e. For
edges of (B,D) or of G[B] ∪ G[D], a symmetric argument applies.
We complete our study of classical algorithms for hierarchical clustering by showing that the
standard agglomerative heuristics can perform poorly when measured using Dasgupta’s cost function (see Theorems B.1, B.2). Thus, the recursive sparsest-cut-based approach seems to be more
reliable in the worst case for inputs given as a similarity graph. To understand better the success
of the agglomerative heuristics, we restrict our attention random graphs (Section 5) and to inputs
given as dissimilarity graphs and show that in these contexts agglomerative heuristics are efficient.
4.2 Dissimilarity-Based Inputs: Approximation Algorithms
In this section, we consider inputs given as dissimilarity graphs and focus on the problem of
maximizing the analogue of Dasgupta’s function: Find T maximizing the value function: val(T ) =
N ∈T val(N), where for each node N ofT with children N1, N2, val(N) = w(V (N1),V (N2)) · V (N).
This optimization problem is NP-hard [23], and hence we focus on approximation algorithms.
We show (Theorem 4.3) that average-linkage achieves a 2/3 approximation for the problem. We
then introduce a simple algorithm based on locally densest cuts and show (Theorem 4.6) that it also
achieves a 2/3 − ε approximation for the problem.
Chatziafratis, Nizadeh, and Charikar [19] showed that the divisive heuristic that recursively
splits the graph randomly into two parts also yields a 2/3-approximation. Thus our results regarding the performance of average-linkage and locally densest cut should be viewed in contrast to
that of other popular heuristics; we show that single-linkage and bisection 2-means could perform
quite poorly and do not achieve a constant factor approximation. Thus, the outcome of this section
11Note that T ∗
A∪C is not necessarily the optimal tree for G[A ∪ C], which is why the lemma was stated in terms of every
tree T ∗, not just on the optimal tree.
Journal of the ACM, Vol. 66, No. 4, Article 26. Publication date: June 2019.  
Hierarchical Clustering: Objective Functions and Algorithms 26:17
is that average-linkage and locally densest cut are simple popular heuristics that seem to be more
reliable than single-linkage or bisection 2-means in the worst-case setting.
We start with the following elementary upper bound on OPT.
Fact 1. For any graph G = (V, E), and weight function w : E → R+, we have OPT ≤ n ·

e ∈E w(e).
4.2.1 Average-Linkage for the Dissimilarity Setting. We show that average-linkage yields a 2/3-
approximation in the dissimilarity setting.
Theorem 4.3. For any graph G = (V, E), and weight function w : E → R+, the average-linkage
algorithm (Algorithm 2) outputs a solution of value at least 2
3n
e ∈E w(e) ≥ 2
3OPT.
ALGORITHM 2: Average-Linkage Algorithm for Hierarchical Clustering (dissimilarity setting)
1: Input: Graph G = (V, E) with edge weights w : E → R+
2: Create n singleton trees.
3: while there are at least two trees do
4: Take trees with roots N1 and N2 minimizing
x ∈V (N1 ),y ∈V (N2 )
w(x,y)/(|V (N1)||V (N2)|)
5: Create a new tree with root N and children N1 and N2
6: return the resulting binary tree T
We recall the definitions of the previous section: When two trees are chosen at Step 4 of Algorithm 2, we say that they are merged. We say that all the trees considered at the beginning of an
iteration of the while loop are the trees that are candidates for the merge or simply the candidate
trees.
We first show the following lemma and then prove the theorem.
Lemma 4.4. Let T be the output tree and A, B be the two children of the root. Then, the following
holds:
|V (A)|w(V (A),V (B)) ≥ 2|V (B)|w(V (A)).
Proof. First, observe that if |V (A)| = 1, then w(V (A)) = 0, and the statement holds trivially.
Assume that |V (A)| > 1 and let a = |V (A)|(|V (A)| − 1)/2.
For any node N0 of T , let child1 (N0) and child2 (N0) be the two children of N0. We first consider
the subtree TA of T rooted at A. We have
• w(V (A)) =
A0 ∈TA w(V (child1 (A0)),V (child2 (A0))),
• a =
A0 ∈TA |V (child1 (A0))|·|V (child2 (A0))|.
By using an averaging argument, there exists A ∈ TA with children A1,A2 such that
w(V (A1),V (A2))
|V (A1)|·|V (A2)|
≥ w(V (A))
a . (6)
We now consider the iteration of the while loop at which the algorithm merged the trees A1 and
A2. Let A1,A2,...,Ak and B1, B2,..., B be the trees that were candidate for the merge at that
iteration and such that V (Ai ) ∩V (B) = ∅ and V (Bi ) ∩V (A) = ∅. Observe that the sets of leaves of
those trees form a partition of the sets V (A) and V (B), so we have
w(V (A),V (B)) =

i,j
w(V (Ai ),V (Bj)),
|V (A)|·|V (B)| =

i,j
|V (Ai )|·|V (Bj)|.
Journal of the ACM, Vol. 66, No. 4, Article 26. Publication date: June 2019.      
26:18 V. Cohen-Addad et al.
By an averaging argument again, there exists Ai, Bj such that
w(V (Ai ),V (Bj))
|V (Ai )|·|V (Bj)|
≤ w(V (A),V (B))
|V (A)|·|V (B)|
. (7)
Now, since the algorithm merged A1,A2 rather than Ai, Bj , by combining Equations (6) and (7), we
have w(V (A))
a ≤ w(V (A1),V (A2))
|V (A1)|·|V (A2)|
≤ w(V (Ai ),V (Bj))
|V (Ai )|·|V (Bj)|
≤ w(V (A),V (B))
|V (A)|·|V (B)|
.
Substituting the value of a completes the proof.
Proof of Theorem 4.3. We proceed by induction on the number of the nodes n in the graph.
For any n < 3, the tree is unique and so the output optimal. Let A, B be the children of the root of
the output tree T . By induction,
val(T ) ≥ (|V (A)| + |V (B)|) · w(V (A),V (B)) + 2
|V (A)|
3 w(V (A)) + 2
|V (B)|
3 w(V (B)). (8)
Applying Lemma 4.4 to A and B implies
(|V (A)| + |V (B)|) · w(V (A),V (B)) ≥ 2(|V (B)|w(V (A)) + |V (A)|w(V (B))).
Dividing both sides by 3 and plugging it into Equation (8) yields
val(T ) ≥ 2
|V (A)| + |V (B)|
3 w(V (A),V (B)) + 2
|V (A)| + |V (B)|
3 (w(V (A)) + w(V (B))).
Observing that n = |V (A)| + |V (B)| and combining
e ∈E w(e) = w(V (A),V (B)) + w(V (A)) +
w(V (B)) with Fact 1 completes the proof.
4.2.2 A Simple Local-Search-based Approximation Algorithm for Worst-Case Inputs. In this section, we introduce a very simple algorithm (Algorithm 4) that achieves a similar approximation
guarantee. The algorithm follows a divisive approach by recursively computing locally densest
cuts using a local search heuristic (see Algorithm 3). This approach is similar to the recursivesparsest-cut algorithm of Section 4.1. Here, instead of trying to solve the densest-cut problem (and
so being forced to use approximation algorithms), we solve the simpler problem of computing a
locally densest cut. This yields both a very simple local-search-based algorithm that has a good
approximation guarantee.
We use the notation A ⊕ x to denote the set obtained by adding x to A if x  A and by removing
x from A if x ∈ A. We say that a cut (A, B) is an ε/n locally densest cut if for any x,
w(A ⊕ x, B ⊕ x)
|A ⊕ x |·|B ⊕ x |
≤

1 +
ε
n
 w(A, B)
|A||B| .
The following local search algorithm computes an ε/n locally densest cut.
Theorem 4.5. Algorithm 3 computes an ε/n locally densest cut in time O(n(n + m)/ε).
ALGORITHM 3: Local Search for Densest Cut
1: Input: Graph G = (V, E) with edge weights w : E → R+
2: Let (u,v) be an edge of maximum weight
3: A ← {v}, B ← V \ {v}
4: while ∃x: w (A⊕x,B ⊕x )
|A⊕x |·|B ⊕x | > (1 + ε/n)
w (A,B)
|A| |B | do
5: A ← A ⊕ x, B ← B ⊕ x
6: return the cut (A, B)
Journal of the ACM, Vol. 66, No. 4, Article 26. Publication date: June 2019.     
Hierarchical Clustering: Objective Functions and Algorithms 26:19
Proof. The proof is straightforward and given for completeness. By definition, the algorithm
computes an ε/n locally densest cut so we only need to argue about the running time. The weight
of the cut is initially at leastwmax, the weight of the maximum edge weight, and in the end at most
mwmax. Since the weight of the cut increases by a factor of (1 + ε/n) at each iteration, the total
number of iterations of the while loop is at most log1+ε /n (mwmax/wmax) = O(n/ε). Each iteration
takes time O(m + n), so the running time of the algorithm is O(n(m + n)/ε).
ALGORITHM 4: Recursive Locally Densest Cut for Hierarchical Clustering
1: Input: Graph G = (V, E), with edge weights w : E → R+, ε > 0
2: Compute an ε/n locally densest cut (A, B) using Algorithm 3
3: Recurse on G[A] and G[B] to obtain rooted trees TA and TB.
4: return the tree T whose root node has two children with subtrees TA and TB.
Theorem 4.6. Algorithm 4 returns a tree of value at least
2n
3 (1 − ε)

e
w(e) ≥
2
3
(1 − ε)OPT,
in time O(n2 (n + m)/ε).
The proof relies on the following lemma.
Lemma 4.7. Let (A, B) be an ε/n locally densest cut. Then,
(|A| + |B|)w(A, B) ≥ 2(1 − ε)(|B|w(A) + |A|w(B)).
Proof. First, assume |A| > 1 and let v ∈ A. By definition of the algorithm,
(1 + ε/n)
w(A, B)
|A||B| ≥ w(A \ {v}, B ∪ {v})
(|A| − 1)(|B| + 1) .
Rearranging,
(|A| − 1)(|B| + 1)
|A||B| (1 + ε/n)w(A, B) ≥ w(A \ {v}, B ∪ {v}) = w(A, B) + w(v,A) − w(v, B).
Summing over all vertices of A, we obtain
|A|
(|A| − 1)(|B| + 1)
|A||B| (1 + ε/n)w(A, B) ≥ |A|w(A, B) + 2w(A) − w(A, B).
Rearranging and simplifying,
(|A| − 1)(|B| + 1)
ε
n
w(A, B) + (|A| − 1)(1 + ε/n)w(A, B) ≥ 2|B|w(A).
Since |B| + 1 ≤ n, this gives
|A|w(A, B) ≥ 2(1 − ε)|B|w(A).
Notice that if |A| = 1, then the above inequality can be obtained trivially as w(A) = 0.
Proceeding similarly with B and summing the two inequalities yields the lemma.
Proof of Theorem 4.6. We first show the approximation guarantee. We proceed by induction
on the number of vertices. The base case is trivial. By inductive hypothesis,
val(T ) ≥ nw(A, B) +
2
3 · (1 − ε)(|A|w(A) + |B|w(B)),
Journal of the ACM, Vol. 66, No. 4, Article 26. Publication date: June 2019.     
26:20 V. Cohen-Addad et al.
where n = |A| + |B|. Lemma 4.7 implies
nw(A, B) = (|A| + |B|)w(A, B) ≥ 2(1 − ε)(|B|w(A) + |A|W (B)).
Hence,
|A| + |B|
3 w(A, B) ≥
2
3
(1 − ε)(|B|w(A) + |A|w(B)).
Therefore,
val(T ) ≥
2n
3 (1 − ε)(w(A, B) + w(A) + w(B)) = (1 − ε)
2n
3

e
w(e).
To analyze the running time, observe that by Theorem 4.5, a recursive call on a graph G =
(V 
, E
) takes time O(|V 
|(|V 
| + |E
|)/ε) and that the number of recursive calls is O(n).
Remark 3. In Appendix B, we show that other commonly used algorithms, single-linkage, or
bisection 2-Center, can perform arbitrarily poorly (see Theorem B.3). Hence, average-linkage is
more robust for dissimilarity inputs.
5 A GENERAL HIERARCHICAL STOCHASTIC BLOCK MODEL
Lyzinski et al. [35] studied a Hierarchical Stochastic Block Model (HSBM) and gave an algorithm
that recovers the “ground-truth” hierarchical clustering when the hidden clusters have linear size
and the ratio between the minimum edge probability and the maximum edge probability is O(1).
Krishamurthy et al. [32] provide active learning algorithms that guarantee exact recovery of the
ground-truth tree under certain conditions.
We introduce a generalization of HSBM and give a (1 + o(1))-approximation algorithm to recover a near-optimal hierarchical clustering under a more general setting. Our algorithm is very
similar to the widely used linkage approach and remains easy to implement and parallelize. The
take-home message is that, on “structured inputs” the linkage (agglomerative) heuristics perform
provably well, taking a step toward explaining their success in practice.
The graphs generated from our model are a noisy version of a “ground-truth hierarchical clustering tree” (see Definition 5.2). For a motivating example, the tree of life has a natural associated
hierarchical clustering, but because of extinct species, the input is imperfect and noisy. Our definition uses the notion of a generating tree (Definition 2.2) which can be associated to an ultrametric
(and therefore to a “natural” hierarchical clustering). Each edge of the graph thus generated has a
certain probability of being present, which only depends on the underlying ground-truth tree and
the least common ancestor two endpoints.
Definition 5.1 (Hierarchical Stochastic Model (HSM)). Let T
 be a generating tree for an n-vertex
graph G¯, called the expected graph, such that all weights are in [0, 1]. A hierarchical stochastic
model is a random graph G such that for every two vertices u and v, the edge {u,v} is present
independently with probability w({u,v}) = W (LCAT (σ−1 (u), σ−1 (v))), where w and W are the
weights functions associated with T
 as per Definition 2.2. In words, the probability of an edge
being present is given by the weight of the lowest common ancestor of the corresponding vertices
in T
.
Definition 5.2 (Hierarchical Stochastic Block Model (HSBM)). A hierarchical stochastic model is
a k-hierarchical stochastic block model (k-HSBM) if T contains k disjoint subtrees, spanning all
leaves, such that W is constant inside each of the k subtrees.
We let pmin denote the weight of the root node of T
 for both HSM and HSBM. T
 is called a
ground-truth tree (see Figure 5 for an illustration). For any treeT , whenever there is a possibility of
ambiguity, we use cost(T ;G) and cost(T ;G¯) to denote the the costs of the cluster tree T for graphs
Journal of the ACM, Vol. 66, No. 4, Article 26. Publication date: June 2019.       
Hierarchical Clustering: Objective Functions and Algorithms 26:21
Fig. 5. An illustration of an HSBM. Nodes represent authors on arXiv.org, and there is an edge between two
nodes if the two authors have a joint publication. Typically, authors of the same community have more edges
within their community and sub-communties. This is reflected by the increase of the connection probability
of a from the root toward the leaves.
G and forG¯, respectively: Observe that cost(T ;G¯) = E[ cost(G;T ) ], where the expectation is taken
with respect to the randomness over the edges in G.
5.1 Objective Functions and Ground-Truth Tree
In this section, we show that the cost of the ground-truth tree (which is optimal for the expected
graph G¯) is near-optimal for the realized graph. All results in this section assume that the cost
function is the one introduced by Dasgupta [23]; under some smoothness assumption on the cost
functions the results can be generalized to other cost functions with (possibly) slightly worse constant factors.
In Proposition 5.4 we show that the treeT minimizing the expected cost for a graphG generated
from an HSM is minimized for a ground-truth tree (note that the ground-truth tree might not be
unique, but they are all of the same cost). Furthermore, we show in Theorem 5.5 that, under mild
assumptions, even in the realized graph, the cost induced by the ground-truth tree is within a
(1 + o(1)) factor of the cost of the optimal tree.
For the proof of Proposition 5.4, we will make use of a slightly generalized version of the Hoeffding bound (see Reference [29]).
Proposition 5.3 ([29]). Let X = m
i=1 Xi be a sum of m independent random variables with ai ≤
Xi ≤ bi for all i. Then for any t > 0:
P[ |X − E[X ]| ≥ t ] ≤ exp 
− 2t 2
m
i=1 (bi − ai )2

. (9)
Proposition 5.4. Let G be a graph generated according to an HSM (see Definition. 5.1). Then a
tree T is a ground-truth tree for G if and only if
E[cost(T ) ] = min
T  E[cost(T 
) ].
Journal of the ACM, Vol. 66, No. 4, Article 26. Publication date: June 2019.  
26:22 V. Cohen-Addad et al.
Proof. Let G¯ be the expected graph. By linearity of expectation, for any tree T (not necessarily
the ground-truth one) E[ cost(T ;G) ] = cost(T ;G¯). By the definition of admissibility, cost(T ;G¯) =
minT  cost(T 
;G¯) if an only if T is generating (see Definition 3.1).
The following shows that the ground-truth tree is a (1 + o(1))-approximation of the optimal
solution.
Theorem 5.5. Let T
 be a ground-truth tree for a graph G generated from an HSM (see Definition. 5.1). If pmin = ω(

logn/n), then, with high probability,
cost(T
;G) ≤ (1 + o(1)) min
T  cost(T 
;G) = (1 + o(1))OPT.
Proof. It suffices to show that with high probability, the following holds: For every binary tree
T with n leaves labeled by the vertices of G,
|cost(T ;G) − E[ cost(T ;G) ]| ≤ o(E[ cost(T ) ]). (10)
Indeed, if Equation (10) holds, then cost(T
;G) ≤ (1 + o(1))E[ cost(T
;G) ]; we know from Proposition 5.4 that E[ cost(T
;G) ] = minT  E[ cost(T 
;G) ] and by Equation (10) again for any tree T 
,
E[ cost(T 
;G) ] ≤ (1 + o(1))cost(T 
;G).
To prove Equation (10), we observe that the number of possible cluster trees (including labelings
of the leaves to vertices of G) is bounded by 2c ·n log n , for some constant c. Fix a cluster tree T 
. It
suffices to prove that for any fixed c > 0,
P[ |cost(T 
;G) − E[ cost(T 
;G) ]| ≥ o(E[ cost(T 
;G) ]) ] ≤ exp(−2cn logn).
We can write
cost(T 
;G) =

N ∈T 
(|V (child1 (N))| + |V (child2 (N))|)

i ∈V (child1 (N ))
j ∈V (child2 (N ))
1(i,j)∈E =

1≤i<j ≤n
Zi,j,
where Zi,j = |V (LCAT  (i, j))| · 1(i,j)∈E . Observe that the random variables Zi,j are independent. We
apply Proposition 5.3 with t = E[ cost(T 
;G) ] ·
√
6c
	log n
n /pmin = o(E[ cost(T 
;G) ]) and derive:
P
⎡
⎢
⎢
⎢
⎢
⎢
⎢
⎣
|cost(T 
;G) − E[ cost(T 
;G) ]| ≥ E[ cost(T 
) ] ·
√
6c ·
	log n
n
pmin
⎤
⎥
⎥
⎥
⎥
⎥
⎥
⎦
≤ exp






−
2


E[ cost(T 
;G) ] ·
√
6c ·
	 logn
n
pmin 2

i<j |V (LCAT  (i, j))|
2






.
Now, assume that the following claim holds.
Claim 1. Let κ(n) denote the cost of a unit weight clique. Then:
(1) E[cost(T 
;G) ] ≥ κ(n) · pmin
(2)
i<j |V (LCAT  (i, j))|
2 ≤ n · κ(n).
Using Claim 1 and the fact that κ(n) ≥ n3/6:
exp






−
2


E[ cost(T 
;G) ] ·
√
6c ·
	 logn
n
pmin 2

i<j |V (LCAT  (i, j))|
2






≤ exp 
−2 · κ(n) · 6c · logn
n2

≤ exp(−2c · n logn).
Journal of the ACM, Vol. 66, No. 4, Article 26. Publication date: June 2019.             
Hierarchical Clustering: Objective Functions and Algorithms 26:23
Note that the assumption on pmin is necessary for our proof technique, which relies on taking
Union bound over all possible generating trees. It remains an interesting open question whether
this assumption is necessary. Furthermore, this proof is the “bottleneck” for the choice of pmin.
We now turn to the proof of Claim 1.
Proof of Claim 1. Let G¯ denote the expected graph. E[ cost(T 
;G) ] is equal to cost(T 
;G¯),
which is a linear function of the edge weights of G¯. However, the clique has the same cost for
every tree, so consider its cost for tree T 
. The minimum edge weights is pmin for G¯, the edge
weight is 1 for every edge of the clique; hence, the first statement.
For the second statement we write

i<j
|V (LCAT  (i, j))|
2 ≤ n

i<j
|V (LCAT  (i, j))| = nκ(n).
5.2 Algorithm Linkage++, a (1 + ε)-Approximation Algorithm in the HSBM
We start with some useful notation for a k-HSBM: n1,n2,...,nk , the sizes of the k bottom-level
clusters, p1,p2,...,pk , the value of W inside each cluster pmax = maxi pi , and nmin, the size of the
smallest of the k bottom-level clusters. We extend σ to be the correspondence between clusters
and the roots of the k subtrees.
For a graphG, we denote by u the adjacency vector of vertex u, i.e., the column of the adjacency
matrix corresponding to the vertexu. Given a graph generated from a k-HSBM (or from the planted
partition model) with k bottom-level clusters, we say that is satisfies the separation condition if
there exists a universal constant c such that for each pair of vertices u,v belonging to different
bottom-level clusters (or simply clusters in the planted partition), they satisfy
E[ u ] − E[ v ]2
2 ≥ c · k · ((pmax + ω(log6 n/n)) · n/nmin + log(n)), (11)
where E[ u ] is the entrywise expectation. We establish the following theorem.
Theorem 5.6. Let G be a graph generated from a k-HSBM (see Definition 5.2). If G satisfies the
separation condition (11), pmin = ω(

logn/n), and nmin ≥ n1/4 · log1/4 n, then, with high probability,
Algorithm 5 outputs a tree T that satisfies cost(T ;G) = (1 + o(1))OPT.
We consider a simple algorithm, called linkage++, which works in two phases (see Algorithm 5)
(1) Perform a singular value decomposition on the adjacency matrix to embed nodes in Euclidean space. Then apply single-linkage using the Euclidean distance until we obtain k
clusters.
(2) Consider these as bottom-level clusters and apply single-linkage using the edge density
between these clusters in the input graph G to finish building the hierarchical clustering
tree.
The idea of using spectral techniques for graph partitioning goes back at least to Bopanna [13].
We use a result of McSherry [37] that considers the planted partition model.
Theorem 5.7 ([37], Observation 11 and a simplification of Theorem 12). LetG be a random
graph generated from the planted partition model satisfying the separation condition and where each
of the k clusters contains at least nmin vertices.
Then, the algorithm of Reference [37, Theorem 12] with inputs G, k maps V to points
{ζ (1),..., ζ (n)} in a k-dimensional subspace of Rn such that the following holds: with probability at least 1 − 1/n3 over the random graph G and with probability 1/k over the random bits of the
algorithm, there exists η > 0 such that for any two vertices u and v:
Journal of the ACM, Vol. 66, No. 4, Article 26. Publication date: June 2019.  
26:24 V. Cohen-Addad et al.
ALGORITHM 5: Linkage++
1: Input: Unweighted graph G = (V, E) on n vertices.
2: Parameter: An integer k.
3: Output: A Hierarchical Clustering Tree of G.
4: Let ζ (1),..., ζ (n) denote the n points corresponding to vertices of V , spanning a kdimensional subspace of Rn, that are obtained by applying Theorem 5.7 with parameters G,
k.
5: Run the single-linkage algorithm on points {ζ (1),..., ζ (n)}, using Euclidean distance, until
there are exactly k clusters Cζ
1 ,...,Cζ
k .
6: Define similarity between clusters by sim(Cζ
i ,Cζ
j ) = w(Cζ
i ,Cζ
j )/(|Cζ
i ||Cζ
j |), where w(Cζ
i ,Cζ
j )
is the number of edges of G between the corresponding vertices.
7: while there is more than one cluster do
8: Let A, B denote the two clusters maximizing sim(A, B).
9: Define cluster D = A ∪ B and let sim(D, E) = max(sim(A, E),sim(B, E)) for all clusters E.
10: Add D to and remove A and B from the set of clusters.
11: The sequence of merges in the while-loop (Steps 7 to 10) induces a hierarchical clustering tree T 
k with k leaves {Cζ
1 ,...,Cζ
k }. For an internal node N of T 
k created by merging
A, B ⊆ {Cζ
1 ,...,Cζ
k }, define the weight, W 
(N) = sim(A, B) = maxCζ
i ∈A,Cζ
j ∈B sim(Cζ
i ,Cζ
j ). Replace each leaf of T 
k by an arbitrary binary tree on |Cζ
k | leaves labeled according to the corresponding vertices of V to obtain a hierarchical tree T .
12: Repeat the algorithm 2k logn times and output the tree T that minimizes cost(T ;G).
(1) if u and v are in the same cluster then ζ (u) − ζ (v)2
2 ≤ η and
(2) if u and v are in different clusters then ζ (u) − ζ (v)2
2 > 2η.
Our analysis relies on analyzing the probability of occurrence of two events E1 and E2.
Event E1. Fix one of the 2k logn iterations of Lines 4–11 of Algorithm 5. We define event E1 as the
event that after Line 4 of the algorithm, the resulting vectors satisfy the conclusion of Theorem 5.7,
i.e.,
(1) if u and v are in the same cluster then ζ (u) − ζ (v)2
2 ≤ η and
(2) if u and v are in different clusters then ζ (u) − ζ (v)2
2 > 2η.
We let ψ (u) ∈ [k] denote the cluster containing vertex u. From the above theorem, we deduce
the following lemma.
Lemma 5.8. LetG be generated by a k-HSBM. Assume event E1 holds. Then, the k clusters obtained
after Step 5 correspond exactly to the k hidden bottom-level clusters.
Proof. By event E1, any linkage algorithm, e.g., single-linkage, performing merges starting
from the set {ζ (1),..., ζ (n)} until there are k clusters will merge clusters at a distance of at most
η and hence, the clusters obtained after Step 5 correspond to the assignment ψ.
Let C∗
1,...,C∗
k be the hidden bottom-level clusters, i.e., C∗
i = {v|ψ (v) = i}. Let π denote the permutation π : [k] → [k] such that Cζ
j corresponds to C∗
π (j)
. Let V (C∗
i ) denote the set of vertices in
the bottom-level clusters.
Journal of the ACM, Vol. 66, No. 4, Article 26. Publication date: June 2019. 
Hierarchical Clustering: Objective Functions and Algorithms 26:25
Event E2. We define E2 as the event that the graph G satisfies that for any pair of
bottom-level clusters Ci,Cj , |w(Ci,Cj) − E(Ci,Cj ) | < ε2E(Ci,Cj ), where E(Ci,Cj ) = |V (Ci )|·|V (Cj)| ·
W (LCAT
(Ci,Cj)).
γ -approximate Ground-Truth Tree. We say that a treeT = (N, E) is a γ -approximate ground-truth
tree for G and T
if there exists a weight functionW  : N → R+ such that for any two vertices u,v,
we have that
(1) γ −1
W 
(LCAT (u,v)) ≤ W (LCAT
(u,v)) ≤ γW 
(LCAT (u,v)) and
(2) for any node N of T and any node N  descendant of N in T , W 
(N) ≤ W 
(N 
).
Lemma 5.9. Let G be generated according to a k-HSBM. Assume that event E2 occurs and that the
clusters obtained after Step 5 correspond to the assignment ψ. Then the output tree T of Algorithm 5
is a (1 + ε/3)-approximate ground-truth tree.
Proof. The hypothesis of lemma assumes that the bottom level clusters are identified correctly
as per the assignment ψ. Let C∗
1,...,C∗
k denote these bottom-level clusters. We focus on the tree
T 
k (cf. Algorithm 5), which has leaves given by bottom level clusters. Let T

k be the generating tree
which is truncated to have exactly k leaves also given by the k bottom level clusters.
Since E2 occurs, for as sufficiently small ε, we have that for every pair i, j ∈ [k]:
W (LCAT
 (C∗
i ,C∗
j ))
1 + ε/3
≤ sim(C∗
i ,C∗
j ) ≤ (1 + ε/3)W (LCAT
 (C∗
i ,C∗
j )). (12)
Let N denote the set of all nodes of T (including internal nodes). For any internal node N ∈ N ,
define Λ(N) to denote the set of leaves in the subtree rooted at N. Recall that W  is defined as
follows (cf. Algorithm 5)—for every internal node N ∈ N , if NL and NR denote its left and right
children, then:
W 
(N) = max C∗
i ∈Λ(NL ),C∗
j ∈Λ(NR )
sim(C∗
i ,C∗
j ).
Our goal is to show that for every i, j ∈ [k]
W (LCAT
 (C∗
i ,C∗
j ))/(1 + ε/3) ≤ W 
(LCAT  (C∗
i ,C∗
j )) ≤ (1 + ε/3)W (LCAT
 (C∗
i ,C∗
j )). (13)
First, we prove the LHS of Inequality (13),
W 
(LCAT  (C∗
i ,C∗
j )) ≥ sim(C∗
i ,C∗
j ) ≥ W (LCAT
 (C∗
i ,C∗
j ))/(1 + ε/3).
Thus it only remains to prove the RHS of Inequality (13). Consider an arbitrary pair of bottomlevel clusters C∗
i ,C∗
j , and let N = LCAT 
k (C∗
i ,C∗
j ) be the least common ancestor of C∗
i and C∗
j in T 
k .
Let NL and NR be the left and right child of N in T 
k . Similarly, let N = LCAT
 (C∗
i ,C∗
j ) be the least
common ancestor of C∗
i and C∗
j in T

k , and let NL and NR be the left and right child of N in T

k . We
consider the the following two cases:
Case 1: Λ(NL ) ⊆ Λ(NL ) and Λ(NR ) ⊆ Λ(NR ). Observe that in this case the clusters being merged
are A = Λ(NL ) and B = Λ(NR ) and let C∗
l ∈ Λ(NL ) and C∗
k ∈ Λ(NR ) be such that sim(A, B) =
sim(C∗
l ,C∗
k ). Without loss of generality, we have that C∗
i ∈ Λ(NL ) and C∗
j ∈ Λ(NR ) and as a result,
we get
W 
(N) = sim(A, B) = sim(C∗
l ,C∗
k ) ≤ (1 + ε/3)W (N),
where the last inequality follows from the fact that LCAT
 (C∗
l ,C∗
k ) = N and as E2 holds.
Journal of the ACM, Vol. 66, No. 4, Article 26. Publication date: June 2019.                     
26:26 V. Cohen-Addad et al.
Case 2: ∃C∗
l ∈ Λ(NL ),C∗
l  Λ(NL ) or ∃C∗
k ∈ Λ(NR ),C∗
k  Λ(NR ). Without loss of generality, letC∗
i ∈
Λ(NL ) and suppose that there exists C∗
l ∈ Λ(NL ),C∗
l  Λ(NL ). Then there must exist a descendant
of NL (possibly including NL itself), which for the first time merges sets A and B satisfying: ∃C∗
i ∈
A,C∗
i ∈ Λ(NL ) and ∃C∗
l ∈ B,C∗
l  Λ(NL ), such that sim(A, B) = sim(C∗
i,C∗
l ). Then note that by
definition, LCAT
 (C∗
i,C∗
l ) must be an ancestor of N (possibly including N itself).
Thus, we have
W 
(N) ≤ sim(C∗
i,C∗
l ) ≤ (1 + ε/3)W (LCAT

k
(C∗
i,C∗
l )) ≤ (1 + ε/3)W (N).
The proof follows by observing that T is obtained from T 
k by simply replacing the leaves of T 
k by
arbitrary binary subtrees for each of the bottom-level clusters.
The following lemma allows us to bound the cost of an approximate ground-truth tree.
Lemma 5.10. LetG be a graph generated according to a k-HSBM. LetT be aγ -approximate groundtruth tree. Then, cost(T ;G) ≤ γ (1 + o(1))OPT.
Proof. Let T
 be a ground-truth tree for G. Let G¯ be the expected graph associated to T
 and G.
By Theorem 3.2 the cost of T
 is optimal for G¯. Furthermore, by Theorem 5.5, we have that the
cost of T
 on G¯ and the cost of T
 are within a factor of (1 + o(1)). We thus need to show that
cost(T ;G) ≤ γ (1 + o(1))cost(T
;G).
Recall that T is the γ -approximate ground-truth tree and we define a new graph G¯T which has
the same set of vertices V as G¯: For each pair of vertices u,v ∈ V we create an edge in G¯T with
weight wG¯T (u,v) = W 
(LCAT (u,v)). By definition of W 
, it follows that T is generating for G¯T ,
and so applying Theorem 3.2, we obtain that the cost of T for G¯
T , denoted by cost(T ;G¯
T ), is less
than the cost of T
 for G¯
T , cost(T
;G¯
T ).
Now recall that the cost of a given tree T , for a given graph G can be rewritten as follows:
cost(T ;G
) =
u,v wG (u,v) · |V (LCAT (u,v))|, where wG (u,v) is the weight of the edge u,v in
G
.
Thus, since by definition of T , we have γ −1
W 
(LCAT (a,b)) ≤ W (LCAT
(a,b)) ≤ γW 
(LCAT (a,b)).
Hence,
cost(T ;G¯) =

u,v
wG¯ (u,v) · |V (LCAT (u,v))| ≤ 
u,v
γ · wG¯T (u,v) · |V (LCAT (u,v))|
= γ · cost(T ;G¯
T ) ≤ γ · cost(T
;G¯
T ).
Similarly, one can who that cost(T
;G¯
T ) ≤ γ · cost(T
;G¯). Combining the two yields the lemma.
Proof of Theorem 5.6. We first analyze the probability of event E1. By Theorem 5.7, for one
execution of Line 4 of Algorithm 5, E1 holds with probability at least 1 − 1/n3 over the random
graph G and probability at least 1/k over the random bits of the algorithm. Since Line 12 of Algorithm 5 repeats the execution 10 · k · logn times with independent random bits, the probability
that event E1 holds for at least one of the 10 · k · logn executions is at least 1 − e−10 log n − 1/n2.
We next analyze the probability of event E2. Fix two bottom-level clusters. Thanks to our assumptions on pmin and nmin, the expected number of edges of the cut between those two clusters is
at least n2
minpmin = ω(logn). Applying Chernoff bounds, the value of the cut is concentrated around
its expectation with a probability of at least 1 − 2−4 log n. Taking a union bound over all (
k
2 ) pairs of
bottom-level clusters, using that k ≤ n, we get that the probability for E2 to hold for some choice
of ε = o(1) is at least 1 − n22−4 log n ≥ 1 − 1/n2.
Journal of the ACM, Vol. 66, No. 4, Article 26. Publication date: June 2019.                             
Hierarchical Clustering: Objective Functions and Algorithms 26:27
Consider an execution during which events E1 and E2 both occur. Combining Lemmas 5.8, 5.9,
and 5.10 imply the conclusion of Theorem 5.6.
We note that k might not be known in advance. However, different values of k can be tested
and an O(1)-estimate on k is enough for the proofs to hold. Thus, it is possible to run Algorithm 5
O(logn) times with different “guesses” for k and take the minimum-cost tree of these runs.
6 SEMI-RANDOM MODELS
The notation for random graphs used in this section is described in detail in Section 5.
6.1 Algorithm for Semi-Random Model using SDP Relaxations
We show that in random and semi-random (defined below) graph models generated according to
an HSM, an SDP-based algorithm can be used to guarantee an O(1)-approximation with highprobability in a regime beyond that proved in Theorem 5.6. The proof of the following result
follows using the technique of Makarychev et al. [36] to obtain O(1)-approximations to problems
such as sparsest cut and small-set expansion (SSE) in random and semi-random settings combined
with Theorem 4.1 that shows that approximations to (roughly) balanced min-cut problems can
be used to obtain an equivalent approximation ratio for the problem of finding a minimum cost
hierarchical cluster tree.
To generate a random graph, G = (V, E), we use an HSM (Definition 5.1). The semi-random
model simply considers a random graph generated as above and an adversary is allowed to remove edges from G, but not add any. Note that in either case the comparison is to the cost of the
generating tree on the graph G¯ (cf. Definition 5.1).
6.2 Algorithm in Semi-Random Model using SDP Relaxations
This section is dedicated to the proof of the following theorem.
Theorem 6.1. LetG be a graph generated from the HSM (Definition 5.1) withpmin = Ω(logn/n2/3).
Then, there exists a randomized polynomial time algorithm that with probability 1 − o(1) outputs a
tree T such that
cost(T ;G) = O(OPT(G¯)), (14)
where OPT(G¯) denotes the value of the optimal tree for G¯ and we note that OPT(G¯) = cost(T
;G¯),
where T
is the generating tree. Furthermore, the above holds even in the semi-random case, i.e., when
an adversary is allowed to remove any subset of the edges from G.
Similar to Section 5, the assumption on pmin is crucial to our proof techniques; it remains an
interesting open question whether the assumption is necessary. It is worth mentioning, that the
assumption on pmin in Theorem 6.1 is much weaker than the one in Section 5, where we prove a
(1 + o(1))-approximation.
6.2.1 Background. In this section, we recall the work of Makarychev et al. [36]. Essentially all of
this section is directly cited from this work and we only provide it in this article for completeness.
While we do not require to go into details, we define the crude SDP for Small-Set Expansion
(SSE) used by Makarychev et al. [36] below. u¯ denotes some vector representation corresponding
to vertex u in the SDP. The reader may refer to SDP solutions φ occurring in the rest of this
section to mean feasible solutions to the following SDP. Note that solving the SSE problem gives
Journal of the ACM, Vol. 66, No. 4, Article 26. Publication date: June 2019.   
26:28 V. Cohen-Addad et al.
a (roughly) balanced sparse cut when ρ = Θ(1),
min
1
2

(u,v)∈E(G)
u¯ −v¯2
subject to
for all u ∈ V,

v
u¯,v¯ ≤ ρ|V | (Spreading Constraints)
for all u,v,w ∈ V, u¯ −v¯2 + v¯ − w¯ 2 ≥ u¯ −v¯2 (2
2-triangle inequalities)
for all u,v ∈ V, u¯,v¯ ≥ 0
for all u ∈ V, u¯2 = 1.
Definition 6.2 (Heavy Set, Hδ,φ (M) [36]). Let V be a set of n vertices and M ⊆ N. Consider an SDP
solution φ : V → H. We say that a vertex u ∈ M is δ-heavy in M if the 2
2-ball of radius δ around
φ(u) contains at least δ 2
n vectors from φ(M), i.e., |{v ∈ M|φ(v) ∈ Ball(φ(u), δ )}| ≥ δ 2
n. We denote
the set of all vertices that are δ-heavy in M by Hδ,φ (M).
Definition 6.3 (Geometric Expansion [36]). A graph G = (V, E) satisfies the geometric expansion
property with cut value X at scale δ if for every SDP solution φ : V → H satisfying Hδ,φ (V ) = ∅
(recall that Hδ,φ (V ) is the set of δ-heavy vertices in V ):



{(u,v) ∈ E| 
φ(u) − φ(v)

2 ≤ δ/2}


 ≤ 2δ 2
X.
A graph G = (V, E) satisfies the geometric expansion property with cut value X up to scale 2−T
for T ∈ N if it satisfies the geometric expansion property for every δ ∈ {2−t |1 ≤ t ≤ T }.
Theorem 6.4 (Theorem 3.4 from Reference [36]). Let G = (V, E) be a graph that satisfies
the geometric expansion property with cut value X at scale up to c

log |V |. Then, there exists a
randomized polynomial time that with high probability outputs a partition L, R of V such that
|cut(L, R)| = O(X) and |L|, |R|≥|V |/3.
6.2.2 Geometric Expansion of HSM. Let G¯n = (V¯
n, E¯
n,w) be a graph generated according to an
ultrametric, where for each e ∈ E¯
n, w(e) ∈ (0, 1). In this case, we allow w(e) to depend on n—in
particular it is possible that w(e) → 0 as n → ∞. Let G = (V, E) be an unweighted random graph
with |V | = |V¯
n | = n generated from G¯ as follows. An edge (u,v) is added to G with probability
w((u,v)), for the corresponding vertices u,v ∈ V¯
n. Note that this is simply a hierarchical stochastic
models (Definition 5.1).
For the rest of this discussion we assume that V = V¯
n as a natural bijection exists between the
two vertex sets. Let T
 be a generating tree for G¯. Let U ⊆ V and let T
|U be the restriction of T
 to
leaves in U (removing unnecessary leaves and reducing internal nodes as necessary). Let N (U ) be
the root of T
|U . Consider the following procedure where the nodes appear as leaves in the left and
right subtrees of the root of T
|U . Suppose we follow the convention that the left subtree is never
any smaller than the right subtree inT
|U . We say that the canonical node ofT
|U is the first left node
NL encountered in a top-down traversal starting from N (U ) such that 2|U |/3 ≥ V (NL ) ≥ |N |/3.
We define UL = V (NL ), and UR = U \ UL. We say that (UL,UR ) is the canonical cut of U . It is easy
to see that such a cut always exists since the tree is binary and left subtrees are never smaller than
right subtrees. Let Ernd = {(u,v) | u ∈ UL,v ∈ UR }.
Lemma 6.5. For a random graph G generated as described in Theorem 6.1, with probability at
least 1 − o(1), for every subset U of size at least n2/3

logn, the subgraph (U, Ernd ) is geometrically
Journal of the ACM, Vol. 66, No. 4, Article 26. Publication date: June 2019.           
Hierarchical Clustering: Objective Functions and Algorithms 26:29
expanding with cut cost
X = C · max{w(L, R), |U | · D · log2 D, |U | · D · logn} (15)
up to scale 1/
√
D. Furthermore, the result also applies in the semi-random setting where an adversary
may remove any subset of edges from the random graph G.
Before we give the proof of Lemma 6.5, we show how it implies Theorem 6.1.
Proof of Theorem 6.1. We apply Theorem 4.1, where it is essentially established that provided
one obtains a ϕ approximation to the 1/3-balanced min-cut problem (i.e., minimize cut subject to
the constraint that both sides have at least 1/3 of the vertices being cut), the recursive algorithm
gives a O(ϕ) approximation for minimizing Dasgupta’s cost function.
We observe that cost(T
;G¯) = Ω(n3 · pmin) = Ω(n7/3 logn). Thus, we notice that once we obtain
setsU of size n0 = n2/3

logn, since there are at most n/n0 of them, even if we use an arbitrary tree
on any suchU , together this can only addO( n
n0 · n3
0) = O(n7/3 · logn). Thus, we only need to be able
to obtain suitable approximations during the recursive procedure as long as |U | ≥ n2/3 logn. This
is precisely given by using Lemma 6.5. Observe that in Equation (15), w(L, R) = Ω(|U |
2 · pmin) =
Ω(n2/3 log2 n), |U |D log2 D = o(|U |D logn), and D|U | logn = O(n2/3 log2 n). Thus, the algorithm of
Reference [36] given by Theorem 6.4 returns a cut that is a constant factor approximation to the
1/3-balanced min-cut problem on the induced subgraph ofG¯ on the vertex setU . This observation
together with a slight modification of the charging argument in the proof of Theorem 4.1 to account
for the case where subgraphs have size less than n2/3 logn finishes the proof.
In the following, we give the deferred proof of Lemma 6.5. The proof is essentially identical to
that of Theorem 5.1 in Reference [36]. However, as there are some minor modifications, we are
unable to cite their result directly and hence provide the entire proof here for completeness.
Proof of Lemma 6.5. Fix some subset U and let UL,UR be the canonical cut of U given by the
generating tree T
 of G¯. Let Eall = {(u,v)|u ∈ UL,v ∈ UR } and let Ernd = Eall ∩ E, where E is the set
of edges in the realized random graph G = (V, E). As the adversary in the semi-random graph
can only remove edges it suffices to show that (U, Ernd) is geometrically expanding with high
probability. We fix the parameter δ = 2−t (where 1 ≤ t ≤ T ) and prove that the graph (U, Ernd) is
geometrically expanding with cut value X at scale δ. The probability that this fails to happen will
be low enough for us to take a simple union bound over all the possible values of δ.
The condition Hδ,φ (U ) = ∅ implies that
|{(u,v) ∈ U × U | φ(u) − φ(v)2
δ }| ≤ δ 2
n2
.
We need to bound the probability of the bad event, the existence of an SDP solution φ : U → H
such that
|{(u,v) ∈ U × U | φ(u) − φ(v)2 ≤ δ }| ≤ δ 2
n2
, (16)






(u,v) ∈ Ernd | φ(u) − φ(v)2 ≤
δ
2





≥ 2δ 2
X. (17)
Makarychev et al. [36] show that if φ satisfying Equations (16) and (17) exists, then provided
|Ernd | ≤ 2X, there exists φ : U → Nδ satisfying:





(u,v) ∈ U × U | φ
(u) − φ
(v)2 ≤
3
4
δ




≤
5
4
δ 2
n2
, (18)





(u,v) ∈ Ernd | φ
(u) − φ
(v)2 ≤
3
4
δ




≥
3
4
δ 2
X, (19)
where Nδ ⊂ H is a set of size exp(O(log2 δ−1)).
Journal of the ACM, Vol. 66, No. 4, Article 26. Publication date: June 2019.    
26:30 V. Cohen-Addad et al.
The remainder of the proof is showing that the existence of φ is a very low-probability event.
First, as |Ernd | = w(UL,UR ) ≤ X, P[|Ernd | ≥ 2X] ≤ e−c0X . Note that if we fix a φ : U → Nδ , the
probability (over the random choice of Ernd) that Equations (18) and (19) hold is at most e−c1δ 2X ,
by using the Chernoff bound. Finally, we note that there are at most |Nδ |
|U | such φ
, thus we can
safely take a union bound provided X/D ≥ c3 · |U | log2 D. Finally, there are n|U | subsets of size |U |
and again we can safely apply a union bound provided X/d ≥ c4 |U | logn. The choice of X ensures
that this happens.
APPENDICES
A PERFECT GROUND-TRUTH INPUTS AND BEYOND
In this section, we focus on ground-truth inputs. We state and (re)prove results that when the
input is a perfect ground-truth input, commonly used algorithms (single-linkage, average-linkage,
and complete-linkage; as well as some divisive algorithms—the bisection 2-Center and sparsest-cut
algorithms) yield a tree of optimal cost, hence (by Definition 3.1) a ground-truth tree. Several of
these results are folklore (and straightforward when there are no ties), but we have been unable to
pin down a reference, so we include them for completeness (Section A.1). We also introduce a faster
optimal algorithm for “strict” ground-truth inputs (Section A.2). The proofs present no difficulty.
The meat of this section is Section A.3, where we go beyond ground-truth inputs; we introduce
δ-adversarially perturbed ground-truth inputs and design a simple, more robust algorithm that,
for any admissible objective function, yields a δ-approximation.
ALGORITHM 6: Linkage Algorithm for Hierarchical Clustering (similarity setting)
1: Input: A graph G = (V, E) with edge weights w : E → R+
2: Create n singleton trees. Root labels: C = {{v1},..., {vn }}
3: Define sim : C×C → R+: sim(C1,C2) =
⎧⎪⎪⎪⎪⎪
⎨
⎪⎪⎪⎪⎪
⎩
1
|C1 | |C2 |

x ∈C1,y ∈C2
w((x,y)) Average-Linkage
maxx ∈C1,y ∈C2 w((x,y)) Single-Linkage
minx ∈C1,y ∈C2 w((x,y)) Complete-Linkage
.
4: while there are at least two trees do
5: Take the two trees with root labels C1,C2 such that sim(C1,C2) is maximum
6: Create a new tree by making those two tree children of a new root node labeled C1 ∪C2
7: Remove C1,C2 from C, add C1 ∪C2 to C, and update sim
8: return the resulting binary tree T
A.1 Perfect Ground-Truth Inputs Are Easy
In the following, we refer to the tie breaking rule of Algorithm 6 as the rule followed by the algorithm for deciding which ofCi,Cj orCk ,C to merge, when maxC1,C2 ∈C sim(C1,C2) = sim(Ci,Cj) =
sim(Ck ,C ).
Theorem A.1. 12 Assume that the input is a (dissimilarity or similarity) ground-truth input. Then,
for any admissible objective function, the agglomerative heuristics average-linkage, single-linkage,
and complete-linkage (see Algorithm 6) return an optimal solution. This holds regardless of the tie
breaking rule used by Algorithm 6.
12This theorem may be folklore, at least when there are no ties, but we have been unable to find a reference.
Journal of the ACM, Vol. 66, No. 4, Article 26. Publication date: June 2019.    
Hierarchical Clustering: Objective Functions and Algorithms 26:31
Proof. We focus on the similarity setting; the proof for the dissimilarity setting is almost identical. We define the candidate trees aftert iterations of the while loop to be sets of trees in C at that
time. The theorem follows from the following statement, which we will prove by induction on t: If
Ct = {C1,...,Ck } denotes the set of clusters after t iterations, then there exists a generating tree
Tt for G, such that the candidate trees are subtrees of Tt .
For the base case, note that initially each candidate tree contains exactly one vertex and the
statement holds. For the general case, let C1,C2 be the two trees that constitute the tth iteration.
By induction, there exists a generating tree Tt−1 for G, and associated weights W t−1 (according
to Definition 2.2) such that C1 and C2 are subtrees of Tt−1, rooted at nodes N1 and N2 of Tt−1
respectively.
To define Tt , we start from Tt−1. Consider the path P = {N1, N1, N2,..., Nk , N2} joining N1 to
N2 in Tt−1 and let Nr = LCAT t−1 (N1, N2). If Nr is the parent of N1 and N2, then Tt = Tt−1, else do
the following transformation: remove the subtrees rooted at N1 and at N2; create a new node N∗
as the second child of Nk , and let N1 and N2 be its children. This defines Tt . To define W t , extend
W t−1 by setting W t (N∗) = W (Nr ).
Claim 2. For any Ni, Nj ∈ P, W t−1 (Ni ) = W t−1 (Nj).
Thanks to the inductive hypothesis, with Claim 2 it is easy to verify that W t certifies that Tt is
generating for G.
Proof of Claim 2. Fix a node Ni on the path from Nr to N1 (the argument for nodes on the
path from Nr to N2 is similar). By inductionW t−1 (Ni ) ≥ W t−1 (Nr ). We show that since the linkage
algorithms merge the trees C1 and C2, we also have W t−1 (Ni ) ≤ W t−1 (Nr ) and so W t−1 (Ni ) =
W t−1 (Nr ), hence the claim. Let w0 = W t−1 (Nr ).
By induction, for all u ∈ C1, v ∈ C2, w(u,v) = w0, and thus sim(C1,C2) = w0 in the execution of
all the algorithms. Fix a candidate tree C ∈ Ct , C  C1,C2 and C ⊆ V (Ni ). Since C is a partition
of the vertices of the graph and since candidate trees are subtrees of Tt−1, such a cluster exists.
Thus, for u ∈ C1, v ∈ C
w(u,v) = W t−1 (LCAT t−1 (u,v)) = W t−1 (Ni ) ≥ w0, since Ni is a descendant
of Nr .
It is easy to check that by their definitions, for any of the linkage algorithms, we thus have
that sim(C1,C
) ≥ w0 = sim(C1,C2). But since the algorithms merge the clusters with maximum pairwise similarity, it follows that sim(C1,C
) ≤ sim(C1,C2) = w0 and therefore,W t−1 (Ni ) ≤
W t−1 (Nr ) and so,W t−1 (Ni ) = W t−1 (Nr ) and the claim follows. This is true no matter the tie breaking chosen for the linkage algorithms.
Divisive Heuristics. We now focus on two well-known divisive heuristics: (1) the bisection 2-
Center which uses a partition-based clustering objective (the k-Center objective) to divide the
input into two (non necessarily equal-size) parts (see Algorithm 7), and (2) the recursive sparsestcut algorithm, which can be implemented efficiently for ground-truth inputs (Lemma A.4).
ALGORITHM 7: Bisection 2-Center (similarity setting)
1: Input: A graph G = (V, E) and a weight function w : E → R+
2: Find {u,v} ⊆ V that maximizes minx maxy ∈ {u,v } w(x,y)
3: A ← {x | w(x,u) ≥ maxy ∈ {u,v } w(x,y)}
4: B ← V \ A.
5: Apply Bisection 2-Center on G[A] and G[B] to obtain trees TA,TB respectively
6: return The union tree of TA,TB.
Journal of the ACM, Vol. 66, No. 4, Article 26. Publication date: June 2019.   
26:32 V. Cohen-Addad et al.
Loosely speaking, we show that this algorithm computes an optimal solution if the optimal solution is unique. More precisely, for any similarity graph G, we say that a tree T is strictly generating
for G if there exists a weight functionW such that for any nodes N1, N2, if N1 appears on the path
from N2 to the root, then W (N1) < W (N2) and for every x,y ∈ V , w(x,y) = W (LCAT (x,y)). In
this case we say that the input is a strict ground-truth input. In the context of dissimilarity, an
analogous notion can be defined and we obtain a similar result.
Theorem A.2. 13 For any admissible objective function, the bisection 2-Center algorithm returns
an optimal solution for any similarity or dissimilarity graph G that is a strict ground-truth input.
Proof. We proceed by induction on the number of nodes in the graph. Consider a strictly generating tree T and the corresponding weight function W . Consider the root node Nr of T and
let N1, N2 be the children of the root. Let (α, β) be the cut induced by the root node of T (i.e.,
α = V (N1), β = V (N2)). Define w0 to be the weight of an edge between u ∈ α and v ∈ β for any
u,v (recall that sinceT is strictly generating all the edges between α and β are of same weight). We
show that the bisection 2-Centers algorithm divides the graph into α and β. Applying the inductive
hypothesis on both subgraphs yields the result.
Suppose that the algorithm locates the two centers in β. Then, minx ∈α maxy ∈ {u,v } w(x,y) = w0,
since the vertices of α are connected by an edge of weight w0 to the centers. Thus, the value of
the clustering is w0. Now, consider a clustering consisting of a center c0 in α and a center c1 in β.
Then, for each vertex u, we have maxc ∈ {c0,c1 } w(u,c) ≥ min(W (N1),W (N2)) > W (Nr ) = w0, since
T and W are strictly generating; hence, a strictly better clustering value. Therefore, the algorithm
locates x ∈ α and y ∈ β. Finally, it is easy to see that the partitioning induced by the centers yields
parts A = α and B = β.
Remark 4. To extend our result to (non-strict) ground-truth inputs, one could consider the
following variant of the algorithm (which bears similarities with the popular elbow method for
partition-based clustering): Compute a k-Center clustering for all k ∈ {1,...,n} and partition the
graph according to the k-Center clustering of the smallest k > 1 for which the value of the clustering increases. Mimicking the proof of Theorem A.2, one can show that the tree output by the
algorithm is generating.
We now turn to the recursive sparsest-cut algorithm (i.e., the recursive ϕ-sparsest-cut algorithm
of Section 4.1, for ϕ = 1). The recursive sparsest-cut consists of recursively partitioning the graph
according to a sparsest cut of the graph. We show (1) that this algorithm yields a tree of optimal
cost and (2) that computing a sparsest cut of a similarity graph generated from an ultrametric
can be done in linear time. Finally, we observe that the analogous algorithm for the dissimilarity
setting consists of recursively partitioning the graph according to the densest cut of the graph and
achieves similar guarantees (and similarly the densest cut of a dissimilarity graph generated from
an ultrametric can be computed in linear time).
Theorem A.3. 14 For any admissible objective function, the recursive sparsest-cut (respectively
densest-cut) algorithm computes a tree of optimal cost if the input is a similarity (respectively dissimilarity) ground-truth input.
Proof. The proof is by induction and presents no difficulty; it may be easier to recreate it than
to read it.
Let T be a generating tree and W be the associated weight function. Let Nr be the root of T ,
N1, N2 the children of Nr , and (α = V (N1), β = V (N2)) the induced root cut. Since T is generating,
13This theorem may be folklore, but we have been unable to find a reference.
14This Theorem may be folklore, at least when there are no ties, but we have been unable to find a reference.
Journal of the ACM, Vol. 66, No. 4, Article 26. Publication date: June 2019. 
Hierarchical Clustering: Objective Functions and Algorithms 26:33
all the edges between α and β are of same weight w, which is therefore also the sparsity of (α, β).
For every edge (u,v) of the graph,w(u,v) = W (LCAT (u,v)) ≥ w, so every cut has sparsity at least
w, so (α, β) has minimum sparsity.
Now consider the tree T ∗ computed by the algorithm, and let (γ , δ ) denote the sparsest cut used
by the algorithm at the root (in case of ties it might not be different from (α, β)). By induction, the
algorithm onG[γ ] andG[δ] gives two generating treesTγ andTδ with associated weight functions
Wγ andWδ . To argue that T ∗ is generating, we defineW ∗ as follows, where N∗
r denotes the root of
T ∗,
W ∗ (N) =
⎧⎪⎪
⎨
⎪⎪
⎩
Wγ (N) if N ∈ Tγ
Wδ (N) if N ∈ Tδ
w if N = N∗
r
.
By induction w(u,v) = W (LCAT (u,v)) if either both u,v ∈ γ , or both u,v ∈ δ. For any u ∈ γ ,v ∈
δ, we have w(u,v) = w = W (N∗
r ) = W (LCAT (u,v)). Finally, since w ≤ w(u,v) for any u,v, we
have W (N∗
r ) = w ≤ W (N), for any N ∈ T ∗, and therefore T ∗ is generating.
We then show how to compute a sparsest cut of a graph that is a ground-truth input.
Lemma A.4. If the input graph is a ground-truth input, then the sparsest cut is computed in O(n)
time by the following algorithm: Pick an arbitrary vertex u, let wmin be the minimum weight of edges
adjacent to u, and partition V into A = {x | w(u, x) > wmin} and B = V \ A.
Proof. Let wmin = w(u,v). We show that w(A, B)/(|A||B|) = wmin and, since wmin is the minimum edge weight of the graph, that the cut (A, B) only contains edges of weight wmin. Fix a
generating tree T . Consider the path from u to the root of T and let N0 be the first node on the
(bottom-up) path such that W (N0) = wmin. For any vertex x ∈ A, we have that w(u, x) > wmin.
Hence by definition, we have that N0 is an ancestor of LCAT (u, x). Therefore, for any other node
y such thatw(u,y) = wmin, we have LCAT (u,y) = LCAT (x,y) and so,w(x,y) = W (LCAT (x,y)) =
W (LCAT (u,y)) = wmin. It follows that all the edges in the cut (A, B) are of weight wmin and so, the
cut is a sparsest cut.
A.2 A Near-Linear Time Algorithm
In this section, we propose a simple, optimal, algorithm for computing a generating tree of a
ground-truth input. For any graphG, the running time of this algorithm isO(n2), andO(n) if there
exists a treeT that is strictly generating for the input. For completeness we recall that for any graph
G, we say that a tree T is strictly generating for G if there exists a weight function W such that
for any nodes N1, N2, if N1 appears on the path from N2 to the root, then W (N1) < W (N2) and for
every x,y ∈ V , w(x,y) = W (LCAT (x,y)). In this case we say that the input is a strict ground-truth
input.
The algorithm is described for the similarity setting but could be adapted to the dissimilarity
case to achieve the same performances.
Theorem A.5. For any admissible objective function, Algorithm 8 computes a tree of optimal cost
in time O(n log2 n) with high probability if the input is a strict ground-truth input or in time O(n2)
if the input is a (non-necessarily strict) ground-truth input.
Proof. We proceed by induction on the number of vertices in the graph. Let p be the first pivot
chosen by the algorithm and let B1,..., Bk be the sets defined by p at Step 4 of the algorithm, with
w(p,u) > w(v,p), for any u ∈ Bi,v ∈ Bi+1.
We show that for any u ∈ Bi,v ∈ Bj , j > i, we have w(u,v) = w(p,v). Consider a generating tree T and define N1 = LCAT (p,u) and N2 = LCAT (p,v). Since T,h, σ is generating and
Journal of the ACM, Vol. 66, No. 4, Article 26. Publication date: June 2019.   
26:34 V. Cohen-Addad et al.
ALGORITHM 8: Fast and Simple Algorithm for Hierarchical Clustering on Perfect Data (similarity setting)
1: Input: A graph G = (V, E) and a weight function w : E → R+
2: p ← random vertex of V
3: Let w1 > ··· > wk be the edge weights of the edges that have p as an endpoint
4: Let Bi = {v | w(p,v) = wi}, for 1 ≤ i ≤ k.
5: Apply the algorithm recursively on each G[Bi] and obtain a collection of trees T1,...,Tk
6: Define T ∗
0 as a tree with p as a single vertex
7: For any 1 ≤ i ≤ k, define T ∗
i to be the union of T ∗
i−1 and Ti
8: return T ∗
k
w(p,u) > w(p,v), we have that N2 is an ancestor of N1, by Definition 2.2. Therefore, LCAT (u,v) =
N2, and so w(u,v) = W (N2) = w(p,v). Therefore, combining the inductive hypothesis on any
G[Bi] and by Definition 2.2 the tree output by the algorithm is generating.
A bound of O(n2) for the running time follows directly from the definition of the algorithm.
We now argue that the running time is O(n log2 n) with high probability if the input is strictly
generated from a tree T . First, it is easy to see that a given recursive call on a subgraph with n0
vertices takes O(n0) time (excluding time required for further recursive calls). Now, observe that if
at each recursive call the pivot partitions the n0 vertices of its subgraph into buckets of size at most
2n0/3, then applying the master theorem implies a total running time of O(n logn). Unfortunately,
there are trees where picking an arbitrary vertex as a pivot yields a single bucket of size n − 1.
Thus, consider the node N of T that is the first node reached by the walk from the root that
always goes to the child tree with the higher number of leaves, stopping when the subtree of T
rooted at N contains fewer than 2n/3 but at least n/3 leaves. Since T is strictly generating we have
that the partition into B1,..., Bk induced by any vertex v ∈ V (N) is such that any Bi contains less
than 2n/3 vertices. Indeed, for any u such that LCAT (u,v) is an ancestor of N and x ∈ V (N), we
have that w(u,v) < w(x,v), and so u and x belong to different parts of the partition B1,..., Bk .
Since the number of vertices in V (N) is at least n/3, the probability of picking one of them is
at least 1/3. Therefore, since the pivots are chosen independently, after c logn recursive calls, the
probability of not picking a vertex of V (N) as a pivot is 1/nΩ(c ) for some large-enough constantc.
Taking the union bound yields the theorem.
A.3 Beyond Structured Inputs
Since real-world inputs are unlikely to correspond exactly to our definition of ground-truth inputs introduced in Section 2, we introduce the notion of δ-adversarially perturbed ground-truth
inputs. This notion aims at accounting for noise in the data. We then design a simple and arguably
more reliable algorithm (a robust variant of Algorithm 8) that achieves a δ-approximation for δadversarially perturbed ground-truth inputs in O(n(n + m)) time. An interesting property of this
algorithm is that its approximation guarantee is the same for any admissible objective function.
We first introduce the definition of δ-adversarially perturbed ground-truth inputs. For any real
δ ≥ 1, we say that a weighted graphG = (V, E,w) is a δ-adversarially perturbed ground-truth input
if there exists an ultrametric (X,d), such that V ⊆ X, and for every x,y ∈ V, x  y, e = {x,y} exists, and f (d(x,y)) ≤ w(e) ≤ δ f (d(x,y)), where f : R+ → R+ is a non-increasing function. This
defines δ-adversarially perturbed ground-truth inputs for similarity graphs and an analogous definition applies for dissimilarity graphs.
We now introduce a robust, simple version of Algorithm 8 that returns a δ-approximation if the
input is a δ-adversarially perturbed ground-truth inputs. Algorithm 8 was partitioning the input
Journal of the ACM, Vol. 66, No. 4, Article 26. Publication date: June 2019.  
Hierarchical Clustering: Objective Functions and Algorithms 26:35
graph based on a single, random vertex. In this slightly more robust version, the partition is built
iteratively: Vertices are added to the current part if there exists at least one vertex in the current
part or in the parts that were built before with which they share an edge of high-enough weight
(see Algorithm 9 for a complete description).
ALGORITHM 9: Robust and Simple Algorithm for Hierarchical Clustering on δ-adversarially
perturbed ground-truth inputs (similarity setting)
1: Input: A graph G = (V, E), a weight function w : E → R+, a parameter δ
2: p ← arbitrary vertex of V
3: i ← 0
4: V˜
i ← {p}
5: while V˜
i  V do
6: Let p1 ∈ V˜
i,p2 ∈ V \ V˜
i s.t. (p1,p2) is an edge of maximum weight in the cut (V˜
i,V \ V˜
i )
7: wi ← w(p1,p2)
8: Bi ← {u | w(p1,u) = wi}
9: while ∃u ∈ V \ (V˜
i ∪ Bi ) s.t. ∃v ∈ Bi ∪V˜
i , w(u,v) ≥ wi do
10: Bi ← Bi ∪ {u}.
11: V˜
i+1 ← V˜
i ∪ Bi
12: i ← i + 1
13: Let B1,..., Bk be the sets obtained
14: Apply the algorithm recursively on each G[Bi] and obtain a collection of trees T1,...,Tk
15: Define T ∗
0 as a tree with p as a single vertex
16: For any 1 ≤ i ≤ k, define T ∗
i to be the union of T ∗
i−1 and Ti
17: return T ∗
k
Theorem A.6. For any admissible objective function, Algorithm 9 returns a δ-approximation if the
input is a δ-adversarially perturbed ground-truth input.
To prove the theorem we introduce the following lemma whose proof is temporarily deferred.
The lemma states that the tree built by the algorithm is almost generating (up to a factor of δ in
the edge weights).
Lemma A.7. Let T be a tree output by Algorithm 9, let N be the set of internal nodes of T . For any
node N with children N1, N2 there exists a function ω : N → R+, such that for any u ∈ V (N1),v ∈
V (N2), ω(N) ≤ w(u,v) ≤ δω(N). Moreover, for any nodes N, N 
, if N  is an ancestor of N, we have
that ω(N) ≥ ω(N 
).
Assuming Lemma A.7, the proof of Theorem A.6 is as follows.
Proof of Theorem A.6. Let G = (V, E), w : E → R+ be the input graph and T ∗ be a tree of
optimal cost. By Lemma A.7, the tree T output by the algorithm is such that for any node N with
children N1, N2 there exists a realω(N), such that for anyu ∈ V (N1),v ∈ V (N2),ω(N) ≤ w(u,v) ≤
δω(N). Thus, consider the slightly different input graph G = (V, E,w
), where w : E → R+ is
defined as follows. For any edge (u,v), define w
(u,v) = ω(LCAT (u,v)). Since by Lemma A.7, for
any nodes N, N  of T , if N  is an ancestor of N, we have that ω(N) ≥ ω(N 
) and by Definition 2.2,
T is generating for G
. Thus, for any admissible cost function, we have that for G
, cost(T ;G
) ≤
cost(T ∗;G
).
Journal of the ACM, Vol. 66, No. 4, Article 26. Publication date: June 2019. 
26:36 V. Cohen-Addad et al.
Finally, observe that for any edge e, we have w
(e) ≤ w(e) ≤ δw
(e). It follows that
cost(T ;G) ≤ δcost(T ;G
) for any admissible cost function and cost(T ∗;G
) ≤ cost(T ∗;G). Therefore, cost(T ;G) ≤ δcost(T ∗;G) = δOPT.
Proof of Lemma A.7. We proceed by induction on the number of vertices in the graph (the base
case is trivial). Consider the first recursive call of the algorithm. We show the following claim.
Claim 3. For any 1 ≤ i ≤ k, for any y ∈ V
i , x ∈ Bi , wi ≥ w(x,y) ≥ wi/δ. Additionally, for any
x,y ∈ Bi , w(x,y) ≥ wi/δ.
We first argue that Claim 3 implies the lemma. Let T be the tree output by the algorithm. Consider the nodes on the path from p to the root of T ; let Ni denote the node whose subtree is the
union of T ∗
i−1 and Ti . By definition, V (T ∗
i−1) = V
i and V (Ti ) = Bi . Applying Claim 3 and observing
that wi > wi+1 implies that the lemma holds for all the nodes on the path. Finally, since for any
edge {u,v}, for u,v ∈ Bi , we also have w(u,v) ≥ wi/δ, combining with the inductive hypothesis
on Bi implies the lemma for all the nodes of the subtree Ti .
Proof of Claim 3. Let (X,d) and f be a pair of ultrametric and function that is generating for
G. Fix i ∈ {1,..., k}. For any vertex x ∈ Bi , let σ (x) denote a vertex y that is in V
i or inserted to
Bi before x and such that w(y, x) ≥ wi . For any vertex v, let σi (x) denotes the vertex obtained by
applying σ i times to x (i.e., σ2 (x) = σ (σ (x))). By definition of the algorithm, it holds that for any
x ∈ Bi , ∃s ≥ 1, such that σs (x) ∈ V
i .
Fix x ∈ Bi . For any y ∈ V
i , we have that w(y, x) ≤ wi , since, otherwise, the algorithm would
have added x before.
Now, let y ∈ V
i or y be inserted to Bi prior to x. We show that w(y, x) ≥ wi/δ. Observe that
since X,d is an ultrametric, d(x,y) ≤ max(d(x, σ (x)),d(σ (x),y)).
We now “follow” σ by applying the function σ to σ (x) and repeating until we reach σ (x) =
z ∈ V
i , for some . Combining this with the definition of an ultrametric, it follows that
d(x,y) ≤ max(d(x, σ (x)),d(σ (x), σ2 (x)),...,d(σ (x)
−1
, z),d(z,y)).
If y was in V
i , then we define yˆ = y. Otherwise y is also in Bi (and so was added to Bi before x). We
then proceed similarly as for x and “follow” σ. In this case, let yˆ = σk (y) ∈ V
i , for some k. Applying
the definition of an ultrametric again, we obtain
d(x,y) ≤ max(d(x, σ (x)),d(σ (x), σ2 (x)),...,d(σ −1
, z),d(z,yˆ),d(y, σ (y)),...,d(σk−1 (y),yˆ)).
Assume for now that d(z,yˆ) is not greater than the others. Applying the definition of a δadversarially perturbed input, we have that
δw(x,y) ≥ min(...,w(σ a (x), σ a+1 (x)),...,w(σb (y), σb+1 (y)),...).
Following the definition of σ, we have for all v that w(v, σ (v)) ≥ wi . Therefore, we conclude
δw(x,y) ≥ wi .
We thus turn to the case where d(z,yˆ) is greater than the others. Since both z,yˆ ∈ V
i , we have
that they belong to some Bj0 , Bj1 , where j0, j1 < i. We consider the minimum j such that a pair at
distance at least d(z,yˆ) was added to V
j . Consider such a pair u,v ∈ V
j satisfying d(u,v) ≥ d(z,yˆ)
and suppose w.l.o.g. that v ∈ Bj−1 (we could have either u ∈ Bj−1 or u ∈ V
j−1). Again, we follow the
path σ (v), σ (σ (v)),..., until we reach σr1 (v) ∈ V
j−1, for some r1, and similarly for u: σr2 (u) ∈ V
j−1,
for some r2. Applying the definition of an ultrametric this yields that
d(u,v) ≤ max(...,d(σ a (u), σ a+1 (u)),...,d(σb (v), σb+1 (v)),...,d(σr1 (v), σr2 (u))). (20)
Journal of the ACM, Vol. 66, No. 4, Article 26. Publication date: June 2019.                     
Hierarchical Clustering: Objective Functions and Algorithms 26:37
Now the difference is that V
j−1 does not contain any pair at distance at least d(z,yˆ). Therefore,
we have d(σr1 (v), σr2 (u)) < d(z,yˆ). Moreover, recall that by definition of u,v, d(z,yˆ) ≤ d(u,v).
Thus, d(σr1 (v), σr2 (u)) is not the maximum in Equation (20), since it is smaller than the left-hand
side. Simplifying Equation (20) yields
d(x,y) < d(z,yˆ) ≤ d(u,v) ≤ max(...,d(σ a (u), σ a+1 (u)),...,d(σb (v), σb+1 (v)),...).
By definition of a δ-adversarially perturbed input, we obtain δw(x,y) ≥ min w(σ  (b),
σ+1 (b)) ≥ wj . Now, it is easy to see that for j < i, wi < wj and therefore δw(x,y) ≥ wi .
We conclude that for any y ∈ V
i , x ∈ Bi , wi ≥ w(x,y) ≥ wi/δ and for x,y ∈ Bi , we have that
w(x,y) ≥ wi/δ, as claimed.
B WORST-CASE ANALYSIS OF COMMON HEURISTICS
The results presented in this section show that for both the similarity and dissimilarity settings,
some of the widely used heuristics may perform badly. The proofs are neither difficult nor particularly interesting, but the results stand in sharp contrast to those for structured inputs and help
motivate our study of inputs beyond worst case.
Similarity Graphs. We show that for very simple input graphs (i.e., unweighted trees), the linkage
algorithms (adapted to the similarity setting, see Algorithm 6) may perform badly.
Theorem B.1. There exists an infinite family of inputs on which the single-linkage and completelinkage algorithms output a solution of cost Ω( n
log n · OPT).
Proof. The family of inputs consists of the graphs that represent paths of length n > 2. More
formally, Let Gn be a graph on n vertices such that V = {v1,...,vn } and that has the following
edge weights. Let w(vi−1,vi ) = w(vi,vi+1) = 1, for all 1 < i < n and for any i, j, j  {i − 1,i,i + 1},
define w(vi,vj) = 0. Dasgupta [23] showed that OPT(Gn ) = O(n logn).
Complete-Linkage. We show that the complete-linkage algorithm could perform a sequence
of merges that would induce a tree of cost Ω(n2). The complete-linkage algorithm is defined as
merging the two clusters Ci,Cj that maximize the W (Ci,Cj) = minu ∈Ci,v ∈Cj w(u,v).
Observe that in the instance created w(u,v) ∈ {0, 1}. So, w.l.o.g. the first merges done by the
algorithm will consist of vertices vi,vi+1, for i even since the similarity between these pairs of
clusters is 1 and so maximum. LetVi/2 denote the cluster containing vi,vi+1. Now, observe that for
the resulting clusters, the similarity between them is 0, since w(u,v) = 0 for any u,v that are not
adjacent on the line and so the order of merges could be arbitrary.
Thus, the sequence of merges done by the algorithm could result in a hierarchical clustering tree
where the root node splits the nodes such that on the one side there are all the nodes in clusters
Vj such that j is even and on the other side all the nodes in clusters Vj such that j is odd. Since the
total number of edges between all the odd and even such clusters is Ω(n), the cost induced by the
root node of the tree output is Ω(n2).
Single-Linkage. Recall that the algorithm merges the two candidate clusters Ci,Cj that maximize w(Ci,Cj) = maxu ∈Ci,v ∈Cj w(u,v).
At the beginning, each cluster contains a single vertex and so, the algorithm could merge any
two clusters {vi}, {vj} for j ∈ {i − 1,i + 1}, since the edge weight is 1. Suppose w.l.o.g. that the
algorithm merges v1,v2, let V1 be this cluster. Then, observe that w(V1,v3) = 1 as well and so the
next merge can beV1,v3. An immediate induction shows that the hierarchical clustering tree output
by single-linkage could be a caterpillar: namely, the first split at the root being a cut V −vn,vn,
the next split on G[V −vn] being V −vn −vn−1,vn−1 and so on. As observed by Dasgupta [23],
Journal of the ACM, Vol. 66, No. 4, Article 26. Publication date: June 2019.       
26:38 V. Cohen-Addad et al.
the cost of this tree is Ω(n2) and so the ratio between OPT and single-linkage could be as bad as
Ω(n/ logn).
Theorem B.2. There exists an infinite family of inputs on which the average-linkage algorithm
outputs a solution of cost Ω(n1/3OPT).
Proof. For any n = 2i for some integer i, we define a tree Tn = (V, E) as follows. Let k = n1/3.
Let P = (u1,...,uk ) be a path of length k (i.e., for each 1 ≤ i < k, we have an edge between ui and
ui+1). For eachui , we define a collection Pi = {Pi
1 = (Vi
1 , Ei
1),..., Pi
k = (Vi
k , Ei
k )} of k paths of length
k and for each Pi
j we connect one of its extremities to ui . DefineVi = {ui}

j Vi
j andV = 
i Vi .
Claim 4. OPT(Tn ) ≤ 3n4/3
Proof. Consider the following non-binary solution tree T ∗: Let the root have children
N1,..., Nk such thatV (Ni ) = Vi and for each child Ni let it have children Nj
i such thatV (Nj
i ) = V j
i .
Finally, for each Nj
i let the subtree rooted at Nj
i be any tree.
We now analyze the cost of T ∗. Observe that for each edge e in the path P, we have
|V (LCAT ∗ (e))| = n. Moreover, for each edge e connecting a path Pi
j to ui , we have |V (LCAT ∗ (e))| =
k2 = n2/3. Finally, for each edge e whose both endpoints are in a path Pi
j , we have that
|V (LCAT ∗ (e))| ≤ k = n1/3.
We now sum up over all edges to obtain the overall cost of Tn. There are k = n1/3 edges in P;
They incur a cost of nk = n4/3. There are k2 edges joining a vertex ui to a path Pi
j ; They incur a cost
of k2 · n2/3 = n4/3. Finally, there are k3 edges whose both endpoints are in a path Pi
j ; they incur a
cost of at most k3 · n1/3 = n4/3. Thus, the total cost of this tree is at most 3n4/3 ≥ OPT(Tn ).
We now argue that there exists a sequence of merges performed by the average-linkage algorithm that yields a solution of cost at least n5/3.
Claim 5. There exists a sequence of merges performed by average-linkage and an integer t such
that the candidate trees at time t have leaves sets {{u1,...,uk }} 
i,j{Vi
j }.
Equipped with this claim, we can finish the proof of the proposition. Since there is no edge
between Vi
j and Vi
j for i  i or j
  j the similarity between those trees in the algorithm will
always be 0. However, the similarity between the tree T
 that has leaves set {u1,...,uk } and any
other tree is positive (since there is one edge joining those two sets of vertices in Tn). Thus, the
algorithm will merge T
 with some tree whose vertex set is exactly Vi
j for some i, j. For the same
reasons, the resulting cluster will be merged with a cluster whose vertex set is exactly Vi
j , and
so on. Hence, after n/2k = k2/2 such merges, the tree T
 has a leaves set of size k · k2/2 = n/2.
However, the number of edges from this cluster to the other candidate clusters is k2/2 (since each
other remaining clusters correspond to a vertex set Vi
j for some i, j) For each such edge e we have
|V (LCAT (e))| ≥ n/2. Since there are k2/2 of them, the resulting tree has cost Ω(n5/3). Combining
with Claim 4 yields the theorem.
We thus turn to the proof of Claim 5.
Proof of Claim 5. Given a graph G and a set of candidate trees C. Define G/C to be the graph
resulting from the contraction of all the edges for which both endpoints belong to the same cluster.
We show a slightly stronger claim. We show that for any graphG and candidate trees V such that
(1) All the candidate clusters in V have the same size; and
(2) There exists a bijection ϕ between vertices v ∈ Tn and vertices in G/C.
Journal of the ACM, Vol. 66, No. 4, Article 26. Publication date: June 2019.        
Hierarchical Clustering: Objective Functions and Algorithms 26:39
There exists a sequence of merges and an integer t such that the candidate trees at time t have
leaves sets {{ϕ(u1),...,ϕ(uk )}} 
i,j{ϕ(Vi
j )} where ϕ(Vi
j ) = {ϕ(v) | v ∈ Vi
j }.
This slightly stronger statement yields the claim by observing that Tn and the candidate trees
at the start of the algorithm satisfy the conditions of the statement.
We proceed by induction on the number of vertices of the graph. Let Vi
j = {vi
j (1),...,vi
j (k)}
such that (vi
j (),vi
j ( + 1)) ∈ Ei
j for any 1 ≤  < k, and (vi
j (k),ui ) ∈ E.
We argue that the algorithm could perform a sequence of merges that results in the following
set C of candidate trees. C contains candidate trees Ui = ϕ(u2i−1) ∪ ϕ(u2i ) for 1 ≤ i < k/2, and for
each i, j, candidate trees vi,j,  = ϕ(vi
j (2 − 1) ∪ ϕ(vi
j (2)), for 1 ≤  < k/2. Let s0 be the number of
vertices in each candidate tree.
At first, all the trees contain a single vertex and so, for each pair of adjacent vertices of the graph
the similarity between their corresponding trees in the algorithm is 1/s0. For any non-adjacent
pair of vertices, the corresponding trees have similarity 0. Thus, w.l.o.g. assume the algorithm first
merges u1,u2. Then, the similarity between the newly created tree U 1 and any other candidate
tree C is 0 if there is no edge between u1 and u2 and C or 1/(2s0) if there is one (since U 1 contains
now two vertices). For the other candidate trees the similarity is unchanged. Thus, the algorithm
could merge vertices u3,u4. Now, observe that the similarity betweenU 2 andU 1 is at most 1/(4s0).
Thus, it is possible to repeat the argument assuming that the algorithm merges the candidate
trees corresponding to u5,u6. Repeating this argument k/2 times yields that after k/2 merges, the
algorithm has generated the candidates treesU1,...,Uk/2−1. The other candidate trees still contain
a single vertex. Thus, the algorithm is now forced to merge candidate trees that contain single
vertices that are adjacent (since their similarity is 1/s0 and any other similarity is < 1/s0). Assume,
w.l.o.g. assume that the algorithm merges v1
1 (1),v1
1 (2). Again, applying a similar reasoning to each
v1
1 (2 − 1),v1
1 (2) yields the set of candidate clusters v1,1,1,...,v1,1,k/2−1. Applying this argument
to all sets Vi
j yields that the algorithm could perform a sequence of merges that results in the set
C of candidate clusters described above.
Now, all the clusters have size 2s0 and there exists a bijection between vertices of G/C and Tn/2.
Therefore, combining with the induction hypothesis yields the claim.
Dissimilarity Graphs. We now show that single-linkage, and bisection 2-Center might return
a solution that is arbitrarily bad compared to OPT in some cases. Hence, since average-linkage
achieves a 2/3-approximation in the worst case it seems that it is more robust than the other
algorithms used in practice.
Theorem B.3. For each of the single-linkage, and bisection 2-Center algorithms, there exists a
family of inputs for which the algorithm outputs a solution of value O(OPT/n).
Proof. We define the family of inputs for single-linkage and bisection 2-Center as follows.
For any n > 2, the graph Gn consists of n vertices V = {v1,...,vn−1,u} and the edge weights are
the following: For any i, j ∈ {1,...,n − 1}, w(vi,vj) = 1, for any 1 < i ≤ n − 1, w(vi,u) = 1, and
w(v1,u) = W for some fixed W ≥ n3. Consider the tree T ∗ whose root induces a cut (V \ {u}, {u}).
Then, the value of this tree (and so OPT) is at least nW , since |V (LCAT ∗ (v1,u))| = n.
Single-Linkage. At the beginning, all the clusters are at distance 1 from each other except v1
and u that are at distance W . Thus, suppose that the first merge generates a candidate tree C1
whose leaves set is {v1,v2}. Now, since w(v2,u) = 1, we have that all the clusters are at distance
1 from each other. Therefore, the next merge could possibly generate the cluster C2 with leaves
sets {u,v1,v2}. Assume w.l.o.g. that this is the case and let T be the tree output by the algorithm.
We obtain |V (LCAT (u,v1))| = 3 and so, since for any vi,vj , |V (vi,vj)| ≤ n, we have val(T ) ≤ n2 +
3W ≤ 4W , because W > n2. Hence, val(T ) = O(val(T ∗)/n).
Journal of the ACM, Vol. 66, No. 4, Article 26. Publication date: June 2019.          
26:40 V. Cohen-Addad et al.
Bisection 2-Center. It is easy to see that for any location of the two centers, the cost of the
clustering is 1. Thus, suppose that the algorithm locates centers at v2 and v3 and that the induced
partitioning is {v1,v2,u},V \ {v1,v2,u}. It follows that |V (LCAT (u,v1))| ≤ 3 and so, val(T ) ≤ n2 +
3W ≤ 4W , since W > n2. Again, val(T ) = O(val(T ∗)/n). 