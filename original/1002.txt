Gradient descent (GD) and stochastic gradient descent (SGD) are the workhorses of large-scale machine learning. While classical theory focused on analyzing the performance of these methods in convex optimization
problems, the most notable successes in machine learning have involved nonconvex optimization, and a gap
has arisen between theory and practice. Indeed, traditional analyses of GD and SGD show that both algorithms converge to stationary points efficiently. But these analyses do not take into account the possibility
of converging to saddle points. More recent theory has shown that GD and SGD can avoid saddle points,
but the dependence on dimension in these analyses is polynomial. For modern machine learning, where the
dimension can be in the millions, such dependence would be catastrophic. We analyze perturbed versions
of GD and SGD and show that they are truly efficient—their dimension dependence is only polylog arithmic.
Indeed, these algorithms converge to second-order stationary points in essentially the same time as they take
to converge to classical first-order stationary points.
CCS Concepts: • Theory of computation → Nonconvex optimization; Machine learning theory;
Additional Key Words and Phrases: Saddle points, (stochastic) gradient descent, perturbations, efficiency

1 INTRODUCTION
One of the principal discoveries in machine learning in recent years is an empirical one—that simple algorithms often suffice to solve difficult real-world learning problems. Machine learning algorithms generally arise via formulations as optimization problems, and, despite a massive classical
toolbox of sophisticated optimization algorithms and a major modern effort to further develop that
toolbox, the simplest algorithms—gradient descent, which dates to the 1840s [15] and stochastic
gradient descent, which dates to the 1950s [44]—reign supreme in machine learning.
This empirical discovery is appealing in many ways. First, at the scale of modern machine
learning applications—often involving many millions of data points and millions of parameters—
complex algorithms are generally infeasible, so that the only hope is that simple algorithms might
be not only feasible but successful. Second, simple algorithms are easier to implement, debug, and
maintain. Third, as the field of machine learning transforms into a real-world engineering discipline, it will be necessary to develop solid theoretical foundations for entire systems that employ
machine learning algorithms at their core, and such an effort seems less daunting if the basic ingredients are simple.
These developments were presaged and supported by optimization researchers such as Nemirovskii, Nesterov, and Polyak who, from the 1960s until the present day, have pursued an indepth study of first-order, gradient-based algorithms, developing novel algorithms and accompanying theory [35, 37, 41]. Their results have included lower bounds and algorithms that achieve
those lower bounds. This line of work has made clear that even simple algorithms require delicate
theoretical treatment when they are studied in large-scale settings. Thus, much of the focus has
been on the setting of convex optimization where many of the complexities have been stripped
away. This has allowed the development of an elegant theory, and has provided a solid jumpingoff point for further analysis that has brought additional computational constraints into play—
including distributed platforms, fault tolerance, communication bottlenecks, and asynchronous
computation [42, 46, 52].
The most notable machine-learning success stories, however, have generally involved nonconvex optimization formulations, and a gap has arisen between theory and practice. Attempts to fill
this gap include Bhojanapalli et al. [8] for matrix sensing, Jain et al. [26] and Ge et al. [24] for
matrix completion, Netrapalli et al. [40] and Ge et al. [23] for robust principal component analysis, Bandeira et al. [7] and Mei et al. [34] for synchronization and MaxCut, Boumal et al. [9] for
smooth semidefinite programs, and Choromanska et al. [16] in the setting of learning multi-layer
neural networks. But there remains a need to develop a general theory that relates the convergence
of machine learning algorithms to geometry and dynamics.
In the optimization literature much of the focus of theoretical work on the performance of algorithms has been on the relationship between the number of iterations of the algorithm and a
suitable notion of accuracy. Dimension is often neglected in such analyses, in part because in the
convex setting the numbers of iterations to find a near-optimal solution for even the simplest algorithms, including gradient descent, are provably independent of dimension. In developing algorithmic theory for nonconvex optimization formulations of machine learning problems, however, it is
critically important to study iteration complexity as a function of of dimension, which can be in the
millions. Moreover, we cannot resort to asymptotics—we are interested in problems at all scales.
In the current article, we have three goals. The first is to show that significant progress has been
made in recent years in the theoretical analysis of algorithms for nonconvex machine learning. The
second is to extend that line of analysis to handle both stochastic and non-stochastic algorithms
in a single framework. In both cases, we upper bound the iteration complexity as a function of
both accuracy and dimension. The third is to exhibit a simple proof that exposes the core of the
phenomenon that determines the dimension dependence.
Journal of the ACM, Vol. 68, No. 2, Article 11. Publication date: February 2021.
On Nonconvex Optimization for Machine Learning 11:3
Nonconvex optimization problems are intractable in general. Progress has been in machine
learning by noting that in many problems the principal difficulty is not local minima, either because there are no spurious local minima (we review a list of such problems in Section 3) or because
empirical work has shown that the local minima that are found by local gradient-based algorithms
tend to be effective in terms of the ultimate goal of machine learning, which is performance on
a test set. The problem then becomes one of avoiding saddle points, which are ubiquitous in machine learning architectures. Saddle points slow down gradient-based algorithms and in millions
of dimensions they are potentially a major bottleneck for such algorithms. The theoretical problem
becomes that of characterizing the iteration complexity of avoiding saddle points, as a function of
target accuracy and dimension.
We briefly mention some of the most relevant theoretical context for this problem here, providing a more thorough review of related work in Section 1.1 and in the Appendix. Lee et al. [30, 31]
showed that gradient descent, under random initialization or with perturbations, asymptotically
avoids saddle points with probability one. Ge et al. [22] provided a more quantitative (nonasymptotic) characterization of gradient descent augmented with a suitable perturbation, showing that
its convergence rate in the presence of saddle points is upper bounded by an expression of the
form poly(d, ϵ−1), where d is the dimension and ϵ is the accuracy. While these convergence results
are inspiring, they are significantly worse than the convergence of gradient descent in the convex
setting, or its convergence in the nonconvex setting where convergence to a saddle point is not
excluded—in both cases the rate is independent of d—and they do not seem to accord with the
empirical success of gradient-based methods in high-dimensional problems. Thus we ask whether
these results, which are upper bounds, can be improved.
The current article provides a positive answer to this question. We show that suitably-perturbed
versions of gradient descent and stochastic gradient descent escape saddle points in a number
of iterations that is only polylogarithmic in dimension. More technically, defining a notion of
ϵ-second-order stationarity (see Section 2), which rules out saddle points, to be contrasted with
classical ϵ-first-order stationarity, which simply means near vanishing of the gradient, and which
therefore does not rule out saddle points, we show that:
• Perturbed gradient descent (PGD) finds ϵ-second-order stationary points in O˜ (ϵ−2) iterations, where O˜ (·) hides only absolute constants and polylogarithmic factors. Compared to
the O(ϵ−2) iterations required by gradient descent (GD) to find first-order stationary points
[37], this involves only additional polylogarithmic factors in d.
• In the stochastic setting where stochastic gradients are Lipschitz, perturbed stochastic gradient descent (PSGD) finds ϵ-second-order stationary points in O˜ (ϵ−4) iterations. Compared
to the O(ϵ−4) iterations required by stochastic gradient descent (SGD) to find first-order stationary points [25], this again incurs overhead that is only polylogarithmic in d.
• When stochastic gradients are not Lipschitz, PSGD finds ϵ-second-order stationary points
in O˜ (dϵ−4) iterations—this involves an additional linear factor in d.
1.1 Related Work
In this section, we discuss related work on convergence guarantees for finding second-order stationary points. Some key comparisons are summarized in Table 1, and an augmented table is provided in Appendix A.
Non-stochastic settings. Classical approaches to finding second-order stationary points assume access to second-order information, in particular the Hessian matrix of second derivatives. Examples of such approaches include the cubic regularization method [39] and trust-region
Journal of the ACM, Vol. 68, No. 2, Article 11. Publication date: February 2021.
11:4 C. Jin et al.
Table 1. A High Level Summary of the Results of This Paper and Their Comparison
to Prior State of the Art for GD and SGD Algorithms
Setting Algorithm Iterations Guarantees
Non- GD [38] O(ϵ−2 ) first-order stationary point
stochastic PGD O˜ (ϵ−2 ) second-order stationary point
SGD [25] O(ϵ−4 ) first-order stationary point
Stochastic PSGD (with Assumption C) O˜ (ϵ−4 ) second-order stationary point
PSGD (no Assumption C) O˜ (dϵ−4 ) second-order stationary point
This table only highlights the dependences on d and ϵ. See Section 1 for a description of these results. See Section 1.1 and Appendix A for a more detailed comparison with other related works.
methods [17], both of which require O(ϵ−1.5) queries of gradients and Hessians. This favorable convergence rate is, however, obtained at a high cost per iteration, owing to the fact that Hessian matrices scale quadratically with respect to dimension. In practice researchers have turned to first-order
methods, which only utilize gradients and are therefore are substantially cheaper per iteration.
Before turning to pure first-order algorithms, we mention a line of research that is based on
the assumption of a Hessian-vector product oracle [1, 12]. For architectures such as deep neural networks, Hessian-vector products can be computed efficiently via automatic differentiation,
and it is thus possible to obtain much of the effect of second-order methods with a complexity
closer to that of first-order methods. Indeed, the algorithms have convergence rates of O˜ (ϵ−1.75)
gradient queries [1, 12]. Their implementation, however, involved nested loops, and accordingly a
concern with the setting of hyperparameters. Thus, despite the favorable convergence rate, these
algorithms have not yet found their way into practical implementations.
Given the preference among practitioners for simple, single-loop algorithms, and the striking
empirical successes obtained with such algorithms, it is important to pin down the theoretical
properties of such algorithms. In such analyses, the results for second-order and Hessian-vector
algorithms serve as baselines. Of key interest is the convergence rate not merely as a function
of the accuracy ϵ but also as a function of the dimension d. Indeed, second-order algorithms can
use the structure of the Hessian to readily avoid unhelpful directions even in high-dimensional
spaces. Without the Hessian there is a concern that algorithms may scale poorly as a function of
dimension.
Ge et al. [22] and Levy [33] studied simple variants of gradient descent and found that while
it has favorable scaling in terms of ϵ, it requires poly(d) gradient queries to find second-order
stationary points. These results are only upper bounds, however. The practical success of gradient
descent suggests that the poly(d) scaling may be overly pessimistic. Indeed, in an early version
of the results presented here, Jin et al. [27] showed that a simple perturbed version of gradient
descent finds second-order stationary points in O˜ (ϵ−2) gradient queries, paying only a logarithmic
overhead compared to the rate associated with finding first-order stationary points. This result is
summarized in the first two lines of Table 1. In followup work, Jin et al. [29] show that a perturbed
version of celebrated Nesterov’s accelerated gradient descent [36] enjoys a faster convergence rate
of O˜ (ϵ−1.75), again with logarithmic dimension dependence.
Stochastic setting with Lipschitz gradient. We now turn to the setting in which the learning algorithm only has access to stochastic gradients and where the stochastic is extrinsic; i.e., not under
the control of the algorithm. Most existing work assumes that the stochastic gradients are Lipschitz
(or equivalently that the underlying functions are gradient-Lipschitz, see Assumption C). Under
this assumption, and an additional Hessian-vector product oracle, Allen-Zhu [2], Tripuraneni et al.
Journal of the ACM, Vol. 68, No. 2, Article 11. Publication date: February 2021.
On Nonconvex Optimization for Machine Learning 11:5
[49], Zhou et al. [54] designed algorithms that have an iteration complexity of O˜ (ϵ−3.5). Xu et al.
[50] and Allen-Zhu and Li [3] obtain similar results without the requirement of a Hessian-vector
product oracle. The sharpest rates in this category have been obtained by Fang et al. [20] and Zhou
and Gu [53], who show that the iteration complexity can be further reduced to O˜ (ϵ−3). Again, however, this line of works consists of double-loop algorithms, and it remains unclear whether they
will have an impact on practice.
Among single-loop algorithms that are simple variants of SGD, Ge et al. [22] showed that a
particular variant has an iteration complexity for finding second-order stationary points that is
upper bounded by d4poly(ϵ−1). Daneshmand et al. [18] presented an alternative variant of SGD
and showed that if the variance of the stochastic gradient along the escaping direction of saddle
points is at least γ for all saddle points, then the algorithm finds second-order stationary points in
O˜ (γ −4
ϵ−5) iterations. In general, however, γ scales as 1/d, which implies a complexity of O˜ (d4
ϵ−5).
In the current article, we demonstrate that a simple perturbed version of SGD achieves a convergence rate of O˜ (ϵ−4), which matches the speed of SGD to find a first-order stationary point up
to polylogarithmic factors in dimension. Concurrent to our work, Fang et al. [21] analyzed SGD
with averaging over last few iterates, and obtained a faster convergence rate of O˜ (ϵ−3.5).
General stochastic setting. There is significantly less work in the general setting in which the
stochastic gradients are no longer guaranteed to be Lipschitz. In fact, only the results of Ge et al.
[22] and Daneshmand et al. [18] apply here, and both of them require at least Ω(d4) gradient
queries to find second-order stationary points. The current article brings this dependence down
to linear dimension dependence. See the last three lines in Table 1 for a summary of the results in
the stochastic case.
Other settings. Finally, there are also several recent results in the setting in which objective
functions can be written as a finite sum of individual functions. We refer readers to Allen-Zhu and
Li [3], Reddi et al. [43], and Lei et al. [32] and the references therein for further reading.
1.2 Organization
In Section 2, we review some algorithmic and mathematical preliminaries. Section 3 presents several examples of nonconvex problems in machine learning, demonstrating how second-order stationarity can ensure approximate global optimality. In Section 4, we present the algorithms that
we analyze and present our main theoretical results for perturbed GD and SGD. In Section 5, we
present the proof for the non-stochastic case (perturbed GD), which illustrates some of our key
ideas. The proof for the stochastic setting is presented in the Appendix. We conclude in Section 6.
2 BACKGROUND
In this section, we introduce our notation and present definitions and assumptions. We also
overview existing results in nonconvex optimization in both the deterministic and stochastic settings.
2.1 Notation
We use bold uppercase letters A, B to denote matrices and bold lowercase letters x, y to denote vectors. For vectors we use · to denote the 2-norm, and for matrices we use · and ·F to denote
spectral (or operator) norm and Frobenius norm, respectively. We use λmin(·) to denote the smallest
eigenvalue of a matrix. For a function f : Rd → R, we use ∇f and ∇2 f to denote the gradient and
Hessian and f  to denote the global minimum of function f . We use the notation O(·), Θ(·), Ω(·)
to hide only absolute constants that do not depend on any problem parameter, and the notation
Journal of the ACM, Vol. 68, No. 2, Article 11. Publication date: February 2021.        
11:6 C. Jin et al.
O˜ (·), Θ( ˜ ·), Ω( ˜ ·) to hide absolute constants and factors that are only polylogarithmically dependent
on all problem parameters.
2.2 Nonconvex Optimization and Gradient Descent
In this article, we are interested in solving general unconstrained optimization problems of the
form:
min
x∈Rd
f (x),
where f is a smooth function that can be nonconvex. In particular, we assume that f has Lipschitz
gradients and Lipschitz Hessians, which ensures that the gradient and Hessian cannot change too
rapidly.
Definition 2.1. A differentiable function f is -gradient Lipschitz if
∇f (x1) − ∇f (x2) ≤ x1 − x2  ∀ x1, x2.
Definition 2.2. A twice-differentiable function f is ρ-Hessian Lipschitz if
∇2 f (x1) − ∇2 f (x2) ≤ ρx1 − x2  ∀ x1, x2.
Assumption A. The function f is -gradient Lipschitz and ρ-Hessian Lipschitz.
Our point of departure is the classical Gradient Descent (GD) algorithm, whose update takes
following form:
xt+1 = xt − η∇f (xt ), (1)
where η > 0 is a step size or learning rate. Since the problem of finding a global optimum for
general nonconvex functions is NP-hard, the classical literature in optimization has resorted to a
local surrogate—first-order stationarity.
Definition 2.3. For a differentiable function f , x is a first-order stationary point if ∇f (x) = 0.
Definition 2.4. For a differentiable function f , x is an ϵ-first-order stationary point if
∇f (x) ≤ ϵ.
It is of major importance that gradient descent converges to a first-order stationary point in a
number of iterations that is independent of dimension. This fact, referred to as “dimension-free
convergence” in the optimization literature, is captured in the following classical theorem.
Theorem 2.5 ([37]). For any ϵ > 0, assume the function f (·) is -gradient Lipschitz, and set the
step size as η = 1/. Then, the gradient descent algorithm in Equation (1) will visit an ϵ-stationary
point at least once in the following number of iterations:
(f (x0) − f )
ϵ2 .
Note that in this formulation, the last iterate is not guaranteed to be a stationary point. However,
it is not hard to figure out which iterate is the stationary point by calculating the norm of the
gradient at every iteration.
A first-order stationary point can be a local minimum, a local maximum or even a saddle point:
Definition 2.6. For a differentiable function f , a stationary point x is a
• local minimum, if there exists δ > 0 such that f (x) ≤ f (y) for any y with y − x ≤ δ.
• local maximum, if there exists δ > 0 such that f (x) ≥ f (y) for any y with y − x ≤ δ.
• saddle point, otherwise.
Journal of the ACM, Vol. 68, No. 2, Article 11. Publication date: February 2021.                     
On Nonconvex Optimization for Machine Learning 11:7
For minimization problems, both saddle points and local maxima are clearly undesirable. Our
focus will be “saddle points,” although our results also apply directly to local maxima as well.
Unfortunately, distinguishing saddle points from local minima for smooth functions is still NPhard in general [38]. To avoid these hardness results, we focus on a subclass of saddle points.
Definition 2.7. For a twice-differentiable function f , x is a strict saddle point if x is a stationary
point and λmin(∇2 f (x)) < 0.
A generic saddle point must satisfy that λmin(∇2 f (x)) ≤ 0. Being “strict” simply rules out the
case where λmin(∇2 f (x)) = 0. We reformulate our goal as that of finding stationary points that are
not strict saddle points.
Definition 2.8. For twice-differentiable function f (·), x is a second-order stationary point if
∇f (x) = 0, and ∇2 f (x) 	 0.
Definition 2.9. For a ρ-Hessian Lipschitz function f (·), x is an ϵ-second-order stationary
point if:
∇f (x) ≤ ϵ and ∇2 f (x) 	 −√ρϵ · I.
Our definition again makes use of an ϵ-ball around the stationary point so that we can discuss
rates, and the condition on the Hessian in Definition 2.4 uses the Hessian Lipschitz parameter ρ to
retain a single accuracy parameter and to match the units of the gradient and Hessian, following
the convention of Nesterov and Polyak [39].
Although second-order stationarity is only a necessary condition for being a local minimum,
a line of recent work in the machine learning literature shows that for many popular models in
machine learning, all ϵ-second-order stationary points are approximate global minima. Thus, for
these models finding second-order stationary points is sufficient for solving those problems. See
Section 3 for references and discussion of these results.
2.3 Stochastic Approximation
We turn to the stochastic approximation setting, where we cannot access the exact gradient ∇f (·)
directly. Instead for any point x, a gradient query will return a stochastic gradient g(x; θ ), where θ
is a random variable drawn from a distribution D. The key property that we assume for stochastic gradients is that they are unbiased: ∇f (x) = Eθ∼D [g(x; θ )]. That is, the average value of the
stochastic gradient equals the true gradient. In short, the update of SGD is
Sample θt ∼ D, xt+1 = xt − η∇g(xt ; θt ). (2)
Other than being an unbiased estimator of true gradient, another standard assumption on the
stochastic gradients is that their variance is bounded by some number σ2:
Eθ∼D
g(x, θ ) − ∇f (x)2

≤ σ2
.
When we are interested in high-probability bounds, we make the following stronger assumption
on the tail of the distribution.
Assumption B. For any x ∈ Rd , the stochastic gradient g(x; θ ) with θ ∼ D satisfies:
Eg(x; θ ) = ∇f (x), P (g(x; θ ) − ∇f (x) ≥ t) ≤ 2 exp(−t
2
/(2σ2)), ∀t ∈ R.
We note this assumption is more general than the standard notion of a sub-Gaussian random
vector, which assumes E exp(v, X − EX) ≤ exp(σ2 v2/d) for any v ∈ Rd . The latter assumption requires the distribution to be “isotropic” while our assumption does not. By Lemma C.2, we
Journal of the ACM, Vol. 68, No. 2, Article 11. Publication date: February 2021.          
11:8 C. Jin et al.
know that both bounded random vectors and standard sub-Gaussian random vector are special
cases of our more general setting.
Prior work shows that stochastic gradient descent converges to first-order stationary points in
a number of iterations that is independent of dimension.
Theorem 2.10 ([25]). For any ϵ, δ > 0, assume that the function f is -gradient Lipschitz, that the
stochastic gradient g satisfies Assumption B, and let the step size scale as η = Θ( ˜ −1 (1 + σ2/ϵ2)
−1).
Then, with probability at least 1 − δ, stochastic gradient descent will visit an ϵ-stationary point at
least once in the following number of iterations:
O˜

(f (x0) − f )
ϵ2

1 + σ2
ϵ2
  .
3 ON THE SUFFICIENCY OF SECOND-ORDER STATIONARITY
In this section, we show that for a wide class of nonconvex problems in machine learning and signal
processing, all second-order stationary points are global minima. Thus, for this class of problems,
finding second-order stationary points efficiently is equivalent to solving the problem. We focus
on the underlying global geometry that these problems have in common.
Problems for which second-order stationary points are global minima include tensor decomposition [22], dictionary learning [47], phase retrieval [48], synchronization and MaxCut [7, 34],
smooth semidefinite programs [9], and many problems related to low-rank matrix factorization,
such as matrix sensing [8], matrix completion [24], and robust principale component analysis
[23]. In particular, these papers show that by adding appropriate regularization terms, and under
mild conditions, there are two key geometric properties satisfied by the corresponding objective
functions: (a) All local minima are global minima. There might be multiple local minima due to
permutation, but they are all equally good. (b) All saddle points have at least one direction with
strictly negative curvature, thus are strict saddle points. we summarize the consequences of these
properties in the following proposition, for which we omit the proof as it follows essentially by
definition.
Proposition 3.1. If a function f satisfies (a) all local minima are global minima; (b) all saddle
points (including local maxima) are strict saddle points, then all second-order stationary points are
global minima.
This implies that the core problem for these nonconvex machine-learning applications is to find
second-order stationary points efficiently. If we can prove that some simple variants of GD and
SGD converges to second-order stationary points efficiently, then we immediately establish global
convergence results for these algorithms for all of the above applications (i.e. convergence from
arbitrary initialization).
Before we turn to such convergence results, consider, as an illustrative example of this class
of nonconvex problems, the problem of finding the leading eigenvector of a positive semidefinite
matrix M ∈ Rd×d . We define the following objective:
min
x∈Rd
f (x) = 1
2
xx − M2
F . (3)
Denote the eigenvalues and eigenvectors of M as (λi, vi ) for i = 1,...,d, and assume there is a
gap between the first and second eigenvalues: λ1 > λ2 ≥ λ3 ≥ ··· ≥ λd ≥ 0. In this case, the global
optimal solutions are x = ±
√
λ1v1 giving the top eigenvector direction.
The objective function (3) is nonconvex as a function of x. To optimize this objective via
gradient-descent methods, we need to analyze the global landscape of the objective function. Its
Journal of the ACM, Vol. 68, No. 2, Article 11. Publication date: February 2021.          
On Nonconvex Optimization for Machine Learning 11:9
ALGORITHM 1: Perturbed Gradient Descent
Input: x0, step size η, perturbation radius r.
for t = 0, 1,..., do
xt+1 ← xt − η(∇f (xt ) + ξt ), ξt ∼ N (0, (r 2/d)I)
gradient and Hessian are of the form:
∇f (x) =(xx − M)x
∇2 f (x) =x2
I + 2xx − M.
Therefore, all stationary points satisfy the equation Mx = x2x. Thus they are 0 and ±
√
λivi for
i = 1,...,d. We already know that ±
√
λ1v1 are global minima, thus they are also local minima and
are equivalent up to a sign difference. For the remaining stationary points x†, we note that their
Hessian always has strict negative curvature along the v1 direction: v
1 ∇2 f (x†)v1 ≤ λ2 − λ1 < 0.
Thus these points are strict saddle points. Having established the preconditions for Proposition 3.1,
we are able to conclude:
Corollary 3.2. Assume that M is a positive semidefinite matrix whose top two eigenvalues are
λ1 > λ2 ≥ 0. For the problem of minimizing the objective Equation (3), all second-order stationary
points are global optima.
Further analysis can be carried out to establish the ϵ-robust version of Corollary 3.2. Informally,
it can be shown that under technical conditions, for polynomially small ϵ, all ϵ-second-order stationary point are close to global optima. We refer the readers to Ge et al. [23] for the formal
statement.
4 MAIN RESULTS
In this section, we present our main results on the efficiency of GD and SGD in the nonconvex
setting. We first study the case where the exact gradients are accessible, where we focus on an
algorithm that we refer to as PGD. We then turn to the stochastic setting, and present the results
for Perturbed SGD and its mini-batch version.
4.1 Non-stochastic Setting
We begin by considering the case in which exact gradients are available, such that GD can be
implemented. For convex problems, GD is efficient, but, as can be seen in Equation (1), GD makes
a non-zero step only when the gradient is non-zero, and thus in the nonconvex setting it will
be stuck at saddle points if initialized there. We thus consider a simple variant of GD that adds
randomness to the iterates at each step (Algorithm 1). The question is whether such a simple
procedure can be efficient, particularly in terms of its dimension dependence.
At each iteration, Algorithm 1 is almost the same as gradient descent, except it adds a small
isotropic random Gaussian perturbation to the gradient. The perturbation ξt is sampled from a
zero-mean Gaussian with covariance (r 2/d)I so that Eξt 2 = r 2. We note that Algorithm 1 is
different from that studied in Jin et al. [27], where perturbation was added only when certain
conditions hold.
We now show that if we pick r = Θ( ˜ ϵ ) in Algorithm 1, PGD will find an ϵ-second-order stationary point in a number of iterations that has only a polylogarithmic dependence on dimension.
Theorem 4.1. Let the function f (·) satisfy Assumption A. Then, for any ϵ, δ > 0, the PGD algorithm (Algorithm 1), with parameters η = Θ( ˜ 1/) and r = Θ( ˜ ϵ ), will visit an ϵ-second-order stationary
Journal of the ACM, Vol. 68, No. 2, Article 11. Publication date: February 2021.       
11:10 C. Jin et al.
ALGORITHM 2: Perturbed Stochastic Gradient Descent (PSGD)
Input: x0, step size η, perturbation radius r.
for t = 0, 1,..., do
sample θt ∼ D
xt+1 ← xt − η(g(xt ; θt ) + ξt ), ξt ∼ N (0, (r 2/d)I)
point at least once in the following number of iterations, with probability at least 1 − δ:
O˜

(f (x0) − f )
ϵ2

,
where O˜ and Θ˜ hide polylogarithmic factors in d, , ρ, 1/ϵ, 1/δ, and Δf := f (x0) − f .
Remark 4.2. If we wish to output an ϵ-second-order stationary point with constant probability,
then it suffices to run PGD for the same number of iterations as in Theorem 4.1 and output an
iterate uniformly at random (see Section B.6 for the formal proof). We note that the ϵ-secondorder stationary point that is output in this way may not be the point with the smallest function
value among all iterates.
Remark 4.3. We have chosen the distribution of the perturbations to be Gaussian in Algorithm 1
for simplicity. This choice is not necessary. The key properties needed for the perturbation distributions are (a) that the tail of the distribution is sufficiently light such that an appropriate concentration inequality holds, and (b) the variance in every direction is bounded below.
Comparing Theorem 4.1 to the classical result in Theorem 2.5, our result shows that PGD finds
second-order stationary points in almost the same time as GD finds first-order stationary points,
up to only logarithmic factors. Therefore, strict saddle points are computationally benign for firstorder gradient methods.
Comparing to Theorem 2.5, we see that Theorem 4.1 makes an additional assumption on the
Lipschitz smoothness of the Hessian. Without such an assumption, a γ -strict saddle point (per
Definition 2.7, with λmin(∇2 f (x)) < −γ ) can be arbitrarily close to a second-order stationary point.
Therefore, the additional assumption is essential in separating strict saddle points from secondorder stationary points.
4.2 Stochastic Setting
In the stochastic approximation setting, exact gradients ∇f (·) are no longer available, and the algorithms only have access to unbiased stochastic gradients: g(·; θ ) such that ∇f (x) = Eθ∼D [g(x; θ )].
In machine learning, the stochastic gradient g is often obtained as an exact gradient of a smooth
function: g(·; θ ) = ∇f (·; θ ). We formalize this assumption.
Assumption C. For any θ ∈ supp(D), g(·; θ ) is ˜
-Lipschitz:
g(x1; θ ) − g(x2; θ ) ≤ ˜
x1 − x2  ∀ x1, x2.
In the special case where g(·; θ ) = ∇f (·; θ ) for some twice-differentiable function f (·; θ ), Assumption C ensures that the spectral norm of the Hessian of function f (·; θ ) is bounded by ˜
 for
all θ. Therefore, the stochastic Hessian also enjoys good concentration properties, which allows
algorithms to find points with a second-order characterization. In contrast, when Assumption C
no longer holds, the problem of finding second-order stationary points becomes more of a challenge due to the lack of concentration of the stochastic Hessian. In the current article, we treat
both cases, by allowing ˜
 = +∞ to encode the assertion that Assumption C does not hold.
Journal of the ACM, Vol. 68, No. 2, Article 11. Publication date: February 2021.              
On Nonconvex Optimization for Machine Learning 11:11
ALGORITHM 3: Mini-batch Perturbed Stochastic Gradient Descent (Mini-batch PSGD)
Input: x0, step size η, perturbation radius r.
for t = 0, 1,..., do
sample {θ (1)
t , ··· θ (m)
t }∼D
gt (xt ) ← m
i=1 g(xt ; θ (i)
t )/m
xt+1 ← xt − η(gt (xt ) + ξt ), ξt ∼ N (0, (r 2/d)I)
We are now ready to present a guarantee on the efficiency of PSGD (Algorithm 2) for finding a
second-order stationary point. We make the following choices of parameters for Algorithm 2:
η = Θ˜
 1
 · N

, r = Θ( ˜ ϵ
√
N), where N = 1 + min
⎧
⎨
⎩
σ2
ϵ2 + ˜
2

√ρϵ ,
σ2
d
ϵ2
⎫
⎬
⎭
, (4)
where , ρ, σ, ˜
 are the parameters defined in Assumption A, B, C.
Theorem 4.4. Let the function f satisfy Assumption A and assume that the stochastic gradient
g satisfies Assumption B (or Assumption C optionally). For any ϵ, δ > 0, the PSGD algorithm (Algorithm 2), with parameter (η,r) chosen as in Equation (4), will visit an ϵ-second-order stationary point
at least once in the following number of iterations, with probability at least 1 − δ:
O˜

(f (x0) − f )
ϵ2 · N

.
We note that Remark 4.2 and Remark 4.3 apply directly to Theorem 4.4.
Theorem 4.4 provides a result for both the scenario in which Assumption C holds and when it
does not. In the former case, taking ϵ such that σ2/ϵ2 ≥ ˜
2/(
√ρϵ ), we have that N ≈ 1 + σ2/ϵ2.
Our results then show that perturbed SGD finds a second-order stationary points in O˜ (ϵ−4) iterations, which matches Theorem 2.10 up to logarithmic factors.
In the general case where Assumption C does not hold ( ˜
 = ∞), we have N = 1 + σ2
d/ϵ2, and
Theorem 4.4 guarantees that PSGD finds an ϵ-second-order stationary point in O˜ (dϵ−4) iterations.
Comparing to Theorem 2.10, we see that the algorithm pays an additional cost that is linear in
dimension d.
Finally, Theorem 4.4 can be easily extended to the mini-batch setting, where multiple samples are
used per iteration to compute the stochastic gradient. This reduces the variance of the stochastictiy
in each iteration while the gradients for multiple samples can be efficiently computed in parallel.
We choose parameters in this setting as follows:
η = Θ˜
 1
 · M

, r = Θ( ˜ ϵ
√
M), where M = 1 +
1
m min
⎧
⎨
⎩
σ2
ϵ2 + ˜
2

√ρϵ ,
σ2
d
ϵ2
⎫
⎬
⎭
. (5)
Corollary 4.5 (Mini-batch version). Let the function f satisfy Assumption A and assume that
the stochastic gradient g satisfies Assumption B (or C optionally). Then, for any ϵ, δ,m > 0, the minibatch PSGD algorithm (Algorithm 3), with parameters (η,r) chosen as in Equation (5), will visit an
ϵ-second-order stationary point at least once in the following number of iterations, with probability
at least 1 − δ:
O˜

(f (x0) − f )
ϵ2 · M

.
Corollary 4.5 says that if the mini-batch size m is not too large—m ≤ N, where N is defined
in Equation (4)—then mini-batch PSGD will reduce the number of iterations linearly, while not
increasing the total number of stochastic gradient queries.
Journal of the ACM, Vol. 68, No. 2, Article 11. Publication date: February 2021.                   
11:12 C. Jin et al.
ALGORITHM 4: Perturbed Gradient Descent (Variant)
Input: x0, step size η, perturbation radius r, time interval T , tolerance ϵ.
tperturb = 0
for t = 0, 1,...,T do
if ∇f (xt ) ≤ ϵ and t − tperturb > T then
xt ← xt − ηξt, (ξt ∼ Uniform(B0 (r))); tperturb ← t
xt+1 ← xt − η∇f (xt )
5 PROOFS
We provide full proofs of our main results, Theorem 4.4 and Corollary 4.5, in Appendix B. (Theorem 4.1 follows directly from the proof of Theorem 4.4 by setting σ = 0). These proofs require novel
concentration inequalities and other tools from stochastic analysis to handle the relatively complex way in which stochasticity interacts with geometry in the neighborhood of saddle points.
In the current section, we circumvent some of these complexities by presenting a conceptually
straightforward proof for an algorithm that is a variant of PGD. This algorithm, summarized in
Algorithm 4, removes some of the stochasticity of PGD by restricting the way in which perturbation noise is added. The algorithm is more complex than PGD and is less preferred in practice
especially due to the requirement of an additional hyperparameter T . However, the proof of Algorithm 4 is streamlined, allowing the core concepts underlying the full proof to be conveyed more
simply.
The following theorem is the specialization of Theorem 4.1 to the setting of Algorithm 4.
Theorem 5.1. Let f satisfy Assumption A and define Δf := f (x0) − f . For any ϵ, δ > 0, the PGD
(Variant) algorithm (Algorithm 4), with parameters η,r, T chosen as in Equation (6) and with ι =
c · log(dΔf /(ρϵδ )), has the property that at least one half of its iterations of will be ϵ-second order
stationary points, after the following number of iterations, and with probability at least 1 − δ:
O˜

Δf
ϵ2

.
Here c is an absolute constant.
We proceed to the proof of the theorem. We first specify the choice of hyperparameters η,r, and
T and two quantities F and S that are frequently used:
η = 1

, r = ϵ
400ι3 , T =
√ρϵ · ι, F = 1
50ι3

ϵ3
ρ , S = 1
4ι
ϵ
ρ
. (6)
Our high-level proof strategy is a proof by contradiction: When the current iterate is not an
ϵ-second-order stationary point, it must either have a large gradient or have a strictly negative
Hessian, and we prove that in either case, PGD must yield a significant decrease in function value
in a controlled number of iterations. Also, since the function value cannot decrease more than
f (x0) − f , we know that the total number of iterates that are not ϵ-second order stationary points
cannot be very large.
First, we bound the rate of decrease when the gradient is large.
Lemma 5.2 (Descent Lemma). If f (·) satisfies Assumption A and η ≤ 1/, then the gradient descent sequence {xt } satisfies:
f (xt+1) − f (xt ) ≤ −η∇f (xt )2
/2.
Journal of the ACM, Vol. 68, No. 2, Article 11. Publication date: February 2021.             
On Nonconvex Optimization for Machine Learning 11:13
Proof. Due to the -gradient Lipschitz assumption, we have:
f (xt+1) ≤ f (xt ) + ∇f (xt ), xt+1 − xt +

2
xt+1 − xt 2
= f (xt ) − η∇f (xt )2 + η2
2 ∇f (xt )2 ≤ f (xt ) − η
2
∇f (xt )2
.
We now show that if the starting point has a strictly negative eigenvalue of the Hessian, then
adding a perturbation and following by gradient descent will yield a significant decrease in function value in T iterations.
Lemma 5.3 (Escaping Saddle Points). Assume that f (·) satisfies Assumption A, x˜ satisfies
∇f (x˜) ≤ ϵ, and λmin(∇2 f (x˜)) ≤ −√ρϵ. Let x0 = x˜ + ηξ(ξ ∼ Uniform(B0 (r))). Run gradient descent starting from x0. This yields:
P (f (xT ) − f (x˜) ≤ −F/2) ≥ 1 −
√
d
√ρϵ · ι
2
28−ι
,
where xT is the T th gradient descent iterate starting from x0.
To prove this, we need to prove two lemmas, and the major simplification over Jin et al. [27]
comes from the following lemma that says that if the function value does not decrease too much
over t iterations, then all iterates {xτ }
t
τ =0 will remain in a small neighborhood of x0.
Lemma 5.4 (Improve or Localize). Under the setting of Lemma 5.2, for any t ≥ τ > 0:
xτ − x0  ≤ 	
2ηt(f (x0) − f (xt )).
Proof. Given the gradient update, xt+1 = xt − η∇f (xt ), we have that for any τ ≤ t:
xτ − x0  ≤ 
t
τ =1
xτ − xτ −1
(1)
≤
⎡
⎢
⎢
⎢
⎢
⎣
t

t
τ =1
xτ − xτ −1 2
⎤
⎥
⎥
⎥
⎥
⎦
1
2
=
⎡
⎢
⎢
⎢
⎢
⎣
η2
t

t
τ =1
∇f (xτ −1)2
⎤
⎥
⎥
⎥
⎥
⎦
1
2 (2)
≤
	
2ηt(f (x0) − f (xt )),
where step (1) uses Cauchy-Schwarz inequality and step (2) is due to Lemma 5.2.
Second, we show that the region in which GD will get stuck for at least T iterations if initialized
there (which we refer to as the “stuck region”) is thin. We show this by tracking any pair of points
that differ only in an escaping direction and are at least ω far apart. We show that at least one
sequence of the two GD sequences initialized at these points is guaranteed to escape the saddle
point with high probability, so that the width of the stuck region along an escaping direction is at
most ω.
Lemma 5.5 (Coupling Seqence). Suppose f (·) satisfies Assumption A and x˜ satisfies
λmin(∇2 f (x˜)) ≤ −√ρϵ. Let {xt }, {x
t } be two gradient descent sequences that satisfy: (1)
max{x0 − x˜ , x
0 − x˜ } ≤ ηr; and (2) x0 − x
0 = ηr0e1, where e1 is the minimum eigenvector of
∇2 f (x˜) and r0 > ω := 22−ι
S . Then:
min{f (xT ) − f (x0), f (x
T ) − f (x
0)}≤−F .
Journal of the ACM, Vol. 68, No. 2, Article 11. Publication date: February 2021.                               
11:14 C. Jin et al.
Proof. Assume the contrary, that is, min{f (xT ) − f (x0), f (x
T ) − f (x
0)} > −F. Lemma 5.4
implies localization of both sequences around x˜; that is, for any t ≤ T :
max{xt − x˜ , x
t − x˜ } ≤ max{xt − x0 , x
t − x
0 } + max{x0 − x˜ , x
0 − x˜ }
≤
	
2ηT F + ηr ≤ S , (7)
where the last step is due to our choice of η,r, T , F, S , as in Equation (6), and /√ρϵ ≥ 1.1
However, we can write the update equation for the difference xˆt := xt − x
t as:
xˆt+1 = xˆt − η[∇f (xt ) − ∇f (x
t )] = (I − ηH )xˆt − ηΔt xˆt
= (I − ηH )
t+1
xˆ 0  p(t+1)
−η

t
τ =0
(I − ηH )
t−τ Δτ xˆτ
 q(t+1)
,
where H = ∇2 f (x˜) and Δt =  1
0 [∇2 f (x
t + θ (xt − x
t ) − H]dθ. We note that p(t) arises from the
initial difference xˆ0, and q(t) is an error term that arises from the fact that the function f is not
quadratic. We now use induction to show that the error term q(t) is always small compared to the
leading term p(t). That is, we wish to show:
q(t)≤p(t)/2, t ∈ [T ].
The claim is true for the base case t = 0 as q(0) = 0 ≤ xˆ 0 /2 = p(0)/2. Now suppose the
induction claim is true up to t. Denote λmin(∇2 f (x0)) = −γ . Note that xˆ 0 lies in the direction of the
minimum eigenvector of ∇2 f (x0). Thus for any τ ≤ t, we have:
xˆτ ≤p(τ ) + q(τ ) ≤ 2p(τ ) = 2(I − ηH )
τ xˆ 0  = 2(1 + ηγ )
τ ηr0.
By the Hessian Lipschitz property, we have Δt  ≤ ρ max{xt − x˜ , x
t − x˜ } ≤ ρS , therefore:
q(t + 1) = η

t
τ =0
(I − ηH )
t−τ Δτ xˆτ  ≤ ηρS

t
τ =0
(I − ηH )
t−τ xˆτ
≤ 2ηρS

t
τ =0
(1 + ηγ )
t
ηr0 ≤ 2ηρS T (1 + ηγ )
t
ηr0 ≤ 2ηρS T p(t + 1),
where the second-to-last inequality usest + 1 ≤ T . By our choice of the hyperparameter in Equation (6), we have 2ηρS T ≤ 1/2, which finishes the inductive proof.
Finally, the induction claim implies:
max{xT − x0 , x
T − x0 } ≥1
2
xˆ(T ) ≥ 1
2
[p(T )−q(T )] ≥
1
4
[p(T )
= (1 + ηγ )
T ηr0
4
(1)
≥ 2ι−2
ηr0 > S ,
where step (1) uses the fact (1 + x)
1/x ≥ 2 for any x ∈ (0, 1]. This contradicts the localization property of Equation (7), which finishes the proof.
Equipped with Lemma 5.4 and Lemma 5.5, we are ready to prove Lemma 5.3.
1We note that when /√ρϵ < 1, ϵ-second-order stationary points are equivalent to ϵ-first-order stationary points due to
the function f being -gradient Lipschitz. In this case, the problem of finding ϵ-second-order stationary points becomes
straightforward.
Journal of the ACM, Vol. 68, No. 2, Article 11. Publication date: February 2021.                                                                
On Nonconvex Optimization for Machine Learning 11:15
Fig. 1. Pertubation ball in 3D and “thin
pancake” shape stuck region.
Fig. 2. Pertubation ball in 2D and “narrow
band” stuck region under gradient flow.
Proof of Lemma 5.3. Recall x0 ∼ Uniform(Bx˜ (ηr)). We refer to Bx˜ (ηr) as the perturbation ball
and define the stuck region within the perturbation ball to be the set of points starting from which
GD requires more than T steps to escape:
Xstuck := {x ∈ Bx˜ (ηr)|{xt } is GD sequence with x0 = x, and f (xT ) − f (x0) > −F }.
See Figure 1 and Figure 2 for illustrations. Although the shape of the stuck region can be very
complicated, according to Lemma 5.5 we know that the width of Xstuck along the e1 direction is at
most ηω. That is, Vol(Xstuck) ≤ Vol(Bd−1 0 (ηr))ηω. Therefore:
P (x0 ∈ Xstuck) = Vol(Xstuck)
Vol(Bd
x˜ (ηr))
≤ ηω × Vol(Bd−1 0 (ηr))
Vol(Bd
0 (ηr)) = ω
r
√
π
Γ( d
2 + 1)
Γ( d
2 + 1
2 )
≤
ω
r ·
d
π ≤

√
d
√ρϵ · ι
2
28−ι
.
On the event {x0  Xstuck}, due to our parameter choice in Equation (6), we have:
f (xT ) − f (x˜) = [f (xT ) − f (x0)] + [f (x0) − f (x˜)] ≤ −F + ϵηr +
η2
r 2
2
≤ −F/2.
This finishes the proof.
With Lemma 5.2 and Lemma 5.3 in hand, it is not hard to establish Theorem 5.1.
Proof of Theorem 5.1. First, we set the total number of iterations T to be
T = 8 max  (f (x0) − f )T
F ,
(f (x0) − f )
ηϵ2

= O

(f (x0) − f )
ϵ2 · ι
4

.
Next, we choose ι = c · log(
dΔf
ρϵδ ), with a large-enough absolute constant c such that:
(T
√
d/
√ρϵ ) · ι
2
28−ι ≤ δ .
We then argue that, with probability 1 − δ, Algorithm 4 will add a perturbation at most T/(4T )
times. This is because if otherwise, then we can appeal to Lemma 5.3 every time we add a perturbation and conclude:
f (xT ) ≤ f (x0) −TF/(4T ) < f ,
Journal of the ACM, Vol. 68, No. 2, Article 11. Publication date: February 2021.             
11:16 C. Jin et al.
which cannot happen. Finally, excluding those iterations that are within T steps after adding
perturbations, we still have 3T/4 steps left. They are either large gradient steps, ∇f (xt ) ≥ ϵ, or
ϵ-second-order stationary points. Within them, we know that the number of large gradient steps
cannot be more than T/4. This is true because if otherwise, by Lemma 5.2:
f (xT ) ≤ f (x0) −Tηϵ2
/4 < f ,
which again cannot happen. Therefore, we conclude that at least T/2 of the iterates must be ϵsecond-order stationary points.
6 CONCLUSIONS
We have shown that simple perturbed versions of GD and SGD escape saddle points and find
second-order stationary points in essentially the same time that classical GD and SGD take to find
first-order stationary points. The overheads are only polylogarithmic factors in dimensionality in
both the non-stochastic setting and the stochastic setting with Lipschitz stochastic gradient. In the
general stochastic setting, the overhead is a linear factor in dimension.
Combined with previous literature that shows that all second-order stationary points are global
optima for a broad class of nonconvex optimization problems in machine learning and signal processing, our results directly provide efficient guarantees for solving those nonconvex problem via
simple local search approaches. We now discuss several possible future directions, and further
connections to other fields.
Optimal rates for finding second-order stationary points. Carmon et al. [13] have presented lower
bounds that imply that GD achieves the optimal rate for finding stationary points for gradient
Lipschitz functions. In our setting, we additionally assume that the Hessian is Lipschitz. This implies that GD is no longer necessarily an optimal algorithm. While our results show that variants
of GD are efficient in this setting, one would additionally like to know whether they are close to
optimality.
Focusing on first-order algorithms for functions with both Lipschitz gradient and Lipschitz
Hessian, the best known gradient query complexity for finding second-order stationary points is
O˜ (ϵ−1.75) [1, 12, 29], while the best existing lower bound is Ω(ϵ−12/7) by Carmon et al. [14]. Note
that this lower bound is restricted to deterministic algorithms, and thus does not apply to most
existing algorithms for escaping saddle points as they are all randomized algorithms. For the stochastic setting with Lipschitz stochastic gradients, the best-known query complexity is O˜ (ϵ−3) [20,
53], while a matching lower bound Ω(ϵ−3) has been obtained in recent work [5, 6]
2. See Appendix A for further discussion of the literature.
Escaping high-order saddle points. The current article focuses on escaping strict saddle points
and finding second-order stationary points. More generally, one can define nth-order stationary
points as points that satisfies the KKT necessary conditions for being local minima up to nth-order
derivatives. It becomes more challenging to find nth-order stationary points as n increases, since it
is necessary to escape higher-order saddle points. In terms of worst-case efficiency, Nesterov [38]
rules out the possibility of efficient algorithms for finding nth-order stationary points for all n ≥ 4,
showing that the problem is NP-hard. Anandkumar and Ge [4] present a third-order algorithm that
finds third-order stationary points in polynomial time. It remains open whether simple variants
of GD can also find third-order stationary points efficiently. It is unlikely that the overhead will
2Despite the original hard instance in Arjevani et al. [6] does not satify the Hessian Lipschitz condition, it can be modified
to satisfy such condition using the rescaling technique in Arjevani et al. [5], while giving the same lower bound Ω(ϵ−3).
Journal of the ACM, Vol. 68, No. 2, Article 11. Publication date: February 2021.    
On Nonconvex Optimization for Machine Learning 11:17
still be small or only logarithmic in this case, but it is not clear what to expect for the overhead. A
related question is to identify applications where third-order stationarity is needed, in addition to
second-order stationarity, to achieve global optimality.
Connection to gradient Langevin dynamics. The Bayesian counterpart of SGD is the Langevin
Monte Carlo (LMC) algorithm [45], which performs the following iterative update:
xt+1 = xt − η(∇f (xt ) +
	
2/(ηβ)wt ), where wt ∼ N (0, I).
Here β is known as the inverse temperature. When the step size η goes to zero, the distribution of
the LMC iterates is known to converge to the stationary distribution μ(x) ∝ e−β f (x) [45].
While the LMC algorithm is superficially very close to stochastic gradient descent, the goals for
the two algorithms are quite different.
• Convergence: While the focus of the optimization literature is to find stationary points, the
goal of the LMC algorithm is to converge to a stationary distribution (i.e., to mix rapidly).
• Scaling of Noise: The scaling of the stochasticity in SGD—and in particular the size of
the perturbation that we consider in the current article—is much smaller than the scaling
considered in the LMC literature. Running our algorithm is equivalent to running LMC with
temperature β−1 ∝ d−1. In this low-temperature or small-noise regime, the algorithm can
no longer mix efficiently for smooth nonconvex functions, as it requires Ω(ed ) steps in the
worst case [10]. However, with this small amount of noise, the algorithm can still perform
local search efficiently, and can find a second-order stationary point in a small number of
iterations, as shown in Theorem 4.1.
Recent work of Zhang et al. [51] studied the time that LMC takes to hit a second-order stationary
point as a criterion for convergence, instead of the traditional mixing time to a stationary distribution. In this analysis, the runtime is no longer exponential, but it is still polynomially dependent
on dimension d with large degree.
On the necessity of adding perturbations. We have shown that adding perturbations to the iterations of GD or SGD allows these algorithms to escape saddle points efficiently. As an alternative,
one can also simply run GD with random initialization, and try to escape saddle points using only
the randomness due to the initialization. Although this alternative algorithm exhibits asymptotic
convergence [31], it does not yield efficient convergence in general. Du et al. [19] shows that even
with fairly natural random initialization schemes and non-pathological functions, randomly initialized GD can be significantly slowed by saddle points, taking exponential time to escape them.
APPENDICES
A TABLES OF RELATED WORK
In Table 2 and Table 3, we present a detailed comparison of our results with other related work in
both non-stochastic and stochastic settings. See Section 1.1 for the full text descriptions. We note
that our algorithms are simple variants of standard GD and SGD, which are the simplest among
all the algorithms listed in the table.
Journal of the ACM, Vol. 68, No. 2, Article 11. Publication date: February 2021.
11:18 C. Jin et al.
Table 2. A Summary of Related Work on First-order Algorithms to Find Second-order
Stationary Points in Non-stochastic Settings
Algorithm Iterations Simplicity
Noisy GD [22] d4poly(ϵ−1)
single-loop Normalized GD [33] O(d3 · poly(ϵ−1))
PGD (conference version of this work) [27] O˜ (ϵ−2)
†Perturbed AGD [29] O˜ (ϵ−1.75)
FastCubic [1] O˜ (ϵ−1.75)
Carmon et al. [12] O˜ (ϵ−1.75) double-loop
Carmon and Duchi [11] O˜ (ϵ−2)
This table only highlights the dependencies on d and ϵ .
† denotes followup work on the conference
version of this article [27].
Table 3. A Summary of Related Work on First-order Algorithms to Find Second-order
Stationary Points in the Stochastic Setting
Algorithm Iterations (with
Assumption C)
Iterations (no
Assumption C) Simplicity
Noisy GD [22] d4poly(ϵ−1) d4poly(ϵ−1)
single-loop CNC-SGD [18] O˜ (d4ϵ−5) O˜ (d4ϵ−5)
PSGD (this work) O˜ (ϵ−4) O˜ (dϵ−4) ∗SGD with averaging [21] O˜ (ϵ−3.5) ×
Natasha 2 [2] O˜ (ϵ−3.5) ×
double-loop Stochastic Cubic [49] O˜ (ϵ−3.5) ×
SPIDER [20] O˜ (ϵ−3) ×
SRVRC [53] O˜ (ϵ−3) ×
This table only highlights the dependencies on d and ϵ. ∗ denotes independent work.
B PROOFS FOR STOCHASTIC SETTING
In this section, we provide proofs for our main results—Theorem 4.4 and Corollary 4.5. Theorem 4.1
can be proved as a special case of Theorem 4.4 by taking σ = 0.
B.1 Notation
Recall the update equation of Algorithm 2 is xt+1 ← xt − η(g(xt ; θt ) + ξt ), where ξt ∼
N (0, (r 2/d)I). Throughout this section, we denote ζt := g(xt ; θt ) − ∇f (xt ), as the noise part within
the stochastic gradient. For simplicity, we also denote ˜
ζt := ζt + ξt , which is the summation of noise
from the stochastic gradient and the injected perturbation, and σ˜ 2 := σ2 + r 2. Then the update
equation can be rewritten as xt+1 ← xt − η(∇f (xt ) + ˜
ζt ). We also denote Ft = σ (ζ0,ξ0,..., ζt,ξt )
as the corresponding filtration up to time step t. We choose parameters in Algorithm 2 as follows:
η = 1
ι9 · N, r = ι · ϵ
√
N, T := ι
η
√ρϵ , F := 1
ι5

ϵ3
ρ , S := 2
ι2
ϵ
ρ
, (8)
where N and the log factor ι are defined as:
N = 1 + min
⎧
⎨
⎩
σ2
ϵ2 + ˜
2

√ρϵ ,
σ2
d
ϵ2
⎫
⎬
⎭
, ι = μ · log
dΔf N
ρϵδ
.
Journal of the ACM, Vol. 68, No. 2, Article 11. Publication date: February 2021.      
On Nonconvex Optimization for Machine Learning 11:19
Here, μ is a sufficiently large absolute constant to be determined later. Note also that throughout
this section c denotes an absolute constant that does not depend on the choice of μ. Its value may
change from line to line.
B.2 Descent Lemma
We first prove that the change in the function value can be always decomposed into the decrease
due to the magnitudes of gradients and the possible increase due to randomness in both the stochastic gradients and the perturbations.
Lemma B.1 (Descent Lemma). Under Assumption A, B, there exists an absolute constant c
such that, for any fixed t,t0, ι > 0, if η ≤ 1/, then with at least 1 − 4e−ι probability, the sequence
PSGD(η,r) (Algorithm 2) satisfies: (denote σ˜ 2 = σ2 + r 2)
f (xt0+t ) − f (xt0 ) ≤ −η
8

t−1
i=0
∇f (xt0+i )2 + c · ησ˜ 2 (ηt + ι).
Proof. Since Algorithm 2 is Markovian, the operations in each iteration do not depend on time
step t. Thus, it suffices to prove Lemma B.1 for special case t0 = 0. Recall the update equation:
xt+1 ← xt − η(∇f (xt ) + ˜
ζt ),
where ˜
ζt = ζt + ξt . By assumption, we know ζt |Ft−1 is zero-mean nSG(σ ) (see Definition C.1).
Also ξt |Ft−1 comes from N (0, (r 2/d)I), and thus by Lemma C.2 it is zero-mean nSG(c · r) for some
absolute constant c. By a Taylor expansion and the assumptions of an -gradient Lipschitz and
η ≤ 1/, we have:
f (xt+1) ≤ f (xt ) + ∇f (xt ), xt+1 − xt +

2
xt+1 − xt 2
≤ f (xt ) − η∇f (xt ), ∇f (xt ) + ˜
ζt + η2
2

3
2
∇f (xt )2 + 3 ˜
ζt 2

≤ f (xt ) − η
4
∇f (xt )2 − η∇f (xt ), ˜
ζt +
3
2
η2
 ˜
ζt 2
.
Summing over this inequality, we have the following:
f (xt ) − f (x0) ≤ −η
4

t−1
i=0
∇f (xi )2 − η

t−1
i=0
∇f (xi ), ˜
ζi +
3
2
η2


t−1
i=0
 ˜
ζi 2
. (9)
For the second term on the right-hand side, applying Lemma C.8, there exists an absolute constant
c, such that, with probability 1 − 2e−ι
:
−η

t−1
i=0
∇f (xi ), ˜
ζi ≤ η
8

t−1
i=0
∇f (xi )2 + cησ˜ 2
ι.
For the third term on the right-hand side of Equation (9), applying Lemma C.7, with probability
1 − 2e−ι
:
3
2
η2


t−1
i=0
 ˜
ζi 2 ≤ 3η2


t−1
i=0
(ζi 2 + ξi 2) ≤ cη2
σ˜ 2 (t + ι).
Substituting these inequalities into Equation (9), and noting the fact η ≤ 1/, we have with probability 1 − 4e−ι
:
f (xt ) − f (x0) ≤ −η
8

t−1
i=0
∇f (xi )2 + cησ˜ 2 (ηt + ι).
This finishes the proof.
Journal of the ACM, Vol. 68, No. 2, Article 11. Publication date: February 2021.                                        
11:20 C. Jin et al.
The descent lemma allows us to show the following “Improve or Localize” phenomenon for
perturbed SGD. That is, with high probability over a small number of iterations, either the function
value decreases significantly, or the iterates stay within a small local region.
Lemma B.2 (Improve or Localize). Under the same setting as in Lemma B.1, with probability at
least 1 − 8dt · e−ι
, the sequence PSGD(η,r) (Algorithm 2) satisfies:
∀τ ≤ t : xt0+τ − xt0 2 ≤ cηt · [f (xt0 ) − f (xt0+τ ) + ησ˜ 2 (ηt + ι)].
Proof. By a similar argument as in the proof of Lemma B.1, it suffices to prove Lemma B.2
in the special case t0 = 0. According to Lemma B.1, with probability 1 − 4e−ι
, for some absolute
constant c:

t−1
i=0
∇f (xi )2 ≤
8
η
[f (x0) − f (xt )] + cσ˜ 2 (ηt + ι).
Therefore, for any fixed τ ≤ t, with probability 1 − 8d · e−ι
,
xτ − x0 2 = η2







τ −1
i=0
(∇f (xi ) + ˜
ζi )






2
≤ 2η2
⎡
⎢
⎢
⎢
⎢
⎢
⎣







τ −1
i=0
∇f (xi )2 +

τ −1
i=0
˜
ζi






2⎤
⎥
⎥
⎥
⎥
⎥
⎦
(1)
≤ 2η2
t

τ −1
i=0
∇f (xi )2 + cη2
σ˜ 2
tι ≤ 2η2
t

t−1
i=0
∇f (xi )2 + cη2
σ˜ 2
tι
≤ cηt[f (x0) − f (xt ) + ησ˜ 2 (ηt + ι)].
where in step (1) we use the Cauchy-Schwarz inequality and Lemma C.5. Finally, applying a union
bound for all τ ≤ t, we finish the proof.
B.3 Escaping Saddle Points
Lemma B.1 shows that large gradients contribute to the fast decrease of the function value. In this
subsection, we will show that starting in the vicinity of strict saddle points will also enable PSGD
to decrease the function value rapidly. Concretely, this entire subsection will be devoted to proving
the following lemma:
Lemma B.3 (Escaping Saddle Point). Given Assumption A, B, there exists an absolute constant
cmax such that, for any fixed t0 > 0, ι > cmax log(

d/(ρϵ )), if η,r, F, T are chosen as in Equation (8), and xt0 satisfies ∇f (xt0 ) ≤ ϵ and λmin(∇2 f (xt0 )) ≤ −√ρϵ, then the sequence PSGD(η,r)
(Algorithm 2) satisfies:
P (f (xt0+T ) − f (xt0 ) ≤ 0.1F ) ≥ 1 − 4e−ι and
P (f (xt0+T ) − f (xt0 ) ≤ −F ) ≥ 1/3 − 5dT 2 · log(S √
d/(ηr))e−ι
.
Since Algorithm 2 is Markovian, the operations in each iterations do not depend on time step t.
Thus, it suffices to prove Lemma B.3 for special case t0 = 0. To prove this lemma, we first need to
introduce the concept of a coupling sequence.
Notation. Throughout this subsection, we let H := ∇2 f (x0), e1 be the minimum eigendirection
of H, and γ := λmin(H ). We also let P−1 be the projection onto the subspace complement of e1.
Definition B.4 (Coupling Sequence). Consider sequences {xi} and {x
i} that are obtained as separate runs of the PSGD (algorithm 2), both starting from x0. They are coupled if both sequences
share the same randomness P−1ξτ and θτ , while in e1 direction we have e
1 ξτ = −e
1 ξ
τ .
Journal of the ACM, Vol. 68, No. 2, Article 11. Publication date: February 2021.                   
On Nonconvex Optimization for Machine Learning 11:21
The first thing we can show is that if the function values of both sequences do not exhibit a
sufficient decrease, then both sequences are localized in a small ball around x0 within T iterations.
Lemma B.5 (Localization). Under the notation of Lemma B.6, we have:
P (min{f (xT ) − f (x0),f (x
T ) − f (x0)}≤−F, or
∀t ≤ T : max{xt − x0 2
, x
t − x0 2} ≤ S 2) ≥ 1 − 16dT · e−ι
.
Proof. This lemma follows from applying Lemma B.2 on both sequences and using a union
bound.
The overall proof strategy for Lemma B.3 is to show that localization happens with a very small
probability, thus at least one of the sequence must have sufficient descent. To prove this, we study
the dynamics of the difference of the coupling sequence.
Lemma B.6 (Dynamics of the Coupling Seqence Difference). Consider coupling sequences
{xi} and {x
i} as in Definition B.4 and let xˆt := xi − x
i . Then xˆt = −qh (t) − qsд (t) − qp (t), where:
qh (t) := η

t−1
τ =0
(I − ηH )
t−1−τ Δτ xˆτ , qsд (t) := η

t−1
τ =0
(I − ηH )
t−1−τ ˆ
ζτ , qp (t) := η

t−1
τ =0
(I − ηH )
t−1−τ ˆ
ξτ .
Here Δt :=  1
0 ∇2 f (ψxt + (1 −ψ )x
t )dψ − H, and ˆ
ζτ := ζτ − ζ 
τ , ˆ
ξτ := ξτ − ξ
τ .
Proof. Recall ζi = g(xi ; θi ) − ∇f (xi ), thus, we have the update formula:
xt+1 = xt − η(g(xt ; θt ) + ξt ) = xt − η(∇f (xt ) + ζt + ξt ).
Taking the difference between {xi} and {x
i}:
xˆt+1 = xt+1 − x
t+1 = xˆt − η(∇f (xt ) − ∇f (x
t ) + ζt − ζ 
t + (ξt − ξ
t ))
= xˆt − η[(H + Δt )xˆt + ˆ
ζt + ˆ
ξt] = (I − ηH )xˆt − η[Δt xˆt + ˆ
ζt + e1e
1 ˆ
ξt]
= − η


τ =0
t(I − ηH )
t−τ (Δτ xˆτ + ˆ
ζτ + ˆ
ξτ )),
where Δt :=  1
0 ∇2 f (ψxt + (1 −ψ )x
t )dψ − H. This finishes the proof.
At a high level, we will show that with constant probability, qp (t) is the dominating term that
controls the behavior of the dynamics, and qh (t) and qsд (t) will stay small compared to qp (t). To
show this, we prove the following three lemmas.
Lemma B.7. Denote α (t) := [
t−1 τ =0 (1 + ηγ )
2(t−1−τ )
]
1
2 and β (t) := (1 + ηγ )
t /
√2ηγ . If ηγ ∈ [0, 1],
then (1) α (t) ≤ β (t) for any t ∈ N; and (2) α (t) ≥ β (t)/
√
3 for t ≥ ln(2)/(ηγ ).
Proof. By summing a geometric sequence:
α2 (t) :=

t−1
τ =0
(1 + ηγ )
2(t−1−τ ) = (1 + ηγ )
2t − 1
2ηγ + (ηγ )2 .
Thus, the claim that α (t) ≤ β (t) for any t ∈ N follows immediately. However, note that for
t ≥ ln(2)/(ηγ ), we have (1 + ηγ )
2t ≥ 22 ln 2 ≥ 2, where the second claim follows by a short
calculation.
Journal of the ACM, Vol. 68, No. 2, Article 11. Publication date: February 2021.       
11:22 C. Jin et al.
Lemma B.8. Under the notation of Lemma B.6 and Lemma B.7, letting −γ := λmin(H ), we have
∀t > 0:
P (qp (t) ≤ cβ (t)ηr
√
d ·
√
ι) ≥ 1 − 2e−ι
P (qp (T ) ≥ β (T )ηr
10√
d ) ≥
2
3
.
Proof. Note that ˆ
ξτ is one-dimensional Gaussian random variable with standard deviation
2r/
√
d along the e1 direction. As an immediate result, η t
τ =0 (I − ηH )
t−τ ˆ
ξτ is also a onedimensional Gaussian distribution, since the summation of Gaussians is again Gaussian. Finally
note that e1 is an eigendirection of H with corresponding eigenvalue −γ , and by Lemma B.7
we have α (t) ≤ β (t). Then, the first inequality immediately follows from the standard concentration of Gaussian measure, and the second inequality follows from the fact if Z ∼ N (0, σ2), then
P (|Z | ≤ λσ ) ≤ 2λ/
√
2π ≤ λ.
Lemma B.9. There exists an absolute constant cmax such that, for any ι ≥ cmax, under the notation
of Lemma B.6 and Lemma B.7, and letting −γ := λmin(H ), we have:
P (min{f (xT ) − f (x0),f (x
T ) − f (x0)}≤−F, or
∀t ≤ T : qh (t) + qsд (t) ≤ β (t)ηr
20√
d ) ≥ 1 − 10dT 2 · log 

S √
d
ηr


e−ι
.
Proof. For simplicity, we denote E as the event {∀τ ≤ t : max{xτ − x0 2, x
τ − x0 2} ≤ S 2}.
We use induction to prove following claim for any t ∈ [0, T ]:
P (E ⇒ ∀τ ≤ t : qh (τ ) + qsд (τ ) ≤ β (τ )ηr
20√
d ) ≥ 1 − 10dT t · log 

S √
d
ηr


e−ι
.
Then Lemma B.9 follows directly from combining Lemma B.5 and the induction claim.
Clearly for the base case t = 0, the claim holds trivially, as qsд (0) = qh (0) = 0. Suppose the claim
holds for t, then by Lemma B.8, with probability at least 1 − 2T e−ι
, we have for any τ ≤ t:
xˆτ  ≤ ηqh (τ ) + qsд (τ ) + ηqp (τ ) ≤ cβ (τ )ηr
√
d ·
√
ι.
Then, under the condition max{xτ − x0 2, x
τ − x0 2} ≤ S 2, by the Hessian Lipschitz property,
we have Δτ  =
 1
0 ∇2 f (ψxτ + (1 −ψ )x
τ )dψ −H ≤ ρ max{xτ − x0 , x
τ − x0 } ≤ ρS . This
gives bounds on qh (t + 1) terms as:
qh (t + 1) ≤ η

t
τ =0
(1 + ηγ )
t−τ ρS xˆτ  ≤ ηρS T cβ (t)ηr
√
d
≤ β (t)ηr
40√
d
,
where the last step is due to ηρS T = 1/ι by Equation (8). By picking ι larger than the absolute
constant 40c, we have cηρS T ≤ 1/40.
Recall also that ˆ
ζτ |Fτ −1 is the summation of a nSG(σ ) random vector and a nSG(c · r) random
vector. By Lemma C.5, we know that with probability at least 1 − 4de−ι
:
qsд (t + 1) ≤ cβ (t + 1)ησ√
ι
However, when assumption C is avaliable, we also have ˆ
ζτ |Fτ −1 ∼ nSG( ˜
xˆτ ), by applying
Lemma C.6 with B = α2 (t) · η2 ˜
2S 2;b = α2 (t) · η2 ˜
2 · η2
r 2/d, we know with probability at least
Journal of the ACM, Vol. 68, No. 2, Article 11. Publication date: February 2021.                                          
On Nonconvex Optimization for Machine Learning 11:23
1 − 4d · log(S √
d/(ηr)) · e−ι
:
qsд (t + 1) ≤ cη ˜


t
τ =0
(1 + ηγ )2(t−τ ) · max{xˆτ 2,
η2r 2
d }ι ≤ η ˜

√
T ·
cβ (t)ηr
√
d ·
√
ι. (10)
Finally, combining both cases, and by our choice of step size η,r as in Equation (8) with ι large
enough:
qsд (t + 1) ≤ c
β (t)ηr
√
d · min
⎧⎪
⎨
⎪
⎩
η ˜

√
T ι,
σ
√
dι
r
⎫⎪
⎬
⎪
⎭
≤ β (t)r
40√
d
,
the induction follows by the triangle inequality and a union bound.
We are ready to prove Lemma B.3, which is the focus of this subsection.
Proof of Lemma B.3. We first prove the first claim P (f (xT ) − f (x0) ≤ 0.1F ) ≥ 1 − 4e−ι
. Because of our choice of step size and Lemma B.1, we have with probability 1 − 4e−ι
:
f (xT ) − f (x0) ≤ cησ˜ 2 (ηT + ι) ≤ 0.1F,
where the last step is because our choice of parameters in Equation (8) implies cησ˜ 2 (ηT + ι) ≤
2cF/ι and we pick ι to be larger than an absolute constant 20c.
For the second claim, P (f (xT ) − f (x0) ≤ −F ) ≥ 1/3 − 5dT 2 · log(S √
d/(ηr))e−ι
, we consider coupling sequences {xi} and {x
i} as defined in Definition B.4. Given Lemma B.8
and Lemma B.9, we know that with probability at least 2/3 − 10dT 2 · log(S √
d/(ηr))e−ι if
min{f (xT ) − f (x0), f (x
T ) − f (x0)} > −F—i.e., both sequences are stuck around the saddle
point—we must have:
qp (T ) ≥ β (T )ηr
10√
d , qh (T ) + qsд (T ) ≤ β (T )ηr
20√
d .
By Lemma B.6, when ι ≥ c · log(

d/(ρϵ )) for a large absolute constant c, we have:
max{xT − x0 , x
T − x0 } ≥1
2
xˆ(T ) ≥ 1
2
[qp (T )−qh (T ) + qsд (T )]
≥ β (T )ηr
40√
d = (1 + ηγ )
T ηr
40
2ηγd
≤
2ι
ηr
80
ηd
, > S ,
which contradicts with Lemma B.5. Therefore, we can conclude that P (min{f (xT ) −
f (x0), f (x
T ) − f (x0)}≤−F ) ≥ 2/3 − 10dT 2 · log(S √
d/(ηr))e−ι
. We also know that the marginal distribution of xT and x
T is the same, thus they have same probability to escape the saddle
point. That is,
P (f (xT ) − f (x0) ≤ −F ) ≥
1
2
P (min{f (xT ) − f (x0), f (x
T ) − f (x0)}≤−F )
≥1/3 − 5dT 2 · log(S √
d/(ηr))e−ι
.
This finishes the proof.
B.4 Proof of Theorem 4.4
Lemma B.1 and Lemma B.3 describe the speed of decrease in the function values when either large
gradients or strictly negative curvatures are present. Combining them gives the proof for our main
theorem.
Journal of the ACM, Vol. 68, No. 2, Article 11. Publication date: February 2021.                             
11:24 C. Jin et al.
Proof of Theorem 4.4. First, we set the total number of iterations T to be
T = 100 max  (f (x0) − f )T
F ,
(f (x0) − f )
ηϵ2

= O

(f (x0) − f )
ϵ2 · N · ι
9

.
We will show that the following two claims hold simultaneously with probability 1 − δ:
(1) At most T/4 iterates have large gradient; i.e., ∇f (xt ) ≥ ϵ;
(2) At most T/4 iterates are close to saddle points; i.e., ∇f (xt ) ≤ ϵ and λmin(∇2 f (xt )) ≤
−√ρϵ.
Therefore, at least T/2 iterates are ϵ-second-order stationary point. We prove the two claims separately.
Claim 1. Suppose that within T steps, we have more than T/4 iterates for which gradient is large
(i.e., ∇f (xt ) ≥ ϵ). Recall that by Lemma B.1 we have with probability 1 − 4e−ι
:
f (xT ) − f (x0) ≤ −η
8
T

−1
i=0
∇f (xi )2 + cησ˜ 2 (ηT + ι) ≤ −η

Tϵ2
32 − σ˜ 2 (ηT + ι)

.
Note that by our choice of η,r,T and picking ι larger than some absolute constant, we have Tϵ2/32 −
σ˜ 2 (ηT + ι) ≥ Tϵ2/64 and thus f (xT ) ≤ f (x0) −Tηϵ2/64 < f , which is not possible.
Claim 2. We first define the stopping times that allow us to invoke Lemma B.3:
z1 = inf{τ |∇f (xτ ) ≤ ϵ and λmin(f (xτ )) ≤ −√ρϵ }
zi = inf{τ |τ > zi−1 + T and ∇f (xτ ) ≤ ϵ and λmin(f (xτ )) ≤ −√ρϵ }, ∀i > 1.
Clearly, zi is a stopping time, and it is the ith time in the sequence along which we can apply
Lemma B.3. We also let M be the random variable M = max{i|zi + T ≤ T }. We can decompose the
decrease f (xT ) − f (x0) as follows:
f (xT ) − f (x0) =


M
i=1
[f (xzi+T ) − f (xzi )]
 T1
+ [f (xT ) − f (xzM )] + [f (xz1 ) − f (x0)] +
M

−1
i=1
[f (xzi+1 ) − f (xzi+T )]
 T2
.
For the first term T1, by Lemma B.3 and a supermartingale concentration inequality, for each fixed
m ≤ T :
P 


m
i=1
[f (xzi+T ) − f (xzi )] ≤ −(0.9m − c
√
m · ι)F

≥ 1 − 5dT 2
T · log(S √
d/(ηr))e−ι
.
Since the random variable M ≤ T/T ≤ T , by a union bound, we know that with probability 1 −
5dT 2
T 2 · log(S √
d/(ηr))e−ι
:
T1 ≤ −(0.9M − c
√
M · ι)F .
For the second term, by a union bound and Lemma B.1 for all 0 ≤ t1,t2 ≤ T , with probability 1 −
4T 2
e−ι
:
T2 ≤ c · ησ˜ 2 (ηT + 2Mι)
Journal of the ACM, Vol. 68, No. 2, Article 11. Publication date: February 2021.                       
On Nonconvex Optimization for Machine Learning 11:25
Therefore, if withinT steps we have more thanT/4 saddle points, then M ≥ T/4T , and with probaility
1 − 10dT 2
T 2 · log(S √
d/(ηr))e−ι
:
f (xT ) − f (x0) ≤ −(0.9M − c
√
M · ι)F + c · ησ˜ 2 (ηT + 2Mι) ≤ −0.4MF ≤ −0.4TF/T .
This will gives f (xT ) ≤ f (x0) − 0.4TF/T < f , which is not possible.
Finally, it is not hard to verify, by choosing ι = c · log(
dΔf N
ρϵδ ) for a large enough value of the
absolute constant c, we can make both claims hold with probability 1 − δ.
B.5 Proof of Corollary 4.5
Our proofs for PSGD easily generalize to the mini-batch setting.
Proof of Corollary 4.5. The proof is essentially the same as the proof of Theorem 4.4. The
only difference is that, up to a log factor, mini-batch PSGD reduces the variance σ2 and ˜
2 xˆτ 2 in
Equation (10) by a factor of m, where m is the size of the mini-batch.
B.6 Proof of Remark 4.2
The proof of Theorem 4.4 can be easily modified to prove Remark 4.2.
Proof of Remark 4.2. In the proof of Theorem 4.4, we have shown that if we set the total
number of iterations T to be:
T = O

(f (x0) − f )
ϵ2 · N · ι
9

,
where ι = c · log(
dΔf N
ρϵδ ) for a large enough value of the absolute constant c, with at least 1 − δ
probability, at least T/2 iterates are ϵ-second-order stationary point. Therefore, for δ ≤ 1/6, if we
output an iterate uniformly at random, with at least 1/3 probability, then we will output an ϵsecond-order stationary point.
C CONCENTRATION INEQUALITIES
In this section, we present the concentration inequalities required for this article. Please refer to
the technical note [28] for the proofs of Lemmas C.2, C.3, C.5, and C.6.
Recall the definition of a norm-subGaussian random vector.
Definition C.1. A random vector X ∈ Rd is norm-subGaussian (or nSG(σ )), if there exists σ so
that:
P (X − EX ≥ t) ≤ 2e
− t 2
2σ 2 , ∀t ∈ R.
Note that a bounded random vector and a subGaussian random vector are two special cases of
a norm-subGaussian random vector.
Lemma C.2. There exists an absolute constantc so that following random vectors are nSG(c · σ ).
(1) A bounded random vector X ∈ Rd such that X ≤ σ.
(2) A random vector X ∈ Rd , where X = ξ e1 and the random variable ξ ∈ R is σ-subGaussian.
(3) A random vector X ∈ Rd that is (σ/
√
d)-subGaussian.
Second, we have that if X is norm-subGaussian, then its norm square is subExponential, and its
component along a single direction is subGaussian.
Lemma C.3. There is an absolute constant c so that if the random vector X ∈ Rd is zero-mean
nSG(σ ), then X2 is c · σ2-subExponential, and for any fixed unit vector v ∈ Sd−1, v, X is c · σsubGaussian.
Journal of the ACM, Vol. 68, No. 2, Article 11. Publication date: February 2021.                    
11:26 C. Jin et al.
For concentration, we are interested in the properties of norm-subGaussian martingale difference sequences. Concretely, they are sequences satisfying the following conditions.
Condition C.4. Consider random vectors X1,..., Xn ∈ Rd , and corresponding filtrations Fi =
σ (X1,..., Xi ) for i ∈ [n], such that Xi |Fi−1 is zero-mean nSG(σi ) with σi ∈ Fi−1. That is:
E[Xi |Fi−1] = 0, P (Xi  ≥ t|Fi−1) ≤ 2e
− t 2
2σ 2
i , ∀t ∈ R,∀i ∈ [n].
Similarly to subGaussian random variables, we can also prove a Hoeffding-type inequality for
norm-subGaussian random vectors that is tight up to a log(d) factor.
Lemma C.5 (Hoeffding-type ineqality for norm-subGaussian). Given X1,..., Xn ∈ Rd
that satisfy condition C.4, with fixed {σi}, then for any ι > 0, there exists an absolute constant c such
that, with probability at least 1 − 2d · e−ι
:







n
i=1
Xi






≤ c ·

n
i=1
σ2
i · ι.
When {σi} is also random, we have the following.
Lemma C.6. Given X1,..., Xn ∈ Rd that satisfy condition C.4, then for any ι > 0, and B > b > 0,
there exists an absolute constant c such that, with probability at least 1 − 2d log(B/b) · e−ι
:

n
i=1
σ2
i ≥ B or







n
i=1
Xi






≤ c ·

max ⎧⎪
⎨
⎪
⎩

n
i=1
σ2
i ,b
⎫⎪
⎬
⎪
⎭
· ι.
Finally, we can also provide concentration inequalities for the sum of norm squares of normsubGaussian random vectors, and for the sum of inner products of norm-subGaussian random
vectors with another set of random vectors.
Lemma C.7. Given X1,..., Xn ∈ Rd that satisfy Condition C.4 with fixed σ1 = ··· = σn = σ, then
there exists an absolute constant c such that, for any ι > 0, with probability at least 1 − e−ι
:

n
i=1
Xi 2 ≤ c · σ2 (n + ι) .
Proof. Note there exists an absolute constant c such that E[Xi 2 |Fi−1] ≤ c · σ2, and
Xi 2 |Fi−1 is c · σ2-subExponential. This lemma directly follows from standard Bernstein concentration inequalities for subExponential random variables.
Lemma C.8. Given X1,..., Xn ∈ Rd that satisfy Condition C.4 and random vectors {ui} that satisfy ui ∈ Fi−1 for all i ∈ [n], then for any ι > 0, λ > 0, there exists absolute constantc such that, with
probability at least 1 − e−ι
:


i
ui, Xi ≤ c · λ


i
ui 2
σ2
i +
1
λ · ι.
Proof. For any i ∈ [n] and fixed λ > 0, since ui ∈ Fi−1, according to Lemma C.3 there exists a
constant c such that ui, Xi|Fi−1 is c · ui σi-subGaussian. Thus:
E[eλui,Xi
|Fi−1] ≤ ec ·λ2 ui 2σ 2
i .
Journal of the ACM, Vol. 68, No. 2, Article 11. Publication date: February 2021.               
On Nonconvex Optimization for Machine Learning 11:27
Therefore, consider the following quantity:
Ee
t
i=1 (λui,Xi−c ·λ2 ui 2σ 2
i ) = E
e
t−1 i=1 λui,Xi−c ·
t
i=1 λ2 ui 2σ 2
i · E 
eλut ,Xt 
|Ft−1

≤ E
e
t−1 i=1 λui,Xi−c ·
t
i=1 λ2 ui 2σ 2
i · ec ·λ2 ut 2σ 2
t

= Ee
t−1 i=1 (λui,Xi−c ·λ2 ui 2σ 2
i ) ≤ 1.
By Markov’s inequality, for any t > 0:
P 


t
i=1
(λui, Xi − c · λ2 ui 2
σ2
i ) ≥ t

≤ P 
e
t
i=1 (λui,Xi−c ·λ2 ui 2σ 2
i ) ≥ et

≤ e−t
Ee
t
i=1 (λui,Xi−c ·λ2 ui 2σ 2
i ) ≤ e−t
.
This finishes the proof.                    