Multi-armed bandit problems are the predominant theoretical model of exploration-exploitation tradeoffs in
learning, and they have countless applications ranging from medical trials, to communication networks, to
Web search and advertising. In many of these application domains, the learner may be constrained by one
or more supply (or budget) limits, in addition to the customary limitation on the time horizon. The literature lacks a general model encompassing these sorts of problems. We introduce such a model, called bandits
with knapsacks, that combines bandit learning with aspects of stochastic integer programming. In particular, a
bandit algorithm needs to solve a stochastic version of the well-known knapsack problem, which is concerned
with packing items into a limited-size knapsack. A distinctive feature of our problem, in comparison to the
existing regret-minimization literature, is that the optimal policy for a given latent distribution may significantly outperform the policy that plays the optimal fixed arm. Consequently, achieving sublinear regret in
the bandits-with-knapsacks problem is significantly more challenging than in conventional bandit problems.
We present two algorithms whose reward is close to the information-theoretic optimum: one is based on
a novel “balanced exploration” paradigm, while the other is a primal-dual algorithm that uses multiplicative
updates. Further, we prove that the regret achieved by both algorithms is optimal up to polylogarithmic factors. We illustrate the generality of the problem by presenting applications in a number of different domains,
including electronic commerce, routing, and scheduling. As one example of a concrete application, we consider the problem of dynamic posted pricing with limited supply and obtain the first algorithm whose regret,
with respect to the optimal dynamic policy, is sublinear in the supply.
CCS Concepts: • Theory of computation → Online learning algorithms; Online learning theory;
Regret bounds; Algorithmic mechanism design; Computational pricing and auctions;
Additional Key Words and Phrases: Multi-armed bandits, dynamic pricing, knapsack constraints

1 INTRODUCTION
For more than 50 years, the multi-armed bandit problem (henceforth, MAB) has been the predominant theoretical model for sequential decision problems that embody the tension between
exploration and exploitation, “the conflict between taking actions which yield immediate reward
and taking actions whose benefit (e.g., acquiring information or preparing the ground) will come
only later,” to quote Whittle’s apt summary [57]. Owing to the universal nature of this conflict,
it is not surprising that MAB algorithms have found diverse applications ranging from medical
trials, to communication networks, to Web search and advertising.
A common feature in many of these application domains is the presence of one or more
limited-supply resources that are consumed during the decision process. For example, scientists
experimenting with alternative medical treatments may be limited not only by the number of patients participating in the study but also by the cost of materials used in the treatments. A website
experimenting with displaying advertisements is constrained not only by the number of users
who visit the site but by the advertisers’ budgets. A retailer engaging in price experimentation
faces inventory limits along with a limited number of consumers. The literature on MAB problems
lacks a general model that encompasses these sorts of decision problems with supply limits.
Our article contributes such a model, called bandits with knapsacks (henceforth, BwK), in which
a bandit algorithm needs to solve a stochastic, multi-dimensional version of the well-known
knapsack problem. We present algorithms whose regret (normalized by the payoff of the optimal
policy) converges to zero as the resource budget and the optimal payoff tend to infinity. In fact,
we prove that this convergence takes place at the information-theoretically optimal rate.
1.1 Our Model: Bandits with Knapsacks (BwK)
Problem definition. A learner has a fixed set of potential actions, a.k.a. arms, denoted by X and
called action space. (In our main results, X will be finite, but we will also consider extensions with
an infinite set of arms, see Sections 8 and 7.) There are d resources being consumed by the learner.
Over a sequence of time steps, the learner chooses an arm and observes two things: a reward and
a resource consumption vector. Rewards are scalar-valued, whereas resource consumption vectors
are d-dimensional: the ith component represents consumption of resource i. For each resource i,
there is a pre-specified budget Bi representing the maximum amount that may be consumed, in
total. The process stops at the first time τ when the total consumption of some resource exceeds
its budget. The objective is to maximize the total reward received before time τ .
We assume that the environment does not change over time. Formally, the observations for a
fixed arm x in each time step (i.e., the reward and resource consumption vector) are independent
samples from a fixed joint distribution on [0, 1] × [0, 1]d , called the latent distribution for arm x.
There is a known, finite time horizon T . We model it as one of the resources, one unit of which
is deterministically consumed in each decision period, and the budget is T .
Notable examples. The conventional MAB problem, with a finite time horizon T , naturally fits
into this framework. A more interesting example is the dynamic pricing problem faced by a retailer
selling B items to a population ofT unit-demand consumers who arrive sequentially. Modeling this
as a BwK problem, rounds correspond to consumers, and arms correspond to the possible prices,
Journal of the ACM, Vol. 65, No. 3, Article 13. Publication date: March 2018.
Bandits with Knapsacks 13:3
which may be offered to a consumer. Reward is the revenue from a sale, if any. Resource consumption vectors express the number of items sold and consumers seen, respectively. Thus, if a price p
is offered and accepted, the reward is p and the resource consumption is [ 1
1 ]. If the offer is declined,
then the reward is 0 and the resource consumption is [ 0
1 ].
A “dual” problem of dynamic pricing is dynamic procurement, where the algorithm is “dynamically buying” rather than dynamically selling. The reward refers to the number of bought items,
and the budget constraint B now applies to the amount spent (which is why the two problems
are not merely identical up to sign reversal). If a price p is offered and accepted, then the reward
is 1 and the resource consumption is [ p
1 ]. If the offer is declined, then the reward is 0 and the
resource consumption is [ 0
1 ]. This problem is also relevant to the domain of crowdsourcing: the
items bought then correspond to microtasks ordered on a crowdsourcing platform such as Amazon
Mechanical Turk.
Another simple example concerns dynamic ad allocation for pay-per-click ads with unknown
click probabilities. There is one advertiser with several ads and budget B across all ads, and T
users to show the ads to. The ad platform allocates one ad to a new user in each round. Whenever
a given ad x is chosen and clicked on, the advertiser pays a known amount πx . To model this
as a BwK problem, arms correspond to ads, rewards are the advertiser’s payments, and resource
consumption refers to the amount spent by the advertiser and the number of users seen. Thus, if
ad x is chosen and clicked, the reward is πx and the resource consumption is [ πx
1 ]; otherwise, the
reward is 0 and the resource consumption is [ 0
1 ].
All three examples can be easily generalized to multiple resource constraints: respectively, to
selling multiple products, procuring different types of goods, and allocating ads from multiple
advertisers.
Benchmark and regret. The performance of an algorithm will be measured by its regret: the
worst case, over all possible tuples of latent distributions, of the difference between OPT and the
algorithm’s expected total reward. Here, OPT is the expected total reward of the benchmark: an
optimal dynamic policy, an algorithm that maximizes expected total reward given foreknowledge
of the latent distributions.
In a conventional MAB problem, the optimal dynamic policy is to play a fixed arm, namely
the one with the highest expected reward. In the BwK problem, the optimal dynamic policy is
more complex, as the choice of an arm in a given round depends on the remaining supply of each
resource. In fact, we doubt there is a polynomial-time algorithm to compute the optimal dynamic
policy given the latent distributions; similar problems in optimal control have long been known
to be PSPACE-hard [49].
It is easy to see that the optimal dynamic policy may significantly out-perform the best fixed
arm. To take a simple example, consider a problem instance with d resources and d arms such that
pulling arm i deterministically produces a reward of 1, consumes one unit of resource i, and does
not consume any other resources. We are given an initial endowment of B units of each resource.
Any policy that plays a fixed arm i in each round is limited to a total reward of B before running
out of its budget of resource i. Whereas an algorithm that alternates arms in a round-robin fashion
achieves reward dB: d times larger. Similar, but somewhat more involved examples can be found
for application domains of interest; see Appendix A. Interestingly, in all these examples it suffices
to consider a time-invariant mixture of arms, i.e., a policy that samples in each period from a fixed
probability distribution over arms regardless of the remaining resource supplies. In particular, in
the simple example above it suffices to consider a uniform distribution.
Journal of the ACM, Vol. 65, No. 3, Article 13. Publication date: March 2018.
13:4 A. Badanidiyuru et al.
Alternative definitions. More generally, we could model the budget constraints as a downwardclosed polytope P ⊂ Rd
+ such that the process stops when the sum of resource consumption vectors is no longer in P. However, our assumption that P is a box constraint is virtually without
loss of generality. If P is instead specified by a system of inequalities {Ax  b}, then we can redefine the resource consumption vectors to be Ax instead of x and then the budget constraint is the
box constraint defined by the vector b. The only potential downside of this transformation is that
it increases the dimension of the resource vector space, when the constraint matrix A has more
rows than columns. However, one of our algorithms has regret depending only logarithmically on
d, so this increase typically has only a mild effect on regret.
Our stopping condition halts the algorithm as soon as any budget is exceeded. Alternatively, we
could restrict the algorithm to actions that cannot possibly violate any constraint if chosen in the
current round, and stop if there is no such action. This alternative is essentially equivalent to the
original version: each budget constraint changes by at most one, which does not affect our regret
bounds in any significant way.
1.2 Main Results
We seek regret bounds that are sublinear in OPT, whereas in analyzing MAB algorithms one
typically expresses regret bounds as a sublinear function of the time horizon T . This is because a
regret guarantee of the form o(T ) may be unacceptably weak for the BwK problem, because supply
limits prevent the optimal dynamic policy from achieving a reward close to T . An illustrative
example is the dynamic pricing problem with supply B  T : the seller can only sell B items, each
at a price of at most 1, so bounding the regret by any number greater than B is worthless. To
achieve sublinear regret, the algorithm must be able to explore each arm a significant number of
times without exhausting its resource budgets. Accordingly, we parameterize our regret bound
by B = mini Bi , the smallest budget constraint.
Algorithms. We present an algorithm, called PrimalDualBwK, whose regret is sublinear in OPT as
both OPT and B tend to infinity. More precisely, denoting the number of arms bym, our algorithm’s
regret is
O √
m OPT + OPT
m/B

, (1)
where the O() notation hides logarithmic factors. Note that without resource constraints, i.e., setting B = T , we recover regret O(
√
mOPT), which is optimal up to log factors [10]. In fact, we prove
a slightly stronger regret bound that has an optimal scaling property: if all budget constraints,
including the time horizon, are increased by the factor of α, then the regret bound scales as √
α.
1
The algorithm is computationally efficient, in a strong sense: with machine word size of logT bits
or more, the per-round running time isO(md). Moreover, if each arm j consumes only dj resources
that are known in advance, then the per-round running time is O(m + d + 
j dj).
We also present another algorithm, called BalancedExploration, whose regret bound is
the same up to logarithmic factors for d = O(1). The regret bounds for the two algorithms are
incomparable: while PrimalDualBwK achieves a better dependence on d, BalancedExploration
performs better in some special cases, see Appendix B for a simple example. While PrimalDualBwK
is very computationally efficient, the specification of BalancedExploration involves a mathematically well-defined optimization step for which we do not provide a specific implementation,
see Remark 4.2 fur further discussion.
1The square-root scaling is optimal even for the basic MAB problem, as proved in Auer et al. [10].
Journal of the ACM, Vol. 65, No. 3, Article 13. Publication date: March 2018.      
Bandits with Knapsacks 13:5
Lower bound. We provide a matching lower bound: we prove that the regret bound Equation (1)
is optimal up to polylogarithmic factors; moreover, this holds for any given tuple of parameters.
Specifically, we show that for any given tuple (m, B, OPT), any algorithm for BwK must incur regret
Ω
min
OPT, OPT
m/B +
√
m OPT , (2)
in the worst-case over all instances of BwK with these (m, B, OPT). We also show that this dependence on the smallest budget constraint is inevitable in the worst case.
Applications and special cases. We derive corollaries for the three examples outlined in
Section 1.1:
• We obtain regret O(B2/3) for the basic version of dynamic pricing. This is optimal for each
(B,T ) pair [12]. Prior work [12, 56] achieved O(B2/3) regret w.r.t. the best fixed price, and
O(
√
B) regret assuming “regularity.”2 The former result is much weaker than ours, see Appendix A for a simple example, and the latter result is incomparable.
• We obtain regret O(T/B1/4) for the basic version of dynamic procurement. Prior work
[13] achieves a constant-factor approximation to the optimum with a prohibitively large
constant (at least in the tens of thousands), so our result is a big improvement unless
OPT  T/B1/4.
• We obtain regretO(
√
B) for the basic version of dynamic ad allocation. This is optimal when
B = T (i.e., when the budget constraint is void), by the basic √
T lower bound for MAB.
Our model admits numerous generalizations of these three examples, as well as applications to
several other domains. To emphasize the generality of our contributions, we systematically discuss
applications and corollaries in Section 8. Pointers to prior work on special cases of BwK can be found
in Section 1.5.
1.3 Challenges and Techniques
Challenges. As with all MAB problems, a central issue in BwK is the tradeoff between exploration
and exploitation. A naïve way to resolve this tradeoff is to separate exploration and exploitation:
before the algorithm starts, the rounds are partitioned into “exploration rounds” and “exploitation
rounds,” so that the arms chosen in the former does not depend on the feedback, and the feedback
from the latter is discarded.3 For example, an algorithm may pick an arm uniformly at random for
a pre-defined number of rounds, then choose the best arm given the observations so far, and stick
to this arm from then on. However, it tends to be much more efficient to combine exploration and
exploitation by adapting the exploration schedule to observations. Typically, in such algorithms
all but the first few rounds serve both exploration and exploitation. Thus, one immediate challenge
is to implement this approach in the context of BwK.
The BwK problem is significantly more difficult to solve than conventional MAB problems
for the following three reasons. First, to estimate the performance of a given time-invariant
policy, one needs to estimate the expected total reward of this policy, rather than the per-round
expected reward (because the latter does not account for resource constraints). Second, since
exploration consumes resources other than time, the negative effect of exploration is not limited
to the rounds in which it is performed. Since resource consumption is stochastic, this negative
effect is not known in advance, and can only be estimated over time. Finally, and perhaps most
2“Regularity” is a standard (but limiting) condition that states that the mapping from prices to expected rewards is concave.
3While the intuition behind this definition has been well-known for some time, the precise definition is due to Babaioff
et al. [11], Devanur and Kakade [24].
Journal of the ACM, Vol. 65, No. 3, Article 13. Publication date: March 2018.        
13:6 A. Badanidiyuru et al.
importantly, the optimal dynamic policy can significantly outperform the best fixed arm, as
mentioned above. To compete with the optimal dynamic policy, an algorithm needs, essentially,
to search over mixtures of arms rather than over arms themselves, which is a much larger search
space. In particular, our algorithms improve over the performance of the best fixed arm, whereas
algorithms for explore-exploit learning problems typically do not.4
Our algorithms. Algorithm BalancedExploration explicitly optimizes over mixtures of arms,
based on a simple idea: balanced exploration inside confidence bounds. The design principle underlying many confidence-bound based algorithms for stochastic MAB, including the famous UCB1
algorithm [9] and our algorithm PrimalDualBwK, is generally, “Exploit as much as possible, but
use confidence bounds that are wide enough to encourage some exploration.” The design principle
in BalancedExploration, in contrast, could be summarized as, “Explore as much as possible, but
use confidence bounds that are narrow enough to eliminate obviously suboptimal alternatives.”
Our algorithm balances exploration across arms, exploring each arm as much as possible given
the confidence bounds. More specifically, there are designated rounds when the algorithm picks a
mixture that approximately maximizes the probability of choosing this arm, among the mixtures
that are not obviously suboptimal given the current confidence bounds.
Algorithm PrimalDualBwK is a primal-dual algorithm based on the multiplicative weights update method. It maintains a vector of “resource costs” that is adjusted using multiplicative updates.
In every period it estimates each arm’s expected reward and expected resource consumption, using
upper confidence bounds for the former and lower confidence bounds for the latter; then it plays
the most “cost-effective” arm, namely the one with the highest ratio of estimated resource consumption to estimated resource cost, using the current cost vector. Although confidence bounds
and multiplicative updates are the bread and butter of online learning theory, we consider this way
of combining the two techniques to be quite novel. In particular, previous multiplicative-update
algorithms in online learning theory—such as the Exp3 algorithm for MAB [10] or the weighted
majority [44] and Hedge [31] algorithms for learning from expert advice—applied multiplicative
updates to the probabilities of choosing different arms (or experts). Our application of multiplicative updates to the dual variables of the LP relaxation of BwK is conceptually quite a different usage
of this technique.
Having alternative techniques to solve the same problem is generally useful in a rich problem
space such as MAB. Indeed, one often needs to apply techniques beyond the original models for
which they were designed, perhaps combining them with techniques that handle other facets of the
problem. When pursuing such extensions, some alternatives may be more suitable than others, in
particular, because they are more compatible with the other techniques. We already see examples
of that in the follow-up work: Agrawal and Devanur [3] and Badanidiyuru et al. [15] use some of
the techniques from BalancedExploration and PrimalDualBwK, respectively; see Section 1.4 for
more details.
LP-relaxation. To compare our algorithms to OPT, we compare both to a more tractable benchmark given by time-invariant mixtures of arms. More precisely, we define a linear programming
relaxation for the expected total reward achieved by a time-invariant mixture of arms, and prove
that the optimal value OPTLP achieved by this LP-relaxation is an upper bound for OPT. Therefore
it suffices to relate our algorithms to the time-invariant mixture of arms that achieves OPTLP, and
bound their regret with respect to OPTLP.
Lower bounds. The lower bound Equation (2) is based on a simple example in which all arms
have reward 1 and 0-1 consumption of a single resource, and one arm has slightly smaller expected
4A few notable exceptions are in References [1, 10, 13, 17]. Of these, Besbes and Zeevi [17] and Badanidiyuru et al. [13] are
on special cases of BwK and are discussed later.
Journal of the ACM, Vol. 65, No. 3, Article 13. Publication date: March 2018.
Bandits with Knapsacks 13:7
resource consumption than the rest. To analyze this example, we apply the KL-divergence technique from the MAB lower bound in Auer et al. [10]. Some technical difficulties arise, compared
to the derivation in Auer et al. [10], because the arms are different in terms of the expected
consumption rather than expected reward, and because we need to match the desired value for OPT.
Discretization. In some applications, such as dynamic pricing and dynamic procurement, the
action spaceX is very large or infinite, so our main algorithmic result is not immediately applicable.
However, the action space has some structure that our algorithms can leverage: e.g., a price is just
a number in some fixed interval. To handle such applications, we discretize the action space: we
apply a BwK algorithm with a restricted, finite action space S ⊂ X, where S is chosen in advance.
Immediately, we obtain a bound on regret with respect to the optimal dynamic policy restricted to
S. Further, we select S to balance the tradeoff between |S | and the discretization error: the decrease
in the performance benchmark due to restricting the action space to S. We call this approach
preadjusted discretization. While it has been used in prior work, the key step of bounding the
discretization error is now considerably more difficult, as one needs to take into account resource
constraints and argue about mixtures of arms rather than individual arms.
We bound discretization error for subset S, which satisfies certain axioms, and apply this result to
handle dynamic pricing with a single product and dynamic procurement with a single budget constraint. While the former application is straightforward, the latter takes some work and uses a nonstandard mesh of prices. Bounding the discretization error for more than one resource constraints
(other than time) appears to be much more challenging; we only achieve this for a special case.
1.4 Follow-up Work and Open Questions
Since the BwK problem provides a novel general problem formulation in online learning, it lends
itself to a rich set of research questions in a similar way as the stochastic MAB problem did following Lai and Robbins [42] and Auer et al. [9]. Some of these questions were researched in the
follow-up work.
Follow-up work. Following the conference publication of this article [14], there have been several
developments directly inspired by BwK.
Agrawal and Devanur [3] extend BwK from hard resource constraints and additive rewards to a
more general model that allows penalties and diminishing returns. In particular, the time-averaged
outcome vector v¯ is constrained to lie in an arbitrary given convex set, and the total reward can
be an arbitrary concave, Lipschitz-continuous function of v¯. They provide several algorithms for
this model whose regret scales optimally as a function of the time horizon. Remarkably, these
algorithms specialize to three new algorithms for BwK, based on different ideas. One of these new
BwK algorithms follows the “optimism under uncertainty” approach from Reference [9] (with an
additional trick of rescaling the resource constraints). Despite the apparent simplicity, it is shown
to satisfy our main regret bound Equation (1).
Badanidiyuru et al. [15] extend BwK to contextual bandits: a bandit model where in each round
the “context” is revealed (e.g., a user profile), then the algorithm selects an arm, and the resulting
outcome (in our case, reward and resource consumption) depends on both the chosen arm and
the context. Badanidiyuru et al. [15] merge BwK and contextual bandits with policy sets [43], a wellestablished, very general model for contextual bandits. They achieve regret that scales optimally in
terms of the time horizon and the number of policies (respectively, square-root and logarithmic).
Akin to BalancedExploration, their algorithm is not computationally efficient.
Both Agrawal and Devanur [3] and Badanidiyuru et al. [15] take advantage of various techniques developed in this article. First, both articles use (a generalization of) linear relaxations from
Section 3. In fact, the two claims in Section 3 are directly used in Badanidiyuru et al. [15] to derive
the corresponding statements for the contextual version. Second, Badanidiyuru et al. [15] build
Journal of the ACM, Vol. 65, No. 3, Article 13. Publication date: March 2018.
13:8 A. Badanidiyuru et al.
on the design and analysis of BalancedExploration, and merging them with a technique from
prior work on contextual bandits [28]. Third, the analysis of one of the algorithms in Agrawal and
Devanur [3] relies on the bound on error terms (Lemma 5.6) from our analysis of PrimalDualBwK.
Fourth, the analysis of discretization errors in Badanidiyuru et al. [15] uses a technique from
Section 7.
Two recent developments, Agrawal et al. [6] and Agrawal and Devanur [4], concern the contextual version of BwK. Agrawal et al. [6] consider a common generalization of the extended BwK
model in Reference [3] and the contextual BwK model in Reference [15]. In particular, for the latter
model they achieve the same regret as Badanidiyuru et al. [15], but with a computationally efficient algorithm, resolving the main open question in that article. On a technical level, their work
combines ideas from Reference [3] and a recent break-through in contextual bandits [2]. Agrawal
and Devanur [4] extend the model in Reference [3] to contextual bandits with a linear dependence
on contexts (e.g., see Reference [22]), achieving an algorithm with optimal dependence on the time
horizon and the dimensionality of contexts.5
Open questions (current status). While the general regret bound in Equation (1) is optimal
up to logarithmic factors, better algorithms may be possible for various special cases. To rule
out a domain-specific result that improves upon the general regret bound, one would need to
prove a lower bound that, unlike the one in Equation (2), is specific to that domain. Currently
domain-specific lower bounds are known only for the basic K-armed bandit problem and for
dynamic pricing.
For problems with infinite multi-dimensional action spaces, such as dynamic pricing with multiple products and dynamic procurement with multiple budgets, we are limited by the lack of a general approach to upper-bound the discretization error and choose the preadjusted discretization in
a principled way. A similar issue arises in the contextual extension of BwK studied in Badanidiyuru
et al. [15] and Agrawal et al. [6], even for a single resource constraint. To obtain regret bounds
that do not depend on a specific choice of preadjusted discretization, one may need to go beyond
preadjusted discretization.
The study of multi-armed bandit problems with large strategy sets has been a very fruitful line
of investigation. It seems likely that some of the techniques introduced here could be wedded
with the techniques from that literature. In particular, it would be intriguing to try combining our
primal-dual algorithm PrimalDualBwK with confidence-ellipsoid algorithms for stochastic linear
optimization (e.g., see Dani et al. [23]), or enhancing the BalancedExploration algorithm with the
technique of adaptively refined discretization, as in the zooming algorithm of Kleinberg et al. [41].
It is tempting to ask about a version of BwK in which the rewards and resource consumptions
are chosen by an adversary. Achieving sublinear regret bounds for this version appears hopeless
even for the fixed-arm benchmark. To make progress in the positive direction, one may require
a more subtle notion of benchmark and/or restrictions on the power of the adversary.
1.5 Related Work
The study of prior-free algorithms for stochastic MAB problems was initiated by Lai and Robbins
[42] and Auer et al. [9]. Subsequent work supplied algorithms for stochastic MAB problems in
which the set of arms can be infinite and the payoff function is linear, concave, or Lipschitzcontinuous; see a recent survey [20] for more background. Confidence bound techniques have
been an integral part of this line of work, and they remain integral to ours.
5Agrawal and Devanur [3] prove a similar result for a special case when contexts do not change over time. They also
claimed an extension to time-varying contexts, which has subsequently been retracted (see Footnote 1 in Agrawal and
Devanur [4]).
Journal of the ACM, Vol. 65, No. 3, Article 13. Publication date: March 2018.
Bandits with Knapsacks 13:9
As explained earlier, stochastic MAB problems constitute a very special case of bandits with
knapsacks, in which there is only one type of resource and it is consumed deterministically at rate
1. Several articles have considered the natural generalization in which there is a single resource
(other than time), with deterministic consumption, but different arms consume the resource at
different rates. Guha and Munagala [33] gave a constant-factor approximation algorithm for the
Bayesian case of this problem, which was later generalized by Gupta et al. [34] to settings in which
the arms’ reward processes need not be martingales. Tran-Thanh et al. [53, 54] presented priorfree algorithms for this problem; the best such algorithm achieves a regret guarantee qualitatively
similar to that of the UCB1 algorithm.
Several recent articles study models that, in hindsight, can be cast as special cases of BwK:
• The two articles [53, 54] mentioned above and Ding et al. [27] consider models with a single
resource and unlimited time.
• Dynamic pricing with limited supply has been studied in [12, 16, 17, 56].6
• The basic version of dynamic procurement (as per Section 1.1) has been studied in [13, 51].7
More background on the connection to crowdsourcing can be found in the survey Slivkins
and Vaughan [52].
• Dynamic ad allocation (without budget constraints) and various extensions thereof that
incorporate user/webpage context have received a considerable attention, starting with [43,
47, 48]. In fact, the connection to pay-per-click advertising has been one of the main drivers
for the recent surge of interest in MAB.
• [7, 55] study repeated bidding on a budget, and Cesa-Bianchi et al. [21] study adjusting a
repeated auction (albeit without inventory constraints); see Section 8 for more details on
these special cases.
• Perhaps the earliest article on resource consumption in MAB is György et al. [35]. They
consider a contextual bandit model where the only resource is time, consumed at different
rate depending on the context and the chosen arm. The restriction to a single context is a
special case of BwK.
Preadjusted discretization has been used in prior work on MAB on metric spaces (e.g., References
[36, 37, 41, 45]) and dynamic pricing (e.g., References [12, 16, 18, 39]). However, bounding the
discretization error in BwK is much more difficult.
Our BalancedExploration algorithm extends the “active arms elimination” algorithm [29]
for the stochastic MAB problem, where one iterates over arms that are not obviously suboptimal given the current confidence bounds. The novelty is that our algorithm chooses over mixtures of arms, and the choice is “balanced” across arms. “Policy elimination” algorithm of Dudíik
et al. [28] extends “active arms elimination” in a different direction: to contextual bandits. Like
BalancedExploration, policy elimination algorithm makes a “balanced” choice among objects
that are more complicated than arms, and this choice is not computationally efficient; however,
the technical details are very different.
While BwK is primarily an online learning problem, it also has elements of a stochastic
packing problem. The literature on prior-free algorithms for stochastic packing has flourished
in recent years, starting with prior-free algorithms for the stochastic AdWords problem [25],
6The earlier articles [18, 39] focus on the special case of unlimited supply. While we only cited articles that pursue regretminimizing formulation of dynamic pricing, Bayesian and parametric formulations versions have a rich literature in Operations Research and Economics; see Boer [19] for a literature review. 7The regret bound in Reference [51] is against the best-fixed-price benchmark, which may be much smaller than OPT, see
Appendix A for a simple example. Benchmarks aside, one cannot directly compare our regret bound and theirs, because
they do not derive a worst-case regret bound. Reference [51] is simultaneous work w.r.t. our conference publication.
Journal of the ACM, Vol. 65, No. 3, Article 13. Publication date: March 2018.
13:10 A. Badanidiyuru et al.
and continuing with a series of articles extending these results from AdWords to more general
stochastic packing integer programs while also achieving stronger performance guarantees [5,
26, 30, 46]. A running theme of these articles (and also of the primal-dual algorithm in this article)
is the idea of estimating of an optimal dual vector from samples, then using this dual to guide
subsequent primal decisions. Particularly relevant to our work is the algorithm of Devanur et al.
[26], in which the dual vector is adjusted using multiplicative updates, as we do in our algorithm.
However, unlike the BwK problem, the stochastic packing problems considered in prior work are
not learning problems: they are full information problems in which the costs and rewards of
decisions in the past and present are fully known. (The only uncertainty is about the future.) As
such, designing algorithms for BwK requires a substantial departure from past work on stochastic
packing. Our primal-dual algorithm depends upon a hybrid of confidence-bound techniques from
online learning and primal-dual techniques from the literature on solving packing LPs; combining
them requires entirely new techniques for bounding the magnitude of the error terms that arise
in the analysis. Moreover, our BalancedExploration algorithm manages to achieve strong regret
guarantees without even computing a dual solution.
2 PRELIMINARIES
BwK: problem formulation. There is a fixed and known, finite set of m arms (possible actions),
denoted X. There are d resources being consumed. The time proceeds in T rounds, where T is a
finite, known time horizon. In each round t, an algorithm picks an arm xt ∈ X, receives reward
rt ∈ [0, 1], and consumes some amount ct,i ∈ [0, 1] of each resource i. The values rt and ct,i are
revealed to the algorithm after the round. There is a hard constraint Bi ∈ R+ on the consumption
of each resource i; we call it a budget for resource i. The algorithm stops at the earliest time τ when
one or more budget constraint is violated; its total reward is equal to the sum of the rewards in all
rounds strictly preceding τ . The goal of the algorithm is to maximize the expected total reward.
The vector (rt ;ct,1,ct,2,...,ct,d ) ∈ [0, 1]d+1 is called the outcome vector for round t. We assume
stochastic outcomes: if an algorithm picks arm x, the outcome vector is chosen independently from
some fixed distribution πx over [0, 1]d+1. The distributions πx , x ∈ X are not known to the algorithm. The tuple (πx : x ∈ X) comprises all latent information in the problem instance. A particular BwK setting (such as “dynamic pricing with limited supply”) is defined by the set of all feasible
tuples (πx : x ∈ X). This set, called the BwK domain, is known to the algorithm.
We compare the performance of our algorithms to the expected total reward of the optimal
dynamic policy given all the latent information, which we denote by OPT. (Note that OPT depends
on the latent information, and therefore is a latent quantity itself.) Regret is defined as OPT minus
the expected total reward of the algorithm.
W.l.o.g. assumptions. For technical convenience, we make several assumptions that are w.l.o.g.
We express the time horizon as a resource constraint: we model time as a specific resource, say
resource 1, such that every arm deterministically consumes B1/T units of this resource whenever
it is picked. W.l.o.g., Bi ≤ T for every resource i.
We assume there exists an arm, called the null arm, which yields no reward and no consumption
of any resource other than time. Equivalently, an algorithm is allowed to spend a unit of time
without doing anything. Any algorithm ALG that uses the null arm can be transformed, without
loss in expected total reward, to an algorithm ALG	 that does not use the null arm. Indeed, in each
round ALG	 runs ALG until it selects a non-null arm x or halts. In the former case, ALG	 selects x
and returns the observe feedback to ALG. After ALG halts, ALG	 selects arms arbitrarily.
We say that the budgets are uniform if Bi = B for each resource i. Any BwK instance can be reduced to one with uniform budgets by dividing all consumption values for every resource i by
Journal of the ACM, Vol. 65, No. 3, Article 13. Publication date: March 2018.
Bandits with Knapsacks 13:11
Bi/B, where B = mini Bi . (That is tantamount to changing the units in which we measure consumption of resource i.) Our technical results are for BwK with uniform budgets. We will assume
uniform budgets B from here on.
Useful notation. Let μx = E[πx ] ∈ [0, 1]d+1 be the expected outcome vector for each arm x, and
denote μ = (μx : x ∈ X). We call μ the latent structure of a problem instance. The BwK domain
induces a set of feasible latent structures, which we denote Mfeas.
For notational convenience, we will write μx = (r(x, μ); c1 (x, μ),...,cd (x, μ)). Also, we will
write the expected consumption as a vector c(x, μ) = (c1 (x, μ),...,cd (x, μ)).
If D is a distribution over arms, then let r(D, μ) = 
x ∈X D(x)r(x, μ) and c(D, μ) = 
x ∈X D(x)c(x, μ) be, respectively, the expected reward and expected resource consumption in
a single round if an arm is sampled from distribution D. Let REW(D, μ) denote the expected total
reward of the time-invariant policy that uses distribution D.
High-probability events. We will use the following expression, which we call the confidence
radius.
rad(ν, N) =
Crad ν
N
+
Crad
N . (3)
Here, Crad = Θ(log(d T |X |)) is a parameter that we will fix later; we will keep it implicit in the
notation. The meaning of Equation (3) and Crad is explained by the following tail inequality from
References [12, 41].8
Theorem 2.1 ([12, 41]). Consider some distribution with values in [0, 1] and expectation ν. Let ν
be the average of N independent samples from this distribution. Then,
Pr[ |ν − ν | ≤ rad(ν, N) ≤ 3 rad(ν, N) ] ≥ 1 − e−Ω(Crad )
, for each Crad > 0. (4)
More generally, Equation (4) holds if X1,...,XN ∈ [0, 1] are random variables, ν = 1
N
N
t=1 Xt is the
sample average, and ν = 1
N
N
t=1 E[Xt | X1, ...,Xt−1].
If the expectation ν is a latent quantity, then Equation (4) allows us to estimate ν by a highconfidence interval,
ν ∈ [ν − rad(ν, N), ν + rad(ν, N)], (5)
whose endpoints are observable (known to the algorithm). This estimate is on par with the one
provided by Azuma-Hoeffding inequality (up to constant factors), but is much sharper for small ν.
9
It is sometimes useful to argue about any ν that lies in the high-confidence interval Equation (5),
not just the latent ν = E[ν]. We use the following claim, which is implicit in Kleinberg et al. [41].
Claim 2.2 ([41]). For any ν,ν ∈ [0, 1], Equation (5) implies that rad(ν, N) ≤ 3 rad(ν, N).
3 LP RELAXATION FOR POLICY VALUE
OPT—the expected reward of the optimal dynamic policy given foreknowledge of the distribution
of outcome vectors—is typically difficult to characterize exactly. In fact, even for a time-invariant
policy, it is difficult to give an exact expression for the expected reward due to the dependence of
the reward on the random stopping time when the resource budget is exhausted. To approximate
these quantities, we consider the fractional relaxation of BwK in which the number of rounds in
8Specifically, this follows from Lemma 4.9 in the full version of Kleinberg et al. [41], and Theorem 4.8 and Theorem 4.10
in the full version of Babaioff et al. [12] (both full versions can be found on arxiv.org). 9Essentially, Azuma-Hoeffding inequality states that |ν − ν| ≤ O (
√
Crad/N ), whereas by Theorem 2.1 for small ν it holds
with high probability that rad(ν, N ) ∼ Crad/N .
Journal of the ACM, Vol. 65, No. 3, Article 13. Publication date: March 2018.             
13:12 A. Badanidiyuru et al.
which a given arm is selected (and also the total number of rounds) can be fractional, and the
reward and resource consumption per unit time are deterministically equal to the corresponding
expected values in the original instance of BwK.
The following linear program constitutes our fractional relaxation of the optimal dynamic
policy:
max 
x ∈X
ξx r(x, μ) in ξx ∈ R, for each x ∈ X
s.t. 
x ∈X
ξx ci (x, μ) ≤ B for each resource i
ξx ≥ 0 for each arm x.
(LP-primal)
The variables ξx represent the fractional relaxation for the number of rounds in which a given
arm x is selected. This is a bounded LP (because 
x ξx r(x, μ) ≤ 
x ξx ≤ T ). The optimal value
of this LP is denoted by OPTLP. We will also use the dual LP, shown below.
min B

i
ηi in ηi ∈ R, for each resource i
s.t. 
i
ηi ci (x, μ) ≥ r(x, μ) for each arm x ∈ X
ηi ≥ 0 for each resource i.
(LP-dual)
The dual variables ηi can be interpreted as a unit cost for the corresponding resource i.
Lemma 3.1. OPTLP is an upper bound on the value of the optimal dynamic policy: OPTLP ≥ OPT.
One way to prove this lemma is to define ξx to be the expected number of times arm x is played
by the optimal dynamic policy, and argue that the vector (ξx , x ∈ X) is primal-feasible and that

x ξx r(x, μ) is the expected reward of the optimal dynamic policy. We instead present a simpler
proof using (LP-dual) and a martingale argument. A similar lemma (but for a technically different
setting of online stochastic packing problems) was proved in Devanur et al. [26].
Proof of Lemma 3.1. Let η∗ = (η∗
1,..., η∗
d ) denote an optimal solution to (LP-dual). Interpret
each η∗
i as a unit cost for the corresponding resource i. By strong LP duality, we have B 
i η∗
i =
OPTLP. Dual feasibility implies that for each arm x, the expected cost of resources consumed when
x is pulled exceeds the expected reward produced. Thus, if we let Zt denote the sum of rewards
gained in rounds 1,...,t of the optimal dynamic policy, plus the cost of the remaining resource
endowment after round t, then the stochastic process Z0,Z1,...,ZT is a supermartingale. Let τ
be the stopping time of the algorithm, i.e., the total number of rounds. Note that Z0 = B 
i η∗
i =
OPTLP, and Zτ −1 equals the algorithm’s total payoff, plus the cost of the remaining (non-negative)
resource supply at the start of round τ . By Doob’s optional stopping theorem, Z0 ≥ E[Zτ −1] and
the lemma is proved.
Remark 3.2. Implicit in this proof is a simple, but powerful observation that for any algorithm,
OPTLP − REW ≥ E
⎡
⎢
⎢
⎢
⎢
⎣

t
r(xt, μ) − c(xt, μ) · η∗
⎤
⎥
⎥
⎥
⎥
⎦
.
Each summand on the right-hand side is non-negative, and equals 0 if and only if the arm xt
lies in the support of the primal solution. We use this observation to motivate the design of our
primal-dual algorithm.
Remark 3.3. For each of the two main algorithms, we prove a regret bound of the form
OPTLP − REW ≤ f (OPTLP), (6)
Journal of the ACM, Vol. 65, No. 3, Article 13. Publication date: March 2018. 
Bandits with Knapsacks 13:13
where REW is the expected total reward of the algorithm, and f () depends only on parameters
(B,m,d). This regret bound has an optimal scaling property, highlighted in the Introduction: if all
budget constraints, including the time horizon, are increased by the factor of α, then the regret
bound f (OPTLP) scales as √
α.
Regret bound Equation (6) implies the claimed regret bounds relative to OPT, because
REW ≥ OPTLP − f (OPTLP) ≥ OPT − f (OPT), (7)
where the second inequality follows trivially, because д(x) = max(x − f (x), 0) is a non-decreasing
function of x for x ≥ 0, and OPTLP ≥ OPT.
Let us apply a similar LP-relaxation to a time-invariant policy that uses distribution D over
arms. We approximate the expected total reward of this policy in a similar way: we define a linear
program in which the only variable t represents the expected stopping time of the algorithm.
max t r(D, μ) in t ∈ R
s.t. t ci (D, μ) ≤ B for each resource i
t ≥ 0.
(LP-distr)
The optimal value to (LP-distr), which we call the LP-value of D, is
LP(D, μ) = r(D, μ) min
i
B
ci (D, μ)
. (8)
Observe thatt is feasible for (LP-distr) if and only ifξ = tD is feasible for (LP-primal). Therefore,
OPTLP = sup
D
LP(D, μ).
This supremum is attained by any distribution D∗ = ξ / ξ 1 such that ξ = (ξx : x ∈ X) is an optimal solution to (LP-primal). A distribution D∗ ∈ argmaxD LP(D, μ) is called LP-optimal for μ.
Claim 3.4. For any latent structure μ, there exists a distribution D over arms that is LP-optimal
for μ and moreover satisfies the following three properties:
(a) ci (D, μ) ≤ B/T for each resource i.
(b) D has a support of size at most d.
(c) If D has a support of size exactly 2, then for some resource i, we have ci (D, μ) = B/T .
(Such distribution D will be called LP-perfect for μ.)
Proof. Fix the latent structure μ. It is a well-known fact that for any linear program there exists
an optimal solution whose support has size that is exactly equal to the number of constraints
that are tight for this solution. Take any such optimal solution ξ = (ξx : x ∈ X) for (LP-primal),
and take the corresponding LP-optimal distribution D = ξ /ξ 1. Since there are d constraints
in (LP-primal), distribution D has support of size at most d. If it satisfies (a), then it also satisfies
(c) (else it is not optimal), and we are done.
Suppose property (a) does not hold for D. Then there exists a resource i such that ci (D, μ) >
B/T . Since the ith constraint in (LP-primal) can be restated as ξ 1 ci (D, μ) ≤ B, it follows that
ξ 1 < T . Therefore, the constraint in (LP-primal) that expresses the time horizon is not tight.
Consequently, at most d − 1 constraints in (LP-primal) are tight for ξ, so the support of D has
size at most d − 1.
Let us modify D to obtain another LP-optimal distribution D	 that satisfies properties (a-c).
W.l.o.g., pick i to maximize ci (D, μ) and let α = B
T /ci (D, μ). Define D	
(x) = α D(x) for each nonnull arm x and place the remaining probability in D	 on the null arm. This completes the definition
of D	
.
Journal of the ACM, Vol. 65, No. 3, Article 13. Publication date: March 2018.
13:14 A. Badanidiyuru et al.
Note that cj (D	
, μ) = α cj (D, μ) ≤ B/T for each resource j, with equality for j = i. Hence, D	
satisfies properties (a) and (c). Also, r(D	
, μ) = α r(D, μ), and so
LP(D	
, μ) = r(D	
, μ) B
ci (D	
, μ) = r(D, μ) B
ci (D, μ) = LP(D, μ).
Thus, D	 is LP-optimal. It satisfies property (b), because it adds at most 1 to the support of D.
4 ALGORITHM BALANCEDEXPLORATION
This section presents and analyzes BalancedExploration, one of the two main algorithms. The
design principle behind BalancedExploration is to explore as much as possible while avoiding
obviously suboptimal strategies. On a high level, the algorithm is very simple. The goal is to converge on an LP-perfect distribution. The time is divided into phases of |X | rounds each. In the
beginning of each phase p, the algorithm prunes away all distributions D over arms that with
high confidence are not LP-perfect given the observations so far. The remaining distributions over
arms are called potentially perfect. Throughout the phase, the algorithm chooses among the potentially perfect distributions. Specifically, for each arm x, the algorithm chooses a potentially
perfect distribution Dp,x , which approximately maximizes Dp,x (x), and “pulls” an arm sampled
independently from this distribution. This choice of Dp,x is crucial; we call it the balancing step.
The algorithm halts as soon as the time horizon is met, or any of the constraints is exhausted. The
pseudocode is given in Algorithm 1.
ALGORITHM 1: BalancedExploration
1: For each phase p = 0, 1, 2, ... do
2: Recompute the set Δp of potentially perfect distributions D over arms.
3: Over the next |X | rounds, for each x ∈ X:
4: pick any distribution D = Dp,x ∈ Δp such that D(x) ≥ 1
2 maxD	∈Δp D	
(x).
5: choose an arm to “pull" as an independent sample from D.
6: halt if time horizon is met or one of the resources is exhausted.
We believe that BalancedExploration, like UCB1 [9], is a very general design principle and has
the potential to be a meta-algorithm for solving stochastic online learning problems.
Theorem 4.1. Consider an instance of BwK with d resources,m = |X | arms, and the smallest budget
B = mini Bi . Algorithm BalancedExploration achieves regret
OPTLP − REW ≤ O(log(T ) log(T/m)) 



dmOPTLP + OPTLPdm
B


. (9)
Moreover, Equation (7) holds with f (OPTLP) equal to the right-hand side of Equation (9).
Remark 4.2. The specification of BalancedExploration involves a mathematically well-defined
step—approximate optimization over potentially perfect distributions—for which we do not provide a specific implementation. Yet, BalancedExploration is a bandit algorithm in the sense that
it is a well-defined mapping from histories to actions. We prove an “information-theoretic” statement: there is an algorithm with the claimed regret. Such results are not uncommon in the literature, e.g., References [2, 40, 41], typically as first solutions for new, broad problem formulations,
and are meaningful as proof-of-concept for the corresponding regret bounds and techniques.
Remaining details of the specification. In the beginning of each phase p, the algorithm recomputes a “confidence interval” Ip for the latent structure μ, so that (informally) μ ∈ Ip with high
Journal of the ACM, Vol. 65, No. 3, Article 13. Publication date: March 2018.  
Bandits with Knapsacks 13:15
probability. Then the algorithm determines which distributions D over arms can potentially be
LP-perfect given that μ ∈ Ip . Specifically, let Δp be set of all distributions D that are LP-perfect for
some latent structure μ	 ∈ Ip ; such distributions are called potentially perfect (for phase p).
It remains to define the confidence intervals Ip . For phase p = 0, the confidence interval I0 is
simply Mfeas, the set of all feasible latent structures. For each subsequent phase p ≥ 1, the confidence interval Ip is defined as follows. For each arm x, consider all rounds before phase p in which
this arm has been chosen. Let Np (x) be the number of such rounds, letrp (x) be the time-averaged
reward in these rounds, and let cp,i (x) be the time-averaged consumption of resource i in these
rounds. We use these averages to estimate r(x, μ) and ci (x, μ) as follows:
|r(x, μ) −rp (x)| ≤ rad(rp (x), Np (x)), (10)
|ci (x, μ) −cp,i (x)| ≤ rad(cp,i (x), Np (x)), for each resource i. (11)
The confidence interval Ip is the set of all latent structures μ	 ∈ Ip−1 that are consistent with these
estimates. This completes the specification of BalancedExploration.
For each phase of BalancedExploration, the round in which an arm is sampled from distribution Dp,x will be called designated to arm x. We need to use approximate maximization to choose
Dp,x , rather than exact maximization, because an exact maximizer argmaxD ∈Δp D(x) is not guaranteed to exist.
Proof overview. We start with some properties of the algorithm that follow immediately from the
specification and hold deterministically (with probability 1). Then, we identify several properties
that the algorithm satisfies with very high probability. The rest of the analysis focuses on a “clean
execution” of the algorithm: an execution in which all these properties hold. We analyze the “error
terms” that arise due to the uncertainty on the latent structure, and use the resulting “error bounds”
to argue about the algorithm’s performance.
4.1 Deterministic Properties of BalancedExploration
First, we show that any two latent structures in the confidence interval Ip correspond to similar
consumptions and rewards, for each arm x. This follows deterministically from the specification
of Ip .
Claim 4.3. Fix any phase p, any two latent structures μ	
, μ		 ∈ Ip , an arm x, and a resource i. Then,
|ci (x, μ	
) − ci (x, μ		)| ≤ 6 rad(ci (x, μ	
), Np (x)), (12)
|r(x, μ	
) − r(x, μ		)| ≤ 6 rad(r(x, μ	
), Np (x)). (13)
Proof. We prove Equation (12); Equation (13) is proved similarly.
Let N = Np (x). By specification of BalancedExploration, any μ	 ∈ Ip is consistent with estimate Equation (11):
|ci (x, μ	
) −cp,i (x)| ≤ rad(cp,i (x), N).
It follows that
|ci (x, μ	
) − ci (x, μ		)| ≤ 2 rad(cp,i (x), N).
Finally, we observe that by Claim 2.2,
rad(cp,i (x), N) ≤ 3 rad (ci (x, μ	
), N).
For each phase p and arm x, let D¯p,x = 1
p

q<p Dq,x (x) be the average of probabilities for arm
x among the distributions in the preceding phases that are designated to arm x. Because of the
balancing step in BalancedExploration, we can compare this quantity to D(x), for any D ∈ Δp .
Journal of the ACM, Vol. 65, No. 3, Article 13. Publication date: March 2018.           
13:16 A. Badanidiyuru et al.
(Here, we also use the fact that the confidence intervals Ip are non-increasing from one phase to
another.)
Claim 4.4. D¯p,x ≥ 1
2 D(x) for each phase p, each arm x and any distribution D ∈ Δp .
Proof. Fix arm x. Recall that D¯p,x = 1
p

q<p Dq,x (x), where Dq,x is the distribution chosen
in the round in phase q that is designated to arm x. Fix any phase q < p. Because of the balancing
step, Dq,x (x) ≥ 1
2 D	
(x) for any distribution D	 ∈ Δq. Since the confidence intervals Iq are nonincreasing from one phase to another, we have Ip ⊂ Iq for any q ≤ p, which implies that Δp ⊂ Δq.
Consequently, Dq,x (x) ≥ 1
2 D(x) for each q < p, and the claim follows.
4.2 High-Probability Events
We keep track of several quantities: the averagesrp (x) andcp,i (x) defined above, as well as several
other quantities that we define below.
Fix phase p and arm x. Recall that Np (x) is the number of rounds before phase p in which arm
x is chosen. Now, let us consider all rounds before phase p that are designated to arm x. Let np (x)
denote the number of times arm x has been chosen in these rounds. Let Dp,x = nt (x)/p be the
corresponding empirical probability of choosing x. We compare this to D¯p,x .
Further, consider all rounds in phases q < p. There are N = p|X | such rounds. The average distribution chosen by the algorithm in these rounds is D¯p = 1
N

q<p, x ∈X Dq,x . We are interested in the
corresponding quantities r(D¯p, μ) and ci (D¯p, μ), We compare these quantities to rp = 1
N
N
t=1 rt
andcp,i = 1
N
N
t=1 ct,i , the average reward and the average resource-i consumption in phasesq < p.
We consider several high-probability events that follow from applying Theorem 2.1 to the various quantities defined above. All these events have a common shape: some quantities ν,ν satisfy
Equation (5) for some N. If this is the case, then we know that ν is an N-strong estimator for ν.
Lemma 4.5. For each phase p, arm x, and resource i, with probability e−Ω(Crad ) it holds that:
(a) rp (x) and cp,i (x) are Np (x)-strong estimators for r(x, μ) and ci (x, μ), respectively.
(b) D¯p,x is an p-strong estimator for Dp,x .
(c) r(D¯p, μ) and ci (D¯p, μ) are (p|X |)-strong estimators for rp and cp,i , respectively.
We rely on several properties of the confidence radius rad(), which we summarize below. (We
omit the easy proofs.)
Claim 4.6. The confidence radius rad(ν, N), defined in Equation (3), satisfies the following
properties:
(a) monotonicity: rad(ν, N) is non-decreasing in ν and non-increasing in N.
(b) concavity: rad(ν, N) is concave in ν, for any fixed N.
(c) max(0, ν − rad(ν, N)) is non-decreasing in ν.
(d) ν − rad(ν, N) ≥ 1
4 ν whenever 4Crad
N ≤ ν ≤ 1.
(e) rad(ν, N) ≤ 3Crad
N whenever ν ≤ 4Crad
N .
(f) rad(ν, αN) = 1
α rad(αν, N), for any α ∈ (0, 1].
(g) 1
N
N
=1 rad(ν, ) ≤ O(log N) rad(ν, N).
4.3 Clean Execution Analysis
It is convenient to focus on a clean execution of the algorithm: an execution in which all events in
Lemma 4.5 hold. We assume a clean execution in what follows. Also, we fix an arbitrary phase p
in such execution.
Journal of the ACM, Vol. 65, No. 3, Article 13. Publication date: March 2018.               
Bandits with Knapsacks 13:17
Clean execution analysis falls into two parts. First, we analyze the “error terms”: we look at
the LP-value (respectively, expected reward, or expected resource consumption) of a given distribution, and upper-bound the difference in this quantity between different latent structures μ, μ	
in the confidence interval Ip , or between different potentially perfect distributions D	
,D		 ∈ Δp .
The culmination is Lemma 4.12, which upper-bounds the difference |LP(D	
, μ	
) − LP(D		, μ		)| in
terms of parameters p
d , B, T , and OPTLP. Second, we apply these error bounds to reason about the
algorithm itself. The key quantities of interest are LP-values of the chosen distributions, average
reward/consumption, and the stopping time.
4.3.1 Bounding the Error Terms. Since a clean execution satisfies the event in Claim 4.5(a), it
immediately follows that:
Claim 4.7. The confidence interval Ip contains the (actual) latent structure μ. Therefore, D∗ ∈ Δp
for any distribution D∗ that is LP-perfect for μ.
Claim 4.8. Fix any latent structures μ	
, μ		 ∈ Ip and distribution D ∈ Δp . Then, for each
resource i,
|ci (D, μ	
) − ci (D, μ		)| ≤ O(1) rad (ci (D, μ	
), p/d), (14)
|r(D, μ	
) − r(D, μ		)| ≤ O(1) rad (r(D, μ	
), p/d). (15)
Proof. We prove Equation (14); Equation (15) is proved similarly. Let us first prove the
following:
∀x ∈ X, D(x) |ci (x, μ	
) − ci (x, μ		)| ≤ O(1) rad(D(x)ci (x, μ	
),p). (16)
Intuitively, to argue that we have good estimates on quantities related to arm x, it helps to prove
that this arm has been chosen sufficiently often. Using the definition of clean execution and
Claim 4.4, we accomplish this as follows:
1
p
Np (x) ≥
1
p np (x) = Dp,x
≥ D¯p,x − rad(D¯p,x ,p) (by clean execution)
≥
1
2
D(x) − rad  1
2
D(x),p
	
(by Claim 4.4 and Claim 4.6(c)).
Consider two cases depending on D(x). For the first case, assume D(x) ≥ 8Crad
p . Using Claim 4.6(d)
and the previous equation, it follows that Np (x) ≥ 1
8 p D(x). Therefore:
D(x) |ci (x, μ	
) − ci (x, μ		)| ≤ 6 D(x) rad(ci (x, μ	
), Np (x)) (by Claim 4.3)
≤ 6 D(x) rad(ci (x, μ	
), 1
8 p D(x)) (by monotonicity of rad)
= 48 rad(D(x)ci (x, μ	
x ),p) (by Claim 4.6(f)).
The second case is that D(x) < 8Crad
p . Then Equation (16) follows simply because Crad
p ≤ rad(· ,p).
We have proved Equation (16). We complete the proof of Equation (14) using concavity of
rad(·,p) and the fact that, by the specification of BalancedExploration, D has support of size at
Journal of the ACM, Vol. 65, No. 3, Article 13. Publication date: March 2018. 
13:18 A. Badanidiyuru et al.
most d.
|ci (D, μ	
) − ci (D, μ		)| ≤ 
x ∈X
D(x) |ci (x, μ	
) − ci (x, μ		)|
≤

x ∈X, D(x )>0
O(1) rad(D(x)ci (x, μ	
),p)
≤ O(d) rad 


1
d

x ∈X
D(x)ci (x, μ	
), p

= O(d) rad  1
d ci (D, μ	
), p
	
≤ O(1) rad 
ci (D, μ	
), p
d
	
(by Claim 4.6(f)).
In what follows, we will denote Mp = maxD ∈Δp, μ ∈Ip LP(D, μ).
Claim 4.9. Fix any latent structures μ	
, μ		 ∈ Ip and any distribution D ∈ Δp . Then,
|LP(D, μ	
) − LP(D, μ		)| ≤ O(T ) rad 
Mp /T,
p
d
	
+ O

Mp
T
B
	
rad B
T ,
p
d
	
. (17)
Proof. Since D ∈ Δp , it is LP-perfect for some latent structure μ. Then LP(D, μ) = T r(D, μ).
Therefore:
LP(D, μ	
) − LP(D, μ) ≤ T (r(D, μ	
) − r(D, μ))
≤ O(T ) rad 
r(D, μ),
p
d
	
(by Claim 4.8). (18)
We need a little more work to bound the difference in the LP values in the other direction.
Consider t0 = LP(D, μ	
)/r(D, μ	
); this is the value of the variable t in the optimal solution to
the linear program (LP-distr). Let us obtain a lower bound on this quantity. Assume t0 < T .
Then one of the budget constraints in (LP-distr) must be tight, i.e., t0 ci (D, μ	
) = B for some
resource i.
ci (D, μ	
) ≤ ci (D, μ) + O(1) rad 
ci (D, μ),
p
d
	
(by Claim 4.8)
≤
B
T
+ O(1) rad B
T ,
p
d
	
Let Ψ = rad( B
T , p
d ). It follows that t0 = B/ci (D, μ	
) ≥ T (1 − O( T
B Ψ)). Therefore:
LP(D, μ) − LP(D, μ	
) = T r(D, μ) − t0 r(D, μ	
)
≤ T r(D, μ) −

T

1 − O

T
B
Ψ
	 	
r(D, μ	
)
≤ T 

r(D, μ) − r(D, μ	
)

+ O

T
B
Ψ
	
T r(D, μ)
≤ O(T ) rad 
r(D, μ),
p
d
	
+ O

T
B
Ψ
	
T r(D, μ) (by Claim 4.8).
Using Equation (18) and noting that r(D, μ) = LP(D, μ)/T ≤ Mp /T , we conclude that
|LP(D, μ) − LP(D, μ	
)| ≤ O(T ) rad 
Mp /T, p
d
	
+ O(Mp
T
B ) rad B
T ,
p
d
	
.
We obtain the same upper bound on |LP(D, μ) − LP(D, μ		)|, and the claim follows.
Journal of the ACM, Vol. 65, No. 3, Article 13. Publication date: March 2018.  
Bandits with Knapsacks 13:19
We will use Φp (Mp ) to denote the right-hand side of Equation (17) as a function of Mp .
Claim 4.10.
(a) Fix any latent structure μ∗ ∈ Ip , and any distributions D	
, D		 ∈ Δp . Then,
|LP(D	
, μ∗) − LP(D		, μ∗)| ≤ 2Φp (Mp ).
(b) Fix any latent structure μ	
, μ		 ∈ Ip , and any distributions D	
, D		 ∈ Δp . Then,
|LP(D	
, μ	
) − LP(D		, μ		)| ≤ 3Φp (Mp ).
Proof. (a). Since D	
, D		 ∈ Δp , it holds that D	 and D		 are LP-perfect for some latent structures μ	 and μ		. Further, pick a distribution D∗ that is LP-perfect for μ∗. Then:
LP(D	
, μ∗) ≥ LP(D	
, μ	
) − Φp (Mp ) (by Lemma 4.9 withD = D	
)
≥ LP(D∗
, μ	
) − Φp (Mp )
≥ LP(D∗
, μ∗) − 2Φp (Mp ) (by Lemma 4.9 withD = D∗)
≥ LP(D		, μ∗) − 2Φp (Mp ).
(b). Follows easily from part (a) and Lemma 4.9.
The following claim will allow us to replace Φp (Mp ) by Φp (OPTLP).
Claim 4.11. Φp (OPTLP) ≥ Ω(min(OPTLP, Φp (Mp ))).
Proof. Consider the two summands in Φp (Mp ):
S1 (Mp ) = O(T ) rad 
Mp /T, p
d
	
,
S2 (Mp ) = O(Mp
T
B ) rad B
T ,
p
d
	
.
We consider the following three cases. The first case is that S1 (Mp ) ≥ Mp /12. Solving for Mp ,
we obtain Mp ≤ O(
TdCrad
p ), which implies that
Φp (OPTLP) ≥ Ω(Mp ) ≥ Ω(OPTLP).
The second case is that S2 (Mp ) ≥ Mp /12. Then,
Φp (OPTLP) ≥ S2 (OPTLP) ≥ OPTLP/12.
In remaining case, Φp (Mp ) ≤
Mp
6 . Then from Claim 4.10(b), we get that Mp ≤ 2 OPTLP. Noting that
Φp (M) is a non-decreasing function of M, we obtain
Φp (Mp ) ≤ Φp (2 OPTLP) ≤ 2 Φp (OPTLP).
Claim 4.11 and Claim 4.10 imply our main bound on the error terms:
Lemma 4.12. Fix any latent structure μ	
, μ		 ∈ Ip , and any distributions D	
, D		 ∈ Δp . Then,
|LP(D	
, μ	
) − LP(D		, μ		)| ≤ O(Φp (OPTLP)).
Journal of the ACM, Vol. 65, No. 3, Article 13. Publication date: March 2018.  
13:20 A. Badanidiyuru et al.
4.3.2 Performance of the Algorithm. The remainder of the analysis deals with rewards and resource consumption of the algorithm. We start with lower-bounding the LP-value for the chosen
distributions.
Claim 4.13. For each distribution Dp,x chosen by the algorithm in phase p,
LP(Dp,x , μ) ≥ OPTLP − O(Φp (OPTLP)).
Proof. The claim follows easily from Lemma 4.12, noting that Dp,x ∈ Δp .
The following corollary lower-bounds the average reward; once we have it, it essentially remains
to lower-bound the stopping time of the algorithm.
Corollary 4.14. rp ≥ 1
T (OPTLP − O(logp) Φp (OPTLP)).
Proof. Throughout this proof, denote Φp  Φp (OPTLP). By Claim 4.13, for each distribution
Dq,x chosen by the algorithm in phase q < p it holds that
r(Dq,x , μ) ≥
1
T
LP(Dq,x , μ) ≥
1
T (OPTLP − O(Φq )).
Averaging the above equation over all rounds in phases q < p, we obtain
r(D¯p, μ) ≥
1
T




OPTLP − 1
p

q<p
O(Φq )



≥
1
T (OPTLP − O(Φp logp)).
For the last inequality, we used Claim 4.6(fg) to average the confidence radii in Φq.
Using the high-probability event in Claim 4.5(c):
rp ≥ r(D¯p, μ) − rad(r(D¯p, μ),p|X |).
Now, using the monotonicity of ν − rad(ν, N) (Claim 4.6(c)), we obtain
rp ≥
1
T (OPTLP − O(Φp )) − rad  1
T (OPTLP − O(Φp )), p|X |
	
≥
1
T (OPTLP − O(Φp )) − rad ( OPTLP/T, p|X | )
≥
1
T (OPTLP − O(Φp )).
For the last equation, we use the fact that Φp /T ≥ Ω(rad(OPTLP/T, p
d )) ≥ Ω(rad(OPTLP/T,
p|X |)).
The following two claims help us to lower-bound the stopping time of the algorithm.
Claim 4.15. ci (Dp,x , μ) ≤ B
T + O(1) rad( B
T , p
d ) for each resource i.
Proof. By the algorithm’s specification, Dp,x ∈ Δp , and moreover there exists a latent structure
μ	 ∈ Ip such that Dp,x is LP-perfect for μ	
. Apply Claim 4.8, noting that ci (Dp,x , μ	
) ≤ B
T by LPperfectness.
Corollary 4.16. cp,i ≤ B
T + O(logp) rad( B
T , p
d ) for each resource i.
Proof. Using a property of the clean execution, namely the event in Claim 4.5(c), we have
cp,i ≤ ci (D¯, μ) + rad(ci (D¯, μ), p). (19)
Journal of the ACM, Vol. 65, No. 3, Article 13. Publication date: March 2018.         
Bandits with Knapsacks 13:21
Consider all rounds preceding phase p.
ci (D¯p, μ) = 1
p|X |

q<p, x ∈X
ci (Dq,x , μ)
≤
B
T
+
O(1)
p|X |

q<p, x ∈X
rad B
T ,
p
d
	
(by Claim 4.15)
≤
B
T
+ O(logp) rad B
T ,
p
d
	
(by Claim 4.6(fg)). (20)
For the last inequality, we used Claim 4.6(fg) to average the confidence radii.
Using the upper bound on ci (D¯, μ) that we derived above,
rad 
ci (D¯, μ), p
d
	
≤ O(logp) rad B
T
+ rad B
T ,
p
d
	
, p
d
	
.
Using a general property of the confidence radius that
rad(ν + rad(ν, N), N) ≤ O(rad(ν, N)),
we conclude that
rad 
ci (D¯, μ), p
d
	
≤ O(logp) rad B
T ,
p
d
	
. (21)
We obtain the claim by plugging the upper bounds Equations (20) and (21) into Equation (19).
We are ready to put the pieces together and derive the performance guarantee for a clean execution of BalancedExploration.
Lemma 4.17. Consider a clean execution of BalancedExploration. Then, the total reward
REW ≥ OPTLP − O

log T
|X |

ΦT / |X |(OPTLP).
Proof. Throughout this proof, denote Φp  Φp (OPTLP). Let p be the last phase in the execution
of the algorithm, and let T0 be the stopping time. Letting m = |X |, note that pm < T0 ≤ (p + 1)m.
We can use Corollary 4.14 to bound REW from below:
REW = T0rp+1 > p mrp+1 ≥ p m
T (OPTLP − O(Φp logp)). (22)
Let us bound p m
T from below. The algorithm stops either when it runs out of time or if it runs
out of resources during phase p. In the former case, p = T/m. In the latter case, B = T0cp+1, i
for some resource i, so B ≤ m(p + 1) cp+1, i . Using Corollary 4.16, we obtain the following lower
bound on p:
p m
T ≥ 1 − O

p m logp
B

rad B
T ,
p
d
	
.
Plugging this into Equation (22), we conclude:
REW ≥ OPTLP − O

p m logp
B

rad B
T ,
p
d
	
OPTLP − O

p m logp
T

Φp
≥ OPTLP − O

p m logp
T
 
Φp +
T
B
rad B
T ,
p
d
	
OPTLP	
≥ OPTLP − p m logp
T
O(Φp ).
Journal of the ACM, Vol. 65, No. 3, Article 13. Publication date: March 2018.      
13:22 A. Badanidiyuru et al.
To complete the proof, we observe that (p Φp logp) is increasing in p (by definition of Φp ) and plug
in a trivial upper bound p ≤ T/m.
To finish the proof of Theorem 4.1, we write down the definition of ΦT /m (OPTLP), m = |X |, and
plug in the definition of the confidence radius Equation (3):
ΦT /m (OPTLP)  O(T ) rad 
OPTLP/T, T
d|X |

+ O

OPTLP
T
B
	
rad B
T , T
dm 	
≤ O(log(T ) 



dmOPTLP + OPTLPdm
B


.
5 ALGORITHM PRIMALDUALBWK
This section develops an algorithm, called PrimalDualBwK, that solves the BwK problem using a
very natural and intuitive idea: greedily select arms with the greatest estimated “bang per buck,”
i.e., reward per unit of resource consumption. One of the main difficulties with this idea is that
there is no such thing as a known “unit of resource consumption”: there are d different resources,
and it is unclear how to trade off consumption of one resource versus another. The dual LP in
Section 3 gives some insight into how to quantify this trade-off: an optimal dual solution η∗ can be
interpreted as a vector of unit costs for resources, such that for every arm the expected reward is
less than or equal to the expected cost of resources consumed. Then the bang-per-buck ratio for a
given arm x can be defined asr(x, μ)/(η∗ ·c(x, μ)), where the denominator represents the expected
cost of pulling this arm. The arms in the support of the optimal distribution ξ ∗ are precisely the
arms with a maximal bang-per-buck ratio (by complimentary slackness), and pulling any other
arm necessarily increases regret relative to OPTLP (by Remark 3.2).
To estimate the bang-per-buck ratios, our algorithm will try to learn an optimal dual vector
η∗ in tandem with learning the latent structure μ. Borrowing an idea from References [8, 32, 50],
we use the multiplicative weights update method to learn the optimal dual vector. This method
raises the cost of a resource exponentially as it is consumed, which ensures that heavily demanded
resources become costly, and thereby promotes balanced resource consumption. Meanwhile, we
still have to ensure (as with any multi-armed bandit problem) that our algorithm explores the
different arms frequently enough to gain adequately accurate estimates of the latent structure.
We do this by estimating rewards and resource consumption as optimistically as possible, i.e.,
using upper confidence bound (UCB) estimates for rewards and lower confidence bound (LCB)
estimates for resource consumption. Although both of these techniques—multiplicative weights
and confidence bounds—have been successfully applied in previous online learning algorithms, it
is far from obvious that this particular hybrid of the two methods should be effective. In particular,
the use of multiplicative updates on dual variables, rather than primal ones, distinguishes our
algorithm from other bandit algorithms that use multiplicative weights (e.g., the Exp3 algorithm
[10]) and brings it closer in spirit to the literature on stochastic packing algorithms, especially
Reference [26].
The pseudocode is presented as Algorithm 2. When we refer to the UCB or LCB for a latent parameter (the reward of an arm, or the amount of some resource that it utilizes), these are computed
as follows. Letting νˆ denote the empirical average of the observations of that random variable10 and
letting N denote the number of times the random variable has been observed, the lower confidence
bound (LCB) and upper confidence bound (UCB) are the left and right endpoints, respectively, of
10Note that we initialize the algorithm by pulling each arm once, so empirical averages are always well-defined.
Journal of the ACM, Vol. 65, No. 3, Article 13. Publication date: March 2018.   
Bandits with Knapsacks 13:23
the confidence interval [0, 1] ∩ [νˆ − rad(νˆ, N), νˆ + rad(νˆ, N)]. The UCB or LCB for a vector or matrix are defined componentwise.
ALGORITHM 2: PrimalDualBwK with Parameter ϵ ∈ (0, 1)
1: Initialization
2: In the first m rounds, pull each arm once.
3: v1 = 1 ∈ [0, 1]d .
4: {vt ∈ [0, 1]d is the round-t estimate of the optimal solution η∗ to (LP-dual) in Section
3.}
5: {We interpret vt (i) as an estimate of the (fictional) unit cost of resource i, for each i.}
6: Set ϵ =
ln(d)/B.
7: for rounds t = m + 1,..., τ (i.e., until resource budget exhausted) do
8: For each arm x ∈ X,
9: Compute UCB estimate for the expected reward, ut,x ∈ [0, 1].
10: Compute LCB estimate for the resource consumption vector, Lt,x ∈ [0, 1]d .
11: Expected cost for one pull of arm x is estimated by EstCostx = Lt,x · vt .
12: Pull arm x = xt ∈ X that maximizes ut,x /EstCostx , the optimistic bang-per-buck ratio.
13: Update estimated unit cost for each resource i:
vt+1 (i) = vt (i) (1 + ϵ )
,  = Lt,x (i).
The algorithm is fast: with machine word size of logT bits or more, the per-round running time
is O(md). Moreover, if each arm x consumes only dx resources that are known in advance, then
Lt,x can be implemented as a dx -dimensional vector, and EstCostx can be computed in O(dx )
time. Then the per-round running time is O(m + d + 
x dx ).
Discussion 5.1. The cost update in step 13 requires some explanation. Let us interpret this step
as a separate algorithm that solves a particular problem. The problem is to optimize the total expected payoff when in each round t > m, one chooses a distribution yt = vt /vt 1 over resources,
and receives expected payoff yt · Lt,xt . This is the well-known “best-expert” problem in which actions correspond to resources, and each action i is assigned payoff Lt,xt (i). Step 13 implements a
multiplicative-weights algorithm for solving this problem. In fact, we could have used any other
algorithm for this problem with a similar performance guarantee, as in Proposition 5.4.
But why does solving this particular best-experts problem make sense for PrimalDualBwK? Particularly, why does it make sense to maximize this notion of expected payoffs? Let us view distribution yt as a vector of normalized costs of resources. Consider the total expected normalized
cost consumed by the algorithm after roundm, denote itW . ThenW = τ
t=m+1 yt c(xt, μ). A lower
confidence bound on this quantity isWLCB = τ
t=m+1 yt · Lt,xt , which is precisely the total expected
payoff in the best-experts problem. In the analysis, we relateWLCB and the upper confidence bound
on the total expected reward in the same rounds, REWUCB = τ
t=m+1 ut,xt Specifically, we prove that
for any implementation of step 13, we have
REWUCB ≥ WLCB OPTLP/B with high probability. (23)
(This follows from Equation (31).) Thus, maximizing WLCB is a reasonable goal for the cost update
rule.
Step 13 can also be seen as a variant of the Garg-Könemann width reduction technique [32]. The
ratio ut,x /EstCostx that we optimize in step 12 may be unboundedly large, so in the multiplicative
update in step 13, we rescale this value to Lt,x (i), which is guaranteed to be at most 1; this rescaling
is mirrored in the analysis of the algorithm. Interestingly, unlike the Garg-Könemann algorithm,
Journal of the ACM, Vol. 65, No. 3, Article 13. Publication date: March 2018.   
13:24 A. Badanidiyuru et al.
which applies multiplicative updates to the dual vectors and weighted averaging to the primal
ones, in our algorithm the multiplicative updates and weighted averaging are both applied to the
dual vectors.
Discussion 5.2. From the primal-dual point of view, we could distinguish a “primal” problem in
which one chooses among arms, and a “dual” problem in which one updates the cost vector. In the
primal problem, the choice of costs is deemed adversarial, and the goal is to ensure Equation (23).
In the dual problem, the choice of arms is deemed adversarial, and the goal is to maximize WLCB
to obtain Proposition 5.4. In both problems, one is agnostic as to how the upper/lower confidence
bounds ut and Lt,x are updated over time. As mentioned above, the dual problem falls under a
standard setting of the “best-expert” problem, and is solved via a standard algorithm for this problem. Meanwhile the primal problem is solved via bang-per-buck ratios and an ad hoc application
of the “optimism under uncertainty” principle.
When the rewards and consumptions are deterministic,11 the analysis is completely modular: it
works no matter which algorithm is used to solve the primal (respectively, dual) problem. In the
general case, the primal algorithm also needs to ensure that the “error terms” come out suitably
small.
The following theorem expresses the regret guarantee for PrimalDualBwK.
Theorem 5.3. Consider an instance of BwK with d resources,m = |X | arms, and the smallest budget
B = mini Bi . The regret of algorithm PrimalDualBwK with parameter ϵ =
ln(d)/B satisfies
OPTLP − REW ≤ O

log(dT )
	 
√
m OPTLP + OPTLP m
B

+ O(m) log(dT ) log(T ). (24)
Moreover, Equation (7) holds with f (OPTLP) equal to the right-hand side of Equation (24).
The rest of the section proves this theorem. Throughout, it will be useful to represent the latent values as matrices and vectors. For this purpose, we will number the arms as X = {1,...,m}
and let r ∈ Rm denote the vector whose x-th component is r(x, μ), the expected reward, for each
arm x ∈ X. Similarly, we will let C ∈ Rd×m denote the matrix whose (i, x) entry is ci (x, μ), the
expected resource consumption, for each resource i and each arm x. Let ed
j ∈ {0, 1}
d denote the
d-dimensional j-th coordinate vector.
While PrimalDualBwK uses multiplicative weights update as a general technique, we make use
of a specific performance guarantee in our analysis. To this end, let us recall algorithm Hedge [31]
from online learning theory, also known as the multiplicative weights algorithm. It is an online
algorithm for maintaining a d-dimensional probability vector y while observing a sequence of ddimensional payoff vectors π1,..., πτ . The version presented below, along with the following performance guarantee, is adapted from Kleinberg [38]; a self-contained proof appears in Appendix C.
Proposition 5.4. Fix any parameter ϵ ∈ (0, 1) and any stopping time τ . For any sequence of payoff
vectors π1,..., πτ ∈ [0, 1]d , we have
∀y ∈ Δ[d]
τ
t=1
y
t πt ≥ (1 − ϵ )
τ
t=1
yπt − lnd
ϵ .
11Then the dual problem maximizes W rather than WLCB, and the primal problem ensures Equation (25) rather than Equation (23); see Section 5.1.
Journal of the ACM, Vol. 65, No. 3, Article 13. Publication date: March 2018.   
Bandits with Knapsacks 13:25
ALGORITHM 3: Hedge with Parameter ϵ ∈ (0, 1)
1: v1 = 1 {vt ∈ Rd
+ for each round t.}
2: for each round t = 1, 2, 3,... do
3: Output distribution yt = vt /vt 1.
4: Input payoff vector πt ∈ [0, 1]d .
5: for each resource i do
6: vt+1 (i) = vt (i) (1 + ϵ )
,  = πt (i).
5.1 Warm-Up: The Deterministic Case
To present the application of Hedge to BwK in its purest form, we first consider the “deterministic
case” in which the rewards of the various arms are deterministically equal to the components of a
vectorr ∈ Rm, and the resource consumption vectors are deterministically equal to the columns of
a matrix C ∈ Rd×m. Then there is no need to use upper/lower confidence bounds, so the algorithm
can be simplified considerably, see Algorithm 4. In the remainder of this subsection, we discuss
this algorithm and analyze its regret.
ALGORITHM 4: Algorithm PrimalDualBwK for Deterministic Outcomes, with Parameter ϵ ∈
(0, 1)
1: Initialization
2: In the first m rounds, pull each arm once.
3: For each arm x ∈ X, let rx ∈ [0, 1] and Cx ∈ [0, 1]d
4: denote the reward and the resource consumption vector revealed in Step 2.
5: v1 = 1 ∈ [0, 1]d .
6: {vt ∈ [0, 1]d is the round-t estimate of the optimal solution η∗ to (LP-dual) in Section
3.}
7: {We interpret vt (i) as an estimate of the (fictional) unit cost of resource i, for each i.}
8: Set ϵ =
ln(d)/B.
9: for rounds t = m + 1,..., τ (i.e., until resource budget exhausted) do
10: For each arm x ∈ X,
11: Expected cost for one pull of arm x is estimated by EstCostx = Cx · vt .
12: Pull arm x = xt ∈ X that maximizes rx /EstCostx , the bang-per-buck ratio.
13: Update estimated unit cost for each resource i:
vt+1 (i) = vt (i) (1 + ϵ )
,  = Cx (i).
Algorithm 4 is an instance of the multiplicative-weights update method for solving packing
linear programs. Interpreting it through the lens of online learning, as in the survey by Arora
et al. [8], it is updating a vector yt = vt /vt 1 using the Hedge algorithm, where the payoff vector
in any round t > m is given by πt = Cxt and the goal is to optimize the total (expected) payoff
W = τ
t=m+1 yt · Cxt . Note that W is also the total cost consumed by Algorithm 4.
To see whyW is worth maximizing, let us relate it to the total reward collected by the algorithm
in rounds t > m; denote this quantity by REW = τ −1 t=m+1 rt . We will prove that
REW ≥ W · OPTLP/B for any implementation of Step 13. (25)
For this reason, maximizing W also helps maximize REW. Proving it is a major step in the analysis.
Journal of the ACM, Vol. 65, No. 3, Article 13. Publication date: March 2018.     
13:26 A. Badanidiyuru et al.
Let ξ ∗ denote an optimal solution of the primal linear program (LP-primal) from Section 3, and
let OPTLP = rξ ∗ denote the optimal value of that LP.
For each round t, let zt = em
xt denote the xt th coordinate vector. We claim that
zt ∈ argmax
z ∈Δ[X]
 rz
y
t Cz 
. (26)
In words: zt maximizes the “bang-per-buck ratio” among all distributions z over arms. Indeed, the
argmax in Equation (26) is well-defined as that of a continuous function on a compact set. Say it is
attained by some distribution z over arms, and let ρ ∈ R be the corresponding max. By maximality
of ρ, the linear inequality ρ y
t Cz ≥ rz also holds at some extremal point of the probability simplex
Δ[X], i.e., at some point-mass distribution. For any such point-mass distribution, the corresponding
arm maximizes the bang-per-buck ratio in the algorithm. Claim proved.
Proof of Eqation (25). It follows that
y
t πt = y
t Czt ≤ rt

y
t Cξ ∗

/OPTLP,
W =

t
y
t πt ≤
1
OPTLP

t
rt

y
t Cξ ∗

= 1
OPTLP




t
rt y
t 

Cξ ∗
.
Here the sums are over rounds t with m < t < τ . Now, letting y¯ = 1
REW

t rt yt ∈ [0, 1]d be the
rewards-weighted average of distributions ym+1,...,yτ , it follows that
W ≤
REW
OPTLP
y¯
TCξ ∗ ≤
REW
OPTLP
B.
The last inequality follows because all components of Cξ ∗ are at most B by the primal feasibility
of ξ ∗.
Now, combining Equation (25) and the regret bound for Hedge, we obtain
REW ≥ W · OPTLP/B ≥
⎡
⎢
⎢
⎢
⎢
⎣
(1 − ϵ )

m<t<τ
y πt − lnd
ϵ
⎤
⎥
⎥
⎥
⎥
⎦
·
OPTLP
B
∀y ∈ Δ[d]. (27)
To continue this argument, we need to choose an appropriate vector y to make the right-hand
side large. Recall that πt = Czt , so 
m<t<τ πt is simply the total consumption vector in all rounds
m < t < τ . We know some resource i must be exhausted by the time the algorithm stops, so the
consumption of this resource is at least B. In a formula: τ
t=1 y πt ≥ B, where y = ed
i is the identity
vector for resource i. Plugging in this y into Equation (27), we obtain
REW ≥

(1 − ϵ )(B − m − 1) − lnd
ϵ

·
OPTLP
B
≥ OPTLP −

ϵB + m + 1 +
lnd
ϵ

·
OPTLP
B
= OPTLP − O(
√
B lnd + m) ·
OPTLP
B
if ϵ =
ln d
B .
This completes regret analysis for the deterministic case.
5.2 Analysis Modulo Error Terms
We now commence the analysis of Algorithm PrimalDualBwK. In this subsection, we show how to
reduce the problem of bounding the algorithm’s regret to a problem of estimating two error terms
that reflect the difference between the algorithm’s confidence-bound estimates of its own reward
Journal of the ACM, Vol. 65, No. 3, Article 13. Publication date: March 2018.              
Bandits with Knapsacks 13:27
and resource consumption with the empirical values of these random variables. The error terms
will be treated in Section 5.3.
Recall that the algorithm computes LCBs on expected resource consumption Lt,x ∈ [0, 1]d and
UCBs on expected rewards ut,x ∈ [0, 1], for each round t and each arm x. We also represent the
LCBs as a matrix Lt ∈ [0, 1]d×m whose xth column equals Lt,x , for each arm x. We also represent the UCBs as a vector ut ∈ [0, 1]m over arms whose xth component equals ut,x . Let Ct be the
resource-consumption matrix for round t. That is, Ct ∈ [0, 1]d×m denotes the matrix whose (i, x)
entry is the actual consumption of resource i in round t if arm x were chosen in this round.
As in the previous subsection, let zt = em
xt denote the xt th coordinate vector, and let yt =
vt /vt 1 be the vector of normalized costs. Similar to Equation (26), zt maximizes the “bang-perbuck ratio” among all distributions z over arms:
zt ∈ argmax
z ∈Δ[X]
⎧⎪
⎨
⎪
⎩
u
t,x z
y
t Lt,x z
⎫⎪
⎬
⎪
⎭
. (28)
By Theorem 2.1 and our choice of Crad, it holds with probability at least 1 −T −1 that the confidence interval for every latent parameter, in every round of execution, contains the true value of
that latent parameter. We call this high-probability event a clean execution of PrimalDualBwK. Our
regret guarantee will hold deterministically assuming that a clean execution takes place. The regret
can be at most T when a clean execution does not take place, and since this event has probability
at most T −1 it contributes only O(1) to the regret. We will henceforth assume a clean execution of
PrimalDualBwK.
Claim 5.5. In a clean execution of Algorithm PrimalDualBwK with parameter ϵ =
ln(d)/B, the
algorithm’s total reward satisfies the bound
OPTLP − REW ≤
⎡
⎢
⎢
⎢
⎢
⎣
2OPTLP 


lnd
B
+ m + 1
B


+ m + 1
⎤
⎥
⎥
⎥
⎥
⎦
+
OPTLP
B







m<t<τ
Et zt





∞
+







m<t<τ
δ
t zt






, (29)
where Et = Ct − Lt and δt = ut − rt for each round t > m.
Proof. The claim is proven by mimicking the analysis of Algorithm 4 in the preceding section,
incorporating error terms that reflect the differences between observable values and latent ones. As
before, letξ ∗ denote an optimal solution of the primal linear program (LP-primal), and let OPTLP =
rξ ∗ denote the optimal value of that LP. Let REWUCB = 
m<t<τ u
t zt denote the total payoff the
algorithm would have obtained, after its initialization phase, if the actual payoff at time t were
replaced with the upper confidence bound. Let y = ed
i , where i is a resource exhausted by the
algorithm when it stops; then y(
τ
t=1 Ctzt ) ≥ B. As before,
y 



m<t<τ
Ctzt


≥ B − m − 1. (30)
Finally, let
y¯ = 1
REWUCB

m<t<τ

u
t zt

yt .
Journal of the ACM, Vol. 65, No. 3, Article 13. Publication date: March 2018.          
13:28 A. Badanidiyuru et al.
Assuming a clean execution, we have
B ≥ y¯
Cξ ∗ (ξ ∗ is primal feasible)
= 1
REWUCB

m<t<τ
(u
t zt )(y
t Cξ ∗) (definition of y¯)
≥
1
REWUCB

m<t<τ
(u
t zt )(y
t Ltξ ∗) (clean execution)
≥
1
REWUCB

m<t<τ
(u
t ξ ∗)(y
t Lt zt ) (by Equation (28))
≥
1
REWUCB

m<t<τ
(rξ ∗)(y
t Lt zt ) (clean execution) (31)
≥
OPTLP
REWUCB
⎡
⎢
⎢
⎢
⎢
⎣
(1 − ϵ )y 



m<t<τ
Lt zt 

− lnd
ϵ
⎤
⎥
⎥
⎥
⎥
⎦
(Hedge guarantee)
> (1 − ϵ ) OPTLP
REWUCB
⎡
⎢
⎢
⎢
⎢
⎣
y 



m<t<τ
Ct zt 

− y 



m<t<τ
Et zt 

− lnd
ϵ
⎤
⎥
⎥
⎥
⎥
⎦
≥
OPTLP
REWUCB
⎡
⎢
⎢
⎢
⎢
⎣
(1 − ϵ )B − m − 1 − (1 − ϵ )y 



m<t<τ
Et zt 

− lnd
ϵ
⎤
⎥
⎥
⎥
⎥
⎦
(definition of y; see Equation (30))
REWUCB ≥ OPTLP
⎡
⎢
⎢
⎢
⎢
⎣
1 − ϵ − m + 1
B − 1
B







m<t<τ
Et zt





∞
− lnd
ϵB
⎤
⎥
⎥
⎥
⎥
⎦
. (32)
The algorithm’s actual payoff, REW = τ
t=1 r

t zt , satisfies the inequality
REW ≥ REWUCB −

m<t<τ
(ut − rt )
zt = REWUCB −

m<t<τ
δ
t zt .
Combining this with Equation (32), and plugging in ϵ =
ln(d)/B, we obtain the bound Equation (29), as claimed.
5.3 Error Analysis
We complete the proof of Theorem 5.3 by proving upper bounds on the terms 

m<t<τ Etzt ∞
and |

m<t<τ δtzt | that appear on the right side of Equation (29). Both bounds follow from a more
general lemma, which we present below.
The general lemma considers a sequence of vectors a1,..., aτ in [0, 1]m and another vector
a0 ∈ [0, 1]m. Here at,x ∈ [0, 1] represents a numerical outcome (i.e., a reward or a consumption
of a given resource) if arm x is pulled in round t, and a0,x represents the corresponding expected
outcome. Further, for each round t > m, we have an estimate bt ∈ [0, 1]m for the outcome vector at . We only assume a clean execution of the algorithm, and we derive an upper bound on
|

m<t<τ (bt − at )
zt |.
Lemma 5.6. Consider two sequences of vectors a1,..., aτ and b1,...,bτ , in [0, 1]m, and a vector
a0 ∈ [0, 1]m. For each arm x and each round t > m, let at,x ∈ [0, 1] be the average observed outcome
up to round t, i.e., the average outcome as,x over all rounds s ≤ t in which arm x has been chosen by
the algorithm; let Nt,x be the number of such rounds. Assume that for each arm x and all rounds t
with m < t < τ , we have
|bt,x − a0,x | ≤ 2 rad(at,x , Nt,x ) ≤ 6 rad(a0,x , Nt,x ),
|at,x − a0,x | ≤ rad(at,x , Nt,x ).
Journal of the ACM, Vol. 65, No. 3, Article 13. Publication date: March 2018.                   
Bandits with Knapsacks 13:29
Let A = τ −1 t=1 at,xt be the total outcome collected by the algorithm. Then,







m<t<τ
(bt − at )
zt






≤ O
CradmA + Cradm logT

. (33)
Before proving the lemma, we need to establish a simple fact about confidence radii.
Claim 5.7. For any two vectors a, M ∈ Rm
+ , we have
m
x=1
rad(ax , Mx ) Mx ≤
Cradm(aM) + Cradm. (34)
Proof. The definition of rad(·, ·) implies that rad(ax , Mx ) Mx ≤ √
Crad axMx + Crad. Summing
these inequalities and applying Cauchy-Schwarz,
m
x=1
rad(ax , Mx ) Mx ≤
m
x=1

Crad axMx + Cradm ≤ √
m ·

x ∈X
Crad axMx + Cradm,
and the lemma follows by rewriting the expression on the right side.
Proof of Lemma 5.6. For convenience, denote Nt = (Nt,1,..., Nt,m ), and observe that
Nt =
t
s=1
zs and A = a
τ −1Nτ −1 =
τ −1
s=1
a
s zs .
We decompose the left side of Equation (33) as a sum of three terms,

m<t<τ
(bt − at )
zt =
m
t=1
(at − bt )
zt +
τ −1
t=1
(bt − a0)
zt +
τ −1
t=1
(a0 − at )
zt, (35)
then bound the three terms separately. The first sum is clearly bounded above bym. We next work
on bounding the third sum. Let s = τ − 1.
|(a0 − as )
 Ns | ≤

x ∈X
rad(as,x , Ns,x ) Ns,x (assuming clean execution)
≤
CradmA + Cradm. (by Claim 5.7) (36)
s
t=1
(a0 − at )
 zt = a
0 Ns −
s
t=1
a
t zt = (a0 − as )
 Ns .






s
t=1
(a0 − at )
 zt






= |(a0 − as )
 Ns | ≤
CradmA + Cradm.
Finally, we bound the middle sum in Equation (35).






s
t=1
(bt − a0)
zt






≤ 6
s
t=1

x ∈X
rad(a0,x , Nt,x )zt,x
= 6

x ∈X
N
s,x
=1
rad(a0,x , )
= O 



x ∈X

Crad a0,x Ns,x + Crad log(Ns,x )

≤ O

Cradm a
0 Ns + Cradm logT
	
. (37)
Journal of the ACM, Vol. 65, No. 3, Article 13. Publication date: March 2018.                           
13:30 A. Badanidiyuru et al.
We would like to replace the expression a
0 Ns on the last line with the expression as
 Ns = A.
To do so, recall Equation (36) and apply the following calculation:
a
0 Ns ≤ as
Ns +

CradmA + Cradm
= A +

CradmA + Cradm
≤ √
A +

Cradm
2

Cradma
0 Ns ≤
Cradm √
A +

Cradm

=
CradmA + Cradm.
Plugging this into Equation (37), we bound the middle sum in Equation (35) as






s
t=1
(bt − a0)
zt






≤ O
CradmA + Cradm logT

. (38)
Summing up the upper bounds for the three terms on the right side of Equation (35), we obtain Equation (33).
Corollary 5.8. In a clean execution of PrimalDualBwK,







m<t<τ
δtzt






≤ O
CradmREW + Cradm logT

and







m<t<τ
Etzt





∞
≤ O
CradmB + Cradm logT

.
Proof. The first inequality is obtained by applying Lemma 5.6 with vector sequences at = rt
and bt = ut , and vector a0 = r. In other words, a0 is the vector of expected rewards across all arms.
The second inequality is obtained by applying the same lemma separately for each resource i,
with vector sequences at = (ed
i )
 Ct and bt = ed
i Lt , and vector a0 being the ith row of matrix C.
In other words, a0 is the vector of expected consumption of resource i across all arms.
Proof of Theorem 5.3: If m ≥ B/ log(dT ), then the regret bound in Theorem 5.3 is trivial.
Therefore, we can assume without loss of generality that m ≤ B/ log(dT ). Therefore, recalling
Equation (29), we observe that
2 OPTLP 


lnd
B
+ m + 1
B


= O

m log(dmT ) OPTLP
√
B

.
The term m + 1 on the right side of Equation (29) is bounded above by m log(dmT ). Finally, using
Corollary 5.8 we see that the sum of the final two terms on the right side of Equation (29) is
bounded by
O


Cradm

OPTLP
√
B
+ √
OPTLP
+ Cradm logT

.
The theorem follows by plugging in Crad = Θ(log(dmT )) = O(log(dT )) (because m ≤ B ≤ T ).
Journal of the ACM, Vol. 65, No. 3, Article 13. Publication date: March 2018.                         
Bandits with Knapsacks 13:31
6 LOWER BOUND
We prove that regret Equation (1) obtained by algorithm PrimalDualBwK is optimal up to polylog
factors. Specifically, we prove that any algorithm for BwK must, in the worst case, incur regret
Ω

min 
OPT, OPTm
B
+
√
m OPT , (39)
where m = |X | is the number of arms and B = mini Bi is the smallest budget.
Theorem 6.1. Fix any m ≥ 2, d ≥ 1, OPT ≥ m, and (B1,..., Bd ) ∈ [2, ∞). Let G be the family of
all BwK problem instances with m arms, d resources, budgets (B1,..., Bd ) and optimal reward OPT.
Then any algorithm for BwK must incur regret Equation (39) in the worst case over G.
We treat the two summands in Equation (39) separately:
Claim 6.2. Consider the family G from Theorem 6.1, and let ALG be some algorithm for BwK.
(a) ALG incurs regret Ω(min(OPT,
√
m OPT )) in the worst case over G.
(b) ALG incurs regret Ω(min(OPT, OPTm
B )) in the worst case over G.
Theorem 6.1 follows from Claim 6.2(ab). For part (a), we use a standard lower-bounding example
for MAB. For part (b), we construct a new example, specific to BwK, and analyze it using KLdivergence.
Proof of Claim 6.2(a). Fix m ≥ 2 and OPT ≥ m. Let G0 be the family of all MAB problem instances with m arms and time horizon T = 2 OPT, where the “best arm” has expected reward
μ∗ = OPT/T and all other arms have reward μ∗ − ϵ with ϵ = 1
4
√
m/T . Note that μ∗ ∈ [ 1
2 , 3
4 ] and
ϵ ≤ 1
4 . It is well-known [10] that any MAB algorithm incurs regret Ω( √
m OPT ) in the worst case
over G0.
To ensure that G0 ⊂ G, let us treat each MAB instance in G0 as a BwK instance with d resources,
budgets (B1,..., Bd ), and no resource consumption.
6.1 The New Lower-Bounding Example: Proof Of Claim 6.2(b)
Our lower-bounding example is very simple. There are m arms. Each arm gives reward 1 deterministically. There is a single resource with budget B.
12 The resource consumption, for each arm
and each round, is either 0 or 1. The expected resource consumption is p − ϵ for the “best arm”
and p for all other arms, where 0 < ϵ < p < 1. There is time horizon T < ∞. Let G(p,ϵ ) denote the
family of all such problem instances, for fixed parameters (p, ϵ ). We analyze this family in the rest
of this section.
We rely on the following fact about stopping times of random sums. For the sake of completeness, we provide a proof in Section D.
Fact 6.3. Let St be the sum of t i.i.d. 0-1 variables with expectation q. Let τ ∗ be the first time this
sum reaches a given number B ∈ N. Then E[τ ∗] = B/q. Moreover, for each T > E[τ ∗] it holds that

t>T
Pr[τ ∗ ≥ t] ≤ E[τ ∗
]
2
/T .
Infinite time horizon. It is convenient to consider the family of problem instances, which is the
same as G(p,ϵ ) except that it has the infinite time horizon; denote it G∞
(p,ϵ )
. We will first prove the
desired lower bound for this family, then extend it to G(p,ϵ )
.
12More formally, other resources in the setting of Theorem 6.1 are not consumed. For simplicity, we leave them out.
Journal of the ACM, Vol. 65, No. 3, Article 13. Publication date: March 2018. 
13:32 A. Badanidiyuru et al.
The two crucial quantities that describe algorithm’s performance on an instance in G∞
(p,ϵ ) is the
stopping time and the total number of plays of the best arm. (Note that the total reward is equal
to the stopping time minus 1.) The following claim connects these two quantities.
Claim 6.4 (Stopping time). Fix an algorithm ALG for BwK and a problem instance in G∞
(p,ϵ )
. Consider an execution of ALG on this problem instance. Let τ be the stopping time of ALG. For each round
t, let Nt be the number of rounds s ≤ t in which the best arm is selected. Then,
pE[τ ] − ϵE[Nτ ] = B + 1.
Proof. Let Ct be the total resource consumption after round t. Note that E[Ct] = pt − ϵNt . We
claim that
E[Cτ ] = E[pτ − ϵNτ ]. (40)
Indeed, let Zt = Ct − (pt − ϵNt ). It is easy to see that Zt is a martingale with bounded increments,
and moreover that Pr[τ < ∞] = 1. Therefore, the Optional Stopping Theorem applies to Zt and τ ,
so that E[Zτ ] = E[Z0] = 0. Therefore, we obtain Equation (40).
To complete the proof, it remains to show that Cτ = B + 1. Recall that ALG stops if and only if
Ct > B. Since resource consumption in any round is either 0 or 1, it follows that Cτ = B + 1.
Corollary 6.5. Consider the setting in Claim 6.4. Then:
(a) If ALG always chooses the best arm, then E[τ ] = B + 1/(p − ϵ ).
(b) OPT = B + 1/(p − ϵ ) − 1 for any problem instance in G∞
(p,ϵ )
.
(c) pE[τ ] − ϵE[Nτ ] = (p − ϵ ) (1 + OPT).
Proof. For part(b), note that we have E[τ ] ≤ B + 1/(p − ϵ ), so OPT ≤ B + 1/(p − ϵ ) − 1. By
part (a), the equality is achieved by the policy that always selects the best arm.
The heart of the proof is a KL-divergence argument that bounds the number of plays of the best
arm. This argument is encapsulated in the following claim, whose proof is deferred to Section 6.3.
Lemma 6.6 (Best Arm). Assume p ≤ 1
2 and ϵ
p ≤ 1
16m
B . Then for any BwK algorithm there exists
a problem instance in G∞
(p,ϵ ) such that the best arm is chosen at most 3
4 OPT times in expectation.
Armed with this bound and Corollary 6.5(c), it is easy to lower-bound regret over G∞
(p,ϵ )
.
Claim 6.7 (Regret). If p ≤ 1
2 and ϵ
p ≤ 1
16m
B , then any BwK algorithm incurs regret ϵ
4p OPT over
G∞
(p,ϵ )
.
Proof. Fix any algorithm ALG for BwK. Consider the problem instance whose existence is guaranteed by Lemma 6.6. Let τ be the stopping time of ALG, and let Nt be the number of rounds s ≤ t
in which the best arm is selected. By Lemma 6.6, we have E[Nτ ] ≤ 3
4 OPT. Plugging this into Corollary 6.5(c) and rearranging the terms, we obtain E[τ ] ≤ (1 + OPT)(1 − ϵ
4p ). Therefore, regret of ALG
is OPT − (E[τ ] − 1) ≥ ϵ
4p OPT.
Thus, we have proved the lower bound for the infinite time horizon.
Finite time horizon. Let us “translate” a regret bound for G∞
(p,ϵ ) into a regret bound for G(p,ϵ )
.
We will need a more nuanced notation for OPT. Consider the family of problem instances in
G(p,ϵ ) ∪ G∞
(p,ϵ ) with a particular time horizon T ≤ ∞. Let OPT(p,ϵ,T ) be the optimal expected total
reward for this family (by symmetry, this quantity does not depend on which arm is the best arm).
We will write OPTT = OPT(p,ϵ,T ) when parameters (p, ϵ ) are clear from the context.
Journal of the ACM, Vol. 65, No. 3, Article 13. Publication date: March 2018.   
Bandits with Knapsacks 13:33
Claim 6.8. For any fixed (p, ϵ ) and any T > OPT∞ it holds that OPTT ≥ OPT∞ − OPT2
∞/T .
Proof. Let τ ∗ be the stopping time of a policy that always plays the best arm on a problem
instance in G∞
(p,ϵ )
.
OPT∞ − OPTT = E[τ ∗
] − E[min(τ ∗
,T )]
=

t>T
(t −T ) Pr[τ ∗ = t]
=

t>T
Pr[τ ∗ ≥ t]
≤ E[τ ∗
]/T 2 = OPT2
∞/T .
The inequality is due to Fact 6.3.
Claim 6.9. Fix (p, ϵ ) and fix algorithm ALG. Let REGT be the regret of ALG over the problem instances
in G(p,ϵ ) ∪ G∞
(p,ϵ ) with a given time horizon T ≤ ∞. Then, REGT ≥ REG∞ − OPT2
∞/T .
Proof. For each problem instance I∈ G∞
(p,ϵ )
, let REWT (I) be the expected total reward of ALG
on I, if the time horizon is T ≤ ∞. Clearly, REW∞(I) ≥ REWT (I). Therefore, using Claim 6.8, we
have:
REGT = OPTT − inf
I
REWT (I)
≥ OPTT − inf
I
REW∞(I)
= REG∞ + OPTT − OPT∞
≥ REG∞ − OPT2
∞/T .
Lemma 6.10 (Regret: Finite Time Horizon). Fix p ≤ 1
2 and ϵ = p
16 min(1,
√
m/B). Then for any
time horizon T > 8p
ϵ OPT∞ and any BwK algorithm ALG there exists a problem instance in G(p,ϵ ) with
time horizon T for which ALG incurs regret Ω(OPTT ) min(1,
√
m/B).
Proof. By Claim 6.7, ALG incurs regret at least ϵ
4p OPT∞ for some problem instance in G∞
(p,ϵ )
.
By Claim 6.9, ALG incurs regret at least ϵ
8p OPT∞ for the same problem instance in G(p,ϵ ) with time
horizon T . Since OPT∞ ≥ OPTT , this regret is at least ϵ
8p OPTT = Ω(OPTT ) min(1,
√
m/B).
Let us complete the proof of Claim 6.2(b). Recall that Claim 6.2(b) specifies the values for
(m, B, OPT) that our problem instance must have. Since we have already proved Claim 6.2(a) and
OPTm
B ≤ O(
√
m OPT) for OPT < 3B, it suffices to assume OPT ≥ 3B.
Let ϵ (p) = p
16 min(1,
√
m/B), as prescribed by Lemma 6.10. Then taking ϵ = ϵ (p), we obtain regret Ω(OPTT ) min(1,
√
m/B) for any parameter p ≤ 1
2 and any time horizon T > 8p
ϵ OPT(p,ϵ,∞). It
remains to pick such p and T to ensure that f (p,T ) = OPT, where f (p,T ) = OPT(p,ϵ (p),T ).
Recall from Corollary 6.5(b) that OPT(p,ϵ,∞) = Γ
p − 1, where
Γ = B + 1/

1 − 1
16
min(1,

m/B)
	
is a “constant” for the purposes of this argument, in the sense that it does not depend on p or T .
So, we can state the sufficient condition for proving Claim 6.2(b) as follows:
Pick p ≤
1
2
and T ≥
8Γ
ϵ (p) such that f (p,T ) = OPT. (41)
Journal of the ACM, Vol. 65, No. 3, Article 13. Publication date: March 2018.    
13:34 A. Badanidiyuru et al.
Recall that OPT(p,ϵ,∞) ≥ OPT(p,ϵ,T ) for any T , and OPT(p,ϵ,T ) ≥ 1
2 OPT(p,ϵ,∞) for any T >
2 OPT(p,ϵ,∞) by Claim 6.8. We summarize this as follows: for any T > 2( Γ
p − 1),
Γ
p − 1 ≥ OPT(p,ϵ,T ) ≥
1
2

Γ
p − 1

. (42)
Define p0 = Γ/OPT. Since OPT ≥ 3B, Γ ≤ 16
15 (B + 1) and B ≥ 4, it follows thatp0 ≥ 1
2 . LetT = 8Γ
ϵ (p0 ) .
Then, Equation (42) holds for all p ∈ [p0/4, 1
2 ]. In particular,
f (p0,T ) ≤ Γ/p0 = OPT ≤ f (p0/4,T ).
Since f (p,T ) is continuous in p, there exists p ∈ [p0/4,p0] such that f (p,T ) = OPT. Since p ≤ p0,
we have T ≥ 8Γ
ϵ (p) , satisfying all requirements in Equation (41). This completes the proof of
Claim 6.2(b), and therefore the proof of Theorem 6.1.
6.2 Background on KL-Divergence (For the Proof of Lemma 6.6)
The proof of Lemma 6.6 relies on the concept of KL-divergence. Let us provide some background
to make on KL-divergence to make this proof self-contained. We use a somewhat non-standard
notation that is tailored to the needs of our analysis.
The KL-divergence (a.k.a. relative entropy) is defined as follows. Consider two distributions μ, ν
on the same finite universe Ω.
13 Assume μ  ν (in words, μ is absolutely continuous with respect
to ν), meaning that ν (w) = 0 ⇒ μ(w) = 0 for all w ∈ Ω. Then KL-divergence of μ given ν is
KL(μ  ν )  E w∼(Ω, μ)
log 
μ(w)
ν (w)

=

w ∈Ω
log 
μ(w)
ν (w)

μ(w).
In this formula, we adopt a convention that 0
0 = 1. We will use the fact that
KL(μ  ν ) ≥
1
2 μ − ν 2
1 . (43)
Henceforth, let μ, ν be distributions on the universe Ω∞, where Ω is a finite set. For w =
(w1,w2,...) ∈ Ω∞ and t ∈ N, let us use the notationwt = (w1,...,wt ) ∈ Ωt . Let μt be a restriction
of μ to Ωt : that is, a distribution on Ωt given by
μt (wt )  μ ({u ∈ Ω∞ : ut = wt }).
The next-round conditional distribution of μ given wt , t < T is defined by
μ (wt+1 |wt )  μt+1 (wt+1)
μt (wt ) .
Note that μ(· |wt ) is a distribution on Ω for every fixed wt .
The conditional KL-divergence at round t + 1 is defined as
KLt+1 (μ  ν )  E wt ∼(Ωt , μt )
KL(μ(· |wt )  ν (· |wt )).
In other words, this is the KL-divergence between the next-round conditional distributions μ(· |wt )
and ν (· |wt ), in expectation over the random choice of wt according to distribution μt .
13We use μ, ν to denote distributions throughout this section, whereas μ denotes the latent structure elsewhere in the
article.
Journal of the ACM, Vol. 65, No. 3, Article 13. Publication date: March 2018.                      
Bandits with Knapsacks 13:35
We will use the following fact, known as the chain rule for KL-divergence:
KL(μT  νT ) =

T
t=1
KLt (μ  ν ), for each T ∈ N. (44)
Here, for notational convenience, we define KL1 (μ  ν )  KL(μ1  ν1).
6.3 The KL-Divergence Argument: Proof of Lemma 6.6
Fix some BwK algorithm ALG and fix parameters (p, ϵ ). Let Ix be the problem instance in G∞
(p,ϵ ) in
which the best arm is x. For the analysis, we also consider an instance I0, which coincides with
Ix but has no best arm: that is, all arms have expected resource consumption p. Let τ (I) be the
stopping time of ALG for a given problem instance I, and let Nx (I) be the expected number of
times a given arm x is chosen by ALG on this problem instance.
Consider problem instance I0. Since all arms are the same, we can apply Corollary 6.5(a) (suitably
modified to the non-best arm) and obtain E[τ (I0)] = B + 1/p. We focus on an arm x with the
smallest Nx (I0). For this arm, it holds that
Nx (I0) ≤
1
m

x ∈X
Nx (I0) = 1
mE[τ (I0)] ≤ B + 1
p m . (45)
In what follows, we use this inequality to upper-bound Nx (Ix ). Informally, if arm x is not played
sufficiently often in I0, then ALG cannot tell apart I0 and Ix .
The transcript of ALG on a given problem instance Iis a sequence of pairs {(xt,ct )}t ∈N, where
for each round t ≤ τ (I) it holds that xt is the arm chosen by ALG and ct is the realized resource
consumption in that round. For all t > τ (I), we define (xt,ct ) = (null, 0). To map this to the setup
in Section 6.2, denote Ω = (X ∪ {null}) × {0, 1}. Then the set of all possible transcripts is a subset
of Ω∞.
Every given problem instance Iinduces a distribution over Ω∞. Let μ, ν be the distributions over
Ω∞ that are induced by I0 and Ix , respectively. We will use the following shorthand:
diff[T0,T∗]

T∗
t=T0
ν (xt = x) − μ(xt = x), where 1 ≤ T0 ≤ T∗ ≤ ∞.
For any T ∈ N (which we will fix later), we can write
Nx (Ix ) − Nx (I0) = diff[1, ∞] = diff[1,T ] + diff[T + 1, ∞]. (46)
We will bound diff[1,T ] and diff[T + 1, ∞] separately.
Upper bound on diff[1,T ]. This is where we use KL-divergence. Namely, by Equation (43), we
have
diff[1,T ] ≤
T
2 μT − νT 1 ≤ T
1
2
KL(μT  νT ). (47)
Now, by the chain rule (Equation (44)), we can focus on upper-bounding the conditional KLdivergence KLt (μ  ν ) at each round t ≤ T .
Claim 6.11. For each round t ≤ T , it holds that
KLt (μ  ν ) = μ(xt = x)

p log  p
p − ϵ

+ (1 − p) log  1 − p
1 − p + ϵ
 . (48)
Proof. The main difficulty here is to carefully “unwrap” the definition of KLt (μ  ν ).
Journal of the ACM, Vol. 65, No. 3, Article 13. Publication date: March 2018.  
13:36 A. Badanidiyuru et al.
Fix t ≤ T and let wt ∈ Ωt be the partial transcript up to and including round t. For each arm y,
let f (y|wt ) be the probability that ALG chooses army in round t, given the partial transcriptwt . Let
c(y|I) be the expected resource consumption for army under a problem instance I. The transcript
for round t + 1 is a pair wt+1 = (xt+1,ct+1), where xt+1 is the arm chosen by ALG in round t + 1,
and ct+1 ∈ {0, 1} is the resource consumption in that round. Therefore, if ct+1 = 1, then
μ(wt+1 |wt ) = f (xt+1 |wt ) c(xt+1 |I0) = f (xt+1 |wt ) p,
ν (wt+1 |wt ) = f (xt+1 |wt ) c(xt+1 |Ix ) = f (xt+1 |wt ) 
p − ϵ 1{xt+1=x }

.
Similarly, if ct+1 = 0, then
μ(wt+1 |wt ) = f (xt+1 |wt ) (1 − c(xt+1 |I0)) = f (xt+1 |wt ) (1 − p),
ν (wt+1 |wt ) = f (xt+1 |wt ) (1 − c(xt+1 |Ix )) = f (xt+1 |wt ) 
1 − p + ϵ 1{xt+1=x }

.
It follows that
log μ(wt+1 |wt )
ν (wt+1 |wt ) = 1{xt =x }

log  p
p − ϵ

1{ct+1=1} + log  1 − p
1 − p + ϵ

1{ct+1=0}

.
Taking expectations over wt+1 = (xt,ct ) ∼ μ(· |wt ), we obtain
KL(μ(· |wt )  ν (· |wt )) = f (x |wt )

p log  p
p − ϵ

+ (1 − p) log  1 − p
1 − p + ϵ
 .
Taking expectations over wt ∼ μt , we obtain the conditional KL-divergence KLt (μ  ν ). Equation (48) follows, because
E wt ∼μt
f (x |wt ) = μ(xt = x).
We will use the following fact about logarithms, which is proved using standard quadratic approximations for the logarithm. The proof is in Section D.
Fact 6.12. Assume ϵ
p ≤ 1
2 and p ≤ 1
2 . Then,
p log  p
p − ϵ

+ (1 − p) log  1 − p
1 − p + ϵ

≤
2ϵ2
p .
Now, we can put everything together and derive an upper bound on diff[1,T ].
Claim 6.13. Assume ϵ
p ≤ 1
2 and p ≤ 1
2 . Then, diff[1,T ] ≤ T ϵ
p
B+1
m .
Proof. By Claim 6.11 and Fact 6.12, for each round t ≤ T , we have
KLt (μ  ν ) ≤
2ϵ2
p μ(xt = x).
By the chain rule (Equation (44)), we have
KL(μT  νT ) ≤
2ϵ2
p

T
t=1
μ(xt = x) ≤
2ϵ2
p
Nx (I0) ≤ 2
B + 1
m

ϵ
p
2
.
The last inequality is the place where we use our choice of x, as expressed by Equation (45).
Plugging this back into Equation (47), we obtain diff[1,T ] ≤ T ϵ
p
B+1
m .
Journal of the ACM, Vol. 65, No. 3, Article 13. Publication date: March 2018.                          
Bandits with Knapsacks 13:37
Upper bound on diff[T, ∞]. Consider the problem instance Ix , and consider the policy that
always chooses the best arm. Let ν ∗ be the corresponding distribution over transcripts Ω∞, and let
τ be the corresponding stopping time. Note that ν ∗ (xt = x) if and only if τ > t. Therefore:
diff[T, ∞] ≤
∞
t=T
ν (xt = x) ≤
∞
t=T
ν ∗ (xt = x) =
∞
t=T
ν ∗ (τ > t) ≤ OPT2
/T .
The second inequality can be proved using a simple “coupling argument.” The last inequality follows from Fact 6.3, observing that E[τ ] = OPT.
Putting the pieces together. Assume p ≤ 1
2 and ϵ
p ≤ 1
2 . Denote γ = ϵ
p
B+1
m . Using the upper
bounds on diff[1,T ] and diff[T + 1, ∞] and plugging them into Equation (46), we obtain
Nx (Ix ) − Nx (I0) ≤ γT + OPT2
/T ≤ OPT√
γ
for T = OPT/
√γ . Recall that Nx (I0) < OPT/m. Thus, we obtain
Nx (Ix ) ≤ (
1
m + √
γ ) OPT.
Recall that we need to conclude that Nx (Ix ) ≤ 3
4OPT. For that, it suffices to have γ ≤ 1
16 .
7 BWK WITH PREADJUSTED DISCRETIZATION
In this section, we develop a general technique for preadjusted discretization, and apply it to dynamic pricing with a single product and dynamic procurement with a single budget. For both
applications, our regret bounds significantly improve over prior work. While the dynamic pricing
application is fairly straightforward given the general result, the dynamic procurement application takes some work and uses a non-standard mesh of prices. We also obtain an initial result for
dynamic pricing with multiple products. The main technical challenge is to upper-bound the discretization error; we can accomplish this whenever the expected resource to expected consumption
ratio of each arm can be expressed in a particularly simple way.
7.1 Preadjusted Discretization as a General Technique
The high-level idea behind preadjusted discretization is to apply an existing BwK algorithm with
a restricted, finite action space S ⊂ X that is chosen in advance. Typically S is, in some sense,
“uniformly spaced” in X, and its “granularity” is tuned in advance to minimize regret.
Consider a problem instance with action space restricted to S. Let REW(S) be the algorithm’s
reward on this problem instance, and let OPTLP (S) be the corresponding value of OPTLP, as defined
in Section 3. OPT(X) and OPTLP (X) will refer to the corresponding quantities for the original action
space X. The key two quantities in our analysis of preadjusted discretization are
R(S) = OPTLP (S) − REW(S) (S-regret)
Err(S |X) = OPTLP (X) − OPTLP (S) (discretization error of S relative to X). (49)
Note that algorithm’s regret can be expressed as
OPT(X) − REW(S) ≤ OPTLP (X) − REW(S) = R(S) + Err(S |X).
Now, suppose S is parameterized by ϵ > 0, which controls its “granularity.” Adjusting the ϵ
involves balancing R(S) and Err(S |X): indeed, decreasing ϵ tends to increase R(S) but decrease
Err(S |X). We upper-bound the S-regret via our main algorithmic result14; the challenge is to upperbound Err(S |X).
14We need to use the regret bound in terms of the best known upper bound on OPT, rather than OPT itself, because the
latter is not known to the algorithm. For example, for dynamic pricing one can use OPT ≤ B.
Journal of the ACM, Vol. 65, No. 3, Article 13. Publication date: March 2018.
13:38 A. Badanidiyuru et al.
A typical scenario where one would want to apply preadjusted discretization is when an algorithm chooses among prices. More formally, each arm includes a real-valued vector of prices in
[0, 1] (and perhaps other things, such as the maximal number of items for sale). The restricted action set S consists of all arms such that all prices belong to a suitably chosen mesh M ⊂ [0, 1] with
granularity ϵ. There are several types of meshes one could consider, depending on the particular
BwK domain. The most natural ones are the ϵ-additive mesh, with prices that are integer multiples
of ϵ, and ϵ-multiplicative mesh mesh, with prices of the form (1 − ϵ )
,  ∈ N. Both have been used
in the prior work on MAB in metric spaces [36, 37, 41, 45]) and dynamic pricing (e.g., References
[12, 16, 18, 39]). Somewhat surprisingly, for dynamic procurement, we find it optimal to use a very
different mesh, called ϵ-hyperbolic mesh, in which the prices are of the form 1
1+ϵ  ,  ∈ N.
While in practice the action set X is usually finite (although possibly very large), it is mathematically more elegant to consider infinite X. For example, we prefer to allow arbitrary fractional
prices, even though in practice they may have to be rounded to whole cents. However, recall that
OPTLP in Section 3 is only defined for a finite action space X. To handle infinite X, we define
OPTLP (X) = sup
finite X	 ⊂ X
OPTLP (X	
). (50)
In line with Lemma 3.1, let us argue that OPTLP (X) ≥ OPT(X) even whenX is infinite. Specifically,
we prove this for all versions of dynamic pricing and dynamic procurement, and more generally
for any BwK domain such that for each arm there are only finitely many possible outcome vectors.
Lemma 7.1. Consider a BwK domain with infinite action space X, such that for each arm there are
only finitely many possible outcome vectors. Then OPTLP (X) ≥ OPT(X).
Proof. Fix a problem instance, and consider an optimal dynamic policy for this instance.
W.l.o.g. this policy is deterministic.15 For each round, this policy defines a deterministic mapping from histories to arms to be played in this round. Since there are only finitely many possible
histories, the policy can only use a finite subset of arms, call it X	 ⊂ X. By Lemma 3.1, we have
OPTLP (X) ≥ OPTLP (X	
) ≥ OPT(X	
) = OPT(X).
7.2 A General Bound on Discretization Error
We develop a general bound on discretization error Err(S |X), as defined in Equation (49). To this
end, we consider the expected reward to expected consumption ratios of arms (and the differences
between them), whereas in the work on MAB in metric spaces it suffices to consider the difference
in expected rewards.
To simplify notation, we suppress μ, the (actual) latent structure: e.g., we will write ci (D) =
ci (D, μ), r(D) = r(D, μ), and LP(D, μ) = LP(D) for distributions D and resources i.
Definition 7.2. We say that arm x ϵ-covers arm y if the following two properties are satisfied for
each resource i such that ci (x) + ci (y) > 0:
(i) r(x)/ci (x) ≥ r(y)/ci (y) − ϵ.
(ii) ci (x) ≥ ci (y).
A subset S ⊂ X of arms is called an ϵ-discretization of X if each arm in X is ϵ-covered by some
arm in S.
Theorem 7.3 (Preadjusted Discretization). Fix a BwK domain with action space X. Let S ⊂ X
be an ϵ-discretization of X, for some ϵ ≥ 0. Then the discretization error Err(S |X) is at most ϵdB.
Consequently, for any algorithm with S-regret R(S), we have OPTLP (X) − REW(S) = R(S) + ϵdB.
15A randomized policy can be seen as a distribution over deterministic policies, so one of these deterministic policies must
have same or better expected total reward.
Journal of the ACM, Vol. 65, No. 3, Article 13. Publication date: March 2018.     
Bandits with Knapsacks 13:39
Proof. We need to prove that Err(S |X) ≤ ϵdB. If X is infinite, then (by Equation (50)) it suffices
to prove Err(S |X	
) ≤ ϵdB for any finite subset of X	 ⊂ X. Let D be the distribution over arms in
X	
, which maximizes LP(D, μ). We use D to construct a distribution DS over S, which is nearly
as good.
We define DS as follows. Since S is an ϵ-discretization of X, there exists a family of subsets
(cov(x) ⊂ X : x ∈ S) so that each arm x ∈ S ϵ-covers all arms in cov(x), the subsets are disjoint,
and their union is X. Fix one such family of subsets, and define
DS (x) =

y ∈cov(x )
D(y) min
i: ci (x )>0
ci (y)
ci (x)
, x ∈ S.
Note that 
x ∈S DS (S) ≤ 1 by Definition 7.2(ii). With the remaining probability, the null arm is
chosen (i.e., the algorithm skips a given round).
To argue that LP(DS , μ) is large, we upper-bound the resource consumption ci (DS ), for each
resource i, and lower-bound the reward r(DS ):
ci (DS ) =

x ∈S
ci (x) DS (x)
≤

x ∈S
ci (x)

y ∈cov(x ): ci (x )>0
D(y)
ci (y)
ci (x)
=

x ∈S

y ∈cov(x ): ci (x )>0
D(y)ci (y)
=

y ∈X
D(y)ci (y)
= ci (D). (51)
(Note that the above argument did not use the property (i) in Definition 7.2.)
In what follows, for each arm x define Ix = {i : ci (x) > 0}:
r(DS ) =

x ∈S
r(x) DS (x)
=

x ∈S
r(x)

y ∈cov(x )
D(y) min
i ∈Ix
ci (y)
ci (x)
=

x ∈S

y ∈cov(x )
D(y) min
i ∈Ix
ci (y)r(x)
ci (x)
≥

x ∈S

y ∈cov(x )
D(y) min
i ∈Ix
r(y) − ϵci (y) (by Definition 7.2(i))
=

y ∈X
D(y) min
i r(y) − ϵci (y)
≥

y ∈X
D(y) 


r(y) − ϵ

i
ci (y)

= r(D) − ϵ

i
ci (D). (52)
Journal of the ACM, Vol. 65, No. 3, Article 13. Publication date: March 2018.
13:40 A. Badanidiyuru et al.
Let τ (D) = mini
B
ci (D) be the stopping time in the linear relaxation, so that LP(D) = τ (D)r(D).
By Equation (51), we have τ (DS ) ≥ τ (D). We are ready for the final computation:
LP(DS ) = τ (DS ) r(DS )
≥ τ (D) r(DS )
≥ τ (D) 


r(D) − ϵ

i
ci (D)

(by Equation (52))
≥ r(D) τ (D) − ϵ τ (D)

i
ci (D)
≥ LP(D) − ϵdB.
7.3 Preadjusted Discretization for Dynamic Pricing
We apply the machinery developed above to handle the basic version of dynamic pricing, as defined
in Section 1.1. In fact, our technique easily generalizes to multiple products, in a particular scenario
that we call dynamic bundle-pricing. We present the more general result directly.
The dynamic bundle-pricing problem is defined as follows. There are d products, with limited
supply of each, andT rounds. In each round, a new buyer arrives, an algorithm chooses a bundle of
products and a price, and offers this bundle for this price. The offer is either accepted or rejected.
The bundle is a vector (b1,...,bd ), so that bi ∈ N units of each product i are offered. We assume
that the bundle must belong to a fixed collection F of allowed bundles. Buyers’ valuations over
bundles can be arbitrary (in particular, not necessarily additive); they are drawn independently
from a fixed distribution over valuations. For normalization, we assume that each buyer’s valuation
for any bundle of  units lies in the interval [0, ]; accordingly, the offered price for such bundle
can w.l.o.g. be restricted to the same interval.
Theorem 7.4. Consider the dynamic bundle-pricing problem such that there are d products,
each with supply B. Assume each allowed bundle consists of at most  items, and prices are in
[0, ]. Algorithm PrimalDualBwK with an ϵ-additive mesh, for some ϵ = ϵ (B, |F |, ), has regret
O(d B2/3 (|F |)
1/3).
The basic version from Section 1.1 is a special case with a single product and a single allowed
bundle that consists of one unit of this product. Taking d =  = |F | = 1 in Theorem 7.4, we obtain
regret O(B2/3). This regret bound is optimal for any pair (B,T ), as proved in Babaioff et al. [12].
Corollary 7.5. Consider the dynamic pricing problem, as defined Section 1.1. Algorithm
PrimalDualBwK with an ϵ-additive mesh, for a suitably chosen ϵ = ϵ (B), has regret O(B2/3).
Proof of Theorem 7.4. First, let us cast this problem as a BwK domain. To ensure that per-round
rewards and per-round resource consumptions lie in [0, 1], we scale them down by the factor of .
Accordingly, the rescaled supply constraint is B	 = B/. In what follows, consider the scaled-down
problem instance.
An arm is a pair x = (b,p), where b ∈ F is a bundle and p ∈ [0, 1] is the offered price. Let F (x) be
the probability of a sale for this arm, divided by ; this probability is non-increasing in p for a fixed
bundle b. Then expected per-round reward isr(x) = p F (x), and expected per-round consumption
of product i is ci (x) = bi F (x). Therefore,
r(x)
ci (x) = p
bi
, for each arm x = (b,p) and product i. (53)
Journal of the ACM, Vol. 65, No. 3, Article 13. Publication date: March 2018.              
Bandits with Knapsacks 13:41
This is a crucial domain-specific property that enables preadjusted discretization. It follows that
for any arm x = (b,p), this arm ϵ-covers any arm x 	 = (b,p	
) such that p	 − ϵ ≤ p ≤ p	
. Therefore,
an ϵ-additive mesh S is an ϵ-discretization, for any ϵ > 0.
Consider algorithm PrimalDualBwK with action space S. Using Theorem 5.3 and observing that
OPTLP ≤ dB	
, we obtain S-regret
R(S) ≤ O(d

B	 |S |) = O(d

B	 |F |/ϵ ).
By Theorem 7.3 discretization error is Err(S |X) ≤ ϵdB	
. So, regret relative to OPTLP is
R(S) + Err(S |X) ≤ O(d

B	 |F |/ϵ + ϵdB	
) ≤ O(d) (B/)
2/3 |F |1/3
for a suitably chosen ϵ = (B/)
−1/3 |F |1/3. Recall that this is regret for the rescaled problem instance. For the original problem instance, rewards are scaled up by the factor of , so regret is
scaled up by , too.
One can easily extend Theorem 7.4 to a setting where in each round an algorithm offers several
copies of the same bundle for the same per-bundle price, and an agent can choose how many
copies to buy (if any). More precisely, in each round an algorithm chooses two things: a bundle
from F and the number of copies of this bundle. The latter is restricted to be at most Λ, where Λ is
a known parameter. We call this setting dynamic bundle-pricing with multiplicity Λ. The algorithm
and analysis is essentially the same.
Theorem 7.6. Consider dynamic bundle-pricing with multiplicity Λ. Assume that each product
has supply B, and each allowed bundle consists of at most  items. Algorithm PrimalDualBwK with
an ϵ-additive mesh, for a suitably chosen ϵ = ϵ (B, |F |, , Λ), has regret O(d (BΛ)2/3 (|F |)
1/3).
7.4 Preadjusted Discretization for Dynamic Procurement
Application to dynamic procurement takes a little more work and results in a weaker regret bound,
compared to the application to dynamic pricing. The main reason is that the natural mesh for
dynamic procurement is ϵ-hyperbolic (rather than ϵ-additive). One needs to bound this mesh from
below to make it finite, which increases the mesh size and the discretization error.
While our main goal here is to handle the basic version of dynamic procurement, as defined
in Section 1.1, the same technique easily extends to a generalization where the algorithm can
buy multiple items in each round. The generalization is defined as follows. In each round t, the
algorithm offers to buy up to Λ units at price pt per unit, where pt ∈ [0, 1] is chosen by the algorithm. The outcome is summarized by the number kt of items bought, where kt is an independent
sample from some fixed (but unknown) distribution parameterized by pt and Λ. The algorithm is
constrained by the time horizon T , budget B, and per-round supply constraint Λ. We prove the
following:
Theorem 7.7. Consider dynamic procurement with up to Λ items bought per round. Algorithm
PrimalDualBwK with a suitably chosen action space S yields regret O˜ (Λ5/4
T/B1/4). Specifically, S =
[p0, 1] ∩ M, where M is the ϵ-hyperbolic mesh, for some parameters ϵ,p0 ∈ (0, 1) that depend only on
B, T , and Λ.
Let us model this problem as a BwK domain. The action space is X = [0, 1]: the arms correspond
to all possible prices. (The zero price corresponds to the “null arm”.) To ensure that rewards and
consumptions lie in [0, 1], we scale them down by a factor of Λ, as in Section 7.3, so that reward
in any round t is kt /Λ, and budget consumption is pt kt /Λ. Accordingly, budget is rescaled to
B	 = B/Λ. Henceforth, consider the scaled-down problem instance, unless specified otherwise.
Journal of the ACM, Vol. 65, No. 3, Article 13. Publication date: March 2018.                
13:42 A. Badanidiyuru et al.
Let F (p) be the expected per-round number of items sold for a given price p, divided by Λ; note
that it is non-decreasing in p. Then expected budget consumption is c(p) = p F (p), and expected
reward is simply r(p) = F (p). It follows that
r(p)
c(p) = 1
p
, for each price p.
Like Equation (53), this is a crucial domain-specific property that enables preadjusted
discretization.
By Definition 7.2 price p ϵ-covers price arm q if and only if q < p and 1
p ≥ 1
q − ϵ. This makes
the hyperbolic mesh a natural mesh for this problem, rather than additive or multiplicative ones.
It is easy to see that the ϵ-hyperbolic mesh S on X is an ϵ-discretization of X: namely, each price
q is ϵ-covered by the smallest price p ≥ q that lies in S.
Unfortunately, this mesh has infinitely many points. In fact, it is easy to see that any ϵdiscretization on X must be infinite, even for Λ = 1. To obtain a finite ϵ-discretization, we only
consider prices p ≥ p0, for some parameter p0 to be tuned later. Below we argue that this restriction is not too damaging:
Claim 7.8. Consider dynamic procurement with non-unit supply. Then, for any p0 ∈ (0, 1) it holds
that
OPTLP ([p0, 1]) ≥ OPTLP ([0, 1]) − p0T 2
/B	
.
Proof. When p0 > B	
/T , the bound is trivial, and for the rest of the proof we assume that p0 ≤
B	
/T .
By Equation (50) it suffices to replace OPTLP ([0, 1]) in the claim with OPTLP (X0), for any given
finite subset X0 ⊂ [0, 1]. Let D be an LP-perfect distribution for the problem instance restricted
to X0; such D exists by Claim 3.4. Thus, LP(D) = OPTLP (X) and c(D) ≤ B	
T . Furthermore, D has
a support of size at most 2; denote it as arms p1,p2 ∈ [0, 1], p1 ≤ p2, where the null arm would
correspond to p1 = 0. If p1 ≥ p0, then D has support in the interval [p0, 1], and we are done; so
from here on, we assume p1 < p0. Note that
LP(D) = r(D) min  B	
c(D)
,T

= T r(D).
To prove the desired lower bound on OPTLP ([p0, 1]), we construct a distribution D	 with support
in {0} ∪ [p0, 1] and a sufficiently large LP-value. (Here the zero price corresponds to the null arm.)
Suppose p2 ≤ B	
T . Define D	 by putting probability mass on price B	
T . Since c( B	
T ) ≤ B	
T , we have
LP(D	
) = T r(D	
) = T F 
B	
T

≥ T F (p2) ≥ T r(D) = LP(D),
and we are done. From here on, assume p2 > B	
T .
Now consider the main case: p1 ≤ p0 ≤ B	
T < p2. Define distribution D	 as follows:
D	
(p0) = D(p1),
D	
(p2) = max (0, D(p2) − p0/p2) ,
D	
(0) = 1 − D	
(p0) − D	
(p2).
Journal of the ACM, Vol. 65, No. 3, Article 13. Publication date: March 2018.
Bandits with Knapsacks 13:43
We claim that c(D	
) ≤ B	
T . If D	
(p2) = 0, then c(D	
) = c(p0) ≤ p0 ≤ B	
T . If D	
(p2) > 0, then
D	
(p2) = D(p2) − p0/p2, and therefore,
c(D	
) − c(D) = D(p1) (p0F (p0) − p1F (p1)) − p2F (p2)
p0
p2
≤ p0F (p0) − p0F (p2) ≤ 0.
Then, c(D	
) ≤ c(D) ≤ B	
T . Claim proved.
Therefore, LP(D	
) = T r(D	
). To complete the proof:
r(D	
) − r(D) ≥ D(p1) (F (p0) − F (p1)) − F (p2) p0/p2
≥ −p0/p2 ≥ −p0T/B	
.
OPTLP ([p0, 1]) − OPTLP (X0) = LP(D	
) − LP(D)
= T (r(D	
) − r(D)) ≤ −p0T 2
/B	
.
Suppose algorithm PrimalDualBwK is applied to a problem instance with a finite action space
S. Then by Theorem 5.3 the S-regret is
R(S) = O(
√
mT +T

m/B	), m = |S |.
Let S = [p0, 1] ∩ M, where M is the ϵ-hyperbolic mesh, for some ϵ,p0 ∈ (0, 1). Then, m = |S | ≤ 1
ϵp0
. Moreover, S is an ϵ-discretization for action space X	 = [p0, 1], for the same reason that M is
an ϵ-discretization for the original action space X = [0, 1]. Therefore:
Err(S |X	
) ≤ ϵB	 (by Theorem 7.3)
Err(X	
|X) ≤ p0T 2
/B	 (by Claim 7.8)
OPTLP (X) − REW(S) = R(S) + Err(S |X	
) + Err(X	
|X)
≤ R(S) + ϵB	 + p0T 2
/B	
.
Optimizing the choice of ϵ and p0, we obtain the final regret bound ofO˜ (T (B	
)
−1/4). Recall that this
is the regret bound for the rescaled problem instance. Going back to the original problem instance,
regret is multiplied by a factor of Λ. This completes the proof of Theorem 7.7.
8 APPLICATIONS AND COROLLARIES
We systematically overview various applications of BwK and corresponding corollaries. This section can be read independently of the technical material in the rest of the article.
Some technicalities. In applications with very large or infinite action space X, we apply a BwK
algorithm with a restricted, finite action space S ⊂ X, where S is chosen in advance. Immediately,
we obtain a bound on the S-regret: regret with respect to the value of OPTLP on the restricted action
space (such bound depends on |S |). Instantiating such regret bounds is typically straightforward
once one precisely defines the setting. In some applications, we can choose S using preadjusted
discretization, as discussed in Section 7.
In some of the applications, per-round reward and resource consumption may be larger than 1.
Then one needs to scale them down to fit the definition of BwK and apply our regret bounds, and
scale them back up to obtain regret for the original (non-rescaled) version. We encapsulate this
argument as follows:
Lemma 8.1. Consider a version of BwK with finite action set S, in which per-round rewards are
upper-bounded by r0, and per-round consumption of each resource is at mostc0. Then one can achieve
Journal of the ACM, Vol. 65, No. 3, Article 13. Publication date: March 2018.   
13:44 A. Badanidiyuru et al.
regret
O
r0 |S | OPT + OPT
c0 |S |/B  (54)
by applying algorithm PrimalDualBwK with suitably rescaled rewards, resource consumption, and
budgets.
Proof. Denote R(OPT, B) = √
|S | OPT + OPT√
|S |/B, as in the main regret bound.
To cast this problem as an instance of BwK, consider a rescaled problem instance in which
all rewards are divided by r0, and all consumptions and budgets are divided by c0. Now, we
can apply regret bound Equation (1) for the scaled-down problem instance; we obtain regret
O(R(OPT/r0, B/c0)). Multiply this regret bound by r0 to obtain a regret bound for the original problem instance.
8.1 Dynamic Pricing with Limited Supply
In dynamic pricing, the algorithm is a monopolist seller that interacts with T agents (potential
buyers) arriving one by one. In each round, a new agent arrives, the algorithm makes an offer,
the agent chooses among the offered alternatives, and leaves. The offer specifies which goods are
offered for sale at which prices. The agent has valuations over the offered bundles of goods, and
chooses an alternative that maximizes her utility: value of the bundle minus the price. An agent
is characterized by her valuation function: function from all possible bundles of goods that can be
offered to their respective valuations. For each arriving agent, the valuation function is private:
not known to the algorithm. It is assumed to be drawn from a fixed (but unknown) distribution
over the possible valuation functions, called the demand distribution. Algorithm’s objective is to
maximize the total revenue; there is no bonus for left-over inventory.
Basic version. In the basic version from Section 1.1, the algorithm has B identical items for sale.
In each round, the algorithm chooses a price pt and offers one item for sale at this price, and an
agent either buys or leaves. The agent has a fixed private value vt ∈ [0, 1] for an item, and buys
if and only if pt ≥ vt . Recall from Corollary 7.5 that we obtain regret O(B2/3), which is optimal
according to Reference [12].
Extension: non-unit demands. Agents may be interested in buying more than one unit of the
product, and may have valuations that are non-linear in the number of products bought. Accordingly, let us consider an extension where an algorithm can offer each agent multiple units. More
specifically: in each round t, the algorithm offers up to λt units at a fixed price pt per unit, where
the pair (pt, λt ) is chosen by the algorithm, and the agent then chooses how many units to buy, if
any. We restrict λt ≤ Λ, where Λ is a fixed parameter. We obtain regret O(B Λ)2/3 by Theorem 7.6
(considering a special case when there is a single product and a single allowed bundle with one unit
of this product). One can also consider a version with λt = Λ, so that the algorithm only chooses
prices; then a very similar argument gives regret O(B2/3Λ1/3).
Extension: multiple products. When multiple products are offered for sale, it often makes
sense to price them jointly. Formally, the algorithm has d products for sale, with Bi units of
each product i. (To simplify regret bounds, let us assume Bi = B.) In each round t, the algorithm
chooses a vector of prices (pt,1,...,pt,d ) ∈ [0, 1]d and offers at most one unit of each product i at
price pt,i . The agent then chooses the subset of products to buy. We allow arbitrary demand distributions; we do not restrict correlations between valuations of different products and/or subsets of
products.
Given a finite set S of allowed price vectors, such as an ϵ-additive mesh for some specific ϵ > 0,
we obtain S-regret O(d
√
B |S |). This follows from Lemma 8.1, observing that per-round rewards
Journal of the ACM, Vol. 65, No. 3, Article 13. Publication date: March 2018.          
Bandits with Knapsacks 13:45
are at mostr0 = d, per-round consumption of each resource is at mostc0 = 1, and the optimal value
is OPT ≤ dB.
There may also be a fixed collection of subsets that agents are allowed to buy, e.g., agents may
be restricted to buying at most three items in total. This does not affect our analysis and the regret
bound.
Joint pricing is not needed in the special case when each agent can buy an arbitrary subset
I of products, and her valuations are additive: v(I) = 
i ∈I v(i). Then she buys each product i if and only if the offered price for this product exceeds v(i). Therefore, the problem is
equivalent to a collection of d separate per-product problems, and one can run a separate BwK
algorithm for each product. Using Corollary 7.5 separately for each product, one obtains regret
O(d B2/3).
Extension: network revenue management. More generally, an algorithm may have d products
for sale that may be produced on demand from limited primitive resources, so that each unit of
each product i consumes a fixed and known amount cij ∈ [0, 1] of each primitive resource j. This
generalization is known as network revenue management problem (see Besbes and Zeevi [17] and
references therein). All other details are the same as above; for simplicity, let us focus on a version
in which each agent buys at most one item. Given a finite set S of allowed price vectors, we obtain
S-regret given by Equation (54) with r0 = c0 = d.
In particular, if all resource constraints (including the time horizon) are scaled up by factor γ ,
regret scales as √γ . This improves over the main result in Besbes and Zeevi [17], where (essentially)
regret is stated in terms of γ and scales as γ 2/3.
Extension: bundling and volume pricing. When selling to agents with non-unit demands, an
algorithm may use discounts and/or surcharges for buying multiple units of a product (the latter
may make sense for high-valued products such as tickets to events at the Olympics). More generally, an algorithm can may use discounts and/or surcharges for some bundles of products, where
each bundle can include multiple units of multiple products, e.g., two beers and one snack. In full
generality, there is a collection F of allowed bundles. In each round an algorithm offers a menu
of options that consist of a price for every allowed bundle in F (and the “none” option), and the
agent chooses one option from this menu. Thus, in each round the algorithm needs to choose a
price vector over the allowed bundles.
For a formal result, assume there is a finite set S of allowed price vectors, each bundle in F can
contain at most  units total, and the per-bundle prices are restricted to lie in the range [0, ]. Then,
we obtain S-regret O(d
√
 B |S |). This follows from Lemma 8.1, observing that per-round rewards
are at mostr0 = , per-round consumption of each resource is at mostc0 = , and the optimal value
is OPT ≤ dB.
The action space here is |F |-dimensional, which may result in a prohibitively large number
of allowed price vectors. One can reduce the “dimensionality” of the action space by restricting
how the bundles may be priced. For example, each bundle may be priced at a volume discount x%
compared to buying each unit separately, where x depends only on the number of items in the
bundle.
Moreover, we can analyze preadjusted discretization for a version where in each round the
algorithm chooses only one bundle to offer. By Theorem 7.4, we obtain regret O(B2/3 (|F |)
1/3).
Extension: buyer targeting. Suppose there are  different types of buyers (say men and women),
and the demand distribution of a buyer depends on her type. The buyer type is modeled as a sample
from a fixed but unknown distribution. In each round the seller observes the type of the current
buyer (e.g., using a cookie or a user profile), and can choose the price depending on this type.
Journal of the ACM, Vol. 65, No. 3, Article 13. Publication date: March 2018.            
13:46 A. Badanidiyuru et al.
This can be modeled as a BwK domain where arms correspond to functions from buyer types to
prices. For example, with  buyer types and a single product, the (full) action space is X = [0, 1].
Assuming we are given a restricted action space S ⊂ X, we obtain S-regret O(
√
B |S |).
8.2 Dynamic Procurement and Crowdsourcing Markets
A “dual” problem to dynamic pricing is dynamic procurement, where the algorithm is buying rather
than selling. In the basic version, the algorithm has a budget B to spend, and is facing T agents
(potential sellers) that are arriving sequentially. In each round t, a new agent arrives, the algorithm
chooses a price pt ∈ [1] and offers to buy one item at this price. The agent has private value vt ∈
[0, 1] for an item (unknown to the algorithm), and sells if and only if pt ≥ vt . The value is an
independent sample from some fixed (but unknown) distribution. Algorithm’s goal is to maximize
the number of items bought. Recall from Theorem 7.7 that we obtain regret O(T/B1/4) for this
version.
Application to crowdsourcing markets. The problem is particularly relevant to the emerging
domain of crowdsourcing, where agents correspond to the (relatively inexpensive) workers on a
crowdsourcing platform such as Amazon Mechanical Turk, and “items” bought/sold correspond to
simple jobs (“microtasks”) that can be performed by these workers. The algorithm corresponds to
the “requester”: an entity that submits jobs and benefits from them being completed. The (basic)
dynamic procurement model captures an important issue in crowdsourcing that a requester
interacts with multiple users with unknown values-per-item, and can adjust its behavior (such
as the posted price) over time as it learns the distribution of users. While this basic model ignores
some realistic features of crowdsourcing environments (see a survey Slivkins and Vaughan [52]
for background and discussion), some of these limitations are addressed by the generalizations
that we present below.
Extension: non-unit supply. We consider an extension where agents may be interested in more
than one item, and their valuations may be non-linear. For example, a worker may be interested in
performing several jobs. In each round t, the algorithm offers to buy up to Λ units at a fixed price
pt per unit, where the price pt is chosen by the algorithm and Λ is a fixed parameter. The tth agent
then chooses how many units to sell. Recall from Theorem 7.7 that we obtain regretO˜ (Λ5/4
T/B1/4)
for this extension.
Extension: multiple types of jobs. We can handle an extension in which there are d types of
jobs requested on the crowdsourcing platform, with a separate budget Bi for each type. Each agent
t has a private cost vt,i ∈ [0, 1] for each type i; the vector of private costs comes from a fixed but
unknown distribution over d-dimensional vectors (note that arbitrary correlations are allowed).
The algorithm derives reward ui ∈ [0, 1] from each job of type i. In each round t, the algorithm
offers a vector of prices (pt,1,...,pt,d ), where pt,i is the price for one job of type i. For each type
i, the agent performs one job of this type if and only if pt,i ≥ vt,i , and receives payment pt,i from
the algorithm.
Here arms correspond to the d-dimensional vectors of prices, so that the action space is
X = [0, 1]d . Given the restricted action space S ⊂ X, we obtain S-regret O(d)(√
T |S | +T
√
d |S |/B),
where B is the smallest budget. This follows from Lemma 8.1, observing that per-round rewards
are at most r0 = d, per-round consumption of each budget is at mostc0 = 1, and the optimal value
is OPT ≤ dT .
Extension: additional features. We can also model more complicated “menus” so that each
agent can perform several jobs of the same type. Then in each round, for each type i, the algorithm
specifies the maximal offered number of jobs of this type and the price per one such job. We can
Journal of the ACM, Vol. 65, No. 3, Article 13. Publication date: March 2018.     
Bandits with Knapsacks 13:47
also incorporate constraints on the maximal number of jobs of each type that is needed by the
requester, and/or the maximal amount of money spend on each type.
Extension: competitive environment. There may be other requesters in the system, each offering its own vector of prices in each round. (This is a realistic scenario in crowdsourcing, for
example.) Each seller/worker chooses the requester and the price that maximize her utility. One
standard way to model such a competitive environment is to assume that the “best offer” from the
competitors is a vector of prices that comes from a fixed but unknown distribution. This can be
modeled as a BwK instance with a different distribution over outcomes that reflect the combined
effects of the demand distribution of agents and the “best offer” distribution of the environment.
8.3 Other Applications to Electronic Markets
Ad allocation with unknown click probabilities. Consider pay-per-click (PPC) advertising on
the web (in particular, this is a prevalent model in sponsored search auctions). The central premise
in PPC advertising is that an advertiser derives value from her ad only when the user clicks on
this ad. The ad platform allocates ads to users that arrive over time.
Consider the following simple (albeit highly idealized) model for PPC ad allocation. Users arrive
over time, and the ad platform needs to allocate an ad to each arriving user. There is a set X of
available ads. Each ad x is characterized by the payment-per-click πx and click probability μx ; the
former quantity is known to the algorithm, whereas the latter is not. If an ad x is chosen, then it is
clicked on with probability μx , in which case payment πx is received. The goal is to maximize the
total payment. This setting and various extensions thereof that incorporate user/webpage context
have received a considerable attention in the past several years (starting with References [43, 47,
48]). In fact, the connection to PPC advertising has been one of the main motivations for the recent
surge of interest in MAB.
We enrich the above setting by incorporating advertisers’ budgets. In the most basic version, for
each ad x there is a budget Bx—the maximal amount of money that can be spent on this ad. More
generally, an advertiser can have an ad campaign that consists of a subset S of ads, so that there is
a per-campaign budget S. Even more generally, an advertiser can have a more complicated budget
structure: a family of overlapping subsets S ⊂ X and a separate budget BS for each S. For example,
BestBuy can have a total budget for the ad campaign, and also separate budgets for ads about TVs
and ads about computers. Finally, in addition to budgets (i.e., constraints on the number of times
ads are clicked), an advertiser may wish to have similar constraints on the number of times ads
are shown. BwK allows us to express all these constraints.
Adjusting a repeated auction. An auction is held in every round, with a fresh set of participants.
The number of participants and a vector of their types come from a fixed but unknown distribution.
The auction is adjustable: it has some parameter that the auctioneer adjust over time to optimize
revenue. For example, Cesa-Bianchi et al. [21] studies a repeated second price auction with an
adjustable reserve price, with unlimited inventory of a single product. BwK framework allows to
incorporate limited inventory of items to be sold at the auction, possibly with multiple products.
Repeated bidding.A bidder participates in a repeated auction, such as a sponsored search auction.
In each round t, the bidder can adjust her bid bt based on the past performance. The outcome for
this bidder is a vector (pt,ut ), where pt is the payment andut is the utility received. We assume that
this vector comes from a fixed but unknown distribution. The bidder has a fixed budget. Similar
settings have been studied in References [7, 55], for example.
We model this as a BwK problem where arms correspond to the possible bids, and the single resource is money. Note that (the basic version of) dynamic procurement corresponds to this setting
with two possible outcome vectors (pt,ut ): (0, 0) and (bt, 1).
Journal of the ACM, Vol. 65, No. 3, Article 13. Publication date: March 2018.
13:48 A. Badanidiyuru et al.
The BwK setting also allows to incorporate more complicated constraints. For example, an action
can result in several different types of outcomes that are useful for the bidder (e.g., an ad shown to
a male or an ad shown to a female), but the bidder is only interested in a limited quantity of each
outcome.
8.4 Application to Network Routing and Scheduling
In addition to applications to Electronic Markets, we describe two applications to network routing
and scheduling. In both applications an algorithm chooses between different feasible policies to
handle arriving “service requests,” such as connection requests in network routing and jobs in
scheduling.
Adjusting a routing protocol. Consider the following stylized application to routing in a communication network. Connection requests arrive one by one. A connection request consists of a
pair of terminals; assume the pair comes from a fixed but unknown distribution. The system needs
to choose a routing protocol for each connection, out of several possible routing protocols. The
routing protocol defines a path that connects the terminals; abstractly, each protocol is simply a
mapping from terminal pairs to paths. Once the path is chosen, a connection between the terminals
is established. Connections persist for a significant amount of time. Each connection uses some
amount of bandwidth. For simplicity, we can assume that this amount is fixed over time for every
connection, and comes from a fixed but unknown distribution (although even a deterministic version is interesting). Each edge in the network (or perhaps each node) has a limited capacity: the
total bandwidth of all connections that pass though this edge or node cannot exceed some value.
A connection that violates any capacity constraint is terminated. The goal is to satisfy a maximal
number of connections.
We model this problem as BwK as follows: arms correspond to the feasible routing protocols,
each edge/node is a limited resource, each satisfied connection is a unit reward.
Further, if the time horizon is partitioned in epochs, we can model different bandwidth utilization in each phase; then a resource in BwK is a pair (edge,epoch).
Adjusting a scheduling policy. An application with a similar flavor arises in the domain of
scheduling long-running jobs to machines. Suppose jobs arrive over time. Each job must be assigned to one of the machines (or dropped); once assigned, a job stays in the system forever (or
for some number of “epochs”), and consumes some resources. Jobs have multiple “types” that can
be observed by the scheduler. For each type, the resource utilization comes from a fixed but unknown distribution. Note that there may be multiple resources being consumed on each machine:
for example, jobs in a datacenter can consume CPU, RAM, disk space, and network bandwidth.
Each satisfied job of type i brings utility ui . The goal of the scheduler is to maximize utility given
the constrained resources.
The mapping of this setting to BwK is straightforward. The only slightly subtle point is how to
define the arms: in BwK terms, arms correspond to all possible mappings from job types to machines.
One can also consider an alternative formulation where there are several allowed scheduling
policies (mappings from types and current resource utilizations to machines), and in every round
the scheduler can choose to use one of these policies. Then the arms in BwK correspond to the
allowed policies.
APPENDIXES
A THE OPTIMAL DYNAMIC POLICY BEATS THE BEST FIXED ARM
Let us provide additional examples of BwK problem instances in which the optimal dynamic policy
(in fact, the best fixed distribution over arms) beats the best fixed arm.
Journal of the ACM, Vol. 65, No. 3, Article 13. Publication date: March 2018.
Bandits with Knapsacks 13:49
Dynamic pricing. Consider the basic setting of “dynamic pricing with limited supply”: in each
round a potential buyer arrives, and the seller offers him one item at a price; there are k items
and n > k potential buyers. One can easily construct distributions for which offering a mixture of
two prices is strictly superior to offering any fixed price. In fact, this situation arises whenever the
“revenue curve” (the mapping from prices to expected revenue) is non-concave and its value at
the quantile k/n lies below its concave hull.
Consider a simple example: fix ϵ = kδ−1/2 with δ ∈ (0, 1
2 ), and assume that the buyer’s value
for an item is v = 1 with probability ϵ k
n and v = ϵ with the remaining probability, for some fixed
ϵ ∈ (0, 1).
To analyze this example, let REW(D) be the expected total reward (i.e., the expected total revenue)
from using a fixed distribution D over prices in each round; let REW(p) be the same quantity when
D deterministically picks a given price p.
• Clearly, if one offers a fixed price in all rounds, it only makes sense to offer prices p = ϵ and
p = 1. It is easy to see that REW(ϵ ) = ϵk and REW(1) ≤ n · Pr[sale at price 1 ] = ϵk.
• Now consider a distribution D that picks price ϵ with probability (1 − ϵ ) k
n , and picks price
1 with the remaining probability. It is easy to show that REW(D) ≥ ϵk(2 − o(1)).
So, we see that REW(D) is essentially twice as large compared to the total expected revenue of
the best fixed arm.
Dynamic procurement. A similar example can be constructed in the domain of dynamic procurement. Consider the basic setting thereof: in each round a potential seller arrives, and the buyer
offers to buy one item at a price; there are T sellers and the buyer is constrained to spend at most
budget B. The buyer has no value for left-over budget and each sellers value for the item is drawn
i.i.d. from an unknown distribution. Then a mixture of two prices is strictly superior to offering
any fixed price whenever the “sales curve” (the mapping from prices to probability of selling) is
non-concave and its value at the quantile B/T lies below its concave hull.
Let us provide a specific example. Fix any constant δ > 0, and let ϵ = B1/2+δ . Each seller has
the following two-point demand distribution: the seller’s value for item is v = 0 with probability
B
T , and v = 1 with the remaining probability. We use the notation REW(D) and REW(p) as defined
above.
• Clearly, if one offers a fixed price in all rounds, it only makes sense to offer prices p = 0 and
p = 1. It is easy to see that REW(0) ≤ T · Pr[selling at price 0] = B and REW(1) = B.
• Now consider a distribution D that picks price 0 with probability 1 − B−ϵ
T , and picks price
1 with the remaining probability. It is easy to show that REW(D) ≥ (2 − o(1)) B.
Again, we see that REW(D) is essentially twice as large compared to the total expected sales of
the best fixed arm.
B BALANCEDEXPLORATION BEATS PRIMALDUALBWK SOMETIMES
We provide a simple example in which BalancedExploration achieves much better regret than
(what we can prove for) PrimalDualBwK. The reason is that BalancedExploration is aware of the
BwK domain, whereas PrimalDualBwK is not. More precisely, BalancedExploration is parameterized by Mfeas, the set of all latent structures that are feasible for the BwK domain.
The example is a version of the deterministic example from Section 1.1. There is a time horizonT
and two other resources, both with budget B < T/2. There are m arms, partitioned into two samesize subsets,X1 andX2. Per-round rewards and per-round resource consumptions are deterministic
Journal of the ACM, Vol. 65, No. 3, Article 13. Publication date: March 2018.
13:50 A. Badanidiyuru et al.
for all arms. All arms get per-round reward 1. For each resource i, each arm in Xi only consumes
this resource. Letting ci (x) = ci (x, μ) denote the (expected) per-round consumption of resource i
by arm x, one of the following holds:
(i) c1 (x1) = 1 and c2 (x2) = 1
2 for all arms x1 ∈ X1, x2 ∈ X2, or
(ii) c1 (x1) = 1
2 and c2 (x2) = 1 for all arms x1 ∈ X1, x2 ∈ X2.
Analysis. Note that an optimal dynamic policy alternates the two arms in proportion, 1 : 2 or
2 : 1, depending on the case, and an LP-optimal distribution over arms samples them in the same
proportion.
The key argument is that, informally, BalancedExploration can tell (i) from (ii) after the initial
O(m logT ) rounds. In the specification of BalancedExploration, consider the confidence radius
for the per-round consumption of resource i, as defined in Equation (11). After any one arm x
is played at least C logT rounds, for a sufficiently large absolute constant C, this confidence radius goes below 1/4. Therefore, any two latent structures μ, μ	 in the confidence interval satisfy
|ci (x, μ) − ci (x, μ	
)| < 1
2 . It follows that the confidence interval consists of a single latent structure,
either the one corresponding to (i) or the one corresponding to (ii), which is the correct latent structure for this problem instance. Accordingly, the chosen distribution over arms, being “potentially
perfect” by design, is LP-optimal. Thus, BalancedExploration uses the LP-optimal distribution
over arms after the initial O(m logT ) rounds.
The resulting regret is O˜ (m + √
B), where the √
B term arises, because the empirical frequencies of the two arms can deviate by O(
√
B) from the optimal values. Whereas with algorithm
PrimalDualBwK, we can only guarantee regret O˜ (
√
mB).
C ANALYSIS OF THE HEDGE ALGORITHM
We provide a self-contained proof of Proposition 5.4, the performance guarantee for the Hedge
algorithm from Freund and Schapire [31]. The presentation is adapted from Kleinberg [38].
For the sake of convenience, we restate the algorithm and the proposition. It is an online algorithm for maintaining a d-dimensional probability vector y while observing a sequence of ddimensional payoff vectors π1,..., πτ . The algorithm is initialized with a parameter ϵ ∈ (0, 1).
ALGORITHM: Hedge(ϵ )
1: v1 = 1
2: for t = 1, 2,..., τ do
3: yt = vt /(1vt ).
4: vt+1 = Diag{(1 + ϵ )
πt i }vt .
The performance guarantee of the algorithm is expressed by the following proposition.
Proposition (Proposition 5.4, Restated). For any 0 < ϵ < 1 and any sequence of payoff vectors
π1,..., πτ ∈ [0, 1]d , we have
∀y ∈ Δ[d]
τ
t=1
y
t πt ≥ (1 − ϵ )
τ
t=1
yπt − lnd
ϵ .
Journal of the ACM, Vol. 65, No. 3, Article 13. Publication date: March 2018.   
Bandits with Knapsacks 13:51
Proof. The analysis uses the potential function Φt = 1vt . We have
Φt+1 = 1Diag{(1 + ϵ )
πt i }vt
=

d
i=1
(1 + ϵ )
πt ivt,i
≤

d
i=1
(1 + ϵπt i )vt,i
= Φt

1 + ϵy
t πt

ln(Φt+1) ≤ ln(Φt ) + ln
1 + ϵy
t πt

≤ ln(Φt ) + ϵy
t πt .
On the third line, we have used the inequality (1 + ϵ )
x ≤ 1 + ϵx, which is valid for 0 ≤ x ≤ 1. Now,
summing over t = 1,..., τ , we obtain
τ
t=1
y
t πt ≥
1
ϵ (ln Φτ +1 − ln Φ1) = 1
ϵ ln Φτ +1 − lnd
ϵ .
The maximum of y(
τ
t=1 πt ) over y ∈ Δ[d] must be attained at one of the extreme points of Δ[d],
which are simply the standard basis vectors of Rd . Say that the maximum is attained at ei . Then,
we have
Φτ +1 = 1vτ +1 ≥ vτ +1,i = (1 + ϵ )
π1i+···+πτ i
ln Φτ +1 ≥ ln(1 + ϵ )
τ
t=1
πt i
τ
t=1
y
t πt ≥
ln(1 + ϵ )
ϵ
τ
t=1
πt i − lnd
ϵ
≥ (1 − ϵ )
τ
t=1
yπt − lnd
ϵ .
The last line follows from two observations. First, our choice ofi ensures that τ
t=1 πt i ≥ τ
t=1 yπt
for every y ∈ Δ[d]. Second, the inequality ln(1 + ϵ ) > ϵ − ϵ2 holds for every ϵ > 0. In fact,
− ln(1 + ϵ ) = ln  1
1 + ϵ
	
= ln 
1 − ϵ
1 + ϵ
	
< − ϵ
1 + ϵ
,
ln(1 + ϵ ) > ϵ
1 + ϵ > ϵ (1 − ϵ2)
1 + ϵ = ϵ − ϵ2
.
D FACTS FOR THE PROOF OF THE LOWER BOUND
For the sake of completeness, we provide self-contained proofs for the two facts used in Section 6.
Fact (Fact 6.3, restated). Let St be the sum of t i.i.d. 0-1 variables with expectation q. Let τ be
the first time this sum reaches a given number B ∈ N. Then E[τ ] = B/q. Moreover, for each T > E[τ ]
it holds that

t>T
Pr[τ ≥ t] ≤ E[τ ]
2
/T . (55)
Journal of the ACM, Vol. 65, No. 3, Article 13. Publication date: March 2018.              
13:52 A. Badanidiyuru et al.
Proof. E[τ ] = B/q follows from the martingale argument presented in the proof of Claim 6.4.
Formally, take q = p − ϵ and Nτ = τ .
Assume T > E[τ ]. The proof of Equation (55) uses two properties, one being that a geometric
random variable is memoryless and other being Markov’s inequality. Let us first bound the random
variable τ −T conditional on the event that τ > T :
E[τ −T |τ > T ] =

B
t=1
Pr[ST = t] E[τ −T |τ > T, ST = t]
=

B
t=1
Pr[ST = t] E[τ −T |ST = t]
≤

B
t=1
Pr[ST = t] E[τ −T |ST = 0]
≤ E[τ −T |ST = 0] = E[τ ].
By Markov’s inequality, we have Pr[τ ≥ T ] ≤ E[τ ]
T . Combining the two inequalities, we have

t>T
Pr[τ ≥ t] =

t>T
Pr[τ ≥ T ] Pr[τ ≥ t|τ > T ]
= Pr[τ ≥ T ] E[τ −T |τ > T ]
≤ E[τ ]
2
/T .
Fact (Fact 6.12, restated). Assume ϵ
p ≤ 1
2 and p ≤ 1
2 . Then,
p log  p
p − ϵ

+ (1 − p) log  1 − p
1 − p + ϵ

≤
2ϵ2
p .
Proof. To prove the inequality, we use the following standard inequalities:
log(1 + x) ≥ x − x2
/2 ∀x ∈ [0, 1]
log(1 − x) ≥ −x − x2 ∀x ∈ [0,
1
2
].
It follows that
p log  p
p − ϵ

+ (1 − p) log  1 − p
1 − p + ϵ

= −p log 
1 − ϵ
p

− (1 − p) log 
1 + ϵ
1 − p

≤ p

ϵ
p
+ ϵ2
p2

+ (1 − p)

− ϵ
1 − p
+ ϵ2
(1 − p)2

= ϵ2
p
+ ϵ2
1 − p
≤
2ϵ2
p . 