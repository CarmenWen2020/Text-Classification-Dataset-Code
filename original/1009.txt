We develop a systematic approach, based on convex programming and real analysis for obtaining upper
bounds on the capacity of the binary deletion channel and, more generally, channels with i.i.d. insertions and
deletions. Other than the classical deletion channel, we give special attention to the Poisson-repeat channel
introduced by Mitzenmacher and Drinea (IEEE Transactions on Information Theory, 2006). Our framework
can be applied to obtain capacity upper bounds for any repetition distribution (the deletion and Poisson-repeat
channels corresponding to the special cases of Bernoulli and Poisson distributions). Our techniques essentially
reduce the task of proving capacity upper bounds to maximizing a univariate, real-valued, and often concave
function over a bounded interval. The corresponding univariate function is carefully designed according to
the underlying distribution of repetitions, and the choices vary depending on the desired strength of the
upper bounds as well as the desired simplicity of the function (e.g., being only efficiently computable versus
having an explicit closed-form expression in terms of elementary, or common special, functions). Among our
results, we show the following:
(1) The capacity of the binary deletion channel with deletion probabilityd is at most (1 − d) logφ for d ≥
1/2 and, assuming that the capacity function is convex, is at most 1 − d log(4/φ) for d < 1/2, where
φ = (1 + √
5)/2 is the golden ratio. This is the first nontrivial capacity upper bound for any value of
d outside the limiting case d → 0 that is fully explicit and proved without computer assistance.
(2) We derive the first set of capacity upper bounds for the Poisson-repeat channel. Our results uncover
further striking connections between this channel and the deletion channel and suggest, somewhat
counter-intuitively, that the Poisson-repeat channel is actually analytically simpler than the deletion
channel and may be of key importance to a complete understanding of the deletion channel.
(3) We derive several novel upper bounds on the capacity of the deletion channel. All upper bounds
are maximums of efficiently computable, and concave, univariate real functions over a bounded
domain. In turn, we upper bound these functions in terms of explicit elementary and standard special
functions, whose maximums can be found even more efficiently (and sometimes analytically, for
example, for d = 1/2).
CCS Concepts: • Mathematics of computing → Information theory; • Theory of computation →
Error-correcting codes;
Additional Key Words and Phrases: Information theory, channel coding, error-correcting codes
1 INTRODUCTION
The binary deletion channel is generally regarded as the simplest model for communication in the
presence of synchronization errors. In this model, a transmitter encodes messages as a (potentially
unbounded) stream of bits that is then sent to a receiver over a communications channel. The channel does not corrupt bits. However, each bit may be independently discarded by the channel with
a deletion probability d ∈ [0, 1). The receiver receives the sequence of undiscarded bits, in their
respective order, and has to reconstruct the sent message with vanishing failure probability. Despite the remarkable simplicity of this fundamental model of communication, the capacity of the
deletion channel, i.e., the maximum achievable transmission rate, remains unknown. Apart from
the obvious significance in information, coding, and communications theory, the problem has attracted significant attention from the theoretical computer science community (e.g., [6, 7, 26, 27,
35, 40]). This is due to the problem’s rich combinatorial structure and its fundamental connection
with the understanding of the distribution of long subsequences in bit-strings, which, in turn, is of
significance to such theory problems as pattern matching, edit distance, longest common subsequence, communication complexity problems involving the edit distance, the document exchange
problem (cf. [4]) or secure sketching in cryptography [19], to name a few. It is also closely related to the algorithmic trace reconstruction problem that, in turn, is of significance to real-world
applications ranging from sensor networks to computational biology [35]. Robust codes against
synchronization errors are furthermore crucial for emerging applications such as DNA storage
systems (such as [5, 38]).
1.1 Previous Work
There is already a relatively vast literature on the deletion channel problem, and we are only
able to touch upon some of the major results most relevant to this work. Qualitatively, it is
known that (i) the capacity curve for this channel is continuous, (ii) the capacity is positive for
all d ∈ [0, 1) [15, 16], (iii) the capacity is 1 − Θ(d logd) (as in the binary symmetric channel) when
d → 0 [28, 29] and Θ(1 − d) when d → 1 (as in the binary erasure channel) [13, 20, 36]. Trivially,
the capacity is at most the capacity of the binary erasure channel, i.e., 1 − d. Nevertheless, the
exact capacity of the channel remains elusive. A related problem is to identify the best achievable rate against adversarial, or oblivious, deletions; for which significant progress has been recently made [7, 25]. However, in this work we focus on the Shannon-type capacity over random
deletions (and, more generally, repetitions). Much of the major known results on the subject as
well as the significance to theoretical computer science are discussed in Mitzenmacher’s excellent
survey [35].
On the achievability side, Diggavi and Grossglauser [15, 16] were the first to show that the capacity of the deletion channel is nonzero for all d ∈ [0, 1). A more explicit capacity lower bound of
(slightly better than) (1 − d)/9, for all d, was proved by Drinea and Mitzenmacher [20, 36], where
in the latter work they also introduced and motivated the Poisson-repeat channel. This channel not
only deletes bits but may also insert replicated bits in the stream. More precisely, the channel is
defined by a parameter λ and, given a bit, replicates the bit by a number sampled from a Poisson distribution with mean λ > 0 (the bit is deleted if the number of repetitions is zero). In [36],
the authors establish a connection between the Poisson-repeat and the deletion channel. Namely,
they show that any lower bound on the capacity of any Poisson-repeat channel translates into a
capacity lower bound for any deletion channel. Using a first-order Markov chain for generating
the input distribution, and numerical computations on the resulting capacity bound expressions
for various choices of λ, they derive the claimed capacity lower bound of (1 − d)/9 for the deletion
channel.
Journal of the ACM, Vol. 66, No. 2, Article 9. Publication date: March 2019.
Capacity Upper Bounds for Deletion-Type Channels 9:3
For small deletion probability d → 0, several results show that the deletion capacity behaves
similarly to the symmetric channel. Combined with [16, 42], Kalai, Mitzenmacher, and Sudan [28]
show that in this regime that capacity is 1 − h(d)(1 − o(1)), where h(·) is the binary entropy function. Independently of this work, and based on a parameter continuity argument, Kanoria and
Montanari [29] obtain a more refined asymptotic estimate in this regime that is correct up to the
O(d) term.1
Diggavi, Mitzenmacher, and Pfister [17] obtained capacity upper bounds for all d, including the
first nontrivial upper bound, of 0.7918(1 − d) for d → 1. To show the upper bounds, they consider a genie-aided decoder with access to side information about the deletion process and then
upper bound the capacity of the channel with side information (which is higher than the original capacity) using a combination of classical information-theoretic tools and a computer-based
distribution-optimization component. Different sets of numerical capacity upper bounds were obtained in [22] (and for more general channels in [23]) based on several, carefully designed, genieaided decoders. These constructions essentially reduce the problem to upper bounding the capacity
of a finite variation of the deletion channel problem, whose capacity is in turn numerically computed using the Arimoto-Blahut algorithm (which runs in exponential time in the finite number
of bits). Both [17] and [22] thus cleverly identify a finite-domain capacity problem, that is solved
numerically, and then upper bound the deletion capacity using the numerical results for the finite problem. Such techniques cannot be readily extended to such problems as the Poisson-repeat
channel problem that are inherently infinite.
1.2 Our Main Contributions
Roughly speaking, the techniques of [17] and [22] pursue the following recipe: (i) “enhance” the
channel to one with a higher capacity by carefully considering a “genie-aided” decoder that receives auxiliary information from the channel, (ii) heuristically extract a finite optimization problem to upper bound the capacity of the enhanced (and thus the original) channel, and (iii) numerically solve the finite optimization problem by a computer. While the above general method
results in very strong capacity upper bounds, much of the mathematical structure of the problem
is pushed into the computationally intensive numerical optimization problem in the third step. It is
thus unclear to what extent can the methods be further developed toward a complete understanding of the channel capacity. In this work, rather than setting our goal to improving the best-known
numerical capacity upper bounds for the deletion channel, we focus on gaining deeper insights
about the analytic structure of the problem (nevertheless, as a proof of concept, we are able to improve the best reported numerical upper bounds for small deletion probability; e.g., for d ≤ 0.02).
We develop several tools that further the existing intuitions on the deletion channel problem and
may potentially serve as key steps toward a full characterization of the capacity. As a result, we
are able, for the first time, to develop a single and systematic method that results in a capacity
upper bound curve for the deletion channel that is smooth, convex-shaped, non-trivial for all d
and simultaneously exhibits the correct behavior of c(1 − d), for a constant c < 1, at d → 1 and
1 − Θ(h(d)) at d → 0 (see Figure 16). The fact that our approach obtains the above features in a
natural and organic way suggests that the true capacity of the deletion channel might have the
same qualitative shape as what we obtain.2
1We remark that the constant behind the residual O (d) term is not specified or bounded in [29]. Therefore, while this result
sharply characterizes the limiting behavior of the capacity curve, it cannot be used to obtain concrete, numerical, bounds
on the channel capacity for any nonzero value of d. 2In particular, we believe that this observation further supports a conjecture of Dalai [13] that the capacity curve is convex,
toward which significant progress has already been made in [39].
Journal of the ACM, Vol. 66, No. 2, Article 9. Publication date: March 2019.
9:4 M. Cheraghchi
As discussed above, the best-known reported capacity upper bounds for the deletion channel
[17, 22, 39] are based on identifying a finite, but as large as possible, sub-problem (possibly by
adding side information) and then searching for the optimum solution for the finite problem by a
computer. In contrast, a key focus in our work is to avoid any computer-assisted components in the
proofs as much as possible, so as to gain as much intuition about the mathematical structure of the
problem as possible. Our results, including all the involved distributions in the proofs, are indeed
fully analytical. Namely, we upper bound the capacity of the deletion channel as the maximum
of a univariate real function, which is concave and smooth, over the interval (0, 1) (depicted in
Figure 14). The function to be maximized is explicitly defined in terms of exponentially decaying
sums and is thus computable in polynomial time in the desired accuracy. If desired, computation
of the involved sums can be avoided by using the sharp upper and lower bound estimates on the
function that we provide in terms of both elementary and standard special functions (Figure 15).
The only numerical computation would thus involve finding the maximum value of an explicitly
defined concave function over (0, 1). Even this can be avoided in some cases, leading us to the
first fully explicit capacity upper bound for the deletion channel that is nontrivial for all deletion
probabilities d ∈ (0, 1) and proved without any numerical computation: The deletion capacity is at
most (1 − d) logφ for d ≥ 1/2 and, under the plausible conjecture that the capacity function is convex
[13], at most 1 − d log(4/φ) for d < 1/2, where φ = (1 + √
5)/2 is the golden ratio. We remark that
this, itself, is better than the bounds reported in [17] for all d ≥ 0.70, while our numerical bounds
improve those of [17] for all d ≥ 0.35.
In addition to the classical deletion channel, our methods are generally applicable to any channel
with independent insertions and deletions defined by any given repetition rule. Namely, given an
arbitrary (possibly infinite) distribution D on non-negative integers, our methods can be applied to
upper bound the capacity of a channel—what we call a D-repeat channel, that replaces each input
bit independently with a number of repetitions sampled from D (where the outcome zero would
cause deletion of the bit). For the deletion channel, D would be a known Bernoulli distribution.
For such problems as the Poisson-repeat channel problem, introduced by Mitzenmacher and
Drinea [36], that are inherently infinite (even if only one bit is supplied at the input), the known
methods [17, 22] cannot be readily used, since it is not clear how to identify a finite sub-problem
that can be optimized by a computer-based search. In contrast, we show that our method easily
applies to the Poisson-repeat channel (where D is Poisson with a known mean λ), and thus we
obtain the first set of capacity upper bounds for this channel. Our methods demonstrate striking
connections between the analytical structure of this channel and the deletion channel and suggest
that understanding the Poisson-repeat channel may be the key toward the ultimate characterization of the capacity of the deletion channel. Even though the Poisson-repeat channel may appear
more complex than the deletion channel (since it not only deletes, but also inserts bits), our results suggest that the Poisson-repeat channel may be simpler to analyze. This is mainly due to the
fact that an x-fold convolution of D with itself is a binomial distribution for the deletion channel,
which is a more complex distribution than Poisson and, indeed, contains the latter as a limiting
special case. In fact, we study the Poisson-repeat channel first, which then naturally guides us
toward our results for the deletion channel. Our obtained bounds for both channels are plotted in
Figures 9 and 16 and tabulated in Tables 2 and 3.
To obtain our results, we develop a number of techniques along the way that may be of independent interest. We motivate a systematic study of what we call general mean-limited channels,
and their special case of convolution channels. These are channels, with input and output alphabets
over the reals, defined by a known probability transition rule and a mean-constraint on their output distributions. Special cases include the mean-limited binomial and Poisson channels, which
Journal of the ACM, Vol. 66, No. 2, Article 9. Publication date: March 2019.
Capacity Upper Bounds for Deletion-Type Channels 9:5
model how the deletion and Poisson-repeat channels shrink consecutive runs of bits. The notion
and our techniques can be used to model physical channels studied outside the context of deletiontype channels as well, a notable example being the well-known Poisson channel that is of central
importance to optical communications systems [41]. Indeed, a subsequent work by the author [10]
successfully applies the techniques developed in this work to obtain improved upper bounds on
the capacity of the discrete-time Poisson channel. Furthermore, our contributions in probability
theory include motivating novel distributions over non-negative integers and a first study of them,
which may be of use in other contexts as well. This includes what we define as an “inverse binomial” distribution, as well as distributions obtained by multiplying the probability mass function
of the Poisson distribution p(y) by yy or exp(yHy−1), where H denotes harmonic numbers (see
(48) and (57)). We introduce novel special functions to study such distributions (e.g., generalizations of the log-gamma function; see (114)), which may be of independent interest to mathematical
analysis.
1.3 Preliminaries and Notation
Unless otherwise stated, all logarithms are taken to base e, and the measure of information is converted from nats to bits only for the final numerical estimates. We denote the set of non-negative
real numbers by R≥0 and the set of non-negative integers by N≥0. As is standard in information
theory, we generally use capital letters for random variables. When there is no risk of confusion,
we may use the same symbol for a random variable and its underlying distribution. Support of
a random variable X, denoted by supp(X), is the set of the possible outcomes of X. Calligraphic
letters are used for several purposes: alphabets, distributions, and probability transition rules. The
entropy of a random variable X is denoted by H(X), i.e.,
H(X) := −

x ∈supp(X )
Pr[X = x] log Pr[X = x],
and h(p) denotes the binary entropy function:
h(p) := −p logp − (1 − p) log(1 − p).
The Kullback-Leibler (KL) divergence between the underlying distributions of random variables
X and Y, denoted by DKL (X Y ), is defined as
DKL (X Y ) :=

x ∈supp(X )
Pr[X = x] log(Pr[X = x]/ Pr[Y = x]).
We use I (X;Y ) for the mutual information between jointly distributed random variables X and
Y and then I (X;Y |Z) for the conditional mutual information given a third variable Z. They are
defined as follows:
I (X;Y ) := H(X) − H(X |Y ) = H(Y ) − H(Y |X)
I (X;Y |Z) := H(X |Z) − H(X |Y,Z) = H(Y |Z) − H(Y |X,Z),
where conditional entropy is defined as
H(X |Y ) =

y ∈supp(Y )
Pr[Y = y]H(X |Y = y)
and
H(X |Y = y) = −

x ∈supp(X )
Pr[X = x |Y = y] log Pr[X = x |Y = y].
In the above, a term 0 log 0 is understood to be zero.
Journal of the ACM, Vol. 66, No. 2, Article 9. Publication date: March 2019.    
9:6 M. Cheraghchi
The Bernoulli distribution with meanp is denoted by Berp (so that Berp (0) = 1 − p and Berp (1) =
p). The binomial distribution with x ∈ N≥0 trials and success probability p (thus mean xp) is denoted by Binx,p , which is the distribution of the sum of x independent Bernoulli random variables
with mean p. We have
Binx,p (y) :=

x
y

py (1 − p)
x−y, y = 0, 1,..., x.
The Shannon capacity of a channel Ch is denoted by Cap(Ch) and is defined as
Cap(Ch) := sup I (X;Y ),
where X is the channel input, Y is the channel output, and the supremum is over the choice of
the distribution of X. The Poisson distribution with mean λ, denoted by Poiλ, is supported on
non-negative integers and defined by the mass function
Poiλ (y) := e−λ λy
y!
, y = 0, 1,....
The Poisson distribution Poiλ is the limit of the binomial distribution Binx,p when λ = xp and
x → ∞. The negative binomial distribution of order r > 0 with “success probability” q ∈ (0, 1) is
defined by the probability mass function
NegBinr,q (y) =

y + r − 1
y

(1 − q)
r
qy, y = 0, 1,..., (1)
and has mean μ = qr/(1 − q) [14, Section 5.3]. When r ∈ N, it captures an independent summation
of r geometric random variables.
The run-length encoding of a bit string X starting with bit b ∈ {0, 1} is the unique sequence
of positive integers X1,X2,...,Xt such that X consists of X1 copies of the bit b (a run of length
X1), followed by X2 copies of the negated bit ¯
b, followed by X3 copies of the bit b and so forth.
For example, the run-length encoding of the string 001000100111110 is 2, 1, 3, 1, 2, 5, 1. Note that a
run-length encoding defines the original sequence up to a negation of all bits.
We use the asymptotic notation f (x) ∼ д(x) to mean limx→∞ f (x)/д(x) = 1. We may use the
binomial coefficient (
x
y ) over non-integers, in which case the definition

x
y

:= Γ(1 + x)
Γ(1 + y)Γ(1 + x − y)
should be used.
1.4 Organization
The rest of the article is organized as follows: Section 2 gives a high-level exposition of the entire work, explaining the developed techniques with a focus on intuitions and underlying insights
rather than the technical details. In Section 3, we formally define the notion of general meanlimited channels, as well as the special case of convolution channels. We prove a duality-based
necessary and sufficient condition for achieving the capacity of such channels, as well as the dual
feasibility criteria that certify upper bounds on the capacity. Section 4 formally defines the general notion of D-repeat channels and proves a capacity upper bound for such channels based
on the capacity of a mean-limited channel defined according to D. We use the obtained tools in
Section 5 to obtain our capacity upper bounds for the Poisson-repeat channel. Toward this end,
we construct two dual-feasible solutions for the corresponding mean-limited channel and estimate
their parameters in terms of elementary and standard special functions. In Section 6, guided by
Journal of the ACM, Vol. 66, No. 2, Article 9. Publication date: March 2019.   
Capacity Upper Bounds for Deletion-Type Channels 9:7
the result obtained for the Poisson-repeat channel, we prove our capacity upper bounds for the
deletion channel. We first introduce the notion of inverse binomial distributions and then show
that it is dual feasible for the mean-limited binomial channel. We estimate the parameters of this
distribution in terms of elementary and standard spacial functions. We then apply a truncation
technique, which we first develop for Poisson-repeat channels, to refine the dual feasible solution
and obtain improved capacity upper bounds.
2 HIGH-LEVEL EXPOSITION OF THE TECHNIQUES AND RESULTS
In this work, we formalize and study the notion of “general repeat channels,” which are binary
input channels characterized by a given probability distribution D over non-negative integers. A
D-repeat channel, given a bit, draws an independent sample D ≥ 0 from D and outputs D copies
of the received bit. We define the deletion probability d of the channel to be the probability that D
assigns to zero and let p = p(D) := 1 − d be the retention probability. Thus, what we call the deletion channel corresponds to the case where D is the Bernoulli distribution with mean p. However,
if D is a Poisson distribution with mean λ, then we get a Poisson-repeat channel with deletion
probability d = e−λ. We note that, in general, D need not be uniquely determined by its deletion
parameter d, albeit this is the case for the class of deletion and Poisson-repeat channels.
2.1 Reduction to Mean-Limited and Convolution Channels
Suppose the input to a D-repeat channel is a bit-sequence X = B1, B2,..., Bn with Y being the
output bit-sequence. If μ := E[D], then the expected output length would be μn. By Shannon’s
theorem, it can be seen (as in [18]) that the capacity of the channel, which we denote below by
Cap(D), is the supremum of the normalized mutual information between X and Y, i.e.,
Cap(D) = lim
n→∞
I (X;Y )
n .
A common technique in analyzing the deletion channel is to consider how it acts on runs of
bits rather than the individual bits. Given a run of x > 0 bits, the deletion channel outputs a run
of Binx,p bits, i.e., a sample from the binomial distribution with x trials and success probability
p. For the Poisson-repeat channel, this would be a run of length given by a Poisson sample with
mean λx. In general, the distribution of the output run-length would be the x-fold convolution of
D with itself, which we denote by D⊕x .
Since n grows to infinity, without loss of generality, we can assume that the first input bit B1 is
zero. This allows us to unambiguously think of X as its run-length encoding X = X1,X2,...,Xt ,
where each Xi is a positive integer. Similarly, we may also think of Y by its run-length encoding
Y = Y1,Y2,...,Ym, where each Yi is a positive integer. This will identify Y up to a negation of all
bits. Since the channel has memory, the random variables Yi (unconditionally) are not necessarily
independent. However, this would be the case if the Xi are independent and identically distributed,
in which case the Yi also become identically distributed. Note that, given X, the bit-length of Y and
the parameter m are random variables determined by the channel, and indeed the randomness of
m causes technical difficulties that should be rigorously handled by a careful analysis. However,
in the informal exposition below we pretend that m is known and fixed a priori. One may now
attempt to use the chain rule and write
I (X;Y ) = I (X;Y1) + I (X;Y2 |Y1) + ··· + I (X;Ym |Y1 ...Ym−1) ≤
m
i=1
(H(Yi ) − H(Yi |X,Y1 ...Ym−1)).
Journal of the ACM, Vol. 66, No. 2, Article 9. Publication date: March 2019. 
9:8 M. Cheraghchi
A major difficulty in deriving the capacity of the deletion channel is the fact that, unlike channels
with no synchronization errors, a certain Yi does not only depend on the corresponding Xi but
rather potentially any part of X. Given X, we know that Yi has a binomial distribution with mean
depending on the summation XJ + XJ+2 + ··· + XJ+2K for some random variables J and K that are
in general difficult to analyze. Furthermore, even given a fixed X, the random variables Y1,Y2,...
do not become conditionally independent. Therefore, the above result of the chain rule cannot
be upper bounded by a simpler, single-letter, expression. A natural idea, that has been pursued
previously (e.g., [17, 22]), is to consider “genie-aided” decoders that receive enhanced side information by the channel. The side information is carefully designed so as to reduce the problem to an
i.i.d. channel problem that can be analyzed more conveniently. However, this approach generally
comes at the expense of effectively turning the channel into one with strictly higher capacity and,
consequently, obtaining inherently sub-optimal capacity upper bounds.
The Result of Diggavi, Mitzenmacher, and Pfister [17]: An elegant execution of the above idea,
that in fact inspires the starting point of our work, has been done in [17], which we briefly explain
here. This result is based on the simple idea that if we imagine that the channel places a “comma”
after each run of bits, such that the commas are never deleted by the channel, then this can only
increase the capacity.3 Furthermore, the enhanced channel is equivalent to an i.i.d. channel that
receives a stream of positive integers (i.e., the run-length encoding of X) and passes each integer
independently over a “run-length” channel. The effect of the run-length channel, given an input x,
is to output a sample from Binx,1−d , i.e., the binomial distribution with x trials and success probability p = 1 − d. Now the capacity of the deletion channel can be upper bounded by the capacity
of the run-length channel, normalized by the number of input channel uses (i.e., the length of X).
Since the run-length channel is i.i.d., its capacity is equivalent to the single-letter capacity. Thus,
letting U and V denote the input and output of a single use of the run-length channel, then capacity of the deletion channel is upper bounded by supU I (U ;V )/E[U ], where the supremum is over
the distribution of U over positive integers.4 At this point, a fundamental result of Abdel-Ghaffar
[1] on per-unit-cost capacity can be used to, in turn, upper bound the resulting expression.
In the per-unit-cost capacity problem, a cost function c(u) is defined over the input domain
U of a channel with transition rule P(v|u) and the capacity is defined as sup I (U ;V )/E[c(U )],
where the supremum is over the distribution of U . For the above application, the input domain is
the positive integers and the cost function is identity: c(u) = u. As proved in [1], a necessary and
sufficient condition for a pair (U,V ) to be capacity achieving is the following: Letting
C(V ) := sup
u ∈U
(DKL (Vu V )/c(u)),
where Vu is the output distribution corresponding to a fixed input u and DKL (··) is the KL divergence, the supremum is attained for all u on the support ofU . In this case,C(V ) is the per-unit-cost
capacity. Furthermore, for any distributionV on the output domain (whether or not it corresponds
to an input distribution), the capacity is upper bounded by C(V ) as defined above.
There are two drawbacks with the above approach taken in [17]. First, the side information in
fact genuinely increases the channel’s capacity, and, therefore, the upper bounds resulting from
3For the limiting case d → 1, the authors establish a different channel enhancement argument using carefully placed markers that we do not discuss here.
4It is important to note that U is defined over non-zero integers. Without this consideration, the capacity upper bound
would be infinite. This can be easily seen by considering a distribution U that puts 1 − ϵ of the probability mass on
the zero outcome, for some ϵ > 0, and has mean Θ(ϵ ). A straightforward calculation shows that, in this case, I (U ;V ) =
Ω(ϵ log(1/ϵ )) and that the capacity upper bound becomes Ω(log(1/ϵ )), which can be made arbitrarily large by choosing
a sufficiently small ϵ.
Journal of the ACM, Vol. 66, No. 2, Article 9. Publication date: March 2019.
Capacity Upper Bounds for Deletion-Type Channels 9:9
this method are inherently sub-optimal. One way to see this is to consider the case where p = 1 − d
is small. Consider the run-length channel (U,V ) and a choice ofU that assigns a 1 − p of the probability mass to U = 1 and the rest to U = 2. In this case, one can see that I (U ;V ) = Ω(p log(1/p)),
and, thus, the per-unit-cost capacity is also at least Ω(p log(1/p)). This is while, by the trivial erasure channel upper bound, the deletion capacity must be at most p. Indeed, the numerical upper
bounds reported in [17] exhibit this phenomenon at d close to 1 (notice the distinctive concavity
in this area in Figure 16). The second drawback is that, while the result of Abdel-Ghaffar is in
principle powerful enough to characterize the true per-unit-cost capacity upper bound, it may be
extremely difficult to work with this result analytically. A way around this issue, undertaken in
[17], is to employ a computer-based search. To do so, first a finite-domain distribution (supported
up to an integer M) for U that maximizes the capacity is constructed by a computer-based search,
and the corresponding KL divergence supremum, up to M, is numerically computed. Then, the
resulting V is truncated, and the tail is geometrically redistributed over the remaining (infinite)
input domain. The KL divergence at large values of u can be accurately approximated by a linear
function of u, at which point [1] can be applied with the modified choice of V , allowing the resulting capacity upper bound to be numerically computed. While this approach indeed achieves very
strong numerical capacity upper bounds (especially for small to moderate values of d), much of
the analytic structure of the problem is absorbed by the computer-based search, making further
progress elusive. In this work, we develop a systematic, albeit technically demanding, approach to
overcome the barriers encountered in [17].
Equivalent Reformulation of the Channel: Consider any D-repeat channel Ch with deletion probability d = 1 − p. Rather than introducing side information, which, as demonstrated above, may
result in a channel with strictly larger capacity, we use a careful analysis to decompose the action of
the channel into two steps, forming a Markov chain X − Z − Y, such that the resulting Y from the
two-step process has the exact distribution as the run-length encoding of the output of Ch. Then,
we upper bound the capacity by I (Z;Y ) divided by the number of channel uses in X. Assume, without loss of generality, that the first input bit in X is zero and this bit is promised not to be deleted by
the channel (in particular, the first bit ever output by the channel is also zero). Given the input X,
the channel considers the run-length encoding X1,X2,...,Xt of X. To produce the first run in the
output, the channel starts deleting bits from the even runs X2,X4,... until the first non-deletion
event occurs, say, at run X2j . The odd runs are then combined as Z1 = X1 + X3 + ··· + X2j−1, and
the process continues from the first survived bit in the even runs until the input is fully scanned.
The resulting sequence Z1,Z2,...,Zm is then passed component-wise through a channel Ch
, with
integer input and output, defined according to D, to produce the output sequence Y1,Y2,...,Ym
(for the case of the deletion channel, each Yi is formed simply by passing Zi − 1 through a binomial channel,5 much similar to [17]). We show that the resulting sequence has precisely the same
distribution as the run-length encoding of the output of Ch. By a delicate analysis of this process,
which is depicted in Figure 1, we are able to show that the capacity of Ch is upper bounded as
Cap(Ch) ≤ sup
U
I (U ;V )
1/p + E[U ]
, (2)
where U and V are the input and output distributions of Ch
. This constitutes the first technical
building block in our capacity upper bound proofs. The bound (2) has a similar flavor in form, but
is strictly stronger, than the upper bound expression in [17] (especially for larger d).
5A binomial channel, given a non-negative integer input x, outputs a sample from the binomial distribution with x trials
and success probability p.
Journal of the ACM, Vol. 66, No. 2, Article 9. Publication date: March 2019.
9:10 M. Cheraghchi
Mean-Limited and Convolution Channels: Once (2) is available, one may attempt to apply AbdelGhaffar’s result [1] to obtain a capacity upper bound by using the cost function c(u) = u + 1/p.
Indeed, any upper bound may in principle be obtained by this result, as it provides necessary
and sufficient conditions for characterizing the quantity on the right-hand side of (2). However,
as demonstrated in [17], an analytic approach for obtaining a distribution V that minimizes the
divergence fraction
sup
u ∈U
DKL (Vu V )
u + 1/p ,
or even gets sharply close to the minimum, may be extremely challenging. Instead, [17] uses
a computer-assisted optimization subroutine to construct a satisfactory V and numerically
upper bound the capacity. While this exact numerical optimization subroutine applied on
(2) will strictly improve the numerical capacity upper bounds reported in [17] (since the cost
function c(u) = u + 1/p resulting from (2) is strictly larger than the cost function c(u) = u that
is used in [17]), our aim is to obtain an analytic improvement that avoids extensive numerical
computations and provides deeper insights into the structure of the problem. To overcome this
difficulty, we observe that it would be much more natural to break down the task of finding the
best distribution for V into two steps. First, we restrict the mean of V to a fixed parameter μ and
optimize only over those U such that E[V ] = μ. This fixes the denominator of the divergence
fraction to a constant and allows us to focus on optimizing the non-fractional quantity I (U ;V )
with respect to the fixed mean constraint. Then, we take the supremum of the resulting bounds
over the choice of μ to upper bound (2). Note that the optimal V for the two-step optimization
must satisfy the (necessary and sufficient) conditions of [1] as well, so the two methods for
characterizing the capacity-achieving pair (U,V ) are technically equivalent. However, factoring
out the mean allows for a much more natural and systematic derivation of the right distribution
for V and is what allows us to achieve the desired analytic breakthroughs.
We thus obtain the abstraction of what we call a “mean-limited channel.” Such a channel is defined with respect to certain input and output domains over non-negative reals and a transition
rule P(y|x) for producing an output distribution over the output domain given an input distribution over the input domain. Furthermore, the channel is given a mean parameter μ > 0 and only
accepts those input distributions for which the corresponding output distributions have mean μ.
The capacity of the channel is determined in the standard sense of maximal mutual information
between admissible input and output distributions.
The abstraction is of general and independent interest to the study of communications channels in presence of mean “power constraints,” such as the classical Poisson channel. However,
for our applications, it suffices to consider discrete domains (in particular, non-negative integers)
and the special case of transition rules that are defined by convolutions of distributions, resulting in a special case that we call a “convolution” channel. A convolution channel is defined with
respect to a distribution D over non-negative reals and is denoted by Chμ (D), where μ > 0 is
the output mean constraint. Given an input x, the channel produces a sample from D⊕x (i.e., the
distribution defined by the xth power of the characteristic function of D) in the output. One may
extend the notion of mean-limited channels to allow for m uses and for a total mean constraint
μm. Namely, the channel now accepts an m-dimensional sequence U1,...,Um at input and passes
each Ui through an independent, identical, mean-limited channel to generate the output sequence
V1,...,Vm. The mean-constraint in this case would enforce the condition E[V1 + ··· +Vm] = μm.
It is straightforward to show that the capacity of this channel is achieved by a product distribution.
Journal of the ACM, Vol. 66, No. 2, Article 9. Publication date: March 2019.
Capacity Upper Bounds for Deletion-Type Channels 9:11
2.2 Upper Bounding the Capacity of General Mean-Limited Channels
The appeal in reducing the capacity upper bound problems for D-repeat channels to that of general mean-limited channels is that, for the latter, one may naturally use powerful tools from convex optimization to obtain strong capacity upper bounds in a systematic and completely analytic
fashion. To this end, we prove an analogue of Abdel-Ghaffar’s result [1] for general mean-limited
channels. However, we use a different, direct, proof. Namely, in Section 3.1 we directly write the
mutual information maximization problem as a convex program, form its dual, and observe that
strong duality holds. Hence, we may write down the Karush-Kuhn-Tucker (KKT) conditions that
provide the necessary and sufficient conditions for optimality.
Characterization of channel capacity in terms of the optimum of a convex program and the use
of duality is a standard technique in information theory (cf. [12]). Variations of this technique has
been used, for example, toward understanding the capacity of multiple-antenna systems [32] and
the discrete time Poisson channel [33, 34]. In this section, we derive a variation tailored to our
applications for upper bounding the capacity of mean-limited channels.6
Consider a general mean-limited channel Ch with transition rule P, input domain X, and meanconstraint μ. With a slight overload of the notation, in this section consider an input distribution
X for Ch, and let Y denote the corresponding output distribution. For any fixed input x, denote by
Yx the output random variable when the input is fixed to x. The KKT conditions imply that X is
capacity achieving if and only if, for some real parameters ν0 and ν1, we have the following dual
feasibility conditions:
(∀x ∈ X) : DKL (Yx Y ) ≤ ν1E[Yx ] + ν0, (3)
with equality for all x ∈ supp(X). In this case, the capacity is equal to ν1μ + ν0. Furthermore, if
there is a distribution Y over the output alphabet for which (3) holds (and we call the distribution
dual feasible), then Cap(Ch) ≤ ν1μ + ν0. These results are summarized in Theorem 1.
Perhaps the most technically demanding aspect of this work is to obtain fully analytical dual
feasible solutions Y that provably provide sharp, and explicit, or at least efficiently computable, upper bound estimates on the capacity of the mean-limited Poisson and binomial channels. These, in
turn, lead to capacity upper bounds for the Poisson-repeat and deletion channels. In both cases, we
carefully construct dual feasible distributions parameterized by a parameter q ∈ (0, 1) that controls
the mean μ to any arbitrary positive value. Once the feasibility of these distributions are proved,
we explicitly write down the corresponding real parameters ν0, ν1, as well as the resulting capacity upper bound ν1μ + ν0 (which requires writing μ as a function of q) and plug in the resulting
upper bound in (2). This, in turn, results in an upper bound expression for the capacity of the
original D-repeat problem (e.g., either the Poisson-repeat or deletion channel) as the maximum of
a uni-variate real function in q (which turns out to be concave in q). In all cases, this function is
efficiently computable. In turn, we upper bound this function in terms of either explicit elementary
functions or, more sharply, in terms of the standard special functions. Thus, in particular, we are
able to reduce the problem of upper bounding the capacity of a Poisson-repeat or deletion channel
to finding the maximum of an elementary, concave, function of q. Numerical computation is then
only applied, if necessary, at the very last stage for computing the maximizing value of q for this
function and the corresponding capacity upper bound.
The Poisson-Repeat Channel: To obtain a capacity upper bound for the Poisson-repeat channel,
we use (2) combined with a capacity upper bound for the corresponding mean-limited Poisson
6The variation developed in this section can be generally applied to any mean-limited discrete or continuous channel (albeit
extra care is needed in rigor for continuous channels). In a subsequent work by the author [10], the technique has been
successfully applied to obtain simple and improved upper bounds on the capacity of the discrete-time Poisson channel.
Journal of the ACM, Vol. 66, No. 2, Article 9. Publication date: March 2019.
9:12 M. Cheraghchi
channel using the convex duality conditions described above. Suppose that the Poisson-repeat
channel replaces each bit with a number of bits sampled from a Poisson distribution with mean
λ. Therefore, the deletion probability of this channel (i.e., probability that a bit is replaced by zero
copies) is
d = e−λ =: 1 − p.
The corresponding mean-limited Poisson channel Ch with mean constraint μ takes a non-negative
integer X at input and outputs a fresh sample from the Poisson distribution with mean λX. We
use a convexity argument to show that the following distribution over the non-negative integers,
parameterized by q ∈ (0, 1), satisfies (3) with ν0 = − logy0 and ν1 = − logq:
Pr[Y = y] = y0
yy
y!
(q/e)
y, (4)
where y0 is the normalizing constant and 00 := 1. We refer to the above distribution as the
convexity-based distribution. The values of y0 and μ := E[Y] would in turn be functions of q and
can numerically be computed in polynomial time in the desired accuracy, given the exponential
decay of (4) (see Table 1). This results in a capacity upper bound of
sup
q ∈(0,1)
(−μ logq − logy0)
for the mean-limited channel (Theorem 10). Furthermore, combined with (2), we get the first set of
capacity upper bounds for the Poisson-repeat channel with deletion probability d (Theorem 17):
Cap ≤ sup
q ∈(0,1)
−μ(q) logq − logy0 (q)
−μ(q)/ logd + 1/(1 − d)
. (5)
The function inside the supremum turns out to be concave, and the maximum can efficiently be
found by a simple search (Figure 8). However, it is desirable to have sharp upper bound estimates
on the function (in q) to be maximized. Note that, from Stirling’s approximation, the asymptotic
behavior of (4) is Θ(qy/
√y). Therefore, intuitively, it should be possible to estimate y0 and μ in
terms of the summations ∞
y=1 qy/
√y and ∞
y=1
√yqy , which may be expressed by the polylogarithm function
Lis (z) :=
∞
k=1
zkk−s (6)
(however, obtaining upper and lower bounds requires more work). This allows us to provide a
remarkably sharp upper estimate on the function in (5) in terms of the standard special functions.
This is made precise in Theorem 13 and the quality of the approximation is depicted in Figure 6.
We observe that the gap in (3) (which we shall call the KL gap) achieved by (4) is zero at x = 0
but converges to an absolute constant (namely, 1/2) as x → ∞. This results in sub-optimal capacity
upper bounds. We rectify this issue by replacing the yy = exp(y logy) term in (4) with exp(yψ (y)),
where ψ (y) is the digamma function (essentially we are replacing the logy in the exponent with
harmonic numbers, which have the same asymptotic behavior); namely, we now use what we call
the digamma distribution
Pr[Y = y] = y0
exp(yψ (y))
y! (q/e)
y, (7)
with Pr[Y = 0] = y0. Using the Newton series expansion of harmonic numbers as well as the factorial moments of the Poisson distribution, we show that this alternative choice is also dual feasible,
Journal of the ACM, Vol. 66, No. 2, Article 9. Publication date: March 2019. 
Capacity Upper Bounds for Deletion-Type Channels 9:13
and, in fact, the KL gap in (3) offered by this choice is precisely λxE1 (λx), where E1 (·) is the exponential integral function
E1 (λ) =
 ∞
1
e−λt
t
dt. (8)
Thus the KL gap is zero at x = 0 and exponentially vanishes as x grows (Figure 2). This leads
to a significant improvement in the resulting capacity upper bounds (Figure 9). We note that the
digamma distribution (7) still exhibits the same asymptotic behavior as (4), and thus its parameters
can be similarly approximated. However, we show that the same asymptotic behavior is exhibited
by the well-studied negative binomial distribution (1) of order r = 1/2. Since the parameters of a
negative binomial distribution take remarkably simple forms, we are able to obtain excellent upper
and lower bound estimates on the parameters y0 and μ of the digamma distribution (7), and in turn
the function inside the supremum in (5), in terms of elementary functions. This is made precise in
Section 5.3.2 (Corollary 16). As a result, we obtain several upper bound estimates on the capacity of
the Poisson-repeat channel (either using (4) or the digamma distribution (7) or their upper bound
estimates), which are depicted in Figure 9 and listed in Table 2.
The Deletion Channel: At a first glance, it is natural to get the impression that understanding
the capacity of a Poisson-repeat channel may be a more complex problem than that of a deletion
channel. After all, a deletion channel only deletes bits whereas a Poisson-repeat channel may
cause insertions (repetitions) in addition to deletions. However, our work indicates that, counterintuitively, the deletion channel is analytically more complex than the Poisson-repeat channel. In
fact, we use the above results for the Poisson-repeat channel as a guiding tool toward attacking
the deletion channel problem (which is why the Poisson-repeat channel is discussed first). The
mean-limited channel corresponding to a deletion channel is a binomial channel, which maps an
input x to the binomial distribution Binx,1−d over x trials. However, the Poisson-repeat channel
corresponds to a mean-limited Poisson channel, which maps x to a Poisson random variable with
mean λx = −x logd. A Poisson distribution, being a one-parameter distribution, is analytically
simpler than a two-parameter binomial distribution. Indeed, the Poisson distribution is a limiting
special case of the binomial distribution. We use this intuition to extend our results for the Poissonrepeat channel to the deletion channel. As in the Poisson case, we invoke (2) to reduce the capacity
upper bound problem for the deletion channel to that of the mean-limited binomial channel with
output mean constraint μ. Then, the task of finding a dual feasible distribution Y naturally leads
us to a novel distribution that we call an “inverse binomial” distribution (discussed in Section 6.1),
which is defined, for q ∈ (0, 1), by
Pr[Y = y] = y0

y/p
y

qy exp(−yh(p)/p), (9)
where h(·) is the binary entropy function. The parameter q uniquely determines the normalizing
constant y0 and the mean μ = E[Y] (and the mean can be adjusted to any desired positive value).
We use a convexity argument to show (in Theorem 26) that the above distribution indeed satisfies
(3) with ν0 = − logy0 and ν1 = − logq, thus resulting in a capacity upper bound of
sup
q ∈(0,1)
(−μ logq − logy0)
for the mean-limited binomial channel and a capacity upper bound of
Cap ≤ p sup
q ∈(0,1)
−μ(q) logq − logy0 (q)
1 + μ(q) (10)
Journal of the ACM, Vol. 66, No. 2, Article 9. Publication date: March 2019. 
9:14 M. Cheraghchi
for the deletion channel. As in the Poisson case, it is desirable to obtain sharp upper bound estimates on the term inside the supremum in (10), which turns out to be a concave function of q, in
terms of elementary or common special functions. This is a technical task, and in Section 6.1.2 (in
particular, Theorem 24), we obtain sandwiching bounds for the parameters of an inverse binomial
distribution in terms of the Lerch transcendent (11), a well-studied generalization of the Riemann
zeta function given by (cf. [21, p. 27])
Φ(z,s, α) :=
∞
k=0
zk
(k + α)s = 1
Γ(s)
 ∞
0
t s−1
e−α t
1 − ze−t dt. (11)
Furthermore, we observe that an inverse binomial distribution exhibits the same asymptotic
growth as a negative binomial distribution of order r = 1/2. Using this, we are able to obtain
upper- and lower-bound estimates on the parameters of an inverse binomial distribution in terms
of elementary functions (Corollary 22 in Section 6.1.1). The estimates are excellent if the deletion
probability is not too small (Figure 10).
Interestingly, we show that for p = d = 1/2, the inverse binomial distribution is exactly a negative binomial distribution. Thus, in this case, we can write down the exact parameters of the distribution in terms of elementary functions and show that the term inside the supremum in (10) is simply h(q)/(2 − q), which is maximized at q = 1 − φ, where φ = (
√
5 + 1)/2 is the golden ratio, which
results in the fully explicit capacity upper bound of (logφ)/2 ≈ 0.347120 (bits per channel use) for
the deletion channel with d = 1/2. We may then interpolate between this bound and the trivial values at d = 0, 1 using a convexity technique7 of [39], thereby obtaining fully explicit capacity upper
bounds for general d that are proved without any need for numerical computation (Corollary 36).
As in the Poisson case with (4), the inverse binomial distribution suffers from a constant asymptotic KL gap of 1/2 in (3) (Figure 11). By examining the connection between (4) and the digamma
distribution (7), we develop a systematic “truncation technique” (made precise in Section 5.2) that
allows us to refine (9) to sharply eliminate the KL gap for the binomial case as well. To begin with,
we prove that enforcing the KKT conditions (3) with equality for all x ≥ 0 results in a unique class
of solutions for the distribution of Y, which is exactly
Pr[Y = y] = y0qy exp

−

y
k=0

y
k
 1
pk

1 − 1
p
y−k
H(Bink,p )

=: y0qy exp 
д(y) − yh(p)/p

/y!.
(12)
Therefore, if such a distribution feasibly exists, then it would necessarily be capacity achieving.
However, we observe that the term inside the exponent (what we have labeled as д(y) in (12))
exponentially grows in y, and, therefore, there is no normalizing constant8 y0 that would make
(12) a valid distribution for any q > 0. Our proposed truncation technique adjusts the exponent of
this alleged optimal solution so as to make its growth rate manageable, while still satisfying the
dual feasibility conditions (3) with a potentially nonzero KL gap that exponentially decays in x. To
do so, we first prove the following integral expression for д(y) in (12):
д(y) = E1/p (y) − E1/p−1 (y), where Eϵ (y) =
 1
0
1 − ϵty − (1 − ϵt)
y
t log(1 − t) dt. (13)
We show that Eϵ (y) exponentially grows in y when ϵ > 1 and grows as ϵy(log(ϵy) − 1) + o(y)
when ϵ ≤ 1 (Claim 27). The truncation technique would involve the truncation of the upper bound
7For extending the bounds to d < 1/2, the results of [39] are not tight, so in this regime we rely on the plausible conjecture
that the capacity function is convex [13]. 8Interestingly, using (3), this shows that while the optimal input distribution must have infinite support, it cannot have a
full support. See Remark 30 and, similarly for the Poisson case, Remark 12.
Journal of the ACM, Vol. 66, No. 2, Article 9. Publication date: March 2019.      
Capacity Upper Bounds for Deletion-Type Channels 9:15
of the integral that defines Eϵ (y) (when ϵ > 1) to 1/ϵ. The resulting function, that we call Λ1/ϵ (y),
may be written, after a change of variables, as
Λϵ (y) :=
 1
0
1 − ty − (1 − t)
y
t log(1 − ϵt) dt.
We remark that Λ1 (y) = E1 (y) = log Γ(1 + y). When ϵ ≤ 1, the growth rate of Λϵ (y) is
(y/ϵ )(log(y/ϵ ) + Li(1 − ϵ ) − 1) + o(y),
where Li(·) is the logarithmic integral
Li(z) :=
 z
0
dt
log t
. (14)
Using the factorial moments of the binomial distribution, we show that E[Eϵ (Yx )] = Eϵp (x)
(Proposition 29) and, furthermore, that
E[Λϵ (Yx )] = Ep/ϵ (x) + xpLi(1 − ϵ )/ϵ − η(1 − ϵ ) +
 1
ϵ
(1 − tp/ϵ )
x
t log(1 − t)
dt, where η(z) :=
 z
0
dt
(1 − t) log t
.
From the above results, it follows that letting
дp (y) :=
⎧⎪
⎨
⎪
⎩
Λp (y) − Λp/(1−p) (y) + y
p
	
(1 − p)Li	 1−2p
1−p


− Li(1 − p)


+ η(1 − p) − η
	 1−2p
1−p

 p ∈ (0, 1/2),
Λp (y) − E1/p−1 (y) − yLi(1 − p)/p + η(1 − p) p ∈ [1/2, 1],
and replacing the д(y) in (12) with дp (y) (except for the special case д(0) = 0) results in a refined
distribution for Y that sharply satisfies (3). We refer to this distribution as the truncated distribution.
Despite the complex-looking expression defining the truncated distribution, as we see in Section 6.2.2, the distribution converges pointwise to the dual-feasible solution (7) (the digamma distribution) for the Poisson case as p → 0; therefore, it is indeed a generalization of (7) to arbitrary
values of p. The KL gap for this distribution can be explicitly computed in integral form (using the
above results for the expectations of Eϵ and Λϵ ), which we show to be
Rp (x) =
 1
p
(1 − t)
x − (1 − p)
x
t log(1 − t) dt −
 1
p
1−p
(1 − (1 − p)t)
x − (1 − p)
x
t log(1 − t) dt,
with the second integral understood to be zero for p ≥ 1/2. This gap is zero at x = 0, exponentially
decays as x grows, and converges to xpE1 (xp) (as in the Poisson case) for p → 0 (see Figure 11).
Hence, we obtain several capacity upper bounds, of varying complexities, for the deletion channel.
This depends on the chosen dual feasible distribution for Y, that is, either the truncated variation
of (12) or the inverse binomial distribution (9), or in the latter case, whether the function in (10)
is numerically computed or upper bounded by either elementary or standard special functions
(Figures 14 and 15). The resulting bounds are depicted in Figure 16 and are listed in Table 3. For
the limiting case d → 1, we see that our best upper bound estimate is 0.4644(1 − d), which comes
quite close to the computer-assisted upper bound 0.4143(1 − d) reported in [39], and substantially
improves the 0.7918(1 − d) in [17] (which is also computer-assisted). Our fully explicit upper bound
of (1 − d) logφ ≈ 0.694242(1 − d) is also better than what reported in [17]. Since our methods for
the Poisson-repeat channel converge to what we obtain for the deletion channel in the limit d → 1,
we obtain the same upper bound estimate of 0.4644(1 − d) for the capacity of the Poisson-repeat
channel with deletion probability d → 1 as well. Finally, we analyze our results for the limiting
case d → 0 (Section 6.3.4). We prove that, in this regime, our upper bounds exhibit the asymptotic
behavior of 1 − Θ(h(d)), which is known to be the case for the true capacity of the deletion channel
[28, 29].
Journal of the ACM, Vol. 66, No. 2, Article 9. Publication date: March 2019.
9:16 M. Cheraghchi
3 GENERAL MEAN-LIMITED AND CONVOLUTION CHANNELS
In this section, we consider general classes of channels that we call “general mean-limited” and
“convolution channels.” The input and output alphabets for these channels are the set R≥0 of nonnegative reals. In general, any such channel can be described by a probability transition rule P(y|x)
over the non-negative reals, where x,y ∈ R≥0. The channel takes an input X ∈ R≥0 and outputs
the output random variable Y according to the rule P(y|x). Since the capacity of this channel
may in general be infinite, we restrict the set of possible input distributions by defining a mean
constraint E[Y] = μ, for a parameter μ > 0, and use the notation Chμ (P) and the terminology
general mean-limited channel for the channel defined with respect to the transition rule P(y|x)
and mean constraint μ. The rate achieved by an input distribution for this channel is defined as
I (X;Y )/E[X], and, naturally, the capacity is the supremum of the achievable rates subject to the
given mean constraint.
A natural choice for the transition rule P, that results in what we call a convolution channel, is
via a multiplicative noise distribution D over non-negative reals. For an x ∈ R≥0, let D⊕x denote
the distribution attained by raising the characteristic function of D to the power x. When x is
a positive integer, this would correspond to adding together x independent samples from D or,
equivalently, the x-fold convolution of the distribution D with itself (hence the terminology “convolution channel”). The convolution channel defined with respect to D, that we use the overloaded
notation Chμ (D) for, takes an input X ∈ R≥0 and outputs a sample from D⊕X .
Let λ := E[D]. Note that, since E[Y] = λE[X], the rate achieved by an input distribution X
would be λI (X;Y )/μ, and the capacity is simply (λ/μ) sup I (X;Y ), where the supremum is over
the input distributions X satisfying E[X] = μ/λ.
A convolution channel, or, more generally, any channel Chμ (P), can be defined over continuous
or discrete distributions. In this work, for the sake of concreteness and a unified notation, we focus
on the discrete case (and in fact, the integer case). However, the results can be readily extended
to continuous distributions if differential entropy and mutual information are used (rather than
the discrete Shannon entropy) to measure information, and summations are replaced by the the
analogous integrals.
3.1 Capacity of General Mean-Limited Channels
In this section, we characterize the capacity of a general mean-limited channel Chμ (P), defined with respect to a probability transition rule P(y|x) and output mean constraint μ, as the
optimum solution of a convex program. This particularly provides the technical tool for analyzing
the capacity of convolution channels and, subsequently, general repeat channels.
Recall that the capacity of Chμ (P) is the supremum mutual information I (X;Y ) between the input and output of the channel, where the supremum is taken with respect to all input distributions
X whose corresponding output distribution Y satisfies the given mean constraint E[Y] = μ. Although our methods are general and apply to any (continuous or discrete) transition rule P(y|x),
in this section we focus on discrete distributions. In particular, we assume that the input alphabet
is a discrete set X ⊆ R≥0 and so is the output alphabet Y ⊆ R≥0 (for the purpose of this work, we
may think of both X and Y as the set of non-negative integers).
For each fixed x ∈ X, let Yx denote the random variable Y conditioned on X = x (i.e., Yx is
the output of the channel when a fixed input x is given). The problem of maximizing the mutual
information
I (X;Y ) = H(Y ) − H(Y |X) =

x ∈X
Pr[X = x]DKL (Yx Y ),
Journal of the ACM, Vol. 66, No. 2, Article 9. Publication date: March 2019. 
Capacity Upper Bounds for Deletion-Type Channels 9:17
where DKL (··) denotes the KL divergence, can naturally be written as a convex minimization
program as follows:
minimize
X,Y −I (X;Y ) =

y ∈Y
Y (y) logY (y) +

x ∈X
X (x)H(Yx ), (15)
subject to X  0 (16)


X, 1 = 1 (17)
y ∈Y
y · Y (y) = μ (18)
PX = Y, (19)
where we have used the following notation: X (x) (respectively, Y (y)) in (15) and (18) denotes the
probability assigned by the distribution of X (respectively, the distribution of Y) to the outcome
x (respectively, y). Moreover, with a slight abuse of notation, we may think of X and Y as vectors
of probabilities assigned to the possible outcomes by the distributions that define the underlying
random variables, so that for example X, 1 in (17), where 1 is the all-ones vector, is a shorthand
for the summation of probabilities that define X (which should be equal to 1). Note that for each
fixed x, the entropy H(Yx ) in (15) is a constant value defined by the transition rule P. In (19), P
denotes the (infinite dimensional) transition matrix from X to Y whose entry at row y and column
x is equal to P(y|x).
Following the standard approach in convex optimization, we define slack variables Y 
(y), y ∈ Y,
for each constraint in (19), X
(x), x ∈ X, for each non-negativity constraint in (16), ν0 for (17), and
ν1 for (18). Now the Lagrangian L(X,Y;X
,Y 
, ν0, ν1) for the program (15) may be written as
L(X,Y;X
,Y 
, ν0, ν1) =

y ∈Y
Y (y) logY (y) +

x ∈X
X (x)H(Y |x)
− X
,X + ν0 (X, 1 − 1) + ν1





y ∈Y
y · Y (y) − μ



+ Y 
, PX − Y. (20)
From (20), the derivatives of L with respect to each variable X (x) and Y (y) can be written as
∂L
∂X (x) = H(Y |x) − X
(x) + ν0 + Y 
, P(x )
, (21)
∂L
∂Y (y) = 1 + logY (y) + yν1 − Y 
(y),
where P(x ) denotes the column of P indexed by x.
Observe that one can trivially construct a strictly feasible solution X  0 for (15). Therefore,
Slater’s condition holds and the duality gap for this program is zero. The dual objective function
д(X
,Y 
, ν0, ν1) := inf
X,Y
L(X,Y;X
,Y 
, ν0, ν1)
can now be written by analytically optimizing L with respect to X and Y. Setting ∂L
∂X (x ) = 0 implies
that
EYx [Y 
(Yx )] = −H(Yx ) + X
(x) − ν0, (22)
where the expectation is taken over the random variable Yx . This can be deduced from (21) by
observing that the product Y 
, P(x )
 is precisely the average of the values defined by Y 
(y), y ∈ Y,
Journal of the ACM, Vol. 66, No. 2, Article 9. Publication date: March 2019.        
9:18 M. Cheraghchi
with respect to the measure defined by the xth column of P, i.e., the distribution of Yx . In other
words,
Y 
, P(x )
 =

y ∈Y
Pr[Yx = y] · Y 
(y) = EYx [Y 
(Yx )].
Setting ∂L
∂Y (y) = 0, however, gives us
Y 
(y) = 1 + logY (y) + yν1, (23)
and, thus,
Y (y) = exp(−1 + Y 
(y) − yν1). (24)
Since L linearly depends on variables X (x) and a linear function has bounded infimum only
when the function is zero, д(X
,Y 
, ν0, ν1) is only finite when (22) holds for all x, which we will
assume in the sequel. In this case, we deduce
д(X
,Y 
, ν0, ν1) =

y ∈Y
Y (y) logY (y) − ν0 + ν1





y ∈Y
y · Y (y) − μ



− Y 
,Y
(23) =

y ∈Y
Y (y)((Y 
(y) − 1 − yν1) + yν1 − Y 
(y)) − ν0 − μν1
(24) = −

y ∈Y
exp(−1 + Y 
(y) − yν1) − ν0 − μν1. (25)
The dual program to (15) can now be written, recalling the constraints (22), as
maximize
X
,Y
,ν0,ν1
д(X
,Y 
, ν0, ν1)
subject to (∀x ∈ X) : EYx [Y 
(Yx )] = −H(Yx ) + X
(x) − ν0
X  0,
which we rewrite, using (25), as a convex minimization problem
minimize
Y
,ν0,ν1

y ∈Y
exp(−1 + Y 
(y) − yν1) + ν0 + μν1 (26)
subject to (∀x ∈ X) : EYx [Y 
(Yx )] ≥ −H(Yx ) − ν0. (27)
Now the objective value achieved by any feasible solution to the above dual formulation gives
an upper bound on the maximum attainable mutual information I (X;Y ) and thus a capacity upper
bound. Furthermore, since (15) is convex and satisfies strong duality, KKT conditions imply that
a primal feasible solution X ,Y  for (15) is optimal if and only if there is a dual feasible solution
(Y 
, ν0, ν1) such that, recalling (23),
Y 
(y) = 1 + logY (y) + yν1, (28)
Journal of the ACM, Vol. 66, No. 2, Article 9. Publication date: March 2019.           
Capacity Upper Bounds for Deletion-Type Channels 9:19
for all y ∈ X, and, moreover, (27) holds with equality for all x ∈ X such that X (x) > 0. In this
case, using (28), the dual objective function simplifies to
д(X
,Y 
, ν0, ν1) =

y ∈Y
exp(−1 + Y 
(y) − yν1) + ν0 + μν1
(28) =

y ∈Y
Y (y) + ν0 + μν1
= 1 + ν0 + μν1.
However, we can write
EYx [Y 
(Yx )] (28) =

y ∈Y
Pr[Yx = y](logY (y) + 1 + yν1)
= 1 + ν1E[Yx ] +

y ∈Y
Pr[Yx = y] logY (y)
= 1 + ν1E[Yx ] +

y ∈Y
Pr[Yx = y] log(Y (y)/ Pr[Yx = y]) − H(Yx )
= 1 + ν1μx − DKL (Yx Y ) − H(Yx ),
where we define μx := E[Yx ], so that (27) can be rewritten as
(∀x ∈ X) : DKL (Yx Y ) ≤ 1 + ν1μx + ν0.
We have thus proven the following result (written with a trivial change of the variable ν0):
Theorem 1. Consider a general mean-limited channel Chμ (P) with output alphabet Y and output
mean constraint μ, and let Y be any distribution over Y. Denote by the random variable Yx the output
of the channel given x as the input, and let ν0 and ν1 be any real parameters such that9
(∀x ∈ X) :DKL (Yx Y ) ≤ ν1E[Yx ] + ν0. (29)
Then, capacity of Chμ (P) is at most ν1μ + ν0. Furthermore, the capacity is exactly ν1μ + ν0 if and only
if there is a distribution X over the input alphabet such that the corresponding output distribution is
Y and, moreover,
(∀x ∈ supp(X)) :DKL (Yx Y ) = ν1E[Yx ] + ν0. (30)
Throughout the article, we refer to a distribution Y satisfying the conditions of Theorem 1 as
a dual-feasible distribution. Furthermore, the gap to equality in (29) for a choice of the point x is
referred to as the KL gap at x.
3.2 Extension to Multiple Uses
We now extend the notion of general mean-limited channels to multiple uses and observe that the
capacity is achieved by a product input distribution.
Consider a transition rule P and let Chm
μ (P) be them-fold concatenation of the channel defined
by P. That is, the channel takes an m-dimensional input vector X = (X1,...,Xm ) and applies the
9It is worthwhile to note that having the term ν1E[Yx ] on the right-hand side of (30), as opposed to ν1x, is quite useful for
finding natural dual feasible distributions and causes a mean-adjusting term of the form qy in the distribution of Y to be
naturally absorbed in the constant ν1. However, this would not necessarily be the case if, on the right-hand side, we had
ν1x (as would be the case if the techniques of [1] were applied). For the special case of the convolution channels, E[Yx ] is a
linear function of x and the distinction disappears. However, our best upper bounds (Theorem 4) consider non-convolution
channels as well.
Journal of the ACM, Vol. 66, No. 2, Article 9. Publication date: March 2019.            
9:20 M. Cheraghchi
transition rule P on each Xi independently, resulting in an output vector Y = (Y1,...,Ym ). In this
case, the input distribution is allowed to be arbitrary subject to a total mean constraint
E[Y1 + ··· + Ym] = μ.
Naturally, the achieved rate by an input distribution X is
I (X;Y )
E[X1 + ··· + Xm]
,
and the capacity of the m-fold channel is the supremum of the achievable rates.
It is straightforward to argue that the capacity of Chm
μ (P) is achieved by an independent input
distribution. To see this, consider any input distribution X satisfying the total mean constraint. Let
X = (X
1,...,X
m ) denote a distribution of m independent real numbers such that the marginal
distribution of X
i is identical to that of Xi for all i ∈ [m]. Clearly, this would mean that the output
distribution Y  = (Y 
1 ,...,Y 
m ) corresponding to X consists of m independent entries, where each
Y 
i has the same marginal distribution as Yi . Therefore,
E[Y 
1 + ··· + Y 
m] = E[Y1 + ··· + Ym] = μ,
and, thus, X also satisfies the total mean constraint. However, we may show that the rate achieved
by X is no less than that of X, as follows:
I (X;Y ) =
m
i=1
I (Xi ;Yi |X1,...,Xi−1)
=
m
i=1
(H(Yi |X1,...,Xi−1) − H(Yi |X1,...,Xi ))
≤
m
i=1
(H(Yi ) − H(Yi |X1,...,Xi ))
=
m
i=1
(H(Yi ) − H(Yi |Xi ))
=
m
i=1
I (Xi ;Yi ) =
m
i=1
I (X
i ;Y 
i ) = I (X
;Y 
).
Therefore, we have proved the following:
Lemma 2. Let Chm
μ (P) be anm-use general mean-limited channel. Then, there is a capacity achieving input distribution (X1,X2,...,Xm ), which is a product distribution.
4 GENERAL REPEAT CHANNELS
A natural model to generalize both deletion and Poisson-repeat channels is the following: Let D
be a distribution on non-negative integers. The D-repeat channel is defined to receive a (possibly
infinite) stream of bits and replace each bit independently with a number of copies of the bit
distributed according to D. We call such a channel a general repeat channel with respect to the
repetition rule D. The binary deletion channel and Poisson-repeat channels are repeat channels
with respect to the Bernoulli and Poisson repetition rules, respectively.
To characterize the capacity of a D-repeat channel, without loss of generality, one may assume
that the first-ever bit given to the channel is not deleted by the channel (i.e., it will be replicated at
least once). The effect of this assumption on the capacity is amortized down to zero as the number
of channel uses tends to infinity, and thus we shall make this assumption in the sequel. Similarly,
Journal of the ACM, Vol. 66, No. 2, Article 9. Publication date: March 2019.      
Capacity Upper Bounds for Deletion-Type Channels 9:21
Fig. 1. A diagram of the pre-processor and the run-processor. The white blocks are runs of zeros and the
gray blocks are runs of 1s, and the hatched portions represent the bits that are deleted by the channel. The
random variables G, G
, and G, respectively, denote the random choices of G in the first three iterations
of the pre-processor. In this example, a total of G − 1 bits of X2 and X4 are deleted, as well as G − 1 bits of
X5 and G − 1 bits of X6,X8,.... Consequently, Z1 consists of the concatenation of the runs corresponding
to X1 and X3, the block Z2 represents the remaining portion of X4, and so forth. Finally, each Yi is obtained
from the corresponding Zi according to the repetition rule D.
we may assume that the first input bit ever given to the channel is a 0, as this assumption will also
have no effect on the capacity of the channel.
Consider any input distribution on n-bit sequences X = B1, B2,..., Bn, where we think of n as
growing to infinity. If we know the first bit of X, then we may equivalently think of it as its runlength encoding, which we denote by a sequence of positive integers X1,X2,...,Xt , where we also
think of t (where 1 ≤ t ≤ n) as growing to infinity. Let the “deletion probability” d(D) denote the
probability assigned to the zero outcome by D, and p(D) := 1 − d(D).
We now aim to model the behavior of the D-repeat in a way that can be analyzed more conveniently. Toward this goal, consider the following pre-processor procedure on a bit sequence X:
(1) Let p := p(D). Given the input sequence X, draw a geometrically distributed random variable G ≥ 1 with mean 1/p.
(2) Let the bit sequence X = B
1, B
2,..., B
n be the sequence of “even runs” in X, i.e., the
bits of X corresponding to the runs X2,X4,X6,.... Let i be so that Bi , the ith bit of X,
corresponds to the Gth bit in X (if G > n
, output n − n and terminate). Suppose that the
bit Bi corresponds to the run X2j in X.
(3) Output the integer Z := X1 + X3 + ··· + X2j−1. Repeat the procedure with X = Bi,
Bi+1,..., Bn.
Note that since each iteration of the above procedure eliminates at least one of the Xi , the number
of integers M output by the procedure (which are all positive) must necessarily satisfy 1 ≤ M ≤ n.
Denote by D the distribution D conditioned on the outcome being nonzero. We now define
an auxiliary, run-processor channel as follows: The channel receives, at input, a sequence of positive integers Z1,...,ZM . For each i = 1,..., M, the channel independently computes Yi as will
be described next and then outputs the sequence Y1,...,YM . To compute Yi , the channel outputs
a sample from D⊕D⊕(Zi−1)
, where ⊕ denotes convolution of distributions (i.e., the distribution
of independent random samples from each component added together). A schematic diagram of
the above pre-processor and run-processor procedures appears in Figure 1. We now show that the
combination of the pre-processor and the run-processor is statistically equivalent to action of the
D-repeat channel.
Lemma 3. Let X = B1,..., Bn be a bit-sequence and Z = Z1,...,ZM be the output of the preprocessor on X with respect to a distribution D. Let Y = Y1,...,YM be the output of the run-processor
Journal of the ACM, Vol. 66, No. 2, Article 9. Publication date: March 2019.
9:22 M. Cheraghchi
channel on Z (with respect to D). Then, the distribution of Y is identical to the run-length encoding
of the output of the D-repeat channel on X.
Proof. Recall the convention that the first bit of the input sequence X is assumed to be zero
and that this bit is not deleted by the channel. Given X, the D-repeat channel replaces each Bi
independently with Di ≥ 0 copies sampled from D (except for D1, which is sampled from the
conditional distribution D). Since the decision is made independently for each Bi , the Di can be
sampled in any order without a change in the resulting output distribution. Suppose, therefore,
that the channel first decides on the even runs corresponding to X2,X4,.... Namely, the channel
can be thought of as performing the following procedure to produce the output distribution:
(1) For each bit in the input sequence represented by the even runs X2,X4,..., decide, in
order, whether the bit is deleted (i.e., replaced by zero copies), until the first non-deletion
is found. Suppose the first non-deletion occurs at some Bi located in block 2j.
(2) Consider now the odd runs X1,...,X2j−1, up to the non-deletion position. Replace B1
(which ought not be deleted) by one or more copies as determined by a fresh sample from
D. Furthermore, independently replace each remaining bit in the odd runs with a number
of copies determined by a fresh sample from D.
(3) Restart the process on the remaining bits Bi, Bi+1,..., until the input sequence is
exhausted.
Note that, by the end of each execution of the above procedure, we have revealed that Bi is a nondeletion position. In the subsequent round, Bi will be the first bit in the remaining input sequence,
and this is consistent with the assumption that the first input bit in each round is replaced by
a number of copies sampled from D (i.e., D conditioned on the nonzero outcomes), as opposed
to D.
We see that the number of zeros output by the first iteration of the above process determines
the distribution of the first run-length of the output of the D-repeat channel given X, which is
Y1. Moreover, conditioned on the outcome of the first iteration, the second iteration determines
the distribution of the second run of the output of the D-repeat channel, i.e., Y2, given X and Y1.
Inductively, we see that the th iteration of the above procedure, conditioned on the outcome of the
first  − 1 iterations, produces a number of bits distributed according to the conditional distribution
Y given X,Y1,...,Y−1. Therefore, the entire procedure samples Y1,...,YM according to the exact
run-length encoding distribution of the output of the D-repeat channel, given the input X.
We now observe that the first step of the above procedure corresponds to the first step of the preprocessor; i.e., the geometric random variable G determines the position of the first non-deletion
Bi within the even runs X2,X4,... (recall that 1 − p(D) is the deletion probability of a bit). Furthermore, observe that the combined length of the odd runs X1 + X3 + ··· + X2j−1 is exactly the
random variable Z output by each iteration of the pre-processor.
Now the run-processor takes each Z produced by the th execution of the pre-processor and
replaces it with Y, which is an independent sample from D⊕D⊕Z−1. Observe that a sample from
the above convolution corresponds to the number of bits generated by taking a run of bits of length
Z, replacing the first bit with a non-zero number of copies sampled from D and replacing each
remaining bit by a number of copies independently sampled from D. This is precisely what the
above procedure does with the combined bits from the odd runs X1,X3,...,X2j−1 in each round.
That is, the integer Y output by the run-processor (conditioned on Y1,Y2,...,Y−1) has the same
distribution as the number of bits output by the above procedure in round  (conditioned on the
transcript of the execution the procedure for the first  − 1 rounds).
Journal of the ACM, Vol. 66, No. 2, Article 9. Publication date: March 2019.      
Capacity Upper Bounds for Deletion-Type Channels 9:23
We conclude that the pre-processor followed by the run-processor generate an integer sequence
that has the same distribution as the run-length encoding of the above procedure, which in turn
has the same distribution as the run-length encoding of the output of the D-repeat channel given
input X.
In light of Lemma 3, characterizing the capacity of a D-repeat channel (that we denote by
Cap(D)) is equivalent to characterizing capacity of the cascade of the pre-processor and the runprocessor. Let this cascade channel be denoted by the Markov chain X − Z − Y. We are interested
in
Cap(D) = lim
n→∞ sup
I (X;Y )
n ,
where the supremum is taken over all input distributions X1 ...Xn. Unlike channels with no synchronization errors (e.g., binary symmetric or erasure channels), it is not trivial to show that the
above limit exists and is equal to the capacity. However, the identity follows from [18]. Note that
I (X;Y ) = H(Y ) − H(Y |X) = H(Y ) − (H(Y |X,Z) + I (Y;Z |X))
≤ H(Y ) − H(Y |Z) = I (Y;Z), (31)
so that
Cap(D) ≤ lim
n→∞ sup
I (Y;Z)
n , (32)
where the supremum is still over the distribution of X. A major technical tool that we introduce
is the following theorem, which reduces the task of upper bounding the capacity of a D-repeat
channel to a capacity upper bound problem for a related mean-limited channel. A proof of this
result appears in Section 4.1.
Theorem 4. Consider a D-repeat channel Ch and let Chμ (P) be the general mean-limited channel
with respect to the transition rule P over positive integer inputs that, given an integer 1 + x, outputs
a sample from the convolution D⊕D⊕x , where D is the distribution D conditioned on the outcome
being nonzero. Let λ := E[D], λ := E[D], and 1 − p be the probability assigned to the outcome zero
by D. Then,
Cap(Ch) ≤ sup
μ ≥λ
Cap(Chμ (P))
1/p + (μ − λ)/λ
. (33)
A slightly simpler result to apply is the following corollary of Theorem 4:
Corollary 5. Consider a D-repeat channel Ch and let Chμ (D) be a convolution channel corresponding to D and restricted to non-negative integer inputs. Let 1 − p be the probability assigned to
the outcome zero by D, and λ := E[D]. Then, the capacity of the D-repeat channel can be upper
bounded as
Cap(Ch) ≤ sup
μ ≥0
Cap(Chμ (D))
1/p + μ/λ . (34)
Proof. Let Chμ (P) be the channel defined in the statement of Theorem 4. The mean-limited
channel corresponding to the transition rule P receives a positive integer x and outputs a summation Y = Y0 + Y1 + ··· + Yx−1 of independent random variables where Y0 is sampled from D (i.e.,
D conditioned on the outcome being nonzero) and the rest are sampled from D. Let Ch
μ (P) be a
modification of Chμ (P) with side information, in which the receiver also receives the exact value
of Y0. This side information can only increase the capacity of the channel for the corresponding
parameter μ. Let λ := E[D]. Since the input to Chμ (P) is a positive integer, the modified channel
Ch
μ (P) is equivalent to, and has the same capacity as, the convolution channel Ch
μ−λ (D) (with
Journal of the ACM, Vol. 66, No. 2, Article 9. Publication date: March 2019. 
9:24 M. Cheraghchi
the input restricted to non-negative integers). This is due to the fact that the receiver may simply
subtract the given value of Y0, which is independent of the input and thus bears no information
about the input, from Y and thereby simulate a convolution channel with the matching mean
constraint, which is Ch
μ−λ (D). This means that Cap(Chμ (P)) ≤ Cap(Ch
μ−λ (D)).
Let μ > μ0 be a value of μ that attains the supremum in (33). We now have that
Cap(Chμ (P))
1/p + (μ − λ)/λ
≤
Cap(Ch
μ−λ (D))
1/p + (μ − λ)/λ
≤ sup
μ ≥0
Cap(Chμ (D))
1/p + μ/λ ,
proving the claim.
Remark 6. Compared with Theorem 4, Corollary 5 is in general more convenient to work with.
This is due to the fact that the normalizing constant in the probability mass function of the conditional distribution D in Theorem 4 incurs an additive factor in the entropy expression for D that,
in general, may be of little effect but nevertheless cause significant technical difficulties. However, this convenience comes at cost of potentially obtaining worse capacity upper bounds than
what Theorem 4 would give. For the case of the deletion channel, D is a Bernoulli distribution
and D becomes a trivial, singleton, distribution. Therefore, in this case, Corollary 5 can obtain
the same result as Theorem 4. However, for channels for which D contains substantial entropy;
e.g., the Poisson-repeat channel where D has a large mean, the loss incurred by applying Corollary 5 rather than Theorem 4 may be noticeable and even potentially trivialize the resulting upper
bounds.
4.1 Proof of Theorem 4
To prove Theorem 4, we first recall (32), i.e.,
Cap(D) ≤ lim
n→∞ sup
I (Y;Z)
n ,
where the supremum is over the distribution of the n-bit input sequenceX (andY = Y1,...,YM and
Z = Z1,...,ZM being the corresponding distributions of the outputs of the pre-processor and runprocessor, respectively). Assume that the capacity is not zero (otherwise, the claimed upper bound
would be trivial). To avoid introducing excessive notation for the various error terms involved,
in the sequel we use asymptotic notation as n grows to infinity (with hidden constants possibly
depending on D); so that a o(1) term can be made arbitrarily small as n grows; an ω(1) term grows
with n, and so forth. Consider a large-enough n (that we will tend to infinity in the end) and a
choice for X that approaches the corresponding supremum, so that, for the Y and Z defined by X,
we have
Cap(D) ≤
I (Y;Z)
n + o(1).
The minimum possible n would depend on the desired magnitude of the added o(1) term. We note
that the length M of Y and Z is itself a random variable jointly correlated with X, Y, and Z. This
causes technical difficulties that we first handle by showing below that we may essentially assume
that the length M is large but fixed and known. To do so rigorously, first recall that, denoting
Y M
1 := Y1,...,YM and Z M
1 := Z1,...,ZM , we may write (since the knowledge of either Y or Z
Journal of the ACM, Vol. 66, No. 2, Article 9. Publication date: March 2019.      
Capacity Upper Bounds for Deletion-Type Channels 9:25
uniquely reveals M as well)
I (Y;Z) = I
	
M,Y M
1 ; M,Z M
1


= H
	
M,Y M
1


− H
	
M,Y M
1 |M,Z M
1


= H(M) + H
	
Y M
1 |M


− H
	
Y M
1 |M,Z M
1


= H(M) + I
	
Y M
1 ;Z M
1 |M


≤ logn + I
	
Y M
1 ;Z M
1 |M


,
where for the last inequality we are using the fact that M is always an integer between 1 and n.
Therefore, conditioning on M has no asymptotic effect on the capacity upper bound, and we may
write
Cap(D) ≤
I (Y;Z |M)
n + o(1).
Without loss of generality, in the sequel we assume that X is entirely supported on n-bit sequences that consist of Ω(n/ logn) runs (much lower estimates would also suffice). The contribution of all other sequences to the entropy of X would be o(n), which would have no asymptotic
effect on the achieved rate. For any such input sequence (and, consequently, for the distribution
defined by X), it is straightforward to show (e.g., using Azuma-Hoeffding inequality) that with
overwhelming probability 1 − 1/nω(1)
, the resulting choice of M will also be large; particularly,
that M ≥ m0 for some m0 = Ω(n/ logn). Let us now write
Cap(D) ≤

m ≥1
Pr[M = m]
I (Y;Z |M = m)
n + o(1)
≤

m ≥m0
Pr[M = m]
I (Y;Z |M = m)
n + o(1), (35)
where in the second inequality, we have used the fact that M ≥ m0 with probability 1 − o(1), and
have used the trivial upper bound of 1 for I (Y;Z |M = m)/n when m < m0 (recall that Z is always
the run-length encoding of a bit-string of length at most n, and thus its entropy is at most n).
Consider an alternative, but equivalent, realization of the pre-processor that, given the input
X, first draws an infinite sequence of i.i.d., geometrically distributed random variables G1,G2,...
(each with mean 1/p), and sets G = G in the first step of the th iteration (thus the variables
GM+1,GM+2,... are never looked at).
Note that the total bit-length of X consists of the summation of the produced values of Zi by the
pre-processor plus the corresponding Gi (which represent the deleted bits by the pre-processor,
i.e., hatched part in Figure 1), except for the final GM , which may extend beyond the length of X.
More formally, it is always the case that

M
i=1
(Zi + Gi ) − GM ≤ n ≤

M
i=1
(Zi + Gi ),
or, in other words,
n =

M
i=1
(Zi + Gi ) − Δ, (36)
where 0 ≤ Δ ≤ GM .
We recall that, for all m ≥ 1, we simultaneously have E[
m
i=1 Gi] = m/p. Furthermore, by a
Chernoff-Hoeffding inequality, the summation highly concentrates around its expectation due to
Journal of the ACM, Vol. 66, No. 2, Article 9. Publication date: March 2019.      
9:26 M. Cheraghchi
the Gi being independent; namely, we may observe that with probability 1 − 1/nω(1)
, it is the case
that for all m ≥ m0 we have
m
i=1
Gi = m(1/p + o(1)).
Furthermore, the value ofGM /m0 is o(1) with probability 1 − o(1) by Markov’s inequality. Overall,
combined with (36), it follows that with probability 1 − o(1), we have

M
i=1
(Zi + 1/p ± o(1)) = n. (37)
Note also that the left-hand side of (37) is O(n) (treating p as a constant) with probability 1.
For an integer m, denote by Zi,m the random variable Zi conditioned on the event M = m. Given
the input X, if we condition the output of the pre-processor on the event M = m, then the joint
distribution ofG1,G2,... obviously changes to possibly even a non-product distribution. However,
we may still apply an averaging argument on (37) to show that10, for some set S ⊆ N \ [m0] such
that Pr[M ∈ S] = 1 − o(1), the following holds: For all m ∈ S, we have
m
i=1
(Zi,m + 1/p ± o(1)) = n,
with probability 1 − o(1) over the distribution of Zm
1,m := (Z1,m,Z2,m,...,Zm,m ). This, in turn, implies that, for all m ∈ S, m
i=1
(E[Zi,m] + 1/p ± o(1)) = n(1 + o(1)). (38)
Similarly toZm
1,m, defineYm
1,m := (Y1,m,...,Ym,m ), whereYi,m is the random variableYi conditioned
on the event M = m. In other words, Ym
1,m is the output of the run-processor when given Zm
1,m at
input. We may now rewrite (35) as
Cap(D) ≤

m∈S
Pr[M = m]
I (Y;Z |M = m)
n + o(1)
(38)
≤

m∈S
Pr[M = m]
I (Ym
1,m;Zm
1,m )(1 + o(1))
m
i=1 (E[Zi,m] + 1/p ± o(1)) + o(1). (39)
Consider any fixed m. Note that the effect of the run-processor on Zm
1,m is precisely the same as an
m-use mean-limited channel, as defined in Section 3.2 (albeit without the mean constraint), where
the transition rule P is given by the conditional distribution Ym
i,m |Zm
i,m. That is, given an integer
input 1 + x, the transition rule P outputs a sample from D⊕D⊕x . Therefore, as in the proof of
Lemma 2, since each Yi,m only depends on the corresponding random variable Zi,m, we may write
I
	
Ym
1,m;Zm
1,m


≤
m
i=1
I (Yi,m;Zi,m ). (40)
Furthermore, recall λ := E[D] and λ := E[D], and observe that, for all i,
E[Ym,i] = λ + (E[Zm,i] − 1)λ. (41)
10Some care is needed for the averaging argument. Particularly, we may take advantage of the fact that, with high probability, the concentration bound m
i=1 Gi = m(1/p + o(1)) holds simultaneously for all m ≥ m0. Therefore, it just suffices
to construct S so that, for all m ∈ S, the random variable Gm conditioned on the event M = m is upper bounded by o(m),
which in turn follows by a simple averaging using Markov’s inequality.
Journal of the ACM, Vol. 66, No. 2, Article 9. Publication date: March 2019.       
Capacity Upper Bounds for Deletion-Type Channels 9:27
Using (40) and (41), we may now rewrite (39) as
Cap(D) ≤

m∈S
Pr[M = m]
m
i=1 I (Yi,m;Zi,m )(1 + o(1))
m
i=1 (1/p + (E[Yi] − λ)/λ ± o(1))
+ o(1)
≤

m∈S
Pr[M = m] max
i ∈[m]
I (Yi,m;Zi,m )(1 + o(1))
1/p + (E[Yi] − λ)/λ ± o(1)
+ o(1) (42)
where the second inequality is due to the following simple result:
Proposition 7. For positive real numbers a1,..., am and b1,...,bm, we have
a1 + ··· + am
b1 + ··· + bm
≤ max i=1, ...,m
ai
bi
.
Proof. Without loss of generality, suppose the right-hand side is a1/b1. Then, the inequality is
equivalent to
m
i=1
b1ai ≤
m
i=1
a1bi,
which is true, since, for each i, we have assumed aib1 ≤ bia1.
Now observe that for any i and m, we have
I (Yi,m;Zi,m )
1/p + (E[Yi] − λ)/λ ± o(1)
≤ sup
μ ≥λ
Cap(Chμ (P))
1/p + (μ − λ)/λ ± o(1)
,
since, assuming that μ = E[Yi,m], the random variable Yi,m is sampled by transmitting Zi,m over a
mean-limited channel with transition rule P and mean constraint μ. Therefore, the mutual information I (Yi,m;Zi,m ) would be no more than the capacity of this channel. Using this, (42) further
simplifies to
Cap(D) ≤

m∈S
Pr[M = m] max
i ∈[m]
sup
μ ≥λ
Cap(Chμ (P))(1 + o(1))
1/p + (μ − λ)/λ ± o(1)
+ o(1)
≤ sup
μ ≥λ
Cap(Chμ (P))(1 + o(1))
1/p + (μ − λ)/λ ± o(1)
+ o(1)
= sup
μ ≥λ
Cap(Chμ (P))
1/p + (μ − λ)/λ
,
where the last equality is attained from the fact that, by taking the limit n → ∞, the o(1) terms
vanish. This completes the proof of Theorem 4.
5 UPPER BOUNDS ON THE CAPACITY OF THE POISSON-REPEAT CHANNEL
5.1 Upper Bounds on the Capacity of a Mean-Limited Poisson Channel
Let D be a Poisson distribution with mean λ, and Ch := Chμ (D) be the convolution channel defined with respect to the distribution11D and mean constraint μ. Let P be the probability transition
rule corresponding to Chμ (D) when seen as a general mean-limited channel. Recall that the input
and output alphabets for this channel are both the set of non-negative integers. By Theorem 1, to
upper bound the capacity of Ch, it suffices to exhibit a distribution over non-negative integers and
11We note that, in the context of optical communications, this channel was also considered in [3]. A related channel is the
standard, additive, discrete-time Poisson channel [41] that has been extensively studied in information theory.
Journal of the ACM, Vol. 66, No. 2, Article 9. Publication date: March 2019.      
9:28 M. Cheraghchi
real parameters ν0 and ν1, so that the corresponding random variable Y ∈ N≥0 drawn from this
distribution satisfies (29).
Let Yx be the output of the channel when the input is fixed to x. Explicitly, Yx has a Poisson
distribution with mean E[Yx ] = λx, so that (29) can be rewritten as
DKL (Yx Y ) ≤ λν1x + ν0, x = 0, 1,.... (43)
By the conclusion of Theorem 1, exhibiting any such distributionY and parameters ν0 and ν1 would
imply
Cap(Chμ (D)) ≤ ν1μ + ν0. (44)
We consider the following general form for the distribution of Y:
Pr[Y = y] = y0 exp(f (y))(q/e)
y, y = 0, 1,..., (45)
for some function f : N≥0 → R, real parameter q > 0, and normalizing constant
y0 =



∞
y=0
exp(f (y))(q/e)
y 


−1
,
assuming that the summation is convergent. Given any function f that grows linearly in y or
slower, it is always possible to choose q small enough so that the distribution is well defined.
Moreover, by varying the choice of q, it is possible to set the expectation of Y to match the chosen
parameter12 μ. It turns out to be more convenient to set f (y) := д(y) − logy!, for some function
д : N≥0 → R, and our goal would be to obtain an appropriate choice for д.
Recall that, for any choice of positive integers x and y,
Pr[Yx = y] = e−λx (λx)
y
y! .
The KL divergence DKL (Yx Y ) can now be written as
DKL (Yx Y ) =
∞
y=0
Pr[Yx = y] log Pr[Yx = y]
Pr[Y = y]
(45) = − logy0 − λx (logq) + λx log(λx) − E[д(Yx )]. (46)
Note that the only nonlinear term (in x) in the above is λx log(λx) − E[д(Yx )], so achieving (43) is
equivalent to having real coefficients a,b such that
λx log(λx) − E[д(Yx )] ≤ ax + b, x = 0, 1,.... (47)
At this point, the following feasible choice д(y) = д0 (y) is immediate:
д0 (y) =

0 if y = 0,
y logy if y > 0,
which results in the convexity-based distribution introduced in (4) and recalled below:
Pr[Y = y] = y0
yy
y!
(q/e)
y, (48)
where 00 is to be understood as 1. Numerical estimates on the mean and normalizing constants of this distribution for various choices of q are listed in Table 1. To see that this choice
12By varying q, the mean μ may be adjusted to any arbitrary positive value so long as, for some fixed q0 > 0, the summation
defining y0 diverges to infinity with q = q0 but, on the other hand, converges for all q < q0.
Journal of the ACM, Vol. 66, No. 2, Article 9. Publication date: March 2019.    
Capacity Upper Bounds for Deletion-Type Channels 9:29
satisfies (47), it suffices to note that the functionд(y) defined above is convex, and, thus, by Jensen’s
inequality,
E[д0 (Yx )] ≥ д0 (E[Yx ]) = д0 (λx) = λx log(λx),
so (47) is satisfied for a = b = 0. One can, however, observe that the inequality is strict by a constant
gap as x grows (as we show in Section 5.2).
Using Stirling’s approximation
y! ∼ 
2πy(y/e)
y,
we may write the asymptotic behavior of (48) as
Pr[Y = y] ∼ y0
yy (q/e)
y
√2πy(y/e)y = y0
√2πy
qy, (49)
so we see that (48) can be normalized to a valid distribution if and only if q < 1.
We now present a different choice for д that more closely estimates the linear upper bound
ax + b and, in particular, converges to it as x grows. This alternative choice results in a better
capacity upper bound than the immediate choice above. It is obtained by replacing logy in д0 (y)
with harmonic numbers that asymptotically behave like log y but provide a more refined result.
Explicitly, consider
д(y) =

0 if y = 0,
yψ (y) = y(Hy−1 −γ ) if y > 0, (50)
where ψ (y) = Γ
(y)/Γ(y) is the digamma function, Hn = ψ (n + 1) +γ denotes the nth harmonic
number (where H0 = 0 and, for a positive integer n, Hn = n
k=1 1/k), and γ ≈ 0.57721 is the EulerMascheroni constant. It is known that [2, p. 259]
ψ (y) = logy − 1
2y
+ O
 1
y2

,
so we have
lim
y→∞(yψ (y) − y logy) = −1/2,
and, thus, combined with the Stirling approximation, we see that with this alternative choice of д,
we have a slightly different asymptotic behavior than (49), namely,
Pr[Y = y] ∼ y0
yy (q/e)
y
√2πey(y/e)y = y0
√2πey
qy, (51)
To verify that this choice of д is feasible, we first prove a series expansion for the function д.
Lemma 8. For any y ≥ 0, the function д in (50) can be represented as
д(y) = −γy +
∞
j=2
(−1)j
j
j − 1

y
j

.
Proof. The proof is similar to the derivation for the well-known Newton series expansion of
the digamma function
ψ (y + 1) = −γ −
∞
j=1
(−1)j
j

y
j

. (52)
The function д exhibits a discontinuity at y = 0, where the limit is −1. Let us correct this discontinuity by defining a function д1, which is the same as д except at point y = 0, where we define
д1 (0) = −1. The function д1 is continuous and well defined at all y ≥ 0, which we now express as
Journal of the ACM, Vol. 66, No. 2, Article 9. Publication date: March 2019.     
9:30 M. Cheraghchi
a Newton series expansion. Recall that the Newton series expansion of (any function) д1 around
zero can be written as
д1 (y) =
∞
j=0
cj

y
j

, (53)
where the coefficientcj is defined to be the jth forward difference of the function at zero, namely,
cj =

j
k=0
(−1)
j−k

j
k

д1 (k). (54)
For our particular choice of д1, the forward difference at point y is, understanding yHy−1 at y = 0
by its limit −1,
Δ[д1](y) := д1 (y + 1) − д1 (y) = −γ + (y + 1)Hy − yHy−1 = −γ + y(Hy − Hy−1) + Hy = 1 −γ + Hy .
Taking the second forward difference, we then obtain
Δ(2)
[д1](y) = Δ[д1](y + 1) − Δ[д1](y) = Hy+1 − Hy = 1
y + 1
.
Thus, for any j ≥ 2, the jth forward difference of the function д1 is the (j − 2)nd forward difference
of the function 1
y+1 , which can in turn be written as
Δ(j)
[д1](y) =

j−2
k=0
(−1)
j−k

j − 2
k
 1
y + k + 1
.
One can verify by induction on j that the right-hand side is equal to
(−1)
j
(j − 2)!

j−2
k=0
1
y + k + 1
,
which, at y = 0 and for j ≥ 2, simplifies to (−1)j
/(j − 1) and gives the value of cj . Plugging this
result into (53), we conclude that
д1 (y) = −1 + (1 −γ )y +
∞
j=2
(−1)j
j − 1

y
j

.
Since the functions д and д1 only differ at y = 0, from (54) we see that the jth Newton series
coefficients for the function д is (−1)j + cj , which, for j ≥ 2, is equal to (−1)j
j/(j − 1). The function
can now be expanded as
д(y) = −γy +
∞
j=2
(−1)j
j
j − 1

y
j

,
as desired.
As a corollary of the above lemma, we may derive the following.
Corollary 9. Let д be the function in (50), and Y be a Poisson random variable with mean λ.
Then,
E[д(Y )] = λ(E1 (λ) + log λ),
where E1 (·) is the exponential integral function (8).
Journal of the ACM, Vol. 66, No. 2, Article 9. Publication date: March 2019.           
Capacity Upper Bounds for Deletion-Type Channels 9:31
Fig. 2. The function xE1 (x).
Proof. The main ingredient to use is the simple fact that the jth factorial moment of Y is given
by
E

j!

Y
j

= λj
.
Combining this with the result of Lemma 8 immediately gives
E[д(Y )] = −γλ +
∞
j=2
(−λ)j
j
(j − 1)j!
= −γλ +
∞
j=1
(−λ)j+1
jj! . (55)
We now recall the following basic power series expansion for the exponential integral function [2,
p. 229]: For all x > 0,
E1 (x) = −γ − log x −
∞
j=1
(−x)j
jj! ,
where γ is the Euler-Mascheroni constant. Using this expansion in (55) yields
λE1 (λ) +γλ = −λ log λ + E[д(Y )] +γλ,
which completes the proof.
The result of Corollary 9 immediately implies that the choice of д in (50) satisfies (47) with
a = b = 0, as we have
λx log(λx) − E[д(Yx )] = −λxE1 (λx) ≤ 0, (56)
from the fact that the exponential integral function E1 (x) is, by its integral definition, positive for
all x > 0. The value of λxE1 (λx) exponentially decays down to zero (see Figure 2), and, therefore,
(47) sharply holds for the choice of д in (50). The resulting distribution Y can now be rewritten,
from (45), as
Pr[Y = y] =

y0 if y = 0,
y0 exp(yψ (y))(q/e)
y/y! if y > 0. (57)
We call the distribution defined by (57) the digamma distribution due to the digamma term in the
exponent. Combining (46) and (56) gives us
DKL (Yx Y ) ≤ − logy0 − λx logq = − logy0 − E[Yx ] logq, (58)
and, thus, Theorem 1 gives the upper bound,
Cap(Chμ (D)) ≤ −μ logq − logy0, (59)
Journal of the ACM, Vol. 66, No. 2, Article 9. Publication date: March 2019.     
9:32 M. Cheraghchi
Fig. 3. Plots of μ = E[Y](left) and log(1/y0) (right), for the distribution ofY in (48) (dashed) and the digamma
distribution (57) (solid), as functions of q.
where q (and, accordingly, the normalizing constanty0) must be chosen so that the mean constraint
E[Y] = μ is satisfied (note that this choice is unique for any given μ > 0). Therefore, understanding the upper bound requires a characterization of the relationship among μ, q, and y0. Since the
probability mass function defining Y exhibits an exponential decay, the values μ and y0, as a function of q, can be numerically computed efficiently to any desired accuracy. Plots of these functions,
for both distributions (48) and (57), are depicted in Figure 3. Moreover, their numerical estimates
are listed, for various choices of q ∈ (0, 1), in Table 1. We summarize the result of this section in
the following13:
Theorem 10. Let q ∈ (0, 1) be a given parameter and Y be a random variable distributed according to the digamma distribution (57) for an appropriate normalizing constant y0. Let μ := E[Y] and
D denote any Poisson distribution with positive mean. Then, capacity of the mean-limited Poisson
channel Chμ (D) satisfies
Cap(Chμ (D)) ≤ −μ logq − logy0. (60)
Figure 4 depicts the capacity upper bounds attained by the above result, as well as a similar
result when the dual-feasible distribution for Y is defined by (48).
13An appealing aspect of assigning the mean constraint to the output, rather than the input, distribution is that such results
as Theorem 10 become independent of the channel parameter λ. Therefore, when we apply this result to obtain capacity
upper bounds for the Poisson-repeat channel, the deletion probability p = 1 − e−λ appears only in the final expression to
be optimized (77).
Journal of the ACM, Vol. 66, No. 2, Article 9. Publication date: March 2019.
Capacity Upper Bounds for Deletion-Type Channels 9:33
Fig. 4. Capacity upper bounds (measured in bits) of Theorems 10 and 13 for the mean-limited Poisson channel in terms of μ = E[Y]. The solid plot is the upper bound given by (60) (when the digamma distribution
(57) is used for Y). The dashed plot is given by (67) (when (48) is used for Y). The first inequality in (67) and
its analytic upper bound estimate would completely overlap and be indistinguishable in this plot. Likewise,
the upper bound estimates of Corollary 16 for the digamma distribution (57) would result in essentially the
same plot as the exact one above.
5.2 The Truncation Effect of Replacing Logarithm with Harmonic Numbers
The aim of this section is to provide an intuitive explanation of why the choice of the digamma
distribution (57) for the distribution of Y, that essentially replaces the logarithmic term logy in the
exponent of exp(y logy) in (48) with harmonic numbers (equivalently, ψ (y)), results in improved
capacity upper bounds.
For any analytic choice of д(y) in (47), we can write down the Taylor series expansion of д
around μ as
д(y) = д(μ) + (y − μ)д
(μ) +
1
2
(y − μ)
2
д(μ) +
∞
j=3
(y − μ)
j д(j)
(μ)
j! .
Let Yx be Poisson-distributed with mean λx. Assuming that the above series converges,14 we may
take the expectation of the above and, and noting that the variance of Yx is equal to λx, and letting
μ := λx, write
E[д(Yx )] = д(λx) +
1
2
λxд(λx) +
∞
j=3
μj
д(j)
(λx)
j! ,
where μj := E[(Yx − μ)j
]. Now, with д(y) = y logy as in (48), we would have д(λx) = 1/(λx), and
for j ≥ 3, д(j)
(λx) = O(1/xj−1), so that, for large x, we have the asymptotic behavior
E[д(Yx )] = д(λx) +
1
2
+ o(1).
Therefore, while д(y) satisfies the dual feasibility conditions (47) with a = b = 0 for all x (and
with equality for x = 0), the inequality exhibits an asymptotic constant gap of 1/2 for large x
(see Figure 5 for a depiction). As we saw in Section 5.1, specifically (56), this gap is eliminated by
choosing д(y) according to (50). While this fact is verified in (56), it is worthwhile to provide a
systematic way of deriving a choice of д that exhibits no asymptotic KL gap. To do so, recall that
14Convergence issues may be disregarded for large values of y that have negligible contribution to the probability mass
function of Yx .
Journal of the ACM, Vol. 66, No. 2, Article 9. Publication date: March 2019.  
9:34 M. Cheraghchi
Fig. 5. Plot of the KL gap to equality in (47), as a function of x, attained by the convexity-based distribution
(48) (solid) and the digamma distribution (57) (dashed) for deletion probability d = 0.9 (the plots would
simply be scaled in x for other deletion parameters). By Corollary 9, the second plot coincides with the
function λxE1 (λx), where λ = − logd.
the “ultimate goal” in satisfying the KKT conditions of Theorem 1 would be to exhibit a function
д(y) for which (56) is satisfied with equality for all x ≥ 0. In general, this may be impossible to
achieve with a choice of д that does not grow faster than y logy + O(y) (so that the resulting
expression (45) can be normalized to a valid probability distribution). Nevertheless, we present a
“truncation technique” that obtains an approximate guarantee, in the sense that the KL gap in (47)
exponentially decays as x grows, while maintaining a controllable choice for д.
Assuming that (47) holds with equality and a = b = 0, we must have E[д(Yx )] = μ log μ, where
μ := λx. The right-hand side, using the integral expression
log μ =
 ∞
0
e−t − e−μt
t
dt,
and the Taylor expansion of the exponential function, can be written as
μ log μ =
 ∞
0




μe−t
t
+
∞
j=1
μj (−t)j−2
(j − 1)!



dt (61)
=
 1
0




μe−t
t
+
∞
j=1
μj (−t)j−2
(j − 1)!



dt + μ
 ∞
1
e−t − e−μt
t
dt
=
 1
0




μe−t
t
+
∞
j=1
μj (−t)j−2
(j − 1)!



dt + μE1 (1) − μE1 (μ), (62)
where E1 (·) denotes the exponential integral function (8) and E1 (1) ≈ 0.219384. Using (61), and
noting that the factorial moments of the Poisson distribution are given by E[(
Yx
j )] = μj
/j!, the
following function:
д˜(y) :=
 ∞
0




ye−t
t
+
∞
j=1
j

y
j

(−t)
j−2


dt = y
 ∞
0




e−t − 1
t
+
∞
j=1

y − 1
j

(−t)
j−1


dt, (63)
Journal of the ACM, Vol. 66, No. 2, Article 9. Publication date: March 2019.                 
Capacity Upper Bounds for Deletion-Type Channels 9:35
interpreted formally, is the unique solution to the functional equation
(∀x ≥ 0) E[д˜(y)] = μ log u. (64)
However, the above integral definition of д˜(y) does not converge (recall that  ∞
0 e−t
dt/t is divergent and that the inner summation in (63) only has a finite number of terms for any integer y > 0).
To address this issue, we write down a function whose expectation sharply approximates the desired value (62) for large μ. To do so, it suffices to note that the term μE1 (μ) in (62) is exponentially
small in μ and can thus be ignored in the approximation. Now consider a truncated variation of д˜
defined, by simply truncating the upper limit of the integration at t = 1, as
дˆ(y) := y
 1
0




e−t − 1
t
+
∞
j=1

y − 1
j

(−t)
j−1


dt
= y(−γ − E1 (1)) + y
 1
0
∞
j=1

y − 1
j

(−t)
j−1
dt
= y(−γ − E1 (1)) − y
∞
j=1

y − 1
j

(−1)j
j
= −yE1 (1) + yψ (y),
where, in the above, γ is the Euler-Mascheroni constant and ψ (y) is the digamma function (with
yψ (y) to be understood as zero for y = 0), and we have used the Newton series expansion of the
digamma function (52), recalled below:
ψ (y + 1) = −γ −
∞
j=1
(−1)j
j

y
j

.
From (62), we see that
E[дˆ(Yx )] = μ log μ − μE1 (1) + μE1 (μ), (65)
so that the function дˆ(y) + yE1 (1) provides the desired approximation. This is precisely the function д that we defined in (50).
Remark 11. We could have truncated the integral upper limit to any constant c ∈ [0, 1] and yet
obtain an exponentially sharp approximation of μ log μ for large μ (choosing c > 1, though, would
result in an exponential growth of дˆ(y) in y and, subsequently, an expression for the probability mass function of Y that cannot be normalized to a valid distribution). Among these choices,
truncation at c = 1 provides the closest possible approximation.
Remark 12. The observation that the expression for д˜(y), the solution to the functional equation
(64), does not converge shows that the KKT equality conditions (30) of Theorem 1 cannot be simultaneously satisfied for all x ≥ 0. In other words, for any mean-limited Poisson channel, there is no
input distribution X with full support on non-negative integers that achieves the capacity of the
channel. However, the optimal X must have infinite support (since, otherwise, for large-enough x,
the KL divergence on the left-hand side of (29) becomes infinite, and the KKT conditions would be
violated). The claim that the optimal X cannot have full support makes intuitive sense. Intuitively,
if the input distribution has nonzero support on some x > 0, then the corresponding channel output is expected to be λx with a variance of λx. Therefore, any input x  for which λ|x − x 
| is too
close to the standard deviation √
λx would cause confusion at the decoder and should be avoided.
Journal of the ACM, Vol. 66, No. 2, Article 9. Publication date: March 2019.          
9:36 M. Cheraghchi
Roughly, this means that if x is on the support of the transmitter’s input distribution X, then the
next symbol in the codebook should be picked at x + Ω(√
x/λ).
5.3 Analytic Estimates
It is desirable to provide sharp upper and lower bounds on the mean and normalizing constants of
the convexity-based distribution (48) and the digamma distribution (57) in terms of elementary or
standard special functions, and that is what we achieve in this section.
5.3.1 Estimates by Standard Special Functions. First, we obtain sharp estimates on the parameters of the convexity-based distribution (48) in terms of standard special functions. Since the refined
digamma distribution (57) achieves better capacity upper bounds than (48), and, as we see in the
next section, we are able to estimate the parameters of the former sharply in terms of elementary
functions, the result of this section should be regarded as a side result. However, the techniques
presented here will be used for the more complex problem of approximating the inverse binomial
distribution, for the deletion channel problem, in terms of standard special functions. It is thus
natural to demonstrate the approximation techniques for the mean-limited Poisson channel first
before approaching the slightly more complex case of the binomial channel.
We recall the standard special function Lerch transcendent (11)
Φ(z,s, α) :=
∞
k=0
zk
(k + α)s .
The approximation of the probability mass function for the convexity-based distribution (48) in
terms of the above function is given by the following theorem, which we prove in Appendix A:
Theorem 13. Let q ∈ (0, 1) be a given parameter and Y be a random variable distributed according to the convexity-based distribution ((48)) for an appropriate normalizing constant y0. Let μ :=
E[Y], and consider constants σ := 1/6 and σ := 0.177 ≈ 16/90. Define special functions S0 (q, σ ) :=
Φ(q, 1/2, 1 + σ ) and S1 (q, σ ) := q
√
2π Φ(q, −1/2, 1 + σ ) − σS0 (q, σ ). Then,
(1) We have the bounds
y0 ≥ 1/(1 + S0 (q, σ )) =: y0, y0 ≤ 1/(1 + S0 (q, σ )) =: y0,
μ ≥ S1 (q, σ )/(1 + S0 (q, σ )) =: μ, μ ≤ S1 (q, σ )/(1 + S0 (q, σ )) =: μ. (66)
(2) Let D denote any Poisson distribution with positive mean. Then, capacity of the mean-limited
Poisson channel Chμ (D) satisfies
Cap(Chμ (D)) ≤ −μ logq − logy0 ≤ −μ logq − logy0. (67)
As demonstrated in Figure 6, the expressions in (66) provide remarkably sharp upper and lower
bound estimates on the normalizing constant y0 and the expectation μ, which are accurate within
a multiplicative factor of about 1 ± 0.004 for all q ∈ (0, 1).
5.3.2 Estimate by the Negative Binomial Distribution. In Section 5.3.1, we obtained sharp estimates on the mean and the normalizing constant of the convexity-based distribution Y (48) in
terms of standard special functions. In this section, we provide similar estimates, albeit not as
sharp, for the distribution (48) in terms of elementary functions.
Recall the probability mass function of the negative binomial distribution (1)
NegBinr,q (y) =

y + r − 1
y

(1 − q)
r
qy, y = 0, 1,....
Journal of the ACM, Vol. 66, No. 2, Article 9. Publication date: March 2019.  
Capacity Upper Bounds for Deletion-Type Channels 9:37
Fig. 6. Quality of the approximations in (66) as a function of q. Left: μ/μ (solid) and μ/μ (dashed). Right:
(logy0)/(logy0) (solid) and (logy0)/(logy0) (dashed).
The asymptotic behavior of the function (1) at large y can be understood by the Stirling approximation Γ(1 + y) ∼ √2πy(y/e)
y ; namely,
NegBinr,q (y) = Γ(y + r)(1 − q)
rqy
Γ(y + 1)Γ(r)
∼ (1 − q)
rqy
Γ(r)
y + r
y + 1
e1−r (y + r − 1)
y+r−1
yy
∼ (1 − q)
r
Γ(r) e1−r

1 + r − 1
y
y
qyyr−1
∼ (1 − q)
r
Γ(r) qyyr−1
. (68)
Throughout this section, we focus on the special case r = 1/2, so (68) becomes
NegBinr,q (y) ∼
1 − q
π
qy
√y . (69)
We prove the following key estimate on the binomial coefficient (
y−1/2
y ) that is used to provide
accurate estimates on the parameters of the inverse binomial distribution:
Lemma 14. Let γ := 2/e1+γ ≈ 0.413099 and γ := 1/
√
2e ≈ 0.428882, where γ ≈ 0.57721 is the
Euler-Mascheroni constant. Then, for all y ≥ 1,
y − 1/2
y

γ ≤ exp(yψ (y) − y)
y! ≤

y − 1/2
y

γ .
Proof. Consider the ratio
д(y) =
	
y−1/2
y


exp(yψ (y))/(y!ey ) = Γ(y + 1/2)
Γ(1/2) exp(yψ (y) − y) = 1
√
π Γ(y + 1/2) exp(y − yψ (y)). (70)
We use the following claim (see Appendix B.1):
Claim 15. The function д(y) defined in (70) is a decreasing function of y > 0.
The above claim implies that, for all y ≥ 1, we must have
д(1) ≥ д(y) ≥ lim
y→∞ д(y),
Journal of the ACM, Vol. 66, No. 2, Article 9. Publication date: March 2019.   
9:38 M. Cheraghchi
Fig. 7. Quality of the approximations by Corollary 16 as a function of q. Left: μ/μ (solid) and μ/μ (dashed).
Right: (logy0)/(logy0) (solid) and (logy0)/(logy0) (dashed). The notations μ and μ (y0 and y0), respectively,
refer to the upper and lower bound estimates on μ (y0) by Corollary 16.
assuming that the limit exists (that we will show next). We have
д(0) = 1
√
π Γ(3/2) exp(1 −ψ (1)) = e1+γ
2 ≈ 2.420728.
Recall, from the Stirling estimate (51), that
exp(yψ (y))
y!ey ∼ 1
√2πey ,
and, similarly, from (69), that

y − 1/2
y

∼ 1
√πy ,
and, therefore,
lim
y→∞ д(y) = √
2e ≈ 2.331643.
The result follows.
Using the above approximations, we are able to prove the following lower and upper bound
estimates on the parameters of the digamma distribution (57) in terms of elementary functions
(see Figure 7 for a depiction of the quality of the approximations).
Corollary 16. Let γ and γ be as in Lemma 14, and Y be distributed according to the digamma
distribution (57). Then,
(1) For all y ≥ 1,
γ NegBin1/2,q (y) ≤ 
1 − q Pr[Y = y]/y0 ≤ γ NegBin1/2,q (y). (71)
(2) The normalizing constant y0 satisfies
log
1 +γ
 1
√1 − q − 1
  ≤ − logy0 ≤ log
1 +γ
 1
√1 − q − 1
  . (72)
(3) The mean μ = E[Y] satisfies
γq
2(1 − q)3/2 ≤ μ
y0
≤ γq
2(1 − q)3/2 (73)
Journal of the ACM, Vol. 66, No. 2, Article 9. Publication date: March 2019.     
Capacity Upper Bounds for Deletion-Type Channels 9:39
and
γq
2(1 − q)(√1 − q +γ (1 − √1 − q)) ≤ μ ≤ γq
2(1 − q)(√1 − q +γ (1 − √1 − q)) . (74)
Proof. The first part is immediate from the expression of the digamma distribution (57) combined with the result of Lemma 14. Now let Z be distributed according to NegBin1/2,q and write,
from the definition of the normalizing constant,
1/y0 = 1 +
∞
y=1
Pr[Y = y]/y0
(71)
≤ 1 +
 γ
√1 − q
 ∞
z=1
Pr[Z = z]
= 1 +
 γ
√1 − q
 	
1 − 
1 − q


= 1 +γ
 1
√1 − q − 1

, (75)
which, after taking the logarithms of both sides, proves the upper bound in (72). Proof of the lower
bound is similar. To upper bound the mean, we may write
μ
y0
=
∞
y=1
y Pr[Y = y]/y0
(71)
≤
∞
z=1
γz Pr[Z = z]/

1 − q
= γE[Z]/

1 − q = γq
2(1 − q)3/2 , (76)
which, combined with a similar lower bound, proves (74). Finally, combining this result with (72)
yields (74).
5.4 Derivation of the Capacity Upper Bound for the Poisson-Repeat Channel
To obtain a capacity upper bound for the Poisson-repeat channel, it suffices to combine Corollary 5
with either Theorem 10 or Theorem 13. Let Ch be a D-repeat channel, where D is a Poisson
distribution with mean λ. The probability d assigned by D to zero is thus equal to e−λ, and its
complement is
p := 1 − d = 1 − e−λ .
Since d captures the “deletion probability” of the channel, we parameterize the channel in terms of
p rather than the Poisson parameter λ. Note that λ = − log(1 − p). From the capacity upper bound
of (34) (Corollary 5), we may write
Cap(Ch) ≤ sup
μ>0
Cap(Chμ (D))
−μ/ log(1 − p) + 1/p
.
We may now use Theorem 10 with the corresponding choice for the random variable Y with
parameter q, mean μ, and normalizing constant y0 and write, using the capacity upper bound
Journal of the ACM, Vol. 66, No. 2, Article 9. Publication date: March 2019.        
9:40 M. Cheraghchi
formula (60),
Cap(Ch) ≤ sup
q ∈(0,1)
−μ logq − logy0
−μ/ log(1 − p) + 1/p
. (77)
Of course, this result would remain valid had we used Theorem 13 and the convexity-based distribution (48) for Y. Furthermore, the analytic approximations of Corollary 16 may be used to upper
bound the right-hand side of (77) by the supremum of an elementary function of the channel parameter and q (which can be shown to be concave in q). We summarize the above result in the
following theorem:
Theorem 17. Let Ch be a Poisson-repeat channel with deletion probability d ∈ (0, 1) (or equivalently, repetition mean λ = log(1/d) per bit). For a parameter q ∈ (0, 1), let Y be distributed according
to either the convexity-based distribution (48) or the digamma distribution (57) with an appropriate
normalizing constant y0 and let μ := E[Y] denote its mean. Then,
Cap(Ch) ≤ sup
q ∈(0,1)
−μ logq − logy0
−μ/ logd + 1/(1 − d)
. (78)
Furthermore, let γ := 2/e1+γ ≈ 0.413099 and γ := 1/
√
2e ≈ 0.428882, where γ ≈ 0.57721 is the EulerMascheroni constant. Then,
Cap(Ch) ≤ sup
q ∈(0,1)
−μ logq − logy0
−μ/ logd + 1/(1 − d)
, (79)
where
μ := γq
2(1 − q)(√1 − q +γ (1 − √1 − q)) ,
μ := γq
2(1 − q)(√1 − q +γ (1 − √1 − q)) , (80)
y0 := 1 +γ
 1
√1 − q − 1

.
Remark 18. Observe that the distributions of Y defined by the convexity-based distribution (48)
or the digamma distribution (57) in Theorem 17 only depend on the choice of the parameter q and
not the channel parameter d at all. Therefore, the calculations of mean and normalizing constant
for various choices of q can be reused for the computation of the capacity upper bound for different
choices of d.
Observe that, as p → 0, the right-hand side of (77) converges to
p sup
q ∈(0,1)
−μ logq − logy0
μ + 1 (81)
and that the expression under the supremum is independent of the channel parameter p. The
expression under the supremum is plotted in Figure 8 and can be numerically calculated efficiently,
which results in the following corollary:
Corollary 19. Let C (d) denote the capacity of the Poisson-repeat channel with deletion probability d. Then for d → 1,
C (d) ≤ 0.464421(1 − d) · (1 + o(1)) bits per channel use.
Journal of the ACM, Vol. 66, No. 2, Article 9. Publication date: March 2019. 
Capacity Upper Bounds for Deletion-Type Channels 9:41
Fig. 8. The expression inside the supremum in (81) (measured in bits) plotted as a function of q and with
respect to the digamma distribution (57) (solid) and the convexity-based distribution (48) (dashed) as the
choices for the distribution of Y. The maximums are attained at q ≈ 0.724762 (solid) and q ≈ 0.659046
(dashed), resulting in supremums ≈ 0.464420 (solid) and ≈ 0.601549 (dashed). The analytic estimates of Theorem 13 result in an indistinguishable plot from the dashed one, and a slightly higher supremum of ≈ 0.602987
attained at q ≈ 0.658810. The dotted plot depicts the elementary upper bound resulting from (80), where the
maximum is ≈ 0.479454 and attained at q ≈ 0.727855.
Plots of the resulting capacity upper bounds for general d are given in Figure 9. Moreover, the
corresponding numerical values for the plotted curves are listed, for various choices of the deletion
parameter d, in Table 2.
6 UPPER BOUNDS ON THE CAPACITY OF THE DELETION CHANNEL
6.1 The Inverse Binomial Distribution
Let InvBinp,q be a distribution on non-negative integers, parameterized by p ∈ (0, 1] and q ∈ (0, 1),
and defined by the probability mass function15
InvBinp,q (y) := y0

y/p
y

qy exp(−yh(p)/p), (82)
where h(p) is the binary entropy function and y0 ∈ (0, 1) is the appropriate normalizing constant:
1/y0 = 1 +
∞
y=0

y/p
y

qy exp(−yh(p)/p).
Note that, when p = 1, the above reduces to a geometric distribution.
By the Stirling approximation, we may write the well-known asymptotic expression

x
px
∼ (x/e)
x

2πp(1 − p)x (px/e)px (x (1 − p)/e)(1−p)x
= exp(h(p)x)

2πp(1 − p)x
,
15We call this distribution “inverse binomial,” since, as we see in Section 6.2.1, the mutual information between any binomial
distribution Binx,p and the inverse binomial distribution essentially simplifies to a linear term in x. Therefore, intuitively,
the binomial distribution “neutralizes” any binomial distribution in the divergence computations. Moreover, thinking of y
as the posterior of a binomial sampling BinX,p , we “invert” it back to the expected prior y/p.
Journal of the ACM, Vol. 66, No. 2, Article 9. Publication date: March 2019.    
9:42 M. Cheraghchi
Fig. 9. Upper bounds (in bits per channel use) on the capacity of the Poisson-repeat channel given by Theorem 17, plotted as a function of the deletion probability d = 1 − p = e−λ. The bounds are obtained using (i)
the digamma distribution (57) for Y (solid, thick), (ii) the convexity-based distribution (48) for Y (solid), (iii)
the elementary upper bound estimates of (80) on the parameters of the digamma distribution (57) (dashed),
and (iv) the analytic upper-bound estimates of Theorem 13 on the parameters of (48) (dotted, nearly indistinguishable from the solid curve obtained by a numerical computation of the actual parameters of the
distribution).
and, thus, we can identify the asymptotic behavior of (82) at y → ∞ as
InvBinp,q (y) ∼ y0
qy

2π (1 − p)y
, (83)
implying that the normalizing constant is well defined, leading to a legitimate distribution, exactly
when q ∈ (0, 1). Moreover, the expectation of the distribution can be made arbitrarily small as
q → 0 and arbitrarily large as q → 1 (since, by (83), when q = 1 we have
InvBinp,1 (y)/y0 = Θ(1/
√
y),
and the summation defining y0 becomes divergent). Therefore, by varying the value of q ∈ (0, 1),
it is possible to adjust the expectation of the distribution to any desired positive value.
6.1.1 Estimate by the Negative Binomial Distribution. As was the case for the digamma distribution (57) related to the Poisson-repeat channel, in this section we show that the inverse binomial
distribution can be approximated by a negative binomial distribution of orderr = 1/2. Toward this
goal, we will use the following analytic claim (derived in Appendix B.2):
Claim 20. Let p ∈ (0, 1). The ratio
ρ(y) :=
	
y/p
y


exp(−yh(p)/p)
	
y−1/2
y

 (84)
Journal of the ACM, Vol. 66, No. 2, Article 9. Publication date: March 2019.
Capacity Upper Bounds for Deletion-Type Channels 9:43
is a decreasing function of y > 0 for p < 1/2, is equal to 1 for p = 1/2, and is an increasing function
of y > 0 for p > 1/2.
The fact that, by Claim 20, ρ(y) = 1 for p = 1/2 implies that the inverse binomial distribution
for the special case p = 1/2 is precisely a negative binomial distribution, and thus in this case, we
have
y0 = 
1 − q, E[InvBin1/2,q] = q/(2(1 − q)). (85)
We show that, when p  1/2, the inverse binomial distribution is still reasonably approximated
by a negative binomial distribution of order r = 1/2 and that the quality of this approximation
improves as p gets closer to 1/2. Define
β0 := ρ(1) = (2/p) exp(−h(p)/p). (86)
By combining (69), recalled below,
NegBinr,q (y) ∼
1 − q
π
qy
√y
,
and (83), the ratio in (84) satisfies
β1 := lim
y→∞ ρ(y) = 1

2(1 − p)
. (87)
We observe that, when p = 1/2, we have β0 = β1 = 1, and β0 → 2/e ≈ 0.735759 as p → 0. For p <
1/2, we have β0 > β1, whereas for p > 1/2, we have β0 < β1. This leads to the following analogous
result to (71):
Lemma 21. For a parameter p ∈ (0, 1), let the β0 and β1 be the constants defined in (86) and (87).
Let β := min{β0, β1} and β := max{β0, β1} (in particular, for p = 1/2, we have β = β = 1). Then, for
all y ≥ 1,
β NegBin1/2,q (y) ≤
√1 − q
y0
InvBinp,q (y) ≤ β NegBin1/2,q (y). (88)
The above lemma can now be used to derive upper and lower estimates on the mean and normalizing constant of an inverse binomial distribution. Let Y be distributed according to InvBinp,q
and y0 denote the corresponding normalizing constant in (82). Furthermore, let a random variable
Z be distributed according to NegBin1/2,q. As in the bound (75) on the normalizing constant of the
digamma distribution, we may now proceed by writing
1/y0 = 1 +
∞
y=1
Pr[Y = y]/y0
(88)
≤ 1 + β
√1 − q
∞
z=1
Pr[Z = z]
= 1 + β
√1 − q
	
1 − 
1 − q


= 1 + β
 1
√1 − q − 1

.
Moreover, we may derive a similar expression to (76) for the mean, namely, letting μ := E[Y], that
we have
μ
y0
≤ βq
2(1 − q)3/2 . (89)
Journal of the ACM, Vol. 66, No. 2, Article 9. Publication date: March 2019.    
9:44 M. Cheraghchi
Fig. 10. Quality of the approximations on the parameters of a negative binomial distribution with p = 0.1
in terms of elementary functions (top; using Corollary 22) and standard special functions (bottom; using
Theorem 24). The plots on the left depict the ratios μ/μ (solid) and μ/μ (dashed), as functions of q, where μ
and μ are, respectively, the upper and lower bound estimates on the mean μ. Similarly, the plots on the right
depict the ratios (logy0)/(logy0) (solid) and (logy0)/(logy0) (dashed), as functions of q, where y0 and y0
are, respectively, the upper and lower bound estimates on the normalizing constant y0.
Similarly, we may derive lower bounds on μ and y0 by using the lower bounding constant β. This
leads to the following result, which is analogous to Corollary 16 (see Figure 10 for plots on the
quality of this approximation):
Corollary 22. Consider the inverse binomial distribution InvBinp,q, with mean μ, as defined in
(82). Define β (respectively, β) to be the minimum (respectively, the maximum) of the two constants
(2/p) exp(−h(p)/p) and 1/

2(1 − p). Then,
log
1 + β
 1
√1 − q − 1
  ≤ − logy0 ≤ log
1 + β
 1
√1 − q − 1
  , (90)
βq
2(1 − q)3/2 ≤ μ
y0
≤ βq
2(1 − q)3/2 , (91)
βq
2(1 − q)(√1 − q + β (1 − √1 − q))
≤ μ ≤ βq
2(1 − q)(√1 − q + β (1 − √1 − q)) . (92)
6.1.2 Estimates by Standard Special Functions. The estimates of Corollary 22 provide highquality upper and lower bounds on the mean and the normalizing constant of an inverse binomial
distribution in terms of elementary functions. In fact, the bounds are exact for p = 1/2, and a numerical computation shows that (90) and (92) are within a multiplicative factor of about 1.2 for
Journal of the ACM, Vol. 66, No. 2, Article 9. Publication date: March 2019.    
Capacity Upper Bounds for Deletion-Type Channels 9:45
all p ≤ 0.8 and q ∈ (0, 1). However, as p approaches 1, the quality of the estimates degrade, as the
ratio between β and β tends to infinity when p → 1. In this section, we provide a different set of
upper and lower bounds in terms of standard special functions.
Our starting point is the following claim on the binomial coefficients (see Appendix B.3 for a
derivation):
Claim 23. There are universal constants α, α > 0 such that for all y ≥ 1 and p ∈ (0, 1), we have
exp(yh(p)/p)

2π ((1 − p)y + α)
≤

y/p
y

≤ exp(yh(p)/p)

2π ((1 − p)y + α)
. (93)
In particular, one may take α = 0.19 and α = 0.12.
Now, given a random variable Y that is distributed according to the inverse binomial distribution
(82), we can write
1/y0 = 1 +
∞
y=1

y/p
y

qy exp(−yh(p)/p)
(93)
≤ 1 +
∞
y=1
qy

2π ((1 − p)y + α)
= 1 +
1

2π (1 − p)
∞
y=1
qy

y + α/(1 − p)
= 1 + q Φ(q, 1/2, 1 + α/(1 − p))

2π (1 − p) ,
where Φ(·) denotes the Lerch transcendent (11). Similarly, an upper bound on y0 may be obtained
by replacing α with α in the above. We now upper bound the mean μ = E[Y] as follows:
μ/y0 =
∞
y=1

y/p
y

yqy exp(−yh(p)/p)
(93)
≤
∞
y=1
yqy

2π ((1 − p)y + α)
= 1

2π (1 − p)
∞
y=1
yqy

y + α/(1 − p)
= 1

2π (1 − p)




∞
y=1
qy

y + α/(1 − p) − α
1 − p
∞
y=1
qy

y + α/(1 − p)



= Φ(q, −1/2, 1 + α/(1 − p))

2π (1 − p) − α Φ(q, 1/2, 1 + α/(1 − p))

2π (1 − p)3 ,
and, similarly, a lower bound may be obtained by replacing α with α. The above estimates are
summarized in the following (and depicted in Figure 10):
Theorem 24. For parameters p,q ∈ (0, 1), let Y be distributed according to the inverse binomial
distribution (82), for an appropriate normalizing constant y0, and let μ := E[Y]. Let α, α be the conJournal of the ACM, Vol. 66, No. 2, Article 9. Publication date: March 2019.             
9:46 M. Cheraghchi
stants from Claim 23; namely, α := 0.19 and α := 0.12. Define the functions
S0 (p,q, α) := 1 + q Φ(q, 1/2, 1 + α/(1 − p))

2π (1 − p) , (94)
S1 (p,q, α) := Φ(q, −1/2, 1 + α/(1 − p))

2π (1 − p) − α, Φ(q, 1/2, 1 + α/(1 − p))

2π (1 − p)3 , (95)
where Φ(·) denotes the Lerch transcendent (11). Then, the following estimates hold:
S0 (p,q, α) ≤ 1/y0 ≤ S0 (p,q, α),
S1 (p,q, α) ≤ μ/y0 ≤ S1 (p,q, α),
S1 (p,q, α)/S0 (p,q, α) ≤ μ ≤ S1 (p,q, α)/S0 (p,q, α).
(96)
Compared with the negative binomial estimate (Corollary 22), the above estimates are inferior
around p = 1/2. However, unlike the former, Theorem 24 provides remarkably accurate estimates
on the mean and the normalizing constant of an inverse binomial distribution for all p,q ∈ (0, 1).
6.2 Upper Bounds on the Capacity of a Mean-Limited Binomial Channel
In this section, we consider the convolution channel Chμ (Berp ), where Berp is the Bernoulli distribution with mean p (and d = 1 − p is the deletion probability). The input to this channel is a
non-negative integer X, and the output is a sample from the binomial distribution BinX,p . In particular, we have E[Y] = pE[X], and thus the mean constraint implies that E[X] = μ/p.
6.2.1 Capacity Upper Bound Using the Inverse Binomial Distribution. To upper bound the capacity of Chμ (Berp ), we shall apply Theorem 1 with an appropriate choice for the distribution of Y.
Recall that, for every x ≥ 0, the distribution of Yx is binomial (Binx,p ) with parameters x (number
of trials), p (success probability), and mean E[Yx ] = px. That is, for all integers y ≥ 0,
Pr[Yx = y] =

x
y

py (1 − p)
x−y . (97)
Intuitively, the dual feasibility constraints in (29) should be generally satisfied with as small a KL
gap as possible. Indeed, according to the KKT conditions, equality must hold for every point on
the support of an optimal input distribution X. Moreover, the optimal (and in fact, any feasible) X
must have infinite support, since otherwise, for large enough x the KL divergence DKL (Yx Y ) for
the corresponding output distribution Y would be infinite, violating the dual feasibility constraints
(29). One may hope that, in the ideal case, the optimum distribution would satisfy all constraints
with equality (and that would be necessary if the optimum X had full support). However, in Remark 30, we rule out this possibility.
To identify a feasible choice for Y, we first use convexity to show that an inverse binomial
distribution, as defined in (82), is a feasible choice. Let Y be distributed according to InvBinp,q.
Note that the entropy of Yx can be written as
H(Yx ) = xh(p) − E

log
x
Yx

, (98)
Journal of the ACM, Vol. 66, No. 2, Article 9. Publication date: March 2019.  
Capacity Upper Bounds for Deletion-Type Channels 9:47
where h(p) is the binary entropy function. Then, DKL (Yx Y ) can be written as
DKL (Yx Y ) =
∞
y=0
Pr[Yx = y] log Pr[Yx = y]
Pr[Y = y]
(82) = −H(Yx ) − logy0 − E[Yx ] logq + E[Yx ]h(p)/p − E

log
Yx /p
Yx

(98) = E

log   x
Yx
 
Yx /p
Yx
 
− xp logq − logy0. (99)
Let Yx := x − Yx , and observe that the distribution of Yx is also binomial over x trials but with
success probability 1 − p, i.e., Binx,1−p . Define
f (y) := log 
x
y
 
y/p
y

.
We can now write the expectation in (99) as
E[f (Yx )] = log x! − E[logYx ] − E[logYx ] − E

log
x
Yx

.
By the Bohr-Mollerup theorem (or simply positivity of the trigamma function), − log Γ(y + 1) is a
concave function of y. Moreover, we observe the following, which implies that f (y) is a concave
function of y (see Appendix B.4 for a proof):
Claim 25. The function
f (y) := log
y
py
= log Γ(y + 1)
Γ(py + 1)Γ((1 − p)y + 1)
,
defined for p ∈ (0, 1) and x > 0, is completely monotone. That is, for all integers j = 0, 1,...,
(−1)j f (j)
(y) > 0 for all y > 0.
Therefore, we can apply Jensen’s inequality and deduce that
E[f (Yx )] ≤ f (E[Yx ]) = f (px) = log 1 = 0,
and thus, plugging this result into (99), that for x = 0, 1,...,
DKL (Yx Y ) ≤ −xp logq − logy0. (100)
Now Theorem 1 can be applied with the above choice for Y, which proves the following:
Theorem 26. Let p ∈ (0, 1) and q ∈ (0, 1) be given parameters and Y be a random variable distributed according to the inverse binomial distribution (82) for an appropriate normalizing constant
y0 and mean μ = E[Y]. Then, the capacity of the mean-limited binomial channel Chμ (Berp ) satisfies
Cap(Chμ (Berp )) ≤ −μ logq − logy0. (101)
If desired, the right-hand side of (101) can in turn be upper bounded by elementary functions
using the estimates provided by Corollary 22 or by standard special functions using Theorem 24.
Journal of the ACM, Vol. 66, No. 2, Article 9. Publication date: March 2019.          
9:48 M. Cheraghchi
Fig. 11. Left: Plots of the truncation error Rp (x/p), defined in (125) and (133), as a function of x for various
values of p. From the highest to the lowest plot: p → 0 (i.e., xE1 (x)), and p = 0.1, 0.2,..., 0.9. We recall that
R1 (x) = 0. Right: A similar plot of the KL gaps attained by the inverse binomial distribution (82).
6.2.2 Improving the Capacity Upper Bound Using the Truncation Technique. The choice of the
inverse binomial distribution for the random variable Y in Theorem 26 already achieves strong
capacity upper bounds for the mean-limited binomial channel (and, consequently, the deletion
channel with deletion probability 1 − d, as shown in Section 6.3 and Figure 16) for all p ∈ (0, 1).
However, although Y achieves the feasibility requirement (100) with equality at x = 0, for large x
the inequality remains strict and, as Figure 11 depicts, exhibits a constant asymptotic gap of 1/2
to the linear term on the right-hand side (a similar constant gap exists for the Poisson channel
with the convexity-based distribution (48) for Y, which is eliminated by using the digamma
distribution (57), leading to the improved bounds). In this section, we obtain improved results by
implementing the truncation technique of Section 5.2 for binomially distributed random variables.
We start from the following integral representation for the log-gamma function16 [9]:
log Γ(1 + z) =
 1
0
1 − tz − (1 − t)
z
t log(1 − t) dt, (102)
=
 1
0
∞
j=2
	
z
j


(−t)j−1
log(1 − t) dt. (103)
For an x > 0, let Yx be a sample from Binx,p ; i.e., a binomial random variable over x trials with
success probability p. Recall that
Pr[Yx = y] =

x
y

py (1 − p)
x−y,
and that the factorial moments of Yx are given by
E

Yx
j

=

x
j

pj
. (104)
Define
Ep (x) := E[log(Yx !)] (105)
16A simple proof of this identity is to observe that, letting f (z) denote the right-hand side expression in (102), f (0) = 0, and,
furthermore, f (z + 1) − f (z) = log(1 + z) (which can, in turn, be verified by taking the derivative of the integral expression
of the difference in z and verify that it is indeed equal to 1/(1 + z)). Since the expression defining f (z) is convex in z and
satisfies the same recursion as the log-gamma function, it must be equal to the log-gamma function by the Bohr-Mollerup
theorem.
Journal of the ACM, Vol. 66, No. 2, Article 9. Publication date: March 2019.    
Capacity Upper Bounds for Deletion-Type Channels 9:49
=
∞
y=0

x
y

py (1 − p)
x−y logy!
(103) = E
⎡
⎢
⎢
⎢
⎢
⎢
⎣
 1
0
∞
j=2
	
Yx
j


(−t)j−1
log(1 − t) dt
⎤
⎥
⎥
⎥
⎥
⎥
⎦
(104) =
 1
0
∞
j=2
	
x
j


pj (−t)j−1
log(1 − t) dt
=
 1
0
1 − ptx − (1 − pt)
x
t log(1 − t) dt. (106)
The asymptotic growth of the function Ep is derived below.
Claim 27. For large x > 0 and p ∈ (0, 1], we have Ep (x) = xp(log(xp) − 1) + 1
2 log(xp) ± O(1).
Proof. By Stirling’s approximation, we have
E1 (x) = log Γ(1 + x) = x log x − x + log √
2πx + o(1).
This particularly shows the claim for p = 1. Now, using the integral expression (102) for the loggamma function, and (106), we may write
Ep (x) − log Γ(1 + px) =
 1
0

1 − tpx − (1 − tp)
x
t log(1 − t) − 1 − tpx − (1 − t)
px
t log(1 − t)

dt
=
 1
0
(1 − t)
px − (1 − tp)
x
t log(1 − t) dt.
By decomposing the integration interval in the last expression over small t and the remaining
interval (at which the integrand exponentially vanishes as x grows), and estimating the integrand
for smallt by a series expansion, it is seen that the difference isO(1). The claim follows by applying
Stirling’s approximation on log Γ(1 + px).
Toward designing a distribution Y over positive integers that satisfies the dual constraints (29)
as tightly as possible (and particularly with equality at x = 0 and a sharply vanishing KL gap as
x grows), suppose that the probability mass function for the distribution of Y (that we wish to
design) is given by a generalization of the expression for the inverse binomial distribution (82) as
follows:
p(y) := Pr[Y = y] = y0
qy exp(д(y) − yh(p)/p)
y! , (107)
for a function д : N≥0 → R and appropriate normalizing constant y0. Note that, to have a welldefined distribution, д(y) can asymptotically be at most y logy + O(y). We may write
DKL (Yx Y ) =
∞
y=0
Pr[Yx = y](log(Pr[Yx = y]) − logp(y))
= −xh(p) + E

log
x
Yx

− logp(Yx )

(107) = −xh(p) + log x! − E[log(x − Yx )] − E[logд(Yx )] − E[Yx ](logq − h(p)/p) − logy0
(105) = log x! − E1−p (x) − E[logд(Yx )] − xp logq − logy0. (108)
Journal of the ACM, Vol. 66, No. 2, Article 9. Publication date: March 2019.        
9:50 M. Cheraghchi
Therefore, satisfying (29) is equivalent to having, for some values a and b possibly depending on
p, and for all integers x > 0,
E[д(Yx )] ≥ log x! − E1−p (x) + ax + b. (109)
Given a parameter ϵ ∈ (0, 1], let fϵ be a function such that, for every integer x ≥ 0,
E[fϵ (Yx )] = Eϵ (x). (110)
Such a function exists and can explicitly (and uniquely) be written, using (104) and (106), as
fϵ (y) =
 1
0
∞
j=2
	
y
j


(ϵ/p)j (−t)j−1
log(1 − t) dt (111)
=

y
k=0

y
k

(1/p)
k (1 − 1/p)
y−kEϵ (k) (112)
= Eϵ /p (y), (113)
where the second equality can be seen by observing that the equation for factorial moments (104) of
a binomial distribution still remains syntactically valid even if the “success probability” p defining
the distribution is greater than 1. Note that the summation in (111) is finite for any non-negative
integer y, and, thus, the series representing fϵ (y) is always convergent to a finite value.
When ϵ/p < 1, the terms in (111) exponentially vanish in magnitude as j grows and fϵ (y) maintains a manageable asymptotic growth (as shown by Claim 27). However, for ϵ > p, the value of
fϵ (y) exponentially grows in ϵ/p. In this case, we will use the truncation technique of Section 5.2
to modify fϵ (y) to a function with manageable asymptotic growth rate whose expectation under
Yx still provides a very accurate estimate on Eϵ (x). Toward this goal, let ϵ ∈ (0, 1], and consider
the following truncation of the integral expression (106) for E1/ϵ (y):
Λϵ (y) :=
 1
0
1 − ty − (1 − t)
y
t log(1 − ϵt) dt (114)
=
 ϵ
0
1 − ty/ϵ − (1 − t/ϵ )
y
t log(1 − t) dt (115)
=
 1
0
∞
j=2
	
y
j


(−t)j−1
log(1 − ϵt)
dt (116)
=
 ϵ
0
∞
j=2
	
y
j


(−t/ϵ )j−1
ϵ log(1 − t) dt.
Note that Λ1 (y) = log Γ(1 + y). For any fixed ϵ and t < ϵ, the term (1 − t/ϵ )
y in the integrand of
(115) exponentially vanishes in y, and the aim is to show that the error caused by the truncation
exponentially converges to a linear expression in y as y grows. An important property of the
function Λϵ is its expected value with respect to a binomial distribution, namely, E[Λϵ (Yx )]. Using
Journal of the ACM, Vol. 66, No. 2, Article 9. Publication date: March 2019.     
Capacity Upper Bounds for Deletion-Type Channels 9:51
(116) and (104), we have
E[Λϵ (Yx )] =
 1
0
∞
j=2
	
x
j


pj (−t)j−1
log(1 − ϵt) dt
=
 1
0
∞
j=2
	
x
j


(p/ϵ )j (−ϵt)j−1
log(1 − ϵt) d(ϵt)
=
 ϵ
0
∞
j=2
	
x
j


(p/ϵ )j (−t)j−1
log(1 − t) dt
(106) = Ep/ϵ (x) −
 1
ϵ
∞
j=2
	
x
j


(p/ϵ )j (−t)j−1
log(1 − t) dt
= Ep/ϵ (x) +
 1
ϵ
(1 − tp/ϵ )
x + xtp/ϵ − 1
t log(1 − t) dt
= Ep/ϵ (x) +
 1
ϵ
(1 − tp/ϵ )
x + xtp/ϵ − 1
t log(1 − t) dt
= Ep/ϵ (x) + xpLi(1 − ϵ )/ϵ − η(1 − ϵ ) +
 1
ϵ
(1 − tp/ϵ )
x
t log(1 − t)
dt, (117)
where Li(·) the logarithmic integral function (14), recalled below,
Li(z) :=
 z
0
dt
log t
,
and we have defined
η(z) =
 z
0
dt
(1 − t) log t =
∞
j=1
Li(zj
), (118)
If p ≤ ϵ ≤ 1, then we notice that the integrand in the residual term in (117) is non-positive, and,
thus, in this case,
(117) ≤ Ep/ϵ (x) + xpLi(1 − ϵ )/ϵ − η(1 − ϵ ).
However, the integrand is at least (1 − p)
x /(t log(1 − t)) (note that log(1 − t) < 0). Therefore, we
may also write, for p ≤ ϵ ≤ 1,
(117) ≥ Ep/ϵ (x) + xpLi(1 − ϵ )/ϵ − η(1 − ϵ ) +
 1
ϵ
(1 − p)
x
t log(1 − t)
dt
= Ep/ϵ (x) + xpLi(1 − ϵ )/ϵ + ((1 − p)
x − 1)η(1 − ϵ ). (119)
The asymptotic growth rate of the function Λϵ is described by the following result:
Claim 28. For large x, and a fixed ϵ ∈ (0, 1], the function Λϵ defined in (114) satisfies
Λϵ (x) = x
ϵ (log(x/ϵ ) + Li(1 − ϵ ) − 1) +
1
2 log(x/ϵ ) ± O(1).
Proof. The proof is quite similar to that of Claim 27. When ϵ = 1, we have
Λ1 (x) = log Γ(1 + x),
and the claim follows from Stirling’s approximation. In general, let
Δ(x) := Λϵ (x) − log Γ(1 + x/ϵ ) − xLi(1 − ϵ )/ϵ.
Journal of the ACM, Vol. 66, No. 2, Article 9. Publication date: March 2019.     
9:52 M. Cheraghchi
Using Stirling’s approximation, it suffices to show that |Δ(x)| = O(1). Let us write
log Γ(1 + x/ϵ ) = Λ1 (x/ϵ ) =
 1
0
1 − tx/ϵ − (1 − t)
x /ϵ
t log(1 − t) dt
=
 ϵ
0
1 − tx/ϵ − (1 − t)
x /ϵ
t log(1 − t) dt +
 1
ϵ
1 − tx/ϵ − (1 − t)
x /ϵ
t log(1 − t) dt
=
 ϵ
0
1 − tx/ϵ − (1 − t)
x /ϵ
t log(1 − t) dt − xLi(1 − ϵ )
ϵ −
 1
ϵ
(1 − t)
x /ϵ
t log(1 − t)
dt
=
 ϵ
0
1 − tx/ϵ − (1 − t)
x /ϵ
t log(1 − t) dt − xLi(1 − ϵ )
ϵ + o(1),
where for the last step we have used the fact that (1 − t)
x /ϵ goes down to zero as x grows. Thus,
we have
Δ(x) (114) =
 1
0
1 − tx − (1 − t)
x
t log(1 − ϵt) dt −
 ϵ
0
1 − tx/ϵ − (1 − t)
x /ϵ
t log(1 − t) dt − o(1)
=
 ϵ
0
(1 − t)
x /ϵ − (1 − t/ϵ )
x
t log(1 − t) dt − o(1),
where in the second step we have used a change of variables. The last integral can be shown to be
upper bounded by a constant by decomposing the integral over small t (and using a series estimate
of the integrand) and the remaining interval (over which the integrand tends to zero as x grows).
This completes the proof.
Now we are ready to define an appropriate expression for the distribution of Y that satisfies
(109) for choices of a and b possibly depending on p. Before doing so, let H(Binx,p ) denote the
entropy of the binomial distribution with parameters x and p and consider the following function
defined over the integers y ≥ 0:
λp (y) =

y
k=0

y
k

(1/p)
k (1 − 1/p)
y−kH(Bink,p )
=

y
k=0

y
k

(1/p)
k (1 − 1/p)
y−k (yh(p) − log k! + Ep (k) + E1−p (k))
= yh(p)/p + logy! − E1/p (y) + E1/p−1 (y), (120)
where we have syntactically extended the definition of the function Ep (recall the definition
Ep (x) := E[log(Yx !)]) in (105) to p > 1 and used the fact that the expressions for the moments
(in particular, the mean) of the binomial distribution, regarded syntactically, remain true even if
p > 1, as well as the following observation:
Proposition 29. The functions Ep defined in (105) satisfy, for all integers y ≥ 0 and all q > 0,

y
k=0

y
k

qk (1 − q)
n−kEp (k) = Epq (y).
Proof. This is a direct consequence of the fact that, letting Y ∼ Biny,q and K ∼ BinY,p , we have
K ∼ Biny,pq, and that this is true in a syntactical sense even if p and q are allowed to be larger than
one. In this case, the left-hand side is E[logK!], which, noting that the distribution of K is binomial
(namely, Biny,pq), can be rewritten as Epq (y).
Journal of the ACM, Vol. 66, No. 2, Article 9. Publication date: March 2019.        
Capacity Upper Bounds for Deletion-Type Channels 9:53
Now recall the binomially distributed random variable Yx , and observe that, using (120) and
Proposition 29, we may write
E[λp (Yx )] = xh(p) + Ep (x) − E1 (x) + E1−p (x) = H(Binx,p ),
and, therefore, if there is a normalizing constant y0 that, for some q > 0, makes the following a
legitimate probability mass function over the non-negative integers:
Pr[Y = y] = y0qy exp(−λp (y)), (121)
then the distribution of Y would then satisfy the dual feasibility conditions (29) with equality for all
x. However, as we discussed before, for all p ∈ (0, 1], the value of λp (y) (in particular, the value of
E1/p (y)) exponentially grows in y. Therefore, no value of q and y0 can normalize the above distribution to a legitimate one. To address this, we employ the truncation technique that we developed
in this section to modify λp (y) to a function that exhibits a controllable growth rate but, nevertheless, approximates the behavior of λp (y) and satisfies the dual feasibility conditions sharply.
Toward this goal, we distinguish two cases, namely, when p ∈ (0, 1/2) and when p ∈ [1/2, 1].
Remark 30. As was the case for the mean-limited Poisson channel (see Remark 12), for the meanlimited binomial channel with input distribution X and output distribution Y, we may observe that
the capacity achieving distribution for Y (and, subsequently, the capacity achieving distribution for
X) must have infinite support. Otherwise, for sufficiently large x, the quantity DKL (Yx Y ) would
be infinite, violating the KKT conditions (29). Moreover, we observe that the function λp (y) is the
unique function that satisfies E[λp (Yx )] = H(Binx,p ) for all non-negative integers x (due to the
triangular nature of the corresponding system of linear equations, the solution to this functional
equality must be unique). In turn, this implies that, up to a change in the linear term, the expression
(121) for the distribution of Y is the unique choice that would satisfy the KKT conditions (29) with
equality for all x. However, as this alleged distribution cannot be normalized, and since the KKT
conditions must be satisfied with equality over the entire support of any optimal X, it follows that
the capacity achieving distribution forX cannot have full support over non-negative integers, even
though its support must be infinite. As was discussed for the Poisson case in Remark 12, this makes
intuitive sense from a coding perspective. Intuitively, for every nonzero x on the support of X, the
next higher integer supported by X should be about x + Ω(
x (1 − p)/p) (and the first nonzero x
should be Ω(1/p)). This ensures that the channel outputs corresponding to different elements on
the support of X are sufficiently spread out (as dictated by the corresponding standard deviations)
to avoid substantial confusions on the decoder side.
Case 1: Truncation When p ∈ [1/2, 1]. When p ≥ 1/2, the only exponentially growing term in
(120) is E1/p (y), which we truncate to Λp (y) as defined in (114) and recalled below:
Λp (y) :=
 1
0
1 − ty − (1 − t)
y
t log(1 − pt) dt.
Consequently, we define the function
дp (y) := Λp (y) − E1/p−1 (y) − yLi(1 − p)/p + η(1 − p), (122)
where η is the function in (118), recalled below,
η(z) =
 z
0
dt
(1 − t) log t
,
Journal of the ACM, Vol. 66, No. 2, Article 9. Publication date: March 2019.
9:54 M. Cheraghchi
and E1/p−1 is defined according to (105), namely, Ep (x) := E[log(Yx !)]. We may write, using (117)
and Proposition 29,
E[дp (Yx )] = E1 (x) + xLi(1 − p) − η(1 − p) − E1−p (x) − xLi(1 − p) + η(1 − p) +
 1
p
(1 − t)
xdt
t log(1 − t)
= log x! − E1−p (x) +
 1
p
(1 − t)
xdt
t log(1 − t)
,
where we notice that the “error term”
 1
p
(1 − t)
xdt
t log(1 − t)
is an incomplete variation (or tail) of the integral (103) defining the log-gamma function and exponentially vanishes as x grows.
Recall the Kronecker delta function
δ (y) :=

1 y = 0,
0 y  0,
and note that E[δ (Yx )] = (1 − p)
x . Now define
д(y) := дp (y) − η(1 − p)δ (y) =

0 y = 0
дp (y) y > 0. (123)
By combining the above expectation and (119), we see that
E[д(Yx )] = log x! − E1−p (x) + Rp (x) ≥ log x! − E1−p (x), (124)
where we have defined
Rp (x) :=
 1
p
(1 − t)
x − (1 − p)
x
t log(1 − t) dt ≥ 0. (125)
Note that the error Rp (x) is zero for p = 1 and is always non-negative since the integrand is nonnegative for all t ∈ [p, 1]. Using the above choice for д(y) in the general form of the probability
mass function of Y in (107) results in
DKL (Yx Y ) (108) = −xp logq − logy0 − Rp (x) ≤ −xp logq − logy0,
thus satisfying the dual feasibility conditions (29) for all x ≥ 0 and with the choices ν1 = − logq
and ν0 = − logy0.
The asymptotic behavior of the above д(y) can be deduced from Claims 27 and 28. Namely, we
have
д(y) = y
p (log(y/p) + Li(1 − p) − 1) − y(1 − p)
p

log
y(1 − p)
p

− 1

− y
p
Li(1 − p)
+
1
2 log(y/p) − 1
2 log
y(1 − p)
p

± O(1)
= yh(p)/p + y log(y/e) ± Op (1),
Journal of the ACM, Vol. 66, No. 2, Article 9. Publication date: March 2019.    
Capacity Upper Bounds for Deletion-Type Channels 9:55
which, combined with Stirling’s approximation logy! = y log(y/e) + log √2πy + o(1), implies that
the probability mass function of Y in (107) asymptotically behaves as qy/
√y and can thus be normalized to a legitimate distribution precisely when q ∈ (0, 1). Consequently, as was the case for
the inverse binomial distribution, the expectation of the distribution can be made arbitrarily small
as q → 0 and arbitrarily large as q → 1.
Case 2: Truncation When p ∈ (0, 1/2]. For values of p below 1/2, both E1/p (y) and E1/p−1 in (120)
exponentially grow and must both be truncated. Accordingly, we modify the function дp (y) in
(122) to, recalling the functions Λϵ from (114) and η from (118), the following:
дp (y) := Λp (y) − Λp/(1−p) (y) + y
p

(1 − p)Li
1 − 2p
1 − p

− Li(1 − p)

+ η(1 − p) − η

1 − 2p
1 − p

= Λp (y) − Λp/(1−p) (y) + y
p

(1 − p)Li
1 − 2p
1 − p

− Li(1 − p)

+
 p
1−p
p
dt
t log(1 − t)
, (126)
and, similarly to the previous case, adjust it with a Kronecker delta as follows:
д(y) = дp (y) −
 p
1−p
p
δ (y)dt
t log(1 − t) =

0 y = 0,
дp (y) y > 0. (127)
Remark 31. Note that Li(0) = η(0) = 0, and Λ1 (y) = E1 (y) = log Γ(1 + y). Therefore, we see that
for the boundary case p = 1/2, the expressions given by (122) and (126) coincide.
A remarkable property of the distribution defined with respect to the above choice of д in (127)
is that, in the limit p → 0, the distribution converges to the digamma distribution (57) that we
designed for the mean-limited Poisson channel. Therefore, the distribution designed using the
truncation technique in this section is indeed the right generalization of what we constructed
for the Poisson case to the more general setting of the binomial channel. This is formalized
below.
Proposition 32. Consider the distribution (107), where д(y) is defined according to (127).
Then,
lim
p→0
Pr[Y = y] = y0
exp(yψ (y))(q/e)
y
y! ,
where ψ (·) is the digamma function. That is, the distribution converges, pointwise, to the digamma
distribution (57).
Proof. Recall, from (116), the series expansion
Λϵ (y) =
 1
0
∞
j=2
	
y
j


(−t)j−1
log(1 − ϵt)
dt.
Journal of the ACM, Vol. 66, No. 2, Article 9. Publication date: March 2019.      
9:56 M. Cheraghchi
Let Λ0 (y) := limϵ→0 ϵΛϵ (y). In the limiting case ϵ → 0, the denominator of the above series can be
estimated by ϵt, so that we have
Λ0 (y) =
 1
0
∞
j=2

y
j

(−t)
j−2 dt
=
∞
j=2

y
j

(−1)j
j − 1
= −y
∞
j=1

y − 1
j
 (−1)j
j(j + 1)
= −y




∞
j=1

y − 1
j

(−1)j
j −
∞
j=1

y − 1
j

(−1)j
j + 1



(52) = −y




−γ −ψ (y) −
∞
j=1

y − 1
j

(−1)j
j + 1



= yψ (y) +γy +
∞
j=1
 y
j + 1

(−1)
j
= yψ (y) + (γ − 1)y + 1, (128)
where for the last equality we have used ∞
j=0
	
y
j


(−1)j = (1 − 1)
y = 0. From the expansion of the
logarithmic integral (equivalently, exponential integral combined with logarithm, see [2, p. 229]),
which is,
Li(1 − ϵ ) = γ + log ϵ − ϵ/2 − ϵ2
/24 − O(ϵ3),
we may write
(1 − p)Li
1 − 2p
1 − p

− Li(1 − p) = −γp + (1 − p) log(p/(1 − p)) − logp − O(p)
= −γp + h(p) − O(p3). (129)
Furthermore, by approximating 1/(t log(1 − t)) by −1/t 2 for small t, we can deduce that
lim
p→0
 p
1−p
p
dt
t log(1 − t) = − lim
p→0
 p
1−p
p
dt
t 2 = −1. (130)
By plugging the above results (128), (129), and (130) into (126), we may now see that, for any y > 0,
д(y) = дp (y) = y(ψ (y) + h(p)/p − 1 ± O(p)). (131)
Recalling from (107) that
Pr[Y = y] = y0
qy exp(д(y) − yh(p)/p)
y! ,
we can use (131) to write
lim
p→0
Pr[Y = y] = y0
exp(yψ (y))(q/e)
y
y! ,
as claimed.
Journal of the ACM, Vol. 66, No. 2, Article 9. Publication date: March 2019.                    
Capacity Upper Bounds for Deletion-Type Channels 9:57
An important property of the function д that we need to use is its expectation with respect to a
binomial distribution. This can be expressed, using (117), as
E[д(Yx )] = E1 (x) + xLi(1 − p) − η(1 − p) +
 1
p
(1 − t)
x − (1 − p)
x
t log(1 − t) dt
− E1−p (x) − x (1 − p)Li
1 − 2p
1 − p

+ η

1 − 2p
1 − p

−
 1
p
1−p
(1 − (1 − p)t)
x − (1 − p)
x
t log(1 − t) dt
+ x

(1 − p)Li
1 − 2p
1 − p

− Li(1 − p)

+ η(1 − p) − η

1 − 2p
1 − p

= log x! − E1−p (x) + Rp (x), (132)
where we have defined
Rp (x) :=
 1
p
(1 − t)
x − (1 − p)
x
t log(1 − t) dt −
 1
p
1−p
(1 − (1 − p)t)
x − (1 − p)
x
t log(1 − t) dt
=
 1
p
(1 − t)
x
t log(1 − t)
dt −
 1
p
1−p
(1 − (1 − p)t)
x
t log(1 − t) dt − (1 − p)
x
 p
1−p
p
dt
t log(1 − t)
. (133)
The error quantity Rp (x), depicted in Figure 11, can be shown to be always non-negative:
Claim 33. For all x ≥ 0 and p ∈ (0, 1/2], the quantity Rp (x) defined in (133) is
non-negative.
Proof. We can rearrange (133) as
Rp (x) =
 p
1−p
p
(1 − t)
x − (1 − p)
x
t log(1 − t) dt −
 1
p
1−p
−(1 − t)
x + (1 − p)
x + (1 − (1 − p)t)
x − (1 − p)
x
t log(1 − t) dt
=
 p
1−p
p
(1 − t)
x − (1 − p)
x
t log(1 − t) dt +
 1
p
1−p
(1 − t)
x − (1 − (1 − p)t)
x
t log(1 − t) dt
and observe that the integrands inside both integrals in the second equation are non-negative over
the integration interval.
In fact, as p gets small, Rp (x) converges to pxE1 (px), which, as shown in (65), is the KL gap
achieved by the digamma distribution (57) for the mean-limited Poisson channel. This can be seen
from the result of Proposition 32 (showing that the truncated distribution for the binomial channel
converges to the digamma distribution (57) in the limit p → 0), combined with the fact that the
binomial channel converges to a Poisson channel as p → 0.
Using Claim 33 and (132), we now have that, for all integers x ≥ 0,
E[д(Yx )] ≥ log x! − E1−p (x),
which, similarly to the case p ≥ 1/2, implies that the probability mass function ofY in (107) satisfies
DKL (Yx Y ) (108) = −xp logq − logy0 − Rp (x) ≤ −xp logq − logy0,
thereby satisfying the dual feasibility conditions (29) for all x ≥ 0 and with the choices ν1 = − logq
and ν0 = − logy0.
Journal of the ACM, Vol. 66, No. 2, Article 9. Publication date: March 2019.      
9:58 M. Cheraghchi
Fig. 12. Plots of the probability mass function corresponding to the choice of Y in Theorem 34. For the top
plots, the length represents y, the width represents q, and the height is the probability (left: p = 0.2, right:
p = 0.8). For the bottom plots, the probability is plotted as a function of y, for p = 0.1, 0.3, 0.5, 0.7, 0.9 (from
the lowest to the highest curve), where we have chosen q = 0.1 (left), and q = 0.5 (right).
Finally, we derive the asymptotic growth of д(y) using Claim 28 as follows:
д(y) = y
p (log(y/p) + Li(1 − p) − 1) − y(1 − p)
p

log
(1 − p)y
p

+ Li
1 − 2p
1 − p

− 1

+ y
p

(1 − p)Li
1 − 2p
1 − p

− Li(1 − p)

+
1
2 log(y/p) − 1
2 log
(1 − p)y
p

± O(1)
= yh(p)/p + y log(y/e) + Op (1),
which, as was the case for p ≥ 1/2, confirms that the probability mass function of Y (107) that we
have designed can be normalized to a legitimate distribution (that we have called the truncated
distribution) precisely when q ∈ (0, 1) and that the expectation can be adjusted to any desired
positive value by choosing q appropriately. The resulting probability mass function of Y is plotted,
for various choices of p and q, in Figure 12. Furthermore, the mean of the resulting distribution is
depicted, for various choices of p, in Figure 13.
Wrapping Up. Equipped with an improved choice for the distribution of the channel output
Y (which we call the truncated distribution, as noted in Section 2.2), we may now proceed as in
Section 6.2.1 with the alternative choice applied in Theorem 26, which is restated with the modified
distribution below.
Journal of the ACM, Vol. 66, No. 2, Article 9. Publication date: March 2019.      
Capacity Upper Bounds for Deletion-Type Channels 9:59
Fig. 13. Plots of the mean of the probability distribution corresponding to the choice of Y in Theorem 34.
Left: The mean as a function of q for p = 0.1, 0.5, 0.9 (from the lowest to the highest curve). Right: The inverse
of the mean as a function of q for p = 0.1, 0.3, 0.5, 0.7, 0.9 (from the highest to the lowest curve).
Theorem 34. Let p ∈ (0, 1) and q ∈ (0, 1) be given parameters and Y be a random variable distributed according to the truncated distribution (107). Namely,
Pr[Y = y] = y0
qy exp(д(y) − yh(p)/p)
y! ,
for an appropriate normalizing constant y0 and mean μ = E[Y], where
д(y) :=

0 y = 0
дp (y) y > 0,
and дp (y) is defined for p < 1/2 by (126) and for p ≥ 1/2 by (122); that is,
дp (y) :=
⎧⎪
⎨
⎪
⎩
Λp (y) − E1/p−1 (y) − yLi(1 − p)/p + η(1 − p) p ≥ 1/2,
Λp (y) − Λp/(1−p) (y) + y
p
	
(1 − p)Li 	 1−2p
1−p


− Li(1 − p)


+
 p
1−p
p
d t
t log(1−t) p < 1/2.
Then, capacity of the mean-limited binomial channel Chμ (Berp ) satisfies
Cap(Chμ (Berp )) ≤ −μ logq − logy0. (134)
6.3 Derivation of the Capacity Upper Bound for the Deletion Channel
To complete the derivation of the capacity upper bound for the deletion channel, we combine
the results of Section 6.2.1, and their improvements in Section 6.2.2, with the general framework
developed in Section 4; in particular, Corollary 5.
Denoting by Ch the deletion channel with deletion probability d, and letting p := 1 − d, the general capacity upper bound of Corollary 5 combined with either Theorem 26 (the inverse binomial
distribution) or Theorem 34 (the truncated distribution) gives us the capacity upper bound
Cap(Ch) ≤ sup
μ ≥0
Cap(Chμ (Berp ))
1/p + μ/p
(101)
≤ p sup
q ∈(0,1)
−μ logq − logy0
1 + μ =: CBer(p), (135)
where μ and y0, respectively, denote the mean and normalizing constant of the distribution of
Y (defined by either Theorem 26 or Theorem 34) and each depend on both p and q. Let CBer(p)
Journal of the ACM, Vol. 66, No. 2, Article 9. Publication date: March 2019.
9:60 M. Cheraghchi
denote the expression on the right-hand side of (135) and CBer(p,q) denote17 the expression inside
the supremum in (135). We have proved the following:
Theorem 35. Let Ch be a deletion channel with deletion probability d, and let p := 1 − d. Given a
parameter q ∈ (0, 1), consider a random variable Y distributed according to either Theorem 26 (inverse
binomial) or Theorem 34 (the truncated distribution), with an appropriate normalizing constant y0 (q)
and mean μ(q) = E[Y]. Then,
Cap(Ch) ≤ CBer(1 − d) = (1 − d) sup
q ∈(0,1)
−μ(q) logq − logy0 (q)
1 + μ(q) . (136)
Note that, when p = 1, both Theorem 26 and Theorem 34 assign a geometric distribution to Y.
As we show in Section 6.3.4, in both cases we have CBer(1,q) = h(q).
6.3.1 The Particular Case d = 1/2. Recall that, for the special case p = d = 1/2, the inverse binomial distribution defined in (82) becomes, precisely, a negative binomial distribution. Thus, in
this case, the right-hand side of (136) can be analytically optimized in closed form. Using (85),
recalled below (for p = 1/2),
y0 = 
1 − q, E[InvBin1/2,q] = q/(2(1 − q)),
the expression under the supremum is equal to
CBer(1/2,q) = − q
2(1−q) logq − log √1 − q
1 + q
2(1−q)
= h(q)
2 − q
, (137)
whose maximum is attained at the golden ratio conjugate q = (
√
5 − 1)/2 (shown by equating the
derivative of the expression to zero, which results in a quadratic equation). It can be verified by
straightforward manipulations that, in this case, the resulting capacity upper bound given by (136)
is equal to
CBer(1/2) = 1
4 log 3 + √
5
2 = 1
2 logφ ≈ 0.347120 (in bits per channel use), (138)
where φ = (1 + √
5)/2 is the golden ratio.
An extension of the above result for p = 1/2 to smaller values of p is presented in Appendix B.5.
However, the convexification result of [39] may be used along with our capacity upper bound for
d = p = 1/2 to derive simple and closed-form capacity upper bounds for general deletion probabilities. Namely, we prove the following:
Corollary 36. Let Ch be the deletion channel with deletion probability d. Then,
Cap(Ch) ≤
⎧⎪⎪⎪
⎨
⎪⎪⎪
⎩
(1 − d) logφ ≈ 0.694242(1 − d) d ≥ 1/2,
1 − d log(4/φ) ≈ 1 − 1.305758d d < 1/2,
where φ = (1 + √
5)/2 is the golden ratio, the entropy is in bits per channel use, and the bound for
d < 1/2 holds under the plausible conjecture [13] that the capacity function is convex over d ∈ [0, 1/2].
Proof. Suppose that capacity upper bounds ofc1 and c2, respectively, for deletion probabilities
d1 and d2 are known. Let  ∈ (0, 1) and c be the capacity of the channel at deletion probability
d := d1 + (1 − )d2. Under the assumption that the capacity function for the deletion channel is
17Note that both CBer(p) and CBer(p, q) also depend on the underlying distribution for Y . However, we suppress this
dependence in the notation, which should be clear from the context.
Journal of the ACM, Vol. 66, No. 2, Article 9. Publication date: March 2019.   
Capacity Upper Bounds for Deletion-Type Channels 9:61
Fig. 14. Numerical plots of the capacity upper bound slopeCBer(p,q) (the expression under the supremum in
(135)), measured in bits, for various choices of p and as a function of q, where the distribution of Y is given by
the inverse binomial distribution in Theorem 26 (left) or the improved (truncated) distribution in Theorem 34
(right). The chosen values for p are (from the lowest to the highest curve): p = 10−4, 0.1, 0.2,..., 0.9, andp = 1
(in which case the curve is equal to h(q)).
a convex function of d, it would trivially follow that c ≤ c1 + (1 − )c2, hence proving the claim
by letting d1 = 1/2,c1 = (logφ)/2, and either d2 = 0 with c2 = log 2 or d2 = 1 with c2 = 0. Without
assuming the convexity conjecture, it has been shown in [39, Theorem 1] that, unconditionally,
one has (with entropy measured in bits)
c ≤c1+(1 − )c2+(1 − d) log(1 − d)− (1 − d1) log((1 − d1))−(1 − )(1 − d2) log((1 − )(1 − d2)).
By lettingd2 = 1 and c2 = 0, andd = d1 + (1 − ) (thus, 1 − d = (1 − d1)), one getsc ≤ c1 + (1 −
d1) log((1 − d1)) − (1 − d1) log((1 − d1)) = c1, proving the unconditional claim for d ≥ 1/2. We
remark that one could use the result of [39] with d < 1/2 and get nontrivial upper bounds, unconditionally, for a range of d below 1/2 as well (e.g., d ≥ 0.48).
6.3.2 The General Case. For general p, the function CBer(p,q), under both Theorems 26 and 34,
is numerically plotted18 in Figure 14. Furthermore, if Theorem 26 is used to determine the distribution of Y (i.e., the inverse binomial distribution), then the high-quality upper bound estimates on
the value ofCBer(p,q) in terms of elementary or standard special functions (when p is not too close
to 1) are available via Corollary 22 and Theorem 24. These upper bounds are plotted in Figure 15.
Maximizing the value of CBer(p,q) with respect to q for a given p = 1 − d results in capacity
upper bounds for the deletion channel with deletion probability d. The quality of the upper bound
depends on the result used to determine CBer(p,q), including, and in decreasing order of quality:
(1) Theorem 34 for the distribution of Y (i.e., the truncated distribution),
(2) Theorem 26 for the distribution of Y (i.e., the inverse binomial distribution),
(3) The analytic upper bound estimates of Theorem 24 on the inverse binomial distribution,
(4) The elementary upper bound estimates of Corollary 22 on the parameters of the inverse
binomial distribution.
18We remark that, since the probability mass function of Y has an exponential decay, the function CBer(p, q) can be
numerically computed efficiently and in polynomial time in the desired accuracy. Moreover, the plots suggest that for
every p, this function is concave in q and thus its maximum can also be numerically computed in polynomial time in the
desired accuracy (e.g., by a simple binary search, or the Newton’s method). Even though concavity is evident from Figure 14
(and, similarly, Figure 8 for the Poisson case), it is not proved formally, and we leave it as an interesting remaining task.
The concavity of the upper bound estimates in terms of analytic functions (Figure 15) is, however, straightforward to
analytically verify.
Journal of the ACM, Vol. 66, No. 2, Article 9. Publication date: March 2019.                  
9:62 M. Cheraghchi
Fig. 15. Analytic upper bounds on the capacity slopeCBer(p,q) (the expression under the supremum in (135)),
measured in bits, for various choices of p and as a function of q, where the distribution of Y is given by the
inverse binomial distribution in Theorem 26. The diagram on the left plots the upper bound on CBer(p,q)
in terms of elementary functions, using Corollary 22. The diagram on the right uses the upper bounds in
terms of standard special functions given by Theorem 24. The choices of p are, from the lowest curve to
the highest: p = 10−4, 0.1, 0.2,..., 0.9, and p = 0.999 (the latter being excluded in the first diagram as the
resulting capacity upper bounds are already trivial at p = 0.9).
Plots of the resulting capacity upper bounds for general d are given in Figure 16 and are compared with the best available capacity upper bounds as reported in [39] (which are, in turn, based
on the results of [22] combined with a convexification technique). The corresponding numerical
values for the plotted curves are also listed, for various choices of the deletion probability d, in
Table 3.
6.3.3 The Limiting Case d → 1. When the deletion probability d tends to 1, or equivalently
p = 1 − d → 0, the capacity upper bounds take the form CBer(p) ≤ C0 (1 − d) for an absolute constant C0. The value of C0 depends on which one of the above four results is used and, by numerically maximizing the univariate concave function CBer(p,q) over q ∈ (0, 1), can be approximated,
respectively, as follows (measured in bits):
(1) Under Theorem 34, C0 ≈ 0.4644 with maximizer q ≈ 0.7247,
(2) Under Theorem 26, C0 ≈ 0.6015 with maximizer q ≈ 0.6590,
(3) Using the analytic upper bound estimate of Theorem 24, C0 ≈ 0.6115 with maximizer q ≈
0.6573,
(4) Using the elementary upper bound estimate of Corollary 22, C0 ≈ 0.6196 with maximizer
q ≈ 0.6644.
We also recall that, as we saw in the result of Section 6.3.1 for the particular case d = 1/2, the fully
explicit upper bound of (logφ)/2 for this case can be linearly extended to all d ≥ 1/2, including
the limiting case d → 1, using the convexification technique of [39]. Indeed, Corollary 36 implies
the choice of C0 = logφ ≈ 0.694242(1 − d) (in bits), where φ is the golden ratio, for this case.
Remark 37. We remark that the above upper bound estimate of 0.4644(1 − d) for d → 1 coincides with what we achieve for the Poisson-repeat channel with the same deletion probability in
Corollary 19. This is due to the fact that the truncated distribution (57) (the digamma distribution)
for the Poisson-repeat channel is the limiting distribution of what we obtain in Section 6.2.2 using
the truncation method (as shown by Proposition 32) and that the mean-limited binomial channel
converges a mean-limited Poisson channel in the limit d → 1.
Journal of the ACM, Vol. 66, No. 2, Article 9. Publication date: March 2019.
Capacity Upper Bounds for Deletion-Type Channels 9:63
Fig. 16. Upper bounds (in bits per channel use) on the capacity of the deletion channel, plotted as a function
of the deletion probability d. The bounds are obtained using (i) Theorem 34 (solid, thick), (ii) Theorem 26
(solid, black), (iii) analytic upper bound estimates of Theorem 24 (dashed, black), and (iv) elementary upper
bound estimate of Corollary 22 (dotted, black). The best-known capacity upper bounds reported in [39] are
shown in gray with the circle markers representing the explicitly reported data points. The gray plot with
square markers are the upper bounds reported in [17]. The trivial erasure capacity upper bound 1 − d as well
as the fully analytic upper bounds of Corollary 36 are displayed in dotted light gray. The numeric values
corresponding to the plots are listed in Table 3.
6.3.4 The Limiting Case d → 0. The limiting behavior of the capacity of the deletion channel
for d → 0 is very well understood [28, 29]. In particular, in this case, it is known that the capacity
behaves as 1 − h(d) + O(d) = 1 + d logd + O(d) (in bits per channel use). The goal of this section
is to prove that the capacity upper bounds obtained by Theorem 35 exhibit the correct asymptotic
behavior of 1 − Θ(h(d)) for small d (albeit with a slightly sup-optimal constant behind h(d)). We
demonstrate the result for Theorem 35 applied with the weaker choice of the inverse binomial
distribution. The same approach could be used to obtain an analogous results for the truncated
distribution of Theorem 34.
Theorem 38. Consider the deletion channel Ch with deletion probability d → 0. Then, the capacity
upper bound of Theorem 35 with respect to the inverse binomial distribution (82) takes the form
Cap(Ch) ≤ 1 − (1 − O(d))h(d)/2 = 1 − h(d)/2 + o(d) (bits per channel use).
Proof. Let Y be an inverse binomial random variable with parameter q, normalizing constant
y0, and mean μ := E[Y]. We first recall (135), where
Cap(Ch) ≤ (1 − d) sup
q ∈(0,1)
−μ logq − logy0
1 + μ .
Journal of the ACM, Vol. 66, No. 2, Article 9. Publication date: March 2019.
9:64 M. Cheraghchi
We first rule out the possibility of the optimumq being close to 1 (which is evident from Figure 15).
This is immediate from the estimates on the parameters of an inverse binomial distribution given
by Corollary 22. Namely, Corollary 22 proves that, whenq → 1, we have − logy0 = O(− log(1 − q))
and μ = Ω(1/(1 − q)). Thus, in this regime we have
−μ logq − logy0
1 + μ
≤ − logq − logy0
μ = − logq + O(−(1 − q) log(1 − q)) = O(h(q)) → 0.
Without loss of generality we may therefore assume that there is an absolute constant q0 < 1
such that q ≤ q0 (since we now know that the supremum in (135) over the values of q close
to 1 approaches zero). Logarithm of the binomial coefficient (
y/p
y ) admits the series expansion
(in d)
log  y
1−d
y

= (γ +ψ (y + 1))yd + y
12
	
12γ + 12ψ (y + 1) − π2
y + 6ψ 
(y + 1)y


d2 + O(d3
y2 + d4
y4) + ··· ,
where γ is the Euler-Mascheroni constant, and ψ is the digamma function. Consider a parameter
y1 = O(log(1/d)) to be determined later. For any y ≤ y1, we may thus write
1 ≤
 y
1−d
y

≤ 1 + O(dy logy) = 1 + O(dy2).
From the definition of the inverse binomial distribution (82), and the above estimate, we may write
y0qy exp(−yh(d)/(1 − d)) ≤ Pr[Y = y] ≤ y0qy exp(−yh(d)/(1 − d))(1 + O(dy2)) (139)
for all y ≤ y1. We also recall that, letting δ := exp(−h(d)/(1 − d)), for all y ≥ 0,
 y
1−d
y

≤ exp(yh(1 − d)/(1 − d)) = 1/δy, (140)
and, thus, for all y ≥ 0,
Pr[Y = y]/y0 =
 y
1−d
y

(δq)
y (140)
≤ qy . (141)
Since 1 − q = Ω(1), we may choose y1 large enough so as to ensure that
∞
y=y1
y Pr[Y = y]/y0 ≤ dq < d. (142)
In the sequel, we use asymptotic notation with respect to the vanishing parameter d, i.e., d = o(1).
We may write,
Journal of the ACM, Vol. 66, No. 2, Article 9. Publication date: March 2019.     
Capacity Upper Bounds for Deletion-Type Channels 9:65
1/y0 =
∞
y=0
Pr[Y = y]/y0
=
∞
y=0
 y
1−d
y

(δq)
y
(142)
≤ d +

y1
y=0
 y
1−d
y

(δq)
y
≤ d +
∞
y=0
(δq)
y (1 + O(dy2))
= d +
1
1 − δq + O

dδq(1 + δq)
(1 − δq)3

≤ d +
1
1 − δq
1 + O
 dδq
(1 − δq)2
 
≤
1 + O(d)
1 − δq ,
where in the last inequality we have used the assumptions q ≤ q0, 1 − q0 = Ω(1), d = o(1), and
δ = 1 − Θ(d logd) = 1 − o(1). Upper bounding y0 is slightly simpler:
1/y0 =
∞
y=0
Pr[Y = y]/y0 ≥
∞
y=0
(qδ )
y = 1
1 − δq .
Using a similar approach, we may upper bound μ as follows:
μ/y0 =
∞
y=1
y Pr[Y = y]/y0
(142)
≤ dq +

y1
y=1
y Pr[Y = y]/y0
≤ dq +
∞
y=1
y(δq)
y (1 + O(dy2))
= dq + δq
(1 − δq)2 + O

d(δ 2
q2 + 4δq + 1)
(1 − δq)4

≤ dq + δq
(1 − δq)2

1 + O
 d
(1 − δq)2
 
≤ δq
(1 − δq)2 (1 + O(d)),
so that, using the upper bound on y0,
μ ≤ δq(1 + O(d))
1 − δq .
Journal of the ACM, Vol. 66, No. 2, Article 9. Publication date: March 2019.                 
9:66 M. Cheraghchi
Finally, we lower bound μ as
μ/y0 =
∞
y=1
y Pr[Y = y]/y0 ≥
∞
y=1
y(qδ )
y = δq
(1 − δq)2 ,
and using the lower bound on y0, we write
μ ≥ δq(1 − O(d))
1 − δq .
In conclusion, we have so far shown the estimates
y0 = (1 − δq)(1 − O(d)), μ = δq(1 ± O(d))
1 − δq . (143)
We are now ready to apply the above estimates in (135) and write
Cap(Ch) ≤ (1 − d) sup
q ∈(0,1)
−μ logq − logy0
1 + μ
≤ sup
q ∈(0,q0 )
−μ logq − logy0
1 + μ
(143)
≤ sup
q ∈(0,q0 )
(1 − δq)
− δ q
1−δ q logq − log(1 − δq) ± O(d)
1 ± O(d) (1 + O(d))
≤ sup
q ∈(0,q0 )
(h(δq) + δq log δ ± O(d)) (1 + O(d))
≤ sup
q ∈(0,q0 )
(h(δq) − δqh(d)/(1 − d) ± O(d)) (1 + O(d))
≤ sup
q ∈(0,q0 )
(h(δq) − δqh(d) ± O(d)) (1 + O(d)).
In the above result, the expression under the supremum approaches zero for q → 0 and approaches
1 for δq = 1/2. Therefore, we expect the supremum to occur around q ≈ 1/2 and be close to 1. In
particular, we know that the supremum is away from zero (by a constant) and may thus write the
above as
Cap(Ch) ≤ (1 + O(d)) sup
q ∈(0,q0 )
(h(δq) − δqh(d)). (144)
Consider the function c(ρ) := h(ρ) + ϵρ. By simply equating the derivative of the function to
zero, it follows that the maximum of this function is attained at ρ = eϵ /(1 + eϵ ) and the that
maximum value is
c = eϵ (ϵ + log(1 + e−ϵ )) + log(1 + eϵ )
1 + eϵ = log 2 + ϵ/2 + ϵ2
/8 − O(ϵ4).
By letting ρ := δq and ϵ := −h(d), we conclude that
(144) ≤ (log 2 − h(d)/2)(1 + O(d)) = log 2 − (1 − O(d))h(d)/2,
as desired.
Journal of the ACM, Vol. 66, No. 2, Article 9. Publication date: March 2019.     
Capacity Upper Bounds for Deletion-Type Channels 9:67
7 DISCUSSION AND OPEN PROBLEMS
We introduced a number of new techniques that leave plenty of room for improvement in execution and lead to intriguing problems for future investigation. The first is to understand the loss in
the capacity upper bound (2). Recall that the Markov chain representationX − Z − Y of a D-repeat
channel in Section 2.1 (Figure 1) is exact. However, as shown in (31), the potential loss would correspond to the term I (Y;Z |X). Developing techniques for lower bounding this conditional mutual
information for the optimal input distribution X would readily yield an improvement in the capacity upper bound (assuming that one can show a general Ω(n) lower bound on this conditional
mutual information).
Another intriguing problem is to further improve the quality of the dual feasible distributions
that we introduced for the Poisson-repeat and deletion channels (i.e., the digamma distribution
(7) and the truncated variation of (12)). Can the truncation technique of Sections 5.2 and 6.2.2 be
further refined to result in even better capacity upper bounds for either channel? As we observe in
Remarks 12 and 30, the optimal input distributions for mean-limited Poisson and binomial channels
cannot have full support on all non-negative inputs, although they must have infinite supports.
Our intuition based on the variances suggests that the optimal input distribution is expected to
actually be quite sparse; e.g., supported on points Θ(i
2/λ) for the Poisson case and Θ(i
2 (1 − p)/p)
for the binomial case (i = 0, 1,...). A further intuition is that the KL gap in (3) attained by the
optimal output distribution should take the general form of the gaps attained by our dual-feasible
distributions (Figures 2 and 11) but, additionally, oscillate back and forth to zero (while remaining positive) and reach zero exactly at the sparse set of points supported by the optimal input
distribution.
In the X − Z − Y Markov chain representation of the deletion channel (Figure 1), observe that
each Zi is the sum of a geometric number of the entries in the run-length representation
X1,X2,...,Xt of X and that Yi is obtained by passing Zi − 1 through a binomial channel. Let Z be
the optimal input distribution for a single use of the binomial channel and χ be the characteristic
function of the distribution of 1 + Z. We say the distribution is geometrically infinitely divisible
if e1−1/χ (t) is an infinitely divisible characteristic function [31]. In this case, for all r ∈ (0, 1], one
can identify a random variable Z such that 1 + Z is the sum of a geometric number (with mean
1/r) of independent copies of Z
. Then, one may hope to set up the run-length distribution of the
input sequence X to be i.i.d. from a distribution X such that, for an appropriate r, sum of a geometric (with mean 1/r) copies of X gives the distribution of 1 + Z. If r is chosen appropriately
(i.e., such that it coincides with the deletion probability of an entire run in X), then the distribution
of Z1 − 1,Z2 − 1,... would form a sequence of i.i.d. copies of Z, i.e., the optimal input distribution for the Z − Y link. In this case, one may hope to show that the resulting input distribution
X would be capacity achieving for the deletion channel. We note, however, that currently there is
no general consensus (except for d = 0) on whether the capacity achieving input distribution for
the deletion channel must consist of i.i.d. run-lengths. The optimality of i.i.d. run-lengths has not
been ruled out for any d, and indeed the above intuition on geometric infinite divisibility, if valid,
may suggest this as a possibility.
We obtain sharp estimates on the functions inside the supremums in (5) and (10) in terms of
elementary or standard special functions. Can the supremums themselves (i.e., the capacity upper bounds) be upper bounded in terms of such explicit functions? An effort toward this goal is
demonstrated in Appendix B.5. Furthermore, our work motivates the study of several novel discrete probability distributions that are worth further consideration.
Our techniques are general and can be applied to any repetition rule. An interesting direction
is to apply the developed techniques on other natural repeat channels. In a subsequent work [11],
Journal of the ACM, Vol. 66, No. 2, Article 9. Publication date: March 2019.     
9:68 M. Cheraghchi
Fig. 17. Plot of (

2π (n + σ )( n
e )
n )/Γ(n + 1), as a function of n ≥ 1, with σ = σ (solid) and and σ = σ (dashed)
as determined by (146).
this has been done for a few cases, such as the duplication channel (where each bit is delivered
either once or twice) and the geometric deletion channel (where each bit is delivered a geometric
number of times) and its “sticky” variant (where deletions of bits do not occur).
APPENDIX
A PROOF OF THEOREM 13
Our starting point is the Stirling approximation of the gamma function
n! = Γ(n + 1) ∼ √
2πn 
n
e
n
,
which asymptotically matches n! and, for all n > 0, provides a lower bound on n!. A generalized
form of the approximation is
n! ∼ 
2π (n + σ )

n
e
n
, (145)
for some real parameter σ ≥ 0, so the Stirling approximation is the special case σ = 0. By “fine
tuning” the constant σ, it is possible to obtain sharp lower bounds, and upper bounds, on n! for
all n ≥ 1. Lets us call a value of σ a lower (respectively, upper) bounding constant for (145) if the
resulting estimate provides a lower (respectively, upper) bound on n! for all n ≥ 1. It was shown
by Gosper [24] that σ = 1/6 ≈ 0.166 provides a remarkably accurate lower bound estimate on n!.
The accuracy of this estimate has been studied in [37], where it is shown that
Γ(n + 1) =

n
e
n 
2π (x + 1/6)

1 +
1
144(n + 1/4)2 − 1
12960(n + 1/4)3 −··· 
,
leading to a multiplicative error of about 0.4% even for n = 1 (as opposed to about 8% achieved by
the Stirling approximation). Let us consider fixed choices of lower and upper bounding constants,
respectively, σ and σ, for (145). By inspection (see Figure 17), we find that the following are valid
choices19
σ := 1/6 = 15/90, σ := 0.177 ≈ 16/90. (146)
19The choice of σ has been rigorously validated by [37]. However, we have only numerically validated σ and it would be
interesting to obtain a rigorous proof of its validity.
Journal of the ACM, Vol. 66, No. 2, Article 9. Publication date: March 2019. 
Capacity Upper Bounds for Deletion-Type Channels 9:69
Now let us define functions
S0 (q) :=
∞
y=1
yy
y! (q/e)
y,
S1 (q) :=
∞
y=1
yy+1
y! (q/e)
y .
Using (145), we can compute upper and lower bounds on the values of these functions as follows.
Define
S0 (q, σ ) :=
∞
y=1
yy

2π (y + σ )
	 y
e

y (q/e)
y (147)
= 1
√
2π
∞
y=1
qy
√
y + σ
= q
√
2π
Φ(q, 1/2, 1 + σ ), (148)
where Φ(·) denotes the Lerch transcendent (11). For the special case σ = 0, this slightly
simplifies to
S0 (q, 0) = 1
√
2π
Li1/2 (q),
where Lis (·) denotes the polylogarithm function (6). Similarly, we define
S1 (q, σ ) :=
∞
y=1
yy+1

2π (y + σ )
	 y
e

y (q/e)
y (149)
= 1
√
2π
∞
y=1
(y + σ − σ )qy
√
y + σ
= q
√
2π
Φ(q, −1/2, 1 + σ ) − σS0 (q, σ ), (150)
which, for σ = 0, simplifies to
S1 (q, 0) = 1
√
2π
Li−1/2 (q).
Clearly, we have
S0 (q) ≥ S0 (q, σ ), S1 (q) ≥ S1 (q, σ ), S0 (q) ≤ S0 (q, σ ), S1 (q) ≤ S1 (q, σ ). (151)
Given a parameter q ∈ (0, 1), using the definition (48) of the convexity-based distribution, we may
write
y0 = 1/(1 + S0 (q)), μ := E[Y] = y0S1 (q) = S1 (q)/(1 + S0 (q)).
Using (148) and (150), we may now derive the estimates (66) in the statement. This completes the
proof.
B ANALYTIC CLAIMS
In this section, we verify a number of stand-alone analytic claims that have been used in the proofs.
Journal of the ACM, Vol. 66, No. 2, Article 9. Publication date: March 2019.      
9:70 M. Cheraghchi
Fig. 18. Plot of the function φ in (152).
B.1 Claim 15
Consider the derivative of the function д defined in (70):
д
(y) = 1
√
π (Γ
(y + 1/2) exp(y − yψ (y)) + Γ(y + 1/2) exp(y − yψ (y))(1 − (ψ (y) + yψ 
(y))))
= −1
√
π Γ(y + 1/2) exp(y − yψ (y)) (−ψ (y + 1/2) − 1 +ψ (y) + yψ 
(y)),
where ψ (·) is the digamma function. It can be seen that the function
φ(y) := −ψ (y + 1/2) +ψ (y) + yψ 
(y) − 1, (152)
depicted in Figure 18, is positive for all y > 0 (in fact, this function is completely monotone, which
can be proved by expressing φ(y) using Euler’s integral representation for the digamma function).
It follows that д
(y) < 0 for all y > 0, and thus the ratio д is strictly decreasing.
B.2 Claim 20
For the special case p = 1/2, the ratio is equal to 1 for all y > 0 due to the duplication formula for
the gamma function:
Γ(y)Γ 	
y + 1
2


= √
4π Γ(2y)/4y,
which implies that,

2y
y

= 2Γ(2y)
Γ(y)Γ(y + 1) = 4y Γ
	
y + 1
2


√
π Γ(y + 1) = 4y
√
π

y − 1
2
y

Γ(1/2) = 4y

y − 1
2
y

,
and the result follows by noting that h(1/2) = log 2. Since ρ is always positive for y > 0, it is
increasing (decreasing) if and only if log ρ is increasing (decreasing). We can write the derivative
of log ρ in y by taking the derivative of each constituent term; that is,
p
d
dy log ρ(y) = −h(p) − pψ (y + 1/2) − (1 − p)ψ (y(1/p − 1) + 1) +ψ (y/p + 1) =: ρ1 (y).
One can verify that limy→∞ ρ1 (y) = 0. The derivative of ρ1 (y), in turn, can be written as
p
d
dy ρ1 (y) = −p2
ψ 
(y + 1/2) − (1 − p)
2
ψ 
(y(1/p − 1) + 1) +ψ 
(y/p + 1).
This function is positive and decreasing in y (in fact, completely monotone) when p < 1/2, identically zero when p = 1/2, and negative and increasing (in fact, negated completely monotone) when
Journal of the ACM, Vol. 66, No. 2, Article 9. Publication date: March 2019.   
Capacity Upper Bounds for Deletion-Type Channels 9:71
p > 1/2 (this can be proved using Theorem 39). It follows that ρ1 (y) is increasing (and negative)
when p < 1/2, zero when p = 1/2, and decreasing (and positive) when p > 1/2. The claim follows.
B.3 Claim 23
We do not present a rigorous proof that the particular choices of α and α in the statement are
valid but rather provide a convincing argument. It would be an interesting question to provide a
complete proof. Consider the log-ratio
Rp,α (y) := log
y/p
y

− yh(p)/p +
1
2 log(2π ((1 − p)y + α)). (153)
By Stirling’s approximation, it follows that for any p ∈ (0, 1) and any fixed α, we have
lim
y→∞ Rp,α (y) = 0.
It can be proved, using Theorem 39, that the function −Rp,0 is completely monotone for all p ∈
(0, 1). Therefore, even for y ≥ 0, the constant α = 0 provably provides an upper bound for (
y/p
y ).
That is, for all p ∈ (0, 1),
y/p
p

≤ exp(yh(p)/p)

2π (1 − p)y
.
We now have the more refined task of finding choices of α that makes the above log-ratio always
positive (negative) for all p ∈ (0, 1) and all y ≥ 1 (rather than y ≥ 0). It is worthwhile to note that
the jth derivative of the function −Rp,α (y) (for j ≥ 1) can be computed, letting q := p/(1 − p), as
Aj−1 (−p)j (1 − p)j + (a + y(1 − p))j 	
pj
ψ (j−1)
(1 + y) −ψ (j−1)
	
1 + y
p


+ (1 − p)j
ψ (j−1)
	
1 + y
q

 

pj (a + y(1 − p))j ,
where ψ (j) is the polygamma function, Aj is the number of even permutations on n items (cf.
OEIS A001710), except for the special cases A1 := A0 := 1/2, and that for the first derivative, an
additional constant h(p)/p is to be added to the above expression to obtain the correct derivative.
Figure 19 depicts the log-ratio function and its derivative for various choices of α and p. At y = 1,
we have
Rp,α (1) = (1 − p) log(1 − p)/p + log(2π (1 − p + α))/2.
By setting Rp,α (1) = 0, and solving for α, we obtain the solution
α0 := −1 + p + (1 − p)
2−2/p
2π .
The value of the solution α0 as a function of p is plotted in Figure 19 (the limit at p → 0 is e2/(2π ) −
1 ≈ 0.176, and the minimum occurs a p ≈ 0.405 at which point we have α0 ≈ 0.136). Note that α0
is the transition point for whether the plot for the log-ratio lies above or below the horizontal axis
around y = 1, and we see that this transition always occurs within α ∈ (0.13, 0.18). By choosing
the value of α sufficiently outside this critical region (in particular, for the choices of α and α in
the statement), we may ensure that the log-ratio remains above, or below, zero for y ≥ 1 until the
asymptotic convergence toward zero dominates the behavior of the function. The log-ratios for
the chosen values of α and α and various choices of p are plotted as functions of y in Figure 19.
We observe that, apart from the cases p → 0 and p → 1, the log-ratio Rp,α (y + 1) or its negation
appear to be not only positive but also completely monotone for the chosen values of α.
Journal of the ACM, Vol. 66, No. 2, Article 9. Publication date: March 2019.  
9:72 M. Cheraghchi
Fig. 19. Plots related to the log-ratio function Rp,α in (153). Top: Plots of R0.5,α (y), as functions of y, for
α = 0.1, 0.15, 0.20, 0.25, 0.30 (left, from the lowest to the highest curve) and their derivatives in y (right,
from the highest to the lowest curve). Bottom left: Plot of α0 = −1 + p + (1 − p)
2−2/p /(2π ) as a function
of p. Bottom right: Plots of Rp,α (y) (solid) and Rp,α (y) (dashed) in y for p = 10−3, 10−2, 0.1, 0.3, 0.5, 0.7,
0.9, 0.99, 0.999.
B.4 Claim 25
In this section, we prove Claim 25 that is restated below:
Claim. The function
f (y) := log
y
py
= log Γ(y + 1)
Γ(py + 1)Γ((1 − p)y + 1)
,
defined for p ∈ (0, 1) and x > 0, is completely monotone. That is, for all integers j = 0, 1,...,
(−1)j f (j)
(y) > 0 for all y > 0.
Proof. By definition, f (y) > 0 for all y > 0. Moreover,
f 
(y) = ψ (y + 1) − pψ (py + 1) − (1 − p)ψ ((1 − p)y + 1),
which is positive for all y > 0 by the concavity of the digamma function. Thus, it suffices to show
that f (y) is completely monotone. This claim is a special case of the following result proved
in [30]:
Theorem 39. [30] Let
F (y) := Πm
i=1Γ(Aiy + ai )
Πn
j=1Γ(Bj + bj) .
Then, (log F (y)) is completely monotone if and only if the function
P (u) :=
m
i=1
exp(−aiu/Ai )
1 − exp(−u/Ai )
−
n
i=1
exp(−bju/Bj)
1 − exp(−u/Bj)
Journal of the ACM, Vol. 66, No. 2, Article 9. Publication date: March 2019.    
Capacity Upper Bounds for Deletion-Type Channels 9:73
is non-negative for all u > 0.
For our application, we have
P (u) = 1
eu − 1 − 1
eu/p − 1 − 1
eu/q − 1
.
To show that P (u) ≥ 0, it suffices to verify that
P0 (u) := p
eu − 1 − 1
eu/p − 1
≥ 0
for all u > 0 or, equivalently, that
peu/p − eu + 1 − p ≥ 0
for allu > 0. The left-hand side is zero atu = 0 and has a positive derivative inu (which is eu/p − eu )
for all u ≥ 0. The result follows.
B.5 An Analytically Simple Deletion Capacity Upper Bound for p ≤ 1/2
In this appendix, we show that the result of (137) for p = 1/2 can be extended to smaller values of
p as well, leading to simple and analytic capacity upper bound expressions. In particular, recalling
the notation of Section 6.3, we observe the following:
Claim 40. Let CBer(p,q) be defined with respect to the inverse binomial distribution for Y. Then,
for all p ≤ 1/2, and all q ∈ (0, 1), the following upper bound holds:
CBer(p,q) ≤ β0h(q)
2 − (3 − 2β1)q
, (154)
where β0 and β1 are defined according to (86) and (87), which are both equal to 1 for p = 1/2.
Proof. To derive the claim, consider
f (q) := CBer(p,q) = −μ(q) logq − logy0 (q)
1 + μ(q) ,
where μ(q) and y0 (q) are, respectively, the mean and the normalizing constant of the inverse binomial distribution (82) for the given parameters p and q. Using Corollary 22, we may upper bound
f (q) by the elementary function д(q) defined below as
д(q) :=
− βq log q
2(1−q)(√1−q+β (1−
√1−q)) − log 
1 + β
 1 √1−q − 1
 
1 +
βq
2(1−q)(√1−q+β (1−
√1−q))
,
where β (respectively, β) is the minimum (respectively, the maximum) of the two constants β0 =
(2/p) exp(−h(p)/p) and β1 = 1/

2(1 − p) (so that, for p ≤ 1/2, β = β0 and β = β1). Define the ratio
R(q) := h(q)/д(q). (155)
This ratio is plotted in Figure 20. A direct calculation shows that
lim
q→0
R(q) = 2/β, lim
q→1
R(q) = β/β, lim
q→0
∂R
∂q = (2β − 3)/β.
Suppose p ≤ 1/2, in which case it can be observed that R(q) lies above its tangent line at q → 0
(and for p = 1/2, R(q) actually coincides with the tangent line). Therefore,
R(q) = h(q)
д(q) ≥
2 + (2β − 3)q
β = 2 − (3 − 2β1)q
β0
,
Journal of the ACM, Vol. 66, No. 2, Article 9. Publication date: March 2019.
9:74 M. Cheraghchi
Fig. 20. Left: Plots of the ratio R(q), defined in (155) for p = 0.1, 0.3, 0.5, 0.7, 0.9 (from the highest to the
lowest graph), as a function of q. Right: Plot of R(q) for p = 0.1 and its corresponding tangent line (dashed)
at q = 0.
and the claim follows.
Using the result of Claim 40, we can now prove the following:
Corollary 41. Let p ≤ 1/2, β0 and β1 be defined according to (86) and (87), and Ch be the deletion
channel with deletion probability d = 1 − p. Then,
Cap(Ch) ≤ (1 − d)β0h(q)
2 − (3 − 2β1)q ,
where q ∈ (0, 1) is the solution to q = (1 − q)β1−1/2.
Proof. Recall that
Cap(Ch) ≤ (1 − d) sup
q ∈(0,1)
CBer(p,q) ≤ sup
q ∈(0,1)
β0h(q)
2 − (3 − 2β1)q
,
where the second inequality is due to Claim 40. Let q be the choice of q that maximizes the
right-hand side of (154). The derivative of the right-hand side of (154) in q is equal to
β0
−2 logq − (1 − 2β1) log(1 − q)
(2 − (3 − 2β1)q)2 .
By equating this derivative to zero, we see that q is the solution to q = (1 − q)β1−1/2, and the
result follows.
Journal of the ACM, Vol. 66, No. 2, Article 9. Publication date: March 2019.           
Capacity Upper Bounds for Deletion-Type Channels 9:75
Table 1. Mean and Normalizing Constants for the Distributions Defined for the Poisson
Channel by the Convexity-based Distribution (48) (i = 1) and the Digamma Distribution (57)
(i = 2) for Various Choices of the Parameter q ∈ (0, 1)
For each i ∈ {1, 2}, we have used the notation i := − logy0 and μi := E[Y ] for the corresponding
distribution.
Journal of the ACM, Vol. 66, No. 2, Article 9. Publication date: March 2019.
9:76 M. Cheraghchi
Table 2. Capacity Upper Bounds for the Poisson-repeat Channel with Deletion Probability
d = 1 − p = e−λ, as Plotted in Figure 9
For each d, the quantity (1 − d)ci is the capacity upper bound when method i = 1, 2, 3, 4 is used, and qi is the choice
of q that maximizes the function under the supremum in (77) for the chosen method. The methods are as follows:
The digamma distribution (57) for Y (i = 1, giving the strongest bounds); the elementary upper bounds of (80) on the
parameters of the digamma distribution (57) (i = 2); the convexity-based distribution (48) for Y (i = 3); The analytic
upper bounds of Theorem 13 for the convexity-based distribution defined by (48) (i = 4). Note that the methods i = 3
and 4 give trivial results for sufficiently small d.
Journal of the ACM, Vol. 66, No. 2, Article 9. Publication date: March 2019.
Capacity Upper Bounds for Deletion-Type Channels 9:77
Table 3. Capacity Upper Bounds for the Deletion Channel with Deletion Probability
d as Plotted in Figure 16
For each d, the quantity (1 − d)ci is the capacity upper bound when method i = 1, 2, 3, 4 is used, and qi is the choice
of q that maximizes the function CBer(1 − d, q) under the supremum in (135) for the chosen method. The methods are
as follows: Theorem 34 (the truncated distribution) for the distribution of Y (i = 1, giving the strongest bounds); Theorem 26 (inverse binomial) for the distribution of Y (i = 2); analytic upper bounds of Theorem 24 (i = 3); the elementary
upper bounds of Corollary 22 (i = 4). Note that the analytic upper bound estimates (i = 3, 4) give trivial results for
sufficiently small d.