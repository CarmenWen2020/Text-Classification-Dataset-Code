We present prior robust algorithms for a large class of resource allocation problems where requests arrive
one-by-one (online), drawn independently from an unknown distribution at every step. We design a single
algorithm that, for every possible underlying distribution, obtains a 1 ‚àí œµ fraction of the profit obtained by an
algorithm that knows the entire request sequence ahead of time. The factor œµ approaches 0 when no single
request consumes/contributes a significant fraction of the global consumption/contribution by all requests
together. We show that the tradeoff we obtain here that determines how fast œµ approaches 0, is near optimal:
We give a nearly matching lower bound showing that the tradeoff cannot be improved much beyond what
we obtain.
Going beyond the model of a static underlying distribution, we introduce the adversarial stochastic input
model, where an adversary, possibly in an adaptive manner, controls the distributions from which the requests
are drawn at each step. Placing no restriction on the adversary, we design an algorithm that obtains a 1 ‚àí œµ
fraction of the optimal profit obtainable w.r.t. the worst distribution in the adversarial sequence. Further, if the
algorithm is given one number per distribution, namely the optimal profit possible for each of the adversary‚Äôs
distribution, then we design an algorithm that achieves a 1 ‚àí œµ fraction of the weighted average of the optimal
profit of each distribution the adversary picks.
In the offline setting we give a fast algorithm to solve very large linear programs (LPs) with both packing and covering constraints. We give algorithms to approximately solve (within a factor of 1 + œµ) the mixed
packing-covering problem withO(
Œ≥m log(n/Œ¥ )
œµ 2 ) oracle calls where the constraint matrix of this LP has dimension n √ó m, the success probability of the algorithm is 1 ‚àí Œ¥, and Œ≥ quantifies how significant a single request
is when compared to the sum total of all requests.
We discuss implications of our results to several special cases including online combinatorial auctions,
network routing, and the adwords problem.
CCS Concepts: ‚Ä¢ Theory of computation ‚Üí Design and analysis of algorithms; Online algorithms;
Adversary models;
Additional Key Words and Phrases: Online algorithms, unknown distribution, approximation algorithms,
greedy algorithm

1 INTRODUCTION AND SUMMARY OF RESULTS
There has been an increasing interest in online algorithms for resource allocation problems motivated by their wide variety of applications in Internet advertising, allocating multi-leg flight seats
for customers online, allocating goods to customers arriving online in a combinatorial auction,
and so on. Designing efficient resource allocation algorithms has significant scientific and commercial value. The traditional computer science approach to deal with uncertain future inputs has
been the worst-case competitive analysis. Here nothing is assumed about the sequence of requests
that arrive online, and the benchmark is the optimal algorithm that knows the entire sequence of
requests ahead of time. Several problems in this space have been analyzed in the traditional framework, exemplified, for instance, in the well-studied Adwords problem introduced by Mehta et al.
[19]. While worst-case analysis is a robust framework, for many problems it leads to pessimistic
bounds that rule out obtaining more than a constant fraction of the optimal profit. Consequently,
there has been a drive in the last few years to go beyond worst-case analysis. A frequently used
alternative is to perform stochastic analysis: Assume that the input is drawn from a known distribution and optimize the objective w.r.t. this distribution. While stochastic analysis circumvents
the impossibility results in worst-case analysis, any error in the knowledge of distribution could
render the algorithm suboptimal and sometimes even infeasible.
In this article, we study a middle ground between worst-case and stochastic settings. We assume
that the input is drawn from an underlying distribution that is unknown to the algorithm designer.
We present a single algorithm where for every distribution performs nearly as well as the optimal
algorithm that knows the entire sequence of requests ahead of time. In this sense, the algorithm is
prior robust.
We now give an informal description of the resource allocation framework and our main contributions. See Section 2 for a formal description and theorem statements. We consider a resource
allocation setting where requests arrive online; every request can be served by some subset of several available options; each (request, option) pair consumes some amount of every resource, and
generates some profit. There is a given budget for each resource. Requests are drawn i.i.d. from
an unknown distribution. The goal is to maximize the total profit generated while making sure
that the total consumption of each resource is no more than the corresponding budget. We compare the profit of the algorithms against the offline optimum and prove competitive ratio bounds.
Even for very restricted special cases of this problem, the worst-case setting cannot yield anything
beyond a 1 ‚àí 1
e competitive ratio [15, 19]. While the stochastic setting with a fully known distribution can give near optimal performance guarantees, it often leads to very distribution dependent
algorithms (e.g., see Reference [3] for the special case of the Adwords problem, which requires
knowledge of the entire distribution to perform the optimization). Hence both these approaches
are not satisfactory, and this problem lends itself well to the middle ground of prior robust analysis.
Going beyond i.i.d., our work introduces the adversarial stochastic input (ASI) model as a more
realistic model for analyzing online algorithmic problems. Here the distribution from which the requests arrive is allowed to change over time (unlike i.i.d., where it stays identical for every request).
The adversary decides how to pick the distributions and is even allowed to pick them adaptively.
For many practical applications such as in display advertising, the distribution of requests shows
Journal of the ACM, Vol. 66, No. 1, Article 7. Publication date: January 2019.
Near Optimal Online Algorithms and Fast Approximation Algorithms 7:3
trends that change over the course of time: mornings are different from evenings and weekdays
are different from weekends. Thus a time varying distributional model is more realistic than the
i.i.d. model. A keen reader might notice that the above description includes the worst-case setting as well, and therefore we have to make some extra assumptions, either by restricting how
these distributions can be picked or by allowing the algorithm some extra information about the
distributions. We will describe these in greater detail later.
Apart from the theoretical contribution, the algorithms we design for the ASI models were used to
completely overhaul the display advertising management system at Microsoft, leading to a significant
improvement in revenue (‚âà10%) and better system manageability and enabling new capabilities.1
We believe that our results make a significant contribution to the search for ‚Äúallows-positiveresults-yet-realistic‚Äù models for online algorithms.
First Result: Near-Optimal Prior Robust Online Algorithms for Resource Allocation Problems. A key
parameter on which algorithms for several resource allocation problems depend on is the relative
significance of any single request when compared to the entire sequence of requests. For instance,
for the special case of the Adwords problem, this is the ratio of a single bid to an advertiser‚Äôs
budget. For the Adwords problem, Mehta et al. [19] and Buchbinder et al. [5] design an algorithm
that achieves a worst-case competitive ratio that tends to 1 ‚àí 1/e as the bid to budget ratio (which
we denote by Œ≥ ) tends to 0.2 Devanur and Hayes [7] studied the same problem in the random
permutation model and showed that the competitive ratio tends to 1 as Œ≥ tends to 0. This result
showed that competitive ratio of algorithms in stochastic models could be much better than that
of algorithms in the worst case. The important question since then has been to determine the
optimal tradeoff between Œ≥ and the competitive ratio. Devanur and Hayes [7] showed how to get
a 1- O(œµ ) competitive ratio when Œ≥ is at most O( œµ 3
n log(mn/œµ ) ), where n is the number of advertisers
and m is the number of keywords. Subsequently Agrawal et al. [2] improved the bound on Œ≥ to
O( œµ 2
n log(m/œµ ) ). The articles of Feldman et al. [10] and Agrawal et al. [2] have also shown that the
technique of Devanur and Hayes [7] can be extended to other online problems.
The first main result in this article is the following threefold improvement of previous results (Theorems 2.2 and 2.3), for the i.i.d. with unknown distributions model. All our results apply to the general
class of problems that we call the resource allocation framework. A formal definition of the framework is presented in Section 2.2 and a discussion of many interesting special cases including online
network routing and online combinatorial auctions is presented in Section 7.
(1) We give an algorithm that guarantees a 1 ‚àí œµ approximation factor when Œ≥ = O( œµ 2
log(n/œµ ) ).
(2) We show that our bound on Œ≥ is almost optimal; we show that no algorithm, even if it
knew the distribution, can guarantee a 1 ‚àí œµ approximation when Œ≥ = œâ( œµ 2
log(n) ).
(3) Our algorithms lend themselves to natural generalizations that provide identical guarantees in the more general ASI model that was described earlier. We provide three different
versions of the ASI model in Section 3.5.
Significance.
(1) Regarding the bound on Œ≥ , we remove a factor of n from Œ≥ , making the algorithm more
practical. Consider for instance the Adwords problem and suppose that the bids are all
1This system had been globally operational from 2011 to 2015, when Microsoft made a deal with AOL to allow AOL to sell
the display advertisement on behalf of Microsoft.
2Note that Œ≥ approaching zero is the easiest case. Even with Œ≥ approaching zero, 1 ‚àí 1/e is the best competitive ratio that
any randomized algorithm can achieve in the worst case, illustrating how worst-case analysis leads to pessimistic bounds.
Journal of the ACM, Vol. 66, No. 1, Article 7. Publication date: January 2019.
7:4 N. R. Devanur et al.
in [0,1]. The earlier bound implies that the advertiser budgets need to be of the order of
n logn/œµ2 to get a 1 ‚àí œµ competitive algorithm, where n is the number of advertisers. With
realistic values for these parameters, it seems unlikely that this condition would be met.
While with the improved bounds presented in this article, we only need the advertiser
budget to be of the order of logn/œµ2 and this condition is met for reasonable values of the
parameters. Furthermore, in the more general resource allocation framework, the previous
best upper bound on Œ≥ is from Agrawal, Wang, and Ye [2] and equals O( œµ 2
n log(mK/œµ ) ). Here
K is the number of available ‚Äúoptions‚Äù (see Section 2.2) and in typical applications like
network routing, K could be exponential in n, and thus, the factor saved by our algorithm
becomes quadratic in n.
(2) Our ASI models are realistic models of time varying distributions for which we present
algorithms with asymptotically optimal performance guarantees. We consider three different benchmarks, each progressively stronger than the previous, and require different
levels of information about the distributions to achieve near optimal performance guarantees. For the weakest benchmark, we need just one parameter from the distribution,
while for the strongest benchmark, we still need only 2mn parameters. Note that the distributions themselves can have an arbitrarily large support size,3 and hence the amount
of information we need is much smaller than the description of all the distributions. Our
results for the ASI model can be thought of as generalizations of the ‚ÄúProphet Inequality.‚Äù4
Finally, as mentioned earlier, our algorithms for this model have made a significant impact
on the practice of display advertising management at Microsoft.
Second Result: Prior Robust 1 ‚àí 1/e Approximation Greedy Algorithm for Adwords. A natural algorithm for the Adwords problem that is widely used for its simplicity is the greedy algorithm:
Always match an incoming query to the advertiser that has the maximum effective bid (the minimum of bid and remaining budget) for that query. Because of its wide use, previously the performance of the greedy algorithm has been analyzed by Goel and Mehta [13], who showed that in the
random permutation and the i.i.d. models, it has a competitive ratio of 1 ‚àí 1/e with an assumption
that is essentially that Œ≥ tends to 0.
It has been an important open problem to analyze the performance of greedy algorithm in a
stochastic setting for unbounded Œ≥ , i.e., for all 0 ‚â§ Œ≥ ‚â§ 1. The best factor known so far is 1/2, and
this works for the worst case also. Nothing better was known, even in the stochastic models. The
second result in this article is that for the Adwords problem in the i.i.d. unknown distributions model,
with no assumption on Œ≥ (i.e., Œ≥ could be as big as 1), the greedy algorithm gets an approximation
factor of 1 ‚àí 1/e against the optimal fractional solution to the expected instance (Theorem 2.4).
Our proof technique for this result has been subsequently used to prove a similar result for the
greedy algorithm in online submodular welfare maximization [17]. We note here that there are
other algorithms that achieve a 1 ‚àí 1/e approximation for the Adwords problem with unboundedŒ≥ ,
but the greedy algorithm is the only prior robust (i.e., distribution independent) algorithm known,
and it is quite simple, too. For example Alaei, Hajiaghayi, and Liaghat [3] design a randomized algorithm that obtains a 1 ‚àí 1/e approximation but requires the knowledge of the entire distribution.
Devanur, Sivan, and Azar [8] design a deterministic algorithm that obtains a 1 ‚àí 1/e approximation
but requires a few parameters from the distribution.
3We even allow continuous distributions that have an infinite support.
4The Prophet Inequality is essentially a 1/2-competitive algorithm for the following problem: A sequence of values is
drawn independently from different distributions and presented one at a time. The algorithm may pick at most one of
these values in an online manner, given some knowledge of the distributions, such as the expectation of the maximum of
these values. The goal is to maximize the value picked. See Reference [21].
Journal of the ACM, Vol. 66, No. 1, Article 7. Publication date: January 2019.
Near Optimal Online Algorithms and Fast Approximation Algorithms 7:5
Third Result: Fast Approximation Algorithms for Mixed Packing and Covering Integer Programs.
Charles et al. [6] considered the following (offline) problem: Given a lopsided bipartite graph G =
(L, R, E), that is, a bipartite graph wherem = |L||R| = n, does there exist an assignment M : L ‚Üí
R with (j, M(j)) ‚àà E for all j ‚àà L, and such that for every vertex i ‚àà R, |M‚àí1 (i)| ‚â• Bi for some given
values Bi . Even though this is a classic problem in combinatorial optimization with well known
polynomial time algorithms, the instances of interest are too large to use traditional approaches
to solve this problem. (The value of m in particular is very large.) The approach used by Charles
et al. [6] was to essentially design an online algorithm in the i.i.d. model: Choose vertices from L
uniformly at random and assign them to vertices in R in an online fashion. The online algorithm
is guaranteed to be close to optimal, as long as sufficiently many samples are drawn. Therefore it
can be used to solve the original problem (approximately): The online algorithm gets an almost
satisfying assignment if and only if the original graph has a satisfying assignment (with high
probability).
The third result in this article is a generalization of this result to get fast approximation algorithms
for a wide class of mixed packing and covering integer programs (IPs) inspired by problems in the
resource allocation framework (Theorem 2.5). Problems in the resource allocation framework where
the instances are too large to use traditional algorithms occur fairly often, in particular in the management of display advertising systems, where these algorithms are being used. Formal statements
and a more detailed discussion are presented in Section 2.4.
High-level Description of Techniques. The underlying idea used for all these results can be summarized at a high level thusly: Consider a hypothetical algorithm called Hypothetical-Oblivious
that knows the distribution from which the input is drawn and uses an optimal solution w.r.t. this
distribution. Now suppose that we can analyze the performance of Hypothetical-Oblivious by considering a potential function and showing that it decreases by a certain amount in each step. Now
we can design an algorithm that does not know the distribution as follows: Consider the same potential function, and in every step choose the option that minimizes the potential function. Since
the algorithm minimizes the potential in each step, the decrease in the potential for this algorithm
is better than that for Hypothetical-Oblivious, and hence we obtain the same guarantee as that for
Hypothetical-Oblivious. The choice of potential function varies across the results; also, whether
we minimize or maximize the the potential function varies across the results.
For instance, in our first result (Theorem 2.2), the performance of Hypothetical-Oblivious is
analyzed using Chernoff bounds. The Chernoff bounds are proven by showing bounds on the expectation of the moment generating function of a random variable. Thus the potential function
is the sum of the moment generating functions for all the random variables to which we apply
the Chernoff bounds. The proof shows that in each step this potential function decreases by some
multiplicative factor. The algorithm is then designed to achieve the same decrease in the potential function. A particularly pleasing aspect about this technique is that we obtain very simple
proofs; e.g., the proof of the second result mentioned above (that greedy is 1 ‚àí 1/e competitive,
Theorem 2.4) is extremely simple: The potential function in this case is simply the total amount of
unused budgets, and we show that this amount (in expectation) decreases by a factor of 1 ‚àí 1/m
in each step where there are m steps in all.
Multiplicative-Weight Updates. Our techniques and the resulting algorithms for our first and
third results (Theorem 2.2 and Theorem 2.5) are similar to the algorithms of Young [22, 23] for
derandomizing randomized rounding and the fast approximation algorithms for solving covering/packing linear programs (LPs) of Plotkin, Shmoys, and Tardos [20], Garg and Koenemann [12],
and Fleischer [11]. In fact, Arora et al. [4] showed that all these algorithms are related to the multiplicative weights update method for solving the experts problem and especially highlighted the
Journal of the ACM, Vol. 66, No. 1, Article 7. Publication date: January 2019.
7:6 N. R. Devanur et al.
similarity between the potential function used in the analysis of the multiplicative update method
and the moment generating function used in the proof of Chernoff bounds and Young‚Äôs algorithms.
Hence it is no surprise that our algorithm that uses Chernoff bounds is also a multiplicative update
algorithm. Our algorithm is closer in spirit to Young‚Äôs algorithms than others. The main difference
is that our algorithm solves an online problem, rather than an offline one, and hence will run short
of essential distribution dependent parameters to run the multiplicative weights-based algorithm
directly: We show that these parameters can be estimated near optimally. And, further, we introduce more adversarial models of online input, namely the varying ASI models, and come up
with varying levels of knowledge of the distribution that are sufficient to be able to design good
algorithms for these models. And for the offline case, a basic difference of our algorithm from this
previous set of results is that our algorithm uses the special structure of the polytope
k xj,k ‚â§ 1
(as against the more general polytopes in these works) in giving a more efficient solution. For
instance, for our offline problem the number of oracle calls required will have a quadratic dependence on Œ≥m if we used the Plotkin et al. [20] algorithm, whereas using the special structure of the
polytope, we obtain a linear dependence on Œ≥m.
It is possible that our algorithm can also be interpreted as an algorithm for the experts problem.
In fact, Mehta et al. [19] asked if there is a 1 ‚àí o(1) competitive algorithm for Adwords in the i.i.d.
model with small bid to budget ratio, and in particular if the algorithms for experts could be used.
They also conjectured that such an algorithm would iteratively adjust a budget discount factor
based on the rate at which the budget is spent. Our algorithms for resource allocation problem
when specialized for Adwords look exactly like that, but we do not provide formal connections to
the experts framework. This was done in follow-up works [1, 14] that showed that essentially the
same algorithm as ours can be thought of as using a subroutine of Multiplicative-weight updates
on a suitably defined learning with experts problem.
Follow-up Work. There has been a number of follow-up articles since the conference version of
this article has been published. Alaei et al. [3] show that for the Adwords problem with a known
distribution, it is enough for Œ≥ to be O(œµ2) to get a 1 ‚àí œµ approximation. Simultaneously, Devanur
et al. [8] showed the same dependence of Œ≥ = O(œµ2) for the Adwords problem but requiring only
a few parameters from the distribution. Kapralov et al. [17] study a generalized version of the adwords problem where an advertiser‚Äôs profit, instead of being budget additive, could be an arbitrary
submodular function of the queries assigned to him. For this problem in the worst-case setting,
they show that no algorithm can obtain better than a 1
2 approximation, which the greedy algorithm
already achieves. For the same problem in the i.i.d. setting, they show, using techniques we develop
in this work, that the greedy algorithm obtains a 1 ‚àí 1
e approximation. Kesselheim et al. [18] gave
similar guarantees as us, for the random permutation model (i.i.d. without replacement), and also
get the improved bound of Œ≥ = O(œµ2) for the special case of the Adwords problem. However, the algorithms in Reference [18] are computationally expensive, requiring us to solve a linear program
for serving every single request, whereas our algorithm performs a much simpler optimization
in every step: For the adwords problem, for instance, it takes only a linear time to perform each
step‚Äôs optimization. Both Agrawal and Devanur [1] and Gupta and Molinaro [14] showed that essentially the same algorithm as ours also works for the random permutation model, with the same
guarantees, while also relating it formally to the learning from experts framework. Agrawal and
Devanur [1] also greatly generalize the resource allocation framework to handle arbitrary concave
objectives and convex constraints. Eghbali et al. [9] interpret our algorithm as an exponentiated
sub-gradient algorithm, show that it works for the random permutation model, and give a slight
generalization to handle additively separable concave reward functions.
Journal of the ACM, Vol. 66, No. 1, Article 7. Publication date: January 2019. 
Near Optimal Online Algorithms and Fast Approximation Algorithms 7:7
2 PRELIMINARIES AND MAIN RESULTS
2.1 Resource Allocation Framework
We consider the following framework of optimization problems. There are n resources, with resource i ‚àà A having a capacity ofci . There are m requests; each request j ‚àà J can be satisfied by
a vector xj ‚àà {0, 1}
K , with coordinates xj,k , such that
k xj,k ‚â§ 1. Think of vector xj as picking a
single option to satisfy a request from a total of K options. We use K to denote the set of options.
The vector xj consumes ai,j ¬∑ xj amount of resource i and gives wi,j ¬∑ xj amount of type i profit.5
The ai,j ‚Äôs and wi,j ‚Äôs are non-negative vectors of length K (and so are the xj ‚Äôs). The co-ordinates
of the vectors ai,j and wi,j will be denoted by aijk and wijk , respectively, i.e., the kth option consumes aijk amount of resource i and gives a type i profit of wijk . The objective is to maximize
the minimum among all types of profit subject to the capacity constraints on the resources. The
following is the linear program relaxation of the resource allocation problem:
Maximize min
i ‚ààA

j ‚ààJ
wi,j ¬∑ xj s.t.
‚àÄi ‚àà A,

j ‚ààJ
ai,j ¬∑ xj ‚â§ ci
‚àÄ j ‚àà J,

k ‚ààK
xj,k ‚â§ 1
‚àÄ j ‚àà J, k ‚àà K , xj,k ‚â• 0.
Note that dropping a request by not picking any option at all is feasible, too. For expositional
convenience, we will denote not picking any option at all as having picked the ‚ä• option (‚ä• may
not be in the set K ) for which aij‚ä• = 0 and wij‚ä• = 0 for all i, j.
We consider two versions of the above problem. The first is an online version with stochastic
input: Requests are drawn from an unknown distribution. The second is an offline problem when
the number of requests is much larger than the number of resources, and our goal is to design a
fast PTAS for the problem.
2.2 Near-Optimal Online Algorithm for Resource Allocation
We now consider an online version of the resource allocation framework. Here requests arrive
online. We consider the i.i.d. model, where each request is drawn independently from a given distribution. The distribution is unknown to the algorithm. The algorithm knowsm, the total number
of requests. To define our benchmark, we now define the expected instance.
Expected Instance. Consider the following expected instance of the problem, where everything
happens as per expectation. It is a single offline instance that is a function of the given distribution
over requests and the total number of requests m. Every request in the support of the distribution
is also a request in this instance. The capacities of the resources in this instance are the same as in
the original instance. Suppose request j has a probability pj of arriving in the given distribution.
The resource consumption of j in the expected instance is given by mpjai,j for all i and the type i
profit is mpjwi,j . The intuition is that if the requests were drawn from this distribution, then the
expected number of times request j is seen is mpj . To summarize, the LP relaxations of a random
instance with set of requests R, and the expected instance E are as follows (slightly rewritten for
convenience):
5While this notation seems to imply that the number of resource types is equal to the number/set of profit types, namely
n, this choice was made purely to reduce clutter in notation. In general the number/set of resource types could be different
from that of the number of profit types, and it is straightforward to verify that our proofs go through for the general case.
Journal of the ACM, Vol. 66, No. 1, Article 7. Publication date: January 2019.    
7:8 N. R. Devanur et al.
LP relaxations for random and expected instances (1)
Random Instance R Expected Instance E
Maximize Œª s.t. Maximize Œª s.t.
‚àÄ i ‚àà A,

j ‚ààR,k ‚ààK wijkxj,k ‚â• Œª ‚àÄ i ‚àà A,

j ‚ààJ,k ‚ààK mpjwijkxj,k ‚â• Œª
‚àÄ i ‚àà A,

j ‚ààR,k ‚ààK aijkxj,k ‚â§ ci ‚àÄ i ‚àà A,

j ‚ààJ,k ‚ààK mpjaijkxj,k ‚â§ ci
‚àÄ j ‚àà R,

k ‚ààK xj,k ‚â§ 1 ‚àÄ j ‚àà J,

k ‚ààK xj,k ‚â§ 1
‚àÄ j ‚àà R, k ‚àà K , xj,k ‚â• 0. ‚àÄ j ‚àà J, k ‚àà K , xj,k ‚â• 0.
We now prove that the fractional optimal solution to the expected instanceWE is an upper bound
on the expectation of WR, where WR is the offline fractional optimum of the actual sequence of
requests in a random instance R.
Lemma 2.1. WE ‚â• E[WR].
Proof. The average of optimal solutions for all possible sequences of requests is a feasible
solution to the expected instance with a profit equal to E[WR]. Thus the optimal profit for the
expected instance could only be larger.
The approximation factor of an algorithm in the i.i.d. model is defined as the ratio of the expected profit of the algorithm to the fractional optimal profit WE for the expected instance. Let
Œ≥ = max({
aijk
ci }i,j,k ‚à™ { wijk
WE }i,j,k ) be the parameter capturing the significance of any one request
when compared to the total set of requests that arrive online. The main result is that as Œ≥ tends to
zero, the approximation factor ratio tends to 1. In fact, we give the almost optimal tradeoff.
Theorem 2.2. For any œµ ‚â• 1/m, Algorithm 2 achieves an objective value of WE (1 ‚àí O(œµ )) for the
online resource allocation problem with probability at least 1 ‚àí œµ, assuming Œ≥ = O( œµ 2
log(n/œµ ) ). Algorithm 2 does not require any knowledge of the distribution at all.
Theorem 2.3. There exist instances with Œ≥ = œµ 2
log(n) such that no algorithm, even with complete
knowledge of the distribution, can get a 1 ‚àí o(œµ ) approximation factor.
Oracle Assumption. We assume that we have the following oracle available to us: Given a request
j and a vector v, the oracle returns the vector xj that maximizes v.xj among all xjs in {0, 1}
K
satisfying
k ‚ààK xj,k ‚â§ 1. This assumption boils down to being able to find the maximum among
K numbers, but K may be exponential in some cases. For the Adwords and display ads problem
(described below), K is actually equal to n, and this is trivial. For network routing (described in
Section 7), K could be exponential in the size of the network, and this assumption corresponds
to being able to find the shortest path in a graph in polynomial time. For combinatorial auctions
(described in Section 7), this corresponds to the demand query assumption: Given prices on various
items, the buyer should be able to decide in polynomial time which bundle gives her the maximum
utility. (While this is not always achievable in polynomial time, there cannot be any hope of a
posted pricing solution for combinatorial auction without this minimum assumption.)
Extensions and Special Cases. The extensions of Theorem 2.2 to the various generalizations of the
i.i.d. model, including the adversarial stochastic input model, are presented in Section 3.5. We refer
the reader to Section 7 for a discussion on several problems that are special cases of the resource
Journal of the ACM, Vol. 66, No. 1, Article 7. Publication date: January 2019.        
Near Optimal Online Algorithms and Fast Approximation Algorithms 7:9
allocation framework and have been previously considered. Here, we discuss two special cases:
the Adwords problem and the display ads problem.
(1) Adwords. In the adwords problem, there are n advertisers with a daily budget of Bi for
advertiseri. There are m keywords/queries that arrive online, and advertiseri has a bid of
bij for query j. This is a special case of the resource allocation framework where the set
of options K matches the set of resources/advertisers A, i.e., each query can be given to
at most one advertiser and will consume budget just from that advertiser. Let xij denote
the indicator variable for whether or not query j was allocated to agent i. After all allocation is over, agent i pays min(

j ‚ààJ bijxij, Bi ), i.e., the minimum of the sum of the bids
for queries allocated to i and his budget Bi . The objective is to maximize the sum of the
payments from all advertisers‚Äîthis is again a special case of the resource allocation framework where this only a single profit type, and we just want to maximize it. One could raise
a technical objection that this is not a special case of the resource allocation framework,
because the budget constraint is not binding: The value of the allocated bids to an advertiser can exceed his budget, although the total payment from the advertiser will be at most
the budget. But it is not difficult to see that the LP relaxation of the offline problem can be
written as in LP (2), which is clearly a special case of resource allocation framework LP.
Note that the benchmark is anyway an upper bound even on the expected optimal fractional solution. Therefore, any algorithm that gets an Œ± approximation factor for resource
allocation is also guaranteed to get the same approximation factor for Adwords. The only
notable thing being that an algorithm for resource allocation when used for adwords will
treat the budget constraints as binding and obtain the guarantee promised in Theorem 2.2.
(In our 1 ‚àí 1/e approximation algorithm for adwords in Section 5 that holds for all values
of Œ≥ (‚â§1 of course), we use this facility to exceed budget.)
(2) Display Ads. In the display ads problem, there are n advertisers andm impressions arrive
online. Advertiseri has wantsci impressions in total and paysvij for impression j and will
get paid a penalty of œÅi for every undelivered impression. If over-delivered, then he will
pay his bid for the first ci impressions delivered. Letting bij = vij + œÅi , we can write the
LP relaxation of the offline display ads problem as in LP (2), which is clearly a special case
of the resource allocation LP, where just like the Adwords special case the set of options
K is equal to the set of resources/advertisers A, and there is only a single profit type.
LP relaxations for Adwords and Display Ads (2)
Adwords Display Ads
Maximize
i ‚ààA,j ‚ààJ bijxij s.t. Maximize
i ‚ààA,j ‚ààJ bijxij s.t.
‚àÄ i ‚àà A,

j ‚ààJ bijxij ‚â§ Bi ‚àÄ i ‚àà A,

j ‚ààJ xij ‚â§ ci
‚àÄ j ‚àà J,

i ‚ààA xij ‚â§ 1 ‚àÄ j ‚àà J,

i ‚ààA xij ‚â§ 1
‚àÄ i ‚àà A, j ‚àà J, xij ‚â• 0. ‚àÄ i ‚àà A, j ‚àà J, xij ‚â• 0.
2.3 Greedy Algorithm for Adwords
As noted in the Introduction, the greedy algorithm is widely implemented due to its simplicity, but
its performance was known to be only a 1/2 approximation even in stochastic models. We show
that the greedy algorithm obtains a 1 ‚àí 1/e approximation for all Œ≥ , i.e., 0 ‚â§ Œ≥ ‚â§ 1.
Journal of the ACM, Vol. 66, No. 1, Article 7. Publication date: January 2019.       
7:10 N. R. Devanur et al.
Theorem 2.4. The greedy algorithm achieves an approximation factor of 1 ‚àí 1/e for the Adwords
problem in the i.i.d. unknown distributions model for all Œ≥ , i.e., 0 ‚â§ Œ≥ ‚â§ 1.
We note here that the competitive ratio of 1 ‚àí 1/e is tight for the greedy algorithm [13]. It is,
however, not known to be tight for an arbitrary algorithm.
2.4 Fast Approximation Algorithms for Large Mixed Packing
and Covering Integer Programs
Charles et al. [6] consider the following problem: Given a bipartite graph G = (L, R, E), where
m = |L||R| = n, does there exist an assignment M : L ‚Üí R with (j, M(j)) ‚àà E for all j ‚àà L and
such that for every vertex i ‚àà R, |M‚àí1 (i)| ‚â• Bi for some given values Bi . Sincem is very large classic
matching algorithms are not useful. Charles et al. [6] gave an algorithm that runs in time linear6
in the number of edges of an induced subgraph obtained by taking a random sample from L of size
O( m log n
mini {Bi }œµ 2 ), for a gap-version of the problem with gap œµ. Such an algorithm is very useful in a
variety of applications involving ad assignment for online advertising, particularly when mini{Bi}
is large.
We consider a generalization of the above problem inspired by the resource allocation framework. In fact, we consider the following mixed covering-packing integer program. Suppose that
there are n packing constraints, one for each i ‚àà [n] of the form m
j=1 ai,j ¬∑ xj ‚â§ ci and n covering
constraints, one for each i ‚àà [n] of the form m
j=1 wi,j ¬∑ xj ‚â• di . Each xj (with coordinates xj,k ) is
constrained to be in {0, 1}
K and satisfy
k xj,k ‚â§ 1. The ai,j ‚Äôs and wi,j ‚Äôs (and hence xj ‚Äôs) are nonnegative vectors of length K with coordinates aijk and wijk . Does there exist a feasible solution to
this system of constraints? The gap-version of this problem is as follows. Distinguish between the
two cases, with a high probability, say 1 ‚àí Œ¥:
‚Ä¢ YES: There is a feasible solution.
‚Ä¢ NO: There is no feasible solution even if all the ci ‚Äôs are multiplied by 1 + œµ and all the di ‚Äôs
are multiplied by 1 ‚àí œµ.
We note that solving (offline) an optimization problem in the resource allocation framework can
be reduced to the above problem through a binary search on the objective function value,
Let Œ≥ = max({
aijk
ci }i,j,k ‚à™ { wijk
di }i,j,k ).
Theorem 2.5. For any œµ > 0, assuming Œ≥ = O( œµ 2
log(n/œµ ) ), Algorithm 5 solves the gap version of the
mixed covering-packing integer program with Œò( Œ≥m log(n/Œ¥ )
œµ 2 ) oracle calls.
2.5 Chernoff Bounds
We present here the form of Chernoff bounds that we use throughout the rest of this article. Let
X =
i Xi , where Xi ‚àà [0, B] are i.i.d. random variables. Let E[X] = Œº. Then, for all œµ > 0,
Pr[X < Œº(1 ‚àí œµ )] < exp 
‚àíœµ2Œº
2B

.
Consequently, for all Œ¥ > 0, with probability at least 1 ‚àí Œ¥,
X ‚àí Œº ‚â• ‚àí
2ŒºB ln(1/Œ¥ ).
6In fact, the algorithm makes a single pass through this graph.
Journal of the ACM, Vol. 66, No. 1, Article 7. Publication date: January 2019.    
Near Optimal Online Algorithms and Fast Approximation Algorithms 7:11
Similarly, for all œµ ‚àà [0, 2e ‚àí 1],
Pr[X > Œº(1 + œµ )] < exp 
‚àíœµ2Œº
4B

.
Consequently, for all Œ¥ > exp(
‚àí(2e‚àí1)
2Œº
4B ), with probability at least 1 ‚àí Œ¥,
X ‚àí Œº ‚â§

4ŒºB ln(1/Œ¥ ).
For œµ > 2e ‚àí 1,
Pr[X > Œº(1 + œµ )] < 2‚àí(1+œµ )Œº/B .
3 NEAR-OPTIMAL PRIOR ROBUST ONLINE ALGORITHMS
FOR RESOURCE ALLOCATION
For convenience, we begin by rewriting the LP relaxation of a random instance R of the online
resource allocation problem and the expected instance (already defined in Section 2.2 as LP (1)).
LPs for random and expected instances (3)
Random Instance R Expected Instance E
Maximize Œª s.t. Maximize Œª s.t.
‚àÄ i ‚àà A,

j ‚ààR,k ‚ààK wijkxj,k ‚â• Œª ‚àÄ i ‚àà A,

j ‚ààJ,k ‚ààK mpjwijkxj,k ‚â• Œª
‚àÄ i ‚àà A,

j ‚ààR,k ‚ààK aijkxj,k ‚â§ ci ‚àÄ i ‚àà A,

j ‚ààJ,k ‚ààK mpjaijkxj,k ‚â§ ci
‚àÄ j ‚àà R,

k ‚ààK xj,k ‚â§ 1 ‚àÄ j ‚àà J,

k ‚ààK xj,k ‚â§ 1
‚àÄ j ‚àà R, k ‚àà K , xj,k ‚â• 0. ‚àÄ j ‚àà J, k ‚àà K , xj,k ‚â• 0.
We showed in Lemma 2.1 that WE ‚â• E[WR]. All our approximation guarantees are w.r.t. the
stronger benchmark of WE , which is the optimal fractional solution of the expected instance. We
would like to remind the reader that while the benchmark is allowed to be fractional, the online
algorithm of course is allowed to find only integral solutions.
We divide the rest of this section into four subsections. The subsections progressively weaken
the assumptions on knowledge of the distribution of the input.
(1) In Section 3.1, we develop a hypothetical algorithm called Hypothetical-ObliviousConservative, denoted by P
, that achieves an objective value of WE (1 ‚àí 2œµ ) w.p. at
least 1 ‚àí œµ assuming Œ≥ = O( œµ 2
log(n/œµ ) ). Theorem 3.1 is the main result of this section.
The algorithm is hypothetical, because it assumes knowledge of the entire distribution,
whereas the goal of this article is to develop algorithms that work without distributional
knowledge.
(2) In Section 3.2, we design an algorithm for the online resource allocation problem
that achieves the same guarantee as the Hypothetical-Oblivious-Conservative algorithm
P
, without any knowledge of the distribution except for a single parameter of the
distribution‚Äîthe value of WE . Theorem 3.2 is the main result of this section.
(3) In Section 3.3 we design an algorithm for the online resource allocation problem
that achieves an objective value of at least WE (1 ‚àí O(œµ )) w.p. at least 1 ‚àí œµ assuming
Œ≥ = O( œµ 2
log(n/œµ ) ) without any knowledge at all about the distribution. The algorithm in
Journal of the ACM, Vol. 66, No. 1, Article 7. Publication date: January 2019.        
7:12 N. R. Devanur et al.
Section 3.2 serves as a good warm-up for the algorithm in this section. Theorem 2.2 is the
main result of this section.
(4) In Section 3.5, we relax the assumption that the distribution from which the requests
are drawn is i.i.d.; we give three different generalizations of the i.i.d. model with strong
revenue guarantees as in the i.i.d. model.
3.1 Completely Known Distributions
When the distributions are completely known, we first compute the expected instance and solve
its LP relaxation (LP (3)) optimally. Let x‚àó
jk denote the optimal solution to the expected LP (3). The
Hypothetical-Oblivious algorithm P works as follows: When request j arrives, it serves it using
option k with probability x‚àó
jk . Let X‚àó
i,t denote the amount of resource i consumed in step t for the
algorithm P. Thus the total amount of resource i consumed over the entire m steps of algorithm
P is m
t=1 X‚àó
i,t . Note that E[X‚àó
i,t] =
j,k pjaijkx‚àó
jk ‚â§ ci
m . Thus, we can bound the probability that
Pr[
m
t=1 X‚àó
i,t ‚â• ci (1 + œµ )] using Chernoff bounds. We explicitly derive this bound here, since we
use this derivation in designing the algorithm in Section 3.2.
Since we cannot exceed ci amount of resource consumption by any non-zero amount, we need
to be more conservative than P. So we analyze the following algorithm P
, called HypotheticalOblivious-Conservative, instead of P: When request j arrives, it serves it using option k with
probability x‚àó
jk
1+œµ , where œµ is an error parameter of algorithm designer‚Äôs choice. Let X
i,t denote
the amount of resource i consumed in step t for the algorithm P
. Note that E[X
i,t] ‚â§ ci
(1+œµ )m .
Thus, even with a (1 + œµ ) deviation using Chernoff bounds, the resource consumption is at
most ci .
We begin by noting that X
i,t ‚â§ Œ≥ci by the definition of Œ≥ . For all œµ ‚àà [0, 1], we have
Pr
‚é°
‚é¢
‚é¢
‚é¢
‚é¢
‚é£
m
t=1
X
i,t ‚â• ci
1 + œµ
(1 + œµ )
‚é§
‚é•
‚é•
‚é•
‚é•
‚é¶
= Pr
‚é°
‚é¢
‚é¢
‚é¢
‚é¢
‚é£
m
t=1 X
i,t
Œ≥ci
‚â•
1
Œ≥
‚é§
‚é•
‚é•
‚é•
‚é•
‚é¶
= Pr 	
(1 + œµ )
m
t=1 Xi, t
Œ≥ ci ‚â• (1 + œµ )
1
Œ≥


‚â§ E
	
(1 + œµ )
m
t=1 Xi, t
Œ≥ ci


/(1 + œµ )
1
Œ≥
= E
‚é°
‚é¢
‚é¢
‚é¢
‚é¢
‚é£
m
t=1
(1 + œµ )
Xi, t
Œ≥ ci
‚é§
‚é•
‚é•
‚é•
‚é•
‚é¶
/(1 + œµ )
1
Œ≥
‚â§ E
‚é°
‚é¢
‚é¢
‚é¢
‚é¢
‚é£
m
t=1


1 + œµ
X
i,t
Œ≥ci


‚é§
‚é•
‚é•
‚é•
‚é•
‚é¶
/(1 + œµ )
1
Œ≥
‚â§
‚é°
‚é¢
‚é¢
‚é¢
‚é¢
‚é£
m
t=1

1 + œµ
(1 + œµ )Œ≥m ‚é§
‚é•
‚é•
‚é•
‚é•
‚é¶
/(1 + œµ )
1
Œ≥
‚â§
 eœµ
(1 + œµ )1+œµ
 1
Œ≥ (1+œµ )
‚â§ e
‚àíœµ2
4Œ≥ 1
1+œµ
‚â§ œµ
2n
,
Journal of the ACM, Vol. 66, No. 1, Article 7. Publication date: January 2019.                  
Near Optimal Online Algorithms and Fast Approximation Algorithms 7:13
where the first inequality follows from Markov‚Äôs inequality, the second from convexity of exponential function together with with the fact that X
i,t ‚â§ Œ≥ci , the third from E[X
i,t] ‚â§ ci
(1+œµ )m ,
and the fourth from 1 + x ‚â§ ex ; the fifth is standard for all œµ ‚àà [0, 1], and the sixth follows from
Œ≥ = O(œµ2/ log(n/œµ )) for an appropriate choice of constant inside the big O coupled with n ‚â• 2.
Remark 1. At first sight, this bound might seem anomalous‚Äîthe bound œµ
2n is increasing in œµ,
i.e., the probability of a smaller deviation is smaller than the probability of a larger deviation. The
reason for this anomaly is that Œ≥ is related to œµ as Œ≥ = O( œµ 2
log(n/œµ ) ), and the smaller the Œ≥ , the better
revenue we can get (i.e., more granular requests leads to lesser wastage from errors, and hence
more revenue). Thus a small deviation for small Œ≥ has a smaller probability than a larger deviation
for a larger Œ≥ .
Similarly, let Y
i,t denote the revenue obtained from type i profit in step t for the algorithm P
.
Note that E[Y
i,t] =
j,k pjwijk
x‚àó
jk
1+œµ ‚â• WE
(1+œµ )m . By the definition of Œ≥ , we have Y
i,t ‚â§ Œ≥WE . For all
œµ ‚àà [0, 1] we have
Pr
‚é°
‚é¢
‚é¢
‚é¢
‚é¢
‚é£
m
t=1
Y
i,t ‚â§
WE
1 + œµ
(1 ‚àí œµ )
‚é§
‚é•
‚é•
‚é•
‚é•
‚é¶
= Pr
‚é°
‚é¢
‚é¢
‚é¢
‚é¢
‚é£
m
t=1 Y
i,t
Œ≥WE
‚â§
1 ‚àí œµ
Œ≥ (1 + œµ )
‚é§
‚é•
‚é•
‚é•
‚é•
‚é¶
= Pr 	
(1 ‚àí œµ )
m
t=1 Yi, t
Œ≥WE ‚â• (1 ‚àí œµ ) 1‚àíœµ
Œ≥ (1+œµ )


‚â§ E
	
(1 ‚àí œµ )
m
t=1 Yi, t
Œ≥WE


/(1 ‚àí œµ ) 1‚àíœµ
Œ≥ (1+œµ )
= E
‚é°
‚é¢
‚é¢
‚é¢
‚é¢
‚é£
m
t=1
(1 ‚àí œµ )
Yi, t
Œ≥WE
‚é§
‚é•
‚é•
‚é•
‚é•
‚é¶
/(1 ‚àí œµ ) 1‚àíœµ
Œ≥ (1+œµ )
‚â§ E
‚é°
‚é¢
‚é¢
‚é¢
‚é¢
‚é£
m
t=1


1 ‚àí œµ Y
i,t
Œ≥WE


‚é§
‚é•
‚é•
‚é•
‚é•
‚é¶
/(1 ‚àí œµ ) 1‚àíœµ
Œ≥ (1+œµ )
‚â§
‚é°
‚é¢
‚é¢
‚é¢
‚é¢
‚é£
m
t=1

1 ‚àí œµ
(1 + œµ )Œ≥m ‚é§
‚é•
‚é•
‚é•
‚é•
‚é¶
/(1 ‚àí œµ ) 1‚àíœµ
Œ≥ (1+œµ )
‚â§
 e‚àíœµ
(1 ‚àí œµ )1‚àíœµ
 1
Œ≥ (1+œµ )
‚â§ e
‚àíœµ2
2Œ≥ 1
1+œµ
‚â§ œµ
2n
.
Thus, we have all the capacity constraints satisfied, (i.e.,
i X
i,t ‚â§ ci ), and all resource profits are
at least WE
1+œµ (1 ‚àí œµ ) (i.e.,
i Y
i,t ‚â• WE
1+œµ (1 ‚àí œµ ) ‚â• WE (1 ‚àí 2œµ )), with probability at least 1 ‚àí 2n ¬∑ œµ/2n =
1 ‚àí œµ. This proves the following theorem:
Theorem 3.1. For any œµ > 0, the Hypothetical-Oblivious-Conservative algorithm P
 achieves an
objective value of WE (1 ‚àí 2œµ ) for the online resource allocation problem with probability at least
1 ‚àí œµ, assuming Œ≥ = O( œµ 2
log(n/œµ ) ).
Journal of the ACM, Vol. 66, No. 1, Article 7. Publication date: January 2019.                      
7:14 N. R. Devanur et al.
3.2 Unknown Distribution, Known WE
We now design an algorithm7 A without knowledge of the distribution but just knowing a single
parameter WE . Let AsP
m‚àís be a hybrid algorithm that runs A for the first s steps and P
 for the
remaining m ‚àí s steps. Let œµ ‚àà [0, 1] be the error parameter, which is the algorithm designer‚Äôs
choice. Call the algorithm a failure if at least one of the following fails:
(1) For all i,
m
t=1 X A
i,t ‚â§ ci .
(2) For all i,
m
t=1 Y A
i,t ‚â• WE (1 ‚àí 2œµ ).
For any algorithm A, let the amount of resource i consumed in the tth step be denoted by X A
i,t
and the amount of resource i profit be denoted by Y A
i,t . Let Ss (X A
i ) = s
t=1 X A
i,t denote the amount
of resource i consumed in the first s steps, and let Ss (Y A
i ) = s
t=1 Y A
i,t denote the resource i profit
in the first s steps. Similarly to the derivation in Section 3.1 that bounded the failure probability of
P
, we can bound the failure probability of any algorithm A, i.e.,
Pr
‚é°
‚é¢
‚é¢
‚é¢
‚é¢
‚é£
m
t=1
X A
i,t ‚â• ci
1 + œµ
(1 + œµ )
‚é§
‚é•
‚é•
‚é•
‚é•
‚é¶
= Pr
‚é°
‚é¢
‚é¢
‚é¢
‚é¢
‚é£
m
t=1 X A
i,t
Œ≥ci
‚â•
1
Œ≥
‚é§
‚é•
‚é•
‚é•
‚é•
‚é¶
= Pr
‚é°
‚é¢
‚é¢
‚é¢
‚é¢
‚é£
(1 + œµ )
m
t=1 X A
i, t
Œ≥ ci ‚â• (1 + œµ )
1
Œ≥
‚é§
‚é•
‚é•
‚é•
‚é•
‚é¶
‚â§ E
‚é°
‚é¢
‚é¢
‚é¢
‚é¢
‚é£
(1 + œµ )
m
t=1 X A
i, t
Œ≥ ci
‚é§
‚é•
‚é•
‚é•
‚é•
‚é¶
/(1 + œµ )
1
Œ≥
= E
‚é°
‚é¢
‚é¢
‚é¢
‚é¢
‚é£
m
t=1
(1 + œµ )
X A
i, t
Œ≥ ci
‚é§
‚é•
‚é•
‚é•
‚é•
‚é¶
/(1 + œµ )
1
Œ≥ (4)
Pr
‚é°
‚é¢
‚é¢
‚é¢
‚é¢
‚é£
m
t=1
Y A
i,t ‚â§
WE
1 + œµ
(1 ‚àí œµ )
‚é§
‚é•
‚é•
‚é•
‚é•
‚é¶
= Pr
‚é°
‚é¢
‚é¢
‚é¢
‚é¢
‚é£
m
t=1 Y A
i,t
Œ≥WE
‚â§
1 ‚àí œµ
Œ≥ (1 + œµ )
‚é§
‚é•
‚é•
‚é•
‚é•
‚é¶
= Pr
‚é°
‚é¢
‚é¢
‚é¢
‚é¢
‚é£
(1 ‚àí œµ )
m
t=1 Y A
i, t
Œ≥WE ‚â• (1 ‚àí œµ ) 1‚àíœµ
Œ≥ (1+œµ )
‚é§
‚é•
‚é•
‚é•
‚é•
‚é¶
‚â§ E
‚é°
‚é¢
‚é¢
‚é¢
‚é¢
‚é£
(1 ‚àí œµ )
m
t=1 Y A
i, t
Œ≥WE
‚é§
‚é•
‚é•
‚é•
‚é•
‚é¶
/(1 ‚àí œµ ) 1‚àíœµ
Œ≥ (1+œµ )
= E
‚é°
‚é¢
‚é¢
‚é¢
‚é¢
‚é£
m
t=1
(1 ‚àí œµ )
Y A
i, t
Œ≥WE
‚é§
‚é•
‚é•
‚é•
‚é•
‚é¶
/(1 ‚àí œµ ) 1‚àíœµ
Œ≥ (1+œµ ) . (5)
In Section 3.1, our algorithm A was P
(and therefore we can use E[X
i,t] ‚â§ ci
(1+œµ )m and E[Y
i,t] ‚â•
WE
(1+œµ )m ), the total failure probability that is the sum of Equations (4) and (5) for all the i‚Äôs would
have been n ¬∑ [ œµ
2n + œµ
2n ] = œµ. The goal is to design an algorithm A for stage r that, unlike P, does
not know the distribution and knows justWE but obtains the same œµ failure probability. That is we
7Note that the notation A that we use for the set of advertisers/resources is different from the non-calligraphic A that we
use for an algorithm. Also, it is immediate from the context which one we are referring to.
Journal of the ACM, Vol. 66, No. 1, Article 7. Publication date: January 2019.                  
Near Optimal Online Algorithms and Fast Approximation Algorithms 7:15
want to show that the sum of Equations (4) and (5) over all i‚Äôs is at most œµ:

i
E
	
m
t=1 (1 + œµ )
X A
i, t
Œ≥ ci


(1 + œµ )
1
Œ≥
+

i
E
	
m
t=1 (1 ‚àí œµ )
Y A
i, t
Œ≥WE


(1 ‚àí œµ ) 1‚àíœµ
Œ≥ (1+œµ )
‚â§ œµ.
For the algorithm AsP
m‚àís , the above quantity can be rewritten as

i
E
	
(1 + œµ )
Ss (X A
i ) Œ≥ ci
m
t=s+1 (1 + œµ )
Xi, t
Œ≥ ci


(1 + œµ )
1
Œ≥
+

i
E
	
(1 ‚àí œµ )
Ss (Y A
i )
Œ≥WE
m
t=s+1 (1 ‚àí œµ )
Yi, t
Œ≥WE


(1 ‚àí œµ ) 1‚àíœµ
Œ≥ (1+œµ )
,
which, by using (1 + œµ )
x ‚â§ 1 + œµx for 0 ‚â§ x ‚â§ 1, is in turn upper bounded by

i
E
	
(1 + œµ )
Ss (X A
i ) Œ≥ ci
m
t=s+1
	
1 + œµ Xi, t
Œ≥ ci

 

(1 + œµ )
1
Œ≥
+

i
E
	
(1 ‚àí œµ )
Ss (Y A
i )
Œ≥WE
m
t=s+1
	
1 ‚àí œµ Y
i, t
Œ≥WE

 

(1 ‚àí œµ ) 1‚àíœµ
Œ≥ (1+œµ )
.
Since for allt, the random variablesX
i,t ,X A
i,t ,Y
i,t , andY A
i,t are all independent, and E[X
i,t] ‚â§ ci
(1+œµ )m
and E[Y
i,t] ‚â• WE
(1+œµ )m , the above is in turn upper bounded by

i
E
	
(1 + œµ )
Ss (X A
i ) Œ≥ ci

1 + œµ
(1+œµ )Œ≥m m‚àís


(1 + œµ )
1
Œ≥
+

i
E
	
(1 ‚àí œµ )
Ss (Y A
i )
Œ≥WE

1 ‚àí œµ
(1+œµ )Œ≥m m‚àís


(1 ‚àí œµ ) 1‚àíœµ
Œ≥ (1+œµ )
. (6)
Let F [AsP
m‚àís ] denote the quantity in Equation (6), which is an upper bound on failure probability of the hybrid algorithm AsP
m‚àís . By Theorem 3.1, we know that F [P
m] ‚â§ œµ. We now prove that
for all s ‚àà {0, 1,...,m ‚àí 1}, F [As+1P
m‚àís‚àí1] ‚â§ F [AsP
m‚àís ], thus proving that F [Am] ‚â§ œµ, i.e., running the algorithmAfor all them steps results in a failure with probability at most œµ. To design such
anA, we closely follow the derivation of Chernoff bounds, which is what established that F [P
m] ‚â§
œµ in Theorem 3.1. However the design process will reveal that, unlike algorithm P
that needs the
entire distribution, just the knowledge of WE will do for bounding the failure probability by œµ.
Assuming that for all s < p, the algorithm A has been defined for the first s + 1 steps in such a
way that F [As+1P
m‚àís‚àí1] ‚â§ F [AsP
m‚àís ], we now define A for the p + 1th step in a way that will
ensure that F [Ap+1P
m‚àíp‚àí1] ‚â§ F [ApP
m‚àíp ]. We have
F [Ap+1
P
m‚àíp‚àí1
] =

i
E
	
(1 + œµ )
Sp+1 (X A
i ) Œ≥ ci

1 + œµ
(1+œµ )Œ≥m m‚àíp‚àí1


(1 + œµ )
1
Œ≥
+

i
E
	
(1 ‚àí œµ )
Sp+1 (Y A
i )
Œ≥WE

1 ‚àí œµ
(1+œµ )Œ≥m m‚àíp‚àí1


(1 ‚àí œµ ) 1‚àíœµ
Œ≥ (1+œµ )
‚â§

i
E
	
(1 + œµ )
Sp (X A
i ) Œ≥ ci

1 + œµ
X A
i,p+1
Œ≥ ci
 
1 + œµ
(1+œµ )Œ≥m m‚àíp‚àí1


(1 + œµ )
1
Œ≥
Journal of the ACM, Vol. 66, No. 1, Article 7. Publication date: January 2019.                                
7:16 N. R. Devanur et al.
+

i
E
	
(1 ‚àí œµ )
Sp (Y A
i )
Œ≥WE

1 ‚àí œµ
Y A
i,p+1
Œ≥WE
 
1 ‚àí œµ
(1+œµ )Œ≥m m‚àíp‚àí1


(1 ‚àí œµ ) 1‚àíœµ
Œ≥ (1+œµ )
. (7)
Define
œïi,s = 1
ci
	 (1 + œµ )
Ss (X A
i ) Œ≥ ci

1 + œµ
(1+œµ )Œ≥m m‚àís‚àí1
(1 + œµ )
1
Œ≥


œài,s = 1
WE
	 (1 ‚àí œµ )
Ss (Y A
i )
Œ≥WE

1 ‚àí œµ
(1+œµ )Œ≥m m‚àís‚àí1
(1 ‚àí œµ ) 1‚àíœµ
Œ≥ (1+œµ )


.
Define the step p + 1 of algorithm A as picking the following option k‚àó for request j, where
k‚àó = arg mink
‚éß‚é™
‚é®
‚é™
‚é©

i
aijk ¬∑ œïi,p ‚àí

i
wijk ¬∑ œài,p
‚é´‚é™
‚é¨
‚é™
‚é≠
. (8)
For the sake of clarity, the entire algorithm is presented in Algorithm 1.
ALGORITHM 1: Algorithm for stochastic online resource allocation with unknown distribution,
known WE
Input: Capacities ci for i ‚àà [n], the total number of requests m, the values of Œ≥ and WE , an error
parameter œµ > 0.
Output: An online allocation of resources to requests
1: Initialize œïi,0 = 1
ci
	 
1+ œµ
(1+œµ )Œ≥m m‚àí1
(1+œµ )
1
Œ≥


, and, œài,0 = 1
WE
	 
1‚àí œµ
(1+œµ )Œ≥m m‚àí1
(1‚àíœµ ) 1‚àíœµ Œ≥ (1+œµ )


2: for s = 1 to m do
3: If the incoming request is j, then use the following option k‚àó:
k‚àó = arg min k ‚ààK ‚à™{‚ä•}
‚éß‚é™
‚é®
‚é™
‚é©

i
aijk ¬∑ œïi,s‚àí1 ‚àí

i
wijk ¬∑ œài,s‚àí1
‚é´‚é™
‚é¨
‚é™
‚é≠
.
4: X A
i,s = aijk‚àó , Y A
i,s = wijk‚àó
5: Update œïi,s = œïi,s‚àí1 ¬∑
‚é°
‚é¢
‚é¢
‚é¢
‚é¢
‚é¢
‚é£
(1+œµ )
X A
i,s Œ≥ ci
1+ œµ
(1+œµ )Œ≥m
‚é§
‚é•
‚é•
‚é•
‚é•
‚é•
‚é¶
, and, œài,s = œài,s‚àí1 ¬∑
‚é°
‚é¢
‚é¢
‚é¢
‚é¢
‚é¢
‚é£
(1‚àíœµ )
Y A
i,s Œ≥WE
1‚àí œµ
(1+œµ )Œ≥m
‚é§
‚é•
‚é•
‚é•
‚é•
‚é•
‚é¶
6: end for
By the definition of step p + 1 of algorithm A given in Equation (8), it follows that for any
two algorithms with the first p steps being identical, and the last m ‚àí p ‚àí 1 steps following the
Hypothetical-Oblivious-Conservative algorithm P
, algorithm A‚Äôs p + 1th step is the one that minimizes expression (7). In particular it follows that expression (7) is upper bounded by the same
expression where the p + 1th step is according to X
i,p+1 and Y
i,p+1, i.e., we replace X A
i,p+1 by X
i,p+1
Journal of the ACM, Vol. 66, No. 1, Article 7. Publication date: January 2019.         
Near Optimal Online Algorithms and Fast Approximation Algorithms 7:17
and Y A
i,p+1 by Y
i,p+1. Therefore, we have
F [Ap+1
P
m‚àíp‚àí1
] ‚â§

i
E
	
(1 + œµ )
Sp (X A
i ) Œ≥ ci
	
1 + œµ Xi,p+1
Œ≥ ci

 
1 + œµ
(1+œµ )Œ≥m m‚àíp‚àí1


(1 + œµ )
1
Œ≥
+

i
E
	
(1 ‚àí œµ )
Sp (Y A
i )
Œ≥WE
	
1 ‚àí œµ Y
i,p+1
Œ≥WE

 
1 ‚àí œµ
(1+œµ )Œ≥m m‚àíp‚àí1


(1 ‚àí œµ ) 1‚àíœµ
Œ≥ (1+œµ )
‚â§

i
E
	
(1 + œµ )
Sp (X A
i ) Œ≥ ci

1 + œµ
(1+œµ )Œ≥m  1 + œµ
(1+œµ )Œ≥m m‚àíp‚àí1


(1 + œµ )
1
Œ≥
+

i
E
	
(1 ‚àí œµ )
Sp (Y A
i )
Œ≥WE

1 ‚àí œµ
(1+œµ )Œ≥WE
 1 ‚àí œµ
(1+œµ )Œ≥m m‚àíp‚àí1


(1 ‚àí œµ ) 1‚àíœµ
Œ≥ (1+œµ )
= F [ApP
m‚àíp ].
This completes the proof of the following theorem.
Theorem 3.2. For any œµ > 0, Algorithm 1 achieves an objective value ofWE (1 ‚àí 2œµ ) for the online
resource allocation problem with probability at least 1 ‚àí œµ, assuming Œ≥ = O( œµ 2
log(n/œµ ) ). The algorithm
A does not require any knowledge of the distribution except for the single parameter WE .
3.3 Completely Unknown Distribution
We first give a high-level overview of this section before going into the details. In this section, we
design an algorithm A without any knowledge of the distribution at all. The algorithm is similar
in spirit to the one in Section 3.2 except that since we do not have knowledge ofWE , we divide the
algorithm into many stages. In each stage, we run an algorithm similar to the one in Section 3.2
except that instead of WE , we use an estimate of WE that gets increasingly accurate with each
successive stage.
More formally, the algorithm runs in l stages {0, 1,...,l ‚àí 1}, where l is such that œµ2l = 1, and
œµ ‚àà [1/m, 1/2] (we need œµ ‚â§ 1/2 so thatl is at least 1) is the error parameter of algorithm designer‚Äôs
choice. Further, we need m ‚â• 1
œµ so that œµm ‚â• 1. We assume that œµm is an integer for clarity of
exposition. Stage r handles tr = œµm2r requests for r ‚àà {0,... l ‚àí 1}. The first œµm requests are used
just for future estimation, and none of them are served. For convenience, we sometimes call this
pre-zero stage as stage ‚àí1 and let t‚àí1 = œµm. Stage r ‚â• 0 serves t ‚àà [tr + 1,tr+1]. Note that in the
optimal solution to the expected instance of stage r, no resource i gets consumed by more than tr ci
m , and every resource i gets a profit of trWE
m , i.e., consumption and profit have been scaled down
by a factor of tr
m . As in the previous sections, with a high probability, we can only reach close to trWE
m . Further, since stage r consists of only tr requests, which is much smaller than m for small r,
it follows that for small r, our error in how close to we get to trWE
m will be higher. Indeed, instead
of having the same error parameter of œµ in every stage, we set stage-specific error parameters that
get progressively smaller and become close to œµ in the final stages. These parameters are chosen
such that the overall error is still O(œµ ), because the later stages having more requests matter more
than the former. There are two sources of error/failure that we detail below.
Journal of the ACM, Vol. 66, No. 1, Article 7. Publication date: January 2019.         
7:18 N. R. Devanur et al.
(1) The first source of failure stems from not knowing WE . Instead, we estimate a quantity
Zr that is an approximation we use for WE in stage r, and the approximation gets better
as r increases. We use Zr to set a profit target of tr Zr
m for stage r. Since Zr could be much
smaller than WE , our algorithm could become very suboptimal. We prove that with a
probability of at least 1 ‚àí 2Œ¥, we have WE (1 ‚àí 3œµx,r‚àí1) ‚â§ Zr ‚â§ WE (see next for what œµx,r
is), where Œ¥ = œµ
3l . Thus for all the l stages, these bounds are violated with probability at
most 2lŒ¥ = 2œµ/3.
(2) The second source of failure stems from achieving this profit target in every stage. We
set error parameters œµx,r and œµy,r such that for every i stage r consumes at most tr ci
m (1 +
œµx,r ) amount of resource i, and for every i we get a profit of at least tr Zr
m (1 ‚àí œµy,r ), with
probability at least 1 ‚àí Œ¥. Thus the overall failure probability, as regards falling short of
the target tr Zr
m by more than œµy,r and exceeding tr ci
m by more than œµx,r , for all the l stages
together is at most Œ¥ ¬∑ l = œµ/3.
Thus summing over the failure probabilities we get œµ/3 + 2œµ/3 = œµ. We have that with probability at least 1 ‚àí œµ, for every i, the total consumption of resource i is at most l‚àí1 r=0
tr ci
m (1 +
œµx,r ), and total profit from resource i is at least l‚àí1 r=0
trWE
m (1 ‚àí 3œµx,r‚àí1)(1 ‚àí œµy,r ). We set œµx,r = 4Œ≥m ln(2n/Œ¥ )
tr forr ‚àà {‚àí1, 0, 1,...,l ‚àí 1} and œµy,r =
2wmaxm ln(2n/Œ¥ )
tr Zr forr ‚àà {0, 1,...,l ‚àí 1} (we define œµx,r starting from r = ‚àí1, with t‚àí1 = œµm, just for technical convenience). From this it follows that l‚àí1 r=0
tr ci
m (1 + œµx,r ) ‚â§ ci and l‚àí1 r=0
trWE
m (1 ‚àí 3œµx,r‚àí1)(1 ‚àí œµy,r ) ‚â• WE (1 ‚àí O(œµ )), assuming
Œ≥ = O( œµ 2
log(n/œµ ) ). The algorithm is described in Algorithm 2. This completes the high-level overview
of the proof. All that is left to prove is the points 1 and 2 above, on which we would have proved
our main theorem, namely Theorem 2.2, which we recall below. Theorem 2.2 For any œµ ‚â• 1/m,
Algorithm 2 achieves an objective value of WE (1 ‚àí O(œµ )) for the online resource allocation problem with probability at least 1 ‚àí œµ, assuming Œ≥ = O( œµ 2
log(n/œµ ) ). Algorithm 2 does not require any
knowledge of the distribution at all.
Detailed Description and Proof. We begin with the first point in our high-level overview above,
namely by describing how Zr is estimated and proving its concentration around WE . After stage
r (including stage ‚àí1), the algorithm computes the optimal fractional objective value er to the
following instance Ir : The instance has the tr requests of stage r, and the capacity of resource i
is capped at tr ci
m . Using the optimal fractional objective value er of this instance, we set Zr+1 = er
1+œµx,r ¬∑ m
tr . The first task now is to prove that Zr+1 as estimated above is concentrated enough
around WE . This basically requires proving concentration of er .
Lemma 3.3. With a probability at least 1 ‚àí 2Œ¥, we have
trWE
m (1 ‚àí 2œµx,r ) ‚â§ er ‚â§
trWE
m (1 + œµx,r ).
Proof. We prove that the lower and upper bound hold with probability 1 ‚àí Œ¥ each, thus proving
the lemma.
We begin with the lower bound on er . Note that the expected instance of the instance Ir has
the same optimal solution x‚àó
jk as the optimal solution to the full expected instance (i.e., the one
without scaling down by tr
m ). Now consider the algorithm P
(r), which is the same as the P
defined
in Section 3.1 except that œµ is replaced by œµx,r , i.e., it serves request j with option k with probability x‚àó
jk
1+œµx,r . When P
(r) is run on instance Ir , with a probability at least 1 ‚àí Œ¥
2n , at most tr ci
m amount of
resource i is consumed, and with probability at least 1 ‚àí Œ¥
2n , at least trWE
m
1‚àíœµx,r
1+œµx,r resource i profit is
Journal of the ACM, Vol. 66, No. 1, Article 7. Publication date: January 2019.       
Near Optimal Online Algorithms and Fast Approximation Algorithms 7:19
ALGORITHM 2: Algorithm for stochastic online resource allocation with unknown distribution
Input: Capacitiesci for i ‚àà [n], the total number of requests m, the value of Œ≥ , an error parameter
œµ > 1/m.
Output: An online allocation of resources to requests
1: Set l = log(1/œµ )
2: Initialize t‚àí1 : t‚àí1 ‚Üê œµm
3: for r = 0 to l ‚àí 1 do
4: Compute er‚àí1: the optimal solution to the tr‚àí1 requests of stage r ‚àí 1 with capacities capped
at tr‚àí1ci
m .
5: Set Zr = er‚àí1
1+œµx,r‚àí1 ¬∑ m
tr‚àí1
6: Set œµx,r =
4Œ≥m ln(2n/Œ¥ )
tr , œµy,r =
2wmaxm ln(2n/Œ¥ )
tr Zr
7: Set œïr
i,0 = œµx,r
Œ≥ ci
	 
1+ œµx,r
mŒ≥  tr ‚àí1
(1+œµx,r )
(1+œµx,r ) tr mŒ≥ 

, and, œàr
i,0 = œµy,r
wmax 	 
1‚àí œµy,r
mŒ≥  tr ‚àí1
(1‚àíœµy,r )
(1‚àíœµy,r ) tr Zr mwmax 

8: for s = 1 to tr do
9: If the incoming request is j, then use the following option k‚àó:
k‚àó = arg min k ‚ààK ‚à™{‚ä•}
‚éß‚é™
‚é®
‚é™
‚é©

i
aijk ¬∑ œïr
i,s‚àí1 ‚àí

i
wijk ¬∑ œàr
i,s‚àí1
‚é´‚é™
‚é¨
‚é™
‚é≠
.
10: X A
i,tr +s = aijk‚àó , Y A
i,tr +s = wijk‚àó
11: Update œïr
i,s = œïr
i,s‚àí1 ¬∑
‚é°
‚é¢
‚é¢
‚é¢
‚é¢
‚é¢
‚é£
(1+œµx,r )
X A
i, tr +s Œ≥ ci
1+ œµx,r
mŒ≥
‚é§
‚é•
‚é•
‚é•
‚é•
‚é•
‚é¶
, and, œàr
i,s = œàr
i,s‚àí1 ¬∑
‚é°
‚é¢
‚é¢
‚é¢
‚é¢
‚é¢
‚é£
(1‚àíœµy,r )
Y A
i, tr +s wmax
1‚àí œµy,r
mŒ≥
‚é§
‚é•
‚é•
‚é•
‚é•
‚é•
‚é¶
12: end for
13: end for
obtained. Thus with a probability at least 1 ‚àí 2n ¬∑ Œ¥
2n = 1 ‚àí Œ¥, P
(r) achieves an objective value of at
least trWE
m (1 ‚àí 2œµx,r ). Therefore the optimal objective value er will also be at least trWE
m (1 ‚àí 2œµx,r ).
To prove the upper bound, we consider the primal and dual LPs that define er in LP (9) and the
primal and dual LPs defining the expected instance in LP (10). In the latter, for convenience, we
use mpj Œ≤j as the dual multiplier instead of just Œ≤j .
Primal and dual LPs defining er (9)
Primal defining er Dual defining er
Maximize Œª s.t. Minimize
j ‚ààIr Œ≤j + tr
m

i Œ±ici s.t.
‚àÄ i,

j ‚ààIr ,k wijkxj,k ‚â• Œª ‚àÄ j ‚àà Ir, k, Œ≤j +
i (Œ±iaijk ‚àí œÅiwijk ) ‚â• 0
‚àÄ i,

j ‚ààIr ,k aijkxj,k ‚â§ tr ci
m

i œÅi ‚â• 1
‚àÄ j ‚àà Ir,

k xj,k ‚â§ 1 ‚àÄ i, œÅi ‚â• 0, Œ±i ‚â• 0
‚àÄ j ‚àà Ir, k, xj,k ‚â• 0. ‚àÄ j ‚àà Ir, Œ≤j ‚â• 0.
Journal of the ACM, Vol. 66, No. 1, Article 7. Publication date: January 2019.          
7:20 N. R. Devanur et al.
Primal and dual LPs defining the expected instance (10)
Primal for the expected instance Dual for the expected instance
Maximize Œª s.t. Minimize
j mpj Œ≤j +
i Œ±ici s.t.
‚àÄ i,

j,k mpjwijkxj,k ‚â• Œª ‚àÄ j, k, mpj

Œ≤j +
i (Œ±iaijk ‚àí œÅiwijk )

‚â• 0
‚àÄ i,

j,k mpjaijkxj,k ‚â§ ci

i œÅi ‚â• 1
‚àÄ j,

k xj,k ‚â§ 1 ‚àÄ i, œÅi ‚â• 0, Œ±i ‚â• 0
‚àÄ j, k, xj,k ‚â• 0. ‚àÄ j, Œ≤j ‚â• 0,
Note that the set of constraints in the dual of LP (10) is a superset of the set of constraints in the
dual of LP (9), making any feasible solution to dual of LP (10) also feasible to dual of LP (9). In in
particular, the optimal solution to dual of LP (10) given by Œ≤‚àó
j ‚Äôs, Œ±‚àó
i ‚Äôs, and œÅ‚àó
i ‚Äôs is feasible for dual
of LP (9). Hence, the value of er is upper bounded the objective of dual of LP (9) at Œ≤‚àó
j ‚Äôs, Œ±‚àó
i ‚Äôs, and
œÅ‚àó
i ‚Äôs. That is, we have
er ‚â§

j ‚ààIr
Œ≤‚àó
j +
tr
m

i
Œ±‚àó
ici .
We now upper bound the right-hand side by applying Chernoff bounds on
j ‚ààIr Œ≤‚àó
j . Since the dual
LP in LP (10) is a minimization LP, the constraints there imply that Œ≤‚àó
j ‚â§ wmax. Applying Chernoff
bounds, we have
er ‚â§ tr

j
pj Œ≤‚àó
j +

4tr




j
pj Œ≤‚àó
j



wmax ln(1/Œ¥ ) +
tr
m

i
Œ±‚àó
ici
‚â§
trWE
m +
trWE
m œµx,r
where the first inequality holds with probability at least 1 ‚àí Œ¥, and the second inequality uses the
fact the optimal value of the expected instance (dual of LP (10)) is WE . This proves the required
upper bound on er that er ‚â§ trWE
m (1 + œµx,r ) with probability at least 1 ‚àí Œ¥.
Going back to our application of Chernoff bounds above, to apply it in the form above, we
require that the multiplicative deviation from mean 4wmax ln(1/Œ¥ )
tr

j pj Œ≤‚àó
j
‚àà [0, 2e ‚àí 1]. If
j pj Œ≤‚àó
j ‚â• œµWE
m ,
then this requirement would follow. Suppose, however, that
j pj Œ≤‚àó
j < œµWE
m . Since we are happy if
the excess over mean is at most trWE
m œµx,r , let us look for a multiplicative error of
tr WE œµx,r
m
tr

j pj Œ≤‚àó
j
. Based
on the fact that
j pj Œ≤‚àó
j < œµWE
m and that œµx,r > œµ for all r, the multiplicative error can be seen to
be at least a constant and can be made larger than 2e ‚àí 1 depending on the constant inside the
big O of Œ≥ . We now use the version of Chernoff bounds for multiplicative error larger than 2e ‚àí 1,
which gives us that a deviation of trWE
m œµx,r occurs with a probability at most 2‚àí(1+
tr WE œµx,r m
tr
j pj Œ≤‚àó
j
)
tr
j pj Œ≤‚àó
j
wmax ,
where the division by wmax is because of the fact that Œ≤‚àó
j ‚â§ wmax. Noting that wmax ‚â§ Œ≥WE , we get
that this probability is at most Œ¥/n that is at most Œ¥.
Journal of the ACM, Vol. 66, No. 1, Article 7. Publication date: January 2019.                     
Near Optimal Online Algorithms and Fast Approximation Algorithms 7:21
Lemma 3.3 implies that WE (1 ‚àí 3œµx,r‚àí1) ‚â§ Zr ‚â§ WE, ‚àÄr ‚àà {0, 1,...,l ‚àí 1}. The rest of the proof
is similar to that of Section 3.2 and is focused on the second point in the high-level overview
we gave in the beginning of Section 3.3. In Section 3.2 we knew WE and obtained a WE (1 ‚àí 2œµ )
approximation with no resource i consumed beyond ci with probability 1 ‚àí œµ. Here, instead ofWE
we have an approximation for WE in the form of Zr that gets increasingly accurate as r increases.
We set a target of tr Zr
m for stage r and show that with a probability of at least 1 ‚àí Œ¥ we get a profit
of tr Zr
m (1 ‚àí œµy,r ) from every resource i and no resource i consumed beyond tr ci
m (1 + œµx,r ) capacity.8
As in Section 3.2, call stage r of algorithm A a failure if at least one of the following fails:
(1) For all i,
tr+1
t=tr +1 X A
i,t ‚â§ tr ci
m (1 + œµx,r ).
(2) For all i,
tr+1
t=tr +1 Y A
i,t ‚â• tr Zr
m (1 ‚àí œµy,r ).
Let Sr
s (Xi ) = tr +s
t=tr +1 Xi,t denote the amount of resource i consumed in the first s steps of stage
r, and let Sr
s (Yi ) = tr +s
t=tr +1 Yi,t denote the resource i profit in the first s steps of stage r,
Pr
‚é°
‚é¢
‚é¢
‚é¢
‚é¢
‚é£
tr+1
t=tr +1
X A
i,t ‚â•
trci
m (1 + œµx,r )
‚é§
‚é•
‚é•
‚é•
‚é•
‚é¶
= Pr
‚é°
‚é¢
‚é¢
‚é¢
‚é¢
‚é£
tr+1
t=tr +1 X A
i,t
Œ≥ci
‚â•
tr
mŒ≥
(1 + œµx,r )
‚é§
‚é•
‚é•
‚é•
‚é•
‚é¶
= Pr
‚é°
‚é¢
‚é¢
‚é¢
‚é¢
‚é£
(1 + œµx,r )
tr+1 t=tr +1 X A
i, t
Œ≥ ci ‚â• (1 + œµx,r )
(1+œµx,r ) tr
mŒ≥
‚é§
‚é•
‚é•
‚é•
‚é•
‚é¶
‚â§ E
‚é°
‚é¢
‚é¢
‚é¢
‚é¢
‚é£
(1 + œµx,r )
tr+1 t=tr +1 X A
i, t
Œ≥ ci
‚é§
‚é•
‚é•
‚é•
‚é•
‚é¶
/(1 + œµx,r )
(1+œµx,r ) tr
mŒ≥
=
E
‚é°
‚é¢
‚é¢
‚é¢
‚é¢
‚é£
tr+1
t=tr +1 (1 + œµx,r )
X A
i, t
Œ≥ ci
‚é§
‚é•
‚é•
‚é•
‚é•
‚é¶
(1 + œµx,r )
(1+œµx,r ) tr
mŒ≥
. (11)
Pr
‚é°
‚é¢
‚é¢
‚é¢
‚é¢
‚é£
tr+1
t=tr +1
Y A
i,t ‚â§
trZr
m (1 ‚àí œµy,r )
‚é§
‚é•
‚é•
‚é•
‚é•
‚é¶
= Pr
‚é°
‚é¢
‚é¢
‚é¢
‚é¢
‚é£
tr+1
t=tr +1 Y A
i,t
wmax
‚â§
trZr
mwmax
(1 ‚àí œµy,r )
‚é§
‚é•
‚é•
‚é•
‚é•
‚é¶
= Pr
‚é°
‚é¢
‚é¢
‚é¢
‚é¢
‚é£
(1 ‚àí œµy,r )
tr+1 t=tr +1 Y A
i, t
wmax ‚â• (1 ‚àí œµy,r )
(1‚àíœµy,r ) tr Zr mwmax
‚é§
‚é•
‚é•
‚é•
‚é•
‚é¶
‚â§ E
‚é°
‚é¢
‚é¢
‚é¢
‚é¢
‚é£
(1 ‚àí œµy,r )
tr+1 t=tr +1 Y A
i, t
wmax
‚é§
‚é•
‚é•
‚é•
‚é•
‚é¶
/(1 ‚àí œµy,r )
(1‚àíœµy,r ) tr Zr mwmax
=
E
	
tr+1
t=tr +1 (1 ‚àí œµy,r )
Y A
i, t
wmax 

(1 ‚àí œµy,r )
(1‚àíœµy,r ) tr Zr mwmax
. (12)
If our algorithm A was P (and therefore we can use E[X‚àó
i,t] ‚â§ ci
m and E[Y‚àó
i,t] ‚â• WE
m ‚â• Zr
m ), then
the total failure probability for each stage r that is the sum of Equations (11) and (12) for all
the i‚Äôs would have been n ¬∑ [e
‚àíœµ2
x,r
4Œ≥
tr
m + e
‚àíœµ2
y,r
2
tr Zr mwmax ] = n ¬∑ [ Œ¥
2n + Œ¥
2n ] = Œ¥. The goal is to design an
8Note that we are allowed to consume a bit beyond tr ci
m , because our goal is just that overall we do not consume beyond
ci and not that for every stage we respect the tr ci
m constraint. In spite of this (1 + œµx,r ) excess consumption in all stages,
since stage ‚àí1 consumes nothing at all, we will see that no excess consumption occurs at the end.
Journal of the ACM, Vol. 66, No. 1, Article 7. Publication date: January 2019.            
7:22 N. R. Devanur et al.
algorithm A for stage r that, unlike P, does not know the distribution but also obtains the same Œ¥
failure probability, just as we did in Section 3.2. That is, we want to show that the sum of Equations (11) and (12) over all i‚Äôs is at most Œ¥:

i
E
	
tr+1
t=tr +1 (1 + œµx,r )
X A
i, t
Œ≥ ci


(1 + œµx,r )
(1+œµx,r ) tr
mŒ≥
+

i
E
	
tr+1
t=tr +1 (1 ‚àí œµy,r )
Y A
i, t
wmax 

(1 ‚àí œµy,r )
(1‚àíœµy,r ) tr Zr mwmax
‚â§ Œ¥ .
For the algorithm AsPtr ‚àís , the above quantity can be rewritten as

i
E
	

1 + œµx,r
 Sr
s (X A
i ) Œ≥ ci
tr+1
t=tr +s+1

1 + œµx,r
 X ‚àó
i, t
Œ≥ ci


(1 + œµx,r )
(1+œµx,r ) tr
mŒ≥
+

i
E
	 
1 ‚àí œµy,r
 Sr
s (Y A
i ) wmax tr+1
t=tr +s+1

1 ‚àí œµy,r
 Y‚àó
i, t
wmax 

(1 ‚àí œµy,r )
(1‚àíœµy,r ) tr Zr mwmax
,
which, by using (1 + œµ )
x ‚â§ 1 + œµx for 0 ‚â§ x ‚â§ 1, is in turn upper bounded by

i
E
	

1 + œµx,r
 Sr
s (X A
i ) Œ≥ ci
tr+1
t=tr +s+1
	
1 + œµx,r
X‚àó
i, t
Œ≥ ci

 

(1 + œµx,r )
(1+œµx,r ) tr
mŒ≥
+

i
E
	 
1 ‚àí œµy,r
 Sr
s (Y A
i ) wmax tr+1
t=tr +s+1
	
1 ‚àí œµy,r
Y ‚àó
i, t
wmax 
 

(1 ‚àí œµy,r )
(1‚àíœµy,r ) tr Zr mwmax
.
Since for all t, the random variables X‚àó
i,t , X A
i,t , Y‚àó
i,t , and Y A
i,t are all independent, and E[X‚àó
i,t] ‚â§ ci
m ,
E[Y‚àó
i,t] ‚â• WE
m , and WE
wmax ‚â• 1
Œ≥ , the above is in turn upper bounded by

i
E
	

1 + œµx,r
 Sr
s (X A
i ) Œ≥ ci

1 + œµx,r
mŒ≥ tr ‚àís


(1 + œµx,r )
(1+œµx,r ) tr
mŒ≥
+

i
E
	 
1 ‚àí œµy,r
 Sr
s (Y A
i ) wmax 
1 ‚àí œµy,r
mŒ≥ tr ‚àís


(1 ‚àí œµy,r )
(1‚àíœµy,r ) tr Zr mwmax
. (13)
Let Fr [AsPtr ‚àís ] denote the quantity in Equation (13), which is an upper bound on failure probability of the hybrid algorithm AsPtr ‚àís for stage r. We just showed that Fr [Ptr ] ‚â§ Œ¥. We now prove
that for alls ‚àà {0, 1,...,tr ‚àí 1}, Fr [As+1Ptr ‚àís‚àí1] ‚â§ Fr [AsPtr ‚àís ], thus proving that Fr [Atr ] ‚â§ Œ¥, i.e.,
running the algorithm A for all the tr steps of stage r results in a failure with probability at most Œ¥.
Assuming that for all s < p, the algorithm A has been defined for the first s + 1 steps in such a
way that Fr [As+1Ptr ‚àís‚àí1] ‚â§ Fr [AsPtr ‚àís ], we now define A for the p + 1th step of stage r in a way
that will ensure that Fr [Ap+1Ptr ‚àíp‚àí1] ‚â§ Fr [ApPtr ‚àíp ]. We have
Journal of the ACM, Vol. 66, No. 1, Article 7. Publication date: January 2019.        
Near Optimal Online Algorithms and Fast Approximation Algorithms 7:23
Fr [Ap+1
Pm‚àíp‚àí1
] =

i
E
	
(1 + œµx,r )
Sr
p+1 (X A
i ) Œ≥ ci

1 + œµx,r
mŒ≥ tr ‚àíp‚àí1


(1 + œµx,r )
(1+œµx,r ) tr
mŒ≥
+

i
E
	
(1 ‚àí œµy,r )
Sr
p+1 (Y A
i ) wmax 
1 ‚àí œµy,r
mŒ≥ tr ‚àíp‚àí1


(1 ‚àí œµy,r )
(1‚àíœµy,r ) tr Zr mwmax
‚â§

i
E
	
(1 + œµx,r )
Sr
p (X A
i ) Œ≥ ci

1 + œµx,r
X A
i, tr +p+1
Œ≥ ci
 
1 + œµx,r
mŒ≥ tr ‚àíp‚àí1


(1 + œµx,r )
(1+œµx,r ) tr
mŒ≥
+

i
E
	
(1 ‚àí œµy,r )
Sr
p (Y A
i ) wmax 
1 ‚àí œµy,r
Y A
i, tr +p+1
wmax  
1 ‚àí œµy,r
mŒ≥ tr ‚àíp‚àí1


(1 ‚àí œµy,r )
(1‚àíœµy,r ) tr Zr mwmax
. (14)
Define
œïr
i,s = œµx,r
Œ≥ci
	 (1 + œµx,r )
Sr
s (X A
i ) Œ≥ ci

1 + œµx,r
mŒ≥ tr ‚àís‚àí1
(1 + œµx,r )
(1+œµx,r ) tr
mŒ≥ 

œàr
i,s = œµy,r
wmax 	 (1 ‚àí œµy,r )
Sr
s (Y A
i ) wmax 
1 ‚àí œµy,r
mŒ≥ tr ‚àís‚àí1
(1 ‚àí œµy,r )
(1‚àíœµy,r ) tr Zr mwmax


.
Define the step p + 1 of algorithm A as picking the following option k‚àó for request j:
k‚àó = arg mink
‚éß‚é™
‚é®
‚é™
‚é©

i
aijk ¬∑ œïr
i,p ‚àí

i
wijk ¬∑ œàr
i,p
‚é´‚é™
‚é¨
‚é™
‚é≠
.
By the above definition of step p + 1 of algorithm A (for stage r), it follows that for any two algorithms with the first p steps being identical, and the lasttr ‚àí p ‚àí 1 steps following the HypotheticalOblivious algorithm P, algorithm A‚Äôs p + 1th step is the one that minimizes expression (14). In particular, it follows that expression (14) is upper bounded by the same expression where the p + 1th
step is according to X‚àó
i,tr +p+1 and Y‚àó
i,tr +p+1, i.e., we replace X A
i,tr +p+1 by X‚àó
i,tr +p+1 and Y A
i,tr +p+1 by
Y‚àó
i,tr +p+1. Therefore, we have
Fr [Ap+1
Pm‚àíp‚àí1
] ‚â§

i
E
	
(1 + œµx,r )
Sr
p (X A
i ) Œ≥ ci
	
1 + œµx,r
X‚àó
i, tr +p+1
Œ≥ ci

 
1 + œµx,r
mŒ≥ tr ‚àíp‚àí1


(1 + œµx,r )
(1+œµx,r ) tr
mŒ≥
+

i
E
	
(1 ‚àí œµy,r )
Sr
p (Y A
i ) wmax 	
1 ‚àí œµy,r
Y ‚àó
i, tr +p+1
wmax 
 
1 ‚àí œµy,r
mŒ≥ tr ‚àíp‚àí1


(1 ‚àí œµy,r )
(1‚àíœµy,r ) tr Zr mwmax
‚â§

i
E
	
(1 + œµx,r )
Sr
p (X A
i ) Œ≥ ci

1 + œµx,r
mŒ≥  1 + œµx,r
mŒ≥ tr ‚àíp‚àí1


(1 + œµx,r )
(1+œµx,r ) tr
mŒ≥
Journal of the ACM, Vol. 66, No. 1, Article 7. Publication date: January 2019.         
7:24 N. R. Devanur et al.
+

i
E
	
(1 ‚àí œµy,r )
Sr
p (Y A
i ) wmax 
1 ‚àí œµy,r
mŒ≥  1 ‚àí œµy,r
mŒ≥ tr ‚àíp‚àí1


(1 ‚àí œµy,r )
(1‚àíœµy,r ) tr Zr mwmax
= Fr [ApPtr ‚àíp ].
This completes the proof of Theorem 2.2.
3.4 Approximate Estimations
Our Algorithm 2 in Section 3.3 required periodically computing the optimal solution to an offline
instance. Similarly, our Algorithm 1 in Section 3.2 requires the value ofWE to be given. Suppose we
could only approximately estimate these quantities, do our results carry through approximately?
That is, suppose the solution to the offline instance is guaranteed to be at least 1
Œ± of the optimal, and
the stand-in that we are given for WE is guaranteed to be at least 1
Œ± of WE . Both our Theorem 2.2
and Theorem 3.2 go through with just the WE replaced by WE/Œ±. Every step of the proof of the
exact version goes through in this approximate version, and so we skip the formal proof for this
statement.
3.5 Adversarial Stochastic Input
In this section, we relax the assumption that requests are drawn i.i.d. every time step. Namely the
distribution for each time step need not be identical, but an adversary gets to decide which distribution to sample a request from. The adversary could even use how the algorithm has performed
in the firstt ‚àí 1 steps in picking the distribution for a given time step t. The relevance of this model
for the real world is that for settings like display ads, the distribution fluctuates over the day. In
general, a day is divided into many chunks, and within a chunk, the distribution is assumed to
remain i.i.d. This is exactly captured by this model.
We give algorithms that give guarantees against three different benchmarks in the three models below. The benchmarks get successively stronger, and hence the information sought by the
algorithm also increases successively.
3.5.1 ASI Model 1. In this model, the guarantee we give is against the worst distribution over
all time steps picked by the adversary. More formally, let WE (t) denote the optimal profit for the
expected instance of distribution of time step t. Our benchmark will be WE = mint WE (t). Given
just the single number WE , our Algorithm 1 in Section 3.2 will guarantee a revenue of WE (1 ‚àí 2œµ )
with a probability of at least 1 ‚àí œµ assuming Œ≥ = O( œµ 2
log(n/œµ ) ), just like the guarantee in Theorem 3.2.
Algorithm 1 works for this ASI model, because the proof did not use the similarity of the distributions beyond the fact that E[X‚àó
i,t |X‚àó
i,t‚àÄt < t] ‚â§ ci
m for all values of X‚àó
i,t, and E[Y‚àó
i,t |Y‚àó
i,t‚àÄt <
t] ‚â• WE
m for all values ofY‚àó
i,t (HereX‚àó
i,t andY‚àó
i,t denote the random variables for resource consumption and profit at time t following from allocation according the optimal solution to the expected
instance of the distribution used in stage t). In other words, distributions being identical and independent is not crucial, but the fact that the expected instances of these distributions have a
minimum profit guarantee in spite of all the dependencies between the distributions is sufficient.
Both of these inequalities remain true in this model of ASI also, and thus it easy to verify that
Algorithm 1 works for this model.
3.5.2 ASI Model 2. In this model, which is otherwise identical to model 1, our benchmark is
stronger, namely WE =
m
t=1 WE (t)
m : This is clearly a much stronger benchmark than mint WE (t).
Journal of the ACM, Vol. 66, No. 1, Article 7. Publication date: January 2019.  
Near Optimal Online Algorithms and Fast Approximation Algorithms 7:25
Correspondingly, our algorithm requires more information than in model 1: We ask for WE (t) for
every t, at the beginning of the algorithm.
A slight modification of our Algorithm 1 in Section 3.2 will give a revenue of
m
t=1 WE (t)
m (1 ‚àí 2œµ )
with probability at least 1 ‚àí œµ, i.e., WE (1 ‚àí 2œµ ) w.p. at least (1 ‚àí œµ ). Among the two potential functions œïi,s andœài,s , we modifyœài,s in the most natural way to account for the fact that distributions
change every step.
Define
œïi,s = 1
ci
	 (1 + œµ )
Ss (X A
i ) Œ≥ ci

1 + œµ
(1+œµ )Œ≥m m‚àís‚àí1
(1 + œµ )
1
Œ≥


œài,s = 1
WE
	 (1 ‚àí œµ )
Ss (Y A
i )
Œ≥WE
m
t=s+2

1 ‚àí œµWE (t)
(1+œµ )WEŒ≥m 
(1 ‚àí œµ ) 1‚àíœµ
Œ≥ (1+œµ )


.
Note that when WE (t) = WE for all t, then we get precisely the œài,s defined in Section 3.2 for
Algorithm 1. We present our algorithm below in Algorithm 3.
Algorithm 3 works for this ASI model much for the same reason Algorithm 1 worked for
ASI model 2: All the proof needs is that E[X‚àó
i,t |X‚àó
i,t‚àÄt < t] ‚â§ ci
m for all values of X‚àó
i,t, and
E[Y‚àó
i,t |Y‚àó
i,t‚àÄt < t] = WE (t)
m for all values of Y‚àó
i,t (Here X‚àó
i,t and Y‚àó
i,t denote the random variables for
resource consumption and profit at time t following from allocation according the optimal solution
to the expected instance of the distribution used in stage t).
ALGORITHM 3: Algorithm for stochastic online resource allocation in ASI model 2
Input: Capacities ci for i ‚àà [n], the total number of requests m, the values of Œ≥ and WE (t) for
t ‚àà [m], an error parameter œµ > 0.
Output: An online allocation of resources to requests
1: Initialize œïi,0 = 1
ci
	 
1+ œµ
(1+œµ )Œ≥m m‚àí1
(1+œµ )
1
Œ≥


, and, œài,0 = 1
WE
	 m
t=2
	
1‚àí œµWE (t )
(1+œµ )WEŒ≥m 

(1‚àíœµ ) 1‚àíœµ Œ≥ (1+œµ )


2: for s = 1 to m do
3: If the incoming request is j, then use the following option k‚àó:
k‚àó = arg min k ‚ààK ‚à™{‚ä•}
‚éß‚é™
‚é®
‚é™
‚é©

i
aijk ¬∑ œïi,s‚àí1 ‚àí

i
wijk ¬∑ œài,s‚àí1
‚é´‚é™
‚é¨
‚é™
‚é≠
.
4: X A
i,s = aijk‚àó , Y A
i,s = wijk‚àó
5: Update œïi,s = œïi,s‚àí1 ¬∑
‚é°
‚é¢
‚é¢
‚é¢
‚é¢
‚é¢
‚é£
(1+œµ )
X A
i,s Œ≥ ci
1+ œµ
(1+œµ )Œ≥m
‚é§
‚é•
‚é•
‚é•
‚é•
‚é•
‚é¶
, and, œài,s = œài,s‚àí1 ¬∑
‚é°
‚é¢
‚é¢
‚é¢
‚é¢
‚é¢
‚é£
(1‚àíœµ )
Y A
i,s Œ≥WE
1‚àí œµ
(1+œµ )Œ≥m
‚é§
‚é•
‚é•
‚é•
‚é•
‚é•
‚é¶
6: end for
We skip the proof for the profit guarantee ofWE (1 ‚àí 2œµ ), since it is almost identical to the proof
in Section 3.2 for Algorithm 1.
3.5.3 ASI Model 3. In this model, which is otherwise identical to models 1 and 2, our benchmark
is even stronger: namely the optimal profit of the expected instance with all the time varying
distributions (explicitly spelled out in LP (15)). This benchmark WE is the strongest benchmark
possible. Correspondingly, our algorithm requires more information than in model 2: We ask for
Journal of the ACM, Vol. 66, No. 1, Article 7. Publication date: January 2019.   
7:26 N. R. Devanur et al.
WE,i (t) for every i and t and ci (t) for every i and t at the beginning of the algorithm, whereWE,i (t)
and ci (t) are the amount of type i profit obtained and type i resource consumed by the optimal
solution to the expected instance in LP (15) at step t. Namely WE,i (t) =
j,k pj,twijkx‚àó
j,k,t and
ci (t) =
j,k pj,taijkx‚àó
j,k,t , where x‚àó
j,k,t ‚Äôs are the optimal solution to LP (15).
Primal and dual LPs defining the expected instance (15)
Primal for ASI model 3 Dual for ASI model 3
Maximize Œª s.t. Minimize
j,t pj,t Œ≤j ,t +
i Œ±ici s.t.
‚àÄ i,

t,j,k pj,twijkxj,k,t ‚â• Œª ‚àÄ j, k, pj,t

Œ≤j,t +
i (Œ±iaijk ‚àí œÅiwijk )

‚â• 0
‚àÄ i,

t,j,k pj,taijkxj,k,t ‚â§ ci

i œÅi ‚â• 1
‚àÄ j,t,

k xj,k,t ‚â§ 1 ‚àÄ i, œÅi ‚â• 0, Œ±i ‚â• 0
‚àÄ j, k,t xj,k,t ‚â• 0. ‚àÄ j, Œ≤j ‚â• 0.
A slight modification of our Algorithm 1 in Section 3.2 will give a revenue of WE (1 ‚àí 2œµ ) with
probability at least 1 ‚àí œµ. We modify the two potential functions œïi,s and œài,s in the most natural
way to account for the fact that distributions change every step. LetWE,i = m
t=1WE,i (t), and thus,
our benchmark WE is simply mini WE,i . Note also that m
t=1 ci (t), call it c‚àó
i , is at most ci by the
feasibility of the optimal solution to LP (15).
Define
œïi,s = 1
ci
	 (1 + œµ )
Ss (X A
i ) Œ≥ ci
m
t=s+2

1 + œµci (t)
(1+œµ )ciŒ≥

(1 + œµ )
1
Œ≥


œài,s = 1
WE,i
	 (1 ‚àí œµ )
Ss (Y A
i )
Œ≥WE,i m
t=s+2

1 ‚àí œµWE,i (t)
(1+œµ )WE,iŒ≥

(1 ‚àí œµ ) 1‚àíœµ
Œ≥ (1+œµ )


.
We present our algorithm below in Algorithm 4.
Algorithm 4 works for this ASI model much for the same reason Algorithm 3 worked for
ASI model 2: All the proof needs is that E[X‚àó
i,t |X‚àó
i,t‚àÄt < t] = ci (t)
m for all values of X‚àó
i,t and
E[Y‚àó
i,t |Y‚àó
i,t‚àÄt < t] = WE,i (t)
m for all values of Y‚àó
i,t (Here X‚àó
i,t and Y‚àó
i,t denote the random variables
for resource consumption and profit at time t following from allocation according the optimal
solution to the expected instance captured by LP (15)).
We skip the proof for the profit guarantee ofWE (1 ‚àí 2œµ ), since it is almost identical to the proof
in Section 3.2 for Algorithm 1.
4 PROOF OF NEAR-OPTIMALITY OF ONLINE ALGORITHM
FOR RESOURCE ALLOCATION
In this section, we construct a family of instances of the resource allocation problem in the i.i.d.
setting for which Œ≥ = œâ(œµ2/ logn) will rule out a competitive ratio of 1 ‚àí O(œµ ). The construction
Journal of the ACM, Vol. 66, No. 1, Article 7. Publication date: January 2019.           
Near Optimal Online Algorithms and Fast Approximation Algorithms 7:27
ALGORITHM 4: Algorithm for stochastic online resource allocation in ASI model 3
Input: Capacitiesci (t) and ci , and profitsWE,i (t) fori ‚àà [n], t ‚àà [m], the total number of requests
m, the values of Œ≥ and an error parameter œµ > 0.
Output: An online allocation of resources to requests
1: Initialize œïi,0 = 1
ci
	 m
t=2
	
1+ œµci (t )
(1+œµ )ci Œ≥


(1+œµ )
1
Œ≥


, and, œài,0 = 1
WE,i
	 m
t=2
	
1‚àí œµWE,i (t )
(1+œµ )WE,i Œ≥


(1‚àíœµ ) 1‚àíœµ Œ≥ (1+œµ )


2: for s = 1 to m do
3: If the incoming request is j, then use the following option k‚àó:
k‚àó = arg min k ‚ààK ‚à™{‚ä•}
‚éß‚é™
‚é®
‚é™
‚é©

i
aijk ¬∑ œïi,s‚àí1 ‚àí

i
wijk ¬∑ œài,s‚àí1
‚é´‚é™
‚é¨
‚é™
‚é≠
.
4: X A
i,s = aijk‚àó , Y A
i,s = wijk‚àó
5: Update œïi,s = œïi,s‚àí1 ¬∑
‚é°
‚é¢
‚é¢
‚é¢
‚é¢
‚é¢
‚é£
(1+œµ )
X A
i,s Œ≥ ci
1+ œµ
(1+œµ )Œ≥m
‚é§
‚é•
‚é•
‚é•
‚é•
‚é•
‚é¶
, and, œài,s = œài,s‚àí1 ¬∑
‚é°
‚é¢
‚é¢
‚é¢
‚é¢
‚é¢
‚é¢
‚é£
(1‚àíœµ )
Y A
i,s Œ≥WE,i
1‚àí œµ
(1+œµ )Œ≥m
‚é§
‚é•
‚é•
‚é•
‚é•
‚é•
‚é•
‚é¶
6: end for
closely follows the construction by Agrawal et al. [2] for proving a similar result in the randompermutation model.
The instance has n = 2z resources with B units of each resource, and Bz(2 + 1/Œ±) + ‚àö
Bz requests
where Œ± < 1 is some scalar. Each request has only one ‚Äúoption,‚Äù i.e., each request can either be
dropped or, if served, consumes the same number of units of a specific subset of resources (which
we construct below). This means that a request is simply a scalar times a binary string of length
2z , with the ones (or the scalars) representing the coordinates of resources that are consumed by
this request if served.
The requests are classified into z categories. Each category in expectation consists of m/z =
B(2 + 1/Œ±) + ‚àö
B/z requests. A category, indexed by i, is composed of two different binary vectors
vi and wi (each of length 2z ). The easiest way to visualize these vectors is to construct two 2z √ó z
0 ‚àí 1 matrices, with each matrix consisting of all possible binary strings of length z, written one
string in a row. The first matrix lists the strings in ascending order and the second matrix in
descending order. The ith column of the first matrix multiplied by the scalar Œ± is the vector vi and
the ith column of the second matrix is the vectorwi . There are two properties of these vectors that
are useful for us:
(1) The vectors vi/Œ± and wi are complements of one another.
(2) Any matrix of z columns, with column i being either vi/Œ± or wi has exactly one row with
all ones in it.
We are ready to construct the i.i.d. instance now. Each request is drawn from the following
distribution. A given request could be, for each i, of type:
(1) vi and profit 4Œ± with probability B
Œ±zm ,
(2) wi and profit 3 with probability B
zm ,
(3) wi and profit 2 with probability  B
zm ,
Journal of the ACM, Vol. 66, No. 1, Article 7. Publication date: January 2019.  
7:28 N. R. Devanur et al.
(4) wi and profit 1 with probability B
zm ,
(5) Zero vector with zero profit with probability 1 ‚àí 2B
zm ‚àí
 B
zm ‚àí B
Œ±zm .
We use the following notation for request types: A (2,wi ) request stands for a wi type request
of profit 2. Observe that the expected instance has an optimal profit of OPT = 7B. This is obtained
by picking for each i the B
Œ± z vectors of type vi and profit 4Œ±, along with B
z vectors of type wi with
profit 3. Note that this exhausts every unit of every item, and thus, combined with the fact that the
most profitable requests have been served, the value of 7B is indeed the optimal value. This means
that any algorithm that obtains a 1 ‚àí œµ competitive ratio must have an expected profit of at least
7B ‚àí 7œµB.
Let ri (w) and ri (v) be the random variables denoting the number of vectors of type wi and vi
picked by some 1 ‚àí œµ competitive algorithm ALG. Let ai (v) denote the total number of vectors of
type vi that arrived in this instance.
Lemma 4.1. For some constant k, the ri (w)‚Äôs satisfy

i
E[|ri (w) ‚àí B/z|] ‚â§ 7œµB + 4
‚àö
Œ±kBz.
Proof. Let Y denote the set of indices i for which ri (w) > B/z. One way to upper bound the
total number of vectors of type v picked by ALG is the following. Split the set of indices into Y
and X = [z] \ Y. The number of v‚Äôs from Y is, by chosen notation,
i ‚ààY ri (v). The number of v‚Äôs
from X, we show, is at most B‚àí

i‚ààY ri (w)
Œ± . Note that since there are only B copies of every item, it
follows that Œ±[

i ri (v)] ‚â§ B and
i ri (w) ‚â§ B. Further, by property 2 ofvi ‚Äôs andwi ‚Äôs, we have that
Œ±[

i ‚ààX ri (v)] +
i ‚ààY ri (w) ‚â§ B. This means that the number of v‚Äôs from X is at most B‚àí

i‚ààY ri (w)
Œ± .
Let P =
i ‚ààY (ri (w) ‚àí B/z) and M =
i ‚ààX (B/z ‚àí ri (w)). Showing E[P + M] ‚â§ 7œµB + 4
‚àö
Œ±kBz
proves the lemma. By an abuse of notation, let ALG also be the profit obtained by the algorithm
ALG and let bestwi (t) denote the most profitable t requests of type wi in a given instance. Note
that 4B + z
i=1bestwi (B/z) ‚â§ 7B = OPT. We upper bound E[ALG] as
E[ALG] ‚â§ E
‚é°
‚é¢
‚é¢
‚é¢
‚é¢
‚é£
z
i=1
bestwi (ri (w))
‚é§
‚é•
‚é•
‚é•
‚é•
‚é¶
+ 4Œ±
‚é°
‚é¢
‚é¢
‚é¢
‚é¢
‚é£
B ‚àí
i ‚ààY E[ri (w)]
Œ±
+

i ‚ààY
E[ri (v)]
‚é§
‚é•
‚é•
‚é•
‚é•
‚é¶
‚â§ E
‚é°
‚é¢
‚é¢
‚é¢
‚é¢
‚é£
z
i=1
besti (B/z) + 3P ‚àí M
‚é§
‚é•
‚é•
‚é•
‚é•
‚é¶
+ 4 

B ‚àí E
‚é°
‚é¢
‚é¢
‚é¢
‚é¢
‚é£

i ‚ààY
(ri (w) ‚àí B/z) + |Y |B/z
‚é§
‚é•
‚é•
‚é•
‚é•
‚é¶


+ 4Œ± E
‚é°
‚é¢
‚é¢
‚é¢
‚é¢
‚é£

i ‚ààY
ri (v)
‚é§
‚é•
‚é•
‚é•
‚é•
‚é¶
‚â§ OPT ‚àí E[P + M] + 4Œ± E
‚é°
‚é¢
‚é¢
‚é¢
‚é¢
‚é£

i ‚ààY

ri (v) ‚àí B
Œ±z ‚é§
‚é•
‚é•
‚é•
‚é•
‚é¶

Since P =

i ‚ààY
(ri (w) ‚àí B/z)

‚â§ OPT ‚àí E[P + M] + 4Œ± E
‚é°
‚é¢
‚é¢
‚é¢
‚é¢
‚é£

i ‚ààY

ai (v) ‚àí B
Œ±z ‚é§
‚é•
‚é•
‚é•
‚é•
‚é¶
(Since ri (v) ‚â§ ai (v))
‚â§ OPT ‚àí E[P + M] + 4Œ± E
‚é°
‚é¢
‚é¢
‚é¢
‚é¢
‚é¢
‚é¢
‚é£

i:ai (v)‚â• B
Œ± z

ai (v) ‚àí B
Œ±z 
‚é§
‚é•
‚é•
‚é•
‚é•
‚é•
‚é•
‚é¶
Journal of the ACM, Vol. 66, No. 1, Article 7. Publication date: January 2019.                     
Near Optimal Online Algorithms and Fast Approximation Algorithms 7:29
‚â§ OPT ‚àí E[P + M] + 4Œ± ¬∑ z ¬∑ k ¬∑
 B
Œ±z
(where k is some constant from Central Limit Theorem)
‚â§ OPT ‚àí E[P + M] + 4
‚àö
Œ±kBz (where k is k2).
The inequality that follows from CLT uses the fact that for a random variable X ‚àº (m,c/m) (X is
binomially distributed with success probability of c/m), whenever c = œâ(1), and c ‚â§ m, we have
that E[X |X ‚â• c] = c + k
‚àö
c, for some constant k
. In this case, we have B
Œ± z in place of c. For example, if n = log(m) (and thus z = logn = log logm), as long as B = œâ(log logm) and B ‚â§ m, then
the CLT inequality will hold. Note that Œ± could have been any constant and this argument still
holds.
We are now ready to prove Theorem 2.3, which we restate here for convenience. Theorem 2.3
There exist instances with Œ≥ = œµ 2
log(n) such that no algorithm, even with complete knowledge of the
distribution, can get a 1 ‚àí o(œµ ) approximation factor.
Proof. We first give the overview of the proof before providing a detailed argument.
Overview. Lemma 4.1 says that ri (w) has to be almost always close to B/z for all i. In particular,
the probability that
i |ri (w) ‚àí B/z| ‚â§ 4(7œµB + 4
‚àö
Œ±kBz) is at least 3/4. In this proof, we show, in
an argument similar to the one in Agrawal et al. [2], that if this has to be true, then one has to
lose a revenue of Œ©(‚àö
Bz) ‚àí 4(7œµB + 4
‚àö
Œ±kBz). Since Œ± can be set to any arbitrary constant, this
means that we lose a revenue of Œ©(‚àö
Bz) ‚àí 28œµB. Since OPT is 7B, to get a 1 ‚àí œµ approximation,
we require that Œ©(‚àö
Bz) ‚àí 28œµB ‚â§ 7œµB. Thus, we need B ‚â• Œ©(logm
œµ 2 ). In other words, we require
Œ≥ = 1
B ‚â§ O( œµ 2
logm ).
In Detail. We now proceed to prove the claim that a revenue loss of Œ©(‚àö
Bz) ‚àí 4(7œµB + ‚àö
Œ±kBz)
is inevitable. We just showed that with a probability of at least 3/4,
i |ri (w) ‚àí B/z| ‚â§ 4(7œµB +
4
‚àö
Œ±kBz). For now, we assume that ri (w) should be exactly B/z and later account for the probability 1/4 leeway and also the 4(7œµB + 4
‚àö
Œ±kBz) error that is allowed by Lemma 4.1. With this
assumption, we show that for each i there is a loss of Œ©(‚àö
B/z).
For each i let oi denote the number of (1,wi ) requests that the algorithm served in total. With
a constant probability the number of 3‚Äôs and 2‚Äôs (of type wi ) exceed B/z. If oi = Œ©(‚àö
B/z), then
there is a loss of at least Œ©(‚àö
B/z) because of picking 1‚Äôs instead of 2‚Äôs or 3‚Äôs. This establishes the
Œ©(‚àö
B/z) loss that we wanted to prove for this case.
Suppose oi < Œ©(‚àö
Bz). For each i, let Ri be the set of requests of typewi with profit either 1 or 3.
For every i, with a constant probability 2B/z ‚àí 2
‚àö
B/z ‚â§ |Ri | ‚â§ 2B/z + 2
‚àö
B/z. Conditional on the
set Ri , we make the following two observations:
‚Ä¢ the types of requests in Ri are independent random variables that take value 1 or 3 with
equal probability.
‚Ä¢ the order of requests in Ri is a uniformly random permutation of Ri .
Now consider any (2,wi ) request, say, the tth request, of profit 2. With a constant probability, this
request can be served without violating any capacity constraints, and thus, the algorithm has to
decide whether or not to serve this request. In at least 1/2 of the random permutations of Ri , the
number of bids from set Ri before the bid t is less than B/z. Conditional on this event, the profits
of requests in Ri before t, with a constant probability could:
Journal of the ACM, Vol. 66, No. 1, Article 7. Publication date: January 2019.   
7:30 N. R. Devanur et al.
(1) take values such that there are enough (3,wi ) requests after t to make the total number
of wi requests picked by the algorithm to be at least B/z;
(2) take values such that even if all the (3,wi ) requests after t were picked, the total number
of wi requests picked is at most B/z ‚àí ‚àö
B/z with a constant probability.
In the first kind of instances (where number of (3,wi ) requests are more than B/z) retaining (2,wi )
causes a loss of 1 as we could have picked a 3 instead. In the second kind, skipping (2,wi ) causes
a loss of 1, since we could have picked that 2 instead of a 1. Thus there is an inevitable constant
probability loss of 1 per (2,wi ) request. Thus, in expectation, there is a Œ©(‚àö
B/z) loss.
Thus, whether oi = ‚àö
B/z or oi < ‚àö
B/z, we have established a loss of Œ©(‚àö
B/z) per i and thus a
total expected loss of Œ©(‚àö
Bz). This is under the assumption that ri (w) is exactly B/z. There is a
leeway of 4(7œµB + 4
‚àö
Œ±kBz) granted by Lemma 4.1. Even after that leeway, since Œ± can be made an
arbitrarily small constant and Lemma 4.1 still holds, we have the loss at Œ©(‚àö
Bz) ‚àí 28œµB. Now after
the leeway, the statement
i |ri (w) ‚àí B/z| ‚â§ 4(7œµB + 4
‚àö
Œ±kBz) has to hold only with probability
3/4. But even this puts the loss at Œ©(‚àö
Bz) ‚àí 21œµB.
Therefore, E[ALG] ‚â§ OPT ‚àí Œ©(‚àö
Bz) ‚àí 21œµB. Since OPT = 7B, we have E[ALG] ‚â§ OPT(1 ‚àí
Œ©(‚àö
z/B) ‚àí 21œµ ), and to get the 1 ‚àí O(œµ ) approximation we need Œ©(‚àö
z/B ‚àí 21œµ ) ‚â§ O(œµ ), implying that B ‚â• Œ©(z/œµ2) = Œ©(logm/œµ2).
5 GREEDY ALGORITHM FOR ADWORDS
In this section, we give a simple proof of Theorem 2.4, which we restate below for convenience.
Theorem 2.4. The greedy algorithm achieves an approximation factor of 1 ‚àí 1/e for the Adwords
problem in the i.i.d. unknown distributions model for all Œ≥ , i.e., 0 ‚â§ Œ≥ ‚â§ 1.
As noted in Section 2.2, where the Adwords problem was introduced, the budget constraints are
not hard, i.e., when a query j arrives, with a bid amount bij > remaining budget of i, we are still
allowed to allot that query to advertiser i, but we only earn a revenue of the remaining budget of
i and not the total value bij .
Goel and Mehta [13] prove that the greedy algorithm gives a (1 ‚àí 1/e) approximation to the
adwords problem when the queries arrive in a random permutation or in i.i.d. but under an assumption that almost gets down to Œ≥ tending to zero, i.e., bids being much smaller than budgets.
We give a much simpler proof for a (1 ‚àí 1/e) approximation by greedy algorithm for the i.i.d.
unknown distributions case, and our proof works for all Œ≥ .
Let pj be the probability of query j appearing in any given impression. Let yj = mpj . Let xij denote the offline fractional optimal solution for the expected instance. Let wi (t) denote the amount
of money spent by advertiser i at time step t, i.e., for the tth query in the greedy algorithm (to
be described below). Let fi (0) =
j bijxijyj . Let fi (t) = fi (0) ‚àí t
r=1wi (r). Let f (t) = n
i=1 fi (t).
Note that fi (0) is the amount spent by i in the offline fractional optimal solution to the expected
instance.
Consider the greedy algorithm that allocates the query j arriving at time t to the advertiser who
has the maximum effective bid for that query, i.e., argmaxi min{bij , Bi ‚àí t‚àí1 r=1wi (r)}. We prove
that this algorithm obtains a revenue of (1 ‚àí 1/e)

i,j bijxijyj and thus gives the desired 1 ‚àí 1/e
competitive ratio against the fractional optimal solution to the expected instance. Consider a hypothetical algorithm that allocates queries to advertisers according to the xij ‚Äôs. We prove that this
hypothetical algorithm obtains an expected revenue of (1 ‚àí 1/e)

i,j bijxijyj and argue that the
greedy algorithm only performs better. Let wh
i (t) and f h
i (t) denote the quantities analogous to
wi (t) and fi (t) for the hypothetical algorithm, with the initial value f h
i (0) = fi (0) =
j bijxijyj .
Journal of the ACM, Vol. 66, No. 1, Article 7. Publication date: January 2019.         
Near Optimal Online Algorithms and Fast Approximation Algorithms 7:31
Let f h (t) = n
i=1 f h
i (t). Let EXCEEDi (t) denote the set of all j such that bij is strictly greater than
the remaining budget at the beginning of time step t, namely bij > Bi ‚àí t‚àí1 r=1wh
i (r).
Lemma 5.1. E[wh
i (t)|f h
i (t ‚àí 1)] ‚â• f h
i (t‚àí1)
m .
Proof. The expected amount amount of money spent at time step t, is given by
E[wh
i (t)|f h
i (t ‚àí 1)] =

j ‚ààEXCEEDi (t)

Bi ‚àí
t‚àí1
r=1
wh
i (r)

xijyj
m +

jEXCEEDi (t)
bij
xijyj
m . (16)
If
j ‚ààEXCEEDi (t) xijyj ‚â• 1, then by Equation (16),
E[wh
i (t)|f h
i (t ‚àí 1)] ‚â•
Bi ‚àí t‚àí1 r=1wh
i (r)
m ‚â• f h
i (0) ‚àí t‚àí1 r=1wh
i (r)
m = f h
i (t ‚àí 1)
m .
Suppose, however,
j ‚ààEXCEEDi (t) xijyj < 1. We can write E[wh
i (t)|f h
i (t ‚àí 1)] as
E[wh
i (t)|f h
i (t ‚àí 1)] = f h
i (0)
m ‚àí

j ‚ààEXCEEDi (t)


bij ‚àí 

Bi ‚àí
t‚àí1
r=1
wh
i (r)



xijyj
m . (17)
Since bij ‚â§ Bi , and
j ‚ààEXCEEDi (t) xijyj < 1, Equation (17) can be simplified to
E[wh
i (t)|f h
i (t ‚àí 1)] >
f h
i (0)
m ‚àí
t‚àí1 r=1wh
i (r)
m
= f h
i (t ‚àí 1)
m .
Lemma 5.2. The hypothetical algorithm satisfies the following: E[f h (t)|f h (t ‚àí 1)] ‚â§ f h (t ‚àí
1)(1 ‚àí 1/m).
Proof. From the definition of f h
i (t), we have
f h
i (t) = f h
i (t ‚àí 1) ‚àí wh
i (t)
E[f h
i (t)|f h
i (t ‚àí 1)] = f h
i (t ‚àí 1) ‚àí E[wh
i (t)|f h
i (t ‚àí 1)] ‚â§ f h
i (t ‚àí 1)
	
1 ‚àí 1
m


,
where the inequality is due to Lemma 5.1. Summing over all i gives the Lemma.
Lemma 5.3. E[GREEDY] ‚â• (1 ‚àí 1/e)

i,j bijxijyj .
Proof. Lemma 5.2 proves that for the hypothetical algorithm, the value of the difference
f h (t ‚àí 1) ‚àí E[f h (t)|f h (t ‚àí 1)], which is the expected amount spent at time t by all the advertisers together, conditioned on f h (t ‚àí 1), is at least f h (t‚àí1)
m . But, by definition, conditioned on
the amount of money spent in first t ‚àí 1 steps, the greedy algorithm earns the maximum revenue at time step t. Thus, for the greedy algorithm, too, the statement of the Lemma 5.2 must
hold, namely E[f (t)|f (t ‚àí 1)] ‚â§ f (t ‚àí 1)(1 ‚àí 1/m). This means that E[f (m)] ‚â§ f (0)(1 ‚àí 1/m)
m ‚â§
f (0)(1/e). Thus the expected revenue earned is
E
‚é°
‚é¢
‚é¢
‚é¢
‚é¢
‚é£
m
r=1
w(r)
‚é§
‚é•
‚é•
‚é•
‚é•
‚é¶
= f (0) ‚àí E[f (m)]
‚â• f (0) (1 ‚àí 1/e)
= (1 ‚àí 1/e)

i,j
bijxijyj
and this proves the lemma.
Journal of the ACM, Vol. 66, No. 1, Article 7. Publication date: January 2019.                    
7:32 N. R. Devanur et al.
Lemma 5.3 proves Theorem 2.4.
6 FAST APPROXIMATION ALGORITHM FOR LARGE MIXED PACKING
AND COVERING INTEGER PROGRAMS
In this section, we consider the mixed packing-covering problem stated in Section 2.4 and prove
Theorem 2.5. We restate the integer program for the mixed covering-packing problem here,
‚àÄ i,

j,k
aijkxj,k ‚â§ ci
‚àÄ i,

j,k
wijkxj,k ‚â• di
‚àÄ j,

k
xj,k ‚â§ 1
‚àÄ j, k, xj,k ‚àà {0, 1}. (18)
The goal is to check whether there is a feasible solution to this IP. We solve a gap version of this
problem. Distinguish between the two cases with a high probability, say, 1 ‚àí Œ¥:
‚Ä¢ YES: There is a feasible solution.
‚Ä¢ NO: There is no feasible solution even with a slack, namely even if all of the ci ‚Äôs are multiplied by 1 + 3œµ (1 + œµ ) and all of the di ‚Äôs are multiplied by 1 ‚àí 3œµ (1 + œµ ).
We use 1 + 3œµ (1 + œµ ) and 1 ‚àí 3œµ (1 + œµ ) for slack instead of just 1 + œµ and 1 ‚àí œµ purely to reduce
notational clutter in what follows (mainly for the NO case).
Like in the online problem, we refer to the quantities indexed by j as requests, aijk as resource i
consumption, and wijk as resource i profit, and the quantities indexed by k as options. There are a
total of m requests, n resources, and K options, and the ‚Äúzero‚Äù option is denoted by ‚ä•. Recall that
the parameter Œ≥ for this problem is defined by Œ≥ = max({
aijk
ci }i,j,k ‚à™ { wijk
di }i,j,k ). Our algorithm
needs the values of m, n, and Œ≥ (an upper bound on the value of Œ≥ also suffices).
High-level Overview. We solve this offline problem in an online manner via random sampling.
We sample T = Œò( Œ≥m log(n/Œ¥ )
œµ 2 ) requests j from the set of possible requests uniformly at random
with replacement and then design an algorithm that allocates resources online for these requests.
At the end of serving T requests, we check whether the obtained solution proportionally satisfies
the constraints of IP (18). If yes, then we declare YES as the answer and declare NO otherwise.
At the core of the solution is the online sampling algorithm we use, which is identical to the
techniques used to develop the online algorithm in Sections 3.2 and 3.3. We describe our algorithm
in Algorithm 5.
The main theorem of this section is Theorem 2.5, which we restate here: Theorem 2.5 For
any œµ > 0, Algorithm 5 solves the gap version of the mixed covering-packing problem with
Œò( Œ≥m log(n/Œ¥ )
œµ 2 ) oracle calls.
Detailed Description and Proof. The proof is in two parts. The first part proves that our algorithm
indeed answers YES when the actual answer is YES with a probability at least 1 ‚àí Œ¥. The second
part is the identical statement for the NO case.
The YES Case. We begin with the case where the true answer is YES. Let x‚àó
jk denote some feasible
solution to the LP relaxation of IP (18). In a spirit similar to that of Sections 3.1, 3.2, and 3.3, we
define the algorithm P as follows. It samples a total of T = Œò( Œ≥m log(n/Œ¥ )
œµ 2 ) requests uniformly at
random, with replacement, from the total pool of m requests. When request j is sampled, P serves
Journal of the ACM, Vol. 66, No. 1, Article 7. Publication date: January 2019.   
Near Optimal Online Algorithms and Fast Approximation Algorithms 7:33
ALGORITHM 5: Online sampling algorithm for offline mixed covering-packing problems
Input: The mixed packing and covering IP (18), failure probability Œ¥ > 0, and an error parameter
œµ > 0.
Output: Distinguish between the cases ‚ÄúYES‚Äù where there is a feasible solution to IP (18), and
‚ÄúNO‚Äù where there is no feasible solution to IP (18) even if all the ci ‚Äôs are multiplied by 1 + 3œµ (1 + œµ )
and all of the di ‚Äôs are multiplied by 1 ‚àí 3œµ (1 + œµ ).
1: Set T = Œò( Œ≥m log(n/Œ¥ )
œµ 2 )
2: Initialize œïi,0 = 1
ci
	 
1+ œµ
mŒ≥ T ‚àí1
(1+œµ )
(1+œµ ) T
mŒ≥ 

, and, œài,0 = 1
di
	 
1‚àí œµ
mŒ≥ T ‚àí1
(1‚àíœµ )
(1‚àíœµ ) T
mŒ≥ 

3: for s = 1 to T do
4: Sample a request j uniformly at random from the total pool of m requests
5: If the incoming request is j, then use the following option k‚àó:
k‚àó = arg min k ‚ààK ‚à™{‚ä•}
‚éß‚é™
‚é®
‚é™
‚é©

i
aijk ¬∑ œïi,s‚àí1 ‚àí

i
wijk ¬∑ œài,s‚àí1
‚é´‚é™
‚é¨
‚é™
‚é≠
.
6: X A
i,s = aijk‚àó , Y A
i,s = wijk‚àó
7: Update œïi,s = œïi,s‚àí1 ¬∑
‚é°
‚é¢
‚é¢
‚é¢
‚é¢
‚é¢
‚é£
(1+œµ )
X A
i,s Œ≥ ci
1+ œµ
mŒ≥
‚é§
‚é•
‚é•
‚é•
‚é•
‚é•
‚é¶
, and, œài,s = œài,s‚àí1 ¬∑
‚é°
‚é¢
‚é¢
‚é¢
‚é¢
‚é¢
‚é£
(1‚àíœµ )
Y A
i,s
Œ≥ di
1‚àí œµ
mŒ≥
‚é§
‚é•
‚é•
‚é•
‚é•
‚é•
‚é¶
8: end for
9: if ‚àÄi
T
t=1 X A
i,t < T ci
m (1 + œµ ), and, T
t=1 Y A
i,t > T di
m (1 ‚àí œµ ) then
10: Declare YES
11: else
12: Declare NO
13: end if
j using option k with probability x‚àó
jk . Thus, if we denote by X‚àó
i,t the consumption of resource i in
step t of P, then we have E[X‚àó
i,t] = m
j=1 1
m

k aijkx‚àó
jk ‚â§ ci
m . This inequality follows from x‚àó
jk being
a feasible solution to LP relaxation of Equation (18). Similarly, let Y‚àó
i,t denote the resource i profit
in step t of P. We have E[Y‚àó
i,t] ‚â• di
m . We now write the probability that our condition for YES is
violated for some algorithm A,
Pr
‚é°
‚é¢
‚é¢
‚é¢
‚é¢
‚é£

T
t=1
X A
i,t ‚â•
Tci
m (1 + œµ )
‚é§
‚é•
‚é•
‚é•
‚é•
‚é¶
= Pr 	 T
t=1 X A
i,t
Œ≥ci
‚â•
T
mŒ≥
(1 + œµ )


= Pr 	
(1 + œµ )
T
t=1 X A
i, t
Œ≥ ci ‚â• (1 + œµ )
(1+œµ ) T
mŒ≥ 

‚â§ E
	
(1 + œµ )
T
t=1 X A
i, t
Œ≥ ci


/(1 + œµ )
(1+œµ ) T
mŒ≥
=
E
	
T
t=1 (1 + œµ )
X A
i, t
Œ≥ ci


(1 + œµ )
(1+œµ ) T
mŒ≥
(19)
Journal of the ACM, Vol. 66, No. 1, Article 7. Publication date: January 2019.          
7:34 N. R. Devanur et al.
Pr
‚é°
‚é¢
‚é¢
‚é¢
‚é¢
‚é£

T
t=1
Y A
i,t ‚â§
Tdi
m (1 ‚àí œµ )
‚é§
‚é•
‚é•
‚é•
‚é•
‚é¶
= Pr 	 T
t=1 Y A
i,t
Œ≥di
‚â•
T
mŒ≥
(1 ‚àí œµ )


= Pr 	
(1 ‚àí œµ )
T
t=1 Y A
i, t
Œ≥ di ‚â• (1 ‚àí œµ )
(1‚àíœµ ) T
mŒ≥ 

‚â§ E
	
(1 ‚àí œµ )
T
t=1 Y A
i, t
Œ≥ di


/(1 ‚àí œµ )
(1‚àíœµ ) T
mŒ≥
=
E
	
T
t=1 (1 ‚àí œµ )
Y A
i, t
Œ≥ di


(1 ‚àí œµ )
(1‚àíœµ ) T
mŒ≥
. (20)
If our algorithm A was P (and, therefore, we can use E[X‚àó
i,t] ‚â§ ci
m and E[Y‚àó
i,t] ‚â• di
m ), then the
total failure probability in the YES case, which is the sum of Equations (19) and (20) for all the i‚Äôs
would have been at most Œ¥ if T = Œò( Œ≥m log(n/Œ¥ )
œµ 2 ) for an appropriate constant inside Œò. The goal is
to design an algorithm A that, unlike P, does not first solve LP relaxation of IP (18) and then use
x‚àó
jk ‚Äôs to allocate resources but allocates online and also obtains the same Œ¥ failure probability, just
as we did in Sections 3.2 and 3.3. That is we want to show that the sum of Equations (19) and (20)
over all i‚Äôs is at most Œ¥:
E
	
T
t=1 (1 + œµ )
X A
i, t
Œ≥ ci


(1 + œµ )
(1+œµ ) T
mŒ≥
+
E
	
T
t=1 (1 ‚àí œµ )
Y A
i, t
Œ≥ di


(1 ‚àí œµ )
(1‚àíœµ ) T
mŒ≥
‚â§ Œ¥ .
For the algorithm AsPT ‚àís , the above quantity can be rewritten as

i
E
	
(1 + œµ )
Ss (X A
i ) Œ≥ ci
T
t=s+1 (1 + œµ )
X ‚àó
i, t
Œ≥ ci


(1 + œµ )
(1+œµ ) T
mŒ≥
+

i
E
	
(1 ‚àí œµ )
Ss (Y A
i )
Œ≥ di
T
t=s+1 (1 ‚àí œµ )
Y‚àó
i, t
Œ≥ di


(1 ‚àí œµ )
(1‚àíœµ ) T
mŒ≥
,
which, by using (1 + œµ )
x ‚â§ 1 + œµx for 0 ‚â§ x ‚â§ 1, is in turn upper bounded by

i
E
	
(1 + œµ )
Ss (X A
i ) Œ≥ ci
T
t=s+1
	
1 + œµ
X‚àó
i, t
Œ≥ ci

 

(1 + œµ )
(1+œµ ) T
mŒ≥
+

i
E
	
(1 ‚àí œµ )
Ss (Y A
i )
Œ≥ di
T
t=s+1
	
1 ‚àí œµ
Y ‚àó
i, t
Œ≥ di

 

(1 ‚àí œµ )
(1‚àíœµ ) T
mŒ≥
.
Since for all t, the random variables X‚àó
i,t , X A
i,t , Y‚àó
i,t , and Y A
i,t are all independent, and E[X‚àó
i,t] ‚â§ ci
m ,
E[Y‚àó
i,t] ‚â• di
m , the above is in turn upper bounded by

i
E
	
(1 + œµ )
Ss (X A
i ) Œ≥ ci

1 + œµ
mŒ≥ T ‚àís


(1 + œµ )
(1+œµ ) T
mŒ≥
+

i
E
	
(1 ‚àí œµ )
Ss (Y A
i )
Œ≥ di

1 ‚àí œµ
mŒ≥ T ‚àís


(1 ‚àí œµ )
(1‚àíœµ ) T
mŒ≥
. (21)
Let F [AsPT ‚àís ] denote the quantity in Equation (21), which is an upper bound on failure probability of the hybrid algorithm AsPT ‚àís . We just said that F [PT ] ‚â§ Œ¥. We now prove that for all
s ‚àà {0, 1,...,T ‚àí 1}, F [As+1PT ‚àís‚àí1] ‚â§ F [AsPT ‚àís ], thus proving that F [AT ] ‚â§ Œ¥, i.e., running the
algorithm A for all the T steps of stage r results in a failure with probability at most Œ¥.
Assuming that for all s < p, the algorithm A has been defined for the first s + 1 steps in such a
way that F [As+1PT ‚àís‚àí1] ‚â§ F [AsPT ‚àís ], we now define A for the p + 1th step of stage r in a way
Journal of the ACM, Vol. 66, No. 1, Article 7. Publication date: January 2019.          
Near Optimal Online Algorithms and Fast Approximation Algorithms 7:35
that will ensure that F [Ap+1PT ‚àíp‚àí1] ‚â§ F [ApPT ‚àíp ]. We have
F [Ap+1
Pm‚àíp‚àí1
] =

i
E
	
(1 + œµ )
Sp+1 (X A
i ) Œ≥ ci

1 + œµ
mŒ≥ T ‚àíp‚àí1


(1 + œµ )
(1+œµ ) T
mŒ≥
+

i
E
	
(1 ‚àí œµ )
Sp+1 (Y A
i )
Œ≥ di

1 ‚àí œµ
mŒ≥ T ‚àíp‚àí1


(1 ‚àí œµ )
(1‚àíœµ ) T
mŒ≥
‚â§

i
E
	
(1 + œµ )
Sp (X A
i ) Œ≥ ci

1 + œµ
X A
i,p+1
Œ≥ ci
 
1 + œµ
mŒ≥ T ‚àíp‚àí1


(1 + œµ )
(1+œµ ) T
mŒ≥
+

i
E
	
(1 ‚àí œµ )
Sp (Y A
i )
Œ≥ di

1 ‚àí œµ
Y A
i,p+1
Œ≥ di
 
1 ‚àí œµ
mŒ≥ T ‚àíp‚àí1


(1 ‚àí œµ )
(1‚àíœµ ) T
mŒ≥
. (22)
Define
œïi,s = 1
ci
‚é°
‚é¢
‚é¢
‚é¢
‚é¢
‚é¢
‚é¢
‚é¢
‚é£
(1 + œµ )
Ss (X A
i ) Œ≥ ci

1 + œµ
mŒ≥ T ‚àís‚àí1
(1 + œµ )
(1+œµ ) T
mŒ≥
‚é§
‚é•
‚é•
‚é•
‚é•
‚é•
‚é•
‚é•
‚é¶
; œài,s = 1
di
‚é°
‚é¢
‚é¢
‚é¢
‚é¢
‚é¢
‚é¢
‚é¢
‚é£
(1 ‚àí œµ )
Ss (Y A
i )
Œ≥ di

1 ‚àí œµ
mŒ≥ T ‚àís‚àí1
(1 ‚àí œµ )
(1‚àíœµ ) T
mŒ≥
‚é§
‚é•
‚é•
‚é•
‚é•
‚é•
‚é•
‚é•
‚é¶
.
Define the step p + 1 of algorithm A as picking the following option k for request j:
k‚àó = arg min k ‚ààK‚à™{‚ä•}
‚éß‚é™
‚é®
‚é™
‚é©

i
aijk ¬∑ œïi,p ‚àí

i
wijk ¬∑ œài,p
‚é´‚é™
‚é¨
‚é™
‚é≠
.
By the above definition of step p + 1 of algorithm A, it follows that for any two algorithms with
the first p steps being identical, and the last T ‚àí p ‚àí 1 steps following the Hypothetical-Oblivious
algorithm P, algorithm A‚Äôs p + 1th step is the one that minimizes expression (22). In particular,
it follows that expression (22) is upper bounded by the same expression where the p + 1thstep is
according to X‚àó
i,p+1 and Y‚àó
i,p+1, i.e., we replace X A
i,p+1 by X‚àó
i,p+1 and Y A
i,p+1 by Y‚àó
i,p+1. Therefore, we
have
F [Ap+1
PT ‚àíp‚àí1
] ‚â§

i
E
	
(1 + œµ )
Sp (X A
i ) Œ≥ ci
	
1 + œµ
X‚àó
i,p+1
Œ≥ ci

 
1 + œµ
mŒ≥ T ‚àíp‚àí1


(1 + œµ )
(1+œµ ) T
mŒ≥
+

i
E
	
(1 ‚àí œµ )
Sp (Y A
i )
Œ≥ di
	
1 ‚àí œµ
Y ‚àó
i,T +p+1
Œ≥ di

 
1 ‚àí œµ
mŒ≥ T ‚àíp‚àí1


(1 ‚àí œµ )
(1‚àíœµ ) T
mŒ≥
Journal of the ACM, Vol. 66, No. 1, Article 7. Publication date: January 2019.        
7:36 N. R. Devanur et al.
‚â§

i
E
	
(1 + œµ )
Sp (X A
i ) Œ≥ ci

1 + œµ
mŒ≥  1 + œµ
mŒ≥ T ‚àíp‚àí1


(1 + œµ )
(1+œµ ) T
mŒ≥
+

i
E
	
(1 ‚àí œµ )
Sp (Y A
i )
Œ≥ di

1 ‚àí œµ
mŒ≥  1 ‚àí œµ
mŒ≥ T ‚àíp‚àí1


(1 ‚àí œµ )
(1‚àíœµ ) T
mŒ≥
= F [ApPT ‚àíp ].
The NO Case. We now proceed to prove that when the real answer is NO, our algorithm says
NO with a probability at least 1 ‚àí Œ¥. To prove this result (formally stated in Lemma 6.3), we use as
a tool the fact that when the integer program in Equation (18) is in the NO case where even a slack
of 3œµ (1 + œµ ) will not make it feasible, then even the LP relaxation of Equation (18) will be infeasible
with a slack of 2œµ. We prove this statement now by proving its contrapositive in Lemma 6.1.
Lemma 6.1. If the LP relaxation of (18) is feasible with a slack of s, then the integer program in
Equation (18) is feasible with a slack of s(1 + œµ ) + œµ.
Proof. To prove this, we write the LP relaxation of the integer program in Equation (18) slightly
differently below.
Primal and dual LPs corresponding to integer program in Equation (18) (23)
Primal LP corresponding to IP (18) Dual LP corresponding to IP (18)
Minimize Œª s.t. Maximize
i (œÅi ‚àí Œ±i ) ‚àí
j Œ≤j s.t.
‚àÄ i, Œª ‚àí
j,k
aijk xj,k
ci ‚â• ‚àí1 ‚àÄ j, k, Œ≤j ‚â•
i

œÅi
wijk
di ‚àí Œ±i
aijk
ci

‚àÄ i, Œª +
j,k
wijk xj,k
di ‚â• 1
i (Œ±i + œÅi ) ‚â§ 1
‚àÄ j,

k xj,k ‚â§ 1 ‚àÄ i, Œ±i, œÅi ‚â• 0
‚àÄ j, k, xj,k ‚â• 0 ‚àÄ j, Œ≤j ‚â• 0.
Œª ‚â• 0
The optimal value Œª‚àó of the primal LP in Equation (23) represents the slack in the YES/NO problem, i.e., if Œª‚àó = 0, then we have zero slack and hence are in the YES case. Else, we are in the
NO case with a slack equal to Œª‚àó. Given this, all we have to show is that when the LP in Equation (23) has an optimal value of Œª‚àó, then the corresponding integer program‚Äôs optimal solution is
at most Œª‚àó (1 + œµ ) + œµ. To see this is true, let x‚àó
j,k denote the optimal solution to primal LP (23). Consider the integral solution that does a randomized rounding of the x‚àó
j,k ‚Äôs and allocates according
to these rounded integers, and let Xjk be the corresponding {0, 1} random variable. Let random
variable Xij =
k
aijkXjk
ci . By the definition of LP (23), we have E[

j Xij] ‚â§ 1 + Œª‚àó. Noting that
each of the Xij ‚Äôs is at most Œ≥ , by Chernoff bounds it follows that Pr[

j Xij ‚â• (1 + Œª‚àó)(1 + œµ )] ‚â§
e
‚àí œµ2
4Œ≥ , which given that Œ≥ = O( œµ 2
log(n/œµ ) ), is at most œµ
2n (the probability derivation is just like the
derivation in Section 3.1). Likewise, if we define by Yij the random variable
k
wijkXjk
di , then by
Journal of the ACM, Vol. 66, No. 1, Article 7. Publication date: January 2019.             
Near Optimal Online Algorithms and Fast Approximation Algorithms 7:37
the definition of LP (23), we have E[

j Yij] ‚â• 1 ‚àí Œª‚àó. By Chernoff bounds, we get an identical argument, we get Pr[

j Yij ‚â§ (1 ‚àí Œª‚àó)(1 ‚àí œµ )] ‚â§ œµ
2n . By doing a union bound over the 2n Chernoff
bounds, we have that the randomized rounding integer solution is feasible with an optimal value of
at most Œª‚àó + œµ + Œª‚àó
œµ with probability at least 1 ‚àí œµ. This means that there exists an integer solution
of value Œª‚àó + œµ + Œª‚àó
œµ, and this proves the lemma.
Corollary 6.2. For a NO instance with a slack of 3œµ (1 + œµ ), the LP relaxation of the instance is
still infeasible with a slack of 2œµ. In particular, this implies that the optimal value of the primal Œª‚àó
in Equation (23) is at least 2œµ, and likewise the optimal dual value in Equation (23) is
i (œÅ‚àó
i ‚àí Œ±‚àó
i ) ‚àí
j Œ≤‚àó
j ‚â• 2œµ.
Lemma 6.3. For a NO instance, if T ‚â• Œò( Œ≥m log(n/Œ¥ )
œµ 2 ), then
Pr
‚é°
‚é¢
‚é¢
‚é¢
‚é¢
‚é¢
‚é£
maxi
ST

X A
i

ci
<
T
m (1 + œµ ) & min
i
ST

Y A
i

di
>
T
m (1 ‚àí œµ )
‚é§
‚é•
‚é•
‚é•
‚é•
‚é•
‚é¶
‚â§ Œ¥ .
Proof. Let R denote the set of requests sampled. Consider the following LP.
Sampled primal and dual LPs (24)
Sampled primal LP Sampled dual LP
Minimize Œª s.t. Maximize T
m

i (œÅi ‚àí Œ±i ) ‚àí
j ‚ààR Œ≤j s.t.
‚àÄ i, Œª ‚àí
j ‚ààR,k
aijk xj,k
ci ‚â• ‚àíT
m ‚àÄ j ‚àà R, k, Œ≤j ‚â•
i

œÅi
wijk
di ‚àí Œ±i
aijk
ci

‚àÄ i, Œª +
j ‚ààR,k
wijk xj,k
di ‚â• T
m

i (Œ±i + œÅi ) ‚â§ 1
‚àÄ j ‚àà R,

k xj,k ‚â§ 1 ‚àÄ i, Œ±i, œÅi ‚â• 0
‚àÄ j, k, xj,k ‚â• 0 ‚àÄ j ‚àà R, Œ≤j ‚â• 0.
Œª ‚â• 0
If the primal in LP (24) has an optimal objective value at least T œµ
m , then by definition of our
Algorithm 5, we would have declared NO, i.e., if the sampled LP itself had a slack of œµ (scaled by
T
m ), then no integral allocation based on those samples can obtain a smaller slack. We now show
that by picking T = Œò( Œ≥m ln(n/Œ¥ )
œµ 2 ), the above LP (24) will have its optimal objective value at least
T œµ
m , with a probability at least 1 ‚àí Œ¥. This makes our algorithm answer NO with a probability at
least 1 ‚àí Œ¥.
Now, the primal of LP (24) has an optimal value equal to that of the dual, which in turn is lower
bounded by the value of dual at any feasible solution. One such feasible solution is Œ±‚àó, Œ≤‚àó, œÅ‚àó, which
is the optimal solution to the full version of the dual in LP (24), namely the one written in LP (23),
where R = [m],T = m. This is because the set of constraints in the full version of the dual is clearly
a superset of the constraints in the dual of LP (24). Thus, the optimal value of the primal of LP (24)
is lower bounded by value of dual at Œ±‚àó, Œ≤‚àó, œÅ‚àó, which is
= T
m 


i
œÅ‚àó
i ‚àí Œ±‚àó
i


‚àí

j ‚ààR
Œ≤‚àó
j . (25)
Journal of the ACM, Vol. 66, No. 1, Article 7. Publication date: January 2019.              
7:38 N. R. Devanur et al.
For proceeding further in lower bounding (25), we apply Chernoff bounds to
j ‚ààR Œ≤‚àó
j . The fact
that the dual of the full version of LP (24) is a maximization LP, coupled with the constraints there
in imply that Œ≤‚àó
j ‚â§ Œ≥ . Further, let œÑ ‚àó denote the optimal value of the full version of LP (24), i.e.,

i (œÅ‚àó
i ‚àí Œ±‚àó
i ) ‚àí
j Œ≤‚àó
j = œÑ ‚àó. Now, the constraint
i (Œ±‚àó
i + œÅ‚àó
i ) ‚â§ 1 coupled with the fact that œÑ ‚àó ‚â• 0
implies
j Œ≤‚àó
j ‚â§ 1. We are now ready to lower bound the quantity in Equation (25). We have the
optimal solution to primal of LP (24)
‚â•
T
m 


i
œÅ‚àó
i ‚àí Œ±‚àó
i


‚àí

j ‚ààR
Œ≤‚àó
j
‚â•
T
m

i
(œÅ‚àó
i ‚àí Œ±‚àó
i ) ‚àí 



T
j Œ≤‚àó
j
m +

4T (

j Œ≤‚àó
j )Œ≥ ln(1/Œ¥ )
m





Since Œ≤‚àó
j ‚àà [0,Œ≥ ]

‚â•
TœÑ ‚àó
m ‚àí
4TŒ≥ ln(1/Œ¥ )
m
= TœÑ ‚àó
m
‚é°
‚é¢
‚é¢
‚é¢
‚é¢
‚é£
1 ‚àí
Œ≥m ln(1/Œ¥ )
T ¬∑ 4
œÑ ‚àó2
‚é§
‚é•
‚é•
‚é•
‚é•
‚é¶
, (26)
where the second inequality is a ‚Äúwith probability at least 1 ‚àí Œ¥‚Äù inequality, i.e., we apply Chernoff
bounds for
j ‚ààS Œ≤‚àó
j , along with the observation that each Œ≤‚àó
j ‚àà [0,Œ≥ ]. The third inequality follows
from
j Œ≤‚àó
j ‚â§ 1 and
i (œÅ‚àó
i ‚àí Œ±‚àó
i ) ‚àí
j Œ≤‚àó
j = œÑ ‚àó. Setting T = Œò( Œ≥m ln(n/Œ¥ )
œµ 2 ) with an appropriate constant inside the Œò, coupled with the fact that œÑ ‚àó ‚â• 2œµ in the NO case (see Corollary 6.2), it is easy
to verify that the quantity in Equation (26) is at least T œµ
m .
Going back to our application of Chernoff bounds above, to apply it in the form above, we
require that the multiplicative deviation from mean 4Œ≥m ln(1/Œ¥ )
T
j Œ≤‚àó
j
‚àà [0, 2e ‚àí 1]. If
j Œ≤‚àó
j ‚â• œµ2, then
this requirement would follow. Suppose, however, that
j Œ≤‚àó
j < œµ2. Since we are happy if the excess
over mean is at most T œµ
m , let us look for a multiplicative error of T œµ
m
T
j Œ≤‚àó
j
m
. Based on the fact that

j Œ≤‚àó
j < œµ2 the multiplicative error can be seen to be at least 1/œµ that is larger than 2e ‚àí 1 when
œµ < 1
2e‚àí1 . We now use the version of Chernoff bounds for multiplicative error larger than 2e ‚àí 1,
which gives us that a deviation of T œµ
m occurs with a probability at most 2
‚àí(1+ T œµ
m
T
j Œ≤‚àó
j m
)
T
j Œ≤‚àó
j
mŒ≥
, where
the division by Œ≥ is because of the fact that Œ≤‚àó
j ‚â§ Œ≥ . This probability is at most ( Œ¥
n )
1/œµ , which is at
most Œ¥.
The proofs for the YES and NO cases together prove Theorem 2.5.
7 SPECIAL CASES OF THE RESOURCE ALLOCATION FRAMEWORK
We now list the problems that are special cases of the resource allocation framework and have
been previously considered. The Adwords and Display ads special cases were already discussed in
Section 2.2.
7.1 Network Routing and Load Balancing
Consider a graph (either undirected or directed) with edge capacities. Requests arrive online; a
request j consists of a source-sink pair, (sj,tj) and a bandwidth œÅj . To satisfy a request, a capacity
of œÅj must be allocated to it on every edge along some path from sj to tj in the graph. In the
Journal of the ACM, Vol. 66, No. 1, Article 7. Publication date: January 2019.                      
Near Optimal Online Algorithms and Fast Approximation Algorithms 7:39
throughput maximization version, the objective is to maximize the number of satisfied requests
while not allocating more bandwidth than the available capacity for each edge (different requests
could have different values on them, and one could also consider maximizing the total value of
the satisfied requests). Our Algorithm 2 for resource allocation framework directly applies here
and the approximation guarantee there directly carries over. Kamath et al. [16] consider a different
version of this problem where requests according to a Poisson process with unknown arrival rates.
Each request has an associated holding time that is assumed to be exponentially distributed, and
once a request has been served, the bandwidth it uses up gets freed after its holding time (this
is unlike our setting where once a certain amount of resource capacity has been consumed, it
remains unavailable to all future requests). Some aspects of the distribution are assumed to be
known, namely that the algorithm knows the average rate of profit generated by all the incoming
circuits, the average holding time, and also the target offline optimal offline solution that the online
algorithm is aiming to approximate (again this is unlike our setting where no aspect of the request
distribution is known to the algorithm, and there could even be some adversarial aspects like in
the ASI model). When each request consumes at most Œ≥ fraction of any edge‚Äôs bandwidth, Kamath
et al. [16] give an online algorithm that achieves an expected profit of (1 ‚àí œµ ) times the optimal
offline solution when Œ≥ = O( œµ 2
log n ).
7.2 Combinatorial Auctions
Suppose we have n items for sale, with ci copies of item i. Bidders arrive online, and bidder j has
a utility function Uj : 2[n] ‚Üí R. If we posted prices pi for each item i, then bidder j buys a bundle
S that maximizes Uj (S) ‚àí
i ‚ààS pi . We assume that bidders can compute such a bundle. The goal is
to maximize social welfare, the total utility of all the bidders, subject to the supply constraint that
there are only ci copies of itemi. First, incentive constraints aside, this problem can be written as an
LP in the resource allocation framework. The items are the resources and agents arriving online are
the requests. All the different subsets of items form the set of options. The utility Uj (S) represents
the profit wj,S of serving agent j through option S, i.e., subset S. If an item i ‚àà S, then ai,j,S = 1 for
all j and zero otherwise. Incentive constraints aside, our algorithm for resource allocation at step
s will choose the option k‚àó (or, equivalently, the bundle S) as specified in point 8 of Algorithm 2,
i.e., minimize the potential function. That is, if step s falls in stage r, then
k‚àó = arg mink
‚éß‚é™
‚é®
‚é™
‚é©

i
aijk ¬∑ œïr
i,s‚àí1 ‚àí wj,k ¬∑ œàr
s‚àí1
‚é´‚é™
‚é¨
‚é™
‚é≠
(note that unlike Algorithm 2 there is no subscripting for wj,k ). This can be equivalently written
as
k‚àó = arg max k
‚éß‚é™
‚é®
‚é™
‚é©
wj,k ¬∑ œàr
s‚àí1 ‚àí

i
aijk ¬∑ œïr
i,s‚àí1
‚é´‚é™
‚é¨
‚é™
‚é≠
.
Now, maximizing the above expression at step s is the same as picking the k to maximize wj,k ‚àí
i pi (s)aijk , wherepi (s) = œïr
i,s‚àí1
œà r
s‚àí1
. Thus, if we post a price ofpi (s) on itemi for bidder numbers, then
he will do exactly what the algorithm would have done otherwise. Suppose that the bidders are
i.i.d. samples from some distribution (or they arrive as in the adversarial stochastic input model).
We can use Theorem 2.2 to get an incentive compatible posted price auction9 with a competitive
ratio of 1 ‚àí O(œµ ) whenever Œ≥ = mini{ci} ‚â• Œ©(log(n/œµ )
œµ 2 ). Further, if an analog of Theorem 2.2 also
9Here we assume that each agent reveals his true utility function after he makes his purchase. This information is necessary
to compute the prices to be charged for future agents.
Journal of the ACM, Vol. 66, No. 1, Article 7. Publication date: January 2019.    
7:40 N. R. Devanur et al.
holds in the random permutation model, then we get a similar result for combinatorial auctions in
the offline case: We simply consider the bidders one by one in a random order.