We give fault-tolerant algorithms for establishing synchrony in distributed systems in which each of the n
nodes has its own clock. Our algorithms operate in a very strong fault model: we require self-stabilisation, i.e.,
the initial state of the system may be arbitrary, and there can be up to f < n/3 ongoing Byzantine faults, i.e.,
nodes that deviate from the protocol in an arbitrary manner. Furthermore, we assume that the local clocks of
the nodes may progress at different speeds (clock drift) and communication has bounded delay. In this model,
we study the pulse synchronisation problem, where the task is to guarantee that eventually all correct nodes
generate well-separated local pulse events (i.e., unlabelled logical clock ticks) in a synchronised manner.
Compared to prior work, we achieve exponential improvements in stabilisation time and the number of
communicated bits, and give the first sublinear-time algorithm for the problem:
• In the deterministic setting, the state-of-the-art solutions stabilise in time Θ(f ) and have each node
broadcast Θ(f log f ) bits per time unit. We exponentially reduce the number of bits broadcasted per
time unit to Θ(log f ) while retaining the same stabilisation time.
• In the randomised setting, the state-of-the-art solutions stabilise in time Θ(f ) and have each node
broadcast O(1) bits per time unit. We exponentially reduce the stabilisation time to polylog f while
each node broadcasts polylog f bits per time unit.
These results are obtained by means of a recursive approach reducing the above task of self-stabilising
pulse synchronisation in the bounded-delay model to non-self-stabilising binary consensus in the synchronous model. In general, our approach introduces at most logarithmic overheads in terms of stabilisation time
and broadcasted bits over the underlying consensus routine.
CCS Concepts: • Theory of computation → Distributed algorithms;
Additional Key Words and Phrases: Transient faults, byzantine faults, clock drift, agreement
1 INTRODUCTION
Many of the most fundamental problems in distributed computing relate to timing and fault tolerance. Even though most distributed systems are inherently asynchronous, it is often convenient
to design such systems by assuming some degree of synchrony provided by reliable global or
distributed clocks. For example, the vast majority of existing Very Large Scale Integrated (VLSI)
circuits operate according to the synchronous paradigm: an internal clock signal is distributed
throughout the chip neatly controlling alternation between computation and communication
steps. Of course, establishing the synchronous abstraction is of high interest in numerous other
large-scale distributed systems, as it makes the design of algorithms considerably easier.
However, as the accuracy and availability of the clock signal is typically one of the most basic
assumptions, clocking errors affect system behavior in unpredictable ways that are often hard—if
not impossible—to tackle at higher system layers. Therefore, reliably generating and distributing
a joint clock is an essential task in distributed systems. Unfortunately, the cost of providing faulttolerant synchronisation and clocking is still poorly understood.
1.1 Pulse Synchronisation
In this work, we study the self-stabilising Byzantine pulse synchronisation problem [13, 16], which
requires the system to achieve synchronisation despite severe faults. We assume a fully connected
message-passing system of n nodes, where
(1) an unbounded number of transient faults may occur anywhere in the network, and
(2) up to f < n/3 of the nodes can be faulty and exhibit arbitrary ongoing misbehaviour.
In particular, the transient faults may arbitrarily corrupt the state of the nodes and result in loss of
synchrony. Moreover, the nodes that remain faulty may deviate from any given protocol, behave
adversarially, and collude to disrupt the other nodes by sending them different misinformation even
after transient faults have ceased. Note that this also covers faults of the communication network,
as we may map faults of communication links to one of their respective endpoints. The goal is
now to (re-)establish synchronisation once transient faults cease, despite up to f < n/3 Byzantine
nodes. That is, we need to consider algorithms that are simultaneously (1) self-stabilising [7, 15]
and (2) Byzantine fault-tolerant [23].
More specifically, the problem is as follows: after transient faults cease, no matter what is the
initial state of the system, the choice of up to f < n/3 faulty nodes, and the behaviour of the faulty
nodes, we require that after a bounded stabilisation time all the non-faulty nodes must generate
pulses that
• occur almost simultaneously at each correctly operating node (i.e., have small skew), and
• satisfy specified minimum and maximum frequency bounds (accuracy).
While the system may have arbitrary behaviour during the initial stabilisation phase due to the
effects of transient faults, eventually the above conditions provide synchronised unlabelled clock
ticks for all non-faulty nodes as shown in Figure 1.
In order to meet these requirements, it is necessary that nodes can estimate the progress of time.
To this end, we assume that nodes are equipped with (continuous, real-valued) hardware clocks
that run at speeds that may vary arbitrarily within 1 and ϑ, where ϑ ∈ O(1). That is, we normalise
minimum clock speed to 1 and assume that the clocks have drift bounded by a constant. Observe
that in an asynchronous system, i.e., one in which communication and/or computation may take
unknown and unbounded time, even perfect clocks are insufficient to ensure any relative timing
guarantees between the actions of different nodes. Therefore, we additionally assume that the
nodes can send messages to each other that are received and processed within at most d ∈ Θ(1)
Journal of the ACM, Vol. 66, No. 5, Article 32. Publication date: August 2019.
Self-Stabilising Byzantine Clock Synchronisation Is Almost as Easy as Consensus 32:3
Fig. 1. An example execution of a pulse synchronisation algorithm. After the initial stabilisation phase (delimited by the vertical lines), correct nodes start generating well-separated pulses with bounded skew and
accuracy.
time. The clock speeds and message delays can behave adversarially within the respective bounds
given by ϑ and d.
In summary, this yields a highly adversarial model of computing, where further restrictions
would render the task infeasible:
(1) transient faults are arbitrary and may involve the entire network,
(2) ongoing faults are arbitrary, cover erroneous behavior of both the nodes and the communication links, and the problem is not solvable if f ≥ n/3 [11], and
(3) without any bounds on the accuracy of local clocks and on the communication delay, good
synchronisation cannot be achieved: Even without clock drift, unbounded message delays
lead to unbounded skew [27], and if clocks have unbounded drift, trivial indistinguishability arguments show that no bounds on pulse frequency can be maintained.
1.2 Background and Related Work
If one takes any one of the elements described above out of the picture, then this greatly simplifies
the problem. Without permanent/ongoing faults, the problem becomes trivial: it suffices to have
all nodes follow a designated leader. Without transient faults [22], straightforward solutions are
given by elegant classics [31, 32], where [32] also guarantees asymptotically optimal skew [27].
Taking the uncertainty of unknown message delays and drifting clocks out of the equation leads to
the so-called digital clock synchronisation problem [3, 14, 24, 26], where communication proceeds
in synchronous rounds and the task is to agree on a consistent (bounded) round counter. While
this abstraction is unrealistic as a basic system model, it yields conceptual insights into the pulse
synchronisation problem in the bounded-delay model. Moreover, it is useful to assign numbers
to pulses after pulse synchronisation is solved, in order to get a fully-fledged shared system-wide
clock [25].
In contrast to these relaxed problem formulations, the pulse synchronisation problem was initially considered to be very challenging—if not impossible—to solve. In a seminal article, Dolev and
Welch [16] proved otherwise, albeit with an algorithm having an impractical exponential stabilisation time. In a subsequent line of work, the stabilisation time was reduced to polynomial [6] and
then linear in f [9]. However, the linear-time algorithm relies on simulating multiple instances
of synchronous consensus algorithms [28] concurrently, which results in a high communication
complexity.
The consensus problem [23, 28] is one of the fundamental primitives in fault-tolerant computing.
Most relevant to this work is synchronous binary consensus with (up to f ) Byzantine faults. Here,
node v is given an input x (v) ∈ {0, 1}, and it must output y(v) ∈ {0, 1} such that the following
properties hold:
Journal of the ACM, Vol. 66, No. 5, Article 32. Publication date: August 2019.
32:4 C. Lenzen and J. Rybicki
(1) Agreement. There exists y ∈ {0, 1} such that y(v) = y for all correct nodes v.
(2) Validity. If for x ∈ {0, 1} it holds that x (v) = x for all correct nodes v, then y = x.
(3) Termination. All correct nodes eventually decide on y(v) and terminate.
In this setting, two of the above main obstacles are not present: the system is properly initialised
(no self-stabilisation required) and computation proceeds in synchronous rounds, i.e., well-ordered
compute-send-receive cycles. This confines the task to understanding how to deal with the interference from Byzantine nodes. Synchronous consensus is extremely well studied; see, e.g., [30] for
a survey. It is known that precisely (n − 1)/3 faults can be tolerated in a system of n nodes [28],
Ω(n f ) messages need to be sent in total [10], the connectivity of the communication network
must be at least 2f + 1 [8], deterministic algorithms require f + 1 rounds [1, 19], and randomised
algorithms can solve the problem in constant expected time [18]. In constrast, no non-trivial lower
bounds on the time or communication complexity of pulse synchronisation are known.
The linear-time pulse synchronisation algorithm in [9] relies on simulating (up to) one synchronous consensus instance for each node simultaneously. Accordingly, this protocol requires each
node to broadcast Θ(f log f ) bits per time unit. Moreover, the use of deterministic consensus is
crucial, as failure of any consensus instance to generate correct output within a prespecified time
bound may result in loss of synchrony, i.e., the algorithm would fail after apparent stabilisation.
In [13], these obstacles were overcome by avoiding the use of consensus by reducing the pulse
synchronisation problem to the easier task of generating at least one well-separated “resynchronisation point”, which is roughly uniformly distributed within any period of Θ(f ) time. This can be
achieved by trying to initiate such a resynchronisation point at random times, in combination with
threshold voting and locally checked timing constraints to rein in the influence of Byzantine nodes.
In a way, this seems much simpler than solving consensus, but the randomisation used to obtain a
suitable resynchronisation point strongly reminds of the power provided by shared coins [2, 3, 18,
29]—and this is exactly what the core routine of the expected constant-round consensus algorithm
from [18] provides.
1.3 Contributions
Our main result is a framework that reduces pulse synchronisation to an arbitrary synchronous
binary consensus routine at very small overheads. In other words, given any efficient algorithm
that solves consensus in the standard synchronous model of computing without self-stabilisation,
we show how to obtain an efficient algorithm that solves the self-stabilising pulse synchronisation
problem in the bounded-delay model with clock drift.
While we build upon existing techniques, our approach has many key differences. First of all,
while Dolev et al. [13] also utilise the concept of resynchronisation pulses, these are generated
probabilistically. Moreover, their approach has an inherent time bound of Ω(f ) for generating
such pulses. In contrast, we devise a new recursive scheme that allows us to (1) deterministically
generate resynchronisation pulses in Θ(f ) time and (2) probabilistically generate resynchronisation pulses in o(f ) time. To construct algorithms that generate resynchronisation pulses, we
employ resilience boosting and filtering techniques inspired by our recent line of work on digital
clock synchronisation in the synchronous model [24, 26]. One of its main motivations was to gain a
better understanding of the linear time/communication complexity barrier that research on pulse
synchronisation ran into, without being distracted by the additional timing uncertainties due to
communication delay and clock drift. The challenge here is to port these newly developed tools
from the synchronous model to the bounded-delay bounded-drift model in a way that keeps them
in working condition.
Journal of the ACM, Vol. 66, No. 5, Article 32. Publication date: August 2019.
Self-Stabilising Byzantine Clock Synchronisation Is Almost as Easy as Consensus 32:5
Table 1. Summary of Pulse Synchronisation Algorithms for f ∈ Θ(n)
time bits type notes reference
poly f O(log f ) det. [6]
O(f ) O(f log f ) det. [9]
O(f ) O(log f ) det. this work and [4]
2O (f ) O(1) rand. adversary cannot predict coin flips [16]
O(f ) O(1) rand. adversary cannot predict coin flips [13]
polylog f polylog f rand. private channels, (*) this work and [21]
O(log f ) poly f rand. private channels this work and [18]
For each respective algorithm, the first two columns give the stabilisation time and the number of bits broadcasted by a node per time unit. The third column denotes whether algorithm is deterministic or randomised. The
randomised algorithms stabilise in the given time with high probability. The fourth column indicates additional
details or model assumptions. All algorithms tolerate f < n/3 faulty nodes except for (*), where it is required that
f < n/(3 + ε ) for an arbitrary, but fixed constant ε > 0.
The key to efficiency is a recursive approach, where each node participates in only log f  consensus instances, one for each level of recursion. On each level, the overhead of the reduction over
a call to the consensus routine is a constant multiplicative factor both in time and bit complexity; concretely, this means that both complexities increase by overall factors of O(log f ). Applying
suitable consensus routines yields exponential improvements in bit complexity of deterministic and
time complexity of randomised solutions, respectively:
(1) In the deterministic setting, we exponentially reduce the number of bits each node broadcasts per time unit to Θ(log f ), while retaining Θ(f ) stabilisation time. This is achieved
by employing the phase king algorithm [4] in our construction.
(2) In the randomised setting, we exponentially reduce the stabilisation time to polylog f ,
where each node broadcasts polylog f bits per time unit. This is achieved using the algorithm by King and Saia [21]. We note that this slightly reduces resilience to f < n/(3 + ε)
for any fixed constant ε > 0 and requires private communication channels.
(3) In the randomised setting, we can also obtain a stabilisation time of O(log f ), polynomial communication complexity, and optimal resilience of f < n/3 by assuming private
communication channels. This is achieved using the consensus routine of Feldman and
Micali [18]. This almost settles the open question by Ben-Or et al. [3] whether pulse synchronisation can be solved in expected constant time.
The running time bounds of the randomised algorithms (2) and (3) hold with high probability and
the additional assumptions on resilience and private communication channels are inherited from
the employed consensus routines. Here, private communication channels mean that Byzantine
nodes must make their decision on which messages to send in round r based on knowledge of
the algorithm, inputs, and all messages faulty nodes receive up to and including round r. The
probability distribution is then over the independent internal randomness of the correct nodes
(which the adversary can only observe indirectly) and any possible randomness of the adversary.
Our framework does not impose these additional assumptions: stabilisation is guaranteed for f <
n/3 on each recursive level of our framework as soon as the underlying consensus routine succeeds
(within prespecified time bounds) constantly many times in a row. Our results and prior work are
summarised in Table 1.
Regardless of the employed consensus routine, we achieve a skew of 2d, where d is the maximum
message delay. This is optimal in our model, but overly pessimistic if the sum of communication
and computation delay is not between 0 and d, but from (d−,d+), where d+ − d−  d+. In terms
Journal of the ACM, Vol. 66, No. 5, Article 32. Publication date: August 2019.
32:6 C. Lenzen and J. Rybicki
of d+ and d−, a skew of Θ(d+ − d−) is asymptotically optimal [27, 32]. We remark that in [20], it is
shown how to combine the algorithms from [13] and [32] to achieve this bound without affecting
the other properties shown in [13]; we are confident that the same technique can be applied to the
algorithm proposed in this work. Finally, all our algorithms work with any clock drift parameter
1 < ϑ ≤ 1.004, that is, the nodes’ clocks can have up to 0.4% drift. In comparison, cheap quartz
oscillators achieve ϑ ≈ 1 + 10−5.
1.4 Hardness of Pulse Synchronisation
We consider our results of interest beyond the immediate improvements in complexity of the best
known algorithms for pulse synchronisation. Since our framework may employ any consensus
algorithm, it proves that pulse synchronisation is, essentially, as easy as synchronous consensus
—a problem without the requirement for self-stabilisation or any timing uncertainty. Apart from
the possibility for future improvements in consensus algorithms carrying over, this accentuates
the following fundamental open question:
Is pulse synchronisation at least as hard as synchronous consensus?
Due to the various lower bounds and impossibility results on consensus [8, 10, 19, 28] mentioned
earlier, a positive answer would immediately imply that the presented techniques are near-optimal.
However, one may speculate that pulse synchronisation may rather have the character of (synchronous) approximate agreement [12, 17], as precise synchronisation of the pulse events at
different nodes is not required. Considering that approximate agreement can be deterministically
solved in O(logc) rounds, where c is the range of the input values, a negative answer is a clear
possibility as well. Given that all currently known solutions either explicitly solve consensus,
leverage techniques that are likely to be strong enough to solve consensus, or are very slow, this
would suggest that new algorithmic techniques and insights into the problem are necessary.
2 PRELIMINARIES
In this section, we describe the model of computation, introduce notation used in the subsequent
sections, and formally define the pulse synchronisation and resynchronisation problems.
2.1 Notation
We use N = {1, 2,...} to denote positive integers and N0 = N ∪ {0}. For any k ∈ N, we define
the shorthand [k] = {0, 1,..., k − 1}. Finally, we write R+ = [0, ∞) for the set of non-negative real
numbers. For a,b ∈ R+ we use the notation [a,b) and (a,b] for half-open intervals and [a,b] for
closed intervals. Finally, we write R+ ∪ {∞} = [0, ∞].
2.2 Reference Time and Clocks
Throughout this work, we assume a global reference time that is not available to the nodes in the
distributed system. The reference time is only used to reason about the behaviour of the system. A
clock is a strictly increasing functionC : R+ → R+ that maps the reference time to local (perceived)
time. That is, at reference time t, clock C indicates that the local time is C(t). We say that a clock
C has drift at most ϑ − 1 > 0 if for any t,t ∈ R+, where t < t
, the clock satisfies
t − t ≤ C(t
) − C(t) ≤ ϑ (t − t).
That is, if we have two such clocks, then their measurements of elapsed time are at most factor ϑ
apart.
Journal of the ACM, Vol. 66, No. 5, Article 32. Publication date: August 2019.
Self-Stabilising Byzantine Clock Synchronisation Is Almost as Easy as Consensus 32:7
2.3 The Bounded-Delay Model
We consider a bounded-delay message-passing model of distributed computation. The system is
modelled as a fully connected network of n nodes, whereV denotes the set of all nodes. We assume
that each node has a unique identifier from the set [n]. Each node v ∈ V has local clock C(v) with
maximum drift ϑ − 1 for a known global constant ϑ > 1. We assume that the nodes cannot directly
read their local clock values, but instead they can set up local timeouts of predetermined length.
That is, a node v can request to be signalled afterT time units have passed on the node’s own local
clock C(v) since the timeout was started.
For communication, we assume sender authentication, that is, each node can distinguish the
senders of the messages it receives. In other words, every incoming communication link is labelled
with the identifier of the sender. Unlike in fully synchronous models, where communication and
computation proceeds in lock-step at all nodes, we consider a model in which each message has an
associated delay in (0,d). For simplicity, we assume that the maximum delay d ∈ Θ(1) is a known
constant and we consider d as the basic time unit in the system. We note that even though we
assume continous, real-valued clocks, any constant offset in clock readings, e.g., due to discrete
clocks, can be modelled by increasing d if needed.
We assume that the system can experience transient faults that arbitrarily corrupt the state of
the entire system; we formally define below what this entails in our model. Once the transient
faults cease, we assume that up to f of the n nodes in the system may remain Byzantine faulty,
that is, they have arbitrary (mis)behaviour and do not necessarily follow the given protocol. We
use F ⊆ V , where |F | ≤ f , to denote an arbitrary set of faulty nodes and G = V \ F is the set of
correct nodes.
2.4 Algorithms, Configurations, and Executions
Algorithms. We assume that each node executes a finite state machine whose state transitions
can depend on the current state of the node, the set of recently received messages, and local timeouts. Formally, an algorithm is a tuple A = (S, P,M, T , δ, μ), where
• S is a finite set of states,
• P⊆S is a subset of states that trigger a pulse event,
• M is a finite set of messages,
•T⊆ R+ × 2S is a finite set of timers,
• δ : V ×S×Mn × {0, 1}
h → S, where h = |T |, is the state transition function, and
• μ : V ×V ×S→M is a message function.
We now explain in detail how the system state evolves and algorithms operate.
Local Configurations, Timers, and Timeouts. The local configuration x (v,t) of a node v ∈ G at
time t ∈ R+ consists of
(1) its current state s(v,t) ∈ S,
(2) the state of its input channels m(v,t) ∈ Mn,
(3) its local clock value C(v,t) ∈ R+, and
(4) timer states Tk (v,t) ∈ [0,Tk ] for each (Tk , Sk ) ∈ T and k ∈ [h].
Recall that we assume that the transient faults have left the system in an arbitrary state at time
t = 0. This entails that for each node v ∈ V the initial values at t = 0 for (1)–(5) are arbitrary.
In the following, we use the shorthand Tk for timer (Tk , Sk ) ∈ T . We say that timer Tk of node
v expires at time t if Tk (v,t) changes to 0 at time t. It is expired at time t if Tk (v,t) = 0. Timers
may cause state transitions of nodes when expiring. Let e (v,t) ∈ {0, 1}
h indicate which timers are
Journal of the ACM, Vol. 66, No. 5, Article 32. Publication date: August 2019.
32:8 C. Lenzen and J. Rybicki
expired, that is, ek (v,t) = 1 ifTk is expired and e (v,t) = 0 otherwise. If at time t the value of either
m(v,t) changes (that is, the input channels of node v ∈ G are updated due to a received message)
or some local timer expires, the node updates its current state to s = δ (v,s
,m(v,t), e (v,t)), where
s is the node’s state prior to this computation. If s  s
, we say that node v transitions to state
s at time t (and write s(v,t) = s). We remark that this definition allows for the possibility that
state transitions happen in arbitrary short succession. However, our algorithms are designed such
that only a (small) constant number of transitions is possible in constant time, and computational
delays can be treated by interpreting them as part of communication delays.
For convenience, let us define the predicate Δ(v,s,t) = 1 if v ∈ G transitions to s at time t and
Δ(v,s,t) = 0 otherwise. When node v transitions to state s ∈ S, it resets all timers (Tk , Sk ) for
which s ∈ Sk . Accordingly, at each time t > 0, the timer state is defined as
Tk (v,t) = max{0,Tk (v,treset) − (C(v,t) − C(v,treset)},
where treset is the most recent time node v reset the timer Tk or time 0, that is,
treset = max({0}∪{t ≤ t : Δ(v,s,t
) = 1,s ∈ S}).
Note that Tk (v,treset) = Tk unless treset = 0, since with the exception of the arbitrary initial states
the timer state is reset to Tk at time treset.
The bottom line is that, at all timest ∈ R+, the timer state Tk (v,t) ∈ [0,Tk ] indicates how much
time needs to pass on the local clock C(v, ·) of node v until the timer expires; the rather involved
definition of how timers behave in order to achieve this property is owed to the requirement of
self-stabilisation.
Communication. We say that a node u sends the message μ(u,v,s(u,t)) ∈ M to node v at time
t, if the value of μ(u,v,s(u,t)) changes at time t. Moreover, node u is said to broadcast the message
a at time t if it sends the message a to every v at time t.
As we operate in the bounded-delay setting, sent messages do not arrive at their destinations
immediately. To model this, let the communication delay function duv : R+ → R+ be a strictly increasing function such that 0 < duv (t) − t < d. The input channels of node u ∈ G satisfy
mv (u,duv (t)) =

μ(v,u,s(v,t)) if v ∈ G
b(u,v,t) otherwise,
where b(u,v,t) ∈ M is the message a faulty nodev ∈ F decides to transmit to a correct node u ∈ G
at time t. We assume the adversary can freely choose the communication delay functions duv .
Thus, the adversary can control what correct nodes receive from faulty nodes and how long the
messages sent by correct nodes traverse (up to the maximum delay bound d). Intuitively,mv (u,t) ∈
M denotes the most recent message node v received from node u at time t. Since transient faults
may result in arbitrarily corrupted communication channels at time 0, we assume that mv (u,t) ∈
M is arbitrary for t < duv (0).
The Adversary and Executions. After fixing f ,n ∈ N and an algorithm A, we assume that an
adversary chooses
(1) the set F ⊆ V of faulty nodes such that |F | ≤ f ,
(2) the initial local configuration x (v, 0) for all v ∈ V ,
and for all t ∈ R+ and any u,v ∈ V
(3) the local clock values C(v,t),
(4) the message delay functions duv (t), and
(5) the messages b(u,v,t) sent by faulty nodes.
Journal of the ACM, Vol. 66, No. 5, Article 32. Publication date: August 2019.  
Self-Stabilising Byzantine Clock Synchronisation Is Almost as Easy as Consensus 32:9
Note that if the algorithm A is deterministic, then the adversary’s choices for (1)–(5) together
with A determine the execution, that is, local configurations x (v,t) for all v ∈ G and t ≥ 0.
Randomisation may be used in black-box calls to a consensus subroutine only. For brevity,
we postpone the discussion of randomisation to Section 7, which covers the results obtained
by utilising randomised consensus routines. We remark that minor adjustments to the above
definitions may be necessary, depending on the precise model of randomness and power of the
adversary; however, this does not affect the reasoning about our framework, which is oblivious
to how the employed consensus routine operates.
Logical State Machines and Sliding Window Memory Buffers. For ease of presentation, we do not
describe our algorithms in the above low-level state machine formalism, but instead use high-level
state machines, where state transitions are conditioned on timer expiration and sliding window
memory buffers. While these are not part of the above-described formalism, they are straightforward to implement using additional local timers and states.
Formally, we use a set X of logical states and identify each state s ∈ S with a logical state (s) ∈
X. That is, we have a surjective projection  : S→X that maps each state s onto its equivalence
class (s), i.e., the logical state. In addition, we associate all timers with some logical state, that is,
for every (Tk , Sk ) ∈ T , we have that Sk ∈ X is an equivalence class of states.
We employ sliding window buffers in our algorithms. A sliding window buffer of length T stores
the set of nodes from which (a certain type of) a message has been received within time T on
the node’s local clock. Since the local configuration x (v, 0) of a node v is arbitrary at time 0, we
have that by time T + d the contents of the sliding window buffer are guaranteed to be valid: if the
buffer of v ∈ G contains a message m from u ∈ G at time t ≥ T + d, then u must have sent an m
message to v during the interval (t −T − d,t) of reference time. Vice-versa, if u sends a messagem
at time t to v, the buffer is guaranteed to contain the message during the interval (t + d,t +T/ϑ )
of reference time. We also allow the algorithms to clear the sliding window buffers at any point in
time by removing all the messages currently contained in the buffer. That is, when a node clears
its sliding window buffer at time t, then the buffer contains no message seen before time t.
2.5 Pulse Synchronisation Algorithms
In the pulse synchronisation problem, the task is to have all the correct nodes locally generate
pulse events in an almost synchronised fashion, despite arbitrary initial states and the presence
of Byzantine faulty nodes. In addition, these pulses have to be well separated. Let p(v,t) ∈ {0, 1}
indicate whether a correct node v ∈ G generates a pulse at time t. Moreover, let pk (v,t) ∈ [t, ∞)
denote the time when node v generates the kth pulse event at or after time t and pk (v,t) = ∞ if
no such time exists. We say that the system has stabilised from time t onwards if
(1) p1 (v,t) ≤ t + Φ+ for all v ∈ G,
(2) |pk (v,t) − pk (u,t)| < σ for all u,v ∈ G and k ≥ 1,
(3) Φ− ≤ pk+1 (v,t) − min{pk (u,t) : u ∈ G} ≤ Φ+ for all v ∈ G and k ≥ 1,
where Φ− and Φ+ are the accuracy bounds controlling the separation of the generated pulses. That
is, (1) all correct nodes generate a pulse during the interval [t,t + Φ+]; (2) the kth pulse of any
two correct nodes is less than σ time apart; and (3) for any pair of correct nodes, their subsequent
pulses are at least Φ− − σ, but at most Φ+ time apart.
We say that A is an f -resilient pulse synchronisation algorithm with skew σ and accuracy
Φ = (Φ−, Φ+) with stabilisation time T (A), if for any choices of the adversary such that |F | ≤ f ,
there exists a time t ≤ T (A) such that the system stabilises from time t onwards. This scenario is
illustrated in Figure 4.
Journal of the ACM, Vol. 66, No. 5, Article 32. Publication date: August 2019.   
32:10 C. Lenzen and J. Rybicki
Fig. 2. An example execution of a resynchronisation algorithm. Eventually, all correct nodes will generate
a pulse within a small time window of length ρ. After this, all correct nodes refrain from generating a new
pulse for at least Ψ time units.
Finally, we call a pulse synchronisation algorithm A a T -pulser if the accuracy bounds satisfy
Φ−, Φ+ ∈ Θ(T ) and A has skew σ ≤ 2d. We use M(A) to denote the maximum number of bits a
correct node sends on a channel per unit time when executing algorithm A.
2.6 Resynchronisation Algorithms
In our pulse synchronisation algorithms, we use so-called resynchronisation pulses to facilitate
stabilisation. The resychronisation pulses are provided by resynchronisation algorithms that solve
a weak variant of pulse synchronisation: the guarantee is that eventually all correct nodes generate
a single resynchronisation pulse almost synchronously, which is followed by a long period of
silence (i.e., no new resynchronisation pulse). At all other times, the behaviour can be arbitrary.
See Figure 2 for illustration.
Formally, we say that B is an f -resilient resynchronisation algorithm with skew ρ and separation window Ψ that stabilises in time T (B), if the following holds: for any choices of the adversary
such that |F | ≤ f , there exists a time t ≤ T (B) such that every correct node v ∈ G locally generates a resynchronisation pulse at time r(v) ∈ [t,t + ρ) and no other resynchronisation pulse before
time t + ρ + Ψ. We call such a resynchronisation pulse good. In particular, we do not impose any
restrictions on what the nodes do outside the interval [t,t + ρ + Ψ), that is, there may be spurious
resynchronisation pulses outside this interval.
Again, we denote by M(B) the maximum number of bits a correct node sends on a channel per
unit time when executing B.
2.7 Synchronous Consensus Routines
As we rely on synchronous consensus algorithms, we briefly define the synchronous model of
computation for the sake of completeness. In the synchronous model, the computation proceeds
in discrete rounds, that is, the nodes have access to a common global clock. In each round r ∈ N
the nodes (1) send messages based on their current state, (2) receive messages, and (3) perform
local computations and update their state for the next round.
In synchronous binary consensus, we assume that each v ∈ V is given a private input bit x (v) ∈
{0, 1}, starts from a fixed initial state (no self-stabilisation), and is to compute output y(v) ∈ {0, 1}.
Journal of the ACM, Vol. 66, No. 5, Article 32. Publication date: August 2019.
Self-Stabilising Byzantine Clock Synchronisation Is Almost as Easy as Consensus 32:11
However, there are f Byzantine faulty nodes. An f -resilient synchronous consensus routine C
with round complexity T (C) guarantees:
(1) Agreement. There exists y ∈ {0, 1} such that y(v) = y for all v ∈ G.
(2) Validity. If, for x ∈ {0, 1}, it holds that x (v) = x for all v ∈ G, then y = x.
(3) Termination. Each v ∈ G decides on y(v) and terminates by round T (C).
We use M(C) to denote the maximum number of bits any v ∈ G sends to any other node in a single
round of any execution of C.
3 THE TRANSFORMATION FRAMEWORK
Our main contribution is a modular framework that allows us to turn any non-self-stabilising
synchronous consensus algorithm into a self-stabilising pulse synchronisation algorithm in the
bounded-delay model. In particular, the transformation yields only a small overhead in time and
communication complexity. As our construction is relatively involved, we opt to present it in a
top-down fashion. First, we give our main theorem together with its corollaries. Then we state the
auxiliary results we need to prove the main theorem and later discuss how these auxiliary results
are established.
3.1 The Main Result
Before we formally state our main result, we make the following definition.
Definition 1 (Family of Consensus Routines). Let R, M, N : N0 → N0 be functions that satisfy the
following conditions:
(i) for any f0, f1 ∈ N, we have N (f0 + f1) ≤ N (f0) + N (f1), and
(ii) both M(f ) and R(f ) are increasing.
We say that C, R, M, N is a family of synchronous consensus routines if for any f ≥ 0 and n ≥
N (f ), there exists a synchronous consensus routine C ∈ C such that
• C runs on n nodes and is f -resilient,
• each correct node terminates in T (C) = R(f ) rounds,
• and each correct node sends at most M(C) = M(f ) bits to any other node per round.
Our main technical result states that given such a family of consensus routines, we can obtain
pulse synchronisation algorithms with only small additional overhead. We emphasise that the
algorithms in C are not assumed to be self-stabilising.
Theorem 1. Let C, R, M, N be a family of synchronous consensus routines and f ≥ 0, n ≥ N (f ),
and 1 < ϑ ≤ 1.004. Then there exists an f -resilient R(f )-pulser A whose stabilisation time T (A) and
number of bits M(A) sent over each channel per time unit satisfy
T (A) ∈ O


d +
log

f 
k=0
R(2k )



and M(A) ∈ O


1 +
log

f 
k=0
M(2k )



,
where the sums are empty when f = 0.
In the deterministic case, the phase king algorithm [5] provides a family of synchronous consensus routines that satisfy the requirements. Since in the phase king algorithm, all nodes communicate by broadcasts (i.e., send the same information to all other nodes) and the additional communication by our framework satisfies this property as well, the same is true for the derived pulser.
Moreover, the phase king protocol achieves optimal resilience [28] with N (f ) = 3f + 1, constant
Journal of the ACM, Vol. 66, No. 5, Article 32. Publication date: August 2019.      
32:12 C. Lenzen and J. Rybicki
message size M(f ) ∈ O(1), and asymptotically optimal [19] round complexity R(f ) ∈ Θ(f ). Thus,
this immediately yields the following result.
Corollary 1. For any f ≥ 0 and n > 3f , there is a deterministic f -resilient Θ(f )-pulser over n
nodes that stabilises in O(f ) time and has correct nodes broadcast O(log f ) bits per time unit.
Employing randomised consensus algorithms in our framework is straightforward. We now
summarise the main results related to randomised pulse synchronisation algorithms; the details
are discussed later in Section 7. First, by applying our construction to a fast and communicationefficient randomised consensus algorithm, e.g., the one by King and Saia [21], we get an efficient
randomised pulse synchronisation algorithm.
Corollary 2. Suppose we have private channels. For any f ≥ 0, constantε > 0, and n > (3 + ε)f ,
there is a randomised f -resilient (polylog f )-pulser over n nodes that stabilises in polylog f time
w.h.p. and has nodes broadcast polylog f bits per time unit.
We can also utilise the constant expected time protocol by Feldman and Micali [18]. With some
care, we can show that for R(f ) ∈ O(1), Chernoff’s bound readily implies that the stabilisation
time is not only in O(log f ) in expectation, but also with high probability.
Corollary 3. Suppose we have private channels. For any f ≥ 0 and n > 3f , there is a randomised
f -resilient Θ(log f )-pulser over n nodes that stabilises inO(log f ) time w.h.p. and has nodes broadcast
poly f bits per time unit.
3.2 Proof of Theorem 1
The proof of the main result takes an inductive approach. In the inductive step, we assume two
pulse synchronisation algorithms with small resilience. We then use these to construct (via some
hoops we discuss later) a new pulse synchronisation algorithm with higher resilience. This step is
formalised in the following technical lemma, which we prove later.
Lemma 1. Let f ,n0,n1 ∈ N and define the values
n = n0 + n1, f0 = (f − 1)/2, f1 = (f − 1)/2.
Suppose there exists
• for both i ∈ {0, 1} an fi-resilient R-pulser Ai that runs on ni nodes with accuracy Φi = (Φ−
i , Φ+
i )
satisfying Φ+
i /Φ−
i ≤ φ for a sufficiently small constant φ > ϑ, and
• an f -resilient consensus routine C for a network of n nodes that has running time R and uses
messages of at most M bits.
Then there exists a R-pulser A that
• runs on n nodes and has resilience f ,
• stabilises in time T (A) ∈ max{T (A0),T (A1)} + O(R),
• sends M(A) ∈ max{M(A0), M(A1)} + O(M) bits over each channel per time unit,
• has skew 2d, and
• has accuracy bounds Φ− and Φ+ that satisfy Φ+/Φ− ≤ φ.
We observe that Theorem 1 is a relatively straightforward consequence of the above lemma.
Theorem 1. Let C, R, M, N be a family of synchronous consensus routines and f ≥ 0, n ≥ N (f ),
and 1 < ϑ ≤ 1.004. Then there exists an f -resilient R(f )-pulser A whose stabilisation time T (A) and
Journal of the ACM, Vol. 66, No. 5, Article 32. Publication date: August 2019.
Self-Stabilising Byzantine Clock Synchronisation Is Almost as Easy as Consensus 32:13
number of bits M(A) sent over each channel per time unit satisfy
T (A) ∈ O


d +
log

f 
k=0
R(2k )



and M(A) ∈ O


1 +
log

f 
k=0
M(2k )



,
where the sums are empty when f = 0.
Proof. We prove the claim for f ∈ N0 = {0} ∪ 
k ∈N0 ([2k , 2k+1) ∩ N) using induction on k. As
base case, we use f = 0. This is trivial for all n > 0, as the following algorithm shows. Let T > 0 be
arbitrary. We can pick a single node as a designated leader who generates a pulse wheneverT time
units have passed on its local clock. Whenever the leader node pulses, all other nodes observe this
within d time units. When the other nodes observe a pulse from the leader, they generate a pulse
locally. Thus, for f = 0 we obtain a T -pulser that stabilises in O(d) time and sends messages of
O(1) bits at most once every T/ϑ ∈ Θ(T ) time. Choosing T = R(0) and noting that R(0) ≥ 1 (even
without faults, consensus requires communication if n > 1), the claim follows for f = 0.
For the inductive step, consider f ∈ [2k , 2k+1) and suppose that, for all f  < 2k and n ≥ N (f 
),
there exists an f 
-resilient R(f 
)-pulser algorithm B on n nodes with
T (B) ≤ α



d +
log f 

k=0
R(2k
)



and M(B) ≤ β


1 +
log f 

k=0
M(2k
)



,
where α and β are sufficiently large constants. In particular, we can now apply Lemma 1 with
f0, f1 ≤ f /2 < 2k and any n ≥ N (f ), as N (f ) ≥ N (f0) + N (f1) guarantees that we may choose
some n0 ≥ N (f0) and n1 ≥ N (f1) such that n = n0 + n1. This yields an f -resilient R(f )-pulser A
over n nodes, with stabilisation time
T (A) ≤ max{T (A0),T (A1)} +γ R(f )
≤ α



d +
log

f /2
k=0
R(2k )



+γ R(2log f 
),
≤ α



d +
log

f 
k=0
R(2k )



,
where γ ≤ α is a constant; the second step uses that R(f ) is increasing. Similarly, the bound on
the number of sent bits follows from Lemma 1 and the induction assumption:
M(A) ≤ max{M(A0), M(A1)} +γ 
M(f )
≤ β


1 +
log

f /2
k=0
M(2k )



+γ 
M(2log f 
),
≤ β


1 +
log

f 
k=0
M(2k )



,
where γ  ≤ β is a constant and we used that M(f ) is increasing.
3.3 The Auxiliary Results
In order to show Lemma 1, we use two main ingredients: (1) a pulse synchronisation algorithm whose stabilisation mechanism is triggered by a resynchronisation pulse and (2) a
Journal of the ACM, Vol. 66, No. 5, Article 32. Publication date: August 2019.                         
32:14 C. Lenzen and J. Rybicki
resynchronisation algorithm providing the latter. These ingredients are formalised in the following
two theorems which are proven in Sections 5 and 6, respectively.
Theorem 2. Let f ≥ 0, n > 3f , and (2 + √
32)/7 > ϑ > 1. Suppose for a network of n nodes, there
exists
• an f -resilient synchronous consensus algorithm C, and
• an f -resilient resynchronisation algorithm B with skew ρ ∈ O(d) and sufficiently large separation window Ψ ∈ O(R) that tolerates clock drift of ϑ,
where C runs in R = R(f ) rounds and lets nodes send at most M = M(f ) bits per round and channel.
Then there exists φ0 (ϑ ) ∈ 1 + O(ϑ − 1) such that, for any constant φ > φ0 (ϑ ) and sufficiently large
T ∈ O(R), there exists an f -resilient pulse synchronisation algorithm A for n nodes that
• has skew σ = 2d,
• satisfies the accuracy bounds Φ− = T and Φ+ = Tφ,
• stabilises in T (B) + O(R) time, and
• has nodes send M(B) + O(M) bits per time unit and channel.
To apply the above theorem, we require suitable consensus and resynchronisation algorithms.
We rely on consensus algorithms from prior work and construct efficient resynchronisation algorithms ourselves. The idea is to combine pulse synchronisation algorithms that have low resilience
to obtain resynchronisation algorithms with high resilience.
Theorem 3. Let f ,n0,n1 ∈ N and 1 < ϑ ≤ 1.004. Define
n = n0 + n1, f0 = (f − 1)/2, f1 = (f − 1)/2.
For any Ψ ∈ Ω(1) and sufficiently small constant φ > φ0 (ϑ ), there exists a boundT0 ∈ Θ(Ψ) such that
the following claim holds. If, for both i ∈ {0, 1}, there exists pulse synchronisation algorithm Ai that
• runs on ni nodes and has resilience fi ,
• has skew σ = 2d, and
• has accuracy bounds Φ−
i = T and Φ+
i = Tφ, where T0 ≤ T and T ∈ O(Ψ),
then there exists a resynchronisation algorithm B that
• runs on n nodes and has resilience f ,
• has skew ρ ∈ O(d) and separation window of length Ψ,
• generates a resynchronisation pulse by time T (B) ∈ max{T (A0),T (A1)} + O(Ψ), and
• has nodes send M(B) ∈ max{M(A0), M(A1)} + O(1) bits per time unit and channel.
Given a suitable consensus algorithm, one can readily combine Theorems 2 and 3 to obtain
Lemma 1. Note that for both theorems, it turns out that φ0 (ϑ ) = 1 + 5(ϑ − 1)/(2 + 2ϑ − 3ϑ2) will
do. Therefore, we can reduce the problem of constructing an f -resilient pulse synchronisation
algorithm to finding algorithms that tolerate up to f /2 faults and recurse; Figure 3 illustrates
how these two types of algorithms are interleaved.
3.4 Proof of Lemma 1
Proof. From Theorem 3, we get that for any sufficiently large Ψ ∈ Θ(R), there exists a resynchronisation algorithm B with skew ρ ∈ O(d) and separation window of length Ψ that
• runs on n nodes and has resilience f ,
• stabilises in time max{T (A0),T (A1)} + O(Ψ) = max{T (A0),T (A1)} + O(R), and
• has nodes send max{M(A0), M(A1)} + O(1) bits per time unit and channel.
Journal of the ACM, Vol. 66, No. 5, Article 32. Publication date: August 2019.
Self-Stabilising Byzantine Clock Synchronisation Is Almost as Easy as Consensus 32:15
Fig. 3. Recursively building a 2-resilient pulse synchronisation algorithm A(7, 2) over 7 nodes. The construction utilises low resilience pulse synchronisation algorithms to build high resilience resynchronisation algorithms which can then be used to obtain highly resilient pulse synchronisation algorithms. Here, the base
case consists of trivial 0-resilient pulse synchronisation algorithms A(2, 0) and A(3, 0) over two and three
nodes, respectively. Two copies of A(2, 0) are used to build a 1-resilient resynchronisation algorithm B(4, 1)
over 4 nodes using Theorem 3. The resynchronisation algorithm B(4, 1) is used together with a synchronous
consensus algorithm C(4, 1) to obtain a pulse synchronisation algorithm A(4, 1) via Theorem 2. Now, the
1-resilient pulse synchronisation algorithm A(4, 1) over 4 nodes is used together with the trivial 0-resilient
algorithm A(3, 0) to obtain a two-resilient resynchronisation algorithm B(7, 2) for 7 nodes. This is then used
together with a 2-resilient consensus algorithm C(7, 2) to obtain the final pulse synchronisation algorithm
A(7, 2). White nodes represent correct nodes and black nodes represent faulty nodes. The gray blocks contain too many faulty nodes for the respective algorithms to correctly operate, and hence, they may have
arbitrary output.
We feed B and C into Theorem 2, yielding a pulse synchronisation algorithm A with the claimed
properties, as the application of Theorem 2 increases the stabilisation time by an additional O(R)
time units and adds O(M) bits per time unit and channel.
3.5 Organisation of the Remainder of the Article
We dedicate the remaining sections to fill in the details we have omitted above. Namely,
• Section 4 describes a Byzantine-tolerant pulse synchronisation algorithm that is not selfstabilising. We utilise the algorithm in Section 5, but the section also serves to provide a
gentle introduction to the notation and style of proofs we use in the following sections;
• Section 5 gives the proof of Theorem 2;
• Section 6 gives the proof of Theorem 3; and
• Section 7 extends our framework to operate with randomised consensus algorithms. This
establishes Corollaries 2 and 3.
4 BYZANTINE-TOLERANT PULSE SYNCHRONISATION
In this section, we describe a non-self-stabilising pulse synchronisation algorithm, which we utilise
later in our construction of the self-stabilising algorithm. The algorithm given here is a variant
of the Byzantine fault-tolerant clock synchronisation algorithm by Srikanth and Toeug [31] that
avoids transmitting clock values in favour of unlabelled pulses.
Journal of the ACM, Vol. 66, No. 5, Article 32. Publication date: August 2019. 
32:16 C. Lenzen and J. Rybicki
Fig. 4. An example execution of a non-self-stabilising pulse synchronisation algorithm with initialisation
signals.
4.1 Pulse Synchronisation without Self-Stabilisation
As previously mentioned, we do not require self-stabilisation for now. However, instead of assuming a synchronous start, we devise an algorithm where all correct nodes synchronously start
generating pulses once all correct nodes have received an initialisation signal within a short time
window. We will later show how to generate such initialisation signals in a self-stabilising manner.
In particular, we allow that before receiving an initialisation signal, a correct node can have arbitrary behaviour, but after a correct node has received this signal, it waits until all correct nodes have
received the signal and start to synchronously generate well-separated pulses, as shown in Figure 4.
In the following, suppose that all correct nodes receive an initialisation signal during the time
window [0, τ ). In other words, nodes can start executing the algorithm at different times, but they
all do so by some bounded (possibly non-constant) time τ . When a node receives the initialisation
signal, it immediately transitions to a special reset state, whose purpose is to consistently initialise
local memory and wait for other nodes to receive the initialisation signal as well; before this,
correct nodes can have arbitrary behaviour.
Later, we will repeatedly make use of this algorithm as a subroutine for a self-stabilising algorithm and we need to consider the possibility that there are still messages from earlier (possibly
corrupted) instances in transit, or nodes may be executing a previous instance in an incorrect way.
Given the initialisation signal, this is easily overcome by waiting for sufficient time before leaving
the starting state: waiting ϑ (τ + d) ∈ O(τ ) local time guarantees that (i) all correct nodes transitioned to the starting state and (ii) all messages sent before these transitions have arrived. Clearing
memory buffers when leaving the starting state thus ensures that no obsolete information from
previous instances is stored by correct nodes.
The goal of this section is to establish the following theorem:
Theorem 4. Let n > 1, f < n/3, and τ > 0. If every correct node receives an initialisation signal
during [0, τ ), then there exists a pulse synchronisation algorithm P that
• runs on n nodes and has resilience f ,
• has v ∈ G generate its first pulse (after the initialisation signal) at a time t0 (v) ∈ O(ϑ2
dτ ),
• has skew 2d,
• has accuracy bounds Φ− ∈ Ω(ϑd) and Φ+ ∈ O(ϑ2
d), and
• lets each node broadcast at most one bit per time unit.
4.2 Description of the Algorithm
The algorithm is illustrated in Figure 5. In the figure, the circles denote the basic logical states
(reset, start, ready, propose, pulse) of the state machine for each node. The two states reset
and start are used in the initialisation phase of the algorithm, which takes place during [0, τ ) when
Journal of the ACM, Vol. 66, No. 5, Article 32. Publication date: August 2019.
Self-Stabilising Byzantine Clock Synchronisation Is Almost as Easy as Consensus 32:17
Fig. 5. The state machine for the non-self-stabilising pulse synchronisation algorithm. State transitions
occur when the condition of the guard in the respective edge is satisfied (labelled gray boxes). Here, all
transition guards involve checking whether a local timer expires or a node has received sufficiently many
messages from nodes in state propose. The only communication that occurs is when a node transitions
to state propose; when this happens a node broadcasts this information to all others. The notation T 
indicates the expiration of a timer of length T that was reset when transitioning to the current state, that
is, T time units have passed on the local clock since the transition to the current state. The box labelled
propose indicates that a node clears its sliding window messages buffers when transitioning from reset to
start and pulse to ready. That is, the node forgets who it has “seen” in propose, the previous iteration. The
algorithm assumes that during the interval [0, τ ) all nodes transition to reset. This starts the initialisation
phase of the algorithm. Eventually, all nodes transition to pulse within a short time window and start
executing the algorithm. Whenever a node transitions to state pulse, it generates a local pulse event. Table 2
lists the constraints imposed on the timeouts.
the nodes receive their initialisation signals. In Figure 5, directed edges between the states denote
possible state transitions and labels give the conditions (transition guards) when the transition
is allowed to (and must) occur. The notation is as follows: Tk  denotes the condition that timer
(Tk , {s}) has expired, where s is the state that is left when the guard is satisfied.
The boxes labelled with propose indicate that when a node transitions to the designated state,
it clears its memory buffers immediately. Throughout this section, we use H(v,t) to denote the
total number of nodes at time t from which v has received a propose message since last clearing
its memory buffers. For the purposes of our analysis, we use K(v,t) to denote the number of
correct nodes from which v has received a propose message at time t since last clearing its sliding
window buffers. Moreover, without loss of generality, we may assume that the sliding window
buffers have infinite length, that is, messages never expire unless the buffer is explicitly cleared
during a transition to start and ready.
The constraints we impose on the timeouts are given in Table 2. The expression “> f propose
messages received” denotes the condition H(v,t) > f . For simplicity, we assume in all our descriptions that every time a node transitions to a logical state, it broadcasts the name of the state
to all other nodes. Given that we use a constant number of (logical) states per node (and in our
algorithms, nodes can only undergo a constant number of state transitions in constant time), this
requires broadcasting O(1) bits of information per time unit. In fact, closer inspection reveals that
1 bit per iteration of the cycle suffices here: the only relevant information is whether a node is in
state propose or not.
4.3 Algorithm Analysis
The algorithm relies heavily on the property that there are at most f < n/3 faulty nodes. This
allows the use of the following “vote-and-pull” technique. If some correct node receives a propose
Journal of the ACM, Vol. 66, No. 5, Article 32. Publication date: August 2019.
32:18 C. Lenzen and J. Rybicki
Table 2. The List of Conditions
Used in the Non-self-stabilising
Pulse Synchronisation
Algorithm Given in Figure 5
(1) T0/ϑ ≥ τ + d
(2) T1/ϑ ≥ (1 − 1/ϑ )T0 + τ
(3) T2/ϑ ≥ 3d
(4) T3/ϑ ≥ (1 − 1/ϑ )T2 + 2d
Recall that d, ϑ ∈ O (1) and τ is a parameter of the algorithm.
message from at least n − f different nodes at time t, then we must have that at least n − 2f > f of
these originated from correct nodes during the interval (t − d,t), as every message has a positive
delay of less than d. Furthermore, it follows that before time t + d all correct nodes receive more
than f propose messages.
In particular, this “vote-and-pull” technique is used in the transition to states propose and pulse.
Suppose at some point all nodes are in ready. If some node transitions to pulse, then it must have
observed at least n − f nodes in propose by Guard G3. This in turn implies that more than f correct
nodes have transitioned to propose. This in turn will (in short time) “pull” nodes that still remain
in state ready into state propose. Thus, Guard G3 will eventually be satisfied at all the nodes. The
same technique is also used in the transition from start to propose during the initialisation phase
of the algorithm.
Remark 1. Suppose f < n/3. Let u,v ∈ G, t ≥ d and I = (t − d,t + d). If H(v,t) ≥ n − f , then
K(u,t
) > f for some t ∈ I assuming u does not clear its message buffers during the interval I.
For all v ∈ G and t > 0, let p(v,t) ∈ {0, 1} indicate whether v transitions to state pulse at time
t. That is, we have p(v,t) = 1 if node v ∈ G transitions to state pulse at time t and p(v,t) = 0
otherwise.
Lemma 2. There exists t0 < τ +T0 +T1 + d such that for all v ∈ G it holds that p(v,t) = 1 for
t ∈ [t0,t0 + 2d).
Proof. Let v ∈ G. Node v receives the initialisation signal during some time treset(v) ∈ [0, τ )
and transitions to state reset. From reset, the node transitions to start at some time tstart(v) ∈
[treset(v) +T0/ϑ,treset(v) +T0] when the timer T0 in Guard G1 expires. Since T0/ϑ ≥ τ + d by Constraint (1), we get that tstart(v) ≥ τ + d. Thus, for all u,v ∈ G, we have treset(u) + d ≤ tstart(v).
Moreover, v transitions to propose at some time tpropose (v) ∈ [tstart(v),tstart(v) +T1] when
Guard G2 is satisfied. Hence, any v ∈ G transitions to state propose no later than time tstart(v) +
T1 ≤ treset(v) +T0 +T1 ≤ τ +T0 +T1. Let tpropose ≥ τ + d be the minimal time some node v ∈ G
transitions to state propose after transitioning to reset during [0, τ ). Observe that since a correct
node v clears its message buffers when transitioning from reset to start, we have that, for any
t ∈ [tstart(v),tpropose) ⊆ [τ + d,tpropose), the sliding window memory buffer of v contains no messages from correct nodes at time t, i.e., K(v,t) = 0 and H(v,t) ≤ f . Thus, node v will not receive
a propose message from any correct node u ∈ G before time tpropose.
Note that tpropose ∈ [(T0 +T1)/ϑ, τ +T0 +T1) by Guard G1 and Guard G2. By Constraint (2) and
our previous bounds, we have thattpropose ≥ T0/ϑ + (1 − 1/ϑ )T0 + τ = T0 + τ ≥ tstart(u) for anyu ∈
G. Hence, after timeT0 + τ , no u ∈ G clears its memory buffer before transitioning to pulse at time
tpulse (u). In particular, we now have that tpropose (v) ≤ τ +T0 +T1 and hence all nodes transition
Journal of the ACM, Vol. 66, No. 5, Article 32. Publication date: August 2019.
Self-Stabilising Byzantine Clock Synchronisation Is Almost as Easy as Consensus 32:19
to pulse by some time tpulse (v) < τ +T0 +T1 + d, as each u ∈ G must have received a propose
message by this time from least n − f correct nodes, meeting the condition of Guard G3.
Let t0 = min{tpulse (v) : v ∈ G} < τ +T0 +T1 + d be the minimal time some correct node transitions to state pulse. It remains to argue that tpulse (v) ∈ [t0,t0 + 2d). By Constraint (3), no correct
node clears its memory buffer before time t0 + 3d. Since some node v ∈ G transitioned to pulse
at time t0, we must have that its condition in Guard G3 was satisfied. That is, node v must have
received a propose message from at least n − f nodes since clearing its memory buffer at time
tstart(v), that is, H(v,t0) ≥ n − f and thus K(v,t0) > f .
As these messages must have been received after time tpropose, by then each u ∈ G already
reached state startand, by Constraint (3), no correct node can reset its propose flags again before
time t0 + 3d, it follows that K(u,t0 + d) > f for each u ∈ G. In particular, each u ∈ G transitions to
state propose by time t0 + d. It now follows that, at time t
0 < t0 + 2d, we have K(u,t
0) ≥ n − f for
allu ∈ G, implying that Guard G2 is satisfied for each suchu. Thus,tpulse (u) ∈ [t0,t
0] ⊆ [t0,t0 + 2d)
for each u ∈ G, as claimed.
Let us now fix t0 as given by the previous lemma. For every correct node v ∈ G, we define
p0 (v) = inf{t ≥ t0 : p(v,t) = 1} and pi+1 (v) = pnext(v,pi (v)),
where pnext(v,t) = inf{t > t : p(v,t
) = 1} is the next time after time t node v generates a pulse.
Lemma 3. For all i ≥ 0, there exist
ti+1 ∈ [ti + (T2 +T3)/ϑ,ti +T2 +T3 + 3d) such that pi (v) ∈ [ti,ti + 2d) for all v ∈ G.
Proof. We show the claim using induction on i. For the case i = 0, the claimp0 (v) ∈ [t0,t0 + 2d)
follows directly from Lemma 2. For the inductive step, suppose pi (v) ∈ [ti,ti + 2d) for all v ∈ G.
Each v ∈ G transitions to state ready at a time tready (v) ∈ [ti +T2/ϑ,ti + 2d +T2) by Guard G4.
Moreover, by Constraint (4), we have that tready (v) > ti +T2/ϑ ≥ ti + 3d. As no correct node transitions to propose during [ti + 2d,ti + (T2 +T3)/ϑ ), this implies that no node receives a propose
message from a correct node before the time tpropose (u) when some node u transitions to propose from ready (for the next time after ti + 3d). Observe that tpropose (u) > ti + (T2 +T3)/ϑ >
ti + 2d +T2 by Guard G5 and Constraint (4). Thus, we have tready(v) < ti + 2d +T2 < tpropose (u)
for all u,v ∈ G. Therefore, there exists a time tready < ti + 2d +T2 such that all correct nodes are
in state ready and K(v,tready) = 0 for all v ∈ G.
Next observe that tpropose (v) ≤ ti + 2d +T2 +T3 for any v ∈ G. Hence, every u ∈ G will receive a
propose message from every v ∈ G before time tpropose (v) + d ≤ ti + 3d +T2 +T3. Thus, by Guard
G3 we have that u transitions to pulse yielding that pi+1 (v) ∈ [ti + tready,ti + 3d +T2 +T3) ⊆
[ti + (T2 +T3)/ϑ,ti +T2 +T3 + 3d). Let ti+1 = inf{pi+1 (v) : v ∈ G}. We have already established
that ti+1 ∈ [ti + (T2 +T3)/ϑ,ti +T2 +T3 + 3d). Now using the same arguments as in Lemma 2, it
follows that for each u ∈ G, tpropose (u) < ti+1 + d, as u must have received more than f propose
messages before time ti+1 + d triggering the condition in Guard G5 for node u. Thus, Guard G3
will be satisfied before time ti+1 + 2d at each u ∈ G, implying that pi+1 (u) ∈ [ti+1,ti+1 + 2d) for
each u ∈ G.
Theorem 4. Let n > 1, f < n/3, and τ > 0. If every correct node receives an initialisation signal
during [0, τ ), then there exists a pulse synchronisation algorithm P that
• runs on n nodes and has resilience f ,
• has v ∈ G generate its first pulse (after the initialisation signal) at a time t0 (v) ∈ O(ϑ2
dτ ),
• has skew 2d,
• has accuracy bounds Φ− ∈ Ω(ϑd) and Φ+ ∈ O(ϑ2
d), and
• lets each node broadcast at most one bit per time unit.
Journal of the ACM, Vol. 66, No. 5, Article 32. Publication date: August 2019.  
32:20 C. Lenzen and J. Rybicki
Proof. The constraints in Table 2 are satisfied by setting
T0 = ϑ (τ + d)
T1 = ϑ2 (1 − 1/ϑ )(τ + d) + τ
T2 = ϑ3d
T3 = ϑ2 (1 − 1/ϑ )3d + 2d.
By Lemma 2, we get that there exists t0 ∈ O(ϑ2
dτ ) such that all nodes generate the first pulse
during the interval [t0,t0 + 2d). Applying Lemma 3, we get that, for all i > 0, we have that nodes
generate the ith pulse during the interval [ti,ti + 2d), where ti ∈ [ti−1 + (T2 +T3)/ϑ,ti−1 +T2 +
T3 + 3d) ⊆ [Φ−, Φ+). Note that T2 +T3 ∈ Θ(ϑ2
d) and ϑ,d ∈ O(1). These observations give the first
four properties. For the final property, observe that nodes only need to communicate when they
transition to propose. By Guard G4, correct nodes wait at least forT2/ϑ = 3d reference time before
transitioning to propose again after generating a pulse. Hence, nodes need to broadcast at most
one bit every 3d > d time.
5 SELF-STABILISING PULSE SYNCHRONISATION
In this section, we show how to use a resynchronisation algorithm and a synchronous consensus
routine to devise self-stabilising pulse synchronisation algorithms. We obtain the following result:
Theorem 2. Let f ≥ 0, n > 3f , and (2 + √
32)/7 > ϑ > 1. Suppose for a network of n nodes, there
exists
• an f -resilient synchronous consensus algorithm C, and
• an f -resilient resynchronisation algorithm B with skew ρ ∈ O(d) and sufficiently large separation window Ψ ∈ O(R) that tolerates clock drift of ϑ,
where C runs in R = R(f ) rounds and lets nodes send at most M = M(f ) bits per round and channel.
Then there exists φ0 (ϑ ) ∈ 1 + O(ϑ − 1) such that, for any constant φ > φ0 (ϑ ) and sufficiently large
T ∈ O(R), there exists an f -resilient pulse synchronisation algorithm A for n nodes that
• has skew σ = 2d,
• satisfies the accuracy bounds Φ− = T and Φ+ = Tφ,
• stabilises in T (B) + O(R) time, and
• has nodes send M(B) + O(M) bits per time unit and channel.
5.1 Overview of the Ingredients
The pulse synchronisation algorithm presented in this section consists of two state machines running in parallel:
(1) the so-called main state machine that is responsible for pulse generation, and
(2) an auxiliary state machine, which assists in initiating consensus instances and stabilisation.
The main state machine indicates when pulses are generated and handles all the communication
between nodes except for messages sent by simulated consensus instances. The latter are handled
by the auxiliary state machine. The transitions in the main state machine are governed by a series
of threshold votes, local timeouts, and signals from the auxiliary state machine.
As we aim to devise self-stabilising algorithms, the main and auxiliary state machines may be
arbitrarily initialised. To handle this, a stabilisation mechanism is used in conjunction to ensure
that, regardless of the initial state of the system, all nodes eventually manage to synchronise their
Journal of the ACM, Vol. 66, No. 5, Article 32. Publication date: August 2019. 
Self-Stabilising Byzantine Clock Synchronisation Is Almost as Easy as Consensus 32:21
Fig. 6. Constructing a self-stabilising (SS) and Byzantine fault-tolerant pulse synchronisation algorithm A
in the bounded-delay model (BD) out of Byzantine fault-tolerant non-stabilising pulse synchronisation algorithm P, synchronous consensus algorithm C, and resynchronisation algorithm B. All algorithms run on
the same set of nodes. (1) The resynchronisation algorithm B eventually outputs a good resynchronisation
pulse, which resets the stabilisation mechanism used by the auxiliary state machine. (2) The auxiliary state
machine simulates the executions of C using P. Simulations are initiated either due to nodes transitioning
to a special wait state of the main state machine (see Figure 7) or a certain time after a resynchronisation
pulse. (3) The main state machine. It generates pulses when a consensus instance outputs “1” and, when
stabilised, guarantees re-initialisation of the consensus algorithm by the auxiliary state machine.
state machines. The stabilisation mechanism relies on the following three subroutines which are
summarised in Figure 6:
(a) a resynchronisation algorithm B,
(b) the non-self-stabilising pulse synchronisation algorithm P from Section 4, and
(c) a synchronous consensus algorithm C.
Resynchronisation Pulses. Recall that resynchronisation algorithm B solves a weak variant of a
pulse synchronisation: it guarantees that eventually, within some bounded time T (B), all correct
nodes generate a good resynchronisation pulse such that no new resynchronisation pulse is generated before Ψ time has passed. Note that, at all other times, algorithm B is allowed to generate
pulses at arbitrary frequencies and not necessarily at all correct nodes. Nevertheless, at some point,
all correct nodes are bound to generate a good resynchronisation pulse in rough synchrony. We
leverage this property to cleanly re-initialise the stabilisation mechanism from time to time. Observe that the idea is somewhat similar to the use of initialisation signals in Section 4, but now
we have less control over the incoming “initialisation signals” (i.e., resynchronisation pulses). To
summarise, we assume throughout this section that every correct node v ∈ G
• receives a single resynchronisation pulse at time tv ∈ [0, ρ), and
• does not receive another resynchronisation pulse before time tv + Ψ,
where Ψ is sufficiently large value we determine later. Later in Section 6, we devise efficient algorithms that produce the needed resynchronisation pulses.
Simulating Synchronous Consensus. The two subroutines (b) and (c) are used in conjunction as
follows. We use the variant of the Srikanth-Toueg pulse synchronisation algorithm P described in
Section 4 to simulate a synchronous consensus algorithm (in the bounded-delay model). Note that
while this pulse synchronisation algorithm is not self-stabilising, it works properly even if nonfaulty nodes initialise the algorithm at different times as long as they do so within a time interval
of length τ .
Assuming that the nodes initialise the non-self-stabilising pulse synchronisation algorithm
P within at most time τ apart, it is straightforward to simulate round-based (i.e., synchronous)
Journal of the ACM, Vol. 66, No. 5, Article 32. Publication date: August 2019.
32:22 C. Lenzen and J. Rybicki
Fig. 7. The main state machine. When a node transitions to state pulse (double circle) it will generate a local
pulse event and send a pulse message to all nodes. When the node transitions to state wait it broadcasts
a wait message to all nodes. Guard G1 employs a sliding window memory buffer, which stores any pulse
messages that have arrived within time T1 (as measured by the local clock). When a correct node transitions
to pulse it resets a local T1 timeout. Once this expires, either Guard G1 or Guard G1’ become satisfied.
Similarly, the timer Twait is reset when the node transitions to wait. Once it expires, Guard G2’ is satisfied
and the node transitions from wait to recover. The node can transition to pulse state when Guard G2 is
satisfied, which requires an output 1 signal from the auxiliary state machine given in Figure 8.
algorithms: a pulse generated by P indicates that a new round of the synchronous algorithm can
be started. By setting the delay between two pulses large enough, we can ensure that
(1) all nodes have time to execute the local computations of the synchronous algorithm, and
(2) all messages related to a single round arrive before a new pulse occurs.
Employing Silent Consensus. We utilise so-called silent consensus routines in our construction.
Silent consensus routines satisfy exactly the same properties as usual consensus routines (validity,
agreement, and termination) with the addition that correct nodes send no messages in executions
in which all nodes have input 0.
Definition 2 (Silent consensus). A consensus routine is silent, if in each execution in which all
correct nodes have input 0, correct nodes send no messages.
Any synchronous consensus routine can be converted into a silent consensus routine essentially
for free. In our prior work [24], we showed that there exists a simple transformation that induces
only an overhead of two rounds while keeping all other properties of the algorithm the same.
Theorem 5 ([24]). Any consensus protocol C that runs in R rounds can be transformed into a silent
consensus protocol C that runs in R + 2 rounds. Moreover, the resilience and message size of C and
C are the same.
Thus, without loss of generality, we assume throughout this section that the given consensus
routine C is silent. Moreover, this does not introduce any asymptotic loss in the running time or
number of bits communicated.
5.2 High-level Idea of the Construction
The high-level strategy used in our construction is as follows: We run the resynchronisation algorithm in parallel to the self-stabilising pulse synchronisation algorithm we devise in this section.
The resynchronisation algorithm will send the resynchronisation signals it generates to the pulse
synchronisation algorithm as shown in Figure 6.
The pulse synchronisation algorithm consists of the main state machine given in Figure 7 and the
auxiliary state machine given in Figure 8. The auxiliary state machine is responsible for generating
the output signals that drive the main state machine (Guard G2 and Guard G2’).
Journal of the ACM, Vol. 66, No. 5, Article 32. Publication date: August 2019.
Self-Stabilising Byzantine Clock Synchronisation Is Almost as Easy as Consensus 32:23
Fig. 8. The auxiliary state machine. The auxiliary state machine is responsible for initialising and simulating
the consensus routine. The gray boxes denote states which represent the simulation of the consensus routine
C. When transitioning to either run 0 or run 1, the node locally initialises the (non-self-stabilising) pulse
synchronisation algorithm from Section 4 and a new instance of C. If the node transitions to run 0, it uses
input 0 for the consensus routine. If the node transitions to run 1, it uses input 1. When the consensus
simulation declares an output, the node transitions to either output 0 or output 1 (sending the respective
output signal to the main state machine) and immediately to state listen. The timeouts Tlisten, T2, and
Tconsensus are reset when a node transitions to the respective states that use a guard referring to them.
The timeout Tactive in Guard G3 (dashed line) is reset by the resynchronisation signal from the underlying
resynchronisation algorithm B. Both input 0 and input 1 have a self-loop that is activated if Guard G4 is
satisfied. This means that if Guard G4 is satisfied while in these states, the timer T2 is reset.
The auxiliary state machine employs a consensus routine to facilitate agreement among the
nodes on whether a new pulse should occur. If the consensus simulation outputs 1 at some node,
then the auxiliary state machine signals the main state machine to generate a pulse. Otherwise, if
the consensus instance outputs 0, then this is used to signal that something is wrong and the node
can detect that the system has not stabilised. We carefully set up our construction so that once
the system stabilises, any consensus instance run by the nodes is guaranteed to always output 1
at every correct node.
As we operate under the assumption that the initial state is arbitrary, the non-trivial part in our
construction is to get all correct nodes synchronised well enough to even start simulating consensus jointly in the first place. This is where the resynchronisation algorithm comes into play. We
make sure that the algorithm either stabilises or all nodes get “stuck” in a recovery state recover.
To deal with the latter case, we use the resynchronisation pulse to let all nodes synchronously
reset a local timeout. Once this timeout expires, nodes that are in state recover start a consensus
instance with input “1”. By the time this happens, either
Journal of the ACM, Vol. 66, No. 5, Article 32. Publication date: August 2019.
32:24 C. Lenzen and J. Rybicki
• the algorithm has already stabilised (and thus no correct node is in state recover), or
• all correct nodes are in state recover and jointly start a consensus instance that will output
“1” (by validity of the consensus routine).
In both cases, stabilisation is guaranteed.
Receiving a Resynchronisation Signal. The use of the resynchronisation signal is straightforward: when a correct node u ∈ G receives a resynchronisation signal from the underlying
resynchronisation algorithm B, node u resets its local timeout Tactive used by the auxiliary state
machine in Figure 8. Upon expiration of the timeout, Guard G3 in the auxiliary state machine is
activated only if the node is in state recover at the time.
Main State Machine. The main state machine, which is given in Figure 7, is responsible for generating the pulse events and operates as follows. If a node is in state pulse, it generates a local
pulse event and sends a pulse message to all other nodes. Now suppose a node u ∈ G transitions
to state pulse. Two things can happen:
• If a node u ∈ G is in state pulse and observes at least n − f nodes also generating a pulse
within a short enough time window (Guard G1), it is possible that all correct nodes generated a pulse in a synchronised fashion. If this happens, then Guard G1 ensures that node u
proceeds to the state wait. As the name suggests, the wait state is used to wait before
generating a new pulse, ensuring that pulses obey the desired frequency bounds.
• Otherwise, if a node is certain that not all correct nodes are synchronised, it transitions
from pulse to state recover (Guard G1’).
Once a node is in either wait or recover, it will not leave the state before the consensus algorithm
outputs “1”, as Guard G2 needs to be satisfied in order for a transition to pulse to take place.
The simulation of consensus is handled by the auxiliary state machine, which we discuss below.
The nodes use consensus to agree whether sufficiently many nodes transitioned to the wait state
within a small enough time window. If the system has stabilised, all correct nodes transition to
wait almost synchronously, and hence, after stabilisation every correct node always uses input
“1” for the consensus instance.
Once a node transitions to state wait, the node keeps track of how long it has been there. If
the node observes that it has been there longer than it would take for a consensus simulation to
complete under correct operation (indicating that the system has not yet stabilised), it transitions
to state recover. Also, if the consensus instance outputs “0”, the node knows something is wrong
and transitions to recover. During the stabilisation phase, nodes that transition to recover refrain
from using input “1” for any consensus routine before the local timeout Tactive expires; we refer to
the discussion of the auxiliary state machine.
Once the system stabilises, the behaviour of the main state machine is simple, as only Guard G1
and Guard G2 can be satisfied. This implies that correct nodes alternate between the pulse and
wait states. Under stabilised operation, we get that all correct nodes:
• transition to pulse within a time window of length 2d,
• observe that at least n − f nodes transitioned to pulse within a short enough time window
ensuring that Guard G1 is satisfied at every correct node,
• transition to wait within a time window of length O(d),
• correctly initialise a simulation of the consensus algorithm C with input “1”, as correct nodes
transitioned to wait in a synchronised fashion (see auxiliary state machine),
• all correct nodes remain in wait until Guard G2 or Guard G2’ become satisfied.
Journal of the ACM, Vol. 66, No. 5, Article 32. Publication date: August 2019.
Self-Stabilising Byzantine Clock Synchronisation Is Almost as Easy as Consensus 32:25
Finally, we ensure that (after stabilisation) all correct nodes remain in state wait in the main
state machine longer than it takes to properly initialise and simulate a consensus instance. This
is achieved by using the Twait timeout in Guard G2’. Due to the validity property of the consensus
routine and the fact that all correct nodes use input 1, this entails that Guard G2 is always satisfied
before Guard G2’, such that all correct nodes again transition to pulse within a time window of
length 2d.
Auxiliary State Machine. The auxiliary state machine given in Figure 8 is slightly more involved.
However, the basic idea is simple:
(a) nodes try to check whether at least n − f nodes transition to the wait state in a short
enough time window (that is, a time window consistent with correct operation) and
(b) then use a consensus routine to agree whether all nodes saw this.
Assuming that all correct nodes participate in the simulation of consensus, we get the following:
• If the consensus algorithm C outputs “0”, then some correct node did not see n − f nodes
transitioning to wait in a short time window, and hence, the system has not yet stabilised.
• If the consensus algorithm C outputs “1”, then all correct nodes agree that a transition to
wait happened recently.
In particular, the idea is that when the system operates correctly, the consensus simulation will
always succeed and output “1” at every correct node.
The above idea is implemented in the auxiliary state machine as follows. Suppose that a correct
node u ∈ G is in the listen state and the local timeout Tactive is not about to expire (recall that
Tactive is only reset by the resynchronisation signal). Node u remains in this state until it is certain
that at least one correct node transitions to wait in the main state machine. Once this happens,
Guard G4 is satisfied and node u transitions to the read state. In the read state, node u waits for
a while to see whether it observes (1) at least n − f nodes transitioning to wait in a short time
window or (2) less than n − f nodes doing this.
In case (1), node u can be certain that at least n − 2f > f correct nodes transitioned to wait.
Thus, node u can also be certain that every correct node observes at least f + 1 correct nodes
transitioning to wait; this will be a key property later. In case (2), node u can be certain that the
system has not stabilised. If case (1) happens, we have that Guard G5 is eventually satisfied. Node u
then transitions to input 1 indicating that node u is willing to use input “1” in the next simulation
of consensus unless it is in the recover state in the main state machine. In case (2), we get that
Guard G5’ becomes satisfied and u transitions to input 0. This means that u insists on using input
“0” for the next consensus simulation.
Once node u ∈ G transitions to either input 0 or input 1, it will remain there until the local
timeout of length T2 expires (see Guard G6, Guard G6’, and Guard G7). However, if Guard G4
becomes satisfied while node u is in either of the input states, then the local timeout is reset again.
We do this because, if Guard G4 becomes satisfied while u is in one of the input states, (i) the
same may be true for other correct nodes that are in state listen and (ii) node u can be certain
that the system has not stabilised. Resetting the timeout helps in ensuring that all correct nodes
jointly start the next consensus instance (guaranteeing correct simulation), if Guard G4 is satisfied
at all correct nodes at roughly the same time. In case this does not happen, resetting the timeout
at least makes sure that there will be a time when no correct node is currently trying to simulate
a consensus instance. These properties are critical for our proof of stabilisation.
Journal of the ACM, Vol. 66, No. 5, Article 32. Publication date: August 2019.
32:26 C. Lenzen and J. Rybicki
5.3 Outline of the Proof
The key difficulty in achieving stabilisation is to ensure the proper simulation of a consensus
routine despite the arbitrary initial state. In particular, after the transient faults cease, we might
have some nodes attempting to execute consensus, whereas some do not. Moreover, nodes that
are simulating consensus might be simulating different rounds of the consensus routine, and so
on. To show that such disarray cannot last indefinitely long, we use the following arguments:
• if some correct node attempts to use input “1” for consensus, then at least f + 1 correct
nodes have transitioned to wait in the main state machine (Lemma 4), that is, all correct
nodes see if some other correct node might be initialising a new consensus instance with
input “1” soon,
• if some correct node transitions to wait at time t, then there is a long interval of length
Θ(T2) during which no correct node transitions to wait (Lemma 5), that is, correct nodes
cannot transition to wait state too often,
• if some correct node attempts to use input “1” for consensus, then all correct nodes initialise
a new consensus instance within a time window of length τ ∈ Θ((1 − 1/ϑ )T2) (Lemma 6),
• if all correct nodes initialise a new consensus instance within a time window of length τ ,
then all correct nodes participate in the same consensus instance and successfully simulate
an entire execution of C (Lemma 7).
The idea is that the timeout T2 will be sufficiently large to ensure that consensus instances are
well-separated: if a consensus instance is initialised with input “1” at some correct node, then
there is enough time to properly simulate a complete execution of the consensus routine before
any correct node attempts to start a new instance of consensus.
Once we have established the above properties, it is easy to see that if synchronisation is established, then it persists. More specifically, we argue that if all correct nodes transition to pulse
at most time 2d apart, then all correct nodes initialise a new consensus instance within a time
window of length τ using input “1” (Lemma 8). Thus, the system stabilises if all correct nodes
eventually generate a pulse with skew at most 2d.
Accordingly, a substantial part of the proof is arguing that all nodes eventually transition to
pulse within time window of 2d. To see that this is bound to occur eventually, we consider an
interval [α, β] of length Θ(R) and use the following line of reasoning:
• if all correct nodes are simultaneously in state recover at some time before timeout Tactive
expires at any correct node, then Guard G3 in the auxiliary state machine becomes satisfied
at all correct nodes and a new consensus instance with all-1 input is initialised within a
time window of length τ (Lemma 9),
• if some correct node attempts to use input “1” during the interval [α, β], then either (a) all
correct nodes end up in recover before timeout Tactive expires at any node or (b) all correct
nodes eventually transition to pulse within time 2d (Lemma 10),
• if no correct node attempts to use input “1” during the time interval [α, β], all correct nodes
will be in state recover before the timeout Tactive expires at any node (Lemma 14).
In either of the latter two cases, we can use the first argument to guarantee stabilisation (Corollary 4 and Corollary 5). Finally, we need to argue that all the timeouts employed in the construction
can be set so that our arguments work out. The constraints related to all the timeouts are summarised in Table 3 and Lemma 16 shows that these can be satisfied. We now proceed to formalise
and prove the above arguments in detail. The structure of the proof is summarised in Figure 9.
Journal of the ACM, Vol. 66, No. 5, Article 32. Publication date: August 2019.
Self-Stabilising Byzantine Clock Synchronisation Is Almost as Easy as Consensus 32:27
Table 3. The Timeout Conditions Employed in the Construction of Section 5
(5) d, ϑ ∈ O(1)
(6) T1 = 3ϑd
(7) Tlisten = (ϑ − 1)T1 + 3ϑd
(8) T2 > ϑ (Tlisten + 3T1 + 3d)
(9) (2/ϑ − 1)T2 > 2Tlisten +Tconsensus + 5T1 + 4d
(10) τ = max 
(1 − 1/ϑ )T2 +Tlisten + d + max{Tlisten + d, 3T1 + 2d}, (1 − 1/ϑ )Tactive + ρ

(11) Tconsensus = ϑ (τ +T (R))
(12) Twait = T2 +Tconsensus
(13) Tactive ≥ 4T2 +Tlisten + ϑ (Tlisten +Twait − 5T1 − 4d + ρ)
(14) Tactive ≥ 2T2 +Tconsensus + ϑ (2Tlisten +T1 +Twait + 3d + 2T2 + 2Tconsensus)
Fig. 9. The overall structure of the proof of Theorem 2. The bold rectangles denote results that are informally
discussed in Section 5.3.
5.4 Analysing the State Machines
First let us observe that, after a short time, the arbitrary initial contents of the sliding window
message buffers have been cleared.
Remark 2. By time t = max{T1,Tlisten} + d ∈ O(ϑ2
d) the sliding window memory buffers used
in Guard G2, Guard G4, and Guard G5 for each u ∈ G have valid contents: if the buffer of Guard
G2 contains a message m from v ∈ G at any time t ≥ t, then v ∈ G sent the message m during
(t −T1 − d,t); similarly, for Guard G4 and Guard G5, this holds for the interval (t −Tlisten − d,t).
Without loss of generality, we assume that this has happened by time 0. Moreover, we assume
that every correct node received the resynchronisation signal during the time interval [0, ρ). Thus,
Journal of the ACM, Vol. 66, No. 5, Article 32. Publication date: August 2019.
32:28 C. Lenzen and J. Rybicki
the contents of message buffers are valid from time 0 on and every node has reset itsTactive timeout
during [0, ρ). Hence, the timeout Tactive expires at any node u ∈ G during [Tactive/ϑ,Tactive + ρ).
We use T (R) ∈ O(ϑ2
dR) to denote the maximum time a simulation of the R-round consensus
routine C takes when employing the non-stabilising pulse synchronisation algorithm given in
Section 4. We assume that the consensus routine C is silent, as by Theorem 5, we can convert any
consensus routine into a silent one without any asymptotic loss in the running time.
First, we highlight some useful properties of the simulation scheme implemented in the auxiliary
state machine.
Remark 3. If node v ∈ G transitions to run 0 or run 1 at time t, then the following holds:
• node v remains in the respective state during [t,t + τ ),
• node v does not execute the first round of C before time t + τ , and
• node v leaves the respective state and halts the simulation of C before time t +Tconsensus.
Now let us start by showing that if some node transitions to input 1 in the auxiliary state
machine, then there is a group of at least f + 1 correct nodes that transition to wait in the main
state machine in rough synchrony.
Lemma 4. Suppose node v ∈ G transitions to input 1 at time t ≥ Tlisten + d. Then there is a time
t ∈ (t −Tlisten − d,t) and a set A ⊆ G with |A| ≥ f + 1 such that each w ∈ A transitions to wait
during [t
,t].
Proof. Since v transitions to input 1, it must have observed at least n − f distinct wait messages within time Tlisten in order to satisfy Guard G5. As f < n/3, we have that at least f + 1 of
these messages came from nodes A ⊆ G, where |A| ≥ f + 1. The claim follows by choosing t to
be the minimal time during (t −Tlisten − d,t) at which some w ∈ A transitioned to wait.
Next we show that if some correct node transitions to wait, then this is soon followed by a long
time interval during which no correct node transitions to wait. Thus, transitions to wait are well
separated.
Lemma 5. Suppose node v ∈ G transitions to wait at time t ≤ (Tactive −T2)/ϑ. Then no u ∈ G
transitions to wait during [t + 3T1 + d,t +T2/ϑ − 2T1 − d).
Proof. Since v ∈ G transitioned to wait at time t, it must have seen at least n − f nodes transitioning to pulse during the time interval (t − 2T1,t). Since f < n/3, it follows that n − 2f ≥ f + 1
of these messages are from correct nodes. Let us denote this set of nodes by A ⊆ G.
Consider any node w ∈ A. As node w transitioned to pulse during (t − 2T1 − d,t), it transitions
to state recover or to state wait at time tw ∈ (t − 2T1 − d,t +T1). Either way, as it also transitioned to listen, transitioning to pulse again requires to satisfy Guard G3 or one of Guard G6,
Guard G6’, and Guard G7 while being in states input 0 or input 1, respectively. By assumption,
Guard G3 is not satisfied before time Tactive/ϑ ≥ t +T2/ϑ, and the other options require a timeout of T2 to expire, which takes at least time T2/ϑ. It follows that w is not in state pulse during
[t +T1,t +T2/ϑ − 2T1 − d).
We conclude that no w ∈ A is observed transitioning to pulse during [t +T1 + d,t +T2/ϑ −
2T1 − d). Since |A| > f , we get that no u ∈ G can activate Guard G1 and transition to wait during
[t + 3T1 + d,t +T2/ϑ − 2T1 − d), as n − |A| < n − f .
Using the previous lemmas, we can show that if some correct node transitions to state input 1
in the auxiliary state machine, then every correct node eventually initialises and participates in
the same new consensus instance. That is, every correct node initialises the underlying Srikanth–
Toueg pulse synchronisation algorithm within a time interval of length τ .
Journal of the ACM, Vol. 66, No. 5, Article 32. Publication date: August 2019.  
Self-Stabilising Byzantine Clock Synchronisation Is Almost as Easy as Consensus 32:29
Lemma 6. Suppose node u ∈ G transitions to input 1 at time t ∈ [Tlisten + d, (Tactive −T2)/ϑ]. Then
each v ∈ G transitions to state run 0 or state run 1 at time tv ∈ [t0,t0 + τ ), where t0 = t −Tlisten −
d +T2/ϑ. Moreover, Guard G4 cannot be satisfied at any node v ∈ G during [t + 3T1 + 2d,t ∗ +T1/ϑ ),
where t ∗ := minv ∈G {p(v,t + d)}.
Proof. By Lemma 4, there exists a set A ⊆ G such that |A| ≥ f + 1 and each w ∈ A transitions
to wait at a time tw ∈ (t −Tlisten − d,t). This implies that Guard G4, and thus also Guard G9,
becomes satisfied for v ∈ G at time t
v ∈ [t −Tlisten − d,t + d). Thus, every v ∈ G transitions to
state read, input 0, or input 1 at time t
v ; note that if v was in state input 0 or input 1 before
this happened, it transitions back to the same state due to Guard G4 being activated and resets its
local T2 timer. Moreover, by time t
v ≤ rv < t
v +Tlisten < t + d +Tlisten node v transitions to either
input 0 or input 1, as either Guard G5 or Guard G5’ becomes activated in case v transitions to
state read at time t
v .
Now we have that node v remains in either input 1 or input 0 for the interval [rv ,rv +T2/ϑ ),
as none of Guard G6, Guard G6’, and Guard G7 are satisfied before the local timer T2 expires.
Moreover, by applying Lemma 5 to any w ∈ A, we get that no v ∈ G transitions to wait during
the interval
[tw + 3T1 + d,tw +T2/ϑ − 2T1 − d) ⊇ (t + 3T1 + d,t +T2/ϑ − 2T1 −Tlisten − 2d).
Recall that for each v ∈ G, t
v < t + d. After this time, v cannot transition to pulse again without transitioning to run 0 or run 1 first. Since t +T2/ϑ − 2T1 −Tlisten − 2d > t +T1 + d by Constraint (8), we get that every w ∈ G has arrived in state wait or recover by time t +T2/ϑ − 2T1 −
Tlisten − 2d. Thus, no such node transitions to state wait during [t +T2/ϑ − 2T1 −Tlisten − 2d,t ∗ +
T1/ϑ ): first, it must transition to pulse, which requires to satisfy Guard G2, i.e., transitioning to
state output 1, and then a timeout of T1 must expire; here, we use that we already observed that
t +T2/ϑ − 2T1 −Tlisten − 2d > t + d, i.e., by definition the first node w ∈ G to transition to pulse
after time t +T2/ϑ −T1 −Tlisten − 2d does so at time t ∗. We conclude that Guard G4 cannot be satisfied at any v ∈ G during the interval [t + 3T1 + 2d,t ∗ +T1/ϑ ), i.e., the second claim of the lemma
holds.
We proceed to showing that each v ∈ G transitions to state run 0 or state run 1 at time tv ∈
[t0,t0 + τ ), i.e., the first claim of the lemma. To this end, observe that w transitions to either state
run 0 or run 1 at some time t ∈ (t
w ,t ∗). By the above observations, t ≥ rw +T2/ϑ ≥ t −Tlisten −
d +T2/ϑ = t0. Node w initialises the Srikanth-Toueg algorithm given in Figure 5 locally at time
t
. In particular, by the properties of the simulation algorithm given in Remark 3, we have that w
waits until time t + τ before starting the simulation of C, and hence,w remains in run 0 or run 1
at least until time t + τ before the simulation of C produces an output value. Thus, we get that
t ∗ ≥ t + τ ≥ t0 + τ .
Recall that each v ∈ G resets timeout T2 at time rv ∈ [t
v ,t + d +Tlisten) ⊆ [t −Tlisten − d,t +
Tlisten + d) and does not reset it during [t + 3T1 + 2d,t ∗ +T1/ϑ ), as it does not satisfy Guard G4
at any time from this interval. When T2 expires at v, it transitions to run 0 or run 1. Because
t ∗ +T1/ϑ > t0 + τ ≥ t + max{Tlisten + d, 3T1 + 2d} +T2 by Constraint (10), this happens at time
tv ∈ [t −Tlisten − d +T2/ϑ,t0 + τ ] = [t0,t0 + τ ], as claimed.
Next, we show, that if all correct nodes initialise a new instance of the Srikanth-Toueg pulse
synchronisation algorithm within a time interval of length τ , then every correct node initialises,
participates in, and successfully completes simulation of the consensus routine C.
Lemma 7. Suppose there exists a time t0 such that each node v ∈ G transitions to run 0 or run 1 at
some time tv ∈ [t0,t0 + τ ). Let t be the minimal time larger than t0 at which some u ∈ G transitions
Journal of the ACM, Vol. 66, No. 5, Article 32. Publication date: August 2019. 
32:30 C. Lenzen and J. Rybicki
to either output 0 or output 1. If Guard G4 is not satisfied at any v ∈ G during [t0,t + 2d), then
t ≤ t0 +Tconsensus/ϑ − 2d and there are times t
v ∈ [t
,t + 2d), v ∈ G, such that:
• each v ∈ G transitions to output 1 or output 0 at time t
v (termination),
• this state is the same for each v ∈ G (agreement), and
• if each v ∈ G transitioned to state run 1 at time tv , then this state is output 1 (validity).
Proof. When v ∈ G transitions to either run 0 or run 1, it sends an initialisation signal to the
non-self-stabilising Srikanth-Toueg algorithm described in Section 4. Using the clock given by this
algorithm, the nodes simulate the consensus algorithm C. If node v ∈ G enters state run 0, it uses
input “0” for C. Otherwise, if v enters run 1 it uses input “1”.
Note that Theorem 4 implies that, if all nodes initialise the Srikanth-Toueg algorithm within
time τ apart, then the simulation of C takes at most τ +T (R) ∈ O(ϑ2
d(τ + R)) time. Moreover,
all nodes will declare the output in the same round, and hence, declare the output within a time
window of 2d, as the skew of the pulses is at most 2d.
Now let us consider the simulation taking place in the auxiliary state machine. If Guard G4 is
not satisfied during [t0,t + 2d) and the timer Tconsensus does not expire at any node v ∈ G during
the simulation, then by time t + 2d ≤ t0 + τ +T (R) ∈ O(ϑ2
d(τ + R)) the nodes have simulated R
rounds of C and declared an output. By assumption, Guard G4 cannot be satisfied prior to time t +
2d. At node v ∈ G, the timer Tconsensus is reset at time tv ≥ t0. Hence, it cannot expire again earlier
than time t0 +Tconsensus/ϑ ≥ t0 + τ +T (R) by Constraint (11). Hence, the simulation succeeds.
Since the simulation of the consensus routine C completes at each v ∈ G at some time t
v ∈
[t
,t + 2d), we get that Guard G8 or Guard G9 is satisfied at time t
v at nodev. Hence,v transitions
to either of the output states depending on the output value of C. The last two claims of the lemma
follow from the agreement and validity properties of the consensus routine C.
Now we can show that if all correct nodes transition to pulse within a time window of length 2d,
then all correct nodes remain synchronised with skew 2d and controlled accuracy bounds. Thus,
the system stabilises.
Lemma 8. Suppose there exists an interval [t,t + 2d) such that for all v ∈ G it holds that p(v,tv ) =
1 for some tv ∈ [t,t + 2d). Then there exists t ∈ [t +T2/ϑ,t + (T2 +Tconsensus)/ϑ − 2d) such that
pnext(v,tv ) ∈ [t
,t + 2d) for all v ∈ G.
Proof. First, observe that if any node v ∈ G transitions to pulse at time tv , then node v transitions to state listen in the auxiliary state machine at time tv . To see this, note that node v must
have transitioned to output 1 in the auxiliary state machine at time tv in order to satisfy Guard
G2 leading to state pulse. Furthermore, once this happens, node v transitions immediately from
output 1 to listen in the auxiliary state machine. Note that no v ∈ G transitioned to wait during (tv −T2/ϑ,tv ), as v waits for at least this time before initiating another consensus instance
after transitioning to output 1. Hence, Constraint (8) ensures that no correct node stores any
wait messages from any other correct node in its wait sliding window buffer. It follows that each
v ∈ G is in state listen at time tv and cannot leave before time t +T1/ϑ.
Next, note that v ∈ G will not transition to recover before time tv +T1/ϑ ≥ t + 3d by Guard
G1 and Constraint (6). By assumption, every u ∈ G transitions to pulse by time t + 2d, and thus,
node v observes a pulse message from at least n − f correct nodes u ∈ G by time t + 3d. Thus, all
correct nodes observe a pulse message from at least n − f nodes during [t,t +T1/ϑ ) = [t,t + 3d)
satisfying Guard G1. Hence, every correct node v ∈ G transitions to wait during the interval [t +
T1/ϑ,t +T1 + 2d) and remains there until Guard G2 or Guard G2’ is activated. Denote by t
v the
next transition of v ∈ G to output 1 or output 0 after time tv (t
v := ∞ if no such time exists) and
set t := minv ∈G {t
v }. Guard G2’ cannot be activated before time min{t
,t +Twait/ϑ }.
Journal of the ACM, Vol. 66, No. 5, Article 32. Publication date: August 2019. 
Self-Stabilising Byzantine Clock Synchronisation Is Almost as Easy as Consensus 32:31
Now let us consider the auxiliary state machine. From the above reasoning, we get that every
node v ∈ G will observe at least n − f nodes transitioning from pulse to wait during the interval
[t +T1/ϑ,t +T1 + 3d). As we have Tlisten/ϑ ≥ (1 − 1/ϑ )T1 + 3d by Constraint (7), both Guard G4
and Guard G5 become satisfied for v during the same interval. Thus, node v transitions to state
input 1 during the interval [t +T1/ϑ,t +T1 + 3d). Here, we use that Guard G3 is not active at any
v ∈ G that is not in recover, implying that t ≥ min{t +T2/ϑ,t +Twait/ϑ } = t +T2/ϑ > t +T1 +
3d by Constraint (8) and Constraint (12). Thus, Guard G2’ cannot become active before time t +
T1 + 3d, yielding that each v ∈ G transitions to read before Guard G3 can become active. We claim
that v transitions to run 1 during [t + (T1 +T2)/ϑ,t +T1 +T2 + 3d) ⊆ [t +T2/ϑ,t +T2/ϑ + τ ) by
Constraint (10). Assuming otherwise, some v ∈ G would have to transition to state wait before
time T2/ϑ + τ . However, t ≥ T2/ϑ + τ by Remark 3 and t +Twait/ϑ = t + (T2 +Tconsensus)/ϑ > t +
T2/ϑ + τ by Constraint (12) and Constraint (11).
Note that t +T1 + 4d < t +T2/ϑ by Constraint (6) and Constraint (8) and that no correct node
transitions to wait again after time t +T1 + 3d before transitioning to output 1 and spending, by
Constraint (6), at least T1/ϑ > 2d time in pulse. Therefore, we can apply Lemma 7 with t0 = t +
T2/ϑ, yielding a time t < t +T2/ϑ +Tconsensus/ϑ − 2d such that eachv ∈ G transitions to output 1
in the auxiliary state machine at time t
v ∈ [t
,t + 2d). This triggers a transition to pulse in the
main state machine.
5.5 Ensuring Stabilisation
We showed above that if all correct nodes eventually generate a pulse with skew 2d, then the pulse
synchronisation routine stabilises. In this section, we show that this is bound to happen. The idea
is that if stabilisation does not take place within a certain interval, then all nodes end up being
simultaneously in state recover in the main state machine and state listen in the auxiliary state
machine, until eventually timeout Tactive expires. This in turn allows the “passive” stabilisation
mechanism to activate by having timer Tactive expire at every node and stabilise the system as
shown by the lemma below.
Lemma 9. Let t < Tactive/ϑ and t ∗ = minv ∈G {pnext(v,t)}. Suppose the following holds for every
node v ∈ G:
• node v is in state recover and listen at time t, and
• Guard G4 is not satisfied at v during [t,t ∗ + 2d),
Then t ∗ < Tactive + ρ +Tconsensus/ϑ and every v ∈ G transitions to pulse at time tv ∈ [t ∗,t ∗ + 2d).
Proof. Observe that Guard G3 is not satisfied before timeTactive/ϑ. As Guard G4 is not satisfied
during [t,t ∗), no correct node leaves state Tlisten before time Tactive/ϑ. Since no v ∈ G can activate
Guard G2 during [t,t ∗), we have that every v ∈ G remains in state recover during this interval.
Let t0 ∈ [Tactive/ϑ,Tactive + ρ) be the minimal time (after time t) when Guard G3 becomes satisfied
at some node v ∈ G. From the properties of the simulation algorithm given in Remark 3, we get
that no w ∈ G transitions away from run 1 before time t0 + τ ≥ Tactive/ϑ + τ .
Since τ ≥ (1 − 1/ϑ )Tactive + ρ by Constraint (10), we conclude that no w ∈ G transitions to
output 1 (and thus pulse) before time Tactive + ρ. Therefore, each w ∈ G transitions to run 1
at some time rw ∈ [Tactive/ϑ,Tactive + ρ) ⊆ [t0,t0 + τ ). Recall that Guard G4 is not satisfied during [t0,t ∗ + 2d). Hence, we can apply Lemma 7 to the interval [t0,t0 + τ ), implying that every w ∈ G transitions to output 1 at some time tw ∈ [t ∗,t ∗ + 2d). Thus, each w ∈ G transitions
from recover to pulse at time tw . Finally, Lemma 7 also guarantees that t ∗ < t0 +Tconsensus/ϑ ≤
Tactive + ρ +Tconsensus/ϑ.
Journal of the ACM, Vol. 66, No. 5, Article 32. Publication date: August 2019.  
32:32 C. Lenzen and J. Rybicki
We will now establish a series of lemmas in order to show that we can either apply Lemma 8
directly or in conjunction with Lemma 9 to guarantee stabilisation. In the remainder of this section,
we define the following abbreviations:
α = Tlisten + d
β = α +Twait +γ + 4T1 + 3d + 2δ
β = (Tactive −T2 −Tconsensus)/ϑ
δ = T1 + d + 2Tlisten +T2 +Tconsensus
γ = T2/ϑ −Tlisten − 5T1 − 3d.
Remark 4. We have that γ < δ < β ≤ β < Tactive/ϑ, where the inequality β ≤ β is equivalent
to Constraint (14).
In the following, we consider the time intervals [α, β] and [α, β
] that depend on different timeouts. We distinguish between two cases and show that in either case stabilisation is ensured. The
cases are:
• no correct node transitions to input 1 during [α, β], and
• some correct node transitions to input 1 during [α, β
].
These cases correspond to our proof strategy described in Section 5.3: If the first case occurs, all
nodes end up in the recover state and the “passive” stabilisation mechanism guarantees stabilisation after the timer Tactive expires. If the first case does not hold, then we must be in the latter case
if β ≥ β holds, which happens to be true when Constraint (14) is satisfied. In the second case, we
show that a consensus instance will be run by all correct processors, which then can be used to
ensure that all nodes agree that a pulse should be generated (output “1”) or that the system has
not stabilised (output “0”), which leads to everyone transitioning to the recover state. We start by
analysing the case where some correct node transitions to input 1 during the interval [α, β
].
Lemma 10. Suppose node v ∈ G transitions to input 1 at time t ∈ [α, β
]. Then there exists a time
t ∈ [t,Tactive/ϑ − 2d) such that one of the following holds:
(1) every u ∈ G satisfies p(u,tu ) = 1 for some tu ∈ [t
,t + 2d), or
(2) every u ∈ G is in state recover and listen at time t + 2d and Guard G4 is not satisfied at
u during [t + 2d,t ∗ + 2d], where t ∗ := minw ∈G {p(w,t + 2d)}.
Proof. By Constraint (6), we have T1/ϑ > 2d. As α ≤ t ≤ β < (Tactive −T2)/ϑ, we can apply
Lemma 6 to time t. Due to Constraint (8), we can apply Lemma 7 with time t0 = t −Tlisten − d +
T2/ϑ, yielding a time t ≤ β −Tlisten − 3d +T2/ϑ +Tconsensus/ϑ < Tactive/ϑ − 2d such that each u ∈
G transitions to the same output state output 0 or output 1 in the auxiliary state machine at
time tu ∈ [t
,t + 2d). If this state is output 1, each u ∈ G transitions to output 1 at time tu ∈
[t
,t + 2d). This implies that Guard G2 is activated and u transitions to pulse so that p(u,tu ) = 1.
If this state is output 0, Guard G2’ implies that each u ∈ G either remains in or transitions to state
recover at time tu . Moreover, node u immediately transitions to state listen in the auxiliary state
machine. Finally, note that Lemma 6 also states that Guard G4 cannot be satisfied again before time
t ∗ +T1/ϑ > t ∗ + 2d.
Corollary 4. Suppose node v ∈ G transitions to input 1 at time t ∈ [α, β
]. Then there exists
t < Tactive + ρ +Tconsensus/ϑ such that every u ∈ G satisfies p(v,tu ) = 1 for some tu ∈ [t
,t + 2d).
Proof. We apply Lemma 10. In the first case of Lemma 10, the claim immediately follows. In
the second case, it follows by applying Lemma 9.
Journal of the ACM, Vol. 66, No. 5, Article 32. Publication date: August 2019.  
Self-Stabilising Byzantine Clock Synchronisation Is Almost as Easy as Consensus 32:33
We now turn our attention to the other case, where no node v ∈ G transitions to input 1 during
the time interval [α, β].
Lemma 11. If no v ∈ G transitions to input 1 during [α, β], then no v ∈ G transitions to state run
1 during [α +T1 +Twait, β].
Proof. Observe that any v ∈ G that does not transition to pulse during [α, α +T1 +Twait] must
transition to recover at some time from that interval once Guard G2’ is satisfied. Note that leaving
state recover requires Guard G2 to be satisfied, and hence, a transition to output 1 in the auxiliary
state machine. However, as no node v ∈ G transitions to input 1 during [α, β], it follows that each
v ∈ G that is in state input 1 in the auxiliary state machine at time t ∈ [α +T1 +Twait, β] is also
in state recover in the main state machine. Thus, Guard G6 cannot be satisfied. We conclude that
no correct node transitions to state run 1 during the interval [α +T1 +Twait, β].
We now show that if Guard G4 cannot be satisfied for some time, then there exists an interval
during which no correct node is simulating a consensus instance.
Lemma 12. Let t ∈ (d, β − 2δ ) and suppose Guard G4 is not satisfied during the interval [t,t +γ ].
Then there exists a time t ∈ [t, β − δ] such that nov ∈ G is in state run 0 or run 1 during (t − d,t
].
Proof. Observe that Guard G3 cannot be satisfied before time t +γ +T2/ϑ < Tactive/ϑ at any
correct node, and by the assumption, Guard G4 is not satisfied during the interval [t,t +γ ]. We
proceed via a case analysis.
First, suppose v ∈ G is in state listen at time t. As Guard G3 or Guard G4 cannot be satisfied
during [t,t +γ ] node, v remains in listen until time t +γ . Moreover, if v leaves listen during
[t +γ ,t +γ +T2/ϑ], it must do so by transitioning to read. Hence, it cannot transition to run 0
or run 1 before Guard G6, Guard G6’, or Guard G7 is satisfied. Either way, v cannot reach states
run 0 or run 1 before time t +γ +T2/ϑ. Hence, in this case, v is not in the two execution states
during I1 = [t,t +γ +T2/ϑ ) = [t,t + 2T2/ϑ −Tlisten − 5T1 − 3d).
Let us consider the second case, where v is not in listen at time t. Note that the timer T2
cannot be reset at v during the interval [t +Tlisten,t +γ ], as this can happen only if v transitions
to input 0 or input 1 and Guard G4 cannot be satisfied during [t,t +γ ]. Hence, the only way
for this to happen is if v transitions from read to either input 0 or input 1 during the interval
[t,t +Tlisten].
It follows that v cannot transition to run 0 or run 1 during the interval (t +Tlisten +T2,t +γ +
T2/ϑ ). Moreover, if v transitions to these states before t +Tlisten +T2, then v must transition away
from them by time t +Tlisten +T2 +Tconsensus, as Guard G8 or Guard G9 become satisfied. Therefore,
nodev is in listen, read, input 0, or input 1 during I2 = [t +Tlisten +T2 +Tconsensus,t +γ +T2/ϑ ).
Applying Constraint (9), we get that
Tlisten +T2 +Tconsensus < 2T2/ϑ −Tlisten − 5T1 − 4d,
and hence, by setting t = t +Tlisten +T2 +Tconsensus + d < β − δ, we get that (t − d,t
] ⊆ I1 ∩ I2.
That is, in either case, v is not in state run 0 or run 1 during this short interval.
Next, we show that the precondition of the previous lemma will be satisfied in due time.
Lemma 13. There exists t ∈ [α +T1 +Twait + d, α + 4T1 +Twait + 3d +γ ) ⊂ (d, β − 2δ ) such that
Guard G4 is not satisfied during [t,t +γ ] at any v ∈ G.
Proof. Let t ∈ [α +T1 +Twait, α +T1 +Twait + d +γ ] be a time when somev ∈ G transitions to
state wait. If such t does not exist, the claim trivially holds for t = α +T1 +Twait + d. Otherwise,
Journal of the ACM, Vol. 66, No. 5, Article 32. Publication date: August 2019.  
32:34 C. Lenzen and J. Rybicki
since t ≤ Tlisten +T1 +Twait + 2d +γ < (Tactive −T2)/ϑ by Constraint (13), we can apply Lemma 5
to time t
, which yields that no u ∈ G transitions to wait during
[t + 3T1 + d,t +T2/ϑ − 2T1 − d) = [t −Tlisten − d,t +γ ),
where t = t +Tlisten + 3T1 + 2d. Thus, Guard G4 is not satisfied at any v ∈ G during [t,t +γ ].
Using the above statements, we can now infer that if no node transitions to input 1 for a while,
this implies that no node transitions to pulse for a certain period. Subsequently, we will use this
to conclude that then the preconditions of Lemma 9 are satisfied.
Lemma 14. Suppose no v ∈ G transitions to input 1 during [α, β]. Then no v ∈ G transitions to
pulse during [β − δ, β].
Proof. First, we apply Lemma 13 to obtain an interval [t,t +γ ] ⊆ [α +T1 +Twait + d, β − 2δ]
during which Guard G4 cannot be satisfied at any v ∈ G. Applying Lemma 12 to this interval
yields an interval (t − d,t
] ⊂ (t, β − δ] during which no v ∈ G is in state run 0 or run 1. This
implies that no node is running a consensus instance during this time interval, and moreover, no
messages from prior consensus instances are in transit to or arrive at any correct node at time t
.
In particular, any v ∈ G that (attempts to) simulate a consensus instance at time t or later must
first reinitialise the simulation by transitioning to run 0 or run 1.
Next, let us apply Lemma 11, to see that no v transitions to run 1 during the interval [α +T1 +
Twait, β] ⊃ [t
, β]. Thus, if any node v ∈ G attempts to simulate C, it must start the simulation by
transitioning to run 0. This entails that any correct node attempting to simulate C does so with
input 0. Because C is a silent consensus routine (see Definition 2), this entails that v does not send
any message related to C unless it receives one from a correct node first, and in absence of such a
message, it will not terminate with output 1. We conclude that no v ∈ G transitions to output 1
during [t
, β] ⊇ [β − δ, β]. The claim follows by observing that a transition to pulse in the main
state machine requires a transition to output 1 in the auxiliary state machine.
Lemma 15. Suppose nov ∈ G transitions to pulse during [β − δ, β]. Then, at time β, everyv ∈ G is
in state recover in the main state machine and state listen in the auxiliary state machine. Moreover,
Guard G4 is not satisfied during [β,t ∗ + 2d), where t ∗ = minv ∈G {pnext(v, β)}.
Proof. As no v ∈ G transitions pulse during [β − δ, β], either Guard G1 or Guard G2’ lead
each v ∈ G to recover. More precisely, every v ∈ G is in state recover of the main state machine
during [β − δ +T1 +Twait, β]. Next, observe that Guard G4 is not satisfied during [β − δ +T1 +
Tlisten + d, β], and because β < Tactive/ϑ, Guard G3 cannot be active at any time from this interval
either. It follows that each v ∈ G is in state listen of the auxiliary state machine during [β − δ +
T1 + d + 2Tlisten +T2 +Tconsensus, β] = [β, β], i.e., the first claim of the lemma holds. For the second
claim, observe that Guard G4 cannot be satisfied before the next transition to wait by a correct
node occurs. This cannot happen before T1/ϑ time has passed after a correct node transitioned to
pulse by Guard G1. Since T1/ϑ > 2d by Constraint (6), the claim follows.
We can now show that if no input 1 transitions occur during [α, β], then all nodes end up in
the recover state in the main state machine before Tactive timeout expires at any node. This will
eventually activate Guard G3 at every correct node, leading to a correct simulation of C with all 1
inputs.
Corollary 5. Suppose no v ∈ G transitions to input 1 during [α, β]. Then there exists a time
t < Tactive + ρ +Tconsensus such that every v ∈ G transitions to pulse at time tv ∈ [t,t + 2d).
Proof. By Lemma 14 and Lemma 15, the prerequisites of Lemma 9 are satisfied at time t = β <
Tactive/ϑ.
Journal of the ACM, Vol. 66, No. 5, Article 32. Publication date: August 2019.    
Self-Stabilising Byzantine Clock Synchronisation Is Almost as Easy as Consensus 32:35
It remains to show that the constraints in Table 3 can be satisfied for some ϑ > 1 such that all
timeouts are in O(R).
Lemma 16. Let 1 < ϑ < (2 + √
32)/7 ≈ 1.094. The constraints in Table 3 can be satisfied with all
timeouts in O(R).
Proof. Recall that R is the number of rounds the consensus routine needs to declare output and
T (R) is the time required to simulate the consensus routine. We parametrise T2 and Tactive using
X and Y, where we require that X ∈ Θ(R) is chosen so that T (R)/X ≤ ε, for a constant ε > 0 to be
determined later. We then can set
T1 := 3ϑd
Tlisten := 3ϑ2
d
T2 := X
τ := max{(1 − 1/ϑ )X + (3ϑ2 + 9ϑ + 3)d, (1 − 1/ϑ )Y + ρ}
Tconsensus := ϑ (τ + εX)
Twait := X +Tconsensus
Tactive := Y,
immediately satisfying Constraint (6), Constraint (7), Constraint (10), Constraint (11), and Constraint (12). Moreover, Constraint (8) holds by requiring thatX is at least a sufficiently large constant.
For the remaining constraints, denote by C(ϑ,d) a sufficiently large constant subsuming all
terms that depend only on ϑ and d and abbreviate T 
consensus := (ϑ − 1) max{X,Y } + εϑX (i.e., the
non-constant terms of Tconsensus). To satisfy Constraint (9), Constraint (13), and Constraint (14), it
is then sufficient to guarantee that
(2/ϑ − 1)X > T 
consensus + C(ϑ,d)
Y ≥ 4X + ϑ (X +T 
consensus) + C(ϑ,d)
Y ≥ 2X +T 
consensus + 3ϑ (X +T 
consensus) + C(ϑ,d),
where the second inequality automatically holds if the third is satisfied. We also note that Y ≥ X is
a necessary condition to satisfy the third inequality, implying that we may assume thatT 
consensus =
(ϑ − 1)Y + εX. We rearrange the remaining inequalities, yielding
(2/ϑ − 1 − ε)X > (ϑ − 1)Y + C(ϑ,d)
(2 + 2ϑ − 3ϑ2)Y ≥ (2 + 3ϑ (1 + εϑ ))X + C(ϑ,d). (1)
Recall that ϑ and C(ϑ,d) are constant, and ε is a constant under our control. Hence, these inequalities can be satisfied if and only if
(2/ϑ − 1)(2 + 2ϑ − 3ϑ2) > (2 + 3ϑ )(ϑ − 1).
The above inequality holds for all ϑ ∈ (1, (2 + √
32)/7). Note that, as ϑ,ε, andC(ϑ,d) are constants,
we can choose X ∈ Θ(R) as initially stated, implying that all timeouts are in O(R), as desired.
Corollary 6. For ϑ < (2 + √
32)/7 and suitably chosen timeouts, in any execution there exists
t ∈ O(R) such that every v ∈ G transitions to pulse during the time interval [t,t + 2d).
Proof. We choose the timeouts in accordance with Lemma 16. If nov ∈ G transitions to input 1
during [α, β], Corollary (5) yields the claim. Otherwise, some node v ∈ G transitions to state
input 1 during the interval [α, β] ⊆ [α, β
] and the claim holds by Corollary 4.
Journal of the ACM, Vol. 66, No. 5, Article 32. Publication date: August 2019.  
32:36 C. Lenzen and J. Rybicki
Next, we observe that the accuracy bounds can be set to be within a constant factor apart from
each other.
Corollary 7. Let ϑ < (2 + √
32)/7 and φ0 (ϑ ) = 1 + 5(ϑ − 1)/(2 + 2ϑ − 3ϑ2) ∈ 1 + O(ϑ − 1). For
any constant φ > φ0 (ϑ ), we can obtain accuracy bounds satisfying Φ+/Φ− ≤ φ and Φ−, Φ+ ∈ Θ(R).
Proof. By Lemma 8, the accuracy bounds we get from the constuction are Φ− = T2/ϑ and Φ+ =
(T2 +Tconsensus)/ϑ. Choosing the timeouts as in the proof of Lemma 16, we get that Φ+/Φ− = 1 +
(ϑ − 1)Y/X + ε, where ε is an arbitrarily small constant. Checking Inequality (1), we see that we can
choose Y/X = (2 + 3(1 + ε))/(2 + 2ϑ − 3ϑ2). Choosing ε sufficiently small, the claim follows.
5.6 Proof of Theorem 2
Finally, we are ready to prove the main theorem of this section.
Theorem 2. Let f ≥ 0, n > 3f , and (2 + √
32)/7 > ϑ > 1. Suppose for a network of n nodes, there
exists
• an f -resilient synchronous consensus algorithm C, and
• an f -resilient resynchronisation algorithm B with skew ρ ∈ O(d) and sufficiently large separation window Ψ ∈ O(R) that tolerates clock drift of ϑ,
where C runs in R = R(f ) rounds and lets nodes send at most M = M(f ) bits per round and channel.
Then there exists φ0 (ϑ ) ∈ 1 + O(ϑ − 1) such that, for any constant φ > φ0 (ϑ ) and sufficiently large
T ∈ O(R), there exists an f -resilient pulse synchronisation algorithm A for n nodes that
• has skew σ = 2d,
• satisfies the accuracy bounds Φ− = T and Φ+ = Tφ,
• stabilises in T (B) + O(R) time, and
• has nodes send M(B) + O(M) bits per time unit and channel.
Proof. By the properties of the resynchronisation algorithm B, we get that a good resynchronisation pulse occurs within time T (B). Once this happens, Corollary (6) shows all correct nodes
transition to pulse during [t,t + 2d) for t ∈ T (B) + O(R). By Lemma 8, we get that the algorithm
stabilises by time t and has skew σ = 2d. From Corollary (7), we get that the accuracy bounds can
be set to be within factor φ apart without affecting the stabilisation time asymptotically.
To analyse the number of bits sent per time unit, first observe that the main state machine
communicates whether a node transitions to pulse or wait. This can be encoded using messages
of size O(1). Moreover, as node remains Ω(d) time in pulse or wait, the main state machine sends
onlyO(1) bits per time unit. Second, the auxiliary state machine does not communicate apart from
messages related to the simulation of consensus. The non-self-stabilising pulse synchronisation
algortihm sends messages only when a node generates a pulse and the time between pulses is
Ω(d). Thus, while simulating C, each node broadcasts at most M(C) + O(1) bits per time unit.
6 RESYNCHRONISATION ALGORITHMS
In this section, we give the second key component in the proof of Theorem 1. We show that given
pulse synchronisation algorithms for networks of small size and with low resilience, it is possible
to obtain resynchronisation algorithms for large networks and with high resilience. More precisely,
we show the following theorem:
Theorem 3. Let f ,n0,n1 ∈ N and 1 < ϑ ≤ 1.004. Define
n = n0 + n1, f0 = (f − 1)/2, f1 = (f − 1)/2.
Journal of the ACM, Vol. 66, No. 5, Article 32. Publication date: August 2019.  
Self-Stabilising Byzantine Clock Synchronisation Is Almost as Easy as Consensus 32:37
For any Ψ ∈ Ω(1) and sufficiently small constant φ > φ0 (ϑ ), there exists a boundT0 ∈ Θ(Ψ) such that
the following claim holds. If, for both i ∈ {0, 1}, there exists pulse synchronisation algorithm Ai that
• runs on ni nodes and has resilience fi ,
• has skew σ = 2d, and
• has accuracy bounds Φ−
i = T and Φ+
i = Tφ, where T0 ≤ T and T ∈ O(Ψ),
then there exists a resynchronisation algorithm B that
• runs on n nodes and has resilience f ,
• has skew ρ ∈ O(d) and separation window of length Ψ,
• generates a resynchronisation pulse by time T (B) ∈ max{T (A0),T (A1)} + O(Ψ), and
• has nodes send M(B) ∈ max{M(A0), M(A1)} + O(1) bits per time unit and channel.
6.1 The High-Level Idea
Our goal is to devise a self-stabilising resynchronisation algorithm with skew ρ ∈ O(d) and separation window Ψ for n nodes that tolerates f < n/3 faulty nodes. That is, we want an algorithm that
guarantees that there exists a time t such that all correct nodes locally generate a single resynchronisation pulse during the interval [t,t + ρ) and no new pulse during the interval [t + ρ,t + ρ + Ψ).
Note that a correct resynchronisation algorithm is also allowed to generate various kinds of spurious resynchronisation pulses, such as pulses that are followed by a new resynchronisation pulse
too soon (i.e., before Ψ time units have passed) or pulses that are only generated by a subset of the
correct nodes.
The Algorithm Idea. In order to illustrate the idea behind our resynchronisation algorithm, let
us ignore clock drift and suppose we have two sources of pulses that generate pulses with fixed
frequencies. Whenever either source generates a pulse, then a resynchronisation pulse is triggered
as well. If the sources generate pulses with frequencies that are coprime multiples of (a sufficiently
large) C ∈ Θ(Ψ), then we are guaranteed that eventually one of the sources produces a pulse followed by at least Ψ time units before a new pulse is generated by either of the two sources. See
Figure 10(a) for an illustration.
Put otherwise, suppose ph (v,t) ∈ {0, 1} indicates whether node v observes the pulse source
h ∈ {0, 1} generating a pulse at time t. Using the above scheme, the output variable for the resynchronisation algorithm would be r(v,t) = maxh∈ {0,1}{ph (v,t)}. If, eventually, each source h generates a pulse roughly every Ch time units, setting C0 < C1 to be coprime integer multiples of
C ∈ Θ(Ψ) (we allow for a constant-factor slack to deal with clock drift, etc.), we eventually have
a time when a pulse is generated by one source, but no source will generate another pulse for at
least Ψ time units.
Obviously, if we had such reliable self-stabilising pulse sources for n nodes and f < n/3 faulty
nodes, then we would have effectively solved the pulse synchronisation problem already. However,
our construction given in Section 5 relies on having a resynchronisation algorithm. Thus, in order
to avoid this chicken-and-egg problem, we partition the set of n nodes into two and have each part
run an instance of a pulse synchronisation algorithm with resilience almost f /2. That is, we take
two pulse synchronisation algorithms with low resilience, and use these to obtain a resynchronisation algorithm with high resilience. This allows us to recursively construct resynchronisation algorithms starting from trivial pulse synchronisation algorithms that do not tolerate any faulty nodes.
The final obstacle to this construction is that we cannot guarantee that both instances with
smaller resilience operate correctly, as the total number of faults exceeds the number that can be
tolerated by each individual instance. We overcome this by enlisting the help of all nodes to check,
for each instance, whether its output appears to satisfy the desired frequency bounds. If not, its
Journal of the ACM, Vol. 66, No. 5, Article 32. Publication date: August 2019.
32:38 C. Lenzen and J. Rybicki
Fig. 10. Idea of the resynchronisation algorithm. We take two pulse sources (top two rows) with coprime
frequencies and output the logical OR of the two sources (bottom row). Here, the dim gray lines delimit time
intervals of length C ∈ Θ(Ψ). The blue regions indicate intervals when a correctly operating source should
generate a pulse. In this example, the pulses of the first source should occur approximately every 2C time
units, whereas the pulses of the second source should occur approximately every 3C time units. (a) Two
correct sources that pulse with set frequencies. All correct nodes observe the same input pulses, and hence,
produce the same output pulses. A good resynchronisation pulse is a pulse that is followed by a green block
indicating a silence of at least Ψ time units. (b) One faulty source that produces spurious pulses. As one of
the sources is faulty, two nodes may have different observations on the output of a faulty source given in
View 1 (upper figure) and View 2 (lower figure). In this example, a spurious input pulse occurs, that is, an
input pulse that is inconsistently detected by different nodes. We devise our construction so that only pulses
that follow the frequency bounds of the source are accepted and if the source fails to adhere to the frequency
bounds, it will be ignored for some time. Here, the nodes observing View 1 do not receive the input pulse, and
thus, they ignore any pulses from the first source for a while (gray region). However, nodes observing View 2
receive a spurious pulse that adheres to the frequency bounds (and do not detect any suspicious behaviour).
Thus, nodes with View 2 output a pulse, although nodes observing View 1 do not (red region). Once the
faulty source is silenced, the correctly working source has time to produce a good resynchronisation pulse
no matter what the faulty source does. This ensures that nodes observing either view generate an output
signal within a small enough time window that is followed by at least Ψ time of silence.
output is conservatively filtered out (for a sufficiently large period of time) by a voting mechanism.
This happens only for an incorrect output, implying that the fault threshold for the respective
instance must have been exceeded. Accordingly, the other instance is operating correctly and,
thanks to the absence of further interference from the faulty instance, succeeds in generating a
resynchronisation pulse. Figure 10(b) illustrates this idea.
Using Two Pulse Synchronisers. We now overview how to use two pulse synchronisation algorithms to implement our simple resynchronisation algorithm described above. Let
n0 = n/2 and n1 = n/2
f0 = (f − 1)/2 and f1 = (f − 1)/2.
Observe that we have n = n0 + n1 and f = f0 + f1 + 1. We partition the set V of n nodes into two
sets Vh for h ∈ {0, 1} such that V = V0 ∪V1, where V0 ∩V1 = ∅ and |Vh | = nh. We now pick two
pulse synchronisation algorithms A0 and A1 with the following properties:
Journal of the ACM, Vol. 66, No. 5, Article 32. Publication date: August 2019.
Self-Stabilising Byzantine Clock Synchronisation Is Almost as Easy as Consensus 32:39
• Ah runs on nh nodes and tolerates fh faulty nodes,
• Ah stabilises in time T (Ah ) and sends M(Ah ) bits per time unit and channel, and
• Ah has skew σ ∈ O(d) and accuracy bounds Φh = (Φ−
h , Φ+
h ), where Φ−
h , Φ+
h ∈ O(Ψ).
We let the nodes in set Vh execute the pulse synchronisation algorithm Ah.
An optimistic approach would be to use each Ah as a source of pulses by checking whether at
least nh − fh nodes in the setVh generated a pulse within a time window of (roughly) length σ = 2d.
Unfortunately, we cannot directly use the pulse synchronisation algorithms A0 and A1 as reliable
sources of pulses. There can be a total of f = f0 + f1 + 1 < n/3 faulty nodes, and thus, it may be
that for one h ∈ {0, 1} the set Vh contains more than fh faulty nodes. Hence, the algorithm Ah may
never stabilise and can generate spurious pulses at uncontrolled frequencies. In particular, the
algorithm may always generate pulses with frequency less than Ψ, preventing our simple solution
from working. However, we are guaranteed that at least one of the algorithms stabilises.
Lemma 17. If there are at most f faulty nodes, then there exists h ∈ {0, 1} such that Ah stabilises
by time T (Ah ).
Proof. Observe that f0 + f1 + 1 = f . In order to prevent both algorithms from stabilising, we
need to have at least f0 + 1 faults in the setV0 and f1 + 1 faults in the setV1, totalling f0 + f1 + 2 > f
faults in the system.
Once the algorithm Ah for some h ∈ {0, 1} stabilises, we have at least nh − fh correct nodes in
setVh locally generate pulses with skew σ and accuracy bounds Φh = (Φ−
h , Φ+
h ). However, it may be
that the other algorithm A1−h never stabilises. Moreover, the algorithm A1−h may forever generate
spurious pulses at arbitrary frequencies. Here, a spurious pulse refers to any pulse that does not
satisfy the skew σ and accuracy bounds of A1−h. For example, a spurious pulse may be a pulse that
only a subset of nodes generate, one with too large skew, or a pulse that occurs too soon or too
late.
In order to tackle this problem, we employ a series of threshold votes and timeouts to filter out
any spurious pulses generated by an unstabilised algorithm that violate timing constraints. This
way, we can impose some control on the frequency at which an unstabilised algorithm may trigger
resynchronisation pulses. As long as these frequency bounds are satisfied, it is inconsequential if a
non-stabilised algorithm triggers resynchronisation pulses at a subset of the nodes only. We want
our filtering scheme to eventually satisfy the following properties:
• If Ah has stabilised, then all pulses generated by Ah are accepted.
• If Ah has not stabilised, then only pulses that respect given frequency bounds are accepted.
More precisely, in the first case the filtered pulses respect (slightly relaxed) accuracy bounds Φh
of Ah. In the second case, we enforce that the filtered pulses must either satisfy roughly the same
accuracy bounds Φh or they must be sufficiently far apart. That is, the nodes will reject any pulses
generated by Ah if they occur either too soon or too late.
Once we have the filtering mechanism in place, it becomes relatively easy to implement our
conceptual idea for the resynchronisation algorithm. We apply the filtering mechanism for both
algorithms A0 and A1 and use the filtered outputs as a source of pulses in our algorithm, as
illustrated in Figure 11. We are guaranteed that at least one of the sources eventually produces
pulses with well-defined accuracy bounds. Furthermore, we also know that the other source must
either respect the given accuracy bounds or refrain from generating pulses for a long time. In
the case that both sources respect the accuracy bounds, we use the coprimality of frequencies
to guarantee a sufficiently large separation window for the resynchronisation pulses. Otherwise,
Journal of the ACM, Vol. 66, No. 5, Article 32. Publication date: August 2019. 
32:40 C. Lenzen and J. Rybicki
Fig. 11. Example of the resynchronisation construction for eight nodes tolerating two faults. We partition
the network into two parts, each running a pulse synchronisation algorithm Ai . The output of Ai is fed
into the respective filter and any pulse that passes the filtering is used as a resynchronisation pulse. The
filtering consists of (1) having all nodes in the network participate in a threshold vote to see if anyone thinks
a pulse from Ai occurred (i.e., enough nodes running Ai generated a pulse) and (2) keeping track of when
was the last time a pulse from Ai occurred to check that the accuracy bounds of Ai are respected: pulses
that appear too early or too late are ignored. Moreover, if Ai generates pulses at incorrect frequencies, the
filtering mechanism blocks all pulses generated by Ai for Θ(Ψ) time.
Fig. 12. Construction of an f -resilient resynchronisation algorithm on n nodes from fi-resilient pulse synchronisation algorithms on ni nodes, where f = f0 + f1 + 1 and n = n0 + n1. The n nodes are divided into
two groups of n0 and n1 nodes. These groups run pulse synchronisation algorithms A0 and A1, respectively.
At least one of these algorithms is guaranteed to stabilise eventually. Here, A1 (gray block) has too many
faulty nodes and does not stabilise. All of the n nodes together run two filtering mechanisms F0 and F1 for
the outputs of A0 and A1, respectively. These ensure that no correct node locally generates a resynchronisation pulse without all correct nodes registering this event, and then apply timeout constraints to enforce
the desired frequency bounds.
we exploit the fact that the unreliable source stays silent long enough for the reliable source to
generate a pulse with a sufficiently large separation window.
6.2 Filtering Spurious Pulses
Our pulse filtering scheme follows a similar idea as our recent construction of synchronous counting algorithms [24]. However, considerable care is needed to translate the approach from the (much
simpler) synchronous round-based model to the bounded-delay model with clock drift. We start
by describing the high-level idea of the approach before showing how to implement the filtering scheme in the bounded-delay model considered in this work. Figure 12 illustrates how the
underlying pulse synchronisation algorithms are combined with the filtering mechanism.
For convenience, we refer to the nodes in set Vh as block h. First, for each block h ∈ {0, 1}, every
node v ∈ G performs the following threshold vote:
Journal of the ACM, Vol. 66, No. 5, Article 32. Publication date: August 2019.
Self-Stabilising Byzantine Clock Synchronisation Is Almost as Easy as Consensus 32:41
(1) If v observes at least nh − fh nodes in Vh generating a pulse, vote for generating a resynchronisation pulse.
(2) If at least n − f nodes inV voted for a pulse by block h (within the time period this should
take), then v accepts it.
The idea here is that if some correct node accepts a pulse in Step 2, then every correct node must
have seen at least n − 2f ≥ f + 1 votes due to Step (1). Moreover, once a node observes at least
f + 1 votes, it can deduce that some correct node saw at least nh − fh nodes in block h generate
a pulse. Thus, if any correct node accepts a pulse generated by block h, then all correct nodes are
aware that a pulse may have happened.
Second, we have the nodes perform temporal filtering by keeping track of when block h last
(may have) generated a pulse. To this end, each node has a local “cooldown timer” that is reset
if the node suspects that block h has not yet stabilised. If a pulse is accepted by the above voting
mechanism, then a resynchronisation pulse is triggered if the following conditions are met:
(1) the cooldown timer has expired, and
(2) not too much time has passed since the most recent pulse from h.
A correct node v ∈ G resets its cooldown timer if it
(1) observes at least f + 1 votes for a pulse from block h, but not enough time has passed
since v last saw at least f + 1 votes,
(2) observes at least f + 1 votes, but not n − f votes in a timely fashion, or
(3) has not observed a pulse from block h for too long, that is, block h should have generated
a new pulse by now.
Thus, whenever a block h ∈ {0, 1} triggers a resynchronisation pulse at nodev ∈ G, then each node
u ∈ G either resets its cooldown timer or also triggers a resynchronisation pulse. Furthermore,
if v ∈ G does not observe a pulse from block h within the right time window, it will also reset
its cooldown counter. Finally, each node refuses to trigger a resynchronisation pulse when its
cooldown timer is active. Note that if Ah stabilises, then eventually the cooldown timer for block
h expires and is not reset again. This ensures that eventually at least one of the blocks triggers
resynchronisation pulses.
Implementation in the Bounded-Delay Model. We implement the threshold voting and temporal
filtering with two state machines depicted in Figure 13 and Figure 14. For each block h ∈ {0, 1},
every node runs a single copy of the voter and validator state machines in parallel. In the voter
state machine given in Figure 13, there are two key states, fail and go, which are used to indicate
a local signal for the validator state machine in Figure 14.
The key feature of the voter state machine is the voting scheme: if some node v ∈ G transitions
from vote to go, then all nodes must transition to either fail or go. This is guaranteed by the fact
that a node only transitions to go if it has observed at least n − f nodes in the state vote within
a short time window. This in turn implies that all nodes must observe at least n − 2f > f nodes
in vote (in a slightly larger time window). Thus, any node in state idle must either transition
directly to fail or move on to listen. If a node transitions to state listen, then it is bound to
either transition to go or fail.
The validator state machine in turn ensures that any subsequent go transitions of v are at least
Tmin,h/ϑ or Tcool/ϑ time units apart. Moreover, if any fail transition occurs at time t, then any
subsequent go transition can occur at time t +Tcool/ϑ at the earliest. The voter state machine also
handles generating a fail transition if the underlying pulse synchronisation algorithm does not
produce a pulse within timeTmax,h. This essentially forces the act transitions in the validator state
Journal of the ACM, Vol. 66, No. 5, Article 32. Publication date: August 2019.
32:42 C. Lenzen and J. Rybicki
Fig. 13. The voter state machine is the first of the two state machines used to trigger resynchronisation
pulses by block h ∈ {0, 1}. Every node runs a separate copy of the voter machine for both blocks. The voter
state machine performs a threshold vote to ensure that if at some node a resynchronisation pulse is (or might
be) triggered by block h, then all correct nodes see this by observing at least f + 1 vote messages within Tatt
local time. Note that nodes immediately transition from fail and go to state idle, as there are no guards
blocking these transitions. The two states are used to signal the validator state machine given in Figure 14
to generate resynchronisation pulses or to refrain from doing so until the cooldown timer expires.
Fig. 14. The validator state machine is the second of the two state machines used to trigger resynchronisation
pulses by block h ∈ {0, 1}. Every node runs a separate copy of the validator state machine for both blocks. The
validator checks that the go and fail transition signals in the voter state machine given in Figure 13 satisfy
the minimum time bound and that the go transitions occur in a timely manner. Note that the transition
from state ignore to itself resets the timer Tcool.
machine to occur between accuracy bounds Λ−
h ≈ Tmin,h/ϑ and Λ+
h ≈ Tmax,h or at leastTcool/ϑ time
apart. Furthermore, if the underlying pulse synchronisation algorithm Ah stabilises, then the act
transitions roughly follow the accuracy bounds of Ah.
We give a detailed analysis of the behaviour of the two state machines later in Sections 6.5–6.6.
The conditions we impose on the timeouts and other parameters are listed in Table 4. As before,
the system of inequalities listed in Table 4 can be satisfied by choosing the timeouts carefully. This
is done in Section 6.7.
Lemma 18. Let σ and 1 < ϑ < φ be constants such that ϑ2
φ < 31/30. There exists a constant
Ψ0 (ϑ,φ,d) such that, for any given Ψ > Ψ0 (ϑ,φ,d), we can satisfy the constraints in Table 4 by
choosing
Journal of the ACM, Vol. 66, No. 5, Article 32. Publication date: August 2019.
Self-Stabilising Byzantine Clock Synchronisation Is Almost as Easy as Consensus 32:43
Table 4. The Conditions Employed in the Construction of Section 6
(15) Tmin,h = Φ−
h − ρ
(16) Tmax,h = ϑ (Φ+
h +Tvote)
(17) Tvote = ϑ (σ + 2d)
(18) Tidle = ϑ (σ + d)
(19) Tatt = ϑ (Tvote + 2d)
(20) ρ = Tvote
(21) Tcool ∈ Θ(max{Φ+
h , Ψ})
(22) T ∗ = maxh∈ {0,1}{T (Ah ) + 2Φ+
h } +Tcool + σ + 2d + ρ
(23) Φ−
h > Ψ + 2ρ
(24) Φ−
h ≥ Tvote +Tidle +Tatt + σ + 2d
(25) Tmin,h < Tcool
(26) Tmin,h/ϑ > Ψ + ρ
(27) Tcool/ϑ > 15β
(28) C0 = 4,C1 = 5
(29) Λ+
h = Tmax,h +Tvote
(30) Λ−
h = Tmin,h/ϑ
(31) β > 2Ψ + 4(Tvote + d) + ρ
(32) β · (Ch · j) ≤ j · Λ−
h < j · Λ+
h + ρ ≤ β · (Ch · j + 1) for 0 ≤ j ≤ 3
Here h ∈ {0, 1}.
(1) X ∈ Θ(Ψ),
(2) Φ−
0 = X and Φ+
0 = φX,
(3) Φ−
1 = rX and Φ+
1 = φrX for a constant r > 1, and
(4) all remaining timeouts in O(X).
The Resynchronisation Algorithm. We now have described all ingredients of our resynchronisation algorithm. It remains to define what is the output of our resynchronisation algorithm. First,
for each h ∈ {0, 1}, v ∈ G and t ≥ 0, let us define an indicator variable for act transitions:
rh (u,t) =

1 if node u transitions to act at time t
0 otherwise.
Furthermore, for each v ∈ V0 ∪V1, we define the output of our resynchronisation algorithm as
follows:
r(v,t) = max{r0 (v,t),r1 (v,t)}.
We say that block h triggers a resynchronisation pulse at node v if rh (v,t) = 1. That is, node v
generates a resynchronisation pulse if either block 0 or block 1 triggers a resynchronisation pulse
at node v. Our goal is to show that there exists a time t ∈ O(Φ+ + Ψ) such that every node v ∈ G
generates a resynchronisation pulse at some time t ∈ [t,t + ρ) and neither block triggers a new
resynchronisation pulse before time t + Ψ at any node v ∈ G. First, however, we observe that the
communication overhead incurred by running the voter and validator state machines is small.
Lemma 19. In order to compute the values r(v,t) for each v ∈ G and t ≥ 0, the nodes send at most
max{M(Ah )} + O(1) bits per time unit.
Proof. For both h ∈ {0, 1}, we have every v ∈ Vh broadcast a single bit when Ah generates a
pulse locally at node v. Observe that we can assume w.l.o.g. that Ah generates a pulse only once
per time unit even during stabilisation: we can design a wrapper for Ah that filters out any pulses
Journal of the ACM, Vol. 66, No. 5, Article 32. Publication date: August 2019. 
32:44 C. Lenzen and J. Rybicki
that occur within e.g., time d of each other. As we have Φ−
h > d by Constraint (23), this does not
interfere with the pulsing behaviour once the algorithm has stabilised.
In addition to the pulse messages, it is straightforward to verify that the nodes only need to
communicate whether they transition to states idle, vote, or pass in the voter state machine. Due
to the Tvote and Tmax,h timeouts, a node v ∈ G cannot transition to the same state more than once
within d time, as these timeouts are larger than ϑd by Constraint (16) and Constraint (17).
6.3 Proof of Theorem 3
We take a top-down approach for proving Theorem 3. We delay the detailed analysis of the voter
and state machines themselves to later sections and now state the properties we need in order to
prove Theorem 3.
First of all, as we are considering self-stabilising algorithms, it takes some time for Ah to stabilise,
and hence, also for the voter and validator state machines to start operating correctly. We will show
that this is bound to happen by time
T ∗ ∈ max{T (Ah )} + O(max{Φ+
h } +Tcool) ⊆ max{T (Ah )} + O(Ψ).
The exact value of T ∗ is given by Constraint (22) and will emerge later in our proofs. We show
in Section 6.4 that the resynchronisation pulses triggered by a single correct block have skew
ρ = Tvote = ϑ (σ + 2d) ∈ O(d) and the desired separation window of length Ψ; we later argue that a
faulty block cannot incessantly interfere with the resynchronisation pulses triggered by the correct
block.
Lemma 20 (Stabilisation of Correct Blocks). Suppose h ∈ {0, 1} is a correct block. Then there
exist timesrh,0 ∈ [T ∗,T ∗ + Φ+
h + ρ) and fori ≥ 0 the timesrh,i+1 ∈ [rh,i + Φ− − ρ,rh,i + Φ+ + ρ] satisfying the following properties for all v ∈ G and i ≥ 0:
• rh (v,t) = 1 for some t ∈ [rh,i,rh,i + ρ),
• rh (v,t
) = 0 for any t ∈ (t,rh,i + ρ + Ψ).
We also show that if either block h ∈ {0, 1} triggers a resynchronisation pulse at some correct
node v ∈ G, then for every u ∈ G (1) a resynchronisation pulse is also triggered by h at roughly
the same time or (2) node u refrains from generating a resynchronisation pulse for a sufficiently
long time. The latter holds true because node u observes that a resynchronisation pulse might have
been triggered somewhere (due to the threshold voting mechanism); u thus resets its cooldown
counter, that is, transitions to state ignore in Figure 13. Formally, this is captured by the following
lemma, shown in Section 6.5.
Lemma 21 (Grouping Lemma for Validator Machine). Let h ∈ {0, 1} be any block, t ≥ T ∗, and
v ∈ G be such that rh (v,t) = 1. Then there exists t ∗ ∈ [t − 2Tvote − d,t] such that, for all u ∈ G, we
have
rnext(u,t
∗) ∈ [t
∗
,t
∗ + 2(Tvote + d)] ∪ [t
∗ +Tcool/ϑ, ∞],
where rnext(u,t ∗) = inf{t ≥ t ∗ : rh (u,t
) = 1}.
Now, with the above two lemmas in mind, we take the following proof strategy. Fix a correct
block k ∈ {0, 1} and let rk,0 be the time given by Lemma 20. Observe that if no node v ∈ G has
r1−k (u,t) = 1 for any t ∈ [rk,0,rk,0 + ρ + Ψ), then all correct nodes succeed in creating a resynchronisation pulse with separation window Ψ. However, it may be the case that the other (possibly faulty) block 1 − k spoils the resynchronisation pulse by also triggering a resynchronisation
pulse too soon at some correct node. That is, we may have some v ∈ G that satisfiesr1−k (u,t) = 1,
where t ∈ [rk,0,rk,0 + ρ + Ψ). But then all nodes observe this and the filtering mechanism now
Journal of the ACM, Vol. 66, No. 5, Article 32. Publication date: August 2019. 
Self-Stabilising Byzantine Clock Synchronisation Is Almost as Easy as Consensus 32:45
guarantees that the faulty block either obeys the imposed frequency constraints for the following
pulses or nodes will ignore them. Either way, we can argue that a correct resynchronisation pulse
is generated by the correct block k soon enough.
Accordingly, assume that the faulty block interferes, i.e., generates a spurious resynchronisation pulse at some node u ∈ G at time r1−k,0 ∈ (rk,0,rk,0 + ρ + Ψ). If there is no such time, the
resynchronisation pulse of the correct block would have the required separation window of Ψ.
Moreover, for all v ∈ G and i ≥ 0, we define
rh,0 (v) = inf{t ≥ rh,0 : rh (t,v) = 1}
rh,i+1 (v) = inf{t > rh,i (v) : rh (t,v) = 1}.
Furthermore, for convenience, we define the notation
Dh (u) =

∅ if block h is correct
[rh,0 (u) +Tcool/ϑ, ∞] otherwise.
For the purpose of our analysis, we “discretise” our time into chunks of length β ∈ Θ(Ψ). For
any integers Y > X ≥ 0, we define
I0 (X) = rk,0 − 2(Tvote + d) + X · β,
I1 (X) = rk,0 + 2(Tvote + d) + Ψ + X · β,
I (X,Y ) = [I0 (X), I1 (Y )) .
We abbreviate Λ−
h = Tmin,h/ϑ and Λ+
h = Tmax,h +Tvote (Constraint (29) and Constraint (30)). The
following lemma is useful for proving Theorem 3. We defer its proof to Section 6.6.
Lemma 22 (Resynchronisation Freqency). For any i ≥ 0 and v ∈ G, it holds that
rh,i (v) ∈

rh,0 − 2(Tvote + d) + i · Λ−
h,rh,0 + 2(Tvote + d) + i · Λ+
h

∪ Dh (v).
Corollary 8. Let h ∈ {0, 1} and 0 ≤ i ≤ 3. For any v ∈ G, we have
rh,i (v) ∈ I (i · Ch,i · Ch + 1) ∪ Dh (v).
Proof. Recall thatr1−k,0 ∈ (rk,0 − 2(Tvote + d),rk,0 + ρ + Ψ). By Constraint (32), for alli ≤ 3 and
h ∈ {0, 1}, it holds that
β · Ch · i ≤ i · Λ−
h < i · Λ+
h + ρ ≤ β · (Ch · i + 1).
Using Lemma 22, this inequality, and the above definitions, a straightforward manipulation shows
that rh,j (v) lies in the interval
[rh,0 − 2(Tvote + d) + i · Λ−
h,rh,0 + 2(Tvote + d) + i · Λ+
h ) ∪ Dh (v)
⊆ [rk,0 − 2(Tvote + d) + i · Λ−
h,rk,0 + 2(Tvote + d) + Ψ + ρ + i · Λ+
h ) ∪ Dh (v)
⊆ [rk,0 − 2(Tvote + d) + β · (Ch · i),rk,0 + 2(Tvote + d) + Ψ + β · (Ch · i + 1)) ∪ Dh (v)
= [I0 (i · Ch ), I1 (i · Ch + 1)) ∪ Dh (v)
= I (i · Ch,i · Ch + 1) ∪ Dh (v).
With the above results, we can now show that eventually the algorithm outputs a good resynchronisation pulse.
Lemma 23. There exists a time t ∈ max{Ah } + O(Ψ) such that for all v ∈ G there exists a time
tv ∈ [t,t + ρ] satisfying
• r(v,tv ) = 1, and
• r(v,t
) = 0 for h ∈ {0, 1} and any t ∈ (tv ,tv + Ψ).
Journal of the ACM, Vol. 66, No. 5, Article 32. Publication date: August 2019.  
32:46 C. Lenzen and J. Rybicki
Proof. Suppose block k ∈ {0, 1} is correct. The lemma follows by proving that we have the
following properties for some t ≤ I1 (11) and each v ∈ G:
• rk (v,tv ) = 1 for some tv ∈ [t,t + ρ], and
• rh (v,t
) = 0 for h ∈ {0, 1} and any t ∈ (tv ,tv + Ψ).
Recall that r1−k,0 ∈ (rk,0 − 2(Tvote + d),rk,0 + ρ + Ψ), as otherwise the claim trivially follows for
t = rk,0. Consider any v ∈ G. Corollary 8 and the fact that C0 = 4 by Constraint (28) imply
r0,0 (v) ∈ I (0, 1) ∪ D0 (v),
r0,1 (v) ∈ I (4, 5) ∪ D0 (v),
r0,2 (v) ∈ I (8, 9) ∪ D0 (v),
r0,3 (v) ∈ I (12, 13) ∪ D0 (v).
As Tcool/ϑ ≥ 15β by Constraint (27), it follows for all t ≥ r0,0 that if r0 (v,t) = 1, then
t ∈ I (0, 1) ∪ I (4, 5) ∪ I (8, 9) ∪ [I0 (12), ∞].
Similarly, as C1 = 5, for all t ≥ r1,0 we get that if r1 (v,t) = 1, then
t ∈ I (0, 1) ∪ I (5, 6) ∪ I (10, 11) ∪ [I0 (15), ∞].
Let k be the correct block we have fixed. Recall that Dk (v) = ∅. The claim now follows from a
simple case analysis:
(1) If k = 0, then rk,2 (v) ∈ I (8, 9) and r1−k (v,t
) = 0 for all t ∈ [I1 (6), I0 (10)) ⊃ [I0 (8), I1 (9) +
Ψ + ρ) (by Constraint (31)).
(2) If k = 1, then rk,2 (v) ∈ I (10, 11) and r1−k (v,t
) = 0 for all t ∈ [I1 (9), I0 (12)) ⊃
[I0 (10), I1 (11) + Ψ + ρ) (by Constraint (31)).
Thus, in both cases,t = minv ∈G {rk,2 (v)} satisfies the claim of the lemma, provided thatt ≤ I1 (11) ∈
max{Ah } + O(Ψ). This is readily verified from the constraints given in Table 4.
Theorem 3. Let f ,n0,n1 ∈ N and 1 < ϑ ≤ 1.004. Define
n = n0 + n1, f0 = (f − 1)/2, f1 = (f − 1)/2.
For any Ψ ∈ Ω(1) and sufficiently small constant φ > φ0 (ϑ ), there exists a boundT0 ∈ Θ(Ψ) such that
the following claim holds. If, for both i ∈ {0, 1}, there exists pulse synchronisation algorithm Ai that
• runs on ni nodes and has resilience fi ,
• has skew σ = 2d, and
• has accuracy bounds Φ−
i = T and Φ+
i = Tφ, where T0 ≤ T and T ∈ O(Ψ),
then there exists a resynchronisation algorithm B that
• runs on n nodes and has resilience f ,
• has skew ρ ∈ O(d) and separation window of length Ψ,
• generates a resynchronisation pulse by time T (B) ∈ max{T (A0),T (A1)} + O(Ψ), and
• has nodes send M(B) ∈ max{M(A0), M(A1)} + O(1) bits per time unit and channel.
Proof. Computation shows that for ϑ ≤ 1.004, we have that ϑ2
φ0 (ϑ ) < 31/30, where φ0 (ϑ ) =
1 + 5(ϑ − 1)/(2 + 2ϑ − 3ϑ2) is given in Corollary (7). Thus, Lemma 18 shows that for a sufficiently
small choice of φ > φ0 (ϑ ) > ϑ, we can pick Φ−
h ∈ Θ(Ψ), where 1 < max{Φ+
h /Φ−
h } ≤ φ, such that the
conditions given in Table 4 are satisfied. Thus, using our assumption, we can choose the algorithms
Ah with these accuracy bounds Φh; note that here we use sufficiently small ϑ and φ that satisfy
both our initial assumption and the preconditions of Lemma 18.
Journal of the ACM, Vol. 66, No. 5, Article 32. Publication date: August 2019. 
Self-Stabilising Byzantine Clock Synchronisation Is Almost as Easy as Consensus 32:47
In order to compute the output value r(v,t) ∈ {0, 1} for each v ∈ G and t ≥ 0, Lemma 19
shows that our resynchronisation algorithm only needs to communicate O(1) bits per time unit
in addition to the message sent by underlying pulse synchronisation algorithms A0 and A1. By
Lemma 23, we have that a good resynchronisation pulse with skew ρ ∈ O(d) happens at a time
t ∈ max{Ah } + O(Ψ).
6.4 Proof of Lemma 20
We now show that eventually a correct block h ∈ {0, 1} will start triggering resynchronisation
pulses with accuracy bounds Λh = (Λ−
h, Λ+
h ). Our first goal is to show that after the algorithm Ah
has stabilised in a correct block h, all correct nodes will start transitioning to go in a synchronised
fashion. Then we argue that eventually the transitions to the state go will be coupled with the
transitions to act.
Recall that the pulse synchronisation algorithm Ah has skew σ and accuracy bounds Φh =
(Φ−
h , Φ+
h ). Let ph (v,t) ∈ {0, 1} indicate whether nodev ∈ Vh \ F generates a pulse according to algorithm Ah at time t. If blockh ∈ {0, 1} is corrrect, then by timeT (Ah ) the algorithm Ah has stabilised.
Moreover, then there exists a time T (Ah ) ≤ ph,0 ≤ T (Ah ) + Φ+
h such that each v ∈ Vh \ F satisfies
ph (v,t) = 1 for some t ∈ [ph,0,ph,0 + σ ). Since block h and algorithm Ah are correct, there exist
for each v ∈ Vh \ F and i ≥ 0 the following values:
ph,i (v) = inf{t ≥ ph,i : ph (v,t) = 1}  ∞,
ph,i+1 ∈ [ph,i + Φ−
h ,ph,i + Φ+
h ),
ph,i+1 (v) ∈ [ph,i+1,ph,i+1 + σ ).
That is, Ah generates a pulse at node v ∈ Vh for the ith time after stabilisation at time ph,i (v).
First, let us observe that the initial “clean” pulse makes every correct node transition to go or
fail, thereby resetting the Tmax,h timeouts, where the nodes will wait until the next pulse.
Lemma 24. Suppose block h ∈ {0, 1} is correct. Each correct node v ∈ G is in state idle at time ph,1
and its local Tmax,h timer does not expire during the interval [ph,1,ph,1 + σ + d).
Proof. First, observe that if the timer Tmax,h is reset at time ph,0 or later, then it will not expire before time ph,0 +Tmax,h/ϑ > ph,0 + Φ+
h + σ + d ≥ ph,1 + σ + d by Constraint (16) and Constraint (17). Because every node receives nh − fh pulse messages from different nodes inVh during
(ph,0,ph,0 + σ + d) and Tidle = ϑ (σ + d) by Constraint (18), every node that is in state idle at time
ph0 leaves this state during (ph0 ,ph0 + σ + d). Recall that nodes cannot stay in states go or fail.
Because nodes leave states listen, vote, and pass when the timeout Tvote, listen expires, any
node not in state idle must transition to this state within Tvote time. Accordingly, each correct
node resets its Tmax,h timer during (ph,0,ph,0 +Tvote + σ + d).
Next, note that during (ph0 +Tidle + σ + d,ph1 ), no correct node has any pulse messages from
correct nodes inVh in its respective buffer. Therefore, no correct node can transition to vote during
this time interval. This, in turn, implies that no correct node has any vote messages from correct
nodes in its respective buffer with timeout Tatt during (ph0 +Tidle +Tatt + σ + 2d,ph1 ). Therefore,
no correct node can leave state idle during this period. Finally, any correct nodes not in state idle
at time ph0 +Tidle +Tatt + σ + 2d must transition back to idle by time ph0 +Tvote +Tidle +Tatt + σ +
2d. As Tvote +Tidle +Tatt + σ + 2d ≤ Φ− by Constraint (24), the claim follows.
Let us define an indicator variable for go transitions:
дh (v,t) =

1 if node u transitions to go at time t
0 otherwise.
Journal of the ACM, Vol. 66, No. 5, Article 32. Publication date: August 2019.    
32:48 C. Lenzen and J. Rybicki
Similarly to above, we now also define for v ∈ G and i ≥ 1 the values
дh,1 = inf{t ≥ p1 : дh (u,t) = 1,u ∈ G},
дh,1 (v) = inf{t ≥ дh,1 : дh (v,t) = 1},
дh,i+1 (v) = inf{t > дh,i (v)}.
In words, the time дh,1 is the minimal time that some correct node transitions to state go in the
voter state machine of block h at or after the second pulse of Ah after stabilisation. The two other
values indicate the ith time a correct node v ∈ G transitions to go starting from time дh,1.
We now show that starting from the second pulse ph,1 of a correct block h, the go signals essentially just “echo” the pulse.
Lemma 25. If block h is correct, then, for all v ∈ G and i > 0, it holds that
дh,i (v) ∈ (ph,i + σ + 2d,ph,i + σ + 2d + ρ),
where ρ = ϑ (σ + 2d) = Tvote. Moreover, node v does not transition to state fail at any time t ≥ ph,1.
Proof. By Lemma 24, we have that each v ∈ G is in state idle at time ph,1 and Guard G3 is
not active during [ph,1,ph,1 + σ + d). During (ph,1,ph,1 + σ + d), i.e., within Tidle local time by
Constraint (18), each node receives nh − fh pulse messages from different nodes in Vh and thus
transitions to vote. Thus, all nodes receive n − f vote messages from different nodes during
(ph,1,ph,1 + σ + 2d). As Tvote/ϑ = σ + 2d by Constraint (17), each correct node transitions to pass
before Tvote, listen expires and transitions to go at a time from (ph,1 + σ + 2d,ph,1 + σ + 2d +
Tvote) = (ph,1 + σ + 2d,ph,1 + σ + 2d + ρ). In particular, it resets its buffers after all pulse and vote
messages from correct nodes are received. Consequently, correct nodes stay in state idle until
either the next pulse or Tmax,h expires. The former occurs at all correct nodes in Vh no earlier
than time ph,2 > ph,1 +Tatt + σ + 2d > ph,1 +Tvote + σ + 2d by Constraint (19) and Constraint (24),
and each pulse is received before time ph,2 + σ + d < ph,1 + Φ+
h +Tvote by Constraint (16) and Constraint (17). Thus, each correct node stay in state idle until time ph,2 withTmax,h expiring no earlier
than time ph,2 + σ + d. Consequently, we can repeat the above reasoning inductively to complete
the proof.
We define the following time bound for each h ∈ {0, 1}:
T ∗
h = T (Ah ) +Tcool + 2Φ+
h + σ + 2d + ρ.
We now show that by time T ∗
h we are guaranteed that transitions to go and act have become
coupled if block h is correct.
Lemma 26. Suppose block h ∈ {0, 1} is correct. Then, for any v ∈ G and t ≥ T ∗
h , it holds that
rh (v,t) = 1 if and only if дh (v,t) = 1.
Proof. Note that node u ∈ G can transition to state act from wait in the validator state machine only if the voter state machine transitions to go. Consider the time дh,i (v) fori > 0. Observe
that there are three states in which v may be at this time: wait, hold, or ignore. We argue that,
in each case, node v eventually is in state wait in the validator state machine when the voter state
machine transitions to state go, and thus, node v transitions to act.
First of all, note that, by Lemma 25, node v does not transition to the state fail at any time
t ≥ ph,1. We utilise this fact in each of the three cases below:
(1) In the first case, node v transitions from wait to act at time дh,i (v) and hence we have
rh (v,дh,i (v)) = 1. By applying both Lemma 25 and Constraint (15), we get that дh,i+1 (v) ≥
дh,i (v) + Φ−
h ≥ дh,i (v) +Tmin,h. Moreover, v does not transition to the fail state in the
Journal of the ACM, Vol. 66, No. 5, Article 32. Publication date: August 2019. 
Self-Stabilising Byzantine Clock Synchronisation Is Almost as Easy as Consensus 32:49
voter state machine at any time t ≥ дh,j . Hence, by induction, node v transitions from
state wait to act at time дh,j (v) for each j ≥ i.
(2) In the second case,v transitions from hold to ignore at time дh,i (v). By time r ≤ дh,i (v) +
Tcool, node v transitions to wait. Hence, for any j with дh,j (v) ≥ r, the first case applies.
(3) In the third case, v resets its Tcool timeout and remains in state ignore until, at a time
r ≤ дh,i (v) +Tcool, its Tcool timer expires and v transitions to wait. Again, for any j with
дh,j (v) ≥ r, the first case applies.
Now consider the time дh,1 (v) ∈ [ph,1 + σ + 2d,ph,1 + σ + 2d + ρ) given by Lemma 25. From the
above case analysis, we get that node v is in state wait by time
дh,1 (v) +Tcool < ph,1 + σ + 2d + ρ +Tcool ≤ T ∗
h ,
and from then on each transition to go entails a transition to act, as claimed.
Lemma 20 (Stabilisation of Correct Blocks). Suppose h ∈ {0, 1} is a correct block. Then there
exist timesrh,0 ∈ [T ∗,T ∗ + Φ+
h + ρ) and fori ≥ 0 the timesrh,i+1 ∈ [rh,i + Φ− − ρ,rh,i + Φ+ + ρ] satisfying the following properties for all v ∈ G and i ≥ 0:
• rh (v,t) = 1 for some t ∈ [rh,i,rh,i + ρ),
• rh (v,t
) = 0 for any t ∈ (t,rh,i + ρ + Ψ).
Proof. First, observe that, by Constraint (22), we have T ∗ = max{T ∗
h }. Let ph,j ∈ [T ∗ − σ −
2d,T ∗ − σ − 2d + Φ+
h ] for some j > 0. By Lemma 25, we have that for all i ≥ j and v ∈ G it holds
that
дh,i (v) ∈ (ph,i + σ + 2d,ph,i + σ + 2d + ρ),
and by Lemma 26, we have rh (v,t) = 1 for all дh,i (v) = t ≥ T ∗ and rh (v,t) = 0 for all othert ≥ T ∗.
We set rh,i = minv ∈G {дh,j+i (v)}. As ph,i+1 − ph,i ∈ [Φ−, Φ+] for all i ≥ 0, this shows all required
time bounds but rh (v,t
) = 0 for each v ∈ G, i, and t ∈ (дh,j+i (v),rh,i + ρ + Ψ). The latter follows
because Φ− > Ψ + 2ρ by Constraint (23).
6.5 Proof of Lemma 21
Lemma 21 (Grouping Lemma for Validator Machine). Let h ∈ {0, 1} be any block, t ≥ T ∗, and
v ∈ G be such that rh (v,t) = 1. Then there exists t ∗ ∈ [t − 2Tvote − d,t] such that, for all u ∈ G, we
have
rnext(u,t
∗) ∈ [t
∗
,t
∗ + 2(Tvote + d)] ∪ [t
∗ +Tcool/ϑ, ∞],
where rnext(u,t ∗) = inf{t ≥ t ∗ : rh (u,t
) = 1}.
In order to show Lemma 21, we analyse how the voting and validator state machines given in
Figure 13 and Figure 14 behave. We show that the voter machines for a single block h ∈ {0, 1} are
roughly synchronised in the following sense: if some correct node transitions to go, then every
correct node will transition to either go or fail within a short time window.
Lemma 27. Let t ≥ 2Tvote + d andv ∈ G such that дh (v,t) = 1. Then there existst ∗ ∈ (t − 2Tvote −
d,t] such that all correct nodes transition to go or fail during the interval [t ∗,t ∗ + 2(Tvote + d)).
Proof. Note that, at any time t ≥ Tvote + d, any vote message stored at a correct node (supposedly) sent by another correct node must actually have been sent at a time greater than 0. Since
v ∈ G satisfies дh (v,t) = 1, this means that it transitioned to listen at a time t ∈ [t −Tvote,t],
implying that v received at least n − 2f vote messages from different correct nodes during the
interval [t −Tvote,t
]. Hence, every correct u ∈ G must receive at least n − 2f > f vote messages
from different nodes during the interval I = (t −Tvote − d,t + d).
Journal of the ACM, Vol. 66, No. 5, Article 32. Publication date: August 2019.  
32:50 C. Lenzen and J. Rybicki
Let t ∗ < t be the minimal time a correct node transitions to vote during the interval I. Consider
any node u ∈ G. By the above observations, u has stored at least f + 1 vote messages in the buffer
using timeoutTatt at some time t ∈ [t ∗,t + d] (where we use Constraint (19)) and must transition
to listen in case it is in state idle. Any node that is not in state idle will transition to go or fail
within Tvote time by Guard G4. Overall, each correct node must transition to fail or go during the
interval [t ∗,t +Tvote + d] ⊆ [t ∗,t ∗ + 2(Tvote + d)].
We now show a similar synchronisation lemma for the validator state machines as well: if some
correct node transitions to act and triggers a resynchronisation pulse, then every correct node
triggers a resynchronisation pulse or transitions to ignore within a short time window.
Lemma 28. Let t ≥ 2Tvote + d and suppose rh (u,t) = 1 for some v ∈ G. Then there exists a time
t ∗ ∈ (t − 2Tvote − d,t] such that all correct nodes transition to act or ignore during the time interval
[t ∗,t ∗ + 2(Tvote + d)).
Proof. Suppose some node v transitions to state act at time t. Then it must have transitioned
to state go in the voter state machine at time t as well. By Lemma 27, we get that there exists t ∗ ∈
(t − 2Tvote − d,t] such that all correct nodes transition to go or fail during the interval [t ∗,t ∗ +
2(Tvote + d)). Once u ∈ G transitions to either of these states in the voter state machine, this causes
a transition in the validator state machine to either act or ignore, as can be seen from Figure 14.
Hence, during the same interval, all correct nodes will either transition to act or ignore.
Observe that once v ∈ G transitions to ignore at time t, then it cannot transition back to act
beforeTcool time has passed on its local clock, that is, before time t +Tcool/ϑ. Thus, Lemma 28 now
implies Lemma 21, as T ∗ ≥ 2Tvote + d.
6.6 Proof of Lemma 22
We now aim to prove Lemma 22. Hence, letrh,0 be as defined in Section 6.3. We have shown above
that if block h is correct, then the resynchronisation pulses generated by block h are coupled with
the pulses generated by the underlying pulse synchronisation algorithm Ah. We will argue that
any block h ∈ {0, 1}, including a faulty one, must either respect the accuracy bounds Λh = (Λ−
h, Λ+
h )
when triggering resynchronisation pulses or refrain from triggering a resynchronisation pulse for
at least time Tcool/ϑ.
Lemma 29. Let u ∈ G, h ∈ {0, 1}, and i ≥ 0. Then
rh,i+1 (u) ∈ [rh,i (u) + Λ−
h,rh,i (u) + Λ+
h] ∪ Dh (u).
Proof. First, observe that in case rh,i (u) = ∞ for any i ≥ 0, by definition rh,i+1 (u) = ∞ ∈ Dh (u)
and the claim holds.
Hence, let t = rh,i (u)  ∞. Since u transitions to act at time t, it follows that u also transitions
to state go at time t, that is, дh (u,t) = 1. Therefore, u will transition to state idle in the voter state
machine and state hold in the validator state machine. Observe that u cannot transition to act
again before time t + min{Tmin,h,Tcool}/ϑ, that is, before either local timer Tmin,h or Tcool expires.
Since Tmin,h < Tcool by Constraint (25), we get that rh,i+1 (u) ≥ t +Tmin,h/ϑ = t + Λ−
h.
Next note that u transitions to fail when (1) the local timer Tmax,h expires when in state
idle or (2) Tvote, which is reset only upon leaving idle, expires when not in state idle. Thus,
by time t +Tmax,h +Tlisten, node u transitioned to fail or go again. This implies that by time
t +Tmax,h +Tvote node u has transitioned to either act or ignore in the validator state machine.
Hence, we get that rh,i+1 (u) ≤ t +Tmax,h +Tvote = rh,i (u) + Λ+
h or rh,i+1 (u) ≥ rh,i (u) +Tcool/ϑ ≥
rh,0 (u) +Tcool/ϑ. Therefore, the claim follows.
Journal of the ACM, Vol. 66, No. 5, Article 32. Publication date: August 2019.    
Self-Stabilising Byzantine Clock Synchronisation Is Almost as Easy as Consensus 32:51
Lemma 22 readily follows.
Lemma 22 (Resynchronisation Freqency). For any i ≥ 0 and v ∈ G, it holds that
rh,i (v) ∈

rh,0 − 2(Tvote + d) + i · Λ−
h,rh,0 + 2(Tvote + d) + i · Λ+
h

∪ Dh (v).
Proof. Again, observe that ifrh,i (v) = ∞, then the claim vacuously holds for all j ≥ i. We prove
the claim by induction on increasing i, so w.l.o.g. we may assume that rh,i  ∞ for all i ≥ 0. The
base case i = 0 follows directly from the definition of rh,0 and Lemma 21. By applying Lemma 29
to indexes i, v, and h, and using the induction hypothesis, we get that rh,i+1 (v) lies in the interval
[rh,i (v) + Λ−
h,rh,i (v) + Λ+
h] ∪ [rh,i (v) +Tcool/ϑ, ∞]
⊆ [rh,0 − 2(Tvote + d) + (i + 1) · Λ−
h,rh,0 + 2(Tvote + d) + (i + 1) · Λ+
h ) ∪ Dh (v).
6.7 Proof of Lemma 18
Lemma 18. Let σ and 1 < ϑ < φ be constants such that ϑ2
φ < 31/30. There exists a constant
Ψ0 (ϑ,φ,d) such that, for any given Ψ > Ψ0 (ϑ,φ,d), we can satisfy the constraints in Table 4 by
choosing
(1) X ∈ Θ(Ψ),
(2) Φ−
0 = X and Φ+
0 = φX,
(3) Φ−
1 = rX and Φ+
1 = φrX for a constant r > 1, and
(4) all remaining timeouts in O(X).
Proof. We show that we can satisfy the constraints by setting
Φ−
0 = X
Φ+
0 = φX
Φ−
1 = rX
Φ+
1 = rφX
Ψ = aX
β = bX
Tcool = cX,
where a = b/3, b = 6/25 · ϑφ, c = 16ϑb, and r = 31/25, and by picking a sufficiently large X >
X0 (ϑ,φ,d). Here X0 (ϑ,φ,d) depends only on the given constants. Note that the choice of Tcool
satisfies Constraint (21).
First, let us pick the values for the remaining timeouts and variables as given by Constraints
(15)–(20), (22), and (28)–(30); it is easy to check that these equalities can be satisfied simultaneously.
Regarding Constraint (23), observe that 2/25 · ϑφ < 1 and
Φ−
h ≥ X > 2/25 · ϑφX + 2ρ = aX + 2ρ = Ψ + 2ρ
when X > 2ρ/(1 − 2/25 · ϑφ) = 2ϑ (σ + 2d)/(1 − 2/25 · ϑφ), that is, X is larger than a constant.
Furthermore, Constraint (24) is also satisfied by picking the constant bounding X from below to
be large enough.
To see that Constraint (25) holds, observe thatTcool = cX = 16ϑ2
φX · 6/25 > 96/25 · X > 31/25 ·
X = rX ≥ Φ−
h for both h ∈ {0, 1}. Assuming X > 2ρ · 375/344 = 2ρ/(1 − 2/25 · 31/30), Constraint
(26) is satisfied since
Φ−
h − ρ ≥ X − ρ > 2/25 · 31/30 · X + ρ > 2/25 · ϑ2
φX + ρ > b/3 · X + ρ = aX + ρ = Ψ + ρ.
Journal of the ACM, Vol. 66, No. 5, Article 32. Publication date: August 2019.  
32:52 C. Lenzen and J. Rybicki
Constraint (27) is satisfied as Tcool/ϑ = cX/ϑ = 16bX = 16β > 15β. Having X > 3/b · (5ρ + 4d)
yields that Constraint (31) is satisfied, since then
2Ψ + 4(Tvote + d) + ρ = 2Ψ + 5ρ + 4d = 2b/3 · X + 5ρ + 4d < bX = β.
It remains to address Constraint (32). As Constraint (15) and Constraint (30) hold, the first inequality of Constraint (32) is equivalent to
ϑβCh = 6/25 · ϑ2
φXCh ≤ Φ−
h − ρ.
We have set C0 = 4 in accordance with Constraint (28). For X > ρ/(1 − 24/25 · ϑ2
φ),
24/25 · ϑ2
φX < X − ρ = Φ−
0 − ρ
thus shows that the inequality holds. Concerning h = 1, we setC1 = 5. Recalling that ϑ2
φ < 31/30,
we may assume that X > ρ/(31/25 − 6/5 · ϑ2
φ), yielding
6/5 · ϑ2
φX < 31/25 · X − ρ = rX − ρ = Φ−
1 − ρ,
i.e., the first inequality of Constraint (32) is satisfied for h = 1. The middle inequality is trivially
satisfied, as Λ−
h < Λ+
h. By the already established equalities, the final inequality in Constraint (32)
is equivalent to
jϑΦ+
h + ((ϑ + 1)j + 1)ρ ≤ β (Ch · j + 1)
for all h ∈ {0, 1} and 0 ≤ j ≤ 3.
LetAj = ((ϑ + 1)j + 1)ρ and observe that 25/3 · A3 > 25/4 · A2 > 5A1. For anyX > 25/3 · A3 and
h = 0, a simple calculation thus shows
ϑφX + A1 < 30/25 · ϑφX = 5bX,
2ϑφX + A2 < 54/25 · ϑφX = 9bX,
3ϑφX + A3 < 78/25 · ϑφX = 13bX.
Since Φ+
0 = φX, β = bX, and C0 = 4, this covers the case of h = 0. Similarly, as r = 31/25 = 1 +
b/(ϑφ), we have
ϑφrX + A1 < 6bX,
2ϑφrX + A2 < 11bX,
3ϑφrX + A3 < 16bX,
covering the case of h = 1 with C1 = 5. Overall, we conclude that Constraint (32) is satisfied.
Finally, observe that, in all cases, we assumed that X is bounded from below by a function
X0 (ϑ,φ,d) that depends only on the constants ϑ, φ, and d. Thus, the constraints can be satisfied by
picking X > X0 (ϑ,φ,d) which yields that we can satisfy the constraints for any Ψ > Ψ0 (ϑ,φ,d) =
aX0 (ϑ,φ,d).
7 RANDOMISED ALGORITHMS
While we have so far only considered deterministic algorithms, our framework also extends to randomised algorithms. In particular, this allows us to obtain faster algorithms by simply replacing the
synchronous consensus algorithms we use by randomised variants. Randomised consensus algorithms can break the linear-time lower bound [19] for deterministic algorithms [21, 29]. This in turn
allows us to construct the first pulse synchronisation algorithms that stabilise in sublinear time.
Typically, when considering randomised consensus, one relaxes the termination property: it
suffices that the algorithm terminates with probability 1 and gives probabilistic bounds on the
(expected, w.h.p., etc.) round complexity. However, our framework operates based on a deterministic termination guarantee, where the algorithm is assumed to declare its output in R rounds.
Journal of the ACM, Vol. 66, No. 5, Article 32. Publication date: August 2019. 
Self-Stabilising Byzantine Clock Synchronisation Is Almost as Easy as Consensus 32:53
Therefore, we instead relax the agreement property so that it holds with a certain probability only.
Formally, node v is given an input x (v) ∈ {0, 1}, and it must output y(v) ∈ {0, 1} such that the
following properties hold:
(1) Agreement. With probability at least p, there exists y ∈ {0, 1} such that y(v) = y for all
correct nodes v.
(2) Validity. If, for x ∈ {0, 1}, it holds that x (v) = x for all correct nodes v, then y(v) = x for
all correct nodes v.
(3) Termination. All correct nodes decide on y(v) and terminate within R rounds.
This modification is straightforward, as the following lemma shows.
Lemma 30. Let C be a randomised synchronous consensus routine that terminates in R rounds
in expectation and deterministically satisfies agreement and validity conditions. Then there exists a
randomised synchronous consensus routine C that deterministically satisfies validity and terminates
within 2R rounds, and satisfies agreement with probability at least 1/2. All other properties, such as
message size and resilience, of C and C are the same.
Proof. The modified algorithm operates as follows. We run the original algorithm for (up to)
2R rounds. If it terminates at node v, then node v outputs the decision of the algorithm. Otherwise,
node v outputs its input value so that y(v) = x (v). This deterministically guarantees validity: if all
correct nodes have the same input, the original algorithm can only output that value. Concerning
agreement, observe that by Markov’s bound, the original algorithm has terminated at all nodes
within 2R rounds with probability at least 1/2. Accordingly, agreement holds with probability at
least 1/2.
We remark that the construction from [24] that generates silent consensus routines out of regular ones also applies to randomised algorithms (as produced by Lemma 30), that is, we can obtain
suitable randomised silent consensus routines to be used in our framework.
Our framework makes use of consensus in the construction underlying Theorem 2 only. For
stabilisation, we need a constant number of consecutive consensus instances to succeed. Thus, a
constant probability of success for each individual consensus instance is sufficient to maintain an
expected stabilisation time of O(R) for each individual level in the stabilisation hierarchy. This is
summarised in the following variant of Theorem 2.
Corollary 9. Let f ≥ 0 and n > 3f . Suppose, for a network of n nodes, there exist
• an f -resilient resynchronisation algorithm B with skew ρ ∈ O(d) and separation window Ψ ≥
Ψ0 for a sufficiently large Ψ0 ∈ O(R) and
• an f -resilient randomised synchronous consensus algorithm C,
where C runs in R = R(f ) rounds, lets nodes send at most M = M(f ) bits per round and channel, and
agreement holds with constant probability. Then there exists a randomised f -resilient pulse synchronisation algorithm A for n nodes with skewσ = 2d and accuracy bounds Φ−, Φ+ ∈ Θ(R) that stabilises
in expected T (B) + O(R) time and has nodes send M(B) + O(M) bits per time unit and channel.
Note that once one of the underlying pulse synchronisation algorithms used in the construction
of Theorem 3 stabilises, the resynchronisation algorithm itself stabilises deterministically, as it
does not make use of consensus or randomisation. Applying linearity of expectation and the same
recursive pattern as before, Theorem 1 thus generalises as follows:
Corollary 10. Let C, R, M, N be a family of randomised synchronous consensus routines, where
each C ∈ C satisfies agreement with constant probability. Then, for any f ≥ 0, n ≥ N (f ), andT ≥ T0
Journal of the ACM, Vol. 66, No. 5, Article 32. Publication date: August 2019. 
32:54 C. Lenzen and J. Rybicki
for someT0 ∈ Θ(R(f )), there exists aT -pulser A with skew 2d. The number of bits M(A) sent per time
unit and channel and the expected stabilisation time T (A) satisfy
T (A) ∈ O


d +
log

f 
k=0
R(2k )



and M(A) ∈ O


1 +
log

f 
k=0
M(2k )



,
where the sums are empty when f = 0.
However, while randomised algorithms can be more efficient, typically they require additional
restrictions on the model, e.g., that the adversary must not be able to predict future random decisions. Naturally, such restrictions then also apply when applying Corollary 9 and, subsequently,
Corollary 10. A typical assumption is that communication is via private channels. That is, the faulty
nodes’ behaviour at time t is a function of all communication from correct nodes to faulty nodes
during the interval [0,t], the inputs, and the consensus algorithm only.
We can now obtain pulse synchronisation algorithms that are efficient both with respect to
stabilisation time and communication. For example, we can make use of the randomised consensus
algorithm by King and Saia [21].
Theorem 6 ([21] and Lemma 30). Suppose communication is via private channels. There is a
family of randomised synchronous consensus routines that satisfy the following properties:
• the algorithm satisfies agreement with constant probability,
• the algorithm satisfies validity,
• the algorithm terminates in R(f ) ∈ polylog f rounds,
• the number of bits sent by each node in each round is at most M(f ) ∈ polylog f , and
• n(f ) > (3 + ε)f for a constant ε > 0 that can be freely chosen upfront.
We point out that the algorithm by King and Saia [21] actually satisfies stronger bounds on the
total number of bits sent by each node than what is implied by our statement. As our framework
requires nodes to broadcast a constant number of bits per time unit and level of recursion of the
construction, we obtain the following corollary.
Corollary 2. Suppose we have private channels. For any f ≥ 0, constantε > 0, and n > (3 + ε)f ,
there is a randomised f -resilient (polylog f )-pulser over n nodes that stabilises in polylog f time
w.h.p. and has nodes broadcast polylog f bits per time unit.
Note that it is trivial to boost the probability for stabilisation by repetition, as the algorithm
must stabilise in polylog f time regardless of the initial system state. This was exploited in the
above corollary. However, in case of a uniform (or slowly growing) running time as function of f ,
it is useful to apply concentration bounds to show a larger probability of stabilisation. Concretely,
the algorithm by Feldman and Micali [18] offers constant expected running time, regardless of f ;
this translates to constant probability of success for an O(1)-round algorithm in our setting.
Theorem 7 ([18] and Lemma 30). Suppose that communication is via private channels. There
exists a family of randomised synchronous consensus routines that satisfy the following properties:
• the algorithm satisfies agreement with constant probability,
• the algorithm satisfies validity,
• the algorithm terminates in R(f ) ∈ O(1) rounds,
• the total number of bits broadcasted by each node is poly f ,
• and n(f ) = 3f + 1.
Journal of the ACM, Vol. 66, No. 5, Article 32. Publication date: August 2019.      
Self-Stabilising Byzantine Clock Synchronisation Is Almost as Easy as Consensus 32:55
Employing this consensus routine, every O(1) time units there is a constant probability that the
next level of recursion stabilises. Applying Chernoff’s bound over the (at most) log f recursive
levels of stabilisation, this yields stabilisation in O(log f ) time with high probability.
Corollary 3. Suppose we have private channels. For any f ≥ 0 and n > 3f , there is a randomised
f -resilient Θ(log f )-pulser over n nodes that stabilises inO(log f ) time w.h.p. and has nodes broadcast
poly f bits per time unit.
8 CONCLUSIONS
In this work, we have seen that self-stabilising pulse synchronisation under Byzantine faults can be
achieved efficiently in the bounded-delay model with bounded clock drift: the problem reduces to
the task of solving (non-stabilising) synchronous binary consensus efficiently. With deterministic
algorithms, a linear stabilisation time in the number f of faults is possible with nodes broadcasting
O(log f ) per time unit. On the other hand, we see that one can obtain sublinear time algorithms
by using randomisation at the expense of more bits broadcast per time unit.
We now conclude by highlighting some interesting open problems in the area:
• The construction presented here was based on a reduction to consensus. This raises the
question whether there is a reduction from consensus, that is, is pulse synchronisation at
least as hard as consensus? As no reduction in the other direction is known, the true complexity of pulse synchronisation still remains an open question. It may very well be that
pulse synchronisation is strictly easier than synchronous consensus.
• The reduction presented in this work is fairly complicated. Are there simple and efficient
algorithms for achieving pulse synchronisation in a self-stabilising manner?
• Can the techniques used in this work be used to make existing practical non-self-stabilising
clock synchronisation algorithms self-stabilising?