We study the problem of computing conjunctive queries over large databases on parallel architectures without
shared storage. Using the structure of such a query q and the skew in the data, we study tradeoffs between
the number of processors, the number of rounds of communication, and the per-processor load—the number
of bits each processor can send or can receive in a single round—that are required to compute q. Since each
processor must store its received bits, the load is at most the number of bits of storage per processor.
When the data are free of skew, we obtain essentially tight upper and lower bounds for one round algorithms, and we show how the bounds degrade when there is skew in the data. In the case of skewed data, we
show how to improve the algorithms when approximate degrees of the (necessarily small number of) heavyhitter elements are available, obtaining essentially optimal algorithms for queries such as skewed simple joins
and skewed triangle join queries.
For queries that we identify as treelike, we also prove nearly matching upper and lower bounds for multiround algorithms for a natural class of skew-free databases. One consequence of these latter lower bounds is
that for any ε > 0, using p processors to compute the connected components of a graph, or to output the path,
if any, between a specified pair of vertices of a graph with m edges and per-processor load that is O(m/p1−ε )
requires Ω(logp) rounds of communication.
Our upper bounds are given by simple structured algorithms using MapReduce. Our one-round lower
bounds are proved in a very general model, which we call the Massively Parallel Communication (MPC) model,
that allows processors to communicate arbitrary bits. Our multi-round lower bounds apply in a restricted
version of the MPC model in which processors in subsequent rounds after the first communication round are
only allowed to send tuples.
Categories and Subject Descriptors: H.2.4 [Systems]: Parallel Databases
General Terms: Algorithms, Theory
Additional Key Words and Phrases: Parallel computation, lower bounds

1 INTRODUCTION
Most of the time spent during big data analysis today is allocated in data processing tasks, such as
identifying relevant data, cleaning, filtering, joining, grouping, transforming, extracting features,
and evaluating results (Chaudhuri 2012; EMC Corporation). These tasks form the main bottleneck
in big data analysis, and it is a major challenge to improve the performance and usability of data
processing tools. The motivation for this article comes from the need to understand the complexity
of query processing in big data management.
Query processing on big data is typically performed on a shared-nothing parallel architecture. In
this setting, the data are stored on a large number of independent servers interconnected by a fast
network. The servers perform local computations and can also communicate with each other to
exchange data. Starting from MapReduce (Dean and Ghemawat 2004), the past decade has seen the
development of several massively parallel frameworks that support big data processing, including
PigLatin (Olston et al. 2008), Hive (Thusoo et al. 2009), Dremmel (Melnik et al. 2010), Spark (Spark),
and Myria (Halperin et al. 2014).
Unlike traditional query processing, the time complexity is no longer dominated by the number
of disk accesses. Typically, a query is evaluated by a sufficiently large number of servers such that
the entire data can be kept in main memory. In these systems, the new complexity parameter is the
communication cost, which depends on both the amount of data being exchanged and the number
of global synchronization barriers (rounds).
Contributions. We define the Massively Parallel Communication (MPC) model, to analyze the
tradeoff between the number of rounds and the amount of communication required in a massively parallel computing environment. We include the number of servers p as a parameter, and
we allow each server to be infinitely powerful, subject only to the data to which it has access.
The computation proceeds in rounds, where each round consists of local computation followed by
global exchange of data between all servers.
An algorithm in the MPC model is characterized by the number of servers p, the number of
rounds r, and the maximum number of bits L, or maximum load, that each server receives at any
round. There are no other restrictions on communication between servers. Though the storage
capacity of each server is not a separate parameter, since a server needs to store the data it has
received to operate on it, the load L is always a lower bound on the storage capacity of each server.
An ideal parallel algorithm with input size M would distribute the input data equally among the p
servers, so each server would have a maximum load of M/p, and would perform the computation
in a single round. In the degenerate case where L = M, the entire data can be sent to a single server,
and thus there exists no parallelism.
The focus of the MPC model on the communication load captures a key property of the system
architectures assumed by MapReduce and related programming abstractions.1 Since there is no
restriction on the form of communication or the kinds of operations allowed for processing of local
data, the lower bounds we obtain in the MPC model apply much more generally than those bounds
based on specific assumed primitives or communication structures such as those for MapReduce.
We establish both lower and upper bounds in the MPC model for computing a full conjunctive
query q, in three different settings.
First, we restrict the computation to a single communication round and to input data without
skew. In particular, given a query q over relations S1, S2,... such that an input relation Sj has size
1This focus is somewhat related to the generic approach to communication in Valiant’s Bulk Synchronous Parallel (BSP)
model (Valiant 1990), which the MPC model simplifies and strengthens. We discuss the relationship of the MPC model to
a variety of MapReduce and parallel models in Section 6.
Journal of the ACM, Vol. 64, No. 6, Article 40. Publication date: October 2017.
Communication Steps for Parallel Query Processing 40:3
Mj (in bits), we examine the minimum load L for which it is possible to compute q in a single
round. We show that any algorithm that correctly computes q requires a load
L ≥ maxu


j Muj
j
p


1/

j uj
,
where u = (u1,...,u ) is a fractional edge packing for the hypergraph of q. Our lower bound applies to the strongest possible model in which servers can encode any information in their messages and have access to a common source of randomness. This is stronger than the lower bounds
in Afrati et al. (2012) and Koutris and Suciu (2011), which assume that the units being exchanged
are tuples. We further show that a simple algorithm, which we call the HyperCube algorithm,
matches our lower bound for any conjunctive query when the input data have no skew. As an
example, for the triangle query C3 = S1 (x,y), S2 (y, z), S3 (z, x) with sizes M = |S1 | = |S2 | = |S3 |, we
show that the lower bound for the load is Ω(M/p2/3), and the HyperCube algorithm can match
this bound.
Second, we study how skew influences the computation. A value in the database is skewed and is
called a heavy hitter when it occurs with much higher frequency than some predefined threshold.
Since data distribution is typically done using hash-partitioning, unless they are handled differently from other values, all tuples containing a heavy hitter will be sent to the same server, causing
it to be overloaded. The standard technique that handles skew consists of first detecting the heavy
hitters, then treating them differently from the other values, for example, by partitioning tuples
with heavy hitters on the other attributes.
In analyzing the impact of skew, we first provide bounds on the behavior of algorithms that
are not given special information about heavy hitters and hence are limited in their ability to
deal with skew. We then consider a natural model for handling skew which assumes that at
the beginning of the computation all servers know the identity of all heavy hitters, and the (approximate) frequency of each heavy hitter. (It will be easy to see that there can only be a small
number of heavy hitters and this kind of information can be easily obtained in advance.2) Given
these statistics, we present upper and lower bounds for the maximum load for full conjunctive
queries. In particular, we present a general lower bound that holds for any conjunctive query.
We next give matching upper bounds for the class of star joins, which are queries of the form
q(z, x1,..., xk ) = S1 (z, x1), S2 (z, x2),..., Sk (z, xk ) (this includes the case of the simple join query
for k = 2), as well as the triangle query.
Third, we establish lower bounds for multiple communication rounds, for a restricted version
of the MPC model, called tuple-based MPC model. The messages sent in the first round are still
unrestricted, but in subsequent rounds the servers can send only tuples, either base tuples in the
input tables, or join tuples corresponding to a subquery; moreover, the destinations of each tuple
may depend only on the tuple content, the message received in the first round, the server, and
the round. We should note that many of the most frequently used MapReduce algorithms for
query processing are tuple based (e.g., parallel hash-join, broadcast join). Here, we prove that
the number of rounds required is, essentially, given by the depth of a query plan for the query,
where each operator is a subquery that can be computed in one round with the required load. For
example, to compute a length k chain query Lk , if we want to achieve L = O(M/p), then the optimal
computation is a bushy join tree, where each operator is L2 (a two-way join) and the optimal
number of rounds is log2 k. If we allow a larger load, L = O(M/p1/2), then we can use L4 as operator
2For example, we can compute all frequencies by performing a parallel aggregation, or we can find the heavy hitters by
sampling; both of these methods can be executed in one round, with load O (M/p).
Journal of the ACM, Vol. 64, No. 6, Article 40. Publication date: October 2017.     
40:4 P. Beame et al.
Table 1. Roadmap for the Organization of the Results Presented in This Article
Number of Rounds Data Distribution Upper Bound Lower Bound
1 round (MPC model)
no skew Section 3.1 Section 3.2
skew (oblivious) Section 4.1 Section 4.1
skew (with information) Section 4.2.1 Section 4.2.3
multiple rounds (tuplebased MPC model)
no skew Section 5.1 Section 5.2
(a four-way join), and the optimal number of rounds decreases to log4 k. More generally, we show
nearly matching upper and lower bounds based on graph-theoretic properties of the query.
We further show that our results for conjunctive path queries imply that any tuple-based MPC
algorithm with load L < M requires Ω(logp) rounds to compute the connected components of
sparse undirected graphs of size M (in bits). This is an interesting contrast to the results of Karloff
et al. (2010), which show that connected components (and indeed minimum spanning trees) of
undirected graphs can be computed in only two rounds of MapReduce provided that the input
graph is sufficiently dense.
By being explicit about the number of processors, in the MPC model we must directly handle
issues of load balancing and skew in task (reducer) sizes that are often ignored in MapReduce
algorithms but are actually critical for good performance (e.g., see Kwon et al. (2012)). When task
sizes are similar, standard analysis shows that hash-based load balancing works well. However,
standard bounds do not yield sharp results when there is significant deviation in sizes. To handle
such situations, we prove a sharp Chernoff bound for weighted balls in bins that is particularly
suited to the analysis of hash-based load balancing with skewed data. This bound, which is given
in Section 3, should be of independent interest.
Organization. We start by presenting the MPC model and defining important notions in
Section 2. In Section 3, we describe the upper and lower bounds for computation restricted to
one round and data without skew. We study the effect of data skew in Section 4. In Section 5, we
present upper and lower bounds for the case of multiple rounds. We conclude by discussing the
related work in Section 6. In Table 1, the reader can view a more detailed roadmap for the results
of this article.
2 MODEL
In this section, we present in detail the MPC model and introduce the necessary terminology and
background.
2.1 Massively Parallel Communication
In the MPC model, computation is performed by p servers, or processors, connected by a complete
network of private channels. The computation proceeds in steps, or rounds, where each round
consists of two distinct phases:
Communication Phase. The servers exchange data, each by communicating with all other
servers (sending and receiving data).
Computation Phase. Each server performs only local computation.
The input data of size M bits is initially uniformly partitioned among the p servers, that is,
each server stores M/p bits of the data: This describes the way the data are typically partitioned
in a distributed storage system. We do not assume that the data are partitioned in any particular
Journal of the ACM, Vol. 64, No. 6, Article 40. Publication date: October 2017.
Communication Steps for Parallel Query Processing 40:5
way, in other words, an algorithm in the MPC model must return the correct result no matter
how the input data are partitioned. At the end of the execution, the output must be present in the
union of the p processors. The lower bounds in this work hold even if we allow the output to have
overlap among the processors; however, all algorithms guarantee that an output tuple will appear
in exactly one processor.
The complexity of an algorithm in the MPC model is captured by two parameters:
The number of rounds r. This parameter denotes the number of synchronization barriers that
an algorithm requires.
The maximum load L. This parameter denotes the maximum load among all servers at any
round, where the load is the amount of data received by a server during a particular round.
Normally, all of the data are exchanged during the first communication round, so the load L is
at least M/p. On the other hand, the load is strictly less than M: Otherwise, if we allowed a load
L = M, then any problem can solved trivially in one round by simply sending the entire data to
server 1 and then computing the answer locally. Typical loads will be of the form M/p1−ε , for some
0 ≤ ε < 1 that depends on the query. The bounds on the load depend on both the amount of the
input data and the size of the query. We assume that the query to be executed is known to all
servers. In many cases of interest, the query is of small constant size, but in the worst case the
dependence on the query size is at most linear.
We do not allow the number of rounds to reachr = p, because any problem can be solved trivially
in p rounds by sending at each round M/p bits of data to server 1, until this server accumulates
the entire data. In fact, all of the algorithms presented in this article require a constant number of
rounds, r = O(1).
Input Servers. As explained above, the data are initially distributed uniformly on the p servers;
we call this form of input partitioned input. Our lower bounds for queries over a fixed relational vocabulary S1,..., S consider an alternative, more powerful model, where each relation Sj is stored
on a separate server, called an input server; during the first round the  input servers distribute
their data to the p workers and then no longer participate in the computation. The input-server
model is potentially more powerful, since the jth input server has access to the entire relation Sj ,
whose size is much larger than M/p. We state and prove all our lower bounds for the input-server
model. This is w.l.o.g., because any algorithm in the partitioned-input model with load L can be
converted into an input-server algorithm with the same load, as follows. Denote fj = |Sj |/(

i |Si |)
for all j = 1,...,: We assume these numbers are known by all input servers, because we assume
the statistics |Sj | known to the algorithm. Then, each input server j holding the relation Sj will
partition Sj into fjp equal fragments and then will simulate fjp workers, each processing one
of the fragments. Thus, our lower bounds for the input-sever model immediately apply to the
partitioned-input model.
Randomization. The MPC model allows randomization. The random bits are available to all
servers and are computed independently of the input data. The algorithm may fail to produce
its output with a small probability η > 0, independent of the input. For example, we use randomization for load balancing and abort the computation if the amount of data received during a round
would exceed the maximum load L, but this will only happen with exponentially small probability.
To prove lower bounds for randomized algorithms, we use Yao’s Lemma (Yao 1977). We first
prove bounds for deterministic algorithms, showing that any algorithm fails with probability at
least η over inputs chosen randomly from a distribution μ. This implies, by Yao’s Lemma, that
every randomized algorithm with the same resource bounds will fail on some input (in the support
of μ) with probability at least η over the algorithm’s random choices.
Journal of the ACM, Vol. 64, No. 6, Article 40. Publication date: October 2017.    
40:6 P. Beame et al.
2.2 Conjunctive Queries
In this article, we consider a particular class of problems for the MPC model, namely computing answers to conjunctive queries over a database. We fix an input vocabulary S1,..., S, where
each relation Sj has a fixed arity aj ; we denote a =
j=1 aj . The input data consist of one relation
instance for each symbol.
We consider full conjunctive queries (CQs) without self-joins, denoted as follows:
q(x1,..., xk ) = S1 (x¯1),..., S (x¯ ). (1)
Sj (x¯j) is called an atom, where x¯j denotes a vector of variables. The query is full, meaning that
every variable in the body appears in the head (for example, q(x) = S (x,y) is not full), and without self-joins, meaning that each relation name Sj appears only once (for example, q(x,y, z) =
S (x,y), S (y, z) has a self-join). The first restriction, to full conjunctive queries, is a limitation: Our
lower bounds do not carry over to general conjunctive queries (but the upper bounds do carry
over). The second restriction, to queries without self-joins, is w.l.o.g.3
The hypergraph of a query q is defined by introducing one node for each variable in the body
and one hyperedge for each set of variables that occur in a single atom. We say that a conjunctive
query is connected if the query hypergraph is connected. For example, q(x,y) = R(x), S (y) is not
connected, whereas q(x,y) = R(x), S (y),T (x,y) is connected. We use vars(Sj) to denote the set of
variables in the atom Sj , and atoms(xi ) to denote the set of atoms where xi occurs; k and  denote
the number of variables and atoms in q, as in Equation (1). The connected components of q are the
maximal connected subqueries of q.
Characteristic of a Query. The characteristic of a conjunctive query q as in Equation (1) is defined
as χ (q) = a − k −  + c, where a =
j aj is the sum of arities of all atoms, k is the number of variables,  is the number of atoms, and c is the number of connected components of q.
4 Intuitively,
a larger value of the characteristic implies that variables will be more “shared” among the atoms,
that is, they will occur more often. The characteristic of a query will play a crucial role at the
design of multi-round algorithms in Section 5.
For a query q and a set of atoms M ⊆ atoms(q), define q/M to be the query that results from
contracting the edges in the hypergraph of q. As an example, if we define
Lk = S1 (x0, x1), S2 (x1, x2),..., Sk (xk−1, xk ),
then we have that L5/{S2, S4} = S1 (x0, x1), S3 (x1, x3), S5 (x3, x5).
Lemma 2.1. The characteristic of a query q satisfies the following properties:
(a) If q1,...,qc are the connected components of q, then χ (q) = c
i=1 χ (qi ).
(b) For any M ⊆ atoms(q), χ (q/M) = χ (q) − χ (M).
3To see this, denote q the query obtained from q by giving distinct names S
j, S
j ,... to repeated occurrences of the same
relation Sj . Any algorithm A for q can be used to compute q with the same load, by first having each server copy locally its
fragment of Sj into new relations S
j, S
j ,..., then executing A on the new inputs. Conversely, any algorithm A for q can
also be converted into an algorithm A for q
, having the same load as A has on an input that is  times larger. The algorithm
A is obtained as follows. First, each server will copy its fragment of Sj into new relations S
j, S
j ,... Namely, for each atom
S
j (x, y, z,...) in q derived from some atom Sj (x, y, z,...) in q, the server will insert for each tuple (a, b, c,...) ∈ Sj
a new tuple ((a, “x”), (b, “y”), (c, “z”),...) ∈ S
j . That is, each value a in the first column is replaced by the pair (a, “x”),
where x is the variable occurring in that column and similarly for all other columns. This copy operation can be done
locally by all servers, without communication, and each input relation is copied at most  times. Finally, run the algorithm
A on the copied relations. 4In the preliminary version of this article (Beame et al. 2013), we defined χ (q) with the opposite sign (as −a + k +  − c);
we find the current definition more natural since now χ (q) ≥ 0 for every q.
Journal of the ACM, Vol. 64, No. 6, Article 40. Publication date: October 2017.             
Communication Steps for Parallel Query Processing 40:7
(c) χ (q) ≥ 0.
(d) For any M ⊆ atoms(q), χ (q) ≥ χ (q/M).
Proof. Property (a) is immediate from the definition of χ, since the connected components
of q are disjoint with respect to variables and atoms. Since q/M can be produced by contracting
according to each connected component of M in turn, by property (a) and induction it suffices to
show that property (b) holds in the case that M is connected. If a connected M has kM variables,
M atoms, and total arity aM , then the query after contraction, q/M, will have the same number
of connected components, kM − 1 fewer variables, and the terms for the number of atoms and
total arity will be reduced by aM − M for a total reduction of aM − kM − M + 1 = χ (M). Thus,
property (b) follows.
By property (a), it suffices to prove (c) whenq is connected. If q is a single atomSj , then χ (Sj) ≥ 0,
since the number of variables is at most the arity aj of the atom. If q has more than one atom, then
let Sj be any such atom: Then χ (q) = χ (q/Sj) + χ (Sj) ≥ χ (q/Sj), because χ (Sj) ≥ 0. Property (d)
follows from (b) using the fact that χ (M) ≥ 0.
For a simple illustration of property (b), consider the example above L5/{S2, S4}, which is equivalent to L3. We have χ (L5) = 10 − 6 − 5 + 1 = 0, and χ (L3) = 6 − 4 − 3 + 1 = 0, and also χ (M) = 0
(because M consists of two disconnected components, S2 (x1, x2) and S4 (x3, x4), each with characteristic 0). For a more interesting example, consider the query K4 whose graph is the complete
graph with four variables,
K4 = S1 (x1, x2), S2 (x1, x3), S3 (x2, x3), S4 (x1, x4), S5 (x2, x4), S6 (x3, x4),
and denote M = {S1, S2, S3}. Then K4/M = S4 (x1, x4), S5 (x1, x4), S6 (x1, x4), and the characteristics
are as follows: χ (K4) = 12 − 4 − 6 + 1 = 3, χ (M) = 6 − 3 − 3 + 1 = 1, χ (K4/M) = 6 − 2 − 3 + 1 = 2.
Finally, we define a class of queries that will be used later in the article.
Definition 2.2. A conjunctive query q is treelike if q is connected and χ (q) = 0.
For example, the query Lk is treelike; in fact, a query over a binary vocabulary is treelike if and
only if its hypergraph is a tree. Over non-binary vocabularies, if a query is treelike, then it is acyclic,
but the converse does not hold even when the query is connected: q = S1 (x0, x1, x2), S2 (x1, x2, x3)
is acyclic but not treelike.5 An important property of treelike queries is that every connected subquery will be also treelike.
Fractional Edge Packing. A fractional edge packing (also known as a fractional matching) of a
query q is any feasible solution u = (u1,...,u ) of the following linear constraints:
∀i ∈ [k] : 
j:i ∈Sj
uj ≤ 1 (2)
∀j ∈ [] : uj ≥ 0.
The edge packing associates a non-negative weight uj to each atom Sj such that for every variable xi , the sum of the weights for the atoms that contain xi do not exceed 1. If all inequalities in
Equation (2) are equalities, then we say that the solution u is tight. The dual notion is a fractional
5There are a number of notions of hypergraph acyclicity in the literature, see Brault-Baron (2016) for a recent survey, but
our notion is more restrictive than any of them because it prohibits any pair of variables being contained in more than one
atom.
Journal of the ACM, Vol. 64, No. 6, Article 40. Publication date: October 2017.      
40:8 P. Beame et al.
vertex cover of q, which is a feasible solution v = (v1,...,vk ) to the following linear constraints:
∀j ∈ [] : 
i:i ∈Sj
vi ≥ 1
∀i ∈ [k] : vi ≥ 0.
At optimality, maxu

j uj = minv

i vi ; this quantity is denoted τ ∗ and is called the fractional
vertex covering number of q.
Example 2.3. An edge packing of the query L3 = S1 (x1, x2), S2 (x2, x3), S3 (x3, x4) is any solution
to u1 ≤ 1, u1 + u2 ≤ 1, u2 + u3 ≤ 1, and u3 ≤ 1. In particular, the solution (1, 0, 1) is a tight edge
packing; it is also an optimal packing, thus τ ∗ = 2.
A fractional edge cover is a feasible solution u = (u1,...,u ) to a system of linear constraints
as above, where ≤ is replaced by ≥ in Equation (2). Every tight fractional edge packing is a
tight fractional edge cover and vice versa. The optimal value of a fractional edge cover is denoted ρ∗. The fractional edge packing and cover have no connection, and there is no relationship between τ ∗ and ρ∗. For example, for q = S1 (x,y), S2 (y, z), we have τ ∗ = 1 < ρ∗ = 2, while for
q = S1 (x), S2 (x,y), S3 (y) we have τ ∗ = 2 > ρ∗ = 1. The two notions coincide, however, when they
are tight, meaning that a tight fractional edge cover is also a tight fractional edge packing and
vice versa. The fractional edge cover has been used recently in several articles to prove bounds on
query size and the running time of a sequential algorithm for the query (Atserias et al. 2008; Ngo
et al. 2012); for the results in this article we need the fractional packing.
2.3 Entropy
Let us fix a finite probability space. For random variables X and Y, the entropy and the conditional
entropy are defined, respectively, as follows:
H(X) = −

x
P (X = x) log P (X = x), (3)
H(X | Y ) =

y
P (Y = y)H(X | Y = y). (4)
The entropy satisfies the following basic inequalities:
H(X | Y ) ≤ H(X)
H(X,Y ) = H(X | Y ) + H(Y ). (5)
Assuming additionally that X has a support of size n:
H(X) ≤ logn. (6)
2.4 Friedgut’s Inequality
Friedgut (2004) introduces the following class of inequalities. Each inequality is described by a
hypergraph, which in our article corresponds to a query, so we will describe the inequality using
query terminology. Fix a query q as in Equation (1), and let n > 0 be an integer. For every atom
Sj (x¯j) of arity aj , we introduce a set of naj variables wj (aj) ≥ 0, where aj ∈ [n]
aj . If a ∈ [n]
a, then
we denote by aj the vector of size aj that results from projecting on the variables of the relation
Sj . Let u = (u1,...,u ) be a fractional edge cover for q. Then:

a∈[n]k


j=1
wj (aj) ≤


j=1




aj ∈[n]
aj
wj (aj)
1/uj 


uj
. (7)
Journal of the ACM, Vol. 64, No. 6, Article 40. Publication date: October 2017.         
Communication Steps for Parallel Query Processing 40:9
We illustrate Friedgut’s inequality on the queries C3 and L3:
C3 (x,y, z) = S1 (x,y), S2 (y, z), S3 (z, x)
L3 (x,y, z,w) = S1 (x,y), S2 (y, z), S3 (z,w). (8)
Consider the cover (1/2, 1/2, 1/2) forC3 and the cover (1, 0, 1) for L3. Then, we obtain the following
inequalities, where α, β,γ stand for w1,w2,w3, respectively:

x,y,z ∈[n]
αxy · βyz · γzx ≤
 
x,y ∈[n]
α2
xy 
y,z ∈[n]
β2
yz 
z,x ∈[n]
γ 2
zx

x,y,z,w ∈[n]
αxy · βyz · γzw ≤

x,y ∈[n]
αxy · max
y,z ∈[n]
βyz ·

z,w ∈[n]
γzw ,
where we used the fact that limu→0 (
 β
1
u
yz )
u = max βyz .
Friedgut’s inequalities immediately imply a well-known result developed in a series of articles (Grohe and Marx 2006; Atserias et al. 2008; Ngo et al. 2012) that gives an upper bound on the
size of a query answer as a function on the cardinality of the relations. For example, in the case of
C3, consider an instance S1, S2, S3, and set αxy = 1 if (x,y) ∈ S1; otherwise, αxy = 0 (and similarly
for βyz ,γzx ). We obtain then |C3 | ≤ √
|S1 |·|S2 |·|S3 |. Note that all these results are expressed in
terms of a fractional edge cover. When we apply Friedgut’s inequality in Section 3 to a fractional
edge packing, we ensure that the packing is tight.
3 ONE COMMUNICATION STEP WITHOUT SKEW
In this section, we consider the case where the data have no skew, and the computation is restricted
to a single communication round.
We will say that a database is a matching database over a domain [n] if, in each attribute of
each relation, every domain value occurs at most once; equivalently, every attribute is a key. In
particular, every value in each relation has degree bounded by 1, that is, the frequency of each value
is exactly 1. Our lower bounds in this section will hold for such matching databases. The upper
bound, and in particular the load analysis for the algorithm, hold for more general databases, with
a small amount of skew, which we will formally define in Section 3.1.
We assume that all input servers know the cardinalities m1,...,m of the relations S1,..., S,
and denote m = (m1,...,m ) the vector of cardinalities. We also denote M1,..., M the number
of bits needed to represent each relation, and call M = (M1,..., M ) derived statistics. Since each
tuple of Sj requires aj logn bits to represent, where n is the size of the domain of the database, we
have Mj ≤ ajmj logn. Our upper bounds are stated in terms of the cardinalities m; in that case,
we define the derived statistics Mj
def
= ajmj logn. Our lower bounds are stated in terms of M only
and apply to more general encodings; this is necessary, because for restricted classes of databases
Mj can be much smaller than ajmj logn. For example, the standard encoding of a relation of arity
2 and cardinality mj uses Mj = 2mj logn bits, but if the relation is restricted to be a matching of
cardinality n (equivalently, the relation is a permutation over the domain [n]), then it requires only
Mj = logn! bits. As an extreme case, if Sj is a unary relation, then it can be encoded using a bitmap,
and Mj = logn.
3.1 The HyperCube Algorithm
We describe here an algorithm that computes a conjunctive query in one step. Such an algorithm
was introduced by Afrati and Ullman (2010) for MapReduce, is similar to an algorithm by Suri and
Vassilvitskii (2011) to count triangles, and also uses ideas that can be traced back to Ganguly et al.
Journal of the ACM, Vol. 64, No. 6, Article 40. Publication date: October 2017.      
40:10 P. Beame et al.
(1992) for parallel processing of Datalog programs. We call this the HyperCube (HC) algorithm,
following Beame et al. (2013).
The HC algorithm is parametrized by a vector of shares : Each variable xi , where i = 1,..., k,
is assigned a share pi , such that k
i=1 pi = p. The name of the algorithm results from the fact that
each server is mapped to a distinct point in the k-dimensional hypercube P = [p1] ×···× [pk ].
The description of the HC algorithm is as follows:
ALGORITHM 1 : HyperCube Algorithm
Input: shares p = (p1,...,pk ), query q, relations S1,..., S.
1: Assign each server in [p] to a distinct point in P = [p1] ×···× [pk ]
2: Choose k independent hash functions hi : [n] → [pi]
3: Communication: send each tuple Sj (t) to the subcube
D(t) = {y ∈P|∀s = 1,..., aj : his (t[is ]) = yis }
4: Computation: each server computes q on its local instance
The correctness of the HC algorithm follows from the observation that, for every potential
tuple (a1,..., ak ), the server (h1 (a1),...,hk (ak )) contains all the necessary information to decide
whether it belongs in the answer or not.
Example 3.1. We illustrate how to compute the triangle query C3 (x1, x2, x3) = S1 (x1, x2),
S2 (x2, x3), S3 (x3, x1). Consider the sharesp1 = p2 = p3 = p1/3. Each of the p servers is uniquely identified by a triple (y1,y2,y3), where y1,y2,y3 ∈ [p1/3]. In the first communication round, the input
server storing S1 sends each tuple S1 (α1, α2) to all servers with index (h1 (α1),h2 (α2),y3) for all
y3 ∈ [p1/3]: Notice that each tuple is replicated p1/3 times. The input servers holding S2 and S3 proceed similarly with their tuples. After round 1, any three tuples S1 (α1, α2), S2 (α2, α3), S3 (α3, α1) that
contribute to the output tuple C3 (α1, α2, α3) will be seen by the server y = (h1 (α1),h2 (α2),h3 (α3)):
Any server that detects three matching tuples outputs them.
3.1.1 Analysis of the HC Algorithm. The main technical result of this section is the analysis
of the HC algorithm. Theorem 3.2 considers the hypercube partition of a single relation R and
gives conditions under which this partition is balanced; the analysis of the HC algorithm follows
immediately. To state and prove Theorem 3.2, it is convenient to allow the input relation R to
be a bag rather than a set, that is, each tuple may occur multiple times in R; the size m = |R|, or
cardinality of R, is defines as the total number of tuples, counting multiplicities.
Let R be a relation of arity r, that is, a bag of tuples. For a tuple J over a subsetU ⊆ [r], we define
the degree of the tuple J in relation R, denoted dJ (R), as dJ (R) = |{t ∈ R | t[U ] = J}|. For a full tuple
J (i.e., U = [r]), dJ (R) is the multiplicity of J in R, and for the empty tuple J = (), dJ (R) = |R|. Note
that in the special case of matching databases, for every non-empty tuple J we have dJ (R) = 1.
Theorem 3.2. Let R(A1,...,Ar ) be a relation of arity r of sizem. Let p1,...,pr be integers, pi ≥ 2
for all i, and let p = i pi . We hash partition R into p bins as follows. We identify each bin with an
r-tuple in [p1] ×···× [pr ], and sent each tuple (a1,..., ar ) is to the bin (h1 (a1),...,hr (ar )), where
h1,...,hr are independent and perfectly random hash functions, with co-domains [p1],...,[pr ], respectively. Then the expected load in every bin is m/p and the following hold:
(1) Fix Δ > 0 a constant. If for every non-empty tuple J, dJ (R) ≤ Δ, then for any constant δ > 0,
the probability that the maximum load of any server exceeds (1 + δ )m/p is is exponentially
small in (m/(pΔ))1/r .
Journal of the ACM, Vol. 64, No. 6, Article 40. Publication date: October 2017.   
Communication Steps for Parallel Query Processing 40:11
(2) For every c > 0 there is a c > 0 such that for 2 ≥ δ >
ln p
cr 2p1/2 and β = δ
cr 3 ln p if for every
tuple J over U ⊆ [r], dJ (R) ≤ β |U |
 m
d∈U pd , then the probability that the maximum load of
any server exceeds (1 + δ )m/p is at most p−cr .
(3) If for every tuple J over U ⊆ [r], dJ (R) ≤  m
d∈U pd , then for every c > 0 there is a c > 0 such
that the probability that the maximum load of any server exceeds (
c ln p
ln ln p )
rm/p is at most p−c .
Denote Ly the load of a server y = (y1,...,yr ) ∈ [p1] ×···× [pr ]; it is straightforward to check
that E[Ly1,...,yr ] = m/p, where the expectation is taken over the choices of the random hash
functions. The theorem establishes conditions under which the maximum load of all servers is
≤ (1 + δ )m/p. To explain it, we establish a necessary condition. If every server has load Ly ≤
(1 + δ )m/p, then, for any subset of attributesU ⊆ [r] and any sub-cube z ∈ d ∈U [pd ], the accumulated load at the sub-cube z is Lz
def
=
y:

d∈U yd=zd Ly ≤ (1 + δ )m/
d ∈U pd . This implies that, for
any U -tuple J, it is necessary for its degree to be bounded by dJ (R) ≤ (1 + δ )m/
d ∈U pd ; otherwise, the J tuple will cause at least one sub-cube z to have load Lz > (1 + δ )m/
d ∈U pd . The theorem establishes stricter conditions that are sufficient. Item 1 says, essentially, that if all degrees are
bonded by a constant, then the probability that the maximum load exceeds (1 + δ )m/p decreases
exponentially withm/(pΔ). Item 2 essentially allows the degrees to increase up to β |U |
m/
d ∈U pd ,
where β = O(δ/ lnp) < 1: In that case, the probability that the maximum load exceeds (1 + δ )m/p
decreases polynomially in p; notice that here there is a lower bound how small we can choose
δ. Finally, item 3 allows the degrees to approach their upper bound  m
d∈U pd by requiring δ to be
larger, δ = Ω(lnp/ ln lnp).
The condition pi ≥ 2 is without loss of generality: If some pi = 1, then we can simply project
away the attribute xi of R and apply the theorem to the remaining relation. If R is a set (has
no duplicate tuples), then after projecting away xi it may become a bag; this is the reason why
Theorem 3.2 is stated in terms of bags.
This theorem immediately implies the following statement on the behavior of the HC algorithm:
Corollary 3.3. Let p = (p1,...,pk ) be the shares of the HC algorithm. Fix 0 < δ < 1 and suppose
that every relation Sj (xi1 ,..., xir ) satisfies the condition of Theorem 3.2 with respect to the integers
pi1 ,...,pir and the constants δ,c. Then, the probability over the choices of the random hash functions
that the maximum load per server of the HC algorithm exceeds
(1 + δ ) maxj
mj
i:i ∈Sj pi
is exponentially small when all degrees are bounded by some constant or polynomially small when
the degrees are bounded as in Theorem 3.2 items 2 or 3.
Theorem 3.2 follows from repeated application of a Chernoff-like inequality, Lemma 3.4, which,
together with its corollary, is of independent interest. Let D(q
||q) def
= q ln(
q
q ) + (1 − q
) ln(
1−q
1−q )
denote the relative entropy (also known as the KL-divergence) of Bernoulli indicator variables with
probabilities q and q.
Lemma 3.4. Let w = (w1,...,wn ) be a sequence of non-negative numbers, and m, k be two numbers such that ||w||1 ≤ m and ||w||∞ ≤ m/k. Let X1,...,Xn be a sequence of i.i.d.6 random variables,
6As in Impagliazzo and Kabanets (2010), we can relax the i.i.d. requirement and allow the random variables to be identical,
and negatively correlated, that is, for any set S ⊆ [n], Pr(

i∈S Xi ) ≤ μ |S |
.
Journal of the ACM, Vol. 64, No. 6, Article 40. Publication date: October 2017.          
40:12 P. Beame et al.
Xi ∈ {0, 1}, and μ = E[Xi]. Then, for all γ such that μ < γ < 1, the following holds:
P



i ∈[n]
wiXi > γ · m


≤ exp (−k · D(γ ||μ)) . (9)
Proof. The proof follows along similar lines to constructive proofs of Chernoff bounds in
Impagliazzo and Kabanets (2010). We may assume w.l.o.g. that ||w||1 = m. Indeed, if ||w||1 < m,
then we extend w to a longer sequence w such that ||w
||1 = m and ||w
||∞ ≤ m/k, by defining
w
n+1 = w
n+2 = ··· = m/k, possibly setting the last value w
n to < m/k, to ensure that
i w
i = m;
then P(

i=1,n wiXi > γm) ≤ P(

i=1,n w
iXi > γm) (where Xn+1,Xn+2,... are independent random variables identical to X1), and the claim for w follows from the similar claim for w
. Thus, we
will assume ||w||1 = m.
Choose a random S ⊆ [n] by including each element i independently with probability qi :
P(i ∈ S) = qi
def
= 1 − (1 − q)
kwi /m .
Let E denote the event that
i ∈[n]wiXi ≥ γ · m. Then
E
⎡
⎢
⎢
⎢
⎢
⎣

i ∈S
Xi = 1
⎤
⎥
⎥
⎥
⎥
⎦
≥ E
⎡
⎢
⎢
⎢
⎢
⎣

i ∈S
Xi = 1 | E
⎤
⎥
⎥
⎥
⎥
⎦
· P(E).
We bound both expectations. First, we see that
E
⎡
⎢
⎢
⎢
⎢
⎣

i ∈S
Xi = 1
⎤
⎥
⎥
⎥
⎥
⎦
=

S ⊆[n]
μ |S | 
i ∈S
qi

iS
(1 − qi ) =

S ⊆[n]

i ∈S
(μqi )

iS
(1 − qi )
=

i ∈[n]
(μqi + (1 − qi )) =

i ∈[n]
(μ + (1 − μ)(1 − qi )) (10)
=

i ∈[n]
	
μ + (1 − μ)(1 − q)
kwi /m 

≤

i ∈[k]
(μ + (1 − μ)(1 − q)) (11)
= (1 − q(1 − μ))
k . (12)
We prove inequality (11). The function f (w) def
= μ + (1 − μ)(1 − q)
kw/m is log-convex,7 therefore the multi-variate function F (w) def
= i f (wi ) = i ∈[n](μ + (1 − μ)(1 − q)
kwi /m ) is also logconvex. Its maximum value on the polytope {w | 0 ≤ wi ≤ m/k,

i wi = m} is obtained at some
vertex of the polytope. Every vertex of this polytope consists of k values wi = m/k, one
value wi = r · m/k, where r def
= k − k < 1, and all other values wi = 0. At each such vertex,
the value of F (w) is (μ + (1 − μ)(1 − q))k  (μ + (1 − μ)(1 − q)
r ) and inequality (11) follows from
μ + (1 − μ)(1 − q)
r ≤ (μ + (1 − μ)(1 − q))r (by the concavity of the function xr ).
7Write д(w) = log f (w). Then д(w) = log(a + be−cw ) for some a, b, c > 0; hence д
(w) = −bce−cw /(a + be−cw ) =
−bc/(aecw + b) is increasing and so д is convex.
Journal of the ACM, Vol. 64, No. 6, Article 40. Publication date: October 2017.           
Communication Steps for Parallel Query Processing 40:13
Second, for any outcome of X1,...,Xn that satisfies E, the probability that S misses all indices
i such that Xi = 0 is
E
⎡
⎢
⎢
⎢
⎢
⎣

i ∈S
Xi = 1 | E
⎤
⎥
⎥
⎥
⎥
⎦
=

i:Xi=0
(1 − q)
kwi m = (1 − q)
k
i:Xi =0 wi
m ≥ (1 − q)
k (m−γm)
m = (1 − q)
k (1−γ )
, (13)
since E implies that
i:Xi=0wi = m −
i:Xi=1wi ≤ m −γm. Combining Equations (12) and (13),
we obtain the follows:
P(E) ≤

1 − q(1 − μ)
(1 − q)1−γ
k
.
As noted in Impagliazzo and Kabanets (2010), by looking at its first derivative one can show that
the function F (q) = 1−q(1−μ)
(1−q)1−γ takes its minimum at q = q∗ = γ −μ
γ (1−μ) , where it has value e−D(γ | |μ)
,
and we obtain the following:
P(E) ≤ e−k ·D(γ | |μ)
,
as required.
Before we prove Theorem 3.2, we apply this lemma to weighted balls-in-bins.
Corollary 3.5 (Weighted Balls in Bins). Suppose that n balls of weightsw1,...,wn with total
weight m = n
i=1wi are each uniformly and independently sent to one of p bins. Let 0 < δ < p − 1. If
all weights wi are at most Δ = βm/p for some β > 0, then
P (some bin has weight ≥ (1 + δ )m/p) ≤ p · e
−D( 1+δ
p | | 1
p )·p/β ≤ p · e−h(δ )/β ,
where h(x) = (1 + x) ln(1 + x) − x.
Proof. The bound of the corollary using KL divergence follows by applying Lemma 3.4 with μ =
1/p, γ = (1 + δ )/p, and k = p/β and then taking a union bound over the p bins. Now, by definition,
D

1 + δ
p
|| 1
p

= (1 + δ )
p
ln(1 + δ ) + (p − 1 − δ )
p
ln 
1 − δ
p − 1

= 1
p

(1 + δ ) ln(1 + δ ) + (p − 1 − δ ) ln 
1 − δ
p − 1

= 1
p

(1 + δ ) ln(1 + δ ) − (p − 1 − δ )
 δ
(p − 1)
+
δ 2
2(p − 1)2 +
δ 3
3(p − 1)3 + ··· 
= 1
p

(1 + δ ) ln(1 + δ ) −

δ − δ 2
2(p − 1)
− δ 3
6(p − 1)2 −··· 
≥
1
p
[(1 + δ ) ln(1 + δ ) − δ] = h(δ )/p,
which yields the bound in terms of the function h.
This corollary naturally bounds the result of applying a uniformly random hash function from
[n] to [p] to a bag of m elements where wi is the multiplicity of item i.
Remark 3.6. We observe that the failure bound in Corollary 3.5 expressed in terms of h(δ ) also
trivially holds even when (1 + δ )/p ≥ 1 since the failure probability is 0 in that case. In general,
that failure probability has the following qualitative properties:
(1) When β is O(p/m) (equivalently, Δ is constant), the failure bound is similar to the usual
Chernoff bounds, decaying exponentially in m/p for every constant δ > 0.
Journal of the ACM, Vol. 64, No. 6, Article 40. Publication date: October 2017.      
40:14 P. Beame et al.
(2) When β is O(1/ lnp), there is a constant δ > 0 for which the failure bound is < 1/p.
(3) When β is a constant, there is some δ that is Θ(lnp/ ln lnp) such that the failure bound is
< 1/p.
We generalize now Lemma 3.4 from a sequence (wi ) to a multi-dimensional array. IfU is a finite
set, then we call I ∈ [n]
U a U -tuple and for d ∈ U , we write the corresponding lower-case id for
the component of I indexed by d. A multi-dimensional array with dimensions U , or U -dimensional
array, is a sequence w = (wI )I ∈[n]U of non-negative numbers. If I and J are U -tuples and V -tuples,
respectively, where U and V are disjoint, then I, J is the (U ∪V )-tuple that combines both tuples. If w is a U -dimensional array, V ⊆ U and J ∈ [n]
V , then wJ ∈ [n]
U −V denotes the (U −V )-
dimensional array (wJ )K
def
= wJ,K for K ∈ [n]
VU . We identify [n]
r with [n]
[r] and use the standard
norms ||w||1 =
I ∈[n]U wI and ||w||∞ = maxI ∈[n]U wI .
Theorem 3.7. Let r > 0 and w = (wI )I ∈[n]r be an r-dimensional array of non-negative weights.
Let m, k1,..., kr > 0 be numbers such that w satisfies the following degree constraints:
∀ U ⊆ [r] ∀ J ∈ [n]
U , ||wJ ||1 ≤ m d ∈U kd
. (14)
For each d ∈ [r], let X(d) = (X (d)
1 ,...,X (d) n ) be a sequence of i.i.d. random variables X(d)
i ∈ {0, 1}, let
μd = E[X (d)
i ], and let γd satisfy μd < γd < 1. Further assume that all variables (X (d)
i )i ∈[n],d ∈[r] are
independent, and for I ∈ [n]
U , write XI
def
= d ∈U X (d)
id .
Then
P



I ∈[n]r
wIXI >



d ∈[r]
γd



· m


≤ 2

∈[r]

d ∈[−1]
(kd + 1) · exp (−k · D(γ ||μ )) .
Proof. We prove the theorem by induction on r. Note that, in particular, Equation (14) implies
that ||w||1 ≤ m and ||w||∞ ≤  m
d∈[r ] kd . The case when r = 1 follows immediately from Lemma 3.4,
so we assume that r > 1. By definition,

I ∈[n]r
wIXI =

J ∈[n]r−1

ir ∈[n]
wJ,irXJX (r)
ir =

J ∈[n]r−1
wX(r )
J XJ ,
where we write J,ir for the r-tuple whose first r − 1 components are given by J and whose last
component is ir and where
wX(r )
J
def
=

ir ∈[n]
wJ,irX (r)
ir ,
yielding an (r − 1)-dimensional array of weights, denoted wX(r )
, that depends on the outcomes of
the random variables X(r)
.
Thus, if we write m def
= γr · m, then we have
P



I ∈[n]r
wIXI >



d ∈[r]
γd



· m


=P



J ∈[n]r−1
wX(r )
J XJ >



d ∈[r−1]
γd



· m


.
We will show that, except for a small failure probability, the (r − 1)-dimensional array wX(r )
satisfies the analog of Equation (14) with respect to m and r − 1, and hence we can apply the inductive hypothesis to this (r − 1)-dimensional subproblem to achieve the final bound on the failure
probability.
Journal of the ACM, Vol. 64, No. 6, Article 40. Publication date: October 2017.                     
Communication Steps for Parallel Query Processing 40:15
Consider someU ⊆ [r − 1]. The probability that the analog of Equation (14) with respect to r − 1
and m fails for U is
P

∃ J ∈ [n]
U . ||wX(r )
J ||1 > m
d ∈U kd

= P


∃ J ∈ [n]
U . || 
ir ∈[n]
wJ,irX (r)
ir ||1 > γr
m d ∈U kd



= P


∃ J ∈ [n]
U .

ir ∈[n]
||wJ,ir ||1 · X (r)
ir > γr
m d ∈U kd



.
For simplicity of notation, for each J ∈ [n]
U , define v(J ) ∈ Rn byv(J )
ir
def
= ||wJ,ir ||1 forir ∈ [n]. With
this notation, the probability that the analog of Equation (14) fails for U is
P


∃ J ∈ [n]
U .

ir ∈[n]
v(J )
ir · X (r)
ir > γr
m d ∈U kd



. (15)
By Equation (14), for each J,
||v(J )
||1 =||wJ ||1 ≤ m d ∈U kd
, and
||v(J )
||∞ = max
ir ∈[n]
||wJ,ir ||1 ≤ m d ∈U ∪ {r } kd
.
We could apply Lemma 3.4 to the one-dimensional array v(J ) to obtain a bound on P(

ir ∈[n] v(J )
ir ·
X(r)
ir > γr  m
d∈U kd ), but Equation (15) is the union of n|U | such events. Instead, we will show that
we can partition [n]
U into at most K = 2
d ∈U kd sets B1,..., BK , and bound Equation (15) as a
union of only K events, and then apply Lemma 3.4 to each. To perform this partition we use the
fact that:

J ∈[n]U
||v(J )
||1 =

J ∈[n]U
||wJ ||1 = ||w||1 ≤ m.
We partition the set [n]
U into “bins” B1,..., BK by applying a greedy bin-packing algorithm using the || · ||1 norm of the v(J ) vectors with bins of capacity C = m/
d ∈U kd . That is, we consider the tuples J in decreasing order of ||v(J )
||1 value and continue to add J to the current bin
while the total of the ||v(J )
||1 values is at most m/
d ∈U kd . Since the J are considered in decreasing order of ||v(J )
||1, for each bin Bj we have
J ∈Bj ||v(J )
||1 > C/2. From this and the fact that

J ∈[n]U ||v(J )
||1 ≤ m the greedy packing uses at most 2m/C = 2
d ∈U kd = K bins as claimed. (An
algorithm like first-fit decreasing might improve the packing, but it will not improve the worst-case
analysis.) For each bin Bj , define vector u(j) ∈ Rn by
u(j)
ir
def
= max
J ∈Bj
v(J )
ir
and observe that
||u(j)
||1 ≤

J ∈Bj
||v(J )
||1 ≤ m d ∈U kd
||u(j)
||∞ ≤ max
J ∈Bj
||v(J )
||∞ ≤ m d ∈U ∪ {r } kd
, and

ir ∈[n]
u(j)
ir · X (r)
ir ≥

ir ∈[n]
v(J )
ir · X (r)
ir for all J ∈ Bj .
Journal of the ACM, Vol. 64, No. 6, Article 40. Publication date: October 2017.                      
40:16 P. Beame et al.
For each j ∈ [K], applying Lemma 3.4 with w replaced by u(j)
, m replaced by  m
d∈U kd , γ replaced
by γr , μ replaced by μr , and k replaced by kr , we obtain that the total probability in Equation (15)
is at most
2

d ∈U
kd · exp(−kr · D(γr ||μr )).
Thus the probability that there is some U ⊆ [r − 1] for which the analog of Equation (14) for r − 1
and m does not hold is at most

U ⊆[r−1]
2

d ∈U
kd · exp(−kr · D(γr ||μr )) = 2

d ∈[r−1]
(kd + 1) · exp(−kr · D(γr ||μr )).
In the remaining case when Equation (14) does hold for r − 1 and m
, we can apply the inductive
hypothesis to say that the failure probability is at most
2

∈[r−1]

d ∈[−1]
(kd + 1) · exp(−k · D(γ ||μ )),
and hence the total failure probability is at most
2

∈[r]

d ∈[−1]
(kd + 1) · exp(−k · D(γ ||μ ))
as required.
We now apply this theorem to prove our bounds for multidimensional hashing.
Proof of Theorem 3.2. The value of the expectation follows immediately by the independence
and uniformity of the individual hash functions.
Suppose that for every tuple J over U ⊆ [r] we have
dJ (R) ≤ β |U | m d ∈U pd
for some β > 0. For every (i1,...,ir ) ∈ [n]
[r]
, define wi1,...,ir to be the multiplicity of (i1,...,ir )
in R; these together comprise an [r]-dimensional non-negative array w. If we set kd = pd /β for
all d ∈ [r], then the bounds on dJ (R) for all J ⊆ [r] imply that property (14) holds for w. Fix
one of the p servers and, for all d ∈ [r], id ∈ [n], define the random indicator variable X(d)
id = 
1 if hd (id ) = yd
0 otherwise . Since the hash functions are drawn from a set of independent and uniformly
random hash functions, X (d)
id are independent random variables; moreover, μd
def
= E[X (d)
id ] = 1/pd
and for δ  > 0 define γd = (1 + δ 
)μd = (1 + δ 
)/pd for each d ∈ [r].
Suppose, first, that γd < 1 for all d ∈ [r]. Then we can apply Theorem 3.7 to say that the probability that this server receives more than (1 + δ 
)
rm/p tuples is at most
2

∈[r]

d ∈[−1]
(pd /β + 1) · exp 
−pd · D

1 + δ 
pd
|| 1
pd

/β

and, by Corollary 3.5, this is at most
2

∈[r]

d ∈[−1]
(pd /β + 1) · exp(−h(δ 
)/β). (16)
As noted in Remark 3.6, this bound also holds when γd = (1 + δ 
)/pd ≥ 1 for some choices of d,
because the failure probabilities of the corresponding terms in the induction of Theorem 3.7 are
actually 0. Therefore, we can avoid the hypothesis that γd < 1, and conclude that the probability
that any server receives more than (1 + δ 
)
rm/p tuples is at most p times Equation (16).
Journal of the ACM, Vol. 64, No. 6, Article 40. Publication date: October 2017.                 
Communication Steps for Parallel Query Processing 40:17
We now consider constraints on β and δ  that are sufficient to prove the various parts of the
theorem. For β = 1, which corresponds to item (3), we have pd /β + 1 = pd + 1 < p5/3
d since pd ≥ 2.
Then
2

∈[r]

d ∈[−1]
(pd /β + 1) ≤ 2

∈[r]

d ∈[−1]
p5/3
d ≤ 2

∈[r]
p5/3
/3r−+1 ≤ p5/3
,
and the total failure probability is at most p8/3
e−h(δ
)
. Plugging in 1 + δ  = (c + 8/3) lnp/ ln lnp we
have that the probability that any server receives more than (1 + δ 
)
rm/p tuples is at most p−c .
Next consider the case that δ  = δ/(2r) where δ ≤ 2. Then (1 + δ 
)
r ≤ eδ /2 ≤ 1 + δ and the
Taylor series expansion for h(x) in this range yields that h(δ 
) ≥ δ 2
3r 2 . If we now set β = δ 2
cr 2 ln p ,
then e−h(δ
)/β ≤ p−c
/12. In this case,
pd /β + 1 = c
r 2
pd lnp
δ 2 + 1 < c
r 2 lnp
δ 2 · (pd + 1).
Then
2

∈[r]

d ∈[−1]
(pd /β + 1) <

c
r 2 lnp
δ 2
r−1
p5/3
by the same reasoning as in item (3). Using the lower bound on δ in terms of p and r and choosing
c = 24cr forc ≥ 2 yields that the probability that any server receives more than (1 + δ )m/p tuples
is bounded above by p−cr . This proves item (2).
Finally, suppose that for every tuple J we have that dJ (R) is at most Δ. In this case we choose β
such that for every J on subset U ⊆ [r], β |U |
m/
d ∈U pd ≥ Δ; that is,
β = max
U ⊆[r]

Δ d ∈U pd
m
1/ |U |
= (Δp/m)
1/r,
since the bound is clearly monotone increasing in |U |. Again choosing δ  = δ/(2r), we obtain that
the probability that any server has load more than (1 + δ )m/p is at most β−(r−1)
p8/3
e−h(δ
)/β which
is exponentially small in 1/β = (m/(pΔ))1/r , yielding item (1).
3.1.2 Choosing the Shares. Here we discuss how to compute the shares pi to optimize the
expected load per server. Afrati and Ullman (2010) compute the shares by optimizing the total
load
j mj/
i:i ∈Sj pi subject to the constraint i pi = p, which is a non-linear system that can
be solved using Lagrange multipliers. Our approach is to optimize the maximum load per relation, maxj mj/
i:i ∈Sj pi ; the total load per server is at most  times larger. This leads to a linear optimization problem, as follows. We express here the load in terms of the derived statistics
Mj = ajmj logn, as L = maxj Mj/
i:i ∈Sj pi , in to facilitate the connection to the lower bounds in
Section 3.2. First, write the shares as pi = pei where ei ∈ [0, 1] is called the share exponent for xi ,
denote λ = logp L and μj = logp Mj (we will assume w.l.o.g. that Mj ≥ p, hence μj ≥ 1 for all j).
Then, we optimize the LP:
minimize λ
subject to 
i ∈[k]
−ei ≥ −1
∀j ∈ [] : 
i ∈Sj
ei + λ ≥ μj
∀i ∈ [k] :ei ≥ 0, λ ≥ 0. (17)
Let e∗ be the optimal value of the solution to Equation (17), and denote Lupper = pe ∗
.
Journal of the ACM, Vol. 64, No. 6, Article 40. Publication date: October 2017.                  
40:18 P. Beame et al.
Theorem 3.8 (Upper Bound). Fix a query q over a database with relations S1,..., S, cardinalities m = (m1,...,m ), and derived statistics M = (M1,..., M ), and let p be a number of servers.
Let e = (e1,..., ek ) be the optimal solution to Equation (17) and denote pi = pei . Let β > 0 be some
constant such that, for every relation Sj and every tuple J over U ⊆ [aj], its degree is bounded by
dJ (Sj) ≤ β

|U |
mj
i∈U pi
. Then the HC algorithm with shares pi achieves O(Lupper ) maximum load with
high probability.
A special case of interest is when all sizes Mj are equal, therefore μ1 = ··· = μ = μ. In that
case, the optimal solution to Equation (17) can be obtained from an optimal fractional vertex cover
v∗ = (v∗
1,...,v∗
k ) by setting ei = v∗
i /τ ∗ (where τ ∗ =
i v∗
i ). To see this, we note that any feasible
solution (λ, e1,..., ek ) to Equation (17) defines the vertex cover vi = ei /(μ − λ), and in the opposite direction every vertex cover defines the feasible solution ei = vi/(

i vi ), λ = μ − 1/(

i vi );
furthermore, minimizing λ is equivalent to minimizing
i vi . Thus, when all relations have the
same size M, at optimality λ∗ = μ − 1/τ ∗, and Lupper = M/p1/τ ∗
. We illustrate more examples in
Section 3.5.
3.2 The Lower Bound
In this section, we prove a lower bound on the maximum load per server over databases with sizes
M (expressed in bits); our bounds apply immediately to the special case when M are derived from
cardinalities m, Mj = ajmj logn, but hold in more general cases when Mj is much smaller than
ajmj logn; with some abuse, we call M derived statistics in all cases.
Fix a query q and a fractional edge packing u of q. Denote:
L(u, M,p) =


j=1 Muj
j
p


1/

j uj
. (18)
Further denote Llower = maxu L(u, M,p), where u ranges over all edge packings for q. In the next
two sections we prove (more detailed versions of) the following theorem, which shows that Llower
yields a lower bound on the load required by any p-processor one-round MPC algorithm computing q over a database with derived statistics M.
Theorem 3.9. Let M be the vector of derived statistics for the input database to a query q. Let p be
the number of processors.
(a) There is a (natural) probability distribution I on databases with statistics M such that for any
one-round p-processor deterministic MPC algorithm A with load L < Llower /4, the expected
number of outputs produced by A on input I chosen from I is strictly less than the expected
number of outputs required to compute q on input I.
(b) There is a constant δ > 0 such that the worst-case load L for any one-round p-processor randomized MPC algorithm to compute a conjunctive query q having c components with probability at least 1/2c requires L ≥ cδ · Llower .
To gain some intuition behind the formula (18) and Llower , consider the case when all derived
statistics are equal, M1 = ··· = M = M. Then Llower = M/p1/

j uj , and this quantity is maximized
when u is a maximum fractional edge packing, whose value is τ ∗, the fractional vertex covering
number for q. Thus, Llower = M/p1/τ ∗
, which is the same expression as Lupper . In fact, this equivalence holds in general:
Journal of the ACM, Vol. 64, No. 6, Article 40. Publication date: October 2017.                 
Communication Steps for Parallel Query Processing 40:19
Theorem 3.10 (Eqivalence). For any vector of derived statistics M and number of processors p,
we have the following:
Llower = Lupper = max
u∈pk (q)
L(u, M,p),
where pk(q) is the set of extreme points of the convex polytope defined by the fractional edge packing
constraints in Equation (2).
If additionally M1 = ··· = M = M, then
Llower = Lupper = M/p1/τ ∗
,
where τ ∗ is the fractional vertex covering number for q.
Together, Theorems 3.9 and 3.10 immediately imply the following theorem.
Corollary 3.11 (Optimality Without Skew). Under the conditions of Theorem 3.8, the HC
algorithm with shares chosen as in that theorem, achieves a load that is within a constant factor of
the optimal load for any one-round MPC algorithm where the constant depends only linearly on the
ratio of the number of relations to the number of connected components in the query q.
In the remainder of this section, we prove Theorem 3.10 and then prove Theorem 3.9 in the next
two sections.
Proof of Theorem 3.10. The open segment defined by two points x,y ∈ Rn is the set
{(1 − t)x + ty | t ∈ (0, 1)}. A vertex of a polytope P ⊆ Rn is a point z ∈ P that that does not belong to any open segment defined by two points x,y ∈ P. Let V (P) denote the set of vertices
of P. When P is given by a set of linear inequalities, each vertex is obtained by converting a
maximal set of inequalities into equalities. If P is the fractional edge packing polytope given by
Equation (2), then V (P) = pk(q) and |pk(q)| ≤ (
k+
 ), because each vertex is obtained by converting  of the k +  inequalities into equalitiesand then solving for the  variables u.
By definition, Lupper = pe ∗
, where e∗ is the optimal solution to the primal LP problem (17). Consider its dual LP:
maximize 
j ∈[]
μj fj − f0
subject to 
j ∈[]
fj ≤ 1
∀i ∈ [k] : 
j:i ∈Sj
fj − f0 ≤ 0
∀j ∈ [] :fj ≥ 0, f0 ≥ 0. (19)
Let A ⊆ R+1 denote the polytope of feasible solutions, φ : A → R denote the objective function φ(f0, f1,..., f ) =
j μj fj − f , and f ∗ = maxφ(A) denote the optimal value. Let A0 ⊂ A be
the subset of feasible solutions where f0 > 0. (A0 is no longer a polytope.) We claim that f ∗ =
maxφ(A0). Indeed, by the primal-dual theorem, we have f ∗ = e∗, and, assuming μj > 1 for all j,
we have e∗ > 0, because λ = 0 is not part of any feasible solution of the primal LP. Since f ∗ > 0,
for any optimal solution f of the dual LP at least one fj > 0, and thus f ∈ A0.
Journal of the ACM, Vol. 64, No. 6, Article 40. Publication date: October 2017.            
40:20 P. Beame et al.
Consider now the following non-linear optimization problem:
maximize
1
u0
·




j ∈[]
μjuj − 1



subject to 
j ∈[]
uj ≤ u0
∀i ∈ [k] : 
j:i ∈Sj
uj ≤ 1
∀j ∈ [] : uj ≥ 0, u0 > 0. (20)
Let B0 ⊆ R+1 denote the set of feasible solutions, and let B ⊃ B0 denote the polytope obtained
by relaxing u0 > 0 to u0 ≥ 0. Let ψ : B0 → R denote the objective function, ψ (u0,u1,...,u ) =
(

j μjuj − 1)/u0, and u∗ = supψ (B0) the optimal value. First, we prove that u∗ = f ∗ and, moreover, ψ reaches the maximum in B0, that is, supψ (B0) = maxψ (B0). Let F : R+1 → R+1 be
the function F (u0,u1,...,u ) = (1/u0,u1/u0,...,u/u0). Then it is easily verified that F maps
B0 to A0, F (B0) ⊆ A0, meaning that for any feasible solution (u0,u1,...,u ) ∈ B0, its image is
a feasible solution (f0, f1,..., f ) ∈ A0. We also check that F (B0) = A0 by noting that F has
an inverse, F −1 (f0, f1,..., f ) = (1/f0, f1/f0,..., f/f0), and F −1 (A0) ⊆ B0. Moreover, φ ◦ F = ψ,
because φ(F (u0,u1,...,u )) =
j μjuj/u0 − 1/u0 = (

j μjuj − 1)/u0 = ψ (u0,...,u ). This implies
that f ∗ = maxφ(A0) = maxφ(F (B0)) = maxψ (B0) = u∗.
Second, we prove that the optimal value u∗ is attained at a vertex of the polytope B, that is,
at some (u0,u1,...,u ) ∈ B0 ∩V (B). The objective of Equation (19) is maximized at a vertex of
the polytope A, hence it is obtained at some vertex in A0 ∩V (A). Thus, it suffices to show that
F maps the vertices B0 ∩V (B) one-to-one to A0 ∩V (A), or, equivalently, that it maps a nonvertex to a non-vertex. Consider a non-vertex tu + t
u ∈ B0, where u = (u0,u1,...,u ) ∈ B0, u =
(u
0,u
1,...,u
 ) ∈ B0, t,t ∈ (0, 1) and t + t = 1. It follows by direct calculation that F (tu + t
u
) =
tu0
tu0+t
u
0
F (u) + t
u
0
tu0+t
u
0
F (u
), proving that it is also a non-vertex.
Third, we prove that we can add the constraint u0 =
j=1,  uj to Equation (20) without affecting the optimal value u∗. Indeed, if (u0,u1,...,u ) is any optimal solution for Equation (20),
then u∗ = (

j μjuj − 1)/u0 ≤ (

j μjuj − 1)/(

j=1,  uj), because u0 ≥
j=1,  uj , and
j μjuj − 1 >
0 (since u∗ = f ∗ > 0 when μj > 1 for all j, as we have shown earlier).
Finally, we observe that a vector (

j uj,u1,...,u ) is a feasible solution of Equation (20) iff
u = (u1,...,u ) is a fractional edge packing of the hypergraph of the query q. This implies that
u∗ = maxu∈pk (q) (

j μjuj − 1)/(

j uj), and the first claim of the theorem follows from
Lupper = pe ∗
= pu∗
= max
u∈pk (q)
p(

j μjuj−1)/(

j uj ) = max
u∈pk (q)



j Muj
j
p


1/

j uj
= Llower .
The second claim follows immediately by noting that, when M1 = ··· = M = M, then
L(u, M,p) = M/p1/

j uj and therefore maxu∈pk (q) L(u, M,p) = M/p1/ maxu∈pk (q)

j uj = M/p1/τ ∗
.
3.3 Lower Bound in Expectation
In this section, we prove part (a) of Theorem 3.9. The analysis in this section will also be used in
the following section to extend the lower bound to randomized algorithms and prove part (b) of
Theorem 3.9.
To prove the lower bound, we will define a probability space from which the input databases are
drawn. Given the cardinalities of the  relations of q, m, we first choose a domain size n ≥ maxj mj ,
Journal of the ACM, Vol. 64, No. 6, Article 40. Publication date: October 2017.                                                  
Communication Steps for Parallel Query Processing 40:21
to be specified later, and choose independently and uniformly each relation Sj from all matchings of
[n]
aj with exactlymj tuples. This will determine the derived statistics M. We call this the matching
probability space. Observe that the probability space contains only databases with relations without
skew (in fact, all degrees are exactly 1). We write E[|q(I)|] for the expected number of answers to
q under the above probability space.
Theorem 3.12 (Lower Bound in Expectation). Fix cardinalities m, and consider any deterministic MPC algorithm that runs in one communication round on p servers, and outputs a subset of the
answers of a query q on an input database with statistics m. Let u be any fractional edge packing of
q. If s is any server and Ls is its load, then server s reports at most
L

j uj
s
(

j uj/4)

j uj
j=1 Muj
j
· E[|q(I)|]
answers in expectation, where I is randomly chosen from the matching probability space with cardinalities m and domain size n = (maxj mj)
2, where M = (M1,..., M ) are the derived statistics.
Therefore, the p servers of the algorithm report at most
 4L
(

j uj) · L(u, M,p)

j uj
· E[|q(I)|]
answers in expectation, where L = maxs ∈[p] Ls is the maximum load of all servers. The same bounds
also hold if all relations have equal cardinalities m1 = ··· = m = m, arity aj ≥ 2, and n = m.
Before we prove Theorem 3.12, we show how it implies part (a) of Theorem 3.9.
Proof of Theorem 3.9(a). Let u∗ denote a fractional edge packing in pk(q) that achieves
L(u∗, M,p) = Llower , which must exist by Theorem 3.10. Since u∗ = (u∗
1,...,u∗
 ) is an optimal vertex of pk(q), we have
j u∗
j ≥ 1. Therefore, by Theorem 3.12, the expected number of output tuples
that the algorithm produces on input I is strictly less than the number E[|q(I)|] of tuples required
in expectation, unless L ≥ (

j u∗
j ) · L(u∗, M,p)/4 ≥ Llower /4. In the case where all relations have
equal size and arity at least 2, we obtain L ≥ Llower .
To prove Theorem 3.12 we first analyze E[|q(I)|], which is a particularly nice function of the
characteristic of the query when the relations have equal sizes.
Lemma 3.13. The expected number of answers to q is E[|q(I)|] = nk−a
j=1mj . In particular, if
n = m1 = ··· = m, then E[|q(I)|] = nc−χ (q)
, where c is the number of connected components of q.
Proof. For any relation Sj , and any tuple aj ∈ [n]
aj , the probability that Sj contains aj is
P (aj ∈ Sj) = mj/naj . Given a tuple a ∈ [n]
k of the same arity as the query answer, let aj denote
its projection on the variables in Sj . Then:
E[|q(I)|] =

a∈[n]k
P




j=1
(aj ∈ Sj)



=

a∈[n]k


j=1
P (aj ∈ Sj)
=

a∈[n]k


j=1
mjn−aj = nk−a 

j=1
mj .
The key idea for the rest of the argument is that, since the algorithm is not allowed to report
any false positives, before a server can output any tuple, it must be certain that the tuple is in the
answer, based on the messages that it has received during the single communication round. We
Journal of the ACM, Vol. 64, No. 6, Article 40. Publication date: October 2017.                       
40:22 P. Beame et al.
show how the number of bits received imposes on upper bound on the number of certain tuples
in the answer. We first show that a bound on the number of bits received about one input relation
imposes an upper bound on the expected number (over the random choice of the input database) of
the tuples that the server is certain belong to that relation. Second, we apply Friedgut’s inequality
to bound the expected number of certain tuples in the query’s answer as a function of the expected
number of certain tuples in each input relation.
Let us fix some server s ∈ [p], and let msg(I) denote the function specifying the message the
server receives on input I. Recall that, in the input-sever model, each input relation Sj is stored at
a separate input server, and therefore the message received by s consists of  separate messages
msgj = msgj (Sj), for each j = 1,...,. One should think of msgj is a bit string. Once the server s
receives msgj it “knows” that the input relation Sj is in the set {Sj | msgj (Sj) = msgj}. This justifies
the following definition: Given a message msgj , the set of tuples known by the server is as follows:
Kmsgj
(Sj) = {t ∈ [n]
aj | for all instances Sj ⊆ [n]
aj , msgj (Sj) = msgj ⇒ t ∈ Sj},
where aj is the arity of Sj . We also define Kmsg(q) to be the set of output tuples known by the
server on receiving a message msg.
Clearly, an algorithm A may output a tuple a ∈ [n]
k as answer to the query q iff, for every j,
aj ∈ Kmsgj
(Sj) for all j = 1,...,, where aj denotes the projection of a on the variables in the
atom Sj .
We will first prove an upper bound for each |Kmsgj
(Sj)| in Section 3.3.1. Then, in Section 3.3.2,
we use this bound, along with Friedgut’s inequality, to establish an upper bound for |Kmsg(q)|
and hence prove Theorem 3.12.
3.3.1 Bounding the Knowledge of Each Relation. Let us fix a server s, and an input relation Sj .
Recall that Mj = ajmj logn denotes the number of bits using a naive encoding of Sj . An algorithm
A may use fewer bits, Mj , by exploiting the fact that Sj is a uniformly chosen aj-dimensional
matching. There are precisely ( n
mj
)
aj (mj !)
aj−1 different aj-dimensional matchings of arity aj and
size mj and thus the number of bits N necessary to represent the relation is given by the entropy:
Mj = H(Sj) = aj log 
n
mj

+ (aj − 1) log(mj !). (21)
We will prove later that Mj is Ω(Mj). The following lemma provides a bound on the expected
knowledge Kmj (Sj) the server may obtain from Sj :
Lemma 3.14. Suppose that the size of Sj is mj and define
γj =

1 if mj = n
2 if mj ≤ n/2
and that the message msgj (Sj) has at most fj · Mj bits, for some fj < 1. Then E[|Kmsgj
(Sj)|] ≤
γj · fj · mj , where the expectation is taken over random choices of the matching Sj .
It says that, if the message msgj has only a fraction fj of the bits needed to encode Sj , then a
server receiving this message knows, in expectation, only a fraction γj fj of the mj tuples in Sj .
Notice that the bound holds only in expectation: A specialized encoding may choose to use very
few bits to represent a particular matching Sj ⊆ [n]
aj . When a server receives that message, then it
knows all tuples in Sj ; however, then there will be fewer bit combinations left to encode the other
matchings Sj .
Proof. The entropyH(Sj) in Equation (21) has two parts, corresponding to the two parts needed
to encode Sj : For each attribute of Sj we need to encode a subset ⊆ [n] of size mj , and for each
Journal of the ACM, Vol. 64, No. 6, Article 40. Publication date: October 2017.   
Communication Steps for Parallel Query Processing 40:23
attribute except one we need to encode a permutation over [mj]. Fix a value msgj of the message received by the server from the input Sj , and let k = |Kmsgj
(Sj)|. Because Sj is uniformly
distributed, the conditional entropy H(Sj |msgj (Sj) = msgj) is as follows:
log |{Sj | msgj (Sj) = msgj}| ≤ aj log 
n − k
mj − k

+ (aj − 1) log((mj − k))!, (22)
since msgj fixes k tuples of Sj . We will next show that
log |{Sj | msgj (Sj) = msgj}| ≤ 
1 − k
γjmj

.Mj . (23)
In other words, we claim that the entropy has decreased by at least a 1 − k/(γjmj) factor. We show
this by proving that each of the two parts of the entropy in Equation (21) is decreased by at least
that factor. We tackle the second term first.
Since log(x) is an increasing function, it follows that mj−k
i=1 (log(i)/(mj − k)) ≤ mj
i=1 (log(i)/mj),
which is equivalent to
log((mj − k)!)
log(mj !) ≤ mj − k
mj
,
and hence log((mj − k)!) ≤ (1 − k
mj
) log(mj !).
In the case that mj = n, the first term of both Equations (21) and (22) is 0, and γj = 1, so this
suffices to prove Equation (23). It remains to prove Equation (23) when mj ≤ n/2 for which γj = 2,
and for that we show
log 
n − k
mj − k

≤

1 − k
2mj

log 
n
mj

. (24)
Since
	 n−k
mj−k


	 n
mj

 = mj · (mj − 1) ··· (mj − k + 1)
n · (n − 1) ··· (n − k + 1) ≤
mj
n
k
,
we have
log 
n − k
mj − k

≤ log 
n
mj

− k log(n/mj) =


1 − k log(n/mj)
log 	 n
mj





log 
n
mj

.
To show Equation (24), it there suffices to show that when k ≤ mj ≤ n/2, log( n
mj
) ≤ 2mj log(n/mj).
For this, we use the bound ( n
mj
) ≤ nH2 (mj/n), where H2 (x) = −x log(x) − (1 − x) log(1 − x) is the
binary entropy function. Then H2 (x) = f (x) + f (1 − x), where f (x) = −x log(x). Since f is a decreasing function on R+, f (x) ≥ f (1 − x) for x ∈ (0, 1/2] and hence since mj ≤ n/2,
log 
n
mj

≤ nH2 (mj/n) ≤ 2n f (mj/n) = 2mj log(n/mj),
which implies Equation (24) and hence Equation (23).
Finally, we use Equation (23) to prove the lemma. For that we apply the chain rule
for entropy, H(Sj, msgj (Sj)) = H(msgj (Sj)) + H(Sj |msgj (Sj)), and then use the fact that
Journal of the ACM, Vol. 64, No. 6, Article 40. Publication date: October 2017.    
40:24 P. Beame et al.
H(Sj, msgj (Sj)) = H(Sj) (since Sj completely determines msgj (Sj)) and apply the definition of
H(Sj |msgj (Sj)) together with Equation (23):
H(Sj) = H(msgj (Sj)) +

msgj
P (msgj (Sj) = msgj) · H(Sj |msgj (Sj) = msgj)
≤ fj · H(Sj) +

msgj
P (msgj (Sj) = msgj) · H(Sj |msgj (Sj) = msgj)
≤ fj · H(Sj) +

msgj
P (msgj (Sj) = msgj) ·

1 − |Kmsgj (Sj)|
γjmj


H(Sj)
= fj · H(Sj) +


1 −

msgj
P (msgj (Sj) = msgj)
|Kmsgj (Sj)|
γjmj



H(Sj)
= fj · H(Sj) +

1 − E[|Kmsgj (Sj ) (Sj)|]
γjmj


H(Sj), (25)
where the first inequality follows from the assumed upper bound on |msgj (Sj)|, the second inequality follows from Equation (23), and the last two lines follow by definition. Dividing both
sides of Equation (25) by H(Sj) since H(Sj) is not zero and rearranging, we obtain the required
statement.
3.3.2 Bounding the Knowledge of the Query. We now use Lemma 3.14 to derive an upper bound
on the number of answers to q(I) that a server s can report, which will complete the proof of
Theorem 3.12. Recall that Lemma 3.14 assumed that the message msgj (Sj) is at most a fraction
fj of the entropy of Sj . We do not know the values of fj , instead we know that the entire msg(I)
received by the servers (the concatenation of all  messages msgj (Sj)) has at most L bits. For each
relation Sj , define
fj = maxSj ⊆[n]
aj |msgj (Sj)|
Mj
.
Thus, fj is the largest fraction of bits of Sj that the server receives, over all choices of the matching
Sj . We immediately derive an upper bound on the fj ’s. We have
j=1 maxSj |msgj (Sj)| ≤ L, because
each relation Sj can be chosen independently, which implies
j=1 fjMj ≤ L.
For aj ∈ [n]
aj , let wj (aj) denote the probability that the server knows the tuple aj . In other
words wj (aj) = P (aj ∈ Kmsgj (Sj ) (Sj)), where the probability is over the random choices of Sj .
Lemma 3.15. For any relation Sj :
(a) ∀aj ∈ [n]
aj : wj (aj) ≤ mj/naj , and
(b)
aj ∈[n]
aj wj (aj) ≤ γj · fj · mj where γj is defined as in Lemma 3.14.
Proof. To show (a), notice that wj (aj) ≤ P (aj ∈ Sj) = mj/naj , while (b) follows from the fact

aj ∈[n]
aj wj (aj) = E[|Kmsgj (Sj ) (Sj)|] ≤ γj · fj · mj by Lemma 3.14.
Since the server receives a separate message for each relation Sj , from a distinct input server,
the events a1 ∈ Kmsg1 (S1),..., a ∈ Kmsg (S ) are independent, hence:
E[|Kmsg(I ) (q)|] =

a∈[n]k
P (a ∈ Kmsg(I ) (q)) =

a∈[n]k


j=1
wj (aj).
Journal of the ACM, Vol. 64, No. 6, Article 40. Publication date: October 2017.                   
Communication Steps for Parallel Query Processing 40:25
We now use Friedgut’s inequality. Recall that to apply the inequality, we need to find a fractional edge cover. Let us pick any fractional edge packing u = (u1,...,u ). Given q, defined as in
Equation (1), consider the extended query, which has a new unary atom for each variable xi :
q
(x1,..., xk ) = S1 (x¯1),..., S (x¯ ),T1 (x1),...,Tk (xk ).
For each new symbol Ti , define u
i = 1 −
j:xi ∈vars(Sj ) uj . Since u is a packing, u
i ≥ 0. Let us define
u = (u
1,...,u
k ).
Lemma 3.16. (a) The assignment (u, u
) is both a tight fractional edge packing and a tight fractional
edge cover for q
. (b)
j=1 ajuj + k
i=1 u
i = k.
Proof. (a) is straightforward, since for every variable xi we have u
i +
j : xi ∈vars(Sj ) uj = 1.
Summing up:
k =

k
i=1



u
i +

j : xi ∈vars(Sj )
uj



=

k
i=1
u
i +


j=1
ajuj,
which proves (b).
We will apply Friedgut’s inequality to the extended query q
. Set the variables w(−) used in
Friedgut’s inequality as follows:
wj (aj) =P(aj ∈ Kmsgj (Sj ) (Sj)) for Sj , tuple aj ∈ [n]
aj
w
i (α) =1 for Ti , value α ∈ [n].
Recall that, for a tuple a ∈ [n]
k we use aj ∈ [n]
aj for its projection on the variables in Sj ; with
some abuse, we write ai ∈ [n] for the projection on the variable xi . Assume first that uj > 0, for
j = 1,...,. Then:
E[|Kmsg (q)|] =

a∈[n]k


j=1
wj (aj)
=

a∈[n]k


j=1
wj (aj)

k
i=1
w
i (ai )
≤


j=1




a∈[n]
aj
wj (a)
1/uj 


uj

k
i=1




α ∈[n]
w
i (α)
1/u
i 


u
i
=


j=1




a∈[n]
aj
wj (a)
1/uj 


uj

k
i=1
nu
i .
Note that, since w
i (α) = 1 we have w
i (α)
1/u
i = 1 even if u
i = 0. Write wj (a)
1/uj =
wj (a)
1/uj−1
wj (a), and use Lemma 3.15 to obtain

a∈[n]
aj
wj (a)
1/uj ≤ (mj/naj )
1/uj−1 
a∈[n]
aj
wj (a)
≤ (mjn−aj )
1/uj−1
γj fj · mj
= γj fj · m1/uj
j · n(aj−aj /uj )
.
Journal of the ACM, Vol. 64, No. 6, Article 40. Publication date: October 2017.                       
40:26 P. Beame et al.
Plugging this in the bound, we have shown that
E[|Kmsg (q)|] ≤


j=1

γj fj · m1/uj
j · n(aj−aj /uj )
uj
·

k
i=1
nu
i
=


j=1
(γj fj)
uj ·


j=1
mj · n(

j=1 ajuj−a) · n
k
i=1 u
i
=


j=1
(γj fj)
uj ·


j=1
mj · n−a+(

j=1 ajuj+
k
i=1 u
i )
=


j=1
(γj fj)
uj ·


j=1
mj · nk−a
=


j=1
(γj fj)
uj · E[|q(I)|]. (26)
Here, the third equality follows from Lemma 3.16(b), and the last equality follows from Lemma 3.13.
If some uj = 0, then we can derive the same lower bound as follows: We can replace each uj with
uj + δ for any δ > 0 still yielding an edge cover. Then we have
j ajuj +
i u
i = k + aδ, and hence
an extra factor naδ multiplying the term n+k−a in Equation (26); however, we obtain the same
upper bound since, in the limit as δ approaches 0, this extra factor approaches 1.
Given Equation (26), the final step is to upper bound the quantity
j=1 (γj fj)
uj ; using the fact
that
j=1 fjMj ≤ L. Recall that u =
j uj , then:


j=1
(γj fj)
uj =


j=1

fjMj
uj
uj 

j=1

γjuj
Mj
uj
≤


j=1 fjMj

j uj



j uj 

j=1

γjuj
Mj
uj
≤
 L

j uj

j uj 

j=1

γjuj
Mj
uj
=


j=1
 γjL
u · Mj
uj 

j=1
(uj)
uj
≤


j=1
 γjL
u · Mj
uj
.
Here, the first inequality comes from the weighted version of the Arithmetic Mean-Geometric
Mean inequality. The last inequality holds since uj ≤ 1 for any j.
To prove Theorem 3.12 it suffices to prove that γj/Mj ≤ 4/Mj in each of the two cases. To do
so, we need a lower bound on the number of bits Mj needed to represent relation Sj .
Proposition 3.17. The number of bits Mj needed to represent Sj is given by the following:
(a) If n ≥ m2
j , then Mj ≥ Mj/2.
(b) If n = mj and aj ≥ 2, then Mj ≥ Mj/4.
Journal of the ACM, Vol. 64, No. 6, Article 40. Publication date: October 2017.                                      
Communication Steps for Parallel Query Processing 40:27
Proof. For the first item, we have:
Mj ≥ aj log 
n
mj

≥ ajmj log(n/mj) ≥ (1/2)ajmj log(n) = Mj/2.
For the second item, we have:
Mj ≥ (aj − 1) log(mj !) ≥ aj − 1
2 mj log(mj) ≥ (aj − 1)
2aj
Mj ≥ Mj/4,
where the last inequality comes from the assumption that aj ≥ 2.
Since γj = 2 in the first case and γj = 1 in the second case, we obtain that γj/Mj ≤ 4/Mj and
hence complete the proof of Theorem 3.12. (Recall that L in this subsection denotes the load of an
arbitrary server, which was denoted Li in the statement of the theorem.)
3.4 Lower Bound for Randomized Algorithms
In this section, we prove the lower bound on the load of randomized algorithms given in Theorem 3.9(b). The general idea of the argument is to apply Yao’s lemma (Yao 1977), which implies that
for any probability space I of database instances I, if every deterministic algorithm (with given
resource constraints) fails to compute q(I) with probability ≥1 − δ, taken over the random choices
of I, then there exists an instance I in the support of I on which every randomized algorithm A
(with the same resource constraints) fails with probability ≥ 1 − δ, taken over the random choices
of A.
While Theorem 3.12 does prove a bound for deterministic algorithms given a distribution on
instances, it does not directly yield a lower bound of the form we need to apply Yao’s lemma since
it is phrased in terms of the expected size of the output rather than the probability of success.
Indeed, the space I of uniformly chosen random matchings is not useful for this purpose. For
example, for a connected query with a large characteristic χ (q), E[|q(I)|] is O(1/n) and therefore
P (q(I)  ∅) is O(1/n), which means that a naive deterministic algorithm that always returns the
empty answer will fail only with a very small probably, O(1/n).
Instead, we will define an event Eα for some constant α with 0 ≤ α < 1 which, in the case that q
is connected, is the event that |q(I)| > α μ for μ = E[|q(I)|]. We will then apply Yao’s lemma using
the probability distribution I|Eα of random matchings conditioned on Eα .
First, using the second moment method, we prove for connected queries q that the event Eα
occurs with significant probability.
Lemma 3.18. Let I be a random matching database for a connected conjunctive query q, and let
μ = E[|q(I)|]. Then, for any α ∈ [0, 1) we have:
P (|q(I)| > α μ) ≥ (1 − α)
2 μ
μ + 1
.
Proof. To prove the bound, we will use a version of the second moment method, the PaleyZygmund inequality, for the random variable |q(I)|:
P (|q(I)| > α μ) ≥ (1 − α)
2 μ2
E[|q(I)|
2]
.
To bound E[|q(I)|
2], we construct a new query q that consists of q plus a copy of q with a disjoint
set of new variables; q has two connected components. For example, if q = R(x,y), S (y, z), we
define q = R(x,y), S (y, z), R(x 
,y
), S (y
, z
). Since the two copies of q in q have disjoint variables,
Journal of the ACM, Vol. 64, No. 6, Article 40. Publication date: October 2017.  
40:28 P. Beame et al.
they are independent and we have:
E[|q(I)|
2
] = E[|q
(I)|] =

a,a∈[n]k


j=1
P(aj ∈ Sj ∧ a
j ∈ Sj).
Consider two tuples a, a ∈ [n]
k . If a = a
, then
j=1 P(aj ∈ Sj ∧ a
j ∈ Sj) =
j=1 P(aj ∈ Sj). If
a  a
, but a, a agree on at least one attribute position, then
j=1 P(aj ∈ Sj ∧ a
j ∈ Sj) = 0, because
every Sj is a matching and the query is connected. If a  a and they differ on all attribute positions, then for each j, P(aj ∈ Sj ∧ a
j ∈ Sj) = P(aj ∈ Sj)P(a
j ∈ Sj), because the random matching Sj
selects the tuples aj and a
j independently. This implies:
E[|q(I)|
2
] ≤

aa∈[n]k


j=1
P (aj ∈ Sj)P (a
j ∈ Sj) +

a∈[n]k


j=1
P (aj ∈ Sj)
= (n2k − nk )

j
(mj/naj )
2 + nk 
j
mj/naj
= (1 − n−k )μ2 + μ ≤ μ2 + μ,
where the last equality follows from Lemma 3.13. Plugging in this bound yields the statement of
the lemma.
We now generalize the definition of the event Eα to arbitrary queries. If the connected components of query q are q1,...,qc , then we define Eα to be the event that for each i ∈ [c]. qi (I) > α μi
where μi = E[|qi (I)|].
Let u∗ be an edge packing that maximizes L(u, M,p), and write
θ =

4L
(

j u∗
j ) · Llower



j u∗
j
.
Theorem 3.12 implies that, for any one-round deterministic algorithm with load ≤ L, E[|A(I)|] ≤
θ · E[|q(I)|].
For a deterministic algorithm A that computes the answers to a query q over a randomized
instance I, the event that |q(I) \ A(I)| > 0 is the event that the algorithm A fails to return all the
required output tuples. The next lemma shows how we can use our upper bound on the expected
number of tuples returned to obtain a lower bound on the probability of failure. (The choice of
α = 1/3 is a simple value that nearly optimizes the bound.)
Lemma 3.19. Let I be a random matching database for a query q with connected components
q1,...,qc . Let A be a deterministic algorithm such that E[|A(I)|] ≤ θ · E[|q(I)|], for some θ ≤ 1.
Let μi = E[|qi (I)|] and let Eα denote the event that |qi (I)| > α μi for all i ∈ [c]. Then,
P (|q(I) \ A(I)| > 0 | E1/3) ≥ 1 − 9c
θ .
Proof. Define Tα = c
i=1 (α μi + 1). Note that, since |q(I)| = |q1 (I)|···|qc (I)|, if Eα is true
for I, then |q(I)| ≥ Tα . Moreover, since the distribution I is independent between components, we
have μ = c
i=1 μi . Now
P (|q(I) \ A(I)| > 0 | Eα ) = P (|A(I)| < |q(I)||Eα )
≥ P (|A(I)| < Tα | Eα ) by definitions of Tα and Eα
= 1 − P (|A(I)| ≥ Tα | Eα ).
Journal of the ACM, Vol. 64, No. 6, Article 40. Publication date: October 2017.                   
Communication Steps for Parallel Query Processing 40:29
Additionally, we have
E[|A(I)] ≥ Tα · P (|A(I)| ≥ Tα )
≥ Tα · P (|A(I)| ≥ Tα ∧ Eα )
= Tα · P (Eα )P (|A(I)| ≥ Tα | Eα ).
Combining the above two inequalities and the assumption that E[|A(I)|] ≤ θ μ, we can now write
P (|q(I) \ A(I)| > 0 | Eα ) ≥ 1 − E[|A(I)|]
Tα · P (Eα ) ≥ 1 − θ μ
Tα · P (Eα )
.
Since the properties of qi (I) for different i are independent, we can write
P (Eα ) =
c
i=1
P (|qi (I)| > α μi ) ≥
c
i=1
(1 − α)
2
μi/(μi + 1)
by applying Lemma 3.18 separately to each connected component qi . Therefore
P (|q(I) \ A(I)| > 0 | Eα ) ≥ 1 − θ μ
Tα
·
c
i=1
μi + 1
μi (1 − α)2 = 1 − θ
(1 − α)2c ·
c
i=1 (μi + 1)
Tα
= 1 − θ
(1 − α)2c ·
c
i=1
μi + 1
α μi + 1
using the definition of Tα . We now distinguish two cases for each term in the product, depending
on the value of μi :
—If μi ≥ 1/α, then since α μi ≤ α μi + 1 we also have μi+1
α μi +1 ≤ (μi + 1)/(α μi ) ≤ (1 +
1/μi )/α ≤ (1 + α)/α.
—If μi < 1/α, then α μi = 0. Thus, μi+1
α μi +1 = μi + 1 < 1/α + 1 = (1 + α)/α.
Choosing α = 1/3 (for which (1+α )
α ·(1−α )2 = 9 and is nearly minimal) we obtain that
P (|q(I) \ A(I)| > 0 | E1/3) ≥ 1 − 9c
θ
as required.
We are now ready to apply Yao’s Lemma to the distribution I|E1/3 to obtain the lower bound
for randomized algorithms.
Theorem 3.20. Let q be a conjunctive query having c connected components. Fix any vector of
cardinalities m for q and let A be any one round, randomized MPC algorithm A for q. Then for any
domain size n ≥ (maxj mj)
2, if A has maximum load L ≤ δ · Llower for some constant δ < 1/36,
where M is the set of derived statistics for q, then there exists an instance I with cardinalities m
and derived statistics M such that the randomized algorithm A fails to compute q(I) correctly with
probability ≥ 1 − (36δ/c)
c .
Proof. Let q be a conjunctive query with c connected components. By Theorem 3.12, for the
edge packing u∗ that achieves the maximum value Llower of L(u∗, M,p), we have that for any
Journal of the ACM, Vol. 64, No. 6, Article 40. Publication date: October 2017.  
40:30 P. Beame et al.
deterministic algorithm A, E[|A(I)|] ≤ θ · E[|q(I)|] for
θ =

4L
(

j u∗
j ) · Llower



j u∗
j
≤

4δ

j u∗
j



j u∗
j
by our assumption on L,
≤ (4δ/c)
c ,
because 4δ ≤ 1 and because an optimal u∗ has
j u∗
j ≥ c since q has c connected components.
Hence by Lemma 3.19, the probability of failure under distribution I|E1/3,
P (|q(I) \ A(I)| > 0 | E1/3) ≥ 1 − 9c
θ ≥ 1 − 9c (4δ/c)
c = 1 − (36δ/c)
c .
Applying Yao’s Lemma using the distribution I|E1/3 we obtain the claimed lower bound for randomized algorithms.
We now can derive part (b) of Theorem 3.9.
Proof of Theorem 3.9(b). We apply Theorem 3.20 with δ = 1/72 and see that any randomized
algorithm with load at most δ · Llower has a probability of failure at least 1 − 1/2c .
3.5 Discussion
We present here examples and applications of the theorems proved in this section. Table 2 presents
the computation of the optimal share exponents and τ ∗ for several classes of queries.
The Speedup of the HyperCube. Denote u∗ the fractional edge packing that maximizes
L(u, M,p) (18). When the number of servers increases, the load decreases at a rate of 1/p1/

j u∗
j ,
which we call the speedup of the HyperCube algorithm. We call the quantity 1/

j u∗
j the speedup
exponent. We have seen that, when all cardinalities are equal, then the speedup exponent is 1/τ ∗,
but when the cardinalities are unequal then the speedup exponent may be better.
Example 3.21. Consider the triangle query
C3 = S1 (x1, x2), S2 (x2, x3), S3 (x3, x1)
and assume that the relation sizes are M1, M2, M3. Then, pk(C3) has five vertices, and each gives a
different value for L(u, M,p) = (Mu1
1 Mu2
2 Mu3
3 /p)
1/(u1+u2+u3 )
:
u L(u, M,p)
(1/2, 1/2, 1/2) (M1M2M3)
1/3/p2/3
(1, 0, 0) M1/p
(0, 1, 0) M2/p
(0, 0, 1) M3/p
(0, 0, 0) 0
(The last row is justified by the fact that L(u, M,p) ≤ max(M1, M2, M3)/p1/(u1+u2+u3 ) → 0 when
u1 + u2 + u3 → 0.) The load of the HC algorithm is given by the largest of these quantities, in other
words, the optimal solution to the LP (17) that gives the load of the HC algorithm can be given
in closed form, as the maximum over these five expressions. To compute the speedup, suppose
M1 < M2 = M3 = M. Then there are two cases. When p ≤ M/M1, the optimal packing is (0, 1, 0)
(or (0, 0, 1)) and the load is M/p. HyperCube achieves linear speedup by computing a standard join
of S2  S3 and broadcasting the smaller relation S1; it does this by allocating shares p1 = p2 = 1,
Journal of the ACM, Vol. 64, No. 6, Article 40. Publication date: October 2017.              
Communication Steps for Parallel Query Processing 40:31
Table 2. Query Examples: Ck = Cycle Query, Lk = Linear Query, Tk = Star Query, and Bk,m = Query
with ( k
m ) Relations, Where Each Relation Contains a Distinct Set of m of the k Head Variables
Share Value Lower Bound for
Conjunctive Query Exponents τ ∗ (q) Space Exponent
Ck (x1,..., xk ) = k
j=1 Sj (xj, x(j mod k)+1) 1
k ,..., 1
k k/2 1 − 2/k
Tk (z, x1,..., xk ) = k
j=1 Sj (z, xj) 1, 0,..., 0 1 0
Lk (x0, x1,..., xk ) = k
j=1 Sj (xj−1, xj) 0, 1
k/2 , 0, 1
k/2 ,... k/2 1 − 1/k/2
Bk,m (x1,..., xk ) = 
I ⊆[k], |I |=m SI (x¯I ) 1
k ,..., 1
k k/m 1 − m/k
The share exponents presented are for the case where the relation sizes are equal.
p3 = p. When p > M/M1 then the optimal packing is (1/2, 1/2, 1/2) the load is (M1M2M3)
1/3/p2/3,
and the speedup decreases to 1/p2/3.
The following lemma sheds some light into how the HyperCube algorithm exploits unequal
cardinalities.
Lemma 3.22. Let q be a query, over a database with derived statistics M, and let u∗ =
argmaxuL(u, M,p), and L = L(u∗, M,p). Then:
(1) If for some j, Mj < L, then u∗
j = 0.
(2) Let M = maxk Mk . If for some j, Mj < M/p, then u∗
j = 0.
(3) When p increases, the speedup exponent remains constant or decreases, eventually reaching
1/τ ∗.
Proof. We prove the three items of the lemma.
(1) If we modify a fractional edge packing u by setting uj = 0, we still obtain a fractional edge
packing. We claim that the function f (uj) = L(u, M,p) is strictly decreasing in uj on (0, ∞):
The claim implies the lemma, because f (0) > f (uj) for any uj > 0. The claim follows by noticing that f (uj) = p(uj logp Mj+b)/(uj+c ) where a,b,c are positive constants, hence f is monotone on
uj ∈ (0, ∞), and f (uj) = L > Mj = f (∞), implying that it is monotonically decreasing.
(2) This follows immediately from the previous item by noticing that M/p ≤ L; to see the latter, let
k be such that Mk = M, and let u be the packing uk = 1, uj = 0 for j  k. Then M/p = L(u, M,p) ≤
L(u∗, M,p) = L.
(3) Consider two edge packings u, u
, denote u =
j uj , u =
j u
j , and assume u < u
. Let f (p) =
L(u, M,p) and д(p) = L(u
, M,p). We have f (p) = c/p1/u and д(p) = c
/p1/u
, where c,c are constants independent of p. Then f (p) < д(p) if and only if p > (c/c
)
1/(1/u−1/u
)
, since 1/u − 1/u > 0.
Thus, as p increases from 1 to ∞, initially we have f (p) < д(p), then f (p) > д(p), and the
crossover point is (c/c
)
1/(1/u−1/u
)
. Therefore, the value
j u∗
j can never decrease, proving the
claim. To see that the speedup exponent reaches 1/τ ∗, denote u∗ the optimal vertex packing
(maximizing
j uj) and let u be any edge packing s.t. u =
j uj < τ ∗. Then, when p1/u−1/τ ∗
>
(
j Mu∗
j
j )
1/τ ∗
/(
j Muj
j )
1/u , we have L(u∗, M,p) > L(u, M,p).
The first two items in the lemma say that, if M is the size of the largest relation, then the only
relations Sj that matter to the HC algorithm are those for which Mj ≥ M/p; any smaller relation
will be broadcast by the HC algorithm. The last item says that the HC algorithm can take advantage
of unequal cardinalities and achieve speedup better than 1/p1/τ ∗
, for example, by allocating fewer
Journal of the ACM, Vol. 64, No. 6, Article 40. Publication date: October 2017.         
40:32 P. Beame et al.
shares to the smaller relations, or even broadcasting them. As p increases, the speedup decreases
until it reaches 1/p1/τ ∗
.
Space Exponent. Let |I | =
j Mj denote the size of the input database. Sometimes it is convenient
to study algorithms whose maximum load per server is given as L = O(|I |/p1−ε ), where 0 ≤ ε < 1
is a constant parameter ε called the space exponent of the algorithm. The lower bound given by
Theorem 3.12 can be interpreted as a lower bound on the space exponent. To see this, consider
the special case, when all relations have equal size M1 = ··· = M = M; then the load can also be
written as L = O(M/p1−ε ), and, denoting u∗ the optimal fractional edge packing, we have
j u∗
j =
τ ∗ and L(u∗, M,p) = M/p1/τ ∗
. Table 2 shows the space exponents for several classes of queries.
Theorem 3.12 implies that any algorithm with a fixed space exponent ε will report at most as
many answers:
O

 L
L(u∗, M,p)
τ ∗


· E[|q(I)|] = O(pτ ∗[ε−(1−1/τ ∗ )]
) · E[|q(I)|]. (27)
Therefore, if the algorithm has a space exponent ε < 1 − 1/τ ∗, then, as p increases, it will return
a smaller fraction of the expected number of answers. This supports the intuition that achieving
parallelism becomes harder when p increases: an algorithm with a small space exponent may be
able to compute the query correctly when p is small, but will eventually fail, when p becomes large
enough.
Replication Rate. Given an algorithm that computes a conjunctive query q, let Ls be the load of
server s, where s = 1,...,p. The replication rate r of the algorithm, defined in Afrati et al. (2012),
is r = p
s=1 Li /|I |. In other words, the replication rate computes how many times on average each
input bit is communicated. The authors in Afrati et al. (2012) discuss the tradeoff between r and the
maximum load in the case where the number of servers is not given, but can be chosen optimally.
We show next how we can apply our lower bounds to obtain a lower bound for the tradeoff between
the replication rate and the maximum load.
Corollary 3.22. Let q be a conjunctive query with derived statistics M. Any algorithm that computes q with maximum load L, where L ≤ Mj for every Sj ,
8 must have replication rate
r ≥
cL

j Mj
maxu


j=1

Mj
L
uj
,
where u ranges over all fractional edge packings of q and c = maxu(

j uj/4)

j uj .
Proof. Let fs be the fraction of answers returned by server s, in expectation, where I is
a randomly chosen matching database with statistics M. Let u be an edge packing for q and
c(u) = (

j uj/4)

j uj ; by Theorem 3.12, fs ≤ L

j uj s
c (u)
j Muj
j
. Since we assume all answers are returned,
1 ≤

p
s=1
fs =

p
s=1
L

j uj
s
c(u)
j Muj
j
≤
L

j uj−1 p
s=1 Ls
c(u)
j Muj
j
= L

j uj−1
r|I |
c(u)
j Muj
j
,
where we used the fact that
j uj ≥ 1 for the optimal u. The claim follows by noting that |I | =
j Mj .
8If L > Mj , then we can send the whole relation to any processor without cost.
Journal of the ACM, Vol. 64, No. 6, Article 40. Publication date: October 2017.                        
Communication Steps for Parallel Query Processing 40:33
In the specific case where the relation sizes are all equal to M, the above corollary tells us that
the replication rate must be r = Ω((M/L)
τ ∗−1). Hence, the ideal case where r = o(1) is achieved
only when the maximum vertex cover number τ ∗ is equal to 1 (which happens if and only if a
variable occurs in every atom of the query).
Example 3.24. Consider again the triangle query C3 and assume that all sizes are equal to M. In
this case, the edge packing that maximizes the lower bound is (1/2, 1/2, 1/2), and τ ∗ = 3/2. Thus,
we obtain an Ω(√
M/L) bound for the replication rate for the triangle query.
4 HANDLING DATA SKEW IN ONE COMMUNICATION STEP
In this section, we discuss how to compute queries in the MPC model in the presence of skew. We
first start by presenting an example where the HC algorithm that uses the optimal shares from
Equation (17) fails to work when the data have skewed, even though it is asymptotically optimal
when the relations are of low degree.
Example 4.1. Let q(x,y, z) = S1 (x, z), S2 (y, z) be a simple join query, where both relations have
cardinality m (and size in bits M). The optimal shares are p1 = p2 = 1, and p3 = p. This allocation
of shares corresponds to a standard parallel hash-join algorithm, where both relations are hashed
on the join variable z. When the data have no skew, the maximum load is O(M/p) with high
probability.
However, if the relation has skew, the maximum load can be as large as O(M). This occurs in
the case where all tuples from S1 and S2 have the same value for variable z.
As we can see from the above example, the problem occurs when the input data contain values
with high frequency of occurrence, which we call outliers, or heavy hitters. We will consider two
different scenarios when handling data skew. In the first scenario, in Section 4.1, we assume that
the algorithm has no information about the data apart from the size of the relations.
In the second scenario, presented in Section 4.2, we assume that the algorithm knows about the
outliers in our data. All the results in this section are limited to single-round algorithms.
4.1 The HyperCube Algorithm with Skew
We answer the following question: What are the optimal shares for the HC algorithm such that
the maximum load is minimized over all possible distributions of input data? In other words, we
limit our treatment to the HyperCube algorithm, but we consider data that can heavily skewed,
as in Example 4.1. Notice that the HC algorithm is oblivious of the values that are skewed, so it
cannot be modified to handle these cases separately. Our analysis is based on the following lemma
about hashing.
Lemma 4.2. Let R(A1,...,Ar ) be a relation of arity r > 1 with m tuples. Let p1,...,pr be integers,
p = i ∈[r] pi and suppose that m ≥ (1 + δ )
r mini pi . Suppose that we hash each tuple (a1,..., ar ) to
the bin (h1 (a1),...,hr (ar )), where h1,...,hr are independent and uniformly random hash functions.
Let δ > 0. The probability that there is a bin that receives > (2 + δ )m/ mini pi tuples from R is at most
2pr · exp(−(m/ mini pi )
1/rh(δ )/(1 + δ )).
Proof. Assume without loss of generality that p1 = mini pi . Define k = a · p1, where a =
(m/p1)
1/r /(1 + δ ) = (m/ mini pi )
1/r /(1 + δ ). We partition R into r + 1 relations R1,..., Rr,C as follows: Begin with S = R. While there is an d ∈ [r] such that Sd , the projection of S on coordinate d,
has |Sd | ≥ k, for each j ∈ Sd choose one tuple t ∈ S with td = j and move t from S to Rd . Let C be
the set of tuples of R remaining in S.
Journal of the ACM, Vol. 64, No. 6, Article 40. Publication date: October 2017. 
40:34 P. Beame et al.
Observe that by definition, the projections C1,...,Cr of C satisfy Cd < k for all d ∈ [r]. Write
md = |Rd |. Since at most 1 of the ≥ k tuples added to Rd in each step had any fixed dth coordinate,
by construction, there are at most md /k tuples t in Rd such that td = j.
The event that some bin receives > (2 + δ )m/ mini pi = (2 + δ )m/p1 tuples is contained in the
union of the following r + 1 events: The r events that some bin receives > (1 + δ )md /p1 tuples
from Rd for d ∈ [r] and the event that some bin receives > m/p1 tuples from C.
Since each pd ≥ p1, we can upper bound the probability for the dth event by an analysis for
a hash function hd that uses p1 bins rather than pd bins. So, by Corollary 3.5 with p = p1 and
β = p1/k = 1/a, for each d ∈ [r] we have
P(some bin receives > (1 + δ )md /p1 tuples from Rd ) ≤ p · e−a·h(δ )
.
We upper bound the probability of the last event by the probability that for some binb more than
m/p1 tuples from C1 ×···× Cr map to b. The distribution of the number of such tuples mapping
to b is given by the product of independent binomial distributions B(k, 1/pd ) for d ∈ [r]. If each
binomial B(k, 1/pd ) is ≤ (m/p1)
1/r , then the number of tuples is at most m/p1; therefore if > m/p1
tuples map to b, then one of the binomials B(k, 1/pd ) must be > (m/p1)
1/r . This is upper bounded
by the probability that B(k, 1/p1) is > (m/p1)
1/r .
Since k/p1 = a = (m/p1)
1/r /(1 + δ ), by standard Chernoff bounds (or Corollary 3.5 with p = p1
and β = 1/a), the probability that B(k, 1/p1) is > (m/p1)
1/r is at most e−a·h(δ )
. Since there are r
coordinates and p bins,
P(some bin receives > (1 + δ )md /p1 tuples from C) ≤ pr · e−a·h(δ )
.
Adding the bounds for all events together and plugging in the value of a yields the claimed
bound.
Since h(δ )/(1 + δ ) > 1 is an unbounded function of δ, we immediately obtain the following
corollary.
Corollary 4.3. Let p1,...,pr be integers and p = i ∈[r] pi . Let R(A1,...,Ar ) be a relation
of arity r with m ≥ lnr (2pr) · mini pi tuples. Suppose that we hash each tuple (a1,..., ar ) to the
bin (h1 (a1),...,hr (ar )), where h1,...,hr are independent and uniformly random hash functions.
Then, the probability that the maximum load exceeds O(m/(mini pi )) is exponentially small in
(m/ mini pi )
1/r .
Corollary 4.4. Let p = (p1,...,pk ) be the shares of the HC algorithm. For any relations, with
high probability the maximum load per server is
O

maxj
Mj
mini:i ∈Sj pi

.
The above bound is tight: We can always construct an instance for given shares such that the
maximum load is at least as above. Indeed, for a relation Sj with i = arg mini ∈Sj pi , we can construct
an instance with a single value for any attribute other than xi and Mj values for xi . In this case,
the hashing will be across only one dimension with pi servers, and so the maximum load has to
be at least Mj/pi for the relation Sj .
Journal of the ACM, Vol. 64, No. 6, Article 40. Publication date: October 2017.  
Communication Steps for Parallel Query Processing 40:35
As in the previous section, if L denotes the maximum load per server, then we must have that
Mj/ mini ∈Sj pi ≤ L. Denoting λ = logp L and μj = logp Mj , the load is optimized by the following
LP:
minimize λ
subject to 
i ∈[k]
−ei ≥ −1
∀j ∈ [] :hj + λ ≥ μj
∀j ∈ [],i ∈ Sj :ei − hj ≥ 0
∀i ∈ [k] :ei ≥ 0, ∀j ∈ [] : hj ≥ 0 λ ≥ 0. (28)
Following the same process as in the previous section, we can obtain the dual of the above LP,
and after transformations obtain the following non-linear program with the same optimal objective
function:
maximize

j ∈[] μjuj − 1

j ∈[] uj
subject to ∀i ∈ [k] : 
j:i ∈Sj
wij ≤ 1
∀j ∈ [] :uj ≤

i ∈Sj
wij
∀j ∈ [] :uj ≥ 0
∀i ∈ [k], j ∈ [] :wij ≥ 0. (29)
4.2 Skew with Information
We discuss here the case where there is additional information known about skew in the input
database. We will present a general lower bound for arbitrary conjunctive queries and show an
algorithm that matches the bound for star queries
q = S1 (z, x1), S2 (z, x2),..., S (z, x ),
which are a generalization of the join query and for the triangle query as well. In Beame et al.
(2014), we show how our algorithmic techniques (the BinHC algorithm) can be used to compute
arbitrary conjunctive queries; however, there is a substantial gap between the upper and lower
bounds in the general case.
We first introduce some necessary notation for the star query. For each relation Sj with |Sj | =
mj , and each assignment h ∈ [n] for a variable z, we define its frequency as mj (h) = |σz=h (Sj)|.
We will be interested in assignments that have high frequency, which we call heavy hitters. To
design algorithms that take skew into account, we will assume that every input server knows the
assignments with frequency ≥ mj/p for every relation Sj , along with their frequency. Because each
relation can contain at most p heavy hitters, the total number over all relations will be O(p). Since
we are considering cases where the number of servers is much smaller than the data, an O(p)
amount of information can be easily stored in the input server.
To prove the lower bound, we will make a stronger assumption about the information available
to the input servers. Given a conjunctive query q, fix a set of variables x and let d = |x|. Also, let
xj = x ∩ vars(Sj) for every relation Sj , and dj = |xj |. A statistics of type x, or x-statistics is a vector
m = (m1,...,m ), where mj is a function mj : [n]
xj → N. We associate with m the function m :
[n]
x → (N)
, where m(h) = (m1 (h1),...,m (h )), and hj denotes the restriction of the tuple h to
Journal of the ACM, Vol. 64, No. 6, Article 40. Publication date: October 2017.                
40:36 P. Beame et al.
the variables in xj . We say that an instance of Sj satisfies the statistics if for any tuple hj ∈ [n]
xj , its
frequency is preciselymj (hj). When x = ∅, thenm simply consists of  numbers, each representing
the cardinality of a relation; thus, a x-statistics generalizes the cardinality statistics. Recall that we
use upper case M = (M1,..., M ) to denote the corresponding derived statistics expressed in bits,
that is, Mj (h) = ajmj (h) log(n).
In the particular case of the star query, we will assume that the input servers know the
z-statistics; in other words, for every assignment h ∈ [n] of variable z, we know that its frequency in relation Sj (z, xj) is precisely mj (h). Observe that in this case the cardinality of Sj is
|Sj | =
h∈[n]mj (h).
4.2.1 Algorithm for Star Queries. The algorithm uses the same principle popular in virtually
all parallel join implementations to date: Identify the heavy hitters and treat them differently
when distributing the data. However, the analysis and optimality proof is new, to the best of our
knowledge.
Let H denote the set of heavy hitters in all relations. Note that |H| ≤ p. The algorithm will deal
with the tuples that have no heavy hitter values (light tuples) by running the vanilla HC algorithm,
which runs with shares pz = p and pxj = 1 for every j = 1,...,. For this case, the load analysis
of Corollary 3.3 will give us a maximum load ofO˜ (maxj Mj/p) with high probability, whereO˜ hides
a polylogarithmic factor that depends on p. For heavy hitters, we will have to adapt its function as
follows.
To compute q, the algorithm must compute for each h ∈ H the subquery
q[h/z] = S1 (h, x1),... Sk (h, xk ),
which is equivalent to computing the Cartesian product qz = S
1 (x1),..., S
k (xk ), where S
1 (x1) =
S1 (h, x1) and S
2 (x2) = S2 (h, x2), and each relation S
j has cardinality mj (h) (and size in bits Mj (h)).
We call qz the residual query. The algorithm will allocate ph servers to compute q[h/z] for each
h ∈ H, such that
h∈H ph = Θ(p). Since the unary relations have no skew, they will be of low
degree and thus the maximum load Lh for each h is given by
Lh = O

max
u∈pk (qz )
L(u, M(h),ph )

.
For the star query, we have pk(qz ) = {0, 1} \ (0, 0,..., 0). At this point, since ph is not specified, it
is not clear which edge packing in pk(qz ) maximizes the above quantity for each h. To overcome
this problem, we further refine the assignment of servers to heavy hitters: We allocate ph,u servers
to each h and each u ∈ pk(qz ), such that ph =
u ph,u. Now, for a given u ∈ pk(qz ), we can evenly
distribute the load among the heavy hitters by allocating servers proportionally to the “heaviness”
of executing the residual query. In other words we want ph,u ∼ j Mj (h)
uj for every h ∈ H. Hence,
we will choose:
ph,u =

p ·
j Mj (h)
uj )

h∈H
j Mj (h)
uj

.
Since x ≤ x + 1, and |H| ≤ p, we can compute that the total number of servers we
need is at most ( + 1) · |pk(qz )| · p, which is Θ(p). Since we have that L(u, M(h),ph ) =
(
j Mj (h)
uj /ph )
1/

j uj , the maximum load Lh for every h ∈ H will be
Lh = O

max
u∈pk (qz )

h∈H
j Mj (h)
uj
p
1/(

j uj )


.
Journal of the ACM, Vol. 64, No. 6, Article 40. Publication date: October 2017.                     
Communication Steps for Parallel Query Processing 40:37
Plugging in the values of pk(qz ), we obtain the following upper bound on the algorithm for the
heavy hitter case:
O

max
I ⊆[]

h∈H
j ∈I Mj (h)
p
1/ |I |


. (30)
Observe that the terms depend on the frequencies of the heavy hitters and can be much larger
than the bound O(maxj Mj/p) we obtain from the light hitter case. In the extreme, a single heavy
hitter h with mj (h) = mj for j = 1,..., will demand maximum load equal to O((j Mj/p)
1/ ).
4.2.2 Algorithm for Triangle Query. We show here how to compute the triangle query C3 =
R(x,y), S (y, z),T (z, x) when all relation sizes are equal to m (and have M bits). As with the star
query, the algorithm will deal with the tuples that have no heavy hitter values, that is, the frequency is less than m/p1/3, by running the vanilla HC algorithm. For this case, the load analysis
of Corollary 3.3 will give us a maximum load of O˜ (M/p2/3) with high probability.
Next, we show how to handle the heavy hitters. We distinguish two cases.
Case 1. In this case, we handle the tuples that have values with frequency ≥ m/p in at least two
variables. Observe that we did not set the heaviness threshold to m/p1/3 for reasons that we will
explain in the next case.
Without loss of generality, suppose that both x,y are heavy in at least one of the two relations
they belong to. The observation is that there at most p such heavy values for each variable, and
hence we can send all tuples of R(x,y) with both x,y heavy (at most p2) to all servers. Then, we
essentially have to compute the query S
(y, z),T 
(z, x), where x and y can take only p values. We
can do this by computing the join on z; since the frequency of z will be at most p for each relation
(because every value of z can relate to at most p values of x or y), the maximum load from the join
computation will be O(M/p).
Case 2. In this case, we handle the remaining output: This includes the tuples where one variable
has frequency ≥ m/p1/3, and the other variables are light, that is, have frequency ≤ m/p. Without
loss of generality, assume that we want to compute the query q for the values of x that are heavy
in either R or T . Observe that there are at most 2p1/3 of such heavy hitters. If Hx denotes the set
of heavy hitter values for variable x, then the residual query q[h/x] for each h ∈ H is
q[h/x] = R(h,y), S (y, z),T (z,h),
which is equivalent to computing the query qx = R
(y), S (y, z),T 
(z) with cardinalities
mR (h),m,mT (h), respectively. As before, we allocate ph servers to compute q[h/x] for each h ∈ H.
If there is no skew, then the maximum load Lh is given by the following formula:
Lh = O


max


M
ph
,

MR (h)MT (h)
ph






.
The above formula holds, because there only two edge packings that will give the optimal load:
uS = 1,uR = uT  = 0 and uS = 0,uR = uT  = 1. Notice now that the only cause of skew for qx may
be that y or z are heavy in S (y, z). However, we assumed that the frequencies for both y, z are
≤ m/p, so there will be no skew (this is why we set the heaviness threshold for Case 1 to m/p
instead of m/p1/3).
Journal of the ACM, Vol. 64, No. 6, Article 40. Publication date: October 2017.            
40:38 P. Beame et al.
We can now set ph = ph,1 + ph,2 (for each of the quantities in the max expression), and choose
the allocated servers similarly to how we chose for the star queries:
ph,1 =

p ·
MS (h)
M

ph,2 =

p · MR (h)MT (h)

h∈Hx MR (h)MT (h)

.
We now get a load of
L = O


max


M
p ,

h MR (h)MT (h)
p






.
Summing up all the cases, we obtain that the load of the 1-round algorithm for computing triangles
is
L = O˜


max


M
p2/3 ,

h MR (h)MT (h)
p ,

h MR (h)MS (h)
p ,

h MS (h)MT (h)
p






.
In the next section, we show that this bound is tight for 1-round algorithms.
4.2.3 Lower Bound. The lower bound we present here holds for any conjunctive query, and
generalizes the lower bound in Theorem 3.12, which was over databases with simple cardinality
statistics m = (m1,...,m ), to databases with a fixed degree sequence. If the degree sequence is
skewed, then the new bounds can be stronger, proving that skew in the input data makes query
evaluation harder.
Let us fix statistics m of type x. We define qx as the residual query, obtained by removing all
variables x, and decreasing the arities of Sj as necessary (the new arity of relation Sj is aj − dj).
Clearly, every fractional edge packing of q is also a fractional edge packing of qx, but the converse
does not hold in general. If u is a fractional edge packing of qx, then we say that u saturates a
variable xi ∈ x, if
j:xi ∈vars (Sj ) uj ≥ 1; we say that u saturates x if it saturates all variables in x.
For a given x and u that saturates x, define
Lx(u, M,p) =

h∈[n]x
j Mj (hj)
uj )
p
1/

j uj
, (31)
where M is the derived statistics of type x based on m.
Theorem 4.5. Fix statistics m of type x such that aj > dj for every relation Sj . Consider any deterministic MPC algorithm that runs in one communication round on p servers and has maximum load
L in bits. Then, for any fractional edge packing u of q that saturates x, we must have
L ≥ min
j
(aj − dj)
4aj
· Lx(u, M,p),
where M are the derived statistics of type x.
Note that, when x = ∅ then Lx(u, M,p) = L(u, M,p), as defined in Equation (18). However, our
theorem does not imply Theorem 3.12, since it does not give a lower bound on the expected size
of the algorithm output as a fraction of the expected output size.
Proof. For h ∈ [n]
x and aj ∈ Sj , we write aj  h to denote that the tuple aj from Sj matches
with h at their common variables xj and denote (Sj)h the subset of tuples aj that match h: (Sj)h =
{aj | aj ∈ Sj , aj  h}. Let Ih denote the restriction of I to h, in other words Ih = ((S1)h,..., (S )h).
We pick the domain n such that n = (maxj{mj})
2 and construct a probability space for instances
I defined by the derived x-statistics M as follows. For a fixed tuple h ∈ [n]
x, the restriction Ih is
Journal of the ACM, Vol. 64, No. 6, Article 40. Publication date: October 2017.                   
Communication Steps for Parallel Query Processing 40:39
a uniformly chosen instance over all matching databases with cardinality vector M(h), which is
precisely the probability space that we used in the proof of Theorem 3.10. In particular, for every
aj ∈ [n]
xj , the probability that Sj contains aj is P (aj ∈ Sj) = mj (hj)/naj−dj . Lemma 3.13 immediately gives
E[|q(Ih)|] = nk−d 

j=1
mj (hj)
naj−dj
. (32)
Let us fix some server and let msg(I) be the message the server receives on input I. As in the
previous section, let Kmsgj (Sj) denote the set of tuples from relation Sj known by the server. Let
wj (aj) = P (aj ∈ Kmsgj (Sj ) (Sj)), where the probability is over the random choices of Sj . This is upper
bounded by P (aj ∈ Sj):
wj (aj) ≤ mj (hj)/naj−dj , if aj  h. (33)
We derive a second upper bound by exploiting the fact that the server receives a limited number
of bits, in analogy with Lemma 3.14:
Lemma 4.6. Let Sj a relation with aj > dj . Suppose that the size of Sj is mj ≤ n/2 (or mj = n) and
that the message msgj (Sj) has at most L bits. Then, we have E[|Kmsgj (Sj)|] ≤ 4L
(aj−dj ) log(n) .
Observe that in the case where aj = dj for some relation Sj , the x-statistics fix all the tuples of
the instance for this particular relation, and hence E[|Kmsgj (Sj)|] = mj .
Proof. We can express the entropy H(Sj) as follows:
H(Sj) = H(msgj (Sj)) +

msgj
P (msgj (Sj) = msgj) · H(Sj | msgj (Sj) = msgj)
≤ L +

msgj
P (msgj (Sj) = msgj) · H(Sj | msgj (Sj) = msgj). (34)
For every h ∈ [n]
x, let Kmsgj ((Sj)h) denote the known tuples that belong in the restriction of Sj
to h. Following the proof of Lemma 3.14, and denoting by Mj (hj) the number of bits necessary to
represent (Sj)h, we have
H(Sj | msgj (Sj) = msgj) ≤

h∈[n]x


1 − |Kmsgj ((Sj)h)|
2mj (hj)


Mj (hj)
= H(Sj) −

h∈[n]x
|Kmsgj ((Sj)h)|
2mj (hj) Mj (hj)
≤ H(Sj) −

h∈[n]x
|Kmsgj ((Sj)h)|
2mj (hj) mj (hj)
aj − dj
2 log(n)
= H(Sj) − (1/4) · |Kmsgj (Sj)|(aj − dj) log(n),
where the last inequality comes from Proposition 3.17. Plugging this in Equation (34), and solving
for E[|Km (Sj)|],
E[|Kmsgj (Sj)|] ≤
4L
(aj − dj) log(n)
.
This concludes our proof.
Journal of the ACM, Vol. 64, No. 6, Article 40. Publication date: October 2017.    
40:40 P. Beame et al.
Let qx be the residual query, and recall that u is a fractional edge packing that saturates x.
Define the extended query qx
 to consist of qx, where we add a new atom S
i (xi ) for every variable
xi ∈ vars(qx). Define u
i = 1 −
j:i ∈Sj uj . In other words, u
i is defined to be the slack at the variable
xi of the packing u. The new edge packing (u, u
) for the extended query q
x has no more slack, and
hence it is both a tight fractional edge packing and a tight fractional edge cover for qx. By adding
all equalities of the tight packing we obtain the following:


j=1
(aj − dj)uj +
k
−d
i=1
u
i = k − d. (35)
We next compute how many output tuples fromq(Ih) will be known in expectation by the server.
Note that q(Ih) = qx(Ih), and thus,
E[|Kmsg (q(Ih))|] = E[|Kmsg (qx(Ih))|]
=

ah


j=1
wj (aj)
=

ah


j=1
wj (aj)
k
−d
i=1
w
i (ai )
≤
k
−d
i=1
nu
i ·


j=1




aj h
wj (aj)
1/uj 


uj
.
By writingwj (aj)
1/uj = wj (aj)
1/uj−1
wj (aj) for aj  h, we can bound the sum in the above quantity
as follows using Equation (33),

aj h
wj (aj)
1/uj ≤

mj (hj)
naj−dj
1/uj−1 
aj h
wj (aj) = (mj (hj)ndj−aj )
1/uj−1
Lj (h),
where Lj (h) =
aj hwj (aj). Notice that for every relation Sj , we have
hj ∈[n]
xj Lj (hj) =
aj ∈[n]
aj wj (aj). We can now write
E[|Kmsg (q(Ih))|] ≤ n
k−d i=1 u
i


j=1
	
Lj (h)mj (hj)
1/uj−1
n(dj−aj )(1/uj−1)

uj
=


j=1
Lj (h)
uj ·


j=1
mj (hj)
−uj · E[|q(Ih)|], (36)
where the equality follows from Equation (35) and Equation (32).
Summing over all p servers, we obtain that the expected number of answers that can be output
for q(Ih) is at most p · E[|Kmsg (q(Ih))|]. If for some h ∈ [n]
x this number is not at least E[|q(Ih)|],
then the algorithm will fail to compute q(I). Consequently, for every h, we must have that
Journal of the ACM, Vol. 64, No. 6, Article 40. Publication date: October 2017.              
Communication Steps for Parallel Query Processing 40:41

j=1 Lj (hj)
uj ≥ (1/p) ·

j=1mj (hj)
uj . Summing the inequalities for every h ∈ [n]
x:
1
p ·

h∈[n]x


j=1
mj (hj)
uj ≤

h∈[n]x


j=1
Lj (hj)
uj
≤


j=1




hj ∈[n]
xj
Lj (hj)



uj
by Friedgut’s inequality
≤


j=1
 4L
(aj − dj) log(n)
uj
by Lemma 4.6.
Solving for L and using the fact that Mj = ajmj log(n), we obtain that for any edge packing u that
saturates x,
L ≥

min
j
aj − dj
4aj

·

h∈[n]x
j Mj (hj)
uj
p
1/

j uj
,
which concludes the proof.
To see how Theorem 4.5 applies to the star query, we assume that the input servers know zstatistics M; in other words, for every assignment h ∈ [n] of variable z, we know that its frequency
in relation Sj is mj (h). Then, for any edge packing u that saturates z, we obtain a lower bound of
L ≥ (1/8) ·

h∈[n]
j Mj (h)
uj
p
1/

j uj
.
Observe that the set of edge packings that saturate z and maximize the above quantity is {0, 1} \
(0,..., 0). Hence, we obtain a lower bound
L ≥ (1/8) · max
I ⊆[]

h∈[n]
j ∈I Mj (h)
p
1/ |I |
.
It is not hard to see that our upper and lower bounds for the star query are tight up to constant
factors: First, note that the set of heavy hitters H is a subset of [n]; hence, the upper bound (30)
for the heavy hitter case matches the lower bound. The upper bound O(maxj Mj/p) for the light
hitter case also matches the lower bound by setting I to be a singleton set, that is, |I | = 1.
By similarly varying over the choices of subsets of vertices saturated by u, one obtains a lower
bound matching the upper bound for triangle queries in Section 4.2.2.
5 MULTIPLE COMMUNICATION STEPS
In this section, we discuss the computation of queries in the MPC model in the case of multiple
steps. We will establish both upper and lower bounds on the number of rounds needed to compute
a query q.
To prove our results, we restrict both the structure of the input and the type of computation
in the MPC model. In particular, our multi-round algorithms will process only queries where the
relations are of equal size and the data have no skew. Additionally, our lower bounds are proven
for a restricted version of the MPC model, called the the tuple-based MPC model, which limits the
way communication is performed.
Journal of the ACM, Vol. 64, No. 6, Article 40. Publication date: October 2017.                     
40:42 P. Beame et al.
5.1 An Algorithm for Multiple Rounds
In Section 3, we showed that in the case where all relations have size equal to M and are matching
databases (i.e., the degree of any value is exactly one), we can compute a conjunctive query q in
one round with maximum load
L = O(M/p1/τ ∗ (q)
),
where τ ∗ (q) denotes the fractional vertex covering number of q. Hence, for any ε ≥ 0, a conjunctive
query q with τ ∗ (q) ≤ 1/(1 − ε) can be computed in one round in the MPC model with load L =
O(M/p1−ε ); recall from Section 3.5 that we call the parameter ε the space exponent.
We define now the class of queries Γr
ε using induction on r. For r = 1, we define
Γ1
ε = {q | τ ∗ (q) ≤ 1/(1 − ε)}.
For r > 1, we define Γr
ε to be the set of all conjunctive queries q constructed as follows. Let
q1,...,qm ∈ Γr−1 ε be m queries, and let q0 ∈ Γ1
ε be a query over a different vocabulary V1,...,Vm,
such that |vars(qj)| = arity(Vj) for all j ∈ [m]. Then, the query q = q0[q1/V1,...,qm/Vm], obtained
by substituting each view Vj in q0 with its definition qj , is in Γr
ε . In other words, Γr
ε consists of
queries that have a query plan of depth r, where each operator is a query computable in one step
with maximum load O(M/p1−ε ).
Proposition 5.1. Every conjunctive query q ∈ Γr
ε with input a matching database where each
relation has size M can be computed by an MPC algorithm in r rounds with maximum load L =
O(M/p1−ε ).
Proof. The proof follows by induction on the number of roundsr. For the base case, by definition a query in Γ1
ε can be computed in one round with load L = O(M/p1−ε ). For the inductive step,
consider a query q ∈ Γr
ε . Then, it can be expressed as q = q0[q1/V1,...,qm/Vm]. By the inductive
hypothesis, each viewVj can be computed in r − 1 rounds with loadO(M/p1−ε ). Additionally, since
the database is matching, the resulting size of each view will be at most M. Since q0 ∈ Γ1
ε , we can
compute the final result from q by an additional round with load O(M/p1−ε ).
We next present two examples that provide some intuition on the structure of the queries in the
class Γr
ε .
Example 5.2. Consider the query Lk in Table 2 with k = 16; we can construct a query plan of
depth r = 2 and load L = O(M/p1/2) (thus with space exponent ε = 1/2). For ease of notation, let
us write xi:j = xi, xi+1,..., xj . The first step computes in parallel four queries,
V1 :q1 = S1 (x0:1), S2 (x1:2), S3 (x2:3), S4 (x3:4),
V2 :q2 = S5 (x4:5), S6 (x5:6), S7 (x6:7), S8 (x7:8),
V3 :q3 = S9 (x8:9), S10 (x9:10), S11 (x10:11), S12 (x11:12),
V4 :q4 = S13 (x12:13), S14 (x13:14), S15 (x14:15), S16 (x15:16).
Each qi is isomorphic to the query L4, and therefore τ ∗ (q1) = ··· = τ ∗ (q4) = 2. Hence, qi ∈ Γ1
1/2
and each qi can be computed in one round with load L = O(M/p1/2). Now we can express L16 by
using the views V1,...,V4 as the query
q0 = V1 (x0:4),V2 (x4:8),V3 (x8:12),V4 (x12:16).
It is easy to see that τ ∗ (q0) = 2 as well, and hence q0 ∈ Γ1
1/2. Since L16 = q0[q1/V1,...,q4/V4], and
q0,...,q4 ∈ Γ1
1/2, we conclude that L16 ∈ Γ2
1/2
We can generalize the above approach for any Lk . For any ε ≥ 0, let kε be the largest integer
such that Lkε ∈ Γ1
ε . In other words, τ ∗ (Lkε ) ≤ 1/(1 − ε), and so we choose kε = 21/(1 − ε). Then,
Journal of the ACM, Vol. 64, No. 6, Article 40. Publication date: October 2017. 
Communication Steps for Parallel Query Processing 40:43
for any k ≥ kε , Lk can be computed using Lkε as a building block at each round: The plan will have
a depth of logkε (k) and will achieve a load of L = O(M/p1−ε ).
Example 5.3. Consider the query SPk = k
i=1 Ri (z, xi ), Si (xi,yi ). Since τ ∗ (SPk ) = k, the one
round algorithm can achieve a load of O(M/p1/k ).
When we consider multiple rounds, we can show that for every k, SPk ∈ Γ2
0 ; hence there exists
a query plan with two rounds and load O(M/p). To achieve this, we first compute the views Vi :
qi = Ri (z, xi ), Si (xi,yi ) for every i = 1,..., k. Each query qi is in Γ1
0 . Now we can write SPk as
q0[q1/V1,...,qk /Vk ], where q0 = k
i=1 Vi (z, xi,yi ). The query q0 has τ ∗ (q0) = 1, hence q0 ∈ Γ1
0 .
We should note here that there is no connection between the above query plans and plans
formed by query decomposition methods (e.g., General Hypertree Decompositions (GHDs) (Gottlob et al. 2009)). First, the notion of width in a decomposition is different from the space exponent
ε. Second, since we want to construct plans that can be executed within few rounds, minimizing
the depth of a query plan is an additional objective to minimizing the space exponent.
We next present an upper bound on the number of rounds needed to compute any query if we
want to achieve a given load L = O(M/p1−ε ); in other words, we ask what is the minimum number
of rounds for which we can achieve a space exponent ε.
Let rad(q) = minu maxv d(u,v) denote the radius of a query q, where d(u,v) denotes the distance between two nodes in the hypergraph of q. For example, rad(Lk ) = k/2 and rad(Ck ) =
k/2.
Lemma 5.4. Fix ε ≥ 0, let kε = 21/(1 − ε), and let q be any connected query. Define
r(q) =

logkε (rad(q)) + 1 if q is treelike,
logkε (rad(q)) + 2 otherwise.
Then, q can be computed in r(q) rounds on any matching database input with relations of size M with
maximum load L = O(M/p1−ε ).
Proof. By definition of rad(q), there exists some node v ∈ vars(q), such that the maximum distance of v to any other node in the hypergraph of q is at most rad(q). If q is treelike, then we
can decompose q into a set of at most |atoms(q)|
rad(q) (possibly overlapping) paths P of length
≤ rad(q), each having v as one endpoint. Since it is essentially isomorphic to L, a path of length
 ≤ rad(q) can be computed in at most logkε (rad(q)) rounds using the query plan from Proposition 5.1 together with repeated use of the one-round HyperCube algorithm for paths of length
kε . Moreover, all the paths in P can be computed in parallel, because |P| is a constant depending
only on q. Since every path will contain variable v, we can compute the join of all the paths in one
final round with load O(M/p).
The only difference for general connected queries is that q may also contain atoms that join
vertices at distance rad(q) from v that are not on any of the paths of length rad(q) from v: These
can be covered using paths of length rad(q) + 1 from v. To get the final formula, we apply the
equality loga (b + 1) = loga (b) + 1, which holds for positive integers a,b.
As an application of the above lemma, Table 3 shows the number of rounds required by different
types of queries.
5.2 Lower Bounds for Multiple Rounds
To show lower bounds for the case of multiple rounds, we will need to restrict the communication
in the MPC model; to do this, we define a restriction of the MPC model that we call the tuple-based
MPC model.
Journal of the ACM, Vol. 64, No. 6, Article 40. Publication date: October 2017.   
40:44 P. Beame et al.
Table 3. The Tradeoff between Space and Communication
Rounds for Several Queries
ε r r = f (ε)
q space exponent rounds to achieve rounds/space
query for 1 round load O(M/p) tradeoff
Ck 1 − 2/k log k ∼ log k
log(2/(1−ε ))
Lk 1 − 1
k/2 log k ∼ log k
log(2/(1−ε ))
Tk 0 1 NA
SPk 1 − 1/k 2 NA
5.2.1 Tuple-Based MPC. In the general MPC model, we did not have any restrictions on the
messages sent between servers at any round. In the tuple-based MPC model, we will impose some
structure on how we can communicate messages.
Let I be the input database instance, q be the query we want to compute, and A an algorithm. For a server s ∈ [p], we denote by msg1
j→s (A, I) the message sent during round 1 by the
input server for Sj to the server s, and by msgk
s→s (A, I) the message sent to server s from
server s at round k ≥ 2. Let msg1
s (A, I) = (msg1
1→s (A, I),..., msg1
→s (A, I)) and msgk
s (A, I) =
(msgk
1→s (A, I),..., msgk
p→s (A, I)) for any round k ≥ 2.
Further, we define msg≤k
s (A,i) to be the vector of messages received by servers during the first
k rounds, and msg≤k (A,i) = (msg≤k
1 (A,i),..., msg≤k
p (A,i)).
Define a join tuple to be any tuple inq
(I), where q is any connected subquery of q. An algorithm
A in the tuple-based MPC model has the following two restrictions on communication during
rounds k ≥ 2, for every server s
— the message msgk
s→s (A, I) is a set of join tuples.
— for every join tuple t, the servers decides whether to include t in msgk
s→s (A, I) based only
on the parameters t,s,s
,r, and the messages msg1
j→s (A, I) for all j such that t contains a
tuple in Sj .
The restricted model still allows unrestricted communication during the first round; the information msg1
s (A, I) received by servers in the first round is available throughout the computation.
However, during the following rounds, server s can only send messages consisting of join tuples,
and, moreover, the destination of these join tuples can depend only on the tuple itself and on
msg1
s (A, I).
The restriction of communication to join tuples (except for the first round during which arbitrary, for example, statistical, information can be sent) is natural and the tuple-based MPC model
captures a wide variety of algorithms for multiway join, including the most natural MapReduce
algorithms. Since the servers can perform arbitrary inferences based on the messages that they
receive, even a limitation to messages that are join tuples starting in the second round, without a
restriction on how they are routed, would still essentially have been equivalent to the fully general
MPC model. For example, any server wishing to send a sequence of bits to another server can encode the bits by sending a sequence of tuples that the two exchanged in previous rounds, or (with
slight loss in efficiency) using the understanding that the tuples themselves are not important,
but some arbitrary fixed Boolean function of those tuples is the true message being communicated. This explains the need for the condition on routing tuples that the tuple-based MPC model
imposes.
Journal of the ACM, Vol. 64, No. 6, Article 40. Publication date: October 2017. 
Communication Steps for Parallel Query Processing 40:45
5.2.2 A Lower Bound. We present here a general lower bound for connected conjunctive
queries in the tuple-based MPC model.
We first introduce a combinatorial object associated with every query q, called the (ε,r)-plan,
which is central to the construction of the multi-round lower bound. We next define this notion
and also discuss how we can construct such plans for various classes of queries.
Given a query q and a set M ⊆ atoms(q), recall that q/M is the query that results from contracting the edges M in the hypergraph of q. Also, we define M = atoms(q)\M. Recall that
Γ1
ε = {q | τ ∗ (q) ≤ 1/(1 − ε)}.
Definition 5.5. Let q be a connected conjunctive query. A set M ⊆ atoms(q) is ε-good for q if it
satisfies the following two properties:
(1) Every connected subquery of q that is in Γ1
ε contains at most one atom in M.
(2) χ (M) = 0 (and thus χ (q/M) = χ (q) by Lemma 2.1).
For ε ∈ [0, 1) and integer r ≥ 0, an (ε,r)-plan M is a sequence M1,..., Mr , with
M0 = atoms(q) ⊃ M1 ⊃ ··· Mr such that (a) for j = 0,...,r − 1, Mj+1 is ε-good for q/Mj ,
and (b) q/Mr  Γ1
ε .
We provide some intuition about the above definition with the next two lemmas, which show
how we can obtain such plans for the queries Lk and Ck , respectively.
Lemma 5.6. The query Lk admits an (ε, logkε (k) − 2)-plan for any integer k > kε = 21/(1 −
ε).
Proof. We will prove using induction in the number of roundsr that for every integerr ≥ 0, if
k ≥ kr+1 ε + 1, then Lk admits an (ε,r)-plan. This proves the lemma, because then for a given k the
smallest integer r we can choose for the plan is r = logkε (k − 1) − 1 = logkε (k) − 2.
For the base case r = 0, we need to show that if k ≥ kε + 1, then Lk admits an (ε, 0)-plan. Since
r = 0, it suffices to show that condition (b) holds, that is, q/M0  Γ1
ε . But M0 = atoms(q), hence
Lk /M0 = Lk . In other words, for r = 0 a query admits an (ε, 0)-plan if and only if it is not possible
to compute the query in one round with space exponent ε. Since Lk  Γ1
ε , the claim follows.
For the induction step, let k0 ≥ kr+1 ε + 1; then from the inductive hypothesis we have that for
every k ≥ kr
ε + 1 the query Lk has an (ε,r − 1)-plan. Define M to be the set of atoms where we
include every kε th atom Lk0 , starting with S1; in other words, M = {S1, Skε +1, S2kε+1,...}. Observe
now that after the contraction we have:
Lk0 /M = S1 (x0, x1), Skε+1 (x1, xkε +1), S2kε+1 (xkε+1, x2kε+1),...
Observe that the query Lk0 /M is isomorphic to Lk0/kε . Since k0/kε ≥kr
ε + 1/kε  = kr
ε + 1, from
the inductive hypothesis the query Lk0 /M admits an (ε,r − 1)-plan. By definition, this implies a
sequence M1,..., Mr−1; we will show that the extended sequence M, M1,..., Mr−1 is an (ε,r)-plan
for Lk0 . To show this, it is sufficient to prove that M is ε-good for Lk0 .
Indeed, we have χ (Lk0 /M) = χ (Lk0/kε ) = χ (Lk0 ) and thus property (2) is satisfied. Additionally, recall that Γ1
ε consists of queries for which τ ∗ (q) ≤ 1/(1 − ε); thus
the connected subqueries of Lk0 that are in Γ1
ε are precisely queries of the form
Sj (xj−1, xj), Sj+1 (xj, xj+1),..., Sj+k−1 (xj+k−2, xj+k−1), where k ≤ kε . By choosing M to contain every kε th atom, no such subquery in Γ1
ε will contain more than one atom from M and thus
property (1) is satisfied as well. Intuitively, this property guarantees that no algorithm can find
which tuples join from two different atoms of M with space exponent ε.
Lemma 5.7. The query Ck admits an (ε, logkε (k/(mε + 1)))-plan, for every integer k > mε =
2/(1 − ε).
Journal of the ACM, Vol. 64, No. 6, Article 40. Publication date: October 2017.    
40:46 P. Beame et al.
Proof. The proof is similar to the proof for the query Lk . Indeed, observe that any set M of
atoms that are (at least) kε positions apart along the cycle Ck is a ε-good set for Ck . Further, the
contracted query Ck /M is isomorphic to Ck/kε . The only difference is that the base case forr = 0
is that k ≥ mε + 1. Thus, the inductive step is that for every integer r ≥ 0, if k ≥ kr
ε (mε + 1) then
Ck admits an (ε,r)-plan.
The above examples of queries show how we can construct (ε,r)-plans. We next present the
main theorem of this section, which tells us how we can use such plans to obtain lower bounds on
the number of communication rounds needed to compute a conjunctive query.
Theorem 5.8 (Lower Bound for Multiple Rounds). Let q be a conjunctive query that admits
an (ε,r)-plan. For every randomized algorithm in the tuple-based MPC model that computes q in r + 1
rounds and with load L ≤ cM/p1−ε for a sufficiently small constant c, there exists an instance I with
relations of size M where the algorithm fails to compute q with probability Ω(1).
The constant c in the above theorem depends on the query q and the parameter ε. To state the
precise expression the constant c, we need some additional definitions.
Definition 5.9. Let q be a conjunctive query and M be an (ε,r)-plan for q. We define τ ∗ (M) to
be the minimum of τ ∗ (q/Mr ) and τ ∗ (q
), where q ranges over all connected subqueries of q/Mj−1,
j ∈ [r], such that q  Γ1
ε .
Proposition 5.10. Let q be a conjunctive query and M be an (ε,r)-plan for q. Then, τ ∗ (M) >
1/(1 − ε).
Proof. For every q  Γ1
ε , we have by definition that τ ∗ (q
) > 1/(1 − ε). Additionally, by the
definition of an (ε,r)-plan, we have that τ ∗ (q/Mr ) > 1/(1 − ε).
Further, for a given query q, let us define the following sets:
C(q) = {q | q is a connected subquery of q}
Cε (q) = {q | q  Γ1
ε , q is a connected subquery of q}
Sε (q) = {q | q  Γ1
ε , q is a minimal connected subquery of q}
and let
β (q,M) =

1
τ ∗ (q/Mr )


τ ∗ (M)
+
r
k=1

q∈Sε (q/Mk−1 )
 1
τ ∗ (q)
τ ∗ (M)
.
We can now present the precise statement.
Theorem 5.11. If q has an (ε,r)-plan M, then any deterministic tuple-based MPC algorithm running in r + 1 rounds with maximum load L reports at most
β (q,M) ·

(r + 1)L
M
τ ∗ (M)
p · E[|q(I)|]
correct answers in expectation over a uniformly at random chosen matching database I where each
relation has size M.
The above theorem implies Theorem 5.8 by following the same proof as in Theorem 3.20. Indeed,
for L ≤ cM/p1−ε , we obtain that the output tuples will be at most θ · E[|q(I)|], where θ = β (q,M) ·
((r + 1)c)
τ ∗ (M)
. If we choose the constant c such that θ < 1/9, then we can apply Lemma 3.19 to
show that for any randomized algorithm we can find an instance I where it will fail to produce the
output with probability Ω(1).
Journal of the ACM, Vol. 64, No. 6, Article 40. Publication date: October 2017.        
Communication Steps for Parallel Query Processing 40:47
In the rest of this section, we present the proof of Theorem 5.11. Let A be an algorithm that
computes q in r + 1 rounds. The intuition is as follows. Consider an ε-good set M; then any matching database i consists of two parts, i = (iM ,iM ),
9 where iM are the relations for atoms in M, and
iM are all the other relations. We show that, for a fixed instance iM , the algorithm can be used
to compute q/M(iM ) in r + 1 rounds; however, the first round is almost useless, because the algorithm can discover only a tiny number of join tuples with two or more atoms Sj ∈ M (since every
subquery q of q that has two atoms in M is not in Γ1
ε ). This shows that the algorithm can compute
most of the answers in q/M(iM ) in only r rounds, and we repeat the argument until a one-round
algorithm remains.
To formalize this intuition, we need some notation. For two relations A, B we write A  B, called
the semijoin, to denote the set of tuples in A for which there is a tuple in B that has equal values
on their common variables. We also write A  B, called the antijoin, to denote the set of tuples in
A for which no tuple in B has equal values on their common variables.
Let A be a deterministic algorithm with r + 1 rounds, k ∈ [r + 1] a round number, s a server,
and q a subquery of q. We define:
K A,k,s msg (q
) = {a ∈ [n]
vars(q
) | ∀ matching database i, msg≤k
s (A,i) = msg ⇒ a ∈ q
(i)}
K A,k
msg (q
) =

p
s=1
K A,k,s msgs (q
).
Using the above notation, K A,k,s
msg≤k
s (A,i)
(q
) and K A,k
msg≤k (A,i)
(q
) denote the set of join tuples from
q known at round k by server s, and by all servers, respectively, on input i. Further, A(i) =
K A,r+1
msg≤r+1 (A,i)
(q) is w.l.o.g. the final answer of the algorithm A on input i. Finally, let us define
J A,q (i) =

q∈C(q)
K A,1
msg≤1 (A,i)
(q
)
J
A,q ε (i) =

q∈Cε (q)
K A,1
msg≤1 (A,i)
(q
).
J
A,q ε (i) is precisely the set of join tuples known after the first round but restricted to those that
correspond to subqueries that are not computable in one round. Since these queries are not computable in one round, Equation (27) implies that the join tuples we can infer is a vanishing fraction
(w.r.t. p) of the total number of join tuples; thus, the number of tuples in J
A,q ε (i) will be small.
We can now state the two lemmas we need as building blocks to prove Theorem 5.11.
Lemma 5.12. Let q be a query, and M be any ε-good set for q. If A is an algorithm with r + 1
rounds for q, then for any matching database iM over the atoms of M, there exists an algorithm A
with r rounds for q/M using the same number of processors and the same total number of bits of
communication received per processor such that, for every matching database iM defined over the
atoms of M:
|A(iM ,iM )|≤|q(iM ,iM )  J
A,q ε (iM ,iM )| + |A
(iM )|.
In other words, the algorithm returns no more answers than the (very few) tuples in J
A,q ε , plus
what another algorithm A that we define next computes for q/M using one fewer round.
9We will use i to denote a fixed matching instance, as opposed to I which denotes a random instance.
Journal of the ACM, Vol. 64, No. 6, Article 40. Publication date: October 2017.   
40:48 P. Beame et al.
Proof. We call q/M the contracted query. While the original query q takes as input the complete
database i = (iM ,iM ), the input to the contracted query is only iM . Observe also that for different
matching databasesiM , the lemma produces different algorithms A
. We fix now a matching database iM .
The construction of A is based on the following two constructions, which we call contraction
and retraction.
Contraction. We first show how to use the algorithm A to derive an algorithm Ac for q/M that
uses the same number of rounds as A.
For each connected component qc of M, we choose a representative variable zc ∈ vars(qc ).
The query answer qc (iM ) is a matching instance, since qc is treelike (because χ (M) = 0). Denote
σ = {σx | x ∈ vars(q)}, where, for every variable x ∈ vars(q), σx : [n] → [n] is the following permutation. If x  vars(M), then σx is defined as the identity, that is, σx (a) = a for every a ∈ [n].
Otherwise, if qc is the unique connected component such that x ∈ vars(qc ) and a ∈ qc (iM ) is the
unique tuple such that ax = a, we define σx (a) = azc . In other words, we think of σ as permuting
the domain of each variable x ∈ vars(q). Observe that σ is known to all servers, since iM is a fixed
instance.
It holds that σ (q(i)) = q(σ (i)), and σ (iM ) = idM , where idM is the identity matching database
(where each relation in M is {(1, 1,...), (2, 2,...),...}). Therefore,10 if Π denotes the projection
operator, then
q/M(iM ) = σ−1 (Πvars(q/M) (q(σ (iM ), idM ))).
Using the above equation, we can now define the algorithm Ac that computes the query
q/M(iM ). First, each input server for Sj ∈ M replaces Sj with σ (Sj). Second, we run A unchanged,
substituting all relations Sj ∈ M with the identity. Finally, we apply σ−1 to the answers and return
the output. Hence, we have:
Ac (iM ) = σ−1 (Πvars(q/M) (A(σ (iM ), idM ))). (37)
Retraction. Next, we transform Ac into a new algorithm Ar , called the retraction ofAc , that takes
as input iM as follows.
—In round 1, each input server for Sj sends (in addition to the messages sent by Ac ) every
tuple in aj ∈ Sj to all servers s that eventually receive aj . In other words, the input server
sends t to every s for which there exists k ∈ [r + 1] such that aj ∈ K Ac,k,s
msg≤k
s (Ac ,iM )
(Sj). This
is possible because of the restrictions in the tuple-based MPC model: All destinations of
aj depend only on Sj , and hence can be computed by the input server. Note that this does
not increase the total number of bits received by any processor, though it does mean that
more communication, possibly all of it, will be performed during the first round. Therefore,
over the course of the entire inductive argument, the load is never more than the number
of rounds r times the original load.
—In round 2, Ar sends no tuples.
—In rounds k ≥ 3, Ar sends a join tuple t from server s to server s if server s knows t at
round k, and also algorithm Ac sends t from s to s at round k.
Observe first that the algorithm Ar is correct, in the sense that the output Ar (iM ) will be
a subset of q/M(iM ). We now need to quantify how many tuples Ar misses compared to the
10We assume vars(q/M) ⊆ vars(q); for that, when we contract a set of nodes of the hypergraph, we replace them with
one of the nodes in the set.
Journal of the ACM, Vol. 64, No. 6, Article 40. Publication date: October 2017. 
Communication Steps for Parallel Query Processing 40:49
contracted algorithm Ac . Let QM = {q | q subquery of q/M, |q
| ≥ 2}, and define
J Ac
+ (iM ) =

q∈QM
K Ac,1
msg1 (Ac,i)
(q
).
The set J Ac
+ (iM ) is exactly the set of non-atomic tuples known by Ac right after round 1: These
are also the tuples that the new algorithm Ar will choose not to send during round 2.
Lemma 5.13. (Ac (iM )  J Ac
+ (iM )) ⊆ Ar (iM ).
Proof. We will prove the statement by induction on the number of rounds: For any subquery
q of q/M, if server s knows t ∈ (q
(iM )  J Ac
+ (iM )) at round k for algorithm Ac , then server s
knows t at round k for algorithm Ar as well.
For the induction base, in round 1 we have by construction that K Ac,1,s
msg1
s (Ac ,iM )
(Sj) ⊆
K Ar ,1,s
msg1
s (Ar ,iM )
(Sj) for every Sj ∈ M, and thus any tuple t (join or atomic) that is known by server s
for algorithm Ac will be also known for algorithm Ar .
Consider now some round k + 1 and a tuple t ∈ (q
(iM )  J Ac
+ (iM )) known by servers for algorithm Ac . If q is a single relation, then the statement is correct since by construction all atomic
tuples are known at round 1 for algorithm Ar . Otherwise, q ∈ QM . Let t1,...,tm be the subtuples
at server s from which tuple t is constructed, where tj ∈ qj (iM ) for every j = 1,...,m. Observe
that tj ∈ (qj (iM )  J Ac
+ (iM )). Thus, if ti was known at round k by some server s for algorithm
Ac , by the induction hypothesis it would be known by server s for algorithm Ar as well, and
thus it would have been communicated to server s at round k + 1.
From the above lemma it follows that
Ac (iM ) ⊆ Ar (iM ) ∪ (q/M(iM )  J Ac
+ (iM )). (38)
Additionally, by the definition ofε-goodness, if a subquery q of q has two atoms in M, then q  Γ1
ε .
Hence, we also have
J Ac
+ (iM ) ⊆ σ−1 (Πvars(q/M) (J
A,q ε (σ (i)))). (39)
Since Ar send no information during the second round, we can compress it to an algorithm A
that uses only r rounds. Finally, since M is ε-good, we have χ (q/M) = χ (q) and thus |Ac (iM )| =
|A(iM ,iM )|. Combining everything together,
|A(iM ,iM )| = |Ac (iM )|
≤ |Ar (iM )| + |(q/M(iM )  J Ac
+ (iM ))|
≤ |A
(iM )| + |(q/M(iM )  σ−1 (Πvars(q/M) (J
A,q ε (σ (i))))|
≤ |A
(iM )| + |Πvars(q/M) (q(i)  J
A,q ε (i))|
≤ A
(iM )| + |q(i)  J
A,q ε (i)|.
This concludes the proof.
Lemma 5.14. Let q be a conjunctive query and q a subquery of q. Let B be any algorithm that
outputs a subset of answers to q (i.e., for every database i, B(i) ⊆ q
(i)). Let I be a uniformly at
random chosen matching database for q, and I  = Iatoms (q) its restriction over the atoms in q
.
If E[|B(I 
)|] ≤ γ · E[|q
(I 
)|], then E[|q(I)  B(I 
)|] ≤ γ · E[|q(I)|].
Journal of the ACM, Vol. 64, No. 6, Article 40. Publication date: October 2017.             
40:50 P. Beame et al.
Proof. Let y = vars(q
) and d = |y|. By symmetry, the quantity E[|σy=a (q(I))|] is independent
of a and therefore equals E[|q(I)|]/nd . Notice that by construction σy=a (B(i
)) ⊆ {a}. We now have
E[|q(I)  B(I 
)|] =

a∈[n]d
E[|σy=a (q(I))  σy=a (B(I 
))|]
=

a∈[n]d
E[|σy=a (q(I))|] · P (a ∈ B(I 
))
= 	
E[|q(I)|]/nd 

·

a∈[n]d
P(a ∈ B(I 
))
= E[|q(I)|] · E[|B(I 
)|]/nd
≤ E[|q(I)|] · (γ · E[|q
(I 
)|])/nd
≤ γ · E[|q(I)|].
where the last inequality follows from the fact that E[|q
(I 
)|] ≤ nd (since for every database i
,
we have |q
(i
)| ≤ nd ).
Proof of Theorem 5.11. Given an (ε,r)-plan atoms(q) = M0 ⊃ M1 ⊃ ... ⊃ Mr , we define Mˆ
k =
Mk − Mk−1 for k ≥ 1. Let A be an algorithm for q that uses (r + 1) rounds.
We start by applying Lemma 5.12 for algorithm A and the ε-good set M1. Then, for every matching database iM¯ 1 = i
M1
, there exists an algorithm A(1)
i
M
1
for q/M1 that runs in r rounds such that for
every matching database iM1 we have
|A(i)|≤|q(i)  J
A,q ε (i)| + |A(1)
i
M
1
(iM1 )|.
We can iteratively apply the same argument. For k = 1,...,r − 1, let us denote Bk = A(k)
iMk
the
inductively defined algorithm for query q/Mk , and consider the ε-good set Mk+1. Then, for every
matching database i
Mk+1 there exists an algorithm Bk+1 = A(k+1)
iMk+1
for q/Mk+1 such that for every
matching database iMk+1 , we have
|Bk (iMk )|≤|q/Mk (iMk )  J
Bk,q/Mk
ε (iMk )| + |Bk+1 (iMk+1 )|.
We can now combine all the above inequalities for k = 0,...,r − 1 to obtain
|A(i)|≤|q(i)  J
A,q ε (iMr ,i
M1
,...,i
Mr
)|
+ |q/M1 (iM1 )  J
B1,q/M1
ε (iMr ,i
M2
,...,i
Mr
)|
+ ...
+ |q/Mr−1 (iMr−1 )  J
Br−1,q/Mr−1
ε (iMr ,i
Mr
)|
+ |Br (iMr )|. (40)
We now take the expectation of Equation (40) over a uniformly chosen matching database I and
upper bound each of the resulting terms. Observe, first, that for all k = 0,...,r, we have χ (q/Mk ) =
χ (q), and, hence, by Lemma 3.13, we have E[|q(I)|] = E[|(q/Mk )(IMk )|].
We start by analyzing the last term of the equation, which is the expected output of an algorithm
Br that uses one round to compute q/Mr . By the definition of τ ∗ (M), we have τ ∗ (q/Mr ) ≥ τ ∗ (M).
Since the number of bits received by each processor in the first round of algorithm Br is at most
Journal of the ACM, Vol. 64, No. 6, Article 40. Publication date: October 2017.        
Communication Steps for Parallel Query Processing 40:51
the total for the original algorithm A, it therefore is at most (r + 1)L, so we can apply Theorem 3.12
to obtain that:
E[Br (IMr )] ≤ p

(r + 1)L
τ ∗ (q/Mr )M


τ ∗ (q/Mr )
E[|(q/Mr )(IMr )|
≤ p

(r + 1)L
τ ∗ (q/Mr )M


τ ∗ (M)
E[|q(I)||.
We next bound the remaining terms. Note that IMk−1 = (IMr , I
Mk
,..., I
Mr
) and consider the expected number of tuples in J = J
Bk−1,q/Mk−1
ε (IMk−1 ). The algorithm Bk−1 = A(k−1)
IMk−1
itself depends on
the choice of I
Mk−1
; still, we show that J has a small number of tuples. Every subquery q of q/Mk−1
that is not in Γ1
ε (and hence contributes to J) has τ ∗ (q
) ≥ τ ∗ (M). For each fixing I
Mk−1 = iMk−1
,
the expected number of tuples produced for subquery q by Bq, where Bq is the portion of the
first round of A(k−1)
iMk−1
that produces tuples for q
, satisfies E[|Bq (IMk−1 )|] ≤ γ (q
) · E[|q
(IMk−1 )|],
where
γ (q
) = p

(r + 1)L
τ ∗ (q)M
τ ∗ (M)
,
since each processor in a round of A(k−1)
iMk−1
(and hence Bq) receives at most r + 1 times the communication bound for a round of A. We now apply Lemma 5.14 to derive
E[|q(I)  Bq (IMk−1 )|] = E[|(q/Mk−1)(IMk−1 )  Bq (IMk−1 )|]
≤ γ (q
) · E[|(q/Mk−1)(IMk−1 )|]
= γ (q
) · E[|q(I)|].
Averaging over all choices of I
Mk−1 = iMk−1 and summing over the number of different queries
q ∈ S(q/Mk−1), where we recall that Sε (q/Mk−1) is the set of all minimal connected subqueries
q of q/Mk−1 that are not in Γ1
ε , we obtain
E[|q(I)J
Ak−1,q/Mk−1
ε (IMk−1 )|] ≤

q∈Sε (q/Mk−1 )
γ (q
) · E[|q(I)|].
Combining the bounds obtained for the r + 1 terms in Equation (40), we conclude that
E[|A(I)|] ≤






1
τ ∗ (q/Mr )


τ ∗ (M)
+
r
k=1

q∈Sε (q/Mk−1 )
 1
τ ∗ (q)
τ ∗ (M)




×

(r + 1)L
M
τ ∗ (M)
p · E[|q(I)|]
= β (q,M) ·

(r + 1)L
M
τ ∗ (M)
p · E[|q(I)|],
which proves Theorem 5.11.
Journal of the ACM, Vol. 64, No. 6, Article 40. Publication date: October 2017.            
40:52 P. Beame et al.
5.3 Application of the Lower Bound
We show now how to apply Theorem 5.8 to obtain lower bounds for several query classes and
compare the lower bounds with the upper bounds.
The first class is the queries Lk , where the following corollary is a straightforward application
of Theorem 5.8 and Lemma 5.6.
Corollary 5.15. Any tuple-based MPC algorithm that computes Lk with load L = O(M/p1−ε )
requires at least logkε k rounds of computation.
Observe that this gives a tight lower bound for Lk , since in the previous section we showed that
there exists a query plan with depth logkε k and load O(M/p1−ε ).
Second, we give a lower bound for treelike queries and for that we use a simple observation:
Proposition 5.16. Let q be a treelike query, and q be any connected subquery of q. If there exists
a tuple-based algorithm that computes q with load L in r rounds, then any tuple-based algorithm
that computes q with load L needs at least r rounds.
Proof. Given any tuple-based MPC algorithm A for computing q in r rounds with maximum
load L, we construct a tuple-based MPC algorithm A that computes q in r rounds with at most
load L. A will interpret each instance over q as part of an instance for q by using the relations in
q and using the identity permutation (Sj = {(1, 1,...), (2, 2,...),...}) for each relation in q \ q
.
Then A runs exactly as A for r rounds; after the final round, A projects out for every tuple all the
variables not in q
. The correctness of A follows from the fact that q is treelike.
Define diam(q), the diameter of a query q, to be the longest distance between any two nodes
in the hypergraph of q. In general, rad(q) ≤ diam(q) ≤ 2 rad(q). For example, rad(Lk ) = k/2,
diam(Lk ) = k, and rad(Ck ) = diam(Ck ) = k/2.
Corollary 5.17. Any tuple-based MPC algorithm that computes a treelike query q with load L =
O(M/p1−ε ) needs at least logkε (diam(q)) rounds.
Proof. Let q be the subquery of q that corresponds to the diameter of q. Notice that q is
a connected query, and, moreover, it behaves exactly like Ldiam(q). Hence, by Lemma 5.15 any
algorithm needs at least logkε (diam(q)) to compute q
. By applying Proposition 5.16, we have
that q needs at least that many rounds as well.
Let us compare the lower bound rlow = logkε (diam(q)) and the upper bound rup =
logkε (rad(q)) + 1 from Lemma 5.4. Since diam(q) ≤ 2rad(q), we have that rlow ≤ rup. Additionally, rad(q) ≤ diam(q) impliesrup ≤ rlow + 1. Thus, the gap between the lower bound and the upper
bound on the number of rounds is at most 1 for treelike queries. When ε < 1/2, these bounds are
matching, since kε = 2 and 2rad(q) − 1 ≤ diam(q) for treelike queries.
Third, we study one instance of a non treelike query, namely the cycle query Ck . The lemma is
a direct application of Lemma 5.7.
Lemma 5.18. Any tuple-based MPC algorithm that computes the query Ck with load L =
O(M/p1−ε ) requires at least logkε (k/(mε + 1)) + 2 rounds, where mε = 2/(1 − ε).
For cycle queries we also have a gap of at most 1 between this lower bound and the upper bound
in Lemma 5.4.
Example 5.19. Let ε = 0 and consider two queries, C5 and C6. In this case, we have kε = mε = 2
and rad(C5) = rad(C6) = 2.
For query C6, the lower bound is then log2 (6/3) + 2 = 3 rounds, while the upper bound is
log2 (3) + 1 = 3 rounds. Hence, in the case ofC6 we have tight upper and lower bounds. For query
Journal of the ACM, Vol. 64, No. 6, Article 40. Publication date: October 2017.  
Communication Steps for Parallel Query Processing 40:53
C5, the upper bound is again log2 (3) + 1 = 3 rounds, but the lower bound becomes log2 (5/3) +
2 = 2 rounds. The exact number of rounds necessary to compute C5 is thus open.
Finally, we show how to apply Lemma 5.15 to show that transitive closure even on some
very simple graphs requires many rounds. In particular, we consider the problem ConnectedComponents, for which, given an undirected graph G = (V, E) with input a set of edges, the
requirement is to label the nodes of each connected component with the same label, unique to
that component. Our very simple graphs will be disjoint unions of paths, and the lower bound
holds when the input edges are distributed for the algorithm in a structured fashion that is natural
for such graphs. For this class of inputs, the multiway join view naturally captures the problem.
Moreover, the tuple-based restriction of these algorithms captures a natural restricted class of
MapReduce algorithms for general connectivity in which the grouping by key only gathers together portions of the graph that are known to be connected, which for our restricted class of
graphs means being part of a join tuple. The routing by key of MapReduce yields the restriction
on the routing decisions for join tuples in our model.
Theorem 5.20. Let G be an input graph of size M. For any 0 ≤ ε < 1, there is no algorithm in the
tuple-based MPC model that computes Connected-Components on unions of disjoint paths using
p processors and load L = O(M/p1−ε ) in o(logp) rounds.
The idea of the proof is to construct input graphs for Connected-Components whose components correspond to the output tuples for Lk for k = pδ for some small constant δ depending on ε
and use the round lower bound for solving Lk . Notice that the size of the query Lk is not fixed but
depends on the number of processors p.
Proof. Since larger ε implies a more powerful algorithm, we assume without loss of generality
that ε = 1 − 1/t for some integer constant t > 1. Let δ = 1/(2t(t + 2)). The family of input graphs
and the initial distribution of the edges to servers will look like an input to Lk , where k = pδ .
In particular, the vertices of the input graph G will be partitioned into k + 1 sets P1,..., Pk+1,
each partition containing m/k vertices. The edges of G will form permutations (matchings) between adjacent partitions, Pi, Pi+1, for i = 1,..., k. Thus, G will contain exactly k · (m/k) = m
edges. This construction creates essentially k binary relations, each with m/k tuples and size
Mk = (m/k) log(m/k).
Since k < p, we can assume that the adversary initially places the edges of the graph so each
server is given edges only from one relation. It is now easy to see that any tuple-based algorithm
in MPC that solves Connected-Components for an arbitrary graph G of the above family in r
rounds with load L implies an (r + 1)-round tuple-based algorithm with the same load that solves
Lk when each relation has size M. Indeed, the new algorithm runs the algorithm for connected
components for the first r rounds and then executes a join on the labels of each node. Since each
tuple in Lk corresponds exactly to a connected component in G, the join will recover all the tuples
of Lk .
Since the query size is not independent of the number of servers p, we have to carefully compute
the constants for our lower bounds. Consider an algorithm for Lk with load L ≤ cM/p1−ε , where
M = m log(m). Let r = logkε k − 2. Observe also that kε = 2t since ε = 1 − 1/t.
We will use the (ε,r)-plan M for Lk presented in the proof of Lemma 5.6, apply Theorem 5.11,
and compute the factor β (Lk ,M). First, notice that each query Lk /Mj for j = 0,...,r is isomorphic
to Lk/k j
ε
. Then, the set Sε (Lk/k j
ε
) consists of at most k/kj
ε paths q of length kε + 1. By the choice of
r, Lk /Mr is isomorphic to L, where kε + 1 ≤  < k2
ε . Further, we have that τ ∗ (M) = τ ∗ (Lkε+1) =
(kε + 1)/2 = t + 1, since kε = 2t.
Journal of the ACM, Vol. 64, No. 6, Article 40. Publication date: October 2017.  
40:54 P. Beame et al.
Thus, we have
β (Lk ,M) =
 1
τ ∗ (Lk /Mr )
τ ∗ (M)
+
r
j=1

q∈Sε (q/Mk−1 )
 1
τ ∗ (q)
τ ∗ (M)
≤ (1 − ε)
τ ∗ (M)


1 +
r
j=1
k
kj−1 ε



≤ (2k + 1)(1 − ε)
τ ∗ (M)
.
Observe now that M/Mk = 1/(1/k − log(k)/(k log(m))) ≤ 2k, assuming that m ≥ p2δ . Consequently, Theorem 5.11 implies that any tuple-based MPC algorithm using at most logkε k − 1
rounds reports at most the following fraction of the required output tuples for the Lk query:
β (Lk ,M) · p

(r + 1)L
Mk
τ ∗ (M)
≤ (2k + 1)(2ck(r + 1)/t)
τ ∗ (M) · p1−τ ∗ (M)(1−ε )
= (2k + 1)(2ck(r + 1)/t)
1+t · p1−(1+t)(1−ε )
≤ c1kt+2 (c2r)
1+t · p1−(1+t)(1−ε )
≤ c3kt+2 (log2 k)
c4 · p1−(1+t)(1−ε )
≤ c3 (δ log2 p)
c4 · pδ (t+2)+1−(1+t)(1−ε )
= c3 (δ log2 p)
c4 · pδ (t+2)−1/t
= c3 (δ log2 p)
c4 · p−1/2t
,
where c3,c4 are constants. Since t > 1, the fraction of the output tuples is o(1) as a function of the
number of processors p. This implies that any algorithm that computes Connected-Components
on G requires at least logkε pδ  − 2 = Ω(logp) rounds.
6 RELATED WORK
MapReduce-Related Models. Several computation models have been proposed to understand the
power of MapReduce and other related massively parallel programming frameworks (Feldman
et al. 2010; Karloff et al. 2010; Koutris and Suciu 2011; Afrati et al. 2012; Pietracaprina et al. 2012).
These all identify the number of communication rounds as a main complexity parameter but differ
in their treatment of the communication.
The first of these models was the Massive, Unordered, Distributed (MUD) model of Feldman
et al. (2010). It takes as input a sequence of elements and applies a binary merge operation repeatedly, until obtaining a final result, similarly to a User Defined Aggregate in database systems.
The article compares MUD with streaming algorithms: A streaming algorithm can trivially simulate MUD, and the converse is also possible if the merge operators are computationally powerful
(beyond PTIME).
Karloff et al. (2010) define MRC, a class of multi-round algorithms based on using the MapReduce primitive as the sole building block, and fixing specific parameters for balanced processing.
The number of processors p is Θ(N1−ϵ ), and each can exchange MapReduce outputs expressible
in Θ(N1−ϵ ) bits per step, resulting in Θ(N2−2ϵ ) total storage among the processors on a problem
of size N. Their focus was algorithmic, showing simulations of other parallel models by MRC, as
well as the power of two round algorithms for specific problems.
Pietracaprina et al. (2012) propose a MapReduce model where the parameters are the number
of rounds, the maximum size m of the memory of each reducer, and the total amount of memory
Journal of the ACM, Vol. 64, No. 6, Article 40. Publication date: October 2017.   
Communication Steps for Parallel Query Processing 40:55
M. The MRC model is a special case where m = O(N1−ϵ ) and M = O(N2−2ϵ ), but there is no
restriction on the number of processors. Another MapReduce model is introduced by Goodrich
et al. (2011), where again the maximum size of each reducer is a parameter.
Lower bounds for the single round MapReduce model are first discussed by Afrati et al. (2012),
who derive an interesting tradeoff between reducer size and replication rate. This is nicely illustrated by Ullman’s drug interaction example (Ullman 2012). There are n (= 6,500) drugs, each
consisting of about 1MB of data about patients who took that drug, and one has to find all drug
interactions by applying a user defined function (UDF) to all pairs of drugs. To see the tradeoffs,
it helps to simplify the example, by assuming we are given two sets, each of size n, and we have to
apply a UDF to every pair of items, one from each set, in effect computing their Cartesian product.
There are two extreme ways to solve this. One can use n2 reducers, one for each pair of items; while
each reducer has size 2, this approach is impractical because all of the data are replicated n times.
At the other extreme one can use a single reducer that handles the entire data; the replication rate
is 1, but the size of the reducer is 2n, which is also impractical. As a tradeoff, partition each set into
д groups of size n/д, and use one reducer for each of the д2 pairs of groups: The size of a reducer is
2n/д, while the replication rate is д. Thus, there is a tradeoff between the replication rate and the
reducer size, which was also shown to hold for several other classes of problems (Afrati et al. 2012).
There are two significant limitations of this prior work: (1) As powerful and as convenient as
the MapReduce framework is, the operations it provides may not be able to take full advantage
of the resource constraints of modern systems. The lower bounds say nothing about alternative
ways of structuring the computation that send and receive the same amount of data per step.
(2) Even within the MapReduce framework, the only lower bounds apply to a single communication round, and say nothing about the limitations of multi-round MapReduce algorithms.
While it is convenient that MapReduce hides the number of servers from the programmer, when
considering the most efficient way to use resources to solve problems it is natural to expose information about those resources to the programmer. In this article, we take the view that the number
of servers p should be an explicit parameter of the model, which allows us to focus on the tradeoff
between the amount of communication and the number of rounds. For example, going back to our
Cartesian product problem, if the number of servers p is known, there is one optimal way to solve
the problem: partition each of the two sets into д = √p groups, and let each server handle one pair
of groups.
A model with p as explicit parameter was proposed by Koutris and Suciu (2011), who showed
both lower and upper bounds for one round of communication. In this model only tuples are
sent and they must be routed independent of each other. For example, Koutris and Suciu (2011)
proves that multi-joins on the same attribute can be computed in one round, while multi-joins on
different attributes, like R(x), S (x,y),T (y) require strictly more than one round. The study was
mostly focused on understanding data skew, the model was limited, and the results do not apply
to more than one round.
The MPC model we introduce in this article is more general than the above models, allowing
arbitrary bits to represent communicated data, rather than just tuples, and unbounded computing
power of servers so the lower bounds we show for it apply more broadly. Moreover, we establish
lower bounds that hold even in the absence of skew.
Other Parallel Models. The prior parallel model that is closest to the MPC model is Valiant’s Bulk
Synchronous Parallel (BSP) model (Valiant 1990). The BSP model operates in synchronous rounds
of supersteps consisting of possibly asynchronous steps. In addition to the number of processors,
there is a superstep size, L, there is the notion of an h-relation, a mapping in which each processor
sends and receives at most h bits, as well as an architecture-dependent bandwidth parameter д
Journal of the ACM, Vol. 64, No. 6, Article 40. Publication date: October 2017.
40:56 P. Beame et al.
that says that a superstep has to have at least дh steps in order for the processors to deliver an
h-relation during a superstep. In the MPC model, the notion of load L largely parallels the notion
of h-relation (though we technically only need to bound the number of bits each processor receives
to obtain our lower bounds) but the other parameters are irrelevant, because we strengthen the
model to allow unbounded local computation and hence the only notion of time in the model is
the number of synchronous rounds (supersteps).
The finer-grained LogP model (Culler et al. 1996) does away with the synchronization barriers and the notion of h-relations inherent in the BSP model and has a more continuous notion of
relaxed synchrony based on a latency bound and bound on processor overhead for setting up communication rather than based on supersteps. While its finer grain computation and relaxed asyncrhony allowed tighter modeling of a number of parallel architectures, it seems less well matched
to system architectures for MapReduce-style computations than either the BSP or MPC models.
Communication Complexity. The results we show belong to the study of communication complexity, for which there is a very large body of existing research (Kushilevitz and Nisan 1997).
Communication complexity considers the number of bits that need to be communicated between
cooperating agents to solve computational problems when the agents have unlimited computational power. Our model is related to the so-called number-in-hand multi-party communication
complexity, in which there are multiple agents and no shared information at the start of communication. This has already been shown to be important to understanding the processing of massive
data: Analysis of number-in-hand (NIH) communication complexity has been the main method for
obtaining lower bounds on the space required for datastream algorithms (e.g., Alon et al. (1999)).
However, there is something very different about the results that we prove here. In almost all
prior lower bounds, there is at least one agent that has access to all communication between
agents.11 (Typically, this is either via a shared blackboard to which all agents have access or a
referee who receives all communication.) In this case, no problem on N bits whose answer is M
bits long can be shown to require more than N + M bits of communication. In our MPC model, all
communication between servers is private and we restrict the communication per processor per
step, rather than the total communication. Indeed, the privacy of communication is essential to
our lower bounds, since we prove lower bounds that apply when the total communication is much
larger than N + M. (Our lower bounds for some problems apply when the total communication is
as large as N1+δ .)
Follow-up Work. Several recent works follow up on the results in this article. In Koutris et al.
(2016) and Ketsman and Suciu (2017) the authors construct parallel algorithms in the MPC model
that areworst-case optimal, that is, match the maximum load of the worst behaved data distribution.
They show that for every full conjunctive query q, there exists a data distribution of input size M
for which every MPC algorithm has load Ω(M/p1/ρ∗
), where ρ∗ is the minimum fractional edge
cover of the query q. Moreover, for binary conjunctive queries where the relations have equal size
it is shown that there exists an algorithm that achieves load O˜ (M/p1/ρ∗
). In Afrati et al. (2017), the
authors propose a multi-round algorithm for acyclic conjunctive queries where the load depends
not only on the input but also the output size.
11Though private-messages models have been defined before, we are aware of only two lines of work where lower bounds
make use of the fact that no single agent has access to all communication: (1) Results of Gál and Gopalan (2007) and Guha
and Huang (2009) use the assumption that communication is both private and (multi-pass) one-way, but unlike the bounds
we prove here, their lower bounds are smaller than the total input size; (2) Tiwari (1987) defined a distributed model of
communication complexity in networks in which in input is given to two processors that communicate privately using
other helper processors. However, this model is equivalent to ordinary public two-party communication when the network
allows direct private communication between any two processors, as our model does.
Journal of the ACM, Vol. 64, No. 6, Article 40. Publication date: October 2017.
Communication Steps for Parallel Query Processing 40:57
The Hypercube algorithm was also implemented as part of massively parallel systems in Chu
et al. (2015) and Vitorovic et al. (2016) and compared against more traditional parallel query plans.
7 CONCLUSION
In this article, we introduce a simple but powerful model, the MPC model, that allows us to analyze
query processing in massively parallel systems. The MPC model captures two important parameters: the number of communication rounds and the maximum load that a server receives during
the computation. We prove the first tight upper and lower bounds for the maximum load in the
case of one communication round and input data without skew. Then, we show how to handle
skew for several classes of queries. Finally, we analyze the precise tradeoff between the number of
rounds and maximum load for the case of multiple rounds.
Our work leaves open many interesting questions. The analysis for multiple rounds works for a
limited class of inputs, since we have to assume that all relations have the same size. Further, our
lower bounds are (almost) tight only for a specific class of queries (treelike queries), so it remains
open how we can obtain lower bounds for any conjunctive query, where relations have different
size.
The effect of data skew in parallel computation is another exciting research direction. Although
we have some understanding on how to handle skew in a single round, it is an open how skew
influences computation when we have multiple rounds and what are the tradeoffs we can obtain
in such cases.