We say an algorithm on n × n matrices with integer entries in [−M, M] (or n-node graphs with edge weights
from [−M, M]) is truly subcubic if it runs in O(n3−δ · poly(log M)) time for some δ > 0. We define a notion
of subcubic reducibility and show that many important problems on graphs and matrices solvable in O(n3)
time are equivalent under subcubic reductions. Namely, the following weighted problems either all have truly
subcubic algorithms, or none of them do:
• The all-pairs shortest paths problem on weighted digraphs (APSP).
• Detecting if a weighted graph has a triangle of negative total edge weight.
• Listing up to n2.99 negative triangles in an edge-weighted graph.
• Finding a minimum weight cycle in a graph of non-negative edge weights.
• The replacement paths problem on weighted digraphs.
• Finding the second shortest simple path between two nodes in a weighted digraph.
• Checking whether a given matrix defines a metric.
• Verifying the correctness of a matrix product over the (min, +)-semiring.
• Finding a maximum subarray in a given matrix.
Therefore, if APSP cannot be solved in n3−ε time for any ε > 0, then many other problems also need essentially cubic time. In fact, we show generic equivalences between matrix products over a large class of algebraic
structures used in optimization, verifying a matrix product over the same structure, and corresponding triangle detection problems over the structure. These equivalences simplify prior work on subcubic algorithms
for all-pairs path problems, since it now suffices to give appropriate subcubic triangle detection algorithms.
Other consequences of our work are new combinatorial approaches to Boolean matrix multiplication over
the (OR,AND)-semiring (abbreviated as BMM). We show that practical advances in triangle detection would
imply practical BMM algorithms, among other results. Building on our techniques, we give two improved
BMM algorithms: a derandomization of the combinatorial BMM algorithm of Bansal and Williams (FOCS’09),
and an improved quantum algorithm for BMM.
CCS Concepts: • Theory of computation → Problems, reductions and completeness; Shortest paths;
1 INTRODUCTION
Many computational problems on graphs and matrices have natural cubic time solutions. For example, n × n matrix multiplication over any algebraic structure can be done in O(n3) operations.
For algebraic structures that arise in optimization, such as the (min, +)-semiring, it is of interest
to determine when we need only a subcubic number of operations.1 The all-pairs shortest paths
problem (APSP) also has an O(n3) time algorithm on n-node graphs, known for over 40 years [29,
69]. One of the “Holy Grails” of graph algorithms is to determine whether this cubic complexity
is basically inherent, or whether a significant improvement (say, O(n2.9) time) is possible. (It is
known that this question is equivalent to finding a faster algorithm for (min, +) matrix multiplication [28, 47]). Many researchers believe that cubic time is essentially necessary: there are n2
pairs of nodes, and in the worst case, we should not expect to improve too much on O(n) time
per pair.2 (We should note that a long line of work has produced subcubic algorithms with small
poly(logn) improvements in the running time. This work recently culminated inn3/2Θ(log1/2 n) time
algorithms [18, 71] that “shave” all polylogarithms, unfortunately without giving a truly subcubic
solution.)
Related to APSP is the replacement paths problem (RPP): given nodes s and t in a weighted
directed graph and a shortest path P from s to t, compute the length of the shortest simple path
that avoids edge e, for all edges e on P. This problem is studied extensively (cf. References [10, 27,
39, 40, 52, 55, 74]) for its applications to network reliability. A slightly subcubic time algorithm is
not hard to obtain from a slightly subcubic APSP algorithm, but nothing faster than this is known.
It does seem that cubic time may be inherent, since for all edges in a path (and there may be Ω(n)
of them), we need to recompute a shortest path. A well-studied restriction of RPP is to find the
second shortest (simple) path between two nodes s and t. This problem also has an O(n3) time
algorithm, but again nothing much faster is known. Here, the cubic complexity does not seem to
be so unavoidable: we simply want to find a certain type of path between two endpoints. Similarly,
finding a minimum weight cycle in a graph with non-negative weights is only known to be possible
in slightly subcubic time.3
An even simpler example is that of finding a triangle in an edge-weighted graph where the sum
of edge weights is negative. Exhaustive search of all triples of nodes takes about O(n3) time, and
applying the best APSP algorithm makes this n3−o(1) time, but we do not know a truly subcubic
algorithm. Previous work has suggested that this negative triangle problem might have a faster
algorithm, since the node-weighted version of the problem can be solved faster [20, 21, 63, 64,
67]. (In fact, the node-weighted version of the problem is no harder than the unweighted triangle
detection problem, which is solvable in O(n2.38) time [36].) Since the cubic algorithm for negative
1Note that in the specific case when the structure is a field, it is well known that one can solve the problem much faster
than O (n3) operations [19, 59]. However, it is unknown if this fact can be used to compute the matrix product fast on many
other important structures such as commutative semirings.
2Of course, as noted by a referee, a similar heuristic argument would say that n × n matrix multiplication requires Ω(n3)
time, which has long known to be false. One should be careful with intuition.
3Note that if we allowed negative weights, this problem is NP-hard.
Journal of the ACM, Vol. 65, No. 5, Article 27. Publication date: August 2018.
Subcubic Equivalences Between Path, Matrix, and Triangle Problems 27:3
triangle is so simple, and many restrictions of the problem have faster algorithms, it would appear
that cubic complexity is unnecessary for finding a negative triangle.
We give theoretical evidence that these open algorithmic questions may be hard to resolve, by
showing that they and other well-studied problems are all surprisingly equivalent, in the sense
that there is a substantially subcubic algorithm for one of them if and only if all of them have substantially subcubic algorithms. Compare with the phenomenon of NP-completeness: one reason
P versus NP looks so hard to resolve is that many researchers working in different fields have all
been working on essentially the same (NP-complete) problem, with no concrete resolution of the
problem in sight. Our situation is analogous: either these problems really need essentially cubic
time, or we are missing a fundamental insight that would make all of them simultaneously easier.
Informally, we say that an algorithm on n × n matrices (or n-node graphs) with entries in
{−M,..., M} ∪ {−∞, ∞} is truly subcubic if it uses O(n3−δ · poly(log M)) time for some δ > 0. In
general, poly log M factors are natural: truly subcubic ring matrix multiplication algorithms (such
as Strassen’s) have poly log M overhead if one counts the bit complexity of operations. We develop
subcubic reductions between many problems solvable in O˜ (n3) time, proving Theorem 1.1 below.
Theorem 1.1. The following problems (with weights in {−M,..., M} ∪ {−∞, ∞}) either all have
truly subcubic algorithms, or none of them do:
(1) The all-pairs shortest paths problem on weighted digraphs (APSP).
(2) The all-pairs shortest paths problem on undirected weighted graphs.
(3) Detecting if a weighted graph has a triangle of negative total edge weight.
(4) Listing up to n3−δ negative triangles in an edge-weighted graph, for a fixed δ > 0.
(5) Computing the matrix product over the (min, +)-semiring.
(6) Verifying the correctness of a matrix product over the (min, +)-semiring.
(7) Checking whether a given matrix defines a metric.
(8) Finding a minimum weight cycle in a graph of non-negative edge weights.
(9) The replacement paths problem on weighted digraphs.
(10) Finding the second shortest simple path between two nodes in a weighted digraph.
(11) Finding a maximum subarray of a given matrix.
Note the only previously known equivalence in the above was that of (1), (2), and (5).
Out of the above reductions, only the reduction from (1) to (3) actually introduces a poly(log M)
factor in the running time. We prove that there is in fact a randomized subcubic reduction from
(1) to (3) that replaces the poly(log M) factor with a logn factor and works with high probability
(cf. Section 4.3). Therefore, any O(n3−δ )-time algorithm for one of the above problems can be
converted into an O(n3−δ
)-time (randomized) algorithm for any of the other problems.
An explicit definition of our reducibility concept is given in Section 3. The truly subcubic runtimes may vary depending on the problem: for example, anO˜ (n2.9) algorithm for negative triangle
implies an O˜ (n2.96) algorithm for APSP.
Perhaps the most interesting aspect of Theorem 1.1 is that some of the listed problems are
decision problems, while others are functions. Hence, to prove lower bounds for these decision
problems, it would suffice to prove them for certain multi-output functions. It is counterintuitive
that an O(n2.9) algorithm returning one bit can be used to compute a function returning n2 bits,
in O(n2.96) time. Nevertheless, it is possible and in retrospect, our reductions are very natural.
Subsequent to the conference version of this article, some more subcubic equivalences to APSP
have been proven. Notably, Abboud, Grandoni, and Vassilevska Williams [1] showed that computing the Radius, Median, or (Unique Path) Betweenness Centrality of a graph are all subcubically
equivalent to APSP.
Journal of the ACM, Vol. 65, No. 5, Article 27. Publication date: August 2018.
27:4 V. V. Williams and R. R. Williams
A few equivalences in Theorem 1.1 follow from a more general theorem, which can be used to
simplify prior work on all-pairs path problems.
In general, we consider (min, ) structures defined over a set R of the form {−M,..., M} ∪
{−∞, ∞} for some M ∈ Z+ ∪ {∞}, together with an operation  : R × R → R.
4 We define a type
of (min, ) structure that we call extended, which allows for an “identity matrix” and an “allzeroes matrix” over the structure. (For definitions, see the Preliminaries.) Almost all structures
we consider in this article are extended, including the Boolean semiring over OR and AND, the
(min, max)-semiring, and the (min, +)-semiring. In Section 4, we prove:
Theorem 1.2 (Informal Statement of Theorems 4.1 and 4.2). Let R¯ be an extended (min, )
structure. The following problems over R¯ either all have truly subcubic algorithms, or none of them
do:
• Negative Triangle Detection. Given an n-node graph with weight function w : V ×V →
R, find nodes i, j, k such that w(i, j) ∈ R, w(i, k) ∈ R, w(k, j) ∈ R, and (w(i, k)  w(k, j)) +
w(i, j) < 0.
• Matrix Product. Given two n × n matrices A, B with entries from R, compute the product of
A and B over R¯.
• Matrix Product Verification. Given three n × n matrices A, B, C with entries from R, determine if the product of A and B over R¯ is C.
The above relationship between matrix product and its verification is particularly surprising, as
n × n matrix product verification over rings can be done inO(n2) randomized time [11, 31], but it is
not known whether ring matrix multiplication can be efficiently reduced to this fast verification.
Spinrad [57] (Open Problem 8.2) and Alon [2] asked if the verification of various matrix products
can be done faster than the products themselves. Our reductions rely crucially on the fact that the
addition operation in a (min, ) structure is a minimum.
Another consequence of Theorem 1.2 is an equivalence for “combinatorial” algorithms:
Theorem 1.3. Either all of the following have truly subcubic “combinatorial” algorithms, or none
of them:
• Boolean matrix multiplication (BMM).
• Detecting if a graph has a triangle.
• Listing up to n3−δ triangles in a graph for constant δ > 0.
• Verifying the correctness of a matrix product over the Boolean semiring.
The notion of a “combinatorial” algorithm does not have a formal definition. Intuitively, such
algorithms are not only theoretically but also practically efficient. In the above theorem, by combinatorial we mean an algorithm with low leading constants. One can verify that the reductions
in our article have low leading constants and low overhead; hence, any simple fast triangle algorithm would yield a simple (and only slightly slower) BMM algorithm. The relation between BMM
and the triangle problem has been investigated by many researchers, e.g., Reference [73] (Open
Problem 4.3(c)) and Reference [57] (Open Problem 8.1).
An extra bullet can be added to Theorem 1.3 relating the above problems to context-free grammar (CFG) parsing. The CFG parsing problem is to determine whether a given n-symbol string can
be generated by a given CFG of size д. Valiant [61] showed that the problem can be reduced to
BMM, showing that any O(n3−ε ) time algorithm for BMM implies an O(дn3−ε ) time algorithm for
CFG parsing. Lee [42] showed a converse: that anyO(дn3−ε ) time algorithm for CFG parsing would
4An analogous treatment is possible for (max, ) structures. We omit the details, as they merely involve negations of
entries.
Journal of the ACM, Vol. 65, No. 5, Article 27. Publication date: August 2018.
Subcubic Equivalences Between Path, Matrix, and Triangle Problems 27:5
imply an O(n3−ε /3) time algorithm for n × n BMM. The reductions in References [61] and [42] are
combinatorial algorithms. In this sense, CFG parsing for constant size grammars can be added to
Theorem 1.3.
Using Theorem 1.3, we also show how our techniques can be used to design alternative approaches to Boolean matrix multiplication (BMM). More concretely, Theorem 1.3 can already yield
new BMM algorithms, with a little extra work. First, we can derandomize the combinatorial BMM
algorithm of Bansal and Williams [6]:
Theorem 1.4. There is a deterministic combinatorial O(n3/ log2.25 n)-time algorithm for BMM.
Subsequent to our result, there have been two improved combinatorial BMM algorithms, by
Chan [17] and Yu [75]. The latter is the current fastest algorithm, running in O(n3/ log4 n) time.
Yu’s improvement is partially based on Theorem 6.1 presented here.
The BMM algorithm of Reference [6] uses randomness in two different ways: it reduces BMM
to a graph theoretic problem, computes a pseudoregular partition of the graph in randomized
quadratic time, then it uses random samples of nodes along with the partition to speed up the
solution of the graph problem. We can avoid the random sampling by giving a triangle algorithm
with O(n3/ log2.25 n) running time and applying a stronger version of Theorem 1.3 that shows an
equivalence preserving polylogarithmic factors (see Corollary 4.1). To get a deterministic triangle
algorithm, we show (using a new reduction) that in fact any polynomial time algorithm for computing a pseudoregular partition suffices for obtaining a subcubic triangle algorithm. With this
relaxed condition, we can replace the randomized quadratic algorithm for pseudoregularity with
a deterministic polynomial time algorithm of Alon and Naor [4]. A similar result holds for APSP:
assuming that APSP requires essentially cubic time, we obtain essentially quadratic time lower
bounds for a natural weighted graph query problem, for any polynomial amount of processing on
the graph.
In more detail, for a graph with edge weight function c : E → Z, a price query is an assignment
of node weights p : V → Z. Such a query has a yes answer if and only if there is a (u,v) ∈ E such
that p(u) + p(v) > c(u,v). (Intuitively, the p(v) are “prices” on the nodes, the c(u,v) are costs of
producing u and v, and a price query asks if there is an edge we are willing to “sell” at the prices
given by the query.)
Theorem 1.5. Suppose there is a constant k > 0 and a function f (n) such that every n-node edgeweighted graph can be preprocessed inO(nk ) time so that any subsequent price query can be answered
inO(n2/f (n)) time. Then, the negative triangle detection problem is solvable inO(n3/f (n1/(2k)
)) time.
In particular, if the hypothesis above is true for f (n) = 2Ω(logδ n) for some constant δ > 0, then
negative triangle is in n3/2Ω(logδ n) time. Our reduction from APSP to negative triangle is tight
in this case and the hypothesis would imply that APSP also is in n3/2Ω(logδ n) time. If δ = 1, then
APSP is in truly subcubic time. Subsequent work [71] has in fact shown that the consequence of
Theorem 1.5 is true for δ = 1/2, i.e., that APSP is in n3/2Ω(√log n) time.
We also obtain a new quantum algorithm for BMM in the query complexity setting, improving
the previous best by Buhrman and Špalek [13]:
Theorem 1.6. There is an O˜ (min{n1.3L17/30,n1.5L1/4})-query quantum algorithm for computing
the product of two n × n Boolean matrices, where L is the number of ones in the output matrix.
The first time bound of Theorem 1.6 is obtained by simply applying the best known quantum
algorithm for triangle [45] to our generic reduction from matrix product to triangle detection, already improving the previous best [13] output-sensitive quantum algorithm for BMM. The second
Journal of the ACM, Vol. 65, No. 5, Article 27. Publication date: August 2018.
27:6 V. V. Williams and R. R. Williams
time bound is obtained by applying ideas of Lingas [43, 44]. Our results have been subsequently
improved by Le Gall [41] and Jeffery, Kothari, and Magniez [37].
1.1 A Little Intuition
One of our key observations is the counterintuitive result that subcubic algorithms for certain
triangle detection problems can be used to obtain subcubic matrix products in many forms,
including products that are not known to be subcubic. Let us first review some intuition for why
fast triangle detection should not imply fast matrix multiplication, then discuss how our approach
circumvents it. For simplicity, let us focus on the case of Boolean matrix multiplication (BMM)
over OR and AND.
First, note that triangle detection returns one bit, while BMM returns n2 bits. This seems to
indicate that O(n2.99) triangle detection would be useless for subcubic BMM, as the algorithm
would need to be run Ω(n2) times. Furthermore, BMM can determine for all edges if there is a
triangle using the edge, while triangle detection only determines if some edge is in a triangle.
Given our intuitions about quantifiers, it looks unlikely that the universally quantified problem
could be efficiently reduced to the existentially quantified problem. So there appears to be strong
intuition for why such a reduction would not be possible.
However, there is an advantage in calling triangle detection on small graphs corresponding to
small submatrices. Let A and B be n × n matrices over {0, 1}. Triangle detection can tell us if A · B
contains any entry with a 1: Set up a tripartite graph with parts S1, S2 and S3, each containing n
nodes that we identify with the set [n] := {1,...,n}. The edge relation for S1 × S2 is defined by A,
and the edge relation for S2 × S3 is defined by B (in the natural way). A path of length two from
i ∈ S1 to j ∈ S3 corresponds to a 1 in the entry (A · B)[i, j]. Therefore, putting all possible edges
between S1 and S3, there is a triangle in this graph if and only if A · B contains a 1-entry. (Note, we
are already relying on the fact that our addition operation is OR.)
The above reasoning can also be applied to submatrices A and B
, to determine if A · B contributes a 1-entry to the matrix product. More generally, triangle detection can tell us if a product
of two submatrices contains a 1-entry, among just those entries of the product that we have not
already computed. That is, we only need to include edges between S1 and S3 that correspond to
undetermined entries of the product. Hence triangle detection can tell us if submatrices A and B
have any new 1-entries to contribute to the current matrix product so far.
On the one hand, if all possible pairs of submatrices from A and B do not result in finding
a triangle, then we have computed all the 1-entries and the rest must be zeroes. On the other
hand, when we detect a triangle, we determine at least one new 1-entry (i, j) in A · B, and we can
keep latter triangle detection calls from recomputing this entry by simply removing the edge (i, j)
between S1 and S3. By balancing the number of triangle detection subproblems we generate with
the number of 1-entries in A · B, we get a subcubic runtime for matrix multiplication provided
that the triangle algorithm was also subcubic. (In fact, we get an output sensitive algorithm.) With
additional technical effort and a simultaneous binary search method, these ideas can be generalized
to any matrix product where “addition” is a minimum operator.
2 PRELIMINARIES
Throughout the article, for two integers a and b with a < b, we denote by [a,b] the interval of
integers between a and b, i.e., {a, a + 1,...,b}.
Unless otherwise noted, all graphs in the article are directed, and have n vertices and m edges.
Whenever an algorithm in our article uses ∞ or −∞, these can be substituted by numbers of suitably large absolute value. We use ω to denote the smallest real number such that n × n matrix multiplication over an arbitrary field can be done in nω+o(1) additions and multiplications over the field.
Journal of the ACM, Vol. 65, No. 5, Article 27. Publication date: August 2018.
Subcubic Equivalences Between Path, Matrix, and Triangle Problems 27:7
Structures and Extended Structures. We give a general definition encompassing all algebraic
structures for which our results apply. Let R be a set of the form [−M, M] ∪ {−∞, ∞}, where
M ∈ Z+ ∪ {∞}. We call M the maxint of R.
A (min, ) structure over R is defined by a binary operation  : R × R → R. We use the variable
R to refer to a (min, ) structure. We say a (min, ) structure is extended if R contains elements
ε0 and ε1 such that for all x ∈ R, x  ε0 = ε0  x = ∞ and ε1  x = x for all x ∈ R. That is, R is
extended to include two additional elements: ε0, which is a type of annihilator, and ε1, which is a
left identity. We use the variable R¯ to refer to an extended structure.
The elements ε0 and ε1 allow us to define (for every n) an n × n identity matrix In and a n × n
zero matrix Zn over R¯. More precisely, In[i, j] = ε0 for all i  j, In[i,i] = ε1, and Zn[i, j] = ε0 for all
i, j. We shall omit the subscripts of In and Zn when the dimension is clear.
Examples of extended structures R¯ are the (OR,AND) (or Boolean) semiring,5 as well as the
(min, max) and (min, +) semirings 6 (also called subtropical and tropical), and the (min, ≤) structure
used to solve all-pairs earliest arrivals [62, 68]. An example of a structure that is not extended is
the “existence dominance” structure defined with R = Z ∪ {−∞, ∞}, and a  b = 0 if a ≤ b and
a  b = 1, otherwise.
Matrix Products Over Structures. The matrix product of two n × n matrices over R is
(A  B)[i, j] = min
k ∈[n]
(A[i, k]  B[k, j]).
It is easy to verify that for all matrices A over an extended R¯, I  A = A and Z  A = A  Z = F ,
where F [i, j] = ∞ for all i, j. The problem of matrix product verification over an extended structure R¯ is to determine whether mink ∈[n](A[i, k]  B[k, j]) = C[i, j] for all i, j ∈ [n], where A, B,C
are given n × n matrices with entries from R. Although it looks like a simpler problem, matrix
product verification for the (min, +) semiring (for instance) is not known to have a truly subcubic
algorithm.
Negative Triangles Over Structures. The negative triangle problem over R is defined on a weighted
tripartite graph with parts I, J,K. All edge weights are from R. The problem is to detect if there
are i ∈ I, j ∈ J, k ∈ K so that (w(i, k)  w(k, j)) + w(i, j) < 0. Note that if one negates all weights
of edges between I and J, the condition becomes (w(i, k)  w(k, j)) < w(i, j), and since R is symmetric, all edge weights are still from R. Thus, the negative triangle problem can be equivalently
defined with the condition (w(i, k)  w(k, j)) < w(i, j).
In the special case when  = +, the tripartiteness requirement is unnecessary, and the negative
triangle problem is defined on an arbitrary graph with edge weights from R. This holds for the
negative triangle problem over both the (min, +) and Boolean semirings.
2.1 Prior and Related Work
Matrix Products and Path Problems. Matrix multiplication is fundamental to computer science.
The case of multiplying over a field is well known to admit surprisingly fast algorithms using
the magic of subtraction, beginning with the famous O(nlog2 7) time algorithm of Strassen [59].7
After many improvements on Strassen’s original result, the current best upper bound on matrix
multiplication over an arbitrary field is by Le Gall [34], very slightly improving upon the looser
5Observe the Boolean semiring is easily embedded into the R-structure with R = [0, 0] ∪ {−∞, ∞ }, ε0 = ∞ and ε1 = 0,
where x  y = x + y. 6For the (min, +) semiring when R is taken to be [−M, M] ∪ {−∞, ∞ } for a finite integer M, we replace + with the 
operation where a  b = a + b when a + b ∈ [−M, M], a  b = −∞ if a + b < −M and a  b = ∞, otherwise. 7Note that Strassen’s algorithm also works over an arbitrary ring, as it does not use division.
Journal of the ACM, Vol. 65, No. 5, Article 27. Publication date: August 2018. 
27:8 V. V. Williams and R. R. Williams
bound O(n2.373) [72], which together with an independent result by Stothers [22, 58] improved on
the longstanding bound of O(n2.376) of Coppersmith and Winograd [19].
Over algebraic structureswithout subtraction, there has been little progress in the search for truly
subcubic algorithms. These “exotic” matrix products are extremely useful in graph algorithms and
optimization. For example, matrix multiplication over the (max, min)-semiring, with max and min
operators in place of plus and times (respectively), can be used to solve the all-pairs bottleneck
paths problem (APBP) on arbitrary weighted graphs, where we wish to find a maximum capacity
path from s to t for all pairs of nodes s and t. Related work [24, 66] has shown that fast matrix
multiplication (over fields) can be applied to obtain a truly subcubic algorithm over the (max, min)-
semiring, yielding truly subcubic APBP. Matrix multiplication over the (min, +)-semiring (also
known as the distance product) can be used to solve all-pairs shortest paths (APSP) in arbitrary
weighted graphs [28]. That is, truly subcubic distance product would imply truly subcubic APSP,
one of the “Holy Grails” of graph algorithms. The fastest known algorithms for distance product
are a recent randomized n3/2Ω(log1/2 n)
-time solution by Williams [71], a deterministic version with
analogous runtime by Chan and Williams [18], and an O˜ (Mnω ) algorithm where M is the largest
weight in the matrices, due to Alon, Galil, and Margalit [3] (following Yuval [76]). Unfortunately,
the latter algorithm is pseudopolynomial (exponential in the bit complexity) and can only be used
to efficiently solve APSP in special cases such as when the weights in the graph are small [56, 77],
or when the weights are on the nodes instead of the edges [15, 16].
Many over the years have asked if APSP can be solved faster than cubic time. For an explicit
reference, Shoshan and Zwick [56] asked if the distance product of two n × n matrices with entries
in {1,..., M} can be computed in O(n3−δ log M) for some δ > 0. (Note that an APSP algorithm of
similar runtime would follow from such an algorithm.)
Triangles and Matrix Products. Itai and Rodeh [36] were the first to show that triangle detection
can be done with Boolean matrix multiplication.
The trilinear decomposition of Pan [48, 49] implies that any bilinear circuit for computing the
trace of the cube of a matrix A (i.e., tr(A3)) over a ring can be used to compute matrix products
over that ring. So in a sense, algebraic circuits that can count the number of triangles in a graph
can be turned into matrix multiplication circuits. Note, this correspondence relies heavily on the
algebraic circuit model: it is non-black box in an extreme way. (Our reductions are all black box.)
The k Shortest Paths Problem. A natural generalization of the s,t-shortest path problem is
that of returning the first k of the shortest paths between s and t. In the early 1970s, Yen [74]
and Lawler [40] presented an algorithm that solved this problem for directed graphs with nonnegative edge weights; with Fibonacci heaps [30] their algorithm runs in O(k(mn + n2 logn))
time. Eppstein [27] showed that if the paths can have cycles, then the problem can be solved
in O(k + m + n logn) time. When the input graph is undirected, even the k shortest simple paths
problem is solvable inO(k(m + n logn)) time [39]. For directed unweighted graphs, the best known
algorithm for the problem is the O˜ (km√
n) time randomized combinatorial algorithm of Roditty
and Zwick [55]. Roditty [51, 52] noticed that the k shortest simple paths can be approximated
fast, culminating in Bernstein’s [10] O˜ (km/ε) running time for a (1 + ε)-approximation. When the
paths are to be computed exactly, however, the best running time is still the O(k(mn + n2 logn))
time of Yen and Lawler’s algorithm.
Roditty and Zwick [54, 55] showed that the k shortest simple paths can be reduced to k computations of the second shortest simple path, and so anyT (m,n) time algorithm for the second shortest
simple path implies an O(kT (m,n)) algorithm for the k shortest simple paths. The second shortest
simple path always has the following form: take a prefix of the shortest path P to some node x, then
Journal of the ACM, Vol. 65, No. 5, Article 27. Publication date: August 2018.
Subcubic Equivalences Between Path, Matrix, and Triangle Problems 27:9
take a path to some node y on P using only edges that are not on P (this part is called a detour),
then take the remaining portion of P to t. The problem then reduces to finding a good detour.
Verifying a Metric. In the metricity problem, we are given an n × n nonnegative matrix A and
want to determine whether it defines a metric space on [n], that is, that A is symmetric, has 0s on
the diagonal, and its entries satisfy the triangle inequality. Matrices with this property are called
distance matrices, and the metricity problem just asks whether a given matrix is a distance matrix.
Brickell et al. [12] studied a more general problem, called the metric nearness problem (MNP).
MNP is defined with respect to a matrix norm || · ||, and is as follows: given a nonnegative symmetric n × n matrix D with 0s on the diagonal, find a distance matrix D that is closest to D in the
|| · || norm. Brickell et al. [12] gave algorithms for MNP for different choices for the norm. Solving the metricity problem is exactly checking the stopping condition for their algorithms. They
showed some connections of MNP to APSP and asked whether the metricity problem can be solved
faster than APSP. Theorem 4.2 partially answers their question in the sense that subcubic metricity
implies subcubic APSP.
The Maximum Subarray Problem. The Maximum Subarray problem is as follows: given an n × n
matrix A of integers, find i, j, k,l ∈ [n] with i ≤ j, k ≤ l maximizing j
x=i
l
y=k A[x,y], that is, find
a contiguous subarray of A of maximum sum.
Bentley [9] introduced the problem in his Programming Pearls column in 1984. An O(n3) time
algorithm for the maximum subarray problem is a simple exercise in divide-and-conquer. Tamaki
and Tokuyama [60] showed how to use the ideas behind the standard divide-and-conquer solution to reduce the problem to computing the distance product of two matrices. Their reduction implies that if the distance product of two n × n matrices is in T (n) time, then the Maximum Subarray problem for n × n matrices can be solved in time t(n) satisfying the recurrence
t(n) ≤ 4t(n/2) + O(T (n)). The recurrence solves to t(n) ≤ O(
log n
i=0 4i
T (n/2i )). Using this reduction, Tamaki and Tokuyama obtained the first subcubic algorithm for the problem just by applying the slightly subcubic time algorithms for distance product. Moreover, for all T for which
T (n/2) ≤ T (n)/(4 + ε) for some ε > 0, we get that t(n) ≤ O(T (n)), and hence, the Tamaki and
Tokuyama reduction shows that if distance product is in O(n3−ε ) time for some ε > 0, then the
Maximum Subarray problem can also be solved in O(n3−ε ) time.
Prior Reductions of APSP to Other Problems. Roditty and Zwick [53] consider the incremental
and decremental versions of the single source shortest path problem in weighted and unweighted
directed graphs. They show that either APSP has a truly subcubic algorithm, or any data structure for the decremental/incremental single source shortest paths problem must either have been
initialized in cubic time, or its updates must take amortized Ω(n2) time, or its query time must be
Ω(n). They also give a similar relationship between the problem for unweighted directed graphs
and combinatorial algorithms for BMM.
3SUM and its Relatives. In this work, we build a complexity class of hard problems based on the
difficulty of APSP. There is another well-known class of hard problems built around the 3SUM
problem: Given a set of numbers, are there three that sum to zero? There is a simple quadratic-time
algorithm for this problem, and it is a major open problem to find a n1.99 time algorithm. Gajentaan
and Overmars [33] showed that for many problems Π solvable in quadratic time, one can reduce
3SUM to Π in such a way that a subquadratic algorithm for Π implies one for 3SUM. Hence under
the conjecture that the 3SUM problem is hard to solve faster, many other Π are also hard.8
8Sometimes Π is defined to be 3SUM-hard if “Π is in subquadratic time implies 3SUM is in subquadratic time.” This definition
leaves something to be desired: If 3SUM is in subquadratic time, then all problems are 3SUM-hard, and if 3SUM is not
in subquadratic time, then no subquadratic problem is 3SUM-hard. Hence, the 3SUM-hardness of some problems would
Journal of the ACM, Vol. 65, No. 5, Article 27. Publication date: August 2018.   
27:10 V. V. Williams and R. R. Williams
Most reductions showing 3SUM-hardness have been reductions between decision problems.
One strength of our work is that we reduce function problems like APSP to decision problems
like negative weight triangle. A significant advance in the theory of 3SUM-hardness was made by
Patrascu [50] who showed how to reduce 3SUM to interesting function problems, such as triangle
listing in sparse graphs. At the heart of his results is a reduction from 3SUM to Convolution3SUM: given an array A of n numbers, determine if there are i, j such that A[i] + A[j] = A[(i + j)
(mod n)]. Convolution-3SUM has an obvious quadratic-time algorithm: simply try all pairs i, j!
Theorem 2.1 (Patrascu [50]). If Convolution-3SUM is in O(n2/f 2 (n · f (n))) time, then 3SUM on
n numbers is in O(n2/f (n)) time.
In the notation of this article, the above theorem says that 3SUM ≡2 Convolution-3SUM, i.e., the
two problems are subquadratic equivalent.
3 SUBCUBIC REDUCIBILITY
Here, we formally define the notion of subcubic reducibility used in this article and prove a few
consequences of it. Recall that an algorithm with oracle access to B has special workspace in memory
reserved for oracle calls, and at any step in the algorithm, it can call B on the content of the special
workspace in one unit of time and receive a solution to B in the workspace.
Let Σ be an underlying alphabet. We define a size measure to be any function m : Σ →
N ∪ {∞, −∞}. In this article, the size measure on n-node weighted graphs with integer weights
from [−M, M] ∪ {−∞, ∞} (or n × n matrices with entries in this range) is typically taken to be
n · (log M)
c for a fixed constant c. That is, we measure the complexity of a square matrix by its
dimension times some polynomial in the bit complexity of the weights. This notion of measure is
easier to understand in the context of the following definition.
Definition 3.1. LetA and B be computational problems with a common size measurem on inputs.
We say that there is a subcubic reduction from A to B if for every constantε > 0 there are constants
δ > 0 and d, and an algorithm A with oracle access to B, satisfying three properties:
• For every instance x of A, A(x) solves the problem A on x.
• A runs in O(m3−δ ) time on instances of size m.
9
• For every instance x of A of size m, let mi be the size of the ith oracle call to B in A(x).
Then,
i m3−ε i ≤ d · m3−δ .
We use the notation A ≤3 B to denote the existence of a subcubic reduction from A to B and
define A ≡3 B if A ≤3 B and B ≤3 A. In such a case, we say that A and B are subcubic-equivalent.
There is a natural extension of the concept to O(nq ) running times, for any constant q ≥ 1,
by replacing all occurrences of 3 in the above definition with q. For such reductions, we denote
their existence by A ≤q B and say there is a sub-q reduction from A to B, for values of q such as
“quadratic,” “cubic,” “quartic,” and so on.
First let us observe that the reducibility relation is transitive.
Proposition 1. Let A, B, C be problems so that A ≤q B and B ≤q C. Then A ≤q C.
depend on the complexity of 3SUM itself. Note this is not the definition of Reference [33]—they give a reducibility notion
similar to but weaker than ours.
9Note that for the size measure n · (log M)
c , this translates to n3−δ · (log M)(3−δ )c time.
Journal of the ACM, Vol. 65, No. 5, Article 27. Publication date: August 2018.  
Subcubic Equivalences Between Path, Matrix, and Triangle Problems 27:11
Proof. By definition, we have:
(1) For every ε > 0 there exist constants δ > 0 and dA and an algorithm PA,ε for A that on
all instances x of size n runs in O(nq−δ ) time and makes oracle calls to B on instances
x1,..., xt of sizes |x1 |,... |xt | where
i |xi |
q−ε ≤ dA · nq−δ .
(2) For every ε > 0 there exist constants δ  > 0 and dB and an algorithm PB,ε for B that on
all instances y of size m runs in O(mq−δ
) time and makes oracle calls to C on instances
y1,...,yr of sizes |y1 |,..., |yr | where
i |yi |
q−ε ≤ dB · mq−δ .
We will show that:
(3) For every ε > 0 there exist constants δ  > 0 and d and an algorithm Pε for A that on
all instances z of size n runs in O(nq−δ ) time and makes oracle calls to C on instances
z1,..., zw of sizes |z1 |,..., |zw | where
i |zi |
q−ε
≤ d · nq−δ
.
Let ε > 0 be given. Consider PB,ε and let δ  > 0 be the value corresponding to ε
, as in 2. Pick
ε = δ 
. Consider algorithm PA,ε and let δ > 0 be the value corresponding to ε, as in 1. Replace each
oracle call on an instance xi from algorithm PA,ε running on input x with a call to PB,ε on xi . This
forms a new algorithm Pε that makes oracle calls to C.
The running time of the Pε is
O

nq−δ +

i
|xi |
q−δ


.
As we picked ε = δ 
,

i |xi |
q−δ
=
i |xi |
q−ε ≤ dA · nq−δ (from 1), and the runtime of Pε is
O(nq−δ ).
From 2, for each xi , the algorithm makes oracle calls to C on instances xi1,..., xir , where

j |xi,j |
q−ε ≤ dB · |xi |
q−δ . Hence,

ij
|xij |
q−ε
≤

i
dB · |xi |
q−δ
= dB

i
|xi |
q−ε ≤ (dAdB )nq−δ ,
where the last inequality is from 1. We can set δ  = δ and d = dAdB, and so A ≤q C.
Corollary 3.1. The relation ≤q is a partial order, and the relation ≡q is an equivalence relation.
Now let us verify that the definition gives us the property we want. In the following, let A and B
be computational problems on n × n matrices with entries in [−M, M] (or equivalently, weighted
graphs on n nodes).
Proposition 2. IfA ≤3 B, then a truly subcubic algorithm for B implies a truly subcubic algorithm
for A.
Proof. If there is an O(m3−εpoly log M)-time algorithm for B for instances with measure m,
then running the algorithm for B along with the reduction yields an algorithm for A with runtime
O(m3−δ +
i m3−ε i poly log M) ≤ O(m3−δ poly log M).
Strongly Subcubic Reductions. All subcubic equivalences proved in this article have one additional
property in their reductions: the number of oracle calls and the sizes of oracle calls depend only on
the input and not on the parameter ε. (In some other reductions, such as the example below, this
is not the case.) Let us define a reduction with this property to be a strongly subcubic reduction.
Definition 3.2. LetA and B be computational problems with a common size measurem on inputs.
We say that there is a strongly subcubic reduction from A to B if there exists an algorithm A with
oracle access to B and a constant d, satisfying the following properties:
Journal of the ACM, Vol. 65, No. 5, Article 27. Publication date: August 2018.               
27:12 V. V. Williams and R. R. Williams
• For every instance x of A, A(x) solves the problem A on x.
• A runs in O(m3−γ ) time on instances of size m for some γ > 0 independent of m.
• For every instance x of A of size m, let mi be the size of the ith oracle call to B in A(x).
Then, for every ε > 0, there exists a δ > 0 such that
i m3−ε i ≤ d · m3−δ .
We now show that these stronger reductions have the nice quality that, with respect to polylogarithmic (or even slightly higher) improvements, running times are preserved.
Theorem 3.1. If there is a strongly subcubic reduction from A to B, then
• If the size measure is m = n(log M)
r /3, then for all c > 0, an O(n3 (log M)
r /(logn +
log log M)
c ) time algorithm for B implies an O(n3 (log M)
3r /(logn + log log M)
c ) time algorithm for A. If the size measure is n, then an O(n3/ logc n) algorithm for B implies an
O(n3/ logc n) algorithm for A.
• If the size measure is n, then for all α > 0, an n3/2Ω(logα n) algorithm for B implies an
n3/2Ω(logα n) algorithm for A.
Proof. First let n be the input size measure. First, we show that

i
n3
i ≤ d · n3
. (1)
A strongly subcubic reduction gives us a fixed algorithm such that for all sizes n, the number of
oracle calls and the sizes of oracle calls {ni} depend only on the input (and not the parameter ε).
Then, for all ε > 0, there is a δ > 0 satisfying

i
n3−ε i ≤ dn3−δ < dn3
.
Since d, {ni}, and n are independent of ε, this means that for every fixed set {ni}, d, and n, we can
take the limit on both sides of the above inequality as ε → 0. We obtain that for every n and every
set of oracle call sizes {ni} on an input of size n,

i n3
i ≤ dn3.
Now consider an algorithm for B that runs in O(n3/ logc n) time. Then, an algorithm for A that
uses the reduction calling B as an oracle would run inO(n3−γ +
i n3
i / logc ni ) time for some γ > 0.
Let a < δ/3. Then,

i
n3
i / logc ni =

i:ni <na
n3
i / logc ni +

i:ni ≥na
n3
i / logc ni,
which is at most
O


n3−δ · n3a +

i:ni ≥na
n3
i / logc (na )



,
since the number of oracle calls is at most O(n3−δ ). The first term is n3−ε
for some ε > 0, by our
choice of a. By Equation (1), we have
O


n3−ε
+

i:ni ≥na
n3
i /(ac · logc n)



≤ O(n3
/ logc n).
Suppose now that the input measure m is m = n(log M)
r /3. Then, an O(n3 (log M)
r /(logn +
log log M)
c ) time algorithm for B is an O(m3/(logm)
c ) time algorithm for B. By the argument above, using m instead of n, we obtain that a strongly subcubic reduction with respect
to measure m, implies that there is also an O(m3/(logm)
c ) time algorithm for A, and hence an
O(n3 (log M)
r /(logn + log log M)
c ) time algorithm for A.
Journal of the ACM, Vol. 65, No. 5, Article 27. Publication date: August 2018.              
Subcubic Equivalences Between Path, Matrix, and Triangle Problems 27:13
For the proof of the second item, suppose the size measure is n and consider an algorithm for
B that runs in n3/2c logα n time. Then, an algorithm for A that uses the reduction calling B as an
oracle would run in O(n3−γ +
i n3
i /2c logα ni ) time for some γ > 0. Similar to above, let a < δ/3
and use that

i
n3
i /2c logα n =

i:ni <na
n3
i /2c logα ni +

i:ni ≥na
n3
i /2c logα ni ,
which is at most
O


n3−δ · n3a +

i:ni ≥na
n3
i /2c logα na 


.
The first term is n3−ε
for some ε > 0. By Equation (1), the second term is O(n3/2caα logα n ) =
n3/2Ω(logα n)
.
It can be shown that strongly subcubic reductions are necessary for Theorem 3.1 to hold. If the
sizes of oracle calls or their number depend on ε, then one can find cases where polylog factors
are diminished in the algorithm for A. (In fact, the reduction below of Matoušek is one example.)
Subcubic reductions were certainly implicit in prior work (even in the generic setting we give
here) but have not been studied systematically. For one example, Matoušek [46] showed that computing dominances in Rn between pairs of n vectors can be done in O(n(3+t)/2) time, where t ≥ ω
is any upper bound on the exponent ω of n × n integer matrix multiplication. For the current
best upper bound on the matrix multiplication exponent ω < 2.373, Matoušek’s algorithm runs in
O(n2.687) time. The algorithm works by making O(n3/2/nt /2) calls to n × n integer matrix multiplication. (Note this is not a strongly subcubic reduction, since the number of calls depends on t.)
Notice that for any t < 3, the running time O(n(3+t)/2) is truly subcubic. Hence, we can say
Dominances in Rn ≤3 Integer Matrix Multiplication.
Another example is that of 3SUM-hardness in computational geometry, as mentioned in Section 2.1.
A proof that a problem Π is 3SUM-hard implies 3SUM ≤2 Π, but the notion of reduction used
in Reference [33] is weaker than ours. (They only allow O(1) calls to the oracle for Π.)
4 EQUIVALENCES BETWEEN PROBLEMS ON GENERIC STRUCTURES
A generic approach to computing fast (min, ) matrix products (for an arbitrary binary operation
) would be of major interest. Here, we prove truly subcubic equivalences between matrix products, negative triangles and matrix product verification for (min, ) structures. (For definitions,
see the Preliminaries.) For simplicity, most of our theorems will assume T (n)-time algorithms;
they can easily be extended to handle dependencies on the weights. However, all of the proofs
have the property that if the original maximum weight absolute value was W , then the maximum
weight absolute value after the reduction is poly(nW ), and hence the subcubic reduction properties
are preserved.
Reminder of Theorem 1.2. Let R¯ be an extended (min, ) structure. The following problems over
R¯ either all have truly subcubic algorithms or none of them do:
• Negative Triangle Detection. Given a tripartite n-node graph with node partitions I, J,K
and weight function w : E → R, find nodes i ∈ I, j ∈ J, k ∈ K such that (w(i, k)  w(k, j)) +
w(i, j) < 0.
• Matrix Product. Given two n × n matrices A, B with entries from R, compute the product of
A and B over R¯.
• Matrix Product Verification. Given three n × n matrices A, B, C with entries from R, determine if the product of A and B over R¯ is C.
Journal of the ACM, Vol. 65, No. 5, Article 27. Publication date: August 2018.        
27:14 V. V. Williams and R. R. Williams
4.1 Matrix Product Verification Implies Negative Triangle Detection
We start by showing that matrix product verification can solve the negative triangle problem over
any extended structure R¯ in the same asymptotic runtime. For two problems A and B, we write
A ≤3 B to express that there is a subcubic reduction from A to B. (For formal definitions, see Section 3.)
Theorem 4.1 (Negative Triangle Over R ≤ ¯ 3 Matrix Product Verification Over
R¯). Suppose matrix product verification over R¯ can be done in time T (n). Then the negative triangle
problem for graphs over R¯ can be solved in O(T (2n)) time.
Proof. From the tripartite graphG = (I ∪ J ∪ K, E) given by the negative triangle problem over
R¯, construct matricesA, B,C as follows. For each edge (i, j) ∈ (I × J) ∩ E setC[i, j] = −w(i, j). Similarly, for each edge (i, k) ∈ (I × K) ∩ E set A[i, k] = w(i, k) and for each edge (k, j) ∈ (K × J) ∩ E
set B[k, j] = w(k, j). When there is no edge in the graph, the corresponding matrix entry in A or B
is set to ε0 and in C it is set to ∞. The problem becomes to determine whether there are i, j, k ∈ [n]
so that A[i, k]  B[k, j] < C[i, j]. Let A be the n × 2n matrix obtained by concatenating A to the
left of the n × n identity matrix I. Let B be the 2n × n matrix obtained by concatenating B on top of
C. Then, A  B is equal to the componentwise minimum of A  B andC. One can easily complete
A
, B
, and C to be square 2n × 2n matrices, by concatenating:
• an n × 2n matrix of all ε0’s to the bottom of A
,
• a 2n × n matrix of all ε0’s to the right of B
, and
• n columns of all ε0’s and n rows of all ε0’s to the right and bottom of C, respectively.
Notice that all entries ofA
, B
,C are from R, since R is symmetric and containsε0,ε1, and ∞. Run
matrix product verification on A
, B
,C. Suppose there are i, j so that mink (A
[i, k]  B
[k, j])
C[i, j]. Then, since
min
k (A
[i, k]  B
[k, j]) = min{C[i, j], min
k (A[i, k]  B[k, j])} ≤ C[i, j],
there must exists a k ∈ [n] so that A[i, k]  B[k, j] < C[i, j]. In other words, i, k, j is a negative
triangle over R¯. If, however, for all i, j we have mink (A
[i, k]  B
[k, j]) = C[i, j], then for all i, j
we have mink (A[i, k]  B[k, j]) ≥ C[i, j], and there is no negative triangle.
4.2 Negative Triangle Detection Implies Matrix Multiplication
Next, we show that from negative triangle detection over a (min, ) structure R, we can obtain
the full matrix product over R. Specifically, we prove the following.
Theorem 4.2 (Matrix Product Over R ≤3 Negative Triangle Over R). Let T (n) be a function so that T (n)/n is nondecreasing. Suppose the negative triangle problem over R in an n-node
graph can be solved in T (n) time. Then, the product of two n × n matrices over R can be performed in
O(n2 · T (n1/3) logW ) time, where W is the maxint of R.
Before we proceed, let us state some simple but useful relationships between triangle detecting,
finding, and listing.
Lemma 4.1 (Folklore). Let T (n) be a function so that T (n)/n is nondecreasing. If there is a T (n)
time algorithm for negative triangle detection over R on a graph G = (I ∪ J ∪ K, E), then there is an
O(T (n)) algorithm that returns a negative triangle over R in G if one exists.
Proof of Lemma 4.1. The algorithm is recursive: it proceeds by first splitting I, J, andK each into
two roughly equal partsI1 and I2, J1 and J2, and K1 and K2. Then, it runs the detection algorithm on
all eight induced subinstances (Ii, Jj ,Kk ), i, j, k ∈ {1, 2}. If none of these return “yes,” then there is
Journal of the ACM, Vol. 65, No. 5, Article 27. Publication date: August 2018.  
Subcubic Equivalences Between Path, Matrix, and Triangle Problems 27:15
no negative triangle in G. Otherwise, the algorithm recurses on exactly one subinstance on which
the detection algorithm returns “yes.” The base case is when |I | = |J| = |K| = 1 and then one just
checks whether the three nodes form a negative triangle in O(1) time. The running time becomes
T 
(n) = 8T (n/2) +T 
(n/2),T 
(1) = O(1).
If T (n) = n f (n) for some nondecreasing function f (n), then T (n) = 2 n
2 f (n) ≥ 2 n
2 f (n/2) =
2T (n/2). Hence, T (n/2i ) ≤ T (n)/2i for all positive integers i, and the recurrence above solves to
T 
(n) = O(T (n)).
It will be useful in our final algorithm to have a method for finding many triangles, given an
algorithm that can detect one. We can extend Lemma 4.1 in a new way, to show that subcubic
negative triangle detection implies subcubic negative triangle listing, provided that the number of
negative triangles to be listed is subcubic.
Theorem 4.3 (Negative Triangle Listing Over R ≤3 Negative Triangle Over R). Let δ >
0 be fixed, and let Δ = O(n3−δ ). Suppose there is a truly subcubic algorithm for negative triangle
detection over R. Then there is a truly subcubic algorithm that lists Δ negative triangles over R in
any graph with at least Δ negative triangles.
Proof of Theorem 4.3. Let P be an O(n3−ε logc M) time algorithm for negative triangle over R
for ε > 0 and where M is the maxint of R. Let Δ = O(n3−δ ) for δ > 0. Given a 3n-node tripartite
graph G = (I ∪ J ∪ K, E) with at least Δ negative triangles over R, we provide a procedure to list
Δ negative triangles over R.
We partition the nodes in I, J,K into Δ1/3 parts, each of size O(n/Δ1/3). For all Δ triples I  ⊂
I, J  ⊂ J,K ⊂ K of parts, run P in O(Δ(n/Δ1/3)
3−ε logc M) time overall to determine all triples
that contain negative triangles over R.
On the triples that contain negative triangles, we run the following procedure. We begin by
creating a list L0 consisting of all the triples {I 
, J 
,K
} above with |I 
| = |J 
| = |K
| = O(n/Δ1/3)
for which P determined that they contain a negative triangle over R.
In general, we will iterate through i and build a list Li , for each integer i ranging from 0 to
log n
Δ1/3 , so that Li contains a list of disjoint triples{I 
, J 
,K
} with |I 
| = |J 
| = |K
| = O(n/(2i
Δ1/3))
so that each triple contains a negative triangle over R. In iteration i, we will process list Li , building
list Li+1 as follows. Li+1 is initialized to be the empty list. Now, we process each triple in Li in turn.
Let {I 
, J 
,K
} be the next triple in Li to be processed. Split I 
, J 
, and K each into two roughly
equal halves. On each of the eight possible triples of halves, run P and determine the triples of
halves that contain negative triangles. Place each such triple into the list Li+1. If |Li+1 | becomes
≥Δ, then run the algorithm from Lemma 4.1 using P on the triples in Li+1 (there are at most Δ + 7
of them), and return the triangles found. In this case, one is guaranteed to return at least Δ (and at
most Δ + 7) different negative triangles over R. If, however, |Li+1 | < Δ, then move on to the next
triple in Li looking for more triples of halves to add to Li+1. If Li becomes empty, then increment
i to i + 1 and process Li+1. If i becomes equal to log n
Δ1/3 , then each triple in Li contains exactly
three nodes, and since the algorithm maintains the invariant that |Li | < Δ (otherwise, it returns
≥Δ triangles), we can return that the instance does not contain at least Δ triangles. (Alternatively,
one can return a maximal subset of the negative triangles in Li .)
For each i, the list Li contains triples that contain O( n
2iΔ1/3 ) nodes, and each Li has O(Δ) triples.
Therefore, for each iteration i, the running time isO(Δ · ( n
2iΔ1/3 )
3−ε logc M). Since ε < 3, the overall
runtime becomes asymptotically
Δ
 n
Δ1/3
3−ε
logc M ·

i
 1
23−ε
i
= O 
Δε /3
n3−ε logc M

.
Journal of the ACM, Vol. 65, No. 5, Article 27. Publication date: August 2018.  
27:16 V. V. Williams and R. R. Williams
When Δ ≤ O(n3−δ ), the runtime is
O(n3−ε+3ε /3−δ ε /3 logc M) = O(n3−δ ε /3 logc M),
which is truly subcubic for any ε, δ > 0.
Next, we show that fast negative triangle detection over R implies a fast algorithm for finding
many edge-disjoint negative triangles over R. Consider a tripartite graph with parts I, J,K. We
say a set of triangles T ⊆ I × J × K in the graph is I J-disjoint if for all (i, j, k) ∈ T , (i
, j

, k
) ∈ T ,
(i, j)  (i
, j

).
Lemma 4.2. Let T (n) be a function so that T (n)/n is nondecreasing. Given a T (n) algorithm
for negative triangle detection over R, there is an algorithm A that outputs a maximal set L of
I J-disjoint negative triangles over R in a tripartite graph with distinguished parts (I, J,K), in
O(T (n1/3)n2) time. Furthermore, if there is a constant ε : 0 < ε < 1 such that for all large enough
n, T (n) ≥ T (21/3
n)/(2(1 − ε)), then there is an output-sensitive O(T (n/|L|
1/3)|L|)-time algorithm.10
In particular, Lemma 4.2 implies that given any graph on n nodes, we can determine those pairs
of nodes that lie on a negative triangle in O(T (n1/3)n2) time. The condition required for the output
sensitive algorithm holds for all subcubic polynomials, but it does not necessarily hold for runtimes
of the form n3/f (n) with f (n) = no(1)
. In the special case whenT (n) is Θ(n3/ logc n) for a constant
c, the output sensitive algorithm only multiplies a log |L| factor to the runtime. Notice also that
although we state Lemma 4.2 only for runtimes in terms of n, one can modify its proof to obtain a
similar result for runtimes that also depend on the maxint of R.
Proof. Algorithm A maintains a global list L of negative triangles over R, which is originally
empty and will be the eventual output of the algorithm. Let a be a parameter to be set later. At each
point the algorithm works with a subgraph G˜ of the original graph, containing all of the nodes, all
of the edges between I and K and between J and K but only a subset of the edges between I and
J. In the beginning G˜ = G and at each step A removes an edge from G˜.
Algorithm A starts by partitioning each set I, J,K into na parts where each part has at most
n(1−a)
 nodes each. It iterates through all n3a possible ways to choose a triple of parts (I 
, J 
,K
),
so that I  ⊂ I, J  ⊂ J, and K ⊂ K. For each triple (I 
, J 
,K
) in turn, it considers the subgraph G
of G˜ induced by I  ∪ J  ∪ K and repeatedly uses Lemma 4.1 to return a negative triangle over
R. Each time a negative triangle (i, j, k) is found in G
, the algorithm adds (i, j, k) to L, removes
edge (i, j) from G˜ and attempts to find a new negative triangle in G
. This process repeats until G
contains no negative triangles, in which case algorithm A moves on to the next triple of parts.
Now, let us analyze the running time ofA. For a triple of parts (I 
, J 
,K
) let eIJK be the number
of edges (i, j) in I  × J  that are found in the set of I 
J 
-disjoint negative triangles when (I 
, J 
,K
)
is processed by A. Let T (n) be the complexity of negative triangle detection over R. Then, the
runtime can be bounded from above as
O



all n3a triples I
, J
,K

eIJK · T (n1−a ) +T (n1−a )



. (2)
Note that the sum of all eIJK is at most n2, since if edge (i, j) ∈ I  × J  is reported to be in a
negative triangle, then it is removed from the graph. Hence there is a constant c > 0 such that
10The condition is satisfied, for instance, when T (n)/n3−δ is nonincreasing for some δ > 0.
Journal of the ACM, Vol. 65, No. 5, Article 27. Publication date: August 2018.     
Subcubic Equivalences Between Path, Matrix, and Triangle Problems 27:17
Equation (2) is upper bounded by
c · T (n1−a ) ·

all n3a triples I
, J
,K

eIJK + 1
	 ≤ c · T (n1−a ) ·



n3a +

all n3a triples I
, J
,K
eIJK



≤ c · T (n1−a ) · (n3a + n2).
Setting a = 2/3, the runtime becomes O(n2
T (n1/3)).
To get an output-sensitive algorithm A
, we make the following modification. For all i =
1,..., 2 logn, run algorithm A with a := i/(3 logn). In stage i, stop the algorithm when the list
L contains at least 2i I J-disjoint negative triangles. Let Li−1 be the list of I J-disjoint negative triangles found in stage i − 1. If |L| = |Li−1 |, then return L; otherwise, set Li := L and continue with
stage i + 1.
The runtime of A is
log

|L |
i=1
T (n1−i/(3 log n)
) ·



n3i/(3 log n) +

all n3i /(3 logn) triples I
, J
,K
(eIJK )



≤
log

|L |
i=1

ni/ log n + 2i

· T (n1−i/(3 log n)
) = 2
log

|L |
i=1
2i
T (2log n−i/3) = 2
log

|L |
i=1
2i
T (n/2i/3).
Since there is a constant ε < 1 so that for all n, T (n) ≥ T (21/3
n)/(2(1 − ε)), then for all i,
2i
T (n/2i/3) ≤ 2i+1 (1 − ε)T (n/2(i+1)/3), and hence the runtime is bounded by
O


T (n/|L|
1/3)|L|
log

|L |
i=0
(1 − ε)
i 


= O(T (n/|L|
1/3)|L|).
We are now ready to prove Theorem 4.2, via a simultaneous binary search on entries of the
matrix product. The “oracle” used for binary search is our algorithm for I J-disjoint triangles.
Proof of Theorem 4.2. Let A and B be the given n × n matrices, and letC = A  B be the matrix
to be computed. LetW be the maxint of R. We will binary search on [−W ,W ] for the finite entries
of C.
We maintain two n × n matrices S and H, initializing them as S[i, j] := −W and H[i, j] := W + 1
for all i, j ∈ [n]. The algorithm proceeds in iterations. It maintains the invariant that for every
i, j ∈ [n], either C[i, j] ∈ {−∞, ∞} or S[i, j] ≤ C[i, j] < H[i, j], that is, S[i, j] and H[i, j] are a lower
and upper bounds on the finite product entries. The invariant is true at the onset of the algorithm
by our initialization.
In each iteration, we create a complete tripartite graph G on partitions I, J, and K. The edges
of G have weights w(·) so that for i ∈ I, j ∈ J and k ∈ K, w(i, k) = A[i, k], w(k, j) = B[k, j] and
w(i, j) = −(S[i, j] + H[i, j])/2 if (S[i, j] + H[i, j])/2 ≤ W and w(i, j) = −∞, otherwise.
Notice that the weights of the edges ofG are always in R, since we guaranteed that −w(i, j) ≤ W
or w(i, j) = −∞, and in addition, if w(i, j)  {−∞, ∞}, then we have by the invariant −w(i, j) =
(S[i, j] + H[i, j])/2≥(−W + C[i, j])/2≥−W .
Now, using the algorithm from Lemma 4.2 on G, we generate a list L of I J-disjoint negative
triangles over R for G in O(T (n)) time.
From this we modify S and H as follows. If (i, j) appears in a triangle in L fori ∈ I, j ∈ J, then we
set H[i, j] := −w(i, j) if w(i, j) is finite, and make no changes otherwise. If (i, j) does not appear in
a triangle in L, then we set S[i, j] := −w(i, j) if w(i, j) is finite, and we make no changes otherwise.
Notice that the invariant is still satisfied. Indeed, if (i, j) appears in L, then there is some k ∈
K for which A[i, k]  B[k, j] < −w[i, j], and hence in particular, C[i, j] = mink A[i, k]  B[k, j] <
Journal of the ACM, Vol. 65, No. 5, Article 27. Publication date: August 2018.                
27:18 V. V. Williams and R. R. Williams
−w[i, j]. Otherwise, if (i, j) is not in L, then for all k ∈ K, A[i, k]  B[k, j] ≥ −w(i, j), and hence in
particular C[i, j] ≥ −w(i, j).
We continue iterating until for all i, j, H[i, j] ≤ S[i, j] + 1. Each iteration shrinks the interval
[S[i, j],H[i, j]) for every i, j roughly in half, except for the special case w(i, j) = −∞ when the
interval is not changed. However,w(i, j) = −∞ if and only if S[i, j] = W and H[i, j] = W + 1, so the
loop ending condition H[i, j] ≤ S[i, j] + 1 is satisfied. Hence the number of iterations is O(logW ).
First, we show that we can easily tell whether some C[i, j] is not finite. Notice that for any
finite choice of w(i, j), if C[i, j] = ∞, then (i, j) will not appear in L, and if C[i, j] = −∞, then (i, j)
will appear in L. After the last iteration, if C[i, j] = −∞, then H[i, j] = −W , and if C[i, j] = ∞,
then S[i, j] = W and H[i, j] = W + 1. Thus, we can set C[i, j] = −∞ for all i, j with H[i, j] = −W .
To determine the (i, j) with C[i, j] = ∞, we can run one more call to negative triangle detection
usingw(i, j) = −∞ for all (i, j). If some (i, j) with S[i, j] = W and H[i, j] = W + 1 is in L, then there
exists some k ∈ K such thatw(i, k)  w(k, j) < ∞, and hence C[i, j] = mink w(i, k)  w(k, j) ≤ W .
However, by the invariant,C[i, j] ≥ W and we can conclude thatC[i, j] = W . Otherwise if S[i, j] =
W and H[i, j] = W + 1 and (i, j) is not in L, then C[i, j] = ∞.
After determining all infinite entries, for all i, j for which −W ≤ S[i, j] ≤ W , we set C[i, j] :=
S[i, j]. To see that this correctly computes C, notice that after the last iteration the algorithm has
determined (by the invariant) that for every pair of nodes i, j with finite C[i, j], S[i, j] ≤ C[i, j] <
H[i, j] ≤ S[i, j] + 1. Since the finite values C[i, j] are integers, we can safely set C[i, j] := S[i, j] for
all (i, j).
Corollary 4.1. Suppose the negative triangle problem over R is in O(n3/ logc n) time for some
constantc. Then the product of n × n matrices over R can be done inO((logW )n3/ logc n) time where
W is the maxint of R.
An important special case of matrix multiplication is that of multiplying rectangular matrices.
Negative triangle detection can also give a speedup in this case as well.
Theorem 4.4. Suppose the negative triangle problem over R is in T (n) time. Let m,n,p be such
that mp ≤ n3 and √p ≤ m ≤ p2. Then, two matrices of dimensions m × n and n × p can be multiplied
over R in O(mp · T (n1/3) logW ) time, where W is the maxint of R.
If T (n) = nc , then the runtime is O(mp(n)
c/3). Notice that if c < 3 and if p = n(3−c )/3, then the
runtime would beO(mn), for allm ∈ [n(3−c )/6,n2(3−c )/3]. That is, for any c < 3, there is some p ≥ nε
such that multiplication of m × n and n × p matrices over R can be done optimally, for any m ∈
[
√p,p2]. Theorem 4.4 follows from a more general lemma:
Lemma 4.3. Let T (n) be a function so that T (n)/n is nondecreasing. Suppose there is a T (n) time
algorithm for negative triangle detection over R in an n node graph. Let W be the maxint of R. Let A
be an m × n matrix over R and let B be an n × p matrix over R, so that their product over R does not
contain −∞ entries. Then:
• There is an algorithm that computes  finite entries of the product over R of A and B in O( ·
T ((mnp/)
1/3) logW ) time, whenever  ≤ m3,n3,p3.
• If there is a constantε : 0 < ε < 1 such that for all large enough n,T (n) ≥ T (21/3
n)/(2(1 − ε)),
then there is anO( · T ((mnp/)
1/3) logW )-time algorithm for computing the product ofAand
B over R, where  is the number of finite entries in the product matrix, whenever mp ≤ n3 and
√p ≤ m ≤ p2.
Proof. Following the ideas from Theorem 4.2,  distinct I J-disjoint negative triangles over R
can be found in
O(( + a3) · T ((mnp)
1/3
/a))
Journal of the ACM, Vol. 65, No. 5, Article 27. Publication date: August 2018.          
Subcubic Equivalences Between Path, Matrix, and Triangle Problems 27:19
time, where a is a bucketting parameter such that a ≤ m,n,p. Since 1/3 ≤ m,n,p in the first bullet
of the theorem, we set a = 1/3, and we get a runtime of O( · T ((mnp/)
1/3)) for finding  distinct
I J-disjoint negative triangles. Armed with this algorithm and given an m × n matrix A and an
n × p matrix B over R with no −∞s, we create the corresponding instance of negative triangle
listing as in Theorem 4.2, where the I × K edge weights are taken from A, the K × J edge weights
are taken from B, and the I × J weights are all ∞. On this graph, we run the I J-disjoint negative
triangles algorithm from above and determine the indices of  finite entries of the product. (Since
the product does not contain −∞ entries, all entries that are < ∞ will be finite.)
Now, we start a binary search procedure as follows: for each of the  finite entries (i, j) that
we have found above, we set S[i, j] = −W ,H[i, j] = W + 1, and for all other entries we set S[i, j] =
W [i, j] = W + 1. Then, we let the binary search procedure run exactly as in Theorem 4.2, except
that in each search we use the procedure that lists  I J-negative triangles. These  triangles are
guaranteed to be the original  ones, as any other (i, j) will have w(i, j) set to −∞, and thus no
triangles going through (i, j) can be negative. The number of binary search steps is O(logW ) and
each one takes O( · T ((mnp/)
1/3)) time and we have proven the first bullet.
To get an output-sensitive algorithm as in the second bullet of the theorem statement, for each
i = 1,..., log(mp), we set a = 2i/3 and attempt to list 2i negative triangles over R as in the first
bullet. One of these attempts will compute all  finite entries as there will be some i with 2i ≥
 > 2i−1. We can run the algorithm from the first bullet, since for each i, 2i ≤ mp ≤ m3,n3,p3. The
running time together with the binary search is now
log

(mp)
i=1
2i · T ((mnp/2i
)
1/3) logW .
Assuming there is an ε < 1 so that for all n, T (n) ≥ T (21/3
n)/(2(1 − ε)), then for all i,
2i
T ((mnp)
1/3
/2i/3) ≤ 2i+1 (1 − ε)T ((mnp)
1/3
/2(i+1)/3).
Hence the running time is O( · T ((mnp/)
1/3) logW ), where  is the number of finite entries in
the output.
4.3 Strongly Polynomial Subcubic Reductions
Applying a very useful randomization trick by Chan [14] in a new way, the logarithmic dependence
on M in our generic reductions can be replaced with a polylogarithmic dependence on n. That is,
the running time of our reductions can be made strongly polynomial, independent of the weights.
The only reduction we need to improve is the one from matrix product to negative triangle:
Theorem 4.5 (Matrix Product Over R ≤3 Negative Triangle Over R, Strongly Polynomial). LetT (n) be a function so thatT (n)/n2 is nondecreasing. Suppose the negative triangle problem
over R in an n-node graph can be solved in T (n) time. Then the product of two n × n matrices over R
can be computed in O(n2 · T (n1/3) logn) time, with high probability.
In this section, we provide a Monte Carlo algorithm as in the statement of the theorem; however,
it is not hard to obtain a Las Vegas algorithm as well.
Fix an instance of negative triangle with node sets I, J,K and weight function w. Let i ∈ I, j ∈ J,
k ∈ K. Recall that the triple (i, j, k) is a negative triangle iff (w(i, k)  w(k, j)) + w(i, j) < 0. Fix a
total ordering < on the nodes in K in the negative triangle instance. For any i ∈ I, j ∈ J, a node
k ∈ K is called a minimum witness for (i, j) if (i, j, k) is a negative triangle but (i, j, k
) is not a
negative triangle for all k < k according to the ordering.
First, we show that any superlinear time negative triangle detection algorithm can be converted
to a minimum witness negative triangle finding algorithm, running in roughly the same time.
Journal of the ACM, Vol. 65, No. 5, Article 27. Publication date: August 2018.                   
27:20 V. V. Williams and R. R. Williams
Lemma 4.4 (Minimum Witness Finding From Detection). LetT (n) be a function so thatT (n)/n
is nondecreasing. Let A be an algorithm that detects a negative triangle over R in an n-node graph in
T (n) time. Then, in O(T (n)) time, given an n-node weighted graph G one can find a single negative
triangle (i, j, k) over R such that k is the minimum witness for pair (i, j) or determine that there is no
negative triangle in G over R.
Proof. If there are only three remaining nodes, i ∈ I, j ∈ J, k ∈ K, then return {i, j, k) if it forms
a negative triangle over R. Otherwise, |I |, |J|, |K| ≥ 2. Split each part I, J,K into two pieces of
roughly n/2 nodes each, so that K is split into K1, K2, where K1 contains the first half of the nodes
of K according to the ordering, and K2 contains the rest of the nodes of K. Then, for all eight triples
a,b,c ∈ [2]3, use the negative triangle detection algorithm on the graphs induced by the unions
of parts Ia ∪ Jb ∪ Kc , in 8T (n/2) time. Let c ∈ {1, 2} be the smallest index such that at least one of
the four subgraphs containing Kc contains a negative triangle over R. If no subgraph contains a
negative triangle, then return that there are no negative triangles. Otherwise, recurse on one of
the subgraphs containing Kc that contains a negative triangle over R.
Suppose that the recursive call returns a triangle i ∈ Ia, j ∈ Jb , k ∈ Kc . By induction suppose that
k is the smallest witness in Kc for (i, j). If c = 1, then k must also be the smallest witness for (i, j)
in the entire K. If c = 2, since there are no negative triangles with witnesses in K1, then k must
also be the smallest witness for (i, j) in the entire K.
The running time recurrence is T 
(n) ≤ 8T (n/2) +T 
(n/2). This solves to
T 
(n) = 8
log
n
i=1
T (n/2i
) ≤ O(T (n))
for any superlinear nondecreasing function T (n).
Now, we show that minimum witnesses for all pairs of nodes can be found efficiently, given a
negative triangle detection algorithm.
Lemma 4.5 (All Pairs Minimum Witness Triangles). Let T (n) be a function so that T (n)/n
is nondecreasing. Given a T (n) algorithm for negative triangle detection over R, there is an
O(T (n1/3)n2)-time algorithm A that outputs a maximal set of I J-disjoint negative triangles over R in
a tripartite graph with distinguished parts (I, J,K), so that each of the triangles is a minimum witness
triangle.
Proof. Proceed as in Lemma 4.2, with a few changes. Partition I, J,K into na parts of roughly
n1−a nodes each, where the nodes of K are partitioned consecutively in their sorted order. Then,
go through the triples of pieces Ii, Jj ,Kk , but here make sure that all triples containing Kk with
k < k are processed before those containing Kk . For each triple, find a minimum witness triangle
with x ∈ Ii,y ∈ Jj , z ∈ Kk , add xyz to the running list L, and remove edge (x,y) from the graph (by
setting its weight to ∞). Since we are processing the triples in nondecreasing order of K and z is
the minimum witness for (x,y) in Kk , it must also be the minimum witness in K. As in Lemma 4.2,
the number of triangles returned is at most n2, and the running time is minimized at O(n2
T (n1/3))
when a = 2/3.
Proof of Theorem 4.5. Let C ≥ 1 be a parameter. We will analyze the algorithm A shown in
Figure 1.
It follows from Lemma 4.5 that the running time of A is O(n2
T (n1/3) logn). We will now prove
that with probability at least 1 − 1/nC, algorithm A outputs the matrix product of A and B over R.
In the call of A on n × n matrices A and B, K = {1,...,n} is sorted according to π. Fix any i ∈
I, j ∈ J. Consider iterating through k ∈ {1,...,n} in reverse order of π, and letTi,j be the number of
Journal of the ACM, Vol. 65, No. 5, Article 27. Publication date: August 2018.   
Subcubic Equivalences Between Path, Matrix, and Triangle Problems 27:21
Fig. 1. Algorithm for reducing matrix product over R to all-pairs minimum witness triangles over R.
times that the minimum value of A[i, k]  B[k, j] changes before finally settling on the minimum
value. We first show that E[Ti,j] ≤ 1 + lnn, using a standard backwards analysis argument.
Let Yk be an indicator variable that is 1 if and only if (A[i, k]  B[k, j]) < (A[i, k
]  B[k
, j])
for all k > k. E[Yk ] is exactly the fraction of permutations over n − k + 1 elements {k,...,n} for
which k is the first element, i.e., E[Yk ] = ((n − k + 1) − 1)!/(n − k + 1)! = 1/(n − k + 1). Hence,
E[Ti,j] =
n
k=1
E[Yk ] =
n
k=1
1
n − k + 1
≤ 1 + lnn.
Analogous to the standard analysis of treaps (see Reference [25], Problems 1.8 and 2.7), we observe
that for a fixed (i, j) and for all S ⊆ [n],
Pr
⎡
⎢
⎢
⎢
⎢
⎣


k ∈S
Yk
⎤
⎥
⎥
⎥
⎥
⎦
≤

k ∈S
Pr[Yk = 1].
This follows from the fact that the Yk satisfy a limited independence condition: conditioned on
any setting of values for Yk where k < k, the probability that Yk = 1 is still 1/(n − k + 1). Hence,
a Chernoff bound applies toTi,j , meaning that for sufficiently largeC ≥ 1, we have Pr[Ti,j > 2C(1 +
lnn)] ≤ e−3 ln n = 1/n3.
11
Algorithm A runs the all-pairs minimum witness algorithm (FindMinWitness) for 2C(1 + lnn)
times. By the calculation above, for any fixed (i, j), the probability that the minimum value of
A[i, k]  B[k, j] is not found is at most 1/n3. By the union bound, the probability that some pair
(i, j) does not have its minimum computed is at most n2/n3 = 1/n. Hence, with 1 − 1/n probability
all minima are computed by A.
5 PROBLEMS EQUIVALENT TO ALL-PAIRS SHORTEST PATHS
The goal of this section is to prove Theorem 1.1 from the Introduction.
11We thank an anonymous referee for suggesting this analysis; our earlier analysis incurred an extra log n multiplicative
factor.
Journal of the ACM, Vol. 65, No. 5, Article 27. Publication date: August 2018.   
27:22 V. V. Williams and R. R. Williams
Reminder of Theorem 1.1 The following problems (with weights in {−M,..., M} ∪ {−∞, ∞}) either
allhave truly subcubic algorithms, or noneof them do:
(1) The all-pairs shortest paths problem on weighted digraphs (APSP).
(2) The all-pairs shortest paths problem on undirected weighted graphs.
(3) Detecting if a weighted graph has a triangle of negative total edge weight.
(4) Listing up to n3−δ negative triangles in an edge-weighted graph, for a fixed δ > 0.
(5) Computing the matrix product over the (min, +)-semiring.
(6) Verifying the correctness of a matrix product over the (min, +)-semiring.
(7) Checking whether a given matrix defines a metric.
(8) Finding a minimum weight cycle in a graph of non-negative edge weights.
(9) The replacement paths problem on weighted digraphs.
(10) Finding the secondshortest simple path between two nodes in a weighted digraph.
(11) Finding a maximum subarray in a given matrix.
The subcubic equivalence of problems (1), (3), (4), (5), and (6) directly follow from Theorems 4.1, 4.2, and 4.3. The rest of the equivalences are proved in the following paragraphs. Most
of these equivalences use the negative triangle problem, since it is so easy to reason about.
The equivalence between problems (1) and (2) is probably folklore, but we have not seen it in
the literature, so we include it for completeness.
Theorem 5.1 (Undirected APSP ≡3 Directed APSP). Let δ,c > 0 be any constants. APSP in
undirected graphs with weights in [0, M] is in T (n, M) time if and only if APSP in directed graphs
with weights in [−M, M] is in T (n, Θ(M)) time.
Proof of Theorem 5.1. Clearly, undirected APSP in graphs with nonnegative weights is a special case of directed APSP, since we can replace each undirected edge by two directed edges in
opposite directions. We show that a truly subcubic algorithm for undirected APSP can be used to
compute the (min, +) product of two matrices in truly subcubic time, and hence directed APSP is
in truly subcubic time.
Suppose that there is a truly subcubic algorithm P for undirected APSP. Let A and B be the
n × n matrices whose (min, +) product we want to compute. Suppose the entries of A and B are in
[−M, M]
12. Consider the edge-weighted undirected tripartite graphG with n-node partitionsI, J,K
such that there are no edges between I and K, and for all i ∈ I, j ∈ J, k ∈ K, (i, j) and (j, k) are edges
with w(i, j) = A[i, j] + 6M and w(j, k) = B[j, k] + 6M. Using P, compute APSP in G. Notice that all
weights are nonnegative.
Any path on at least three edges in G has weight at least 15M, and any path on at most two
edges has weight at most 2 × 7M < 15M. Hence, P will find for every two nodes i ∈ I, k ∈ K, the
shortest path between i and k using exactly two edges, thus computing the (min, +) product of A
and B.
Theorem 5.2 (Metricity ≡3 Negative Triangle). Let T (n, M) be nondecreasing. Then there is
an O(n2) +T (O(n),O(M)) algorithm for negative triangle in n node graphs with weights in [−M, M]
if and only if there is an O(n2) +T (O(n),O(M)) algorithm for the metricity problem on [n] such that
all distances are in [−M, M].
Proof of Theorem 5.2. Given an instance D of the metricity problem, consider a complete
tripartite graph G on 3n nodes n nodes in each of the partitions I, J,K. For any i ∈ I, j ∈ J, k ∈ K,
define the edge weights to be w(i, j) = D[i, j],w(j, k) = D[j, k], and w(i, k) = −D[i, k]. A negative
12Infinite edge weights can be replaced with suitably large finite values, WLOG.
Journal of the ACM, Vol. 65, No. 5, Article 27. Publication date: August 2018. 
Subcubic Equivalences Between Path, Matrix, and Triangle Problems 27:23
triangle in G gives i ∈ I, j ∈ J, k ∈ K so that D[i, j] + D[j, k] − D[i, k] < 0, i.e., D[i, j] + D[j, k] <
D[i, k]. Hence, D satisfies the triangle inequality iff there are no negative triangles in G. Checking
the other properties for a metric takesO(n2) time. This shows that Metricity ≤3 Negative Triangle.
For the opposite direction, let G be a given graph with edge weights w : E → Z such that
for all e ∈ E, w(e) ∈ [−M, M] for some M > 0. Build a tripartite graph with n node partitions
I, J,K and edge weights w
(·) so that for any i ∈ I, j ∈ J, k ∈ K, w
(i, j) = 2M + w(i, j),w
(j, k) =
2M + w(j, k), and w
(i, k) = 4M − w(i, k). For all pairs of distinct nodes a,b so that a,b are in the
same partition, let w
(a,b) = 2M. Finally, let w
(x, x) = 0 for all x. Clearly, w satisfies all requirements for a metric, except possibly the triangle inequality. For any three vertices x,y, z in the same
partition w
(x,y) + w
(y, z) = 4M > 2M = w
(x, z).
Consider triples x,y, z of vertices so that x and y are in the same partition and z is in a different
partition. We have: w
(x, z) + w
(z,y) ≥ M + M = 2M = w
(x,y) and w
(x, z) − w
(y, z) ≤ 2M =
w
(x,y). Furthermore, if i ∈ I, j ∈ J, and k ∈ K, then w
(i, k) + w
(k, j) ≥ M + 3M ≥ w(i, j) and
w
(i, j) + w
(j, k) ≥ M + 3M ≥ w(i, k).
Hence, the only possible triples that could violate the triangle inequality are triples with i ∈
I, j ∈ J, k ∈ K, and w is not a metric iff there exist i ∈ I, j ∈ J, k ∈ K such that w
(i, j) + w
(j, k) <
w
(i, k). That is, w is a metric if and only if i, j, k forms a negative triangle in G.
Theorem 5.3 (Minimum Cycle ≡3 Negative Triangle). If there is a T (n, M) algorithm for finding a minimum weight cycle in graphs on n nodes and weights in [1, M], then there is a T (n,O(M))
algorithm for finding a minimum weight triangle in n-node graphs with weights in [−M, M].
It is known that the Minimum-Cycle problem in directed or undirected graphs can be reduced
to APSP via a subcubic reduction. Hence, we get that APSP ≡3 Minimum-Cycle.
Proof. LetG = (V, E) be given withw : E → [−M, M]. Consider graphG
, which is justG with
weights w : E → [7M, 9M] defined as w
(e) = w(e) + 8M. For any k and any cycle C in G with k
edges, w
(C) = 8Mk + w(C), and hence 7Mk ≤ w
(C) ≤ 9Mk. Hence, all cycles C with ≥ 4 edges
have w
(C) ≥ 28M and all triangles have w weight ≤ 27M < 28M. That is, the minimum weight
cycle in G is exactly the minimum weight triangle in G.
We now prove that the Maximum Subarray problem is equivalent to Negative Triangle. One
direction was already proven by Tamaki and Tokuyama [60]; here, we prove the other.
Theorem 5.4 (Maximum Subarray ≡3 Negative Triangle). The negative triangle problem in
an n node (tripartite) graph with weights in [−M, M] can be reduced in O(n2) time to the Maximum
Subarray problem for an n × n matrix with entries in [−O(M),O(M)].
Proof. We are given a graphG with three parts A, B,C and edges E ⊆ (A × B) ∪ (B × C) ∪ (A ×
C) with weight function w : E → {−W ,...,W }, and we are searching for a triangle of negative
sum. We will first turnG into a complete graph by adding each original non-edge (x,y) with weight
w(x,y) = 2W . Note that no original non-edge can be completed to a negative triangle as all other
edges have weight ≥−W . For the non-edges within A, B, and C, make their edge weights slightly
higher, 8W . Now, we can assume that G is a complete graph with edge weights in {−M,..., M}
between different partitions, and of weight 4M within the partitions, where M = 2W .
Now, we will reduce the negative triangle problem to the following problem: given a complete
undirected graph H with a self-loop at each node and with weights w : V ×V → {0,..., 101M},
find four nodes i, j, k,l for which w
(i, j) + w
(k,l) − w
(i, k) − w
(j,l) is maximized.
H will contain the node sets A, B,C and a copy A of A. Denote the copy in A of node a ∈ A
with a
. H is a complete graph on these nodes. The weights w are as follows:
Journal of the ACM, Vol. 65, No. 5, Article 27. Publication date: August 2018.  
27:24 V. V. Williams and R. R. Williams
• In A × A
, we have weight 0 edges between two copies of the same node and an edge of
weight 10M otherwise, i.e., w
(a, a
) = 0 and w
(a, x) = 10M for all x ∈ A
, x  a
.
• For all edges (a,b) ∈ A × B in H, let their weight be w
(a,b) = 100M + w(a,b) (recall, w is
the weight function of G). Similarly, for all (c, a
) ∈ C × A
, let w
(c, a
) = 100M + w(c, a).
Notice that these edges have weight between 99M and 101M. We call these heavy edges.
• For all (b,c) ∈ B × C, set w
(b,c) = w(b,c).
• For all pairs of nodes in A × C and A × B, make their edge w weights 50M.
• The edges within A,A
, B, and C come from the original graph G, i.e., for each s,t in A
(respectively, A
, B, C), we get an edge (s,t) in A (respectively, A
, B, C) of weight 4M.
• Finally, we will add a self-loop at each node, of weight 40M.
Notice that all edge weights w above are nonnegative except for w(b,c) for b ∈ B,c ∈ C, in which
case they are ≥ −M, so that all weights are ≥ −M. Also notice that the non-heavy edges have
weight ≤ 50M.
Consider now any 4-tuple i, j, k,l of not necessarily distinct nodes in this new graph H and
consider w
(i, j) + w
(k,l) − w
(i, k) − w
(j,l).
If (i, j) and (k,l) are not both heavy edges, then w
(i, j) + w
(k,l) ≤ 101M + 50M = 151M and
−w
(i, k) − w
(j,l) ≤ 2M (since all edge weights are ≥ − M). Thus, in this case w
(i, j) + w
(k,l) −
w
(i, k) − w
(j,l) ≤ 153M.
However, suppose that both (i, j) and (k,l) are heavy edges. First, let us consider the case when
the nodes are not distinct. Then, without loss of generality i = k, andw
(i, j) + w
(k,l) − w
(i, k) −
w
(j,l) = w
(i, j) + w
(i,l) − w
(i,i) − w
(j,l) = 160M + w(i, j) + w(i,l) − w
(j,l) ≤ 163M.
The final case is when all four nodes are distinct and both (i, j) and (k,l) are heavy edges. This
can happen in four ways (up to symmetry):
• If i ∈ A, j ∈ B,l ∈ C and k ∈ A
, then w
(i, j) + w
(k,l) − w
(i, k) − w
(j,l) = 200M −
w(i, j) − w(k,l) − w
(i, k) − w(j,l). This equals 200M − w(i, j) − w(j,l) − w(l,i) ≥ 197M
when k is the copy of i ∈ A in A
, and otherwise equals 190M − w(i, j) − w(k,l) − w(j,l) ≤
193M.
• If i ∈ A, j ∈ B, k ∈ C and l ∈ A
, then w
(i, j) + w
(k,l) − w
(i, k) − w
(j,l) = 100M +
w(i, j) + w(k,l) ≤ 102M.
• If i ∈ A, j ∈ B, k ∈ A,l ∈ B, then w
(i, j) + w
(k,l) − w
(i, k) − w
(j,l) = (200M − 8M) +
w(i, j) + w(k,l) ≤ 196M. (The case i ∈ C, j ∈ A
, k ∈ C,l ∈ A is symmetric.)
• If i ∈ A, j ∈ B,l ∈ A, k ∈ B, then w
(i, j) + w
(k,l) − w
(i, k) − w
(j,l) = (200M − 200M) +
w(i, j) + w(k,l) − w(i, k) − w(j,l) ≤ 4M. (The case i ∈ C, j ∈ A
,l ∈ C, k ∈ A is symmetric.)
Thus, to maximize the quantity w
(i, j) + w
(k,l) − w
(i, k) − w
(j,l), we need to be in the first
case above. More precisely, we get that i ∈ A, j ∈ B, l ∈ C, and k ∈ A
, and that k is the copy of i
in A
, so that the maximized quantity is 200M − w(i, j) − w(j,l) − w(l,i) = 200M− (the minimum
weight of a triangle). If there is a negative triangle in particular, then we would get a value of
>200M. In other words, the maximum value is >200M if and only if the original graph had a
negative triangle.
We will reduce the above problem to the Maximum Subarray problem.
Let A∗ be the n × n matrix with A∗[i, j] = w
[i, j] for every (i, j) in the above graph (including
the self-loops: A∗[i,i] = 40M).
Create the n × n matrix D where D(i, j) = A∗[i, j] − A∗[i − 1, j] for all i, j. Then create the matrix
E[i, j] = D[i, j] − D[i, j − 1].
Journal of the ACM, Vol. 65, No. 5, Article 27. Publication date: August 2018. 
Subcubic Equivalences Between Path, Matrix, and Triangle Problems 27:25
Consider a submatrix of E from the ath row to the uth row and from the bth column to the vth
column. For any i,
v
j=b
E[i, j] = D[i,b] − D[i,b − 1] + D[i,b + 1] − D[i,b] + ··· + D[i,v] − D[i,v − 1] = D[i,v] − D[i,b − 1].
Then, u
i=a
v
j=b E[i, j] = (
u
i=a D[i,v]) − (
u
i=a D[i,b − 1]). Now, for any x,
u
i=a
D[i, x] = A∗
[a, x] − A∗
[a − 1, x] + A∗
[a + 1, x] − A∗
[a, x] + ··· + A∗
[u, x] − A∗
[u − 1, x]
= A∗
[u, x] − A∗
[a − 1, x],
and so
u
i=a
v
j=b
E[i, j] = A∗
[u,v] − A∗
[a − 1,v] − A∗
[u,b − 1] + A∗
[a − 1,b − 1].
The Maximum Subarray problem for matrix E seeks to maximize the above quantity over all
choices of a,b,u,v. The above quantity is a sum as in the four node problem, and what we are
maximizing is exactly what we needed in that problem. Hence, if the maximum subarray of E has
sum >200M, then G has a negative triangle, and otherwise G doesn’t. Thus, we have a subcubic
reduction from negative triangle and hence also APSP to the Maximum Subarray problem.
To complete the proof of Theorem 1.1, it remains to show the equivalences of Replacement Paths
and Second Shortest Paths with the other problems.
5.1 Replacement Paths and Second Shortest Paths
The replacement paths and second shortest simple path problems have been known to be closely
related to APSP in an informal sense. For instance, any algorithm for APSP can solve the two
problems in asymptotically the same time: remove all edges from the shortest path P between
s and t and compute APSP in the remaining graph. This computes the minimum weight detour
for all pairs of nodes on P, and so in additional O(n2) time, one can solve both the replacement
paths problem and the second shortest simple path problem. It was not clear, however, that the
two problems cannot be solved faster than APSP. For instance, Roditty [52] took his fast approximation algorithms as evidence that the two problems might be easier than APSP. In an attempt to
explain why it has been so hard to find fast algorithms, Hershberger et al. [35] showed that in the
path comparison model of Karger et al. [38] the replacement paths problem needs Ω(m√
n) time.
This bound does not apply to second shortest path and is the only known lower bound for these
problems.
Here, we present a reduction that shows that if the second shortest simple path in a directed
graph with n nodes can be found in time that is truly subcubic in n, then APSP is in truly subcubic
time. Thus, the two problems are equivalent with respect to subcubic algorithms, for dense graphs.
Since the second shortest simple path problem is a special case of the replacement paths problem,
our result implies that for dense graphs the replacement paths problem is equivalent to APSP, with
respect to subcubic algorithms.
In the next section, we modify the reduction to show that if for some m(n) and nondecreasing
f (n) there is a combinatorial algorithm that runs in O(m(n)
√
n/f (n)) time and computes the second shortest simple path in unweighted directed graphs, then there is anO(n3/f (n)) combinatorial
algorithm for triangle detection, and hence a corresponding subcubic combinatorial algorithm for
BMM. This implies that if there is no truly subcubic combinatorial algorithm for BMM, then to
improve on the algorithm of Roditty and Zwick [55], one would need to use algebraic techniques.
Journal of the ACM, Vol. 65, No. 5, Article 27. Publication date: August 2018.         
27:26 V. V. Williams and R. R. Williams
Theorem 5.5 (Minimum Triangle ≡3 Second Shortest Simple Path). Suppose there is a
T (n,W ) time algorithm for computing the second shortest simple path in a weighted directed
graph with n nodes and integer weights in [0,W ]. Then there is a T (O(n),O(nW )) time algorithm
for finding a minimum weight triangle in an n node graph and integer weights in [−W ,W ], an
O(n2
T (O(n1/3),O(nW )) logW ) time algorithm for the distance product of two n × n matrices with
weights in [−W ,W ], and an O(n2
T (O(n1/3),O(n2
W )) logW n) time algorithm for APSP in graphs
with integer weights in [−W ,W ].
Proof. Let G be an instance of Minimum Triangle with integer weights in [−M, M]. Without
loss of generality,G has three partsU,V,T with no edges within them, and the edges going fromU
to V , fromV to T , and fromT toU . Furthermore, again without loss of generality, all edge weights
are positive (otherwise add M + 1 to all edges, so that now the minimum weight triangle has weight
3(M + 1)+ its original weight). Also without loss of generality, G contains edges between every
two nodes ui ∈ U,vj ∈ V , between any two nodes vj ∈ V and tk ∈ T and between any two nodes
tk ∈ T and ui ∈ U (if some edge does not appear, add it with weight 3M + 1 where M = 4M + 1
is the current maximum edge weight in G). Note that all of these transformations increase the
maximum weight by at most a constant factor. Let M be the new maximum edge weight of G.
Now, we will reduce any instance of minimum weight triangle to one of finding the second
shortest simple path. First, create a path on n + 1 nodes, P = p0 → p1 →···→ pn. For every edge
(pi,pi+1) in P, let it have weight 0. All other edges in the graph we will construct will be positive
and hence P will be the shortest path between p0 and pn.
Create three parts withn nodes each,A = {a1,..., an }, B = {b1,...,bn }, C = {c1,...,cn }so that
for each i, j ∈ [n] there is an edge (ai,bj) with weight w(ui,vj) (the weight in G), and an edge
(bi,cj) with weight w(vi,tj); that is, we have created a copy of G except that the edges between T
and U are removed (no edges between C and A).
Let W = 3M + 1, where recall that M = O(M) is the maximum edge weight of G. Now, for
every j > 0, add an edge from cj to pj with weight jW . All these weights are positive and at most
nW = O(Mn).
For every i < n and any r ∈ [n], add an edge frompi to ar with weight (n − i − 1)W + w(ci+1, ar ).
Since i < n and 0 < w(ci+1, ar ) ≤ O(M), these weights are also positive and at most O(Mn).
An example of the full construction appears in Figure 2.
The second shortest path must have the form p0 →···→ ps followed by a path of length two
from some ai through a node in B to a node ct in C with t > s, followed by an edge (ct,pt ) and
then pt →···→ pn: we are looking for the shortest detour between a node ps and a node pt on P
with t > s.
The weight of a detour between ps and pt going through nodes ai,bj ,ct is
(n − s − 1)W + w(cs+1, ai ) + w(ai,bj) + w(bj,ct ) + tW .
Claim 1. In the graph we have constructed, any optimal detour must have t = s + 1.
Proof of Claim. Clearly t > s. If t ≥ s + 2, then the weight of the detour is at least
(n − s − 1 + s + 2)W + w(cs+1, ai ) + w(ai,bj) + w(bj,ct ) > (n + 1)W .
Consider any detour between ps and ps+1, say going through ai,bj ,cs+1. Its weight is
(n − s − 1 + s + 1)W + w(cs+1, ai ) + w(ai,bj) + w(bj,cs+1) ≤ nW +W = (n + 1)W ,
since W is greater than three times the largest weight in the graph.
Now, the detours between ps and ps+1 have weight nW + w(ai,bj) + w(bj,cs+1) + w(cs+1, ai ). In
particular, the shortest detour between ps and ps+1 has weight nW plus the minimum weight of a
Journal of the ACM, Vol. 65, No. 5, Article 27. Publication date: August 2018. 
Subcubic Equivalences Between Path, Matrix, and Triangle Problems 27:27
Fig. 2. The reduction from minimum weight triangle to second shortest simple path for n = 3.
triangle containing cs+1. The second shortest path hence has weight exactly nW plus the minimum
weight of a triangle in G.
Since the second shortest path problem is a special case of the replacement paths problem, we
have:
Corollary 5.1. If the replacement paths problem in graphs with integer weights in [0, M] is computable in T (n)poly log M time, then APSP in graphs with integer weights in [−M, M] is computable
in O(n2
T (O(n1/3))poly log M) time.
Corollary 5.2. Replacement Paths ≡3 APSP.
6 BOOLEAN MATRIX MULTIPLICATION AND RELATED PROBLEMS
In this section, we describe several applications of our techniques to the problem of finding fast
practical Boolean matrix multiplication algorithms, a longstanding challenge in graph algorithms.
(For more background on this problem, see the Preliminaries.)
As a direct consequence of Theorems 4.2, 4.1, and 4.3, we obtain:
Theorem 6.1. The following either all have truly subcubic combinatorial algorithms, or none of
them do:
(1) Boolean matrix multiplication (BMM).
(2) Detecting if a graph has a triangle.
(3) Listing up to n2.99 triangles in a graph.
(4) Verifying the correctness of a matrix product over the Boolean semiring.
Theorem 6.2. For every constantc, the problems listed in Theorem 6.1 either all have combinatorial
algorithms running in O(n3/ logc n) time, or none of them do.
Another immediate corollary of Theorem 4.3 is an efficient triangle listing algorithm:
Corollary 6.1. There is an algorithm that, given Δ and a graphG on n nodes, lists up to Δ triangles
from G in time O(Δ1−ω/3
nω ) ≤ O(Δ0.21n2.38).
Journal of the ACM, Vol. 65, No. 5, Article 27. Publication date: August 2018.
27:28 V. V. Williams and R. R. Williams
Note when Δ = n3, one recovers the obvious O(n3) algorithm for listing all triangles, and when
Δ = O(1), the runtime is the same as that of triangle detection.
6.1 Output-Sensitive BMM
Lemma 4.3 can be applied to show that in the special case of BMM, there is an improved randomized
output-sensitive algorithm:
Theorem 6.3. Let T (n) be a function so that T (n)/n is nondecreasing. Let L ≥ n logn. Suppose
there is a T (n) time algorithm for triangle detection in an n node graph. Then there is a randomized
algorithm R running in time
O˜ (n2 + L · T (n2/3
/L1/6)),
so that R computes the Boolean product C of two given n × n matrices with high probability, provided
that C contains at most L nonzero entries. When T (n) = O(nΔ) for some 2 ≤ Δ ≤ 3, the runtime
becomes O˜ (n2Δ/3L1−Δ/6).
Proof. The algorithm uses ideas from a paper by Lingas [43, 44]. Lingas showed how to reduce,
in O(n2 logn) time, computing the Boolean matrix product of two n × n matrices to computing
O(logn) Boolean matrix products of an O(
√
L) × n by an n × O(
√
L) matrix and 2 output-sensitive
Boolean matrix products of an O(
√
L) × n by an n × n matrix.
The conditions of Lemma 4.3 require that L ≤ n3, L3/2 and that n
√
L ≤ L3/2,n3. The first condition is clearly met since L ≤ n2, and the second condition is met since L ≥ n logn.
Using Lemma 4.3, we get an asymptotic runtime of
n2 logn + logn · L · T (n1/3) + L · T (n2/3
/L1/6).
Since T (n) is nondecreasing and L ≤ n2, we get that T (n2/3/L1/6) ≥ T (n1/3), and hence we can
bound the runtime by O((n2 + L · T (n2/3/L1/6)) logn).
If T (n) = O(nΔ) for some 2 ≤ Δ ≤ 3 and L ≥ n, then we have L · (n2/3/L1/6)
Δ ≥ n2. Hence, the
runtime is just O˜ (n2Δ/3L1−Δ/6).
6.2 Second Shortest Paths and BMM
Similar to the case of APSP, we can prove a close relationship between BMM and finding the second
simple shortest path between two given nodes in an unweighted directed graph. The relationship
naturally extends to a relationship between BMM and RPP in unweighted directed graphs. The
theorem below shows that in the realm of combinatorial algorithms, Roditty and Zwick’s [54,
55] algorithm for the second shortest simple path problem in unweighted directed graphs would
be optimal, unless there is a truly subcubic combinatorial algorithm for BMM. Furthermore, any
practical improvement of their algorithm would be interesting as it would imply a new practical
BMM algorithm.
Theorem 6.4. Suppose there exist nondecreasing functions f (n) and m(n) with m(n) ≥ n, and a
combinatorial algorithm that runs in O(m(n)
√
n/f (n)) time and computes the second shortest simple
path in any given unweighted directed graph with n nodes and m(n) edges. Then there is a combinatorial algorithm for triangle detection running in O(n3/f (n)) time. If f (n) = nε for some ε > 0, then
there is a truly subcubic combinatorial algorithm for BMM.
Proof. Suppose we are given an instance of triangle detection G = (V, E) where V is identified
with [n]. Let L be a parameter. Partition V into n/L buckets Vb = {bL + 1,...,bL + L} of size L.
We will create n/L instances of the second shortest simple path problem. In instance b (for
b ∈ {0,...,n/L − 1), we will be able to check whether there is a triangle going through a node in
bucket Vb .
Journal of the ACM, Vol. 65, No. 5, Article 27. Publication date: August 2018. 
Subcubic Equivalences Between Path, Matrix, and Triangle Problems 27:29
Fix some b. First, create a path on L + 1 nodes, P = p0 → p1 →···→ pL.
In our construction, we will make sure that P is the shortest path from p0 to pL. The second
shortest path would have to go from p0 to some ps using P, then take a detour (say of length d) to
pt with t > s, and then take P from pt to pL. The length of the second shortest path would then be
L − t + s + d = L + d + (s − t).
Create three parts, A = {a1,..., an }, B = {b1,...,bn },C = {c1,...,cL } so that for each i, j ∈ [n]
there is an edge (ai,bj) iff (i, j) ∈ E and for every i ∈ [n], j ∈ [L], there is an edge (bi,cj) iff (i,bL +
j) ∈ E.
Now, for every j > 0, add a path Rj of length 2j from cj to pj , adding 2j new nodes.
For every i < L add a path Qi of length 2(2L − i), ending at some node qi (thus, adding 4L − 2i
new nodes). The overall number of new nodes is at most 4L(L + 1).
For every r ∈ [n] and i < L, add an edge from qi to ar iff (bL + i + 1,r) ∈ E.
Now, any simple path from p0 to pL that uses nodes from A, B, or C must go through one of the
paths Qi , and hence has length at least 2(2L − L + 1) = 2(L + 1) > L + 1. Hence, P is the shortest
path between p0 and pL.
The second shortest path must have the formp0 →···→ ps followed by a detour to pt fort > s,
followed by pt →···→ pL. The detours between ps and pt look like this: take path Qs from ps to
qs , then a path of length 3 through some ai through a node in B to a node ct in C with t > s, and
then taking path Rt to pt . The length of the detour is
dst = 2(2L − s) + 3 + 2t = 4L + 3 + 2(t − s).
The length of the full path is
L + dst + (s − t) = 5L + 3 + (t − s).
Hence, the closer s and t are, the shorter the path.
Now, G has a triangle (i, j,bL + s) going through Vb iff there is a path with detour between ps−1
and ps going through Qs−1, ai,bj ,cs , Rs . Its length is 5L + 4. For any s,t with t ≥ s + 2, the length
of the path with detour between ps and pt is at least 5L + 3 + 2 > 5L + 4. Hence, the shortest that
a second shortest path can be is 5L + 4. It is exactly of this length (and goes between some ps and
ps+1) iff there is a triangle going through Vb . Computing the length of the second shortest simple
path then will tell us whether the original graph has a triangle going through Vb .
Each of the n/L graphs (for each setting of b) has O(n + L2) nodes and O(n2) edges. For L =
Ω(√
n) the graph has O(L2) nodes and O(n2) edges.
Suppose that for some nondecreasing m(N) and f (N) there is an O(m(N)
√
N/f (N)) combinatorial algorithm for the second shortest simple path in directed unweighted graphs. Then, let L
be such that m(n + 4L(L + 1)) = O(n2). One can find a triangle using a combinatorial algorithm in
time
O(n/L · (n2
L)/f (L2)) = O(n3
/f (L2)) ≤ O(n3
/f (n)).
If f (n) is a polynomial, then there is a truly subcubic combinatorial algorithm for BMM.
6.3 Two New BMM Algorithms
Our results allow us to provide two new algorithms for BMM, relying on the relationship between
BMM and triangle detection.
6.3.1 Output-Sensitive Quantum BMM. In the theory of quantum computing, it is known that
triangle-finding in graphs on n nodes can be solved using only O˜ (n1.3) quantum queries to the
graph [45]. Buhrman and Špalek [13] studied the problem of verifying and computing matrix
products using a quantum algorithm, also in the query complexity setting, where the operations
Journal of the ACM, Vol. 65, No. 5, Article 27. Publication date: August 2018. 
27:30 V. V. Williams and R. R. Williams
counted are the queries to the matrix entries. Among other nice results, their paper showed an
O˜ (n1.5
√
L)-query output-sensitive algorithm for computing the Boolean matrix product of two
n × n matrices, where L is the number of ones in the output matrix. Lemma 4.2 is a black box reduction that implies an improved algorithm by plugging in Magniez, Santha, and Szegedy’s [45]
query-efficient triangle algorithm. Our results have been improved by Le Gall [41] and Jeffery,
Kothari, and Magniez [37]. The latter authors showed that the quantum query complexity of BMM
is Θ( ˜ n
√
L).
Lemma 6.1. There is anO˜ (n1.3L17/30)-query quantum algorithm for computing the Boolean matrix
product of two n × n matrices, where L is the number of ones in the output matrix.
Notice that since L ≤ n2, we always have n1.3L17/30  O˜ (n1.5
√
L).
Proof of Lemma 6.1. Let A and B be the given Boolean matrices. Consider a tripartite graph
with partitions I, J,K so that fori ∈ I and j ∈ J, (i, j) is an edge iff A[i, j] = 1, for j ∈ J, k ∈ K, (j, k)
is an edge iff B[j, k] = 1, and (i, k) is an edge for all i ∈ I, k ∈ K. Note this graph does not need to
be created explicitly; whenever the algorithm needs to query if (a,b) is an edge in the graph, it
can just query A and B, and any output it has already produced. Then, in the output-sensitive part
of the proof of Lemma 4.2, we can just use T (n) = O˜ (n1.3) given by the algorithm of [45]. Notice
that the condition of the lemma is satisfied forT (n) = O˜ (n1.3). Hence, we obtain an algorithm with
quantum query complexity O˜ (n1.3L1−1.3/3) = O˜ (n1.3L17/30).
Using an algorithm by Lingas [44] that we also utilize in Theorem 6.3, the query complexity of
the problem can be shown to be O˜ (n1.5L1/4), which is an improvement for all L ≥ n12/19. We prove
Theorem 1.6.
Reminder of Theorem 1.6. There is an O˜ (min{n1.3L17/30,n1.5L1/4})-query quantum algorithm
for computing the Boolean matrix product of two n × n matrices, where L is the number of ones in the
output matrix.
Proof. Lingas [44] shows that repeating the following approachO(logn) times computes BMM
with high probability, provided the number of ones in the output is ≤ L:
(1) Pick a random permutation of the rows of A and columns of B, and contract each consecutive block of L1/2 rows of A or columns of B into a “superrow” (or “supercolumn”), which
is just the componentwise OR of the corresponding rows or columns.
This part can be computed using nL1/2 Grover searches that find a 1 among n/L1/2 entries.
This takes nL1/2

n/L1/2 queries altogether, resulting in a query complexity ofO(n1.5L1/4).
(2) Compute BMM on the resulting L1/2 × n by n × L1/2 matrices, also extracting witnesses,
and then for each 1 in the output, find the lexicographically first nonzero entry in the
original instance that lies in the corresponding superrow and supercolumn.
All of these operations can be performed classically. The matrix product part does not
require any extra queries to the input. The rest of the computation needs at most L(n/L1/2)
extra queries to the input, since the number of nonzero entries in the collapsed instance
is still at most L, and recovering a real nonzero entry for a collapsed nonzero (i, j) with
witness k can be done by just going through the n/L1/2 rows corresponding to superrow
i and the n/L1/2 columns corresponding to supercolumn j until the first entry x and first
entry y are found such that A[x, k] = B[k,y] = 1.
6.4 Polynomial Preprocessing and Faster Combinatorial BMM
The divide-and-conquer ideas in our theorems are admittedly quite simple, but they are also powerful. It is evident that these ideas are useful for solving function problems via algorithms for
Journal of the ACM, Vol. 65, No. 5, Article 27. Publication date: August 2018.  
Subcubic Equivalences Between Path, Matrix, and Triangle Problems 27:31
related decision problems. These ideas can also be applied to greatly relax the conditions needed
to achieve faster algorithms for the decision problems themselves. Williams [70] showed that it is
possible to preprocess a graph in O(n2+ε ) time (for all ε > 0) such that queries of the form “is S
an independent set?” can be answered in O(n2/ log2 n) time. This data structure can be easily used
to solve triangle detection in O(n3/ log2 n), by simply querying the neighborhoods of each vertex.
Bansal and Williams [6, 7] show that every graph can be (randomly) preprocessed in O(n2+ε ) time
so that any batch of O(logn) independent set queries can be answered in O(n2/ log1.25 n) (deterministic) time. This implies an O(n3/ log2.25 n) randomized triangle detection algorithm. A major
limitation in this approach to fast triangle detection is that the preprocessing time apparently
must be subcubic. In fact, this subcubic requirement is the only reason why Bansal and Williams’
preprocessing algorithm needs randomization. It turns out that in fact any polynomial amount of
preprocessing suffices:
Theorem 6.5. Suppose there are k,c > 0 such that every n-node graph can be preprocessed inO(nk )
time so that all subsequent batches of logn independent set queries S1,..., Slog n can be answered
together in O(n2/ logc n) time. Then triangle detection (and hence Boolean matrix multiplication) is
solvable in O(n3/ logc+1 n) time.
That is, to attain better combinatorial algorithms for BMM, it suffices to answer independent
set queries quickly with any polynomial amount of preprocessing. Theorem 6.5 holds for both randomized and deterministic algorithms: a deterministic preprocessing and query algorithm results
in a deterministic BMM algorithm. The idea behind the proof of Theorem 6.5 resembles a method
by Andersson and Thorup [5] for converting polynomial space static search data structures into
linear space dynamic ones.
Proof of Theorem 6.5. Let a = 1/(2k). Divide the n nodes of the graph into n1−a parts, each part
having at most na + 1 nodes each. For each pair i, j of parts, let Gi,j = (Vi,j, Ei,j) be the subgraph
of G restricted to the nodes in parts i and j. Preprocess Gi,j for independent set queries in O(nak )
time. This stage takes O(n2(1−a)+ak ) ≤ n2−1/k+1/2 ≤ O(n2.5) time.
To determine if G has a triangle, partition the set of nodes of G into at most 1 + n/ logn groups
of logn nodes each. For each group v1,...,vlog n and all pairs of indices i, j = 1,...,n1−a, query
N (v1) ∩Vi,j,..., N (vlog n ) ∩Vi,j for independence. If some query answers “no,” then report that
there is a triangle; if all queries answer “yes” over all nodes then report that there is no triangle.
This stage takes O(n/ logn · n2(1−a) · n2a/(a logc n)) ≤ O(n3/ logc+1 n) time.
Theorem 6.5 makes it easy to give derandomized versions of Bansal and Williams’ algorithms,
since there are deterministic polynomial time algorithms for the problems they need to solve, just
not subcubic ones.
Reminder of Theorem 1.4. There is a deterministic combinatorial algorithm for BMM running in
O(n3/ log2.25 n) time.
Proof of Theorem 1.4. We will show that there is a deterministic combinatorialO(n3/ log2.25 n)
time algorithm for triangle finding. By Corollary 4.1, this yields a deterministic combinatorial
O(n3/ log2.25 n) time algorithm for BMM.
The preprocessing algorithm of Bansal and Williams (Theorem 5.1 in Reference [6]) proceeds by
finding an ε-pseudoregular partition (in the sense of Frieze and Kannan [32]) in O(n2) randomized
time. The resulting independent set query algorithm answers O(logn) independent set queries
in O(n2/ log1.25 n) time and is completely deterministic. Alon and Naor [4] give a deterministic
polynomial time algorithm for computing an ε-pseudoregular partition, which works for all ε ≤
c/

logn for a fixed constantc > 0. By replacing the randomized preprocessing with the algorithm
of Alon and Naor, and applying the reduction of Theorem 6.5, the algorithm is obtained.
Journal of the ACM, Vol. 65, No. 5, Article 27. Publication date: August 2018.  
27:32 V. V. Williams and R. R. Williams
Using the connection between negative triangle and APSP, we can identify a natural query
problem on weighted graphs for which nontrivial solutions would give faster APSP algorithms.
On a graph with an edge weight function c : E → Z, define a price query to be an assignment
of node weights p : V → Z, where the answer to a query is yes if and only if there is an edge
(u,v) ∈ E such that p(u) + p(v) > c(u,v). Intuitively, think of p(v) as a price on node v, the edge
weight c(u,v) as the cost of producing both u and v, and we wish to find for a given list of prices
if there is any edge we are willing to “sell” at those prices.
Reminder of Theorem 1.5. Suppose there is a constant k > 0 and a function f (n) such that every
n-node edge-weighted graph can be preprocessed in O(nk ) time so that any subsequent price query
can be answered in O(n2/f (n)) time. Then, the negative triangle detection problem is solvable in
O(n3/f (n1/(2k)
)) time.
If the hypothesis of the theorem is true for f (n) = 2Ω(logδ (n)) for some δ > 0, then our results also
imply that APSP is solvable in n3/2Ω(logδ (n)) time (by Theorem 4.5). To some, the contrapositive
of Theorem 1.5 may be more interesting: assuming that APSP needs Ω(n3/f (n)) time, there is a
super-polynomial time lower bound on the preprocessing time needed for efficiently answering
price queries.
Proof (Sketch). In Theorem 6.5 (and Reference [6]), faster solutions for the independent set
query problem (with polynomial preprocessing) are shown to imply faster triangle detection. The
key observation here is that the price query problem is the analogue of the independent set query
problem, for finding negative weight triangles. More precisely, given any weighted graph on nodes
{v1,...,vn } and weight function w, the existence of a negative weight triangle can be determined
from n price queries q1,...,qn, one for each node, where the price on node vj in qi is −w(vi,vj).
Obtaining a yes answer to the price query qi is equivalent to the existence of an edge (vj,vj) such
that (−w(vi,vj)) + (−w(vi,vk )) > w(vj,vk ), which is equivalent to the existence of a negative
weight triangle through vi . This already shows that a subcubic time preprocessing algorithm for
answering price queries in subquadratic time implies subcubic-time negative triangle detection.
By applying the same divide-and-conquer preprocessing idea as Theorem 6.5, we find that any
polynomial-time algorithm for preprocessing graphs for price queries implies a subcubic-time algorithm for preprocessing graphs for price queries (with asymptotically the same query time).
7 A SIMPLIFIED VIEW OF ALL-PAIRS PATH PROBLEMS
AND THEIR MATRIX PRODUCTS
In this section, we consider various algebraic structures other than the (min, +) and Boolean semirings. We relate their matrix products and respective triangle problems, showing how several prior
results in the area can be simplified in a uniform way.
Existence-Dominance. The dominance product of two integer matrices A and B is the integer
matrix C such that C[i, j] is the number of indices k such that A[i, k] ≤ B[k, j]. The dominance
product was first studied by Matoušek [46] who showed that for n × n matrices it is computable
in O(n(3+ω)/2). The existence-dominance product of two integer matrices A and B is the Boolean
matrix C such that C[i, j] = 0 iff there exists a k such that A[i, k] ≤ B[k, j]. This product was used
in the design of the first truly subcubic algorithm for the minimum node-weighted triangle problem [63, 67]. Although the existence-dominance product seems easier than the dominance product,
the best known algorithm for it actually computes the dominance product.
The existence-dominance product is defined over the (min, ) structure for which R = Z ∪
{−∞, ∞} and a  b = 0 if a ≤ b and a  b = 1, otherwise. The corresponding negative triangle
problem, the dominance triangle problem, is defined on a tripartite graph with parts I, J,K. The
Journal of the ACM, Vol. 65, No. 5, Article 27. Publication date: August 2018. 
Subcubic Equivalences Between Path, Matrix, and Triangle Problems 27:33
edges between I and J are unweighted, and the rest of the edges in the graph have real weights.
The goal is to find a triangle i, j, k ∈ I × J × K such that w(i, k) ≤ w(k, j).
Minimum Edge Witness. The minimum edge witness product is defined over a restriction of the
(min, ) structure over R = Z ∪ {∞, −∞}, where  = × is integer multiplication. For an integer
matrix A and a {0, 1} matrix B, the (i, j) entry of the minimum edge witness product C of A and B
is equal to mink (A[i, k] × B[k, j]). This product is important as it is in truly subcubic time iff APSP
on node-weighted graphs is in truly subcubic time. Chan [15, 16] used this relation to obtain the
first truly subcubic runtime for node-weighted APSP.
The negative triangle problem corresponding to the minimum edge witness product is again
the dominance triangle problem. Hence, by Theorem 4.2, we can conclude that a truly subcubic
algorithm for the dominance triangle problem (such as Matoušek’s algorithm for the dominance
product) implies truly subcubic node-weighted APSP. That is, we get an alternative subcubic algorithm for node-weighted APSP as a byproduct, although it is a bit slower than the best known.
To obtain his algorithm for node-weighted APSP, Chan [16] gave a completely new algorithm for
minimum edge witness product with exactly the same runtime as Matoušek’s dominance product
algorithm.
(Min-≤). The (min, ≤) structure is defined over R = Z ∪ {∞, −∞}, where the binary operation ≤
on input a,b returns b if a ≤ b and ∞ otherwise. The first author showed [62, 68] that the (min, ≤)
matrix product is in truly subcubic time iff the all-pairs minimum nondecreasing paths problem
(also called earliest arrivals) is in truly subcubic time. The first truly subcubic runtime for the
product, O(n2+ω/3), was obtained by the present authors and R. Yuster [65, 66]. The techniques of
Duan and Pettie [24] also imply an O(n(3+ω)/2) algorithm.
The negative triangle problem over (min, ≤) is the following nondecreasing triangle problem:
given a tripartite graph with partitions I, J,K and real edge weights, find a triangle i ∈ I, j ∈ J, k ∈
K such that w(i, k) ≤ w(k, j) ≤ w(i, j).
Both known algorithms for this problem follow from the algorithms for (min, ≤)-product [24, 65,
66] and are somewhat involved. Below, we give a simpler O(n3/2

T (n)) algorithm, where T (n) is
the best runtime for finding a triangle in an unweighted graph. If matrix multiplication is used, then
the runtime is the same as in Duan-Pettie’s algorithm, O(n(3+ω)/2). Furthermore, the algorithm
can actually be applied O(logn) times to obtain another O˜ (n(3+ω)/2) algorithm for the (min, ≤)-
product.
Theorem 7.1 (Nondecreasing Triangle ≤3 Triangle). If a triangle in an unweighted graph
can be found in T (n) time, then a nondecreasing triangle can be found in O(n3/2

T (O(n))) time, and
(min, ≤) product is in O(n3/2

T (O(n)) logn) time.
Proof. We are given a weighted tripartite graph with partitions I, J, K and are looking for a
triangle i ∈ I, j ∈ J, k ∈ K such that w(i, k) ≤ w(k, j) ≤ w(i, j).
Begin by sorting all the edges in the graph, breaking ties in the following way: edges from I × J
are considered bigger than edges from K × J of the same weight, which are considered bigger
than edges from I × K of the same weight; within I × J or J × K or I × K equal edges are arranged
arbitrarily.
Let t be a parameter. For every vertex v in J or K, consider the sorted order of edges incident
to v and partition it into at most n/t buckets of t consecutive edges each and at most one bucket
with ≤ t; let Bvb denote the bth bucket for node v. For each edge (x,v) such that v is in J or K and
(x,v) is in Bvb , go through all edges (v,y) in Bvb and check whether x,v,y forms a nondecreasing
triangle. This takes O(n2
t) time.
Journal of the ACM, Vol. 65, No. 5, Article 27. Publication date: August 2018.
27:34 V. V. Williams and R. R. Williams
Partition the edges of the graph by takingO(n/t) consecutive groups of ≤ nt edges in the sorted
order of all edges. LetGд denote the дth such group. For each д, consider all buckets Bvb of vertices
v in J or K such that there is some edge (v, x) ∈ Bvb ∩ Gд. There can be at most 4n such buckets:
there are at most n + nt/t = 2n buckets completely contained in Gд and at most 2n straddling Gд–
at most one per vertex per group boundary.
Create a tripartite graph Hд for each д as follows. Hд has partitions HI
д,HJ
д and HK
д . HI
д has a
node for each i ∈ I. For S ∈ {J,K}, HS
д has a node for each node bucket Bvb such that Bvb ∩ Gд  ∅
and v ∈ S. Therefore Hд has ≤ 9n nodes.
The edges of Hд are as follows. For all Bjb ∈ HJ
д and Bkb ∈ HK
д , (Bjb , Bkb ) is an edge if (j, k) is
an edge and it is in Bjb ∩ Bkb. For i ∈ HI
д and Bjb ∈ HJ
д , (i, Bjb ) is an edge in Hд iff (i, j) ∈ E and
there is a bucket b < b such that (i, j) ∈ Bjb. For i ∈ HI
д and Bkb ∈ HK
д , (i, Bkb ) is an edge in Hд
iff (i, k) ∈ E and there is a bucket b > b such that (i, k) ∈ Bkb.
Any triangle i, Bjb , Bk,b in Hд corresponds to a nondecreasing triangle i, j, k in G. If a nondecreasing triangle i, j, k of G is not contained in any Hд, then for some b either both (i, j) and (j, k)
are in Bjb or both (i, k) and (j, k) are in Bkb , both cases of which are already handled.
The runtime isO(n2
t +T (9n) · n/t). Setting t = 
T (9n)/n, the time becomesO(n3/2

T (9n)).
Min-Max. The subtropical semiring (min, max) is defined over R = Z ∪ {∞, −∞}. The (min, max)
matrix product was used by the present authors and R. Yuster [66] to show that the all-pairs bottleneck paths problem is in truly subcubic time. The current best algorithm for the problem runs
in O(n(3+ω)/2) time by Duan and Pettie [24]. The (min, max) product is an important operation in
fuzzy logic, where it is known as the composition of relations ([26], p. 73).
The negative triangle problem over (min, max) is the following I J-bounded triangle problem.
Given a tripartite graph with partitions I, J,K and real weights on the edges, find a triangle i ∈
I, j ∈ J, k ∈ K such that both w(i, k) ≤ w(i, j) and w(j, k) ≤ w(i, j), i.e., the largest triangle edge is
in I × J. We note that any algorithm for the nondecreasing triangle problem also solves the I Jbounded triangle problem: any I J-bounded triangle appears as a nondecreasing triangle either in
the given graph, or in the graph with partitions J andK swapped. Hence, a corollary to Theorem 7.1
is that an I J-bounded triangle can be found in O(n3/2

T (n)) time, where T (n) is the runtime of a
triangle detection algorithm for unweighted graphs.
8 EXTENSION TO 3SUM
Finally, we describe an application of the ideas in this article to the 3SUM problem: given three
lists A, B, C of n integers each, are there a ∈ A, b ∈ B, and c ∈ C such that a + b + c = 0? The AllIntegers-3SUM problem looks like a strict generalization of 3SUM: given three lists A, B, C of n
integers each, output the list of all integers a ∈ A such that there exist b ∈ B,c ∈ C with a + b + c =
0. We can prove a subquadratic equivalence between 3SUM and All-Integers-3SUM.
Theorem 8.1 (All-Integers 3SUM ≡2 3SUM). All-Ints 3SUM is in truly subquadratic time iff
3SUM is in truly subquadratic time.
Proof. One direction is obvious. For the other, we use a randomized hashing scheme proposed
by Dietzfelbinger [23] and used by Baran, Demaine, and Patrascu [8], which maps each distinct
integer to one of √
n buckets.13 For each i ∈ [
√
n], let Ai , Bi , and Ci be the sets containing the
elements hashed to bucket i. The hashing scheme has two nice properties:
13The scheme performs multiplications with a random number and some bit shifts, hence we require that these operations
are not too costly. We can ensure this by first mapping the numbers down to O (log n) bits, e.g., by computing modulo
some sufficiently large Θ(log n) bit prime.
Journal of the ACM, Vol. 65, No. 5, Article 27. Publication date: August 2018.  
Subcubic Equivalences Between Path, Matrix, and Triangle Problems 27:35
(1) For every pair of buckets Ai and Bj , there are two buckets Cki j0 and Cki j1 (which can be
located in O(1) time given i, j), such that if a ∈ Ai and b ∈ Bj , then if a + b ∈ C, then a + b
is in either Cki j0 or Cki j1 .
(2) The total number of elements that are mapped to buckets containing at least 3√
n elements
is O(
√
n) in expectation.
After applying the hash function to all elements, we process all elements that get mapped to
large buckets (those with size > 3
√
n). Without loss of generality, suppose such an element is
a ∈ A. Then, for all elements b ∈ B, we check whether a + b ∈ C. This takes O(n1.5) time overall
in expectation.
The remaining buckets Ai, Bi,Ci for all i ∈ [
√
n] contain O(
√
n) elements each. In particular, we
have reduced the original problem to 2n subinstances of 3SUM ((Ai, Bj ,Ckijb ) for b = 0, 1).
Now for each of these 2n subinstances in turn, call the detection algorithm. We can assume
WLOG that the detection algorithm actually returns a triple a ∈ Ai, b ∈ Bj , c ∈ Ckijb , such that
a + b + c = 0 (if it does not, we can recover a,b,c using a simple self-reduction). For each triple
(a,b,c) detected, remove a from Ai and record that a is in a 3SUM. Try to find a new 3SUM in the
subinstance. When the current subinstance contains no more solutions, move to the next subinstance.
Assuming that there is an O(n2−ε ) time algorithm for 3SUM, the running time from this portion
of the reduction becomes asymptotically
(n + 2n) ·
√
n
2−ε
= O(n2−ε /2).
Therefore, All-Ints-3SUM can be solved in O(n1.5 + n2−ε /2) time.
Theorem 8.1 shows how subquadratic algorithms for 3SUM actually imply subquadratic algorithms for an apparently harder function problem (detecting 3-SUM solutions for all integers in
the set). This should be contrasted with Patrascu’s surprising subquadratic equivalence between
3SUM and the apparently easier function problem Convolution-3SUM discussed in Section 2.1.
Both reductions use the same hash family, but they are applied in different ways.
9 CONCLUSION
We have explored a new notion of reducibility that preserves truly subcubic runtimes. Our main
contributions are subcubic reductions from important function problems (such as all-pairs paths and
matrix products) to important decision problems (such as triangle detection and product verification), showing that subcubic algorithms for the latter entail subcubic algorithms for the former. We
have shown that these reductions and the ideas behind them have many interesting consequences.
We conclude with three open questions arising from this work:
(1) DoesO(n3−δ ) negative triangle detection implyO(n3−δ ) matrix multiplication (over any R)?
Note we can currently show that O(n3−δ ) negative triangle implies O(n3−δ /3) matrix multiplication.
(2) Does a truly subquadratic algorithm for 3SUM imply truly subcubic APSP? It is quite possible that truly subquadratic 3SUM implies truly subcubic negative triangle, which would
answer the question.
(3) Is there a truly subcubic algorithm for minimum edge-weight triangle? Although this question has been asked in prior work, clearly it takes on much greater importance now that
we know it is equivalent to asking for a variety of subcubic algorithms for several fundamental path problems. We feel that if there are truly subcubic algorithms for APSP, they
will likely be found by studying the “simpler” minimum edge-weight triangle problem. 