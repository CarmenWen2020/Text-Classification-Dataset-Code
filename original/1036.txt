In this article, we introduce a new approach to approximate counting in bounded degree systems with higherorder constraints. Our main result is an algorithm to approximately count the number of solutions to a CNF
formula Φ when the width is logarithmic in the maximum degree. This closes an exponential gap between
the known upper and lower bounds.
Moreover, our algorithm extends straightforwardly to approximate sampling, which shows that under
Lovász Local Lemma-like conditions it is not only possible to find a satisfying assignment, it is also possible
to generate one approximately uniformly at random from the set of all satisfying assignments. Our approach
is a significant departure from earlier techniques in approximate counting, and is based on a framework to
bootstrap an oracle for computing marginal probabilities on individual variables. Finally, we give an application of our results to show that it is algorithmically possible to sample from the posterior distribution in an
interesting class of graphical models.
CCS Concepts: • Mathematics of computing → Combinatorial algorithms; Probabilistic algorithms; •
Theory of computation → Random walks and Markov chains;
Additional Key Words and Phrases: Approximate counting, Lovász local lemma, graphical models
1 INTRODUCTION
1.1 Background
In this article, we introduce a new approach to approximate counting in bounded degree systems
with higher-order constraints. For example, if we are given a CNF formula Φ with n variables and
m clauses with the property that each clause contains between k and 2k variables and each variable
belongs to at most d clauses, we ask:
Question 1. How does k need to relate to d for there to be algorithms to estimate the number of
satisfying assignments to Φ within a (1 ± 1/nc ) multiplicative factor?
In the case of a monotone CNF formula where no variable appears negated, the problem is
equivalent to the following: Suppose we are given a hypergraph on n nodes andm hyperedges with
the property that each hyperedge contains between k and 2k nodes and each node belongs to at
most d hyperedges. How does k need to relate to d to be able to approximately compute the number
of independent sets? Here, an independent set is a subset of nodes for which there is no induced
hyperedge. Bordewich, Dyer, and Karpinski [5] gave an MCMC algorithm for approximating the
number of hypergraph independent sets (equivalently, the number of satisfying assignments in a
monotone CNF formula) that succeeds whenever k ≥ d + 2. Bezákova et al. [4] gave a deterministic
algorithm that succeeds whenever k ≥ d ≥ 200 and proved that when d ≥ 5 · 2k/2 it is NP-hard to
approximate the number of hypergraph independent sets even within an exponential factor.
More broadly, there is a rich literature on approximate counting problems. In a seminal work,
Weitz [30] gave an algorithm to approximately count in the hardcore model with parameter λ in
graphs of degree at most d whenever
λ ≤ (d − 1)
d−1
(d − 2)d .
And in another seminal work, Sly [28] showed a matching hardness result that was later improved
in various respects by Sly and Sun [29] and Galanis, S˘tefankovic˘
, and Vigoda [11]. These results
show that approximate counting is algorithmically possible if and only if there is spatial mixing.
Moreover, Weitz’s result can be thought of as a comparison theorem that spatial mixing holds on
a bounded degree graph if and only if it holds on an infinite tree with the same degree bound.
There have been a number of attempts to generalize these results to hypergraphs, many of which
follow the approach of defining analogues of the self-avoiding walk trees used in Weitz’s algorithm
[30]. However, what makes hypergraph versions of these problems more challenging is that spatial
mixing fails, even on trees. And we can see that there are exponential gaps between the upper and
lower bounds, since the algorithms above require k to be linear in d while the lower bounds only
rule out k ≤ 2 logd − O(1). (Here and throughout this article, we will take log to be base 2.)
We can take another vantage point to study these problems. Bounded degree CNF formulae are
also one of the principal objects of study in the Lovász Local Lemma [10], which is a celebrated
result in combinatorics that guarantees when k ≥ logd + log k + O(1) that Φ has at least one satisfying assignment. The original proof of the Lovász Local Lemma was non-constructive and did
not yield a polynomial time algorithm for finding such an assignment, even though it was guaranteed to exist. Beck [3] gave an algorithm followed by a parallel version due to Alon [2] that
can find a satisfying assignment whenever k ≥ 8 logd + log k + O(1). And in a celebrated recent
result, Moser and Tardos [24] gave an algorithm exactly matching the existential result. This was
followed by a number of works giving constructive proofs of various other settings and generalizations of the Lovász Local Lemma [1, 16, 18, 22]. However, these works leave open the following
question:
Question 2. Under the conditions of the Lovász Local Lemma (i.e., when k is logarithmic in d, but
perhaps with a larger constant), is it possible to approximately sample from the uniform distribution
on satisfying assignments?
Approximate counting and approximate sampling problems are well known to be related. When
the problem is self-reducible, they are in fact algorithmically equivalent [20, 26]. However, in our
setting the problem is not self-reducible, because as we fix variables, we could violate the assumption that k is at least logarithmic in d. It is natural to hope that under exactly the same conditions as
the Lovász Local Lemma, there is an algorithm for approximate sampling that matches the limits
of the existential and now algorithmic results. However, the hardness results of Bezákova et al. [4]
Journal of the ACM, Vol. 66, No. 2, Article 10. Publication date: April 2019.
Approximate Counting, the Lovász Local Lemma, and Inference in Graphical Models 10:3
imply that we need at least another factor of two, and that it is NP-hard to approximately sample
when k ≤ 2 logd − O(1).
1
In fact, there is another connection between the Lovász Local Lemma and approximate counting.
Scott and Sokal [29] showed that given the dependency graph of events in the local lemma, the best
lower bound on the probability of an event guaranteed to exist by the Lovász Local Lemma (i.e.,
the fraction of satisfying assignments) is exactly the solution to some counting problem. Harvey,
Srivastava, and Vondrák [17] recently adapted techniques of Weitz to complex polydisks and gave
an algorithm for approximately computing this lower bound. This yields a lower bound on the
fraction of satisfying assignments; however, the actual number could be exponentially larger.
1.2 Our Results
Our main result is an algorithm to approximately count the number of solutions when k is at least
logarithmic in d. In what follows, let c, k, and d be constants. We prove2:
Theorem 1.1 (Informal). Suppose Φ is a CNF formula where each clause contains between k and
2k variables and at most d clauses contain any one variable. For k  60 logd, there is a deterministic
polynomial time algorithm for approximating the number of satisfying assignments to Φ within a
multiplicative (1 ± 1/nc ) factor. Moreover, there is a randomized polynomial time algorithm to sample from a distribution that is 1/nc -close in total variation distance to the uniform distribution on
satisfying assignments.
See Theorems 6.2 and 6.4 for the corresponding formal theorem statements.
This algorithm closes an exponential gap between the known upper bounds [4, 5] and lower
bounds [4]. It also shows that under Lovász Local Lemma-like conditions not only is it possible
to efficiently find a satisfying assignment, it is possible to find a random one. In this regime of
parameters, the solution space need not even be connected. Our approach is a significant
departure from earlier techniques based either on path coupling [5] or adapting Weitz’s approach
to non-binary models and hypergraphs [4, 12, 23, 25, 27]. The results above appear in Theorems 6.2
and 6.4. Moreover, our techniques seem to extend to many non-binary counting problems, as we
explain in Section 7.
Our approach starts from a thought experiment about what we could do if we had access to a
very powerful oracle that could answer questions about the marginal distributions of individual
variables under the uniform distribution on satisfying assignments. We use this oracle and properties of the Lovász Local Lemma (namely, bounds it gives on the marginal distribution of individual
variables) to construct a coupling between two random satisfying assignments so that both agree
outside some logarithmic sized component. If we could construct this coupling, then we could
use brute-force search to find the ratio of the number of satisfying assignments with x = T to
the number with x = F to compute marginals at x. However, the distribution of what component
the coupling produces intimately depends on the powerful oracle we have assumed that we have
access to.
Instead, we abstract the coupling procedure as a random root-to-leaf path in a tree that represents the state of the coupling. We show that at the leaves of this tree, there is a way to fractionally
charge assignments where x = T against assignments where x = F . Crucially, doing so requires
only brute-force search on a logarithmic sized component. Finally, we show that there is a polynomial sized linear program to find a flow through the tree that produces an approximately valid way
1The hardness results in Reference [4] are formulated for approximate counting but carry over to approximate sampling. In
particular, an oracle for approximately sampling from the set of satisfying assignments yields an oracle for approximating
the marginal at any variable. Then one can invoke Lemma 7 in Reference [4]. 2We have not made an attempt to optimize the constant in this theorem.
Journal of the ACM, Vol. 66, No. 2, Article 10. Publication date: April 2019. 
10:4 A. Moitra
Fig. 1. An example cause network with n hidden variables. A sample is generated by choosing each hidden
variable to be T/F independently with equal probability and observing the truth values of each clause.
to fractionally charge assignments with x = T against ones with x = F , and that any such solution
certifies the correct marginal distribution. From these steps, we have thus bootstrapped an oracle
for answering queries about the marginal distribution. Our main results then follow from utilizing
this oracle. In settings where the problem is self-reducible [26] it is well known how to go from
knowing the marginal to approximate counting and sampling. In our setting, the problem is not
self-reducible, because setting variables could result in clauses becoming too small, in which case
k would not be large enough as a function of d. We are able to get around this by using the Lovász
Local Lemma once more to find a safe ordering in which to set the variables.
In an exciting development and just after the initial posting of this article, Hermon, Sly, and
Zhang [19] gave an algorithm for approximately counting the number of independent sets in a
k-uniform hypergraph of maximum degree d provided that k ≥ 2 logd + O(1). Their results are
incomparable to ours, because they correspond to approximately counting in a monotone CNF
formula. The techniques are entirely different and their algorithm matches the hardness result
in Reference [4] up to an additive constant! It remains an interesting question to find similarly
sharp phase transitions for the approximate counting problems studied here, namely for nonmonotone CNFs. And in yet another interesting direction, Guo et al. [13] gave an algorithm based
on connections to “cycle popping” that can uniformly sample from Φ under weaker conditions
on the degree but by imposing conditions on intersection properties of bad events. In a related
direction and building on our techniques, Guo et al. [14] gave an algorithm for approximately
counting the number of q-colorings in a k-uniform hypergraph of maximum degree d, provided
that k ≥ 28 and q ≥ 315d 14
k−14 . This is the first algorithm that can handle q being much smaller than
d without any additional assumptions. While their overall algorithm follows a similar structure to
ours, an important innovation is in circumventing our marking procedure and instead deciding
which variables to set next in the coupling procedure in an adaptive manner.
1.3 Further Applications
Our algorithms have interesting applications in graphical models. Directed graphical models are
a rich language for describing distributions by the conditional relationships of their variables.
However, very little is known algorithmically about learning their parameters or performing basic
tasks such as inference [8, 9]. In most settings, these problems are computationally hard. However,
we can study an interesting class of directed graphical models that we call cause networks. See
Figure 1.
Journal of the ACM, Vol. 66, No. 2, Article 10. Publication date: April 2019.
Approximate Counting, the Lovász Local Lemma, and Inference in Graphical Models 10:5
Definition 1.2. In a cause network, there is a collection of hidden variables x1, x2,..., xn that
are chosen independently to be T or F with equal probability. There is a collection of m observed
variables, each of which is either an OR or an AND of several variables or their negations.
Our goal is: Given a random sample x1, x2,..., xn from the model where we observe the truth
value of each of them clauses, sample from the posterior distribution on the hidden variables. This
generalizes graphical models such as the symptom-disease network where the hidden variables
represent diseases that a patient may have, and the clauses represent observed symptoms. We will
require the following regularity condition on our observations:
Definition 1.3. A collection of observations is k-regular if for every observed variable, the corresponding clause is adjacent to (i.e., shares a variable with) at most 15k/16 OR clauses that are
false and at most 15k/16 AND clauses that are true.
Now, as an immediate corollary, we have:
Corollary 1.4. Given a cause network where each observed variable depends on between k and 2k
hidden variables, each hidden variable affects at most d observed variables and k  70 logd, there is
a polynomial time algorithm for sampling from the posterior distribution for any k-regular collection
of observations.
This is a rare setting where there is an algorithm to solve an inference problem in graphical models
but (i) the underlying graph does not have bounded treewidth and (ii) correlation decay fails. We
believe that our techniques may eventually be applicable to settings where the observed variables
are noisy functions of the hidden variables and where the hidden variables are not distributed
uniformly.
2 PRELIMINARIES
In this article, we will be interested in approximately counting the number of satisfying assignments to a CNF formula. For example, we could be given
Φ = (x1 ∨ x3 ∨ x5) ∧ (x2 ∨ x3 ∨ x8) ∧···∧ (x4 ∨ x5 ∨ x9).
Let us fix some parameters. We will assume that there are n variables and there are m clauses,
each of which is an OR of between k and Ck literals on distinct variables. The constant C will
take values either 2 or 6, because of the way our algorithm will be built on various subroutines.
Finally, we will require a degree bound that each variable appears in at most d clauses. We will be
interested in the relationships between k and d that allow us to approximately count the number
of satisfying assignments in polynomial time.
The celebrated Lovász Local Lemma tells us conditions on k and d, where we are guaranteed
that there is at least one satisfying assignment. Let D be an upper bound on the degree of the
dependency graph. We can take D = 2dk or D = 6dk, depending on whether we are in a situation
where there are at most 2k or at most 6k variables per clause.
Theorem 2.1. [10] If e (D + 1) ≤ 2k , then Φ has at least one satisfying assignment.
Moser and Tardos [24] gave an algorithm to find a satisfying assignment under these same
conditions. However, the assignment that their randomized algorithm finds is fundamentally not
uniform from the set of all satisfying assignments. Our goal is to be able to both approximately
count and uniformly sample when k is logarithmic in d.
There are many more related results, but we will not review them all here. Instead, we state a
version of the asymmetric local lemma given in Reference [15], which gives us some control on
the uniform distribution on satisfying assignments. Let C be the collection of clauses in Φ. Let
Journal of the ACM, Vol. 66, No. 2, Article 10. Publication date: April 2019. 
10:6 A. Moitra
Pr[·] denote the uniform distribution on all assignments—i.e., uniform on {T, F }
n. Finally, for a
clause b, let Γ(b) denote all the clauses that intersect b. We will abuse notation, and for any event
a that depends on some set of the variables, let Γ(a) denote all the clauses that contain any of the
variables on which a depends.
Theorem 2.2. Suppose there is a function x : C → (0, 1) such that for all c ∈ C we have
Pr[c is unsatisfied] ≤ x (c)

b ∈Γ(c )
(1 − x (b)),
then there is at least one satisfying assignment. Moreover, the uniform distribution D on satisfying
assignments satisfies that for any event a
PrD[a] ≤ Pr[a]

b ∈Γ(a)
(1 − x (b))−1
.
Notice that this inequality is one-sided, as it ought to be. After all, if we take b to be some
clause and a to be the event that b is not satisfied, then we know that PrD[a] = 0 even though
Pr[a] is nonzero. However, what this theorem does tell us is that the marginal distribution of D
on any variable is close to uniform. We will establish a quantitative version of this statement in
the following corollary:
Corollary 2.3. Suppose that eDs ≤ 2k . Then, for every variable xi , we have
1
2 − 2
s ≤ PrD[xi = T ] ≤
1
2
+
2
s
.
Proof. We will assume s ≥ 2, since otherwise the corollary is trivial. Set x (c) = 1
Ds for each
clause c, and consider the event a that xi = T . Now invoking Theorem 2.2, we calculate
PrD[xi = T ] ≤ Pr[xi = T ]

b ∈Γ(a)
(1 − x (b))−1
≤
 1
2
 1 − 1
Ds −D
≤
1
2
+
2
s
,
where the last inequality follows, because (1 − 1
Ds )
−D ≤ e
2
s ≤ 1 + 4
s . An identical calculation
works for the event xi = F . All that remains is to check that the condition in Theorem 2.2 holds,
which is a standard calculation: If c is a clause, then
Pr[c is unsatisfied] ≤
 1
Ds  1 − 1
Ds D
.
The left-hand side is at most 2−k , because each clause has at least k distinct variables, and the
right-hand side is at least ( 1
Ds )( 1
e ). Rearranging completes the proof.
Notice that k is still only logarithmic in d but with a larger constant, and by increasing this
constant, we get some useful facts about the marginals of the uniform distribution on satisfying
assignments.
3 A COUPLING PROCEDURE
3.1 Marked Variables
Throughout this section, we will assume that the number of variables per clause is between k
and 6k. Now, we are almost ready to define a coupling procedure. The basic strategy that we will
employ is to start from either x = T and x = F , and then sample from the corresponding marginal
distribution on satisfying assignments. If we sample a variable y next, then Corollary 2.3 tells us
Journal of the ACM, Vol. 66, No. 2, Article 10. Publication date: April 2019.        
Approximate Counting, the Lovász Local Lemma, and Inference in Graphical Models 10:7
that regardless of whether x = T or x = F , each clause has at least k − 1 variables remaining and
so the marginal distribution on y is still close to uniform.
Thus, we will try to couple the conditional distributions, when starting from x = T or x = F
as well as we can, to show that the marginal distribution on variables that are all at least some
distance Δ away must converge in total variation distance. There is, however, an important catch
that motivates the need for a fix. Imagine that we continue in this fashion, sampling variables
from the appropriate conditional distribution. We can reach a situation where a clause c has all of
its variables except y set and yet the clause is still unsatisfied. The marginal distribution on y is
no longer close to uniform. Hence, reaching small clauses is problematic, because then we cannot
say much about the marginal distribution on the remaining variables and it would be difficult to
construct a good coupling.
Instead, our strategy is to use the Lovász Local Lemma once more, but to decide on a set of
variables in advance that we call marked.
Lemma 3.1. Set c0 = e ( 1
2 )( 1
4 )
2
. Suppose that 2e (D + 1) ≤ ck
0 . Then there is an assignment
M : {xi}
n
i=1 → {marked, unmarked}
such that every clause c has at least k
4 marked and at least k
4 unmarked variables.
Proof. We will choose each variable to be marked or unmarked with equal probability, and
independently. Consider the m bad events, one for each clause c, that c does not have enough
marked or enough unmarked variables. Then, we have
Pr[c is bad] ≤ 2e−( 1
2 )( 1
4 )
2k = 2c−k
0 ,
which follows from the Chernoff bound. Now, we can appeal to the Lovász Local Lemma to get
the desired conclusion.
Only the variables that are marked will be allowed to be set to eitherT or F by the coupling procedure. Lemma 3.1 guarantees that every clause c always has enough remaining variables that can
make it true so that the marginal distribution on any marked variable is always close to uniform.
3.2 Factorizing Formulas
Now fix a variable x. We will build up two partial assignments A1 and A2, and will use the notation
A1 (x) = T and A2 (x) = F
to indicate that the first partial assignment sets x toT , and the second one sets x to F . Furthermore,
we will refer to the conditional distribution that is uniform on all satisfying assignments consistent with the decisions made so far in A1 as D1. Similarly, we will refer to the other conditional
distribution as D2. Note that these distributions are updated as more variables are set.
We can now state our goal. Suppose we have partial assignments A1 and A2. Then, we will
want to write
ΦA1 = ΦI1 ∧ ΦO1 ,
where ΦA1 is the subformula we get after making the assignments in A1 and simplifying—i.e.,
removing literals (a variable or its negation) that are set to F , and deleting clauses that already
have a literal set to T . Similarly, we will want to write
ΦA2 = ΦI2 ∧ ΦO2 .
Finally, we want the following conditions to be met:
(1) ΦO1 = ΦO2 (:= ΦO )
(2) ΦI1 and ΦO share no variables, and similarly for ΦI2 and ΦO .
Journal of the ACM, Vol. 66, No. 2, Article 10. Publication date: April 2019. 
10:8 A. Moitra
The crucial point is that if we can find partial assignments A1 and A2 where ΦA1 and ΦA2 meet
the above conditions, then the conditional distribution on all variables in ΦO is exactly the same.
We will use the notation
D1



vars(ΦO )
to denote the conditional distribution of D1 projected onto just the variables in ΦO , and similarly
for D2. Then, we have
Lemma 3.2. If the above factorization conditions are met, then
D1



vars(ΦO )
= D2



vars(ΦO )
.
Proof. From the assumption that ΦA1 = ΦI1 ∧ ΦO and because ΦI1 and ΦO share no variables, it
means that there are no clauses that contain variables from both the subformulas ΦI1 and ΦO . Any
such clause would prevent us from writing the formula ΦA1 in such a factorized form. Thus, the
distribution D1 is simply the cross product of the uniform distributions on satisfying assignments
to ΦI1 and ΦO . An identical statement holds for D2, which completes the proof.
Note that meeting the factorization conditions does not mean that the numbers of satisfying
assignments to ΦA1 and ΦA2 are the same.
3.3 Factorization Via Coupling
Our goal in this subsection is to give a coupling procedure (Algorithm 1) to generate partial assignments A1 and A2 starting from x = T and x = F , respectively, that result in a factorized formula.
In fact, we will set exactly the same set S of variables in both, although not all variables will be set
to the same value in the two partial assignments and this set S will also be random.
There are two important constraints that we will impose on how we construct the partial assignments, which will make it somewhat tricky. First, suppose we have only set the variable x and
next we choose to set the variable y in both A1 and A2. We will want the distribution on how
we set y in the coupling procedure in A1 to match the conditional distribution D1 and similarly
for A2. Now, suppose we terminate with some set S having been set. We can continue sampling
the variables in S¯ from D1, and we are now guaranteed that the full assignment we generate is
uniform from the set of assignments with x = T . An identical statement holds when starting with
x = F . Second, we will want that with very high probability, the coupling procedure terminates
with not too many variables in the formula ΦI1 or ΦI2 . Finally, we will assume that we are given
access to a powerful oracle:
Definition 3.3. We will call the following a conditional distribution oracle: Given a CNF formula
Φ, a partial assignment A, and a variable y, it returns the probability that y = T in a uniformly
random satisfying assignment that is consistent with A.
Such an oracle is obviously very powerful, and it is well known that if we had access to it we
could compute the number of satisfying assignments to Φ exactly with a polynomial number of
queries. However, one should think of the coupling procedure as a thought experiment, which will
be useful in an indirect way to build up towards our algorithm for approximate counting3.
Here, we record some important properties of Algorithm 1. Notice that a clause c can only trigger
the WHILE loop at most once. If it ends up in Case # 1, then it is deleted from the formula. If it
ends up in Case # 2, then all its variables are included in VI and once a variable is included in VI
3Here, by “best coupling at each step,” we mean that sequentially for each variable we maximize the probability that
Pr[A1 (y) = A2 (y)] while preserving the fact that y is set in A1 and A2 according to D1 and D2, respectively.
Journal of the ACM, Vol. 66, No. 2, Article 10. Publication date: April 2019.             
Approximate Counting, the Lovász Local Lemma, and Inference in Graphical Models 10:9
ALGORITHM 1: Coupling Procedure
Input: Monotone CNF Φ, variable x and conditional distribution oracle Z
(1) Using Lemma 3.1, label variables as marked or unmarked
(2) Initialize A1 (x) = T and A2 (x) = F
(3) Initialize VI = {x} and VO = {xi}
n
i=1 \ {x}
(4) While there is a clause c with variables in both VI and VO
(5) Sequentially sample its marked variables (if any) from D1 and D2, using Z to construct best
coupling at each step3
(6) Case # 1: c is already satisfied in both A1 and A2
(7) Let S be the variables in c that have different truth values in A1 and A2.
(8) Update VI ← VI ∪ S, VO ← VO \ S
(9) Delete c
(10) Case # 2: c is not already satisfied by both A1 and A2
(11) Let S be all variables in c (marked or unmarked)
(12) Update VI ← VI ∪ S, VO ← VO \ S
(13) End
it is never removed. Thus, Algorithm 1 clearly terminates. Our first step is to show that when it
does terminate, the formula factorizes. Let CI be the set of remaining clauses that have all of their
variables in VI . Similarly, let CO be the set of remaining clauses that have all of their variables in
VO . Then, set
Φ

I = ∧c ∈CIc
and let ΦI1 and ΦI2 be the simplifications of Φ

I with respect to the partial assignments A1 and A2,
respectively. Similarly, set
Φ

O = ∧c ∈CO c
and let ΦO1 and ΦO2 be the simplifications of Φ

O with respect to the partial assignments A1 and
A2, respectively.
Claim 1. All variables with different truth assignments in A1 and A2 are in VI .
Proof. A variable is set in response to it being contained in some clause c that triggers the
WHILE loop. Any such variable having different truth assignments is moved into VI in both
Case # 1 and Case # 2.
Now, we have an immediate corollary that will help us in proving that Algorithm 1 finds a pair
of partial assignments for which Φ factorizes:
Corollary 3.4. ΦO1 = ΦO2 .
Proof. Recall that ΦO1 and ΦO2 come from simplifying Φ

O (which contains only variables in
VO ) according to A1 and A2. From Claim 1, we know that A1 and A2 are equal when restricted
to VO and, thus, we get the same formula in both cases.
Now that we know they are equal, we can define ΦO = ΦO1 = ΦO2 . What remains is to show
that the subformulas we have are actually factorizations of the original formula Φ:
Lemma 3.5. ΦA1 = ΦI1 ∧ ΦO and ΦA2 = ΦI2 ∧ ΦO .
Proof. When the WHILE loop in Algorithm 1 terminates, every clause c in the original formula
Φ either has all of its variables inVI or inVO or was deleted, because it already contains at least one
variable in both A1 and A2 that satisfies it (although it need not be the same variable). Hence, every
clause in Φ that is not already satisfied in both A1 and A2 shows up in Φ

I ∧ Φ

O . Some clauses that
Journal of the ACM, Vol. 66, No. 2, Article 10. Publication date: April 2019.  
10:10 A. Moitra
are already satisfied in both may show up as well. In any case, this completes the proof, because
the remaining operation just simplifies the formulas according to the partial assignments.
3.4 How Many Steps Does the Coupling Procedure Take to Terminate?
What remains is to bound the probability that the number of variables included in VI is at most t.
First, we need an elementary definition:
Definition 3.6. When a variable xi is given different truth assignments in A1 and A2, we call it
a type 1 error. When a clause c has all of its marked variables set in both A1 and A2, but in at least
one of them is not yet satisfied, we call it a type 2 error.
Note that it is possible for a variable to participate in both a type 1 and type 2 error. In any
case, these are the only reasons that a variable is included in VI in an execution of the coupling
procedure:
Observation 1. All variables in VI are included either due to a type 1 error or a type 2 error, or
both.
Now our approach to showing that VI contains not too many variables with high probability
is to show that if it did, there would be a large collection of disjoint errors. First, we construct a
useful graph underlying the process:
Definition 3.7. Let G be the graph on vertices VI where we connect variables if and only if they
appear in the same clause together (any clause from the original formula Φ).
The crucial property is that this graph is connected:
Observation 2. G is connected.
Proof. This property holds by induction. Assume that at the start of the WHILE loop, the property holds. Then, at the end of the loop, any variable xi added to VI must have been contained in
a clause c that at the start of the loop had one of its variables in VI . This completes the proof.
Now, by Observation 1, for every variable in VI , we can blame it on either a type 1 or a type
2 error. Both of these types of errors are unlikely. But for each variable, charging it to an error
is problematic because of overlaps in the events. In particular, suppose we have two variables xi
and xj that are both included in VI . It could be that both variables are in the same clause c, which
resulted in a type 2 error, in which case we could only charge one of the variables to it. This turns
out not to be a major issue.
The more challenging type of overlap is when two clausesc and c
 both experience type 2 errors
and overlap. In isolation, each clause would be unlikely to experience a type 2 error. But it could
be that c and c
 share all but one of their marked variables, in which case once we know that c
experiences a type 2 error, then c
 has a reasonable chance of experiencing one as well. We will
get around this issue by building what we call a 3-tree. This approach is inspired by Noga Alon’s
parallel algorithmic local lemma [2], where he uses a 2,3-tree.
Definition 3.8. We call a graph T on subset of VI a 3-tree if each vertex is distance at least 3 from
all the others, and when we add edges between vertices at distance exactly 3 the tree is connected.
Next, we show that G contains a large 3-tree:
Lemma 3.9. Suppose that any clause contains between k and 6k variables. Then, any maximal
3-tree contains at least |VI |
2(6dk)2 vertices.
Journal of the ACM, Vol. 66, No. 2, Article 10. Publication date: April 2019.  
Approximate Counting, the Lovász Local Lemma, and Inference in Graphical Models 10:11
Proof. Consider a maximal 3-tree T . We claim that every vertex xi ∈ VI must be distance at
most 2 from some xj inT . If not, then we could take the shortest path from xi toT and move along
it, and at some point we would encounter a vertex that is also not in T whose distance from T is
exactly 3, at which point we could add it, contradicting T ’s maximality. Now, for every xi in T , we
remove from consideration at most (6dk)
2 + (6dk) other variables (all those at distance at most 2
from xi in G). This completes the proof.
Now, we can indeed charge every variable in T to a disjoint error:
Claim 2. If two variables xi and xj in T are the result of type 2 errors for c and c

, then
vars(ci ) ∩ vars(cj) = ∅.
Proof. For the sake of contradiction, suppose that vars(ci ) ∩ vars(cj)  ∅. Then, since c and c

experience type 2 errors, all of their variables are included in VI . This gives a length 2 path from
xi to xj in G, which if they were both included in T , would contradict the assumption that T is a
3-tree.
We are now ready to prove the main theorem of this section:
Theorem 3.10. Suppose that every clause contains between k and 6k variables and that logd ≥
5 log k + 20 and k ≥ 50 logd + 50 log k + 250. Then,
Pr[|VI | ≥ 2(6dk)
2
t] ≤
 1
2
t
.
Proof. First, note that the conditions on k and d imply that the condition in Lemma 3.1 holds.
Now, suppose that |VI | ≥ 2(6dk)
2
t. Then, by Lemma 3.9, we can find a 3-tree T with at least t
vertices. First, we will work towards bounding the probability of any particular 3-tree on t vertices.
We note that, since each clause has at least k/4 marked variables and has at most 6k total variables,
we can bound the probability of a type 1 error as
Pr[x has type 1 error] ≤
4
d11 .
This uses the assumption ed11 (6dk) ≤ 2k/4, which allows us to choose s = d11 in Corollary 2.3.
Also, we can bound the probability of a variable participating in a type 2 error as
Pr[x participates in a type 2 error] ≤ d
 49
100 k/4
,
which follows from Corollary 2.3 using the condition that s ≥ d3 ≥ 100 and that each variable
belongs to at most d clauses and each clause has at least k/4 marked variables. Now, by Claim 2,
we know that clauses that cause the type 2 errors for each vertex in T are disjoint. Thus, putting
it all together, we can bound the probability of any particular 3-tree on t vertices as
 4
d11 + d
 49
100 k/4t
.
Now, it is well known (see References [2, 21]) that the number of trees of size t containing a fixed
vertex in a graph of degree at most Δ is at most (eΔ)t . Moreover, if we connect pairs of vertices inG
that are distance exactly 3 from each other, then we get a new graph H whose maximum degree is
at most Δ = 3(6dk)
3. Thus, putting it all together, we have that the probability that |VI | > 2(6dk)
2
t
can be bounded by
 4
d11 + d
 49
100 k/4t
(3e (6dk)
3)
t ≤
 1
2
t
,
Journal of the ACM, Vol. 66, No. 2, Article 10. Publication date: April 2019.          
10:12 A. Moitra
where the last inequality follows from the assumptions that logd ≥ 5 log k + 20 and k ≥ 50 logd +
50 log k + 250.
Thus, we can conclude that with probability at least 1 − 1
nc the number of variables in VI is at
most τ where τ = 2c(6dk)
2 logn. In our description of the coupling procedure, we relied on access
to a conditional distribution oracle. But suppose for the moment that we could get around this
and run the coupling procedure without such an oracle. Then, whenever the coupling procedure
terminates with |VI | ≤ τ , we could use brute force to count the number of satisfying assignments
to ΦI1 and ΦI2 . The number of candidate assignments we would need to search over is polynomial
in ncd2k2
and, thus, the algorithm would run in polynomial time for any fixed c, d, and k.
The important point is that, because the formula factorizes upon termination of the coupling
procedure, the ratio between the number of satisfying assignments in ΦI1 and ΦI2 is the same as
the ratio of the number of satisfying assignments in ΦA1 and ΦA2 . In the next section, we show
how we can use these ratios to approximate the marginal probability that x is set to true under a
random satisfying assignment of Φ. Then, eventually we will need to find a way around our use
of the conditional distribution oracle.
4 IMPLICATIONS OF THE COUPLING PROCEDURE
In this section, we give an abstraction that allows us to think about the coupling procedure as a
randomly chosen root-to-leaf path in a certain tree whose nodes represent states. First, we make
an elementary observation that will be useful in discussing how this tree is constructed. Recall
that the coupling procedure chooses any clause that contains variables in both VI and VO and then
samples all marked variables in it. We will assume without loss of generality that the choices it
makes are done in lexicographic order. So, if the clauses in Φ are ordered arbitrarily asc1,c2,...,cm
and the variables are ordered as x1, x2,..., xn, if when executing the WHILE loop the algorithm
has a choice of more than one clause, it selects among them the clause ci with the lowest subscript
i. Similarly, given a choice of which marked variable to sample next, it selects the xj with the
lowest subscript j.
The important point is that now we can think of a state associated with the coupling procedure,
which we will denote by σ.
Definition 4.1. The state σ of the coupling procedure specifies the following:
(1) The set of remaining clauses C

—i.e., clauses that have not yet been deleted;
(2) The current partition of the variables into VI and VO ;
(3) The set S of variables whose values have been set, along with their values in both A1 and
A2;
(4) The current clause c∗ being operated on in the while loop, if any.
We will assume that the set M of marked variables is fixed once and for all. Now, the transition
rules from state σ are as follows:
(1) Ifc∗ has any marked variables that are unset, then the algorithm chooses the lexicographically first and sets it.
(2) Ifc∗ has no remaining marked variables to set, then the algorithm updates C

, VI , and VO ,
according to whether c∗ falls into Case # 1 or Case #2 in Algorithm 1 and sets the current
clause to empty.
(3) If the current clause is empty, then the algorithm chooses the lexicographically first clause
from C

, which has at least one variable in each of VI and VO to be c∗.
Finally, we can define the next variable operation:
Journal of the ACM, Vol. 66, No. 2, Article 10. Publication date: April 2019. 
Approximate Counting, the Lovász Local Lemma, and Inference in Graphical Models 10:13
ALGORITHM 2: Decision Tree Sampling
Input: Monotone CNF Φ, stochastic decision tree S
(1) Choose a random root-to-leaf path in S
(2) Choose a uniformly random assignment A1 consistent with A1
(3) Choose a uniformly random assignment A2 consistent with A2
(4) Output A1 with probability q = PrD[x = T ], and otherwise output A2
Definition 4.2. Let R : Σ → {xi}
n
i=1 ∪ {∅} × Σ be the function that takes in a state σ, transitions
to the next state σ 
 that sets some variable y and outputs (y, σ 

).
Note that some states σ do not immediately set a variable—e.g., if the next operation is to choose
the next clause, or update C

, VI , and VO . These latter transitions are deterministic, so we let σ 
 be
the end resulting state and y be the variable that it sets. Now, we can define the stochastic decision
tree underlying the coupling procedure:
Definition 4.3. Given a conditional distribution oracle Z, the function R and a stopping threshold s, the associated stochastic decision tree is the following:
(1) The root node corresponds to the state where only x is set, A1 (x) = T , A2 (x) = F ,VI = {x}
and VO = {xi}
n
i=1 \ {x}.
(2) Each node has either zero or four descendants. If the current node corresponds to state σ,
let (y, σ 

) = R(σ ). Then, if y = ∅ or if |VI | = τ , there are no descendants and the current
node is a leaf corresponding to the termination of the coupling procedure or |VI | being
too large. Otherwise, the four descendants correspond to the four choices for how to set
y in A1 and A2, and are marked with the state σ 

, which incorporates their respective
choices into σ 

.
(3) Moreover, the probability on an edge from a state σ 
 to a state σ 

 where y has been set as
A1 (y) = T and A2 (y) = T is equal to
min(D1 (y), D2 (y))
and the transition to the state where A1 (y) = F and A2 (y) = F has probability
min(1 − D1 (y), 1 − D2 (y)).
Finally, if D1 (y) > D2 (y), then the transition to A1 (y) = T and A2 (y) = F is non-zero
and is assigned all the remaining probability. Otherwise, the transition to A1 (y) = F and
A2 (y) = T is non-zero and is assigned all the remaining probability.
Definition 4.4. We will refer to a leaf node where the coupling has terminated as a coupled leaf
node. Otherwise, we will refer to it as an uncoupled leaf node.
Now, we can use the stochastic decision tree to give an alternative procedure to sample a
uniformly random satisfying assignment of Φ. We will refer to the process of starting from the
root, and choosing a descendant with the corresponding transition probability, until a leaf node is
reached as “choosing a random root-to-leaf path.”
Claim 3. Algorithm 2 outputs a uniformly random satisfying assignment of Φ.
Proof. We could alternatively think of Algorithm 2 as deciding on whether (at the end) to
output A1 or A2 with probability q vs. 1 − q at the outset. Then, if we choose to output A1 and we
only keep track of the choices made for A1, marginally these correspond to sequentially sampling
the assignment of variables from D1. And when we reach a leaf node in S, we can interpret the
Journal of the ACM, Vol. 66, No. 2, Article 10. Publication date: April 2019.
10:14 A. Moitra
Fig. 2. A transformation on the stochastic decision tree that makes it easier to understand what happens
when we condition on the assignment A that is output by Algorithm 2. It will also be used in Section 5.2.
remaining choices to A1 as sampling all unset variables from D1. Thus, the output in this case
is a uniformly random satisfying assignment with x = T . An identical statement holds for when
we choose to output A2, and because we decided between them at the outset with the correct
probability, this completes the proof of the claim.
Now, let σ be the state of a leaf node u and let A1 and A2 be the resulting partial assignments.
Let p1 be the product of certain probabilities along the root-to-leaf path. In particular, suppose
along the path there is a transition with y being set. Let q1 be the probability of the transition to
(A1 (y), A2 (y))—i.e., along the branch that it actually went down. And let q2 be the probability of
the transition to (A1 (y), A2 (y))—i.e., where y is set the same in A1 but is set to the opposite value
as it was in A2. We let p1 be the product of all q1
q1+q2 over all such decisions on the root-to-leaf
path.
Lemma 4.5. Let A be an assignment that agrees with A1. Then, for Algorithm 2,
Pr[terminates at leaf u|outputs assignment A] = p1.
Proof. The idea behind this proof is to think of the random choice of which of the four descendants to transition to as being broken down into two separate random choices where we first
choose A1 (y) and then we choose A2 (y). See Figure 2. Now, we can make the random choices
in Algorithm 2 in an entirely different order. Instead of choosing the transition in the first layer,
then the second layer, and so on, we instead make all of the choices in the odd layers. Moreover,
at each leaf, we choose which assignment consistent with A1 we would output. This is the first
phase. Next, we choose whether to output the assignment consistent with A1 or with A2. Finally,
we make all the choices in the even layers, which fixes the root-to-leaf path, and then we choose
an assignment consistent with A2. This is the second phase.
The key point is that once the output A is fixed, all of the choices in the first phase are determined, because every time a variable y is set, it must agree with its setting in A. Moreover, each
leaf node must choose A for its assignment consistent with A1. And finally, we know that the
sampling procedure must output the assignment consistent with A1, because A agrees with A1
and not A2 (because they differ on how they set x). Thus, conditioned on outputting A, the only
random choices left are those in the second phase. Now, the lemma follows, because the probability of reaching leaf node u is exactly the probability along the path of all of the even layer choices,
which is how we defined p1.
Journal of the ACM, Vol. 66, No. 2, Article 10. Publication date: April 2019.  
Approximate Counting, the Lovász Local Lemma, and Inference in Graphical Models 10:15
We can define p2 in an analogous way to how we defined p1, and the lemma above shows that
p2 is exactly the probability of all the decisions made along the root-to-leaf path conditioned on
the output being A where A agrees with A2. The key lemma is the following:
Lemma 4.6. Let N1 be the number of satisfying assignments consistent with A1 and let N2 be the
number of satisfying assignments consistent with A2. Then,
p1N1
p2N2
= q
1 − q
.
Proof. Let u be a leaf node. Consider a random variable Zu that when we run Algorithm 2 is
non-zero if and only if we end atu. Moreover, letZu = (1 − q) if an assignment with x = T is output,
and Zu = −q if an assignment with x = F is output. Then, clearly E[Zu ] = 0. Now, alternatively,
we can write
E[Zu ] = E
A
[E[Zu |A is output]],
where A is a uniformly random satisfying assignment of Φ, precisely because of Claim 3. Let N be
the total number of such assignments. Then,
E
A
[E[Zu |A]] =
N1
N

(p1)(1 − q) +
N2
N

(p2)(−q).
This follows because the only assignments A that can be output at u must be consistent with either
A1 or A2. Note that these are disjoint events, because in one of them x = T while in the other
x = F . Then, once we know that A is consistent with A1 (which happens with probability N1
N ), the
probability for the decisions made in A2 being such that we reach u is exactly p1, as this was how
it was defined. The final term in the product of three terms is just the value of Zu . An identical
argument justifies the second term. Now, using the fact that the above expression evaluates to zero
and rearranging completes the proof.
5 CERTIFYING THE MARGINAL DISTRIBUTION
5.1 Overview
The stochastic decision tree that we defined in the previous section is a natural representation of
the trajectory of the coupling procedure. Its crucial property is captured in Lemma 4.6, which gives
a relation between
(1) q—the probability that a satisfying assignment to Φ sets x to true
(2) pi—the conditional probability of a satisfying assignment consistent with Ai reaching a
particular leaf node u and
(3) Ni—the number of satisfying assignments consistent with Ai
for i = 1, 2. In particular, at every leaf node u (whether it is coupled or uncoupled), the equation
p1N1
p2N2
= q
1 − q
is satisfied.
Why is this property useful? Imagine that we did not know q but that someone gave use a
stochastic decision tree satisfying the equation above at every leaf node. Furthermore, suppose that
every leaf node is coupled. We claim that we could convince ourselves, in polynomial time, that q
is the correct marginal probability. First, we could compute p1N1
p2N2
. It is straightforward to compute
p1 and p2, and we can use the fact that the coupling terminated to compute N1
N2
. More precisely,
Journal of the ACM, Vol. 66, No. 2, Article 10. Publication date: April 2019.   
10:16 A. Moitra
because the coupling terminated, we know that when we substitute in the partial assignments, the
simplified formulas factorize as
ΦAi = ΦI1 ∧ ΦO
Thus, the ratio N1
N2 is the same as the ratio between the numbers of satisfying assignments to ΦI1
and ΦI2 . Finally, these are formulas on a logarithmic number of variables so we can compute the
number of satisfying assignments in polynomial time by brute-force search.
Now, we could check that at every leaf node u, the ratio p1N1
p2N2 is the same and solve for q in the
equation in Lemma 4.6. But why does this guarantee that q is the correct marginal probability?
Consider any satisfying assignment to Φ that sets x to true. This assignment can contribute to the
count N1 at many different leaf nodes, but when we multiply by p1 it contributes one total unit to
the sum of p1N1 over all leaf nodes. This is true because the values p1 represent the conditional
distribution of which leaf node the coupling procedure maps the assignment to. The same is true
for any satisfying assignment to Φ that sets x to false—it contributes one total unit to the sum of
p2N2 over all leaf nodes. Now, if every satisfying assignment contributes one unit, and we have
broken up the set of all satisfying assignments into fractional groups where the ratio between the
contributions of the satisfying assignments with x set to true and those where x is set to false
is always q
1−q , it follows that this is also the the ratio between the total numbers of satisfying
assignments with x set to true and those where x is set to false.
What happens if there are uncoupled leaf nodes? We can no longer compute the ratio N1
N2
. But
it turns out that we can ignore uncoupled leaf nodes and still be able convince ourselves that q
is close to the correct marginal probability. More precisely, the same sorts of arguments that we
used to show that the coupling procedure terminates after a logarithmic number of steps with high
probability will also imply that when we ignore trajectories in the coupling procedure that have
not terminated in a logarithmic number of steps, we have not changed the total contribution of a
typical satisfying assignment to the sum of p1N1 by much.
Finally, what if we are not given the stochastic decision tree? Our approach is to use linear
programming to search for a stochastic decision tree that satisfies the equation p1N1
p2N2 = q
1−q at every
coupled leaf node. The natural approach would be to think of the probabilities of its transitions
as flows along the tree. However, p1 turns out to be a complicated and non-linear function of this
encoding. Nevertheless, there is an alternate encoding where
p1
N1
N2
= p2
q
1 − q
is a linear equation. We will describe this alternate encoding, which is the last ingredient in our
overall algorithm, in the next subsection.
5.2 One-Sided Stochastic Decision Trees
In this subsection, we will transform a stochastic decision tree into two separate trees, that we call
one-sided stochastic decision trees. The idea is to consider the conditional distribution on trajectories
of the coupling procedure, when we fix what A1 will be at termination. Unlike stochastic decision
trees, one-sided decision trees will have the crucial property that
p1
N1
N2
= p2
q
1 − q
is a linear equation in the transition probabilities. First, we explain the transformation from a
stochastic decision tree to a one-sided stochastic decision tree. See Figure 2 for an illustration. We
will then formally define its properties and what we require of it.
Journal of the ACM, Vol. 66, No. 2, Article 10. Publication date: April 2019.
Approximate Counting, the Lovász Local Lemma, and Inference in Graphical Models 10:17
Now, suppose we are given a stochastic decision tree S. Let us construct the one-sided stochastic
decision tree S1 that represents the trajectory of the partial assignment A1. When we start from the
starting state σ (see Definition 4.1), the four descendants of it in S will now be four grandchildren.
Its immediate descendants will be two nodes u and u

, one representing the choice A1 (y) = T and
one representing A1 (y) = F , where y is the next variable set (see Definition 4.2). The two children
of σ in S that correspond to A1 (y) = T will now be the children of u and the other two children
will now be the children of u

. We will continue in this way so that alternate layers represent nodes
present in S and new nodes.
This alone does not change much the semantics of the trajectory. All we are doing is breaking
up the decision of which of the four children to proceed to, into two separate decisions. The first
decision is based on just A1 and the second is based on A2. However, we will change the semantics
of what probabilities we associate with different transitions. For starters, we will work with total
probabilities. So the total probability incoming into the starting node is 1. Let us see how this works
inductively. Let us now suppose that σ represents the state of some node in S (not necessarily the
starting state) and u and u
 are its descendants in S1. Then, if the total probability into σ in S1 is z,
we place z along both the edges to u and to u

. This is because the decision tree is now from the
perspective of A1, who perhaps has already chosen his assignment uniformly at random from the
satisfying assignments with x = T but has not set all of those values in A1. Hence, his decision is
not a random variable, since given the option of transition to u or u
 he must go to whichever one
is consistent with his hidden values.
However, from this perspective, the choices corresponding to A2 are random, because he has no
knowledge of the assignment that the other player is working with. If we have z total probability
coming into u, then the total probability into its two descendants will be ( q1
q1+q2
)z and ( q2
q1+q2
)z,
respectively, where q1 and q2 were the probabilities on the transitions in S into the two corresponding descendants. In particular, if q1 is the probability of setting A1 (y) = T and A2 (y) = T
and q2 is the probability of setting A1 (y) = T and A2 (y) = F , then ( q1
q1+q2
)z is the total probability
on the transition from u to the descendant where A2 (y) = T and ( q2
q1+q2
)z is the total probability
on the transition from u to the descendant where A2 (y) = F . Note that from Corollary 2.3, we
have that
 q2
q1 + q2

z ≤
 4
s

z.
This is an important property that we will make crucial use of later. Notice that it is a linear
constraint in the total probability. Now, we are ready to define a one-sided stochastic decision
tree, which closely mirrors Definition 4.3.
Definition 5.1. Given the function R and a stopping threshold τ , the associated one-sided stochastic decision tree for A1 is the following:
(1) The root node corresponds to the state where only x is set, A1 (x) = T , A2 (x) = F ,VI = {x}
and VO = {xi}
n
i=1 \ {x}.
(2) Each node has either two descendants and four grand-descendants or zero descendants. If
the current node a corresponds to state σ, let (y, σ 

) = R(σ ). Then, if y = ∅ or if |VI | = τ ,
there are no descendants and the current node is a leaf corresponding to the termination
of the coupling procedure or |VI | being too large. Otherwise, the two descendants correspond to the two choices for how to set y in A1. Each of their two descendants correspond
to the two choices for how to set y in A2. Each grand-descendant is marked with the state
σ 

, which incorporates their respective choices.
(3) Let z be the total probability into a. Then, the total probability into each descendant is z.
Moreover, let the total probability into the grand-descendants with states A1 (y) = T and
Journal of the ACM, Vol. 66, No. 2, Article 10. Publication date: April 2019.  
10:18 A. Moitra
A2 (y) = T and A1 (y) = T and A2 (y) = F be z1 and z2, respectively. Then, z1 and z2 are
nonnegative, sum to z and satisfy z2 ≤ ( 4
s )z. Similarly, let the total probability into the
grand-descendants with states A1 (y) = F and A2 (y) = F and A1 (y) = F and A2 (y) = T
be z3 and z4, respectively. Then, z3 and z4 are nonnegative, sum to z and satisfy z4 ≤ ( 4
s )z.
The one-sided stochastic decision tree for A2 is defined analogously, in the obvious way. Finally,
we record an elementary fact:
Claim 4. There is a perfect matching between the root-to-leaf paths in S1 and S2, so that any pair
of assignments A1 and A2 that takes a root-to-leaf path p in S1, must also take the root-to-leaf path
in S2 to which p is matched.
Proof. Recall that the odd levels in S1 and S2 correspond to the nodes in S. Therefore, from a
root-to-leaf path p in S1, we can construct the root-to-leaf path in S, which in turn uniquely defines
a root-to-leaf path in S2 (because it specifies which nodes are visited in odd layers, and all paths
end on a node in an odd layer).
5.3 An Algorithm for Finding a Valid S1 and S2
We are now ready to prove one of the two main theorems of this section:
Theorem 5.2. Let q = PrD[x = T ] and q
 ≤ q ≤ q

. Then there are two one-sided stochastic decision trees S1 and S2 that, for any pair of matched root-to-leaf paths terminating in u and u

, respectively, satisfy
 q

1 − q


p2N2 ≤ p1N1 ≤
 q


1 − q


p2N2,
where N1 and N2 are number of satisfying assignments consistent with A1 and A2, respectively, and
p1 and p2 are the total probabilities into u and u

, respectively.
Moreover, given q
 and q

 that satisfy q
 ≤ q ≤ q

 there is an algorithm to construct two one-sided
stochastic decision trees S1 and S2 that satisfy the above condition on all matched coupled leaf nodes,
which runs in time polynomial in m and 2τ , where τ is the stopping threshold.
Proof. The first part of the theorem follows from the transformation we gave from a stochastic
decision tree to two one-sided stochastic decision trees. Then, Claim 4 combined with Lemma 4.6
implies q
1−q = p1N1
p2N2
, which then necessarily satisfies q

1−q
 ≤ p1N1
p2N2 ≤ q


1−q

 . Rearranging completes
the proof of the first part.
To prove the second part of the theorem, notice that if τ is the stopping threshold, then the
number of leaf nodes in S1 and in S2 is bounded by 4τ . At each coupled leaf node, from Lemma 3.5,
we can compute the ratio of N1 to N2 as the ratio of the number of satisfying assignments to ΦI1
to the number of satisfying assignments to ΦI2 . This can be done in time polynomial in m and 2τ
by brute force. Finally, the constraints in Definition 5.1 are all linear in the variables that represent
total probability (if we treat 4
s , q

1−q
 , q


1−q

 and all ratios N1
N2 as given constants). Thus, we can find a
valid choice of the total probability variables by linear programming. This completes the proof of
the second part.
Recall that we will be able to choose τ = 2c(6dk)
2 logn and Theorem 3.10 will imply that at most
a 1/nc fraction of the distribution fails to couple. Thus, the algorithm above runs in polynomial
time for any constants d and k. What remains is to show that any valid choice of total probabilities
certifies that q
 ≤ PrD[x = T ] ≤ q

.
Journal of the ACM, Vol. 66, No. 2, Article 10. Publication date: April 2019.    
Approximate Counting, the Lovász Local Lemma, and Inference in Graphical Models 10:19
5.4 A Fractional Matching to Certify q
We are now ready to prove the second main theorem of this section. We will show that having
any two one-sided stochastic decision trees that meet the constraints on the leaves imposed by
Theorem 5.2 is enough to certify that PrD[x = T ] is approximately between q
 and q

. This result
will rest on two facts. Fix any assignment A. Then, either
(1) The assignment has too many clauses that restricted to marked variables are all set to F or
(2) The total probability of A reaching a leaf node u where the coupling procedure failed to
terminate before reaching size τ is at most O( 1
nc ).
Theorem 5.3. Suppose that every clause contains between k and 6k variables and that logd ≥
5 log k + 20 and k ≥ 50 logd + 50 log k + 250. Then any two one-sided stochastic decision trees S1
and S2 that meet the constraints on the leaves imposed by Theorem 5.2 and satisfy τ = 20c(6dk)
2 logn
imply that
q
 − O
 1
nc

≤ PrD[x = T ] ≤ q

 + O
 1
nc

.
The proof of this theorem will use many of the same tools that appeared in the proof of
Theorem 3.10, since in essence we are performing a one-sided charging argument.
Proof. The proof will proceed by constructing a complete bipartite graph H = (U,V, E) and
finding a fractional approximate matching as follows. The nodes in U represent the satisfying
assignments of Φ with x = T . The nodes inV represent the satisfying assignments of Φ with x = F .
Moreover, all but a O( 1
nc ) fraction of the nodes on the left will send between 1 − q

 − O( 1
nc ) and
1 − q
 + O( 1
nc ) flow along their outgoing edges. Finally, all but a O( 1
nc ) fraction of the nodes on
the right will receive between q
 − O( 1
nc ) and q

 + O( 1
nc ) flow along their incoming edges.
First, notice that any assignment A (say with x = T ) is mapped by S1 to a distribution over
leaf nodes (both coupled and uncoupled). Now consider matched pairs of leaf nodes (according to
Claim 4) that correspond to a coupling. Let p1 and p2 be the total probability of the leaf nodes in S1
and S2, respectively. Let N1 and N2 be the total number of assignments that are consistent with A1
and A2, and let N1 and N2 be the corresponding sets of assignments. From the assumption that
 q

1 − q


p2N2 ≤ p1N1 ≤
 q


1 − q


p2N2
and the intermediate value theorem, it follows that there is a q
 ≤ q∗ ≤ q

, which satisfies
 q
∗
1 − q∗

p2N2 = p1N1.
Hence, there is a flow that sends exactly (1 − q∗)p1 units of flow out of each node in N1 and exactly
q∗
p2 units of flow into each node in N2.
If every leaf node is coupled, then we would indeed have the fractional matching we are looking for, just by summing these flows over all leaf nodes. What remains is to handle uncoupled
leaf nodes. Consider any such leaf node u in S1 and the corresponding leaf node v in S2. From
Lemma 3.9, we have that there is a 3-tree T of size at least 10c logn. For each node in T , from
Claim 2 we have there are at least 10c logn disjoint type 1 or type 2 errors.
Case # 1: Suppose that there are at least 3.5c logn disjoint type 1 errors. Fix the 3-tree T , and
look at all root-to-leaf paths that are consistent with just the type 1 errors. Then the sum of their
total probabilities is at most
 4
d11 3.5c log n
.
Journal of the ACM, Vol. 66, No. 2, Article 10. Publication date: April 2019.      
10:20 A. Moitra
This follows, because the constraint that z2 ≤ ( 4
s )z (and similarly for z4) in Definition 5.1 implies
that for each path, we can factor out the above term corresponding to just the decisions where there
are type 1 errors. Moreover, we chose s = d11 exactly as we did in the proof of Theorem 3.10. The
remaining probabilities are conditional distributions on the paths (after having taken into account
the type 1 errors) and sum to at most one. Finally, the total number of 3-trees of size 10c logn
containing x is at most (3e (6dk)
3)
10c log n. Thus, for any assignment A, if we ignore what happens
to it when it ends up at a leaf node that did not couple and that has at least 3.5c logn disjoint type
1 errors, in total we have ignored at most
 4
d11 3.5c log n
(3e (6dk)
3)
10c log n ≤ 1/nc
of its probability, where the last inequality uses the fact that logd ≥ 5 log k + 20.
Case # 2: Suppose that there are at least 6.5c logn disjoint type 2 errors. Each type 2 error can
be blamed on either A1 or A2 or both (e.g., it could be that the clause c might only have all of its
marked variables set to F in A1). Let us suppose that the assignmentAcontributes at least 2.5c logn
disjoint type 2 errors. In this case, we will completely ignore A in the constraints imposed by our
flow. How many such assignments can there be? The probability of getting any such assignment
is bounded by
  49
100 k/42.5c log n
3e (6dk)
3
10c log n
≤ 1/nc ,
where the last inequality has used the fact that k ≥ 50 logd + 50 log k + 250.
Thus, if we ignore the flow constraints for all such assignments, then we will be ignoring at
most a 1/nc fraction of the nodes in U and the nodes in V . The only remaining case is when
the assignment A ends up at a leaf node u that has at least 6.5c logn disjoint type 2 errors, but
it contributes less than 2.5c logn itself. For each type 2 error that it does not contribute to, it
contributes to another type 1 error. The only minor complication is that the node responsible might
not be in the 3-tree T . However, it is distance at most 1 from the 3-tree, because it is contained in
a clause that results in type 2 error that does contain a node in T . Now, by analogous reasoning to
Case #1, if we fix the pattern of these type 1 errors—i.e., we fix the 3-tree and the extra nodes at
distance 1 from it that contribute the missing type 1 errors—then the sum of the total probability
of all consistent root-to-leaf paths is at most
 4
d11 4c log n
.
Now, the number of patterns can be bounded by (4e (6kd)
4)
10c log n, which accounts for the inclusion
of extra nodes that are not in T . Once again, for such an assignment A, if we ignore what happens
to it when it ends up at a leaf node that did not couple and has at least 6.5c logn disjoint type 2
errors but it contributes less than 2.5c logn itself, then in total we have ignored at most
 4
d11 4c log n
(4e (6kd)
4
)
10c log n ≤ 1/nc
of its probability, where the last inequality uses the fact that logd ≥ 5 log k + 20.
Now returning to the beginning of the proof and letting N1 and N2 be the total number of
satisfying assignments with x = T and x = F , respectively. We have that the flow in the bipartite
graph implies
(1 − q

)N1 − O
 1
nc

≤ flowoutU = flowinV ≤ q

N2 + O
 1
nc

Journal of the ACM, Vol. 66, No. 2, Article 10. Publication date: April 2019.        
Approximate Counting, the Lovász Local Lemma, and Inference in Graphical Models 10:21
and the further condition
q

N2 − O
 1
nc

≤ flowinV = flowoutU ≤ (1 − q

)N1 + O
 1
nc

,
which gives q

1−q
 − O( 1
nc ) ≤ q
1−q ≤ q


1−q

 + O( 1
nc ), which completes the proof of the theorem.
6 APPLICATIONS
Here, we show how to use our algorithm for computing marginal probabilities when k is logarithmic in d for approximate counting and sampling from the uniform distribution on satisfying
assignments. Throughout this section, we will assume that each clause contains between k and 2k
variables and make use of Theorem 5.3 as a subroutine, which makes a weaker assumption that
the number of variables per clause is between k and 6k.
6.1 Approximate Counting
There is a standard approach for how to use an algorithm for computing marginal probabilities
to do approximate counting in a monotone CNF, where no variable is negated (see, e.g., Reference [4]). Essentially, we fix an ordering of the variables x1, x2,..., xn and a sequence of formulas Φ1, Φ2,..., Φn. Let Φ1 = Φ and let Φi be the subformula we get when substituting x1 = T,
x2 = T,..., xi−1 = T into Φ and simplifying. Notice that each such formula is a monotone CNF and
inherits the properties we need from Φ. In particular, each clause has at least k variables, because
the only clauses left in Φi (i.e., not already satisfied) are the ones that have all of their variables
unset.
However, such approaches crucially use monotonicity to ensure that no clause becomes too
small (i.e., contains few variables but is still unsatisfied). This is a similar issue to what happened
with the coupling procedure, which necessitated using marked and unmarked variables, the latter
being variables that are never set and are used to make sure no clause becomes too small. We can
take a similar approach here. In what follows, we will no longer assume Φ is monotone.
Lemma 6.1. Suppose that e (D + 1) ≤ (32/31)
k . Then, there is a partial assignment A so that every
clause is satisfied and each clause has at least 7k/8 unset variables. Moreover, there is a deterministic
algorithm to find such a partial assignment that runs in time polynomial in m, n, k, and d.
Proof. Independently for each variable, we will set it toT with probability 1/32, set it to F with
probability 1/32 and to leave it unset with probability 15/16. Now consider the m bad events, one
for each clause c, thatc is either unsatisfied or has not enough unset variables (or both). Let D(p||q)
be the Kullback-Leibler divergence between Bernoulli random variables with parameters p and q,
respectively. Then, we have
Pr[c is bad] ≤ e−D( 1
8 | | 1
16 )k +
 31
32 k
≤ 2
 31
32 k
.
Here, the first term follows from the Chernoff bound and represents the probability that there are
not enough unset variables and the second term is the probability that the clause is unsatisfied.
Moreover, using the fact that D( 1
8 || 1
16 ) ≥ 1
11 , we conclude that the second term is larger than the
first. Now, we can once again appeal to the Lovász Local Lemma to show the existence of a partial
assignment where none of the bad events occur. Finally, we can use the deterministic algorithm of
Chandrasekaran, Goyal, and Haeupler [7] to find such a partial assignment in polynomial time.
Theorem 6.2. Suppose we are given a CNF formula Φ on n variables where every clause contains
between k and 2k variables. Moreover, suppose that logd ≥ 5 log k + 20 and k ≥ 60 logd + 60 log k +
300. Let OPT be the number of satisfying assignments. Then there is a deterministic algorithm that
Journal of the ACM, Vol. 66, No. 2, Article 10. Publication date: April 2019.      
10:22 A. Moitra
outputs a quantity count that satisfies

1 − 1
nc

OPT ≤ count ≤

1 +
1
nc

OPT
and runs in time polynomial in m and ncd2k2
.
Proof. First, we (deterministically) find a partial assignment that satisfies the properties
guaranteed by Lemma 6.1. Note that the conditions on k and d imply that the condition in
Lemma 6.1 holds. Suppose t total variables are assigned a truth value in the partial assignment. Let
x1, x2,..., xt be an ordering of these variables. As above, we construct a sequence of subformulas
Φ1, Φ2,..., Φt , where Φi is obtained by substituting into Φ the assignments for x1, x2,..., xi−1 in
the partial assignment and simplifying the formula. Again let qi be our estimate for the marginal
probabilities that a random satisfying assignment of Φi sets xi to true.
The key point is that the next subformula in the sequence, Φt+1, is empty, because all the clauses
are satisfied by the partial assignment. Moreover, each clause that appears in any formula Φi for
1 ≤ i ≤ t has at most 2k variables and has at least 7k/8 variables, because it has at least that many
unset variables in the partial assignment. Note that the upper and lower bounds on clause sizes
differ by less than a factor of 6. Moreover, we can now output
count  2n−t
n
i=1
 1
qi

=

1 ±
1
nc

OPT,
because Φt+1 has exactly 2n−t , satisfying assignments (every choice of the unset variables) and we
have used the same telescoping product, but now to compute the ratio of the number of satisfying
assignments to Φt+1 divided by the number of satisfying assignments to Φ.
6.2 Approximate Sampling
Here, we give an algorithm to generate an assignment approximately uniformly from the set of all
satisfying assignments. Again, the complication is that our oracle for approximating the marginals
works only if k is at least logarithmic in d, so we need some care in the order we choose to sample
variables. In Algorithm 3, we give the algorithm.
First, we prove that the output is close to uniform.
Lemma 6.3. If the oracle Y outputs a marginal probability that is 1/nc+1 close to the true marginal
distribution for each variable queried, then the output of the Sampling Procedure is a random assignment whose distribution is 1/nc -close in total variation distance to the uniform distribution on all
satisfying assignments.
Proof. The proof of this lemma is in two parts. First, imagine we were instead given access
to an oracle W that answered each query for a marginal distribution with the exact value. Then,
each variable set using the oracle is chosen from the correct marginal distribution. And in the last
step, the set of satisfying assignments is a cross-product of the satisfying assignments for each
component. Thus, the procedure would output a uniformly random assignment from the set of all
satisfying assignments. Second, since at most n variables are queried, we have that with probability
at least 1 − 1/nc all of the random decision of the procedure would be the same if we had given it
answers from W instead of from Y. This now completes the proof.
The key step in the analysis of this algorithm rests on showing that with high probability each
connected component is of logarithmic size.
Journal of the ACM, Vol. 66, No. 2, Article 10. Publication date: April 2019.        
Approximate Counting, the Lovász Local Lemma, and Inference in Graphical Models 10:23
ALGORITHM 3: Sampling Procedure,
Input: CNF Φ, oracle Y for approximating marginals of variables
(1) Using Lemma 3.1, label variables as marked or unmarked
(2) While there is a marked variable x that is unset
(3) Sample x using Y
(4) Initialize VI = {x} and VO to be all unset variables (x is already set)
(5) While there is a clause c with variables in both VI and VO
(6) Sequentially sample its marked variables (if any) using Y
(7) Case # 1: c is satisfied
(8) Delete c
(9) Case # 2: c is unsatisfied
(10) Let S be all variables in c (marked or unmarked)
(11) Update VI ← VI ∪ S, VO ← VO \ S
(12) End
(13) End
(14) For each connected component of the remaining clauses
(15) Enumerate and uniformly choose a satisfying assignment of the unset variables
(16) End
Theorem 6.4. Suppose we are given a CNF formula Φ on n variables where each clause contains
between k and 2k variables. Moreover, suppose that logd ≥ 5 log k + 20 and k ≥ 60 logd + 60 log k +
300. There is an algorithm that outputs a random assignment whose distribution is 1/nc -close in total
variation distance to the uniform distribution on all satisfying assignments. Moreover, the algorithm
runs in time polynomial in m and ncd2k2
.
Proof. The proof of this theorem uses many ideas from the coupling procedure as analyzed in
Section 3. Let Φ
 be the formula at the start of some iteration of the inner WHILE loop. Then, at
the end of the inner WHILE loop, using Lemma 3.5, we can write
Φ
 = Φ

I ∧ Φ

O ,
where Φ

I is a formula on the variables inVI and Φ

O is a formula on the variables inVO . In particular,
no clause has variables in both because the inner WHILE loop terminated. Now, we can appeal to
the analysis in Theorem 3.10, which gives a high probability bound on the size of VI . The analysis
presented in its proof is nominally for a different procedure, the Coupling Procedure, but the
inner WHILE loop of the Sampling Procedure is identical except for the fact that there are no
type 1 errors, because we are building up just one assignment. Thus,
Pr[|VI | ≥ 2(6dk)
2
t] ≤
 1
2
t
.
The inner WHILE loop is run at most n times, so if we choose t ≥ c logn, we get that with probability at least 1 − 1/nc no component has size larger than 2c(6dk)
2 logn. Now, the brute-force
search in the last step can be implemented in time polynomial in m and ncd2k2
, which, combined
with Lemma 6.3, completes the proof.
We can also now prove Corollary 1.4.
Proof. Recall, we are given a cause network and the truth assignment of each observed variable.
First, we do some preprocessing. If an observed variable is an OR of several hidden variables or
their negation and the observed variable is set to F , then we know the assignment of each hidden
variable on which it depends. Similarly, if an observed variable is an AND and it is set to T , again
Journal of the ACM, Vol. 66, No. 2, Article 10. Publication date: April 2019.  
10:24 A. Moitra
we know the assignment of each of its variables. For all the remaining observed variables, we
know there is exactly one configuration of its variables that is prohibited so each yields a clause in
a CNF formula Φ. Moreover, each clause depends on at least 7k/8 variables whose truth value has
not been set, because the collection of observations is k-regular. Finally, each variable is contained
in at most d clauses. The posterior distribution on the remaining hidden variables (whose value
has not already been set) is uniform on the set of satisfying assignments to Φ and, thus, we can
appeal to Theorem 6.4 to complete the proof.
7 CONCLUSION
In this article, we presented a new approach for approximate counting in bounded degree systems
based on bootstrapping an oracle for the marginal distribution. In fact, our approach seems to
extend to non-binary approximate counting problems as well. For example, suppose we are given
a set of hyperedges and our goal is to color the vertices red, green, or blue with the constraint
that every hyperedge has at least one of each color. It is still true that the uniform distribution
on satisfying colorings is locally close to the uniform distribution on all colorings in the sense of
Corollary 2.3. This once again allows us to construct a coupling procedure, but now between a
triple of partial colorings. The coupling can be used to give alternative ways to sample a satisfying
coloring uniformly at random, which in turn yields a method to certify the marginals on any
vertex by solving a polynomial number of counting problems on logarithmic sized hypergraphs.
We chose to work with only binary counting problems to simplify the exposition, but it remains
an interesting question to understand the limits of the techniques that we introduced here. 