We construct near-optimal linear decision trees for a variety of decision problems in combinatorics and discrete geometry. For example, for any constant k, we construct linear decision trees that solve the k-SUM
problem on n elements usingO(n log2 n) linear queries. Moreover, the queries we use are comparison queries,
which compare the sums of two k-subsets; when viewed as linear queries, comparison queries are 2k-sparse
and have only {−1, 0, 1} coefficients. We give similar constructions for sorting sumsets A + B and for solving
the SUBSET-SUM problem, both with optimal number of queries, up to poly-logarithmic terms.
Our constructions are based on the notion of “inference dimension,” recently introduced by the authors in
the context of active classification with comparison queries. This can be viewed as another contribution to
the fruitful link between machine learning and discrete geometry, which goes back to the discovery of the
VC dimension.
CCS Concepts: • Theory of computation → Computational complexity and cryptography;
Additional Key Words and Phrases: Linear decision tree, hyperplane arrangement, inference dimension, comparison queries
1 INTRODUCTION
This article studies the linear decision tree complexity of several combinatorial problems, such
as k-SUM, SUBSET-SUM, KNAPSACK, sorting sumsets, and more. A common feature these problems share is that they are all instances of the following fundamental problem in computational
geometry.
Fig. 1. Primal and dual forms of the point-location problem. H is the green lines/points, x is the black
point/line, and AH (x) = (+, +, 0, −).
The Point-location Problem. Let H ⊂ Rn be a finite set. Consider the problem in which given x ∈
Rn as an input, the goal is to compute the function
AH (x) := (sign(x,h) : h ∈ H) ∈ {−, 0, +}
H ,
where sign : R → {−, 0, +} is the sign function and ·, · is the standard inner product in Rn.
In discrete geometry, this is known as the point-location in a hyperplane-arrangement problem,
in which each h ∈ H is identified with the hyperplane containing the origin and orthogonal to h,
and AH (x) corresponds to the cell in the partition induced by the hyperplanes in H to which the
input point x belongs. We note that the general point-location problem, where the goal is compute
sign(x,h − c) for some h ∈ Rn,c ∈ R, can be reduced to the case described above by embedding
(h,c) as a hyperplane in dimension n + 1.
A dual formulation of this problem has been considered in learning theory, specifically within
the context of active learning: Here each h ∈ H is thought of as a point, x is thought of as the target
half-space to be learned, and computing AH (x) corresponds to learning how each point h ∈ H is
classified by x. In this work, it will often be more intuitive to consider this dual formulation. See
Figure 1 for a planar illustration of both interpretations.
Linear Decision Tree. A linear decision tree for the point-location problem AH is an adaptive
deterministic algorithm T . The set H ⊂ Rn is known in advance, and the input is x ∈ Rn. The
algorithm does not have direct access to x. Instead, at each iteration the algorithm chooses h ∈ Rn
and queries “sign(h, x) =?” (note that h is not necessarily in H). At the end, the algorithm should
be able to compute AH (x) correctly. The query complexity is the maximum over x of the number
of queries performed. Equivalently, such an algorithm can be described by a ternary decision tree
that computes the sign of a linear query at each inner node. A query is s-sparse if it involves at
most s nonzero coefficients. A linear decision tree is s-sparse if all its queries are s-sparse.
Comparison Decision Tree. A comparison decision tree for the point-location problem AH
is a special type of a linear decision tree, where the only queries used are either of the form
sign(h, x) for h ∈ H (label queries) or sign(h − h, x) for h
,h ∈ H (comparison queries). Note
that h − h, x ≥ 0 if and only if h
, x≥h, x, which is why we call these comparison queries.
In the dual version (in which we view H as a set of points), comparison queries have a natural
Journal of the ACM, Vol. 66, No. 3, Article 16. Publication date: April 2019.
Near-optimal Linear Decision Trees for k-SUM and Related Problems 16:3
geometric interpretation: assuming that sign(h
, x) = sign(h, x), a comparison query h − h,
x, corresponds to querying which one of h
,h ∈ H is further from the hyperplane defined by x.
Observe that if all elements h ∈ H are s-sparse, then a comparison decision tree is 2s-sparse.
1.1 Results
Our main result is a method that produces near-optimal decision trees for many natural and wellstudied combinatorial instances for the point-location problems by using comparison decision
trees. We first describe a few concrete instances and then the general framework.
1.1.1 k-SUM. In the k-SUM problem an input array x ∈ Rn of n numbers is given, and the
goal is to decide whether the sum of k distinct numbers is 0. This problem (in particular 3-SUM)
has been extensively studied since the 1990s, as it embeds into many problems in computational
geometry; see, for example, [11]. More recently, it has also been studied in the context of finegrained complexity; see, for example, [17, 20] and the survey [24] for more general context.
The k-SUM problem corresponds to the following point-location problem. Let H ⊆ {0, 1}
n denote all vectors of hamming weight k. Thus, x ∈ Rn contains k numbers whose sum is 0 if and
only if AH (x) contains at least one 0 entry.
In this context, comparison decision trees allow for two types of linear queries: label queries
of the form “
i ∈I xi ≥ 0?,” where I ⊂ [n] has size |I | = k, and comparison queries of the form
“

i ∈I xi ≥
j ∈J xj ?,” where I, J ⊂ [n] have size |I | = |J| = k.
Theorem 1.1. The k-SUM problem on n elements can be computed by a comparison decision tree
of depth O(kn log2 n). In particular, all the queries are 2k-sparse and have only {−1, 0, 1} coefficients.
This improves a series of works. There is a basic deterministic algorithm in time O˜ (n
k/2) that
first sorts all sums of k/2 input elements and then traverses the sorted list in a way that enables us to decide the k-SUM problem. It can be transformed to a linear decision tree with the
same number of queries, which in our language are label and comparison queries with sparsity k.
Erickson [7] showed that Ω(n
k/2) queries are indeed necessary to solve k-SUM if only such
queries are allowed (or, more generally, if only k-sparse linear queries are allowed). Ailon and
Chazelle [1] extended this work by giving lower bounds of the form n1+Ω(1) for sparsity between k
and k + ko(1)
. Our construction has sparsity 2k. It remains open to understand the linear decision
depth needed for sparsity up to 2k.
In a breakthrough work, Grønlund and Pettie [12] were the first to break the n
k/2 bound. They
constructed a randomized (2k − 2)-sparse linear decision tree for k-SUM that makesO(nk/2

logn)
queries. This was improved to O(nk/2) by Gold and Sharir [13].
In the general linear decision tree model, without any sparsity assumptions, a series of works in
discrete geometry have designed linear decision trees for the general point-location problem. In
the context of k-SUM, the best result is of Ezra and Sharir [8], who constructed a linear decision
tree of depth O(n2 log2 n) for any constant k. This improves on previous results of Meyer auf der
Heide [18], Meiser [19], and Cardinal et al. [3].
1.1.2 Sorting A + B. Let A, B ⊂ R be sets of size |A| = |B| = n. Their sumset, denoted by A + B
is the set {a + b : a ∈ A,b ∈ B}. Consider the goal of sorting A + B while minimizing the number
of comparisons (here, by “comparisons” we mean the usual notion in sorting, that is, comparing
two elements of A + B). While it is possible that |A + B| = n2, it is well known that the number of
possible orderings of A + B is only nO (n) [9]. Thus, from an information-theoretic perspective it
is conceivable that A + B can be sorted using only O(n logn) comparisons. However, Fredman [9]
gave a tight bound of Θ(n2) on the number of comparisons needed to sort A + B.
Journal of the ACM, Vol. 66, No. 3, Article 16. Publication date: April 2019.    
16:4 D. M. Kane et al.
It is natural to ask whether enabling the algorithm more access to the data in the form of simple
local queries can achieve o(n2) query-complexity. We show that if the algorithm can use differencescomparisons, then an almost optimal query-complexity of O(n log2 n) suffices to sort A + B. A differences comparison on an array [x1,..., xn] is a query of the form
“xi − xj ≥ xk − xl ?”;
in other words, “Is xi greater than xj more than xk is greater than xl ?”
The problem of sorting A + B corresponds to the following point-location problem. Let
A = {a1,..., an }, B = {b1,...,bn } and identify x ∈ R2n with x = (a1,..., an,b1,...,bn ). Let H ⊂
{−1, 0, 1}
2n consist of vectors with exactly one 1 and one −1 in the first n elements and exactly one
1 and one −1 in the last n elements. Then computing AH (x) corresponds to answering all queries
of the form “ai + bj ≥ ak + bl ?” for all i, j, k,l ∈ [n], which amounts to sorting A + B. In this context, the two types of queries used by comparison decision trees are comparison queries in A + B,
namely “ai + bj ≥ ak + bl ?,” where i, j, k,l ∈ [n] (which correspond to the label queries in the
point location problem), and differences-comparison queries inA + B, namely “ai + bj − ai − bj ≥
ak + bl − ak − bl?,” where i, j, k,l,i
, j

, k
,l ∈ [n] (which correspond to comparison queries in
the point location problem).
Theorem 1.2. Given A, B ⊂ R of size |A| = |B| = n, their sumset A + B can be sorted by a comparison decision tree of depthO(n log2 n). In particular, all queries are 8-sparse with {−1, 0, 1} coefficients.
The problem of sorting sumsets has been considered by Fredman [9], who showed that if only
comparison queries are allowed, then Θ(n2) queries are sufficient and necessary to sort A + B.
Grønlund and Pettie [12] use it in their work and specifically ask for a better linear decision tree
for sorting sumsets.
1.1.3 NP-hard Problems. Several NP-hard problems can be phrased as point-location problems.
For example, the SUBSET-SUM problem is to decide, given a set A of n real numbers and a target t,
whether there exists a subset of A whose sum is t. The KNAPSACK problem is to decide whether
there exists a subset of A whose sum is 1. We focus here on SUBSET-SUM for concreteness.
The SUBSET-SUM problem corresponds to the following point-location problem. Let A =
{a1,..., an } and take x = (a1,..., an, −t) ∈ Rn+1. Let H = {0, 1}
n × {1}. ThenA has a subset whose
sum is t if and only if AH (x) contains at least one 0.
In this context, comparison decision trees have two types of queries: label queries of the form
“

i ∈A ai ≥ t?” for some A ⊆ A and comparison queries of the form “
i ∈A ai ≥
i ∈A ai ?” for
some A
,A ⊆ A.
Theorem 1.3. The SUBSET-SUM problem can be solved using a comparison decision tree of depth
O(n2 logn), where n is the size of the input-set. In particular, all the queries are linear with {−1, 0, 1}
coefficients.
Note that the bound is tight up to the log factor: Indeed, in the corresponding point-location
problem {AH (x) : x ∈ Rn } corresponds to the family of thresholds function on the Boolean
cube {0, 1}
n. It is well known that the number of such functions is 2Θ(n2 ) [14], and thus any decision
tree (even one that uses arbitrary queries, each with a constant number of possible answers) that
computes AH (x) must use at least Ω(n2) queries.
The surprising fact that SUBSET-SUM, an NP-hard problem, has a polynomial time algorithm
in a nonuniform model (namely linear decision trees) was first discovered by Meyer auf der
Heide [18], answering an open problem posed by Dobkin and Lipton [6] and Yao [26]. It originally
required O(n4 logn) linear queries. It was generalized by Meiser [19] to the general point-location
problem and later improved by Cardinal [3] and Ezra and Sharir [8]. This last work, although it
Journal of the ACM, Vol. 66, No. 3, Article 16. Publication date: April 2019.   
Near-optimal Linear Decision Trees for k-SUM and Related Problems 16:5
does not address the SUBSET-SUM problem directly, seems to improves the number of queries to
O(n3 log2 n). Observe that our construction gives a near-optimal number of linear queries, namely
O(n2 logn). Moreover, the queries are simple, in the sense that they involve only {−1, 0, 1} coefficients, and natural from a computational perspective, as they only compare the sums of subsets.
This is unlike the previous works mentioned, which require arbitrary coefficients due to the geometric nature of their techniques.
1.1.4 Other Applications. Our framework (see Corollary 1.13) is somewhat generic and as such
gives near optimal linear decision trees for a host of problems considered in the literature. For
example, the following problems were considered in [12]. We discuss each one briefly and refer
the interested reader to [12] for a deeper discussion.
k-LDT. Given a fixed linear equation ϕ(x1,..., xk ) = α0 + k
i=1 αixi and a set A ⊂ R of size
|A| = n, the goal is to decide if there exist distinct a1,..., ak ∈ A such that ϕ(a1,..., ak ) = 0. This
problem is a variant of the k-SUM problem and can be embedded as a point-location problem in
Rnk+1 as follows. Let x = (1, α1a1,..., α1an,..., αka1,..., αkan ) and H ⊂ {−1, 0, 1}
nk+1 consists
of h that have a “−1” in their first coordinate, a single “+1” in each of the k blocks of size n, and
0 elsewhere. Corollary 1.13 implies a comparison decision tree with O(k2
n log2 n) queries that are
(2k + 2)-sparse and with {−1, 0, 1} coefficients. For constant k, this gives O(n log2 n), which improves upon the previous best bound of O(n2 log2 n) of [8].
Zero Triangles. Let G = (V, E) be a graph on |V | = n vertices and |E| = m edges, which is known
in advance (it is not part of the input). The inputs are edge weights x : E → R. The goal is to decide
whether there is a triangle inG whose sum is zero. This problem clearly embeds as a point-location
problem in Rm. Corollary 1.13 gives a comparison decision tree that solves this problem with
O(m log2m) queries. All the queries are 6-sparse and have {−1, 0, 1} coefficients. This improves
upon the previous bound of O(m5/4) of [12].
All Pairs Shortest Path. The All-Pairs-Shortest-Path (APSP) problem is to compute, given a complete n-vertex graph with non-negative weights on the edges, the shortest path between all pairs
of nodes. It is known that the complexity of this problem is asymptotically equivalent to that
of computing a (min, +) matrix product. Here A, B are n × n matrices with non-negative entries, and the goal is to compute their (min, +) product C, which is the n × n matrix given by
Ci,j = mink (Ai,k + Bk,j). All known algorithms that compute C run in worst-case time n3−o(1)
,
where the best bounds are by Williams [25]. See also [5] for a deterministic algorithm. In an influential paper in 1976, Fredman [10] constructed a linear decision tree of depth O(n2.5) that solves
the problem. Pettie [21] improved this bound to O(mn log α) (m is the number of edges), where
α (m,n) is the inverse-Ackermann function. Corollary 1.13 gives a comparison decision tree that
solves this problem with O(n2 log2 n) queries, which is near-optimal as the input size is 2n2. Moreover, all the queries are either label queries of the form “Ai,k + Bk,j ≥ Ai,k + Bk
,j ?” or comparison
queries of the form “Ai,k + Bk,j − Ai
,k − Bk
,j ≥ Ai,k + Bk,j − Ai
,k − Bk,j?”
1.2 Inference Dimension
Our results are based on the notion of “inference dimension,” which was recently introduced by
the authors of [16] in the context of active learning.
Definition 1.4 (Inference). Let S ⊂ Rn and h, x ∈ Rn. We say that S infers h at x if “sign(h, x)”
is determined by the answers to the label and comparison queries on S. That is, if we set
PS (x) := {x  ∈ Rn : AS∪(S−S ) (x 
) = AS∪(S−S ) (x)},
Journal of the ACM, Vol. 66, No. 3, Article 16. Publication date: April 2019. 
16:6 D. M. Kane et al.
Fig. 2. The cases in the analysis of the inference dimension of R × {1}.
then sign(x 
,h) = sign(x,h) for all x  ∈ PS (x). We further define the inference set of S at x to
be
infer(S, x) := {h ∈ Rn : S infers h at x}.
For each h ∈ infer(S, x), we refer to sign(h, x) as the inferred value of h at x.
An equivalent geometric condition to “S infers h at x” is that the hyperplane defined by h is
either disjoint from PS (x) or contains PS (x).
Example 1.5. Consider the following two examples. The first one uses only label queries, and
the second one uses both label and comparison queries:
(1) Assume that h1,h2, x ∈ Rn satisfy sign(h1, x) = sign(h2, x) = 0, and h is in the linear
space spanned byh1,h2. Then it must hold that sign(h, x) = 0, and so {h1,h2} infersh at x.
(2) Assume that h1,h2, x ∈ Rn satisfy sign(h1, x) = sign(h2 − h1, x) = +, and h is in the
cone spanned by h1,h2 − h1 (i.e. h = αh1 + β (h2 − h1) for α, β > 0). Then it must hold that
sign(h, x) = +, and so {h1,h2} infers h at x.
Definition 1.6 (Inference Dimension). Let H ⊂ Rn. The inference dimension of H is the minimal
d ≥ 1 for which the following holds. For any subset S ⊂ H of size |S | ≥ d, and for any x ∈ Rn,
there exists h ∈ S such that S \ {h} infers h at x.
The concept of inference dimension is central to this article. To illustrate it, we give two examples. The first one is the inference dimension of H = R × {1} ⊂ R2, and the second one is
of H = R2 × {1} ⊂ R3. Both examples are from [16], where they are described using machinelearning terminology: The first one corresponds to (affine) thresholds in R, and the second one to
(affine) thresholds in R2. We note that in the first example, only label queries are used, and in the
second example, both label and comparison queries are used.
Example 1.7 (Inference Dimension of H = R × {1} is d = 3 (see Figure 2)). Let x ∈ R2 and hi ∈ H
for i ∈ [3]. Write hi = (ai, 1) with ai ∈ R, and assume a1 < a2 < a3. Let si = sign(hi, x). Observe
thats1,s2,s3 is either a weakly increasing sequence, or a weakly decreasing sequence, with at most
one 0. There are several cases to consider. In each case we show that two of h1,h2,h3 infer the third
at x:
(1) If s1 = 0, then s2 = s3, so {h1,h2} infers h3 at x. A similar argument holds if si = 0 for
i = 2, 3.
(2) If s1 = s3, then s2 = s1 = s3, hence {h1,h3} infers h2 at x.
(3) If s2 = −s3, then s1 = s2, hence {h2,h3} infers h1 at x.
(4) If s2 = −s1, then s3 = s2, hence {h1,h2} infers h3 at x.
Journal of the ACM, Vol. 66, No. 3, Article 16. Publication date: April 2019.
Near-optimal Linear Decision Trees for k-SUM and Related Problems 16:7
Fig. 3. The cases in the analysis of the inference dimension of R2 × {1}.
Note that in this example, the label queries si = sign(hi, x) suffice for the inference (comparison
queries sign(hi − hj, x) are not used).
Example 1.8 (Inference Dimension ofH = R2 × {1} is d ≤ 10 (see Figure 3)). Let x ∈ R3 and hi ∈ H
for i ∈ [10]. Write hi = (ai, 1) with ai ∈ R2. Let si = sign(hi, x). There must be four points with
the same sign s, say s1 = s2 = s3 = s4 = s. There are three cases to consider. In each case we show
that three of h1,h2,h3,h4 infer the fourth at x:
(1) If s = 0, then as the points h1,...,h4 ∈ R3 are linearly dependent, one of the points must
be spanned by the other three, say, h4 ∈ span(h1,h2,h3). Then the assumption s1 = s2 =
s3 = 0 implies s4 = 0, and hence {h1,h2,h3} infers h4 at x.
Otherwise, there are two cases to consider, based on the convex hull of a1,..., a4 ∈ R2:
(2) Assume that one point, say, a4, is in the convex hull of the other three points, a4 ∈
conv(a1, a2, a3). Then the assumption s1 = s2 = s3 = s implies that also s4 = s, and hence
{h1,h2,h3} infers h4 at x.
(3) Assume that a1,..., a4 are in convex position. Let hi be such that hi, x is minimized
fori ∈ [4]. Determining the minimizer can be done using comparison queries, as hi, x <
hj, x is the same as sign(hj − hi, x) = +. Assume that the minimizer is h1. Consider
the convex hull of a1,..., a4 ∈ R2 that is a quadrilateral in the plane. Assume that the
neighbours of a1 are a2, a3. This implies that a4 lies in the cone originating at a1 and
with rays going through a2, a3. This in turn implies that h4 can be expressed as a positive
combination of h1,h2 − h1,h3 − h1, hence {h1,h2,h3} infers h4 at x.
One may hope that these examples generalize to higher-dimensional spaces. However, the following theorem rules it out.
Theorem 1.9 ([16]). The inference dimension of H = R3 is ∞. That is, for every integer n ≥ 1 there
exists an example of x ∈ R3,H ⊂ R3 of size |H| = n, such that no h ∈ H can be inferred from H \ {h}
at x.
1.3 General Framework
The first step in the proof of Theorem 1.1, Theorem 1.2, and Theorem 1.3 is to show that the sets
H in the corresponding point location problems are of low inference dimension. This requires
further assumptions on the combinatorial structure of H. The following general theorem provides
a uniform treatment for this.
Journal of the ACM, Vol. 66, No. 3, Article 16. Publication date: April 2019.
16:8 D. M. Kane et al.
For h ∈ Zn defines it’s 1 norm as h1 = n
i=1 |hi |.
Theorem 1.10. The inference dimension of H = {h ∈ Zn : h1 ≤ w} is d = O(n logw).
The second step is to show that sets of low inference dimension have efficient comparison decision trees. First, we show this for zero-error randomized comparison decision trees. A zero-error
randomized comparison decision tree is a distribution over (deterministic) comparison decision
trees T , each solves AH (x) correctly for all inputs. The expected query complexity is the maximum
over x of the expected number of queries performed by T (x) to compute AH (x).
Theorem 1.11. Let H ⊂ Rn be a finite set with inference dimension d. Then there exists a zeroerror randomized comparison decision tree that computes AH , whose expected query complexity is
O((d + n logd) log |H|).
A slightly weaker version of Theorem 1.11 appears in [16] (see Theorem 4.1 there). The third
and last step is to de-randomize Theorem 1.11 and obtain a deterministic comparison decision tree.
Theorem 1.12. Let H ⊂ Rn be a finite set with inference dimension d. Then there exists a comparison decision tree that computes AH , whose query complexity is O((d + n log(nd)) log |H|).
The proof of Theorem 1.12 uses a double-sampling argument, a technique originated in the study
of uniform convergence bounds in statistical learning theory [22]. The following corollary summarizes the above theorems concisely. For h ∈ Zn, define h∞ = max |hi |.
Corollary 1.13. Let H ⊂ Zn be such that h∞ ≤ w for all h ∈ H. Then there exists a comparison
decision tree computing AH whose query complexity is O(n log(nw) log |H|).
Proof. Observe that h1 ≤ nh∞ ≤ nw. By Theorem 1.10, the inference dimension of H is
d = O(n log(nw)). The corollary now follows from Theorem 1.12.
One can now verify that Theorem 1.1, Theorem 1.2, and Theorem 1.3 follow from Corollary 1.13
by setting w = 1.
1.4 Proof Overview
Theorem 1.10 Proof Overview. We need to show that if S ⊆ H is sufficiently large (concretely,
at least Ω(n logw)), then there is some h ∈ S whose label sign(h, x) can be inferred from the
comparison and label queries on S \ {h}. We derive this using two simple kinds of derivation rules:
(i) Say we already inferred for h1,h2 that h1, x = h2, x = 0, and h is a linear combination
of h1,h2. Then infer that h, x = 0.
(ii) Say we already inferred that h1, x > 0 and that h3, x≥h2, x (using a comparison
query), and h = h1 + (h3 − h2). Then infer that h, x > 0 (similarly when h1, x < 0).
The challenging part is to show that one of these rules must apply whenever S ⊆ H is sufficiently
large. The easier case is when there are more than n different hi ∈ S with sign(hi, x) = 0: Here
we can apply rule (i), since one of the hi ’s must be in the linear span of the others. The other
case is when there are at least some O(n logw) different hi ∈ S having the same sign(hi, x) that
is not 0. Here we crucially rely on comparison queries in the following way: Assume that the
hi ’s in S are ordered such that 0 < h1, x≤h2, x ≤ .... We show that for some i
∗, hi ∗ can be
expressed as h1 +
i<j αi,j (hj − hi ), where αi,j ≥ 0, and hi ∗ does not participate in the summation.
Note that from this one can infer sign(hi ∗ , x) using derivation rule (ii). To show that such an
i
∗ exists, it turns out (by a simple calculation) that it suffices to show that there are two distinct
Journal of the ACM, Vol. 66, No. 3, Article 16. Publication date: April 2019.    
Near-optimal Linear Decision Trees for k-SUM and Related Problems 16:9
Boolean combinations of the (hi+1 − hi )’s that yield the same vector. The existence of such Boolean
combinations is derived via a pigeon-hole argument, using the assumption that the hi ’s are short
integral vectors.
Theorem 1.11 Proof Overview. Theorem 1.11 is derived by a fairly simple randomized algorithm
that repeats the following procedure until all labels in H are inferred: Sample 2d points from the
set of points whose labels are still not known. Then, query their labels and sort them (using comparison queries) according to the value of their inner product with the target point x. Last, using
the two derivation rules stated above, infer all possible labels of the other points in H and remove
all labeled points.
The correctness of this algorithm follows by showing that at each application of the procedure,
half of the remaining labels are inferred, in expectation (Lemma 4.2). This is derived by a short
symmetrization argument and basic properties of the inference dimension (Claim 4.1).
Theorem 1.12 Proof Overview. In the last step, we show that the above randomized algorithm can
be derandomized; we show that instead of picking 2d points at random in each application of the
procedure, we can deterministically pick a slightly larger subset (of size O(d + n logd)) with the
same inference guarantee. The existence of such a subset is derived via a probabilistic argument
that utilizes the double sampling argument from [22].
1.5 Further Research
We prove that many combinatorial point-location problems have near optimal linear decision trees.
Moreover, these are comparison decision trees in which the linear queries are particularly simple:
both sparse (in many cases) and have only {−1, 0, 1} coefficients. This raises the possibility of
having improved algorithms for these problems in other models of computations. To be concrete,
we focus on 3-SUM below, but the same questions can be asked for any other problem of a similar
flavor.
Uniform Computation. The most obvious question is whether the existence of a near optimal
linear decision tree implies anything about uniform computation. As showed in [12], this can lead
to log-factor savings. It is very interesting whether greater savings can be achieved. We do not
discuss this further here, as this question has been extensively discussed in the literature (see, e.g.,
[24]).
Nondeterministic Computation. Let A ⊂ R be a set of size |A| = n. It is very easy to “prove” that
A is a positive instance of 3-SUM by demonstrating three elements whose sum is zero. However,
it is much less obvious how to prove that A is a negative instance of 3-SUM. This problem was
explicitly studied in [2] in the context of nondeterministic ETH. They constructed such a proof
that can be verified in time O(n3/2) when the elements of A are small integers. It seems plausible
that our current approach may lead to improved bounds and to remove the condition that the
elements are small integers. Thus, we propose the following problem.
Open Problem 1.14. Given a set of n real numbers no three of which sums to 0, is there a proof of
that fact that can be verified in near-linear time?
3-SUM with Preprocessing. Let A ⊂ R of size |A| = n. The 3-SUM with preprocessing problem
allows one to preprocess the set A in quadratic time. Then, given any subset A ⊂ A, the goal is
to solve that 3-SUM problem on A in time significantly faster then n2. Chan and Lewenstein [4]
designed such an algorithm, which solves that 3-SUM problem on any subset in time O(n2−ε ) for
some small constant ε > 0. It is interesting whether our techniques can help improve this to nearlinear time.
Journal of the ACM, Vol. 66, No. 3, Article 16. Publication date: April 2019.
16:10 D. M. Kane et al.
Open Problem 1.15. Given a set of n real numbers, can they be preprocessed in O(n2) time, such
that later, for every subset of the numbers the 3-SUM problem can be solved in time near-linear in n?
Sorting Sumsets. Let A, B ⊂ R be sets of size |A| = |B| = n. We show that their sumset A + B can
be computed with a near-linear number of simple queries. However, the best uniform algorithms
for computing A + B still require time O(n2 logn). Given that the output size of A + B is typically
of size Ω(n2), it is plausible that the logn factor can be shaved.
Open Problem 1.16. Given two sets A, B of n real numbers, design an algorithm that sorts the
sumset A + B in time O(n2). To be concrete, the algorithm should work in the real RAM model.
General Point-location Problem. It is natural to ask whether the techniques used in this article, and in particular the inference-dimension, can be used to improve the state-of-the-art upper
bounds for general point location problems. Unfortunately, unless the set of hyperplanes H has
some combinatorial structure, its inference dimension may be unbounded: In [16] we construct
examples of H ⊂ R3 whose inference dimension is unbounded. Nevertheless, we conjecture that
by generalizing comparison queries (which are ±1 linear combinations of two elements in H) to
arbitrary linear combinations of two elements from H might solve the problem.
Conjecture 1.17. Let H ⊂ Rn. There exists a linear decision tree that computes AH of depth
O(n log |H|). Moreover, all the linear queries are in {αh + βh : α, β ∈ R,h
,h ∈ H}.
We show in [15] a weaker result, giving a linear decision tree using only generalized comparison
queries of depth O(n4 logn log |H|).
High-dimensional Gadgets. Another possibility of circumventing the examples of H ⊆ R3 with
unbounded inference dimension is to find stronger gadgets than comparisons. Define a gadget
with arity t on R3 as a function д : (R3)
t → R3. Given H ⊆ R3 and such a gadget д, a gadget
query is signд(h1,...,ht ), x =? for hi ∈ H. For example a comparison query is a gadget with
arity 2 defined by д(h1,h2) = h1 − h2.
Is there a fixed gadget such that for every H ⊆ R3 there exists an LDT of depth O(log |H|) that
uses only label queries and gadget queries? (in [16] it is shown that for d = 2 the comparison
gadget satisfies it.) Clearly, if the answer is yes for d = 3, then it is interesting to generalize the
construction for general dimension d.
Optimal Bounds. We suspect that our analysis can be sharpened to improve the log-factors that
separate it from the information theoretical lower bounds. For concreteness, we pose the following
conjecture.
Conjecture 1.18. For any H ⊂ {−1, 0, 1}
n, there exists a comparison decision tree that computes
AH with O(n log |H|) many queries. In particular,
• 3-SUM on n real numbers can be solved by a 6-sparse linear decision tree that makesO(n logn)
queries.
• Sorting A + B, where A, B are sets of n real numbers, can be solved by a 8-sparse linear decision
tree that makes O(n logn) queries.
• SUBSET-SUM on n real numbers can be solved by a linear decision tree that makes O(n2)
queries.
Note that Corollary 1.13 gives a bound, which is off by a logn factor for these problems.
Organization. We begin with some preliminaries in Section 2. Theorem 1.10 is proved in Section 3. Theorem 1.11 is proved in Section 4. Theorem 1.12 is proved in Section 5.
Journal of the ACM, Vol. 66, No. 3, Article 16. Publication date: April 2019.
Near-optimal Linear Decision Trees for k-SUM and Related Problems 16:11
2 PRELIMINARIES
Let H ⊆ Rn be a finite set. For every x ∈ Rn, AH (x) denotes the function
AH (x) := (sign(x,h) : h ∈ H) ∈ {−, 0, +}
H ,
where sign : R → {−, 0, +} is the sign function and ·, · is the standard inner product in Rn.
The following lemma is a variant of standard bounds on the number of cells in a hyperplane
arrangement.
Lemma 2.1. Let H ⊂ Rn be a set of size |H| = m. Then |{AH (x) : x ∈ Rn }| ≤ (2em)
n.
Proof. It is well known that a set of m hyperplanes partitions Rn to at most ( m
≤n ) = n
i=0 (
m
i )
open cells. The lemma follows by first choosing i ≤ n linearly independent hyperplanes to which
x belongs and then applying the above bound to the remaining ones (restricted to a subspace of
dimension n − i). Thus



{AH (x) : x ∈ Rn }


 ≤
n
i=0

m
i
  m − i
≤ n − i

=
n
i=0
n−i
j=0

m
i
 m − i
j

=
n
s=0
s
i=0

m
s
 s
i

=
n
s=0

m
s

2s ≤

m
≤ n

2n ≤ (2em)
n,
where the second equality follows from the identity (
m
i )( m−i
j ) = (
m
s )( s
i ), where s = i + j, and the
last inequality follows from the well-known upper bound  m
≤n

≤ (em/n)
n ≤ (em)
n.
3 BOUNDING THE INFERENCE DIMENSION
We prove Theorem 1.10 in this section.
Theorem 1.10 (Restated). The inference dimension of H = {h ∈ Zn : h1 ≤ w} is d =
O(n logw).
Let S ⊂ Zn be such that h1 ≤ w for all h ∈ S. We assume |S | = d, where d is large enough to
be determined later. Fix x ∈ Rn. We will show that there exists h ∈ S such that S \ {h} infers h at x.
Partition S into {Sb : b ∈ {−, 0, +}}, where
Sb := {h ∈ S : sign(h, x) = b}.
We will show that if Sb is sufficiently large, then Sb \ {h} infers h at x for some h ∈ Sb and
b ∈ {−, 0, +}. The simplest case is when S0 is large:
Claim 3.1. If |S0 | > n, then there exists h ∈ S0 such that S0 \ {h} infers h at x. In particular, S \ {h}
infers h at x.
Proof. Let h1,...,hn+1 ∈ S0 be distinct elements such that hn+1 belongs to the linear span of
h1,...,hn. We claim that {h1,...,hn } infers hn+1 at x. More specifically, we claim that having
(i) sign(hi, x) = 0 for i ≤ n, and
(ii) hn+1 ∈ span{hi : i ≤ n}
imply that sign(hn+1, x) = 0. Indeed, by (ii) there exist coefficients αi ’s such that hn+1 = n
i=1 αihi , and, therefore, using (i), it follows that hn+1, x = 
n
i=1 αihi, x = n
i=1 αihi, x =
0.
Thus, we assume from now on that |S0 | ≤ n. We assume without loss of generality that |S+| ≥
|S−| and show that there is some h ∈ S+ such that S+ \ {h} infers h at x. The other case is analogous.
Journal of the ACM, Vol. 66, No. 3, Article 16. Publication date: April 2019.            
16:12 D. M. Kane et al.
Set m = (d − n)/2 and let h1,...,hm ∈ S+ sorted by
0 < h1, x ≤ ... ≤ hm, x.
The idea is to show that some hi satisfies that hi − h1 is in the cone spanned by the hk − hl , where
1 ≤ l ≤ k < i. Then, a simple argument shows that S+ \ {hi} infers hi at x. The existence of such
an hi is derived by a counting argument that boils down to the following lemma.
Claim 3.2. Assume that 2m−1 > (
2e (2w+1)m
n )
n. Then there exist α1,..., αm−1 ∈ {−1, 0, 1}, not all
zero, such that
m
−1
i=1
αi (hi+1 − hi ) = 0.
In particular, this holds for m = O(n logw) with a large-enough constant.
Proof. For any β ∈ {0, 1}
m−1 define f (β) :=  βi (hi+1 − hi ). Note that f (β) ∈ Zn, and since
hi 1 ≤ w for all i, it follows that  f (β)1 ≤ 2w(m − 1) by the triangle inequality. Let F := {f (β) :
β ∈ {0, 1}
m−1}. Next, we bound |F |. We claim that
|F | ≤ 2n

2w(m − 1) + n
n

.
To see that, note that there are 2n possible signs for each f ∈ F . The number of patterns for the
absolute values is at most the number of ways to express 2w(m − 1) as the sum of n + 1 nonnegative
integers. Equivalently, it is the number of ways of placing 2w(m − 1) balls in n + 1 bins, which is
(
2w (m−1)+n
n ). We further simplify
|F | ≤ 2n

2w(m − 1) + n
n

≤ 2n

(2w + 1)m
n

≤

2e (2w + 1)m
n
n
.
By our assumptions 2m−1 > |F |. Thus by the pigeonhole principle there exist distinct β
, β for
which f (β
) = f (β). The claim follows for α = β − β.
We assume that d = O(n logw) with a large-enough constant, so that the conditions of Claim 3.2
hold. Let α1,..., αm−1 ∈ {−1, 0, 1}, not all zero, be such that  αi (hi+1 − hi ) = 0. Let 1 ≤ p ≤ m − 1
be maximal such that αp  0. We may assume that αp = −1, as otherwise we can negate all of
α1,..., αm−1.
Adding hp+1 − h1 = p
i=1 (hi+1 − hi ) to 0 =  αi (hi+1 − hi ), we obtain that
hp+1 − h1 =

p
i=1
(αi + 1)(hi+1 − hi ) =

p−1
i=1
(αi + 1)(hi+1 − hi ),
where the first equality holds as αi = 0 if i > p, and the second equality holds as αp = −1.
We claim that R = {h1,...,hp } infers hp+1 at x, which completes the proof. More specifically,
we claim that having
(i) 0 < h1, x ≤ ... ≤ hp, x,
(ii) hp+1 − h1 = p−1
i=1 (αi + 1)(hi+1 − hi ), where the coefficients αi + 1 ≥ 0 for all i,
Journal of the ACM, Vol. 66, No. 3, Article 16. Publication date: April 2019.       
Near-optimal Linear Decision Trees for k-SUM and Related Problems 16:13
imply that sign(hp+1, x) ≥ 0. Indeed, item (i) implies that x,hi − hj ≥ 0, for every 1 ≤ j < i ≤ p,
and item (ii) implies that hp+1 − h1 is in the cone spanned by hi − hj for 1 ≤ j < i ≤ p. Thus, also
x,hp+1 − h1 ≥ 0, which implies, by the leftmost inequality of item (ii), thatx,hp+1≥x,h1 > 0,
as required.
4 ZERO-ERROR RANDOMIZED COMPARISON DECISION TREE
We prove Theorem 1.11 in this section.
Theorem 1.11 (Restated). Let H ⊂ Rn be a finite set with inference dimension d. Then there
exists a zero-error randomized comparison decision tree that computes AH , whose expected query
complexity is O((d + n logd) log |H|).
We begin with the following claim. Recall that infer(S, x) is the set of h ∈ Rn that can be inferred
from S at x.
Claim 4.1. Let S ⊂ Rn with inference dimension d and |S | = d + m − 1. Then for every x ∈ Rn,
there exist h1,...,hm ∈ S such that
hi ∈ infer(S \ {hi}, x).
Proof. We apply the definition of inference dimension iteratively. Fix x ∈ Rn. Assume that
we constructed h1,...,hi−1 so far for i ≤ m. Let Si = S \ {h1,...,hi−1}. As |Si | ≥ d there exists
hi ∈ Si such that Si \ {hi} infers hi at x. That is, hi ∈ infer(Si \ {hi}, x). But as Si ⊂ S then also
hi ∈ infer(S \ {hi}, x).
Lemma 4.2. Let H ⊂ Rn be a finite set with inference dimension d and |H| > 2d. Let S ⊂ H be a
uniformly chosen subset of size |S | = 2d. Then for every x ∈ Rn,
ES [|infer(S, x) ∩ H|] ≥ |H|
2 .
Proof. Fix x ∈ Rn. We have
ES

|infer(S, x) ∩ H|
|H|

= Pr
S ⊂H,h∈H
[h ∈ infer(S, x)]
≥ Pr
S ⊂H,h∈H\S
[h ∈ infer(S, x)]
= Pr[h2d+1 ∈ infer({h1,...,h2d }, x)],
where h1,...,h2d+1 ∈ H are uniformly chosen distinct elements. The inequality “PrS ⊂H,h∈H [h ∈
infer(S, x)] ≥ PrS ⊂H,h∈H\S [h ∈ infer(S, x)]” follows as h ∈ infer(S, x) for any h ∈ S.
Let R := {h1,...,h2d+1}. By symmetry it holds that
Pr[h2d+1 ∈ infer({h1,...,h2d }, x)] = 1
2d + 1
2

d+1
i=1
Pr
R [hi ∈ infer(R \ {hi}, x)]
= ER

|{hi ∈ R : hi ∈ infer(R \ {hi}, x)}|
2d + 1

.
By Claim 4.1, for any R ⊂ H it holds that |{hi ∈ R : hi ∈ infer(R \ {hi}, x)}| ≥ |R| − d. Thus,
ES

|infer(S, x) ∩ H|
|H|

≥
d + 1
2d + 1
>
1
2
.

We are now in a position to describe the algorithm that establishes Theorem 1.11.
Journal of the ACM, Vol. 66, No. 3, Article 16. Publication date: April 2019.     
16:14 D. M. Kane et al.
Zero-error randomized comparison decision tree for AH
Input: x ∈ Rn
Output: AH (x)
(1) Initialize: H0 = H, i = 0, v(h) =? for all h ∈ H.
(2) Repeat while |Hi | ≥ 2d:
(2.1) Sample uniformly Si ⊂ Hi of size |Si | = 2d.
(2.2) Query sign(h, x) for h ∈ Si and sort the h, x using comparison queries.
(2.3) Compute infer(Si, x) ∩ Hi .
(2.4) For all h ∈ infer(Si, x) ∩ Hi , set v(h) ∈ {−, 0, +} to be the inferred value
of h at x.
(2.5) Set Hi+1 := Hi \ (infer(Si, x) ∩ Hi ).
(2.6) Set i := i + 1.
(3) Query sign(h, x) for all h ∈ Hi , and set v(h) accordingly.
(4) Return v as the value of AH (x).
Analysis. To establish Theorem 1.11, we first show that for every x ∈ Rn, the algorithm terminates after O(log |H|) iterations in expectation. This follows as E[|Hi |] ≤ 2−i |H|, which we show
by induction on i. It clearly holds for i = 0. For i > 0 by Lemma 4.2, if we condition on Hi−1 then
ESi[|Hi | | Hi−1] ≤ |Hi−1 |
2
and hence
E[|Hi |] = EHi−1 [ESi[|Hi | | Hi−1]] ≤ E

|Hi−1 |
2

≤ 2−i
|H|.
Thus, it remains to bound the number of queries in every round. Observe that the only queries to x
are in steps (2.2) and (3). In step (3), the algorithm makes at most 2d label queries. In step (2.2), we
need to compute sign(x,h) for all h ∈ Si , which requires |Si | = 2d label queries and to compute
sign(x,h − h) for all h
,h ∈ Si . This can be done in O(d logd) comparison queries by sorting
the elements {x,h : h ∈ Si} giving some O(d logd log |H|) bound on the expected total number
of queries.
This bound can be improved using Fredman’s sorting algorithm [9].
Theorem 4.3 [9]. Let Π be a family of orderings over a set of m elements. Then there exists a
comparison decision tree that sorts every π ∈ Π using at most
2m + log |Π|
comparisons.
To use Fredman’s algorithm, observe that the ordering “≺” on Si that is being sorted in the ith
round is defined by the inner product with x,
h ≺ h ⇐⇒ h
, x≤h, x.
The following claim bounds the number of such orderings.
Claim 4.4. Let S ⊂ Rn. Let ΠS,x be the ordering on S define by inner product with x ∈ Rn. Then
|{ΠS,x : x ∈ Rn }| ≤ (2e |S |
2)
n .
Proof. Observe that ΠS,x  ΠS,x if and only if there are h
,h ∈ S such that sign(h − h,
x 
)  sign(h − h, x ). Thus, the number of different orderings is at most the size of {AS−S (x) :
Journal of the ACM, Vol. 66, No. 3, Article 16. Publication date: April 2019.   
Near-optimal Linear Decision Trees for k-SUM and Related Problems 16:15
x ∈ Rn }, where S − S = {h − h : h
,h ∈ S}. Since |S − S |≤|S |
2, Lemma 2.1 implies an upper
bound of (2e |S |
2)
n as claimed.
Thus, by using Fredman’s algorithm, we can sort Si with justO(|Si | + n log |Si |) = O(d + n logd)
comparisons in each round, which gives a total number of
O((d + n logd) log |H|)
queries in total.
5 DETERMINISTIC COMPARISON DECISION TREE
We prove Theorem 1.12 in this section, which is a de-randomization of Theorem 1.11.
Theorem 1.12 (Restated). Let H ⊂ Rn with inference dimension d. Then there exists a deterministic comparison decision tree that computes AH , whose query complexity is O((d + n log(nd))
log |H|).
First, note the following straightforward Corollary of Lemma 4.2.
Corollary 5.1. Let H ⊂ Rn be a finite set with inference dimension d. Let S ⊂ H be uniformly
chosen of size |S | = 2d. Then
(∀x ∈ Rn ) : Pr
S

|infer(S, x) ∩ H| ≥ |H|
4

≥
1
4
.
Theorem 1.12 follows by establishing a universal set S that is good for all x ∈ Rn.
Lemma 5.2. Let H ⊂ Rn be a finite set with inference dimension d. Then there exists S ⊆ H of size
|S | = O(d + n logd) such that:
(∀x ∈ Rn ) : |infer(S, x) ∩ H| ≥ |H|
8 .
We first argue that Theorem 1.12 follows directly from the existence of such an S. The algorithm
is a straightforward adaptation of the zero-error randomized comparison algorithm, except that
now we use this set S that works for all x ∈ Rn at once.
Deterministic comparison decision tree for AH
Input: x ∈ Rn
Output: AH (x)
(1) Initialize: H0 = H, i = 0, v(h) =? for all h ∈ H. Let s = O(d + n logd) as in
Lemma 5.2.
(2) Repeat while |Hi | ≥ s:
(2.1) Pick Si ⊂ Hi of size |Si | = s such that
∀x ∈ Rn, |infer(Si, x) ∩ H| ≥ |H|
8 .
(2.2) Query sign(h, x) for h ∈ Si and sort the h, x using comparison queries.
(2.3) Compute infer(Si, x) ∩ Hi .
(2.4) For all h ∈ infer(Si, x) ∩ Hi , set v(h) ∈ {−, 0, +} to be the inferred value
of h at x.
(2.5) Set Hi+1 := Hi \ (infer(Si, x) ∩ Hi ).
(2.6) Set i := i + 1.
(3) Query sign(h, x) for all h ∈ Hi , and set v(h) accordingly.
(4) Return v as the value of AH (x).
Journal of the ACM, Vol. 66, No. 3, Article 16. Publication date: April 2019.  
16:16 D. M. Kane et al.
Analysis. Lemma 5.2 ensures that a set Si always exist. Thus, for any x, the algorithm terminates after O(log |H|) rounds. Observe that the only queries to x are in steps (2.2) and (3). In
step (3) the algorithm makes at most s = O(d + n logd) label queries. In step (2.2), we need to
compute sign(x,h) for all h ∈ Si and to compute sign(x,h − h) for all h
,h ∈ Si , which
can be done sorting the elements {x,h : h ∈ Si}. Using Fredman’s algorithm, this requires
O(|Si | + n log |Si |) = O(d + n log(dn)) many comparisons in each round, which gives a total number of
O((d + n log(dn)) log |H|)
queries.
5.1 Proof of Lemma 5.2
Let S ⊂ H be a uniform subset of size |S | = s, where s = O(d + n logd). Define the event
E(S) :=

∃x ∈ Rn, |infer(S, x) ∩ H| <
|H|
8

.
It suffices to prove that Pr[E(S)] < 1 to prove the existence of S. In fact, as we will see, by choosing
sufficiently large constants in the choice ofs = O(d + n logd), the probability Pr[E(S)] can be made
≤ 1/2 (say), so a random set would also work.
To establish that E(S) < 1 we use a variant of the double sampling method [22] (see also [23]).
Let T ⊂ S be a uniformly chosen subset of size |T | = 2d. Define the event
E(S,T ) :=

∃x ∈ Rn, |infer(T, x) ∩ H| <
|H|
8
and |infer(T, x) ∩ S | ≥ |S |
4

.
We bound Pr[E(S)] in two steps. We first show that (i) Pr[E(S)] ≤ 4 Pr[E(S,T )] and then that
(ii) Pr[E(S,T )] ≤ 1
8 .
Claim 5.3. Pr[E(S)] ≤ 4 Pr[E(S,T )].
Proof. For each S for which E(S) holds fix xS ∈ Rn such that |infer(S, xS ) ∩ H| < |H |
8 . Then
Pr[E(S,T ) | S] ≥ Pr
|infer(T, xS ) ∩ H| <
|H|
8
and |infer(T, xS ) ∩ S | ≥ |S |
4

.
The first condition holds with probability one, since T ⊂ S and hence infer(T, xS ) ⊂ infer(S, xS ).
For the second condition, asT ⊂ S is a uniformly chosen subset of size |T | = 2d, Corollary 5.1 gives
Pr
T

|infer(T, xS ) ∩ S | ≥ |S |
4





S

≥
1
4
.
Thus
Pr[E(S,T ) | S] ≥
1
4
As this holds for every S for which E(S) holds, we have Pr[E(S,T )|E(S)] ≥ 1/4, which implies the
claim.
We next bound the probability of E(S,T ). We will prove that for every fixed T ,
Pr[E(S,T ) | T ] ≤
1
8
,
which will conclude the proof. So fix T ⊂ H of size |T | = 2d. Let T −T denote the set {h − h :
h
,h ∈ T }, and let T ∗ = T ∪ (T −T ). Recall that AT ∗ (x) is defined by
AT ∗ (x) = (sign(h, x) : h ∈ T ∗) ∈ {−, 0, +}
T ∗
.
Journal of the ACM, Vol. 66, No. 3, Article 16. Publication date: April 2019.          
Near-optimal Linear Decision Trees for k-SUM and Related Problems 16:17
Observe that the set infer(T, x) depends only on AT ∗ (x); that is, if AT ∗ (x 
) = AT ∗ (x ), then
infer(T, x 
) = infer(T, x ). Let XT ⊂ Rn be a set that contains one representative from each equivalence class of the relation x  ∼ x  ⇐⇒ AT ∗ (x 
) = AT ∗ (x ). Thus we can rephrase the event
E(S,T ) as
E(S,T ) =

∃x ∈ XT , |infer(T, x) ∩ H| <
|H|
8
and |infer(T, x) ∩ S | ≥ |S |
4

.
The advantage of consideringXT is that now we can bound the probability of E(S,T ) using a union
bound that depends on the (finite) set XT . More specifically, let
X
T :=
	
x ∈ XT : |infer(T, x) ∩ H| <
|H|
8


.
We thus established the following claim.
Claim 5.4. For every T ⊂ H,
Pr[E(S,T ) | T ] ≤

x ∈X
T
Pr
S

|infer(T, x) ∩ S | ≥ |S |
4





T

.
To conclude, it suffices to upper bound |X
T | and the probability that |infer(T, x) ∩ S | ≥ |S |
4 for
x ∈ X
T . Lemma 2.1 gives an upper bound on |XT | that also bounds |X
T |,
|X
T |≤|XT | = |AT ∗ | ≤ (2e |T ∗ |)
n = 2O (n log d)
.
We next bound the probability (over S ⊃ T ) that |infer(T, x) ∩ S | ≥ |S |
4 for x ∈ X
T .
Claim 5.5. Fix T ⊂ H of size |T | = 2d and fix x ∈ X
T . Assume that s ≥ 10|T |, and let S be a uniformly sampled set of size |S | = s such that T ⊂ S ⊂ H. Then
Pr
T

|infer(T, x) ∩ S | ≥ |S |
4




T

≤ 2−Ω(s )
.
Proof. Let R = S \ T . It suffices to bound the probability of the event that |infer(T, x) ∩ R| ≥ |R |
6 .
Indeed, if |infer(T, x) ∩ S | ≥ |S |
4 , then
|infer(T, x) ∩ R| ≥ |S |
4 − |T | = |R| + |T |
4 − |T | ≥ |R|
6 ,
where in the last inequality we used the assumption that |R| ≥ 9|T |.
The set R is a uniform subset of H\T of size |R| = |S |−|T |. By assumption, at most |H\T |
8 of the
elements in H \ T are in infer(T, x). By the Chernoff bound, the probability that at least |R|/6 of the
sampled elements belong to infer(T, x) is thus exponentially small in |R|. This finishes the proof
as |R| ≥ (9/10)s.
We now conclude the proof.
Pr[E(S,T ) | T ] ≤ |X
T |2−Ω(s ) ≤ 2O (n log d)−Ω(s ) ≤ 1/8,
as we choose s = O(d + n logd) with a large-enough hidden constant. Then we also have
Pr[E(S,T )] ≤ 1/8 and
Pr[E(S)] ≤ 4 Pr[E(S,T )] ≤ 1/2.             