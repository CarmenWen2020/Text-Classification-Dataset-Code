Stateless model checking is a powerful method for program verification that, however, suffers from an exponential growth in the number of explored executions. A successful technique for reducing this number,
while still maintaining complete coverage, is Dynamic Partial Order Reduction (DPOR), an algorithm originally introduced by Flanagan and Godefroid in 2005 and since then not only used as a point of reference but
also extended by various researchers. In this article, we present a new DPOR algorithm, which is the first
to be provably optimal in that it always explores the minimal number of executions. It is based on a novel
class of sets, called source sets, that replace the role of persistent sets in previous algorithms. We begin by
showing how to modify the original DPOR algorithm to work with source sets, resulting in an efficient and
simple-to-implement algorithm, called source-DPOR. Subsequently, we enhance this algorithm with a novel
mechanism, called wakeup trees, that allows the resulting algorithm, called optimal-DPOR, to achieve optimality. Both algorithms are then extended to computational models where processes may disable each other,
for example, via locks. Finally, we discuss tradeoffs of the source- and optimal-DPOR algorithm and present
programs that illustrate significant time and space performance differences between them. We have implemented both algorithms in a publicly available stateless model checking tool for Erlang programs, while the
source-DPOR algorithm is at the core of a publicly available stateless model checking tool for C/pthread programs running on machines with relaxed memory models. Experiments show that source sets significantly
increase the performance of stateless model checking compared to using the original DPOR algorithm and
that wakeup trees incur only a small overhead in both time and space in practice.
CCS Concepts: • Theory of computation → Verification by model checking; Logic and verification; •
Software and its engineering → Formal software verification; Software testing and debugging;
Additional Key Words and Phrases: Dynamic partial order reduction, software model checking, systematic
testing, concurrency, source sets, wakeup trees
1 INTRODUCTION
Verification and testing of concurrent programs is difficult, since one must consider all the different ways in which processes/threads can interact. Model checking (Clarke et al. 1983; Queille
and Sifakis 1982) addresses this problem by systematically exploring the state space of a given
program and verifying that each reachable state satisfies a given property. Applying model checking to realistic programs is problematic, however, since it requires capturing and storing a large
number of global states. Stateless model checking (Godefroid 1997) avoids this problem by exploring
the state space of the program without explicitly storing global states. A special runtime scheduler
drives the program execution, making decisions on scheduling whenever such decisions may affect
the interaction between processes. Stateless model checking has been successfully implemented in
tools, such as VeriSoft (Godefroid 2005), Chess (Musuvathi et al. 2008), and Concuerror (Christakis
et al. 2013), the tool we use in this article.
While stateless model checking is applicable to realistic programs, it suffers from combinatorial
explosion, as the number of possible interleavings grows exponentially with the length of program execution. There are several approaches that limit the number of explored interleavings,
such as depth bounding (Russell and Norvig 2009) and context bounding (Musuvathi and Qadeer
2007). Among them, partial order reduction (POR) (Clarke et al. 1999; Godefroid 1996; Peled 1993;
Valmari 1991) stands out, as it provides full coverage of all behaviours that can occur in any interleaving, even though it explores only a representative subset. POR is based on the observation
that two interleavings can be regarded as equivalent if one can be obtained from the other by
swapping adjacent, non-conflicting (independent) execution steps. In each such equivalence class
(called a Mazurkiewicz trace (Mazurkiewicz 1987)), POR explores at least one interleaving. This
is sufficient for checking most interesting safety properties, including race freedom, absence of
global deadlocks, and absence of assertion violations (Clarke et al. 1999; Godefroid 1996; Valmari
1991).
Existing partial order reduction approaches are essentially based on two techniques, both of
which reduce the set of process steps that are explored at each scheduling point:
• The persistent set technique, which explores only a provably sufficient subset of the enabled processes. This set is called a persistent set (Godefroid 1996); variations are stubborn
sets (Valmari 1991) and ample sets (Clarke et al. 1999).
• The sleep set technique (Godefroid 1996), which maintains information about the past exploration in a so-called sleep set, which contains processes whose exploration would be
provably redundant.
These two techniques are independent and complementary and can be combined to obtain increased reduction.
The construction of persistent sets is based on information about possible future conflicts between threads. Early approaches analyzed such conflicts statically, leading to over-approximations
and therefore limiting the achievable reduction. Dynamic Partial Order Reduction (DPOR)
(Flanagan and Godefroid 2005b) improves the precision by recording actually occurring conflicts
during the exploration and using this information to construct persistent sets on-the-fly, “by need.”
DPOR guarantees the exploration of at least one interleaving in each Mazurkiewicz trace when
the explored state space is acyclic and finite. This is the case in stateless model checking in which
only executions of bounded length are analyzed (Flanagan and Godefroid 2005b; Godefroid 2005;
Musuvathi et al. 2008).
Challenge. Since DPOR is excellently suited as a reduction technique, several variants, improvements, and adaptations for different computation models have appeared in the literature (Flanagan
Journal of the ACM, Vol. 64, No. 4, Article 25. Publication date: August 2017.
Source Sets: A Foundation for Optimal Dynamic Partial Order Reduction 25:3
and Godefroid 2005b; Kastenberg and Rensink 2008; Lei and Carver 2006; Saarikivi et al. 2012; Sen
and Agha 2007; Tasharofi et al. 2012). The obtained reduction can, however, vary significantly
depending on several factors, for example, the order in which processes are explored at each point
of scheduling. For a particular implementation of DPOR, up to an order of magnitude of difference in the number of explored interleavings has been observed when different strategies are
used (Tasharofi et al. 2012). Heuristics for choosing which next process to explore have been investigated without conclusive results (Lauterburg et al. 2010). It has also been shown that for specific
communication models, specializations of DPOR algorithms, obtained by carefully defining when
execution steps are in conflict (for example, exploiting the transitivity of the dependency relation
in actor systems (Tasharofi et al. 2012)), can achieve better reduction.
Let us explain one fundamental reason for the above variation in obtained reduction. In DPOR,
the combination of persistent set and sleep set techniques guarantees to explore at least one complete interleaving in each Mazurkiewicz trace. Moreover, it has already been proven that the use
of sleep sets is sufficient to prevent the complete exploration of two different but equivalent interleavings (Godefroid et al. 1995). At first sight, this seems to imply that sleep sets can give optimal
reduction. What it actually implies, however, is that when the algorithm initiates the exploration
of an interleaving that is equivalent to an already explored one, the exploration will begin but it
will be blocked sooner or later by the sleep sets in what we call a sleep-set-blocked exploration.
When only sleep sets are used for reduction, the exploration effort will include an arbitrary number of sleep-set-blocked explorations. It is here where persistent sets enter the picture and limit the
number of initiated explorations. Computation of smaller persistent sets leads to fewer sleep-setblocked explorations. However, as we show in this article, persistent sets are not powerful enough
to completely prevent sleep-set-blocked exploration.
Another reason for the observed non-optimal reduction is that most published algorithms define conflicts as occurring when two statements access the same shared object, and at least one of
them modifies the object. However, in many cases more reduction can be achieved by using a more
refined definition of conflicts. For instance, two writes to a shared variable need not conflict if they
write the same value. As another example, a send statement and a receive statement that access
the same message queue need not be in conflict, unless the two statements concern the same message. Thus, a reduction technique would benefit from a refined definition of dependencies, which
considers not only the accessed shared object but also the actual effect of each program statement
in a given execution. Such a refined definition would be particularly beneficial for programs that
employ message passing communication.
In view of these variations, a fundamental challenge is to develop an optimal DPOR algorithm
that (i) always explores the minimum number of interleavings, regardless of scheduling decisions,
(ii) can be efficiently implemented, and (iii) is applicable to a variety of computation models, including concurrency via message passing, communication via shared variables protected by locks, or
in more general forms of interprocess communication. Such an optimal DPOR algorithm could be
standardly implemented in stateless model checkers and test generation tools for concurrent systems. It would also attain the maximal reduction that can be achieved using information about conflicts between processes, implying that any further reductions must be obtained by other means, for
example, by symbolic techniques, SAT-based approaches such as the recently proposed SATCheck
technique (Demsky and Lam 2015), or by techniques that take alternative notions of causality into
account such as Maximal Causality Reduction (Huang 2015).
Contributions. In this article, we present a fundamentally new DPOR technique that is based on
a new theoretical foundation for partial order reduction, in which persistent sets are replaced by
a novel class of sets, called source sets. Source sets subsume persistent sets (i.e., any persistent set
Journal of the ACM, Vol. 64, No. 4, Article 25. Publication date: August 2017.
25:4 P. A. Abdulla et al.
is also a source set), and source sets are often smaller than persistent sets. Moreover, source sets
are provably minimal, in the sense that the set of explored processes from some scheduling point
must be a source set in order to guarantee exploration of all Mazurkiewicz traces. When a minimal
persistent set contains more elements than the corresponding source set, the additional elements
will always initiate sleep-set-blocked explorations. This implies that a necessary and sufficient
condition for the correctness of any DPOR algorithm is that the set of explored process steps is a
source set. For instance, correctness of persistent-set-based DPOR algorithms is often proved by
establishing that the set of explored process steps is always a persistent set. The results of this
article imply that in such proofs, it is enough to show the weaker property that this set is always
a source set. We thus claim that source sets are the “right” conceptual foundation for developing
DPOR techniques.
To show the power of source sets, we develop a simple DPOR algorithm, called source-DPOR,
which is based on source sets. It is derived by modifying the classical persistent-set-based DPOR
algorithm by Flanagan and Godefroid (2005b) so persistent sets are replaced by source sets. The
modification only consists in a small change to a single test in the algorithm. The power of source
sets can be observed by noting that with this small modification, source-DPOR achieves significantly better reduction in the number of explored interleavings than the classical DPOR algorithm. In fact, source-DPOR explores the minimal number of interleavings for a large number of
our benchmarks in Section 11.
To further demonstrate the power of source sets, we use them to develop a provably optimal
DPOR algorithm. This is done by combining source sets with a novel mechanism, called wakeup
trees, thereby deriving the algorithm optimal-DPOR. Wakeup trees control the initial steps of future
explorations, implying that optimal-DPOR never encounters any sleep-set-blocked (i.e., redundant)
exploration. An important feature of wakeup trees is that they are simple data structures that
are constructed from already-explored interleavings, hence they do not increase the amount of
exploration. On the other hand, they allow us to reduce the number of explored executions. In our
benchmarks, maintenance of the wakeup trees reduces total exploration time when source-DPOR
encounters sleep-set-blocked explorations, and, furthermore, it never requires more than 10% of
additional time in the cases where there are none or only a few sleep-set-blocked explorations.
Memory consumption is practically always the same between our two DPOR algorithms, and in
our experience the space cost of maintaining wakeup trees is very small. Still, as we will show, one
can construct programs where the size of wakeup trees grows exponentially, and, consequently,
the memory requirements of optimal-DPOR can be considerably worse than those of the sourceDPOR algorithm. However, each branch in the wakeup tree is a prefix of some actually explored
execution, and hence the size of the wakeup trees can never be larger than the size of all explored
executions. Therefore, memory consumption is a problem only in situations where any DPORbased algorithm needs to explore an exponential set of traces and does not seem to affect the time
performance of the optimal algorithm; see Section 9.
We show the applicability of our algorithms to a wide range of computation models, including shared variables and message passing, by formulating them in a general setting, which only
assumes that we can compute a happens-before relation (also called a causal ordering) between
the events in an execution. For systems with shared variables, the happens-before relation can be
based on the variables that are accessed or modified by events. For message passing systems, the
happens-before relation can be based on correlating the transmission of a message with the corresponding reception. Our approach allows us to make finer distinctions, leading to better reduction, than many other approaches that define a happens-before relation that is based on program
statements, possibly taking into account the local state in which they are executed (Clarke et al.
1999; Flanagan and Godefroid 2005b; Godefroid 1996, 2005; Lauterburg et al. 2010; Tasharofi et al.
Journal of the ACM, Vol. 64, No. 4, Article 25. Publication date: August 2017.
Source Sets: A Foundation for Optimal Dynamic Partial Order Reduction 25:5
Fig. 1. Writer-readers code excerpt.
2012; Valmari 1991). For instance, we allow a send transition to be dependent with another send
transition only if the order in which the two messages are received is significant. Similarly to preceding articles on DPOR, we assume, for simplicity of exposition, that threads are deterministic.
Our techniques can be extended to control nondeterminism without introducing fundamentally
new concepts, whereas data nondeterminism typically requires additional symbolic techniques.
We have implemented both source-DPOR and optimal-DPOR as extensions for Concuerror (Christakis et al. 2013), a stateless model checking tool for Erlang programs. Erlang’s concurrency model focuses primarily on message passing, but it is also possible to write programs that
manipulate shared data structures. Our evaluation shows that on a wide selection of benchmarks,
including benchmarks from the DPOR literature, but more importantly on real Erlang applications of considerable size, we obtain optimal or very close to optimal reduction in the number
of interleavings even with source-DPOR, therefore significantly outperforming the original DPOR
algorithm not only in number of explored interleavings but in total execution time as well.
Organization. In the next section, we illustrate the basic new ideas of our techniques. We introduce our computational model and formulation of the partial order framework in Section 3. To
simplify presentation, we initially assume that processes do not disable each other. In Section 4,
we introduce source sets and establish that source sets are necessary and sufficient for soundness of any DPOR algorithm. The source-DPOR algorithm is described in Section 5. We formalize
the concept of wakeup trees in Section 6, before describing the optimal-DPOR algorithm in Section 7. We then extend the algorithms to computational models where processes may disable each
other, for example, via locks, in Section 8. Section 9 discusses some tradeoffs in computational
cost between source-DPOR and optimal-DPOR. Implementation of the algorithms is described in
Section 10, and experimental evaluation in Section 11. The article ends by surveying related work
and offering some concluding remarks.
2 BASIC IDEAS
In this section, we give an informal introduction to the concepts of source sets and wakeup trees,
and their improvement over existing approaches, using some small examples.
Source Sets. In Figure 1, the three processes p, q, and r perform dependent accesses to the shared
variable x. In this example, let us consider two accesses as dependent if they access the same
variable and one of them is a write. Since there are no writes to y and z here, the accesses to y
and z are not dependent with anything else. For this program, there are four Mazurkiewicz traces
(i.e., equivalence classes of executions), each characterized by its sequence of accesses to x (three
accesses can be ordered in six ways, but two pairs of orderings are equivalent, since they differ
only in the ordering of adjacent reads, which are not dependent).
Any POR method selects some subset of {p,q,r} to perform some first step in the set of explored
executions. It is not enough to select only p, since then executions where some read access happens
before the write access of p will not be explored. In DPOR, assume that the first execution to be
explored is p.q.q.r.r (we denote executions by the dotted sequence of scheduled process steps). A
DPOR algorithm will detect the dependency between step (1) by p and step (2) by q and note
that it seems necessary to explore sequences that start with a step of q. The DPOR algorithm will
Journal of the ACM, Vol. 64, No. 4, Article 25. Publication date: August 2017.
25:6 P. A. Abdulla et al.
also detect the dependency between (1) and (3) and possibly note that it is necessary to explore
sequences that start with a step of r.
Existing DPOR methods guarantee that the set of processes explored from the initial state is a
persistent set. In short, a set P of processes is persistent in the initial state if, in any execution from
the initial state, the first step that is dependent with the first step of some process in P must be taken
by some process in P. In this example, the only persistent set that contains p in the initial state is
{p,q,r}. To see this, suppose that, for example, r is not in the persistent set P, that is, P = {p,q}.
Then the execution r.r contains no step from a process in P, but its second step is dependent with
the first step of p, which is in P. In a similar way, one can see that also q must be in P.
In contrast, our source set-based algorithms allow S = {p,q} as the set of processes explored
from the initial state. The set S is sufficient, since any execution that starts with a step of r is
equivalent to some execution that starts with the first (local) step of q. The set S is not a persistent
set, but it is a source set. Intuitively, a set S of processes is a source set if for each execution E from
the initial state there is some process proc in S such that the first step in E that is dependent with
proc is taken by proc itself, that is, proc is not preceded by any process that is dependent with proc.
To see that {p,q} is a source set, note that when E isr.r, then we can choose q as proc, noting that
r.r is not dependent with the first step of q. Any persistent set is also a source set, but, as shown
by this example, the converse is not true.
Our algorithm source-DPOR combines source sets with sleep sets, and will explore exactly four
interleavings, whereas any algorithm based on persistent sets will explore at least five (if the first
explored execution starts with p), some of which will be sleep set blocked if sleep sets are used.
If we extend the example to include n reading processes instead of just two, then the number of
sleep-set-blocked explorations increases significantly (see Table 4 in Section 10).
Sleep Sets. To make this section self-contained, we here briefly review the concept of sleep sets.
Sleep sets (Godefroid 1996; Godefroid and Wolper 1991) use information about past explorations
to prevent redundant future explorations. For each prefix E of the execution that is currently being
explored, a sleep set is maintained. The sleep set contains a set of processes, whose exploration
after E would be redundant for the reason that an equivalent execution has already been explored
by the DPOR algorithm. The sleep set at each prefix E is manipulated as follows: (i) after exploring
the interleavings that extend E with some process p, the process p is added to the sleep set at E, and
(ii) when exploring executions that extend E.p, the sleep set at E.p is initially obtained as the sleep
set at E with all processes that are dependent with p removed. The effect is that the algorithm need
never explore a step of a process in the sleep set. We illustrate this on the program of Figure 1.
After having explored executions starting with p, the process p is added to the sleep set at the
empty execution, following rule (i). When initiating the exploration of executions that start with
q, the process p is in the sleep set at q, according to rule (ii). Therefore, p should not be explored
after q, since executions that start with q.p are equivalent to executions that start with p.q, and
such executions have already been explored.
Wakeup Trees. As mentioned, by utilizing source sets, source-DPOR will explore a minimal number of executions for the program of Figure 1. There are cases, however, where source-DPOR encounters sleep-set-blocked explorations.
We illustrate this by the example in Figure 2, a program with four processes, p,q,r,s. Two events
are dependent if they access the same shared variable, that is, x, y, or z. Variables m,n,l are local.
Each statement accessing a global variable has a unique label; for example, processs has three such
statements labeled (5), (6), and (7). Statements that operate on local variables are assumed to
be part of the previous labeled statement. For example, label (6) marks the read of the value of
y, together with the assignment to l, and the condition check on n. If the value of n is 1, then the
Journal of the ACM, Vol. 64, No. 4, Article 25. Publication date: August 2017.
Source Sets: A Foundation for Optimal Dynamic Partial Order Reduction 25:7
Fig. 2. Program with control flow.
Fig. 3. Explored interleavings for the program of Figure 2.
condition check on l is also part of (6), which ends just before the assignment to x that has the
label (7). Similar assumptions are made for the other local statements.
Consider a DPOR algorithm that starts the exploration with p, explores the interleaving
p.r.r.s.s.s (marked in Figure 3 with an arrow from top to bottom), and then detects the race between events (1) and (7). It must then explore some interleaving in which the race is reversed,
that is, the event (7) occurs before the event (1). Note that event (7) will occur only if it is
preceded by the sequence (3)–(4)–(5)–(6) and not preceded by a step of process q. Thus, an
interleaving that reverses this race must start with the sequence r.r.s.s. Such an interleaving is
shown in Figure 3 between the two chunks labeled “SSB traces” (Sleep Set Blocked traces).
Having detected the race in p.r.r.s.s.s, source-DPOR adds r to the source set at the initial state.
However, it does not “remember” thatr must be followed by r.s.s to reverse the race. After exploring r, it may therefore continue with q. However, afterr.q any exploration is doomed to encounter
sleep set blocking, meaning that the exploration reaches a state in which all enabled processes
are in the sleep set. To see this, note that p is in the sleep set when exploring r, and will remain
there forever in any sequence that starts with r.q (as explained above, it is removed only after the
sequence r.r.s.s.s). This corresponds to the left chunk of “SSB traces” in Figure 3.
Optimal-DPOR solves this problem by replacing the backtrack set with a structure called a
wakeup tree. This tree contains initial fragments of executions that are guaranteed not to encounter
Journal of the ACM, Vol. 64, No. 4, Article 25. Publication date: August 2017.
25:8 P. A. Abdulla et al.
sleep set blocking. In the example, the Optimal-DPOR algorithm will handle the race between (1)
and (7) by adding the sequence r.r.s.s.s to the wakeup tree. The point is that after r.r.s.s.s, the
process p has been removed from the sleep set, and so sleep set blocking is avoided.
Events versus Transitions. A majority of approaches to partial order reduction define dependencies as a relation between syntactic statements in the program (Clarke et al. 1999; Flanagan and
Godefroid 2005b; Godefroid 1996, 2005; Lauterburg et al. 2010; Tasharofi et al. 2012; Valmari 1991),
often referred to as transitions. The notion of transition is sometimes refined by including the local
state from which it is executed. There are also also approaches where dependencies are defined
between occurrences of transitions in a given execution (Lei and Carver 2006; Sen and Agha 2007),
sometimes referred to as events. In general, events allow finer distinctions when defining dependencies than transitions. For instance, two send operations need not be dependent unless the order
in which the two messages are received is significant. This information may not be available if the
send operations are regarded as syntactic statements, even if the local states of the sending process
is considered. On the other hand, if the send operations are considered in the context of a particular
execution, then the definition of dependencies can be appropriately refined. Thus, a reduction technique in which the definition of dependencies is based on events can achieve better reduction than
one based on transitions, for instance, when applied to programs that employ message passing.
We illustrate that events can be more suitable than transitions on a small example.
p: q:
send(q, first); receive m1;
send(q, second); receive m2;
L := append(append(L, m1), m2);
Here, process p sends two messages to process q, and process q receives two messages and appends
them to the list L. Messages are received in FIFO order. For this example, we assume that receive
statements must return a message but are non-blocking; if a receive is ever attempted by a process
when its incoming message queue is empty, then the process dies.
For this example, there are three possible outcomes of executing the program, characterized by
the number of messages received by q. Thus, we would want to define a definition of dependencies,
which gives rise to three different Mazurkiewicz traces: p.p.q.q, p.q.q.p, and q.p.p. We note that
a transition-based definition, which regards any send statement as dependent with any receive
statement will give rise to four Mazurkiewicz traces. To reduce this number, we must define the
transmission of the second message by p to be independent of the reception of the first message by
q, thereby making the executions p.p.q.q and p.q.p.q equivalent. This is possible if the definition
of dependencies is based on events, since that allows us to take the context of the execution into
account. In this particular example, this difference allows us to reduce the number of explored
transition sequences from four to three. In larger programs, where message queues may contain
several messages, the reduction in explored interleavings can be substantial.
3 FRAMEWORK
In this section, we introduce the technical background material. First, we present the general model
of concurrent systems for which the algorithms are formulated, thereafter the assumptions on the
happens-before relation, and, finally, the notions of independence and races.
3.1 Abstract Computation Model
We consider a concurrent system composed of a finite set of processes (or threads). Each process
executes a deterministic program, whereby statements act on the state of the system, which is
Journal of the ACM, Vol. 64, No. 4, Article 25. Publication date: August 2017.
Source Sets: A Foundation for Optimal Dynamic Partial Order Reduction 25:9
made up of the local states of each process and the shared state of the system (state will refer to
the global state, unless explicitly specified otherwise). We do not restrict the system to a specific
mode of process interaction, allowing instead the use of shared variables, messages, and so on.
We assume that the state space does not contain cycles and that executions have bounded length.
This means that executions must terminate by themselves within a bounded number of steps. Our
algorithms, as presented in this article, are unsound if executions are simply truncated beyond
some given bound. Soundness, in the sense of covering all executions up to a given length, can
be restored at the expense of refining the control over execution scheduling and exploring some
executions beyond the imposed bound; the details of this are beyond the scope of this article.
Let Σ be the set of states of the system. The system has a unique initial state s0 ∈ Σ. We assume
that the program executed by a process p can be represented as a partial function executep : Σ → Σ
that moves the system from one state to a subsequent state. Each such application of the function
executep represents an atomic execution step of process p, which may depend on and affect the
state. We let each execution step (or just step for short) represent the combined effect of some
global statement together with the following finite sequence of local statements (that only access
and affect the local state of the process), ending just before the next global statement. This avoids
consideration of interleavings of local statements of different processes in the analysis. Such an
optimization is common in tools (e.g., Verisoft (Godefroid 1997)).
An execution sequence E of a system is a finite sequence of execution steps of its processes
that is performed from the initial state s0. Since each execution step is deterministic, an execution
sequence E is uniquely characterized by the sequence of processes that perform steps in E. For
instance, p.p.q denotes the execution sequence where first p performs two steps, followed by a
step of q. The sequence of processes that perform steps in E also uniquely determine the state of
the system after E, which is denoted s[E]. The execution of a process is said to block in some state
s if the process cannot continue (i.e., executep (s) is undefined); for example, trying to receive a
message in a state where the message queue is empty. For a state s, let enabled(s) denote the set of
processes p that are enabled in s (i.e., for which executep (s) is defined). We say that E is maximal
if enabled(s[E]) = ∅, that is, no process is enabled after E. We use . to denote concatenation of
sequences of processes. Thus, if p is not blocked after E, then E.p is an execution sequence. We use
w,w
,... to range over arbitrary sequences of processes.
An event of E is a particular occurrence of a process in E. More precisely, an event is a pair p,i,
representing the ith occurrence of process p in the execution sequence. We use e, e
,... to range
over events, as well as:
• E 	 w to denote that E.w is an execution sequence.
• w \ p to denote the sequence w with its first occurrence of p removed.
• dom(E) to denote the set of events p,i that are in E, that is, p,i ∈ dom(E) iff E contains
at least i steps of p.
• dom[E](w), where E.w is an execution sequence, to denote dom(E.w) \ dom(E), that is, the
events in E.w which are in w.
• next[E](p) to denote dom[E](p) as a special case.
• e to denote the process p of an event e = p,i.
• op[E](e) to denote the operation performed by event e in execution sequence E.
• <E to denote the total order between events in E, that is, e <E e denotes that e occurs before
e in E.
• E ≤ E to denote that the sequence E is a prefix of the sequence E.
To simplify the presentation, we first, in Section 3 to 7, make the assumption that a process does
not disable another process, that is, if p is enabled and another process q performs a step, then
Journal of the ACM, Vol. 64, No. 4, Article 25. Publication date: August 2017.  
25:10 P. A. Abdulla et al.
p is still enabled. We formalize this as the following assumption, so it can be referenced in the
subsequent exposition.
Assumption 3.1. If E 	 p and E 	 q with p  q, then E 	 p.q.
Assumption 3.1 is valid for concurrent programs that communicate via message passing; that
is, it is valid for programs that follow the actor model of concurrency and Erlang programs in
particular. However, note that even in such programs a process can disable itself, for example,
after a step such that the next statement is a receive statement. In Section 8, we thereafter show
how our DPOR techniques can be extended to the situation where this assumption is not made,
and processes can disable each other, for example, by using locks or await statements.
3.2 Event Dependencies
A central concept in DPOR algorithms is that of a happens-before relation (Lamport 1978) between
events in an execution sequence (also called a causal relation (Sen and Agha 2007)). We denote the
happens-before relation in the execution sequence E by →E . Intuitively, for an execution sequence
E, and two events e and e in dom(E), e→Ee means that e “causally precedes” e or that the ordering between e and e may influence the outcome of the execution. For instance, e can be the
transmission of a message that is received by e
, e can be a write operation to a shared variable
that is accessed by e
, or e and e can both write to the same shared variable.
Our algorithms assume a function (called a happens-before assignment), which assigns a
“happens-before” relation to any execution sequence. In order not to restrict to a specific computation model, we take a general approach, where the happens-before assignment is only required
to satisfy a set of natural properties, which are collected in Definition 3.2. As long as it satisfies
these properties, its precision can vary. For instance, the happens-before assignment can let any
transmission to a certain message buffer be causally related with a reception from the same buffer.
However, better reduction can be attained if the assignment does not make the transmission of a
message dependent with the reception of a different message.
In practice, the happens-before assignment function is implemented by relating accesses to the
same variables, transmissions, and receptions of the same messages, and so on, typically using
vector clocks (Mattern 1989). In Section 10, we describe such an assignment, suitable for Erlang
programs.
Definition 3.2 (Properties of valid happens-before relations). A happens-before assignment, which
assigns a unique happens-before relation →E to any execution sequence E, is valid if it satisfies
the following properties for all execution sequences E:
(1) →E is an irreflexive partial order on dom(E), which is included in <E .
(2) The execution steps of each process are totally ordered, that is, p,i→E p,i+1 whenever
p,i+1 ∈ dom(E),
(3) If E ≤ E, then →E and →E are the same on dom(E
).
(4) Any linearization E of →E on dom(E) is an execution sequence that has exactly the same
“happens-before” relation →E as →E . This means that the relation →E induces a set of
equivalent execution sequences, all containing the same set of events and with the same
“happens-before” relation. We use
• E  E to denote that dom(E) = dom(E
) and that E and E are linearizations of the same
“happens-before” relation and
• [E] to denote the equivalence class of E.
(5) If E  E
, then s[E] = s[E].
Journal of the ACM, Vol. 64, No. 4, Article 25. Publication date: August 2017. 
Source Sets: A Foundation for Optimal Dynamic Partial Order Reduction 25:11
(6) For any sequences E, E
, and w, such that E.w is an execution sequence, we have E  E
iff E.w  E
.w.
(7) If p, q, and r are different processes, then if next[E](p)→E.p.r next[E.p](r) and next[E](p)
E.p.q next[E.p](q), then next[E](p)→E.p.q.r next[E.p.q](r).
The first six properties should be obvious for any reasonable happens-before relation; the only
non-obvious one would be the last one. Intuitively, Property (7) states that if, after some sequence
E, the next step of p happens before the next step of r, then the step of p still happens before
the step of r even when some step of another process, which is not dependent with p, is inserted
between p and r. This property holds for all computation models in which the happens-before
assignment is based on whether events access shared objects in potentially conflicting ways. As
an example, p and q can be reading a shared variable that is written by r. Another example is when
p sends a message that is received by r. If an intervening process q is independent with p, then it
cannot affect this message, and therefore r still receives the same message. It also holds for several
more refined happens-before assignments, which are based on actual values involved in accesses
to shared objects. An example is provided by the program in Figure 9 (Section 8).
Properties (4) and (5) of Definition 3.2 together imply, as a special case, that if e and e are two
consecutive events in E with eEe
, then they can be swapped, and the state after the two events
remains the same.
3.3 Independence and Races
We now define independence between events of a computation. If E.p and E.w are both execution
sequences, then E 	p♦w denotes that E.p.w is an execution sequence such that next[E](p)  E.p.we
for any e ∈ dom[E.p](w). In other words, E 	p♦w states that the next event of p would not “happen
before” any event inw in the execution sequence E.p.w. Intuitively, it means that p is independent
withw after E. In the special case whenw contains only one process q, then E 	p♦q denotes that the
next steps of p and q are independent after E. We use E p♦w to denote that E.p.w is an execution
sequence for which E 	p♦w does not hold.
For an execution sequence E and an event e ∈ dom(E), let
• pre(E, e) denote the prefix of E up to, but not including, the event e,
• notdep(e, E) denote the sub-sequence of E consisting of the events that occur after e but do
not “happen after” e (i.e., the events e that occur after e such that e  Ee
).
A central concept in most DPOR algorithms is that of a race. Let e and e be two events in
dom(E), where e <E e
. We say that
• e is in a race with e
, denoted e E e if e  e
 (i.e., these are events from different processes) and e→Ee and there is no event e ∈ dom(E), different from e and e, such that
e→Ee→Ee
,
• e is in a reversible race with e
, denoted e E e
, if e E e and in any equivalent execution
sequence E  E where e occurs immediately before e
, then e
 was not blocked before the
occurrence of e.
Intuitively, e E e denotes that e and e are co-enabled, that is, there is an equivalent execution
sequence E  E in which e and e are adjacent. Moreover, e E e denotes that e does not enable
e
, so the order of e and e can be reversed.
Whenever a DPOR algorithm detects a race, then it will check whether the events in the race
can be executed in the reverse order. Since the events are related by the happens-before relation,
Journal of the ACM, Vol. 64, No. 4, Article 25. Publication date: August 2017.           
25:12 P. A. Abdulla et al.
Fig. 4. A sample run of the program in Figure 1 is shown to the left. This run is annotated by a happens-before
relation (the dotted arrows). To the right, the happens-before relation is shown as a partial order. Notice that
E 	q♦r, since q and r are not happens-before related in E
.r.q. We also observe that I[E](w) = {q}, as q
is the only process occurring in w and its first occurrence has no predecessor in the dotted relation in w.
Furthermore, WI[E](w) = {q,r}, since r is not happens-before related to any event in w.
this may lead to a different state: Therefore, the algorithm must try to explore a corresponding
execution sequence.
In Figure 4, there are two pairs of events e, e such that e E e
, namely p, 1,q, 2 and
p, 1,r, 2. It also holds for both these pairs that e E e
, since both q and r are enabled before
p, 1. In other words, both the races in the program are reversible.
4 SOURCE SETS
In this section, we define the new concept of source sets. Intuitively, source sets are sets of processes
that perform “first steps” in the possible future execution sequences. A set of processes is a source
set if any possible future execution sequence contains a “first step” of some process in the source
set. We first define two related notions of possible “first steps” in a sequence.
Definition 4.1 (Initials and Weak Initials). For an execution sequence E.w, the set I[E](w) of processes that are initials and the set WI[E](w) of processes that are weak initials are defined as follows:
(1) p ∈ I[E](w) iff there is a sequence w such that E.w  E.p.w
(2) p ∈ WI[E](w) iff there are sequences w and v such that E.w.v  E.p.w
The following lemma gives alternative characterizations of the sets I[E](w) and WI[E](w).
Lemma 4.2. For an execution sequence E.w and a process p, we have
(1) p ∈ I[E](w) iff p ∈ w and there is no other event e ∈ dom[E](w) with e→E.w next[E](p),
(2) p ∈ WI[E](w) iff either p ∈ I[E](w) or p ∈ enabled(s[E]) and E 	p♦w.
Intuitively, a process in I[E](w) or WI[E](w) has no “happens-before” predecessors in dom[E](w)
and is in I[E](w) if it actually occurs in w.
Proof. We prove each of the cases separately.
(1) (Forward direction) From E.w  E.p.w
, using Property (4) of Definition 3.2, we infer
dom[E](w) = dom[E](p.w
), which implies p ∈ w. Furthermore, the fact that p can be
moved to the beginning of w implies that p cannot have any predecessors in dom[E](w),
that is, there is no other event e ∈ dom[E](w) with e→E.w next[E](p).
(Reverse direction) If p ∈ w and there is no other event e ∈ dom[E](w) with
e→E.w next[E](p), then we can linearize dom[E](w) in such a way that p occurs first, that
is, there is a w with E.w  E.p.w
.
(2) (Forward direction) From E.w.v  E.p.w
, using Property (4) of Definition 3.2, we infer
dom[E](w.v) = dom[E](p.w
). If p ∈ w, then the events in v can be removed from w to
obtain dom[E](w) = dom[E](p.w) for somew, implying p ∈ I[E](w). If p  w, then p ∈ v.
Since p can be moved before w
, it cannot happen-after any event in w; since p occurs
Journal of the ACM, Vol. 64, No. 4, Article 25. Publication date: August 2017.  
Source Sets: A Foundation for Optimal Dynamic Partial Order Reduction 25:13
the after w, it cannot happen-before any event in w, and hence E 	p♦w. Since p can be
performed directly after E, we have p ∈ enabled(s[E]).
(Reverse direction) If p ∈ I[E](w), then p ∈ WI[E](w) follows directly (letting v be the
empty sequence). If p ∈ enabled(s[E]) and E 	p♦w, then p is not related with w in the
happens-before relation, and hence E.w.p  E.p.w, implying p ∈ WI[E](w).
Definition 4.3 (Source Sets). Let E be an execution sequence, and let W be a set of sequences w,
such that E 	 w for each w ∈ W . A set P of processes is a source set forW after E if for each w ∈ W
we have WI[E](w) ∩ P  ∅.
The key property is that if P is a source set for W after E, then for each execution sequence of
form E.w with w ∈ W , there is a process p ∈ P and a sequence w such that E.p.w  E.w.v for
some sequencev. Therefore, when an exploration algorithm intends to cover all suffixes inW after
E, the set of processes that are chosen for exploration from s[E] must be a source set forW after E.
We formulate this observation as a theorem.
Theorem 4.4 (Key Property of Source Sets). Let E be an execution sequence, let W be a set of
continuations of E, and let W  be a subset of W such that for each w ∈ W there is a w ∈ W  with
E.w  E.w.v for somev. Then the set of first processes of sequences inW  is a source set forW after E.
This theorem implies that a necessary condition for the correctness of any DPOR algorithm is
that the set of explored process steps is a source set. It it also a sufficient condition: The proofs
of correctness for our algorithms source-DPOR (Theorem 5.2) and optimal-DPOR (Theorem 7.4)
contain as key lemmas the property that the set of explored processes is a source set (Claim 5.4
and Claim 7.5, respectively).
Before continuing, we first generalize the relations p ∈ I[E](w) and p ∈ WI[E](w) to the case
where p is a sequence. These generalizations will be used in the proof of soundness of the sourceDPOR algorithm, and in the definition of wakeup trees.
Definition 4.5. Let E be an execution sequence and let v and w be sequences of processes.
• Let v [E] w denote that there is a sequence v such that E.v.v and E.w are execution
sequences with E.v.v  E.w. Intuitively, v [E] w if, after E, the sequence v is a possible
way to start an execution that is equivalent to w.
• Let v∼[E]w denote that there are sequences v and w such that E.v.v and E.w.w are
execution sequences with E.v.v  E.w.w
. Intuitively, v∼[E]w if, after E, the sequence v is
a possible way to start an execution that is equivalent to an execution sequence of form
E.w.w
.
It can be seen that Definition 4.5 generalizes Definition 4.1, since for a process p we have p ∈
I[E](w) iff p [E] w and p ∈ WI[E](w) iff p∼[E]w.
As examples, in Figure 4, we have q.r [E] q.q.r.r but q.q [E] r.r. We also have q.q∼[E]r.r,
since E
.q.q.r.r  E
.r.r.q.q. Note that ∼[E] is not transitive. The relation v∼[E]w can be checked
using the properties specified in the following lemma.
Lemma 4.6. The relation v∼[E]w holds if either
(1) v = , or
(2) v is of form p.v
, and either
(a) p ∈ I[E](w) (“p is an initial of w after E”) and v
∼[E.p](w \ p), or
(b) E 	p♦w (“p is independent of w after E”) and v
∼[E.p]w.
Proof. We examine each case separately.
Journal of the ACM, Vol. 64, No. 4, Article 25. Publication date: August 2017.  
25:14 P. A. Abdulla et al.
(1) If v = , then let v = w and w = . Then E.v.v  E.v  E.w  E.w.w
, and hence
v∼[E]w.
(2) If v = p.v
, then
(a) If p ∈ I[E](w) and v
∼[E.p](w \ p), then by p ∈ I[E](w) and Definition 4.1(1) we get that
there exists w such that E.w  E.p.w
. In the proof of that Lemma (forward direction), it is easy to see that such a w is exactly w \ p, therefore E.w  E.p.(w \ p).
By v
∼[E.p](w \ p), we get that there exist sequences v and w such that E.v.v 
E.p.v
.v  E.p.(w \ p).w  E.w.w, and hence v∼[E]w.
(b) If E 	p♦w and v
∼[E.p]w, then from E 	p♦w we have that E.p.w  E.w.p. From
v
∼[E.p]w, we also have that there exist v and w such that E.v.v  E.p.v
.v 
E.p.w.w  E.w.p.w
, and hence v∼[E]w.
The following lemma states some useful properties.
Lemma 4.7. Let E be an execution sequence, and let v, w, and w be sequences. Then
(1) E.w  E.w implies that (i)v [E] w iffv [E] w
, (ii)w [E] v iffw [E] v, and (iii)v∼[E]w
iff v∼[E]w
;
(2) v [E] w and w∼[E]w imply v∼[E]w
;
(3) v [E] w and w [E] w imply v∼[E]w;
(4) p ∈ WI[E](w) and w [E] w imply p ∈ WI[E](w
);
(5) p ∈ WI[E](w) and E 	p♦q and E 	q♦w imply p ∈ WI[E](q.w).
All of the above properties follow easily from the definitions, so we omit their proofs.
5 SOURCE-DPOR
Having established the concept of source sets, Algorithm 1 presents the source-DPOR algorithm.
As mentioned in the introduction, this algorithm is derived from the classical persistent-set-based
DPOR algorithm of Flanagan and Godefroid (2005b) by replacing persistent sets by source sets.
The modification only consists in a small change to a single test in the algorithm. Recall that, as
mentioned at the end of Section 3.1, in this section we assume that processes cannot disable each
other; in Section 8, we show how the algorithm is extended to the case where process can disable
each other.
5.1 Algorithm
Source-DPOR uses the recursive procedure Explore(E, Sleep) to perform a depth-first search, where
E can be interpreted as the stack of the search, that is, the past execution sequence explored so far
and Sleep is a sleep set, that is, a set of processes that (provably) need not be explored from s[E]. For
each prefix E ≤ E, the algorithm maintains a set backtrack(E
) of processes that will eventually
be explored from E
, allowing additions to any backtrack set of a state in the stack to be made by
recursive calls to Explore.
Explore(E, Sleep) initializes backtrack(E) to consist of an arbitrary enabled process that is not
in Sleep (line 3). Thereafter, for each process p in backtrack(E) that is not in Sleep, the algorithm
performs two phases: race detection (lines 5–9) and state exploration (lines 10–12).
In the race detection phase, the algorithm first finds the events e ∈ dom(E) that are in a
reversible race with the next step of p (line 5). For each such event e ∈ dom(E), the algorithm must
explore execution sequences in which the race is reversed. Such sequences are obtained by leaving
unchanged that part of E, which follows E and depends neither on e nor on next[E](p), and thereafter performing the event next[E](p) instead of e. The unchanged part is obtained as notdep(e, E).
Note that some events in notdep(e, E) may happen-before next[E](p), and it is then necessary to
Journal of the ACM, Vol. 64, No. 4, Article 25. Publication date: August 2017. 
Source Sets: A Foundation for Optimal Dynamic Partial Order Reduction 25:15
ALGORITHM 1: Source-DPOR algorithm.
// In all algorithms, we use “:=” notation for data shared among recursive calls and “let” for local variables.
Initial call: Explore(, ∅)
1 Explore(E, Sleep)
2 if ∃r ∈ (enabled(s[E]) \ Sleep) then // If not at a maximal or “sleep set blocked” execution...
3 backtrack(E) := {r}; // ... initialize backtrackwith arbitrary r
4 while ∃p ∈ (backtrack(E) \ Sleep) do // Pick an unexplored p
5 foreach e ∈ dom(E) such that(e E.p next[E](p)) do // e and next[E](p) race
6 let E = pre(E, e);
7 let v = notdep(e, E).p; // Find events independent with e
8 if I[E](v) ∩ backtrack(E
) = ∅ then // Ensure an initial exists in backtrack or...
9 add some q ∈ I[E](v) to backtrack(E
); // ... add an initial
10 let Sleep = {q ∈ Sleep | E 	p♦q}; // Compute next sleep set
11 Explore(E.p, Sleep
); // Recursively call Explore
12 add p to Sleep; // Mark p as explored
perform them before next[E](p) can be executed. To this, we add p to ensure that next[E](p) is
executed instead of e. Let v be notdep(e, E).p. By construction, any execution sequence that starts
with E
.v is guaranteed to be inequivalent to any execution sequence that starts with E. To ensure
that an execution sequence of form E
.v will be explored, the algorithm checks (at line 8) whether
some process in I[E](v) is already in backtrack(E
). If not, then a process in I[E](v) is added to
backtrack(E
). This ensures that a sequence equivalent to E
.v has been or will be explored.
The exploration (phase) is started recursively from E.p, using an appropriately initialized sleep
set. According to rule (ii) in the explanation of sleep sets in Section 2, the sleep set for the exploration of E.p should be initialized to be the set of processes currently in the sleep set of E that are
independent with p after E (i.e., Sleep = {q ∈ Sleep | E 	q♦p}).
On Source Sets and Persistent Sets. The mechanism by which source-DPOR produces source sets
rather than persistent sets is the test at line 8. In DPOR algorithms based on persistent sets, such
as those of Flanagan and Godefroid (2005b), Lauterburg et al. (2010), Tasharofi et al. (2012), Sen
and Agha (2006), and Saarikivi et al. (2012), this test must be stronger and at least guarantee that
backtrack(E
) contains a process q such that q performs some event in v that “happens-before”
next[E](p) in E.p. Thus, if the test at line 8 and the addition at line 9 are strengthened into
if e ∈ dom[E](v).

e
→E.p next[E](p) ∧ e
 ∈ (I[E](v) ∩ backtrack(E
))
then
add e
 for some e ∈ dom[E](v) with
e
→E.p next[E](p) ∧ e
 ∈ I[E](v)

to backtrack(E
),
then the algorithm ensures that backtrack(E
) will be a persistent set when Explore(E, Sleep) returns. These lines guarantee that the first event in v that is dependent with some process in
backtrack(E
) is performed by some process in backtrack(E
), thus making backtrack(E
) a persistent set. In contrast, our test at line 8 does not require the added process to perform an event
that “happens-before” next[E](p). Consider, for instance, that v is just the sequence q.p, where q is
independent with p after E
. Then, since the event of q does not “happen-before” the event of p,
there is an execution sequence E
.p.q in which p is dependent with the process e in backtrack(E
)
but need not be in backtrack(E
). On the other hand, since q ∈ I[E](p.q), the set backtrack(E
)
(together with the initial sleep set at E
) is still a source set for the possible continuations after E
.
Journal of the ACM, Vol. 64, No. 4, Article 25. Publication date: August 2017.       
25:16 P. A. Abdulla et al.
Fig. 5. Structure of correctness proof for Algorithm 1.
5.2 Correctness
In this section, we prove that Algorithm 1 is correct in the sense that for each execution sequence E,
it explores an execution sequence in [E.v] for some v. In particular, if E is maximal, then Algorithm 1 explores an execution sequence in [E]. We formalize this statement in Theorem 5.2.
In Figure 5, we give an overview of the structure of its proof.
We first prove some auxiliary properties of initials and weak initials (Lemma 5.1). We then state
Lemma 5.3 (a statement about each call to Explore, from which Theorem 5.2 can be derived) and
prove it by induction on the order in which states (i.e., execution sequences) are backtracked by
the algorithm. A key idea, stated as Claim 5.4, is that when Explore(E, Sleep) returns, the set Sleep
(the final sleep set) will be a source set for W after E, where W is the set of suffixes w such that
E.w is an execution sequence. To prove Claim 5.4 by contradiction, we assume that there is some
w ∈ W that does not have a weak initial in the final sleep set. Then there is a sequence of form
E.q.wR, which (as proven in Claim 5.5) does not have any weak initial in the final sleep set. Now,
by the induction hypothesis (which is applicable by IH-condition 5.6), the algorithm will actually
explore a sequence equivalent to E.q.wR, in which it will detect a particular race. However, the
algorithm guarantees (lines 8 and 9) that the backtrack set at E contains a process that contradicts
the initial assumption. Having shown that the final sleep set is a source set for W after E, we can
prove Lemma 5.3 by induction (using IH-condition 5.7), thereby also establishing the converse
of Theorem 4.4.
We begin by establishing some properties that will be used in the proof:
Lemma 5.1. The following properties hold:
(1) If E 	 w, E 	 w and E.w  E.w
, then
(a) I[E](w) = I[E](w
)
(b) WI[E](w) = WI[E](w
)
(2) If E 	 w.p, then
(a) I[E](w) ⊆ I[E](w.p)
(b) WI[E](w.p) ⊆ WI[E](w)
(3) E 	q♦w → I[E.q](w) = I[E](w)
Proof. We prove each of the cases, in order:
1(a). If p ∈ I[E](w), then by Definition 4.1(1) there exists w such that E.p.w  E.w. Since
E.w  E.w
, we then have E.p.w  E.w
, which by the same definition entails p ∈
I[E](w
).
1(b). If p ∈ WI[E](w), then by Definition 4.1(2) there exists w and v such that E.p.w 
E.w.v. Since E.w  E.w
, by Property (6) of Definition 3.2, we also have E.w.v  E.w
.v
and transitively E.p.w  E.w
.v, which by Definition 4.1(2) entails p ∈ WI[E](w
).
Journal of the ACM, Vol. 64, No. 4, Article 25. Publication date: August 2017.
Source Sets: A Foundation for Optimal Dynamic Partial Order Reduction 25:17
2(a). Trivial, from Property (3) of Definition 3.2, since E.w is a prefix of E.w.p, and therefore
any event not having others happening before it in w maintains this status.
2(b). If p ∈ w, then p ∈ WI[E](w.p) implies p ∈ I[E](w), which implies p ∈ WI[E](w). If p
w, then if p = p, p ∈ WI[E](w.p) implies E 	p♦w and hence p ∈ WI[E](w); otherwise, p ∈
WI[E](w.p) implies E 	p
♦w.p, which also implies E 	p
♦w and therefore p ∈ WI[E](w).
3. From Property (4) of Definition 3.2 and the definition of E 	q♦w, it follows immediately
that
E 	q♦w → E.q.w  E.w.q. (1)
Also, from Definition 4.1(1),
p ∈ I[E.q](w) ⇔ E.q.w  E.q.p.w (for some w
), (2)
p ∈ I[E](w) ⇔ E.w  E.p.w (for some w). (3)
We will assume that p is in one set and show that p is also in the other:
(i) (Forward direction) Assume p ∈ I[E.q](w). From Equation (2), the events in
dom[E.q](p.w
) are the same as in dom[E.q](w) and they have the same (non-) dependency with next[E](q), because E 	q♦w.
Therefore,
E 	q♦(p.w
) → E.q.p.w  E.p.w
.q similar to (1). (4)
Then
p ∈ I[E.q](w) → E.q.w  E.q.p.w by (2)
→ E.w.q  E.p.w
.q by (1) and (4)
→ E.w  E.p.w by Property (6) of Def. 3.2
→ p ∈ I[E](w) by (3).
(ii) (Reverse direction) Assume p ∈ I[E](w). From E 	q♦w, it follows that E.w.q is an
execution sequence, therefore:
p ∈ I[E](w) → E.w  E.p.w by (3)
→ E.w.q  E.p.w.q by Property (6) of Def. 3.2. (5)
The events in dom[E.q](p.w) are the same as dom[E.q](w), and they have the
same (non-)dependency with next[E](q), because E 	q♦w. Therefore
E 	q♦(p.w) → E.q.p.w  E.p.w.q similar to (1). (6)
Continuing from (5),
(5) = E.w.q  E.p.w.q
→ E.q.w  E.q.p.w by (1) and (5)
→ p ∈ I[E.q](w) by (2).
This concludes the proof of all cases of the lemma.
Theorem 5.2 (Correctness of Source-DPOR). For all execution sequences E, Algorithm 1 explores some execution sequence E
, which is in [E.v] for some v. In particular, for all maximal execution sequences E, Algorithm 1 explores some execution sequence E
, which is in [E].
Journal of the ACM, Vol. 64, No. 4, Article 25. Publication date: August 2017.  
25:18 P. A. Abdulla et al.
The proof is given at the end of this section, after establishing a series of intermediate lemmas.
We begin by introducing some notation. Let:
• init_sleep(E) be the value of Sleep when calling Explore(E, Sleep),
• final_sleep(E) be the value of Sleep when Explore(E, Sleep) returns,
• done(E) be final_sleep(E) \ init_sleep(E), that is, the set of processes that are explored from
s[E] (added in Sleep by line 12),
• to_be_explored(E) be the set {w | E 	 w ∧ (I[E](w) ∩ init_sleep(E) = ∅)} of possible continuations w of E such that I[E](w) ∩ init_sleep(E) = ∅; intuitively, this is the set of sequences
that should be explored by the call Explore(E, Sleep),
• e E p denote that p is not blocked after E and that e E.p next[E](p).
We are now ready to state the lemma that will eventually be used to prove correctness.
Lemma 5.3. Whenever a call to Explore(E, Sleep) returns during Algorithm 1, then for all sequences w ∈ to_be_explored(E), the algorithm has explored some execution sequence E.w which is
in [E.w.v] for some v.
Proof. The proof is by induction on the order in which states (i.e., execution sequences) are
backtracked by the algorithm. The first time Explore returns, it has just explored a maximal sequence, so the base case holds trivially. Consider a sequence E. As inductive hypothesis, we assume that the statement in the lemma holds for all execution sequences E.p with p ∈ done(E).
Whenever we invoke the induction hypothesis, we need to establish a condition of form w ∈
to_be_explored(E.p) for some process p and sequence w
. We will refer to this as the IH-condition
and try to separate its establishment from the rest of the flow in the proof. We structure the proof
of the lemma as a sequence of claims.
Claim 5.4. done(E) is a source set for to_be_explored(E) after E.
By the definition of source sets (Definition 4.3), this claim states that for each w ∈
to_be_explored(E), there is some p ∈ done(E) such that p ∈ WI[E](w). Since, by the definition of
to_be_explored(E), the set of sequences w with E 	 w is the union of to_be_explored(E) and the
set {w | E 	 w ∧ init_sleep(E)  ∅}, it implies that the set final_sleep(E) is a source set for
{w | E 	 w}.
Proof of Claim 5.4. By contradiction. Assume a w ∈ to_be_explored(E), such that for all p ∈
done(E) we have p  WI[E](w). We will prove that such a sequence cannot exist.
If done(E) is empty, then no process was explored from that state: This can only happen if the
check on line 2 has failed (otherwise process p on that line will later be added by line 12), which
contradicts that to_be_explored(E) is non-empty. Therefore, done(E) is not empty.
For each process p ∈ done(E), let wp be the longest prefix of w such that E 	p♦wp , and let ep be
the first event in dom[E](w) which is not in wp . We have
p  ep (otherwise from E 	p♦wp we derive p ∈ WI[E](w)). (7)
Let q be such that wq is a longest prefix among the prefixes wp for p ∈ done(E). If there are several
processes p such that wp is the same longest prefix, then let q be the process among these which
is explored first from s[E]. Finally, let wR be wq .eq (Figure 6).
Given these definitions, all of the following are valid execution sequences:
E 	 w → E 	 wq .eq
→ E.wq 	 eq (8)
Journal of the ACM, Vol. 64, No. 4, Article 25. Publication date: August 2017.         
Source Sets: A Foundation for Optimal Dynamic Partial Order Reduction 25:19
Fig. 6. Illustrating E,w, wp , ep , wq, eq, and wR.
as well as the following:
E 	q♦wq → E.wq 	 q
→ E.wq 	 q.eq by (7), (8) and Assumption 3.1
→ E 	 q.wq .eq by E 	q♦wq
→ E 	 q.wR by the definition of wR . (9)
We also note that
p  WI[E](wR ) for all p ∈ done(E) by the construction of wR . (10)
As the next step in the proof of Claim 5.4, we establish the following claim:
Claim 5.5. For all p ∈ done(E) with p  q we have p  WI[E](q.wR ).
Proof of Claim 5.5. We distinguish among the following cases:
1. If ep  eq, then wp is a strict prefix of wq. Therefore:
ep ∈ wq → p  WI[E](wq ) by E p♦wq
→ p  WI[E](wq .q) by 5.1.2b
→ p  WI[E](q.wq ) by E.q.wq  E.wq .q, 5.1.1b
→ p  WI[E](q.wR ) by 5.1.2b.
2a. If ep = eq and E p♦q, then p  WI[E](q.wR ) follows directly.
2b. If ep = eq, E 	p♦q, and E p♦q.wq, then p  WI[E](q.wR ) follows directly.
2c. If ep = eq, E 	p♦q, and E 	p♦q.wq, then:
E 	p♦q.wq → E 	p♦wq .q by E 	q♦wq
→ E.wq 	p♦q (11)
ep = eq → E.wq p♦eq
→ E.wq p♦q.eq by (11) and Property (7) of Def. 3.2
→ E p♦wq .q.eq
→ E p♦q.wq .eq by E 	q♦wq
→ p  WI[E](q.wR ).
This concludes the proof of Claim 5.5.
We will next invoke the inductive hypothesis for E.q and wR. Let us first establish the IHcondition:
IH-condition 5.6. wR ∈ to_be_explored(E.q).
Journal of the ACM, Vol. 64, No. 4, Article 25. Publication date: August 2017.                         
25:20 P. A. Abdulla et al.
Proof of IH-condition 5.6. That E.q 	 wR was established in (9). We must prove that
I[E.q](wR ) ∩ init_sleep(E.q) = ∅. By the handling of sleep sets, init_sleep(E.q) may extend
init_sleep(E) by some of the processes in done(E). From the assumptions for w we have that for all
p ∈ done(E):
p  WI[E](w) → p  I[E](w) by Lemma 4.2(1)
→ p  I[E](wq ) by 5.1.2a
→ p  I[E.q](wq ) by E 	q♦wq, 5.1.3. (12)
Any other process in init_sleep(E.q) was also in init_sleep(E) (passed along by line 10). Again, from
the assumptions for w, we have that for all p ∈ init_sleep(E),
I[E](w) ∩ init_sleep(E) = ∅ → p  I[E](w)
→ p  I[E.q](wq ) (same reasoning). (13)
Since in (12) and (13) we considered I[E.q](wq ), the only process that can be in I[E.q](wR ) ∩
init_sleep(E.q) is eq. We have two cases.
• If there is no event e in wq such that e→E.wR eq, then by the construction of eq, we have
E q♦eq, implying eq  init_sleep(E.q) (as it is filtered by line 10).
• Otherwise, let e be the last event in wq such that e→E.wR eq. Let w
q be the prefix of wq
that precedes e, and let w
q be the suffix of wq that follows e, that is, wq = w
q .e.w
q . By
construction, we have E.w
q e♦eq and E.w
q 	e♦q. By Property (7) of Definition 3.2, we
infer E.w
q e♦(q.eq ), which implies e→E.wq .q.eq eq, which implies e→E.q.wq .eq eq. Hence
e→E.q.wR eq, and so eq  I[E.q](wR ).
This concludes the establishment of IH-condition 5.6.
Using the inductive hypothesis for E.q, the algorithm explores some sequence of the form E.q.w
such that
E.q.w  E.q.wR .v for some v
. (14)
Note that in w
, the event eq exists, but need not be the last occurring of the events ep for
p ∈ done(E), since such independent events may have been reordered from wR .v to w
.
Let w
q be the prefix of w up to, but not including, eq. By the construction of wq, we have
next[E](q) E.q.wq eq. From (14), it also follows that next[E](q) E.q.w
q eq (this is the same race
as the race next[E](q) E.q.wq eq, that is, between next[E](q) and eq over the sequence wq).
Since the sequence E.q.w is one actually explored by the algorithm, line 5 will detect the race
next[E](q) E.q.w
q eq. At that point,
• p in the algorithm will correspond to eq,
• E in the algorithm will correspond to E, and
• e in the algorithm will correspond to next[E](q).
The algorithm will extract (lines 6 and 7) the sequence v = notdep(next[E](q), E.q.w
q ).eq from
E.q.w
q .eq by removing events that have next[E](q) happening-before them and identify its initials
Journal of the ACM, Vol. 64, No. 4, Article 25. Publication date: August 2017.                             
Source Sets: A Foundation for Optimal Dynamic Partial Order Reduction 25:21
I[E](v). Below let us use ∗
→ to denote multiple steps. For any such q ∈ I[E](v) it follows that
q ∈ I[E](v) → q ∈ I[E](q.w
q .eq ) by Lemma 4.2(1) and definition of notdep
∗
→ q ∈ I[E](q.w
) by Lemma 5.1.2a
→ q ∈ I[E](q.wR .v
) by (14), Lemma 5.1.1a
→ q ∈ WI[E](q.wR ) by Lemma 4.2(2). (15)
The algorithm then guarantees (in lines 8 and 9) that there is one such q ∈ I[E](v) that is added
to backtrack(E). Additionally, by (15), we have that no other event happens-before q in E.q.w
.
Therefore q  init_sleep(E); otherwise, it would have stayed in the sleep set (transferred by line 10)
and could not have been chosen for execution.
Together, q ∈ backtrack(E) and q  init_sleep(E) imply that q ∈ done(E), but this contradicts
Claim 5.5. Such a sequencew can therefore not exist, and this concludes the proof of Claim 5.4.
We now move on to the proof of Lemma 5.3. By Claim 5.4, for any E.w, we have p ∈ WI[E](w)
for some p ∈ done(E). Let q be the process among these that is explored first from s[E]. We intend
to apply the inductive hypothesis for E.q and w \ q:
IH-condition 5.7. (w \ q) ∈ to_be_explored(E.q).
Proof of IH-condition 5.7 We must establish two properties: (i) that E.q 	 w \ q and
(ii) that I[E.q](w \ q) ∩ init_sleep(E.q) = ∅. We infer E.q 	 w \ q from E 	 w and q ∈ WI[E](w).
For the second property, I[E.q](w \ q) may extend I[E](w) only by processes q ∈ w such that
next[E](q)→E.w q
, j, where q
, jis the first event of q inw. However, any such process q cannot
be in init_sleep(E.q), since in this case E q♦q so q is removed from the sleep set when exploring
E.q. Given also that init_sleep(E.q) \ init_sleep(E) does not contain any process in I[E](w) (recall
that q was the first process in I[E](w) to be explored), we infer IH-condition 5.7.
The conclusion from the inductive hypothesis is that the algorithm explores some sequence
E.q.wˆ that is in [E.q.(w \ q).v] for some v. We have two cases:
• q ∈ w. From q ∈ WI[E](w), we infer E.q.(w \ q).v  E.w.v, which establishes the lemma.
• q  w, that is, (w \ q) = w. From E 	q♦w, we infer E.q.wˆ  E.q.w.v  E.w.q.v.
This concludes the proof of Lemma 5.3.
Using Lemma 5.3, we can finally conclude the proof of the main theorem.
Proof of Theorem 5.2. Since Algorithm 1 is initially called with Explore(, ∅), Lemma 5.3 implies correctness in the sense that for all execution sequences E the algorithm explores some execution sequence E that is in [E.v] for some v. In particular, if E is maximal, then E  E.
6 WAKEUP TREES
As we described earlier, source-DPOR may still lead to sleep-set-blocked explorations. We therefore
present an algorithm, called optimal-DPOR, which is provably optimal in that it always explores
exactly one interleaving per Mazurkiewicz trace and never encounters sleep set blocking. OptimalDPOR is obtained by combining source sets with a novel mechanism, called wakeup trees, which
control the initial steps of future explorations.
Wakeup trees can be motivated by looking at lines 6–9 of Algorithm 1. At these lines, it is
found that some execution sequence starting with E
.v should be performed to reverse a detected
race. However, at line 9, only a single process from the sequence v is entered into backtrack(E
),
thus “forgetting” information about how to reverse this race. Since the new exploration after E
.q
does not “remember” this sequence v, it may explore a completely different sequence, which could
Journal of the ACM, Vol. 64, No. 4, Article 25. Publication date: August 2017.         
25:22 P. A. Abdulla et al.
Fig. 7. A wakeup tree. The order of exploration ≺ is p.q, followed by p.r.r, p.r, and so on, until we finish
at .
potentially lead to sleep set blocking. To prevent such a situation, we replace the backtrack set by
a so-called wakeup tree. The wakeup tree contains initial fragments of the sequences that are to be
explored after E
. Each fragment guarantees that no sleep set blocking will be encountered during
the exploration.
6.1 Formal Definition
Let us define an ordered tree as a pair B, ≺, where B (the set of nodes) is a finite prefix-closed
set of sequences of processes, with the empty sequence  being the root. The children of a node
w, of form w.p for some set of processes p, are ordered by ≺. In B, ≺, such an ordering between
children has been extended to the total order ≺ on B by letting ≺ be the induced post-order relation
between the nodes in B. This means that if the children w.p1 and w.p2 are ordered as w.p1 ≺ w.p2,
thenw.p1 ≺ w.p2 ≺ w in the induced post-order. An example of the ordering of sequences is given
in Figure 7.
Definition 6.1 (Wakeup Tree). Let E be an execution sequence, and P be a set of processes. A
wakeup tree after E, P is an ordered tree B, ≺, such that the following properties hold:
(1) WI[E](w) ∩ P = ∅ whenever w is a leaf of B;
(2) wheneveru.p andu.w are nodes in B withu.p ≺ u.w, andu.w is a leaf, thenp  WI[E.u](w).
Intuitively, a wakeup tree after E, P is intended to consist of initial fragments of sequences
that should be explored after E to avoid sleep set blocking, when P is the current sleep set at E. To
see this, note that if q ∈ P, then (by the way sleep sets are handled) q cannot be in I[E](w) for any
sequence w that is explored after E. If, however, E 	q♦w, then q is still in the sleep set at E.w and
may never be removed. To prevent this, we therefore require q  WI[E](w), which is the same as
Property 1, that is, WI[E](w) ∩ P = ∅. Property 2 implies that when a process p is added to the sleep
set at E.u, after exploring E.u.p, then by the same reasoning as above, it will have been removed
from the sleep set when we reach E.u.w.
The empty wakeup tree is the tree {}, ∅, which consists only of the root . We state a useful
property of wakeup trees.
Lemma 6.2. If B, ≺ is a wakeup tree after E, P and w,w ∈ B and w is a leaf that satisfies
w
∼[E]w, then w  w
.
The lemma states that any leafw is the smallest (w.r.t. ≺) node in the tree that is consistent with
w after E.
Proof. We prove the lemma by contradiction. Assume w ≺ w. Then by the definition of ordered trees there are u,p,v,v such that w = u.p.v and w = u.v such that u.p ≺ u.v. Since u.v
Journal of the ACM, Vol. 64, No. 4, Article 25. Publication date: August 2017.  
Source Sets: A Foundation for Optimal Dynamic Partial Order Reduction 25:23
is a leaf, we have p  WI[E.u](v) by Property 2 of Definition 6.1. Hence p[E.u]v, which implies
u.p[E]u.v, which implies u.p.v
[E]u.v, that is, w
[E]w.
For a wakeup tree B, ≺ and a process p ∈ B, define subtree(B, ≺,p) to denote the subtree of
B, ≺ rooted at p, that is, subtree(B, ≺,p) = B
, ≺
, where B = {w | p.w ∈ B} and ≺ is the
restriction of ≺ to B
.
6.2 Inserting Sequences in a Wakeup Tree
Let B, ≺ be a wakeup tree after E, P. For any sequencew such that E.w is an execution sequence
with WI[E](w) ∩ P = ∅, we need an operation insert[E](w,B, ≺) that satisfies the following properties:
(1) insert[E](w,B, ≺) is also a wakeup tree after E, P,
(2) any leaf of B, ≺ remains a leaf of insert[E](w,B, ≺), and
(3) insert[E](w,B, ≺) contains a leaf u with u∼[E]w.
A simple implementation of insert[E](w,B, ≺) is the following: Let v be the smallest (w.r.t.
to ≺) sequence in B such that v∼[E]w. If v is a leaf, then the result of insert[E](w,B, ≺) can be
taken as B, ≺ (leaving the tree unmodified). Otherwise, let w be a shortest sequence such that
w [E] v.w
, and add v.w as a new leaf, which is ordered after all already existing nodes in B of
form v.w.
As an illustration, using the program of Figure 1, assume that a wakeup tree B, ≺ after , ∅
contains p as the only leaf. Then the operation insert[](q.q,B, ≺) adds q.q as a new leaf with
p ≺ q.q. If we thereafter perform insert[](r.r,B, ≺), then the wakeup tree remains the same,
since q.q∼[]r.r, and q.q is already a leaf.
7 OPTIMAL-DPOR
In this section, we present a DPOR algorithm that achieves optimal reduction.
7.1 Algorithm
The optimal-DPOR algorithm, shown as Algorithm 2, performs a depth-first search using the recursive procedure Explore(E, Sleep, WuT), where E and Sleep are as in Algorithm 1, and WuT is a
wakeup tree after E, Sleep, containing extensions of E that are guaranteed to be explored (in order) by Explore(E, Sleep, WuT). If WuT is empty, then Explore(E, Sleep, WuT) is free to explore any
extension of E.
Like Algorithm 1, the algorithm runs in two phases: race detection (lines 2–7) and state exploration (lines 8–21), but it is slightly differently organized. Instead of analyzing races at every
invocation of Explore, races are analyzed in the entire execution sequence only when a maximal
execution sequence has been generated. The reason for this is that the test at line 6 is precise only
when the used sequence v, which is defined at line 5, includes all events in the entire execution
that do not “happen after” e, as well as those that occur after e
. Therefore v can be defined only
when E is a maximal execution sequence.
In the race detection phase, Algorithm 2 must be able to access the current sleep set for each
prefix E of the currently explored execution sequence E. For each such prefix E
, the algorithm
therefore maintains a set of processessleep(E
), which is the current sleep set at E
. In a similar way,
for each prefix E of E, the algorithm maintains wut(E
), which is the current wakeup tree at E
.
Let us now explain the race detection phase, which is entered whenever the exploration reaches
the end of a complete sequence (i.e., enabled(s[E]) = ∅). In this phase, the algorithm investigates all
races that can be reversed in the just explored sequence E. Such a race consists of two events e and
Journal of the ACM, Vol. 64, No. 4, Article 25. Publication date: August 2017.  
25:24 P. A. Abdulla et al.
ALGORITHM 2: Optimal-DPOR algorithm.
Initial call: Explore(, ∅,{}, ∅)
1 Explore(E, Sleep, WuT)
2 if enabled(s[E]) = ∅ then // Race detection only at maximal execution sequences
3 foreach e, e  ∈ dom(E) such that (e E e 
) do // For each racing pair e, e
4 let E = pre(E, e); // Goto state before e
5 let v = notdep(e, E).e
; // Find events independent with e
6 if sleep(E
) ∩ WI[E](v) = ∅ then // Has an equivalent already been explored?
7 wut(E
) := insert[E](v,wut(E
)); // If not, insert into the wakeup tree
8 else // If not at a maximal execution sequence, explore...
9 if WuT  {}, ∅ then
10 wut(E) := WuT; // ... either using an existing wakeup tree
11 else
12 choose p ∈ enabled(s[E]); // ... or by selecting an arbitrary p...
13 wut(E) := {,p}, {(p,)}; // ... and making a wakeup tree from it
14 sleep(E) := Sleep;
15 while ∃p ∈ wut(E) do // While the wakeup tree is not empty...
16 let p = min≺{p ∈ wut(E)}; // ... pick next branch, ...
17 let Sleep = {q ∈ sleep(E) | E 	p♦q}; // ... compute next sleep set...
18 let WuT = subtree(wut(E),p); // ... and wakeup tree (a subtree of the current),...
19 Explore(E.p, Sleep
, WuT
); // ... and do a recursive call to Explore
20 remove all sequences of form p.w from wut(E); // When done, cleanup...
21 add p to sleep(E); // ... and mark p as explored
e in E, such that e E e
. Let E = pre(E, e) and let v = notdep(e, E).e

, that is, the sub-sequence
of E consisting of the events that occur after e but do not “happen after” e, followed by e
 (this
notation is introduced at lines 4 and 5). The reversible race e E e indicates that there is another
execution sequence, which performsv after E and in which the race is reversed, that is, the event e
happens before the event e. Since E
.v is incompatible with the currently explored computation,
the algorithm must now make sure that it will be explored if it was not explored previously. If
some p ∈ sleep(E
) is in WI[E](v), then some execution equivalent to one starting with E
.v will
have been explored previously. If not, then we perform the operation insert[E](v,wut(E
)) to make
sure that some execution equivalent to one starting with E
.v will be explored in the future.
In the exploration phase, which is entered if exploration has not reached the end of a maximal
execution sequence, first the wakeup tree wut(E) is initialized to WuT. If WuT is empty, then (as
we will state in Lemma 7.1) the sleep set is empty, and an arbitrary enabled process is entered
into wut(E). The sleep set sleep(E) is initialized to the sleep set that is passed as argument in this
call to Explore. Thereafter, each sequence in wut(E) is subject to exploration. We find the first (i.e.,
minimal) single-process branch p in wut(E) and call Explore recursively for the sequence E.p. In
this call, the associated sleep set Sleep is obtained from sleep(E) in the same way as in Algorithm 1.
The associated wakeup tree WuT is obtained as the corresponding subtree of wut(E). Thereafter,
Explore is called recursively for the sequence E.p with the modified sleep set Sleep and wakeup
tree WuT
. After Explore(E.p, Sleep
, WuT
) has returned, the sleep set sleep(E) is extended with p,
and all sequences beginning with p are removed from wut(E).
Journal of the ACM, Vol. 64, No. 4, Article 25. Publication date: August 2017.       
Source Sets: A Foundation for Optimal Dynamic Partial Order Reduction 25:25
7.2 Correctness
Let us now prove the correctness of the optimal-DPOR algorithm. Throughout, we assume a particular completed execution of optimal-DPOR. This execution consists of a number of terminated
calls to Explore(E, Sleep, WuT) for some values of the parameters E, Sleep, and WuT. Let E denote
the set of execution sequences E that have been explored in some call Explore(E, ·, ·). Define the
ordering ∝ on E by letting E ∝ E if Explore(E, ·, ·) returned before Explore(E
, ·, ·). Intuitively, if
one were to draw an ordered tree that shows how the exploration has proceeded, then E would
be the set of nodes in the tree, and ∝ would be the post-order between nodes in that tree.
For an arbitrary execution sequence E ∈ E let
• final_sleep(E) denote the value of sleep(E) at the point when Explore(E, Sleep, WuT)
returns;
• done(E) denote final_sleep(E) \ Sleep, that is, the set of processes that are explored froms[E].
We begin by establishing some useful invariants.
Lemma 7.1. Whenever Algorithm 2 is inside a call Explore(E, Sleep, WuT), then the following two
invariants hold:
(1) wut(E) is a wakeup tree after E,sleep(E),
(2) if WuT is empty, then Sleep is empty.
Proof. We establish these two invariants jointly by induction over the steps of the algorithm.
The properties hold at the beginning of the initial call, since sleep(E) and wut(E) are both empty.
We verify that each property is preserved by the steps of the algorithm.
(1) We need to consider the following cases.
• Whenever wut(E) is updated at line 7, it follows by Property 1 of the operation
insert[E](v,wut(E)) that wut(E) remains a wakeup tree after E,sleep(E).
• Consider a new call to Explore(E.p, Sleep
, WuT 
) at line 19, which occurs inside a
call Explore(E, Sleep, WuT), and where Sleep and WuT  are obtained from the current values of sleep(E) and wut(E) by Sleep = {q ∈ sleep(E) | E 	p♦q} and WuT  =
subtree(wut(E),p), at lines 17 and 18. Since the parameters Sleep and WuT  will be used
to initialize sleep(E.p) (at line 14) and wut(E.p) (at line 10), we must check that WuT  is a
wakeup tree after E.p, Sleep
. By the inductive hypothesis, wut(E) is a wakeup tree after E,sleep(E). We establish that WuT  is a wakeup tree after E.p, Sleep
 by checking
the two properties of Definition 6.1. Let WuT  = B
, ≺
 and let wut(E) = B, ≺.
(a) Let w be a leaf of B
; then p.w is a leaf of B. Let q ∈ Sleep
. By the update at line
17, we have q ∈ sleep(E) and E 	p♦q. By the inductive hypothesis q  WI[E](p.w),
which using E 	p♦q implies q  WI[E.p](w). Hence WI[E.p](w) ∩ Sleep = ∅ whenever w is a leaf of B
.
(b) Let u.q and u.w be nodes in B with u.q ≺ u.w, and u.w is a leaf. Then p.u.q and
p.u.w are nodes in B with p.u.q ≺ p.u.w, and p.u.w a leaf. By the inductive hypothesis, it follows that q  WI[E.p.u](w), which is precisely what we must establish for
the inductive step.
If wut(E.p) is initialized at line 13 instead of at line 10, then this initialization is performed in a call of form Explore(E.p, Sleep
, WuT 
), where WuT  is the empty wakeup
tree. By Invariant 2 of the inductive hypothesis, Sleep is empty. Since {,p}, {(p,)}
is trivially a wakeup tree after E.p, ∅, the property follows.
• Consider the update to sleep(E) and wut(E) at lines 20 and 21. We must prove that they
preserve the two properties of Definition 6.1.
Journal of the ACM, Vol. 64, No. 4, Article 25. Publication date: August 2017.   
25:26 P. A. Abdulla et al.
(a) Since the update removes nodes from wut(E), and p is added to sleep(E), it suffices
to check that p  WI[E](w) whenever w is a leaf of wut(E) that remains after the
operation, that is,w is not of form p.w
. Before the operation, it was the case that p
and w were nodes of wut(E) with p ≺ w and w a leaf. From the inductive hypothesis, Property 2 of Definition 6.1 implies that p  WI[E](w), which completes the
proof for this case.
(b) Since wut(E) is modified by removing a branch from the root, this property is
preserved.
(2) Consider a new call to Explore(E.p, Sleep
, WuT 
) at line 19. If WuT  is empty, then p must
have been a leaf in wut(E) just before the call. By Property 1 of the inductive hypothesis,
wut(E) was then a wakeup tree after E,sleep(E); therefore (by Property 1 of Definition 6.1), we have E p♦q for all q ∈ sleep(E), which by construction of Sleep at line 17
implies that Sleep is empty.
Lemma 7.2. If E.p ∝ E.w, then p  I[E](w).
Proof. We use a proof by contradiction. Assume that E.p ∝ E.w but p ∈ I[E](w). The property p ∈ I[E](w) implies that w is of form w
.p.w with E 	p♦w
. The property E.p ∝ E.w implies
that the sequence E.w is explored after the call to Explore(E.p, ·, ·) has returned. After the call to
Explore(E.p, ·, ·) has returned, we have that p ∈ sleep(E) by the insertion at line 21. By the way
sleep sets are updated at line 17, and the assumption E 	p♦w
, we conclude that p remains in the
sleep set at the calls Explore(E.w, ·, ·) whenever w is a prefix of w
. Thus p ∈ sleep(E.w
) just
before the call Explore(E.w
.p, ·, ·) is performed. By Invariant 1 of Lemma 7.1, we then have that
wut(E.w
) is a wakeup tree after E.w
,sleep(E.w
). To perform the call Explore(E.w
.p, ·, ·), it
must be the case that p is a node in wut(E.w
). But this contradicts Property (1) of Definition 6.1,
which says that p  WI[E.w](p.v) where v is chosen so p.v is a leaf of wut(E.w
). Thus, the lemma
is proven.
The following lemma captures the relationship between wakeup trees and E, ∝.
Lemma 7.3. Let E, ∝ be the tree of explored execution sequences. Consider some point in the
execution, and the wakeup tree wut(E) at that point, for some E ∈ E.
(1) If w ∈ wut(E) for some w, then E.w ∈ E.
(2) If w ≺ w for w,w ∈ wut(E), then E.w ∝ E.w
.
Proof. The properties follow by noting how the exploration from any E ∈ E is controlled by
the wakeup tree wut(E) at lines 15–21.
We can now prove that Algorithm 2 is correct in the sense that for each maximal execution
sequence E, it explores an execution sequence in [E]. This is formalized in Theorem 7.4. Similarly
to Lemma 5.3, Theorem 7.4 is proven by induction on the order in which states (i.e., execution sequences) are backtracked by the algorithm. The inductive step is proven by contradition, assuming
that some maximal sequence E.w is unexplored after the call for E returns. The proof of this inductive step is structured as a sequence of claims, structured in the way shown in Figure 8. First
it is shown that the assumption implies Claim 7.5, which is analogous to Claim 5.4 in the proof of
Lemma 5.3 and essentially states that w is a counterexample to the statement that final_sleep(E) is
a source set for the set of sequencesw with E 	 w. Thereafter, the sequence of Claims 7.6–7.9 establish that the algorithm must have explored some sequence that exposes a race, which by Claim 7.10
causes the algorithm to include a leaf with properties that contradict the initial assumption in the
inductive step, thereby concluding the proof of the theorem.
Journal of the ACM, Vol. 64, No. 4, Article 25. Publication date: August 2017.        
Source Sets: A Foundation for Optimal Dynamic Partial Order Reduction 25:27
Fig. 8. Structure of correctness proof for Algorithm 2.
Theorem 7.4 (Correctness of Optimal-DPOR). Whenever a call to Explore(E, Sleep, WuT) returns during Algorithm 2, then for all maximal execution sequences E.w, the algorithm has explored
some execution sequence E that is in [E.w].
Since the initial call to the algorithm uses the arguments Explore(, ∅,{}, ∅), Theorem 7.4 implies that for all maximal execution sequences E the algorithm explores some execution sequence
E that is in [E]. Since any execution sequence can be extended to a maximal one, the theorem
also implies the analog of Theorem 5.2, namely that for all execution sequences E, Algorithm 2
explores some execution sequence E that is in [E.v] for some v.
Proof. By induction on the set of execution sequences E that are explored during the considered execution, using the ordering ∝ (i.e., the order in which the corresponding calls to Explore
returned).
Base Case: This case corresponds to the first sequence E for which the call Explore(E, ·, ·) returns.
By the algorithm, E is already maximal, so the theorem trivially holds.
Inductive Step: Let us assume that there exist values E, Sleep and WuT, such that when the call to
Explore(E, Sleep, WuT) returns, there is a maximal sequence E.w such that the algorithm has not
explored any execution sequence E in [E.w]. We will show that this leads to a contradiction.
For such w to exist, E cannot be maximal, so final_sleep(E) contains at least one process. For
p ∈ final_sleep(E), define
• E
p , such that E
p ≤ E, E
p .p ∈ E, and E
p .p is the last execution sequence of this form that
precedes E (w.r.t. ∝). If p ∈ done(E), then E
p = E, but if p ∈ Sleep, then E
p is a strict prefix
of E.
• w
p , as E = E
p .w
p . It follows that E
p 	p♦w
p .
Inductive Hypothesis: The theorem holds for all execution sequences E with E ∝ E.
Claim 7.5. WI[E](w) ∩ final_sleep(E) = ∅.
Using the terminology of source sets, this claim states that w is a counterexample to the statement that final_sleep(E) is a source set for the set of sequences w with E 	 w.
Proof. By contradiction. Assume that there is a p ∈ WI[E](w) with p ∈ final_sleep(E). Since w
is maximal, p ∈ WI[E](w) implies p ∈ I[E](w), therefore from Definition 4.1(1) there is a w such
that E.w  E.p.w  E
p .w
p .p.w  E
p .p.w
p .w. By the inductive hypothesis applied to E
p .p, the
algorithm has explored some execution sequence in [E
p .p.w
p .w] = [E.w], which contradicts
the main assumption about w.
Journal of the ACM, Vol. 64, No. 4, Article 25. Publication date: August 2017. 
25:28 P. A. Abdulla et al.
For p ∈ final_sleep(E), also define
• wp , as the longest prefix of w such that E 	p♦wp ,
• ep , as the first event in dom[E](w) that is not in wp . Such an event ep must exist, otherwise
wp = w, which implies E 	p♦w, which implies p ∈ WI[E](w), which contradicts Claim 7.5.
Finally, define
• q ∈ final_sleep(E), such that wq is a longest prefix among wp . If there are several processes
p ∈ final_sleep(E) such that wp is the same longest prefix, then pick q such that E
q .q is
minimal (w.r.t. ∝).
• wR, as wq .eq.
• Sleep as the initial sleep set argument in the exploration of E
q .q, that is, it started by calling
Explore(E
q .q, Sleep
, ·).
Claim 7.6. E
q 	 q.w
q .wR.
Proof. Since E
q 	 q (actually explored) and E
q 	q♦(w
q .wq ), it follows that E
q 	 w
q .wq .q. Given
also that E
q 	 w
q .wq .eq and q does not disable eq (since q and eq are different), we conclude that
E
q 	 w
q .wq .q.eq and hence E
q 	 q.w
q .wq .eq which can be written as E
q 	 q.w
q .wR.
Claim 7.7. WI[E
q .q](w
q .wR ) ∩ Sleep = ∅.
Proof. We will establish the stronger property WI[E
q .q](w
q .wq ) ∩ Sleep = ∅. Claim 7.6 has
shown that the execution sequence is valid. The proof is then by contradiction: Assume that some
process p is in WI[E
q .q](w
q .wq ) ∩ Sleep
.
By the construction of Sleep at line 17, the process p must be in sleep(E
q ) just before the
call to Explore(E
q .q, Sleep
, ·) and satisfy E
q 	p♦q. From p ∈ WI[E
q .q](w
q .wq ), E
q 	p♦q, and Property 4.7(5), it follows that p ∈ WI[E
q ](q.w
q .wq ), which, using E
q .q.w
q .wq  E
q .w
q .wq .q (which
follows from E
q 	q♦(w
q .wq )), implies p ∈ WI[E
q ](w
q .wq .q), which by Property 4.7(4) implies p ∈
WI[E
q ](w
q .wq ).
Since, again by Property 4.7(4), p ∈ WI[E
q ](w
q .wq ) also implies p ∈ WI[E
q ](w
q ), during exploration of E
q .w
q no event in w
q removes p from the sleep set. Thus, p  w
q and p will end up in
sleep(E
q .w
q ) and from there in final_sleep(E
q .w
q ), which means p ∈ final_sleep(E). It hence cannot
hold that p ∈ I[E](wq ), since this by Property 4.2(1) implies p ∈ I[E](w), and hence p ∈ WI[E](w),
violating Claim 7.5. Since there can be no event in wq happening-before p (otherwise the same
event would prohibit p ∈ WI[E
q ](w
q .wq )) the only way for p  I[E](wq ) is by p  wq.
Therefore p  w
q .wq which by p ∈ WI[E
q ](w
q .wq ) entails E
q 	p♦w
q .wq. By choice of q, we then
have necessarily that ep = eq (otherwise wp would be longer than wq). But since among the processes p with ep = eq we chose q to be the first one for which a call of the form Explore(E
q .p, ·, ·)
was performed, we have that p  sleep(E
q ) just before the call to Explore(E
q .q, Sleep
, ·), whence
p  Sleep
. Thus we have a contradiction.
Claim 7.8. Let z be any sequence such that E
q .q.w
q .wR .z is maximal (such a z can always be
found, since E
q .q.w
q .wR is an execution sequence). Then, the algorithm explores some sequence E
q .q.z
in [E
q .q.w
q .wR .z
].
Proof. From Claim 7.7, it follows WI[E
q .q](w
q .wR .z
) ∩ Sleep = ∅. Therefore, no execution
sequence in [E
q .q.w
q .wR .z
] was explored before the call to Explore(E
q .q, Sleep
, ·); otherwise,
there would be a call Explore(E.p, ·, ·) with E a prefix of E
q and p ∈ Sleep
, and, defining
w by E.w = E
q, we would have E 	p♦w and p ∈ WI[E
q .q](w
q .wR .z
), thus contradicting
Journal of the ACM, Vol. 64, No. 4, Article 25. Publication date: August 2017.          
Source Sets: A Foundation for Optimal Dynamic Partial Order Reduction 25:29
WI[E
q .q](w
q .wR .z
) ∩ Sleep = ∅. By the inductive hypothesis for E
q .q applied to w
q .wR .z
, the algorithm then explores some sequence E
q .q.z in [E
q .q.w
q .wR .z
].
By the construction ofwR, we have next[E
q ](q) E
q .q.w
q .wR .z eq. From E
q .q.z  E
q .q.w
q .wR .z
,
it follows that the same race between next[E](q) and eq will also occur in E
q .q.z, that is, we have
next[E
q ](q) E
q .q.z eq. Since the sequence E
q .q.z is actually explored by the algorithm, it will encounter the race next[E
q ](q) E
q .q.z eq. When handling it,
• E in the algorithm will correspond to E
q .q.z in this proof,
• e in the algorithm will correspond to next[E
q ](q) in this proof,
and
• e in the algorithm will correspond to eq in this proof,
• v = (notdep(next[E
q ](q), E
q .q.z).eq ) will be the sequence v at line 5 in Algorithm 2.
Claim 7.9. w
q .wR [E
q ] v.
Proof. Let
• x be notdep(next[E
q ](q), E
q .q.z)
• x  be notdep(next[E
q ](q), E
q .q.w
q .wR .z
). Note that w
q .wq is a prefix of x 
.
From E
q .q.z  E
q .q.w
q .wR .z (Claim 7.8) and the definitions of x and x 
, it follows that E
q .x 
E
q .x  and hence that E
q .x.eq  E
q .x 
.eq. Let x  be obtained from x  by adding eq just after the
prefixw
q .wq (i.e., in the same place that it has inw
q .wR .z
). Since eq “happens after” next[E
q ](q) in
E
q .q.w
q .wR .z
, it follows that no events in x  “happen after” eq in E
q .q.w
q .wR .z
. Hence E
q .x  
E
q .x 
.eq. Sincew
q .wR is a prefix of x , we havew
q .wR [E
q ] x , which by E
q .x   E
q .x 
.eq implies
w
q .wR [E
q ] x 
.eq, which by E
q .x.eq  E
q .x 
.eq implies w
q .wR [E
q ] v.
Claim 7.10. sleep(E
q ) ∩ WI[E
q ](w
q .wR ) = ∅.
Proof. Assume that some process p is in WI[E
q ](w
q .wR ). Then
(1) If p ∈ w
q, then it has no event happening before it, which implies that it cannot have been
in sleep(E
q ), since then it could not have been taken out of the sleep set. Thus p  sleep(E
q ).
(2) Therefore p  w
q. By p ∈ WI[E
q ](w
q .wR ), we have that E
q 	p♦w
q, which assuming p ∈
sleep(E
q ), means thatp will still be in the sleep set afterw
q and thereforep ∈ final_sleep(E).
Then:
(a) If p ∈ wR, then p ∈ I[E](wR ), from which we get p ∈ I[E](w) therefore p ∈ WI[E](w).
Since p ∈ final_sleep(E) this contradicts Claim 7.5.
(b) Therefore p  wR must hold. From p ∈ WI[E
q ](w
q .wR ) and p  w
q .wR, we have that
E
q 	p♦w
q .wR, which implies E
q .w
q 	p♦wR, which is equivalent to E 	p♦wR. But then
wR = wq .eq is a prefix of wp , implying that wp is strictly longer than wq. This contradicts the fact that q was chosen as the process in final_sleep(E) with the longest prefix
wq satisfying E 	q♦wq.
Therefore, there can be no such p ∈ WI[E
q ](w
q .wR ) and Claim 7.10 is proven.
From Claim 7.9, Claim 7.10, and Property 4.7(4), we get sleep(E
q ) ∩ WI[E
q ](v) = ∅. Thus, the test
at line 6 will succeed, and after performing line 7, the wakeup treewut(E
q ) will (by the specification
of insert) contain a leaf y such that y∼[E
q ]v. Since at this point q ∈ wut(E
q ) and q  WI[E
q ](v) we
have, by definition of insert, that E
q q♦y. By Property 7.3(1), we then have E
q .y ∈ E.
From Claim 7.9 and y∼[E
q ]v, it follows by Property 4.7(2) that y∼[E
q ]w
q .wR. Furthermore, from
E
q 	q♦w
q (which follows from E
q 	q♦w
q .wq) and E
q q♦y, it follows that y is not a prefix of w
q.
Journal of the ACM, Vol. 64, No. 4, Article 25. Publication date: August 2017.                       
25:30 P. A. Abdulla et al.
Let u be the longest common prefix of y and w
q .wR. We claim that w
q is a strict prefix of u. Otherwise, there are different processes p,p and a sequence v such that u.p
.v = w
q and u.p is a
prefix of y. From y∼[E
q ]w
q .wR and Property 4.7(2), we infer y∼[E
q ]u.p
. If u.p ∈ wut(E
q ) when y
is inserted, then we infer by Lemma 6.2 and Property 7.3(2) that E
q .y ∝ E
q .u.p
. If u.p  wut(E
q )
when y is inserted, then we also infer E
q .y ∝ E
q .u.p
, since then y will be explored before u.p
.
Thus E
q .y ∝ E
q .u.p
, which implies E
q .u.p ∝ E
q .u.p
.v, which by handling of sleep sets implies p  I[E
q .u](p
.v). By y∼[E
q ]w
q, implying u.p∼[E
q ]u.p
.v, we have p∼[E
q .u]p
.v, which
is the same as p ∈ WI[E
q .u](p
.v). By p  I[E
q .u](p
.v), this implies E
q .u 	p♦(p
.v). This implies that p ∈ sleep(E
q .w
q ), that is, p ∈ sleep(E). Hence by the construction of wR, we have p
WI[E
q .u.p.v](wR ), which together with E
q .u 	p♦(p
.v) implies p  WI[E
q .u](p
.v.wR ), which
implies u.p[E
q ]u.p
.v.wR, which implies y[E
q ]w
q .wR, which contradicts the construction of y.
Thus,w
q is a strict prefix of u. Since E
q .y, and hence E
q .u, is explored by the algorithm, we have
E
q .u ∝ E
q .w
q. Moreover, since u is a prefix ofw
q .wR, we infer that E
q .u is a prefix of E
q .w
q .w. This
means that there is a sequencew such that E
q .u.w  E.w. It follows by the inductive hypothesis
applied to E
q .u that the algorithm has explored some maximal sequence in [E
q .u.w] and hence
in [E.w]. This contradicts the assumption at the beginning of the inductive step. This concludes
the proof of the inductive step, and Theorem 7.4 is proven.
7.3 Optimality
In this section, we prove that optimal-DPOR is optimal in the sense that it never explores two
different but equivalent execution sequences and never encounters sleep set blocking. The following theorem, which is essentially the same as Theorem 3.2 of Godefroid et al. (1995), establishes
that sleep sets alone are sufficient to prevent exploration of two equivalent maximal execution
sequences.
Theorem 7.11. Optimal-DPOR never explores two maximal execution sequences that are
equivalent.
Proof. Assume that E1 and E2 are two equivalent maximal execution sequences that are explored by the algorithm. Then they are both in E. Assume w.l.o.g. that E1 ∝ E2. Let E be their
longest common prefix, and let E1 = E.p.v1 and E2 = E.v2. By Lemma 7.2, we have p  I[E](v2),
which contradicts E1  E2 and the maximality of E1 and E2.
We will now prove that Algorithm 2 is optimal in the sense that it never encounters sleep set
blocking. Let us first define this precisely.
Definition 7.12 (Sleep Set Blocking). A call to Explore(E, Sleep, WuT) is sleep set blocked during
the execution of Algorithm 2 if enabled(s[E])  ∅ and enabled(s[E]) ⊆ Sleep.
Now let us state and prove the corresponding optimality theorem.
Theorem 7.13 (Optimality of Optimal-DPOR). During any execution of Algorithm 2, no call
to Explore(E, Sleep, WuT) is ever sleep set blocked.
Proof. Consider a call Explore(E, Sleep, WuT) during the exploration. Then any sequence in
WuT is enabled after E. By Lemma 7.1, WuT is a wakeup tree after E, Sleep. Thus, if Sleep  ∅,
then WuT contains a sequence w such that Sleep ∩ WI[E](w) = ∅. Letting p be the first process in
w, this implies p  Sleep, implying that p is enabled and thus enabled(s[E])  Sleep.
8 EXTENDING SOURCE-DPOR AND OPTIMAL-DPOR TO SUPPORT BLOCKING
In this section, we consider how to extend the techniques of the previous sections for the general
situation in which processes can disable each other. In other words, we will no longer make use
Journal of the ACM, Vol. 64, No. 4, Article 25. Publication date: August 2017.            
Source Sets: A Foundation for Optimal Dynamic Partial Order Reduction 25:31
ALGORITHM 3: Source-DPOR algorithm considering locks.
Initial call: Explore(, ∅)
1 Explore(E, Sleep)
2 foreach lock l do
3 foreach p whose next operation after E is lock(l) do
4 let e be the last event in E with op[E](e) = lock(l);
5 let E = pre(E, e);
6 let v = notdep(e, E).p;
7 if I[E](v) ∩ backtrack(E
) = ∅ then
8 add some q ∈ I[E](v) to backtrack(E
);
9 if ∃r ∈ (enabled(s[E]) \ Sleep) then
10 backtrack(E) := {r};
11 while ∃p ∈ (backtrack(E) \ Sleep) do
12 foreach e ∈ dom(E) such that(e E.p next[E](p)) do
13 let E = pre(E, e);
14 let v = notdep(e, E).p;
15 if I[E](v) ∩ backtrack(E
) = ∅ then
16 add some q ∈ I[E](v) to backtrack(E
);
17 let Sleep = {q ∈ Sleep | E 	p♦q};
18 Explore(E.p, Sleep
);
19 add p to Sleep;
of Assumption 3.1. Processes can disable each other via shared synchronization objects, such as
locks, condition variables, and so on. It is also possible for processes to disable each other via lessvisible mechanisms; one such method is to use await statements, by means of which any update
to a shared variable can disable an await statement that depends on this shared variable. When
extending our techniques to the case in which processes can disable each other, it turns out that the
general case of await statements requires a more sophisticated modification than the case of locks.
We will therefore first consider how to extend Algorithm 1 and 2 to handle locks and thereafter
consider the general case.
8.1 Handling Locks
Let us extend Algorithm 1 and 2 to the case that disabling between processes happens only via
locks. We assume a set of locks that are disjoint from ordinary memory locations and only used in
lock operations. Each lock l can take values 0 or 1 (initial value) and supports the operations
lock(l) and unlock(l), where lock(l) decrements l from 1 to 0 if l is 1 and blocks if l is 0
and where unlock(l) assigns 1 to l (and never blocks). The only operation that can disable another process is lock(l), which disables any other operation of form lock(l). We assume that the
happens-before relation relates a lock(l) operation with any other operation of form lock(l) or
unlock(l) on the same lock, that the lock(l) operation is independent of other operations, and
that two unlock(l) operations by different threads are independent. Note that we do not assume
that programs use locks in a “well-structured” way or that the program is data-race-free.
8.1.1 Extending Source-DPOR to Handle Locks.. To handle locks, the basic modification to Algorithm 1 is to extend the concept of races. An important part of Algorithm 1 (at line 5) is to detect
Journal of the ACM, Vol. 64, No. 4, Article 25. Publication date: August 2017. 
25:32 P. A. Abdulla et al.
a race between two events e and next[E](p). Let us denote next[E](p) by e
. In the absence of locks,
whenever e is in a race with e
, and e is performed before e
, then e must appear sometime later
in the execution, since it cannot be disabled by e. This implies that the race detection mechanism
of line 5 in Algorithm 1 is sufficient for detecting any race. However, in the presence of locks,
we must detect races also in the case where e has been disabled by e. The needed extension is
to consider the case where e performs a lock() operation, regardless of whether e
 is blocked or
not, and to investigate alternative executions in which e is performed before the most recently
performed lock() operation. Such an extension should work when e
 is blocked, since it is necessary to explore the case where e occurs before the most recently performed lock() operation,
when it is not blocked. Additionally, it should also work when e
 is not blocked. The reason is that
Algorithm 1 will only find the “race” between the enabled lock event e and the previous unlock()
operation: Obviously, these two events cannot be reversed. Instead, it is necessary to consider the
race between e and the most recently performed lock() operation; these events can be performed
in reverse order.
Our extension of Algorithm 1 to programs with locks is shown in Algorithm 3. The extension
consists of adding an extra case to the race detection phase, found at lines 2–8. This extra case
considers all processes p that are about to perform a lock() operation on some lock l, regardless
of whether they are blocked. For each such process, we find the preceding lock(l) operation in E,
since that operation is in a race with the next step of process p, according to our extended definition
of “race.” Note that the next step of process p, which is a lock(l) operation, can be either enabled or
disabled. Thereafter, the race detection proceeds analogously as in the non-blocking case: At line 6,
the sequence v is constructed as a sequence of events that can follow E in an execution where
p takes the lock before e. At line 7, it is tested whether the backtrack set at E already contains a
process that makes such an extension of E possible; otherwise, such a process is added at line 8.
8.1.2 Correctness of Algorithm 3.. Let us next consider how to extend the proof of correctness
for Algorithm 1 to become a proof of correctness for Algorithm 3. Here we present the needed
modifications to the proof of Theorem 5.2. We thus consider how to extend the proof of Lemma 5.3.
The proof remains the same as in Section 5.2 up to Property (9), stated in the proof of Claim 5.4.
This property, that is, E 	 q.wR, is no longer true if there is a lock l such that next[E](q) and eq
perform the operation lock(l), since eq is now disabled after E.q.wq. We therefore here explain
how the proof should be continued for this case; all other cases can be handled as in Section 5.2.
We first infer, from E 	q♦wq, that wq does not contain any operation on the lock l. We then
replace Claim 5.5 by the following stronger one:
For all p ∈ done(E) with p  q we have p  WI[E](q.wq ).
This property is actually shown in Cases 1 and 2a in the proof for Claim 5.5. The other cases, that
is, Cases 2b and 2c, are not applicable when next[E](q) and eq both perform the operation lock(l),
since these cases assume ep = eq and E 	p♦q, which is not possible when eq performs the operation
lock(l). This follows by noting that, since the next events of p and q conflict with eq, they must
both be lock or unlock operations on l. Since next[E](q) performs the operation lock(l), we infer
E p♦q.
We next establish a slight modification of IH-condition 5.6, namely that wq ∈
to_be_explored(E.q). The proof of this is contained in the proof of IH-condition 5.6: We
first note in formula (12) that p  I[E.q](wq ) for any p ∈ done(E) and then in formula (13) that
p  I[E.q](wq ) for any p ∈ init_sleep(E), concluding that I[E.q](wq ) ∩ init_sleep(E.q) = ∅.
We next claim that the algorithm will actually explore an execution sequence of form E.q.w that
is in [E.q.wq .v] for some v (i.e., such that wq [E.q] w
), such that if w contain some lock(l)
Journal of the ACM, Vol. 64, No. 4, Article 25. Publication date: August 2017.          
Source Sets: A Foundation for Optimal Dynamic Partial Order Reduction 25:33
operation(s), then eq is the first of these. To prove this claim, note that after E.q.wq, process eq is
prepared to perform a lock(l) operation and will continue to be prepared to do so all the time
until it actually performs such a lock(l) operation (note that immediately after E.q.wq, the lock l
is still taken). We split the proof into two cases:
(1) If to_be_explored(E.q) contains a sequence of form wq .z in which z contains an unlock(l)
operation, then to_be_explored(E.q) also contains such a sequence of form wq .z in which
event eq immediately follows the first of these unlock(l) operations, since process eq is
continuously prepared to perform lock(l) operation after E.q.wq. The inductive hypothesis for E.q guarantees that the algorithm explores E.q.w for some w with z [E.q] w
.
Since the operations lock(l) are dependent with each other, eq is the first event in w to
perform a lock(l) operation.
(2) If to_be_explored(E.q) contains no sequence of form wq .z in which z contains an
unlock(l) operation, then the inductive hypothesis for E.q guarantees that the algorithm
explores E.q.w for some w with wq [E.q] w
. Since w must be in to_be_explored(E.q),
we conclude that w contains no unlock(l), and hence no lock(l) operation.
Having proven this claim, let u be the shortest prefix ofw such thatwq [E.q] u. Note that after
wq, the event eq (with operation lock(l)) is the next to be performed by eq, although the lock l
may still be taken immediately after u. Since, according to the just-proven claim, u contains no
lock(l) operation, then next[E](q) is the last event in E.q.u that performs a lock(l) operation,
and so the race detection at lines 2–8 of Algorithm 3 will make sure that done(E) contains some
q with q ∈ I[E](v), where v = v
.eq for v = notdep(next[E](q), E.q.u). In this case, we derive a
contradiction in a similar way as in the proof of Claim 5.4. By the construction of v
, we have
v [E.q] w
. From wq [E.q] w and v [E.q] w
, and Property 4.7(3), we infer wq∼[E.q]v
. This
implies, using E 	q♦wq and E 	q♦v
, that wq∼[E]v (this can be established in a similar way as
Lemma 5.1.3). Now q ∈ I[E](v) means that either (i) q ∈ I[E](v
) or that (ii) q = eq and E 	q
♦v
.
Case (i) implies (by the construction of v
) that next[E](q
) does not perform any operation on
the lock l and (using wq∼[E]v
) that q ∈ WI[E](wq ). Since eq performs the operation lock(l),
this implies q ∈ WI[E](wR ), which contradicts Property (10). In Case (ii), from wq [E.q] u and
E 	q♦wq, the construction ofv implieswq [E.q] v
. Also considering E 	q♦v
, we inferwq [E] v
.
Since q ∈ done(E), the construction of wR implies that E q
♦wq, which together with wq [E] v
implies E q
♦v
, contradicting the assumption E 	q
♦v for case (ii).
This concludes the proof of Claim 5.4. The rest of the proof proceed as in the proof of
Theorem 5.2.
8.1.3 Extending Optimal-DPOR to Handle Locks.. Let us next present our extension of Algorithm 2 to programs with locks, resulting in Algorithm 4. Similarly as in Algorithm 3, the algorithm extends the lock detection phase to consider races between lock() operations, one of which
is potentially disabled. The extension for locks is handled by lines 8–14. These lines handle the case
when event e in a race e E e is actually blocked for the reason that both e and e are lock(l)
operations on the same lock l. In this case, e may not be present in the execution sequence E at a
position where it is blocked. For this case, we construct the sequence v as in line 6 of Algorithm 3.
First, at line 10, the events that happen-after e are removed from E, resulting in the suffix w. We
then construct v by adding a process p that is prepared to perform a lock(l) operation. Since by
construction w does not contain any operations on the lock l, and since the lock could be taken
after E by e, it is ensured that the lock can be taken after E
.w by any process that is ready to do
so. Thereafter, we perform the same test as in Algorithm 2 to decide whether the wakeup tree at
E must be extended to explore some sequence that is equivalent to E
.v.
Journal of the ACM, Vol. 64, No. 4, Article 25. Publication date: August 2017.          
25:34 P. A. Abdulla et al.
ALGORITHM 4: Optimal-DPOR algorithm considering locks.
Initial call: Explore(, ∅,{}, ∅)
1 Explore(E, Sleep, WuT)
2 if enabled(s[E]) = ∅ then
3 foreach e, e  ∈ dom(E) such that (e E e 
) do
4 let E = pre(E, e);
5 let v = (notdep(e, E).e
);
6 if sleep(E
) ∩ WI[E](v) = ∅ then
7 wut(E
) := insert[E](v,wut(E
));
8 foreach e ∈ dom(E) such that op[E](e) = lock(l) do
9 let E = pre(E, e);
10 let w = notdep(e, E);
11 foreach p such that op[E.w.p](next[E.w](p)) = lock(l) do
12 let v = w.p;
13 if sleep(E
) ∩ WI[E](v) = ∅ then
14 wut(E
) := insert[E](v,wut(E
));
15 else
16 if WuT  {}, ∅ then
17 wut(E) := WuT;
18 else
19 choose p ∈ enabled(s[E]);
20 wut(E) := {p}, ∅;
21 sleep(E) := Sleep;
22 while ∃p ∈ wut(E) do
23 let p = min≺{p ∈ wut(E)};
24 let Sleep = {q ∈ sleep(E) | E 	p♦q};
25 let WuT = subtree(wut(E),p);
26 Explore(E.p, Sleep
, WuT
);
27 add p to sleep(E);
28 remove all sequences of form p.w from wut(E);
Let us next consider how to extend the soundness proof of Algorithm 2 to cover Algorithm 4.
The overall structure of the proof remains the same; below we indicate where and how the proof
needs to be extended.
The first extension is needed when we get to Claim 7.6, and the event performed by q is of
form lock(l). In this case, eq must also perform lock(l) (by another process). But it is obvious
that now E
q 	 q.w
q .wR does not hold, since both next[E
q ](q) and eq take the lock. We therefore replace Claim 7.6 with E
q 	 q.w
q .wq. For similar reasons, Claim 7.7 must be replaced by the stronger
WI[E
q .q](w
q .wq ) ∩ Sleep = ∅, which is actually also proven in the text of the proof of Claim 7.7.
Continuing along these lines, the text of Claim 7.8 should be changed to the following:
Let z be any sequence such that E
q .q.w
q .wq .z is maximal. (Such a z can always
be found, since E
q .q.w
q .wq is an execution sequence.) Then, the algorithm explores
some sequence E
q .q.z in [E
q .q.w
q .wq .z
].
Journal of the ACM, Vol. 64, No. 4, Article 25. Publication date: August 2017.   
Source Sets: A Foundation for Optimal Dynamic Partial Order Reduction 25:35
Fig. 9. Example motivating line 11 of Algorithm 6.
After the proof of Claim 7.8, the algorithm will handle the new kind of race (lines 8–14) between
lock(l) operations, with v = w.p. At the end, we end up with the same Claim 7.9 as in the proof,
that is, w
q .wR [E
q ] v, where the last process in wR is actually eq. Here, it is important to observe
that eq, being of form lock(l), is actually independent with all events in w except possibly the
ones already in w
q .wq. Thereafter, the proof goes on just as in the non-blocking case.
8.2 Allowing Arbitrary Blocking in the Execution Model
Let us next consider how to extend algorithms Source-DPOR and Optimal-DPOR to a general
execution model, where disabling between processes can happen via any operations, in a computational model that need only satisfy the requirements of Definition 3.2. If disabling between
processes happens only via clearly identified synchronization mechanisms, such as locks, then
Algorithm 3 and 4 with obvious modifications are sufficient. However, if disabling can occur in
less obvious ways, the modification is less straightforward. The reason is that, in the general case,
races may be more difficult to detect. We illustrate this by a small example.
Consider the program in Figure 9. All four statements are atomic. In this example, we define the
following happens-before relation on the events in an execution sequence: Let the statement of s
be related with any other statement, and let the statements of processes p, q, and r all be unrelated
if the statement of s does not occur somewhere between them. The statements of processes p, q,
and r are all related if the statement of s occurs between them. Such a happens-before relation
is somewhat non-standard; another natural choice of happens-before relation would be to let it
relate all pairs of statements, since they all touch the variable x. However, our choice is also consistent with the requirements on a happens-before relation in Definition 3.2, since the order in
which the atomic statements of p, q, and r are performed does not matter when they occur consecutively. With this happens-before assignment, the program has only two Mazurkiewicz traces:
one in which only p, q, and r occur (whereafter s is blocked) and one consisting of r.s.p.q and
r.s.q.p.
To illustrate the difficulty of detecting races, suppose that we first explore the sequence p.q.r,
after which s is blocked. In some way, we must now define a “race.” There is a race between
p and s, but such a race is visible only in explorations that start with r.p. More generally, this
example shows that in the performed execution sequence (in this case p.q.r), one may have to
extract a subsequence (in this case r) to make the race visible. The subsequence r should clearly
be consistent with p.q.r in the sense that r [] p.q.r.
Let us from this example extract a general pattern for how to extend the race detection phase
of our algorithms. In this race detection, we should try to find a sequence that plays the role of v,
as defined in line 6 in Algorithm 3 and in line 12 of Algorithm 4. In Algorithms 3 and 4, we could
take v to be the sequence of all statements that do not happen-after p, that is, q.r.s, since any
happens-before closed subset of v will enable s. However, in the present case, it may happen that
only a specific subsequence of v can enable s. In this case, the subsequence is r, implying that we
should takev asr.s. Continuing with the example in Figure 9, we see that there are several possible
choices for the sequence v, but of all these only r will lead to a state where s can be disabled by
Journal of the ACM, Vol. 64, No. 4, Article 25. Publication date: August 2017. 
25:36 P. A. Abdulla et al.
ALGORITHM 5: Source-DPOR algorithm for general blocking.
Initial call: Explore(, ∅)
1 Explore(E, Sleep)
2 foreach p that is blocked in s[E] do
3 foreach e ∈ dom(E) do
4 let E = pre(E, e);
5 let w = notdep(e, E);
6 foreach subsequence u of w such that u [E] w do
7 if e disables p after E.u then
8 let v = u.p;
9 if I[E](v) ∩ backtrack(E
) = ∅ then
10 add some q ∈ I[E](v) to backtrack(E
);
11 if ∃p ∈ (enabled(s[E]) \ Sleep) then
12 backtrack(E) := {p};
13 while ∃p ∈ (backtrack(E) \ Sleep) do
14 foreach e ∈ dom(E) such that(e E.p next[E](p)) do
15 let E = pre(E, e);
16 let v = notdep(e, E).p;
17 if I[E](v) ∩ backtrack(E
) = ∅ then
18 add some q ∈ I[E](v) to backtrack(E
);
19 let Sleep = {q ∈ Sleep | E 	p♦q};
20 Explore(E.p, Sleep
);
21 add p to Sleep;
another process. It is, of course, possible to develop optimization schemes that would disregard
many of these choices, but we here refrain from doing this to keep the structure of the algorithm
simple.
Let us now incorporate these insights into general extensions of Algorithms 3 and 4. The generalization of Algorithm 3 is shown in Algorithm 5. The extended race detection phase is shown
at lines 2–10. Whenever a process p is blocked, we try to detect whether some other process e disables p. As described above, we may have to movee past some subsequenceu to make the disabling
visible. As in Algorithm 3, the algorithm first extracts the sequence w that does not happen-after
e. Thereafter, it considers all possible subsequences u of w that are consistent w.r.t. the happensbefore relation. Whenever e disables some process p after u, then the sequence u.p is used as v in
the race detection phase. The rest of the race detection proceeds as before.
The proof of soundness for Algorithm 5 can be obtained by a small modification of the proof
for Algorithm 1. As for Algorithm 3, we must replace wR by wq in Claim 5.4, Claim 5.5, and in
related properties, just as in the proof for Algorithm 3. We define w
q precisely as in the proof
for Algorithm 3 and note that w
q [E.q] w
. Let w
q be the reordering of the events in w
q as they
occur in w
. Then we can take u as w
q at line 6 of the algorithm. By construction q disables eq
after E.w
q , and the rest of the proof proceeds as in the proof for Algorithm 3.
The extension of Algorithm 4 is shown in Algorithm 6. The corresponding extension of the race
detection phase is now at lines 8–15.
Journal of the ACM, Vol. 64, No. 4, Article 25. Publication date: August 2017.      
Source Sets: A Foundation for Optimal Dynamic Partial Order Reduction 25:37
ALGORITHM 6: Optimal-DPOR algorithm for general blocking.
Initial call: Explore(, ∅,{}, ∅)
1 Explore(E, Sleep, WuT)
2 if enabled(s[E]) = ∅ then
3 foreach e, e  ∈ dom(E) such that (e E e 
) do
4 let E = pre(E, e);
5 let v = (notdep(e, E).e
);
6 if sleep(E
) ∩ WI[E](v) = ∅ then
7 wut(E
) := insert[E](v,wut(E
));
8 foreach e ∈ dom(E) do
9 let E = pre(E, e);
10 let w = notdep(e, E);
11 foreach subsequence u of w such that u [E] w do
12 foreach p such that e disables p after E
.u do
13 let v = u.p;
14 if sleep(E
) ∩ WI[E](v) = ∅ then
15 wut(E
) := insert[E](v,wut(E
));
16 else
17 if WuT  {}, ∅ then
18 wut(E) := WuT;
19 else
20 choose p ∈ enabled(s[E]);
21 wut(E) := {p}, ∅;
22 sleep(E) := Sleep;
23 while ∃p ∈ wut(E) do
24 let p = min≺{p ∈ wut(E)};
25 let Sleep = {q ∈ sleep(E) | E 	p♦q};
26 let WuT = subtree(wut(E),p);
27 Explore(E.p, Sleep
, WuT
);
28 add p to sleep(E);
29 remove all sequences of form p.w from wut(E);
Let us consider how to extend the soundness proof of Algorithm 2 to cover Algorithm 6. As
for the locking case, the structure of the proof remains the same. The main modifications are as
follows:
• Claim 7.6 is replaced by E
q 	 q.w
q .wq;
• Claim 7.7 is replaced by WI[E
q .q](w
q .wq ) ∩ Sleep = ∅;
• in Claim 7.8, wR is replaced by wq.
In the text between Claim 7.8 and Claim 7.9, the text about race detection should be replaced by
the observation that by choosing u as the subsequence ofw withw
q .wq  E
qu, then the test at line
12 will succeed. The rest of the proof proceeds as for Algorithm 2.
Journal of the ACM, Vol. 64, No. 4, Article 25. Publication date: August 2017.    
25:38 P. A. Abdulla et al.
Fig. 10. The program of Figure 2 extended with more processes and global variables.
9 COMPARISON AND TRADEOFFS BETWEEN SOURCE-DPOR
AND OPTIMAL-DPOR
In this section, we compare source-DPOR and optimal-DPOR on synthetic programs that highlight
interesting performance differences, based on the number of explored interleavings, as well as in
time and memory consumption. More specifically, we present:
(1) A program that shows that the use of wakeup trees allows optimal-DPOR to avoid exploration of an exponential number of sleep set blocked traces. In contrast, non-optimal
DPOR algorithms in general, and source-DPOR in particular, are forced to explore these
traces and therefore can require arbitrarily more time than optimal-DPOR. As we describe,
this does not happen only under the worst scheduling scenario but also even when the
scheduling algorithm that is employed is a “reasonable” one.
(2) A simpler program, which we also use as a benchmark in the next section, that also shows
that a wakeup sequence is necessary for the optimal algorithm to avoid sleep set blocking.
(3) A program that shows that wakeup trees may make the optimal algorithm require exponentially more space than source-DPOR.
The first program, shown in Figure 10, is an extended version of the example presented earlier
in Figure 2. It has been extended in the following way: n determines how many groups of four
processes (which correspond to p, q, r, and s from Figure 2) are used. Each of the qi, ri , and si
processes operate on the respective xi, yi , and zi variables. The code of the pi processes reads xi
and only spawns qi+1,ri+1 and si+1 if the write on xi by si has been completed. This pair of readwrite operations is in a race. The other processes also have racing read-write pairs on variables yi
and zi .
When starting an interleaving to reverse the race of the top sn process with the top pn process,
the only way for pn to be removed from the sleep set is for each “lower” group to be scheduled
exactly as shown in Figure 3, as this is the only way forsn to execute its write to xn. If a “wrong step”
is taken at any point, then sn will not do the write and the respective trace will be sleep set blocked.
For n = 1, there exist four interleavings where the read on x1 by p1 happens before the respective write by s1 and one more where it appears afterwards, just as shown in Figure 3. However,
with just the first step (which must be taken by r1) specified, it can be the case for source-DPOR
to schedule q1 before s1 has read y1, thus altering s1’s behaviour and leading to sleep set blocking.
As there are four ways to schedule qn, rn, and sn processes, only one of which reveals the write on
xn, pn will remain in the sleep set and the other three schedules will encounter sleep set blocking.
For higher values of n, when the race of xi in the ith group is reversed successfully, four new
interleavings are possible by the i + 1-th group. The number of Mazurkiewicz traces for this program is 4n + 1: four traces for the “top” group, plus a fifth that splits in four when it spawns the
Journal of the ACM, Vol. 64, No. 4, Article 25. Publication date: August 2017.
Source Sets: A Foundation for Optimal Dynamic Partial Order Reduction 25:39
Table 1. Traces Explored by Source- and Optimal-DPOR for the
Program in Figure 10
Traces Explored Time
n Source Optimal Sleep Set Blocked Source Optimal
1 8 5 3 0.03s 0.02s
2 20 9 11 0.08s 0.04s
3 41 13 28 0.20s 0.07s
4 78 17 61 0.41s 0.10s
5 145 21 124 0.81s 0.17s
6 268 25 243 1.56s 0.20s
7 373 29 344 2.38s 0.26s
8 674 33 641 4.70s 0.34s
9 1,222 37 1,185 8.79s 0.44s
Fig. 11. Pseudocode of the lastzero(N) example; measurements for it can be found in Section 11.
next group of processes and so on until the final group, which does not split and has just a single
extra interleaving.
Wrong schedulings within the ith group are still sleep set blocked. Additionally, to reverse the
pair operating on xi+1, all the lower groups must be scheduled correctly, but this is only possible in
exactly one interleaving, whereas all deviations lead to sleep-set-blocked interleavings. As a result,
the tree of the “previous” level (where pi+1 was not in a sleep set) is duplicated as sleep-set-blocked
explorations, leading to the exponential growth of traces that encounter sleep set blocking shown
in Table 1. This program shows therefore an exponential difference between non-optimal DPOR
algorithms (and source-DPOR) and optimal-DPOR. As seen in Table 1, this difference does not just
affect the number of traces that the two algorithms explore but also the time performance of the
two algorithms. Thus, optimal-DPOR can perform exponentially better in time than non-optimal
DPOR algorithms.
The second program is lastzero(N), whose pseudocode is shown in Figure 11. Its N+1 processes
operate on an array of N+1 elements that are all initially zero. In this program, process 0 searches
the array for the highest index that has a zero value, while the other N processes read one of the
array values and write that value increased by one in the next index. The final state of the program
is uniquely defined by the values of i and array[1..N].
Here, again, process 0 has control flow that depends on the values in the array, which are all
exposed to racing operations. This is generally a typical case where source-DPOR may encounter
sleep set blocking, which optimal-DPOR can avoid. If some of the processes with higher indices
have not completed their execution, then processes with lower indices that are in a sleep set,
because of a race with process 0’s access, will never be removed, as process 0 will “stop” at a
high index and not access the lower indices. Again, an accurate wakeup sequence is necessary to
avoid sleep set blocking in this program. We present measurements for the lastzero(N) program
in Section 11.
Journal of the ACM, Vol. 64, No. 4, Article 25. Publication date: August 2017.
25:40 P. A. Abdulla et al.
Fig. 12. Pseudocode of the wakeup_stress(N) example.
Table 2. Nodes in Largest Backtrack Set/Wakeup Tree, Memory and Time Usage of the
Source- and Optimal-DPOR Algorithms (As Implemented in Concuerror) for the Program
in Figure 12
Source Optimal
n Traces Explored Nodes Memory Time Nodes Memory Time
1 2 2 22MB 0.01s 7 22MB 0.01s
2 4 3 22MB 0.02s 21 22MB 0.02s
3 12 4 22MB 0.05s 79 22MB 0.06s
4 48 5 22MB 0.24s 374 23MB 0.24s
5 240 6 22MB 1.21s 2,159 27MB 1.23s
6 1,440 7 22MB 8.74s 14,698 42MB 8.96s
7 10,080 8 22MB 1m02s 115,091 114MB 1m07s
8 80,640 9 22MB 9m30s 1,018,094 801MB 9m48s
9 725,760 10 22MB 89m14s 9,916,765 8,364MB 99m2s
The third and final program is wakeup_stress(N), whose pseudocode is shown in Figure 12.
Here each of N processes increments counter2 atomically, and two more processes, 0 and
N + 1, increment counter1, with process N + 1 waiting for all N processes to finish before doing its
operation. Under the assumption that the inc operations are dependent (and wait operations are
not), this program has 2 ∗ (N!) Mazurkiewicz traces, differing in the order in which the N processes
increment counter2 and processes 0 and N + 1 increment counter1.
Whenever an attempt is made to reverse a race, any chosen process will eventually execute an
appropriate inc operation, removing any other process from the sleep set, so source-DPOR will
never encounter sleep set blocking.
On the other hand, all such inc operations are also racing with each other, therefore introducing
new branches in a wakeup tree. Particularly for the race between processes 0 and N + 1, each of the
N! schedules of the N processes (that must all finish to enable process N + 1) differs in the happensbefore order of the inc counter2 operations. As a result, the wakeup tree at the point where the
first inc counter1 operation is originally executed will eventually have N! branches.
In Table 2, we show the total memory usage of Concuerror (the stateless model checking tool
which, as we describe in the next section, is our implementation vehicle) when exploring the interleavings of wakeup_stress(N). It is evident that the base memory usage of the tool is approximately
22MB, but as the number of interleavings increases the memory used by wakeup trees, which
are present only in optimal-DPOR, starts to become noticeable and follows an exponential trend.
The timing measurements, however, show that the overhead of maintaining the wakeup trees is
Journal of the ACM, Vol. 64, No. 4, Article 25. Publication date: August 2017.
Source Sets: A Foundation for Optimal Dynamic Partial Order Reduction 25:41
negligible, even in this case, and the optimal algorithm has a time performance that is very similar
to source-DPOR.
In general, the purpose of wakeup trees is to “guide the first steps” of future explorations with
enough information to ensure that the only traces explored are Mazurkiewicz traces which are
distinct from both all previous explored traces and from each other. As a result, an exponential
blowup in the size of wakeup trees can only be the result of still needing to explore exponentially
many Mazurkiewicz traces, which in turn means that the exponential worst case kicks in only
when the DPOR algorithm needs to do exponential work anyway.
10 IMPLEMENTATION
In this section, we describe our implementation in the context of Concuerror, a stateless model
checking tool for Erlang. Erlang is an industrially relevant programming language based on the
actor model of concurrency (Armstrong 2010). In Erlang, actors are realized by language-level processes implemented by the runtime system instead of being directly mapped to operating system
threads. Each Erlang process has its own private memory area (stack, heap, and mailbox) and communicates with other processes via message passing. A call to the spawn function creates a new
process p and returns a process identifier (PID) that can be used to send messages to p. Messages
are sent asynchronously using the ! operation (or send function). Messages get placed in the mailbox of the receiving process in the order they arrive. A process can then consume messages using
selective pattern matching in receive expressions, which are blocking operations when a process
mailbox does not contain any matching message. Optionally, a receive may contain an after
clause that specifies a timeout value (either an integer or the special value infinity) and a value
to be returned if the timeout time (in ms) is exhausted.
Erlang processes do not share any memory by default. Still, the Erlang implementation comes
with a key-value store mechanism, called Erlang Term Storage (ETS), that allows processes to create memory areas where terms shared between processes can be inserted, looked up, and updated.
Such areas are the ETS tables that are explicitly declared public. The runtime system automatically serializes accesses to these tables when this is necessary and also comes with mechanisms
that guarantee atomicity of some operations (e.g., a bulk insert). Each ETS table is owned by the
process that created it and its memory is reclaimed by the runtime system when this process exits
if no other process has inherited this table.
Erlang has all the ingredients needed for concurrency via message passing and most of the
ingredients (e.g., reads and writes to shared data, etc.) needed for concurrent programming using
shared memory. Unsurprisingly, Erlang programs are prone to “the usual” errors associated with
concurrent execution, although the majority of them revolves around message passing and misuse
of built-in primitives implemented in C.
Figure 13 shows the program of Figure 1 written in Erlang, generalized to N instead of just two
readers. On line 5, a public ETS table named tab is created and is shared between N+1 processes: N
readers and one writer. The writer inserts a key-value pair, using k as a key. Each of the N readers
spawned on line 9 tries to read two entries from this table: some entry with a different key in each
process (an integer in the range 1..N) and the entry keyed by k. The receive expression on line 10
forces the process executing the readers code to get stuck at this point, ensuring that the process
owning the table stays alive, which in turn preserves the ETS table.
As mentioned, our implementation vehicle is Concuerror (Christakis et al. 2013), a stateless
model checking tool for finding concurrency errors in Erlang programs or verifying their absence.
The tool is publicly available at http://parapluu.github.io/Concuerror/.
Given a program and a test to run, Concuerror uses a stateless search algorithm to systematically
explore the execution of the test under conceptually all process interleavings. To achieve this,
Journal of the ACM, Vol. 64, No. 4, Article 25. Publication date: August 2017.
25:42 P. A. Abdulla et al.
Fig. 13. Writer-readers program in Erlang.
the tool employs a source-to-source transformation that inserts instrumentation at preemption
points (i.e., points where a context switch is allowed to occur) in the code under execution. This
instrumentation allows Concuerror to take control of the scheduler when the program is run,
without having to modify the Erlang VM in any way. In the current VM, a context switch may occur
at any function call. Concuerror inserts preemption points only at process actions that interact
with (i.e., inspect or update) shared state. Concuerror supports the complete Erlang language and
can instrument programs of any size, including any libraries they use.
For benchmarking purposes, we extended a previous version of Concuerror1 with three DPOR
algorithms: (i) the “classic” DPOR algorithm with the sleep set extension as presented by Flanagan
and Godefroid (2005a), (ii) source-DPOR, and (iii) optimal-DPOR. To implement these, we had to
encode rules for dependencies between operations that constitute preemption points. These rules
are shared between all DPOR variants. For lookups and inserts to ETS tables (i.e., reads and writes
to shared data), the rules are standard (two operations conflict if they act on the same key and at
least one is an insert). For sending and receiving operations, the “happens before” relation (→E ) is
as follows:
• Two sends are ordered by →E if they send to the same process, even if the messages are
the same. (Note that if we would not order two sends that send the same message, then
when we reorder them, the corresponding receive will not “happen after” the same send
statement.)
• A send “happens before” the receive statement that receives the message it sent. A race
exists between these statements only if the receive has an after clause.
• A receive that executes its after clause “happens before” a subsequent send that sends a
message that it can consume.
There are also other race-prone primitives in Erlang, but it is beyond the scope of this article to
describe how they interact.
Concuerror uses a vector clock (Mattern 1989) for each process at each state to calculate the
happens-before relation for any two events. The calculation of the vector clocks uses the ideas
presented in the original DPOR article (Flanagan and Godefroid 2005b). The only special case is
for the association of a send with a receive, where we instrument the message itself with the
vector clock of the sending process.
1The current (2016–2017) version of Concuerror supports the optimal-DPOR algorithm by default and the source-DPOR
algorithm as an option. The “classic” algorithm is no longer available as an option.
Journal of the ACM, Vol. 64, No. 4, Article 25. Publication date: August 2017.
Source Sets: A Foundation for Optimal Dynamic Partial Order Reduction 25:43
11 EXPERIMENTS
We report experimental results that compare the performance of the three DPOR algorithms that,
for convenience, we will refer to as “Classic” (for the algorithm of Flanagan and Godefroid (2005a)),
“Source,” and “Optimal.” We ran all benchmarks on a desktop with an i7-3770 CPU (3.40GHz)
with 16GB of RAM running Debian Linux 3.2.0-4-amd64. The machine has four physical cores,
but presently Concuerror uses only one of them. In all benchmarks, Concuerror was started with
the option -p inf (use “infinity” as preemption bound), which instructs the tool to explore
traces without limiting the number of preemptions in each explored trace, that is, to verify these
programs.2
Performance on two “standard” benchmarks. First, we report performance on the two benchmarks from the DPOR article (Flanagan and Godefroid 2005b): filesystem and indexer. These are
benchmarks that have been used in the literature to evaluate another DPOR variant (DPOR-CR
of Saarikivi et al. (2012)) and a technique based on unfoldings by Kähkönen et al. (2012). As both
programs use locks, we had to emulate a locking mechanism using Erlang. To make this translation, we used particular language features for these two benchmarks.
filesystem. This benchmark uses two lock-handling primitives, called acquire and release.
The assumptions made for these primitives are that an acquire and a release
operation on the same lock are never co-enabled and should therefore not be interleaved in a way that differs from the one in which they occur. Thus, acquires
are the only operations that can be swapped, if possible, to get a different interleaving.
We implemented the lock objects in Erlang as separate processes. To acquire the
lock, a process sends a message with its identifier to the “lock process” and waits
for a reply. On receiving the message, the lock process uses the identifier to reply
and then waits for a release message. Other acquire messages are left in the
lock’s mailbox. On receiving the release message, the lock process loops back
to the start, retrieving the next acquire message and notifying the next process.
This behavior can be implemented in Erlang using two selective receives.
indexer. This benchmark uses a CAS primitive instruction to check whether a specific entry in a matrix is zero and sets it to a new value. The “Erlang way” to do this
is to try to execute an insert_new operation on an ETS table: If another entry
with the same key exists, then the operation returns false; otherwise, the operation returns true and the table now contains the new entry. Two insert_new
operations on the same key are always dependent.
Both benchmarks are parametric on the number of threads they use. For filesystem, we used 14,
16, 18, and 19 threads. For indexer, we used 12 and 15 threads.
Table 3 shows the number of traces that the three algorithms explore as well as the time it
takes to explore them. It is clear that our algorithms, which in these benchmarks explore the same
(optimal) number of interleavings, beat “classic” DPOR with sleep sets by a margin that becomes
wider as the number of threads increases. As a sanity check, Kähkönen et al. (2012) report that their
unfolding-based method is also able to explore only 8 paths for indexer(12), while their prototype
implementation of DPOR extended with sleep sets and support for commutativity of reads and
2The version of Concuerror that we used to obtain the experimental results of this section is available at https://github.
com/aronisstav/Concuerror/releases/tag/JACM_submission
Journal of the ACM, Vol. 64, No. 4, Article 25. Publication date: August 2017.
25:44 P. A. Abdulla et al.
Table 3. Performance of DPOR Algorithms on Two Benchmarks
Traces Explored Time
Benchmark Classic Source Optimal Classic Source Optimal
filesystem(14) 4 2 2 0.54s 0.36s 0.35s
filesystem(16) 64 8 8 8.13s 1.82s 1.78s
filesystem(18) 1,024 32 32 2m11s 8.52s 8.86s
filesystem(19) 4,096 64 64 8m33s 18.62s 19.57s
indexer(12) 78 8 8 0.74s 0.11s 0.10s
indexer(15) 341,832 4,096 4,096 56m20s 50.24s 52.35s
Table 4. Performance of DPOR Algorithms on More Benchmarks
Traces Explored Time
Benchmark Classic Source Optimal Classic Source Optimal
readers(2) 5 4 4 0.02s 0.02s 0.02s
readers(8) 3,281 256 256 13.98s 1.31s 1.29s
readers(13) 797,162 8,192 8,192 86m 7s 1m26s 1m26s
lastzero(5) 241 79 64 1.08s 0.38s 0.32s
lastzero(10) 53,198 7,204 3,328 4m47s 45.21s 27.61s
lastzero(15) 9,378,091 3,025,87 147,456 1,539m11s 55m 4s 30m13s
writes explores between 51 and 138 paths (with 85 as median value). The numbers we report (78
for “classic” DPOR and 8 for our algorithms) are very similar.
Performance on two synthetic benchmarks. Next, we compare the algorithms on two synthetic
benchmarks that expose differences between them. The first is the readers program in Figure 13.
The results, for 2, 8, and 13 readers, are shown in Table 4. For “classic” DPOR, the number of explored traces isO(3N) here, while source- and optimal-DPOR only explore 2N traces. Both numbers
are exponential in N but, as can be seen in the table, for, for example, N = 13 both source-DPOR and
optimal-DPOR finish in about one and a half minutes, while the DPOR algorithm with the sleep
set extension (Flanagan and Godefroid 2005a) explores two orders of magnitude more (mostly
sleep-set-blocked) traces and needs almost one and a half hours to complete.
The second benchmark is the lastzero(N) program presented earlier in Figure 11. As can be seen
in Table 4, source-DPOR explores about twice as many traces as optimal-DPOR and, naturally,
even if it uses a cheaper test, takes almost twice as much time to complete.
Performance on real programs. Finally, we evaluate the algorithms on four Erlang applications.
The programs are as follows:
dialyzer. A parallel static code analyzer (Aronis and Sagonas 2012) included in the Erlang/
OTP distribution.
gproc. An extended process dictionary (https://github.com/uwiger/gproc).
poolboy. A worker pool factory (https://github.com/devinus/poolboy).
rushhour. A program that uses processes and ETS tables to solve the Rush Hour puzzle in
parallel.
The last program, rushhour, is complex but self-contained (917 lines of code). The first three programs, besides their code, call many modules from the Erlang libraries, which Concuerror also
Journal of the ACM, Vol. 64, No. 4, Article 25. Publication date: August 2017.
Source Sets: A Foundation for Optimal Dynamic Partial Order Reduction 25:45
Table 5. Performance of DPOR Algorithms on Four Real Programs
Traces Explored Time
Benchmark Classic Source Optimal Classic Source Optimal
dialyzer 12,436 3,600 3,600 14m46s 5m17s 5m46s
gproc 14,080 8,328 8,104 3m 3s 1m45s 1m57s
poolboy 6,018 3,120 2,680 3m 2s 1m28s 1m20s
rushhour 793,375 536,118 528,984 145m19s 101m55s 105m41s
Table 6. Memory Requirements (in MB) for Selected Benchmarks
filesystem(19) indexer(15) gproc rushhour
Classic 92.98 245.32 557.31 24.01
Source 66.07 165.23 480.96 24.01
Optimal 76.17 174.60 481.07 31.07
instruments. The total number of lines of instrumented code for testing the first three programs is
44, 596, 9, 446, and 79, 732, respectively.
Table 5 shows the results. Here, the performance differences are not as profound as in synthetic
benchmarks. Still, some general conclusions can be drawn:
• Both source-DPOR and optimal-DPOR explore less traces than “classic” (from 50% up to 3.5
times fewer) and require less time to do so (they run from 42% up to 2.65 times faster).
• Even in real programs, the number of sleep-set-blocked explorations is significant.
• Regarding the number of traces explored, source-DPOR is quite close to optimal but manages to completely avoid sleep-set-blocked executions in only one program (in dialyzer).
• Source-DPOR is faster overall but only slightly so compared to optimal-DPOR even though
it uses a cheaper test. In fact, its maximal performance difference percentage-wise from
optimal-DPOR is a bit less than 10% (in dialyzer again).
Although we do not include a full set of memory consumption measurements, we mention that
all algorithms have very similar, and quite low, memory needs. Table 6 shows numbers for gproc,
the real program that requires most memory, and for all benchmarks where the difference between
source and optimal is more than 1MB. From these numbers, it can also be deduced that the size
of the wakeup tree is small. In fact, the average size of the wakeup trees for these programs is
less than three nodes (it ranges from 2.14 to 2.38 nodes). In our experience, this holds more generally: In all practical applications we have tried, the space requirements of the optimal algorithm
are very moderate and the exponential worst-case memory requirements do not seem to manifest themselves in practice. Further evidence for this provides the fact that, so far at least, we
have not received any issue from a user reporting extensive memory consumption when running
Concuerror.
12 RELATED WORK
In early approaches to stateless model checking, it was observed that reduction was needed to
combat the explosion in number of explored interleavings. Consequently, since then, several reduction methods have been proposed, including partial order reduction and bounding techniques
such as bounding the depth (Godefroid 1997), the number of context switches, and the number of
preemptive context switches (Musuvathi and Qadeer 2007) or the number of delays that an otherwise deterministic scheduler is allowed (Emmi et al. 2011). Since early persistent set techniques
Journal of the ACM, Vol. 64, No. 4, Article 25. Publication date: August 2017.
25:46 P. A. Abdulla et al.
(Clarke et al. 1999; Godefroid 1996; Valmari 1991) relied on static analysis, sleep set techniques
were also used to dynamically prevent explorations from processes that would be provably redundant. However, it was observed that, although sleep sets are sufficient to prevent the complete
exploration of different but equivalent interleavings (Godefroid et al. 1995), additional techniques
were needed to reduce sleep-set-blocked exploration.
The dynamic partial order reduction algorithm of Flanagan and Godefroid (2005b) showed how
to construct persistent sets on-the-fly “by need,” leading to better reduction. Similar techniques
have been combined with dynamic symbolic execution, which is also known as concolic testing,
where new test runs are initiated in response to detected races by flipping these races using postponed sets (Sen and Agha 2007). Since then, several variants, improvements, and adaptations of
DPOR for stateless model checking (Lauterburg et al. 2010; Tasharofi et al. 2012) and concolic
testing (Saarikivi et al. 2012; Sen and Agha 2006) have appeared, all based on persistent sets. Our
source-DPOR and optimal-DPOR algorithms can be applied to all these contexts to provide increased or optimal reduction in the number of explored interleavings.
A related area isreachability testing, in which test executions of concurrent programs are steered
by the test harness. Lei and Carver (2006) present a technique for exploring all Mazurkiewicz traces
in a setting with a restricted set of primitives (message passing using FIFO channels and monitors)
for process interaction. The scheduling of new test executions explicitly pairs message transmissions with receptions and could potentially require significant memory, compared to the more
lightweight approach of stateless model checking. The technique of Lei and Carver guarantees
to avoid re-exploration of different but equivalent maximal executions (corresponding to Theorem 7.11) but reports blocked executions. Moreover, their technique requires a non-trivial amount
of memory for storing interleavings that are yet to be explored.
Kahlon et al. (2009) present a normal form for executions of concurrent programs and prove that
two different normal-form executions are not in the same Mazurkiewicz trace. This normal form
can be exploited by SAT- or SMT-based bounded model checkers, but it cannot be used by stateless
model checkers that enumerate the execution sequences by state-space exploration. Kähkönen
et al. (2012) and, very recently, Rodríguez et al. (2015) use unfoldings (McMillan 1995), which can
also obtain optimal reduction in number of interleavings. However, unfolding-based techniques
have significantly larger cost per test execution than DPOR-like techniques, and the technique
of Kähkönen et al. (2012) also needs an additional post-processing step for checking non-local
properties such as races and deadlocks. A technique for using transition-based partial order reduction for message-passing programs, without moving to an event-based formulation is to refine
the concept of dependency between transitions to that of conditional dependency (Godefroid 1996;
Godefroid and Pirottin 1993; Katz and Peled 1992).
Another line of work that can be used to test multi-threaded programs that take inputs from
unbounded domains are approaches based on generalized symbolic execution, starting with the
work of Khurshid et al. (2003), which took advantage of the partial order and symmetry reduction
capabilities of the model checker component of the Java PathFinder tool. Broadly speaking, most
early approaches based on symbolic execution do not achieve the reduction in explored traces that
DPOR techniques can offer and do not define any notion of optimality.
Recently, Huang (2015) characterized a generalization (i.e., weaker form) of Mazurkiewicz traces
based on a new criterion: the maximal causal model for a concurrent computation from a given
execution trace, a notion defined by Serbanuta et al. (2012). Maximal causality characterizes the
largest possible set of equivalent interleavings in each (weaker form of) Mazurkiewicz trace by
taking semantic information into consideration, namely the values of reads and writes. When
applied in stateless model checking, maximal causality enables exploration of the entire state
space of a concurrent program with respect to a given input with a provably minimal number of
Journal of the ACM, Vol. 64, No. 4, Article 25. Publication date: August 2017.
Source Sets: A Foundation for Optimal Dynamic Partial Order Reduction 25:47
executions. The corresponding algorithm, called Maximal Causality Reduction (MCR), relies on an
offline constraint analyzer to formulate constraints that are then solved using an SMT solver. Its
implementation, which is not publicly available, was compared against an implementation of the
original DPOR algorithm with iterative context bounding (Musuvathi and Qadeer 2007) and was
found to outperform it in most cases. In view of this result, it would be interesting to compare MCR,
both theoretically and experimentally, with a better DPOR algorithm, such as optimal-DPOR, at
some point.
13 CONCLUDING REMARKS
We have presented a new theoretical foundation for partial order reduction, based on source sets,
and two new algorithms for dynamic partial order reduction, called source-DPOR and optimalDPOR. The latter algorithm, which combines source sets with a novel mechanism called wakeup
trees to achieve optimality, is the first DPOR algorithm to be provably optimal in the sense that
it is guaranteed both to completely explore the minimal number of executions and avoid even
initiating executions that lead to sleep set blocking. As shown in the experimental evaluation, the
extra overhead of maintaining wakeup trees is very moderate in practice (never more than 10% in
our experiments), which is a good tradeoff for having an optimality guarantee and the possibility
to run arbitrarily (exponentially) faster than other DPOR algorithms. However, as also shown in
this article, wakeup trees can be exponential in size in situations where an exponential number
of executions needs to be explored by the optimal algorithm. In such situations, the source-DPOR
algorithm, which maintains less information than the optimal algorithm, could be used as a fallback
in a tool that implements both algorithms. Another reason to prefer the source-DPOR algorithm
is its implementation simplicity; indeed, one only needs to substitute persistent sets with source
sets in an implementation of the original DPOR algorithm of Flanagan and Godefroid (2005b) to
obtain an implementation of source-DPOR.
Due to its implementation simplicity, we have chosen the source-DPOR algorithm to be the
core algorithm used in Nidhugg, a stateless model checking tool for C/pthreads programs under
sequential consistency and also under relaxed memory models, and achieved performance that
outperforms state-of-the-art tools of similar capabilities (Abdulla et al. 2015). Recently, we have
applied Nidhugg to systematically test various flavors of the Read-Copy-Update (RCU) synchronization mechanism of the Linux kernel. We have been able to reproduce, within seconds, various
safety and liveness bugs that have been reported for Tree RCU’s implementation and have verified the Grace-Period guarantee, the basic guarantee that RCU offers, on non-preemptible builds of
several Linux kernel versions. The source-DPOR algorithm has been instrumental in verifying this
property in reasonable time. For more information, refer to Kokologiannakis and Sagonas (2017).
We intend to further explore the ideas behind source sets and wakeup trees, not only for verification but also for new ways of testing concurrent programs.