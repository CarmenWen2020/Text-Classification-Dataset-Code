We study coding schemes for multiparty interactive communication over synchronous networks that suffer
from stochastic noise, where each bit is independently flipped with probability ε. We analyze the minimal
overhead that must be added by the coding scheme to succeed in performing the computation despite the
noise.
Our main result is a lower bound on the communication of any noise-resilient protocol over a synchronous
star network with n parties (where all parties communicate in every round). Specifically, we show a task that
can be solved by communicating T bits over the noise-free network, but for which any protocol with success
probability of 1 − o(1) must communicate at least Ω(T log n
log log n ) bits when the channels are noisy. By a 1994
result of Rajagopalan and Schulman, the slowdown we prove is the highest one can obtain on any topology,
up to a log logn factor.
We complete our lower bound with a matching coding scheme that achieves the same overhead; thus, the
capacity of (synchronous) star networks is Θ(log logn/ logn). Our bounds prove that, despite several previous
coding schemes with rate Ω(1) for certain topologies, no coding scheme with constant rate Ω(1) exists for
arbitrary n-party noisy networks.
CCS Concepts: • Mathematics of computing → Coding theory; • Theory of computation → Communication complexity; Interactive computation;
Additional Key Words and Phrases: Multiparty interactive communication, coding theory, star network, communication complexity, lower bounds, random noise
1 INTRODUCTION
Assume a network of n remote parties who perform a distributed computation of some function
of their private inputs, while their communication may suffer from stochastic noise. The task of
coding for interactive communication asks for coding schemes that allow the parties to correctly
compute the needed function while limiting the overhead incurred by the coding. For the twoparty case, n = 2, Schulman, in a pioneering line of results [37–39], showed how to convert any
protocol that takes T rounds when the communication is noiseless into a resilient protocol that
succeeds with high probability and takesO(T ) rounds when the communication channel is a binary
symmetric channel (BSC),1 that is, when the communication may suffer from random noise.
For the general case of n parties, Rajagopalan and Schulman [36] showed a coding scheme that
succeeds with high probability and takes O(T logn) rounds in the worst case. Here, a “round”
means simultaneous communication of a single bit over each one of the channels. More precisely,
the communication of the coding scheme in Reference [36] depends on the specific way the parties
are connected to each other. Specifically, the scheme takes O(T log(d + 1)) rounds, where d is the
maximal number of neighbors a party may have. Thus, for certain topologies like a line or a cycle,
the slowdown is constant O(1), however, in the worst case, i.e., when the topology is a complete
graph, the scheme has a slowdown of O(logn).
The work of Alon et al. [2] shows how to improve the O(logn) slowdown when the network’s
topology is a complete graph. Specifically, they provide a coding scheme with high probability
of success and slowdown of O(1) for a rich family of “highly connected” topologies, including
the complete graph. Therefore, a constant-slowdown coding scheme is achievable either when the
degree is constant [36], or when the connectivity is high [2], i.e., when many disjoint paths connect
every two parties.
The main outstanding open question left by these works is whether a constant-slowdown coding
scheme can be obtained for all topologies. We answer this question in the negative and show a
lower bound on the slowdown of any coding scheme with high probability of success, over a star
network:
Theorem 1.1 (Main, Lower Bound). Assume n parties connected as a star, and let ε < 1/2 be
given. There exists an n-party protocol χ that takes T rounds assuming noiseless channels, such that
any coding scheme that simulates χ with probability above 1/5 when each channel is a BSCε , takes
Ω(T log n
log log n ) rounds.
By making “simulating χ” (i.e., computing the transcript generated by χ, assuming noiseless channels) the interactive task to be performed, Theorem 1.1 implies the Ω( log n
log log n ) slowdown in interactive coding. Note that the coding of Reference [36] implies a slowdown of O(logn) in this case,
hence, our result is tight up to an O(log logn) term.
We complement our lower bound with a matching upper bound and show that coding with a
slowdown ofO( log n
log log n ) is achievable and therefore tight for interactive coding over a star topology.
Theorem 1.2 (Upper Bound). Assume n parties connected as a star, and let ε < 1/2. For any nparty protocol χ that takes T rounds assuming noiseless channels, there exists a coding scheme that
simulates χ assuming each channel is a BSCε , takes N = O(T log n
log log n ) rounds, and succeeds with
probability 1 − 2−Ω(N )
.
1The BSC channel, parametrized by a probability ε , flips each bit independently with probability ε , and leaves the bit
unflipped with probability 1 − ε .
Journal of the ACM, Vol. 65, No. 1, Article 4. Publication date: December 2017.
Constant-Rate Coding for Multiparty Interactive Communication Is Impossible 4:3
The upper bound follows quite straightforwardly from an observation by Alon et al. [2], showing
that as long as one round of the noiseless χ can be simulated with high probability, then the entire
protocol χ can be simulated with high probability by employing the techniques of Reference [36].
Over a star, it is quite simple to simulate log logn rounds of an arbitrary noiseless χ using only
O(logn) noisy rounds, with high probability. Thus, we can apply the technique of References [2, 36]
on segments of log logn rounds of χ, and achieve the stated coding scheme. We prove Theorem 1.2
in Section 4.
We devote Section 5 to prove the more involved lower bound of Theorem 1.1. Below we give
a rather intuitive overview of our lower bound result (focusing on the binary case) and the techniques we use.
1.1 Lower Bound: Overview and Techniques
To achieve our lower bound of Ω( log n
log log n ) on the slowdown, we consider protocols for the pointer
jumping task of depthT , between n parties (also called clients) and the center of the star (also called
the server). In the pointer jumping task, each client gets as an input a binary tree of depthT , where
each edge is labeled with a single bit. The server’s input is a 2n-ary tree of depthT where each edge
is labeled with an n-bit string. Solving the pointer jumping task is equivalent to performing the
following protocol: all parties begin from the root of their trees. At each round, simultaneously for
1 ≤ i ≤ n, the i-th client receives a bit bi from the center and descends in his tree to the bi-th child
of its current node. The client then sends back to the server the label of the edge through which it
traversed. The server receives, at each round, the string B = b1 ··· bn from the clients and descends
to the B-th child of its current node. If the edge going to that node is labeled with the n-bit string
b
1 ··· b
n, then the server sends b
i to the i-th client. The above process repeats until the parties
reach the T -th level in their respective tree. At the end, each party outputs the leaf it has reached
(equivalently, it outputs the “path” it traversed). Note that the T -level pointer jumping task can
be solved using 2T rounds of alternating noiseless communication. By alternating we mean here
that the server speaks on, say, odd rounds, while the clients speak on even rounds. Also note that
the pointer jumping task is complete for interactive communication, i.e., any interactive protocol
for n + 1 parties connected as a star can be represented as a specific input-instance of the above
pointer jumping task. See Section 3 for further details about the multiparty pointer jumping task.
Next, we assume the channels are noisy. In fact, we can weaken the noise model and assume
that the noise erases bits rather than flipping them, that is, we consider the binary erasure channel,
BECε ; see Definition 2.1. Note that since the considered noise model is weaker, our lower bound
becomes stronger.
Consider any protocol that solves the pointer jumping task of depth T , assuming the channels
are BEC1/3. We divide the protocol into segments of length 0.1 logn rounds each and show that
at each such segment the protocol “advances” by at most O(log logn) levels in the underlying
pointer jumping task, in expectation. Very roughly, the reason for this slow progress follows from
the observation that during each segment of 0.1 logn rounds, with high probability there exists a
set of √
n clients whose communication was completely erased. It follows that the server is missing
knowledge on √
n parties and thus cannot infer its next node with high probability. On average, the
server sends a very small amount of information on the labels descending from its current node that
belong to the “correct” path. As a result, the clients practically receive no meaningful information
on the next level(s) of the server. This, in turn, limits the amount of information they can send on
their “correct” paths to O(log logn) bits in expectation, thus limiting the maximal advancement in
the underlying pointer jumping task. For instance, if some client who does not know the correct
path in his input pointer-jumping tree communicates to the server all the labels descending from
Journal of the ACM, Vol. 65, No. 1, Article 4. Publication date: December 2017.   
4:4 M. Braverman et al.
its current node, say in a breadth-first manner, the information sent during 0.1 logn rounds can
contain, at most, O(log logn) levels of this client’s correct path.
Not surprisingly, the technical execution of the above strategy requires tools for careful and
accurate bookkeeping of the information the parties have learned at any given time of the (noisy)
execution. The basic definition of information a party has about a random variable X sampled
from a space ΩX we employ is I (X) def
= log |ΩX | − H(X), where H(X) is Shannon’s entropy of X
given the party’s current knowledge. Note that if a priori X is uniformly distributed, I (X) is exactly
the mutual information between what the party knows and X. However, this information notion
behaves more nicely under conditioning (i.e., when changing what the party knows about X as the
protocol progresses), and seems generally easier to work with. Indeed, this notion was previously
used when bounding the information in pointer jumping tasks [29, 33, 35].
A central notion in our analysis is the cutoff round of the protocol, which relates to the deepest
level of the underlying pointer jumping task that the parties can infer from the communication
they have received so far. Very roughly, if the cutoff is k, then parties have small information
on labels below level k in the underlying tree of the party (or parties) connected to them. More
precisely, for any (partial) transcript π the parties observe, we define cutoff(π ) to be the minimal
round 1 ≤ k ≤ T for which the parties have a small amount of information about labels in the
underlying pointer jumping task that lies in the subtree rooted at the end of the correct path of
depth k, conditioned on the transcript π and on the correct path up to level k (see Definition 5.2
for the exact formulation).
The core of our analysis shows that, given a certain cutoff, cutoff(π ) = , and assuming the
parties communicate the next 0.1 logn rounds of the protocol (denote the observed transcript in
this new part as Πnew ), then in expectation over the possible inputs, noise, and randomness of the
protocol, the cutoff does not increase by more than O(log logn); that is,
E[cutoff(π, Πnew ) | cutoff(π ) = ] ≤  + O(log logn).
This implies that, unless the protocol runs for Ω(T log n
log log n ) rounds, then the cutoff at the end of
the protocol is substantially smaller than T , with high probability. Using Fano’s inequality, this, in
turn, implies that the protocol cannot output the correct path (beyond the cutoff round) with high
probability.
Bounding the information revealed by the parties at each step is the deepest technical contribution of this paper, and is done in methods which are close in spirit to a technique by Kol and
Raz [29] for obtaining lower bounds in the two-party case.2 We bound separately the information
that the server reveals and the information the clients reveal in each segment of 0.1 logn rounds
(conditioned on a given cutoff level, i.e., on the transcript of the protocol so far and on the correct
path up to the cutoff level).
Very informally, we show that the information revealed during a single chunk on labels below
a continuation of the correct path (i.e., the information captured by the “new” cutoff), can be
bounded by the product of (i) the probability to guess the continuation of the correct path (between
the current and the new cutoff levels) and (ii) the information that the transcript so far contains
on all the labels (either on the correct path or not) that lie below the new cutoff level. Indeed,
if a party wants to give information about the labels of its correct path, but that party doesn’t
know the correct path, it can’t do much more than guess the path and send information about that
guess; alternatively, it can give information on labels in all possible paths, where the amount of
information of each label corresponds to the probability of this label to be part of the correct path.
2In fact, it is an interesting question whether our techniques can be used to simplify the analysis in Reference [29].
Journal of the ACM, Vol. 65, No. 1, Article 4. Publication date: December 2017.   
Constant-Rate Coding for Multiparty Interactive Communication Is Impossible 4:5
We bound each one of the above terms separately. For the first part (i), we bound the guessing
probability of a continuation of the correct path as a function of the information the observed
transcript contains on the labels below the current cutoff in the tree of the other parties. For instance, guessing the correct path in the server’s tree depends on the amount of information the
transcript gives on labels along the correct path in the clients’ trees, at the same levels (because
these labels exactly determine the path the server should take in his tree). The definition of the
cutoff and the fact that these levels lie below the cutoff level, give a bound the amount of information we have on these labels, which can be translated to a bound on the probability of guessing
the corresponding path. Fano’s inequality is not strong enough for our needs (i.e., sub-exponential
guessing probability from sub-exponentially small information), and we devise a tighter bound
via a careful analysis of the positive and negative parts of the Kullback–Leibler divergence; see
Lemma 2.15. This (entropy vs. min-entropy) relation may be of independent interest.
To bound the second part (ii), we observe that the information on labels below the current cutoff
is bounded in expectation using the definition of the cutoff, up to possibly additional 0.1n logn bits
that were communicated during the new segment of 0.1 logn rounds.
The fact that the bound of part (ii) works only in expectation is a major hurdle, because it prevents us from bounding the above product directly (these two multiplicands are dependent!). We
detour around this issue by narrowing down the probability space by conditioning on additional
information that makes the two multiplicands independent. However, conditioning on additional
information potentially increases the information we wish to bound, then it is essential to carefully
limit the amount of additional information we condition on, so that the bound remains meaningful. Giving more details (yet still very intuitively speaking), we condition on all the labels that lie
between the old and new cutoff levels, of either the server’s input or the clients’ input, according
to the specific information we are currently bounding. This conditioning takes out the dependency caused by the interaction (since the labels of one side are fixed up to some given level) and
makes the labels below the new cutoff independent of labels above it; specifically, the correct path
between the current and the new cutoff (which is involved in the first multiplicand) is conditionally independent of the labels below the new cutoff (which are involved in the second one). This
independence allows us to bound the expectation of the above product by bounding each term
separately as described above.
1.2 Related Work
As mentioned above, coding for interactive communication in the presence of random noise was
initiated by Schulman for the two-party case [37–39]. The coding scheme of Schulman [38, 39]
achieves slowdown of O(1) and exponentially high success probability; however, it is not computationally efficient and can take exponential time in the worst case. Gelles, Moitra, and Sahai [20,
21] showed how to obtain an efficient coding scheme while maintaining a constant slowdown and
exponentially high success probability. Braverman [4] gave another efficient coding scheme, yet
with a slightly reduced success probability. Other related work in the two-party setting considers
the case of adversarial noise rather than random noise, in various settings [1, 3, 5, 7, 8, 10, 12, 14,
18, 22–24]; see [16] for a survey.
In the two-party setting, the minimal possible slowdown over a BSCε as a function of the noise
parameter ε, was initially considered by Kol and Raz [29], who showed a lower bound of 1 +
Ω(
ε log 1/ε) on the slowdown. Later, Haeupler [26] showed that the order in which the parties
are speaking affects the slowdown, and if the parties are assumed to be alternating, a slowdown of
1 + O(
√
ε) is achievable. When the noise is adversarial rather than random, the slowdown increases
to 1 + O(

ε log log 1/ε) [26]. The slowdown in other types of channels, such as the binary erasure
Journal of the ACM, Vol. 65, No. 1, Article 4. Publication date: December 2017.  
4:6 M. Braverman et al.
channel BECε or channels with noiseless feedback, was considered by Gelles and Haeupler [17],
who showed efficient coding schemes with an optimal slowdown of 1 + Θ(ε log 1/ε) over these
channels.
As for the multiparty case, the work of Rajagopalan and Schulman [36] was the first to give a
coding scheme for the case of random noise over arbitrary topology, with a slowdown ofO(log(d +
1)) for d, the maximal degree of the connectivity graph. As in the two-party case, that scheme is
not efficient, but can be made efficient using References [20, 21]. Alon, Braverman, Efremenko,
Gelles, and Haeupler [2] considered coding schemes over d-regular graphs with mixing time3 m,
and obtained a slowdown ofO(m3 logm). This implies a coding scheme with a constant slowdown
O(1) whenever the mixing time is constant, m = O(1), e.g., over complete graphs.
For the case of adversarial noise in the multiparty setting, Jain, Kalai, and Lewko [28] showed an
asynchronous coding scheme for star topologies with slowdown O(1) for up to O(1/n)-fraction of
noise. A communication-balanced version of that scheme was given by Lewko and Vitercik [31].
Hoza and Schulman [27] showed a coding scheme in the synchronous model that works for any
topology, tolerates O(1/n)-fraction of noise, and demonstrates a slowdown of O( m
n logn) where
m here is the number of edges in the given connectivity graph.
Finally, we mention the work of Gallager [15]. This article assumes a different setting than
the above works, namely, the case where parties are all connected via a noisy broadcast channel
(the noisy blackboard model [13]). Gallager showed that a slowdown of O(log logn) is achievable
for the task where each party begins with a bit and needs to output the input bits of all other
parties. Goyal, Kindler, and Saks [25] showed that this slowdown is tight by providing a matching
slowdown of Ω(log logn) for the same task in the noisy broadcast model. It is not clear whether
there is a direct connection between results in these two models—there does not seem to be a way
to translate results in either direction.
In a subsequent work, Gelles and Kalai [19] revisit the question of slowdown in the multiparty
case, and relax the assumption taken in References [2, 36] as well as in this article—that at each
round all the parties must send a bit to each one of their neighbors. Using the machinery we develop
in this article, Gelles and Kalai show a communication slowdown of Ω(logn) for interactive coding
over cycle graphs if parties are not required to communicate at each and every round. This is
somewhat surprising in light of the constant upper bound on the slowdown implied by Reference
[36] for graphs with a constant degree, such as the cycle whose degree is d = 2.
2 PRELIMINARIES
For n ∈ N we denote by [n] the set {1, 2,... ,n}. The log() function is taken to base 2. We denote
the natural logarithm by ln(). If X is a random variable with distribution PX , we write Ex∼X [f (x)]
(or simply Ex [f (x)]) to denote the expectation over the distribution of X, namely, Ex∼X [f (x)] def
=
x PX (x)f (x). For an event E, we let Ex∼X |E[f (x)] def
=
x PX |E (x)f (x) be the expectation over
the conditional distribution PX |E.
2.1 Coding Over Noisy Networks
Given an undirected graphG = (V, E) we assume a network with |V | parties, where u,v ∈ V share
a communication channel if (u,v) ∈ E. In the case of a noisy network, each such link is assumed
to be a BSCε or a BECε .
3Intuitively speaking, the mixing time of a graph is the minimal number of steps a random walk needs to end up at every
node with approximately equal probability.
Journal of the ACM, Vol. 65, No. 1, Article 4. Publication date: December 2017.  
Constant-Rate Coding for Multiparty Interactive Communication Is Impossible 4:7
Definition 2.1 (Channels [11]). For ε ∈ [0, 1] we define the binary symmetric channel BSCε :
{0, 1}→{0, 1} in which the input bit is flipped with probability ε, and remains the same with
probability 1 − ε. The binary erasure channel BECε : {0, 1}→{0, 1, ⊥} turns each input bit into an
erasure mark ⊥ with probability ε, or otherwise keeps the bit intact. When a channel is accessed
multiple times, each instance is independent.
A round of communication in the network means the simultaneous transmission of 2|E| messages: for any (u,v) ∈ E, u sends a bit tov and receives a bit fromv. A protocol for an n-party function f (x1,... , xn ) = (y1,... ,yn ) is a distributed algorithm between n parties {p1,...,pn }, where
each pi begins the protocol with an input xi , and after N rounds of communication outputs yi . The
communication complexity of a protocol, CC(), is the number of bits sent throughout the protocol. Note that given any network G, the round complexity of a protocol and its communication
complexity differ by a factor of 2|E|.
Assume χ is a protocol over a noiseless network G. We say that a protocol χ  simulates χ over a
channel C with rate R if, when χ  is run with inputs (x1,..., xn ) over the network G where each
communication channel is C, each party outputs with high probability its respective transcript
in the execution of χ on the inputs (x1,..., xn ) and it holds that CC(χ )/CC(χ
) = R. Note that
given a transcript of χ, the parties can compute the output values χ (x1,..., xn ). We also use the
term slowdown to denote the inverse of the rate, R−1, that is, the (multiplicative) increase in the
communication due to the coding.
2.2 Information, Entropy, and Min-Entropy
Throughout, we will use UΩ to denote a random variable uniformly distributed over the finite and
discrete domain Ω. In particular, Un denotes a random variable uniformly distributed over {0, 1}
n.
Definition 2.2 (Information). Let X be a random variable over a finite discrete domain Ω. The
information of X is given by
I (X) def
= log |Ω| − H(X),
where H(X) is the Shannon entropy of X, H(X) =
x ∈Ω Pr(X = x) log(1/ Pr(X = x)).
Given a random variable Y, the conditional information of X given Y is
I (X | Y ) def
= log |Ω| − H(X | Y )
= EyI (X | Y = y).
Also note that I (X) = D(X 	UΩ) where D(	) is the Kullback-Leibler divergence (Definition 2.7).
Lemma 2.3 (Superadditivity of Information). Let X1,...,Xn be n random variables. Then,
n
i=1
I (Xi ) ≤ I (X1,...,Xn ).
The equality is satisfied when X1,...,Xn are mutually independent.
Proof. Using the subadditivity of the entropy function [11], we get
n
i=1
I (Xi ) =

i
(log |Ωi | − H(Xi )) ≤ log


i
|Ωi |


− H(X1,...,Xn ) = I (X1,...,Xn ).
Lemma 2.4. Let X,Y be random variables over the finite discrete domains ΩX and ΩY , respectively.
Then,
Journal of the ACM, Vol. 65, No. 1, Article 4. Publication date: December 2017.       
4:8 M. Braverman et al.
(1) I (X | Y ) = I (X) + I (X;Y )
(2) I (X | Y ) ≤ I (X) + log |ΩY |
(3) I (X | Y ) ≤ I (X,Y ),
where I (X;Y ) = H(X) + H(Y ) − H(X,Y ) is the mutual information between X and Y (not to be confused with I (X,Y ) = log |ΩX | + log |ΩY | − H(X,Y )).
Proof. We prove the three claims by order,
(1) I (X | Y ) = log |ΩX | − H(X | Y )
= log |ΩX | − H(X) + H(Y ) − H(Y | X)
= I (X) + I (X;Y ).
(2) Follows from (1) and the fact that I (X;Y ) ≤ log |ΩY |.
(3) I (X,Y ) = log |ΩX | + log |ΩY | − H(X,Y )
≥ log |ΩX | + H(Y ) − (H(Y ) + H(X | Y ))
= I (X | Y ).
Definition 2.5 (Min-entropy). Let X be a random variable over a discrete domain Ω. The minentropy of X is given by
H∞(X) = log(1/pmax (X)).
pmax (X) is the probability of the most probable value of X, i.e., pmax (X) def
= maxx ∈Ω Pr(X = x). At
times, pmax is called the guessing probability of X.
We relate information (or, entropy) with the guessing probability (or min-entropy) via the next
Lemma, which is a special case of Fano’s inequality (see, e.g., Reference [11]).
Lemma 2.6. Let X be a random variable over a discrete finite domain Ω. It holds that
I (X) ≥ pmax (X) log(|Ω|) − h(pmax (X)),
where h(x) = −x log x − (1 − x) log(1 − x) is the binary entropy.
Proof. The lemma is an immediate corollary of the following version of Fano’s inequality,
H(X) ≤ log |Ω|(1 − 2−H∞ (X )
) + h(2−H∞ (X )
). (1)
Let us prove Equation (1). Assume without loss of generality that Ω = {1,... ,n}. Let pi =
Pr(X = i), and again assume without loss of generality that for any i < j, it holds that pi ≥ pj .
Thus, pmax (X) = p1. If p1 = 1, the claim is trivial. Otherwise,
H(X) = p1 log 1
p1
+
n
i=2
pi log 1
pi
. (2)
Define Y to have the same distribution of X conditioned on X  1, i.e., Pr(Y = 1) = 0 and
Pr(Y = i) = pi/(1 − p1) for i ∈ {2,...,n}. Note that
H(Y ) =
n
i=2
pi
1 − p1
log 1 − p1
pi
=

n
i=2
pi
1 − p1
log 1
pi


− log 1
1 − p1
.
Going back to Equation (2), we have
H(X) = p1 log 1
p1
+ (1 − p1)H(Y ) + (1 − p1) log 1
1 − p1
≤ h(p1) + (1 − p1) log |Ω|,
Journal of the ACM, Vol. 65, No. 1, Article 4. Publication date: December 2017.    
Constant-Rate Coding for Multiparty Interactive Communication Is Impossible 4:9
which holds because H(Y ) ≤ log(|Ω| − 1) < log |Ω|. Then Equation (1) and the lemma follow by
substituting p1 = pmax (X) = 2−H∞ (X )
.
We note here that similar claims to the above lemmas hold when we additionally condition on
some event E; indeed, one can apply these lemmas on the random variable (X | E).
Another key tool we use is the Kullback-Leibler divergence.
Definition 2.7 (KL Divergence [30]). Let X,Y be random variables over a discrete domain Ω. The
KL divergence of X and Y is
D(X 	Y ) def
=

ω∈Ω
Pr(X = ω) log 
Pr(X = ω)
Pr(Y = ω)

.
Define Ω+ = {ω ∈ Ω : Pr(X = ω) > Pr(Y = ω)} and Ω− = Ω\Ω+. We can split the KL divergence
into its positive and negative parts,
D(X 	Y ) = D+(X 	Y ) − D−(X 	Y ),
where D+(X 	Y ) =
ω∈Ω+ Pr(X = ω) log(
Pr(X=ω)
Pr(Y=ω) ) and D−(X 	Y ) = −
ω∈Ω− Pr(X = ω)
log(
Pr(X=ω)
Pr(Y=ω) ).
Lemma 2.8. Let X,Y be random variables over a discrete domain Ω. Then, for every Ω ⊆ Ω it
holds that

ω∈Ω
Pr(X = ω) log 
Pr(X = ω)
Pr(Y = ω)

≤ D+(X 	Y ).
Proof. Immediate from the definition of D+(·	·).
Lemma 2.9 (Pinsker Ineqality [34]). Let X,Y be random variables over a discrete domain Ω,
then
	X − Y 	2 ≤ 2 ln(2) · D(X 	Y ),
where 	X − Y 	 =
ω∈Ω | Pr(X = ω) − Pr(Y = ω)|.
We now upper bound the negative part of the KL divergence. Note that one can easily show
that D−(X 	Y ) ≤ 1, but we will need a better upper bound that applies when D−(X 	Y )  1.
Lemma 2.10.
D−(X 	Y ) ≤
 2
ln(2)
D(X 	Y ).
Proof. For every ω ∈ Ω, let pω
def
= Pr(X = ω) and qω
def
= Pr(Y = ω). We can relate any negative
term of the divergence with a difference of probabilities via the following claim:
Claim 2.11. For pω ≤ qω it holds that ln(2)pω log qω
pω ≤ qω − pω.
Proof. Note that the equality holds for pω = qω. If we take the derivative with respect to qω,
the LHS is pω
qω and the RHS is 1. Since pω
qω ≤ 1 when pω ≤ qω, the claim holds.
Note that by definition, D−(X 	Y ) =
ω: pω ≤qω pω log qω
pω . From the claim above it holds that
D−(X 	Y ) ≤ 1
ln(2) 	X − Y 	. The lemma then follows from Pinsker’s inequality (Lemma 2.9).
Journal of the ACM, Vol. 65, No. 1, Article 4. Publication date: December 2017.          
4:10 M. Braverman et al.
2.3 Technical Lemmas
We now prove several technical lemmas which we will use for our lower bound proof. Their operational meaning will be explained in Section 5.
Lemma 2.12. Let Z,D,X1,...,Xn be random variables. Let f : Z → [n] be some function. Suppose
that, conditioned on D = d, Z and (X1,...,Xn ) are independent. Denote the guessing probability
pmax (f (Z) | D = d) = 2−H∞ (f (Z )|D=d)
, then
Ez∼Z |D=d I (Xf (Z ) | D = d,Z = z) ≤ pmax (f (Z) | D = d) · I (X1,...,Xn | D = d).
Proof.
Ez∼Z |D=d I (Xf (Z ) | D = d,Z = z) =

z
Pr(Z = z | D = d)I (Xf (z) | D = d,Z = z)
=
n
i=1




z:f (z)=i
Pr(Z = z | D = d)



I (Xi | D = d)
=
n
i=1
Pr(f (Z) = i | D = d)I (Xi | D = d)
≤
n
i=1
pmax (f (Z) | D = d) · I (Xi | D = d)
≤ pmax (f (Z) | D = d) · I (X1,...,Xn |D = d).
The second line follows since Z and (X1,...,Xn ) are independent conditioned on D = d, by grouping together terms with the same f (Z) value. The last inequality follows from the super-additivity
of information (Lemma 2.3).
Lemma 2.13. Let X1,...,Xn ≥ 0 and Y1,...,Yn ≥ 0 be random variables, with expectations μi =
E[Xi] and ξi = E[Yi], and assume that n
i=1 μi ≤ C1 and n
i=1 ξi ≤ C2, for some constants C1,C2.
Set M(t1,t2) = argmini{(Xi < t1) ∧ (Yi < t2)} to be the minimal index i for which both Xi < t1 and
Yi < t2. Then,
E[M(t1,t2)] ≤ 1 +
C1
t1
+
C2
t2
.
Proof.
E[M(t1,t2)] =
n
i=1
Pr[M(t1,t2) ≥ i]
≤ 1 +
n
i=1
Pr[M(t1,t2) > i]
= 1 +
n
i=1
Pr[(X1 ≥ t1 ∨ Y1 ≥ t2) ∧···∧ (Xi ≥ t1 ∨ Yi ≥ t2)]
Journal of the ACM, Vol. 65, No. 1, Article 4. Publication date: December 2017.     
Constant-Rate Coding for Multiparty Interactive Communication Is Impossible 4:11
≤ 1 +
n
i=1
Pr[Xi ≥ t1 ∨ Yi ≥ t2]
≤ 1 +
n
i=1
(Pr[Xi ≥ t1] + Pr[Yi ≥ t2])
≤ 1 +
n
i=1

μi
t1
+ ξi
t2

≤ 1 +
C1
t1
+
C2
t2
,
where the penultimate inequality is due Markov’s inequality.
Lemma 2.14. Let T be a set of binary random variables, ordered as a tree of depth n. For any fixed
path P of depth i ≤ n starting from the root of T , let T [P] be the set of variables along that path, and
let pmax (T [P]) = 2−H∞ (T [P]) be the maximal probability that some assignment toT [P] can obtain. For
any i ≤ n define
pmax (i) = max P s.t. |P |=i
	
pmax (T [P])


.
Then for any t ≥ 6 it holds that
n
i=t
pmax (i) < 2I (T ) + 4

I (T ) + 20 · 2−t /3
.
This lemma is an immediate corollary of the following stronger Lemma 2.15, that proves a similar
claim when considering any subset S of n random variables of arbitrary size |Σ|. In particular, for
the special case of Lemma 2.14, the random variables are binary |Σ| = 2, and the subset S contains
variables along a single path inT (note that the parameter n in the above lemma corresponds to |S |
of Lemma 2.15).
Lemma 2.15. Let B = (B1,..., Bn ) be a sequence of n discrete random variables, where Bi ∈ Σ. For
any S ⊆ [n] we let B(S) def
= {Bi | i ∈ S} be the variables indexed by S. Let pmax (B(S)) = 2−H∞ (B(S )) be
the maximal probability that B(S) can attain. For 1 ≤ i ≤ n, let
pmax (i) def
= max
|S |=i pmax (B(S)).
Then it holds that for any t ≥ 2e
log |Σ|
,
n
i=t
pmax (i) < 2I (B) + 4

I (B) + 20 · |Σ|
−t /3
.
Proof. Let Σ be a fixed finite set. For any given string a ∈ Σn we let νa
def
= Pr[B = a] the probability that B attains the value a.
For any 1 ≤ i ≤ n, consider pmax (i), and fix Si ⊂ [n] of size |Si | = i and βi = b1b2 ··· bi ∈ Σi to be
the specific values for which Pr[B(Si ) = βi] = pmax (i), i.e., the certain subset of size i of variables in
B and their assignments that are maximal; we know that at least one such subset and assignment
exists by the definition of pmax (i).
Journal of the ACM, Vol. 65, No. 1, Article 4. Publication date: December 2017.   
4:12 M. Braverman et al.
Define Vi
def
= {a ∈ Σn | a(Si ) = βi} to be all the strings a of length n over Σ whose restriction to
Si equals βi . Define
Wi
def
= Vi \



j>i
Vj



to be the set of all the strings a ∈ Σn such a(Si ) = βi , but for any j > i, a(Sj)  βj . Let wi
def
=
Pr(Wi ) =
a ∈Wi νa. Note that the sets W1,W2,... ,Wn are disjoint by definition, and that Vi ⊆
∪n
j=i
Wj . It is easy to verify that pmax (i) ≤ n
j=i wi . Then, we get
n
i=t
pmax (i) ≤
n
i=t
n
j=i
wj =
n
j=t
(j − t + 1) · wj ≤
n
j=t
j · wj . (3)
Next, we upper bound the term n
j=t jwj . Fix a specific j and consider the sum

a ∈Wj νa log(|Σ|
nνa ). This sum can be bounded by

a ∈Wj
νa log(|Σ|
nνa ) ≥



a ∈Wj
νa



log 
|Σ|
n

a ∈Wj νa
|Wj |

= wj log 
|Σ|
n
|Wj |
wj

≥ wj log(|Σ|
j
wj),
where the first inequality follows from the convexity of the function4 x log(cx), and the last inequality holds since |Wj |≤|Vj |≤|Σ|
n−j
. Therefore,
n
j=t

a ∈Wj
νa log (|Σ|
nνa ) ≥
n
j=t
wj log 
|Σ|
j
wj

= log |Σ|
n
j=t
jwj +
n
j=t
wj logwj . (4)
Claim 2.16. For any n,t, Σ such that |Σ|
−t /2 ≤ e−1, it holds that
n
j=t
wj log(1/wj) ≤ 10 log |Σ|·|Σ|
−t /3 + log |Σ|
2
n
j=t
jwj .
Proof. For any given j, split the sum to indices where wj < |Σ|
−j/2 and indices where wj ≥
|Σ|
−j/2: 
wj ≥ |Σ|
−j /2
wj log(1/wj) ≤ wj · log(|Σ|
j/2) ≤ j log(|Σ|)
2 wj
and, assuming |Σ|
−j/2 < e−1, the function x log(1/x) is increasing on (0, e−1), thus

wj ≤ |Σ|
−j /2
wj log(1/wj) ≤ |Σ|
−j/2 log(|Σ|
j/2) ≤ j log |Σ|
2|Σ|
j/2 .
Combining the above, we get
n
j=t
wj log(1/wj) ≤ log |Σ|
2
n
j=t
jwj + log |Σ|
2
n
j=t
j
|Σ|
j/2
≤ log |Σ|
2
n
j=t
jwj + 10 log |Σ|·|Σ|
−t /3
,
where the last inequality is a crude bound that follows from the fact that |Σ| ≥ 2, and that the
infinite sum ∞
j=t
j
x j for any x > 1 converges to (t(
√
x−1)+1)x1/2−t /2
(
√
x−1)2 .
4Recall that any convex function f satisfies f (x1 )+···+f (xn )
n ≥ f (
x1+···+xn
n ).
Journal of the ACM, Vol. 65, No. 1, Article 4. Publication date: December 2017.            
Constant-Rate Coding for Multiparty Interactive Communication Is Impossible 4:13
Continuing with Equation (4), it follows from Claim 2.16 that
n
j=t

a ∈Wj
νa log (|Σ|
nνa ) ≥ log |Σ|
n
j=t
jwj +
n
j=t
wj logwj
≥ log |Σ|
n
j=t
jwj −


10 log |Σ|·|Σ|
−t /3 + log |Σ|
2
n
j=t
jwj



≥ log |Σ|
2
n
j=t
jwj − 10 log |Σ|·|Σ|
−t /3
.
Rearranging the above we conclude that
n
j=t
jwj ≤
2
log |Σ|
n
j=t

a ∈Wj
νa log(|Σ|
nνa ) + 20 · |Σ|
−t /3
. (5)
Denote by UΣ the random variable sampled uniformly from Σ, and let U ⊗n
Σ be n independent
instances of UΣ. Note that D(B	U ⊗n
Σ ) = I (B) by definition. Furthermore, since the Wj are disjoint,
we have by Lemma 2.8 (and Definition 2.7) that
n
j=t

a ∈Wj
νa log(|Σ|
nνa ) ≤ D+(B	U ⊗n
Σ )
= D(B	U ⊗n
Σ ) + D−(B	U ⊗n
Σ ).
Substituting the above into Equation (5), and noting that Lemma 2.10 implies that D−(B	U ⊗n
Σ ) ≤
 2
ln 2 I (B), we get
n
j=t
jwj ≤ 2I (B) +
1
log |Σ|
 8
ln 2I (B) + 20 · |Σ|
−t /3
.
The above and Equation (3) complete the proof.
3 MULTIPARTY INTERACTIVE COMMUNICATION OVER NOISY NETWORKS
In the following, we assume a network of n + 1 parties that consists of a server pS and n clients
p1,... ,pn. The network consists of a communication channel (pi,pS ) for every i ∈ [n], that is, the
topology is a star.
3.1 The Pointer Jumping Task
We assume the parties want to compute a generalized pointer jumping task. Formally, the pointer
jumping task of depthT over star networks is the following. Each client pi holds a binary tree xi of
depth T where each edge is labeled by a bit b. The server holds a 2n-ary tree xS of depth T where
each edge of the tree is labeled with an n-bit string from {0, 1}
n.
The server starts from the root of xS . At each round, the server receives from the clients n bits
which it interprets as an index i ∈ [2n]. The server then transmits back to the clients the label on
the i-th edge descending from his current node (one bit per client). The node at the end of this
edge becomes the server’s new node. Similarly, each client receives at each round a bit b from the
server, and sends back the label of the edge indexed by b descending from its current node. For
the first round, we can assume that the clients take the left child of the root of xi and transmit to
the server the label of that edge. The above is repeated until both the server and the clients have
Journal of the ACM, Vol. 65, No. 1, Article 4. Publication date: December 2017.   
4:14 M. Braverman et al.
Fig. 1. An illustration of the inputs, the “correct” path (marked with bold lines) and the sub-input conditioned
on a partial correct path.
reached depthT in their trees. The parties then output the path from the root to their current node
(i.e., to a leaf at depth T ).
We denote this “correct” output of party pi by pathi . The entire output is denoted path =
(pathS , path1,..., pathn ). For a certain party i ∈ [n] ∪ {S} and a level 1 ≤ k ≤ T , we let pathi (k)
be the first k edges of pathi .
We use the following notations throughout. Given any tree T of depth N, we denote its first k
levels by T ≤k and its N − k last levels by T >k . Given a path z = (e1, e2,...), we denote by T [z]
the subtree of T rooted at the end of the path that begins at the root of T and follows the edgesequence z. [For instance, many times z will be the correct path so far (e.g., until some round )
in the input tree xi ; then we will care about the subtrees xi[pathi ()], effectively obtaining a new
instance of the pointer jumping task, with a smaller depth.] We let x = (xS , x1,... , xn ) be the entire input and also use the short notation x = (xS , x[n]) for the server’s and clients’ parts, respectively. The above notation composes in a straightforward way, e.g., x ≤k , x[n]
≤k , and xS ≤k denote
the appropriate set of partial trees in x, x[n], and xS , respectively, and x[path()] denotes the set
of subtrees xi[pathi ()], for i ∈ [n] ∪ {S}. We will sometimes be negligent and write xi[path()]
for xi[pathi ()]. See Figure 1 for an illustration of some of the notations.
The above pointer jumping task is complete for the case of a star network. That is, any noiseless
protocol over a star network can be described as a pointer jumping task by setting the inputs
(xS , x1,... , xn ) appropriately. For our purpose, we will have the inputs distributed randomly. That
is, for every client, the label on each edge is distributed uniformly in {0, 1} independently of all
other edges; for the server, the labels are independent and uniform over {0, 1}
n. We denote the
random variable describing the input of pi by Xi . The correct path also becomes a random variable
that we denote PATHi , and which is a function of the inputs. The same holds for the subtree of a
certain input, given the certain path of some depth  and so on.
Lastly, we denote by π an observed transcript (possibly noisy) of the protocol. That is, π is
the string received by the parties (in some natural order); note that no single party observes the
entire transcript π, but each party observes some part of it. The corresponding random variable
is denoted Π. At times, π will denote a partial transcript, that is, the communication observed by
the parties up to some round k of the protocol.
Journal of the ACM, Vol. 65, No. 1, Article 4. Publication date: December 2017.       
Constant-Rate Coding for Multiparty Interactive Communication Is Impossible 4:15
3.2 Independence of Inputs Conditioned on the Transcript
An important property that will be needed for our lower bound is the fact that the inputs of the
users are independent, even when conditioned on the transcript so far. This implies that only party pi
is capable of sending useful information about its input xi , regardless of the transcript so far (and,
therefore, if the communication of pi is noisy, the information is lost; it is impossible that a different
party pj compensates for this loss). This claim is well known in the folklore, and we now prove it
formally.
Lemma 3.1. Conditioned on the observed transcript Π, the random variables XS ,X1,... ,Xn are
mutually independent.
Proof. The proof goes by induction on the length of Π. The base case where |Π| = 0 is trivial
from the definition of the inputs XS ,X1,... ,Xn.
Assume the claim holds for some transcript Π = π of length  − 1, and consider the next bit Π,
sent without loss of generality by pi , where i ∈ {S} ∪ [n]. This bit (in case it was not changed by
the channel) depends only on Xi and the previous communication Π, that is, Π = f (Π,Xi ). To
simplify notations, denote by Xi = (XS ,X1,... ,Xi−1,Xi+1,... ,Xn ) all the variables except Xi .
We have
Pr(X1 = x1,... ,XS = xS | Π = π, Π = b)
= Pr(X1 = x1,... ,XS = xS , Π = b | Π = π )
Pr(Π = b | Π = π ) by definition
= Pr(Xi = xi | Π = π ) Pr(Xi = xi, Π = b | Π = π )
Pr(Π = b | Π = π )
by induction, since
Xi, f (Xi, Π) ⊥ Xi | Π
=



ji
Pr(Xj = xj | Π = π )



Pr(Xi = xi, Π = b | Π = π )
Pr(Π = b | Π = π )
=

ji
Pr(Xj = xj | Π = π, Π = b) × Pr(Xi = xi | Π = π, Π = b),
where the last transition follows since Xi and Xi are independent given Π, thus conditioning on
a function of either Xi or Π does not change the probability.
Finally, note that if b was changed by the channel, b = b ⊕ E, the claim still holds since the
noise E is independent of all the other variables (i.e., we can condition on E and reduce to the case
above). If the bit b was erased (in the case of a BEC) then the claim trivially holds.
As a corollary to the above, note that, conditioned on any piece of information that the parties
can communicate as part of their transcripts, the variables XS ,X1,...,Xn remain independent.
Specifically, the above holds if we condition on the correct path (up to some level), or on some
levels of the inputs—we can assume a protocol in which the parties simply communicate that
information (so it is a part of Π), and apply the above lemma.
Corollary 3.2. The random variables XS ,X1,... ,Xn are independent, conditioned on the observed transcript Π = π, the correct path PATH = path (up to some level), and parts of the inputs.
4 UPPER BOUND
Showing an upper bound of O(logn/ log logn) on the slowdown for multiparty interactive communication on star networks is rather straightforward. Essentially, all that we need to show is
that, for every logn rounds of communication, the parties can advance Θ(log logn) levels in the
underlying pointer jumping task.
Journal of the ACM, Vol. 65, No. 1, Article 4. Publication date: December 2017.                        
4:16 M. Braverman et al.
Theorem 4.1. For any ε < 1/2 and anyT > 0, there exists a coding scheme for the pointer jumping
task of depth T over a star network with n + 1 parties, that takes Oε (T log n
log log n ) rounds and succeeds
with high probability if each communication channel is a BSCε .
Proof. First, let us recall the existence of good error correction codes.
Lemma 4.2 (Shannon Coding Theorem [40]). For any discrete memoryless channel CH with
capacity C and any k, there exists a code ECC : {0, 1}
k → {0, 1}
n and ECC−1 : {0, 1}
n → {0, 1}
k with
n = O( 1
C k) such that for any m ∈ {0, 1}
k it holds that
Pr 
ECC−1 (CH(ECC(m)))  m
	
< 2−Ω(n)
.
The coding scheme is as follows. Assume that the parties have already correctly solved the
pointer jumping task until a certain depth γ ≥ 0. Each client encodes the next log logn levels of
his input (this is a subtree of size logn, rooted at the current position) using a good Shannon
error correcting code given by Lemma 4.2. The encoded message is of length O(logn), and we
are guaranteed that the server can correctly decode the entire subtree with probability 1 − n−c ,
for some constant c > 1 to our choice. Using a union bound, the server gets all the subtrees of
the clients with high probability 1 − n−c+1. Next, the server computes the correct path (of length
log logn) that corresponds to each party, and sends an encoding of this path to the corresponding
party. The process then repeats from the new depth γ + log logn. The entire scheme therefore
takes T
log log n · O(logn) rounds and succeeds with probability 1 − T
log log n · n−Ω(1)
.
However, T may be very large with respect to n. To further improve the probability of success and prove Theorem 1.2, we use a theorem by Rajagopalan and Schulman (see Reference [2,
Section 3]).
Theorem 4.3 [2, 36]. For any T round protocol over any n-party network G with maximal degree d, there exists a coding scheme Π, that takes O(T ) rounds and succeeds with probability 1 − n
(2(d + 1)p)Ω(T ) given that any symbol transmitted in the network is correctly received with probability 1 − p.
In the scheme we describe above, any log logn symbols are correctly decoded with probability 1 − p, where we can choose p to be small enough, e.g., by taking p = O(n−2). In this case,
Theorem 4.3 guarantees a coding scheme for the pointer jumping task with the same slowdown of O(logn/ log logn) as above, which succeeds with probability 1 − n−Ω(T / log log n)
, that is,
1 − 2−Ω(T log n/ log log n)
.
5 LOWER BOUND
In this section, we prove our main theorem of a lower bound of Ω( log n
log log n ) on the slowdown of
coding for interactive communication over star networks. Toward the lower bound, we can assume
the noisy channel is actually a BECε rather than a BSCε . This only makes the noise model weaker,
and renders the lower bound stronger. In the following, we assume the channel erasure probability
is ε = 1/3. The specific value of ε < 1 only affects the constants involved and does not affect the
validity of our result. Fixing its value will allow an easier exposition of the result.
Our main theorem is the following,
Theorem 5.1. There exists a constant c such that for large enough n, any protocol that solves the
pointer jumping task of depth T (for some T > log logn) over star networks with n + 1 parties in less
than c · T log n
log log n rounds, assuming each communication channel is a BEC1/3, has a success probability
at most 1/5.
Journal of the ACM, Vol. 65, No. 1, Article 4. Publication date: December 2017.  
Constant-Rate Coding for Multiparty Interactive Communication Is Impossible 4:17
We begin by defining the cutoff of the protocol: an information-based measure of progress that
is related to the advancement in the underlying pointer jumping task.5
Definition 5.2 (Cutoff). For any transcript π, and any input x = (xs , x1,... , xn ), the cutoff of the
protocol cutoff(π, x) is the minimal number k, such that both the equations below are satisfied:
I (XS [pathS (k)] | Π = π, PATH(k) = path(k)) ≤ 2−0.1
√
n, and (6)
n
i=1
I (Xi[pathi (k)] | Π = π, PATH(k) = path(k)) ≤ 0.01n. (7)
If no such k exists we set cutoff = T .
The operational meaning of the cutoff is that if k is the cutoff level, then the parties know very
little information on the correct paths beyond the first k edges in that path. Recall that path(k) is
fully determined by the first k levels of the input, x ≤k . Therefore, if cutoff(π, x) = k, then for any
x  such that x ≤k = x ≤k , it holds that cutoff(π, x
) = k. Furthermore, the cutoff is only a function
of the path up to level k, that is, if cutoff(π, x) = k then for any input x  that induces the same
path(k) as x, it holds that cutoff(π, x
) = k; When the path is fixed (but we do not care about the
specific input), we will usually abuse notations and write cutoff(π, path(k)) = k.
Our analysis will actually bound the cutoff in two steps. Very roughly, at the first step, we will
bound the information of the server given a “new” chunk of communication πnew , yet bound the
clients’ information without this new chunk. At the second step we condition on πnew both for the
server and the clients. For the first step described above, we define the following “server cutoff”:
Definition 5.3 (Server Cutoff). Given any transcript π, and any input x = (xS , x1,... , xn ), and
given any continuation of the transcript πnew , we define the server’s cutoff level cutoffS (π, πnew , x)
as the minimal number k for which
I (XS [pathS (k)] | Π = π ◦ πnew , PATH(k) = path(k)) ≤ 2−0.2
√
n, and (8)
n
i=1
I (Xi[pathi (k)] | Π = π, PATH(k) = path(k)) ≤ 0.01n. (9)
If no such k exists we set cutoffS = T .
The following proposition shows that in order for a protocol to output the correct value with
high probability, the cutoff (given the complete transcript) must be ≈ T . Hence, protocols that
succeed with high probability must produce transcripts whose cutoff is large in expectation.
Proposition 5.4. Fix a protocol that solves the pointer jumping task of depthT over a star network
with n + 1 parties, that succeeds with probability at least 1/5 on average, i.e., a protocol for which
PrX,Π(correct output) ≥ 1/5. Then,
EX,Π[cutoff(Π,X)] ≥
 1
5 − 2
n

T .
Proof. Recall that the event cutoff(π, x) = k depends only on π and path(k) and is independent of x>k . We show that if cutoff(π, path(k)) = k for some k < T , then the protocol gives the
correct output with only small probability of 2/n. This will bound the probability of the event
5The reader is encouraged to recall the definition of the pointer jumping task and our notations such as path(k),
Xi[pathi (k)], and the like, stated in Section 3.1.
Journal of the ACM, Vol. 65, No. 1, Article 4. Publication date: December 2017.     
4:18 M. Braverman et al.
cutoff(Π,X) < T by 1/5 − 2/n, and will prove that in expectation (over all inputs and possible
transcripts), the cutoff is at least T/5 − 2T/n.
Claim 5.5. Given π and k < T and path(k) such that cutoff(π, path(k)) = k,
Pr[correct output | Π = π, PATH(k) = path(k)] <
2
n
.
Proof. Let L be the n-bit label of PATHS (k + 1). Note that this label is included in the subtree
XS [path(k)]. If cutoff(π, path(k)) = k, then by the cutoff’s definition,
I (XS [pathS (k)] | Π = π, PATH(k) = path(k)) ≤ 2−0.1
√
n,
and by Lemma 2.6 it holds that
2−H∞ (L |Π=π,PATH(k)=path(k)) ≤
1 + 2−0.1
√
n
|L| ≤
2
n
.
Then, the probability that the protocol is correct is at least the probability that the clients (here
treated as a single party) output the correct label L:
Pr[correct output | Π = π, PATH(k) = path(k)] ≤ 2−H∞ (L |Π=π,PATH(k)=path(k),X[n])
= 2−H∞ (L |Π=π,PATH(k)=path(k))
≤
2
n
,
where the equality holds since the input of the server is independent of the input of the users conditioned on π and path(k). This is implied by Lemma 3.1 (as also stated by Corollary 3.2): consider
a protocol that, after completing the pointer jumping task, communicates the correct path during
its last T rounds. That is, path(k) is simply part of the transcript of this protocol. Now Lemma 3.1
suggests that, because the inputs are independent when conditioned on that transcript, and because the path is simply the suffix of the transcript, then the inputs are independent conditioned
on both the correct path and the prefix of the transcript (that doesn’t contain the path).
The above holds for any k < T and any π, path(k) for which cutoff(π, path(k)) = k. Therefore,
conditioned on the event that cutoff(Π,X) < T the protocol outputs the correct value with probability at most 2/n, that is, PrX,Π[correct output | cutoff(Π,X) < T ] ≤ 2/n. Since the protocol is
correct with probability 1/5 on average over the inputs and randomness of the protocol (and the
noise), the claim follows. Indeed,
1
5
≤ PrX,Π[correct output]
= Pr[cutoff(Π,X) < T ] Pr[correct output | cutoff(Π,X) < T ]
+ Pr[cutoff(Π,X) = T ] Pr[correct output | cutoff(Π,X) = T ]
≤ Pr[cutoff(Π,X) < T ] · 2/n + Pr[cutoff(Π,X) = T ] · 1,
ergo,
Pr[cutoff(Π,X) = T ] ≥
1
5 − 2
n
and
EX,Π[cutoff(Π,X)] ≥ T
 1
5 − 2
n

,
as claimed.
To prove the main theorem, we show that during every 0.1 logn rounds of communication, the
cutoff level increases by at most O(log logn), in expectation. Formally,
Journal of the ACM, Vol. 65, No. 1, Article 4. Publication date: December 2017.  
Constant-Rate Coding for Multiparty Interactive Communication Is Impossible 4:19
Theorem 5.6. Given a protocol for the pointer jumping task, let π be the transcript of the protocol
observed up to some round, and let Πnew be a random variable describing the observed transcript over
the next 0.1 logn rounds. Then, for any  ≤ T , and for any x ≤ it holds that
E [cutoff(π ◦ Πnew ,X) 


 Π = π, PATH() = path(), cutoff(π,X) = ] ≤  + O(log logn).
Note that the expectation is over the inputs, the noise, and the protocol’s randomness.
With the above propositions, the proof of Theorem 5.1 is immediate: if a protocol outputs the
correct answer with probability at least 1/5, it must be that the expected cutoff level at the end
of the protocol is > T/5 − o(T ), but this would take O(T log n
log log n ) rounds of communication, in
expectation. Formally,
Proof (Theorem 5.1). Using Theorem 5.6, for any protocol for the pointer jumping task there
exists a (small enough) constantc > 0 such that after running cT log n
log log n rounds of the protocol, the
expected cutoff for the observed transcript is small, EX,Π[cutoff(Π,X)] < T/10. Therefore, it cannot be that the protocol correctly solves the T -depth pointer jumping with probability above 1/5
as this will contradict Proposition 5.4.
We now turn to prove the key technical Theorem 5.6. Intuitively speaking, the main idea is the
following. We cut the protocol into chunks of length 0.1 logn rounds and treat each one separately, showing that the cutoff level cannot increase during any chunk by more than O(log logn)
in expectation. We can assume that at the beginning of each chunk, all the parties are given the
information about the correct path up to the depth matching the current cutoff level, and reduce
this case (in some sense6) to a new instance of the pointer jumping task starting at that depth.
During the 0.1 logn rounds of the next chunk, with probability at least 1 − 2−
√
n, there exists
a subset Q of √
n parties about which the server does not have much information (beyond the
cutoff point) whose communication was completely erased by the channel throughout this chunk.
We can assume that, other than this set of parties Q, the communication is noiseless. In this case,
it is quite intuitive that the cutoff level cannot increase by too much: the server is missing any
relevant information about the inputs of parties in Q beyond the cutoff level, thus the information
that it sends during that chunk is practically meaningless, and the server’s cutoff level remains
more or less the same. Additionally, since the server did not communicate a lot of meaningful
information about his input, the clients do not know how to proceed and cannot send too much
relevant information; thus, their cutoff level does not increase too much as well. On the other
hand, in the rare case where no subsetQ exists (i.e., the communication in this chunk is practically
noiseless), the cutoff may tremendously increase; however, since this event is so rare, it will add
only O(1) to the accumulated cutoff level throughout the entire protocol, in expectation.
Proof (Theorem 5.6). We begin by showing that with high probability, there exists a subset of
size √
n of the clients, for which the server knows very little information beyond the cutoff level,
and yet in the next 0.1n logn rounds their communication was completely erased by the channel.
Definition 5.7. Given a transcript π and an input x so that cutoff(π, x) = k. For i ∈ [n], we say
that a client pi is critical if
I (Xi[PATHi (k)] 


 Π = π, PATH(k) = path(k)) ≤ 0.02.
6The main difference is that previous communication may have leaked some information on this new instance, and we
need to account for this information as well.
Journal of the ACM, Vol. 65, No. 1, Article 4. Publication date: December 2017.       
4:20 M. Braverman et al.
Lemma 5.8. Let π be the transcript so far and consider the next 0.1 logn rounds of communication.
Denote by Esilence the event that there exists a subset Q of parties of size at least √
n, such that all the
parties in Q are critical and all the bits sent by parties in Q were erased by the channel. Then,
Pr[Esilence ] > 1 − 2−
√
n .
Proof. There are at least n/2 critical parties, or otherwise,

i
I (Xi[PATHi (k)] 


 Π = π, PATH(k) = path(k)) ≥ n
2 · 0.02 ≥ 0.01n,
and k cannot be the cutoff round, by Definition 5.2. Moreover, note that the probability that all the
0.1 logn transmissions of a specific party pi are erased (or even the 0.2 logn bits sent and received
by this party), is 1
3
0.1 log n ≥ n−0.4. LetQ be the set of all critical parties whose entire communication
was erased by the channel. By Chernoff bound and assuming large enough n we have
Pr 
|Q| < √
n
	
< exp 
−n0.6
4

.
Here we use the fact that ε = 1/3; however, it is clear that for any other constant ε, we can reduce
the length of a chunk to be c logn such that, say, εc log n ≥ n−0.4 and all the other proofs below
remain valid, maybe up to adjusting the constants as needed.
For any  ≤ T , any fixing path(), and any transcript π, denote by E(π,path(), ), the event
that (Π = π, PATH() = path(), cutoff(π,X) = ). Recall that whether the cutoff is  depends
only on π and the first  levels the correct path, therefore E(π,path(), ) is either empty or equal
to (Π = π, PATH() = path()). For any continuation πnew
S of bits sent by the server in the
new chunk define ES
(π,π new
S ,path(), ) the event (Π = π, Πnew
S = πnew
S , PATH() = path(), cutoffS
(π, Πnew
S ,X) = ).
The proof of the theorem will follow from the next three propositions:
Proposition 5.9. For any  ≤ T , any path() and any transcript π
E[cutoffS (π, Πnew ,X) | E(π,path(), ), Esilence ] ≤  + 40.
Proposition 5.10. Split the observed new transcript Πnew = (Πnew
S , Πnew
[n] ) to the parts corresponding to information sent by the server and by the clients, respectively. For any  ≤  ≤ T , any
fixing path(), any transcript π, and any (server’s) new transcript πnew
S ,
E[cutoff(π ◦ Πnew ,X) | E(π,path(), ), ES
(π,π new
S ,path(), )
, Esilence ] ≤  + 5 log logn.
Proposition 5.11. For any  ≤ T , any fixing path() and any transcript π
E[cutoff(π ◦ Πnew ,X) | E(π,path(), ), Esilence ] ≤  + O(n logn log logn).
The above three propositions prove the theorem: When the good event Esilence doesn’t happen, the
cutoff increases by at mostO(n logn log logn) (Proposition 5.11), but this happens with probability
at most Pr[Esilence ] < 2−
√
n (Lemma 5.8), thus the expected contribution to the increase of the
cutoff by such chunks is bounded by a negligible amount of O(n logn log logn) · 2−
√
n. Otherwise,
assuming the previous cutoff was , then with the information of Πnew , the server’s cutoff level,
according to Proposition 5.9, is in expectation at most S ≤  + 40. Finally, given that the server’s
cutoff is S , Proposition 5.10 guarantees that the new cutoff (i.e., when considering Πnew for both
the server and the clients), is in expectation at most S + 5 log logn =  + O(log logn).
In the following three subsections, we prove the above three propositions in turn.
Journal of the ACM, Vol. 65, No. 1, Article 4. Publication date: December 2017.                                               
Constant-Rate Coding for Multiparty Interactive Communication Is Impossible 4:21
5.1 Bounding the Server’s Cutoff: Proof of Proposition 5.9
To prove Proposition 5.9, we need to find the minimal round k that satisfies Equations (8) and (9),
and show that this round is in expectation at most  + 40, provided that the old cutoff level is , and
that Esilence occurs. We begin in Section 5.1.1 by bounding the information on XS revealed by the
transcript so far, as a function of k, towards satisfying Equation (8). In Section 5.1.2, we bound the
information on the Xi ’s as a function of k, towards satisfying Equation (9). Finally, in Section 5.1.3,
we use the two bounds on the information to derive a bound the new server’s cutoff k.
5.1.1 Bounding the Information in Equation (8). Recall the setting: the protocol has run for some
rounds, producing the transcript Π = π so that the cutoff until that point is . In other words, we
are given (π, path()) ∈ E(π,path(), ).
Now we run the protocol for another 0.1 logn rounds and obtain a new transcript Πnew , describing the bits observed in those new 0.1 logn rounds, up to erasures. We condition on the event
Esilence that guarantees that there is a set of √
n critical clients whose communication (in Πnew )
was completely erased. Next, we reveal to all parties the correct path of depth  [i.e., we condition
on PATH() = path()], and we wish to find the expected new cutoff induced by π ◦ Πnew .
Let us first set some notations that will be used throughout the first part of the proof. Let Z (k) =
PATHS (k + ) be the correct path of length k in XS below the cutoff level.
7 Given specific transcripts
π, πnew , a specific path path() and specific fixing xS [pathS ()]≤k of the k first levels of the input
of the server in the subtree induced by pathS (), we define the short-handed events
E def
= (Π = π, Πnew = πnew , PATH() = path()), and
E+ def
= (E,XS [Z (0)]≤k = xS [path()]≤k ).
The information measure in Equation (8) conditions exactly on E. However, we will need to
condition on even a smaller event space (i.e., on E+) to utilize independence between several
variables. To this end, we use the following claim, that proves an independence between the
correct path (between levels  and  + k) and the server’s input at depths below  + k, when
conditioning on E+. This will be instrumental when using Lemma 2.12 to bound the information
measure related with the cutoff.
Claim 5.12. Conditioned on the event
E+ = 
XS [Z (0)]
≤k = xS [path()]
≤k , Π = π, Πnew = πnew , PATH() = path()

,
the variables PATH(k + ) and XS [Z (0)]
>k are independent.
Proof. Once we condition on XS [Z (0)]
≤k = xS [path()]
≤k and PATHS () = pathS (), then
PATH(k + ) becomes a function only of X[n]
> ∩ X[n]
≤k+, and these are all independent of X >
S ,
when conditioned on the transcript and on the other parts of E+ (which can be included as part
of the transcript), via Corollary 3.2.
7Since we condition on PATH() = path(), the remaining unfixed random variables are only the suffix of length k.
Journal of the ACM, Vol. 65, No. 1, Article 4. Publication date: December 2017.                                 
4:22 M. Braverman et al.
We now get to the core of the proof. For any k > 0, define the random functions
S∗ (k | πnew , path(k + )) def
=
I (XS [pathS (k + )] | Π = π, Πnew = πnew , PATH(k + ) = path(k + )),
S (k | πnew , xS [pathS ()]≤k ) def
=
Eρ∼PATH(k+)|E+
I (XS [ρS (k + )] | Π = π, Πnew = πnew , PATH(k + ) = ρ,XS [Z (0)]≤k = xS [pathS ()]≤k ).
To clarify the above notation, note that ρ ∼ PATH(k + ) is a variable of the expectation going over
all respective paths of length k +  (for all parties), and we can write ρ = (ρ1,..., ρn, ρS ) according
to its parts.
The random variables S∗ (k) precisely describe the measure we need to bound for Equation (8);
however, we will actually bound the measure S (k), which is similar to S∗ (k) up to conditioning on
E+ rather than on E. The measure S (k) upper bounds S∗ (k) in expectation (via Claim 5.13 below),
thus it suffices to bound S (k) to obtain a bound on S∗ (k) and satisfy Equation (8). We take this
detour because we cannot bound S∗ (k) directly; however, bounding S (k) is possible once we take
advantage of the independence between PATH and XS in non-overlapping depths of the trees, as
stated by Lemma 5.12.
Claim 5.13. Given any π, πnew , path() and any k,
Epath(k+)|E,Esilence S∗ (k | πnew , path(k + )) ≤ ExS [pathS ()]≤k |E,Esilence S (k | πnew , xS [pathS ()]≤k ).
Proof. First, note that E determines whether Esilence occurs or not (indeed: πnew determines
which bits are erased, and π, path() determine the set of critical parties), therefore it suffices to
condition on E alone. Starting with the definition of S (k),
ExS [pathS ()]≤k |ES (k | πnew , xS [path()]≤k )
= ExS [pathS ()]≤k |E
Eρ∼PATH(k+)|xS [pathS ()]≤k, E
I (XS [ρS (k + )] | Π = π, Πnew = πnew , PATH(k + ) = ρ,XS [Z (0)]≤k = xS [pathS ()]≤k ),
exchanging the order of expectations, and using Definition 2.2,
= Eρ∼PATH(k+)|E
ExS [pathS ()]
≤k |ρ, E
I (XS [ρS (k + )] | Π = π, Πnew = πnew , PATH(k + ) = ρ,XS [Z (0)]≤k = xS [pathS ()]≤k )
= Eρ∼PATH(k+)|EI (XS [ρS (k + )] | Π = π, Πnew = πnew , PATH(k + ) = ρ,XS [Z (0)]≤k ),
conditioning on XS [Z (0)]≤k can only increase the information (Lemma 2.4), thus,
≥ Eρ∼PATH(k+)|EI (XS [ρS (k + )] | Π = π, Πnew = πnew , PATH(k + ) = ρ)
= Eρ∼PATH(k+)|ES∗ (k | πnew , ρ).
Now we can bound the measure S (k). We show that the expected sum of this quantity, for
k ≥ 30, is exponentially small. This will be used in Section 5.1.3 to show that the first k∗ for which
S∗ (k∗) < 2−0.2
√
n as required for Equation (8), is bounded in expectation by 40.
Journal of the ACM, Vol. 65, No. 1, Article 4. Publication date: December 2017.                                      
Constant-Rate Coding for Multiparty Interactive Communication Is Impossible 4:23
Lemma 5.14. Given any (π, path()) ∈ E(π,path(), ),
T
−
k=30
Eπ new,xS [path()]
≤k |π,path(),Esilence

S (k | πnew , xS [path()]
≤k )
	
≤ n logn · 2−0.5
√
n .
Proof. The outline of the proof is as follows. First we use Lemma 2.12 to bound S (k |
πnew , xS [path()]
≤k ) as the product of the probability to guess the correct path between layers
 and  + k, and the information on the subtrees rooted in level  + k. We then bound each part
independently to obtain the stated claim.
Let the {Xi} of Lemma 2.12 be all the subtrees of XS rooted at the end of a path of depth k + ,
whose prefix is pathS (). Note that those subtrees and (the last k edges in each of) PATH(k + )
are independent conditioned on E+, due to claim 5.12 above. Also note that the union of all these
subtrees is contained in XS [Z (0)]
>k . It follows that (Lemma 2.12)
S (k | πnew , xS [path()]
≤k ) ≤ pmax (Z (k) | E+) × I (XS [Z (0)]
>k | E+). (10)
First, we bound the second term. We show that the expected amount of information we gain in
the new chunk of communication on the input of the server (below the cutoff level ) is bounded
by the ≈ 0.2n logn bits that were communicated in the new chunk.
Claim 5.15. Given any (π, path()) ∈ E(π,path(), ), for any k it holds that
Eπ new,xS [path()]
≤k |π,path(),Esilence

I

XS [Z (0)]
>k 




 E+	
≤ n logn.
Proof. Note thatXS [Z (0)] = (XS [Z (0)]
≤k ,XS [Z (0)]
>k ). The claim follows using Lemma 2.4(3),
Eπ new,xS [path()]
≤k |π,path(),Esilence I

XS [Z (0)]
>k 




 XS [Z (0)]≤k = xS [path()]≤k , E

= Eπ new |π,path(),Esilence I

XS [Z (0)]
>k 




 XS [Z (0)]
≤k , E

≤ Eπ new |π,path(),Esilence I (XS [Z (0)] | E) ,
where the equality comes from performing the expectation over xS [path()]
≤k , and the transition
is via Lemma 2.4(3), and recalling that XS [Z (0)] = (XS [Z (0)]
≤k ,XS [Z (0)]
>k ). Substituting E back
for better clarity, via Definition 2.2 we get
= Eπ new |π,path(),Esilence I (XS [Z (0)] 


 PATH() = path(), Π = π, Πnew = πnew )
= I

XS [Z (0)] 




 PATH() = path(), Π = π, Πnew 
≤ I (XS [Z (0)] 


 Π = π, PATH() = path()) + log |ΩΠnew |
≤ 2−0.1
√
n + 0.2n logn
≤ n logn,
where Πnew is distributed like Πnew conditioned on (Esilence , Π = π, PATH() = path()). The
penultimate transition holds since cutoff(π, path()) = , thus without Πnew the information is
bounded by 2−0.1
√
n ≤ 1. Furthermore, Πnew contains only 0.2n logn bits, some of which may be
erased, but this gives no extra information on XS [in fact, half of these bits are sent by the clients
and those are (conditionally) independent of XS and give no further information, but we can count
them as well]. Therefore, conditioning on Πnew can increase the information by at most 0.2n logn
in expectation due to Lemma 2.4(2).
Since Claim 5.15 bounds the second part of Equation (10) only in expectation, we cannot bound
directly the expectation of the product without showing that these two parts are independent.
Journal of the ACM, Vol. 65, No. 1, Article 4. Publication date: December 2017.                                       
4:24 M. Braverman et al.
To this end, we bound the first term directly (not in expectation), and show that the bound is
independent of the expectation variables.
Bounding pmax (Z (k) | E+) is based on the technical Lemma 2.14. We use the fact that the correct
path Z (k) in the server’s tree is determined by the labels on the correct paths in the clients’ trees.
Since the amount of information on these labels (beyond the cutoff point) is small, Lemma 2.14
asserts that the probability to guess Z (k) is also small.
First, note that in the derivation below we consider only πnew for which Esilence occurs; other
transcripts never appear in the expectation of the lemma’s statement. Also recall we are guaranteed
that (π, path()) ∈ E(π,path(), ). For any specific k, we can think of Z (k) as composed of n binary
variables where each represents the path induced by a different client, Z (k) def
= (Z1 (k),...,Zn (k)).
Let a1 (k), a2 (k),..., an (k) be n paths of length k that attain the maximal probability, that is, paths
that satisfy
Pr[Z1 (k) = a1 (k),Z2 (k) = a2 (k),...,Zn (k) = an (k) | E+] = pmax (Z (k) | E+). (11)
Note that Z1 (k),...,Zn (k) andXS [Z (0)]
≤k induce paths P1 (k),..., Pn (k) on X1,...,Xn, respectively. Each Pi starts at the end of pathi () and is of length k. That path is uniquely determined by
the i-th bit of the labels along Z (k) in XS [Z (0)]. Then, Equation (11) equals
pmax (Z (k) | E+) = Pr[label(P1 (k)) = a1 (k),...,label(Pn (k)) = an (k) | E+].
Via Corollary 3.2, the labels of Pi are independent of labels of Pj for j  i, conditioned on E+
(because these labels are just part of the variables Xi ), and the above equals
pmax (Z (k) | E+) =

i ∈[n]
Pr[label(Pi (k)) = ai (k) | E+]
≤

i ∈Q
Pr[label(Pi (k)) = ai (k) | E+]
≤

i ∈Q
max
P
i (k)
Pr[label(P
i (k)) = ai (k) | E+],
where Q is the set of all critical clients (for cutoff(π, path()) = ) whose communication was completely erased, and P
i (k) is any path of length k +  in Xi , whose prefix is path(). That is, instead
of looking at a specific path Pi , we are looking at all the possible paths, and take the one that
maximizes the probability.
Since the communication of any party i ∈ Q is fully erased in πnew , the probability of
label(Pi (k)) is independent of πnew .
8 Also note that once we consider the path Pi (k) that maximizes the probability (out of all possible paths), then the specific path we take no longer matters. Then, the above probability is just the probability that some label pattern occurs in Xi (between levels  and k + ), and this probability is (conditionally) independent of XS by Corollary 3.2.
Continuing with the above, explicitly writing the elements of E+ and removing the conditioning
on XS [Z (0)]
≤k (which are just parts of XS ) and the conditioning on πnew as explained above, we
obtain
pmax (Z (k) | E+) ≤

i ∈Q
max
P
i (k)
Pr[label(P
i (k)) = ai (k) | Π = π, PATH() = path()]. (12)
8We can assume that both the incoming and outgoing communication of pi is erased. However, in fact, a stronger claim
holds even if we only assume the outgoing communication is erased. The incoming bits are sent by the server and, conditioned on π , are independent of Xi ; see also Claim 5.20.
Journal of the ACM, Vol. 65, No. 1, Article 4. Publication date: December 2017.                  
Constant-Rate Coding for Multiparty Interactive Communication Is Impossible 4:25
We observe that we can use the bound in Equation (12) not only for a specific k, but even for
their sum for k ≥ 30. This observation will be useful shortly. Formally,
T
−
k=30

i ∈Q
max
P
i (k)
Pr[label(P
i (k)) = ai (k) | Π = π, PATH() = pathi ()]
≤

i ∈Q
T
−
k=30
max
P
i (k)
Pr[label(P
i (k)) = ai (k) | Π = π, PATH() = pathi ()],
since exchanging the order of summation and product just adds positive terms. Then we can use
Lemma 2.14 to bound the summation,
≤

i ∈Q

2Ii + 4

Ii + 20 · 2−30/3

,
where here Ii = I (Xi[pathi ()] | Π = π, PATH() = path()). Since each party i ∈ Q is critical
we know by Definition 5.7 that ∀i ∈ Q, Ii ≤ 0.02, and since |Q| ≥ √
n when Esilence occurs
(Lemma 5.8), we conclude that
T
−
k=30

i ∈Q
max
P
i (k)
Pr[label(P
i (k)) = ai (k) | Π = π, PATH() = pathi ()]
≤

i ∈Q

2 · 0.02 + 4
√
0.02 + 20 · 2−30/3

≤ 2−0.5
√
n . (13)
Putting all the ingredients together, we now bound the expectation of
k ≥30 S (k |
πnew , xS [pathS ()]≤k ) over all the possible new transcripts and fixings of xS [pathS ()]≤k that
occur with positive probability conditioned on Esilence and (π, path()) ∈ E(π,path(), ), and complete the proof of this lemma. Starting with Equation (10),
T
−
k=30
Eπ new,xS [path()]≤k |π,path(),Esilence 
S (k | πnew , xS [path()]≤k )
	
≤
T
−
k=30
Eπ new,xS [path()]≤k |π,path(),Esilence 
pmax (Z (k) | E+) × I (XS [Z (0)]
>k | E+)
	
,
now we can bound pmax (Z (k) | E+) using Equation (12) [note that the expectation is only on transcripts and inputs in Esilence , E(π,path(), ) as assumed in the derivation of Equation (12)]:
≤
T
−
k=30
Eπ new,xS [path()]≤k |π,path(),Esilence
⎡
⎢
⎢
⎢
⎢
⎢
⎣

i ∈Q
max
P
i
Pr[label(P
i ) = ai | Π = π, PATH() = path()]
× I (XS [Z (0)]
>k | E+)
⎤
⎥
⎥
⎥
⎥
⎦
.
Journal of the ACM, Vol. 65, No. 1, Article 4. Publication date: December 2017.                                         
4:26 M. Braverman et al.
Now, the first term of the product is constant with respect to the expectation,
≤
T
−
k=30

i ∈Q
max
P
i
Pr[label(P
i ) = ai | Π = π, PATH() = path()]
× Eπ new,xS [path()]≤k |π,path(),Esilence 
I (XS [Z (0)]
>k | E+)
	
≤ 2−0.5
√
n × n logn,
where the last step is due to Equation (13) and Claim 5.15.
5.1.2 Bounding the Information in Equation (9). Similarly to the information about the
server’s XS , we need to bound the information about the clients’ Xi ’s to satisfy Equation (9), but
note that here we only consider π and not πnew (thus, there is no need to condition on Esilence ).
Still, the information measure in Equation (9) may have increased due to the fact we condition
on path(k + ) instead of path(). We now show that this cannot lead to increasing the server’s
cutoff level by more than a constant.
We will abuse notations in this second part and redefine Z1 (k),...,Zn (k) to be the correct
paths in X1,...,Xn of length k + , that is, we let Zi (k) = PATHi (k + ). Given any (π, path()) ∈
E(π,path(), ) we define
C∗
i (k | path(k + )) def
= I

Xi[pathi (k + )] 




 Π = π, PATH(k + ) = path(k + )

,
C∗ (k | path(k + )) def
=
n
i=1
C∗
i (k | path(k + )),
which is indeed the measure we need to bound to satisfy Equation (9). As above, we will bound
C∗ (k) via C(k). Redefine the event E as
E def
= (Π = π, PATH() = path()),
and let
Ci (k | xi[pathi ()]
≤k ) def
= E
ρ∼PATH(k+)|xi[pathi ()]
≤k, E
I (Xi[ρi] | Xi[pathi ()]
≤k = xi[pathi ()]
≤k , PATH(k + ) = ρ, E),
C(k | x[n][path()]
≤k ) def
=
n
i=1
Ci (k | xi[pathi ()]
≤k ).
Indeed, the quantity C(k) gives an upper bound on C∗ (k), in expectation on the fixing of the k
levels of the clients beyond the cutoff level. Formally,
Claim 5.16. Given any π, path(), and for any k, and any i ∈ [n],
Epath(k+)|EC∗
i (k | path(k + )) ≤ Exi[pathi ()]
≤k |ECi (k | xi[pathi ()]
≤k ).
Journal of the ACM, Vol. 65, No. 1, Article 4. Publication date: December 2017.                                    
Constant-Rate Coding for Multiparty Interactive Communication Is Impossible 4:27
Proof. The proof is very similar to the proof of Claim 5.13.
Exi[path()]
≤k |ECi (k | xi[pathi ()]
≤k )
= Exi[path()]
≤k |E
E
ρ∼PATH(k+)|xi[pathi ()]
≤k, E
I (Xi[ρi] | Xi[pathi ()]
≤k = xi[pathi ()]
≤k , PATH(k + ) = ρ, E)
= Eρ∼PATH(k+)|E
Exi[path()]
≤k |ρ, E
I (Xi[ρi] | Xi[pathi ()]
≤k = xi[pathi ()]
≤k , PATH(k + ) = ρ, E)
= Eρ∼PATH(k+)|EI (Xi[ρi] | Xi[pathi ()]
≤k , PATH(k + ) = ρ, E).
Using Lemma 2.4(1) we get
≥ Eρ∼PATH(k+)|EI (Xi[ρi] | PATH(k + ) = ρ, E)
= Eρ∼PATH(k+)|EC∗
i (k | ρ).
Next, we bound the sum of expectations of C(k) for k ≥ 10.
Lemma 5.17. Given any (π, path()) ∈ E(π,path(), ),
T
−
k=10
Ex[n][path()]
≤k |E 
C(k | x[n][path()]
≤k )
	
< 0.08n.
Proof. The proof follows the same steps of Lemma 5.14, but the scenario here is somewhat
simpler. We use Lemma 2.12 on each Ci : again the variables {Xi} of Lemma 2.12 are set to be all
various subtrees Xi[Zi (k)] obtained by all the possible different Zi (k) that are consistent with E.
Again note that, similar to the reasoning in Claim 5.12, the path Zi (k) is independent of the labels
in the subtrees of Xi rooted at the end of a path of length k +  with prefix pathi (), conditioned
on E and on Xi[pathi ()]
≤k ; this independence is required for applying Lemma 2.12. Also note
that the union of all these subtrees is exactly Xi[Z (0)]
>k . Lemma 2.12 then implies that
Ci (k | xi[pathi ()]
≤k ) ≤ pmax 
Zi (k) 




 Xi[pathi ()]
≤k = xi[pathi ()]
≤k , E

× I

Xi[pathi ()]
>k 




 Xi[pathi ()]
≤k = xi[pathi ()]
≤k , E

. (14)
We begin with bounding the term pmax (Zi (k) | Xi[pathi ()]
≤k = xi[pathi ()]
≤k , E). For any
specific k, assume a path ai (k) of length k that maximizes this probability,
Pr[Zi (k) = ai (k) | Xi[pathi ()]
≤k = xi[pathi ()]
≤k , E].
Once fixing ai (k), it is implied that there exists a path P (k) of length k in XS (starting from level ,
as a continuation of pathS () which is fixed given path()),9 whose labels, restricted to the i-th
9We note that the paths in XS and the corresponding labels in X[n] are shifted by one level in depth, which is due to the
alternating nature of the protocol and our arbitrary decision to let the clients start (assuming all of them take the left son
of their root node). To ease the readability of the proof, we will neglect this edge issue and omit the ±1 shift in the indices.
Journal of the ACM, Vol. 65, No. 1, Article 4. Publication date: December 2017.                                             
4:28 M. Braverman et al.
bit, is exactly ai (k). The probability to have a path with such labels is bounded by
≤ max
P (k)
Pr[labeli (P (k)) = ai (k) | Xi[pathi ()]
≤k = xi[pathi ()]
≤k , E]
= max
P (k)
Pr[labeli (P (k)) = ai (k) | E],
where the last step follows from Corollary 3.2 that guarantees us the independence of the labels
(of XS
>) from all the other inputs Xi
>, even when conditioning on the transcript so far π, and
on E.
We can then bound the sum of the guessing probability of Zi (k) for k ≥ 10:
T
−
k=10
pmax (Zi (k) | Xi[pathi ()]
≤k = xi[pathi ()]
≤k , E)
≤
T
−
k=10
max
P Pr[labeli (P (k)) = ai (k) | E]
≤ 2I + 4
√
I + 20 · 2−10/3
≤ 8, (15)
where the penultimate transition is via Lemma 2.14 by letting T of the lemma be all the labels of
XS [pathS ()]
>, setting I = I (XS [pathS ()]
> | E), and recalling that  is the cutoff level (given
(π, path()) ∈ E(π,path(), )), which in turn implies by its definition that I ≤ 2−0.1
√
n ≤ 1.
Now that the first term of Equation (14) is bounded by a fixed number, bounding the expectation
of the C(k) reduces to bounding the expectation of the second term in Equation (14).
T
−
k=10
Ex[path()]
≤k |E 
C(k | x[path()]
≤k )
	
=
T
−
k=10
Ex[path()]
≤k |E
⎡
⎢
⎢
⎢
⎢
⎣
n
i=1
Ci (k | xi[pathi ()]
≤k )
⎤
⎥
⎥
⎥
⎥
⎦
≤
T
−
k=10
Ex[path()]
≤k |E
⎡
⎢
⎢
⎢
⎢
⎣
n
i=1
pmax 
Zi (k) 




 Xi[pathi ()]
≤k = xi[pathi ()]
≤k , E

× I

Xi[pathi ()]
>k 




 Xi[pathi ()]
≤k = xi[pathi ()]
≤k , E

⎤
⎥
⎥
⎥
⎥
⎦
≤
T
−
k=10
Ex[path()]
≤k |E
⎡
⎢
⎢
⎢
⎢
⎣
n
i=1
max
P Pr[labeli (P) = ai (k) | E]
× I (Xi[pathi ()]
>k | Xi[pathi ()]
≤k = xi[pathi ()]
≤k , E)
⎤
⎥
⎥
⎥
⎥
⎦
≤
T
−
k=10
n
i=1
max
P Pr[labeli (P) = ai (k) | E]
× Ex[path()]
≤k |E 
I (Xi[pathi ()]
>k | Xi[pathi ()]
≤k = xi[pathi ()]
≤k , E)
	
.
Journal of the ACM, Vol. 65, No. 1, Article 4. Publication date: December 2017.                                             
Constant-Rate Coding for Multiparty Interactive Communication Is Impossible 4:29
Now, by the definition of information, and using Lemma 2.4(3),
=
T
−
k=10
n
i=1
max
P Pr[labeli (P) = ai (k) | E] × I (Xi[pathi ()]
>k | Xi[pathi ()]
≤k , E)
≤
n
i=1
T
−
k=10
max
P Pr[labeli (P) = ai (k) | E] × I (Xi[pathi ()] | E).
Using Equation (15),
≤ 8
n
i=1
I (Xi[pathi ()] | E),
recall that  is the cutoff level, i.e., that (π, path()) ∈ E(π,path(), ),
≤ 8 · 0.01n
≤ 0.08n.
5.1.3 Completing the Proof of Proposition 5.9. With the above bounds on the information revealed as a function of the increase k in the new server’s cutoff level, we use Lemma 2.13 to bound
the expected increase in cutoffS .
Proof (Proposition 5.9). Given (π, path()) ∈ E(π,path(), ) consider the following two series
of non-negative random variables:

S˜(k) def
= Eπ new,path(k+30+)|π,path(),Esilence [S∗ (k + 30 | πnew , path(k + 30 + ))]

k ≥0
, and
⎧⎪
⎨
⎪
⎩
C˜(k) def
= Epath(k+30+)|π,path(),Esilence
⎡
⎢
⎢
⎢
⎢
⎣
n
i=1
C∗
i (k + 30 | path(k + 30 + ))
⎤
⎥
⎥
⎥
⎥
⎦
⎫⎪
⎬
⎪
⎭k ≥0
.
We note again that C(k) (as defined in Section 5.1.2) does not assume the event Esilence , while
the above C˜(k) does. This has no effect on the bounds derived in Section 5.1.2, as this event is
completely independent of C(k): the information in C() is conditioned only on π and not on πnew ,
while the event Esilence relates only to πnew and is independent of any previous communication.
Lemma 5.14 and Claim 5.13 tell us that
k S˜(k) ≤ n logn · 2−0.5
√
n, and similarly Lemma 5.17 and
Claim 5.16 certify that
k C˜(k) ≤ 0.08n. Therefore, from Lemma 2.13 it follows that the expectation
of the minimal k∗ for which S˜(k∗) < 2−0.2
√
n as well as C˜(k∗) < 0.01n is bounded by
E[k∗
] ≤ 1 + n logn · 2−0.5
√
n
2−0.2
√
n +
0.08n
0.01n ≤ 10.
We recall that the server’s cutoff is the minimal round k in which both the information described
by S∗ (k) is below 2−0.2
√
n andC∗ (k) is below 0.01n. From the above, it is then immediate that, given
any (π, path()) ∈ E(π,path(), ), we can bound the expected increase in the server’s cutoff by
E [cutoffS (π, Πnew ,X) | π, path(), Esilence ] = Eπ new,x |π,path(),Esilence [cutoffS (π, πnew , x)]
≤  + 30 + 10
=  + 40,
thus,
E

cutoffS (π, Πnew ,X) | E(π,path(), ), Esilence 	
≤  + 40,
as claimed.
Journal of the ACM, Vol. 65, No. 1, Article 4. Publication date: December 2017.                                   
4:30 M. Braverman et al.
5.2 Bounding the Cutoff: Proof of Proposition 5.10
Next, we show that, given that the server’s cutoff did not increase by much after observing πnew ,
the protocol’s cutoff (when considering πnew for both the server and the clients) cannot increase
by more than O(log logn) beyond the server’s cutoff.
Proof (Proposition 5.10). Let us first recall the setting. We are given  ≤  ≤ T , and π,
path(), πnew
S , so that the following holds. The cutoff assuming the old transcript is
, that is,
(π, path(
)) ∈ E(π,path(), ), The server’s cutoff given π, πnew
S is , that is, (π, πnew
S , path()) ∈
ES
(π,π new
S ,path(), )
. Additionally, we assume that the event Esilence occurs in the new segment of
communication, i.e., we only care about πnew
[n] that have positive probability given Esilence and the
fixed transcript and path given above. We want to show that the new cutoff (i.e., given the new
transcript), is at most  + O(log logn) in expectation over the inputs and πnew
[n] .
The proof resembles the proof of Proposition 5.9: we bound the information on the respective
subtrees of XS and X[n] using Lemma 2.12 and Lemma 2.14, and then bound the expected depth of
the new subtrees whose information is below the threshold [i.e., satisfying Equations (6)–(7)] via
Lemma 2.13.
Recall we can split πnew = (πnew
S , πnew
[n] ) into the parts sent by the server and the clients, respectively. Throughout the proof we will be using the short notations
E = (Π = π, Πnew = πnew , PATH() = path()) ,
ES = (Π = π, Πnew
S = πnew
S , PATH() = path()).
For i ∈ [n] define Zi (k) = PATHi (k + ). Given any π, πnew
S , path() we define the random
functions
C∗
i (k | πnew
[n] , path(k + )) def
=
I

Xi[pathi (k + )] | Π = π, Πnew = πnew , PATH(k + ) = path(k + )

and,
Ci (k | πnew
[n] , xi[pathi ()]
≤k ) def
=
E
ρ∼PATH(k+)|xi[pathi ()]
≤k, EI (Xi[ρi] | Xi[pathi ()]
≤k = xi[pathi ()]
≤k , PATH(k + ) = ρ, E),
C(k | πnew
[n] , x[n][path()]
≤k ) def
=
n
i=1
Ci (k | πnew
[n] , xi[pathi ()]
≤k ).
A reminder that
i C∗
i (k) is indeed the quantity we wish to bound [to satisfy Equation (7)], and
that for any πnew
[n] , the measure C(k) upper bounds
i C∗
i (k) in expectation, via Claim 5.16 (note
that Claim 5.16 can be used as is, by considering the entire transcript π ◦ πnew as the transcript
we condition on, in that claim).
Lemma 5.18. Given any (π, πnew
S , path()) ∈ ES
(π,π new
S ,path(), )
,
T
−
k=3 log log n
Eπ new
[n] ,x[n][path()]
≤k |ES

C 
k | πnew
[n] , x[n][path()]
≤k 	
< 21n.
Proof. The first part of the proof follows the same reasoning and notational conventions used
in the proof of Proposition 5.9 (or specifically, Lemma 5.17), and we don’t repeat here the detailed
Journal of the ACM, Vol. 65, No. 1, Article 4. Publication date: December 2017.                                           
Constant-Rate Coding for Multiparty Interactive Communication Is Impossible 4:31
arguments leading to the following derivation:
T
−
k=3 log log n
Eπ new
[n] ,x[n][path()]
≤k |ES

C(k | πnew
[n] , x[n][path()]
≤k )
	
=
T
−
k=3 log log n
n
i=1
Eπ new
[n] ,x[n][path()]
≤k |ES

Ci (k | πnew
[n] , xi[pathi ()]
≤k )
	
≤
T
−
k=3 log log n
n
i=1
Eπ new
[n] ,x[n][path()]
≤k |ES

pmax (Zi (k) | Xi[pathi ()]
≤k = xi[pathi ()]
≤k , E)
× I (Xi[pathi ()]
>k | Xi[pathi ()]
≤k = xi[pathi ()]
≤k , E)

≤
T
−
k=3 log log n
n
i=1
Eπ new
[n] ,x[n][path()]
≤k |ES

max
P (k)
Pr[labeli (P (k)) = ai (k) | E]
× I (Xi[pathi ()]
>k | Xi[pathi ()]
≤k = xi[pathi ()]
≤k , E)

≤
n
i=1
T
−
k=3 log log n
max
P (k)
Pr[labeli (P (k)) = ai (k) | E]
× Eπ new
[n] ,x[n][path()]
≤k |ES

I (Xi[pathi ()]
>k 




 Xi[pathi ()]
≤k = xi[pathi ()]
≤k , E)
	
=
n
i=1
T
−
k=3 log log n
max
P (k)
Pr[labeli (P (k)) = ai (k) | E]
× Eπ new
[n] |ES

I (Xi[pathi ()]
>k | Xi[pathi ()]
≤k , E)
	
,
which by Lemma 2.4(3) gives
=
n
i=1
T
−
k=3 log log n
max
P (k)
Pr[labeli (P (k)) = ai (k) | E] × Eπ new
[n] |ES

I (Xi[pathi ()] | E)

. (16)
We now bound the two multiplicands of Equation (16) separately.
Claim 5.19. For any i ∈ [n],
T
−
k=3 log log n
max
P (k)
Pr[labeli (P (k)) = ai (k) | E] ≤ 6 · 2−0.1
√
n +
20
logn
.
Proof. Recall that P (k) describes a path of length k in XS [path()]. The maximal probability
guess of the labels of P (k) (restricted to the i-th bit) for k ≥ 3 log logn is given by Lemma 2.14,
setting the variable T of the lemma as T = XS [path()] (restricted to the i-th bit in each label),
and using the fact that cutoffS (π, πnew , path()) = , so that I (T ) ≤ I (XS [path()] | E) ≤ 2−0.2
√
n.
Journal of the ACM, Vol. 65, No. 1, Article 4. Publication date: December 2017.                                       
4:32 M. Braverman et al.
Thus,
T
−
k=3 log log n
max
P (k)
Pr[labeli (P (k)) = ai (k) | E] ≤ 2I (T ) + 4

I (T ) + 20 · 2− log log n
≤ 6 · 2−0.1
√
n +
20
logn
.
Before we bound the second multiplicand of Equation (16), we prove the following technical
claim.
Claim 5.20. Let Π = π be an observed transcript up to some point, and let Πnew be a continuation of Π. Write Πnew = (Πnew
S , Πnew
[n] ) splitting the observed transcript to the corresponding indices
sent by the server and by the clients, respectively. Then, X[n] is independent of Πnew
S conditioned on
(Π = π, Πnew
[n] = πnew
[n] ).
Proof. First, we assume there are no erasures in Πnew
S . Consider the string Πnew
S : each bit in it
is a function of XS and the communication the server sees, that is, Πnew
S = f (XS , Π[n], Πnew
[n] ). It is
clear that if we fix and condition on a specific (Π = π, Πnew
[n] = πnew
[n] ), then Πnew
S = д(XS ) where
the function д is determined solely by π, πnew
[n] .
Pr(X[n] = x[n], Πnew
S = πnew
S | Π = π, Πnew
[n] = πnew
[n] )
= Pr(X[n] = x[n], f (XS , Π[n], Πnew
[n] ) = πnew
S | Π = π, Πnew
[n] = πnew
[n] )
= Pr(X[n] = x[n],д(XS ) = πnew
S | Π = π, Πnew
[n] = πnew
[n] )
by Lemma 3.1, XS and X[n] are independent, conditioned on any (partial) transcript,
= Pr(X[n] = x[n] | Π = π, Πnew
[n] = πnew
[n] ) Pr(д(XS ) = πnew
S | Π = π, Πnew
[n] = πnew
[n] )
= Pr(X[n] = x[n] | Π = π, Πnew
[n] = πnew
[n] ) Pr(Πnew
S = πnew
S | Π = π, Πnew
[n] = πnew
[n] ),
which completes the proof. The same holds if bits from Πnew
S are flipped or erased, since the noise
is independent of all the other variables.
Claim 5.21.
n
i=1
Eπ new
[n] |ES

I (Xi[pathi ()] | E)
 ≤ n logn.
Proof. Writing E explicitly in the claim’s statement, we have
n
i=1
Eπ new
[n] |ES I (Xi[pathi ()] | Π = π, PATH() = path(), Πnew
S = πnew
S , Πnew
[n] = πnew
[n] ).
For any i ∈ [n], Claim 5.20 suggests that the event Πnew
S = πnew
S is independent ofX[n] conditioned
on the transcript so far (and the paths, and so on). Therefore, conditioning on Πnew
S = πnew
S does
not change the (conditional) distribution of X[n], and we can remove the conditioning on Πnew
S =
πnew
S without affecting the information,
=
n
i=1
Eπ new
[n] |ES I (Xi[pathi ()] | Π = π, PATH() = path(), Πnew
[n] = πnew
[n] ),
by linearity of expectation and superadditivity of information (Lemma 2.3),
≤ Eπ new
[n] |ES I (X[n][path()] | Π = π, PATH() = path(), Πnew
[n] = πnew
[n] )
= I (X[n][path()] | Π = π, PATH() = path(), Πnew
[n] ),
Journal of the ACM, Vol. 65, No. 1, Article 4. Publication date: December 2017.                  
Constant-Rate Coding for Multiparty Interactive Communication Is Impossible 4:33
where Πnew
[n] is distributed according to Πnew
[n] conditioned on ES . Recall that Πnew
[n] contains up
to 0.1n logn bits (some may be erased); similarly, Πnew
[n] also contains at most 0.1n logn bits of
information. Using Lemma 2.4(2),
≤ I (X[n][path()] | Π = π, PATH() = path()) + 0.1n logn.
Now note that, conditioned on (Π = π, PATH() = path()), the variables X1,...,Xn are mutually
independent by Lemma 3.1, thus the superadditivity (Lemma 2.3) in this case satisfies an equality,
= 0.1n logn +
n
i=1
I (Xi[pathi ()] | Π = π, PATH() = path()).
Finally, since  is the server’s cutoff given the transcript π (and πnew
S ), we get
≤ 0.1n logn + 0.01n
≤ n logn.
Substituting the bounds in Claim 5.19 and Claim 5.21 back into Equation (16) completes the
proof of Lemma 5.18.
Now that we have bounded the information on the clients’ trees, we need to bound the information on the server’s tree as well [to satisfy Equation (6)]. This repeats the same methods we
have seen above, but in a slightly relaxed setting: the server is currently at the cutoff level, and the
communication πnew
[n] doesn’t give any new information on XS .
We denote by Z (k) = PATHS (k + ) the correct path of length k in XS , below the server’s cutoff
level. Given any π, πnew
S , path() define
S∗ 
k 




 πnew
[n] , path(k + )
 def
=
I

XS [pathS (k + )] 




 Π = π, Πnew = πnew , PATH(k + ) = path(k + )

,
S

k 




 πnew
[n] , xS [path()]
≤k  def
=
E
ρ∼PATH(k+)|xS [path()]
≤k, EI

XS [ρS (k + )] 




 XS [Z (0)]
≤k = xS [path()]
≤k , PATH(k + ) = ρ, E

.
Note that an immediate corollary of the derivation in Claim 5.13 is the following:
Corollary 5.22. Given any π, path(), and πnew = (πnew
S , πnew
[n] ), it holds that
Epath(k+)|ES∗ 
k 




 πnew
[n] , path(k + )

≤ ExS [path()]
≤k |ES

k 




 πnew
[n] , xS [path()]
≤k 
.
We can now continue to bound the sum of expectations of the quantities S∗ (k).
Lemma 5.23. Given any (π, πnew
S , path()) ∈ E(π,π new
S ,path(), ), and any πnew
[n] assuming Esilence ,
T
−
k=10
Epath(k+)|E 
S∗ 
k 




 πnew
[n] , path(k + )
	
≤ n · 2−0.2
√
n .
Proof. The proof follows at large the arguments of Lemma 5.14, and we repeat here the minimal
required details.
Lemma 2.12 asserts that
S (k | πnew , xS [path()]
≤k ) ≤ pmax (Z (k) | XS [path()]
≤k = xS [path()]
≤k , E)
× I (XS [Z (0)]
>k | XS [path()]
≤k = xS [path()]
≤k , E), (17)
Journal of the ACM, Vol. 65, No. 1, Article 4. Publication date: December 2017.                                       
4:34 M. Braverman et al.
where, again, the {Xi} of Lemma 2.12 are all the subtrees of XS rooted at the end of a path of
depth k + , whose prefix is pathS (). We note that those subtrees and (the last k edges in each of)
PATH(k + ) are independent conditioned on E, due to claim 5.12, and that the union of all these
subtrees is contained within XS [Z (0)]
>k .
Starting with the term in the Lemma’s statement, we use Corollary 5.22 and Equation (17) to get
T
−
k=10
Epath(k+)|E 
S∗ (k | πnew
[n] , path(k + ))	
≤
T
−
k=10
ExS [path()]
≤k |E 
S (k | πnew
[n] , xS [path()]
≤k )
	
≤
T
−
k=10
ExS [path()]
≤k |E 
pmax (Z (k) | XS [path()]
≤k = xS [path()]
≤k , E)
× I (XS [Z (0)]
>k | XS [path()]
≤k = xS [path()]
≤k , E)

. (18)
To ease the readability, in the following let us use the shorthand notation
E+ = 
XS [path()]
≤k = xS [path()]
≤k , E

.
Using a similar reasoning to the derivation of Equation (12), we now bound pmax (Z (k) | E+)
as a function of the information we have on labels below the cutoff. Again we think of Z (k) as
composed of n binary variables that each depends on a different user, Z (k) def
= (Z1 (k),...,Zn (k)),
and let a1 (k), a2 (k),..., an (k) be n paths of length k that attain the maximal probability. Recall
that Z1 (k),...,Zn (k) and XS [Z (0)]
≤k induce paths P1 (k),..., Pn (k) on X1,...,Xn, respectively.
Each Pi starts at the end of pathi () and is of length k. That path is uniquely determined by the
i-th bit of the labels along Z (k) in XS [Z (0)]. Then, we can write
pmax (Z (k) | E+) = Pr[label(P1 (k)) = a1 (k),...,label(Pn (k)) = an (k) | E+].
Via Corollary 3.2, the labels of Pi are independent of labels of Pj for j  i, conditioned on E+
(because these labels are just part of the variables Xi ), and the above equals
pmax (Z (k) | E+) =

i ∈[n]
Pr[label(Pi (k)) = ai (k) | E+]
≤

i ∈Q
Pr[label(Pi (k)) = ai (k) | E+]
≤

i ∈Q
max
P
i (k)
Pr[label(P
i (k)) = ai (k) | E+],
where Q here is the set of all clients that were completely erased in the new part. Since Esilence
occurs, we know that Q is non-empty, so we can choose a specific party i ∈ Q, assume all other
probabilities are 1, and get
pmax (Z (k) | E+) ≤ max
P
i (k)
Pr[label(P
i (k)) = ai (k) | E+].
We write E+ explicitly, and remind that for any party in Q (and specifically for our chosen
party i) the part πnew
i is completely erased and thus independent of the probability of seeing
a specific label in the input. Furthermore, as explained earlier, once we go over all the possible
paths P
i , the probability of label(P
i (k)) is merely the probability to see some labels in Xi in those
specific levels, and those are independent ofXS . Also, recall that, given the transcript π and the fact
Journal of the ACM, Vol. 65, No. 1, Article 4. Publication date: December 2017.                         
Constant-Rate Coding for Multiparty Interactive Communication Is Impossible 4:35
that party i was completely erased, πnew
S is a function of only Xi , which is again (conditionally)
independent of the probability to see certain labels in Xi (Corollary 3.2). We get,
pmax (Z (k) | E+) ≤ max
P
i (k)
Pr[label(P
i (k)) = ai (k) | Π = π, PATH() = path()].
Continuing with Equation (18),
≤
T
−
k=10
ExS [path()]
≤k |E max
P
i (k)
Pr[label(P
i (k)) = ai (k) | Π = π, PATH() = path()]
× I (XS [Z (0)]
>k | XS [path()]
≤k = xS [path()]
≤k , E)
=
T
−
k=10
max
P
i (k)
Pr[label(P
i (k)) = ai (k) | Π = π, PATH() = path()]
× ExS [path()]
≤k |EI (XS [Z (0)]
>k | XS [Z (0)]
≤k = xS [path()]
≤k , E)
=
T
−
k=10
max
P
i (k)
Pr[label(P
i (k)) = ai (k) | Π = π, PATH() = path()]
× I (XS [Z (0)]
>k | XS [Z (0)]
≤k , E)
with Lemma 2.4(3), and recalling that  is the server’s cutoff,
≤
T
−
k=10
max
P
i (k)
Pr[label(P
i (k)) = ai (k) | Π = π, PATH() = path()] × I (XS [Z (0)] | E)
≤
T
−
k=10
max
P
i (k)
Pr[label(P
i (k)) = ai (k) | Π = π, PATH() = path()] × 2−0.2
√
n,
which, by Lemma 2.14, is bounded by
≤ 
2Ii + 4

Ii + 20 · 2−10/3

2−0.2
√
n
with Ii ≤ I (Xi[pathi ()] | Π = π, PATH() = path()). We know that  is the server cutoff, which
implies
j ∈[n] Ij < 0.01n, and, thus, for any party and specifically for our chosen party i, we have
Ii < 0.01n. Then, for large enough n,
≤ 
2 · 0.01n + 4
√
0.01n + 20 · 2−10/3

· 2−0.2
√
n
≤ n · 2−0.2
√
n .
Finally, we can bound the expected increase of the cutof via Lemma 2.13. Similar to Proposition 5.9, given (π, πnew
S , path()) ∈ ES
(π,π new
S ,path(), ) consider the following two series of nonnegative random variables:

S˜(k) def
=
Eπ new
[n] ,path(k+)|π,π new
S ,path(),Esilence 
S∗ (k + 3 log logn | πnew
[n] , path(k + 3 log logn + ))	
k ≥0
and
Journal of the ACM, Vol. 65, No. 1, Article 4. Publication date: December 2017.                                                 
4:36 M. Braverman et al.
⎧⎪
⎨
⎪
⎩
C˜(k) def
= Eπ new
[n] ,path(k+3 log log n+)|π,π new
S ,path()
⎡
⎢
⎢
⎢
⎢
⎣
n
i=1
C∗
i (k + 3 log logn | πnew
[n] , path(k + ))
⎤
⎥
⎥
⎥
⎥
⎦
⎫⎪
⎬
⎪
⎭k ≥0
.
Lemma 5.23 shows that
k S˜(k) ≤ n · 2−0.2
√
n (it is bounded for any transcript πnew
[n] for which
Esilence occurs, and thus also in expectation over these transcripts). Lemma 5.18 (along with
Claim 5.16) proves that
k C˜(k) ≤ 21n. With these bounds, Lemma 2.13 then guarantees that the
expectation of the minimal round k∗ for which S˜(k∗) < 2−0.1
√
n as well asC˜(k∗) ≤ 0.01n is bounded
by
E[k∗
] ≤ 1 + n · 2−0.2
√
n
2−0.1
√
n +
21n
0.01n ≤ 2500.
We conclude that, for large enough n,
E

cutoff(π, Πnew ,X)








ES
(π,π new
S ,path(), )
, Esilence 
= Eπ new
[n] ,x |ES
(π,πnew
S ,path(), )
,Esilence [cutoff(π, πnew , x)]
≤  + 3 log logn + 2500
≤  + 4 log logn.
5.3 Bounding the Cutoff When Esilence Occurs: Proof of Proposition 5.11
In this section, we take care of the rare event Esilence where there is no subset of size √
n critical
players whose communication is completely erased. We claim that during 0.1 logn rounds in which
Esilence did not happen, the cutoff level cannot increase by more than O(n logn log logn).
The idea of this proof is to show that a transmission of a single bit (whether erased or not),
could be simulated by τ segments (each of 0.1 logn rounds), where we assume that in each one
of the segments Esilence occurs. That is, given a fixed (noisy) transcript B of 0.1 logn rounds in
which Esilence did not happen, we perform the following thought-experiment in which we are
given access to a special channel through which the parties perform 0.1 logn rounds of alternating
communication, and it is guaranteed that the erasure pattern induced by the channel satisfies
Esilence .
Using multiple utilization of the above special-channel, the parties simulate the transcript
B = B1 ··· B0.1n log n, bit by bit: the first bit, B1, is simulated by letting the party that sends B1 in
the original protocol input B1 again and again to the special channel10; all the other parties input random bits to the special channel. The above process repeats until B1 is not erased by the
special-channel and is received correctly by the other side. At this point, the parties continue to
simulate the second bit of B. Note that this is only a thought experiment, so we can assume an
all-knowledgable oracle that tells the parties when the simulation of a given bit succeeds, when to
stop, and so on.
It holds that simulating a single bit of B may take τ utilization of the special channel, where τ
is a random variable whose expectation is a small constant (and in particular, finite). This means
that the information that crossed the special channel during those τ segments (of 0.1 logn noisy
rounds each) bounds the information communicated by a single bit of B. Moreover, during the
first τ − 1 segments, no useful information has passed across the channel. Indeed, these segments
merely contain random bits erased by random noise and they have no effect on the cutoff. The
last block may increase the cutof; however, we know that Esilence occurred in this segment, and
10If B1 = ⊥, we can assume the party inputs random bits, but if this is the case, then it is clear that B1 can be simulated
using a single segment of simulation. In the following, we assume B1  ⊥.
Journal of the ACM, Vol. 65, No. 1, Article 4. Publication date: December 2017.             
Constant-Rate Coding for Multiparty Interactive Communication Is Impossible 4:37
thus the expected increase in the cutoff in this segment is bounded by O(log logn) as given by
Proposition 5.9 and Proposition 5.10. Therefore, simulating the entire block B bit-by-bit in the
above manner can increase the cutoff by at most O(n logn · log logn), in expectation.
Proof (Proposition 5.11). Let B = B1 ··· B0.1n log n be the observed transcript Πnew of a block
of communication with arbitrary erasure noise. For any i ∈ [0.1n logn], assume that Bi is communicated in the original protocol by pi ∈ S ∪ [n].
We simulate each Bi independently via multiple segments of 0.1 logn noisy rounds. Assume
that all parties except for pi try to communicate random bits in the simulation, and that pi tries
to communicate Bi . Let τi be the minimal number of 0.1 logn-round segments it takes until Bi
is communicated unerased across the channel, conditioned that, in each such segment, the event
Esilence occurs. Denote the (noisy) transcript of these segments by (Π1
i ,..., Πτi
i ) def
= Πi . It is easy
to verify that E[τi] < ∞, in fact, E[τi] is bounded by a small constant.
We begin by claiming that Πi = (Π1
i ,..., Πτi
i ) contains the information in Bi , and other information that is independent of the inputs. Therefore, conditioned on any previous communication π
,
the cutoff given the simulation transcript Πi is equal to the cutoff given Bi .
Claim 5.24. For any π
,
E[cutoff(π  ◦ Bi,X)] = E[cutoff(π  ◦ Π1
i ··· Πτi
i ,X)].
Proof. Via a trivial reordering of indices, we can write Π1
i ··· Πτi
i as (Bi, RND, NOISE1
i ···
NOISEτi
i ), where RND is the string of random bits communicated by all the other parties and
NOISEj
i is the erasure pattern observed in the j-th block of the simulation, Πj
i ; note that we
assume that Esilence occurs in all such blocks. Moreover, note that for each j, NOISEj
i have exactly
the same distribution: the noise pattern depends only on the identities of critical parties, yet these
are fully determined by π  and the inputs up to the cutoff assuming π
. In particular, they are
independent of RND and the other noise patterns.
Furthermore, the noise pattern is conditionally independent of the inputs X, given (π
,
path(
), Esilence ). The event Esilence restricts the noise to one that fully corrupts at least √
n critical
parties. The identity of these parties is only a function of being critical or not. Hence, additionally
conditioning on the specific inputs X the parties may hold does not change the distribution.
Using the above argument, X is conditionally independent of all the information in Π1
i ··· Πτi
i
except for Bi . Then, for any k, path(k) it holds that
I

XS [path
S (k)] 




 Π = π
, PATH(k) = path
(k), Πnew = (Π1
i ,..., Πτi
i )

= I

XS [path
S (k)] 




 Π = π
, PATH(k) = path
(k), Πnew = (Bi, RND, NOISE1
i ··· NOISEτi
i )

= I

XS [path
S (k)] 




 Π = π
, PATH(k) = path
(k), Πnew = Bi

.
A similar argument applies for
j ∈[n] I (Xj[path
j (k)] | Π = π
, PATH(k) = path
(k), Πnew = Bi ).
The claim then follows by the definition of the cutoff.
Next we wish to bound the cutoff increase due to Bi by bounding the cutoff increase in the
simulation. Since each segment in the simulation is one in which Esilence happened, we can bound
the expected increase in the cutoff via Propositions 5.9 and 5.10.
Proposition 5.25. Given any (π
, path(
),
) ∈ E(π
,path(), ), and any Bi ,
E[cutoff(π  ◦ Π1
i ··· Πτi
i ,X)] <  + O(log logn).
Proof. First we note that all the segments Π1
i ··· Πτi−1
i have no effect on the cutoff. Indeed, as
argued above, these transcripts contain only random bits and random noise patterns, which are
Journal of the ACM, Vol. 65, No. 1, Article 4. Publication date: December 2017.                                     
4:38 M. Braverman et al.
conditionally independent of X given E(π
,path(), ). The only block that increases the cutoff is the
last one, Πτi .
In the last block, Esilence occurs and we can use Propositions 5.9 and 5.10 to bound the progress
of the cutoff. We must also condition on the event wherepi is not blocked in this segment. However,
the probability for a party not to be blocked in a particular segment in which Esilence occurred is
rather high. In particular, it is easy to verify that (assuming large enough n)
Pr[pi is not completely blocked | Esilence ] > 1/2. (19)
Indeed, recalling we assume that the underlying channel is BEC1/3, we have
Pr[pi is completely blocked | Esilence ] ≤ Pr[pi is completely blocked]
Pr[Esilence ] ≤
1
3
0.1 log n
1 − 2−
√
n ≤ n−0.1
.
Denote by E˜ the event (Esilence , E(π
,path(), ),pi is not completely blocked in Πτi
i ), then we
have
E[cutoff(π  ◦ Πi,X) −
|E
]
=
∞
c=1
Pr[cutoff(π  ◦ Πi,X) −  ≥ c | E
]
≤
∞
c=1
Pr[cutoff(π  ◦ Πi,X) −  ≥ c | Esilence , E(π
,path(), )]
Pr[pi is not completely blocked in Πτi
i ]
≤
∞
c=1
2 Pr[cutoff(π  ◦ Πi,X) −  ≥ c | Esilence , E(π
,path(), )]
≤ 2 · O(log logn) = O(log logn),
where the last transition follows from Propositions 5.9 and 5.10: Denote the variables of those
propositions with hats, and set πˆ = π  ◦ Π1
i ◦···◦ Πτi−1
i , the new transcript is the Π
new = Πτi
i ,
and ˆ
 =
. The penultimate transition follows from Equation (19).
Proposition 5.25 and Claim 5.24 together prove that for any (π
, path(
),
) ∈ E(π
,path(), ),
and any Bi ,
E[cutoff(π  ◦ Bi,X)] <  + O(log logn).
That is, a single bit of communication in the original protocol increases the cutoff in expectation
by at most O(log logn), regardless of the noise that occurred in B. Using the above repeatedly
bit-by-bit over the 0.1n logn bits of Πnew = B, we get that this segment increases the cutoff by at
most O(n logn log logn) in expectation, which completes the proof of Proposition 5.11.
6 CONCLUSION AND OPEN QUESTIONS
In this article, we have shown a lower bound of Ω(logn/ log logn) on the communication of any
protocol for the pointer jumping task over a star, assuming each channel is a BSC. This implies
that the best interactive coding in this setting has a code rate of at most O(log logn/ logn). In
particular, the coding of Rajagopalan and Schulman [36] is optimal, up to log logn terms. Towards
this end, we introduced the “cutoff” of a protocol—a new information-theory notion via which we
were able to bound how much progress a coding scheme could have made, in terms of the progress
of the underlying noiseless protocol.
It is already well established that topology matters in communication [9] and in network coding [32]. Our work (along with previous results [2, 36]) suggests that the same holds also for the
field of interactive communication when the noise is random. While for certain topologies (e.g.,
Journal of the ACM, Vol. 65, No. 1, Article 4. Publication date: December 2017.                                                     
Constant-Rate Coding for Multiparty Interactive Communication Is Impossible 4:39
a line, a cycle, a complete graph) one can achieve a coding scheme with slowdown O(1), other
topologies necessitate a slowdown of Θ(logn/ log logn), e.g., the star topology. The main open
question is to better characterize the way topology affects slowdown.
Open Question 1. For any function f (n) ∈ o(logn), define the exact set of topologies for which
n-party interactive coding schemes with f (n) slowdown exist. In particular, characterize the set of
topologies for which n-party interactive coding schemes with O(1) slowdown exist.
While Reference [36] shows that, given any topology, interactive coding with O(logn) slowdown exists, our lower bound demonstrates a necessary slowdown of only Ω(logn/ log logn).
This gap leads to the following question:
Open Question 2. Show a topology (if such exists) for which Ω(logn) slowdown is necessary for
n-party interactive coding.
Currently, we do not have a candidate topology for an ω(logn/ log logn) slowdown, when the
parties communicate bits.
6.1 Channels with a Large Alphabet Size
Another interesting question asks what happens if the parties are allowed to send symbols from an
alphabet Σ whose size is super-constant, say, |Σ| = logΩ(1) n. Interestingly, assuming such a large
alphabet, one can get a stronger lower bound on the blowup of the round complexity and thus of
the communication complexity. Namely, a blowup of O(logn), i.e., without the log logn term, can
be exhibited for interactive coding of the pointer jumping task over a star network.
The intuitive explanation comes from examining the protocol of the upper bound (Theorem 1.2):
the protocol suggests that the clients can always encode a subtree of their input of depth log logn
using O(logn) bits. The lower bound (Theorem 5.1) suggests that this is the best they can do.
However, when the parties send symbols from a larger alphabet, the depth of the subtree they
can communicate by sending O(logn) symbols substantially decreases to a constant. For example,
say that each symbol comes from an alphabet Σ (so the clients’ input tree is |Σ|-ary tree rather
than a binary tree, and every edge is labeled by a symbol from Σ). Then, encoding d levels of the
tree requires |Σ|
d symbols. Thus, assuming |Σ| = logΩ(1) n, one can communicate only a constant
number of levels when restricted to sending O(logn) symbols. This intuition implies that during
logn rounds, the cutoff advances by at most O(1) in expectation, which in turn implies a bound
on the rate of O(1/ logn).
See also Reference [19] for a subsequent work that uses the above approach to prove a lower
bound of Ω(logn) on the communication blowup over a ring network, assuming a large alphabet
of size Θ(logn).