One powerful theme in complexity theory and pseudorandomness in the past few decades has been the use
of lower bounds to give pseudorandom generators (PRGs). However, the general results using this hardness
vs. randomness paradigm suffer from a quantitative loss in parameters, and hence do not give nontrivial
implications for models where we donâ€™t know super-polynomial lower bounds but do know lower bounds
of a fixed polynomial. We show that when such lower bounds are proved using random restrictions, we can
construct PRGs which are essentially best possible without in turn improving the lower bounds.
More specifically, say that a circuit family has shrinkage exponent Î“ if a random restriction leaving a p fraction of variables unset shrinks the size of any circuit in the family by a factor of pÎ“+o(1)
. Our PRG uses a seed of
length s1/(Î“+1)+o(1) to fool circuits in the family of size s. By using this generic construction, we get PRGs with
polynomially small error for the following classes of circuits of size s and with the following seed lengths:
(1) For de Morgan formulas, seed length s1/3+o(1)
;
(2) For formulas over an arbitrary basis, seed length s1/2+o(1)
;
(3) For read-once de Morgan formulas, seed length s.234...;
(4) For branching programs of size s, seed length s1/2+o(1)
.
The previous best PRGs known for these classes used seeds of length bigger than n/2 to output n bits, and
worked only for size s = O(n) [8].
CCS Concepts: â€¢ Theory of computation â†’ Pseudorandomness and derandomization; Expander
graphs and randomness extractors; Circuit complexity;
Additional Key Words and Phrases: Shrinkage, random restrictions, average-case lower bounds
1 INTRODUCTION
Two of the most important general challenges for complexity are to prove constructive lower
bounds for non-uniform measures of computational complexity such as circuit size, and to show
that randomized algorithms have efficient deterministic simulations. The â€œHardness vs. Randomnessâ€ paradigm [1, 6, 7, 21, 23, 36] shows that these questions are linked. More precisely, these
results show how to use any problem that is hard for a class of circuits to create a pseudorandom
generator (PRG) for the same class of circuits. This PRG can then be used to construct a relatively
efficient deterministic version of any probabilistic algorithm with a corresponding complexity.
This has been used to create unconditional PRGs for circuit classes with known lower bounds,
such as for AC0, as well as for obtaining conditional resultsâ€”implications between the existence of
hard problems and derandomization for classes where no strong lower bounds are known. In the
converse direction, it is easy to see that any PRG for a circuit class immediately gives a corresponding lower bound for the class. Somewhat more surprisingly, it has been shown that any efficient
deterministic simulation of some probabilistic algorithms would yield circuit lower bounds [14,
17, 35]. This hardness vs. randomness connection is one of the most important tools in computational complexity. It formalizes the intuition that efficient algorithms for â€œmeta-computational
problemsâ€, where the input is a computational device from a certain class, is linked to our ability
to prove lower bounds for that class.
However, being so general comes at a quantitative price. Ideally, the stretch of a PRG (the output
length as a function of the input length) equals the known lower bound. However, in the hardnessto-randomness constructions, there are a number of stages that each lose a large polynomial factor.
In particular, this means that, for example, a quadratic or cubic circuit lower bound for a class does
not immediately give any nontrivial PRG. For completely generic, â€œblack-boxâ€, reductions between
a hard problem and a PRG, some of these costs are inherent [5, 11, 28, 34]. In particular, this is an
issue for those models where super-linear but not super-polynomial bounds are known, such as
Boolean formulas.
In this work, we show a general method for obtaining tight â€œhardness-to-randomnessâ€ results
from the proofs of lower bounds, rather than as a black-box consequence of the lower bounds. In
particular, our methods apply to lower-bound proofs that involve restricting some of the inputs
to the circuit. Our construction goes in two stages. We start with a lower bound proved by the
following kind of shrinkage argument: if we restrict a size s circuit leaving a p fraction of variables
unset, the expected size of the restricted circuit isO(pÎ“s). The best Î“ for which this holds is known
as the â€œshrinkage exponentâ€ of the circuit class. The first stage of our construction is to derandomize
the shrinkage argument, showing that there is a distribution with similar shrinkage properties that
can be sampled with few bits. This stage of our argument is general, but not totally generic. While
the same general construction and analysis ideas work in a variety of models, the details depend
on the model. Then we show how to go from such a distribution on restrictions to a PRG. This
part is generic, being identical for all models, and is closely related to the generator from [24]. The
total number of bitsr used by the generator is roughly s1/(1+Î“) times the number of bits needed to
sample from the distribution on restrictions.
Every generator using r bits to fool tests of size s = s(r) immediately gives a problem requiring
size Î©(s(r)) to compute in the model. So, if our function s(r) is close to the known lower
bounds, this shows that we have essentially converted all of the â€œhardnessâ€ in the lower bound to
â€œrandomnessâ€. This is indeed the case for a variety of natural models of computation. For Boolean
formulas over the de Morgan basis, we give a generator with s(r) = r 3âˆ’o(1)
, almost matching the
known lower bound of s(n) = Î©(n3/(log2 n log logn) due to HÃ¥stad ([12] with a polylogarithmic
Journal of the ACM, Vol. 66, No. 2, Article 11. Publication date: March 2019.
Pseudorandomness from Shrinkage 11:3
improvement by Tal [31], based on earlier work by [4, 15, 25, 30]). To avoid technicalities, we
assume that the size s is at least the number of input variables n in the following statements:
Theorem 1.1. For any constant c > 0, there is an explicit PRG using a seed of length s1/3 Â·
2O (log2/3 s ) = s1/3+o(1) random bits that sâˆ’c -fools formulas of size s over the de Morgan basis.
For Boolean formulas over an arbitrary basis, our generator has stretch s(r) = r 2âˆ’o(1)
, almost
matching the Khrapchenko bound of s(n) = Î©(n2) [18].
Theorem 1.2. For any constant c > 0, there is an explicit PRG using a seed of length s1/2 Â·
2O (log1/2 s ) = s1/2+o(1) random bits that sâˆ’c -fools formulas of size s over an arbitrary basis.
For branching programs, with size being the total number of edges, we get a similar bound.
Theorem 1.3. For any constant c > 0, there is an explicit PRG using a seed of length s1/2 Â·
2O (log1/2 s ) = s1/2+o(1) random bits that sâˆ’c -fools branching programs of size at most s.
Note that similar to the case for arbitrary formulas, the above nearly matches known-size lower
bounds for branching programs.
We also consider the case of read-once formulas over the de Morgan basis. Here, there is no
sensible notion of lower bound, since all functions computable in the model have size exactly n,
but the notion of shrinkage is defined. The optimal shrinkage exponent for such read-once de
Morgan formulas was shown by [13] and [25] to be Î“ = log 2/ log(
âˆš
5 âˆ’ 1) = 3.27 ... ; using this
result, we get a PRG with stretch s(r) = Î©(r 4.27...).
Theorem 1.4. For any constant c > 0, there is an explicit PRG using a seed of length s1/(Î“+1) Â·
2O (log2/3 s ) = s1/(Î“+1)+o(1) random bits that sâˆ’c -fools read-once formulas of size s over the de Morgan
basis, where Î“ = log 2/ log(
âˆš
5 âˆ’ 1) = 3.27 ....
Any substantial improvement in our PRGs would thus yield better lower bounds than what is
currently known.
Our results dramatically improve previous work. The only directly comparable PRG was by
Bogdanov et al. [8], who constructed a PRG using a (1 âˆ’ Î©(1))n bit seed to output n bits that fool
read-once formulas and read-once branching programs, where the order of the bits is unknown
beforehand.
There has been significant work on read-once branching programs where the order of the bits
is known in advance (e.g., [16, 21, 24]), but that is a much simpler model and the generators of [16]
and [21] are known to fail if the order of bits is unknown [8].
1.1 Outline of Constructions
Our techniques build upon those of [16, 22, 24]. The intuition behind all of these PRGs is to exploit
communication bottlenecks in the computation. Suppose the random inputs to a computation can
be partitioned into two subsets X and Y, and the computation can be simulated with k bits of
communication between these two subsets. Then, given the communication pattern, the two sets
of bits have high entropy conditioned on each other. Then, instead of using independent bits for
the two sides, we can use a randomness extractor to convert the conditional entropy of X given the
communication into random bits to be used in place of Y.
Our construction follows the same basic intuition. The key insight is that shrinkage under random restrictions is a form of communication bottleneck, betweenX, the set of variables with values
specified by the restriction Ï, and Y, the set of variables left unrestricted (and chosen later). Consider a one-way protocol where the player knowing X has to send a message allowing the Y-player
Journal of the ACM, Vol. 66, No. 2, Article 11. Publication date: March 2019.
11:4 R. Impagliazzo et al.
to compute the function f . What this message exactly needs to specify is the restricted function
fÏ . If the circuit size of fÏ is small, much smaller than the size of X, the message can be the circuit
computing the restricted function, showing low communication.
Most of the previous constructions were for computational models like read-once branching
programs, where one had an explicit description of which sets X and Y had a communication
bottleneck, and there was a hierarchical structure available on such sets so that the construction
could be defined recursively. Here, we do not have either one, but we know the bottleneck occurs
for most sets X and their complements. Instead of explicitly partitioning the variables into blocks
with low communication, we randomly sample sets that exhibit this bottleneck until all variables
are covered. So far, we are not able to utilize recursion, which blocks us from making the seed size
subpolynomial (and hence proving superpolynomial lower bounds).
More concretely, consider the case of read-once width w branching programs, where the bits
may be read in any order (as opposed to some fixed order, which is the setting of Nisan [22]). In
this arbitrary-order case, we show that the Nisan-Zuckerman PRG [24], without recursion, gives a
PRG with seed length OËœ (
âˆš
n). Recall that this PRG uses an extractor E : {0, 1}
s Ã— {0, 1}
d â†’ {0, 1}
m
and is defined by G(x,y1,...,yt ) = E(x,y1)E(x,y2) ... E(x,yt ), where x âˆˆ {0, 1}
s and yi âˆˆ {0, 1}
d .
To see that this works, suppose a branching program accepts a uniform input with significantly
different probability than the output of G. By a hybrid argument, changing some Zi = E(X,Yi ) to
uniform must change the probability significantly (note that Yi are independent of each other).
However, if we fix all bits except the m bits corresponding to Zi , we are left with a read-once
branching program on these m bits. There are at most wwm such branching programs on m bits.
Thus, if we condition on a typical such branching program for thesem bits, X still has min-entropy
at least s âˆ’ 2mw logw. As long as this exceeds the min-entropy requirement of the extractor, the
extractor output is close to uniform, contradicting the assumption of significantly different acceptance probabilities. We can set t = m = âˆš
n, s = 4mw logw, and d = O(logn).
For general branching programs, we need to handle variables that are read many times, which
we can do by pseudorandomly permuting the output of the above generator. However, for general
formulas and to get a general reduction, we need to extend the above generator. We do this by
combining the extractor outputs with pseudorandom restrictions that shrink with high probability (and leave every bit unfixed with the same probability). Specifically, for a restriction Ï that
leaves m bits unfixed, we can define the random variable VÏ âˆˆ {0, 1}
n that takes the values of Ï for
the fixed bits and the values of E(X,YÏ ) for the unfixed bits. We do this for enough independent
pseudorandom restrictions that with high probability every coordinate has some Ï which leaves
that coordinate unfixed (via a coupon collector bound). The PRG output is the XOR of all these VÏ .
In fact, the above achieves the desired bounds only when the shrinkage Î“ = 1. For larger shrinkage, we must also apply a k-wise independent distribution to E(X,YÏ ).
Derandomized Shrinkage Bounds. To use our main generator construction, we need a family of
random restrictions that can be sampled with few random bits, and still causes the circuits to
shrink. For branching programs and formulas over an arbitrary basis (shrinkage exponent Î“ = 1),
these are not too hard to get by taking O(logn)-wise independent random restrictions. For formulas over the de Morgan basis and read-once formulas getting such restrictions is far trickier.
The first difficulty we face is that HÃ¥stadâ€™s original proof [12] only shows shrinkage in expectation and does not give a high probability bound for the formulas to shrink. We get around this
difficulty as follows: Let f be a formula over the de Morgan basis. We first show shrinkage under
restrictions for which the probability of being unset p = nâˆ’Î± for some Î± = o(1) and have k = no(1)
-
wise independence. By repeating this process independently, we get shrinkage for all values of p
(both in the known lower bounds and in our PRG construction we need p âˆ¼ n1/(Î“+1)
). To do this,
Journal of the ACM, Vol. 66, No. 2, Article 11. Publication date: March 2019.
Pseudorandomness from Shrinkage 11:5
we decompose the target formula f into O(n/) subformulas Ğ´i of size at most , for a suitable
 < k. Since each Ğ´i now has size at most k, the behavior of Ğ´i under restrictions should be the
same under k-wise independent restrictions or truly random restrictions. Thus, we can roughly
expect each Ğ´i to shrink by pÎ“ in expectation.
For read-once formulas, the events that the different Ğ´i shrink are independent, and hence, by
a Chernoff bound, with high probability the total shrinkage is as promised. For the read-t case
(each variable appears at most t times in the formula), we partition the subformulas into t + 1
color classes, such that within a color class the subformulas are on disjoint sets of variables. We
can then proceed as in the read-once case. For the general case, we condition on heavy variables
(the ones that appear many times) in a subtle way and reduce to the read-t case.
1.2 Related Work
Independently and concucurrently to this work, Komargodski and Raz [19] showed an averagecase lower bound for de Morgan formulas nearly matching Andreevâ€™s [4] worst-case lower bound:
Komargodski and Raz [19] give an explicit function on n variables such that any de Morgan
formula of size n2.499 agrees with the function on at most 1/2 + Îµ fraction of the inputs, where
Îµ = exp(nâˆ’Î©(1)
) is exponentially small. In the course of showing their result, Komargodski and Raz
also show that shrinkage happens with high probability as opposed to in expectation, which compares to Lemmas 4.2 and 4.8 in this work. However, the corresponding results in [19] work with
truly random restrictions and achieve an exponent of 1.5 for de Morgan formulas.
1.3 Subsequent Work
Subsequent to our work, Komargodski et al. [20] used ideas from our article and [19] to improve
the average-case lower bound for de Morgan formulas. Specifically, they gave an explicit function
on n variables such that any de Morgan formula of size n3âˆ’o(1)
/r 2 agrees with the function on at
most 1/2 + 2âˆ’r fraction of the inputs.
Trevisan and Xue [32] used ideas related to ours to give a derandomized switching lemma and
an improved PRG for AC0.
Progress has also been made on PRGs fooling small-width read-once branching programs [9, 26,
29], where the order of the bits is unknown beforehand. However, our PRG fools the much larger
class of branching programs of a given size.
2 PRELIMINARIES
We start with some definitions and notations.
â€”For a restriction Ï âˆˆ {0, 1, âˆ—}n, let the set of active variables be A(Ï) = {i : Ïi = âˆ—}.
â€”For Ï âˆˆ {0, 1, âˆ—}n and f : {0, 1}
n â†’ {0, 1}, define the Ï-restricted function f Ï : {0, 1}
A(Ï) â†’
{0, 1} by f Ï (y) = f (x), where x âˆˆ {0, 1}
n satisfies xi = yi,i âˆˆ A(Ï) and xi = Ïi otherwise.
When there is no ambiguity about subscripts, we sometimes use fÏ to denote f Ï .
â€”Call a distribution D on {0, 1, âˆ—}n p-regular if for every i âˆˆ [n], PrÏâ†D[Ïi = âˆ—] = p. We say
D is k-wise independent if any k coordinates of D are independent. There exist explicit
k-wise independent distributions samplable withO(k(logn) log(1/p)) random bits (see e.g.,
[3]).
â€”For a class of functions F on n variables, we say s : F â†’ N \ [n] is a size function if
|{f âˆˆ F : s(f ) â‰¤ m}| â‰¤ mO (m) for m â‰¥ n. By default, we shall assume that F is closed under negating the input variables and for any f âˆˆ F , s(Ğ´) = O(s(f )) if Ğ´ is obtained from f
by negating some input variables. We also assume that any f âˆˆ F depends on at most s(f )
variables.
Journal of the ACM, Vol. 66, No. 2, Article 11. Publication date: March 2019.    
11:6 R. Impagliazzo et al.
â€”We say two distributions D, D (on the same universe) are Îµ-close if the statistical distance
between D and D is at most Îµ.
â€”We say a generator G : {0, 1}
r â†’ {0, 1}
n Î´-fools a function f : {0, 1}
n â†’ {0, 1} if





Pr x âˆˆu {0,1}n
[f (x) = 1] âˆ’ Pr
y âˆˆu {0,1}r
[f (G(y)) = 1]





â‰¤ Î´ .
Similarly, we say G Î´-fools a class of functions F if G Î´-fools all functions in F . We refer
to the parameter r as the seed-length of the generator G and say G is explicit if G can be
computed in poly(n, 1/Î´ ) time.
â€”Throughout, we use uppercase letters for random variables and lowercase for constants.
As mentioned in the introduction, our generator is motivated by the pseudorandom generator
for small space machines of Nisan and Zuckerman [24]. As in their article, our construction will
make use of extractors for linear min-entropy sources.1
Definition 2.1 (Extractor). We say E : {0, 1}N Ã— {0, 1}
d â†’ {0, 1}
m is a (k,Îµ)-extractor if for every
random variable X over {0, 1}N withHâˆ(X) â‰¥ k, and Y âˆˆu {0, 1}
d , E(X,Y ) isÎµ-close to the uniform
distribution on {0, 1}
m. We say E( ) is explicit if it can be computed in time poly(N,d).
We use the following explicit extractor as given by the work of Zuckerman [37], with a modest
lower bound on Îµ eliminated by the work of Guruswami et al. [10].
Theorem 2.2 [10, 37]. For any Îµ > 0, there is an explicit function E : {0, 1}N Ã— {0, 1}
d â†’ {0, 1}
m
that is an (N/2,Îµ)-extractor with m = N/4 and d = O(log(N/Îµ)).
We will use well-known constructions of k-wise independent random variables.
Lemma 2.3 [2]. For every k â‰¥ 2 there is a PRG Gk : {0, 1}
rk â†’ {0, 1}
n generating a k-wise independent distribution with seed length rk = 1 + k/2 log(2n).
We also use the following large-deviation bounds for k-wise independent random variables. The
first is an easy corollary of Theorem 4 in [27].
Lemma 2.4. Let a1,..., an âˆˆ R+ with maxi ai = m, and suppose X1,...,Xn âˆˆ {0, 1} are k-wise independent indicator random variables with Pr[Xi = 1] = p. LetX =
i aiXi and Î¼ = E[X] = p
i ai .
Then, Pr[X â‰¥ 2k(m + Î¼) ] â‰¤ 2âˆ’k .
When the expectation is small, the following simple lemma sometimes gives better bounds:
Lemma 2.5. Suppose X1,...,Xn âˆˆ {0, 1} are k-wise independent indicator random variables with
Pr[Xi = 1] = p. Let X =
i Xi and Î¼ = E[X] = np. Then, Pr[X â‰¥ k] â‰¤ Î¼k /k!.
Proof. This probability is at most
n
k

pk â‰¤ (np)
k /k!.
3 PSEUDORANDOM GENERATORS FROM SHRINKAGE
We now describe our main construction which allows us to use classical lower bound arguments
based on random restrictions to get pseudorandom generators (PRGs). Our main result will apply
to any class of functions with nontrivial â€œshrinkage exponentâ€. We next define this central notion.
Definition 3.1. Let F be a class of functions with an associated size function s : F â†’ R+ and
let D be a p-regular distribution on {0, 1, âˆ—}n. We say F has shrinkage exponent Î“ with respect to
D if, for all f âˆˆ F ,
E
Ïâ†D[s(fÏ )] = O(pÎ“ Â· s(f ) + 1).
1The min-entropy of a variable X is defined by Hâˆ(X ) â‰¡ âˆ’ maxx (log2 (Pr[X = x])).
Journal of the ACM, Vol. 66, No. 2, Article 11. Publication date: March 2019.               
Pseudorandomness from Shrinkage 11:7
We say F has Îµ-shrinkage exponent Î“ w.r.t D if, there exists a constant c such that for all f âˆˆ F ,
Pr
Ïâ†D

s(fÏ ) > c (pÎ“
s(f ) + 1) Â· log(1/Îµ)

â‰¤ Îµ.
The shrinkage exponent is a classical concept in complexity theory with its origins going back
to the very first lower bounds of Subbotovskaya [30]. The best lower bounds we know for several
important complexity classes such as read-once formulas, de Morgan formulas are based on estimating the shrinkage exponent of the associated class. This connection can be summarized in the
following informal statement:
Theorem 3.2 [4]. If a class F has shrinkage exponent Î“, then there is an explicit Boolean function
h : {0, 1}
n â†’ {0, 1} that cannot be computed by functions in F of size at most nÎ“+1/poly(logn).
Our main result shows that with some additional guarantees on the behavior of F under random restrictions, one can actually get very strong average-case lower bounds, PRGs, for F . Our
construction and its analysis are quite different from that of Andreev, and give the first pseudorandom generators with o(n) seed-length for several well-studied classes of functions like read-once
formulas, de Morgan formulas, branching programs of linear size.
Theorem 3.3. Fix Îµ > 0 and let F be a class of functions from {0, 1}
n to {0, 1} with an associated size function s : F â†’ N. Fix s > 0 and let p = 1/s1/(Î“+1)
. Let Dp be a p-regular distribution on
{0, 1, âˆ—}n such that F hasÎµ-shrinkage exponent Î“ w.r.t D. Then, there exists an explicit pseudorandom
generator G : {0, 1}
r â†’ {0, 1}
n that Î´-fools all functions of size at most s in F for Î´ = O(Îµ Â· r) and
has seed-length
r = O

(R(s) + log(s/Îµ)) Â· log(n/Îµ) Â· s1/(Î“+1)

,
where R(s) denotes the number of bits needed to efficiently sample from Dp .
Proof of Theorem 3.3. Here is a high-level description of the generator. We use the restriction family Dp to sample t restrictions Ï1,..., Ït so that together, the set of active (âˆ—) variables
in them covers [n] (with high probability). We next have to choose the assignments for the active
variables in the restrictions. Instead of choosing these variables independently (which would lead
to no gains in the number of bits used), we use a single string X and independent seeds Y1,...,Yt
(which are much shorter) to set the values for the unassigned variables in the restrictions according
to E(X,Y1), E(X,Y2),..., E(X,Yt ) where E( ) is an explicit extractor as given by Theorem 2.2. Our
PRG then outputs the XOR of these t strings. If the shrinkage exponent Î“ > 1, then the extractor
output isnâ€™t long enough, and we actually use Gk (E(X,Yi )) in place of E(X,Yi ), where Gk generates a suitably chosen k-wise independent distribution. In order to just have one construction, we
actually use Gk regardless of Î“.
More formally, fix p = 1/s1/(1+Î“) and t = log(n/Îµ)/p. For k to be chosen later, let Gk :
{0, 1}
rk â†’ {0, 1}
n be a PRG generating a k-wise independent distribution on {0, 1}
n, from
Lemma 2.3. Let N â‰¥ 4rk and let E : {0, 1}N Ã— {0, 1}
d â†’ {0, 1}
rk be an explicit extractor that works
for entropy rate at least 1/2 sources with d = O(log(N/Îµ)) and error at most Îµ as given by
Theorem 2.2.
We now describe our PRG by giving a randomized algorithm to compute its output.
(1) Sample t independent restrictions Ï1, Ï2,..., Ït from Dp .
(2) Sample X âˆˆu {0, 1}N and Y1,...,Yt âˆˆu {0, 1}
d independently.
(3) For 1 â‰¤ i â‰¤ t, let Zi = Gk (E(X,Yi )).
(4) For 1 â‰¤ i â‰¤ t, define Vi âˆˆ {0, 1}
n by (Vi )j = (Zi )j if j âˆˆ A(Ïi ) and (Ïi )j otherwise.
(5) Output V â‰¡ G(Ï1,..., Ït,X,Y1,...,Yt ) = V1 âŠ• V2 âŠ•Â·Â·Â·âŠ• Vt , where âŠ• denotes bit-wise
XOR.
Journal of the ACM, Vol. 66, No. 2, Article 11. Publication date: March 2019.  
11:8 R. Impagliazzo et al.
We will show that for N = OËœ (pÎ“s) sufficiently large, functions in F of size at most s cannot
distinguish V from a truly random string. We will do so by a hybrid argument. To this end, let Z
i
be independent uniformly random strings in {0, 1}
n and with Ï1,..., Ït as in the definition of V ,
define Ui âˆˆ {0, 1}
n, 1 â‰¤ i â‰¤ t, by (Ui )j = (Z
i )j if j âˆˆ A(Ïi ) and (Ïi )j otherwise.
For 0 â‰¤ i â‰¤ t, letWi = U1 âŠ•Â·Â·Â·âŠ• Ui âŠ• Vi+1 âŠ• Vi+2 âŠ•Â·Â·Â·âŠ• Vt . Then,W0 â‰¡ V andWt = U1 âŠ•Â·Â·Â·âŠ•
Ut . We first observe that Wt is Îµ-close to the uniform distribution on {0, 1}
n.
Claim 3.4. The distribution of Wt is Îµ-close to the uniform distribution on {0, 1}
n.
Proof. Observe that if âˆªt
i=1A(Ïi ) = [n], then Wt is exactly the uniform distribution on {0, 1}
n.
Thus, it suffices to bound the probability that âˆªt
i=1A(Ïi )  [n]. Now, as Dp is p-regular, for every
i âˆˆ [t], j âˆˆ [n], Pr[j âˆˆ A(Ïi )] = p and these events are independent for differenti âˆˆ [t]. Thus, Pr[j
âˆªt
i=1A(Ïi )] = (1 âˆ’ p)
t â‰¤ Îµ/n. The claim now follows by a union bound.
From the above claim, it suffices to show that F cannot distinguish W0 from Wt . Fix a f âˆˆ F
with s(f ) â‰¤ s and i â‰¥ 1. We will show that f cannot distinguish between Wiâˆ’1 and Wi .
Claim 3.5. For i â‰¥ 1, |Pr[f (Wiâˆ’1) = 1] âˆ’ Pr[f (Wi ) = 1]| â‰¤ 3Îµ.
Proof. Let W = U1 âŠ•Â·Â·Â·âŠ• Uiâˆ’1 âŠ• Vi+1 âŠ•Â·Â·Â·âŠ• Vt . Then, Wiâˆ’1 â‰¡ W âŠ• Vi and Wi = W âŠ• Ui . The
intuition behind our argument is as follows: Note that W does not depend on Ïi,Yi . Let fW :
{0, 1}
n â†’ {0, 1} be given as fW (x) = f (x âŠ• W ). Now, by our assumption about the shrinkage
exponent of F , for any fixing of W , with high probability over the choice of Ïi , s(fW Ïi ) â‰¤
cpÎ“s log(1/Îµ) = s0. Let E denote this event. Observe that conditioned on E, the restricted function
fW Ïi can be described with roughly O(s0 log s0) bits (as it has size at most s0). We then argue that
under conditioning on E (which is independent of Yi ), X has min-entropy at least N âˆ’ O(s0 log s0)
even when given the function Ğ´ = fW Ïi . Therefore, for N sufficiently large, E(X,Yi ) is Îµ-close
to a uniformly random string on {0, 1}
rk , so that Gk (E(X,Yi )) is Îµ-close to a k-wise independent
distribution on {0, 1}
n. Finally, as Ğ´ depends only on at most s(fW Ïi ) â‰¤ s0 variables, we get that Ğ´
cannot distinguish Vi from a truly random string if k â‰¥ s0. We now formalize this intuition.
For brevity, let H denote the random variable fW Ïi : {0, 1}
A(Ïi ) â†’ {0, 1}. Observe that H is independent of Yi and the domain of fW Ïi is independent of X (it is a random variable over Ïi as
well as W ). We abstract the two essential properties of the random restriction family Dp that we
shall exploit.
Fact 3.6. With probability at least 1 âˆ’ Îµ, s(H) â‰¤ s0. In particular, with probability at least 1 âˆ’ Îµ:
(1) H can be described by cs0 log s0 bits for a constant c and (2) H is fooled by s0-wise independent
distributions.
Proof. By our assumption about Dp , for every W , PrÏi[s(fW Ïi ) > s0] â‰¤ Îµ. The claim now
follows as the number of functions in F of size at most s0 is s
O (s0 )
0 and any function of size s0 has
at most s0 relevant variables.
Let F 
i denote the set of all possible values for H obtained under arbitrary settings ofW and Ïi .
Let Fi = {Ğ´ âˆˆ F 
i : PrW,Ïi[H = Ğ´] â‰¥ Îµ/s
cs0
0 }. Let E denote the event that the conclusions of Fact 3.6
hold and let E be the event that H âˆˆ Fi and E
. Note that conditioned on E
, the number of possibilities for H is at most s
cs0
0 as H is described completely by cs0 log s0 bits. Therefore,
1 âˆ’ Pr[E] â‰¤ Pr[Â¬E
] + Pr[(H  Fi ) âˆ§ E
] â‰¤ Îµ + s
cs0
0 Â· Îµ/s
cs0
0 â‰¤ 2Îµ, (3.1)
for c a sufficiently large constant.
In the remaining argument, we condition on the event E. From the above equation, this will only
affect our error estimate by an additive 2Îµ. Fix an element Ğ´ âˆˆ Fi . Then, as the random function H
Journal of the ACM, Vol. 66, No. 2, Article 11. Publication date: March 2019.     
Pseudorandomness from Shrinkage 11:9
equals Ğ´ with probability at leastÎµ/s
cs0
0 , conditioning on this event cannot change the min-entropy
of X much:
Hâˆ(X |H = Ğ´) â‰¥ N âˆ’ log(1/Îµ) âˆ’ cs0 log s0.
Recall that H is independent of Yi . Thus, by the definition of the extractor, for N â‰¥ 2cs0 log s0 +
2 log(1/Îµ), E(X,Yi ) is Îµ-close to the uniform distribution on {0, 1}
rk even conditioned on H = Ğ´.
In particular, Zi = Gk (E(X,Yi )) is Îµ-close to a k-wise independent distribution. Therefore, even
conditioned on H = Ğ´, (Vi )A(Ïi ) = (Zi )A(Ïi ) is Îµ-close to k-wise independent. Finally, note that
f (Wiâˆ’1) = (fW Ïi )((Vi )Ïi ) = H((Vi )Ïi ) and similarly, f (Wi ) = H((Ui )Ïi ). Thus, for k â‰¥ s0,
E[f (Wiâˆ’1) | E,H = Ğ´] = E[(fW )Ïi ((Vi )Ïi )) | E,H = Ğ´]
= E[Ğ´((Vi )A(Ïi )) | E,H = Ğ´]
= E[Ğ´((Ui )A(Ïi )) | E,H = Ğ´] Â± Îµ (Fact 3.6)
= E[f (Wi )|E,H = Ğ´] Â± Îµ.
As the above is true for every Ğ´ âˆˆ Fi , it follows that
|E[f (Wiâˆ’1) âˆ’ f (Wi ) | E]| â‰¤ Îµ.
Combining Equation (3.1) and the above equation, we get
|E[f (Wiâˆ’1)] âˆ’ E[f (Wi )]| = Pr[E] Â· |E[f (Wiâˆ’1) âˆ’ f (Wi ) | E]| + Pr[Â¬E] Â· |E[f (Wiâˆ’1)
âˆ’ f (Wi ) | Â¬E]| â‰¤ Îµ + Pr[Â¬E] â‰¤ 3Îµ.
The claim now follows.
Combining Claims 3.4 and 3.5 and summing from 1 â‰¤ i â‰¤ t, we get that
|E[f (V )] âˆ’ EU âˆˆu {0,1}n [f (U )]| â‰¤ 3Îµt + Îµ.
Let us now estimate the seed-length of the generator. To generate V, we need to sample
Ï1,..., Ït and X,Y1,...,Yt for a total of
r = (R(s) + d)t + N = O(R(s) + log(s0/Îµ)) Â· (log(n/Îµ))/p + O(s0 log s0).
Substituting s0 = cpÎ“s log(1/Îµ), in the above equation gives us the theorem. The above calculation explains our choice of p = 1/s(Î“+1)
: we want to balance out 1/p and pÎ“s.
We next use Theorem 3.3 to get PRGs for specific classes of functions.
4 PRGS FOR FORMULAS
A formula is a tree where each leaf is labeled by a literal (either a variable or its negation) and
each internal node by an operation of constant arity. Any such tree naturally defines a function f .
Let L(f ) denote the formula size (number of leaves) of f . We assume, without loss of generality,
that L(f ) = n, the number of variables, since we can always add dummy variables otherwise. A de
Morgan formula is a binary tree with the set of allowed operations being {âˆ§, âˆ¨}.
The simplest case for our arguments is formulas over an arbitrary basis, since these have shrinkage 1. More challenging are de Morgan formulas. It has been known for many years that shrinkage
for such general formulas is 2 [12] and for read-once formulas (no variable appears more than once)
is log 2/ log(
âˆš
5 âˆ’ 1) = 3.27 ... [13]. In this section, we show that even pseudorandom restrictions
using no(1) random bits achieve essentially the same shrinkage with high probability. This will be
shown in Lemmas 4.2 and 4.8. We then use Theorem 3.3 to get Theorems 1.2, 1.1, and 1.4.
In our arguments, we will often have to handle â€œheavyâ€ variablesâ€”variables that appear in many
leaves. The following lemma shows that any s variables can only increase the formula size by a
factor of about 2s :
Journal of the ACM, Vol. 66, No. 2, Article 11. Publication date: March 2019.  
11:10 R. Impagliazzo et al.
Lemma 4.1. Let f be any formula, and letH denote any subset of the variables. For each h âˆˆ {0, 1}
H ,
let Ïh denote the restriction formed by setting variables in H to h, leaving all other variables unfixed.
Then L(f ) â‰¤
hâˆˆH (L(f Ïh ) + |H|) â‰¤ 2|H |
(maxhâˆˆH L(f Ïh ) + |H|).
Proof. Let IH=h (x) denote the formula of size |H| which is true iff all variables in H are set to
h. Then f = 
hâˆˆ {0,1}H ((IH=h (x)) âˆ§ (f Ïh )).
We begin with formulas over an arbitrary basis.
4.1 Arbitrary Basis
Here Lemma 4.1 and concentration bounds imply shrinkage with Î“ = 1 forO(logn)-wise independent restrictions.
Lemma 4.2. For any constant c and formula f with L(f ) = n, a (p = 1/
âˆš
n)-regular c logn-wise
independent random restriction Ï yields
Pr 
L(f Ï ) â‰¥ 23
âˆšc log npn
â‰¤ 2nâˆ’c .
Proof. Let k = c logn. The formula f depends on at most n variables x1,..., xn. Let variable xi
appear as a leaf ni times, so n = L(f ) =
i ni . For Î± to be chosen later, call xi heavy if ni â‰¥ p1âˆ’Î± n
and light otherwise. Then, for H, the set of heavy variables, |H| â‰¤ pÎ±âˆ’1. Let H(Ï) denote the heavy
variables set to * by Ï, and h(Ï) = |H(Ï)|. Define a new restriction Ï with Ï
(i) = Ï(i) fori  H(Ï),
and adversarially chosen in {0, 1} otherwise. Lemma 4.1 implies that L(f Ï ) â‰¤ 21+h(Ï)
L(f Ï ). We
bound
Pr
L(f Ï ) â‰¥ 2h+3
k Â· p1âˆ’Î±n

â‰¤ Pr[h(Ï) â‰¥ h] + Pr
L(f Ï ) â‰¥ 4k p1âˆ’Î±n

.
Define random variables Xi = 1 if Ï(xi ) = âˆ—, and 0 otherwise. We bound the first term with
Lemma 2.5. Here, we have Î¼ = |H|p â‰¤ pÎ± . Thus, as long as hÎ± â‰¥ 2c, this term will contribute at
most nâˆ’c .
For the second term, we need only consider light variables L and apply Lemma 2.4. Now the
coefficients are the ni . Note that Î¼ â‰¤ pn and m = maxxi âˆˆL ni < pÎ±âˆ’1
n, so m + Î¼ â‰¤ 2p1âˆ’Î±n. Hence
Lemma 2.4 bounds the second term by 2âˆ’k â‰¤ nâˆ’c .
Setting h = 2c/Î±, we minimize 2h (1/p)
Î± by setting Î± = 2

c/ logn.
This allows us to set Î“ = 1 in Theorem 3.3, yielding Theorem 1.2.
4.2 De Morgan Basis
We follow the high-level intuition described in the introduction by writing each formula as a
composition of smaller subformulas (these, as we will see, are obtained essentially by looking at
specific nodes of the original formula). One subtle issue we face in carrying out the approach is
that the subformulas Ğ´i in our decomposition will have some overlapping nodes, which in turn
forces some additional constraints on these nodes. We next show that these additional constraints
can be removed with only a minor loss. Throughout this section, we assume that Î“ denotes the
shrinkage exponent for the class of formulas under considerationâ€”Î“ = 2 for general formulas and
Î“ = 3.27 ... for read-once formulas.
The lemma below intuitively says that we can write any formula f as a function of smaller
formulas Ğ´i such that the size of any restriction Ï of f , f Ï can be bounded as a sum of the sizes of
the restrictions Ğ´iÏ for a suitably defined Ï
. In doing so, we will introduce some new variables
which will be unrestricted under Ï and the restriction Ï will agree with Ï on the variables of f .
We will also ensure that each of the formulas Ğ´i only depends on at most two newly introduced
variables.
Journal of the ACM, Vol. 66, No. 2, Article 11. Publication date: March 2019.       
Pseudorandomness from Shrinkage 11:11
Lemma 4.3. For any positive  and any formula f on a set of variables X with L(f ) â‰¥ , there
exists at most 6n/ formulas Ğ´i with L(Ğ´i ) â‰¤ , where the Ğ´i may depend on variables outside X, such
that the following holds: For any restriction Ï, L(f Ï ) â‰¤
i L(Ğ´iÏ ), where Ï
(j) = Ï(j) for j âˆˆ X,
and Ï
(j) = âˆ— otherwise. Moreover, each Ğ´i depends on at most 2 variables outside X (called special
variables).
This follows from the following claim:
Claim 4.4. Any binary tree of size n â‰¥  can be decomposed into at most 6n/ subtrees2 of size at
most , such that each subtree has at most two other subtree children. Here, subtree T1 is a child of
subtree T2 if there exists nodes t1 âˆˆ T1, t2 âˆˆ T2, such that t1 is a child of t2.
Proof. Proceed inductively, using the well-known fact that any binary tree of size s can be
divided into two edge-disjoint subtrees, each of size between s/3 and 2s/3. This results in subtrees
of size between /3 and , and hence there are at most 3n/ of them. For each subtree T with more
than two subtree children, find a subtree T  of T with exactly two subtree children, and divide T
into T  and T \ T 
. Note that T \ T  now has one fewer subtree child. Continue doing this until all
subtrees have at most two subtree children. This process can continue at most the original number
of subtrees steps, and hence the total number of such subtrees is as desired.
Proof of Lemma 4.3. View the formula f as a tree. By Claim 4.4, we can decompose f into
subformulas Ğ´i , where each input to Ğ´i is either an ordinary variable in X or a special input: the
output of some other Ğ´j . In each Ğ´i , replace these special inputs with distinct, new variables not
in X. The total number of new variables is at most the number of subformulas.
Weâ€™d now like to show that restricting by Ï is not much worse than restricting by Ï, i.e., requiring a few variables to be * does not hurt the restricted size too much. We want to show this
simply using results about restrictions by Ï as a black box. For general formulas, this follows from
Lemma 4.1. However, for read-once formulas, we need a different method. This method involves
replacing these special variables by relatively short formulas which are unlikely to get fixed. We
show that such read-once formulas exist using a result of Valiant [33] on computing the majority
function by monotone formulas.
Lemma 4.5. For any 0 < p,Îµ < 1, there exists a read-once formula h of size at most (log(1/Îµ)/p)
4
such that a p-regular (truly) random restriction fixes h with probability less than Îµ.
Proof. We shall use Valiantâ€™s result on computing the majority function using monotone formulas [33]. His main result is a probabilistic way to construct monotone formulas for majority.
However, the formulas he constructs come from a distribution on read once formulas of size
poly(1/p) so that, if the inputs have bias 1/2 + p (of being 1), they almost always output 1, and
if they have bias 1/2 âˆ’ p, they almost always output 0. He then boosts the error probability to be
exponentially small in n. We donâ€™t need to do that last step.
The point is that, if a monotone formula has the above property, then it is resistant to restrictions
leaving p fraction of bits unrestricted. Because, if we go back and set the unset bits to 1, we get
random bits biased towards 1 as inputs, and if we set them to 0, we get random inputs biased
towards 0. Since the output has to change with high probability, the circuit cannot be constant
after the restriction.
The precise bound one gets from Valiantâ€™s arguments is O((log(1/Îµ))2/p3.27...) <
(log(1/Îµ)/p)
4.
2A subtree for us is a connected subgraph of the tree.
Journal of the ACM, Vol. 66, No. 2, Article 11. Publication date: March 2019.              
11:12 R. Impagliazzo et al.
Lemma 4.6. Suppose that, for all formulas f of size 0 and a p-regular random restriction Ï,
E[L(f Ï )] â‰¤ pÎ“L(f ). Suppose Ğ´ is a formula with w special variables with L(Ğ´) â‰¥ 0. Let Ï be a
p-regular restriction with the constraint that the special variables in Ğ´ be unrestricted. Then,
E[L(Ğ´Ï )] â‰¤ pÎ“L(Ğ´) + O(w Â· pÎ“âˆ’4 Â· log4 (w pÎ“ L(Ğ´))).
Proof. Construct a new formulaĞ´ by replacing each special variable inĞ´ by the formulah given
in Lemma 4.5 for Îµ = 1/w pÎ“L(Ğ´), on disjoint sets of variables. Let A denote the event that none of
these formulas h is fixed. The key observation is that, conditioned onA, we have L(Ğ´
Ï ) â‰¥ L(Ğ´Ï ).
Therefore,
E[L(Ğ´
Ï )] â‰¥ Pr[A] Â· E[L(Ğ´
Ï ) | A] â‰¥ (1 âˆ’ wÎµ)E[L(Ğ´Ï ) | A].
Thus, for r = (log(1/Îµ)/p)
4,
E[L(Ğ´Ï )|A] â‰¤ (1 + 2wÎµ)E[L(Ğ´
Ï )] â‰¤ (1 + 2wÎµ)(pÎ“ (L(Ğ´) + 2w r)).
The lemma follows.
We next show that k-wise independent restrictions shrink formulas in which no variable is read
too many times with high probability.
Lemma 4.7. There is a large enough constant c, such that for any Îµ > 0, p â‰¥ nâˆ’1/(4Î“), t â‰¤
p9Î“n/(c log(n/Îµ)), and any read-t formula f on n variables with L(f ) = n, a p-regular (k =
c log(n/Îµ)/pÎ“ )-wise independent restriction Ï yields Pr[L(f Ï ) â‰¥ 60pÎ“n] â‰¤ Îµ.
Proof. Set  = 1/p4Î“. Use Lemma 4.3 to get the formulas Ğ´i . By Lemma 4.1 and Lemma 4.6, for
any Ğ´i , we have E[L(Ğ´iÏ] â‰¤ 5pÎ“.
Form a graph where the vertices are the Ğ´i , with an edge between Ğ´i and Ğ´j if they share a
variable. This graph has m vertices, where n/ â‰¤ m â‰¤ 6n/, and degree at most t. Hence, we can
divide the vertices into independent sets of size at least s = m/(t + 1) â‰¥ c log(n/Îµ)/(2pÎ“ ).
For any such independent set I, note that Yi = L(Ğ´iÏ )/ are independent random variables in
the range [0, 1]. Hence, we can apply Lemma 2.4 for large enough c to show that
Pr
â¡
â¢
â¢
â¢
â¢
â£

i âˆˆI
L(Ğ´iÏ ) â‰¥ 2E
â¡
â¢
â¢
â¢
â¢
â£

i âˆˆI
L(Ğ´iÏ )
â¤
â¥
â¥
â¥
â¥
â¦
â¤
â¥
â¥
â¥
â¥
â¦
â‰¤ Îµ/n.
Thus, by a union bound, with probability at least 1 âˆ’ Îµ no such event occurs. The lemma follows
because

I
E
â¡
â¢
â¢
â¢
â¢
â£

i âˆˆI
L(Ğ´iÏ )
â¤
â¥
â¥
â¥
â¥
â¦
â‰¤ 5pÎ“ Â·

I
|I | = 5pÎ“m â‰¤ 30pÎ“
n.
We now remove the assumption that the formula is read t, leading to our final derandomized
shrinkage bound.
Lemma 4.8. For any constantc â‰¥ 11, any p â‰¥ nâˆ’1/Î“, and any formula f on n variables with L(f ) =
n, there is a p-regular distribution on restrictions Ï using a 2O (log2/3 n) bit seed such that
Pr 
L(f Ï ) â‰¥ 23c log2/3 n Â· pÎ“
n

â‰¤ nâˆ’c .
Before proving the lemma we first note that combining the lemma with Theorem 3.3, and the
shrinkage exponent estimates of [12] and [13] directly implies Theorem 1.1 and Theorem 1.4,
respectively.
We now prove Lemma 4.8. We will implement this p-regular restriction as a sequence of r qregular restrictions, where p = qr and q = nâˆ’Î± for some Î± only slightly sub-constant. For each of
Journal of the ACM, Vol. 66, No. 2, Article 11. Publication date: March 2019.             
Pseudorandomness from Shrinkage 11:13
the r rounds of restriction, we will have a set of at most n6Î± heavy variables, which can change in
each round. We handle the heavy variables by conditioning on the values of the restrictions applied
to the heavy variables for the current round and six rounds ahead, and then applying Lemma 2.5.
We handle all other variables with Lemma 4.7. Note that the shrinkage exponents proved in [12]
and [13] have an extra polylogarithmic term. However, the extra factor when restricting byq = nâˆ’Î±
is polyloĞ´(n), so the total extra factor is (polyloĞ´(n))r , which can be absorbed into the 2O (log2/3 n)
term. We now formalize this.
Proof. Set q = p1/r for an r â‰¥ 11 such that q = nâˆ’Î± for Î± to be chosen later. Let k0 = n10Î± ,
and k = rk0. Our pseudorandom p-regular restriction will be the composition of r independent
restrictions Ïi , where each Ïi is a k-wise independent q-regular restriction.
The analysis proceeds in rounds. Let X0 = X denote the n variables for f . Let Xi denote the
unfixed variables in round i, and let ni = |Xi |. Let f0 = f , and let fi denote the restricted formula
after i rounds. Call a variable xj heavy in round i if xj appears more than ti = L(fi )/n10Î± times
in fi . Letting Hi denote the heavy variables in round i, we see that |Hi | â‰¤ n10Î± . Let H = âˆªHi , and
Yi = Xi \ H. Let pi = p/qi
.
We now condition on the heavy variables in a somewhat subtle way. In round 1, we condition
on all of Ï1, as well as the values of all Ïi on the variables H1. Now all the Ïi on H1 determine Ï
on H1. Since all Ïi are k-wise independent, so is Ï. Since k â‰¥ |H1 |, Lemma 2.5 implies that
Pr[ Ï leaves at least s variables from H1 unfixed ] â‰¤ (n10Î±p)
s â‰¤ nâˆ’Î± s . (4.1)
Now, conditioned on all Ïi on H1, Ï1 remains k0-wise independent on X \ H1. Suppose Ï leaves
s1 < s variables unfixed from H1. Then for each setting Ï„ of these s1 variables, Lemma 4.7 implies
that
Pr
L(f Ï1âˆªÏ (H )âˆªÏ„ ) â‰¥ 60qÎ“
n

â‰¤ exp(nâˆ’Î©(Î± )
). (4.2)
Combining (4.1) and (4.2) with Lemma 4.1, we obtain
Pr
L(f Ï1âˆªÏ (H ) ) â‰¥ 2s+6
qÎ“
n

â‰¤ 2nâˆ’Î± s .
We continue in this manner, in round i fixing Ïi as well as all Ïj , j â‰¥ i, on Hi . We do this for
r âˆ’ 11 rounds; as in (4.1), the p becomes pi and we need to ensure that n10Î±pi â‰¤ nâˆ’Î± . Thus,
Pr[L(f Ï ) â‰¥ (2s+6
qÎ“ )
râˆ’11 Â· n] â‰¤ 2(r âˆ’ 11)nâˆ’Î± s .
Since qÎ“r = pÎ“, the extra factor we lose in the formula size beyond pÎ“ is at most 2(s+6)(râˆ’11)
n11Î± .
To make the error at most nâˆ’c , we set s = 2c/Î±. Since p â‰¥ 1/n, we have r â‰¤ 1/Î±. Thus, the extra
factor is at most 2r sn11Î± = 22c/Î±2+11Î± log n. To minimize this exponent (up to constants), we set
Î± = (logn)
âˆ’1/3. We restrictc â‰¥ 11 in the lemma statement so that 2c + 11 â‰¤ 3c. Note that the seedlength is O(k logn) which is 2O (log2/3 n) as stated in the lemma.
5 PRGS FOR BRANCHING PROGRAMS
We now apply our main generator construction to get PRGs for branching programs with seedlength s1/2+o(1) for branching programs of size s. We first formally define branching programs.
Definition 5.1. An n-variable branching program (BP) M is a directed acyclic graph with the
following properties:
â€”There are three special verticesâ€”start which has no incoming edges and two terminal vertices accept, reject which have no outgoing edges.
â€”Every vertex in the graph is labeled with a variable from {x1,..., xn }.
â€”Every non-terminal vertex has two outgoing edges labeled {0, 1}.
Journal of the ACM, Vol. 66, No. 2, Article 11. Publication date: March 2019.   
11:14 R. Impagliazzo et al.
The size of M, s(M), is defined as the number of vertices in M. The length of M is defined as the
length of the longest path from start to either of the terminals. We say M is read-once if no two
vertices in a path from start to the terminals have the same label.
A branching program M as above, naturally induces a function M : {0, 1}
n â†’ {0, 1} that on input
x âˆˆ {0, 1}
n, traverses the graph according to x and outputs 1 if the computation reaches accept and
0 otherwise.
We shall construct an explicit pseudorandom generator for branching programs of size at mosts
with seed-length s1/2+o(1) and error 1/poly(s). Previously, only PRGs with seed-length Î©(n) were
known even for the special case of layered read-once branching programs3 (ROBPs) with each layer
constrained to have a constant number of nodes (constant width). For the more restricted class of
oblivious ROBPs (these are ROBPs where the labeling of the layers is known a priori) of length at
most T and width at most W , Nisan [22] and Impagliazzo et al. [16] gave PRGs with seed-length
O((logT )(log(T/Îµ) + logW )) to get error Îµ.
We now prove Theorem 1.3. The arguments here are basically the same as those from Section 4.1.
To this end, we first show an analogue of Lemma 4.1 for branching programs.
Lemma 5.2. Let f be any BP, and let H denote any subset of the variables. For each h âˆˆ {0, 1}
H ,
let Ïh denote the restriction formed by setting variables in H to h, leaving all other variables unfixed.
Then, s(f ) â‰¤ 2|H | Â· ( maxhâˆˆH s(f Ïh ) + 2).
Proof. Build a complete decision tree T of depth |H| so that leaves correspond to specific assignments for the variables in h. We can now obtain a BP Mf for f , by appending a BP for f Ïh to
the leaf of T corresponding to the assignment h. Clearly, the resulting BP has size as stated.
Lemma 5.3. For any constantc and BP f with s(f ) = n, a (p = 1/
âˆš
s)-regularc log s-wise independent random restriction Ï yields
Pr 
s(f Ï ) â‰¥ 23
âˆšc log n Â· p s
â‰¤ 2 sâˆ’c .
Proof. Let k = c log s. The BP f on at most s variables x1,..., xs . For i â‰¤ s, let the number of
vertices labeled xi be ni . Then, s = s(f ) =
i ni . We now repeat the calculations from Lemma 4.2
for this setting of ni â€™s.
For Î± to be chosen later, call xi heavy if ni â‰¥ p1âˆ’Î±s and light otherwise. Then for H the set
of heavy variables, |H| â‰¤ pÎ±âˆ’1. Let H(Ï) denote the heavy variables set to * by Ï, and h(Ï) =
|H(Ï)|. Define a new restriction Ï with Ï
(i) = Ï(i) fori  H(Ï), and adversarially chosen in {0, 1}
otherwise. Lemma 4.1 implies that s(f Ï ) â‰¤ 2h(Ï)+1
s(f Ï ). We bound
Pr
s(f Ï ) â‰¥ 2h+3
k Â· p1âˆ’Î±s

â‰¤ Pr[h(Ï) â‰¥ h] + Pr
s(f Ï ) â‰¥ 4k p1âˆ’Î±s

.
Define random variables Xi = 1 if Ï(xi ) = âˆ—, and 0 otherwise. We bound the first term with
Lemma 2.5. Here, we have Î¼ = |H|p â‰¤ pÎ± . Thus, as long as hÎ± â‰¥ 2c, this term will contribute at
most sâˆ’c .
For the second term, we need only consider light variables L and apply Lemma 2.4. Now the
coefficients are the ni . Note that Î¼ â‰¤ ps and m = maxxi âˆˆL ni < pÎ±âˆ’1
n, so m + Î¼ â‰¤ 2p1âˆ’Î±s. Hence,
Lemma 2.4 bounds the second term by 2âˆ’k â‰¤ sâˆ’c .
Setting h = 2c/Î±, we minimize 2h (1/p)
Î± by setting Î± = 2

c/ log s.
This allows us to set Î“ = 1 in Theorem 3.3, yielding Theorem 1.2.      