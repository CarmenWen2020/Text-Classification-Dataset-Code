We present a new locally differentially private algorithm for the heavy hitters problem that achieves optimal
worst-case error as a function of all standardly considered parameters. Prior work obtained error rates that
depend optimally on the number of users, the size of the domain, and the privacy parameter but depend
sub-optimally on the failure probability.
We strengthen existing lower bounds on the error to incorporate the failure probability and show that
our new upper bound is tight with respect to this parameter as well. Our lower bound is based on a new
understanding of the structure of locally private protocols. We further develop these ideas to obtain the
following general results beyond heavy hitters.
â€¢ Advanced Grouposition: In the local model, group privacy for k users degrades proportionally to
â‰ˆ
âˆš
k instead of linearly in k as in the central model. Stronger group privacy yields improved maxinformation guarantees, as well as stronger lower bounds (via â€œpacking argumentsâ€), over the central
model.
â€¢ Building on a transformation of Bassily and Smith (STOC 2015), we give a generic transformation
from any non-interactive approximate-private local protocol into a pure-private local protocol. Again
in contrast with the central model, this shows that we cannot obtain more accurate algorithms by
moving from pure to approximate local privacy.
CCS Concepts: â€¢ Theory of computation â†’ Sketching and sampling; Theory of database privacy and
security;
Additional Key Words and Phrases: Differential privacy, heavy hitters, local model
1 INTRODUCTION
In the heavy-hitters problem, each of n users holds as input an item from some domain X. Our
goal is to identify all â€œheavy-hitters,â€ that are the domain elements x âˆˆ X for which many of the
users have x as their input. In this work we study the heavy-hitters problem in the local model
of differential privacy (LDP), where the users randomize their data locally and send differentially
private reports to an untrusted server that aggregates them. In our case, this means that every
user only sends the server a single randomized message that leaks very little information about
the input of that user.
The heavy hitters problem is perhaps the most well-studied problem in local differential privacy
[4, 5, 19, 27, 30]. In addition to the intrinsic interest of this problem, LDP algorithms for heavyhitters provide important subroutines for solving many other problems, such as median estimation,
convex optimization, and clustering [21, 29, 35, 37]. In practice, heavy-hitters algorithms under
LDP have already been implemented to provide organizations with valuable information about
their user bases while providing strong privacy guarantees to their users. Two prominent examples
are by Google in the Chrome browser [14] and Apple in iOS-10 [38], making local heavy-hitter
algorithms the most widespread industrial application of differential privacy to date.
Before describing our new results, we define our setting more precisely. Consider a set of n
users, where each user i holds as input an item xi âˆˆ X. We denote S = (x1,..., xn ), and refer to
S as a â€œdistributed databaseâ€ (as it is not stored in one location, and every xi is only held locally
by user i). We say that a domain element x âˆˆ X is Î”-heavy if its multiplicity in S is at least Î”, i.e.,
if there are at least Î” users whose input is x. Our goal is to identify all Î”-heavy elements for Î”
as small as possible and to estimate their multiplicities in S. Specifically, we receive a sequence of
messagesâ€”one message yi from every useriâ€”at the end of which our goal is to report a list L âŠ‚ X
such that
(1) Every Î”-heavy element x is in L, and
(2) |L| = O(n/Î”), i.e., the list size is proportional to the maximum possible number of Î”-heavy
elements.
We also want, for each x âˆˆ L, to output a value that is within Î” of the multiplicity of x in S (so,
actually, the list L contains frequency estimates as well as elements). The parameter Î” is referred
to as the error of the protocol, as elements with multiplicities less than Î” are â€œmissedâ€ by the
protocol. We refer to the probability that the protocol does not successfully achieve the above
objectives as its failure probability, denoted by Î². The privacy requirement is that for every user
i, the distribution of the message yi should not be significantly affected by the input of the user.
Formally,
Definition 1.1 [11]. An algorithm A : X â†’ Y satisfies Îµ-differential privacy if for every two inputs x, x  and for every output y âˆˆ Y we have
Pr[A(x) = y] â‰¤ eÎµ Â· Pr[A(x 
) = y].
Solving the heavy-hitters problem under LDP becomes harder for smaller choices of Î”,
1 and
the work on LDP heavy-hitters has focused on constructing protocols that can achieve as small a
value of Î” as possible.
In this work we combine ideas from the recent locally private heavy-hitters algorithm of Reference [4] with the recent non-private algorithm of Reference [25], and present a new efficient LDP
1For intuition, observe that we cannot hope to have Î” = 1. Indeed, if only one user is holding some xâˆ— as input (so xâˆ—
is 1-heavy), then the outcome distribution of the protocol remains approximately the same even when this user holds a
different input (in which case xâˆ— does not appear in the data at all, and it is no longer 1-heavy).
ACM Transactions on Algorithms, Vol. 15, No. 4, Article 51. Publication date: October 2019.
Heavy Hitters and the Structure of Local Privacy 51:3
Table 1. Comparison with Previous Works
Performance metric This work Bassily et al. [4] Bassily and Smith [5]
2
Server time OËœ (n) OËœ (n) OËœ (n2.5 )
User time OËœ (1) OËœ (1) OËœ (n1.5 )
Server memory OËœ (
âˆš
n) OËœ (
âˆš
n) OËœ (n2 )
User memory OËœ (1) OËœ (1) OËœ
n1.5

Communication/user O (1) O (1) O (1)
Public randomness/user OËœ (1) OËœ (1) OËœ (n1.5 )
Worst-case error O

1
Îµ Â·

n log  |X |
Î²

O

1
Îµ Â·

n log  |X |
Î²

log  1
Î²

O

log1.5
 1
Î²

Îµ Â·

n log |X |
	
The OËœ notation hides logarithmic factors in n and |X |.
heavy-hitters algorithm achieving optimal worst-case error Î” as a function of the number of users
n, the size of the domain |X |, the privacy parameter Îµ, and the failure probability Î². Prior state-ofthe-art results [4, 5] either obtained error with sub-optimal dependence on the failure probability
Î², or had runtime at least linear in |X | (which is not realistic in real applications, where X may
be the space of all reasonable-length URL domains). Table 1 summarizes our result in comparison
with previous algorithms for the problem.
We remark that while our focus is on achieving the optimal (i.e., smallest possible) error Î”,
our algorithm also allows the server to obtain larger error Î” â‰¥ Î” in exchange for lower space
requirements. Specifically, memory usage and the size of the output list may both be improved to
OËœ (n/Î”
).
1.1 Lower Bound on the Error and New Understandings of the Local Model
We strengthen a lower bound on the error by References [7, 19, 39] and Reference [5] to incorporate the failure probability and show that our new upper bound is tight. Our lower bound is
based (conceptually) on the following new understandings of the local privacy model that allow (in
some cases) to obtain stronger privacy guarantees for locally private protocols. In our case, these
stronger privacy guarantees translate to less accurate algorithms, i.e., to stronger lower bounds on
the achievable error.
1.1.1 Advanced Grouposition. An important property of differential privacy is group privacy.
Recall that differential privacy ensures that no single individual has a significant effect on the
outcome (distribution) of the computation. A similar guarantee also holds for every small group of
individuals, but the privacy guarantee degrades (gracefully) with the size of the group. Specifically,
if an algorithm satisfies an individual-level guarantee of Îµ-differential privacy, then it satisfies kÎµdifferential privacy for every group of k individuals. We observe that in the local model, group
privacy degrades the privacy parameter only by a factor of â‰ˆ
âˆš
k, unlike in the centralized model.
This observation allows us to show improved bounds on the max-informationâ€”a measure of the
dependence of an algorithmâ€™s output on a randomly chosen inputâ€”of locally private protocols [9].
Strong group privacy is, however, a mixed blessing as it also leads to stronger lower bounds (of
the type known as â€œpacking argumentsâ€) for pure-private local algorithms.
2The userâ€™s runtime and memory in Reference [5] can be improved to O (n) if one assumes random access to the public
randomness, which we do not assume in this work. In fact, our protocol can be implemented without public randomness
while attaining essentially the same performances.
ACM Transactions on Algorithms, Vol. 15, No. 4, Article 51. Publication date: October 2019.          
51:4 M. Bun et al.
1.1.2 Pure vs. Approximate LDP. Another important property of differential privacy is that it
also degrades gracefully under composition. This allows us to design algorithms that access their
input database using several (say k) differentially private mechanisms as subroutines and argue
about the overall privacy guarantees. In the case of pure-privacy, the privacy parameter deteriorates by a factor of at most k. However, in the case of approximate-privacy, k-fold composition
degrades the privacy parameter only by a factor of â‰ˆ
âˆš
k (see Reference [13]).
We show that in the non-interactive local model we can enjoy the best of both worlds. First,
we can show that in some cases, (an approximate version of) the composition of locally private
algorithms can satisfy pure-privacy while enjoying the same strong composition guarantees of
approximate-privacy. Motivated by this observation, we then proceed to show that in the local
model approximate-privacy is actually never more useful than pure-privacy (at least for noninteractive protocols). Specifically, building on a transformation of Bassily and Smith [5], we give
a generic transformation from any non-interactive approximate-private local protocol into a pureprivate local protocol with the same utility guarantees. This is the first formal proof that approximate local-privacy cannot enable more accurate analyses than pure local-privacy.
In Section 7, we describe how our new lower bound for the heavy hitters under pure-LDP follows
from advanced grouposition and how the lower bound extends to approximate-LDP via our generic
transformation. In addition, we provide a direct analysis for our lower bound that results from a
direct extension of the argument of References [7, 19, 39] (using ideas from Reference [5]).
2 PRELIMINARIES FROM DIFFERENTIAL PRIVACY
Notations. Databases are (ordered) collections of elements from some data universe X. Two
databases S, S âˆˆ Xn are called neighboring if they differ in at most one entry. Throughout this
article, we use the notation OËœ to hide logarithmic factors in n and |X | (the number of users and the
size of the domain).
Consider a database where each entry contains information pertaining to one individual. An algorithm operating on databases is said to preserve differential privacy if a change of a single record
of the database does not significantly change the output distribution of the algorithm. Intuitively,
this means that individual information is protected: Whatever is learned about an individual could
also be learned with his or her data arbitrarily modified (or without his or her data at all).
Definition 2.1 (Differential Privacy [11]). A randomized algorithm M : Xn â†’ Y is (Îµ, Î´ ) differentially private if for every two neighboring datasets S, S âˆˆ Xn and every T âŠ† Y we have
Pr[M(S) âˆˆ T ] â‰¤ eÎµ Â· Pr[M(S
) âˆˆ T ] + Î´, where the probability is over the randomness of M.
2.1 Local Differential Privacy
In the local model, each individual holds his or her private information locally and only sends
differentially private reports to an untrusted server that aggregates all the reports.
Definition 2.2 (Local Differential Privacy [11, 22]). An algorithm M is a non-interactive local protocol if there are algorithms A (aggregator) and R1,..., Rn (local randomizers) such that M can be
written as M(x1,..., xn ) = A(R1 (x1),..., Rn (xn )). Algorithm M satisfies (Îµ, Î´ )-local differential
privacy if each of the Ri â€™s satisfies (Îµ, Î´ )-differential privacy.
In an interactive local protocol, the local randomizers Ri may be chosen adaptively throughout
the execution, as a function of previously obtained reports from the users. The focus of this work
is on non-interactive protocols.
ACM Transactions on Algorithms, Vol. 15, No. 4, Article 51. Publication date: October 2019.
Heavy Hitters and the Structure of Local Privacy 51:5
3 A HEAVY-HITTERS PROTOCOL WITH OPTIMAL ERROR
Let S = (x1,..., xn ) âˆˆ Xn be a database that may be distributed across n users (each holding one
row). For x âˆˆ X, we will be interested in estimating the the frequency (or multiplicity) of x in S,
i.e.,
fS (x) = |{i âˆˆ [n] : xi = x}|.
Specifically, we would like to solve the following problem while guaranteeing LDP:
Definition 3.1. Let S âˆˆ Xn be a (distributed) database. In the heavy-hitters problem with error Î”
and failure probability Î², the goal is to output a list Est âŠ† XÃ—R of elements and estimates, of size
| Est | = O(n/Î”), such that with probability 1 âˆ’ Î²
(1) For every (x, a) âˆˆ Est we have that |fS (x) âˆ’ a| â‰¤ Î”; and,
(2) For every x âˆˆ X s.t. fS (x) â‰¥ Î” we have that x appears in Est.
As in previous works, our algorithm for the heavy-hitters problem is based on a reduction to
the simpler task of constructing a frequency oracle:
Definition 3.2. Let S âˆˆ Xn be a (distributed) database. A frequency oracle with error Î” and failure
probability Î² is an algorithm that, with probability at least 1 âˆ’ Î², outputs a data structure capable
of approximating fS (x) for every x âˆˆ X with error at most Î”.
Observe that constructing a frequency oracle is an easier task than solving the heavy-hitters
problem, as every heavy-hitters algorithm is in particular a frequency oracle. Specifically, given
a solution Est to the heavy-hitters problem, we can estimate the frequency of every x âˆˆ X as
Ë†
fS (x) = a if (x, a) âˆˆ Est, or Ë†
fS (x) = 0 otherwise. Moreover, observe that ignoring runtime, the two
problems are identical (since we can query the frequency oracle on every domain element to find
the heavy-hitters).
The literature on heavy-hitters and frequency oracles under LDP has focused on the goal of
minimizing the error Î”. However, as Bassily and Smith showed, under LDP, every algorithm for
either task must have worst case error at least
Î©
 1
Îµ

n Â· log |X |

.
This lower bound is known to be tight, as constructions of frequency oracles with matching error
were presented in several works [4, 5, 19, 27]. These works also presented reductions that transform
a frequency oracle into a heavy-hitters algorithm. However, their reductions resulted in a suboptimal dependency of the error in the failure probability Î² (on which the lower bound above is
not informative). In this work we give a new (privacy preserving) reduction from the heavy-hitters
problem to the frequency oracle problem, achieving error that is optimal in all parameters. Our
reduction is a private variant of the recent non-private algorithm in Reference [25], and our final
construction uses the frequency oracle in Reference [4].
3.1 Existing Techniques
In this section, we give an informal survey of the techniques References [4] and [25], and highlight
some of the ideas behind the constructions. This intuitive overview is generally oversimplified and
hides many of the difficulties that arise in the actual analyses.
3.1.1 Reduction with Sub-Optimal Dependence on the Failure Probability. Assume that we have a
frequency oracle protocol with worst-case error Î”, and let S = (x1,..., xn ) âˆˆ Xn be a (distributed)
database, where user i holds the input xi . We now want to use our frequency oracle to identify
ACM Transactions on Algorithms, Vol. 15, No. 4, Article 51. Publication date: October 2019.
51:6 M. Bun et al.
a small set of â€œpotential heavy-hittersâ€. Specifically, we would like to identify all elements whose
frequencies in S are at least 2Î”. We outline the approach in Reference [4].
Let h : X â†’ [Y] be a (publicly known) random hash function, mapping domain elements into
some range of size Y. We will now use h to identify the heavy-hitters. To that end, let xâˆ— âˆˆ X
denote such a heavy-hitter, appearing at least 2Î” times in the database S, and denote yâˆ— = h(xâˆ—).
Assuming that Y is big enough, w.h.p. we will have that xâˆ— is the only input element (from S) that
is mapped (by h) to the hash value yâˆ—. Assuming that this is indeed the case, we will now attempt
to identify xâˆ— one â€œcoordinateâ€ (as defined below) at a time.
Let us assume that elements from X are represented as M symbols from an alphabet [W ]. For
every coordinate m âˆˆ [M], denote Sm = (h(xi ), xi[m])i âˆˆ[n], where xi[m] is the mth symbol of xi .
That is, Sm is a database over the domain ([Y]Ã—[W ]), where the row corresponding to user i is
(h(xi ), xi[m]). Observe that since h is public, every user can compute his or her own row locally.
As xâˆ— is a heavy-hitter, for everym âˆˆ [M] we have that (yâˆ—, xâˆ—[m]) appears in Sm at least 2Î” times.
However, as we assumed that xâˆ— is the only input element that is mapped to yâˆ—, we get that (yâˆ—,w
)
does not appear in Sm at all for every w  xâˆ—[m]. Recall that our frequency oracle has error at
most Î”, and hence, we can use it to accurately determine the symbols of xâˆ—. Using this strategy we
can identify a set of size Y containing all heavy-hitters: For every hash value y âˆˆ [Y], construct a
potential heavy-hitter xË†(y) symbol-by-symbol, where the mth symbol of xË†(y) is the symbol w s.t.
(y,w) is most frequent in Sm. By the above discussion, this strategy identifies all of the heavyhitters.
Recall that we assumed here that Y (the size of the range of the hash function h) is big enough
so that there are no collisions among input elements (from S). In fact, the analysis can withstand
a small number of collisions (roughly âˆš
n, since we are aiming for error bigger than âˆš
n anyway).
Nevertheless, for the above strategy to succeed with high probability, say w.p. 1 âˆ’ Î², we will need
to set Y  âˆš
n/Î². As Î² can be exponentially small, this is unacceptable, and hence, Bassily et al. [4]
applied the above strategy with a large failure probability Î², and amplified the success probability
using repetitions. Unfortunately, using repetitions still causes the error to increase by a factor of

log(1/Î²) (because applying multiple private computations to the same dataset degrades privacy).
Specifically, Bassily et al. [4] obtained the following result.
Theorem 3.3 [4]. Let Îµ â‰¤ 1, and let S âˆˆ Xn be a (distributed) database. There exists an Îµ-LDP
algorithm that returns a list Est of length OËœ (
âˆš
n) such that with probability 1 âˆ’ Î²
(1) For every (x, a) âˆˆ Est we have that |a âˆ’ fS (x)| â‰¤ O( 1
Îµ

n log(
min{n, |X | }
Î² )).
(2) âˆ€ âˆˆ X s.t. fS (x) â‰¥ O( 1
Îµ

n log( |X |
Î² ) log( 1
Î² )), we have that x appears in Est.
The server uses processing memory of sizeOËœ (
âˆš
n), and runs in timeOËœ (n). Each user hasOËœ (1) runtime,
memory, and communication.
3.1.2 A Non-Private Reduction Based on List-Recoverable Codes [25]. Recall that in the reduction
of Bassily et al. [4] there is only one hash function h. If that hash function â€œfailedâ€ for some heavyhitter x (meaning that too many other input elements collide with x), then we cannot identify x and
we needed to repeat. Suppose instead that we modify the reduction to introduce an independent
hash function hm for every coordinatem âˆˆ [M]. The upside is that now, except with exponentially
small probability (in M), every heavy-hitter xâˆ— causes at most a small fraction of the hash functions
to fail. Then hopefully we are able to detect most of the symbols of xâˆ—. If instead of applying this
scheme to xâˆ— itself, then we apply it to an error-correcting encoding of xâˆ— with constant rate and
ACM Transactions on Algorithms, Vol. 15, No. 4, Article 51. Publication date: October 2019.  
Heavy Hitters and the Structure of Local Privacy 51:7
constant relative distance, then we can indeed recover xâˆ— this way.3 However, aside from the fact
that some of the symbols are missing, it now becomes unclear how to concatenate the coordinates
of the elements that we are constructing. Specifically, in the construction of Bassily et al., the fact
that h(xâˆ—) = yâˆ— appeared throughout all coordinates is what we used to concatenate the different
symbols we identified. If the hash functions are independent across m, then this does not work.
We now describe this strategy in more detail and describe the solution proposed by Larsen et al.
[25] to overcome this issue. Fix a heavy-hitter xâˆ—. For every coordinate m âˆˆ [M], where hm does
not fail on xâˆ—, we have that the frequency of (hm (xâˆ—), xâˆ—[m]) â€œstands outâ€ in comparison to every
(hm (xâˆ—),w
), where w  xâˆ—[m], meaning that (hm (xâˆ—), xâˆ—[m]) appears in Sm significantly more
times than any other (hm (xâˆ—),w
). Hence, for every coordinate m âˆˆ [M] and for every hash value
y âˆˆ [Y] we identify (at most 1) symbol w s.t. (y,w) â€œstands outâ€ compared to all other (y,w
) in
Sm. In other words, for every m âˆˆ [M] we construct a list Lm = {(y,w)} of â€œstand outsâ€ in Sm.
The guarantee is that for every heavy-hitter xâˆ—, we have that (hm (xâˆ—), xâˆ—[m]) appears in Lm for
most coordinates m âˆˆ [M]. If we could single out all occurrences of (hm (xâˆ—), xâˆ—[m]) in these lists,
then we could concatenate the symbols xâˆ—[m] and reconstruct xâˆ—. The question is how to match
symbols from different lists. As was observed in Reference [16], which studied the related problem
of 1/1 compressed sensing, this can be done using list-recoverable codes.
Recall that in a (standard) error correcting code, the decoder receives a noisy codeword (represented by, say, M symbols from [W ]) and recovers a (noiseless) codeword that agrees with the
noisy codeword on at least (1 âˆ’ Î±) fraction of the coordinates, assuming that such a codeword exists. An error correcting code thus protects a transmission from any adversarial corruption of an
Î±-fraction of a codeword. A list-recoverable code protects a transmission from uncertainty as well
as corruption. Instead of receiving a single (possibly corrupted) symbol in each coordinate, the decoder for a list-recoverable code receives a short list of size  of possible symbols. Let L1,..., LM
be such a sequence of lists, where list Lm contains the mth symbol of a noisy codeword. The requirement on the decoder is that it is able to recover all possible codewords x that â€œagreeâ€ with at
least (1 âˆ’ Î±) fraction of the lists, in the sense that mth symbol of x appears in Lm. Formally,
Definition 3.4 [18]. An (Î±, , L)-list-recoverable code is a pair of mappings (Enc, Dec), where Enc :
X â†’ [Z]M , and Dec : ([Z] )
M â†’ XL, such that the following holds. Let L1,..., LM âˆˆ [Z]. Then
for every x âˆˆ X satisfying |{m : Enc(x)m âˆˆ Lm }| â‰¥ (1 âˆ’ Î±)M we have that x âˆˆ Dec(L1,..., LM ).
In our case, the lists Lm = {(y,w)} contain the â€œstand outsâ€ of Sm, with the guarantee that for
every heavy-hitter xâˆ—, we have that (hm (xâˆ—), xâˆ—[m]) appears in Lm for most coordinates m. We
can therefore, in principle, use a list-recoverable code to recover all heavy-hitters (in fact, with
this formulation it suffices that the lists Lm contain only the set of heavy items {w} found to be
heavy in Sm). However, there are currently no explicit codes achieving the parameter settings that
are needed for the construction. To overcome this issue, Larsen et al. [25] constructed a relaxed
variant of list-recoverable codes, and showed that their relaxed variant still suffices for their (nonprivate) reduction. Intuitively, their relaxed code utilizes the randomness in the first portion of
every element in the lists (i.e., the outcome of the hash functions). Formally,
Definition 3.5. An (Î±, , L)-unique-list-recoverable code is a pair of mappings (Enc, Dec) where
Enc : X â†’ ([Y]Ã—[Z])
M , and Dec : (([Y]Ã—[Z])
 )
M â†’ XL, such that the following holds. Let
3We remark that this presentation is oversimplified. In the analysis in Reference [25], the problem is not only that each xâˆ—
causes a small fraction of the hash functions to fail. Rather, the problem is that a small fraction of the coordinates are â€œbadâ€
(since otherwise it may be that each x is failed by very few hash functions, but every hm ends up â€œbad,â€ because they fail
for some different xâ€™s).
ACM Transactions on Algorithms, Vol. 15, No. 4, Article 51. Publication date: October 2019.         
51:8 M. Bun et al.
L1,..., LM âˆˆ ([Y]Ã—[Z])
. Assume that for every m âˆˆ [M], if (y, z), (y
, z
) âˆˆ Lm then y  y
. Then
for every x âˆˆ X satisfying |{m : Enc(x)m âˆˆ Lm }| â‰¥ (1 âˆ’ Î±)M we have that x âˆˆ Dec(L1,..., LM ).
Theorem 3.6 [25]. There exist constants C > 1 and 0 < Î± < 1 such that the following holds. For
all constants M â‰¤ log |X | and Y,  âˆˆ N, and for every fixed choice of functions h1,...,hM : X â†’ [Y],
there is a construction of an (Î±, , L)-unique-list-recoverable code
Enc : X â†’ ([Y] Ã— [Z])
M and Dec : (([Y] Ã— [Z])
 )
M â†’ XL,
where L â‰¤ C Â·  and Z â‰¤ (|X |
1/M Â· Y )
C. Furthermore, there is a mapping Enc : 
 X â†’ [Z]M such that
for every x âˆˆ X we have Enc(x) = ((h1 (x), Enc 
(x)1),..., (hM (x), Enc 
(x)M )).
There is a pre-processing algorithm that, given M,Y, ,h1,...,hM , computes a description of the
functions Enc, Dec in poly(M) time.4 Then, evaluating Enc(x) takes linear time and space in addition
to O(M) invocations of the hash functions, and evaluating Dec(L1,..., LM ) takes linear space and
polynomial time.
For completeness, we include the proof in Appendix C. In the following sections we construct
a private version of the algorithm in Reference [25] and show that it achieves optimal error in all
parameters. We remark that the focus in Reference [25] was on space and runtime, and that in fact,
in the non-private literature the meaning of â€œminimal achievable errorâ€ is unclear, as achieving
zero error is trivial (it is â€œonlyâ€ a question of runtime and memory).
3.2 Our Contribution
In the following sections, we construct an LDP heavy-hitters protocol and show that it achieves
optimal error in all parameters. Our private algorithm is obtained from the non-private algorithm
in Reference [25] with the following modifications.
As in Reference [25] showed, by appropriately choosing the parameters of a unique-listrecoverable code, the strategy outlined above identifies all â€œextra heavyâ€ elements, i.e., those with
frequencies  n/ log |X |. To identify Î”-heavy elements for some parameter Î”, the first step in their
algorithm is to hash input elements intoOËœ (n/Î”) buckets. Informally, one can then show that w.h.p.
the weight of every Î”-heavy element increases significantly relative to the size of its bucket, and
in particular becomes â€œextra-heavyâ€ with respect to its bucket. This allows Reference [25] to focus
on identifying â€œextra heavyâ€ elements within each bucket separately.
Under LDP, however, this reduction to the larger error case cannot be applied directly, as it is
unclear at which point to apply the hash function. The server cannot apply it, since it does not
get to see the usersâ€™ inputs, and the users cannot reveal their buckets to the server as this would
compromise privacy. We overcome this issue by appending the outcome of a similar hash function
to the encoding of elements in the unique-list-recoverable code. As we will see in Section 3.4, this
allows us to identify all Î”-heavy elements directly in the original database. We obtain the following
theorem:
Theorem 3.7. Let Î²,Îµ â‰¤ 1, and let S âˆˆ Xn be a (distributed) database. There exists an Îµ-LDP algorithm that returns a list Est of length OËœ (
âˆš
n) such that with probability 1 âˆ’ Î²
(1) âˆ€(x, a) âˆˆ Est, |a âˆ’ fS (x)| â‰¤ O( 1
Îµ

n log(
min{n, |X | }
Î² )).
(2) For every x âˆˆ X s.t. fS (x) â‰¥ O( 1
Îµ

n log( |X |
Î² )), we have that x is included in Est.
4The pre-processing algorithm has deterministic poly(M) time for every M of the form M = Di for some universal constant
D. Otherwise, the algorithm has poly(M) Las Vegas expected runtime. We ignore this issue for simplicity.
ACM Transactions on Algorithms, Vol. 15, No. 4, Article 51. Publication date: October 2019.        
Heavy Hitters and the Structure of Local Privacy 51:9
The server uses processing memory of size OËœ (
âˆš
n) and runs in time OËœ (n). Each user has OËœ (1) runtime,
memory, and communication.
3.3 Additional Preliminaries
We start by presenting the additional preliminaries that enable our construction.
3.3.1 Frequency Oracle. Recall that a frequency oracle is a protocol that, after communicating
with the users, outputs a data structure capable of approximating the frequency of every domain
element x âˆˆ X. Our protocol uses the frequency oracle of Reference [4], named Hashtogram, as a
subroutine.
Theorem 3.8 (Algorithm Hashtogram [4]). Let S âˆˆ Xn be a database that is distributed across
n users, and let Î²,Îµ â‰¤ 1. There exists an Îµ-LDP algorithm such that the following holds. Fix a domain
element x âˆˆ X to be given as a query to the algorithm. With probability at least 1 âˆ’ Î², the algorithm
answers the query x with a(x) satisfying:
|a(x) âˆ’ fS (x)| â‰¤ O


1
Îµ Â·

n log 
min{n, |X |}
Î²
	



.
The server uses processing memory of size OËœ (
âˆš
n) and runs in time OËœ (1) per query (plus OËœ (n) for
instantiation). Each user has OËœ (1) runtime, memory, and communication. The OËœ notation hides logarithmic factors in n and |X |.
Observe that by a union bound, algorithm Hashtogram answers every fixed set ofw queries with
worst case error O( 1
Îµ Â·

n log(
w Â·min{n, |X | }
Î² )). The min{n, |X |} factor is known to be unnecessary
for some regimes of n, |X |, Î², or if one allows a slightly larger runtime or memory. In particular,
Bassily et al. gave a (simplified) analysis of algorithm Hashtogram for the case where the domain
X is small, with the following guarantees.
Theorem 3.9 (Algorithm Hashtogram [4]). Let S âˆˆ Xn be a database that is distributed across
n users, and let Î²,Îµ â‰¤ 1. There exists an Îµ-LDP algorithm such that the following holds. Fix a domain
element x âˆˆ X to be given as a query to the algorithm. With probability at least 1 âˆ’ Î², the algorithm
answers the query x with a(x) satisfying:
|a(x) âˆ’ fS (x)| â‰¤ O


1
Îµ Â·

n log 
1
Î²
	



.
The server uses processing memory of size OËœ (|X |) and runs in time OËœ (1) per query (plus OËœ (|X |
2)
for instantiation). Each user has OËœ (1) runtime, memory, and communication. The OËœ notation hides
logarithmic factors in n and |X |.
We will only apply Theorem 3.9 on small domains X (satisfying |X |
2 = OËœ (n)). Our eventual
analysis in the proof of Theorem 3.14 will make use of both accuracy guarantees of Hashtogram.
3.3.2 The Poisson Approximation. When throwing n balls into R bins, the distribution of the
number of balls in a given bin is Bin(n, 1/R). As the Poisson distribution is the limit distribution
of the binomial distribution when n/R is fixed and n â†’ âˆž, the distribution of the number of balls
in a given bin is approximately Pois(n/R). In fact, in some cases we may approximate the joint
distribution of the number of balls in all the bins by assuming that the load in each bin is an
independent Poisson random variable with mean n/R.
ACM Transactions on Algorithms, Vol. 15, No. 4, Article 51. Publication date: October 2019.    
51:10 M. Bun et al.
Theorem 3.10 (e.g., [28]). Suppose that n balls are thrown into R bins independently and uniformly at random, and let Xi be the number of balls in the ith bin, where 1 â‰¤ i â‰¤ R. Let Y1,...,YR be
independent Poisson random variables with mean n/R. Let f (x1,..., xR ) be a nonnegative function.
Then,
E [f (X1,...,XR )] â‰¤ e
âˆš
nE [f (Y1,...,YR )] .
In particular, the theorem states that any event that takes place with probability p in the Poisson
case, takes place with probability at most peâˆš
n in the exact case (this follows by letting f be the
indicator function of that event).
We will also use the following bounds for the tail probabilities of a Poisson random variable:
Theorem 3.11 [2]. Let X have Poisson distribution with mean Î¼. For 0 â‰¤ Î± â‰¤ 1,
Pr[X â‰¤ Î¼(1 âˆ’ Î±)] â‰¤ eâˆ’Î±2Î¼/2
Pr[X â‰¥ Î¼(1 + Î±)] â‰¤ eâˆ’Î±2Î¼/2
.
3.3.3 Concentration (with Limited Independence). We first state a version of the multiplicative Chernoff bound, including a formulation of the upper tail bound that holds under limited
independence.
Theorem 3.12 [34, Theorem 2]. Let X1,...,Xn be random variables taking values in {0, 1}. Let
X = n
i=1 Xi and Î¼ = E[X]. Then for every 0 â‰¤ Î± â‰¤ 1,
(1) If the Xi â€™s are Î¼Î±-wise independent, then Pr[X â‰¥ Î¼(1 + Î±)] â‰¤ exp(âˆ’Î±2Î¼/3).
(2) If the Xi â€™s are fully independent, then Pr[X â‰¥ Î¼(1 + Î±)] â‰¤ exp(âˆ’Î±2Î¼/3).
(3) If the Xi â€™s are fully independent, then Pr[X â‰¤ Î¼(1 âˆ’ Î±)] â‰¤ exp(âˆ’Î±2Î¼/2).
Next, we present a limited independence version of Bernsteinâ€™s inequality.
Theorem 3.13 [20, Lemma 2]. Let X1,...,Xn be k-wise independent random variables, for an
even integer k â‰¥ 2, that are each bounded by T in magnitude. Let X = n
i=1 Xi and Î¼ = E[X]. Let
Ïƒ2 = 
i E(Xi âˆ’ EXi )
2. Then there exists a constant C > 0 such that for all Î» > 0,
Pr[

X âˆ’ Î¼
 > Î»] â‰¤ Ck Â·

(Ïƒ
âˆš
k/Î»)
k + (Tk/Î»)
k
.
3.3.4 Random Partition of the Users. Suppose that we want to apply k locally private computations to a (distributed) database. One option would be to simply apply all k computations in
parallel, and then to argue about the overall privacy guarantees using composition theorems for
differential privacy. Another option is to first randomly partition the users into k disjoint subsets
and to apply each of the computations to a different subset. In the second option we do not need
to worry about composition (since each computation is applied to a disjoint subset of the users),
but we do need to argue about the incurred sampling error. It turns out that, in many cases, we
can achieve more accurate algorithms using the second option. This technique appeared, e.g., in
References [4, 5].
To illustrate the technique, consider a (distributed) database S âˆˆ Xn, and let q1,...,qk : X â†’
{0, 1} be k predicates mapping X to {0, 1}. Suppose that we want to estimate, for each i âˆˆ [k], the
number of elements x âˆˆ S s.t. qi (x) = 1, that is, estimating
qi (S)  


x âˆˆ S : qi (x) = 1


 .
For every fixed i âˆˆ [k], basic results in local differential privacy allow for estimating qi (S) up
to an error  1
Îµ
âˆš
|S | = 1
Îµ
âˆš
n. Hence, using composition theorems for (pure) differential privacy, by
taking Îµ â† Îµ
k , one could estimate all k counts up to an error  k
Îµ
âˆš
n while guaranteeing (Îµ, 0)-local
differential privacy.
ACM Transactions on Algorithms, Vol. 15, No. 4, Article 51. Publication date: October 2019.   
Heavy Hitters and the Structure of Local Privacy 51:11
ALGORITHM 1: PrivateExpanderSketch
Notation: Let CM ,CY ,C,CÐ´, and Cf be universal constants (to be specified later).
Denote M = CM Â· log |X |/ log log |X |, Y = logCY |X |, and  = C Â· log |X |.
Public randomness: Random partition of [n] into M sets I1,..., IM .
Pairwise independent hash functions h1,...,hM : X â†’ [Y].
(CÐ´ Â· log |X |)-wise independent hash function Ð´ : X â†’ [B].
Tool used: A unique-list-recoverable code (Enc, Dec) with parameters M,Y,  using
h1,...,hM , as in Theorem 3.6.
Setting: Each player i âˆˆ [n] holds a value xi âˆˆ X. Define S = (x1,..., xn ).
âˆ€m âˆˆ [M], let Sm =

Ð´(xi ), Enc(xi )m

i âˆˆIm
=

Ð´(xi ),hm (xi ), Enc 
(xi )m

i âˆˆIm
.
1. âˆ€m âˆˆ [M], use Hashtogram(Sm ) with privacy parameter Îµ
2 to get estimates Ë†
fSm (b,y, z) for fSm (b,y, z)
for every (b,y, z) âˆˆ [B]Ã—[Y]Ã—[Z].
2. For every (m,b) âˆˆ [M]Ã—[B], initialize Lb
m = âˆ….
3. For every (m,b,y) âˆˆ [M]Ã—[B]Ã—[Y]:
(a) Let z = argmaxz { Ë†
fSm (b,y, z)}.
(b) If Ë†
fSm (b,y, z) â‰¥ Cf Â· log log |X |
Îµ Â·
 n log |X | = O
 âˆšn log |X |
ÎµM 	
and |Lb
m | â‰¤ C Â· log |X | then add (y, z)
to Lb
m.
4. For every b âˆˆ [B] let H	b = Dec(Lb
1 ,..., Lb
M ). Define H	 = 
b âˆˆ[B] H	b .
5. Use Hashtogram(S) with privacy parameter Îµ
2 to obtain Ë†
fS (x) â‰ˆ fS (x) for every x âˆˆ H	.
6. Return Est = 

x, Ë†
fS (x)

: x âˆˆ H	

.
Alternatively, let S1,..., Sk be a random partition of S into k subsets. By the Chernoff bound,
with high probability we have that
âˆ€i : qi (Si ) â‰ˆ 1
k Â· qi (S) and |Si | â‰ˆ n
k .
Now, when estimating every qi (Si ) under (Îµ, 0)-local differential privacy, we incurr an error of
 1
Îµ
âˆš
|Si | â‰ˆ 1
Îµ
âˆš
n/k. Renormalizing to S, this gives an error of  k Â· 1
Îµ
âˆš
n/k = 1
Îµ
âˆš
n Â· k, which is
better than the first option by a âˆš
k factor. These ideas will be made formal in the next section.
3.4 The Full Protocol
In this section we construct and analyze our heavy-hitters algorithm. The full algorithm appears
as Algorithm PrivateExpanderSketch. It is immediate from the construction that algorithm
PrivateExpanderSketch satisfies Îµ-LDP. We now proceed with the utility analysis. While we
must make the restriction n â‰¤ |X | for the analysis to go through, we remark that in the complementary case n > |X |, it is possible to just apply the frequency oracle Hashtogram (Theorem 3.8)
to every item in X and obtain the same runtime, memory, and communication guarantees.
Theorem 3.14. Let S âˆˆ Xn be a database that is distributed across n users, with n â‰¤ |X |, and let
Î²,Îµ â‰¤ 1. Algorithm PrivateExpanderSketch returns a list Est of length OËœ (
âˆš
n) s.t. with probability
1 âˆ’ Î² we have that
(1) For every (x, a) âˆˆ Est, we have that |a âˆ’ fS (x)| â‰¤ O( 1
Îµ

n log(
min{n, |X | }
Î² )).
(2) For every x âˆˆ X s.t. fS (x) â‰¥ O( 1
Îµ

n log( |X |
Î² )), we have that x is included in Est.
ACM Transactions on Algorithms, Vol. 15, No. 4, Article 51. Publication date: October 2019.       
51:12 M. Bun et al.
The server uses processing memory of sizeOËœ (
âˆš
n), and runs in timeOËœ (n). Each user hasOËœ (1) runtime,
memory, and communication.
Proof. Item 1 of the lemma follows directly from Theorem 3.8 (the properties of Hashtogram).
We now prove item 2. Below, we will freely assume that n â‰¥ O(log |X |/Îµ2), with item 2 being
vacuously true otherwise. In addition, to simplify notations, we fix Î² = Î˜( 1
|X |
). This is does not
limit our generality, because for smaller values of Î² (that is for Î² < 1/|X |) we can embed X in a
larger domain of size 1
Î² , thereby decreasing the failure probability to Î².
For some constant CH â‰¥ 1, we say that an element x âˆˆ X is heavy if fS (x) â‰¥ CH
Îµ

n Â· log |X |
and non-heavy otherwise. We let H denote the set of all heavy elements. Observe that |H| â‰¤ Îµ Â· 
n/ log |X |. For b âˆˆ [B] denote the set of all heavy elements that are mapped to b under Ð´ as
Hb = {x âˆˆ H : Ð´(x) = b}.
We begin by defining a sequence of events E1,..., E7. We will show that all of these events
occur simultaneously with high probability, and that if they indeed all occur, then the guarantee
of item 2 holds. First consider the following event:
Event E1 (over the choice of Ð´):
For every b âˆˆ [B], the number of heavy elements that are mapped to b under Ð´ is at most
C1 log |X |. Furthermore, the sum of frequencies of all non-heavy elements (i.e., elements
x  H) that are mapped to b is at most C1
Îµ
âˆš
n log3/2 |X |, where C1 > 0 is an absolute constant. That is, âˆ€b âˆˆ [B]:
|Hb | â‰¤ C1 log |X | and 
xH: Ð´(x )=b
fS (x) â‰¤
C1
Îµ
âˆš
n log3/2 |X |
We now show that E1 happens with high probability. For B =  1
10CH Â· Îµ
âˆš
n/ log3/2 |X | â‰¤ |X |, it
follows by the limited independence Chernoff bound (Theorem 3.12, item 1) and a union bound
over all b âˆˆ [B] that for some constant C1 > 0,
Pr
Ð´
[âˆƒb âˆˆ [B] s.t. |{x âˆˆ H : Ð´(x) = b}| > C1 log |X |] <
1
2|X |
. (1)
Here, we made use of the fact that Ð´ : X â†’ [B] is (CÐ´ Â· log |X |)-wise independent for a constantCÐ´.
We next invoke the limited independence formulation of Bernsteinâ€™s inequality (Theorem 3.13).
Fix b âˆˆ [B]. We will consider the collection of random variables Rx indexed by x âˆˆ X\H defined as
follows. Define Rx = 1Ð´(x )=b Â· fS (x). Then each Rx is bounded by T < 1
Îµ

n Â· log |X | and 
xH Rx
has variance
Ïƒ2 =

xH
f 2
S (x)
 1
B âˆ’ 1
B2

â‰¤

xH
1
B Â· f 2
S (x) â‰¤
1
Îµ2 n log2 |X |.
It then follows by Theorem 3.13 and a union bound over all b âˆˆ [B] that
Pr
Ð´
âŽ¡
âŽ¢
âŽ¢
âŽ¢
âŽ¢
âŽ¢
âŽ£
âˆƒb âˆˆ [B] s.t. 
xH: Ð´(x )=b
fS (x) >
C1
Îµ
âˆš
n log3/2 |X |
âŽ¤
âŽ¥
âŽ¥
âŽ¥
âŽ¥
âŽ¥
âŽ¦
<
1
2|X |
. (2)
This shows that Pr[E1] â‰¥ 1 âˆ’ 1
|X |
. We now continue the analysis assuming that Event E1 occurs.
Event E2 (over partitioning [n] into I1,..., IM ):
For every (b,m) âˆˆ [B]Ã—[M] we have |{iâˆˆIm : xiH and Ð´(xi )=b}| â‰¤ 2C1
M Îµ
âˆš
n log3/2 |X |.
ACM Transactions on Algorithms, Vol. 15, No. 4, Article 51. Publication date: October 2019.       
Heavy Hitters and the Structure of Local Privacy 51:13
Conditioned on Event E1, for every b âˆˆ [B] we have that |{i âˆˆ [n] : xi  H and Ð´(xi ) = b}| â‰¤
C1
Îµ
âˆš
n log3/2 |X |. Hence, by the Chernoff bound (Theorem 3.12) and a union bound over all (b,m) âˆˆ
[B]Ã—[M], we get that Pr[E2 |E1] â‰¥ 1 âˆ’ 1
|X |
.
Event E3 (over partitioning [n] into I1,..., IM ):
For every x âˆˆ H there exists a subset Mx
3 âŠ† [M] of size |Mx
3 | â‰¥ (1 âˆ’ Î±
10 )M s.t. for every m âˆˆ Mx
3 we have that |{i âˆˆ Im : xi = x}| â‰¥ fS (x )
2M , where Î± is the constant from
Theorem 3.6.
Recall that M = CM Â· log |X |/ log log |X |. As in Theorem 3.10 (the Poisson approximation), we
analyze event E3 in the Poisson case. To that end, fix x âˆˆ H, and let Ëœ
J1,..., Ëœ
JM be independent
Poisson random variables with mean fS (x )
M . Let us say that m âˆˆ [M] is bad if Ëœ
Jm < fS (x )
2M . Now fix
m âˆˆ [M]. Using a tail bound for the Poisson distribution (see Theorem 3.11), assuming that n â‰¥
8C2
M log |X |, we have thatm is bad with probability at most 1
logCM |X |
. As Ëœ
J1,..., Ëœ
JM are independent,
the probability that there are more than Î±M/10 bad choices for m is at most
 M
Î±M/10 	  1
logCM |X |
	Î± M/10
â‰¤
 10e
Î± logCM |X |
	 Î±CM log |X |
10 log log |X |
=
 10e
Î±
 Î±CM log |X |
10 log log |X |
Â·
 1
|X |
	 Î±C2
M
10
=
 1
|X |
	 Î±C2
M
10 âˆ’o(1)
.
Hence, by Theorem 3.10 we get that
Pr 
There are at least Î±M/10 indices m s.t.
|{i âˆˆ Im : xi = x}| < fS (x)/(2M)

â‰¤ e

fS (x) Â·
 1
|X |
	 Î±C2
M
10 âˆ’o(1)
.
Choosing CM â‰¥ 10
Î± and using the union bound over every x âˆˆ H we get that Pr[E3] â‰¥ 1 âˆ’ 1
|X |
.
Event E4 (over partitioning [n] into I1,..., IM ):
There exists a subset M4 âŠ† [M] of size |M4 | â‰¥ (1 âˆ’ Î±
10 )M s.t. for every m âˆˆ M4 we have n
2M â‰¤ |Im | â‰¤ 2n
M .
An analysis similar to that of Event E3 shows that Pr[E4] â‰¥ 1 âˆ’ 1
|X |
.
Event E5 (over sampling h1,..., hM ):
For every b âˆˆ [B] there exists a subset Mb
5 âŠ† [M] of size |Mb
5 | â‰¥ (1 âˆ’ Î±
10 )M s.t. for every
m âˆˆ Mb
5 we have that hm perfectly hashes the elements of Hb = {x âˆˆ H : Ð´(x) = b}.
We analyze event E5 assuming that event E1 occurs, in which case |Hb | â‰¤ C1 Â· log |X | for every
b âˆˆ [B]. Fix b âˆˆ [B] and m âˆˆ [M]. Recall that hm is a pairwise independent hash function mapping
X to [Y]. Hence, for x   x we have Prhm [hm (x 
) = hm (x)] = 1
Y . Using the union bound over every
ACM Transactions on Algorithms, Vol. 15, No. 4, Article 51. Publication date: October 2019.  
51:14 M. Bun et al.
pair (x, x 
) with x   x âˆˆ Hb , assuming that Y â‰¥ C2
1 Â· logCM +2 |X |, we have
Pr
hm
[âˆƒ(x, x 
) âˆˆ Hb Ã— Hb s.t. x  x 
, hm (x 
) = hm (x)] â‰¤
1
Y

|Hb |
2
	
â‰¤
1
logCM |X |
.
That is, the probability that hm does not perfectly hash the elements of Hb is at most 1
logCM |X |
. As
the hash functions h1,...,hM are independent, the probability that there are more than Î±M/10
choices of m s.t. hm does not perfectly hash Hb is at most
 M
Î±M/10 	  1
logCM |X |
	Î± M/10
â‰¤
 1
|X |
	 Î±C2
M
10 âˆ’o(1)
.
Setting CM â‰¥ 10
Î± and using the union bound over every b âˆˆ [B] we get that Pr[E5 |E1] â‰¥ 1 âˆ’ 1
|X |
.
Event E6 (over sampling h1,..., hM ):
For every x âˆˆ H there exists a subset Mx
6 âŠ† [M] of size |Mx
6 | â‰¥ (1 âˆ’ Î±
10 )M s.t. for
every m âˆˆ Mx
6 we have that |{i âˆˆ Im : xi  H and Ð´(xi ) = Ð´(x) and hm (xi ) = hm (x)}| â‰¤ 1
ÎµM

n Â· log |X |.
We analyze event E6 assuming that event E2 occurs, in which case for every b,m we have that
|{iâˆˆIm : xiH, Ð´(xi )=b}| â‰¤ 2C1
M Îµ
âˆš
n log3/2 |X |. Fix x âˆˆ H and fix m âˆˆ [M]. We have that
E
hm
[


i âˆˆ Im : xiH, Ð´(xi )=Ð´(x), hm (xi )=hm (x)


] =

i âˆˆIm :
xi H,
Ð´(xi )=Ð´(x )
E
hm

1hm (xi )=hm (x )

â‰¤
1
Y Â·
2C1
MÎµ
âˆš
n log3/2 |X |.
Thus, by Markovâ€™s inequality, we have that
Pr
hm




i âˆˆ Im : xiH, Ð´(xi )=Ð´(x), hm (xi )=hm (x)


 â‰¥
1
ÎµM

n Â· log |X |

â‰¤
2C1 log |X |
Y .
Let us say that m is bad for x if 


i âˆˆ Im : xiH, Ð´(xi )=Ð´(x), hm (xi )=hm (x)


 â‰¥ 1
ÎµM

n Â· log |X |. So
m is bad with probability at most 2C1 log |X |
Y . As the hash functions h1,...,hM are independent,
assuming that Y â‰¥ 2C1 Â· logCM +1 |X |, the probability that there are more than Î±M/10 bad choices
for m is at most
 M
Î±M/10 	  1
logCM |X |
	Î± M/10
â‰¤
 1
|X |
	 Î±C2
M
10 âˆ’o(1)
.
Setting CM â‰¥ 10
Î± and using the union bound over every x âˆˆ H we get that Pr[E6 |E2] â‰¥ 1 âˆ’ 1
|X |
.
Event E7 (over the randomness of Hashtogram):
For every b âˆˆ [B] there exists a subset Mb
7 âŠ† [M] of size |Mb
7 | â‰¥ (1 âˆ’ Î±
5 )M s.t. for
every m âˆˆ Mb
7 and every (y, z) âˆˆ [Y]Ã—[Z] we have that 


Ë†
fSm (b,y, z) âˆ’ fSm (b,y, z)


 â‰¤
C7 Â·log log |X |
Îµ

n/ log |X |, where C7 > 0 is an absolute constant.
ACM Transactions on Algorithms, Vol. 15, No. 4, Article 51. Publication date: October 2019.        
Heavy Hitters and the Structure of Local Privacy 51:15
We analyze Event E7 conditioned on Event E4, and let M4 âŠ† [M] denote the set from Event E4.
Recall that |M4 | â‰¥ (1 âˆ’ Î±
10 )M, and that for every m âˆˆ M4 we have that n/2M â‰¤ |Sm | â‰¤ 2n/M.
Fix b âˆˆ [B] and fix m âˆˆ M4. Let us say that m is bad if there exists (y, z) âˆˆ [Y]Ã—[Z]
s.t. | Ë†
fSm (b,y, z) âˆ’ fSm (b,y, z)| > 6
Îµ

|Sm | Â· log(4YZ Â· logCM |X |) = O(
log log |X |
Îµ

n/ log |X |). By the
properties of algorithm Hashtogram (Theorem 3.9), we have that m is bad with probability at
most 1
logCM |X |
. As the coins of Hashtogram are independent across different executions (i.e., for
different values of m), the probability that there are more than Î± |M4 |/10 bad choices for m âˆˆ M4
is at most
 |M4 |
Î± |M4 |/10 	  1
logCM |X |
	Î± |M4 |/10
â‰¤
 1
|X |
	 Î±C2
M
20 âˆ’o(1)
.
Setting CM â‰¥ 20
Î± and using the union bound over every b âˆˆ [B] we get that Pr[E7 |E4] â‰¥ 1 âˆ’ 1
|X |
.
We are now ready to complete the proof, by showing that whenever E1, E2, E3, E4, E5, E6, E7 occur
we have that every heavy element x âˆˆ H appears in the output list H	. To that end, fix x âˆˆ H, and
denote Mx = Mx
3 âˆ© M4 âˆ© MÐ´(x )
5 âˆ© Mx
6 âˆ© MÐ´(x )
7 . Observe that |Mx | â‰¥ (1 âˆ’ Î±)M. We now show that
for every m âˆˆ Mx it holds that (hm (x), Enc 
(x)m ) âˆˆ L
Ð´(x ) m , in which case, by the properties of the
code (Enc, Dec) we have that x âˆˆ H	Ð´(x ) = Dec(L
Ð´(x )
1 ,..., L
Ð´(x )
M ) and, hence, x âˆˆ H	.
So let m âˆˆ Mx . By Event E3 we have that
fSm

Ð´(x),hm (x), Enc 
(x)m

â‰¥
CH Â· log log |X |
2CM Â· Îµ

n/ log |X |.
Hence, by Event E7 we have that
Ë†
fSm

Ð´(x),hm (x), Enc 
(x)m

â‰¥ Cf Â·
log log |X |
Îµ

n/ log |X |, (3)
where Cf =  CH
2CM âˆ’ C7

.
However, let z  Enc 
(x)m, and recall that by Event E5 there does not exist a heavy element
x   x s.t. Ð´(x 
) = Ð´(x) and hm (x 
) = hm (x). In addition, by Event E6, there could be at most
log log |X |
Îµ

n/ log |X | indices i âˆˆ Im s.t. xi is non-heavy and Ð´(xi ) = Ð´(x) and hm (xi ) = hm (x). That
is,
fSm (Ð´(x),hm (x), z) =










âŽ§âŽªâŽªâŽªâŽªâŽª
âŽ¨
âŽªâŽªâŽªâŽªâŽª
âŽ©
i âˆˆ Im :
xi âˆˆ H,
Ð´(xi ) = Ð´(x),
hm (xi ) = hm (x),
Enc 
(xi )m = z
âŽ«âŽªâŽªâŽªâŽªâŽª
âŽ¬
âŽªâŽªâŽªâŽªâŽª
âŽ­










+










âŽ§âŽªâŽªâŽªâŽªâŽª
âŽ¨
âŽªâŽªâŽªâŽªâŽª
âŽ©
i âˆˆ Im :
xi  H,
Ð´(xi ) = Ð´(x),
hm (xi ) = hm (x),
Enc 
(xi )m = z
âŽ«âŽªâŽªâŽªâŽªâŽª
âŽ¬
âŽªâŽªâŽªâŽªâŽª
âŽ­










â‰¤ 0 + log log |X |
Îµ

n/ log |X |.
Hence, by Event E7 we have that
Ë†
fSm (Ð´(x),hm (x), z) â‰¤ (C7 + 1)
log log |X |
Îµ

n/ log |X |. (4)
By Inequalities (3) and (4), and by setting CH large enough, we ensure that Enc 
(x)m is identified in
step 3a as the argmax of Ë†
fSm (Ð´(x),hm (x), Â·). Hence, to show that (hm (x), Enc 
(x)m ) is added to L
Ð´(x ) m
in step 3b, it suffices to show that |L
Ð´(x ) m | â‰¤ C Â· log |X | for some constant C. First recall that, by
Event E1, there are at most C1 Â· log |X | heavy elements x  âˆˆ H s.t. Ð´(x 
) = Ð´(x). Hence, there could
be at most C1 Â· log |X | pairs (y, z) âˆˆ L
Ð´(x ) m s.t. âˆƒx  âˆˆ H s.t. (Ð´(x 
),hm (x 
), Enc 
m (x 
)) = (Ð´(x),y, z).
ACM Transactions on Algorithms, Vol. 15, No. 4, Article 51. Publication date: October 2019.           
51:16 M. Bun et al.
Therefore, it suffices to show that L
Ð´(x ) m contains at most O(log |X |) pairs (y, z) that are added,
because they do not correspond to a heavy element x  âˆˆ H.
To that end, observe that by the condition on step 3b, and by Event E7, in order for a pair (y, z)
to be added to L
Ð´(x ) m it must be that fSm (Ð´(x),y, z) â‰¥ log log |X |
Îµ

n/ log |X |. However, by Event E2,
there are at most 2C1 Â·log log |X |
CM Â·Îµ

n log |X | elements in Sm that yield the same value Ð´(x), but are not
generated from a heavy element. Hence, there could be at most 2C1
CM Â· log |X | pairs (y, z) âˆˆ L
Ð´(x ) m
such that there is no x  âˆˆ H where (Ð´(x 
),hm (x 
), Enc 
m (x 
)) = (Ð´(x),y, z). Overall, we have that
|L
Ð´(x ) m | â‰¤ C Â· log |X | for C = C1 + 2C1/CM .
4 ADVANCED GROUPOSITION AND MAX-INFORMATION
In this section, we show that local differential privacy admits stronger guarantees of group privacy
than differential privacy in the central model. In particular, the protection offered to groups of size
k under LDP degrades proportionally to about âˆš
k, a quadratic improvement over what happens
in the central model. It is not a coincidence that this behavior mirrors the privacy guarantee of
differentially private algorithms under composition. Indeed, the proof of our â€œadvancedâ€ group
privacy bound will follow the same strategy as the proof of the so-called advanced composition
theorem [12, 13].
Definition 4.1. For random variables A, B, define the privacy loss function A,B (y) = ln(Pr[A =
y]/ Pr[B = y]). Define the privacy loss random variable LA,B to be A,B (y) for y â† A.
Observe that an algorithm R is (Îµ, 0)-differentially private if and only if for every neighboring
inputs x, x  and for every possible outcome y we have



R(x ),R(x)


 â‰¤ Îµ.
Just as in the proof of the advanced composition theorem, we will leverage the fact that the
expected privacy loss of any individual local randomizer is LRi (xi ),Ri (x
i ) = O(Îµ2), which is substantially smaller than the worst-case privacy loss of Îµ. Since each local randomizer is applied
independently, the cumulative privacy loss incurred by changing a group of k inputs concentrates
to within O(
âˆš
kÎµ) of its expectation O(kÎµ2).
Theorem 4.2 (Advanced Grouposition for Pure LDP). Let x âˆˆ Xn and x  âˆˆ Xn differ in at
most k entries for some 1 â‰¤ k â‰¤ n. Let A = (R1,..., Rn ) : Xn â†’ Y be Îµ-LDP. Then for every Î´ > 0
and Îµ = kÎµ2/2 + Îµ

2k ln(1/Î´ ), we have
Pr
yâ†A(x )

ln Pr[A(x) = y]
Pr[A(x ) = y] > Îµ

â‰¤ Î´ .
In particular, for every Î´ > 0 and every set T âŠ† Y, we have Pr[A(x) âˆˆ T ] â‰¤ eÎµ
Pr[A(x 
) âˆˆ T ] + Î´.
For simplicity, the above theorem is stated for an algorithm A of the form A(x) =
(R1 (x1),..., Rn (xn )), i.e., for local protocols in which the server simply outputs all of the noisy
reports that it gets from the users. As differential privacy is closed under post-processing, the result also applies for protocols with arbitrary aggregation procedure, that is protocols of the form
A(x) = W(R1 (x1),..., Rn (xn )).
Proof. Without loss of generality, we may assume that x and x  differ in the first k coordinates.
Since each randomizer is applied independently, we may write the privacy loss between A(x) and
ACM Transactions on Algorithms, Vol. 15, No. 4, Article 51. Publication date: October 2019.      
Heavy Hitters and the Structure of Local Privacy 51:17
A(x 
) as
LA(x ), A(x) =

k
i=1
LRi (xi ),Ri (x
i ).
Since each Ri is Îµ-differentially private, Proposition 3.3 of Reference [6] implies
E 
LRi (xi ),Ri (x
i )

â‰¤
1
2
Îµ2
, |LRi (xi ),Ri (x
i ) | â‰¤ Îµ.
Hence, by Hoeffdingâ€™s inequality, we have that for every t > 0,
Pr[LA(x ), A(x) > kÎµ2
/2 + t] â‰¤ eâˆ’t 2/2kÎµ 2
.
Setting t = Îµ

2k ln(1/Î´ ) completes the proof.
We remark that it is straightforward to extend Theorem 4.2 to handle (Îµ, Î´ )-LDP algorithms
using the same ideas as in the proof of the advanced composition theorem (see the discussion in
Section 2.2 of Reference [39]).
Theorem 4.3 (Advanced Grouposition for Approximate LDP). Let x âˆˆ Xn and x  âˆˆ Xn differ
in at most k entries for some 1 â‰¤ k â‰¤ n. Let A = (R1,..., Rn ) : Xn â†’ Y be (Îµ, Î´ )-LDP. Then for every
Î´  > 0, Îµ = kÎµ2/2 + Îµ

2k ln(1/Î´ ), and every set T âŠ† Y, we have Pr[A(x) âˆˆ T ] â‰¤ eÎµ
Pr[A(x 
) âˆˆ
T ] + Î´ + kÎ´ 
.
Our improved group privacy bound immediately implies a strong bound on the max-information
of an LDP protocol. The max-information [9] of an algorithm is a measure of how much information it reveals about a randomly chosen input. The motivation for studying max-information comes
from its ability to ensure generalization in adaptive data analysis [3, 9, 10, 32].
Definition 4.4. Let Z,W be jointly distributed random variables. We say that the Î²-approximate
max-information between Z and W , denoted I
Î²
âˆž(Z;W ), is the minimal value k such that for every
measurable subset T of the support of (Z,W ) with Pr[(Z,W ) âˆˆ T ] > Î², we have
ln Pr[(Z,W ) âˆˆ T ] âˆ’ Î²
Pr[Z âŠ— W âˆˆ T ] â‰¤ k.
Here, Z âŠ— W denotes the product distribution formed from the marginals Z and W . Given an
algorithm A : Xn â†’ R, we say the Î²-approximate max-information of A, denoted I
Î²
âˆž(A,n) â‰¤ k
if I
Î²
âˆž(D; A(D)) â‰¤ k for every distribution D on Xn.
Dwork et al. [9] showed thatÎµ-DP algorithms have max-informationO(Îµn). Moreover, they have
Î²-approximate max-information O(Îµ2
n + Îµ

n log(1/Î²), but only when the input distribution D is
a product distribution. Subsequent work by Rogers et al. [33] extended this analysis to (Îµ, Î´ )-DP
algorithms, for which they showed the restriction to product distributions to be necessary.
Using our group privacy bound for local differential privacy, we show that even under nonproduct distributions, the max-information of Îµ-LDP protocols has the same behavior as Îµ-DP
algorithms on product distributions. This provides a distinct advantage in adaptive data analysis,
as it means Îµ-LDP algorithms can be composed with arbitrary low max-information algorithms
in any order while giving strong generalization guarantees. In contrast, in the central model it
is known that (Îµ, Î´ )-differentially private algorithms can come first in a composition with other
algorithms satisfying max-information bounds,5 but not necessarily second if the composition is
required to itself satisfy a nontrivial max-information bound [32].
5When the data is drawn from a product distribution.
ACM Transactions on Algorithms, Vol. 15, No. 4, Article 51. Publication date: October 2019. 
51:18 M. Bun et al.
Theorem 4.5. Let A : Xn â†’ Y be Îµ-LDP. Then for every Î² > 0, we have I
Î²
âˆž(A,n) â‰¤ nÎµ2/2 +
Îµ

2n ln(1/Î²).
We defer the proof to Appendix A.
5 COMPOSITION FOR RANDOMIZED RESPONSE
In this section, we give a direct proof showing that k instantiations of randomized response obey
privacy guarantees matching advanced composition (that is, with privacy paramter â‰ˆ âˆš
kÎµ), even
under pure Îµ-LDP. Prior work of Duchi et al. [8] showed that the problem of estimating k binary attributes admits an LDP algorithm with similar guarantees as ours. However, their distribution does
not resemble the k-fold composition of randomized response. We interpret our new construction
as evidence that an advanced composition theorem for pure Îµ-LDP might hold for more general,
and potentially even interactive mechanisms.
One conceptual barrier is that the basic composition theorem (giving a linear increase in the
number of computations) is actually tight under pure-differential privacy. That is, the composition of k randomized responses, each (Îµ, 0)-differentially private, do not satisfy (k
Îµ, 0)-differential
privacy for any k < k. To overcome this issue, we will consider a notion we call approximate composition. Specifically, we exhibit for every Î² > 0 an O(Îµ

k ln(1/Î²))-LDP algorithm that when run
on any given input, yields a distribution that is Î²-close in statistical distance to the composition
of k instances of randomized response.6
Let Mi : {0, 1}
k â†’ {0, 1} that performs randomized response on the ith bit of its input. On an
input x, our algorithm first identifies a â€œgoodâ€ set Gx of outputs (y1,...,yk ) âˆˆ {0, 1}
k that have
roughly average likelihood of appearing under the composition M(x) = (M1 (x),..., Mk (x)). By
concentration arguments, M(x) produces an outcome in Gx with probability at least 1 âˆ’ Î². Our
approximate composed algorithm MËœ (x) simply runs M(x), returns the outcome if it is in Gx , and
returns a uniformly random element outside Gx otherwise. The crux of the privacy argument is
to show that elements in this latter case end up being sampled with roughly the same probability
as elements in Gx .
Theorem 5.1. For each i = 1,..., k, let Mi : {0, 1}
k â†’ {0, 1} be the Îµ-differentially private
algorithm
Mi (x) =
âŽ§âŽª
âŽ¨
âŽª
âŽ©
xi w.p. e Îµ
e Îµ +1
1 âˆ’ xi w.p. 1
e Îµ +1 .
Let 0 < Î² < (Îµ
âˆš
k/2(k + 1))2/3 and suppose ÎµËœ = 6Îµ

k ln(1/Î²) â‰¤ 1. Then there exists an algorithm
MËœ : {0, 1}
k â†’ {0, 1}
k such that
(1) MËœ is ÎµËœ-differentially private.
(2) For every x âˆˆ {0, 1}
k , there exists an event E with Pr[E] â‰¥ 1 âˆ’ Î² such that, conditioned on E,
the output MËœ (x) is identically distributed to M(x) = (M1 (x),..., Mk (x)).
Proof. Let M(x) = (M1 (x),..., Mk (x)). For strings x,y âˆˆ {0, 1}
k , let dH (x,y) = |{i âˆˆ [k] : xi
yi}| denote the Hamming distance between x and y. For each x âˆˆ {0, 1}
k , define a â€œgoodâ€ spherical
shell around x by
Gx =

y âˆˆ {0, 1}
k : k
eÎµ + 1 âˆ’

k ln(2/Î²)/2 â‰¤ dH (x,y) â‰¤
k
eÎµ + 1
+

k ln(2/Î²)/2

.
6The usual guarantee of advanced composition also includes an additive O (Îµ 2k) term, but our result only applies to the
typical setting where Îµ 2k â‰¤ Îµ
âˆš
k.
ACM Transactions on Algorithms, Vol. 15, No. 4, Article 51. Publication date: October 2019. 
Heavy Hitters and the Structure of Local Privacy 51:19
It is immediate from Hoeffdingâ€™s inequality that Pr[M(x)  Gx ] â‰¤ Î² for all x âˆˆ {0, 1}
k .
Our â€œapproximateâ€ composed algorithm is as follows.
ALGORITHM 2: Approximate composed algorithm MËœ (x)
(1) Sample y = (y1,...,yk ) â† M(x)
(2) If y âˆˆ Gx , output y;
Else, output a random sample from {0, 1}
k \ Gx .
Since Pr[M(x) âˆˆ Gx ] â‰¥ 1 âˆ’ Î², the accuracy condition (2) in the statement of Theorem 5.1 is
immediate. We now show that Algorithm 2 guarantees ÎµËœ-differential privacy. The main technical
lemma that we need to prove this is the following, which says that each y  Gx is sampled with
probability close to the probability with which any item in Gx is sampled.
Lemma 5.2. For every x âˆˆ {0, 1}
k and y  Gx ,
Pr[MËœ (x) = y] âˆˆ

eâˆ’3Îµ
âˆšk ln(1/Î² )
, e3Îµ
âˆšk ln(1/Î² )

Â·
 1
eÎµ + 1
k/(e Îµ +1)  eÎµ
eÎµ + 1
	k/(e Îµ +1)
.
Proof. Let U be a uniform random variable on {0, 1}
k . If y  Gx , then
Pr[MËœ (x) = y] = Pr[M(x)  Gx ] Â· 1
|{0, 1}k \ Gx |
= 2âˆ’k Â·
Pr[M(x)  Gx ]
Pr[U  Gx ] .
To analyze this ratio, it suffices by symmetry to consider x = 0k , in which case the set of interest
is
B = {0, 1}
k \ G0k =

y âˆˆ {0, 1}
k : |y| <
k
eÎµ + 1 âˆ’

k ln(2/Î²)/2 or |y| >
k
eÎµ + 1
+

k ln(2/Î²)/2

.
We further divide up B and consider the set R defined by
R =

y âˆˆ {0, 1}
k : |y| <
k
eÎµ + 1 âˆ’ 2

k ln(1/Î²) or |y| >
k
eÎµ + 1
+ 2

k ln(1/Î²)

.
Our strategy will be to show that for y âˆˆ B \ R, the ratio Pr[M(0m ) = y]/ Pr[U = y] is roughly as
prescribed. Meanwhile, we will also have Pr[M(0m ) âˆˆ R]  Pr[U âˆˆ B] and Pr[U âˆˆ R]  Pr[U âˆˆ
B], so the contributions of points in B \ R will dominate the ratio of interest.
More formally, we make use of the following two claims.
Claim 5.3. For every y âˆˆ B \ R, we have
Pr[M(0m ) = y]
Pr[U = y] âˆˆ

eâˆ’2Îµ
âˆšk ln(1/Î² )
, e2Îµ
âˆšk ln(1/Î² )

Â· 2k Â·
 1
eÎµ + 1
k/(e Îµ +1)  eÎµ
eÎµ + 1
	ke Îµ /(e Îµ +1)
.
Claim 5.4. Both of the ratios
Pr[M(0m ) âˆˆ R]
Pr[U âˆˆ B] ,
Pr[U âˆˆ R]
Pr[U âˆˆ B] â‰¤ (k + 1)Î²3/2
.
ACM Transactions on Algorithms, Vol. 15, No. 4, Article 51. Publication date: October 2019.       
51:20 M. Bun et al.
These claims are technical and unenlightening, so we defer them to the end of the proof. For
now, we show how to put them together to complete the proof of Lemma 5.2. Let
C = 2k Â·
 1
eÎµ + 1
k/(e Îµ +1)  eÎµ
eÎµ + 1
	ke Îµ /(e Îµ +1)
â‰¥ 1.
Pr[M(0m ) âˆˆ B] = Pr[M(0m ) âˆˆ R] + Pr[M(0m ) âˆˆ B \ R]
â‰¤ (k + 1)Î²3/2 Pr[U âˆˆ B] + e2Îµ
âˆšk ln(2/Î² ) Â· C Â· Pr[U âˆˆ B \ R]
â‰¤ (e2Îµ
âˆšk ln(2/Î² ) Â· C + (k + 1)Î²3/2) Pr[U âˆˆ B]
â‰¤ e3Îµ
âˆšk ln(2/Î² ) Â· C Â· Pr[U âˆˆ B].
Here, the last inequality follows, because e2Îµ
âˆšk ln(2/Î² ) + (k + 1)Î²3/2 â‰¤ e3Îµ
âˆšk ln(2/Î² ) when (k +
1)Î²3/2 â‰¤ ÎµËœ/12 â‰¤ Îµ

k ln(2/Î²)e2Îµ
âˆšk ln(2/Î² )
. Similarly,
Pr[M(0m ) âˆˆ B] â‰¥ Pr[M(0m ) âˆˆ B \ R]
â‰¥ eâˆ’2Îµ
âˆšk ln(2/Î² ) Â· C Â· Pr[U âˆˆ B \ R]
â‰¥ eâˆ’2Îµ
âˆšk ln(2/Î² ) Â· C Â· (Pr[U âˆˆ B] âˆ’ (k + 1)Î²3/2 Pr[U âˆˆ B])
= eâˆ’2Îµ
âˆšk ln(2/Î² ) Â· C Â· (1 âˆ’ (k + 1)Î²3/2) Â· Pr[U âˆˆ B]
â‰¥ eâˆ’3Îµ
âˆšk ln(2/Î² ) Â· C Â· Pr[U âˆˆ B],
where the last inequality holds, because (k + 1)Î²3/2 â‰¤ ÎµËœ/12 â‰¤ 1 âˆ’ eâˆ’Îµ
âˆšk ln(2/Î² )
. Thus, we have
2âˆ’k Â·
Pr[M(0m ) âˆˆ B]
Pr[U âˆˆ B] âˆˆ

eâˆ’3Îµ
âˆšk ln(1/Î² )
, e3Îµ
âˆšk ln(1/Î² )

Â·
 1
eÎµ + 1
k/(e Îµ +1)  eÎµ
eÎµ + 1
	ke Îµ /(e Îµ +1)
.
This completes the proof of Lemma 5.2.
We now use Lemma 5.2 to argue that MËœ is ÎµËœ-differentially private. Fix two inputs x, x  âˆˆ {0, 1}
k .
We will show that for every y âˆˆ {0, 1}
k , we have Pr[MËœ (x) = y] â‰¤ eÎµËœ Pr[MËœ (x 
) = y]. There are four
cases to consider.
Case 1. y âˆˆ Gx and y âˆˆ Gx. Here
Pr[MËœ (x) = y]
Pr[MËœ (x ) = y]
=
 1
e Îµ +1
dH (x,y)  e Îµ
e Îµ +1
kâˆ’dH (x,y)
 1
e Îµ +1
dH (x
,y)  e Îµ
e Îµ +1
kâˆ’dH (x
,y)
=
 1
eÎµ + 1
dH (x,y)âˆ’dH (x
,y)  eÎµ
eÎµ + 1
	dH (x
,y)âˆ’dH (x,y)
â‰¤
 1
eÎµ + 1
âˆ’2
âˆšk ln(2/Î² )/2  eÎµ
eÎµ + 1
	2
âˆšk ln(2/Î² )/2
= eÎµ
âˆš2k ln(2/Î² ) â‰¤ eÎµËœ
.
ACM Transactions on Algorithms, Vol. 15, No. 4, Article 51. Publication date: October 2019.         
Heavy Hitters and the Structure of Local Privacy 51:21
Case 2. y âˆˆ Gx and y  Gx. Here
Pr[MËœ (x) = y]
Pr[MËœ (x ) = y]
â‰¤
 1
e Îµ +1
dH (x,y)  e Îµ
e Îµ +1
kâˆ’dH (x,y)
eâˆ’3Îµ
âˆšk ln(2/Î² ) Â·
 1
e Îµ +1
k/(e Îµ +1)  e Îµ
e Îµ +1
ke Îµ /(e Îµ +1)
= e3Îµ
âˆšk ln(2/Î² ) Â·
 1
eÎµ + 1
dH (x,y)âˆ’k/(e Îµ +1)  eÎµ
eÎµ + 1
	k/(e Îµ +1)âˆ’dH (x,y)
â‰¤ e3Îµ
âˆšk ln(2/Î² ) Â·
 1
eÎµ + 1
âˆ’
âˆšk ln(2/Î² )/2  eÎµ
eÎµ + 1
	âˆšk ln(2/Î² )/2
â‰¤ e (3+
âˆš
2)Îµ
âˆšk ln(2/Î² ) â‰¤ eÎµËœ
.
Case 3. y  Gx and y âˆˆ Gx. Here
Pr[MËœ (x) = y]
Pr[MËœ (x ) = y]
â‰¤ e3Îµ
âˆšk ln(2/Î² ) Â·
 1
e Îµ +1
k/(e Îµ +1)  e Îµ
e Îµ +1
ke Îµ /(e Îµ +1)
 1
e Îµ +1
dH (x
,y)  e Îµ
e Îµ +1
kâˆ’dH (x
,y)
= e3Îµ
âˆšk ln(2/Î² ) Â·
 1
eÎµ + 1
k/(e Îµ +1)âˆ’dH (x
,y)  eÎµ
eÎµ + 1
	dH (x
,y)âˆ’k/(e Îµ +1)
â‰¤ e3Îµ
âˆšk ln(2/Î² ) Â·
 1
eÎµ + 1
âˆ’
âˆšk ln(2/Î² )/2  eÎµ
eÎµ + 1
	âˆšk ln(2/Î² )/2
= e (3+
âˆš
2)Îµ
âˆšk ln(2/Î² ) â‰¤ eÎµËœ
.
Case 4. y  Gx and y  Gx. Here
Pr[MËœ (x) = y]
Pr[MËœ (x ) = y]
â‰¤ e3Îµ
âˆšk ln(2/Î² ) Â·
 1
e Îµ +1
k/(e Îµ +1)  e Îµ
e Îµ +1
ke Îµ /(e Îµ +1)
eâˆ’3Îµ
âˆšk ln(2/Î² ) Â·
 1
e Îµ +1
k/(e Îµ +1)  e Îµ
e Îµ +1
ke Îµ /(e Îµ +1)
= e6Îµ
âˆšk ln(2/Î² ) â‰¤ eÎµËœ
.
In all cases, we thus have Pr[MËœ (x) = y] â‰¤ eÎµËœ Pr[MËœ (x 
) = y]. Modulo the deferred claims, this
completes the proof of Theorem 5.1.
Proof of Claim 5.3. Let y âˆˆ B \ R. We first consider the case where
k
eÎµ + 1 âˆ’ 2

k ln(2/Î²) â‰¤ |y| â‰¤ k
eÎµ + 1 âˆ’

k ln(2/Î²)/2.
Here, we have
Pr[M(0k ) = y]
Pr[U = y] = 2k
 1
eÎµ + 1
|y |  eÎµ
eÎµ + 1
	kâˆ’ |y |
= exp 
Îµ Â·
 k
eÎµ + 1 âˆ’ |y|
	 	 Â· 2k Â·
 1
eÎµ + 1
k/(e Îµ +1)  eÎµ
eÎµ + 1
	ke Îµ /(e Îµ +1)
âˆˆ

1, e2Îµ
âˆšk ln(2/Î² )

Â· 2k Â·
 1
eÎµ + 1
k/(e Îµ +1)  eÎµ
eÎµ + 1
	ke Îµ /(e Îµ +1)
.
An identical argument shows that when
k
eÎµ + 1
+

k ln(2/Î²)/2 â‰¤ |y| â‰¤ k
eÎµ + 1
+ 2

k ln(2/Î²),
ACM Transactions on Algorithms, Vol. 15, No. 4, Article 51. Publication date: October 2019.                            
51:22 M. Bun et al.
we have the relationship
Pr[M(0k ) = y]
Pr[U = y] âˆˆ

eâˆ’2Îµ
âˆšk ln(2/Î² )
, 1

Â· 2k Â·
 1
eÎµ + 1
k/(e Îµ +1)  eÎµ
eÎµ + 1
	ke Îµ /(e Îµ +1)
.
Before proving Claim 5.4, we state and prove the following anti-concentration result for uniform
strings.
Lemma 5.5. Let U be uniformly distributed on {0, 1}
k . Then for every t âˆˆ [0,
âˆš
k/2],
Pr 
|U | â‰¥ k
2
+ t
âˆš
k

â‰¥ exp(âˆ’3t 2)
k + 1 .
Proof. Using the probability mass function of the binomial distribution, we calculate
Pr 
|U | â‰¥ k
2
+ t
âˆš
k

= 2âˆ’k
k/2âˆ’ t
âˆš

k 
j=0

k
j
	
â‰¥ 2âˆ’k
 k
k/2 âˆ’ t
âˆš
k
	
.
Using the consequence of Stirlingâ€™s approximation that (
k
j ) â‰¥ 2k Â·H (j/k)
/(k + 1), where H is the
binary entropy function, this is at least
2âˆ’k Â·
2k Â·H (1/2âˆ’t /
âˆš
k)
k + 1 .
Now we use the fact that H(1/2 âˆ’ Î·) â‰¥ 1 âˆ’ 4Î·2 to lower bound this by
2âˆ’4t 2
k + 1
.
Proof of Claim 5.4. By the lower bound of Lemma 5.5 on the tail bound of the uniform
distribution,
Pr[U âˆˆ B] â‰¥ Pr 
|U | >
k
eÎµ + 1
+

k ln(2/Î²)/2

= Pr
âŽ¡
âŽ¢
âŽ¢
âŽ¢
âŽ¢
âŽ£
|U | >
k
2
+


ln(2/Î²)/2 âˆ’ (eÎµ âˆ’ 1)
âˆš
k
2(eÎµ + 1)


âˆš
k
âŽ¤
âŽ¥
âŽ¥
âŽ¥
âŽ¥
âŽ¦
â‰¥
1
k + 1 exp



âˆ’3 Â·


ln(2/Î²)/2 âˆ’ (eÎµ âˆ’ 1)
âˆš
k
2(eÎµ + 1)


2



.
However, by Hoeffdingâ€™s inequality,
Pr[M(0m ) âˆˆ R] â‰¤ 2 Â· exp 
âˆ’2 Â· (2

ln(2/Î²))2

and
Pr[U âˆˆ R] â‰¤ 2 Â· exp



âˆ’2 Â·

2

ln(2/Î²) âˆ’ (eÎµ âˆ’ 1)
âˆš
k
2(eÎµ + 1)


2



.
This allows us to conclude that both of the ratios Pr[M(0m ) âˆˆ R]/ Pr[U âˆˆ B] and Pr[U âˆˆ R]/ Pr[U âˆˆ
B] are at most
2(k + 1) Â· exp



âˆ’13
2
ln(2/Î²) + (8 âˆ’ 3
âˆš
2)

ln(2/Î²) Â·
(eÎµ âˆ’ 1)
âˆš
k
2(eÎµ + 1) +

(eÎµ âˆ’ 1)
âˆš
k
2(eÎµ + 1)


2



.
ACM Transactions on Algorithms, Vol. 15, No. 4, Article 51. Publication date: October 2019.            
Heavy Hitters and the Structure of Local Privacy 51:23
Using the fact that (eÎµ âˆ’ 1)
âˆš
k/2(eÎµ + 1) â‰¤ 
ln(2/Î²), this is at most
2(k + 1) Â· exp 
âˆ’3
2
ln(2/Î²)

â‰¤ (k + 1) Â· Î²3/2
.
6 FROM APPROXIMATE-PRIVACY TO PURE-PRIVACY: A GENERIC
TRANSFORMATION WITH SHORT REPORTS
In this section, we present our generic transformation from any (non-interactive) (Îµ, Î´ )-LDP protocol into an O(Îµ)-LDP protocol with essentially the same utility guarantees. Our transformation
is based on rejection sampling, an idea that first appeared in the context of differential privacy
in work of Kasiviswanathan et al. [22], where it was used to show an equivalence between the
local model (under pure-privacy) and SQ learning. Rejection sampling was also used by Bassily
and Smith [5] to show that almost any (non-interactive) Îµ-LDP protocol can be transformed into a
protocol where the communication per user is only 1 bit (at the expense of increasing the shared
randomness in the protocol). Our transformation is obtained from the transformation of Bassily
and Smith with the following two modifications:
(1) The transformation of Bassily and Smith applies to any â€œsample resilientâ€Îµ-LDP protocol.
Informally, a sample resilient protocol is one whose outcome on any (distributed) database
S is well-approximated by its outcome on a random subset of (roughly) half of the users in
S. While we do not know of any protocols that are not sample resilient, we would like to
obtain a more general result that avoids this restriction. We show that this restriction can
be removed at the expense of increasing per-user communication to O(log logn), where
n is the number of users.
(2) We generalize the transformation so that it also holds for any (Îµ, Î´ )-LDP protocol. Somewhat surprisingly, instead of obtaining an (Îµ, Î´ )-LDP protocol with short user reports, we
show that, when done carefully, the resulting protocol actually satisfies pureO(Îµ)-privacy.
Our generic transformation is given in algorithm GenProt.
Theorem 6.1. Let Îµ â‰¤ 1/4, and let
5 ln(1/Îµ) â‰¤ T â‰¤
1 âˆ’ eâˆ’Îµ
4Î´eÎµn .
Then Algorithm GenProt satisfies 10Îµ-LDP. Moreover, for every database S = (x1,..., xn ), the total
variation distance between GenProt(S) and M(S) is at most
sup
F
| Pr[GenProt(S) âˆˆ F ] âˆ’ Pr[M(S) âˆˆ F ]| â‰¤ n
 1
2
+ Îµ
T
+
6TÎ´eÎµ
1 âˆ’ eâˆ’Îµ
	
.
To interpret the guarantee of this theorem, let Î² > 0 be a parameter, and suppose Î´ â‰¤
ÎµÎ²/48n ln(2n/Î²). Then setting T = 2 ln(2n/Î²) makes this total variation distance at most Î². With
this setting of parameters, observe that each user must send O(log logn) bits to the server.
We prove the privacy and utility guarantees of Theorem 6.1 individually.
Lemma 6.2. Let Îµ â‰¤ 1/4 and let T â‰¥ 5 ln(1/Îµ). Then Algorithm GenProt satisfies 10Îµ-LDP.
Proof. Fix the public randomness yi,t for (i,t)âˆˆ[n]Ã—[T ]. Fix a user i âˆˆ [n], and let Qi denote
the output of useri in the protocol (from step 2f) after fixing the public randomness. Let xi, x 
i âˆˆ X
be two possible inputs, and let Ð´ âˆˆ [T ] be a possible output of Qi . Our goal is to show that
Pr[Qi (xi ) = Ð´] â‰¤ e10Îµ Â· Pr[Qi (x 
i ) = Ð´].
ACM Transactions on Algorithms, Vol. 15, No. 4, Article 51. Publication date: October 2019. 
51:24 M. Bun et al.
ALGORITHM 3: GenProt
Inputs: Userâ€™s inputs {xi âˆˆ X : i âˆˆ [n]}, privacy parameter Îµ, and parameter T âˆˆ N.
Algorithms used: An (Îµ, Î´ )-LDP protocol M with local randomizers Ai , i âˆˆ [n].
1. For every (i,t) âˆˆ [n]Ã—[T ] generate independent public string: yi,t â† Ai (âŠ¥).
2. For user i = 1 to n do
(a) For every t âˆˆ [T ] compute pi,t = 1
2
Pr[Ai (xi )=yi, t ]
Pr[Ai (âŠ¥)=yi, t ] .
(b) For every t âˆˆ [T ], if pi,t

eâˆ’2Îµ
2 , e 2Îµ
2

then set pi,t = 1
2 .
(c) For every t âˆˆ [T ] sample a bit bi,t from Bernoulli(pi,t ).
(d) Denote Hi = {t âˆˆ [T ] : bi,t = 1}.
(e) If Hi = âˆ…, then set Hi = [T ].
(f) Sample Ð´i âˆˆ Hi uniformly, and send Ð´i to the server.
3. Run M(y1,Ð´1 ,...,yn,Ð´n ).
We calculate:
Pr[Qi (xi ) = Ð´] = Pr[bi,Ð´ = 1] Â· Pr[Qi (xi ) = Ð´|bi,Ð´ = 1] + Pr[bi,Ð´ = 0] Â· Pr[Qi (xi ) = Ð´|bi,Ð´ = 0].
(5)
We now analyze each of these two terms. First observe that if bi,Ð´ = 0, then for Ð´ to be a possible
output, then it must be that Hi = âˆ… before step 2e and that the output is chosen uniformly from
Hi = [T ]. So,
Pr[Qi (xi ) = Ð´|bi,Ð´ = 0] = 1
T Â· Pr[âˆ€t  Ð´ we have bi,t = 0] â‰¤
1
T

e2Îµ
2
	T âˆ’1
.
Next note that if bi,Ð´ = 1, then we have that Hi  âˆ… before step 2e and that the output is chosen
uniformly from Hi . So,
Pr[Qi (xi ) = Ð´|bi,Ð´ = 1] =

T
k=1
Pr[|Hi | = k|bi,Ð´ = 1] Â·
1
k =

T
k=1
Pr[|Hi \ {Ð´}| = k âˆ’ 1] Â·
1
k
=
T
âˆ’1
k=0
Pr[|Hi \ {Ð´}| = k] Â· 1
k + 1 = E
 1
W + 1

, (6)
where W  |Hi \ {Ð´}| = 
tq bi,t is a sum of independent Bernoulli random variables, with different expectations pi,t âˆˆ [ eâˆ’2Îµ
2 , e 2Îµ
2 ] (this is called the Poisson binomial distribution). We now want
to relate the random variable W to the binomial distribution, specifically, to a random variable
WË† defined as the sum of (T âˆ’ 1) i.i.d. Bernoullis with expectations pË† = eâˆ’2Îµ
2 . This is useful as the
binomial distribution is much easier to analyze than the Poisson binomial distribution.
Our current task is to show that E[1/(W + 1)] â‰¤ E[1/(WË† + 1)]. Intuitively, observe thatW is the
sum ofT âˆ’ 1 Bernoullis:W = 
tq bi,t , and recall that we definedWË† by decreasing the expectations
of each of these Bernoulli random variables. Hence, intuitively we should get that E[1/(W + 1)] â‰¤
E[1/(WË† + 1)]. We now make this argument formal.
Claim 6.3. Let X âˆ¼ Bernoulli(p) and XË† âˆ¼ Bernoulli(pË†), where p â‰¥ pË†. Let Y â‰¥ 1 be independent of
X,XË†. Then E[ 1
X+Y ] â‰¤ E[ 1
XË† +Y ].
ACM Transactions on Algorithms, Vol. 15, No. 4, Article 51. Publication date: October 2019.      
Heavy Hitters and the Structure of Local Privacy 51:25
Proof.
E
 1
X + Y

=
 âˆž
0
Pr  1
X + Y â‰¥ z

dz =
 âˆž
0

y
Pr[Y = y] Pr  1
X + y
â‰¥ z

dz
â‰¤
 âˆž
0

y
Pr[Y = y] Pr  1
XË† + y
â‰¥ z

dz = E
 1
XË† + Y

.
Using Claim 6.3 we can replace, one by one, every Bernoulli(p) variable in the sum of W with a
Bernoulli(pË†) variable, without decreasing the expectation E[1/(W + 1)]. So,
(6) â‰¤ E[1/(WË† + 1)] =
T
âˆ’1
k=0
1
k + 1

T âˆ’ 1
k

Â· pË†
k Â· (1 âˆ’ pË†)
T âˆ’1âˆ’k
=
T
âˆ’1
k=0
1
k + 1
(T âˆ’ 1)!
k!(T âˆ’ 1 âˆ’ k)!
Â· pË†
k Â· (1 âˆ’ pË†)
T âˆ’1âˆ’k
=
T
âˆ’1
k=0
1
T
T !
(k + 1)!(T âˆ’ 1 âˆ’ k)!
Â· pË†
k Â· (1 âˆ’ pË†)
T âˆ’1âˆ’k
= 1
T
T
âˆ’1
k=0
 T
k + 1

Â· pË†
k Â· (1 âˆ’ pË†)
T âˆ’1âˆ’k
= 1
T

T
=1

T


Â· pË†âˆ’1 Â· (1 âˆ’ pË†)
T âˆ’
= 1
TpË†

T
=1

T


Â· pË† Â· (1 âˆ’ pË†)
T âˆ’
= 1
TpË†
âŽ¡
âŽ¢
âŽ¢
âŽ¢
âŽ¢
âŽ£
âˆ’ (1 âˆ’ pË†)
T +

T
=0

T


Â· pË† Â· (1 âˆ’ pË†)
T âˆ’
âŽ¤
âŽ¥
âŽ¥
âŽ¥
âŽ¥
âŽ¦
= 1
TpË†

âˆ’ (1 âˆ’ pË†)
T + 1

= 2
T eâˆ’2Îµ
âŽ¡
âŽ¢
âŽ¢
âŽ¢
âŽ¢
âŽ£
1 âˆ’

1 âˆ’ eâˆ’2Îµ
2
	T âŽ¤
âŽ¥
âŽ¥
âŽ¥
âŽ¥
âŽ¦
.
Going back to Equation (5) we have that
Pr[Qi (xi ) = Ð´] = Pr[bi,Ð´ = 1] Â· Pr[Qi (xi ) = Ð´|bi,Ð´ = 1] + Pr[bi,Ð´ = 0] Â· Pr[Qi (xi ) = Ð´|bi,Ð´ = 0]
â‰¤ e2Îµ
2 Â· 2
T eâˆ’2Îµ
âŽ¡
âŽ¢
âŽ¢
âŽ¢
âŽ¢
âŽ£
1 âˆ’

1 âˆ’ eâˆ’2Îµ
2
	T âŽ¤
âŽ¥
âŽ¥
âŽ¥
âŽ¥
âŽ¦
+ e2Îµ
2 Â·
1
T

e2Îµ
2
	T âˆ’1
= 1
T
âŽ¡
âŽ¢
âŽ¢
âŽ¢
âŽ¢
âŽ£
e4Îµ âˆ’ e4Îµ

1 âˆ’ eâˆ’2Îµ
2
	T
+

e2Îµ
2
	T âŽ¤
âŽ¥
âŽ¥
âŽ¥
âŽ¥
âŽ¦
â‰¤
1
T
âŽ¡
âŽ¢
âŽ¢
âŽ¢
âŽ¢
âŽ£
e4Îµ +

e2Îµ
2
	T âŽ¤
âŽ¥
âŽ¥
âŽ¥
âŽ¥
âŽ¦
.
An identical analysis shows that
Pr[Qi (x 
i ) = Ð´] â‰¥
1
T
âŽ¡
âŽ¢
âŽ¢
âŽ¢
âŽ¢
âŽ£
eâˆ’4Îµ âˆ’ eâˆ’4Îµ

1 âˆ’ e2Îµ
2
	T âŽ¤
âŽ¥
âŽ¥
âŽ¥
âŽ¥
âŽ¦
.
So,
Pr[Qi (xi ) = Ð´]
Pr[Qi (x 
i ) = Ð´] â‰¤ e4Îµ +
e 2Îµ
2
T
eâˆ’4Îµ âˆ’ eâˆ’4Îµ
1 âˆ’ e 2Îµ
2
T ,
which is at most e10Îµ for Îµ â‰¤ 1/4 and T â‰¥ 5 ln(1/Îµ).
ACM Transactions on Algorithms, Vol. 15, No. 4, Article 51. Publication date: October 2019.                  
51:26 M. Bun et al.
Lemma 6.4. Let T â‰¤ 1âˆ’eâˆ’Îµ
4Î´ e Îµ n . Then any event that occurs with probability p under the original protocol happens with probability at most
p + n
 1
2
+ Îµ
T
+
6TÎ´eÎµ
1 âˆ’ eâˆ’Îµ
	
in the execution of GenProt.
Proof. Fix user inputs x1,..., xn. For (i,t) âˆˆ [n]Ã—[T ] we denote by Yi,t the random variable
taking value yi,t in line 1 of GenProt. We also denote Yi = Yi,Gi , where Gi is a random variable
taking value Ð´i in line 2f of GenProt. With these notations, our goal is to relate the following two
random variables:
M(A1 (x1),..., An (xn )) and M(Y1,...,Yn ).
To that end, we now define the following events:
Event E: For every i âˆˆ [n] we have that Hi  âˆ… before step 2e.
Event Ei : We have that Hi  âˆ… before step 2e.
We show that E happens with high probability. Fix i âˆˆ [n]. For Îµ â‰¤ 1
4 and Î² = n(1/2 + Îµ)
T , we
have
Pr[Hi = âˆ…] â‰¤

1 âˆ’ eâˆ’2Îµ
2
	T
â‰¤
 1
2
+ Îµ
T
â‰¤ Î²
n .
Hence, using the union bound, we get that Pr[E] â‰¥ 1 âˆ’ Î².
For every i âˆˆ [n] define the set of all â€œgoodâ€ random strings Goodi = {y : Pr[Ai (xi )=y]
Pr[Ai (âŠ¥)=y] âˆˆ eÂ±2Îµ }.
Event Priv: For every i âˆˆ [n] and for every t âˆˆ [T ] we have that Yi,t âˆˆ Goodi .
Event Privi : For every t âˆˆ [T ] we have that Yi,t âˆˆ Goodi .
Event Privi,t : We have that Yi,t âˆˆ Goodi .
To analyze Event Priv we recall the following useful consequence of the definition of differential
privacy:
Observation 6.5 [23]. Let M:Xnâ†’Y be (Îµ, Î´ )-differentially private, and fix neighboring databases
S, S
âˆˆXn. Then,
Pr 
M(S) = y s.t. Pr[M(S) = y]
Pr[M(S) = y]  eÂ±2Îµ

â‰¤
2Î´eÎµ
1 âˆ’ eâˆ’Îµ = O(Î´/Îµ).
By Observation 6.5 and a union bound for every (i,t) âˆˆ [n]Ã—[T ] we have that Pr[Priv] â‰¥ 1 âˆ’ 2nT Î´ e Îµ
1âˆ’eâˆ’Îµ .
ACM Transactions on Algorithms, Vol. 15, No. 4, Article 51. Publication date: October 2019.   
Heavy Hitters and the Structure of Local Privacy 51:27
For (i,t) âˆˆ [n]Ã—[T ] let Bi,t be a random variable taking value bi,t in line 2c of GenProt. Fix
i âˆˆ [n] and fix y âˆˆ Goodi . We calculate:
Pr[Yi = y|E, Priv] = Pr[Yi = y|Ei, Privi]
=

t âˆˆ[T ]
Pr[Gi = t|Ei, Privi] Â· Pr[Yi = y|Gi = t, Ei, Privi]
=

t âˆˆ[T ]
Pr[Gi = t|Ei, Privi] Â· Pr[Yi,t = y|Gi = t, Bi,t = 1, Privi]
=

t âˆˆ[T ]
Pr[Gi = t|Ei, Privi] Â·
Pr[Yi,t = y,Gi = t, Bi,t = 1, Privi]
Pr[Gi = t, Bi,t = 1, Privi]
=

t âˆˆ[T ]
Pr[Gi = t|Ei, Privi] Â·
Pr[Gi = t|Yi,t = y, Bi,t = 1, Privi]
Pr[Gi = t|Bi,t = 1, Privi] Â· Pr[Yi,t = y|Bi,t = 1, Privi].
(7)
Now recall thatGi is a (random) function of Bi,1,..., Bi,T , and observe that given Bi,t = 1, we have
that Gi is independent of Yi,t . Hence,
(7) =

t âˆˆ[T ]
Pr[Gi = t|Ei, Privi] Â· Pr[Yi,t = y|Bi,t = 1, Privi]
=

t âˆˆ[T ]
Pr[Gi = t|Ei, Privi] Â· Pr[Yi,1 = y|Bi,1 = 1, Privi,1]
= Pr[Yi,1 = y|Bi,1 = 1, Privi,1] Â·

t âˆˆ[T ]
Pr[Gi = t|Ei, Privi]
= Pr[Yi,1 = y|Bi,1 = 1, Privi,1]
[0.4em] = Pr[Yi,1 = y, Bi,1 = 1, Privi,1]
Pr[Bi,1 = 1, Privi,1] . (8)
Recall that we fixed y s.t. y âˆˆ Goodi . Hence,
(8) = Pr[Yi,1 = y, Bi,1 = 1]
Pr[Bi,1 = 1, Privi,1]
= Pr[Yi,1 = y] Â· Pr[Bi,1 = 1|Yi,1 = y]
Pr[Bi,1 = 1, Privi,1]
= Pr[Yi,1 = y] Â· 1
2
Pr[Ai (xi )=y]
Pr[Yi,1=y]
Pr[Bi,1 = 1, Privi,1]
=
1
2 Pr[Ai (xi ) = y]
Pr[Bi,1 = 1, Privi,1]
. (9)
ACM Transactions on Algorithms, Vol. 15, No. 4, Article 51. Publication date: October 2019.
51:28 M. Bun et al.
We now analyze Pr[Bi,1 = 1, Privi,1]:
Pr[Bi,1 = 1, Privi,1] =

y âˆˆGoodi
Pr[Yi,1 = y] Â· Pr[Bi,1 = 1|Yi,1 = y]
=

y âˆˆGoodi
Pr[Yi,1 = y] Â·
1
2
Pr[Ai (xi ) = y]
Pr[Yi,1 = y]
= 1
2

y âˆˆGoodi
Pr[Ai (xi ) = y] = 1
2 Pr[Ai (xi ) âˆˆ Goodi].
Hence, as y âˆˆ Goodi , we have
(9) = Pr[Ai (xi ) = y]
Pr[Ai (xi ) âˆˆ Goodi] = Pr[Ai (xi ) = y | Ai (xi ) âˆˆ Goodi].
That is, we have established that for every i âˆˆ [n] and for every y âˆˆ Goodi the following holds:
Pr[Yi = y|E, Priv] = Pr[Ai (xi ) = y | Ai (xi ) âˆˆ Goodi].
Let us denote Good = Good1 Ã— Good2 Ã—Â·Â·Â·Ã— Goodn. For simplicity, we now assume that the postprocessing procedure M is deterministic. For a subset F of possible outputs of M we denote
FY = {(y1,...,yn ) : M(y1,...,yn ) âˆˆ F }.
With this notation we have that
Pr[M(Y1,...,Yn ) âˆˆ F ] â‰¤ Pr[E] + Pr[Priv] + Pr[M(Y1,...,Yn ) âˆˆ F |E, Priv]
â‰¤ Î² +
2nTÎ´eÎµ
1 âˆ’ eâˆ’Îµ + Pr[M(Y1,...,Yn ) âˆˆ F |E, Priv]
= Î² +
2nTÎ´eÎµ
1 âˆ’ eâˆ’Îµ +

(y1,...,yn )âˆˆFY
Pr[Y1 = y1,...,Yn = yn |E, Priv]
= Î² +
2nTÎ´eÎµ
1 âˆ’ eâˆ’Îµ +

(y1,...,yn )âˆˆFY
n
i=1
Pr[Yi = yi |E, Priv]
= Î² +
2nTÎ´eÎµ
1 âˆ’ eâˆ’Îµ +

(y1,...,yn )âˆˆFY âˆ©Good
n
i=1
Pr[Yi = yi |E, Priv]
= Î² +
2nTÎ´eÎµ
1 âˆ’ eâˆ’Îµ +

(y1,...,yn )âˆˆFY âˆ©Good
n
i=1
Pr[Ai (xi ) = yi | Ai (xi ) âˆˆ Goodi]
= Î² +
2nTÎ´eÎµ
1 âˆ’ eâˆ’Îµ +

(y1,...,yn )âˆˆFY
n
i=1
Pr[Ai (xi ) = yi | Ai (xi ) âˆˆ Goodi].
(10)
Let us use the shorthand {A(x) âˆˆ Good} to denote {A1 (x1) âˆˆ Good1,..., An (xn ) âˆˆ Goodn }. So,
(10) = Î² +
2nTÎ´eÎµ
1 âˆ’ eâˆ’Îµ +

(y1,...,yn )âˆˆFY
Pr[A1 (x1) = y1,..., An (xn ) = yn |A(x) âˆˆ Good]
= Î² +
2nTÎ´eÎµ
1 âˆ’ eâˆ’Îµ + Pr[M(A1 (x1),..., An (xn )) âˆˆ F |A(x) âˆˆ Good]
â‰¤ Î² +
2nTÎ´eÎµ
1 âˆ’ eâˆ’Îµ +
1
Pr[A(x) âˆˆ Good]
Â· Pr[M(A1 (x1),..., An (xn )) âˆˆ F ]. (11)
ACM Transactions on Algorithms, Vol. 15, No. 4, Article 51. Publication date: October 2019.        
Heavy Hitters and the Structure of Local Privacy 51:29
Similarly to the analysis of Event Priv, by the privacy properties of the Ai â€™s we have that
Pr[A(x) âˆˆ Good] â‰¥ 1 âˆ’ 2nT Î´ e Îµ
1âˆ’eâˆ’Îµ . Using the fact that 1
1âˆ’Î¶ â‰¤ 1 + 2Î¶ for every 0 â‰¤ Î¶ â‰¤ 1
2 , and ensuring
that 2nT Î´ e Îµ
1âˆ’eâˆ’Îµ â‰¤ 1
2 , we get
(11) â‰¤ Î² +
2nTÎ´eÎµ
1 âˆ’ eâˆ’Îµ +

1 +
4nTÎ´eÎµ
1 âˆ’ eâˆ’Îµ
	
Â· Pr[M(A1 (x1),..., An (xn )) âˆˆ F ]
â‰¤ Î² +
6nTÎ´eÎµ
1 âˆ’ eâˆ’Îµ + Pr[M(A1 (x1),..., An (xn )) âˆˆ F ].
This shows that GenProt increases the probability of any event by at most Î² + 6nT Î´ e Îµ
1âˆ’eâˆ’Îµ .
7 A LOWER BOUND VIA ANTI-CONCENTRATION
In Section 3, we presented a LDP heavy-hitters algorithm with error O( 1
Îµ

n Â· log( |X |
Î² )). Ignoring
the dependence on Î², this error is known to be optimal. Specifically, Bassily and Smith [5] proved
the following lower bound:
Theorem 7.1 [5]. Let Îµ = O(1) and Î´ = o( 1
n log n ). Every non-interactive (Îµ, Î´ )-LDP protocol for
estimating the frequencies of elements from a domain X must have worst-case error Î©( 1
Îµ

n Â· log |X |)
with constant probability.
In this section, we incorporate a tight dependence on the failure probability Î² into the lower
bound. Specifically, we show
Theorem 7.2. Let Îµ = O(1) and Î´ = o( 1
n log n ). Every non-interactive (Îµ, Î´ )-LDP protocol for estimating the frequencies of elements from a domain X achieving worst-case error Î” with probability at
least 1 âˆ’ Î² must have
Î” â‰¥ Î©


1
Îµ

n Â· log 
|X |
Î²
	



.
As we explain below, in light of Theorem 7.1 it suffices to show the following simplified lower
bound, in which we fix X = {0, 1}:
Lemma 7.3. LetÎµ = O(1) and Î´ = o( 1
n log n ), and let A be an (Îµ, Î´ )-LDP protocol for n users, that for
every (distributed) database D âˆˆ {0, 1}
n counts the number of 1â€™s in D to within error Î” with success
probability 1 âˆ’ Î². Then,
Î” â‰¥ Î©


1
Îµ

n Â· log 
1
Î²
	



.
We first show that, together with Theorem 7.1, this lemma implies Theorem 7.2.
Proof of Theorem 7.2. Let A be a non-interactive (Îµ, Î´ )-LDP protocol for estimating the frequencies of elements from a domainX, achieving worst-case error Î” with probability at least 1 âˆ’ Î².
In particular, for a (distributed) database D âˆˆ {0, 1}
n, we can use A to count the number of 1â€™s in D
to within error Î” with success probability 1 âˆ’ Î². To see this, let xâˆ—  xË†âˆ— be two different elements
in X, and consider the following protocol for counting bits. Each user i holds an input bit bi , and
generates an alternative input xi where xi = xâˆ— if bi = 0 and xi = xË†âˆ— if bi = 1. This defines a modified (distributed) database S = (x1,..., xn ). We can now apply A to S and obtain estimations for
the multiplicities of xâˆ— and xË†âˆ— in S, i.e., for the multiplicities of 0 and 1 in D. Hence, by Lemma 7.3,
ACM Transactions on Algorithms, Vol. 15, No. 4, Article 51. Publication date: October 2019.        
51:30 M. Bun et al.
for some global constant c1 > 0 we have that
Î” â‰¥ c1
Îµ

n Â· log 1
Î² . (12)
In addition, by Theorem 7.1, for some global constant c2 > 0 we have that
Î” â‰¥ c2
Îµ

n Â· log |X |. (13)
Combining Inequalities (12) and (13) we get that
Î” â‰¥ c1
2Îµ

n Â· log 1
Î² + c2
2Îµ

n Â· log |X |
â‰¥
min{c1,c2}
4Îµ

n Â· log |X |
Î² ,
where the last inequality is because if |X | â‰¥ 1
Î² , then log |X | â‰¥ 1
2 log |X |
Î² , and if 1
Î² â‰¥ |X |, then
log 1
Î² â‰¥ 1
2 log |X |
Î² .
It remains to prove Lemma 7.3. To that end, we now strengthen an argument of Chan et al. [7]
(see also References [19, 39]) who obtained a lower bound of Î©(âˆš
n) on the error:
Theorem 7.4 [7, 19, 39]. Let Îµ â‰¤ 1/2 and Î´ < 1. Every (Îµ, Î´ )-LDP frequency oracle must have
worst-case error Î©(âˆš
n) with constant probability.
Before presenting the formal proof of Lemma 7.3, we establish some notation and present a
high-level overview of the proof. In this overview, we argue how the result follows from the ideas
presented in the previous sections; namely, from advanced grouposition and from our generic
transformation from approximate to pure LDP. However, we will present the complete proof in a
somewhat more streamlined way that does not go through these generic results.
By our transformation from approximate to pure LDP, we may assume without loss of generality
that we have a pure Îµ-LDP protocol A forn users, which for every (distributed) databaseD âˆˆ {0, 1}
n
counts the number of 1â€™s in D to within error Î” with success probability 1 âˆ’ Î². Our goal is to show
that Î” = Î©( 1
Îµ

n Â· log( 1
Î² )).
We generate an input to A as follows: For some constant C > 0, denote m = CÎµ2
n, and let S =
(X1,...,Xm ) âˆˆ {0, 1}
m be a database chosen uniformly at random. Now define D = (Y1,...,Yn ) âˆˆ
{0, 1}
n, where Yi = Xim/n. That is, the first n
m elements in D are X1, the next n
m elements are X2,
and so on. Now consider applying the protocol A onto the database D. The remainder of the proof
proceeds as follows:
(i) On one hand, the error of A on D is at most Î”, and hence, renormalizing to S, we obtain
an estimate of the number of 1â€™s in S with error at most m
n Î” = CÎµ2Î”.
(ii) However, every bit in S has n
m = 1
CÎµ 2 copies in D. By advanced grouposition, we provide
every element in S with say ( 1
100 , 1
100 )-DP. Using bounds on the mutual information of a
random input and the output of a DP algorithm [5], we get that (a lot of) the elements in
S remain approximately uniform even conditioned on the transcript, and in particular the
output, of the protocol (and they also remain independent).
ACM Transactions on Algorithms, Vol. 15, No. 4, Article 51. Publication date: October 2019. 
Heavy Hitters and the Structure of Local Privacy 51:31
Combining (i) and (ii), and applying an anti-concentration bound to the sum of elements in S, we
will get that
CÎµ2
Î”

|S | Â· log 
1
Î²
	
=

CÎµ2n Â· log 
1
Î²
	
,
which gives the result. We now present a full proof Lemma 7.3 that is based on these ideas, but
does not need to go through our generic transformations.
Proof of Lemma 7.3. Assume that we have an (Îµ, Î´ )-LDP protocol A for n users, that for every
(distributed) database D âˆˆ {0, 1}
n counts the number of 1â€™s in D to within error Î” with success
probability 1 âˆ’ Î². We generate an input to A as follows: For some constant C > 0, denote m =
CÎµ2
n, and let S = (X1,...,Xm ) âˆˆ {0, 1}
m be a database chosen uniformly at random. Now define
D = (Y1,...,Yn ) âˆˆ {0, 1}
n, where Yi = Xim/n. That is, the first n
m elements in D are X1, the next n
m elements are X2, and so on.
Consider the execution of protocol A on the database D = (Y1,...,Yn ), and let R denote public
randomness generated by the server. For i âˆˆ [n], let Ai (R, Â·) denote the (Îµ, Î´ )-differentially private algorithm that acts on the bit Yi , and let Ai be a random variable denoting the outcome of
Ai (R,Yi ). With this notation, we have that the outcome of the protocol is a deterministic function
of T = (R,A1,...,An ), which we call the transcript of the protocol. For each fixed choice of T = t
we denote the outcome of the protocol as EstD (t). We also denote by EstS (t) = m
n EstD (t) the renormalized estimate of the number of 1â€™s in S. Observe that for every fixing of X1=x1,...,Xm=xm and
Y1=y1,...,Yn=yn we have






EstD (t) âˆ’
n
i=1
yi






= m
n Â·






EstS (t) âˆ’
m
i=1
xi






. (14)
By construction, every bit Xj âˆˆ S is used by the n
m mechanisms
A(jâˆ’1) n
m +1, A(jâˆ’1) n
m +2,..., Aj n
m .
We abbreviate
Bj (r,Xj) = (A(jâˆ’1) n
m +1 (r,Xj),..., Aj n
m (r,Xj)).
That is, Bj (r,Xj) is the vector of outcomes of the n
m mechanisms that operate on Xj when the
serverâ€™s randomness is R = r. Let C2 > 1 be a constant, and recall that m = CÎµ2
n. By choosing
C = C(C2) to be a large-enough constant we have that Bj (r, Â·) satisfies ( 1
C2
, 1
C2
)-differential privacy
for every choice of r (see advanced composition in Reference [13]). We now use the following
theorem to argue that, for every r, the mutual information between Xj and Bj (r,Xj) is low:
Theorem 7.5 [5]. Let V be uniformly distributed over {0, 1}
d . Let Q : {0, 1}
d â†’ Z be an (Îµ, Î´ )-
differentially private algorithm, and let Z denote Q(V ). Then, we have that
I (V ;Z) = O

Îµ2 +
Î´d
Îµ
+
Î´
Îµ
log(Îµ/Î´ )
	
.
Recall that Xj is uniform in {0, 1}, and that Bj (r, Â·) satisfies ( 1
C2
, 1
C2
)-differential privacy. Hence,
by choosingC2 to be a large-enough constant, for every j âˆˆ [m] we have that I (Xj ; Bj (r,Xj)) â‰¤ 1
10 ,
so
H(Xj |Bj (r,Xj)) = H(Xj) âˆ’ I (Xj ; Bj (r,Xj)) â‰¥
9
10 .
ACM Transactions on Algorithms, Vol. 15, No. 4, Article 51. Publication date: October 2019. 
51:32 M. Bun et al.
Recall thatH(Xj |Bj (r,Xj)) is defined as the average ofH(Xj |Bj (r,Xj) = bj) over all possible values
bj in the range of Bj (r, Â·). So,
9
10
â‰¤

bj
Pr[Bj (r,Xj) = bj] Â· H(Xj |Bj (r,Xj) = bj)
â‰¤

bj :H (Xj |Bj (r,Xj )=bj )â‰¥ 1
2
Pr[Bj (r,Xj) = bj] +

bj :H (Xj |Bj (r,Xj )=bj )< 1
2
Pr[Bj (r,Xj) = bj] Â·
1
2
= Pr 
Bj (r,Xj) = bj s.t. H(Xj |Bj (r,Xj) = bj) â‰¥
1
2

+
1
2 Â· Pr 
Bj (r,Xj) = bj s.t. H(Xj |Bj (r,Xj) = bj) <
1
2

â‰¤ Pr 
Bj (r,Xj) = bj s.t. H(Xj |Bj (r,Xj) = bj) â‰¥
1
2

+
1
2
.
Let t = (r, a1,..., an ) be a possible transcript, and let bj denote the portion of t corresponding
to Bj , that is, bj = (a(jâˆ’1) n
m +1,..., aj
n
m ). We say that index j âˆˆ [m] is good for the transcript t if
H(Xj |Bj (r,Xj) = bj) â‰¥ 1/2.
Then, for every r and every j âˆˆ [m], we have that
Pr[T = t s.t. j is good for t|R = r] = Pr 
Bj (r,Xj) = bj s.t. H(Xj |Bj (r,Xj) = bj) â‰¥
1
2

â‰¥
2
5
,
where the second probability is only over Xj and the coins of Bj (r,Xj). Consider the following
event:
Event E1 (over sampling S âˆˆ {0, 1}m and the execution of A):
T = t s.t. there are at least m
5 choices for j âˆˆ [m] which are good for t.
For every fixture of R = r, by the Chernoff bound, assuming that m â‰¥ 20 ln(3/Î²), we have that
Pr[E1 |R = r] â‰¥ 1 âˆ’ Î²
3 . Hence, Pr[E1] â‰¥ 1 âˆ’ Î²
3 . We continue with the proof assuming that event E1
occurs.
Fix a transcript t = (r, a1,..., an ), and let j be a good index for t. We have that bj is s.t.
H(Xj |Bj (r,Xj) = bj) â‰¥ 1/2. Hence,
1
10
â‰¤ Pr[Xj = 1|Bj (r,Xj) = bj] â‰¤
9
10 .
Now observe that the executions of Bj (r, Â·) are independent across different jâ€™s, and are independent of the serverâ€™s randomness R (since we fixed r in Bj (r, Â·)). Hence,
Xj |Bj (r,Xj)=bj â‰¡ Xj |T=t.
So we also have that 1
10 â‰¤ Pr[Xj = 1|T = t] â‰¤ 9
10 , and hence,
Var[Xj |T = t] â‰¥ 9/100.
Moreover, for every choice of t it holds that the random variables (X1 |T=t), (X2 |T=t),...,
(Xm |T=t) are independent (this is a general fact about interactive protocolsâ€”if the partiesâ€™ inputs
start independent, they remain independent conditioned on the transcript).
Fix a transcript t. As event E1 has occurred, there is a set G âŠ† [m] of size m
5 of good indexes for
t. We now use the following anti-concentration theorem to argue about (

i âˆˆM Xi | T=t).
ACM Transactions on Algorithms, Vol. 15, No. 4, Article 51. Publication date: October 2019.
Heavy Hitters and the Structure of Local Privacy 51:33
Theorem 7.6 [15, 26]. There exist constants a,b > 0 s.t. the following holds. Let X be a sum of
independent random variables, each attaining values in [0, 1], and let Ïƒ = 
Var[X] â‰¥ 200. Then for
all t âˆˆ [0, Ïƒ 2
100 ], we have
Pr[X â‰¥ E[X] + t] â‰¥ a Â· eâˆ’bt 2/Ïƒ 2
.
An immediate corollary of this theorem is following:
Corollary 7.7. There exist constants a,b,c > 0 s.t. the following holds. Let X be a sum of independent random variables, each attaining values in [0, 1], and let Ïƒ = 
Var[X] â‰¥ 200. Then for every
a â‰¥ Î² â‰¥ 2âˆ’bÏƒ 2
and every interval I âˆˆ R of length |I | â‰¤ c Â·

Ïƒ2 Â· log( a
Î² ) we have
Pr[X  I] â‰¥ Î².
For completeness, we give in Appendix B a simple proof of a weaker version of corollary that
suffices for our application. Applying Corollary 7.7 to the random variables (Xj |T=t) for j âˆˆ G,
we get that there are constants a,b,c > 0 s.t. for every a â‰¥ Î² â‰¥ 2âˆ’bÎµ 2n and every interval I âˆˆ R of
length |I | â‰¤ c Â·

Îµ2n Â· log( a
Î² ) it holds that
Pr
âŽ¡
âŽ¢
âŽ¢
âŽ¢
âŽ¢
âŽ£

i âˆˆG
Xi  I






T=t
âŽ¤
âŽ¥
âŽ¥
âŽ¥
âŽ¥
âŽ¦
â‰¥ 2Î². (15)
Recall that, even conditioned on T=t, the random variables X1,...,Xm are independent. Hence,
Inequality (15) also holds for the sum of all Xj (since Inequality (15) holds for every fixed choice
of {Xj : j  G}). That is,
Pr
âŽ¡
âŽ¢
âŽ¢
âŽ¢
âŽ¢
âŽ¢
âŽ£

i âˆˆ[m]
Xi  I







T=t
âŽ¤
âŽ¥
âŽ¥
âŽ¥
âŽ¥
âŽ¥
âŽ¦
â‰¥ 2Î².
In particular, consider the interval I (t) = EstS (t) Â± c
2 Â·

Îµ2n Â· log
a
Î²

. We have that
Pr
âŽ¡
âŽ¢
âŽ¢
âŽ¢
âŽ¢
âŽ¢
âŽ£







EstS (T ) âˆ’

i âˆˆ[m]
Xi







> c
2 Â·

Îµ2n Â· log 
a
Î²
	âŽ¤
âŽ¥
âŽ¥
âŽ¥
âŽ¥
âŽ¥
âŽ¦
= Pr
âŽ¡
âŽ¢
âŽ¢
âŽ¢
âŽ¢
âŽ¢
âŽ£

i âˆˆ[m]
Xi  I (T )
âŽ¤
âŽ¥
âŽ¥
âŽ¥
âŽ¥
âŽ¥
âŽ¦
â‰¥ Pr[E1] Â· Pr
âŽ¡
âŽ¢
âŽ¢
âŽ¢
âŽ¢
âŽ¢
âŽ£

i âˆˆ[m]
Xi  I (T )







E1
âŽ¤
âŽ¥
âŽ¥
âŽ¥
âŽ¥
âŽ¥
âŽ¦
â‰¥

1 âˆ’ Î²
3
	
Â· 2Î² > Î².
And finally, using Equation (14),
Pr
âŽ¡
âŽ¢
âŽ¢
âŽ¢
âŽ¢
âŽ¢
âŽ£







EstD (T ) âˆ’

i âˆˆ[n]
Yi







> c
2CÎµ Â·

n Â· log 
a
Î²
	âŽ¤
âŽ¥
âŽ¥
âŽ¥
âŽ¥
âŽ¥
âŽ¦
> Î².
That is, with probability greater than Î² the protocol has error at least Î©( 1
Îµ

n Â· log( 1
Î² )) when
estimating the number of ones in the random database D âˆˆ {0, 1}
n. Hence, there must exist some
fixed database on which it has error Î©( 1
Îµ

n Â· log( 1
Î² )) with probability greater than Î².
ACM Transactions on Algorithms, Vol. 15, No. 4, Article 51. Publication date: October 2019.         
51:34 M. Bun et al.
APPENDICES
A PROOF OF THEOREM 4.5
In this section, we prove Theorem 4.5:
Theorem 4.5. Let A : Xn â†’ Y be Îµ-LDP. Then for every Î² > 0, we have I
Î²
âˆž(A,n) â‰¤ nÎµ2/2 +
Îµ

2n ln(1/Î²).
Proof. Let D be a distribution over Xn. Then, for every k > 0,
Pr xâˆ¼D,yâ†A(x )

ln Pr[D = x, A(x) = y]
Pr[D = x] Pr[A(D) = y] > k

= Pr xâˆ¼D,yâ†A(x )

ln Pr[A(x) = y]
Pr[A(D) = y] > k

= Pr xâˆ¼D,yâ†A(x )

ln Pr[A(x) = y]
Exâˆ¼D [Pr[A(x ) = y]] > k

â‰¤ Pr xâˆ¼D,yâ†A(x )

Exâˆ¼D 
ln Pr[A(x) = y]
Pr[A(x ) = y]

> k

(by Jensenâ€™s inequality)
â‰¤ max
x,x Pr
yâ†A(x )

ln Pr[A(x) = y]
Pr[A(x ) = y] > k

.
By Theorem 4.2, this quantity as at most Î² for k = nÎµ2/2 + Îµ

2n ln(1/Î²).
The claim now follows from the general fact that, for any pair of random variables U,V over
the same sample space,
Pr
uâˆ¼U

ln Pr[U = u]
Pr[V = u] > k

â‰¤ Î²
implies that
sup
T :Pr[U âˆˆT ]>Î²
ln Pr[U âˆˆ T ] âˆ’ Î²
Pr[V âˆˆ T ] â‰¤ k.
B SIMPLE VARIANT OF COROLLARY 7.7
In Section 7, we used the following anti-concentration result:
Corollary 7.7 [15, 26]. There exist constants a,b,c > 0 s.t. the following holds. Let X be a sum of
independent random variables, each attaining values in [0, 1], and let Ïƒ = 
Var[X] â‰¥ 200. Then for
every a â‰¥ Î² â‰¥ 2âˆ’bÏƒ 2
and every interval I âˆˆ R of length |I | â‰¤ c Â·

Ïƒ2 Â· log( a
Î² ) we have
Pr[X  I] â‰¥ Î².
This result has relatively simple proofs in the special case where X is the sum of i.i.d. random
variables (see, e.g., Reference [24]). However, we were unable to find a reference with a simple analysis for the case where the variables are independent but not necessarily identically distributed. In
particular, the results of Feller [15] are quite involved, as they are much more general than Corollary 7.7. In this section, we provide a simple analysis for a special case of Corollary 7.7 that suffices
for our applications in Section 7.
Let X1,...,Xn be independent random variables, each attaining values in {0, 1}, and denote
pi = Pr[Xi = 1]. We assume that, for some constant c âˆˆ (0, 1
2 ), for all i âˆˆ [n] we have that 1
2 âˆ’ c â‰¤
pi â‰¤ 1
2 + c.
Our goal is to show that for any interval I of length |I | â‰¤ O(

n Â· log( 1
Î² )) it holds that Pr[
i Xi
I] â‰¥ Î². The first step is to relate the situation to the easier-to-analyze case of i.i.d.random variables,
using the following lemma.
ACM Transactions on Algorithms, Vol. 15, No. 4, Article 51. Publication date: October 2019.   
Heavy Hitters and the Structure of Local Privacy 51:35
Lemma B.1. Let X,Y be independent random variables, where Y âˆˆ R and X âˆˆ {0, 1} with 1
2 âˆ’ c â‰¤
p = E[X] â‰¤ 1
2 + c. Also let X +,X âˆ’ âˆˆ {0, 1} be independent of Y s.t. p+ = E[X +] = 1
2 + c and pâˆ’ =
E[X âˆ’] = 1
2 âˆ’ c. Then for every interval I âŠ† R we have,
Pr[X + Y  I] â‰¥ min 
Pr[X + + Y  I], Pr[X âˆ’ + Y  I]

.
Proof.
Pr[X + Y  I] =

y âˆˆR
Pr
Y [Y = y] Â· Pr
X [X + y  I]
= Pr
Y
âŽ¡
âŽ¢
âŽ¢
âŽ¢
âŽ¢
âŽ¢
âŽ£
Y + 1  I
and
Y  I
âŽ¤
âŽ¥
âŽ¥
âŽ¥
âŽ¥
âŽ¥
âŽ¦
Â· 1 + Pr
Y
âŽ¡
âŽ¢
âŽ¢
âŽ¢
âŽ¢
âŽ¢
âŽ£
Y + 1  I
and
Y âˆˆ I
âŽ¤
âŽ¥
âŽ¥
âŽ¥
âŽ¥
âŽ¥
âŽ¦  Expression A
Â·p + Pr
Y
âŽ¡
âŽ¢
âŽ¢
âŽ¢
âŽ¢
âŽ¢
âŽ£
Y + 1 âˆˆ I
and
Y  I
âŽ¤
âŽ¥
âŽ¥
âŽ¥
âŽ¥
âŽ¥
âŽ¦  Expression B
Â·(1 âˆ’ p).
(16)
Trivially, one of these two expressions is greater or equal to the other. Let us define pËœ to be pËœ = 1
2 âˆ’ c
if A â‰¥ B, and pËœ = 1
2 + c otherwise. Furthermore, let XËœ âˆˆ {0, 1} be s.t. E[XËœ] = pËœ. With this notation
we have that
(16) â‰¥ Pr
Y
âŽ¡
âŽ¢
âŽ¢
âŽ¢
âŽ¢
âŽ¢
âŽ£
Y + 1  I
and
Y  I
âŽ¤
âŽ¥
âŽ¥
âŽ¥
âŽ¥
âŽ¥
âŽ¦
Â· 1 + Pr
Y
âŽ¡
âŽ¢
âŽ¢
âŽ¢
âŽ¢
âŽ¢
âŽ£
Y + 1  I
and
Y âˆˆ I
âŽ¤
âŽ¥
âŽ¥
âŽ¥
âŽ¥
âŽ¥
âŽ¦
Â· pËœ + Pr
Y
âŽ¡
âŽ¢
âŽ¢
âŽ¢
âŽ¢
âŽ¢
âŽ£
Y + 1 âˆˆ I
and
Y  I
âŽ¤
âŽ¥
âŽ¥
âŽ¥
âŽ¥
âŽ¥
âŽ¦
Â· (1 âˆ’ pËœ)
= Pr[XËœ + Y  I] â‰¥ min 
Pr[X + + Y  I], Pr[X âˆ’ + Y  I]

.
Consider again our independent random variables X1,...,Xn âˆˆ {0, 1}, and let I âŠ† R be an interval. Using Lemma B.1 we can replace, one-by-one, every variable Xi with a variable XËœ
i with
expectation either 1
2 + c or 1
2 âˆ’ c (exactly), without decreasing the probability that 
i Xi  I. That
is, we have established the following statement.
Corollary B.2. Let X1,...,Xn âˆˆ {0, 1} be independent, where 1
2 âˆ’ c â‰¤ pi = E[Xi] â‰¤ 1
2 + c. For
every interval I âŠ† R there exist a collection of n independent random variables XËœ
1,...,XËœ
n âˆˆ {0, 1}
where E[XËœ
i] = pËœi âˆˆ { 1
2 + c, 1
2 âˆ’ c}, such that
Pr
âŽ¡
âŽ¢
âŽ¢
âŽ¢
âŽ¢
âŽ£

i
Xi  I
âŽ¤
âŽ¥
âŽ¥
âŽ¥
âŽ¥
âŽ¦
â‰¥ Pr
âŽ¡
âŽ¢
âŽ¢
âŽ¢
âŽ¢
âŽ£

i
XËœ
i  I
âŽ¤
âŽ¥
âŽ¥
âŽ¥
âŽ¥
âŽ¦
.
So, we have a collection of n random variablesXËœ
1,...,XËœ
n of two possible types (either pËœi = 1
2 + c
or pËœi = 1
2 âˆ’ c). Clearly, at least one type appears at least n/2 times. We hence let G âŠ† [n] denote
a set of size n/2 s.t. for all i âˆˆ G we have that XËœ
i are of the same type. We can now apply again
Lemma B.1 to our random variables XËœ
1,...,XËœ
n, and for every i  G replace XËœ
i with a constant 0
or 1 (this amounts to shifting the interval I). This brings us to the following corollary:
Corollary B.3. Let X1,...,Xn âˆˆ {0, 1} be independent, where 1
2 âˆ’ c â‰¤ pi = E[Xi] â‰¤ 1
2 + c. For
every interval I âŠ† R there exist an interval Ë†I âŠ† R of the same length |Ë†I | = |I | and pË† âˆˆ { 1
2 + c, 1
2 âˆ’ c}
s.t. the following holds,
Pr
âŽ¡
âŽ¢
âŽ¢
âŽ¢
âŽ¢
âŽ£

i
Xi  I
âŽ¤
âŽ¥
âŽ¥
âŽ¥
âŽ¥
âŽ¦
â‰¥ Pr 
Bin 
n
2
,pË†

 Ë†I

.
The result now follows from anti-concentration bounds for the Binomial distribution. For instance, we can apply the following bound.
ACM Transactions on Algorithms, Vol. 15, No. 4, Article 51. Publication date: October 2019.                       
51:36 M. Bun et al.
Theorem B.4 [24, Lemma 5.2]. Let 0 < p â‰¤ 1
2 , and let âˆš3np â‰¤ t â‰¤ np/2. Then,
Pr[Bin(n,p) â‰¤ np âˆ’ t] â‰¥ exp 
âˆ’9t 2
np 	
,
Pr[Bin(n,p) â‰¥ np + t] â‰¥ exp 
âˆ’9t 2
np 	
.
To complete the proof, let I âŠ† R be an interval of length at most t. Let Ë†I = [aË†, Ë†
b] and pË† be as in
Corollary B.3. We have that
Pr 
Bin 
n
2
,pË†

 Ë†I

= Pr 
Bin 
n
2
,pË†

< aË†

+ Pr 
Bin 
n
2
,pË†

> Ë†
b

. (17)
There are two cases: Either pË† = 1
2 âˆ’ c or pË† = 1
2 + c. As the two cases are symmetric, we now
proceed assuming that pË† = 1
2 âˆ’ c.
Recall that the median of Bin( n
2 ,pË†) is between 
npË†
2  and 
npË†
2 . Hence, if Ë†
b â‰¤ npË†
2 âˆ’ 1, then
Pr[Bin( n
2 ,pË†) > Ë†
b] â‰¥ 1
2 and the proof is complete. We therefore proceed assuming that Ë†
b â‰¥ npË†
2 âˆ’ 1,
and hence, aË† â‰¥ npË†
2 âˆ’ 1 âˆ’ t â‰¥ npË†
2 âˆ’ 2t. So,
(17) â‰¥ Pr 
Bin 
n
2
,pË†

< npË†
2 âˆ’ 2t

â‰¥ exp 
âˆ’72t 2
pnË†
	
,
where the last inequality follows from Theorem B.4 by asserting that 
3pnË† â‰¤ 3t â‰¤ pnË† /4. This results in the following theorem:
Theorem B.5. There exist constants a,b,c > 0 s.t. the following holds. Let X1,...,Xn âˆˆ {0, 1} be
independent random variables, such that for every i âˆˆ [n] we have 1
10 â‰¤ E[Xi] â‰¤ 9
10 . Then for every
a â‰¥ Î² â‰¥ 2âˆ’bn and every interval I âˆˆ R of length |I | â‰¤ c Â·

n Â· log( 1
Î² ) we have
Pr[X  I] â‰¥ Î².
C PROOF OF THEOREM 3.6 [25]
Recall the definition of unique-list-recoverable codes:
Definition 3.5. An (Î±, , L)-unique-list-recoverable code is a pair of mappings (Enc, Dec) where
Enc : X â†’ ([Y]Ã—[Z])
M , and Dec : (([Y]Ã—[Z])
 )
M â†’ XL, such that the following holds. Let
L1,..., LM âˆˆ ([Y]Ã—[Z])
. Assume that for every m âˆˆ [M], if (y, z), (y
, z
) âˆˆ Lm then y  y
. Then
for every x âˆˆ X satisfying |{m : Enc(x)m âˆˆ Lm }| â‰¥ (1 âˆ’ Î±)M we have that x âˆˆ Dec(L1,..., LM ).
The following theorem is a direct consequence of the results of Reference [25] (appearing implicitly in their analysis). We include their proof here for completeness.
Theorem 3.6 [25]. There exist constants C > 1 and 0 < Î± < 1 such that the following holds. For
all constants M â‰¤ log |X | and Y,  âˆˆ N, and for every fixed choice of functions h1,...,hM : X â†’ [Y],
there is a construction of an (Î±, , L)-unique-list-recoverable code
Enc : X â†’ ([Y] Ã— [Z])
M and Dec : (([Y] Ã— [Z])
 )
M â†’ XL,
where L â‰¤ C Â·  and Z â‰¤ (|X |
1/M Â· Y )
C. Furthermore, there is a mapping Enc : 
 X â†’ [Z]M such that
for every x âˆˆ X we have Enc(x) = ((h1 (x), Enc 
(x)1),..., (hM (x), Enc 
(x)M )).
ACM Transactions on Algorithms, Vol. 15, No. 4, Article 51. Publication date: October 2019.          
Heavy Hitters and the Structure of Local Privacy 51:37
We use the following tools in the construction:
(1) We will use a (standard) error-correcting code (enc, dec) with constant rate that can correct an Î©(1)-fraction of errors. Such codes exist with linear time encoding and decoding
[17, 36]. We partition enc(x) into M contiguous bitstrings of equal length O(log |X |)/M,
and let enc(x)m denote themth bitstring (form âˆˆ [M]). Note that we write (enc, dec) to denote this (standard) error correction code, and (Enc, Dec) for the unique-list-recoverable
code that we are constructing.
(2) We also use a d-regular Î»0-spectral expander7 F on M vertices for some d = O(1), where
Î»0 = Î±d for some (small) constant Î± > 0 to be specified later. Such an F can be constructed
in time poly(M) with d = poly(1/Î±) deterministically, for every M of the form M = Di for
some constant D. See Reference [31].8
We now define Enc : 
 X â†’ [Z]M and Enc : X â†’ ([Y] Ã— [Z])
M . For m âˆˆ [M] we define:
Enc 
(x)m =
enc(x)m,hÎ“(m)1 (x),...,hÎ“(m)d (x)
 and Enc(x)m =
hm (x), Enc 
(x)m

,
where Î“(m)k is the kth neighbor of m in F . Observe that logZ = O(log |X |)/M + d Â· logY, and
hence, Z â‰¤ (|X |
1/M Â· Y )
C for some constant C.
We now explain how the decoder Dec operates. To that end, let L1,..., LM âˆˆ ([Y]Ã—[Z])
 be such
that for every m âˆˆ [M], if (y, z), (y
, z
) âˆˆ Lm then y  y
.
We construct the following graph G on the layered vertex set V = [M] Ã— [Y]. For m âˆˆ [M],
we can view each element (y, z) âˆˆ Lm as suggesting d edges to add to G. Specifically, if z =
(e,y1,...,yd ), then this suggests connecting (m,y) with each of (Î“(m)1,y1),..., (Î“(m)d ,yd ). We
then let G be the graph created by including the at most (d/2) Â·

m |Lm | edges suggested by the
elements across all Lmâ€™s (we only include an edge if both endpoints suggest it).
Fix a domain element x âˆˆ X satisfying |{m : Enc(x)m âˆˆ Lm }| â‰¥ (1 âˆ’ Î±)M. The goal of the decoder Dec is to recover x. Let W be the set of M vertices {(m,hm (x))}M
m=1. Consider first the ideal
case in which the encodings Enc(x)m appear throughout all of the lists Lm, i.e., |{m : Enc(x)m âˆˆ
Lm }| = M. In this case, W would be an isolated connected component in G, and, furthermore, the
induced graph on W would be F . We will use the following lemma (a version of this lemma is
known as the expander mixing lemma):
Lemma C.1 [1]. Let A be the adjacency matrix of a d-regular graph with vertex set V . Suppose
the second largest eigenvalue of A in magnitude is Î» > 0. Then for any S âŠ† V , writing |S | = r|V |,
|âˆ‚S | â‰¥ (d âˆ’ Î»)(1 âˆ’ r)|S |.
By Lemma C.1, in the ideal case, for any subset A of W with |A| = r|W | = rM we have
|E(A,W \ A)| â‰¥ (d âˆ’ Î»0)(1 âˆ’ r)rM â‰¥

r(1 âˆ’ r) âˆ’ Î»0
4d
	
dM.
Let us now understand the possible effects of â€œbadâ€ indexes on W , where we say that m âˆˆ [M]
is bad if Enc(x)m  Lm. We can no longer argue that W is an isolated copy of F , but what we can
say, even with Î±M bad indexes, is that W forms an O(Î±)-spectral cluster:
Definition C.2. An Î·-spectral cluster is a vertex set W âŠ† V of any size satisfying the following two conditions: First, only an Î·-fraction of the edges incident to W leave W , that is,
7We say that a graph is a Î»-spectral expander if the second largest eigenvalue, in magnitude, of its unnormalized adjacency
matrix is at most Î». 8The assumption that M = Di can be removed as follows. The construction only needs a spectral expander, and not an
edge expander, and spectral expansion can be verified efficiently. As a random graph is a spectral expander with high
probability, so we can construct an expander for every M in efficient Las Vegas time.
ACM Transactions on Algorithms, Vol. 15, No. 4, Article 51. Publication date: October 2019.       
51:38 M. Bun et al.
|âˆ‚(W )| â‰¤ Î· Â· vol(W ), where vol(W ) is the sum of edge degrees of vertices insideW . Second, given
any subset A of W , let r = vol(A)/vol(W ) and B = W \ A. Then
|E(A, B)| â‰¥ (r(1 âˆ’ r) âˆ’ Î·)vol(W ).
Note r(1 âˆ’ r)vol(W ) is the number of edges one would expect to see between A and B hadW been
a random graph with a prescribed degree distribution.
Suppose m is bad. First, W might lose at most d edges from F (those corresponding to edges
incident upon vertex m in F ). Second, Lm might contain an element (y, z
) s.t. y = hm (x), but
z  Enc 
(x), possibly causing edges to be included in W that do not appear in F , or even inserting
edges that cross the cut (W ,G \W ). At most d such edges are inserted in this way. Thus, across all
bad indexes, the total number of F -edges deleted withinW is at most Î±dM, and the total number of
edges crossing the cut (W ,G \W ) is also at most Î±dM. Also, the volume vol(W ) is always at most
dM and at least (1 âˆ’ Î±)dM. Thus after considering bad indexes, for any subset A of W as above,
|E(A,W \ A)| â‰¥ 
r(1 âˆ’ r) âˆ’ Î»0
4d âˆ’ Î±
	
dM â‰¥ (r(1 âˆ’ r) âˆ’ Î±0)vol(W ),
for Î±0 â‰¥ Î± + Î»0/(4d) = 5Î±/4. Furthermore, the number of edges leaving W to G \W is
|âˆ‚(W )| â‰¤ Î±dm â‰¤
Î±
1 âˆ’ Î±
vol(W ) â‰¤
5Î±
4
vol(W ) â‰¤ Î±0vol(W ),
for Î± â‰¤ 1/4. ThusW is an Î±0-spectral cluster in G representing x. Our task of identifyingW therefore reduces to a clustering problem (identifying spectral clusters in G), which we can solve using
the following algorithm:
Theorem C.3 [25]. There exists universal constants Î·0 and K such the following holds. There is a
polynomial time and linear space algorithm A that, for any given Î· â‰¤ Î·0 and graphG = (V, E), finds
a family of disjoint vertex sets U1,...U so that every Î·-spectral cluster U âˆ— of G matches some set Ui
in the sense that
â€¢ vol(U âˆ— \ Ui ) â‰¤ 3 Î· Â· vol(U âˆ—).
â€¢ vol(Ui \ U âˆ—) â‰¤ K Î· Â· vol(U âˆ—).
Applying the above theorem to our graph G, we have that algorithm A recovers a W  missing
at most 3Î±0vol(W ) volume from W , and containing at most KÎ±0vol(W ) volume from G \W .
After obtaining W 
, we remove any vertex from W  of degree â‰¤ d/2, so W  contains at most
2KÎ±0vol(W )/d â‰¤ 2KÎ±0M vertices from outsideW . Furthermore, since there are at most Î±dM edges
lost fromW due to bad indexes, at most 2Î±M vertices inW had their degrees reduced to â‰¤d/2, and
thus removing low-degree vertices removed at most 2Î±M vertices from W . Also, as W  is missing
at most 3Î±0vol(W ) volume from W , we have that W  misses at most 6Î±0M vertices with degree
>d/2 from W . Overall, W  contains at most 2KÎ±0M vertices from outside W , and misses at most
(6Î±0 + 2Î±)M vertices from W .
We then form a (corrupted) codeword c by concatenating encoding chunks specified by the
vertices in W 
. Since then at most a ((2K + 6)Î±0 + 2Î±)-fraction of entries in enc(x) and c differ,
for Î± sufficiently small, we successfully decode and obtain the binary encoding of x.
It remains to show that the number of identified elements (denoted as L) is small. To that end,
observe that in the analysis above we had vol(W 
) â‰¥ vol(W ) âˆ’vol(W \W 
) â‰¥ (1 âˆ’ Î± âˆ’ 3Î±0)dM.
Moreover, recall that the total number of edges in G is at most dM, and hence, there could be at
most 2dM
(1âˆ’Î±âˆ’3Î±0 )dM = O() such clusters W 
. So L = O().      