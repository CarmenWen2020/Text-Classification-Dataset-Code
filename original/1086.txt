The Subtree Isomorphism problem asks whether a given tree is contained in another given tree. The problem
is of fundamental importance and has been studied since the 1960s. For some variants, e.g., ordered trees, nearlinear time algorithms are known, but for the general case truly subquadratic algorithms remain elusive.
Our first result is a reduction from the Orthogonal Vectors problem to Subtree Isomorphism, showing that
a truly subquadratic algorithm for the latter refutes the Strong Exponential Time Hypothesis (SETH).
In light of this conditional lower bound, we focus on natural special cases for which no truly subquadratic
algorithms are known. We classify these cases against the quadratic barrier, showing in particular that:
• Even for binary, rooted trees, a truly subquadratic algorithm refutes SETH.
• Even for rooted trees of depthO(log logn), where n is the total number of vertices, a truly subquadratic
algorithm refutes SETH.
• For every constant d, there is a constant εd > 0 and a randomized, truly subquadratic algorithm for
degree-d rooted trees of depth at most (1 + εd ) logd n. In particular, there is an O(min{2.85h,n2})
algorithm for binary trees of depth h.
Our reductions utilize new “tree gadgets” that are likely useful for future SETH-based lower bounds for
problems on trees. Our upper bounds apply a folklore result from randomized decision tree complexity.
CCS Concepts: • Theory of computation → Problems, reductions and completeness; • Mathematics
of computing → Trees;
Additional Key Words and Phrases: Subtree Isomorphism, fine-grained complexity, SETH
1 INTRODUCTION
Trees are among the most frequently used and commonly studied objects in computer science.
One of the most basic and fundamental computational problems on trees is whether one tree is
contained in another, that is, can an isomorphic copy of H be obtained by deleting nodes and
edges of G? This problem is known under three names: Subtree Isomorphism, Tree Pattern Matching, and Subgraph Isomorphism on Trees. There are a few variants of the problem, mainly determined by (1) whether the trees are rooted or unrooted, (2) whether their degrees are bounded,
and (3) whether the trees are ordered, i.e., whether the order of the children of each node must be
preserved by the isomorphism. In this article, we focus on the case of rooted, unordered trees with
degrees bounded by a constant d.
Because of its fundamental importance, the time complexity of Subtree Isomorphism has been
studied since the 1960s, e.g., by Matula [44] and Edmonds (see Reference [45]). The problem is an
interesting special case of the Subgraph Isomorphism problem, studied extensively in theoretical
computer science. Subgraph Isomorphism is well known to be NP-hard, since it generalizes hard
problems such as Clique [34]. Moreover, Cygan et al. [18] showed that subgraph isomorphism
requires 2ω(n) time under the exponential time hypothesis (ETH). Special cases of subgraph isomorphism, especially ones that are in P, have received extensive attention. A recent 85-page article
by Marx and Pilipczuk [42] covers the case in which H is of fixed constant size. Besides fixing the
size of H, there are other non-trivial ways to make the problem polynomial time solvable; Subtree
Isomorphism is the earliest and arguably the most natural one. Polynomial time algorithms were
also obtained for biconnected outerplanar graphs [39], two-connected series-parallel graphs [41],
and more [19, 43], while it is known that further generalizations quickly become NP-hard, e.g.,
when H is a forest and G is a binary tree [24].
The problem is also of practical relevance, since it can model important applications in a wide
variety of areas. Subtree Isomorphism is at the core of many more expressive problems, such as
Largest Common Subtree [6, 7, 35], which generally ask How “similar” are two trees? Application
areas include computational biology [58], structured text databases [36], and compiler optimization
[52]. Several definitions of tree-similarity have been proposed, and the search for fast algorithms
for computing them, both in theory and in practice, has been ongoing for a few decades—see
References [10, 23, 27, 53] for surveys and textbooks. We focus on Subtree Isomorphism and then
briefly discuss how the techniques introduced in this article can be adapted to prove new results
for the Largest Common Subtree problem as well.
Previous Results. According to Matula [45], the first algorithms for Subtree Isomorphism were
proposed in 1968 independently by Edmonds and Matula himself [44]. Ten years later, Reyner
[48] and Matula [45] showed that these algorithms run in polynomial time and the runtime is
O(n2.5). The algorithm executes many calls to a subroutine that solves maximum matching in
bipartite graphs. These results were for rooted trees, and later Chung [14] showed that the same
bounds can be achieved for unrooted trees. In 1983, Lingas [38] shaved a log factor, and the most
recent development was in 1999 by Shamir and Tsur [51] who used the more recent randomized
algorithms for bipartite matching [13] to reduce the runtime to O(nω ), where ω < 2.373 is the
matrix multiplication exponent [22, 56].
Interestingly, in the most basic case of rooted and constant degree trees, even the early algorithms run in O(n2) time, and the fastest known runtime is O(n2/ logn) [38, 51]. For comparison,
when the trees are ordered, a long line of STOC/FOCS papers [15, 16, 21, 32, 33, 37] brought down
the complexity of the problem from quadratic [28] to O(n logn) time [17]. It is natural to wonder
whether the same improvements can be achieved in the case of unordered trees.
ACM Transactions on Algorithms, Vol. 14, No. 3, Article 27. Publication date: June 2018.
Subtree Isomorphism Revisited 27:3
Main Results. Our main result is a conditional lower bound for Subtree Isomorphism. We show
that a truly subquadratic algorithm is unlikely, even on very restricted cases such as those of binary,
rooted trees or rooted trees of depth O(log logn). A matching upper bound, up to no(1) factors, has
been known since the 1960s (we briefly discuss this algorithm in Section 3).
Our lower bounds are conditioned on the well-known Strong Exponential Time Hypothesis
(SETH) of Impagliazzo, Paturi, and Zane [30, 31] which roughly states that as k grows, k-SAT on
n variables requires 2(1−ε )n poly(n) time for all ε > 0. Our result for Subtree Isomorphism is the
first “SETH-hard” problem on trees, which is an exciting addition to the diverse list1 that already
includes problems on vectors [55], (general) graphs [2, 3, 47, 49], sequences [1, 5, 9, 12], and curves
[11]. Our ideas and constructions of “tree gadgets” are useful for proving conditional lower bounds
for other problems on trees. We demonstrate this with a lower bound for the Largest Common
Subtree problem, discussed below.
Theorem 1.1. For all d ≥ 2, Subtree Isomorphism on two rooted, unordered trees of size O(n),
degree d, and height h ≤ 2 logd n + O(log logn) cannot be solved in truly subquadratic O(n2−ε ) time
under SETH.
More generally, if the size of the smaller tree is n and the bigger tree is m, then our lower bound
says that O((nm)
1−ε ) time refutes SETH. We remark that since SETH is believed to hold even
for randomized algorithms, our lower bound is also a barrier for truly subquadratic randomized
algorithms.
To complement our lower bound, we proceed to tackle natural restrictions of the problem algorithmically. The most natural way to restrict tree inputs is to bound the degree or height. Our
lower bound leaves little room for improvement: Even on binary trees of height (2 + o(1)) logn
any algorithm must take quadratic time under SETH (note that the minimum height of a binary
tree is logn).
An intriguing case is when the trees are binary and almost complete, i.e., d = 2 and h =
(1 + o(1)) logn. We are unable to show a super-linear lower bound in this case or to obtain a
deterministic algorithm that runs in truly subquadratic time. Nevertheless, we present a randomized, Las Vegas, algorithm that solves this case in truly subquadratic O(n1.507) time. Our algorithm
solves more general cases:
Theorem 1.2. There is a randomized algorithm for rooted Subtree Isomorphism with expected running time O(min{2.8431h,n2}) for trees H and G of size O(n) and height at most h. In particular, the
algorithm runs in time O(n1.507) for trees of depth (1 + o(1)) · log2 n and is truly subquadratic for
trees of depth h ≤ 1.3267 · log2 n.
Our algorithm is simple, natural, and easy to implement. Perhaps more interesting than the
upper bound itself is that the technique we use to obtain it uses a technique from randomized
decision tree complexity.
We also consider the case of ternary trees, providing a fast Las Vegas algorithm for it. Our
approach is similar to that of the binary tree case. However, here we use a computer program to
analyze the expected running time of the algorithm.
Theorem 1.3. There is a randomized algorithm that can solve Subtree Isomorphism on two rooted
ternary trees of size O(n) and height at most h in expected O(min{6.107h,n2}) time.
Finally, we generalize our algorithms to obtain truly subquadratic algorithms for rooted Subtree
Isomorphism on trees with small height and constant degree d, for any d ≥ 2.
1These are problems with O (nc ) upper bounds for some c > 1 and an O (nc−ε ) algorithm, for some ε > 0, is known to
refute SETH.
ACM Transactions on Algorithms, Vol. 14, No. 3, Article 27. Publication date: June 2018.
27:4 A. Abboud et al.
Theorem 1.4. There is a randomized algorithm that solves Subtree Isomorphism on two rooted
trees of size O(n), constant degree d, and height at most h in expected time
O

min 
d2 − 1
3
d +
2
3
h
,n2
 .
In particular, the algorithm is strongly subquadratic for trees of height
h ≤

log(d2)
log(d2 − 1
3d + 2
3 )
− ϵ

· logd n,
for any constant ϵ > 0.
The bound in the above theorem is not tight for small d, as our algorithms for d = 2 and d = 3
show. To obtain the upper bound, we prove a new randomized query complexity upper bound for
bipartite perfect matching, which could be of independent interest (Lemma 3.3).
This work is another example of a fine-grained study of the complexity of fundamental problems
in P under natural parameterizations. This approach was formalized in two recent works [4, 25].
Techniques and Other Results. To prove our SETH hardness results we show reductions from
Orthogonal Vectors to Subtree Isomorphism in Section 2. The reductions follow all previous SETHhardness results in spirit but require careful constructions of “tree gadgets” that represent vectors,
as well as techniques for combining the gadgets into two big trees H andG for which the existence
of an orthogonal pair of vectors determines whether H is contained in G. Our reduction is clean
and simple, but it gets more tricky when restricted to trees of constant degree.
Our reduction is easily modified to obtain similar lower bounds for related problems such as
Largest Common Subtree on two trees (LCST). This problem is NP-hard when the number of trees
is a parameter or when the two trees are labelled (and unrooted) [57, 59], while some approximation
and parameterized algorithms are known [6, 7, 35]. When the two trees are binary and unlabeled,
the problem can be solved in quadratic time, and an adaptation of Theorem 1.1 shows that even
when the height is (1 + o(1)) logn, a truly subquadratic algorithm refutes SETH.
Theorem 1.5. For all constants d ≥ 2, the Largest Common Subtree problem on two rooted trees of
size at most n, degree d, and height h ≤ logd n + O(log logn) cannot be solved in truly subquadratic
O(n2−ε ) time under SETH.
Theorem 1.5 is surprising when contrasted with our other results. On the one hand, for arbitrary
rooted trees with constant degrees, both Subtree Isomorphism and the harder-looking LCST have
tight quadratic upper and (conditional) lower bounds. On the other hand, we show that under the
further restriction that the trees have small depth (as in Theorem 1.2), Subtree Isomorphism can be
solved in truly subquadratic time, while by Theorem 1.5 the LCST problem cannot, under SETH.
We attribute our new algorithmic results to two ingredients. The first important ingredient
comes from our lower bounds. In particular we noticed that when the trees are binary and the
depth is (1 + ε) logn, it is difficult to implement our reductions. This turned our attention to finding
upper bounds. Knowing the hard cases thus allowed us to focus on the solvable cases. This is an
important by-product of the recent research on conditional lower bounds in P.
The second ingredient was making a connection between this problem and a seminal result
from randomized decision tree complexity [50]. Our algorithm for binary (and ternary) trees is
inspired by the following well-known result from complexity theory: Given a formula represented
by a complete AND-OR tree on n leaves that represent the variables, can you evaluate the formula
without looking at all the inputs? The surprising fact is that this is possible with randomization:
ACM Transactions on Algorithms, Vol. 14, No. 3, Article 27. Publication date: June 2018.    
Subtree Isomorphism Revisited 27:5
To evaluate a gate, we guess which child to check first at random, and if we see a 1 input to an OR
gate, or a 0 input to an AND gate, we do not have to check the other child. Therefore, it is possible
to evaluate the formula by only looking at n1−ε inputs. This result has found many applications in
various areas of complexity theory, learning theory, and quantum query complexity [8].
Other Related Work. In the late 1980s, Subtree Isomorphism was considered from the viewpoint
of efficient parallel algorithms. Lingas and Karpinski [40] placed the problem in randomized NC1.
Gibbons, Miller, Karp, and Soroker [26] independently obtained the same result and also showed an
NC1 reduction from bipartite matching to Subtree Isomorphism. Their reduction takes a matching
instance on n nodes and produces trees on Ω(n3) nodes and therefore does not imply a lower bound
on the time complexity of Subtree Isomorphism even assuming that current matching algorithms
are optimal. Note that any many-to-one reduction from matching (where the input is of size Ω(n2))
will generate trees of size Ω(n2). To get our quadratic lower bound, we reduce from a different
problem, namely Orthogonal Vectors.
Many related cases of the problem can be solved in near-linear time. For example, when both
trees have exactly the same size, we get the Tree Isomorphism problem that was solved inO(n) time
by Hopcroft and Tarjan [29], and later other linear time algorithms were suggested (see Reference
[20] and the references therein). Another example is the case of ordered trees, meaning that there
is an order among the children of a node that cannot be modified in the isomorphism. Also, when
a “subtree” is defined to be a node and all its descendants, “subtree” isomorphism can be solved in
linear time [54].
2 SETH LOWER BOUNDS
The SETH states that for every ε > 0 there exists a k such that k-SAT on n variables cannot be
solved in O(2(1−ε )n ) time. Williams [55] related SETH to a polynomial time problem called Orthogonal Vectors (OV). The inputs to OV are two lists of N vectors in {0, 1}D , and the output is
“yes” if and only if there is a pair of vectors α, β, one from each list, that are orthogonal, i.e., for
all i ∈ [D] either α[i] or β[i] is equal to 0. Williams reduced CNF-SAT to OV so that if OV can
be solved in O(N2−ε ) time when D = ω(log N), for some ε > 0, then CNF-SAT on n variables and
polyn clauses can be solved in O(2(1−ε
)n ) time for some ε > 0, and SETH is false.
In this section, we reduce CNF-SAT, via the OV problem, to different variants of the Subtree
Isomorphism problem to prove our SETH-based lower bounds.
2.1 Hardness for Subtree Isomorphism
A Warm-Up Reduction. We start with a “warm-up” reduction that presents the high-level idea
of our proofs. In Theorem 2.1, we reduce OV to Subtree isomorphism on trees with n = O(ND)
vertices, unbounded degree, and height h = O(D). We later show how to change the construction
to get trees with small constant degree and small height.
Theorem 2.1. Orthogonal Vectors on two lists of N vectors in {0, 1}D can be reduced to Subtree
Isomorphism on two trees of size O(ND) and height O(D).
Proof. Let us denote the vectors of the first list by A = {α1,..., αN } and of the second list by
B = {β1,..., βN } and recall that our goal is to find a pair of vectors α ∈ A, β ∈ B such that for every
coordinate i ∈ [D] either α[i] = 0 or β[i] = 0.
The first ingredient in the reduction is to construct vector gadgets.
For every vector in the first list α ∈ A, we create a vector gadget: a treeHα of sizeO(D) as follows.
First, add a path u1 → u2 →···→ uD+2 and let u1 be the root of Hα . Then, for each coordinate
ACM Transactions on Algorithms, Vol. 14, No. 3, Article 27. Publication date: June 2018.
27:6 A. Abboud et al.
Fig. 1. A simple representation of vectors α, β ∈ {0, 1}D as trees.
i ∈ [D] we consider α[i] and if it is a 1 we add a node ui,1 to the tree Hα as the child of the node
ui , i.e., we add the edge ui → ui,1. Otherwise, if α[i] = 0, then the only child of ui will be ui+1.
We now define the vector gadgets for the vectors in the second list. For every β ∈ B, we create
a vector gadget: a tree Gβ of size O(D) as follows. The first step is similar, we add a path v1 →
v2 →···→ vD+2 and let v1 be the root. The difference is in the second step. For each coordinate
i ∈ [D], we consider β[i] and if it is a 0 we add a node vi,0 to Gβ as the child of the node vi , i.e.,
we add the edge vi → vi,0. The trees Hα and Gβ are shown in Figure 1.
The following simple claim is the key to our reduction and explains our gadget constructions.
Claim 1. Hα is isomorphic to a subtree of Gβ if and only if α, β are orthogonal.
Proof. For the first direction, assume that α, β are orthogonal, and therefore for every i ∈ [D]
we know that either α[i] = 0 or β[i] = 0. We will define a mapping f from Hα to a subgraph of
Gβ such that if {u,v} is an edge in Hα , then {f (u), f (v)} is an edge in Gβ . First, we map the roots
and paths to each other, by setting f (ui ) = vi for all i ∈ {0,...,D + 2}. Then, we consider every
i ∈ [D] for which α[i] = 1 and map ui,1 to the node vi,0 in Gβ . We are guaranteed that vi,0 exists,
because if α[i] = 1, then β[i] must be 0 by the orthogonality of the vectors. It is easy to check that
two neighbours in Hα are mapped to two neighbours in Gβ .
For the other direction, assume Hα is isomorphic to a subgraph ofGβ , and let f be the mapping.
First, note that u1 must be mapped to v1, since these are the roots of the two trees. Then we
observe that uD+2 must be mapped to vD+2 and the path u1 →···→ uD+2 must be mapped to the
path v1 →···→ vD+2, since these are the only paths of length at least (D + 2) in the trees. Now,
leti ∈ [D] be such that α[i] = 1, and note thatui must have two children in this case, which implies
that f (ui ) = vi must also have at least two children in Gβ , which implies that the node vi,0 must
exist, and β[i] = 0. Thus, whenever α[i] = 1, it must be the case that β[i] = 0, and the vectors are
orthogonal.
ACM Transactions on Algorithms, Vol. 14, No. 3, Article 27. Publication date: June 2018. 
Subtree Isomorphism Revisited 27:7
Fig. 2. Reduction from OV to subtree isomorphism on high-degree trees.
The final step is to combine the vector gadgets into two trees H,G in a way such that H is
isomorphic to a subtree of G if and only if there is a pair of orthogonal vectors within our two
lists.
To this end, we define a special vectorγ = 0 to be the all-zero vector inD dimensions. By Claim 1,
for any vector β ∈ {0, 1}D , we have that Hβ is isomorphic to a subtree of Gγ .
We are now ready to define the trees H and G of size O(ND).
G will be composed of a root node д of degree (2N − 1) that has Gβj as a child for every βj ∈ B,
in addition to (N − 1) distinct Gγ gadgets. That is, first, for each j ∈ [N] add the vector gadget
Gβj to G and add the edge д → v1, where v1 is the root of Gβj . And then we add (N − 1) trees
G(1)
γ ,...,G(n−1)
γ toG, and for each j ∈ [N − 1] we add the edge д → v1, where v1 is the root ofG(j)
γ .
H will be constructed in a similar way, except we do not add the γ vector gadgets. Create a root
node h of degree N that has Hαj as a child for every αj ∈ A. As in the definition of G, we add edges
h → u1, where u1 is the root of Hαj for every j ∈ [N]. The trees H and G are shown in Figure 2.
Each triangle represents a subtree.
Before proving the correctness of the reduction, note that the size of each tree is indeed O(ND),
since each gadget has size O(D), and we are combining O(N) gadgets into our trees H,G. To
conclude the proof, we claim that H is isomorphic to a subgraph of G if and only if there is a pair
of orthogonal vectors.
Claim 2. In the above reduction, H is isomorphic to a subtree of G if and only if there is a pair
α ∈ A, β ∈ B of orthogonal vectors.
Proof. For the first direction, assume that there is a pair of orthogonal vectors α ∈ A, β ∈ B,
and we will show that H is isomorphic to a subtree of G. Consider the mapping which maps Hα to
Gβ as in Claim 1, and then for each of the (N − 1) Hα subtrees, for α  α, we map it to a different
Gγ subtree of G. Finally, the root h is mapped to д. It is easy to check that neighbours in H are
mapped to neighbours in G.
For the other direction, assume that H is isomorphic to a subgraph of G, and let f be the corresponding mapping. We know that f (h) = д and for each vector gadget Hαj in H, its image using
our mapping f must be entirely contained in exactly one vector gadgetGx inG, where x ∈ B ∪ {γ }.
Moreover, two gadgets Hα ,Hα cannot be mapped to the same gadget Gx . There are N Hα gadgets
but only (N − 1) Gγ gadgets, and, thus, by the pigeonhole principle, there must be at least one
α ∈ A for which Hα is mapped to a gadget Gx for x  γ , i.e., x = β for some β ∈ B. We conclude
that there is a mapping from Hα to Gβ in which every two neighbours are mapped to neighbours,
that is, that Hα is isomorphic to a subgraph of Gβ , which, by Claim 1, implies that α ∈ A, β ∈ B are
orthogonal.
ACM Transactions on Algorithms, Vol. 14, No. 3, Article 27. Publication date: June 2018.    
27:8 A. Abboud et al.
Shorter Vector Gadgets. Next, we show how our reductions can be implemented with trees of
smaller depth by introducing a new construction of vector gadgets. We will use these gadgets in
our final reductions that prove Theorems 1.1 and 1.5.
Lemma 2.2. Given two vectors α, β ∈ {0, 1}D , we can construct two binary rooted trees Hα ,Gβ of
depth 3 log2 (D) + O(1) in linear time, such that Hα is isomorphic to a subtree of Gβ if and only if
α, β are orthogonal.
Proof. Our constructions will involve careful combinations of “index gadgets,” which are defined as follows. For a sequence of l binary values b1,b2,...,bl , we define a tree “index gadget”
Qb1,b2,...,bl (think ofl as being log2 (D + 1)	 and think of b1,b2,...,bl as bits representing an index
in [D]) to be composed of a path z1 → z2 →···→ zl of length l, in which z1 is the root, and for
all i ∈ [l] we attach a child zi,1 to zi if and only if bi = 1. That is, our index gadget Qb1,b2,...,bl is
representing the index in the natural way: The edge zi → zi,1 will exist if and only if bi = 1.
Our first “vector gadget” Hα is constructed as follows. First, we build a complete binary tree
with D leaves u1,u2,...,uD , where the subtree at each leaf ui will encode the entry α[i] using our
“index gadgets.” We assume that every index i ∈ [D] can be represented by l = log2 (D + 1)	 bits,
and we let i
¯ denote this representation and let i
¯S denote the binary sequence obtained by flipping
each bit of i
¯. For each node ui , we will attach three gadgets, one after the other: First, we will attach
the Qi
¯ index gadget, then we follow it by the Qi
¯S index gadget, and, finally, we append a path of
length either 2 or 3, depending on α[i]. The necessity of this complicated encoding will become
clear in the proof of correctness below. More formally, we first attach the root of Qi
¯ as child to the
vertex ui . Let z
l denote the node of Qi
¯ corresponding to zl in the above construction (i.e., the last
node on the path). We attach the root ofQi
¯S as a child to z
l
. Then, similarly, we let z
l be the node of
Qi
¯S that corresponds to zl in the above construction (i.e., the last node on the path), and we either
attach three nodes z
l → ai → bi → ci if α[i] = 1, or we attach only two nodes z
l → ai → bi .
The second “vector gadget” Gβ is constructed in the same way except that we attach a path of
length 3 if β[i] = 0 (as opposed to 1) and attach a path of length 2 if β[i] = 1. By construction, the
depth of both trees is 3 log2 (D) + O(1) as claimed.
To complete the proof, we show that Hα is isomorphic to a subtree of Gβ if and only if α · β = 0.
The first direction is easy: If the vectors are orthogonal, then the natural mapping from Hα to
Gβ that follows from our construction shows the isomorphism: Map the binary trees on top to
each other so that the ui ’s are mapped to each other, then map the attached Qi
¯ → Qi
¯S subtrees
to each other, and, finally, we can map the paths ai → bi → ci (if α[i] = 1) or ai → bi (if it is 0) to
each other, since in the first case β[i] must be zero and ci will also exist in Gβ .
It remains to show that if Hα is isomorphic to a subtree of Gβ , then α · β = 0. Our index gadgets
Qi
¯ and Qi
¯S will play a crucial role in this part, as they will show that in any mapping between
the leaves of the complete tree we must map ui in Hα to ui in Gβ or else the index gadgets will
not map into each other properly. We claim that for any two indices i, j ∈ [D], we have that i = j
if and only if both Qi
¯ is contained in Qj
¯ and Qi
¯S is contained in Qj
¯S . This is true because of the
following two observations:
• Qi
¯ is isomorphic to a subtree of Qj
¯ if and only if the set of positions in i
¯ with 1 is a subset
of the set of positions of j
¯with 1.
• Qi
¯S is isomorphic to a subtree of Qj
¯S if and only if the set of positions in i
¯with 0 is a subset
of the set of positions of j
¯with 0.
This is sufficient, since i = j if and only if the set of positions in i
¯ with 1 is a subset of the set
of positions of j
¯with 1 and the set of positions in i
¯ with 0 is a subset of the set of positions of j
¯
with 0.
ACM Transactions on Algorithms, Vol. 14, No. 3, Article 27. Publication date: June 2018.
Subtree Isomorphism Revisited 27:9
Therefore, any mapping from Hα to a subtree of Gβ must map the path representing α[i] to the
path representing β[i], for all i ∈ [D]. By construction, this can only happen if α · β = 0.
Constant Degree Trees. Perhaps the most challenging element towards the proof of Theorem 1.1
is the combination of all the vector gadgets into two big trees, without using large degrees.
To see the difficulty, recall the reduction in the proof of Theorem 2.1: In both trees, we added
all N vector gadgets as children of a root of degree N. By doing so, we have essentially allowed
the isomorphism to pick any matching between the gadgets. Combined with the auxiliary gadgets
that we added, this allowed us to show that the final two trees are a “yes” instance of Subtree
Isomorphism if and only if the original vectors contained an orthogonal pair. However, when the
trees have constant degree (say, binary) it is much harder to combine the vector gadgets into two
trees such that any matching between the gadgets can be chosen by the isomorphism. A natural
approach would be to add the gadgets at the leaves of a complete binary tree. One reason this does
not work is that any isomorphism must map the first and second gadgets to adjacent gadgets in
the second tree—that is, only special kinds of matchings can be “implemented.”
We overcome this difficulty with a two-level construction that allows the isomorphism to pick
exactly one gadget from each of the two trees and “match” them, while all the other gadgets do
not affect the outcome.
Theorem 2.3. Given sets of vectorsA, B, we can construct two rooted treesH = H(A) andG = G(B)
such that the following properties hold.
(1) The number of nodes in both trees and the construction time is upper bounded by O(ND).
(2) The degree of both trees is upper bounded by d.
(3) The depth of both trees is upper bounded by 2 logd (N) + O(logD).
(4) H is isomorphic to a subtree of G if and only if there are α ∈ A and β ∈ B with α · β = 0.
Proof. Let {Hα }α ∈A = {Hαi }i ∈[N ] and {Gβ }β ∈B = {Gβi }i ∈[N ] be the two sets of vector gadgets
corresponding to the vectors of A and B that are obtained by the construction in Lemma 2.2. We
will now combine these vector gadgets into two big trees H andG, which will be constructed quite
differently from each other.
Assume that logd (N) is an integer, otherwise add dummy vectors (all 1s vectors) to increase N.
The first tree H will be composed of a complete d-ary tree with N leaves u1,u2,...uN , followed by
a path of length logd (N) + 1, followed by the vector gadgets Hαi . More formally, for every i ∈ [N]
we add
ui → hi,1 → hi,2 →···→ hi,logd (N )+1 → Hαi .
To construct the second tree G, we need to construct vector gadgets Gγ corresponding to the
all-zero vector γ = 0 of length D. As before, we start with a complete d-ary tree with N leaves
v1,v2,...vN and attach a path of length logd (N) + 1 to each leaf, except for vN , which will be
treated differently. Then, we attach a copy of Gγ at the end of each one of these paths, that is,
N − 1 copies in total. Formally, for every i = 1,..., N − 1 we add
vi → hi,1 → hi,2 →···→ hi,logd (N )+1 → Gγ .
Note that none of the vectors in the second list are encoded in this part of G, and they will appear
now in the subtree rooted at vN , which we construct next. Rooted at vN , we add another complete
d-ary tree with N leaves v
1,v
2,...v
N , and then attach the vector gadgets right after these leaves.
That is, for every i ∈ [N], we add v
i → Gβi .
The trees H and G are shown in Figure 3. Each triangle represents a subtree, the gray triangles
represent complete binary trees, and the lines in the middle layer represent paths.
ACM Transactions on Algorithms, Vol. 14, No. 3, Article 27. Publication date: June 2018.  
27:10 A. Abboud et al.
Fig. 3. Reduction from OV to subtree isomorphism on binary trees.
This finishes the construction of H and G, and the first two properties are immediate. The third
property follows from Lemma 2.2, and we now turn to proving the fourth property, which is the
correctness of our construction.
Claim 3. There is a pair of vectors α ∈ A and β ∈ B with α · β = 0 if and only if H is isomorphic
to a subtree of G.
Proof. For the first direction, let αi and βj be a pair of orthogonal vectors, and we will show
that H is contained in G. First, consider the rearrangement of H so that the rightmost leaf of
the complete d-ary tree (where uN used to be) is ui , the node to which the vector gadget Hαi is
attached. We claim that all vector gadgets in H can now be properly mapped to subtrees of G,
without rearranging the vi nodes in G. To see this, first note that all vector gadgets Hαx for x  i
will be paired up with the Gγ vector gadgets, and by Lemma 2.2 and the fact that γ is orthogonal
to any vector, we know that there is a proper mapping. Then, it remains to show that the subtree
of H rooted at ui is contained in the subtree ofG rooted at vN , which follows, because we can map
the vector gadget Hαi to the vector gadget Gβj , since αi · βj = 0.
For the second direction, assume that there is a mapping from H to a subtree of G, and we will
show that there must exist a pair of orthogonal vectors. First, note that under this mapping, there
is some i ∈ [N] such that ui is mapped to vN . This holds, since there are N vertices u1,...,uN but
only N − 1 vertices v1,...,vN −1. By construction of the subtree rooted at vN , this means that the
vector gadget Hαi must be mapped into one of the vector gadgets Gβj for some j ∈ [N] and not
into Gγ . By Lemma 2.2, this can only happen if αi · βj = 0.
Theorem 2.3 and the connection between SETH and OV of Williams [55] imply Theorem 1.1
from the Introduction.
ACM Transactions on Algorithms, Vol. 14, No. 3, Article 27. Publication date: June 2018.  
Subtree Isomorphism Revisited 27:11
2.2 Hardness for Largest Common Subtree
Next, we prove a lower bound for the Largest Common Subtree (LCST) problem, which is a generalization of Subtree Isomorphism. Although the reductions above already imply a quadratic lower
bound for LCST, we will now optimize these reductions and prove a stronger hardness result: We
will show that even on binary trees of depth (1 + o(1)) logn, the LCST cannot be computed in
truly subquadratic time. This will show an interesting gap between LCST and Subtree Isomorphism, since the latter can be solved in truly subquadratic time on such trees—we present such
upper bounds in Section 3. Our strengthened hardness result gives an explanation for why we
are not able to extend our upper bounds to LCST: Such extensions would refute SETH. The next
theorem implies Theorem 1.5 from the Introduction.
Theorem 2.4. If for some ε > 0, then the Largest Common Subtree problem on two trees of size
n can be solved in O(n2−ε ) time, then Orthogonal Vectors on N vectors in {0, 1}D can be solved in
O(N2−ε · DO (1)
) time. The trees produced in the reduction from the Orthogonal Vectors problem have
degree d and height at most logd (N) + O(logD) for arbitrary d ≥ 2.
Proof. We note that the construction provided in Theorem 2.3 is not sufficient for our purposes,
because the height of the produced trees is 2 logd (N) + O(logD), which is larger than what we
want. We will use the more expressive nature of LCST to implement our reduction with smaller
height.
To achieve smaller height, we will try to implement vector gadgets such that the largest common
subtree of two gadgets would be of a certain fixed size E if the vectors are not orthogonal, while it
will be of a larger size E > E if the vectors are orthogonal. This trick was introduced by Backurs
and Indyk in their reduction to Edit-Distance [9] and later used in the reductions to LCS [1]. Here,
we carefully implement such gadgets with degree d trees of small height instead of sequences. We
assume that all vectors in A start with 1 and all vectors in B start with 0. We enforce this by adding
an extra coordinate 1 at the beginning of every vector in set A and adding an extra coordinate 0 at
the beginning of every vectors in set B. This does not change the answer to the problem (whether
there are two orthogonal vectors or not). Also, we assume that all vectors in A have the same
number of entries equal to 1. To enforce this, we subdivide the set A into at most D + 1 smaller
sets so that every set contain vectors with the same number of entries equal to 1. Then we run the
reduction on every one of the smaller subsets of A and B. This increases the runtime to solve the
Orthogonal Vectors problem by a factor of D + 1 but this is a subpolynomial factor in the number
of vectors and therefore negligible.
For each vector in the first list, α ∈ A, we construct a vector gadget Hα as follows. Let H
α be
the vector gadget constructed in Lemma 2.2 corresponding to vector α ∈ A. Then Hα is equal to
r → root(H
α ) for some vertex r, which is the root of Hα .
For each vector in the second list, β ∈ B, we construct a vector gadget Gβ as follows. Let δ be
a vector with D coordinates. The first entry is equal to 1 and the rest of the entries are equal to
0. Let G
β be the vector gadget constructed in Lemma 2.2 corresponding to vector β ∈ B. Then we
obtain Gβ by choosing a vertex r to be its root and adding r → G
δ and r → G
β .
The main idea behind this construction is that, when matching Hα and Gβ , one has a choice:
Either match H
α to G
δ (giving a fixed score, independent of α) or match it to G
β (and the score
then depends on the orthogonality of α and β.) We make this argument formal in the next lemma.
Lemma 2.5. We have that the size of Hα and Hα are equal |Hα | = |Hα | for all α, α ∈ A. The
largest common subtree of Hα and Gβ is of size E = |Hα | if α, β are orthogonal and it is of size
E = E − 1 otherwise.
Proof. |Hα | = |Hα | for all α, α ∈ A, since all vectors in A contain the same number of 1’s.
ACM Transactions on Algorithms, Vol. 14, No. 3, Article 27. Publication date: June 2018.
27:12 A. Abboud et al.
First, if α, β are orthogonal, then by Lemma 2.2 we have that Hα is isomorphic to a subgraph of
Gβ and the LCST has size E
.
For the second case, assume that α, β are not orthogonal. We first remark that there is a common
subtree of size E − 1: Let α denote α where we set the first coordinate of α (which is equal to 1)
to 0, then H
α is a subtree of H
α of size |H
α | = E − 1, and by Lemma 2.2, it is also a subtree of
G
δ , because α · δ = 0. It remains to show that we cannot map the entire tree Hα to a subtree of
Gβ , which follows, because H
α is neither isomorphic to a subtree of G
δ (since α · δ = 1) nor to a
subtree of G
β (since α · β  0).
We are now ready to present the final trees H,G. We construct H as follows. First, we build a
complete d-ary tree with N leaves h1,...,hN at the lowest level. We assume that N is an integer
power of d. This is w.l.o.g., since we assume d to be a constant. For every j ∈ [N], we add hj →
Hαj , where A = {α1,..., αN }. We construct G similarly. Take a complete d-ary tree with leaves
д1,...,дN at the lower level. For every j ∈ [N], we add дj → Gβj , where B = {β1,..., βN }.
Theorem 2.6. The Largest Common Subtree of H and G is of size at most d N −1
d−1 + (N · E) if there
is no pair of orthogonal vectors, and is at least d N −1
d−1 + (N · E + 1) otherwise.
Proof. We must map the nodes hi for every i ∈ [N] to nodes дπ (i), for some permutation π :
[N] → [N]. Notice, however, that π cannot be an arbitrary permutation, since, e.g., π (1) = π (2) ± 1
when d = 2 (the permutation must be implemented by swapping children in a complete binary
tree.)
The total size of the common subtree can be upper bounded by the size d N −1
d−1 of a complete
d-ary tree with N leaves, plus N
i=1 LCST (Hαi ,Gβπ (i) ), for an arbitrary permutation π. If there is
no pair of orthogonal vectors, then by Lemma 2.5, the latter sum is exactly N · E, and the total size
is bounded by d N −1
d−1 + N · E.
On the other hand, if there is an orthogonal pair αi, βj , we can take any mapping in which hi
is mapped to дj while the other hx ’s are mapped arbitrarily to different дy ’s. This induces some
permutation π : [N] → [N] so that hx is mapped to дπ (x ) for every x and j = π (i). One can see
that such a permutation exists by first considering any map that translates the path from the root
of H to hi to the path from the roof of G to дj and then completing the map so that no two hx
map to the same дy . Since αi · βj = 0, Lemma 2.5 implies that this mapping can be completed to a
mapping of score
dN − 1
d − 1
+
	
N
v=1
LCST (Hαv ,Gβπ (v) ) ≥
dN − 1
d − 1
+ (N − 1) · E + (E + 1)
= dN − 1
d − 1
+ (N · E + 1).
3 ALGORITHMS
In this section, we present new algorithms for Subtree Isomorphism on rooted trees with vertices
of bounded degree. Edmonds and Matula independently described a procedure for reducing the
rooted Subtree Isomorphism problem to a polynomially bounded collection of recursively smaller
Subtree Isomorphism problems and how to combine the answers by solving a maximum bipartite
matching problem (see Reference [45]). We follow the same approach but focus on the case where
the degrees are bounded by a constant.
Given two rooted trees H and G, we want to decide whether H is isomorphic to a subtree of G
where the root of H maps to the root of G. Let H1,H2,...,Hk and G1,G2,...,G be the subtrees
ACM Transactions on Algorithms, Vol. 14, No. 3, Article 27. Publication date: June 2018.    
Subtree Isomorphism Revisited 27:13
of H and G, respectively, with roots that are children of the root of H and the root of G. Let G be
a bipartite graph with vertex set V = {u1,...,uk }∪{v1,...,v }, and let (ui,vj) be an edge of G
if and only if Hi is isomorphic to a subtree of Gj . Then H is isomorphic to a subtree of G if and
only if G contains a matching of size k. The Edmonds-Matula procedure constructs the graph G
by recursion and then solves the maximum bipartite matching problem on G.
Designing similar algorithms for rooted Subtree Isomorphism thus involves two challenges: constructing G and solving the maximum bipartite matching problem on G. The currently fastest randomized algorithm for the maximum bipartite matching problem is due to Mucha and Sankowski
[46] and runs in expected timeO((k + )
ω ), whereω < 2.373 is the matrix multiplication exponent.
Improving this algorithm is itself a challenging open problem.
For constructing the graph G, it is not hard to see that any deterministic algorithm needs to
know all edges of G. For randomized algorithms, however, it is not always necessary to know
for every pair ui,vj whether the edge (ui,vj) is in the graph. The expected number of node pair
queries (“is the pair an edge in the graph?”) that a randomized algorithm needs to make to be
able to determine whether a perfect matching exists, is known as the randomized query complexity
(or decision tree complexity) of bipartite perfect matching. It is an easy exercise to check that the
randomized query complexity of the problem is Ω(k). Estimating the exact number of queries is,
however, not straightforward. It is not even clear whether k queries are necessary in expectation
or whether (1 − ε)k queries might be sufficient for some ε > 0. Factoring this into the analysis of
the maximum bipartite matching algorithm complicates things further.
To simplify things, we restrict our attention to the case where the degrees of the trees are
bounded by a constant. In this case we can check in constant time whether G contains the desired perfect matching, once a sufficient number of edge queries have been made. We can thus
focus solely on the randomized query complexity of the bipartite matching problem and its use in
recursive algorithms for the Subtree Isomorphism problem.
It is easy to show that in this case the algorithm of Edmonds and Matula runs in time O(mn),
where |H| = m and |G| = n. The same algorithm is also able to handle labelled vertices, i.e., each
vertex has a label and the labels of H are required to match the labels of the subtree ofG. Moreover,
the algorithm can solve the largest common subtree problem inO(mn) time as well. This is done by
recursively assigning a weight to every edge (ui,vj) of G equal to the size of the largest common
subtree of Hi and Gi and then asking for the matching of largest weight. (We refer to the appendix
for a short complexity analysis and further description of these algorithms.) Our lower bounds
from Theorems 1.1 and 1.5 are thus tight for trees of constant degree.
For the remainder of the section, we restrict our attention to trees of constant degree d and
height h. We first introduce a randomized algorithm that solves the binary problem in expected
time O(min{2.8431h,mn}). For comparison, the corresponding upper bound by Edmonds and
Matula [45] is O(min{4h,mn}), i.e., their algorithm makes four recursive calls at each level of the
tree. In particular, our algorithm is truly subquadratic when h < 1.3267 log2 n. For d = 3, we give
a similar but more complicated case analysis showing that the problem can be solved in expected
time O(min{6.107h,mn}), improving the straightforward O(min{9h,mn}) bound by Edmonds
and Matula. For d > 3, we introduce a randomized algorithm with expected running time upper
bounded by O(min{(d2 − 1
3d + 2
3 )
h,mn}).
3.1 A Faster Algorithm for Binary Trees
For trees with degree at most two, the Edmonds-Matula procedure can be interpreted as follows.
Let HL and HR be the left and right subtrees of H, and let GL and GR be the left and right subtrees
of G. H is isomorphic to a subtree of G if and only if one of the following two conditions are true:
ACM Transactions on Algorithms, Vol. 14, No. 3, Article 27. Publication date: June 2018.     
27:14 A. Abboud et al.
Fig. 4. A randomized, recursive algorithm for rooted Subtree Isomorphism on binary trees.
(1) HL is isomorphic to a subtree of GL, and HR is isomorphic to a subtree of GR.
(2) HL is isomorphic to a subtree of GR, and HR is isomorphic to a subtree of GL.
Each case can be checked with two recursive calls, and checking whether H is isomorphic to a
subtree of G can thus be done with at most four recursive calls, giving an O(4h ) upper bound.
Observe that if HL is not isomorphic to a subtree ofGL, then there is no reason to check whether
HR is isomorphic to a subtree of GR. Similarly, if the algorithm concludes that the first condition
is met, then there is no reason to check the second condition, since we already know that H is isomorphic to a subtree ofG. Based on these observations, we introduce a simple randomized variant
of the algorithm that achieves a significantly better running time by saving recursive calls: Swap
HL and HR with probability 1/2, and swapGL andGR with probability 1/2. Then run the EdmondsMatula algorithm but do not perform unnecessary recursive calls. We give a formal description of
the algorithm in Figure 4. We refer to the algorithm as RandBinarySubIso.
Theorem 3.1. The RandBinarySubIso algorithm runs in expected time O(min{2.8431h,n2}) for
trees H and G of size O(n) and height at most h. In particular, it runs in time O(n1.507) for trees of
height (1 + o(1)) · log2 n and is strongly subquadratic for trees of height h < 1.3267 log2 n.
Before proving Theorem 3.1, we first prove a useful lemma. Let T (h) be the maximum expected
number of times RandBinarySubIso(H,G) makes a recursive call with an empty tree when H and
G are arbitrary rooted trees with height at most h. Let Tyes (h) and Tno (h) be defined similarly, but
under the assumption that the algorithm returns true and false, respectively. Note that T (0) =
Tyes (0) = Tno (0) = 1. Also note that T (h) = max{Tyes (h), Tno (h)}.
Lemma 3.2. For all h ≥ 0,
Tyes (h) ≤ 2.25 · Tyes (h − 1) + 0.5 · Tno (h − 1),
Tno (h) ≤ Tyes (h − 1) + 2 · Tno (h − 1).
Proof. To simplify notation, we writeH ⊆ G whenH is isomorphic to a subtree ofG andH  G
otherwise.
We first show that Tyes (h) ≤ 2.25 · Tyes (h − 1) + 0.5 · Tno (h − 1). Assume therefore that H ⊆ G.
With probability ≥ 1/2, we then have HL ⊆ GL and HR ⊆ GR, such that the algorithm returns true
in line 6 after spending 2 · Tyes (h − 1) time in expectation. On the other hand, with probability
≤ 1/2 the outcomes of lines 5 and 6 depend on the trees in question, and the recursive calls in lines
7 and 8 both return true if reached. More precisely, we get three cases that depend on the trees:
ACM Transactions on Algorithms, Vol. 14, No. 3, Article 27. Publication date: June 2018. 
Subtree Isomorphism Revisited 27:15
(i) HL ⊆ GL and HR ⊆ GR: The recursive calls in lines 5 and 6 both return true, and the algorithm spends 2 · Tyes (h − 1) time in expectation.
(ii) HL  GL and HR  GR: The recursive call in line 5 returns false, and the recursive calls
in lines 7 and 8 both return true. The algorithm spends Tno (h − 1) + 2 · Tyes (h − 1) time in
expectation.
(iii) HL ⊆ GL and HR  GR, or HL  GL and HR ⊆ GR: The recursive call in line 5 returns false
with probability 1/2 and true with probability 1/2. In the second case the recursive call in
line 6 returns false. The recursive calls in lines 7 and 8 both return true. The algorithm
spends Tno (h − 1) + 2.5 · Tyes (h − 1) time in expectation.
The third case thus dominates the two others, and we conclude that Tyes (h) ≤ 2.25 · Tyes (h − 1) +
0.5 · Tno (h − 1).
We next show that Tno (h) ≤ Tyes (h − 1) + 2 · Tno (h − 1). Assume therefore that H  G. We get
the contribution 2 · Tno (h − 1) as follows. In either line 5 or 6, we get the answer false from
a recursive call, and in either line 7 or 8 we also get the answer false from a recursive call.
This amounts to two “no” answers that cost 2 · Tno (h − 1) in expectation. We get the contribution Tyes (h − 1) as follows. With probability at most 1/2 we get the answer true in line 5
(which means that we get false in line 6). Similarly, with probability at most 1/2, we get the
answer true in line 7 (which means that we get false in line 8). In total, we get that Tno (h) ≤
2 · Tno (h − 1) + 1
2Tyes (h − 1) + 1
2Tyes (h − 1).
Proof of Theorem 3.1. Lemma 3.2 gives us that

Tyes (h)
Tno (h)

≤

2.25 0.5
1 2  Tyes (h − 1)
Tno (h − 1)

≤

2.25 0.5
1 2 h
1
1

.
A diagonalization of the matrix yields

2.25 0.5
1 2 
= Q−1 JQ,
where
Q−1 =

1−
√
33
8
1−
√
33
8
1 1


J =

17−
√
33
8 0
0 17+
√
33
8


Q =

− 4√
33
1
2 + 1
2
√
33
4√
33
1
2 − 1
2
√
33


,
and therefore

Tyes (h)
Tno (h)

≤
 0.065 · 1.407h + 0.94 · 2.8431h
−0.109 · 1.407h + 1.109 · 2.8431h

.
Thus, T (h) = O(2.8431h ), which proves the theorem.
3.2 A Faster Algorithm for Ternary Trees
Here we discuss the subtree isomorphism problem for rooted ternary trees. We prove Theorem 1.3
by showing that Subtree isomorphism for rooted ternary trees of height h can be solved in expected
ACM Transactions on Algorithms, Vol. 14, No. 3, Article 27. Publication date: June 2018.                     
27:16 A. Abboud et al.
Fig. 5. The decision tree used for bipartite matching in the degree 3 case.
time O(6.107h ). Just as with the binary case, this running time is lower than the runtime given by
our generic algorithm for constant degree trees in Section 3.3.
Similarly to the binary case, the proof of the theorem proceeds by a recursive approach. In each
recursive call, we consider a randomized decision tree for 3 × 3 bipartite perfect matching, where
each query corresponds to a recursive call on height one less. We then analyze the runtime similar
to the binary tree case: We distinguish between the “yes” and “no” cases of the query answer and
write the running time as two recurrences, one for Tyes , when the algorithm said the trees are
isomorphic, and one for Tno , when they were not. We analyze the randomized decision tree in
terms of the expected number of “yes” and “no” query answers in the worst case.
The randomized query protocol is as follows. Let U and V be the two partitions of the bipartite
matching instance (respectively, U are the subtrees of the root of one tree and V are the subtrees
of the root of the other). First, we pick U or V at random w.p. 1/2. If we pick V , then the names
of U and V are swapped. Now, with probability 1/6, we pick a permutation of the vertices in U ,
and with probability 1/6 we pick a permutation of V . After these two permutations are fixed, the
protocol is deterministic. Let a,b,c be the nodes of U and x,y, z be the nodes of V , in the order of
the chosen permutations. The deterministic decision tree we use is depicted in Figures 5 and 6.
For each of the 29 choices for the answers to the 9 edge queries in the 3 × 3 matching instance,
we consider each of the 72 randomized choices as described above (swap U and V , permute U and
V ) and consider the decision tree, computing the expected number of “yes” and “no” calls. Using
a computer program, we establish that when the instance has no perfect matching, the expected
number of “yes” calls is always at most 26/9, and the expected number of “no” calls is always at
ACM Transactions on Algorithms, Vol. 14, No. 3, Article 27. Publication date: June 2018.
Subtree Isomorphism Revisited 27:17
Fig. 6. The missing subtrees of the decision tree used for bipartite matching in the degree 3 case.
most 37/9; this happens when the complement of the graph consists of a 4-cycle, disjoint from a
single edge. However, if the instance has a perfect matching, then there are two cases that dominate
all others: when the expected number of “yes” calls is 131/36, and the expected number of “no”
calls is 61/36, or when the expected number of “yes” calls is 133/36, and the expected number of
“no” calls is 5/3.
Let us first consider the case in which whenever a perfect matching exists, the expected number
of “yes” calls is at most 133/36, and the expected number of “no” calls is 5/3. We will then show
that any mix of the two options (131/36 “yes” and 61/36 “no” or 133/36 “yes” and 5/3 “no”) is no
worse. The recurrence becomes:

Tyes (h)
Tno (h)

≤

133/36 5/3
26/9 37/9
 Tyes (h − 1)
Tno (h − 1)

≤

133/36 5/3
26/9 37/9
h
1
1

The diagonalization yields

133/36 5/3
26/9 37/9

= Q−1 JQ,
where
J =

281−
√
25185
72 0
0 281+
√
25185
72


Q =

1
2 +
√
25185
3358
1
2 −
√
25185
3358
− 104
√
25185
104
√
25185


,
which gives that

Tyes (h)
Tno (h)

≤

0.17 · 1.7h + 0.831 · 6.107h
−0.2 · 1.69h + 1.21 · 6.107h

.
Thus, the running time overall isO(6.107h ), provided that whenever there is a perfect matching,
the expected number of “yes” calls is 133/36 and the number of “no” calls is 5/3. Let us now show
ACM Transactions on Algorithms, Vol. 14, No. 3, Article 27. Publication date: June 2018.            
27:18 A. Abboud et al.
by induction that any mixture of two options (131/36 “yes” and 61/36 “no” or 133/36 “yes” and 5/3
“no”) is no worse.
Consider any c > 0 and c = 1.4475 ·c. Suppose that for h ≥ 0,Tyes (h) ≤ c · 6.107h andTno (h) ≤
c · 6.107h. (The base case h = 0 holds for some choice of a constant c > 0.) Let us consider
h + 1. By our choice of c
, Tno (h + 1) ≤ 6.107h ·c · (26/9 + 1.4475 · 37/9) ≤ 6.107h+1 ·c
. For the
“yes” case, if the recursion is 131/36 “yes” and 61/36 “no,” then we have Tyes (h + 1) ≤ 131/36 ·
c · 6.107h + 61/36 ·c · 6.107h = c · 6.107h (131/36 + 61/36 · 1.4475) ≤ c · 6.107h+1. If the recusion
is instead 133/36 “yes” and 5/3 “no,” then we have Tyes (h + 1) ≤ 133/36 ·c · 6.107h + 5/3 ·c ·
6.107h = c · 6.107h (133/36 + 5/3 · 1.4475) ≤ c · 6.107h+1.
3.3 An Algorithm for Any Constant Degree
In this section, we describe a way to use randomization to save subtree comparisons in the
Edmonds-Matula algorithm [45] for all degrees d > 3. Recall that the algorithm works as follows.
Given two trees H and G of constant degree d, the goal is to decides whether H is isomorphic to
a subtree of G by using recursion. If the roots of either H or G have less than d children, then we
simply view the missing subtrees as being a special empty subtree.
(1) Let H1,...,Hd be the d subtrees of H, and let G1,...,Gd be the d subtrees of G;
(2) Build a bipartite graph G with d vertices U = {u1,...,ud } on the left and d vertices W =
{v1,...,vd } on the right. For all i, j ∈ [d], connect ui and vj if and only if Hi is isomorphic
to a subtree of Gj . We decide which edges appear in the graph recursively.
(3) Output that H is isomorphic to a subtree of G if and only if there is a perfect matching in
the bipartite graph G.
The runtime of the algorithm isO(min{d2h,n2}), where h is the height. Intuitively, we can improve
the runtime of the algorithm as follows. Perform recursive calls corresponding to edges (ui,vj) in
a random order, and stop as soon as we either detect a perfect matching or rule out the existence
of a perfect matching. It is not difficult to show that this randomized version of the algorithm
performs d2 − Ω(1) recursive calls in expectation out of the d2 possible calls. That is, in expectation,
we save at least a constant number of recursive calls. This implies that the algorithm runs in
O((d2 − Ω(1))h ) expected time, which is faster than the deterministic algorithm. However, we
prove below that we can save Ω(d) recursive calls in expectation using a slightly different variant
of the randomized algorithm.
Lemma 3.3. Let G be a bipartite graph with d vertices U = {u1,...,ud } on the left and d vertices
W = {v1,...,vd } on the right, and suppose we are given query access to the adjacency matrix of
G. There is a randomized query algorithm that decides whether G contains a perfect matching by
making d2 − 1
3d + 2
3 queries in expectation, with probability 0 of making an error.
We use the following two claims to prove the lemma.
Claim 4. Assume that G has a perfect matching. Then the following algorithm finds a perfect
matching after making d2 − d + 2 expected queries: Query edges (ui,vj) in a random order, and stop
when finding a perfect matching.
Proof. Fix a perfect matching present in G and call its d edges “marked.” We stop when all
marked edges have been queried. There are d2 − d unmarked edges. Consider the d marked edges
in a random order of all d2 edges. By symmetry, the probability that any fixed unmarked edge is
before the first marked edge is the same as the probability that it is after the last marked edge
(and not queried) and is the same as the probability that it is after the ith and before the (i + 1)-
st marked edge (i = 1,...,d − 1). Since there are d + 1 equal probablities that must sum to 1, the
ACM Transactions on Algorithms, Vol. 14, No. 3, Article 27. Publication date: June 2018.
Subtree Isomorphism Revisited 27:19
probability that a given unmarked edge is not queried is ≥ 1
d+1 . Therefore, the expected number
of unqueried, unmarked edges is ≥ d2−d
d+1 ≥ d − 2.
Claim 5. Assume that G does not have a perfect matching. Then the following algorithm makes
at most d2 − 1
2d + 1 queries in expectation before determining that G does not contain a perfect
matching.
(1) With probability 1/2 swap U and W;
(2) Randomly permute the vertices of U = {u1,u2,...,ud };
(3) Query all edges adjacent to ui for i going from 1 to d, but stop when ruling out the existence
of a perfect matching, i.e., stop when the set of processed vertices S = {u1,...,ui} contains a
subset S with a neighbourhood N (S
) that is smaller than the size of S
.
Proof. Consider the sets U and W prior to running the algorithm. By Hall’s theorem, the set
U contains a set S such that |N (S
)| < |S
|. We can assume that |S
| = |N (S
)| + 1, since otherwise
we can iteratively remove a vertex from S until this condition is satisfied. Consider two cases.
• d is even: If |S
| ≥ d
2 + 1, then we define T  = W \ N (S
). Because N (S
) ≥ d/2, we get that
|T 
| ≤ d
2 . By our construction ofT 
, we have that N (T 
) ⊆U\ S and, as a result, |N (T 
)| <
|T 
|. Given the first step of the algorithm, with probability at least 1/2 the set U therefore
contains a set S such that |N (S
)| < |S
| ≤ d
2 .
• d is odd: It follows from as similar argument that, with probability at least 1/2, the set U
contains a set S such that |N (S
)| < |S
| ≤ d+1
2 .
We now condition on the set U containing S with |S
| ≤ d+1
2 and |N (S
)| < |S
|.
The algorithm stops once it queries all vertices from S
, since a perfect matching is then ruled
out by Hall’s theorem. The probability that we do not process a given vertex before processing
all vertices in S is 1/(|S
| + 1). Therefore, the expected number of unprocessed vertices when the
algorithm stops is at least
(d − |S
|) · 1
|S| + 1
≥
d − 1
2 · 1
d+1
2 + 1 = d − 1
d + 3
.
Hence, with probability 1/2, we queryd(d − d−1
d+3 ) edges, and overall the number of queried edges
is
1
2

d

d − d − 1
d + 3

+
1
2
d2 = d2

1 − 1 − 1
d
2(d + 3)


≤ d2 − 1
2
d + 1.
In the last inequality we use that d ≥ 3.
Proof of Lemma 3.3. We prove the lemma by using claims 4 and 5.
With probability 1/3, we run the algorithm from Claim 4 and with probability 2/3 we run the
algorithm from Claim 5. Consider the case when G has a perfect matching. Then the expected
number of edges queried is upper bounded by
1
3
(d2 − d + 2) +
2
3
d2 = d2 − 1
3
d +
2
3
.
However, for the case when G does not contain a perfect matching, the expected number of edges
queried is upper bounded by
1
3
d2 +
2
3

d2 − 1
2
d + 1

= d2 − 1
3
d +
2
3
.
Overall, regardless of G, we therefore query at most d2 − 1
3d + 2
3 edges in expectation.
ACM Transactions on Algorithms, Vol. 14, No. 3, Article 27. Publication date: June 2018.      
27:20 A. Abboud et al.
Theorem 3.4. There is a randomized algorithm that solves Subtree Isomorphism on two rooted
trees of size O(n), constant degree d, and height at most h in expected time O((d2 − 1
3d + 2
3 )
h ). In
particular, the algorithm is strongly subquadratic for trees of height
h ≤

2 logd
log(d2 − 1
3d + 2
3 )
− ϵ

· logd n,
for any constant ϵ > 0.
Proof. We run the following randomized, recursive algorithm that decides whether H is isomorphic to a subtree of G.
(1) Let H1,...,Hd be the d subtrees of H, and let G1,...,Gd be the d subtrees of G;
(2) Let G be a bipartite graph with d vertices U = {u1,...,ud } on the left and d vertices
W = {v1,...,vd } on the right. For all i, j ∈ [d], let ui and vj be connected if and only if
Hi is isomorphic to a subtree of Gj .
(3) Decide whether the graph G has a perfect matching by running the algorithm from
Lemma 3.3. Whenever we need to decide whether an edge (ui,vj) is present in G, do
it recursively.
By the proof of Lemma 3.3, it suffices to query d2 − 1
3d + 2
3 edges in expectation for every level.
Given that the height of the trees is upper bounded by h, we get the desired running time by
applying Wald’s equation to every vertex of the tree.
APPENDIX
A ANALYSIS OF THE EDMONDS-MATULA ALGORITHM AND ITS VARIANTS
Lemma A.1. On binary trees, the Edmonds-Matula algorithm takes O(mn) time, where m =
|H|, n = |G|.
Proof. Denote by mL,mR,nL,nR the sizes of HL,HR,GL,GR, the left and right subtrees of H
and G, notice that mL + mR = m − 1, nL + nR = n − 1. The runtime of the algorithm is described
by the recurrence
T (0,n) = T (m, 0) = 1,
T (m,n) = 1 +T (mL,nL ) +T (mR,nR ) +T (mL,nR ) +T (mR,nL ).
Then, by induction, we prove T (m,n) ≤ mn,
T (m,n) = 1 +T (mL,nL ) +T (mR,nR )
+T (mL,nR ) +T (mR,nL )
≤ 1 + mL · nL + mR · nR + mL · nR + mR · nL
= 1 + (mL + mR ) · (nL + nR )
= 1 + (m − 1)(n − 1)
≤ mn.
As mentioned in Section 3, this algorithm is easily extended to solve the labelled version of
the problem or the Largest Common Subtree problem for any constant bounded degree d = O(1).
For completeness, we include pseudo-code of a variant that solves the Labelled Largest Common
Subtree problem, generalizing both.
Lemma A.2. Algorithm 1 solves the Labelled Largest Common Subtree problem in time O(mn)
for rooted trees H,G of bounded degree d = O(1), where m,n are the sizes of H,G respectively.
ACM Transactions on Algorithms, Vol. 14, No. 3, Article 27. Publication date: June 2018.    
Subtree Isomorphism Revisited 27:21
ALGORITHM 1: LLCS (H,G,d)
if Size (F ) = 0 or Size (G) = 0 then
return 0
end if
for i = 1 to d do
for j = 1 to d do
if Label(H.Children[i]) = Label(G.Children[j]) then
Sub[i, j] ← LLCS (Subtree (H.Children[i]), Subtree (G.Children[j]), d)
else
Sub[i, j] ← 0
end if
end for
end for
w ← the weight of a maximum weight bipartite matching in the bipartite graph with
edges defined by Sub[i, j].
return w + 1
Proof. Correctness is straightforward, it is also clear that as d = O(1), Algorithm 1 makes a
constant number of operations excluding the recursive calls. Denote by m1,m2,...,mr the sizes
of the (maximal) subtrees rooted at the r ≤ d children of the root of H, and by n1,n2,...,ns the
sizes of those rooted at the s ≤ d children of the root of G. It holds that r
i=1mi = m − 1 and
s
j=1 nj = n − 1. The runtime of the algorithm is described by the recurrence
T (0,n) = T (m, 0) = 1,
T (m,n) = 1 +
	r
i=1
	s
j=1
T (mi,nj).
Then, by induction, we prove T (m,n) ≤ mn,
T (m,n) = 1 +
	r
i=1
	s
j=1
T (mi,nj)
≤ 1 +
	r
i=1
	s
j=1
mi · nj
= 1 +

	r
i=1
mi


·



	s
j=1
nj

	

= 1 + (m − 1)(n − 1)
≤ mn.    