Properties definable in first-order logic are algorithmically interesting for both theoretical and pragmatic
reasons. Many of the most studied algorithmic problems, such as Hitting Set and Orthogonal Vectors, are
first-order, and the first-order properties naturally arise as relational database queries. A relatively straightforward algorithm for evaluating a property with k + 1 quantifiers takes timeO(mk ) and, assuming the Strong
Exponential Time Hypothesis (SETH), some such properties require O(mk−ϵ ) time for any ϵ > 0. (Here, m
represents the size of the input structure, i.e., the number of tuples in all relations.)
We give algorithms for every first-order property that improves this upper bound to mk /2Θ(√log n)
, i.e.,
an improvement by a factor more than any poly-log, but less than the polynomial required to refute SETH.
Moreover, we show that further improvement is equivalent to improving algorithms for sparse instances of the
well-studied Orthogonal Vectors problem. Surprisingly, both results are obtained by showing completeness of
the Sparse Orthogonal Vectors problem for the class of first-order properties under fine-grained reductions.
To obtain improved algorithms, we apply the fast Orthogonal Vectors algorithm of References [3, 16].
While fine-grained reductions (reductions that closely preserve the conjectured complexities of problems)
have been used to relate the hardness of disparate specific problems both within P and beyond, this is the
first such completeness result for a standard complexity class.
CCS Concepts: • Theory of computation → Computational complexity and cryptography; Problems, reductions and completeness;
Additional Key Words and Phrases: Fine-grained complexity, first-order model checking, orthogonal vectors
1 INTRODUCTION
Fine-grained complexity aims to make complexity theory more relevant to algorithm design (and
vice versa) by giving reductions that better preserve the times required for solving problems, and
connecting algorithmic progress with complexity theory. While some of the key ideas can be traced
back to parameterized algorithms and complexity [18, 20], studies of the exact complexity of NPcomplete problems [22–24, 29], and algorithmic consequences of circuit lower bounds [6, 8, 21, 25,
26, 28, 37], the full power of this approach has emerged only recently. This approach has given
us new circuit lower bounds [32, 34], surprising algorithmic improvements using circuit lower
bound techniques [3, 15, 16, 31], and many new insights into the relative difficulty of substantially improving known algorithms for a variety of problems both within and beyond polynomial
time.
One of the strengths of this approach also makes it seemingly more complicated. Fine-grained
reductions often cut across traditional complexity hierarchies; for example, many results use a
now-standard reduction from the NP-complete SAT problem down to the first-order definable
(a.k.a. uniform AC0) orthogonal vectors problem. (Counterintuitively, this reduces a very hard
problem to a problem in an extremely simple complexity class). However, different complete problems for the same complexity class can have different time complexities, meaning there may not
be fine-grained reductions between them (or at least, that such reductions can be highly nontrivial.) Thus far, fine-grained complexity has remained focused on specific problems, rather than
organizing problems into classes as in traditional complexity. As the field has grown, many fundamental relationships between problems have been discovered, making the graph of known results
a somewhat tangled web of reductions [1, 2, 5, 9, 11–13, 27, 36].
Here, we give the first results in fine-grained complexity that apply to an entire complexity
class, namely the class of first-order definable properties (the uniform version of AC0). This class
is both algorithmically natural in that it contains many standard problems considered before (such
as Hitting Set and Orthogonal Vectors), and motivated by its importance in logic and database theory. It is not difficult to see that checking whether a property expressible by a first-order formula
with k + 1 quantifiers holds on a given structure with m records can be done in O(mk ) time, and if
Strong Exponential Time Hypothesis (SETH) is true, there are such properties that requiremk−o(1)
time to decide.1 For k = 1, this is linear time and so cannot be improved. For each such problem
with k ≥ 2, we give an algorithm that solves it in mk /2Θ(√logm) time. (This improves the standard
algorithm by a factor more than any poly-log, but less than the polynomial improvement needed
to refute SETH.) Moreover, we show that any further improvement is equivalent to a similar algorithmic improvement for the well-studied Orthogonal Vectors problem. Surprisingly, both results
are obtained by showing that (a version of) the Orthogonal Vectors problem iscomplete under finegrained reductions for the class of all first-order properties. This is the first completeness result
for a previously studied complexity class under fine-grained reducibility. To obtain the algorithmic
results, we then apply the counter-intuitive algorithm for the Orthogonal Vectors problem of [3,
16], which uses techniques from circuit lower bounds.
In addition to introducing new algorithms and giving completeness results, our results clarify
and simplify our understanding of “complexity within P.” For many of the known SETH-hard problems of interest such as Edit Distance [9], Longest Common Subsequence [1, 5, 13], Dynamic Time
Warping [1, 13], Fréchet Distance [12], Succinct Stable Matching [27], and so on, the reduction from
SAT passes through the Orthogonal Vectors problem. Thus, if any of these SETH-hard problems
1Informally, SETH is a hypothesis that CNF SAT cannot be solved substantially faster than 2n time; see the Preliminaries
for a formal statement.
ACM Transactions on Algorithms, Vol. 15, No. 2, Article 23. Publication date: December 2018.
Completeness for FO Properties on Sparse Structures 23:3
Fig. 1. A diagram of reductions. We simplify this picture and make the reductions to Edit Distance, LCS, and
so on, more meaningful.
had substantially improved algorithms, all first-order properties would have similarly improved algorithms. Thus FOPC, the hypothesis that some first-order property does not have a substantially
faster algorithm, is a useful intermediary between SETH and many of its consequences. FOPC
is both equivalent to conjectures concerning many of the previously studied problems [11], and
potentially more plausible to SETH-skeptics since it concerns an entire complexity class, while
having most of the consequences of SETH. This is summarized in Figure 1. (See Section 2.3 for
definitions of problems.)
While we concentrate on the general picture of complexity classes, even special cases of our
results for specific problems are of interest. There were no similarly improved algorithms for
Orthogonal Vectors with small total Hamming weight (Sparse OV) or related problems such as
Sperner Family and 2-Set Cover (in the sparse high-dimensional case), and it was not previously
known that the sparse versions of these problems were equivalent.
In addition to having a natural and useful complete problem, the class of first-order properties is
important in itself. This class includes many problems studied in the fine-grained complexity literature such as Hitting Set, Orthogonal Vectors, Sperner Family, Diameter 2, Radius 2, k-Independent
Set, k-Dominating Set, and so on. First-order properties are also extensively studied in complexity,
logic (especially finite model theory and theory of databases) and combinatorics. Algorithms for
model-checking first-order properties are inherent in databases (the core of the relational database
language SQL is equivalent to first-order properties). Roughly speaking, first-order properties are
essentially the uniform version of AC0 in the complexity literature [10].
Since fine-grained complexity is concerned with exact time complexities (distinguishing,
e.g., n1.9 time from n2 time), the problem representation is significant. For graph problems, there
are two standard representations: adjacency lists (which are good for sparse graphs), in which running time is analyzed with respect to the number of edges m, and adjacency matrices (good for
dense graphs), in which the runtime is a function of the number of vertices, n. For several reasons,
we use the sparse adjacency list (list of tuples) representation. First, many of the problems considered such as Orthogonal Vectors are hard already on sparse instances. Second, the complexity of
first-order problems in the dense model is somewhat unclear, at least for numbers of quantifiers
between 3 and 7 [33]. Third, the sparse model is more relevant for first-order model checking, as
databases are represented as lists of records.
ACM Transactions on Algorithms, Vol. 15, No. 2, Article 23. Publication date: December 2018.
23:4 J. Gao et al.
1.1 First-order Properties
The problem of deciding whether a structure satisfies a logical formula is called the model checking
problem. In relational databases, first-order model checking plays an important role, as first-order
queries capture the expressibility of relational algebra. In contrast to the combined complexity,
where the database and query are both given as input, the data complexity measures the running
time when the query is fixed. The combined complexity of first-order queries is PSPACE-complete,
but the data complexity is in LOGSPACE [30]. Moreover, these problems are also major topics
in parameterized complexity theory. In Reference [19], Flum and Grohe organize parameterized
first-order model-checking problems (many of which are graph problems) into hierarchical classes
based on their quantifier structures. Here, we study model checking from the fine-grained complexity perspective.
More specifically, let φ be a fixed first-order sentence containing free predicates of arbitrary
constant arity (and no other free variables). For example, the k-Orthogonal Vectors (k-OV) problem
can be expressed by a (k + 1)-quantifier formulaφ = (∃v1 ∈ A1) ... (∃vk ∈ Ak )(∀i)[
k
j=1 ¬(vj[i] =
1)]. The model-checking problem forφ, denoted MCφ, is deciding whetherφ is true on a given input
structure interpreting predicates in φ (e.g., given k sets of vectors, decide k-OV). We sometimes
refer to structures as “hypergraphs” (“graphs” when all relations are unary or binary), and relations
as edges or hyperedges. We use n to denote size of the universe of the structure and m the total
number of tuples in all its relations (size of the structure). Many graph properties such as k-clique
have natural first-order representations, and set problems such as Hitting Set are representable in
first-order logic using a relation R(u, S) ≡ (u ∈ S).
We use notation MC(Φ) for a class of model-checking problems for φ ∈ Φ, with the main focus on classes of (k + 1)-quantifier φ with k ≥ 1 (denoted MC(k + 1)) and restrictions of this
class to specific quantifier prefixes (e.g., MC(∃∃∀) for 3-quantifier φ with quantifier prefix ∃∃∀
when written in prenex normal form). For formal definitions and more examples see Sections 2.2
and 2.3.
We propose the following conjecture on the hardness of model checking of first-order properties.
First-order property conjecture (FOPC): There is an integer k ≥ 2, so that there is a (k + 1)-
quantifier first-order property that cannot be decided in O(mk−ϵ ) time, for any ϵ > 0.
1.2 Orthogonal Vectors
In the Orthogonal Vectors (OV) problem, we are given a set A of n Boolean vectors of dimension d,
and must decide if there are u,v ∈ A such that u and v are orthogonal, i.e., u[i] · v[i] = 0 for all
indices i ∈ {1,...,d}. Another (equivalent) version is to decide with two sets A and B of Boolean
vectors whether there are u ∈ A and v ∈ B so that u and v are orthogonal. A naïve algorithm for
OV runs in time O(n2
d), and the best known algorithm runs in n2−Ω(1/ log(d/ log n)) [3, 16].
In this article, we introduce a version of OV we call the Sparse Orthogonal Vectors (Sparse OV)
problem, where the input is a list of m vector-index pairs (v,i) for each v[i] = 1 (corresponding to
the adjacency list representation of graphs) and complexity is measured in terms of m; we usually
consider m = O(n1+γ ) for some 0 ≤ γ < 1. The popular hardness conjectures on OV restrict the
dimension d to be between ω(logn) (low dimension) and no(1) (moderate dimension); however in
Sparse OV we do not restrict d.
We thus identify three versions of Orthogonal Vector Conjecture, based on the size of the dimension d. In all three conjectures the complexity is measured in the word RAM model with O(logn)
bit words.
Low-dimension OVC (LDOVC): For all ϵ > 0, there is no O(n2−ϵ ) time algorithm for OV with
dimension d = ω(logn).
ACM Transactions on Algorithms, Vol. 15, No. 2, Article 23. Publication date: December 2018. 
Completeness for FO Properties on Sparse Structures 23:5
Moderate-dimension OVC (MDOVC): For all ϵ > 0, there is noO(n2−ϵpoly(d)) time algorithm
that solves OV with dimension d.
Sparse OVC (SOVC): For all ϵ > 0, there is no O(m2−ϵ ) time algorithm for Sparse OV, where m
is the total Hamming weight of input vectors.
It is known that SETH implies LDOVC [31]. Because MDOVC is a weakening of LDOVC, it
follows from the latter.2 Like LDOVC, MDOVC also implies the hardness of problems including
Edit Distance, LCS, and so on. Here we further show that MDOVC and SOVC are equivalent (see
Lemma 1.1).
OV can be extended to the k-OV problem for any integer k ≥ 2: given k sets A1,...,Ak of
Boolean vectors, determine if there are k different vectors v1 ∈ A1,...,vk ∈ Ak so that for all
indices i,
k
j=1 vj[i] = 0 (that is, their inner product is 0). We naturally define a sparse version of
k-OV similar to Sparse OV, where all ones in the vectors are given in a list.
1.3 Main Results
Completeness. First, we show that conjectures for OV defined on dense (moderate-dimension)
and sparse models are equivalent under fine-grained reductions, which means MDOVC is true iff
SOVC is true (see Lemma 6.2). This also holds for k-OV.
Lemma 1.1. For any integer k ≥ 2, there exist δ, ϵ > 0 and a O(nk−ϵ ) time algorithm solving k-OV
with dimension d = nδ , if and only if there is an ϵ 	 > 0 and a O(mk−ϵ	
) time algorithm for Sparse
k-OV, where m is the total Hamming weight of all input vectors.
Our main result establishes an equivalence of MDOVC and FOPC, showing the completeness of
Sparse OV and hardness of (dense) OV for the class of first-order property problems.
Theorem 1.2. MDOVC, SOVC, and FOPC are equivalent.
This article also proves a hardness and completeness result for k-OV, connecting one combinatorial problem to a large and natural class of logical problems. The following theorem states that
Sparse k-OV is complete for MC(∃k∀) (and its negation form MC(∀k∃)), and hard for MC(∀∃k−1∀)
(and its negation form MC(∃∀k−1∃)) under fine-grained reductions.
Theorem 1.3. If Sparse k-OV with total Hamming weight m can be solved in time O(mk−ϵ ) for
some ϵ > 0, then all problems in MC(∃k∀), MC(∀k∃), MC(∀∃k−1∀), and MC(∃∀k−1∃) are solvable
in time O(mk−ϵ	
) for some ϵ 	 > 0.
MC(∃k∀) and MC(∀k∃) are interesting sub-classes of MC(k + 1): if Nondeterministic SETH is
true, then all SETH-hard problems in MC(k + 1) are contained in MC(∃k∀) or MC(∀k∃) ([14]).
We will also show that the 2-Set Cover problem and the Sperner Family problem, both in
MC(∃∃∀), are equivalent to Sparse OV under fine-grained reductions and thus complete for firstorder properties under fine-grained reductions.
Algorithmic results. Combining our reductions with the surprisingly fast algorithm for Orthogonal Vectors by References [3, 16], we obtain improved algorithms for every problem representable
as a (k + 1)-quantifier first-order property.
Theorem 1.4. There is an algorithm solving MC(k + 1) in time mk /2Θ(√logm)
.
Let us consider the above results in context with prior work on the fine-grained complexity
of first-order properties. In Reference [33], Ryan Williams studied the fine-grained complexity of
2Although dimension d is not restricted, we call it “moderate dimension” because such an algorithm only improves on the
naive algorithm if d = nO (ϵ )
.
ACM Transactions on Algorithms, Vol. 15, No. 2, Article 23. Publication date: December 2018. 
23:6 J. Gao et al.
dense instances of first-order graph properties. He gave an nk+o(1)
-time algorithm for MC(k + 1)
on graphs when k is a sufficiently large constant, and showed that MC(k + 1) requires at least
nk−o(1) time under SETH. His algorithms only apply to graphs (they look difficult to generalize to
even ternary relations), and it seems difficult to point to a specific simple complete problem in this
setting. To compare, our results show that the sparse case of MC(k + 1) (for allc-ary relations, for
all constants c) is captured by very simple problems (e.g., sparse Orthogonal Vectors), which also
leads to an algorithmic improvement for all c-ary relations.
1.4 Organization of This Paper
Section 1 introduced the motivation, some definitions and statements of the main results. In Section 2, we give the formal definitions of the reductions we use, as well as detailed definitions of
first-order properties, with first-order representations of common problems in fine-grained complexity. We present a general outline of the proofs in Section 3, and a high-level explanation of key
techniques in Section 4.
The full proof starts with the reduction from MC(∃k∀) to k-OV (Section 5), with randomized
universe-shrinking self-reduction described in Section 5.1, which is then derandomized in Section 6. Section 8 presents the reduction from MC(∀∃k−1∀) to k-OV, and Section 10 discusses the
mk /2Θ(√logm) time algorithm for Sparse OV and, therefore, MC(k + 1). We conclude with open
problems in Section 11.
2 PRELIMINARIES
2.1 Reductions
To establish the relationship between complexities of different problems, we use the notion of
fine-grained reductions as defined in Reference [36]. These reductions establish conditional hardness results of the form “If one problem has substantially faster algorithms, then so does another
problem.” We will also use exact complexity reductions (see definition 2.2), which strengthen the
above claim to “if one problem has algorithms improved by a factor s(m), then another problem
can be improved by a factor sc (m)” for some constant c. (Note that some fine-grained reductions
already have this property.) The underlying computational model is the Word RAM with O(logn)
bit words.
Definition 2.1 (Fine-grained reduction (≤FGR)) 3. Assume that L1 and L2 are languages andT1 and
T2 are their conjectured running time lower bounds, respectively, where constant factors may be
omitted. Then we say (L1,T1) ≤FGR (L2,T2) if for every ϵ > 0, there exists ϵ 	 > 0, and an algorithm
AL1 for L1 that runs in time T1 (n)
1−ϵ	
on inputs of length n, making q calls to an oracle for L2 with
query lengths n1,...,nq, where q
i=1 (T2 (ni ))1−ϵ ≤ (T1 (n))1−ϵ	
.
Thus, if L2 has an algorithm substantially faster than T2, L1 can be solved substantially faster
than T1.
4
3To be more precise, this is the fine-grained Turing reduction. For the mapping reductions that preserve fine-grained
complexity, we refer to them as fine-grained mapping reductions.
4In almost all fine-grained reductions, T1 ≥ T2, that is, we usually reduce from harder problems to easier problems, which
may seem counter-intuitive. A harder problem L1 can be reduced to a easier problem L2 withT1 > T2 in two ways: by making multiple calls to an algorithm solving L2 and/or by blowing up the size of the L2 instance (e.g., the reduction from CNFSAT to OV [31]). All reductions from higher complexity to lower complexity problems in this article belong to the first type.
Actually, it is harder to fine-grained reduce from a problem with lower time complexity to a problem with higher
time complexity (e.g., prove that (MC(k), mk−1) ≤FGR (MC(k + 1), mk )), because this direction often needs creating
instances with size much smaller than the original instance size.
ACM Transactions on Algorithms, Vol. 15, No. 2, Article 23. Publication date: December 2018.
Completeness for FO Properties on Sparse Structures 23:7
To simplify transferring algorithmic results, we define a stricter variant of fine-grained reductions, which we call exact reductions. These reductions satisfy a stronger reducibility notion.
Definition 2.2 (Exact complexity reduction (≤EC)). Let L1 and L2 be languages and letT1,T2 denote
time bounds. Then (L1,T1) ≤EC (L2,T2) if there exists an algorithmAL1 for L1 running in timeT1 (n)
on inputs of length n, making calls to oracle of L2 with query lengths n1,...,nq, where q is the
number of calls and q
i=1 T2 (ni ) ≤ T1 (n).
That is, if L2 is solvable in time T2 (n), then AL1 solves L1 in time T1 (n).
2.2 Model Checking for First-order Logic
Let R1,..., Rr be predicates of constant arities a1,..., ar (a vocabulary). A finite structure over the
vocabulary R1,..., Rr consists of a universeU of size n together with r lists, one for every Ri , ofmi
tuples of elements fromU on which Ri holds. Letm = r
i=1mi ; viewing the structure as a database,
m is the total number of records in all tables (relations).
We loosely use the term hypergraph to denote an arbitrary structure; in this case, we refer to its
universe as a set of vertices V = {v1,...,vn } and call tuples (v1,...,vai ) such that Ri (v1,...,vai )
holds hyperedges (labeled Ri ). A set of all Ri-labeled hyperedges in a given hypergraph is denoted by ERi or just Ei ; the structure is denoted by G = (V, E1,..., Er ). Similarly, we use the term
graph for structures with only unary and binary relations (edges); here, we mean edge-labeled
vertex-labeled directed graphs with possible self-loops, as we allow multiple binary and unary
relations and relations do not have to be symmetric. This allows us to use graph terminology
such as a degree (the number of (hyper)edges containing a given vertex) or a neighbourhood of a
vertex.
Let φ be a first-order sentence (i.e., formula without free first-order variables) containing predicates R1,..., Rr . Let k be the number of quantifiers in φ. Without changing k, we can write φ in
prenex form. The model-checking problem for a first-order property φ, MCφ, is: given a structure
(hypergraph)G, determine whether φ holds onG (denoted byG |= φ). For a class of formulas Φ, we
use the notation MC(Φ) for a class of model-checking problems for φ ∈ Φ, with shortcuts MC(k)
for Φ = k-quantifier first-order formulas in prenex form, and MC(Q1 ...Qk ) for first-order prenex
formulas with quantifier prefix Q1 ...Qk , with a shortcut Qc
i denoting c consecutive occurrences
of Q (e.g., MC(∃k∀)).
We assume that (hyper)graphs are given as a list of m (hyper)edges, with each hyperedge encoded by listing its elements. In the Word RAM model with O(logn) bit words, the size of an
encoding of a hypergraph is O(n + m) words, and an algorithm can access a hyperedge in constant time. With additionalO(m) time preprocessing, we can compute degrees and lists of incident
edges for each vertex, and store them in a hash table for a constant-time look-up; edges incident
to a vertex can then be listed in time proportional to its degree. We also assume that m ≥ n, with
every vertex incident to some edge, because the interesting instances are in this case. Moreover,
we assume the (hyper)graph is k-partite, where k is the number of variables in φ, so that each variable is selected from a distinct vertex set. From any (hyper)graph, the construction of this k-partite
graph needs a linear time, linear space blowup preprocessing that creates at most k duplicates of
the vertices and k2 duplicates of the edges. Finally, we treat domains of quantifiers as disjoint sets
forming a partition of the universe; any structure can be converted into this form with constant
increase of the universe size. We also view predicates on different variable sets (e.g., R(x1, x2) vs.
R(x2, x4) vs. R(x4, x4)) as different predicates, and partition corresponding edge sets appropriately.
The focus of this article is on sparse structures, that is, the case when m ≤ O(n1+γ ) for some γ
such that 0 ≤ γ < 1. In particular, all Ei are sparse relations; we use the term co-sparse to refer to
complements of sparse relations. We will usually measure complexity as a function ofm. From the
ACM Transactions on Algorithms, Vol. 15, No. 2, Article 23. Publication date: December 2018.
23:8 J. Gao et al.
following baseline algorithm that will be proved in Section 9.1, the sparse assumption is without
loss of generality.
Claim 2.1 (Baseline Algorithm). MC(k + 1) is solvable in time O(nk−1
m).
2.3 Common Problems and Conjectures
In the CNF-SAT problem, given a Boolean formula F in CNF form (conjunction of disjunctions of
(possibly negated) variables), the goal is to determine whether there is an assignment of Boolean
values to variables of F that makes F true. In k-CNF-SAT, every clause (disjunction) can have at
most k literals. We refer to the following conjecture about complexity of solving CNF-SAT:
Strong Exponential Time Hypothesis (SETH)5: For every ϵ > 0, there exists a k ≥ 2 so that
k-CNF-SAT cannot be solved in time O(2n(1−ϵ )
).
Below we list some problems studied in fine-grained complexity, with their first-order definitions on structures with unary and binary relations.
• Graph problems. The input structure isG = (V, E) with a universeV and a binary relation
E.
1. Diameter-2: (∀x1)(∀x2)(∃x3)[E(x1, x3) ∧ E(x3, x2)].
2. Radius-2: (∃x1)(∀x2)(∃x3)[E(x1, x3) ∧ E(x3, x2)].
3. k-Clique: (∃x1) ... (∃xk )[

i,j ∈ {1,...,k },ij E(xi, xj)]. More generally, for a fixed graph H
of k vertices, deciding if H is a subgraph or induced subgraph of the input graph G
(e.g., the k-Independent Set problem) can be expressed in a similar way.
4. k-Dominating Set: (∃x1) ... (∃xk )(∀xk+1)[
k
i=1 E(xi, xk+1)].
• Set problems. The inputs are set families S or S1,...,Sk over a universe U . Here, all
sets are given explicitly and represented by first-order variables. These structures contain
a single binary predicate ∈.
1. Hitting Set:
6 (∃H ∈ S)(∀S ∈ S)(∃x)[(x ∈ H) ∧ (x ∈ S)].
2. k-Set Packing: (∃S1 ∈ S) ... (∃Sk ∈ S) (∀x)[
k
i=1 ((x ∈ Si ) → 
ji (x  Sj))].
3. k-Empty Intersection (k-OV): (∃S1 ∈ S1) ... (∃Sk ∈ Sk )(∀u ∈ U )[
k
i=1 ¬(u ∈ Si )].
4. k-Set Cover: (∃S1 ∈ S1) ... (∃Sk ∈ Sk )(∀u ∈ U )[
k
i=1 (u ∈ Si )].
5. Set Containment: (∃S1 ∈ S1)(∃S2 ∈ S2) (∀u ∈ U )[(¬(u ∈ S1)) ∨ (u ∈ S2)].
k-Empty Intersection is equivalent to k-OV, and Set Containment is equivalent to Sperner Family problem. See Section 1.2 for definitions and conjectures for variants of the Orthogonal Vectors
problem.
The Hitting Set Conjecture states that ∀ϵ > 0 there is no O(n2−ϵ ) time algorithm for the Hitting Set problem with set sizes bounded by d = ω(logn). It implies LDOVC; subquadratic approximation algorithms for Diameter-2 and Radius-2 would, respectively, refute the LDOVC and the
Hitting Set Conjecture [4].
3 OVERVIEW
The main technical part of this article is in the proof of Theorem 1.3 showing hardness of k-OV
for model-checking of ∃k∀ formulas under fine-grained reductions. The idea is to represent ∃k∀
formulas using combinations of basic “k-OV like” problems, each of which is either easy (solvable
substantially faster thanmk time for sparse instances) or can be fine-grained reduced to k-OV. The
5Some define SETH over randomized algorithms instead of deterministic ones.
6Other versions of Hitting Set, where the sets are not given explicitly are second-order logic problems. Our definition here
is consistent with the version in the Hitting Set Conjecture.
ACM Transactions on Algorithms, Vol. 15, No. 2, Article 23. Publication date: December 2018.       
Completeness for FO Properties on Sparse Structures 23:9
Fig. 2. Overview of the reduction process for Theorem 1.2.
latter is achieved using a universe-shrinking self-reduction, which converts a given instance of a
basic problem to a denser instance on a smaller universe, thus reducing Sparse k-OV to k-OV with
dimension nδ and proving Lemma 1.1. Converting an MC(∃k∀) to the “hybrid problem” combining
all 2k basic problems is done for graphs (all relations have arity at most 2); however, we show that
this is the hardest case. Additionally, MC(∀∃k−1∀) is reduced to MC(∃k∀).
In Theorem 1.2 and Theorem 1.4, we consider the class of all k + 1-quantifier first-order properties MC(k + 1), and reduce it to 2-OV, proceeding to use a faster algorithm for 2-OV to speed
up model checking. The first step is to brute-force over first k − 2 quantified variables, reducing
to three-quantifier case at the cost O(nk−2). The quantifier prefix ∃∃∀, with 2-OV and other basic
problems (to be defined in Section 5.1), is the hardest (∃3, ∀∃∃ and their complements are easy, and
the rest reduce to ∃∃∀). Appealing to lemmas in the proof of Theorem 1.3 with k = 2 completes
the proof of Theorem 1.2 (see Figure 2 for details), and applying the OV algorithm in References [3,
16] gives Theorem 1.4.
3.1 Reduction from MC(k + 1) to OV
The following outlines the reduction from any arbitrary problem in MC(k + 1) to OV for any
integer k ≥ 2, thus proving that FOPC implies SOVC. For the other direction of this eqiuvalence,
SOVC implies FOPC because Sparse OV is in MC(3). The equivalence between SOVC and MDOVC
is proven in Lemma 1.1, which in turn follows from Lemma 5.2, Lemma 6.2, and corollary 5.3.
(1) With brute-force over tuples of first k − 2 variables, we reduce from the (k + 1)-quantifier
problem MCφ down to a 3-quantifier problem MCφ	. Thus, improving theO(m2) algorithm
for MCφ	 implies improving the O(mk ) algorithm for MCφ.
(2) If MCφ	 belongs to one of MC(∃∃∃), MC(∀∀∀), MC(∀∃∃), MC(∃∀∀), then we solve it
directly in time O(m3/2), using ideas similar to triangle detection algorithms. If φ	 has
the quantifier structure ∀∃∀ (or its negated form ∃∀∃), then we reduce MCφ	 to MCφ		,
where φ		 has quantifier structure ∃∃∀, using Lemma 8.1. Otherwise, φ	 is already in ∃∃∀
or equivalent form.
(3) We reduce a general model checking problem for φ		 of the quantifier structure ∃∃∀ to a
graph property problem of the same quantifier structure.
(4) Using Lemma 5.6, we reduce formulas of form ∃∃∀ to a “hybrid” problem, which by
Lemma 5.5 can be reduced to a combination of Sparse OV, Set Containment and 2-Set
Cover (which we call Basic Problems).
ACM Transactions on Algorithms, Vol. 15, No. 2, Article 23. Publication date: December 2018.
23:10 J. Gao et al.
(5) We use a “universe-shrinking” technique (Lemmas 5.2, 6.1, and 6.2) on each of the Basic
Problems, to transform a sparse instance into an equivalent one of small dimension.
(6) After applying this to the Hybrid Problem, we can complement edge relations as needed
to transform all Basic Problems into OV (Lemmas 5.4 and 6.3).
(7) By applying the References [3, 16] algorithm to the instance of low-dimension OV, we get
an improved algorithm.
Figure 2 shows a diagram of the above reduction process.
Moreover, Lemmas 8.1, 5.6, 5.5, and 5.1 also work for any constant k ≥ 2. So for a problem in
MC(∃k∀) or MC(∀∃k−1∀), we can reduce it to k-OV as follows:
(1) If the problem belongs to MC(∀∃k−1∀), then reduce it to MC(∃k∀) using Lemma 8.1.
(2) Eliminate hyperedges, then reduce the MC(∃k∀) to the Hybrid problem using Lemma 5.6.
(3) Reduce from the Hybrid Problem to a combination of 2k Basic Problems, using Lemma 5.5.
(4) Reduce all Basic Problems to k-OV, using Lemma 5.1.
This completes the proof of Theorem 1.3.
4 THE BUILDING BLOCKS
Before the formal presentation of the reduction algorithms, this section gives an intuitive and highlevel view of the techniques used to reduce a first-order property problem to OV, in the proofs of
Theorems 1.2, 1.3, and 1.4. Because of Lemma 1.1, in the remainder of this article, unless specified,
we will use “OV” and “k-OV” to refer to sparse versions of these problems.
4.1 Complementing Sparse Relations
Recall the definitions of the k-Empty Intersection, k-Set Cover and Set Containment problems7
from Section 2.3. These problems have very similar structure: given set families S1 ... Sk containing sets over elements of the universe U , each of them asks whether there is a tuple of sets, one
in each family, such that a formula is satisfied for every element u of the universe. Moreover, the
formulas themselves are disjunctions of u ∈ Si or u  Si , with one predicate for each i. The only
difference is the polarity of the ∈ relation (whether or not it is negated). We will refer to these
types of problems as the Basic Problems; they will be our main building blocks.
For k = 2, this gives us four basic problems: Set Disjointness, 2-Set Cover and two versions
of Set Containment (direct and reversed). In each of them, the input consists of two set families S1,S2 of sizes n1, n2, respectively, and the universe U of size nu . The goal is to decide if
there exist sets S1 ∈ S1 and S2 ∈ S2 such that for every u ∈ U , the corresponding formula ψ
holds. Here,  ∈ {00, 01, 10, 11} codes the sequence of polarities of occurrences of ∈. This naturally
generalizes to arbitrary k, with a Basic Problem for each  ∈ {0, 1}
k ; see Section 5.1 for formal
definitions.
That is, Set Disjointness, 2-Set Cover and Set Containment can be stated as follows. Decide if
∃S1 ∈ S1∃S2 ∈ S2∀u ∈ Uψ holds, where ψ is as follows:
Set Disjointness: There is no common element in S1 and S2:ψ = ψ00 = ¬(u ∈ S1) ∨ ¬(u ∈ S2).
2-Set Cover: Union of S1 and S2 covers all of U : ψ = ψ11 = (u ∈ S1) ∨ (u ∈ S2).
7OV is also 2-EI or Set Disjointess: k-EI is equivalent to k-OV, where vectors are represented by sets containing their 1s.
Set Containment is equivalent to the Sperner Family, and k-Set Cover to k-Dominating Set under linear-time reductions.
ACM Transactions on Algorithms, Vol. 15, No. 2, Article 23. Publication date: December 2018.        
Completeness for FO Properties on Sparse Structures 23:11
Set Containment: For S1 ⊆ S2,ψ = ψ01 = ¬(u ∈ S1) ∨ (u ∈ S2). Similarly, in reversed Set Containment with S2 ⊆ S1, ψ = ψ10 = (u ∈ S1) ∨ ¬(u ∈ S2).
All these problems are first-order properties: We can use unary relations to partition the vertex
set into (S1,...,Sk ,U ), and consider the relation “∈” as a binary relation. We will use the context
of hypergraphs to describe the input structure, as in Section 2.2. We let n (corresponding to the
number of vertices in the input graph) be the sum of n1,...,nk and nu , and let the input size m
(corresponding to the number of edges in the input graph) be the sum of all sets’ sizes in all set
families. Borassi et al. [11] showed that when k = 2, these Basic Problems require time m2−o(1)
under SETH, and that if the size of universe U is poly-logarithmic in the input size, then the
three problems are equivalent under subquadratic-time reductions. The main idea of the reductions
between these problems is to complement all sets in S1, or S2, or both. It is easy to see that S1 ∩ S2 =
∅ ⇐⇒ S1
 ∪ S2
 = U ⇐⇒ S1 ⊆ S2
 ⇐⇒ S2 ⊆ S1
. Therefore, if we could complement the sets,
we can easily prove the equivalences between the three Basic Problems. However, we cannot do
this when nu is large.
For a sparse binary relation such as (u ∈ S1), we say that its complement, such as (u  S1), is cosparse. Suppose we want to enumerate all tuples (S1,u) s.t. u ∈ S1; for that, we can go through all
relations (aka edges) between U and S1, which takes time linear in m. On the contrary, if we want
to enumerate all pairs (S1,u) s.t. u  S1, we cannot do this in linear time, because we cannot touch
the pairs by touching edges between them. Moreover, when nu is as large as Ω(n), the number of
such pairs can reach Θ(m2). When k = 2, a fine-grained reduction between O(m2)-time problems
allows neither quadratic time reductions, nor quadratic size problem instances.
Because of the above argument, it is hard to directly reduce between the Basic Problems, so
instead we reduce each problem to a highly asymmetric instance of the same problem, where
sparse relations are easily complemented to relations that are also sparse. Observe that when the
size of universe U is small enough, complementing all sets can be done in time O(m · |U |), which
can be substantially faster than O(m2). The new instance created also has size O(m · |U |), so that
it is only slightly larger than m. So by carefully choosing the size of U, we can construct truly
subquadratic time reduction algorithms that preserve the improved factor in running time. Using
this technique that we call universe-shrinking self-reduction, we can show that OV, 2-Set Cover and
Set Containment are equivalent under fine-grained reductions.
The self-reduction employs the “high-degree low-degree” trick, which has been also used in
other sparse graph algorithms [7]. First, consider sets of large cardinality: there cannot be too
many of them, because the structure is sparse. Thus, we can do exhaustive search over these sets
to check if any of them is in a solution. For sets of small cardinality, we hash the universe U to
a smaller universe, where complementing the sets does not take too much time and space. From
this reduction, the claim follows:
Claim 4.1. If any one of OV, 2-Set Cover and Set Containment has truly subquadratic time algorithms, then the other two are also solvable in subquadratic time. Thus these problems are all hard for
MC(3).
Claim 4.1 is itself an interesting result: In Reference [11], conditional lower bounds for many
problems stem from the above three problems, forming a tree of reductions. By our equivalence,
the root of the tree can be replaced by the quadratic-time hardness conjecture on any of the three
problems, simplifying the reduction tree. Claim 4.1 also shows that an improved algorithm for any
of these three problems implies improved algorithms for the other two.
Claim 4.1 is proven by derandomizing Lemma 5.1 for k = 2; see Section 6 for details. In Section
5, we give randomized reductions for an arbitrary k.
ACM Transactions on Algorithms, Vol. 15, No. 2, Article 23. Publication date: December 2018.        
23:12 J. Gao et al.
4.2 Sparse and Co-sparse Relations
Having shown how to reduce any two Basic Problems with the same k to each other, we will
now reduce generic first-order properties to the Basic Problems. The detailed processes are
complicated, so here we start with a high-level idea in reductions and algorithm design throughout
the article.
Our algorithms often need to iterate over all tuples or pairs (xi, xj) satisfying some conditions,
to list such tuples, or to count the number of them, performing first-order query processing. A
set of such tuples (pairs) (xi, xj) can be considered a result of a first-order query defined by an
intermediate formula φ	 on the (hyper)graph G (or some intermediate structures). Our reduction
algorithms often generate such queries, evaluate them, and combine the results (e.g., by counting)
to compute the solutions.
There are three possible outcomes of such queries: the result can be a sparse set of tuples, a cosparse set, or neither. If the result of the query is a sparse relation such as [R1 (x1, x2) ∧ ¬R2 (x1, x2)],
then we can iterate over the tuples (say, first enumerate all pairs satisfying R1 (x1, x2), then check
for which of them R2 (x1, x2) is false). Then, we can do further operations on the sparse set of
(x1, x2) tuples resulting from the query. When the result of the query is a co-sparse set such as
for [¬R1 (x1, x2) ∧ ¬R2 (x1, x2)], we cannot directly iterate over pairs satisfying the query. Instead,
we work on its complement (which is sparse, instead of co-sparse), but then do some further processing to filter out those pairs from future use (say, find all pairs (x1, x2) so that at least one of
R1 (x1, x2) or R2 (x1, x2) is true, then exclude those pairs from future use). Sometimes, the result
of a query is neither sparse nor co-sparse, but we will show it is always a combination of sparse
and co-sparse relations. Thus, we need to distinguish them and deal with the sparse and co-sparse
parts separately.
We exemplify this process by considering the query [¬R1 (x1, x2) ∨ ¬R2 (x1, x2)]. For a pair
(x1, x2), to make the formula true, predicates R1, R2 can be assigned values from {(True, False),
(False, True), (False, False)}. In the first two cases, the sets of pairs (x1, x2) satisfying [R1 (x1, x2) ∧
¬R2 (x1, x2)] and [¬R1 (x1, x2) ∧ R2 (x1, x2)] are sparse, while in the last case, the set of pairs satisfying [¬R1 (x1, x2) ∧ ¬R2 (x1, x2)] is co-sparse. So if we want to work on the tuples satisfying this
query, we list tuples satisfying the first two cases directly by enumerating edges, and enumerate
the tuples not satisfying the third case (i.e., the tuples, where either R1 (x1, x2) or R2 (x1, x2) is true),
to exclude them from future use.
In general, a query can be written as a DNF, where the result of each term is a conjunction of
predicates and negated predicates, and therefore either sparse or co-sparse. Then we can deal with
the sparse and co-sparse cases separately. We will use this technique for constructing the Hybrid
Problem in Section 5.2.
Now, we would like to reduce MCφ to OV for an arbitrary φ = (∃x)(∃y)(∀z)ψ (x,y, z). First,
suppose that all predicates R1 ... Rr in ψ are at most binary, and all binary predicates involve z.
One attempt is to create a set Sx for each element x and a set Sy for each element y. Then, we
create elements in universe U by creating 2r elements u(z,0r ),...,u(z,1r ) for each z, where r is the
number of different predicates inψ, and the length-r strings in the subscripts correspond to the 2r
truth assignments of all these predicates. We construct the sets so that Sx (or Sy ) contains element
u(z,a) iff the assignment a falsifies ψ and the relations between x (or y) and z agree with a. In this
way, sets Sx and Sy both contain some element u(z,a) in U iff there is some z such that x,y, z do
not satisfy ψ. Then, if there exists such pair of disjoint sets Sx and Sy , the corresponding x and y
satisfy that for all z, ψ is true.
However, we cannot touch all z’s for each x or y for creating this instance in substantially less
than n2 time. So, we divide the relations of this Set Disjointness instance into sparse and co-sparse
ACM Transactions on Algorithms, Vol. 15, No. 2, Article 23. Publication date: December 2018.
Completeness for FO Properties on Sparse Structures 23:13
ones. For that, we introduce a Hybrid Problem that is a combination of Basic Problems. Depending
on the four combinations of sparsity or co-sparsity on the relations between variables x, z and
y, z, we reduce MCφ not only to OV, but to a combination of OV, Set Containment, reversed Set
Containment (i.e., finding S2 ⊆ S1 instead of S1 ⊆ S2), and 2-Set Cover. (Namely, the sub-problem
Set Disjointness deals with the case, where the relations between x and z and between y and z are
both sparse; the sub-problems Set Containment, reversed Set Containment and 2-Set Cover deals
with the cases, where these relations are sparse and co-sparse, co-sparse and sparse, co-sparse and
co-sparse, respectively.) We decide if there is a pair of sets being the solutions of all sub-problems.
Finally, because these Basic Problems can be reduced to each other, we can use the algorithm for
OV to solve the instance of the Hybrid Problem, and then to solve MCφ.
This approach takes care of binary predicates involving z; to handle relations among existentially quantified variables, additional tools are needed. Thus, the Hybrid Problem definition also
involves a relation R(x,y) and a “sparsity type” designation, specifying whether R codes a sparse
relation between x and y, or its sparse complement. However, this additional information can be
modeled by adding new elements to the universe and strategically placing them in the corresponding sets, thus reducing the more complex case to a combination of four Basic Problems.
See Lemma 5.6 for the proof that covers more complicated cases.
5 COMPLETENESS OF K-OV IN MC(∃K ∀)
This section will prove the completeness of k-OV in MC(∃k∀) problems. Here we only consider
the input structures that are graphs, i.e., where all relations are either unary or binary; see Section 7 of this article for the reduction from hypergraphs to graphs. First, we introduce a class of
Basic Problems, and prove that these problems are equivalent to k-OV under exact complexity reductions. Then, we show that any problem in MC(∃k∀) can be reduced to a combination of Basic
Problems (a.k.a. the Hybrid Problem).
5.1 How to Complement a Sparse Relation: Basic Problems and Reductions
between Them
In this section, we define the Basic Problems for k ≥ 2, generalizing k-OV, k-Set Cover and Set
Containment problems, and prove that these problems are fine-grained reducible to each other
under randomized reductions. In Section 6, we will give deterministic reductions for k = 2.
Let k ≥ 2. We introduce 2k Basic Problems labeled by k-bit binary strings from 0k to 1k . The
input of these problems is the same as that of k-EI defined in Section 2.3: k set families S1 ... Sk of
size n1,...,nk on a universe U of size nu . We define 2k quantifier-free formulas ψ0k ,...,ψ1k such
that
ψ =



i, [i]=0
(¬(u ∈ Si ))


∨



i, [i]=1
(u ∈ Si )



.
Here, i ∈ {1,..., k} and [i], the ith bit of label , specifies whether u is in each Si or not in the ith
term of ψ.
For each , let φ = (∃S1 ∈ S1) ... (∃Sk ∈ Sk )(∀u ∈ U )ψ. For simplicity, we will omit the domains of the variables in these formulas. We call MCφ0k ,..., MCφ1k the Basic Problems. We refer to the Basic Problem MCφ as BP[]. These problems are special cases of first-order model
checking on graphs, where sets and elements correspond to vertices, and membership relations
correspond to edges. Note that BP[0k ] is k-EI, and BP[1k ] is k-Set Cover. When k = 2, BP[01] and
BP[10] are Set Containment problems, and BP[00] is the Set Disjointness problem. For a k-tuple
ACM Transactions on Algorithms, Vol. 15, No. 2, Article 23. Publication date: December 2018.               
23:14 J. Gao et al.
(S1 ∈ S1,..., Sk ∈ Sk ) satisfying (∀u)ψ, we call it a solution of the corresponding Basic Problem
BP[].
We present a randomized8 fine-grained mapping reduction between any two Basic Problems,
thus proving the following lemma, which generalizes Claim 4.1 to k > 2.
Lemma 5.1. Let s(m) be a non-decreasing function such that 2Ω(√logm) ≤ s(m) < m1/5. For
any 1, 2 ∈ {0, 1}
k , there is a randomized exact complexity reduction (BP[1],mk /(s(m))1/6) ≤EC
(BP[2],mk /s(m)).
For problems BP[1] and BP[2], where 1 and 2 only differ in the ith bit, if we are allowed
to complement all sets in Si , we can easily reduce between them. Similarly, if 1 and 2 differ
in more than one bit, we can complement all the sets in corresponding set families. However,
complementing the sets in Si takes time O(ninu ), which might be as large as Θ(m2). To solve this,
we self-reduce BP[1] on the universe U to the same problem on a smaller universe U 	
, and then
complement sets on U 	
. For any given δ, if the size of U 	 is n	
u = O(mδ ), then complementing all
sets in Si only takes time and space m · O(mδ ) = O(m1+δ ).
Lemma 5.2. (Randomized universe-shrinking self-reductions of Basic Problems)
Let label  be any binary string in {0, 1}
k . For any s(m) = 2Ω(√logm)
, given a BP[] instance I of
size m and universe U of size nu , we can either solve it in time O(mk /s(m)), or use time O(mk /s(m))
to create a BP[] instance I 	 of size O(m · s(m)
5) on universe U 	 whose size is n	
u = O(s(m)
5), so that
I ∈ BP[] iff I 	 ∈ BP[] with error probability bounded by O(1/s(m)).
Note that the self-reduction of k-OV actually reduces the Sparse k-OV to a moderate-dimension
version of k-OV, implying Lemma 1.1. The other direction (moderate-dimension k-OV to Sparse
k-OV) is easy since if the dimension d = nδ , then m is at most d · n = n1+δ , as required.
Corollary 5.3. (Reverse direction of Lemma 1.1)
Suppose that for any k ≥ 2 there exists δ, ϵ > 0 and a (randomized) O(nk−ϵ ) algorithm solving
k-OV with dimension d = nδ . Then there is an ϵ 	 > 0 and a (randomized) O(mk−ϵ	
) time algorithm
solving Sparse k-OV.
Proof. The algorithm converts an instance of Sparse k-OV to an instance of k-OV of dimension
nδ using universe-shrinking self-reduction (Lemma 5.2) and then applies assumed O(nk−ϵ ) time
algorithm to the reduced instance. More specifically, let m = O(n1+γ ), where n is the number of
vectors. Choosing s(m) = O(mδ /5(1+γ )
) for some δ > 0 creates an instance of OV with dimension
n	
u = O(s(m)
5) = O(nδ ), and size m	 = O(n1+δ+γ ); number of vectors n remains unchanged. Now,
the reduction takes timeO(mk /(s(m))5) = O(mk−δ /(1+γ )
), and running theO(nk−ϵ ) time algorithm
on the reduced instance takes O(nk−ϵ ) ≤ O(mk−ϵ /(1+γ )
) time. Setting ϵ 	 = min{δ/(1 +γ ), ϵ/(1 +
γ )} completes the proof.
We will present the randomized self-reductions for problems BP[] s.t.   1k in Section 5.1.1.
For BP[1k ], we will prove that it is either easy to solve or easy to complement in Section 5.1.2.
After shrinking the universe, we complement the sets to reduce between two Basic Problems
BP[1] and BP[2] according to the following lemma.
Lemma 5.4 (Reduction between different Basic Problems). For two different labels 1, 2 ∈
{0, 1}
k , given set families S1,...,Sk , let S	
1,...,S	
k be defined such that
8The deterministic reduction will be presented in Section 6.
ACM Transactions on Algorithms, Vol. 15, No. 2, Article 23. Publication date: December 2018.                          
Completeness for FO Properties on Sparse Structures 23:15
S	
i =
⎧⎪
⎨
⎪
⎩

Si
 | Si ∈ Si

, if 1[i]  2[i]
Si, otherwise ,
then, (∃S1 ∈ S1) ... (∃Sk ∈ Sk )(∀u)ψ1 iff (∃S	
1 ∈ S	
1) ... (∃S	
k ∈ S	
k )(∀u)ψ2 .
The proof of correctness is straightforward.
Proof of Lemma 5.1. Pick s	
(m) = s(m)
1/(6k)
. Using Lemma 5.2, we shrink the universe to size
n	
u = s	
(m)
5. So the time complexity in this step is bounded byO(m · s	
(m)
5), which is significantly
less than mk /s(m) even if k = 2.
Let new instance size be m	
. So m	 = m · s	
(m)
5. Given that the constructed instance can be
decided in timem	k /s(m	
), we getm	k /s(m	
) < (m(s(m)
1/(6k)
)
5)
k /s(m) < mk /s(m)
1/6. Thus, by the
two-step fine-grained mapping reductions given by Lemma 5.2 and Lemma 5.4, we have an exact
complexity reduction between any two Basic Problems, completing the proof for Lemma 5.1.
5.1.1 Randomized Universe-shrinking Self-reduction of BP[] Where   1k . This section proves
part of Lemma 5.2, by giving a randomized universe-shrinking self-reduction of BP[], where 
1k . The main idea is to divide the sets into large and small ones. For large sets, there are not too
many of them in the sparse structure, so we can work on them directly. For small sets, we use a
Bloom Filter mapping each element in U to some elements in U 	 at random, and then for each
set on universe U , we compute the corresponding set on universe U 	
. Next we can decide the
same problem on these newly computed sets, instead of sets on U . (Reference [17] used a similar
technique in reducing from Orthogonal Range Search to the Subset Query problem.) Because the
sets are small, it is unlikely that some elements in two different sets on U are mapped to the same
element on U 	
, bounding the error probability.
• Step 1: Large sets. Let d = s(m). For sets of size at least d, we directly check if they are in
any solutions. There are at most O(m/d) = O(m/s(m)) of such large sets. In the outer loop,
we enumerate all large sets in S1,...,Sk . If their sizes are pre-computed, then we can do
the enumeration in O(m/s(m)). Assume the current large set is Si ∈ Si . Because variables
quantified by ∃ are interchangeable, we can interchange the order of variables, and let Si be
the outermost quantified variable S1. On each such Si (or S1 after interchanging), we create
a new formulaψS1 on variables S2,..., Sk ,u from formulaψ, by replacing u ∈ S1 (u  S1) by
a unary relation on u. Then, we decide if the graph induced by S2,...,Sk and U satisfies
(∃S2) ... (∃Sk )(∀u)ψS1 , using the baseline algorithm, which takes time O(mk−1) for each
such large set S1. Thus the overall running time is O(m/s(m)) · O(mk−1) = O(mk /s(m)). If
no solution is found in this step, then proceed to Step 2.
• Step 2: Small sets. Now we can exclude all the sets of size at least d. For sets of size smaller
than d, we do the self-reduction to universe U 	 of size n	
u = s(m)
5. Let t = s(m), and let
h : U → U 	t be a function that independently maps each element u ∈ U to t elements in U 	
at random. On set S ⊆ U, we overload the notation h by defining h(S) = 
u ∈S h(u). For all
set families Si , we compute new sets h(Si ) for all Si ∈ Si . Then, we decide whether the new
sets satisfy the following sentence, which is another BP[] problem:
(∃S1) ... (∃Sk )(∀u)

i, [i]=0
¬(u ∈ h(Si )) ∨

i, [i]=1
(u ∈ h(Si )).
The size of the new instance is O(nt) = O(m · s(m)), and the running time of the selfreduction is also O(nt) = O(m · s(m)). So it is a fine-grained mapping reduction for any
k ≥ 2.
ACM Transactions on Algorithms, Vol. 15, No. 2, Article 23. Publication date: December 2018.                 
23:16 J. Gao et al.
Fig. 3. The universe-shrinking process. S1 = {a,b} and S2 = {a,b,c}. After the mapping h, the new sets are
h(S1) = {a	
,b	
,c	
,d	
} and h(S2) = {a	
,b	
,c	
,d	
, e 	
}.
Figure 3 illustrates an example of the universe-shrinking self-reduction for BP[01], where we
look for S1, S2 so that S1 ⊆ S2. If they exist, then after the self-reduction, it is always true that
h(S1) ⊆ h(S2). Still, it might happen that some S1  S2 but h(S1) ⊆ h(S2). In this case, a false positive occurs. In BP[00], a false negative may occur when there are two disjoint sets, but some elements in S1 ∩ S2 are mapped to the same element in U 	
. Next we will analyze the error probability
of this reduction.
Analysis. Because variables quantified by ∃ are interchangeable, w.l.o.g. for  containing i (i ≥ 1)
zeros and k − i ones, assume BP[] is defined by
(∃S1) ... (∃Sk )(∀u)
⎡
⎢
⎢
⎢
⎢
⎢
⎣




i
j=1
(u  Sj)



∨



k
j=i+1
(u ∈ Sj)



⎤
⎥
⎥
⎥
⎥
⎥
⎦
,
equivalently, (∃S1) ... (∃Sk )[(
i
j=1 Sj) ⊆ (
k
j=i+1 Sj)].
Let sets A = i
j=1 Sj and B = k
j=i+1 Sj . Then the problem is to decide whether there exists
(S1,..., Sk ) so that A ⊆ B. After the self-reduction, let A	 = i
j=1 h(Sj) and B	 = k
j=i+1 h(Sj), and
decide if there exists (S1,..., Sk ) such that A	 ⊆ B	
.
• False positive. A false positive occurs when ∀(S1,..., Sk ),A  B, but ∃(S1,..., Sk ),A	 ⊆
B	
. For a fixed tuple (S1,..., Sk ) such that A  B, an error occurs when h(u) ⊆ B	 for all
u ∈ A − B. The size of B	 is at most kdt. So the error probability Pr[h(u) ⊆ B	
] ≤ (kdt/n	
u )
t =
(ks(m) · s(m)/s(m)
5)
t < s(m)
−t . The size of A − B is bounded by kd, so the probability
Pr[∃u ∈ A − B,h(u) ⊆ B	
] ≤ kd · s(m)
−t . There areO(mk ) tuples of (S1,..., Sk ), so the total
error probability is at mostO(mk ) · kd · s(m)
−t = O(mk · s(m)/s(m)
s (m)
), which is exponentially small.
• False negative. A false negative occurs when ∃(S1,..., Sk ),A ⊆ B, but ∀(S1,..., Sk ),A	
B	
. Fix any tuple (S1, ..., Sk ) that satisfies A ⊆ B in the original instance, and consider the
distribution on the corresponding h(S1), ..,h(Sk ). By definition, B	 = 
u ∈B h(u), and so contains 
u ∈A h(u). So if A	 ⊆ 
u ∈A h(u), we will have A	 ⊆ B	
, and there will not be a false
negative. If not, then there is some u	 ∈ A	 = i
j=1 h(Sj), such that u	  
u ∈A h(u). Then for
each j ∈ {1,...,i}, in each Sj there is a uj ∈ Sj with u	 ∈ h(uj), but not all uj are identical.
ACM Transactions on Algorithms, Vol. 15, No. 2, Article 23. Publication date: December 2018.            
Completeness for FO Properties on Sparse Structures 23:17
(Otherwise the uj ∈ A, so u	 ∈ h(uj) ⊆ 
u ∈A h(u), contradicting u	  
u ∈A h(u)). In particular, this means that for some j1, j2, there are uj1 ∈ Sj1 ,uj2 ∈ Sj2 , such that h(uj1 ) ∩ h(uj2 )
∅. So the error probability is bounded by k2 · Pr[∃(u1 ∈ Sj1 ,u2 ∈ Sj2 ),h(u1) ∩ h(u2)  ∅].
Because |Sj1 | and |Sj2 | are at most d, by Birthday Paradox, the probability is at most
O(k2
d2
t 2/n	
u ) = O(s(m)
−1). This is the upper bound of the error probability for the fixed
(S1,..., Sk ) tuple. Then, the probability of the event “∀(S1,..., Sk ),A	  B	
” is even smaller.
5.1.2 Deterministic Universe-shrinking Self-reduction of BP[1k ]. This section proves the remaining part of Lemma 5.2, by showing BP[1k ] is either easy to solve or easy to complement. BP[1k ]
is the k-Set Cover problem, which decides whether there exist k sets covering the universe U . It
is special in the Basic Problems: When nu is small, the sets are easy to complement; when nu is
large, the problem is easy to solve.
• Case 1: Large universe. If nu > s(m), then in a solution of this problem, at least one set has
size at least nu /k. There are at most m/(k/nu ) = O(m/s(m)) such large sets, thus they can
be listed in time O(m/s(m)), after pre-computation on the sizes of all sets. Our algorithm
exhaustively searches all such large sets. And then, similarly to “Step 1” in Section 5.1.1, for
each of the large sets, we run the baseline algorithm to find the remaining k − 1 sets in the kset cover, which takes time O(mk−1). So the overall running time is O(m/s(m)) · O(mk−1) =
O(mk /s(m)).
• Case 2: Small universe. If nu ≤ s(m), then we do not need a universe-shrinking selfreduction, because the universe is already small enough.
5.2 Hybrid Problem
Next we reduce general MC(∃k∀) problems to an intermediate problem called the Hybrid Problem,
which is a combination of 2k Basic Problems. Then by reducing from the Hybrid Problem to Basic
Problems, we can set up a connection between MC(∃k∀) and OV.
Let k ≥ 2. The input to the Hybrid Problem is:
(1) Set families S1 ... Sk defined on universe U , where U is partitioned into 2k disjoint subuniverses: U = 
∈ {0,1}k U.
(2) A binary relation R defined on pairs of sets from any two distinct set families. R is a
symmetric relation (R(Si, Sj) iff R(Sj, Si )).
(3) type is binary string of length (
k
2 ), indexed by two integers [i, j], s.t. i, j ∈ {1,..., k} and
i < j.
The goal of the problem is to decide if there exist S1 ∈ S1,..., Sk ∈ Sk such that both of the
following constraints are true:
(A) For each  ∈ {0, 1}
k , (S1,... Sk ) is a solution of BP[] defined on sub-universe U.
(B) For all pairs of indicesi, j ∈ {1,..., k},i < j, we have that R(Si, Sj) = true iff type[i, j] = 1.
We let n be the sum of |S1 |,..., |Sk | and U , and let m be the number of tuples in all unary
and binary relations. The Hybrid Problem is a first-order property on graphs with additional constraints. As usual, we assume all relations in the Hybrid Problem are sparse (m ≤ n1+o(1)
). Figure 4
shows a solution to a Hybrid Problem instance when k = 2.
Intuition Behind the Hybrid Problem. In the Hybrid Problem, the set families S1,..., Sk encode
the conditions on relations involving xk+1, while the binary relation R and the types encode the
conditions on relations not involving xk+1. We mentioned in Section 4 that any first-order query
containing two variables can be written in a “normal form,” which is a combination of sparse and
ACM Transactions on Algorithms, Vol. 15, No. 2, Article 23. Publication date: December 2018.         
23:18 J. Gao et al.
Fig. 4. An example of a solution to a Hybrid Problem instance, when k = 2. In sub-universesU00,U01,U10,U11,
sets S1 and S2 are solutions of BP[00](Set Disjointness), BP[01](Set Containment), BP[10](Set Containment
in the reversed direction) and BP[11](2-Set Cover), respectively. Furthermore, type[1, 2] = 1 specifies that
the predicate R on (S1, S2) must be true.
co-sparse relations. The Hybrid Problem is designed for separating sparse relations from co-sparse
ones, for all pairs of variables in formula φ.
The relation between the pair of variables (xi, xk+1), where 1 ≤ i ≤ k can be either sparse
or co-sparse. Because there are k such variables xi , there are 2k cases for a combination
((x1, xk+1),..., (xk , xk+1)). These cases correspond to the 2k Basic Problems. In each Basic Problem, we deal with one of the 2k cases.
For a relation between the pair of variables (xi, xj), where 1 ≤ i < j ≤ k, it also can be either
sparse or co-sparse. We use type[i, j] to distinguish the two cases: When it is set to 1, we expect a
sparse relation for (xi, xj), otherwise a co-sparse relation.
5.2.1 Reduction to Basic Problems.
Lemma 5.5. Let s(m) be a non-decreasing function such that 2Ω(√logm) ≤ s(m) < m1/5. Then,
(Hybrid Problem,mk /(s(m))1/6) ≤EC (k-OV,mk /(s(m))).
Given an instance of the Hybrid Problem, we can do the following modification in time O(m).
For each pair of indices i, j, where 1 ≤ i < j ≤ k, we construct auxiliary elements depending on
the value of type[i, j].
• Case 1: If type[i, j] = 0, then if a pair Si ∈ Si , Sj ∈ Sj occurs in a solution to the Hybrid
Problem, then there should be no edge R(Si, Sj). Let  be the length-k binary string, where
the ith and jth bits are zeros and all other bits are ones. For each edge R(Si, Sj) on Si ∈ Si and
Sj ∈ Sj , we add an extra element uSi Sj in U and let uSi Sj ∈ Si , uSi Sj ∈ Sj . Thus, S	
i ∈ Si and
S	
j ∈ Sj can both appear in the solution only when for all uSi Sj , (uSi Sj  S	
i ) ∨ (uSi Sj  S	
j),
and this holds iff R(S	
i , S	
j) = false.
• Case 2: If type[i, j] = 1, then in a solution to the Hybrid Problem, Si and Sj should have
an edge R(Si, Sj) between them. Let  be the length-k binary string, where the jth bit is
zero and all other bits are ones. For each Sj ∈ Sj , we add an extra element uSj in U and let
uSj ∈ Sj . For each edge R(Si, Sj), we let uSj ∈ Si . Thus, S	
i ∈ Si and S	
j ∈ Sj can both appear
in the solution only when for all uSj , (uSj  S	
j) ∨ (uSj ∈ S	
i ), and it holds iff R(S	
i , S	
j) = true.
ACM Transactions on Algorithms, Vol. 15, No. 2, Article 23. Publication date: December 2018.       
Completeness for FO Properties on Sparse Structures 23:19
After the above construction, we can drop the constraint (B) of the Hybrid Problem. We
will ignore the relation R and type in the Hybrid Problem. The problem now is to decide
whether there exists tuple (S1,..., Sk ) being a solution to all 2k Basic Problems. Then we
can use the reductions in Lemma 5.1 to reduce all these Basic Problems to BP[0k ], and then
combine the 2k instances to a large BP[0k ] instance. Because the reductions do not change
the solutions S1,..., Sk , there exists a solution to the large BP[0k ] instance iff there exists
a solution simultaneously to all the Basic Problem instances. Let U
	 be the sub-universe
of the BP[0k ] instance reduced from the BP[] sub-problem. (S1,..., Sk ) is a solution to all
Basic Problems iff their intersection is empty on every sub-universeU 	
 , iff their intersection
is empty on universe 
∈ {0,1}k U 	
 , i.e., it is a solution of a BP[0k ] instance.
Multiplying the error probability in the reductions between Basic Problems by 2k , which is a
constant number, and then taking a union bound, we get similar bounds of error probability for
the Hybrid Problem.
5.2.2 Turing Reduction from General MC(∃k∀) Problems to the Hybrid Problem. The following
lemma provides the last piece of the proof that sparse k-OV is complete for MC(∃k∀) under finegrained Turing reductions. The result follows by combining this lemma with Lemma 5.5.
Lemma 5.6. For any integer k ≥ 2, any problem in MC(∃k∀) is linear-time Turing reducible to the
Hybrid Problem, namely, (MC(∃k∀),T (m)) ≤EC (Hybrid Problem,T (O(m))).
Consider the problem MCφ where φ = (∃x1) ... (∃xk )(∀xk+1)ψ (x1,..., xk+1). An input graph
G can be preprocessed in linear time to ensure that it is a (k + 1)-partite graph on vertices V =
(V1,...,Vk+1), for example by creating k + 1 copies of original vertex set.
W.l.o.g, we assume that for each occurrence of binary predicate Rt (xi, xj), i ≤ j. Let Pk+1 be the
set of unary and binary predicates inψ that involve variable xk+1, and let Pk+1 denote the set of the
other predicates not including xk+1. A partial interpretation α for Pk+1 is a binary string of length
|Pk+1 |, that encodes the truth values assigned to all predicates in Pk+1. For each i s.t. 1 ≤ i ≤ |Pk+1 |,
if the ith predicate in Pk+1 is assigned to true, then we set the ith bit of α to one, otherwise,
we set it to zero. For a tuple (v1,...,vk ), we say it implies α (denoted by (v1,...,vk ) |= α) iff
when (x1 ← v1,..., xk ← vk ), the evaluations of all predicates in Pk+1 are the same as the values
specified by α.
For each α ∈ {0, 1}
Pk+1 , we create a distinct Hybrid Problem instance Hα . If any of the Hybrid
Problems accepts, then we accept. Let ψ |α (x1,..., xk+1) be ψ after replacing all occurrences of
predicates in Pk+1 by their corresponding truth values specified by α. The following steps show
how to create Hα from α and ψ |α (x1,..., xk+1).
Step 1: Construction of sets. We introduce colors, which are partial interpretations defined on
some specific subsets of the predicates concerning variable xk+1. We call them “colors” because
they can be considered as a kind of labels on (vi,vk+1) pairs. For each i ∈ {1,..., k}, we give all
the unary predicated defined on xi and binary predicates defined on (xi, xk+1) (including those on
(xk+1, xi )) a canonical order. We use Pi to denote the set of these predicates for each i. Let a color
be a partial interpretation for Pi , which is a binary string of length |Pi |, encoding the truth values
assigned to all predicates in Pi . For each j s.t. 1 ≤ j ≤ |Pi |, if the jth predicate in Pi is assigned to
true, then we set the jth bit of the color to one, otherwise, we set it to zero. For a colorci ∈ {0, 1}|Pi |
,
we say (vi,vk+1) |= ci iff when xi ← vi and xk+1 ← vk+1, the values of all predicates in Pi are the
same as the corresponding bits ofci . We refer to the colors where all bits are zeros as the background
colors. These colors are special because they correspond to interpretations where all predicates in
ACM Transactions on Algorithms, Vol. 15, No. 2, Article 23. Publication date: December 2018.     
23:20 J. Gao et al.
Pi are false, i.e., we cannot directly go through all pairs (vi,vk+1), where (vi,vk+1) |= 0|Pi |
, since
this is a co-sparse relation. So we need to deal with these pairs separately.
For a vertex combination (v1,...,vk+1), where (vi,vk+1) |= ci on all 1 ≤ i ≤ k, the k-color-tuple
(c1,...,ck ) forms a color combination, which corresponds to truth values assigned to all the predicates in Pk+1.
For each vi ∈ Vi , where 1 ≤ i ≤ k, we create set Svi in the set family Si . For each vk+1 ∈ Vk+1,
and each color combination (c1,...,ck ) s.t. ci ∈ {0, 1}|Pi | and the values of all predicates specified
by (c1,...,ck ) make ψ |α evaluate to false (in which case we say (c1,...,ck ) does not satisfy ψ |α ),
we create an element u(vk+1,c1,...,ck ) in U . We call a string C ∈ {0, 1}
k an encoding of a color combination (c1,...,ck ) when on all indices i ∈ {1,..., k}, C[i] = 1 iff ci = 0|Pi |
. We put each element
u(vk+1,c1,...,ck ) in the sub-universe UC iff C is an encoding of (c1,...,ck ).
Next we will construct the sets. For each vi ∈ Vi , let Svi be
Svi = {u(vk+1,c1,...,ck ) | (c1,...,ck ) does not satisfy ψ |α , and
((ci  0|Pi |
, (vi,vk+1) |= ci ), or (ci = 0|Pi |
, (vi,vk+1) |= ci = 0|Pi |
))}.
To construct such sets, for each edge on (xi, xk+1) (and (xk+1, xi )), we do the following. Assume
the current vertex pair is (vi,vk+1).
(1) First, let set Svi contain all elements u(vk+1,c1,...,ck ) in U , where ci is a fixed color such that
(vi,vk+1) |= ci , and the other colors cj can be any string in {0, 1}|Pj |
.
(2) Next, let set Svi contain all elements u(vk+1,c1,...,ck ) inU , where ci = 0|Pi | (here (vi,vk+1) |=
ci = 0|Pi | because there is some edge connecting vi and vk+1, meaning at least one bit in
ci is 1), and the other colors cj can be any string in {0, 1}|Pj |
.
In other words, in the sub-universe labeled by 0k , which is made up of elements u(vk+1,c1,...,ck )
such that none of the ci equals 0|Pi |
, and that (c1,...,ck ) does not satisfy ψ |α , a set Svi contains
an element u(vk+1,c1,...,ck ) iff (vi,vk+1) |= ci . However, in any sub-universe labeled by C, where the
ith bit of C is 1, i.e., those are made up of elements u(vk+1,c1,...,ck ) such that ci = 0|Pi | and that
(c1,...,ck ) does not satisfy ψ |α , a set Svi contains an element u(vk+1,c1,...,ck ) iff (vi,vk+1) |= ci =
0|Pi |
.
Analysis. Now we show the above construction achieves constraint (A) in the definition of the
Hybrid Problem.
• Assume that (v1,...,vk ) does not satisfy (∀vk+1)ψ |α (x1,..., xk+1), i.e., there exists some
vk+1 ∈ Vk+1 such that ψ |α (v1,...,vk+1) is false. Then consider the specific color combination (c1,...,ck ), where on each i, (vi,vk+1) |= ci . So (c1,...,ck ) does not satisfy
ψ |α (x1,..., xk+1). Thus there exists an element u(vk+1,c1,...,ck ) in U .
If none of the colors in combination (c1,...,ck ) is the background color, then the encoding of (c1,...,ck ) is the string 0k . Thus, the element u(vk+1,c1,...,ck ) is in sub-universe
U0k . By our construction, u(vk+1,c1,...,ck ) is contained in all of Sv1 ,..., Svk , as shown on the
left side of Figure 5. This is because when we went through all the edges, at the edge between (vi,vk+1), we put u(vk+1,c1,...,ck ) in Svi , since none of the colors is background. Thus
(∃u ∈ U0k )[
k
i=1 (u ∈ Svi )], so it is not the case that (∀u ∈ U0k )[
k
i=1 ¬(u ∈ Svi )], which
means Sv1 ,..., Svk is not a solution of BP[0k ] on sub-universe U0k .
If some of the colors ci in the color combination (c1,...,ck ) equal the background color
0|Pi |
, then in the encoding C of (c1,...,ck ), C[i] = 1. Thus, the element u(vk+1,c1,...,ck ) is
in the sub-universe UC. By our construction, u(vk+1,c1,...,ck ) is contained in sets Svi for all
ACM Transactions on Algorithms, Vol. 15, No. 2, Article 23. Publication date: December 2018.  
Completeness for FO Properties on Sparse Structures 23:21
Fig. 5. The formula is satisfied iff there exists (Sv1 , Sv2 ,..., Svk ) so that there does not exist such an element
u in any of the sub-universes: The left figure illustrates the case where none of c1,...,ck is a background
color. The right is the case where only c1 and c3 are background colors. (The dashed lines stand for nonexisting edges.)
indices i, where ci is not the background color 0|Pi |
, and is not contained in sets Svj for all
indices j, where cj is the background color 0|Pj |
. The latter case is because for each index j,
where cj is the background color, there is no edge connecting the pair of vertices (vj,vk+1).
So we did not put u(vk+1,c1,...,ck ) in Svj .
(The right side of Figure 5 demonstrates the example where c1 and c3 are the background
colors while other colors are not.)
Thus
(∃u ∈ UC )
⎡
⎢
⎢
⎢
⎢
⎢
⎣
	
i ∈ {1,...,k },C[i]=0
(u ∈ Svi ) ∧
	
i ∈ {1,...,k },C[i]=1
(¬(u ∈ Svi ))
⎤
⎥
⎥
⎥
⎥
⎥
⎦
,
so it is not the case that
(∀u ∈ UC )
⎡
⎢
⎢
⎢
⎢
⎢
⎣

i ∈ {1,...,k },C[i]=0
(¬(u ∈ Svi )) ∨

i ∈ {1,...,k },C[i]=1
(u ∈ Svi )
⎤
⎥
⎥
⎥
⎥
⎥
⎦
,
which means Sv1 ,..., Svk is not a solution of BP[C] on sub-universe UC.
• However, assume that (v1,...,vk ) satisfies (∀vk+1)ψ |α (v1,...,vk+1). We claim that for all
 ∈ {0, 1}
k , (Sv1 ,..., Svk ) is a solution to Basic Problem BP[].
Consider the sub-universe UC for each C ∈ {0, 1}
k . If C = 0k , i.e., the sub-universe is U0k
corresponding to BP[0k ], then none of the elements u(vk+1,c1,...,ck ) inU0k contains any background color among its c1,...,ck . For the sake of contradiction, suppose there exists an
element u(vk+1,c1,...,ck ) that is contained in all sets Sv1 ,..., Svk . So by our construction of
sets, for each i ∈ {1,..., k}, (vi,vk+1) |= ci . Recall that the color combination (c1,...,ck ) in
any element u(vk+1,c1,...,ck ) does not satisfy ψ |α . Then this means the vertex vk+1 does not
satisfy ψ |α (v1,...,vk ,vk+1), which leads to a contradiction.
Thus on (Sv1 ,..., Svk ), it is not the case that (∃u ∈ U0k )[
k
i=1 (u ∈ Svi )], implying
(Sv1 ,..., Svk ) satisfies (∀u ∈ U0k )[
k
i=1 ¬(u ∈ Svi )]. So it is a solution of the Basic Problem
BP[0k ] on sub-universe U0k .
If C  0k , for the sake of contradiction, suppose there exists an element u(vk+1,c1,...,ck )
such that among Sv1 ,..., Svk , then it is contained in set Svi iff C[i] = 0. Then by our construction of sets, this means for all i such that C[i] = 0, (vi,vk+1) |= ci ; while for all i such
thatC[i]  0, (vi,vk+1) |= 0|Pi | = ci . Combining the two statements, for all i, (vi,vk+1) |= ci .
Recall again that the color combination (c1,...,ck ) in any element u(vk+1,c1,...,ck ) does not
ACM Transactions on Algorithms, Vol. 15, No. 2, Article 23. Publication date: December 2018.     
23:22 J. Gao et al.
satisfy ψ |α . This implies the vertex vk+1 does not satisfy ψ |α (v1,...,vk+1), which leads to
a contradiction.
Thus on (Sv1 ,..., Svk ), it is not the case that
(∃u ∈ UC )
⎡
⎢
⎢
⎢
⎢
⎢
⎣
	
i ∈ {1,...,k },C[i]=0
(u ∈ Svi ) ∧
	
i ∈ {1,...,k },C[i]=1
(¬(u ∈ Svi ))
⎤
⎥
⎥
⎥
⎥
⎥
⎦
,
implying (Sv1 ,..., Svk ) satisfies
(∀u ∈ UC )
⎡
⎢
⎢
⎢
⎢
⎢
⎣

i ∈ {1,...,k },C[i]=0
(¬(u ∈ Svi )) ∨

i ∈ {1,...,k },C[i]=1
(u ∈ Svi )
⎤
⎥
⎥
⎥
⎥
⎥
⎦
.
So it is a solution of the Basic Problem BP[C] on sub-universe UC.
In summary, there exists a tuple (v1,...,vk ) such that (∀vk+1)ψ |α (v1,...,vk ,vk+1) holds
true, iff there exist sets (Sv1 ,..., Svk ) such that for all  ∈ {0, 1}
k , (Sv1 ,..., Svk ) is a solution
of Basic Problem BP[] on sub-universe U. Thus our reduction satisfies constraint (A) of
the Hybrid Problem.
Step 2: Construction of R and type. Next, we consider the predicates in Pk+1, which are predicates unrelated to variable xk+1. We create edges for predicate R according to the current partial
interpretation α.
For a pair of vertices vi ∈ Vi and vj ∈ Vj , where 1 ≤ i < j ≤ k, we say (vi,vj) agrees with α if
the evaluations of all predicates on (xi, xj) (including (xj, xi )) when xi ← vi, xj ← vj , equals the
truth values of corresponding predicates specified by α.
• Case 1: At least one predicate on (xi, xj) in α is true. (i.e., (xi, xj) is in a sparse relation)
For all edges (vi,vj) (including (vj,vi )), where vi ∈ Vi and vj ∈ Vj and i < j ≤ k, if (vi,vj)
agrees with α, then we create edge R(Svi , Svj ). Finally, we make type[i, j] = 1 in the Hybrid
Problem Hα .
• Case 2: All predicates on (xi, xj) in α are false. (i.e., (xi, xj) is in a co-sparse relation)
For all edges (vi,vj) (including (vj,vi )), where vi ∈ Vi and vj ∈ Vj and i < j ≤ k, if (vi,vj)
does not agree with α, then we create edge R(Svi , Svj ). Finally, we make type[i, j] = 0 in the
Hybrid Problem Hα .
Analysis. We prove that (vi,vj) can appear in the solution of Hα only if when it agrees
with α. If (vi,vj) does not agree with α, then we should not let them be in any solution of
Hα . This is done by the relation R and the string type.
Consider the two cases. If in α some predicates on (xi, xj) are true (i.e., set of tuples that
agree with α is sparse), then in any (vi,vj) that agrees with α, there must be an edge in G
connecting vi and vj . So we can add an edge (defined by relation R) on the corresponding
sets Svi , Svj and require there must be such an edge in the solution (i.e., type being 1).
However, if all predicates on (xi, xj) in α are false (i.e., set of tuples agreeing with α is
co-sparse), then in any (vi,vj) that agrees with α, there should not be any edge connecting
vi andvj . In this case, we turn to consider the tuples (vi,vj) that do not agree with α (which
is a sparse relation, instead of co-sparse). We create edges on the corresponding sets Svi , Svj
and require there must not be such an edge in the solution (i.e., type being 0).
Therefore, a tuple (v1,...,vk ) implies α iff for all i, j ∈ {1,..., k},i < j, the truth value
of relation R(Svi , Svj ) equals whether type[i, j] = 1. Thus our reduction satisfies constraint
(B) of the Hybrid Problem.
ACM Transactions on Algorithms, Vol. 15, No. 2, Article 23. Publication date: December 2018.   
Completeness for FO Properties on Sparse Structures 23:23
From the analyses of the two steps, we have justified that: there exists (v1,...,vk ) so that
(v1,...,vk ) |= α, andψ |α holds for allvk+1 ∈ Vk+1, iff there exists (Sv1 ,..., Svk ) being a solution to
the Hybrid Problem Hα . Thus, if for any α ∈ {0, 1}
Pk+1 , the Hybrid Problem Hα accepts, then there
exists a solution (v1,...,vk ) so that ψ (v1,...,vk ,vk+1) holds for all vk+1 ∈ Vk+1. Otherwise there
does not exist such a solution. From the above argument, we have proved the following claim.
Claim 5.1. The two propositions are equivalent:
(1) MCφ has a solution x1 ← v1,..., xk ← vk such that (∀vk+1 ∈ Vk+1)ψ (v1,...,vk+1) is satisfied.
(2) There exists an α ∈ {0, 1}
Pk+1 so that (Sv1 ,..., Svk ) |= α, and Sv1 ,..., Svk is a solution to the
Hybrid Problem Hα .
The running time of the whole reduction process is linear in the total number of edges in the
graph, because the number of predicates is constant. Thus Lemma 5.6 follows.
6 DERANDOMIZATION
We derandomize the reduction in Section 5 for the k = 2 case, so that the whole proof of Theorems 1.2 and 1.4 is determistic. The derandomization of the randomized universe-shrinking selfreduction uses the technique of nearly disjoint sets similar to the construction of pseudorandom
generator by Nisan and Widgerson in Reference [28].
In this section, for simplicity we use SC(x) (respectively, SD(x)) to denote Set Containment,
a.k.a. the Basic Problem BP[01] (respectively, Set Disjointness, a.k.a. the Basic Problem [00] or
Sparse OV) on universe of size x, and use HP for the Hybrid Problem.
Lemma 6.1. For any 2Ω(√log n) ≤ s < m1/3, there is a deterministic universe-shrinking selfreduction for SC such that (SC(n),m2/s) ≤EC (SC(O(s2 log2 n/ log2 s),m2/s3 log2 n
log2 s )).
Lemma 6.2. For any 2Ω(√log n) ≤ s < m1/3, there is a deterministic universe-shrinking selfreduction for SD such that (SD(n),m2/s) ≤EC (SD(O(s2 logn/ log s),m2/s3 log n
log s )).
The following reduction from the Hybrid Problem to Set Disjointness implies the model checking for any ∃∃∀ sentences on sparse structures can be reduced to moderate-dimension SD, and
then to OV.
Lemma 6.3. For any 2Ω(√log n) ≤ s < m1/3, wherem is the input size to the Hybrid Problem, there is
a deterministic reduction algorithm such that (HP,m2/s) ≤EC (SD(O(s2 log2 n/ log2 s),m2/s7 log3 n
log3 s )).
6.1 Proof of Lemma 6.1
This section presents the derandomization of the universe-shrinking self-reduction in Sections 5.1.1 for the Basic Problem BP[01] (and equivalently BP[10]), i.e., when the corresponding
Basic Problem is the Set Containment problem.
Pick  = O(logn/ log s) and prime number q = O(s logn/ log s), so that s < q and q > n. By
Bertrand’s postulate and that PRIMES is in P, we can find such a q in time O˜ (s logn/ log s).
First, we use the algorithm in Section 5.1.1 to decide if there is a solution containing a set of size
at least s, which takes time O(m2/s). So next we only consider sets of size smaller than s.
We create a new universeU 	 of size q2. LetU 	 beGF (q) × GF (q). Let elementu in universeU correspond to a unique polynomial pu overGF (q) of degree . The number of different polynomials is
q. Since q > n, the number of different polynomials is greater than the number of elements of U .
ACM Transactions on Algorithms, Vol. 15, No. 2, Article 23. Publication date: December 2018.      
23:24 J. Gao et al.
Let h be a hash function so that each element in U is mapped to a set h(u) = {i,pu (i) | i ∈
GF (q)} of size q. For set S ⊆ U , define h(S) = 
u ∈S h(u). Finally, S	
1 = {h(S)|S ∈ S1}, and S	
2 is
constructed similarly. Then we decide the SC(q2) instance that takes S	
1 and S	
2 as input.
If S1 ⊆ S2, then h(S1) ⊆ h(S2), and the call to the SC(q2) instance returns true.
If S1  S2 for all sets, we need to show that for each element u1 ∈ S1\S2, |h(u1) ∩ h(S2)| < q.
Then because |h(u1)| = q, some element in h(u1) is not in h(S2), therefore h(S1)  h(S2). To show
|h(u1) ∩ h(S2)| < q, observe that for each element u2 ∈ S2, the intersection h(u1) ∩ h(u2) has size at
most , the degree of polynomialpu1 − pu2 . There are at mosts elements in S2, thus |h(u1) ∩ h(S2)| ≤
s < q.
Thus, there exist S1 ⊆ S2 in the original instance iff there exist h(S1) ⊆ h(S2) in the constructed
instance.
The time to create the new set is O(mq), which is less than O(m2/s). And its size is m	 ≤ mq.
Thus, if we can solve it in time O(m	2/poly(s)), where s < mϵ for all ϵ > 0, we can solve it in time
O(m2
q2/poly(s)) = O(m2/poly(s)).
6.2 Proof of Lemma 6.2
First, we use the algorithm in Section 5.1.1 to decide if there is a solution containing a set of size
at least s, which takes time O(m2/s). So next we only consider sets of size smaller than s.
Let  = O(logn/ log s), and let q be a prime ≥ s2, thus q = O(s2) = O(s2 log n
log s ). So q > n. By
Bertrand’s postulate, we can find such a q in time O(s2 log n
log s ). We create a universe U 	 of size q.
Each element u of U , which is a string of length logn, can be viewed as the encoding of a
polynomial pu over GF (q) of degree log n
log q ≤ log n
log s = .
Let a be an element in groupGF (q). For each element u inU , we let hash function ha (u) = pu (a).
For set S ⊆ U , define ha (S) = 
u ∈S {ha (u)}. The algorithm in the outermost loop enumerates all
elements a ∈ GF (q). For each a, we compute ha (S) for all sets S in the input. Then we decide if there
are two disjoint sets in the new instance. The algorithm makes q queries to SD(q) instances of input
sizem, each taking timeT (m) = m2/s3 log n
log s = m2/sq, the running time for moderate-dimension OV.
The total time is qT (m) = O(m2/s).
For each pair of different elements u and v in U , the number of elements a in GF (q) so that
pu (a) = pv (a) is at most their degree l < logn. Suppose S1 ∈ S1 and S2 ∈ S2 are a pair of disjoint
sets. ha (S1) and ha (S2) are disjoint if all pairs of their elements are mapped to different elements in
GF (q). The total number of possible collisions is at most s2 logn. Because q > s2 logn, there exists
at least one element a in GF (q) so that all pairs of elements in S1 and S2 are mapped to different
elements by ha.
If there are no disjoint sets, then for each S1 ∈ S1 and S2 ∈ S2, h(S1 ∩ S2) ⊆ h(S1) ∩ h(S2), so
h(S1) and h(S2) are not disjoint. Thus, for every a ∈ GF (q), the call to the SD(logn) instance returns
false.
6.3 Hybrid Problem
In this section, we combine the above two deterministic reductions to solve the Hybrid Problem,
which yields a deterministic reduction for Theorem 1.2 and Theorem 1.4. Here we use a similar
version of the Hybrid Problem as defined in Section 5.2 but without the relation R and the string
type. More formally, we consider the Hybrid Problem defined as follows:
Problem HP
Input: S1,S2, each a set family of sets Si = Ai ∪ Bi ∪Ci ∪ Di , where Ai, Bi,Ci,Di are subsets of
disjoint universes UA,UB,UC,UD , respectively.
ACM Transactions on Algorithms, Vol. 15, No. 2, Article 23. Publication date: December 2018.          
Completeness for FO Properties on Sparse Structures 23:25
Output: Whether there exist Si ∈ S1 and Sj ∈ S2 so that
(1) Ai ∩ Aj = ∅ (Set Disjointness)
(2) Bi ⊆ Bj (Set Containment)
(3) Ci ⊇ Cj (Set Containment reversed)
(4) Di ∪ Dj = UD (2-Set Cover)
The results in Section 5.2 can be applied to this version of Hybrid Problem, so that the model
checking for first-order sentences of form ∃∃∀ can be reduced to the Hybrid Problem. More precisely, (MC(∃∃∀),T (O(m))) ≤ (HP,T (m)).
Proof of Lemma 6.3. First, we decide if there is a solution containing a set of size at least s, as
described in the previous subsections, using time O(m2/s). So next we only consider sets of size
smaller than s.
If |UD | ≥ 2s, then for all pairs of i, j, Di and Dj cannot cover UD , so we return false. Otherwise
for i and all j, we create sets UD \Di and UD \Dj . So D1 ∪ D2 = UD iff (UD \Di ) ∩ (UD \Dj) = ∅. The
resulting instance size is O(ms).
Then, we use Lemma 6.1 self-reductions for Set Containment on the B’s and C’s, so the created
sets B	
i, B	
j andCi,Cj are on universes of sizeO(s2 log2 n
log2 s ). For each j, we create setUB\B	
j , so Bi ⊆ Bj
iff B	
i ⊆ B	
j iff B	
i ∩ (UB\B	
j) = ∅. Similarly for each i we create UC\C	
i , so Ci ⊇ Cj iff C	
i ⊇ C	
j iff
(UC\C	
i ) ∩C	
j = ∅. The resulting instance size is O(m · s2 log2 n
log2 s ).
Finally, we use Lemma 6.2 self-reductions for Set Disjointness on the original A’s. So in each
call to the oracle, the created sets A	
i,A	
j are on universes of size O(s2 log n
log s ). For each i and each
j, we create sets S	
i = A	
i ∪ B	
i ∪ (UC\C	
i ) ∪ (UD \Di ) and S	
j = A	
j ∪ (UB\B	
j) ∪C	
j ∪ (UD \Dj). By the
argument above, S	
i ∩ S	
j = ∅ iff A	
i ∩ A	
j = ∅ and Bi ⊆ Bj and Ci ⊇ Cj and Di ∪ Dj = UD . If Ai ∩
Aj = ∅, then in at least one call to the oracle A	
i ∩ A	
j = ∅ and thus the call will return true as long
as the conditions on B,C,D’s are satisfied. If Ai ∩ Aj  ∅, then all calls return false.
The size of the new instance is O(m · s2 log2 n
log2 s ). In the reduction, we make s2 log n
log d calls to the
algorithm for Set Disjointness on small universe. Thus if SD(O(s2 log2 n
log2 s )) has algorithms in time
O(m2/s7 log5 n
log5 s ), we get running time s2 log n
log d · O((m · s2 log2 n
log2 s )
2/s7 log5 n
log5 s ) = O(m2/s).
This gives a reduction from the general first-order model checking problems to the Hybrid
Problem.
6.4 Extending to More Quantifiers
The derandomization can be extended to quantifiers k + 1 for integer k ≥ 2. The reduction combines the reductions for Set Containment and Set Disjointness.
Recall from Section 5.1, a Basic Problem BP[], where   1k can be considered as deciding
(∃S1) ... (∃Sk )(∀u) [(
i
j=1 (u  Sj)) ∨ (
k
j=i+1 (u ∈ Sj))], or equivalently (∃S1) ... (∃Sk )
[(
i
j=1 Sj) ⊆ (
k
j=i+1 Sj)]. for some i such that 0 ≤ i ≤ k. Again, we map each element in U to
a set of elements in a small universe U 	 by some function h, and thus map each set S in U to a set
h(S) in U 	
.
Let q1, q2 be the q defined in Sections 6.1 and 6.2, respectively. Here q2 is a prime number
larger than sk−i
. For each element u, for each element a ∈ GF (q2) we map it to a set of tuples
h(u) = {uSC,uS D  | uSC ∈ hSC (u),uS D ∈ hS D
a (u)}, where hSC and hS D
a are the functions h and ha
defined in Sections 6.1 and 6.2, respectively, and then we make a query for the BP[] instance
created from the mapping h. Thus we make q2 queries in all, and accept if at least one of the
queries is accepted.
ACM Transactions on Algorithms, Vol. 15, No. 2, Article 23. Publication date: December 2018.          
23:26 J. Gao et al.
If there exist sets S1,..., Sk such that i
j=1 Sj ⊆ k
j=i+1 Sj , by generalizing the analysis in Section 6.2, in at least one query, then the set i
j=1 h(Sj) does not contain any element not in
h(
i
j=1 Sj). And by generalizing the analysis in Section 6.1, in each query, the set k
j=i+1 h(Sj) =
h(
i
j=1 Sj) is always contained in h(
k
j=i+1 Sj) that is contained in k
j=i+1 h(Sj). So we get the
following reduction: (BP[](n),mk /s) ≤EC (BP[](poly(s)),mk /poly(s)).
7 EXTENDING ALGORITHMS AND HARDNESS RESULTS TO HYPERGRAPHS
This section gives a reduction from MC(∃∃∀), i.e., the model checking for ∃∃∀ formulas on hypergraphs, to the model checking for ∃∃∀ formulas on graphs, where there are only unary and
binary relations. We will prove the following lemma.
Lemma 7.1. If MC(∃k∀) on graphs is solvable in time T (m), then MC(∃k∀) on hypergraphs is
solvable in T (O(m)) + O(mk−1/2).
For a three-quantifier formula (∃x)(∃y)(∀z) ψ (x,y, z), where x ∈ X,y ∈ Y, z ∈ Z, we prove that
it can be decided in time O(m3/2 +T (O(m))), where T is the running time for the model checking
of three-quantifier formulas on graphs.
Let relation N (x,y) be the edges of the Gaifman graph, which means N (x,y) = true iff there
exists some z such that there is a hyperedge Ri (x,y, z) = true (the order of x,y, z can be interchanged). Note that each tuple in the relations contributes to only constantly many tuples of N.
So |N | = O(m), and we can construct N in linear time.
Let ψ (x,y, z) be a quantifier-free formula. We define ψ∗ (x,y, z) be ψ (x,y, z), where all occurrences of ternary predicates are replaced by false. Thus, it contains only unary and binary
predicates. Formula (∃x)(∃y)(∀z)ψ (x,y, z) is equivalent to (∃x)(∃y)(∀z)[N (x,y) ∧ψ (x,y, z)] ∨
(∃x)(∃y)(∀z)[¬N (x,y) ∧ψ∗ (x,y, z)].
We can decide (∃x)(∃y)(∀z)[¬N (x,y) ∧ψ∗ (x,y, z)] using the algorithm for graphs, because all
relations are binary. To decide (∃x)(∃y)(∀z)[N (x,y) ∧ψ (x,y, z)], we consider three types of x’s
and y’s. Let deд(x) be the degree of x in the Gaifman graph.
• Type 1: deд(x) ≥ √
m. It is similar to deciding “large sets” for Basic Problems in Section 5.1.1. In the outer loop, enumerate all such x’s. For each x, we modify the model checking problem to an instance of MC(2), by treating x as a constant. The number of such x’s
is at most O(m/
√
m) = O(
√
m), and deciding an MC(2) problem runs in time O(m). So the
total running time is O(
√
m · m) = O(m3/2).
• Type 2: deд(y) ≥ √
m. Use the same method as above by exchanging the order of x and y.
The running time is also O(m3/2).
• Type 3: deд(x) < √
m and deд(y) < √
m. Enumerate all pairs of such x’s and y’s. Then
in the inner loop, we enumerate all their neighbors in Z. In this way, for each z ∈ Z such
that z is a neighbor of x or y, we can categorize it by the truth value of all predicates. For
all other z’s, we know all the predicates are false. Thus we can decide if all z ∈ Z satisfy ψ.
Because all these x’s and y’s are adjacent, the time for enumerating pairs of x and y isO(m),
and the time for enumerating all their neighbors in Z is O(
√
m). So the total running time
is O(
√
m · m) = O(m3/2).
Thus, for each pair (x,y), where N (x,y) = true, we can decide the model checking for
(∀z)ψ (x,y, z) in time O(m3/2). For each pair (x,y), where N (x,y) = false, (∀z)ψ (x,y, z) is true
iff (∀z)[¬N (x,y) ∧ψ∗ (x,y, z)].
Similarly, for MC(∃k∀) problems where φ = (∃x1) ... (∃xk )(∀xk+1)ψ (x1,..., xk+1), we still consider the cases whether there exist some hyperedge between any pair of xi, xj , where i, j ≤ k. We
ACM Transactions on Algorithms, Vol. 15, No. 2, Article 23. Publication date: December 2018.  
Completeness for FO Properties on Sparse Structures 23:27
define relation N (xi, xj) = true iff there exists some xk such that there is some hyperedge containing vertices xi, xj . We also defineψ∗ (x1,..., xk+1) beψ (x1,..., xk+1), where all occurrences of
predicates with arities greater than two are replaced by false. So
φ = (∃x1) ... (∃xk )(∀xk+1)
⎡
⎢
⎢
⎢
⎢
⎢
⎣

i,j ∈ {1,...,k },ij
(N (xi, xj) ∧ψ (x1,..., xk+1)
⎤
⎥
⎥
⎥
⎥
⎥
⎦
∨
⎡
⎢
⎢
⎢
⎢
⎢
⎣



	
i,j ∈ {1,...,k },ij
¬N (xi, xj)



∧ψ∗ (x1,..., xk+1)
⎤
⎥
⎥
⎥
⎥
⎥
⎦
=

i,j ∈ {1,...,k },ij

(∃x1) ... (∃xk )(∀xk+1)[N (xi, xj) ∧ψ (x1,..., xk+1)]

∨ (∃x1) ... (∃xk )(∀xk+1)
⎡
⎢
⎢
⎢
⎢
⎢
⎣



	
i,j ∈ {1,...,k },ij
¬N (xi, xj)



∧ψ∗ (x1,..., xk+1)
⎤
⎥
⎥
⎥
⎥
⎥
⎦
.
To decide (∃x1) ... (∃xk )(∀xk+1)[N (xi, xj) ∧ψ (x1,..., xk+1)], we do exhaustive search on the
k − 2 variables other than xi and xj (which in essence is a quantifier-eliminating downward reduction), which takes a factor of O(mk−2) in the running time. Then we process the variables xi ,
xj , xk in the same way as variables x, y, z in the three-quantifier problem, that takes time O(m3/2).
The total running time is O(mk−1/2).
To decide (∃x1) ... (∃xk )(∀xk+1)[(

i,j ∈ {1,...,k },ij ¬N (xi, xj)) ∧ψ∗ (x1,..., xk+1)], we can use
the algorithm for MC(∃k∀) problems on graphs, because the new formula has only unary and
binary relations.
8 HARDNESS OF k-OV FOR MC(∀∃K −1∀)
In this section, we present an exact complexity reduction from any MC(∀∃k−1∀) problem to a
MC(∃k∀) problem, establishing the hardness of k-OV for these problems. This reduction gives
an extension of the reduction from Hitting Set to Orthogonal Vectors in Reference [4] to sparse
structures.
Lemma 8.1. For k ≥ 2 and s(m) a non-decreasing function such that 2Ω(√logm) ≤ s(m) < m1/5, let
φ	 = (∃x2) ... (∃xk )(∀xk+1)ψ (x1,..., xk+1). There is an exact complexity reduction


MC(∀x1 )φ	, mk
s(
√
m)

≤EC 

MC(∃x1 )φ	, mk
s(m)

.
First, we show that in problem MC(∃x1 )φ	, if graph G satisfies (∃x1)φ	
, then we can find a satisfying value v1 for variable x1 by binary search. We divide the set V1 into two halves, take each
half of V1 and query whether (∃x1)φ	 holds true on the graph induced by this half of V1 together
with the original sets V2,...,Vk+1. If any half of V1 works, then we can shrink the set of candidate
values for x1 by a half, and then recursively query again, until there is only one vertex v1 left. So
it takes O(log |V1 |) calls to find a v1 in some solution. This means as long as there is a solution for
MC∃x1φ	, we can find a satisfying v1 efficiently, with O(logm) queries to the decision problem.
Step 1: Large degree vertices. Let t = m(k−1)/k . We deal with vertices in V1 ...Vk with degree
greater than t. There are at most m/t = m1/k such vertices. After pre-computing the sizes of all
the sets, these large sets can be listed in time O(m1/k ).
• Step 1-1: Large degree vertices in V1. For each vertex v1 ∈ V1 with degree at least t, we
create a formula ψv1 on variables x2,..., xk+1 from formula ψ, by replacing occurrences of
ACM Transactions on Algorithms, Vol. 15, No. 2, Article 23. Publication date: December 2018.         
23:28 J. Gao et al.
unary predicates in ψ on x1 by constants, and replacing occurrences of binary predicates
involving x1 by unary predicates on the other variables. Then we check if the graph induced
by V2,...,Vk+1 satisfies (∃x2) ... (∃xk )(∀xk+1)ψv1 (x2,..., xk+1) by running the baseline algorithm in time O(mk−1). If the new formula is satisfied, then we mark v1 as “good.” The
total time complexity is O(m1/k ) · O(mk−1) = O(mk−1+1/k ).
• Step 1-2: Large degree vertices inV2,...,Vk .Now we exhaustively search over all vertices
v1 ∈ V1 with degree less than t in the outermost loop. For each such v1, we find out all vertices vi ∈ Vi for 2 ≤ i ≤ k, with degree at least t. Again, there are at most O(m1/k ) of them.
◦ Case 1: k > 2. Because variables x2 through xk are all quantified by ∃, we interchange their order so that the variable xi becomes the second-outermost variable x2
(and thus the current vi becomes v2). Next, for each v1 and v2 we construct a new formula ψ(v1,v2 ) on variables x3,..., xk+1, by regarding x1 and x2 as fixed values v1 and v2,
and then modify ψ into ψ(v1,v2 ) similarly to the previous step. Again, we run the baseline algorithm to check whether the graph induced by the current V3,...,Vk+1 satisfies (∃x3) ... (∃xk+1)ψ(v1,v2 ) (x3,..., xk+1), using time O(mk−2). If the formula is satisfied, then we mark the current v1 as “good.” The total time complexity is O(m · m1/k ) ·
(mk−2) = O(mk−1+1/k ).
◦ Case 2: k = 2. For each vertex v2, we mark all the v1’s satisfying ∀x3ψ (v1,v2, x3) as
“good”. This can be done in O(m) using the algorithm for the base case of the baseline
algorithm, by treating the current v2 as constant. So this process runs in time O(m1/k ) ·
O(m) = O(m3/2).
If not all vertices inV1 with degree at leastt are marked “good,” then we reject. Otherwise,
go to Step 2.
Step 2: Small degree vertices. First, we exclude all the large vertices from the graph. Then for
the “good” vertices found in the previous step, we also exclude them from V1.
Now all vertices have degree at most t. In each of V1,...,Vk , we pack their vertices into groups
where in each group the total degree of vertices is at most t. Then the total number of groups is
bounded by O(m/t).
For each k-tuple of groups (G1,...,Gk ), where G1 ⊆ V1,...,Gk ⊆ Vk , we query the oracle deciding MC(∃x1 )φ	 whether it accepts on the subgraph induced by vertices in G1,...,Gk . If so, then
we find a vertex v1 in V1 so that when x1 ← v1, the current subgraph satisfies φ	
. We remove this
v1 from V1. Then we repeat this process to find new satisfying v1’s in V1, and remove these v1’s
from V1. When V1 is empty, or when no new solution is found after all group combinations are
exhausted, the algorithm terminates. If in the end V1 is empty, then all v1 ∈ V1 are in solutions of
MC∃x1φ	, so we accept. Otherwise, we reject.
Each query to MC∃x1φ	 has size m	 = O(kt) = O(t). Because the number of different k-tuples
of groups is O(m/t)
k = O((m/t)
k ), the number of queries made is O((m/t)
k + |V1 |) · O(logm) =
O((m1/k )
k + |V1 |) · O(logm) = O(m logm) times. If MC∃x1φ	 on input size m	 is solvable in time
O(m	k /s(m	
)), then the running time for MC∀x1φ	 is O(m logm) · O(m	k /s(m	
)) = O(m logm ·
(m(k−1)/k )
k /s(m(k−1)/k ) ≤ O(mk /s(
√
m) · logm). The exponent of m is less than k. Thus this is
a fine-grained Turing reduction. Lemma 8.1 follows.
Note that this reduction works not only on graphs but also on structures with relations of arity
greater than two.
9 BASELINE AND IMPROVED ALGORITHMS
In this section, we first present a baseline algorithm for MC(k + 1) that runs in time O(nk−1
m),
which also implicitly gives us a quantifier-eliminating downward reduction from any MC(k + 1)
ACM Transactions on Algorithms, Vol. 15, No. 2, Article 23. Publication date: December 2018.
Completeness for FO Properties on Sparse Structures 23:29
problem to MC(k) problems for k ≥ 2. Then, we show how to get an improved algorithm in time
mk /2Θ(√logm) using our reductions and the result in References [3, 16]. Finally, we present the
algorithms for some specific quantifier structures inO(m3/2), so that these problems are easy cases
in first-order property problems.
9.1 Baseline Algorithm for First-order Properties
This section gives an O(nk−1
m) time algorithm solving MC(k + 1) with any quantifier structure
for k ≥ 1, thus proving Lemma 9.1.
Lemma 9.1 (Quantifier-eliminating downward reduction for MC(k + 1)). Let the running
time of MC(k + 1) on graphs of n vertices and m edges be Tk (n,m). We have the recurrence
Tk (n,m) ≤ n · Tk−1 (n,O(m)) + O(m), for k ≥ 2.
T1 (n,m) = O(m).
By this lemma, if all problems in MC(k) have algorithms in time T (n,m), then any problem in
MC(k + 1) can be solved in time n · T (n,m).
Base Case. We prove that when k = 1, Tk (n,m) = m. For each v1 ∈ V1, the algorithm computes
#(v1) = |{v2 ∈ V2 | (v1,v2) |= ψ }|. Thus we can list the sets of v1 s.t. #(v1) > 0 (if the inner quantifier is ∃), or those that satisfy #(v1) = |V2 | (if it is ∀).
Let there be p1 different unary predicates on v1 and p2 different unary predicates on v2. We
partition the universes V1 and V2, respectively, into 2p1 and 2p2 subsets, based on the truth values
of all the unary predicates of the corresponding variable. The number of different pairs of subsets
is a constant. Each time, we pick a pair consisting of one subset from V1 and one subset from V2,
and replace the unary predicates by constants. In this way, we can just consider binary predicates
in the following argument.
Letψ¯(v1,v2) be the formula where each occurrence of each negated binary relation Ri (v1,v2) is
replaced by false. We enumerate all tuples (v1,v2) connected by at least one edge. For each tuple,
we evaluate ψ (v1,v2) and ψ¯(v1,v2). Let
#ψ (v1) =

v2 adjacent to v1
([ψ (v1,v2) = true] − [ψ¯(v1,v2) = true])
(in which the brackets are Iverson brackets). It can be computed by enumerating all tuples (v1,v2)
connected by at least one edge. Next, because in ψ¯ there are no occurrences of negated binary
predicates, we can compute
#ψ¯(v1) = The number of v2 s.t. ψ¯(v1,v2) holds
by first enumerating all tuples (v1,v2) connected by at least one edge and checking if ψ¯(v1,v2)
holds, and then considering the number of non-neighboring v2’s for each v1, if being a nonneighbor of v1 also makes ψ¯(v1,v2) true. Finally, let #(v1) = #ψ (v1) + #ψ¯(v1).
This algorithm is correct, because whenever a pair (v1,v2) satisfiesψ (v1,v2), there are two cases.
The first is that there exists an edge between v1 and v2. In this case, when we enumerate all edges,
[ψ (v1,v2) = true] equals one and [ψ¯(v1,v2) = true] equals its contribution to #ψ¯(v1). However, if
there does not exist an edge between v1 and v2, then the contribution of (v1,v2) to #ψ (v1) is 0 and
to #ψ¯(v1) is 1.
Whenever a pair (v1,v2) does not satisfy ψ (v1,v2), there are also two cases. If there exists an
edge between v1 and v2, when we enumerate all edges, then [ψ (v1,v2) = true] equals zero and
ACM Transactions on Algorithms, Vol. 15, No. 2, Article 23. Publication date: December 2018.
23:30 J. Gao et al.
[ψ¯(v1,v2) = true] equals its contribution to #ψ¯(v1). However, if there does not exist an edge between v1 and v2, the contributions of (v1,v2) to #ψ (v1) and to #ψ¯(v1) are both 0.
Inductive Step. For k ≥ 2, we give a quantifier-eliminating downward reduction, thus proving the
recurrence relation. Assume φ = (Q1x1) ... (Qk+1xk+1)ψ (x1,..., xk+1) For eachv1 ∈ V1, create new
formula φv1 = (Q2x2) ... (Qk+1xk+1)ψ (x2,..., xk+1), and inψ we replace each occurrence of unary
predicate Ri (x1) with a constant Ri (v1), and replace each occurrence of binary predicate Ri (x1, xj)
(or Ri (xj, x1)) with unary predicate R	
i (xj) whose value equals Ri (v1, xj) (or Ri (xj,v1)), and so on.
Our algorithm enumerates all v1 ∈ V1, and then computes if the graph induced by V2,...,Vk+1
satisfies φv1 . If x1 is quantified by ∃, then we accept iff any of them accepts. Otherwise, we accept
iff all of them accept. The construction of φv1 takes timeO(m). The created graph hasO(n) vertices
and O(m) edges. Thus the recursion follows.
This process is a quantifier-eliminating downward reduction from an MC(k + 1) problem to
an MC(k) problem. It makes O(m) queries, each of size O(m). Then if problems in MC(k) are
solvable in time O(mk−1−ϵ ), then problems in MC(k + 1) are solvable in time m · O(mk−1−ϵ ) =
O(mk−ϵ ). This quantifier-eliminating downward reduction implies that if all MC(k) have T (n,m)
time algorithms, then all MC(k + 1) problems have n · T (n,m) time algorithms.
From the recursion and the base case, we have the running time O(nk−1
m) by induction. The
quantifier-eliminating downward reduction from MC(k + 1) to MC(3) in Lemma 9.1 also works
for hypergraphs. We exhaustively search the first k − 2 quantified variables, and by replacing the
occurrences of these variables by constants in the formula, we can reduce the arities of relations.
After the reduction, we get a hypergraph of max arity at most three.
9.2 Algorithms for Easy Cases
In this section, we show that any (k + 1)-quantifier problem with a quantifier sequence ending
with ∃∃ or ∀∀ is solvable in time O(mk−1/2). First, we use the quantifier-eliminating downward
reduction to reduce the problem to a MC(3) problem. Then from the next subsections, we see that
these problems are solvable in O(m3/2). Reference [35] shows improved algorithms that run in
time O(m1.41) for detecting triangles and detecting induced paths of length 2, which are special
cases of MC(∃∃∃).
Lemma 9.2. Problems in MC(∃∃∃), MC(∀∀∀), MC(∀∃∃), and MC(∃∀∀) are solvable in O(m3/2).
In the first two subsections, we consider when the input structures are graphs. Then in the last
subsection, we consider the cases when the input structures have higher arity relations.
9.2.1 Problems in MC(∃∃∃) and MC(∀∀∀). For problems in MC(∀∀∀), we decide its negation,
which is a MC(∃∃∃) problem.
We define nine Atomic Problems, which are special MC(3) problems. Let the Atomic Problem labeled by  to be MC(∃x ∈X )(∃y ∈Y )(∃z ∈Z ) ψ , and referred to as Δ[]. It is defined on a tripartite graph
on vertex sets (X,Y,Z), whose edge sets are EX Y , EY Z , EX Z defined on (X,Y ), (Y,Z), (X,Z), respectively. The graph is undirected, i.e., EX Y , EY Z , and EX Z are symmetric relations. For simplicity,
we define an edge predicate E so that E(v1,v2) is true iff there is an edge in any of EX Y , EY Z , EX Z
connecting (v1,v2) or (v2,v1). Besides, we use deдY (x) to denote the number of x’s neighbors in Y.
The ψ for all Atomic Problems are defined in Table 1. For problem MCφ, where φ = (∃x ∈ X)
(∃y ∈ Y )(∃z ∈ Z)ψ (x,y, z), we write ψ as a DNF, and split the terms. Then we decide if there
is a term so that there exist x,y, z satisfying this term. On each term t, which is a conjunction
of predicates and negated predicates, we work on the induced subgraph whose vertices satisfy
all the positive unary predicates and falsify all the negated unary predicates defined on them in
t. Then we can remove all unary predicates from the conjunction, which is now a conjunction
ACM Transactions on Algorithms, Vol. 15, No. 2, Article 23. Publication date: December 2018.    
Completeness for FO Properties on Sparse Structures 23:31
Table 1. Atomic Problems
ψ2 = E(x, y) ∧ E(x, z) ψ2+ = E(x, y) ∧ E(x, z) ∧ E(y, z) ψ2− = E(x, y) ∧ E(x, z) ∧ ¬E(y, z)
ψ1 = E(x, y) ∧ ¬E(x, z) ψ1+ = E(x, y) ∧ ¬E(x, z) ∧ E(y, z) ψ1− = E(x, y) ∧ ¬E(x, z) ∧ ¬E(y, z)
ψ0 = ¬E(x, y) ∧ ¬E(x, z) ψ0+ = ¬E(x, y) ∧ ¬E(x, z) ∧ E(y, z) ψ0− = ¬E(x, y) ∧ ¬E(x, z) ∧ ¬E(y, z)
of binary predicates or their negations. (If the conjunction is a single predicate or a single
negated predicate, then we can deal with it easily, so we do nt consider this case here.) If we
define E(x,y) = 
R is a positive binary predicate in t R(x,y) ∧ 
R is a negative binary predicate in t ¬R(x,y), and
define E(y, z) and E(x, z) similarly, then t becomes equivalent with some Atomic Problem, or
a disjunction of Atomic Problems (because variables y and z are interchangeable, the Atomic
Problems and their disjunctions cover all possible cases).
In our algorithm for each problem Δ[], instead of deciding the existence of satisfying x,y, z,
we consider these problems as counting problems, where for each x we compute
# (x) = |{(y, z) | x,y, z satisfy ψ }|.
Problems Δ[2], Δ[1], Δ[0] can be computed straightforwardly.
• In Δ[2], #2 (x) = deдY (x) × deдZ (x).
• In Δ[1], #1 (x) = deдY (x) × (|Z | − deдZ (x)).
• In Δ[0], #0 (x) = (|Y | − deдY (x)) × (|Z | − deдZ (x)).
Next we show for labels  ∈ {2+, 1+, 0+, 2−, 1−, 0−}, problems Δ[] can be computed inO(m3/2).
Algorithm 1 solves Δ[2+],that is, for each x, counting the number of triangles that contain
x. The first part of the algorithm only considers small degree y. On each iteration of the outer
loop, the inner loop is run for at most √
m times. The second part only considers large degree y.
Because there are at most √
m of them, the outer loop is run for at most √
m times. Therefore the
running time of the algorithm is O(m3/2).
ALGORITHM 1: Δ[2+]
1: for all (x,y) ∈ EX Y do  Small degree y
2: if deдZ (y) ≤ √
m then
3: for all z s.t. (y, z) ∈ EY Z do
4: if (x, z) ∈ EX Z then
5: #2+(x) ← #2+(x) + 1
6: end if
7: end for
8: end if
9: end for
10: for all y ∈ Y s.t. deдZ (y) > √
m do  Large degree y
11: for all (x, z) ∈ EX Z do
12: if (x,y) ∈ EX Y and (y, z) ∈ EY Z then
13: #2+(x) ← #2+(x) + 1
14: end if
15: end for
16: end for
17: if #2+(x) > 0 for some x ∈ X then Accept
18: else Reject
19: end if
ACM Transactions on Algorithms, Vol. 15, No. 2, Article 23. Publication date: December 2018.       
23:32 J. Gao et al.
Algorithm 2 solves Δ[1+], which for each x counts (x − y − z) paths where there is no edge
between x and z. The first part is similar as Δ[2+]. The second part first over-counts (x − y − z)
paths for all large degree y without restricting the edge between x and z, and then counts the
number of over-counted cases to exclude them from the final result. In the first block, the inner
loop is run for at most √
m times for each edge in EX Y . The second block takes time O(m). The
outer loop of the third block is run for at most √
m times, because there are at most √
m sets with
degree at least √
m. So in all, the running time is O(m3/2).
ALGORITHM 2: Δ[1+]
1: for all (x,y) ∈ EX Y do  Small degree y
2: if deдZ (y) ≤ √
m then
3: for all z s.t. (y, z) ∈ EY Z do
4: if (x, z)  EX Z then
5: #1+(x) ← #1+(x) + 1
6: end if
7: end for
8: end if
9: end for
10: for all (x,y) ∈ EX Y do  Large degree y
11: if deдZ (y) ≥ √
m then  Over-counting
12: #1+(x) = #1+(x) + deдZ (y)
13: end if
14: end for
15: for all y ∈ Y s.t. deдZ (y) > √
m do
16: for all (x, z) ∈ EX Z do
17: if (x,y) ∈ EX Y and (y, z) ∈ EY Z then
18: #1+(x) ← #1+(x) − 1
19:  if we just over-counted the pair (y, z),then we exclude the pair by subtracting one.
20: end if
21: end for
22: end for
23: if #1+(x) > 0 for some x ∈ X then Accept
24: else Reject
25: end if
For Δ[0+], we first compute #2+(x), which is the result of Δ[2+], and then compute #1+(x) and
#	
1+(x), which are results of Δ[1+] on vertex sets (X,Y,Z) and (X,Z,Y ), respectively. Finally, let
#0+(x) ← |EY Z | − (#2+(x) + #1+(x) + #	
1+(x)).
#2−(x), #1−(x), #0−(x) can be computed by respectively taking the differences of
#2 (x), #1 (x), #0 (x) and #2+(x), #1+(x), #0+(x).
9.2.2 Problems in MC(∀∃∃) and MC(∃∀∀). For problems in MC(∃∀∀), we decide its negation,
which is a MC(∀∃∃) problem.
For problem MCφ, where φ = (∀x ∈ X)(∃y ∈ Y )(∃z ∈ Z)ψ (x,y, z), we use the same algorithm
to compute # (x) for all x ∈ X. If the value of # (x) is greater than zero for all x ∈ X, then we
accept, otherwise reject. Again, we write ψ as a DNF, and split the terms. By the same argument
as the previous lemma, we transform the problem to a disjunction of Atomic Problems. If for all
ACM Transactions on Algorithms, Vol. 15, No. 2, Article 23. Publication date: December 2018.       
Completeness for FO Properties on Sparse Structures 23:33
x ∈ X, at least in one of the Atomic Problem, # (x) is greater than zero, then we accept; otherwise,
reject.
9.2.3 Structures with Higher Arity Relations. The above algorithms can be extended to structures with relations of arity greater than two. First, we write the quantifier-free partψ in DNF and
split each term to a separate ∃∃∃ problem (or ∀∃∃, respectively). Then for each termψt , we decide
if there exist x1, x2, x3 satisfying it. Let ψt 1 be the part of the conjunction containing all ternary
predicates in ψt , and ψt 2 be the rest of term ψt . Thus ψt = ψt 1 ∧ψt 2.
If in ψt some ternary predicate occurs positively, then we can just count #(x1) on the subgraph
where ψt 1 is true.
If all ternary predicates in ψt occur negatively, then we first count #(x1) satisfying formula ψt 2,
and then we count #	
(x1) on the subgraph where ψt 1 is true. Finally, we subtract #	
(x1) from #(x1)
for each x1.
If ψt has no ternary relations, then we just count #(x1) using the algorithm for graphs.
10 IMPROVED ALGORITHMS
In this section, we present an algorithm solving Sparse OV in time m2/2Θ(√logm)
. It is based on
the articles [3, 16], which solves dense OV for vectors of dimension d in time n2−Ω(1/ log(d/ log n)).
Consider the universe-shrinking self-reduction for Sparse OV (Set Disjointness) in Section 6.
We show that for s(m) = 2Θ(√logm)
, by the above theorem, this reduction gives an algorithm in
time m2/2Θ(√logm)
. We deal with large sets and small sets separately. For sets of size at least
s(m), we check if each of them is disjoint with some other set. From the argument for large sets,
this is in time m2/s(m). Then, for sets of size less than s(m), we use the universe-shrinking selfreduction to reduce this instance to a Sparse OV instance on universe of size s(m)
5
6k (in which case
k = 2). Using the algorithm from References [3, 16], we can solve it in time n2−Θ(1/ log(s (m)
5
6k )) ≤
m2−Θ(1/ log(s (m)) ≤ m2/2Θ(logm/ log s (m)) = m2/2Θ(√logm)
. So the total running time is bounded by
m2/2Θ(√logm)
.
By the above argument and Theorem 1.2, since all the Basic Problems are solvable in time
m2/2Θ(√logm)
, so is any other problem in MC(∃∃∀). The reduction from MC(∀∃∀) to MC(∃∃∀)
in Section 8 gives 2Θ(√logm) savings for MC(∀∃∀) problems. Reducing to three-quantifier case by
brute-forcing over the first k − 2 variables we get Theorem 1.4, that states all MC(k + 1) problems
can be solved in mk /2Θ(√logm) time.
11 OPEN PROBLEMS
An obvious open problem is whether a similar kind of equivalence exists for the dense case of OV.
Is it ”fine-grained equivalent” to some natural complexity class?
Our results raise the possibility that many other classes have complete problems under finegrained reducibility, and that this will be a general method for establishing the plausibility of
conjectures on the fine-grained complexity of problems. There is a number of candidates for such
classes. We could drop the restriction that the formula has k quantifiers in all, and look at formulas
with quantifier depth k.
9 We could also stratify the first-order formulas by variable complexity,
the number of distinct variable names in a formula, rather than number of quantifiers. (Variable
complexity arises naturally in database theory, because the variable complexity determines the
arity of some relation in any way of expressing the query as a sequence of sub-queries.) First-order
9For example, ∃(x )(∃y(∃zψ1 (x, y, z) ∧ ∀zψ2 (x, y, z)) ∧ ∀y(∃zψ3 (x, y, z) ∨ ∀zψ4 (x, y, z))) has quantifier depth 3.
ACM Transactions on Algorithms, Vol. 15, No. 2, Article 23. Publication date: December 2018. 
23:34 J. Gao et al.
logic is rather limited, so we could look at augmentations that increase its reach, such as allowing a
total ordering on elements, or allowing the logic to take transitive closures of relations (e.g., to talk
about the reachability relation in a sparse directed graph), or more generally, introduce monotone
fixed point operations. Alternatively, rather than varying the types of formulas, we could restrict
the types of structures, for example, considering structures of bounded treewidth.
It would be interesting to find more reductions between and equivalences among the problems
that are proven hard under some conjecture. For example, Edit Distance, Fréchet Distance, and
Longest Common Subsequence are all almost quadratically hard assuming SETH. Are there any
reductions between these problems? Are they all equivalent as far as having subquadratic algorithms? All of these problems have similar dynamic programming formulations. Can we formalize
a class of problems with such dynamic programming algorithms and find complete problems for
this class? More generally, we would like taxonomies of the problems within P that would classify
more of the problems that have conjectured hardness, or have provable hardness based on conjectures about other problems. Such a taxonomy might have to be based on the structure of the
conjectured best algorithms for the problems rather than on resource limitations.