We consider a distributed private data analysis setting, where multiple parties each hold some sensitive data
and they wish to run a protocol to learn some aggregate statistics over the distributed dataset, while protecting each user’s privacy. As an initial effort, we consider a distributed summation problem. We first show a
lower bound, that is, under information-theoretic differential privacy, any multi-party protocol with a small
number of messages must have large additive error. We then show that by adopting a computational differential privacy notion, one can circumvent this lower bound and design practical protocols for the periodic
distributed summation problem. Our construction has several desirable features. First, it works in the clientserver model and requires no peer-to-peer communication among the clients. Second, our protocol is fault
tolerant and can output meaningful statistics even when a subset of the participants fail to respond. Our
constructions guarantee the privacy of honest parties even when a fraction of the participants may be compromised and colluding. In addition, we propose a new distributed noise addition mechanism that guarantees
small total error.
CCS Concepts: • Security and privacy → Data anonymization and sanitization;
Additional Key Words and Phrases: Differential privacy, distributed private data analysis, periodic aggregation, untrusted aggregator
1 INTRODUCTION
Many real-world applications have benefited tremendously from the ability to collect and mine
data coming from multiple individuals and organizations. These applications have also spurred
numerous concerns over the privacy of user data. In this article, we consider a scenario where n
mutually distrustful users each hold some sensitive data, and an untrusted data aggregator wishes
to compute some aggregate statistics (e.g., sums, averages, variance, distribution) over the n users’
inputs. In this article, we use the term “user” to refer to a party who has private data. Since we
consider parties interacting in some network, we sometimes also refer to a party as a “node” where
appropriate. We will investigate how to design protocols that guarantee the privacy of each user’s
inputs, while still allowing the untrusted aggregator to derive utility out of the distributed dataset.
Throughout this article, we assume a semi-honest adversary who controls the aggregator and
possibly a corrupt coalition of other nodes.
A natural question is what formal privacy notion we should adopt in this setting. One possibility
is to use standard secure multi-party computation (MPC) [23]. MPC allows the n data owners
and the aggregator to engage in a multi-party protocol, at the end of which the aggregator learns
the desired statistic, and no party learns any additional information, MPC guarantees that only
the desired statistic is revealed but does not answer the meta-question whether the statistic itself
is safe to release or which statistic to release. Some statistics, when released, can be harmful to
privacy, for example, releasing the maximum revenue of n business can harm the privacy of the
business with the highest revenue.
Differential privacy, initially proposed by Dwork et al. [14, 17], has become a de facto privacy
notion in the community and allows us to ensure that a statistic is safe to release through sanitization of the output statistic by adding random noise. Intuitively, differential privacy guarantees
that the output statistic should not be swayed too much by changing one individual’s input. This
means that by participating in the dataset or protocol, an individual should not be exposed to too
much privacy risk.
Traditionally, differential privacy was proposed and studied in a setting with a trusted curator
who has access to all users’ data in the clear [14, 15, 17], and the trusted curator is responsible
for introducing appropriate perturbations prior to the publication of any statistics. This setting is
particularly useful when a company or a government agency, in the possession of a dataset, would
like to share it with the public.
In many real-world applications, however, sensitive data are distributed among users (or organizations), and users may not wish to entrust their sensitive data to a centralized party such as
a cloud service provider. In this article, we will consider how to achieve differential privacy in a
distributed data analysis scenario, where participating users are mutually distrustful, and a subset
of the users may be compromised and colluding.
Under this setting, we investigate a distributed summation problem, where an untrusted aggregator wishes to compute the sum of n users’ inputs. We consider both information-theoretic differential privacy (or differential privacy for short) and computational differential privacy (CDP) [31].
Information-theoretic differential privacy secures against a computationally unbounded adversary, while computational differential privacy secures against polynomial-time adversaries.
We prove lower bounds on the utility of distributed data analysis satisfying informationtheoretic differential privacy, with assumptions only on the sparsity of the communication graph,
thereby relaxing constraints on the number of messages and rounds in previous lower bounds.
To circumvent such lower bounds in practice, we consider computational differential privacy. We
demonstrate constructions that satisfy computational differential privacy while achieving asymptotically smaller error using techniques from efficient homomorphic encryption and secure MPC.
Our constructions also offer several properties that will be desirable in a practical deployment
scenario, including support for periodic aggregation, no peer-to-peer communication, and fault
tolerance.
ACM Transactions on Algorithms, Vol. 13, No. 4, Article 50. Publication date: December 2017.
Distributed Private Data Analysis 50:3
1.1 Formal Privacy Notions
To achieve differential privacy, we introduce two formal notions: aggregator obliviousness and distributed differential privacy. Aggregator obliviousness means that the aggregator can learn nothing
more than what can be inferred from the function output. The idea of distributed differential privacy (DD-privacy) is that each user is responsible for generating randomness to perturb its input
such that the output of the function satisfies differential privacy. Our privacy notions also capture
the scenario that a subset of compromised users colludes with the aggregator and still guarantee
the privacy of other users.
1.2 Optimal Lower Bounds for Information-Theoretic Differential Privacy
In Section 3, we consider information-theoretic differential privacy (or differential privacy for
short). We prove lower-bounds both for the general setting of n-party protocols and for the clientserver model,
1 where the only communication allowed is between each user (i.e., client) and the
aggregator (i.e., server).
Loosely speaking, we show that if an aggregator wishes to compute the sum of users’ numbers,
then it must make a large additive error if the protocol preserves differential privacy. We also show
that both of these lower bounds are tight.
Lower Bound for the General Setting (Corollary 3.3). Informally, we show that any n-party
protocol computing the sum, which consumes at most 1
4n(t + 1) messages must incur Ω(√
n) additive error (with constant probability) to preserve differentially privacy against coalitions of up
to t compromised users.
Lower Bound for Client-Server Model (Corollary 3.2). Informally, we show that in the clientserver model, an aggregator would make an additive error Ω(√
n) on the sum from any n-user
protocol that preserves differential privacy. This lower-bound holds regardless of the number of
messages or number of rounds.
Tightness of the Lower Bounds. Both of the aforementioned lower bounds are tight in the
following sense. First, for the client-server model, there exists a naïve protocol, in which each user
perturbs their inputs using Laplace or geometric noise with standard deviation O( 1
ϵ ) and reveals
their perturbed inputs to the aggregator. Such a naïve protocol has additive error O(
√
n); so in
some sense, the naïve protocol is the best one can do in the client-server model.
To see why the lower bound is tight for the general multi-party setting, we combine standard
techniques of secure function evaluation [12] and distributed randomness and show in Section 4.1
that there exists a protocol which requires only O(nt) messages but achieves o(
√
n) error.
Techniques. To prove the aforementioned lower-bounds, we combine techniques from communication complexity and measure anti-concentration techniques used in the metric embedding
literature. Our communication complexity techniques are inspired by the techniques adopted by
McGregor et al. [28], who proved a gap between information-theoretic and computational differential privacy in the two-party setting. The key observation is that independent inputs remain
independent even after conditioning on the transcript of the protocol. As argued by in Reference
[1], if a user communicates with only a small number of other parties, then there must still be
sufficient randomness in that user’s input. Then, using anti-concentration techniques, we show
that the sum of these independent random variables is either much smaller or much larger than
1In the literature of distributed computing, the client-server model also refers to the paradigm in which the server is a
provider of data or computing service to the clients, which typically have limited storage or computing power. However,
as in other works in the cryptography community [25], the term is used in the context of secure multi-party computation
to refer to protocols in which there is a single central party (server) through which all other parties (clients) communicate.
ACM Transactions on Algorithms, Vol. 13, No. 4, Article 50. Publication date: December 2017.
50:4 E. Shi et al.
the mean, both with constant probability, thereby giving a lower bound on the additive error.
Moreover, we generalize the techniques to prove the lower bound for (ϵ, δ )-differentially private
protocols (as opposed to just ϵ-differential privacy). The challenge is that for δ > 0, it is possible for some transcript to break a user’s privacy and there might not be enough randomness left
in its input. However, we show that for small enough δ, the probability that such a transcript is
encountered is small, and hence the argument is still valid.
1.3 Practical Constructions for Distributed Private Data Analysis and Fault Tolerance
We design protocols that achieve O(1) error for summation with small communication cost under the model of computational differential privacy. One natural idea for achieving this is to have
the parties rely on generic multi-party computation (MPC) to “realize” the trusted curator that
implements a differentially private mechanism [1, 16]. Standard MPC techniques ensure that the
aggregator learns nothing but the outcome—thus instead of having each party add “one copy” of
noise to satisfy differential privacy of their individual inputs (in this case the sum of the noises
will be Θ(√
n) in magnitude), it suffices for each party to add a smaller amount of noise individually, such that the sum of all their noises adds up to roughly “one copy” [16]. This allows us to
circumvent the aforementioned information-theoretic lower bounds for small communication.
Our work builds on this general idea, but we make the following contributions.
Combining Multi-party Computation and Distributed Differential Privacy. In Section 4,
we formally decompose the task of achieving (possibly computational) differential privacy in distributed protocols into two-step recipe:
(1) each party first adds some independent noise to their respective input—a process referred
to as “distributed noise generation” [16] and
(2) they then engage in an “input indistinguishable” multi-party computation protocol to
compute the sum of the noisy inputs.
We decompose the proof of formal security into
• the notion of “distributed differential privacy” that characterizes what security is expected
from the distributed noise generation process.
• the notion of (possibly computational) “aggregator obliviousness” against a corrupt coalition that characterizes the input indistinguishability requirement expected from the underlying MPC protocol [30]—for general functions this is a weaker notion than standard,
simulation-based security.
We show that as long as the two building blocks satisfy the two aforementioned notions of security respectively, then the resulting protocol satisfies (possibly computational) differential privacy
against a corrupt coalition.
Note that although earlier works have also made the observation to combine multi-party computation and distributed noise generation [1, 16], we make the contribution of formulating this
approach as a generic framework; one interesting observation we make is that instead of requiring the MPC protocol to have a strong, simulation-secure notion of security, in fact, the weaker
notion of input indistinguishability [30] suffices.
Practical and Fault-Tolerant Constructions. Although in general, one could rely on any
generic MPC protocol as an underlying building block, we would like to design protocols that
take advantage of only client-server interactions (i.e., no peer-to-peer interactions) and support
periodic aggregation and tolerate faults and participant dropout. Towards this goal, our contributions include an efficient multi-input functional encryption scheme for evaluating sums, as well as
extensions of the basic scheme towards periodic aggregation and fault tolerance. In comparison,
ACM Transactions on Algorithms, Vol. 13, No. 4, Article 50. Publication date: December 2017.
Distributed Private Data Analysis 50:5
earlier works [1, 16] are more concerned about theoretical feasibility and thus they employ generic
MPC techniques that are more expensive and require pairwise interactions between all parties. We
elaborate on the set of desirable properties that are important in many practical applications:
• Small communication and no peer-to-peer interaction. In real-world applications,
peer-to-peer communication is undesirable as it requires all users (or their devices such
as laptops or mobile phones) to be online simultaneously and interact with each other. In
our construction, all communication takes place in the client-server model, where users
(clients) only need to talk to the aggregator (server), but not each other. Our protocol requires O(n) communication cost for each aggregation.
• Periodic aggregation. Numerous applications require the ability to perform periodic aggregation (whose related security definitions are given in Section 5). For example, a smart
grid operator may wish to continually monitor the total electricity consumption of each
neighborhood over time, a market researcher may wish to track the fraction of the population watching ESPN during different hours of the day.
Our novel encryption scheme allows for periodic aggregation. We require a one-time key
distribution at system initialization, during which each user obtains a secret encryption key,
and the aggregator obtains a decryption key. Then, in each aggregation time period, the
users can encrypt their noisy inputs with their secret keys. After collecting the ciphertexts
from all parties, the aggregator can then use its decryption key to obtain the noisy sum,
without learning anything additional.
• Fault tolerance, dynamic joins, and leaves. Failures are commonplace in real-world applications. For example, in a smart sensing applications, where data are collected from multiple distributed sensors, it is quite likely that some sensor might be malfunctioning at some
point and fails to respond. Fault tolerance requires that the aggregator be able to estimate the
sum (or average) of the functioning nodes even when a subset of the nodes fail to respond.
To achieve fault tolerance, in Section 7, we use a binary-tree technique that is reminiscent
of Dwork et al. [18] and Chan et al. [8]. By constructing a binary-tree over n users, we
allow the aggregator to estimate the sum of contiguous intervals of users as represented by
nodes in the interval tree, where the basic building block in Section 6 is employed at each
node. The binary-tree technique allows us to handle user failures, joins, and leaves, with a
small logarithmic (or polylog) penalty in terms of communication cost and estimation error.
1.4 Related Work
Differential Privacy. Since its proposal by Dwork et al. [14, 17], a large body of research [3, 11,
19, 26, 32, 36, 38] have been devoted to the design of differentially private algorithms. The bulk of
these works consider the standard setting with a trusted curator who has access to all users’ data
and is responsible for generating random noise to sanitize statistics or data published.
Computational differential privacy was proposed by Mironov et al. [31]. McGregor et al. [28]
demonstrated a gap separation result between differential privacy and computational differential
privacy in the two-party setting.
Differential Privacy for Multi-party Protocols. Dwork et al. [16] considered differential privacy in a multi-party setting and propose a distributed noise generation mechanism against Byzantine failures. Hence, their scheme is more complicated and requires interactions among all users.
In concurrent and independent work to ours, Rastogi et al. [34] also considered the distributed
summation problem. They proposed novel protocols that allow an untrusted aggregator to periodically estimate the sum of n users’ values, without harming each individual’s privacy. Both their
encryption scheme and distributed noise generation process differ from our constructions. In comACM Transactions on Algorithms, Vol. 13, No. 4, Article 50. Publication date: December 2017.
50:6 E. Shi et al.
parison, their encryption scheme requires that the server solicit help from clients to decrypt the
noisy sum. On the other hand, decryption is cheaper in their construction.
In a seminal work by Beimel, Nissim, and Omri [1], they demonstrated a lower bound result for
distributed private data analysis. Specifically, they considered the distributed summation problem,
namely, computing the sum of all parties’ inputs. They proved that any differentially private multiparty protocol with a small number of rounds and small number of messages must have large error.
Similarly, we show that for the distributed summation problem, any differentially private multiparty protocol with a sparse communication graph must have large error, where two nodes are
allowed to communicate only if they are adjacent in the communication graph. However, our
lower bound relaxes the constraint on the small number of messages or rounds and is strictly
stronger than that of Beimel et al. [1]. Similarly, our lower bound in the client-server setting has
no restriction on the number of messages or the number of rounds and is also strictly stronger than
in Reference [1], where it was shown that in the client-server setting, any differentially-private
protocol with a small number of rounds must have large error.
Duchi et al. [13] also considered privacy in the context of learning. Contrary to a general protocol, their lower bound is restricted to the model where each user sends a perturbed version of its
private input to the learning algorithm. Moreover, they use the mutual information between the
original and the perturbed inputs as a measure of privacy. Under this model, they showed sharp
upper and lower bounds on the convergence rates, which measure the quality of the parameter to
be learned in the underlying problem.
Homomorphic Encryption. Most previous works on homomorphic encryption considered homomorphic operations on ciphertexts encrypted under the same key [4, 21]. These schemes do not
directly apply in our case, since if participants encrypted their data under the aggregator’s public
key, the aggregator would not only be able to decrypt the aggregate statistics but also each individual’s values. By contrast, our cryptographic construction allows additive homomorphic operations
over ciphertexts encrypted under different users’ secret keys.
Castelluccia et al. [7] designed a symmetric-key homomorphic encryption scheme that allows
an aggregator to efficiently decrypt the mean and variance of encrypted sensor measurements.
However, they also assumed a trusted aggregator who is allowed to decrypt each individual sensor’s values. Yang et al. [39] designed an encryption scheme that allows an aggregator to compute
the sum over encrypted data from multiple participants. As pointed out by Magkos et al. [27], their
construction only supports a single timestep, and an expensive re-keying operation is required to
support multiple timesteps.
Secure Multi-Party Computation. Secure MPC [23] is a well-known cryptographic technique
allowing n parties with inputs x = (x1, x2,... xn ) respectively to privately compute functions
f1 (x), f2 (x),..., fn (x). At the end of the protocol, party i learns the value of fi (x) but nothing
more. In a sense, MPC is orthogonal and complementary to differential privacy techniques. MPC
does not address potential privacy leaks through harmful inferences from the outcomes of the computation, but one can potentially build differential privacy techniques into a multi-party protocol
to address such concerns.
Most MPC constructions are interactive. Therefore, directly employing MPC in our setting
would require participants to interact with each other whenever an aggregate statistic needs to
be computed. Such a multi-way interaction model may not be desirable in practical settings, especially in a client-server computation model as often seen in cloud computing applications.
2 PRELIMINARIES
Consider a group of n users, indexed by the set [n] := {1, 2,... n}, and a data aggregator indexed
by 0. We use a node or a party to mean either a user or the aggregator. Imagine that each user holds
ACM Transactions on Algorithms, Vol. 13, No. 4, Article 50. Publication date: December 2017.
Distributed Private Data Analysis 50:7
some data xi ∈ D for a certain domain D = {0, 1, 2,..., Δ}, where Δ is a positive integer. We use
a vector x ∈ Dn to denote the set of data values from all users, also referred to as an input configuration. We wish to design a multi-party protocol between the users and the aggregator, such that
at the end of the protocol, the aggregator estimates a function f : Dn → O over all parties’ inputs.
While our definitions are formulated for a general function f , as a first step, our constructions and
lower bound result focus on the distributed summation function, that is, sum(x) :=
i ∈[n] xi .
Suppose that, at the end of one protocol instance, the aggregator learns a noisy outcome f
.
Then, we define the error to be |f
− f (x)|, where f (x) is the true outcome. Our goal is to design
protocols that have small error with high probability, while ensuring privacy.
We are particularly interested in the scenario of periodic aggregation. In the case of periodic
aggregation, we use the set N of positive integers to denote time periods. In every time period
t ∈ N, each user i ∈ [n] has a value x (t)
i ∈ D, and we use a vector x(t) ∈ Dn to denote the data
from all users. For ease of exposition, we first focus our attention on the aggregation algorithm
in one timestep and, as a result, omit the superscript t. When we describe security and privacy
notions, we focus on the case when the protocol is run for one aggregation, and the case for
periodic aggregation is generalized in Section 5.
Adversarial Model. We assume a semi-honest model, where all parties honestly follow the protocol. However, an adversary can compromise a set of parties and observe all messages sent from
and to these corrupted parties, as well as the inputs, private randomness, and internal states of the
corrupted parties. We use the terminology honest parties to refer to parties that are not corrupted
by the adversary. We use the terminology corrupted or compromised parties to refer to parties that
have been corrupted by the adversary. Recall that in the semi-honest model, all parties honestly
follow the protocol, including honest and corrupted parties. We use the notation K to denote the
set of corrupted parties.
Given a protocol Π and an input x ∈ Dn, we use Π(x) to denote the execution of the protocol
on the input. The view Π(x)|K of the coalition K consists of the messages, any input, and private
randomness viewable by the parties in K. We denote by π (x) the transcript of the messages (i.e.,
messages sent and received by all parties) and use π (x)|K to mean the messages sent or received
by parties in K. An adversary A takes the view Π(x)|K as input and can possibly use its own
source of randomness to return 0 or 1. In general, A can be viewed as a function (taking the
view and its randomness) that is measurable with respect to the underlying probability space.
Hence, A = A(Π(x)|K ) can also be viewed as a {0, 1}-random variable. We say that the adversary
is computationally bounded if A is a (probabilistic) polynomial-time Turing machine.
Since we assume a semi-honest adversary, the data pollution attack, where parties inflate or
deflate their input values, is out of the scope of this article. Defense against the data pollution
attack can be considered as orthogonal and complementary to our work and has been addressed
by several works in the literature [16, 33].
Communication Model. Randomized oblivious protocols are considered in References [1, 12],
where the communication pattern (i.e., which node sends message to which node in which round)
is independent of the input and the randomness. We consider a similar notion that is also independent of which round. Given a protocol Π, the communication graph GΠ is defined on the nodes
such that there is an edge between two nodes iff for some input and some randomness, the two
nodes communicate with each other in some round.
The client-server model is a special case where users only communicate with the aggregator and
there is no peer-to-peer communication, and in this case the communication graph is a star graph
with the aggregator at the center.
ACM Transactions on Algorithms, Vol. 13, No. 4, Article 50. Publication date: December 2017.   
50:8 E. Shi et al.
For a node i, we denote by NΠ(i) its set of neighbors in GΠ. The subscript Π is dropped when
there is no risk of ambiguity. Observe that the number of messages sent in each round is only
limited by the number of edges in the communication graph, and without loss of generality, we
assume that there is some finite upper bound on the number of rounds for all possible inputs and
randomness used by the protocol for each aggregation.
User Failure. When a user fails, it does not participate in the protocol, that is, from the perspective
of other parties, the corresponding node disappears. Suppose for any subset T ⊆ [n] of users, the
function f : DT → O is still well defined. A protocol is failure tolerant, if for any subset of failed
users, the aggregator can still make an estimate on f (xT ) for the functioning users in T . For our
distributed summation scenario, we use the notation sum(xT ) :=
i ∈T xi to denote the sum of a
subset of functioning users T .
2.1 Differential Privacy Definitions
Intuitively, differential privacy against a coalition guarantees that if an individual outside the coalition changes its data, the view of the coalition in the protocol will not be affected too much. In
other words, if two input configurations x and y differ only in 1 position outside the coalition K,
then the distribution of Π(x)|K is very close to that of Π(y)|K .
The notion has been studied by Beimel et al. [1, Definition 2.4]. We formally state the definition
for one step aggregation in our notation. (The reader can refer to References [14, 15, 17] for the
classical definition of differential privacy.) The definition is extended to periodic aggregation in
Section 5.
Definition 2.1 (Differential Privacy Against Coalition). Let ϵ > 0 and 0 ≤ δ < 1. A (randomized)
protocol Π preserves information-theoretic (ϵ, δ )-differential privacy against coalition K if, for any
adversary A (even computationally unbounded ones), for all vectors x and y in Dn that differ by
only 1 position corresponding to a party outside K,
Pr[A(Π(x)|K ) = 1] ≤ exp(ϵ ) · Pr[A(Π(y)|K ) = 1] + δ .
Computational differential privacy is achieved if the protocol Π takes an extra security parameter
λ ∈ N, and the inequality holds only for any computationally bounded adversary A (modeled as
a non-uniform probabilistic polynomial-time algorithm), with δ replaced by δ + η(λ), where η is
some negligible function (that depends on A).
Remarks. We use the shorthand ϵ-differential privacy to mean (ϵ, 0)-differential privacy. For periodic aggregation, we consider two input configurations that are the same except for one time
period, in which they differ only at 1 position; the case when inputs differ at multiple time periods
can be handled by the composability of differential privacy.
If we consider a subset S of views such that the adversary A returns 1 iff the view is in S, then
the inequality in Definition 2.1 becomes Pr[Π(x)|K ∈ S] ≤ exp(ϵ ) · Pr[Π(y)|K ∈ S] + δ.
Our definition of computational differential privacy (CDP) is similar to the CDP notion originally
proposed by Mironov et al. [31].
Compromise Model. For simplicity, most of the article will assume a static compromise model
where the compromised set K is specified by the adversary upfront. Note that assuming a weak
compromise model allows us to prove a stronger lower bound. For our upper-bound results for
periodic aggregation under the pre-processing model, we point out later that we can actually prove
security under a stronger compromise model, where nodes can be compromised dynamically, as
long as the randomness of the compromise process is independent of the public parameter choice.
ACM Transactions on Algorithms, Vol. 13, No. 4, Article 50. Publication date: December 2017. 
Distributed Private Data Analysis 50:9
2.2 Mathematical Tools
Two noise distributions are commonly used to perturb the data and ensure differential privacy,
the Laplace distribution [17], and the geometric distribution [22]. The advantage of using the geometric distribution over the Laplace distribution is that we can keep working in the domain of
integers.
Definition 2.2 (Geometric Distribution). Let α > 1. We denote by Geom(α) the symmetric geometric distribution that takes integer values such that the probability mass function at k is α−1
α+1 · α− |k |
.
Fact 1. Let ϵ > 0. Suppose u and v are two integers such that |u −v| ≤ Δ. Let r be a random
variable having distribution Geom(exp( ϵ
Δ )). Then, for any integer k, Pr[u + r = k] ≤ exp(ϵ ) · Pr[v +
r = k].
The preceding property of Geom distribution is useful for designing differentially private mechanisms that output integer values. The targeted statistic f (x) has sensitivity Δ if changing one
coordinate of x will only change f (x) by at most Δ; in this case, adding geometric noise with magnitude proportional to Δ is sufficient to achieve differential privacy. Hence, it suffices to consider
Geom(α) with α = e
ϵ
Δ . Observe that Geom(α) has variance 2α
(α−1)2 . Since
√
α
α−1 ≤ 1
ln α = Δ
ϵ , the magnitude of the error added is O( Δ
ϵ ). However, there is no single trusted party to generate this noise,
and in Section 4, we see how noise generation is distributed.
The following diluted geometric distribution is useful in the description of our protocols.
Definition 2.3 (Diluted Geometric Distribution). Let 0 ≤ β ≤ 1, α > 1. A random variable has βdiluted geometric distribution Geomβ (α) if with probability β it is sampled from Geom(α), and
with probability 1 − β is set to 0.
In many of our protocols, each user will generate some independent geometric noise, and the
total error will be the sum of the noises. Hence, we would like to analyze the sum of independent
geometric random variables. The proof of the following lemma is given in Appendix A.
Lemma 2.4 (Sum of Independent Diluted Geom). Let α > 1 and 0 < τ < 1. Suppose
{Yi} is a finite sequence of independent random variables such that each Yi has distribution
Geomβi (α) for some 0 ≤ βi ≤ 1. Define Y :=
i Yi . Then, with probability at least 1 − τ , |Y | ≤
4
√
α
α−1 ·

max{

i βi, α ln 2
τ } ·
ln 2
τ . In particular, we have the following:
(a) If
i βi ≥ α ln 2
τ , then Pr[|Y | ≥ 4
√
α
α−1 ·

i βi ·

ln 2
τ ] ≤ τ .
(b) If
i βi < α ln 2
τ , then Pr[|Y | ≥ 4α
α−1 · ln 2
τ ] ≤ τ .
2.3 Naïve Scheme
As a warm-up exercise, we describe a Naïve Scheme, where each user generates an independent
Geom(e
ϵ
Δ ) noise, adds the noise to its data, and sends the perturbed data to the aggregator, who
then computes the sum of all the noisy data. Observe that each user’s data are sanitized by an independent copy of geometric noise before release and hence the protocol is information-theoretic
ϵ-differentially private against coalition of any size. As each party adds one copy of independent
noise to its data, n copies of noises would accumulate in the sum. Using Lemma 2.4, the accumulated noise isO(
Δ
√
n
ϵ ) with high probability. In comparison with our lower-bound in Section 3, this
shows that under certain mild assumptions, if one wishes to guarantee small message complexity, the Naïve Scheme is more or less the best one can do in the information-theoretic differential
privacy setting.
ACM Transactions on Algorithms, Vol. 13, No. 4, Article 50. Publication date: December 2017.        
50:10 E. Shi et al.
3 LOWER BOUND FOR INFORMATION-THEORETIC DIFFERENTIAL PRIVACY
In this section, we describe our lower bound results and prove that any protocol consuming at
most 1
4n(t + 1) messages must incur Ω(√
n) additive error. We also show that for the client-server
setting, any protocol satisfying information-theoretic differential privacy must incur Ω(√
n) error.
Recall that in Section 2.3, we described the Naïve Scheme that uses Θ(n) messages and secures
against coalition of any size and has Ω(√
n) error with high probability. Our lower bound result
suggests that in some sense the Naïve Scheme is the best one can do if information-theoretic
differential privacy is required.
We will prove the following main result, and then show how to extend the main theorem to the
aforementioned two communication models.
Theorem 3.1 (Information-Theoretic Lower Bound for Size-k Coalitions). Let 0 < ϵ ≤
ln 99 and 0 ≤ δ ≤ 1
4n . There exists some τ > 0 (depending on ϵ) such that the following holds. Suppose n users, where user i (i ∈ [n]) has a secret bit xi ∈ {0, 1}, participate in a protocol Π to estimate

i ∈[n] xi . Suppose further that the protocol is (ϵ, δ )-differentially private against any coalition of size
k, and there exists a subset ofm users, each of whom has at most k neighbors in the protocol’s communication graph. Then, there exists some configuration of the users’ bits xi ’s such that with probability
at least τ (over the randomness of the protocol), the additive error is at least Ω(
√γ
1+γ ·
√
m), where
γ = 2eϵ .
Note that the assumption that 0 ≤ δ ≤ 1
4n is not a strong limitation. Typically, when we adopt
(ϵ, δ ) differential privacy, we wish to have δ = o( 1
n ) to ensure that no individual user’s sensitive
data are leaked with significant probability.
The following corollaries are special cases of Theorem 3.1, corresponding to the client-server
communication model, and the general model respectively. In both settings, our results improve
upon the lower bounds by Beimel et al. [1]. We will first show how to derive these corollaries from
Theorem 3.1. We then present a formal proof for Theorem 3.1.
Corollary 3.2 (Lower Bound for Client-Server Communication Model). Let 0 < ϵ ≤ ln 99
and 0 ≤ δ ≤ 1
4n . Suppose n users, each having a secret bit, participate in a protocol Π with an aggregator, with no peer-to-peer communication among the n users. Suppose further that the protocol is
(ϵ, δ )-differentially private against the aggregator (which is a coalition of size 1). Then, with constant
probability (depending on ϵ), the aggregator estimates the sum of the parties’ bits with additive error
at least Ω(
√γ
1+γ ·
√
n), where γ = 2eϵ .
Proof. The communication graph is a star with the aggregator at the center. The protocol is
also differentially private against any coalition of size 1, and there are n parties, each of which has
only 1 neighbor (the aggregator). Therefore, the result follows from Theorem 3.1.
Corollary 3.3 (Lower Bound for General Setting). Let 0 < ϵ ≤ ln 99 and 0 ≤ δ ≤ 1
4n . Suppose n users participate with an aggregator in a protocol that is (ϵ, δ )-differentially private against
any coalition of size k. If there are at most 1
4n(k + 1) edges in the communication graph of the protocol, then with constant probability (depending on ϵ), the protocol estimates the sum of the parties’
bits with additive error at least Ω(
√γ
1+γ ·
√
n), where γ = 2eϵ .
Proof. Since there are at most 1
4n(k + 1) edges in the communication graph, there are at least n
2 user nodes with at most k neighbors (otherwise the sum of degrees over all nodes is larger than 1
2k(t + 1)). Hence, the result follows from Theorem 3.1.
Proof Overview for Theorem 3.1. We fix some ϵ > 0 and 0 ≤ δ ≤ 1
4n and consider some protocol
Π that preserves (ϵ, δ )-differential privacy against any coalition of size k.
ACM Transactions on Algorithms, Vol. 13, No. 4, Article 50. Publication date: December 2017.  
Distributed Private Data Analysis 50:11
Suppose that the bits Xi ’s from all users are all uniform in {0, 1} and independent. Suppose M is
the subset of m users, each of whom has at most k neighbors in the communication graph. Since
the protocol preserves privacy against any coalition of size k, every message sent by a user in M
is scrutinized by the coalition. Hence, intuitively, such a user must generate enough independent
randomness to protect its data. If there are m such users, then standard probability argument
suggests that the error will have magnitude around Θ(√
m).
For each i ∈ M, we consider a set P(i) of bad transcripts for i, which intuitively is the set of
transcripts π under which the view of user i’s neighbors can compromise user i’s privacy.
We consider the set P := ∪i ∈M P(i) of bad transcripts (which we define formally later) and show
that the probability that a bad transcript is produced is at most 3
4 . Conditioning on a transcript
π  P, for i ∈ M, each Xi still has enough randomness, as transcript π does not break the privacy
of useri. Therefore, the conditional sum
i ∈M Xi still has enough variance like the sum ofm = |M|
independent uniform {0, 1}-random variables. Using anti-concentration techniques, we can show
that the sum deviates above or below the mean by Ω(√
m), each with constant probability. Since
the transcript determines the estimation of the final answer, we conclude that the error is Ω(√
m)
with constant probability.
Notation. Suppose that each useri’s bit Xi is uniform in {0, 1} and independent. We use X := (Xi :
i ∈ [n]) to denote the collection of the random variables. We use a probabilistic argument to show
that the protocol must, for some configuration of users’ bits, make an additive error of at least
Ω(√
m) on the sum with constant probability.
For convenience, given a transcript π (or a view of the transcript by certain users), we use Pr[π]
to mean Pr[π (X) = π] and Pr[·|π] to mean Pr[·|π (X) = π]; given a collection P of transcripts (or
collection of views), we use Pr[P] to mean Pr[π (X) ∈ P].
We can assume that the estimate made by the protocol is a deterministic function on the whole
transcript of messages, because without loss of generality we can assume that the last message
sent in the protocol is the estimate of the sum.
We will define some event E where the protocol makes a large additive error.
Bad Transcripts. Denote γ := 2eϵ . For i ∈ M, define P(i)
0 := {π : Pr[π|N (i) |Xi = 0] > γ ·
Pr[π|N (i) |Xi = 1]} and P(i)
1 := {π : Pr[π|N (i) |Xi = 1] > γ · Pr[π|N (i) |Xi = 0]}. We denote by P(i) :=
P(i)
0 ∪ P(i)
1 the set of bad transcripts with respect to user i. Let P := ∪i ∈M P(i)
.
Fact 2 (Projection of Events). Suppose U is a collection of possible views of the transcript by
the neighbors of i and define the collection of transcripts PU := {π : π|N (i) ∈ U }. Then, it follows that
PrX,Π[π (X) ∈ PU ] = PrX,Π[π (X)|N (i) ∈ U ].
Lemma 3.4 (Most Transcripts Behave Well). Let ϵ > 0 and 0 ≤ δ ≤ 1
4n . Suppose the protocol
is (ϵ, δ )-differentially private against any coalition of size k, and P is the union of the bad transcripts
with respect to users with at most k neighbors in the communication graph. Then, PrX,Π[P] ≤ 3
4 .
Proof. From definition of P(i)
0 and using Fact 2, we have Pr[P(i)
0 |Xi = 0] > γ · Pr[P(i)
0 |Xi = 1].
Since the protocol is (ϵ, δ )-differentially private against any coalition of size k, we have for each i ∈
M, Pr[P(i)
0 |Xi = 0] ≤ eϵ Pr[P(i)
0 |Xi = 1] + δ. Hence, we have (γ − eϵ ) Pr[P(i)
0 |Xi = 1] ≤ δ, which
implies that Pr[P(i)
0 |Xi = 1] ≤ e−ϵ δ, since γ = 2eϵ .
Hence, we also have Pr[P(i)
0 |Xi = 0] ≤ eϵ Pr[P(i)
0 |Xi = 1] + δ ≤ 2δ. Therefore, we have
Pr[P(i)
0 ] = 1
2 (Pr[P(i)
0 |Xi = 0] + Pr[P(i)
0 |Xi = 1]) ≤ 3δ
2 .
Similarly, we have Pr[P(i)
1 ] ≤ 3δ
2 . Hence, by the union bound over i ∈ M, we have Pr[P] ≤
3nδ ≤ 3
4 , since we assume 0 ≤ δ ≤ 1
4n .
ACM Transactions on Algorithms, Vol. 13, No. 4, Article 50. Publication date: December 2017.   
50:12 E. Shi et al.
We perform the analysis by first conditioning on some transcript π  P. The goal is to show
that PrX[E |π] ≥ τ for some τ > 0. Then, since Pr[P] ≤ 3
4 , we can conclude PrX[E] ≥ τ
4 , and hence
for some configuration x, we have Pr[E |x] ≥ τ
4 , as required.
Conditioning on Transcript π. The first step (Lemma 3.6) is analogous to the techniques of
Reference [28, Lemma 1]. We show that conditioning on the transcript π  P, the random variables
Xi ’s are still independent and still have enough randomness remaining.
Definition 3.5 (γ -random). Let γ ≥ 1. A random variable X in {0, 1} is γ -random if 1
γ ≤ Pr[X=1]
Pr[X=0] ≤
γ .
Lemma 3.6 (Conditional Independence and Randomness). Suppose each user’s bit Xi is uniform and independent, and consider a protocol to estimate the sum that is (ϵ, δ )-differentially private
against any coalition of size t, where 0 ≤ δ ≤ 1
4n . Then, conditioning on the transcript π  P, the
random variables Xi ’s are independent; moreover, for each user i ∈ M that has at most t neighbors in
the communication graph, the conditional random variable Xi is γ -random, where γ = 2eϵ .
Proof. The proof is similar to that of Reference [28, Lemma 1]. Since our lower bound does
not depend on the number of rounds, we can without loss of generality sequentialize the protocol
and assume only one node sends a message in each round. The conditional independence of the
Xi ’s can be proved by induction on the number of rounds of messages. To see this, consider the
first message m1 sent by the user who has input X1, and suppose X	 is the joint input of all other
users. Observe that (X1,m1) is independent of X	
. Hence, we have Pr[X1 = a,X	 = b|m1 = c] = Pr[X1=a,X	
=b,m1=c]
Pr[m1=c] = Pr[X1=a,m1=c] Pr[X	
=b]
Pr[m1=c] = Pr[X1 = a|m1 = c] · Pr[X	 = b|m1 = c], which means
conditioning on m1, the random variables X1 and X	 are independent. After conditioning on m1,
one can view the remaining protocol as one that has one less round of messages. Therefore, by
induction, one can argue that conditioning on the whole transcript, the inputs of the users are
independent.
For each user i having at most t neighbors, the γ -randomness of each conditional Xi can be
proved by using the uniformity of Xi and that π  P(i) is not bad for i.
We first observe that the random variable Xi has the same conditional distribution whether
we condition on π or π|N (i), because as long as we condition on the messages involving node i,
everything else is independent of Xi .
We next observe that if user i ∈ M has at most t neighbors in the communication graph and
π  P(i)
, then by definition we have Pr[π|N (i) |Xi=1]
Pr[π|N (i) |Xi=0] ∈ [γ −1,γ ].
Hence, Pr[Xi=1|π ]
Pr[Xi=0|π ] = Pr[Xi=1| π|N (i)]
Pr[Xi=0| π|N (i)] = Pr[π|N (i) |Xi=1]·Pr[Xi=1]
Pr[π|N (i) |Xi=0]·Pr[Xi=0] = Pr[π|N (i) |Xi=1]
Pr[π|N (i) |Xi=0] ∈ [γ −1,γ ].
We use the superscripted notation X	 to denote the version of the random variable X conditioning on some transcript π. Hence, Lemma 3.6 states that the random variables X	
i ’s are independent,
and each X	
i is γ -random for i ∈ M. It follows that the sum
i ∈M X	
i has variance at least mγ
(1+γ )2 .
The idea is that conditioning on the transcript π, the sum of the users’ bits (in M) has high
variance, and so the protocol is going to make a large error with constant probability. We describe
the precise properties we need in the following technical lemma, which could be proved by using
the Berry-Esseen theorem. However, for completeness, we give its proof in Section 3.1, from which
Theorem 3.1 follows.
Lemma 3.7 (Large Variance Dichotomy). Let γ ≥ 1. There exists τ > 0 (depending on γ ) such
that the following holds. Suppose Zi ’s are m independent random variables in {0, 1} and are all γ -
random, where i ∈ [n]. Define Z :=
i ∈[m] Zi and σ2 := mγ
2(1+γ )2 . Then, there exists an interval [a,b]
of length σ
2 such that the probabilities Pr[Z ≥ b] and Pr[Z ≤ a] are both at least τ .
ACM Transactions on Algorithms, Vol. 13, No. 4, Article 50. Publication date: December 2017.        
Distributed Private Data Analysis 50:13
Proof of Theorem 3.1: Using Lemma 3.7, we set γ := exp(ϵ ) and Zi := X	
i for each i ∈ M. Suppose τ > 0 (depending on γ and hence on ϵ), σ2 := mγ
2(1+γ )2 and the interval [a,b] are as guaranteed
from the lemma. Suppose s is the sum of the bits of users outside M. Let c := a+b
2 + s.
Suppose the protocol makes an estimate that is at most c. Then, conditioning on π, the system
still has enough randomness among users in M, and with probability at least τ , the real sum is
at least b + s, which means the additive error is at least σ
4 . The case when the protocol makes an
estimate greater than c is symmetric. Therefore, conditioning on π  P, the protocol makes an
additive error of at least σ
4 with probability at least τ in any case. Note that this is true even if the
protocol is randomized.
Let E be the event that the protocol makes an additive error of at least σ
4 . We have just proved
that for π  P, PrX,Π[E |π] ≥ τ , where the probability is over the X = (Xi : i ∈ [n]) and the randomness of the protocol Π.
Observe that PrX,Π[E |π] ≥ τ for all transcripts π  P, and from Lemma 3.4, Pr[P] ≤ 3
4 . Hence,
we conclude that PrX,Π[E] ≥ τ
4 . It follows that there must exist some configuration x of the users’
bits such that PrΠ[E |x] ≥ τ
4 . This completes the proof of Theorem 3.1.
3.1 Large Variance Dichotomy (Proof of Lemma 3.7)
Fori ∈ M, let pi := Pr[Zi = 1]. From the γ -randomness of Zi , it follows that 1
1+γ ≤ pi ≤ γ
1+γ
. Without loss of generality, we assume that there are at least m
2 indices for which pi ≥ 1
2 ; otherwise, we
consider 1 − Zi . Let J ⊆ M be a subset of size m
2 such that for each i ∈ J, pi ≥ 1
2 .
Define for i ∈ J, Yi := Zi − pi . Let Y :=
i ∈J Yi , and Z	 :=
i ∈J Zi . It follows that E[Yi] =
0 and E[Y2
i ] = pi (1 − pi ) ≥ γ
(1+γ )2 . Denote σ2 := mγ
2(1+γ )2 , μ := E[Z	
] =
i ∈J pi and ν 2 := E[Y2] =
i ∈J pi (1 − pi ). We have ν 2 ≥ σ2.
The required result can be achieved from the following lemma.
Lemma 3.8 (Large Deviation). There exists τ0 > 0 (depending only on γ ) such that Pr[|Y | ≥
9σ
10 ] ≥ τ0.
We show how Lemma 3.8 implies the conclusion of Lemma 3.7. Since Pr[|Y | ≥ 9σ
10 ] = Pr[Z	 ≥
E[Z	
] + 9σ
10 ] + Pr[Z	 ≤ E[Z	
] − 9σ
10 ], at least one of the latter two terms is at least τ0
2 . We consider
the case Pr[Z	 ≥ E[Z	
] + 9σ
10 ] ≥ τ0
2 ; the other case is symmetric.
By Hoeffding’s Inequality, for all u > 0, Pr[Z	 ≥ E[Z	
] + u] ≤ exp(−2u2
n ). Setting u := 2σ
5 , we
have Pr[Z	 < E[Z	
] + 2σ
5 ] ≥ 1 − exp(− 8γ
25(1+γ )2 ) =: τ1.
We set τ := 1
2 min{ τ0
2 , τ1}. Let Z
 :=
i ∈M\J Zi . Observe that Z
and Z are independent. Hence we
can take the required interval to be [median(Z
) + E[Z] + 2σ
5 , median(Z
) + E[Z] + 9σ
10 ], which has
width σ
2 .
Hence, it remains to prove Lemma 3.8.
2
Fact 3 (Paley-Zygmund Ineqality). Suppose X is a non-negative random variable with finite
variance. Then, for any 0 ≤ θ ≤ 1, we have that
Pr[X ≥ θE[X]] ≥ (1 − θ )
2 ·
E[X]
2
E[X2]
.
Proof of Lemma 3.8: We apply Fact 3 to the non-negative random variable X := Y2.
2The lemma was proved by considering moment generating function in the conference version [9]. However, as suggested
by a journal reviewer, it can be proved by a direct application of the Paley-Zygmund inequality.
ACM Transactions on Algorithms, Vol. 13, No. 4, Article 50. Publication date: December 2017.             
50:14 E. Shi et al.
Observe that E[Y2] ≥ mγ
2(1+γ )2 .
Hence, it suffices to give an upper bound for E[Y4]. Recall that Y =
i ∈J Yi is a sum of m such
Yi ’s, where for each i ∈ J, E[Y2
i ] ≤ 1
4 and trivially E[Y4
i ] ≤ 1.
Therefore, observing that the Yi ’s are independent and E[Yi] = 0, we have E[Y4] ≤
i ∈J E[Y4
i ] +
(
m
2 ) · (
4
2 ) · ( 1
4 )
2 = O(m2).
Hence, substituting θ := 19
100 and X := Y2 in Fact 3 completes the proof.
4 AGGREGATOR OBLIVIOUSNESS AND DISTRIBUTED DIFFERENTIAL PRIVACY
We describe a general framework for designing differentially private multi-party protocols. The
high level idea is as in the following:
• We rely on techniques inspired by secure function evaluation (SFE) to achieve aggregator
obliviousness, that is, for a fixed vector of corrupt parties’ inputs, if the honest parties’ inputs
xK and yK result in the same outcome, then the aggregator (i.e., adversary) should not be
distinguish which honest input combination was used in the computation. Interestingly, we
stress our aggregator obliviousness definition is equivalent to the “input indistinguishable”
notion [30] of multi-party computation, which is weaker than the standard semi-honest,
simulation-secure MPC notion.
• Once aggregator obliviousness is achieved, it suffices to introduce noise by a distributed
input perturbation process, where each party adds noise to their inputs prior to running the
aggregator obliviousness protocol. We formalize the properties that the input perturbation
process needs to satisfy, and we refer to the privacy notion as distributed differential privacy
(DD-privacy).
Although some earlier works such as Dwork et al. [16] and Beimel et al. [1] also suggest combining MPC techniques with distributed noise generation, one interesting observation we make is
that when differential privacy (Definition 2.1) is the goal, it suffices to use the weaker “input indistinguishable” notion of multi-party computation [30] (and in this article, we refer to this notion as
“aggregator obliviousness”). More specificially, the standard notion of (semi-honest) MPC requires
that an adversary cannot learn nothing more than the outcome of the computation; whereas with
input indistinguishability, the adversary may learn something about the honest parties’ inputs;
however, if multiple honest inputs could have resulted in the outcome observed by the adversary,
then the adversary cannot distinguish which honest input was used.
Later, we shall present a lemma (Lemma 4.3) that gives a generic framework for combining
(possibly computational) aggregator obliviousness and DD-privacy and obtaining a (possibly computationally) differentially private protocol. Particularly, DD-privacy is an information-theoretic
notion; and depending on whether our protocol achieves computational or information-theoretic
aggregator obliviousness, such a protocol satisfies either information-theoretic or computational
differential privacy (Definition 2.1). Although our protocol later focuses on the case of summation,
Lemma 4.3 works for any function f . As mentioned, for generic functions, computational input
indistinguishability is a weaker notion than the standard (standalone, semi-honest) simulationbased notion of security for MPC. For the special case of summation (the protocol that we will be
concerned with), however, the two notions are equivalent, since given the output, the simulator
can easily compute a plausible honest input that agrees with the output.
The intuition behind our Lemma 4.3 is the following: intuitively, aggregator obliviousness hides
the honest parties’ input from the semi-honest adversary—for the specific case of sum (in which
case our aggregator oblivious notion equates to standard simulation security), the adversary in
some sense learns nothing but the outcome, and thus it is as if there were a trusted party that
ACM Transactions on Algorithms, Vol. 13, No. 4, Article 50. Publication date: December 2017.   
Distributed Private Data Analysis 50:15
collects everyone’s inputs, evaluates the mechanism, and gives the output to the aggregator. On
the other hand, DD-privacy captures the fact that any change in input by a single honest party
will not cause significant distributional difference in the adversary’s knowledge, including f (x)
and the inputs and random bits consumed by the compromised parties.
We now begin our formal presentation. We first formally define aggregator obliviousness and
DD-privacy. We focus on a single aggregation step in this section. Our definitions will be extended
to periodic aggregation in Section 5.
Definition 4.1 (Aggregator Obliviousness against Coalition). Let η > 0. A protocol Π is
information-theoretically η-aggregator oblivious with respect to the function f : Dn → O against
coalition K ⊂ [n] if, for any adversary A (even computationally unbounded ones), for all vectors
x, y ∈ Un agreeing on all coordinates in K and f (x) = f (y),
| Pr[A(Π(x)|K ) = 1] − Pr[A(Π(y)|K ) = 1]| ≤ η.
Computational aggregator obliviousness is achieved if the protocol Π takes an extra security parameter λ ∈ N such that η = η(λ) is a negligible function, and the inequality holds only for computationally bounded adversaries A.
Remarks. In Section 4.1, we give an example of a 0-aggregator oblivious protocol for computing
sum against computationally unbounded adversaries.
Distributed Differential Privacy: Input Perturbation. Given an aggregator oblivious protocol
Π0, differential privacy can be achieved by input perturbation. Formally, each party i generates
independent randomness ri from some sample space Ω and apply a random function χ : D ×
Ω → D to produce the perturbed input xi := χ (xi,ri ). We denote r := (r1,r2,...,rn ) ∈ Ωn and
x := (x1,..., xn ) = χ (x, r). The randomness of r induces a random variable f (x), on which we
analyze differential privacy. We can define a protocol Π which given input x is simply the protocol
Π0 applied to the perturbed input x.
Definition 4.2 ((ϵ, δ )-DD-Privacy Against Coalition). Suppose ϵ > 0, 0 ≤ δ < 1. We say that the
input perturbation procedure, given by the joint distribution r := (r1,...,rn ) and the randomization function χ achieves (ϵ, δ )-distributed differential privacy (DD-privacy) with respect to the
function f : Dn → O against coalition K (which can be random), if for any vectors x, y ∈ Dn that
differ at one coordinate i  K, for any subset Q ⊆O× ΩK ,
Pr
K,r
[(f (x), rK ) ∈ Q] ≤ exp(ϵ ) · Pr
K,r
[(f (y), rK ) ∈ Q] + δ .
Remark 1. The preceding inequality is implied by the following stronger condition: For all S ⊆ O
Pr
r
[f (x) ∈ S |rK ] ≤ exp(ϵ ) · Pr
r
[f (y) ∈ S |rK ] + δ .
Remark 2 (More general setting). In general, each user i can generate noise ri according to its
own distribution. Moreover, each user i can apply a different randomization function χi (xi,ri ) to
its data xi . In an even more general setting, each user may send its data xi and its randomness ri
(maybe in encrypted form) separately to the aggregator, who computes a randomized aggregate
function f
: Dn × Ωn → O on the encrypted inputs.
For simplicity, this article considers the special case when f
(x, r) = f (x). Furthermore, we assume that each participant applies the same randomization χ function to its data.
We show in Section 4.1 that for the function summation, there exist input perturbation procedures that satisfy DD-privacy. The following lemma shows that aggregator obliviousness and
ACM Transactions on Algorithms, Vol. 13, No. 4, Article 50. Publication date: December 2017.               
50:16 E. Shi et al.
DD-privacy can be combined together to design differentially private protocols. Our periodic aggregation protocol in Section 6 also follows this principle.
Lemma 4.3 (Aggregator Obliviousness and DD-Privacy Give Differential Privacy). Suppose protocol Π0 is η-aggregator oblivious with respect to the function f : Dn → O against a coalition K (which can be random). Moreover, suppose further that for ϵ > 0 and 0 ≤ δ < 1, the input
perturbation procedure x := χ (x, r) is (ϵ, δ )-DD private with respect to f against coalition K. Then,
the resulting protocol Π defined by Π(x) := Π0 (x) is (ϵ, δ + η)-differentially private against coalition
K.
If Π0 is information-theoretically aggregator oblivious, then Π is information-theoretically differentially private; if Π0 is computationally aggregator oblivious, then Π is computationally differentially
private.
Proof. Let x, y ∈ Dn differ at exactly one position i, where i  K. Consider a subset S of the
views of the coalition K in the protocol Π.
The proof works for both information-theoretic and computational versions.
We wish to prove the following inequality:
Pr[A(Π(x)|K ) = 1] ≤ eϵ Pr[A(Π(y)|K ) = 1] + δ + η. (1)
The proof idea is that using DD-privacy, the distribution of f (x) and f (y) are close. Then, we
condition on f (x) and f (y) being the same and use the aggregator obliviousness of Π0 to show
that the distribution of Π0 (x)|K and Π0 (y)|K are close.
The probability on the left-hand side of (1) can be expressed as:
Pr[A(Π(x)|K ) = 1] = EK,r[Pr[A(Π0 (x)|K ) = 1 |f (x), rK ]] = EK,r[hx (f (x), rK )],
where hx : O × ΩK → [0, 1] is a function defined by hx (j, σ ) := Pr[A(Π0 (x)|K ) = 1 |f (x) = j, rK =
σ].
Similarly, be defining hy (j, σ ) := Pr[A(Π0 (y)|K ) = 1 |f (y) = j, rK = σ], the probability on the
right-hand side of (1) is Pr[A(Π(y)|K ) = 1] = EK,r[hy (f (y), rK )].
Observe that fixing some rK = σ ∈ ΩK , we have xK = yK . Hence, by the η-aggregator obliviousness of Π0, we have for all (j, σ ) ∈O× ΩK , hx (j, σ ) ≤ hy (j, σ ) + η.
Inequality (1) follows from the following:
Pr[A(Π(x)|K ) = 1] = EK,r[hx (f (x), rK )], (2)
≤ EK,r[hy (f (x), rK )] + η, (3)
≤ eϵ · EK,r[hy (f (y), rK )] + δ + η = eϵ Pr[A(Π(y)|K ) = 1] + δ + η, (4)
where the penultimate inequality (4) follows from the following claim3 with h = hy .
Claim 1. For any function h : O × ΩK → [0, 1], EK,r[h(f (x), rK )] ≤ eϵ · EK,r[h(f (y), rK )] + δ.
Proof. For each t ∈ [0, 1], define Qt := {(u, σ ) ∈O× ΩK : h(u, σ ) ≥ t}.
Then, observe that
EK,r[h(f (x), rK )] =  1
0 PrK,r[h(f (x), rK ) ≥ t]dt =  1
0 PrK,r[(f (x), rK ) ∈ Qt]dt,
and a similar identity holds for EK,r[h(f (y), rK )].
Using (ϵ, δ )-DD-privacy, we have for each t ∈ [0, 1], PrK,r[(f (x), rK ) ∈ Qt] ≤ eϵ PrK,r[(f (y),
rK ) ∈ Qt] + δ.
Therefore, integrating this inequality over [0, 1] and using the identities (relating an expectation
and an integral over probabilities) give the required claim.
3The simple proof is suggested by a journal reviewer.
ACM Transactions on Algorithms, Vol. 13, No. 4, Article 50. Publication date: December 2017.                              
Distributed Private Data Analysis 50:17
4.1 Example: Information-Theoretic Differentially Private Protocol for Sum
We illustrate an application of Lemma 4.3 by constructing an information-theoretic differentially
private protocol for computing summation.
Theorem 4.4 (Differentially Private Protocols Against Coalitions). Given ϵ > 0, 0 <
δ < 1 and a positive integer k, there exist information-theoretically differentially private protocols
against any coalition of size k between an aggregator and n users each having a secret input xi ∈
D := {0, 1, 2,..., Δ}, such that the protocol uses only O(nk) messages to estimate the sum
i ∈[n] xi ;
the differential privacy guarantees and error bound of each of the protocols are given as follows:
(a) For ϵ-differential privacy, with probability at least 1 − τ , the additive error is at most O( Δ
ϵ ·
exp( ϵ
2Δ )
√
k log 1
τ ).
(b) For (ϵ, δ )-differential privacy, with probability at least 1 − τ , the additive error is at mostO( Δ
ϵ ·
exp( ϵ
2Δ )
 n
n−k+1 log 1
δ log 1
τ ).
To use Lemma 4.3 to achieve differential privacy, we first need an aggregator oblivious protocol
for computing summation. One example is given by Chor and Kushilevitz [12], and we outline its
description for completeness.
Proposition 4.5 (Existence of Aggregator Oblivious Protocol for Summation 12]).
There exists an information-theoretically 0-aggregator oblivious protocol using O(nk) messages
against any coalition of size k with respect to the function summation.
Description of Protocol. Recall each user i ∈ [n] has an input xi ∈ D := {0, 1, 2,..., Δ}. We assume that addition is performed in Zp for some large-enough prime p.
Of the n parties, the parties in [k] will be known as leaders. Since we only consider coalitions
of size k (which includes the aggregator labeled by 0), at least one leader will not be in the coalition. The protocol Π0 consists of several stages. For simplicity of description, a party might send
messages to itself, although they are not counted in the message complexity.
1. Each party i generates k random numbers ρ(j)
i ∈ Zp , for j ∈ [k] uniformly at random, subjecting to
j ∈[k] ρ(j)
i = xi . For each j ∈ [k], user i sends ρ(j)
i to leader j.
2. Each leader j ∈ [k] computes S(j) :=
i ∈[n] ρ(j)
i and sends S(j) to aggregator 0.
3. The aggregator computes
j ∈[k] S(j)
.
Correctness. The aggregator computes
j ∈[k] S(j) =
j ∈[k]

i ∈[n] ρ(j)
i =
i ∈[n] xi .
Number of Messages. The first stage consists of (n − 1) · k messages; the second stage consists
of k messages. The total number of messages is nk; the aggregator can send an additional number
of n messages to all other parties to report the sum.
Lemma 4.6 (Aggregator Obliviousness of Protocol). The protocol Π0 described earlier is
information-theoretically 0-aggregator oblivious against any coalition of size k with respect to the
function summation.
Proof. Consider a coalition of size k. Since any leader receives strictly more information than
a non-leader, we can assume that the coalition consists of the aggregator and k − 1 leaders, say,
users in [k − 1].
Consider two vectors x, y ∈ Dn such that xK = yK and
i ∈[n] xi =
i ∈[n] yi . We argue that
Π0 (x)|K and Π0 (y)|K have the same distribution.
ACM Transactions on Algorithms, Vol. 13, No. 4, Article 50. Publication date: December 2017.           
50:18 E. Shi et al.
Consider the messages received by the coalition K in the first stage. Since each user i generates
k random numbers that sum to xi , any k − 1 of them will be distributed independently uniform at
random. Hence, the messages received by K from any user outside the coalition will be distributed
independently uniform at random.
In the second stage, the only message received by the coalition K from a party outside the
coalition is the message S(k) from leader k. However, conditioning on the messages received so far
and the sum
i ∈[n] xi =
i ∈[n] yi , there can only be one value S(k) that produces the required sum
and hence there is no more randomness in S(k)
. Therefore, it follows that Π0 (x) and Π0 (y) have
the same distribution.
Achieving DD-Privacy with Respect to Summation
Having obtained an aggregator oblivious protocol as in Lemma 4.6, we next describe input perturbation procedures that achieve DD-privacy with respect to the function summation. Recall
that combining with Lemma 4.3, we can obtain differentially private protocols as promised in
Theorem 4.4.
Recall that changing one user’s data can only change the sum by at most Δ, and hence Fact 1
suggests that one independent copy of Geom(α) (where α = e
ϵ
Δ ) that has not been compromised
by the adversary is enough to achieve ϵ-differential privacy. Having only one user to generate the
randomness is not enough, since that user can be compromised. The other extreme is to have every
user generate an independent copy of geometric noise; however, this is equivalent to the Naïve
Scheme, which has error Θ( Δ
√
n
ϵ ) with high probability.
To strike a balance, each user samples from a diluted geometric distribution Geomβ (α), for some
suitable 0 ≤ β ≤ 1 depending on how users are compromised. Formally, we describe the diluted
geometric perturbation procedure for summation.
Diluted Geometric Perturbation Procedure. Each useri, for some 0 ≤ βi ≤ 1, generate an independent diluted Geomβi (exp( ϵ
Δ )) as random noise ri . Specifically, with probability 1 − βi , ri is set
to 0 and with probability βi ,ri is sampled from Geom(α); in the latter case, we say that the user has
generated a copy of Geom(α), although it is possible that a sample of Geom(α) can be 0. The user
produces xi := χ (xi,ri ) with the randomization function χ (x,r) := x + r. Since Geomβi (exp( ϵ
Δ ))
is unbounded, sometimes we might want to consider addition modulo some large integer p.
The following lemma formalizes the argument that differential privacy is achieved as long as at
least one copy of geometric noise is generated from a user not in K.
Lemma 4.7 (One Copy of Geom(α) is Enough for Summation). Suppose for some coalition K
and the diluted geometric perturbation procedure (for some values of βi ’s), the probability of the bad
event B that no copy of Geom(α) (where α = e
ϵ
Δ ) is generated by a user outside coalition K is at most
δ. Then, the input perturbation procedure is (ϵ, δ )-DD-private with respect to summation.
Proof. Let B be the bad event that none of the users from K := [n] \ K has generated a copy of
Geom(α). From the hypothesis, we have PrK,r[B] ≤ δ.
Let x, y be vectors, with each coordinate having range at most Δ, such that the two vectors differ
at exactly one position i  K. Suppose the diluted geometric perturbation procedure produces x
and y from x and y, respectively.
Let Q be some subset of Z × ΩK . We wish to prove that
Pr
K,r
[(sum(x), rK ) ∈ Q] ≤ eϵ Pr
K,r
[(sum(y), rK ) ∈ Q] + δ .
ACM Transactions on Algorithms, Vol. 13, No. 4, Article 50. Publication date: December 2017.         
Distributed Private Data Analysis 50:19
For σ ∈ ΩK , letQσ := {x : (x, σ ) ∈ Q}. Conditioning on the good event B and rK , we know there
is at least one independent copy of Geom(exp( ϵ
Δ )) incorporated into the final sum, hence, by Fact 1,
we have Pr[sum(x) ∈ Qσ |rK = σ, B] ≤ exp(ϵ ) · Pr[sum(y) ∈ Qσ |rK = σ, B].
Observing that Pr[(sum(x), σ ) ∈ Q|rK = σ, B] = Pr[sum(x) ∈ Qσ |rK = σ, B], it follows that we
have Pr[(sum(x), rK ) ∈ Q|rK , B] ≤ exp(ϵ ) · Pr[(sum(y), rK ) ∈ Q|rK , B]. Taking expectation on the
aforementioned inequality gives
Pr[(sum(x), rK ) ∈ Q|B] ≤ exp(ϵ ) · Pr[(sum(y), rK ) ∈ Q|B].
Finally, we have
Pr
K,r
[(sum(x), rK ) ∈ Q]
= Pr
K,r
[(sum(x), rK ) ∈ Q ∩ B] + Pr
K,r
[(sum(x), rK ) ∈ Q ∩ B]
≤ Pr
K,r
[B] Pr
K,r
[(sum(x), rK ) ∈ Q|B] + Pr
K,r
[B]
≤ eϵ · Pr
K,r
[B] Pr
K,r
[(sum(y), rK ) ∈ Q|B] + δ
= eϵ Pr
K,r
[(sum(y), rK ) ∈ Q ∩ B] + δ
≤ eϵ Pr
K,r
[(sum(y), rK ) ∈ Q] + δ,
hence proving the DD-privacy of the diluted geometric perturbation procedure.
Achieving ϵ-DD-Privacy (Proof of Theorem 4.4(a)). For each leaderi ∈ [k], we choose βi = 1,
while everyone else has βi = 0. Since the coalition K has only size k and includes the aggregator, it follows that there must be a party outside K that has generated an independent copy of
geometric noise. Hence, the probability of the bad event B is 0 and so ϵ-DD-privacy is achieved.
Analyzing the sum of k independent geometric distributions by Lemma 2.4 give the error bound
in Theorem 4.4(a).
Achieving (ϵ, δ)-DD-Privacy (Proof of Theorem 4.4(b)). For each party i ∈ [n], set βi to be
β := min{ 1
n−k+1 ln 1
δ , 1}. Then, it follows that the probability of the bad event B is (1 − β)
n−k+1 ≤ δ.
Observing that
i ∈[n] βi ≤ n
n−k+1 ln 1
δ , using Lemma 2.4, the error bound in Theorem 4.4(b) is
proved.
5 DEFINITIONS: PERIODIC AGGREGATION IN THE CLIENT-SERVER MODEL
In previous sections, we have shown a lower bound on the error for protocols achieving
information-theoretic differential privacy. In this section and the next, we will show that this lower
bound can be circumvented by considering a computational notion of differential privacy instead.
More importantly, we wish to make our protocol practical for real-world applications. For this
reason, we specifically focus on protocols in the client-server model where no client-to-client communication is necessary. We are particularly interested in periodic aggregation in the pre-processing
model. Initially, a trusted offline setup phase is performed once and for all, in which the aggregator
and all users obtain their own secret keys. Afterwards, there can be multiple aggregation phases,
and one aggregation is performed in each phase.
It is not hard to modify the notions of computational differential privacy and aggregator obliviousness to the periodic aggregation setting with preprocessing. For completeness, we will provide
formal definitions for this setting in this section.
ACM Transactions on Algorithms, Vol. 13, No. 4, Article 50. Publication date: December 2017.                
50:20 E. Shi et al.
5.1 Computational Differential Privacy for Periodic Aggregation
In Section 2, we define computational differential privacy for a single timestep and assuming no
preprocessing. Computational differential privacy can similarly be defined for periodic aggregation
in the pre-processing model. For completeness, we provide the definition as follows.
Consider the following game between an adversary A and a challenger. The adversary controls
a coalition K of nodes, where K includes the aggregator itself without loss of generality.
• Setup phase. The challenger runs the trusted setup and gives the public parameters params
as well as the secret keys of the compromised set K, that is, {ski : i ∈ K} to the adversary
A. The challenger commits to some choice of b ∈ {0, 1}.
• Aggregation phases. For each timestep t (where t is a counter that increments for each
aggregation query), the adversary adaptively chooses inputs x(t) and y(t)
. The challenger
then performs the aggregation protocol with the adversary: If b = 0, then the challenger
uses x(t)
; otherwise, it uses y(t)
. The challenger basically faithfully executes the honest
algorithms for all honest parties.
• Guess. The adversary A outputs a guess b	 of b.
Definition 5.1 (Computational Differential Privacy for Periodic Aggregation). Let ϵ > 0 and 0 ≤
δ < 1. A (randomized) protocol (ensemble) Π = {Πλ }λ∈N for periodic aggregation preserves computational (ϵ, δ )-differential privacy against coalition K if for any non-uniform probabilistic
polynomial-time algorithm A controlling the coalitionK, there exists a negligible functionη : N →
R+ such that for all λ ∈ N, for all T ∈ N, and for input vectors X := {x(t)
}t ∈[T ] and Y := {y(t)
}t ∈[T ]
in Dn adaptively chosen by the adversary A such that x (t)
i = y(t)
i , for all (i,t) ∈ [n] × [T ] except
for a pair (i0,t0) where i0  K,
Pr[A = 1|b = 1] ≤ eϵ · Pr[A = 1|b = 0] + δ + η,
where the preceding randomness is over the random bits used by the challenger and the adversary
A. In particular, when δ = 0, we say that the protocol satisfies computational ϵ-differential privacy.
5.2 Aggregator Obliviousness for Periodic Aggregation in the Client-Server Model
As before, our plan is to design an aggregator-oblivious cryptographic protocol, and then rely on
input perturbation to perturb the inputs prior to running this cryptographic protocol. We now
formulate such an aggregator-oblivious cryptographic protocol specifically for the periodic aggregation setting with pre-processing.
Suppose at each timestep t, each user i has some data x (t)
i ∈ D, and we denote x(t) :=
(x (t)
1 , x (t)
2 ,..., x (t)
n ). Our goal is to design a privacy mechanism such that for every time period,
the aggregator is able to learn some aggregate statistic f (x) but nothing more. We consider periodic aggregation protocols in the client-server model that have the following format. The protocol
consists of three (possibly randomized) algorithms. An overview of the protocol is illustrated in
Figure 1.
• Setup(n, λ): A one-time setup algorithm, run by a trusted dealer, takes the number of users
n, and a security parameter λ as inputs. It outputs the following:
(params,sk0, {ski}i ∈[n]),
where params are system parameters, that is, a description of the selected algebraic group.
Capability sk0 is distributed to the aggregator, and ski (i ∈ [m]) is a secret key distributed
to user i. The users will later use their secret keys to encrypt, and the aggregator will use
its capability to retrieve the aggregated output in each timestep.
ACM Transactions on Algorithms, Vol. 13, No. 4, Article 50. Publication date: December 2017. 
Distributed Private Data Analysis 50:21
Fig. 1. Overview of our construction. In every time period, each user adds noise ri to its value xi before
encrypting it. The aggregator uses the capability sk0 to decrypt a noisy sum but learns nothing more. The
noisy sum output by this distributed mechanism ensures each user’s differential privacy.
The setup algorithm is performed only once at system initialization and need not be
repeated for each periodic aggregation round.
• Encrypt(ski,t, x (t)
i ): During timestep t, user i uses ski to encrypt its (possibly perturbed)
data x (t)
i . The user uploads the outcome ciphertext c
(t)
i to the aggregator.
• Decrypt(sk0,t, {c
(t)
i }i ∈[n]): During timestep t, after the aggregator collects all users’ ciphertexts {c
(t)
i }i ∈[n], it calls the decryption algorithm Decrypt to retrieve the output f (x(t)
).
Apart from this output, the aggregator is unable to learn anything else.
Aggregator Obliviousness Definition. We now define aggregator obliviousness for periodic
aggregation in the client-server model.
Setup. Challenger runs the Setup algorithm and returns the public parameters param
and the secret keys for the compromised set, that is, {ski : i ∈ K} to the adversary. The
challenger flips a random coin to choose b ∈ {0, 1}.
Aggregation queries. Initially, t = 0; and the timestep counter t increments with each
query. The adversary specifies x(t) and y(t)
. If b = 0, then the challenger returns the ciphertext Encrypt(ski,t, xi ) for all i ∈ K¯ to the adversary. Else if b = 1, then the challenger
returns the ciphertext Encrypt(ski,t,yi ) for all i ∈ K¯ to the adversary.
Guess. The adversary outputs a guess of whether b is 0 or 1.
We say that the adversary wins the game if it correctly guesses b and the following condition
holds. For any timestep t, for any zK ∈ D|K | corresponding to the compromised parties, it holds
that
f

x(t)
K¯ , zK

= f

y(t)
K¯ , zK

,
where x(t) and y(t) are what the adversary queried in each timestep during the game and x(t)
K¯ and
y(t)
K¯ are the coordinates corresponding to the uncompromised parties. In the preceding, for notational simplicity, we rearranged f ’s input to group all terms corresponding to the uncompromised
parties together, and all terms corresponding to the compromised parties together.
Definition 5.2 (Aggregator Obliviousness). A periodic stream aggregation scheme is η-aggregator
oblivious with respect to f : Dn → O, if, for any adversary A,
| Pr[A = 1|b = 1] − Pr[A = 1|b = 0]| ≤ η.
If the preceding definition holds for any adversary, even computationally unbounded ones, then
we have information-theoretic aggregator obliviousness. If the protocol takes a security parameter
ACM Transactions on Algorithms, Vol. 13, No. 4, Article 50. Publication date: December 2017.
50:22 E. Shi et al.
λ ∈ N as input, and the preceding definition holds only for computationally bounded adversaries
(modeled as non-uniform probablistic polynomial-time algorithms), and moreover suppose that
η = η(λ) is a negligible function, then in this case we have computational aggregator obliviousness.
As in single time aggregation, differential privacy can be achieved for periodic aggregation by
applying a DD-private perturbation procedure independently at every timestep to an aggregator
oblivious periodic aggregation scheme. A version of Lemma 4.3 holds for periodic aggregation as
follows.
Lemma 5.3 (Periodic Aggregation: Aggregator Obliviousness and DD-Privacy Give
Differential Privacy). Suppose a periodic aggregation scheme Π0 is η-aggregator oblivious with
respect to the function f : Dn → O. Moreover, suppose further that for ϵ > 0 and 0 ≤ δ < 1, the input perturbation procedurex(t) := χ (x(t)
, r) is (ϵ, δ )-DD private with respect to f . Then, the resulting
periodic aggregation scheme Π defined by Π(x) := Π0 (x) is (ϵ, δ + η)-differentially private, where
independent randomness is used to obtain the perturbation x(t) := χ (x(t)
, r) for each t.
If Π0 is information-theoretically aggregator oblivious, then Π is information-theoretically differentially private; if Π0 is computationally aggregator oblivious, then Π is computationally differentially
private.
Proof. Suppose the inputs x and y differ only at time t0 and for some user i.
Observe that except for time t0, the view of the adversary is exactly the same for either inputs,
because independent randomness is used for each timestep.
For time t0, we can use the result of Lemma 4.3 to show that the protocol restricted to one
timestep satisfies the required differential privacy properties.
6 BLOCK AGGREGATION SCHEME: ACHIEVING AGGREGATOR
OBLIVIOUSNESS AND DD-PRIVACY
We use the framework described in Lemma 4.3 to construct a computationally differentially private
protocol for summation known as Block Aggregation (BA) Scheme,
4 which consists of two parts:
(1) a periodic stream aggregation scheme using cryptography that is computationally aggregator
oblivious with respect to summation and (2) an input perturbation procedure that satisfies DDprivacy with respect to summation. In Section 7, we will use the BA Scheme as a building block to
achieve a protocol that is failure tolerant.
We assume each user’s private data comes from {0, 1,..., Δ}, where Δ > 0 is some integer. However, in our encryption scheme, each user’s (noisy) data can be a number from D := Zp for some
large prime p. The aggregation function we consider is summation sum(x) :=
i ∈[n] xi mod p.
We outline the intuition of the cryptographic construction in Section 6.1 and describe the
construction in Section 6.2 and prove its aggregator obliviousness in Section 6.3; in Section 6.4,
we describe an input perturbation procedure that achieves DD-privacy; we analyze its utility in
Section 6.5.
6.1 Intuition for Cryptographic Scheme
One challenge we face when designing the mechanism is how to minimize the necessary communication between the participants and the data aggregator. If one allows the participants and the
aggregator to engage in an interactive multiple-party protocol in every time period, then standard
secure MPC [23] techniques can be used to ensure that the data aggregator learns only the sum.
However, the requirement that all participants must be simultaneously online and interact with
4We call it Block Aggregation Scheme because we can think of all the n users as a block, in which everyone must participate
to perform aggregation.
ACM Transactions on Algorithms, Vol. 13, No. 4, Article 50. Publication date: December 2017.     
Distributed Private Data Analysis 50:23
each other periodically renders many applications impractical, especially large-scale cloud applications. In contrast, in our solution, after a trusted setup phase between all participants and the
data aggregator, no further interaction is required except that each participant uploads a noisy
encryption to the data aggregator in each time period. The trusted setup may be performed by a
trusted third-party or through a standard secure multi-party protocol.
We now explain the intuition behind our construction. Suppose that for every time period t ∈ N,
the participants and the aggregator had means of determining n + 1 random shares of 0. In other
words, they generate ρ(t)
0 , ρ(t)
1 ,..., ρ(t)
n ∈ Zp , such that
n
i=0
ρ(t)
i = 0.
Specifically, ρ(t)
0 is the aggregator’s capability for time t, and participants 1 through n obtain ρ(t)
1
through ρ(t)
n respectively. Then the following simple idea allows the aggregator to decrypt the sum
of all participants for all time periods, without learning each individual’s values.
Encrypt. To encrypt the value x (t)
i in time period t, user i simply computes the following
ciphertext
ci,t = x (t)
i + ρ(t)
i .
Decrypt. At time t ∈ N, the aggregator receives c
(t)
1 ,...,c
(t)
n . The aggregator may obtain
the plaintext simply by summing up these ciphertexts and its capability ρ(t)
0 ,
V ← ρ(t)
0 +
n
i=1
c
(t)
i .
Since n
i=0 ρ(t)
i = 0, the aggregator obtains V = t
i=1 xi as the desired sum.
The question is how participants and the aggregator can obtain random shares of 0 without having to interact with each other in every time period. Our scheme relies on a trusted setup phase
during which each participant obtains a secret key ski where i ∈ [n], and the aggregator obtains a
capability sk0. Moreover, n
i=0 ski = 0. Let H denote a hash function (modeled as a random oracle)
that maps an integer to an appropriate mathematical group. In every time period t, each participant
computes R(t)
i = H(t)
ski fori ∈ [n], and the aggregator computes R(t)
0 = H(t)
sk0 . Since the ski sum
to zero, 	n
i=0 R(t)
i = 1. We leverage this property to construct a scheme in which the participants
never have to communicate with each other after the trusted setup phase. Furthermore, if Decisional Diffie-Hellman is hard in the mathematical group in question, we prove that the numbers
R(t)
i are “seemingly” random under the random oracle model.
6.2 Basic Construction
Let G denote a cyclic group of prime order p for which Decisional Diffie-Hellman is hard. Let
H : Z → G denote a hash function modeled as a random oracle.
Setup(1λ ). The size of G and p depend on the security parameter λ. A trusted dealer chooses a
random generator д ∈ G, and n + 1 random secrets s0,s1,...,sn ∈ Zp such that s0 + s1 +
s2 + ··· + sn = 0. The public parameters param := д. The data aggregator obtains the capability sk0 := s0, and participant i obtains the secret key ski := si .
Encrypt(param,ski,t, x). For user i to encrypt a value x ∈ Zp for timestep t, it computes the
following ciphertext:
c ← дx · H(t)
ski .
ACM Transactions on Algorithms, Vol. 13, No. 4, Article 50. Publication date: December 2017.   
50:24 E. Shi et al.
We consider the encrypt-once model, where for each time t, each user i uses its private
key to encrypt at most one input.
Decrypt(param,sk0,t,c1,c2,...,cn ). Compute
V ← H(t)
sk0

n
i=1
ci .
Suppose ci = Encrypt(param,sk0,t, xi ) for i ∈ [n]. It is not hard to see that V is of the
form
V = д
n
i=1 xi .
To decrypt the sum n
i=1 xi , it suffices to compute the discrete log of V base д. When
the plaintext space is small, decryption can be achieved through a brute-force search. A
better approach is to use Pollard’s Kangaroo method [29] which requires decryption time
roughly square root in the plaintext space. For example, suppose each participant’s input is in the range {0, 1,..., Δ}. Then the sum of the participants fall within the range
{0, 1,...,nΔ}. In this case, decryption would require √
nΔ time using Pollard’s method. In
other words, we require that nΔ is polynomial in the security parameter λ to ensure successful decryption in polynomial time. Note that the limitation of small plaintext space
is in fact typical of Diffie-Hellman-based encryption schemes when used as additively
homomorphic encryption schemes, for example, El Gamal encryption and the BGN homomorphic encryption scheme [4].
Theorem 6.1. Assuming that the Decisional Diffie-Hellman problem is hard in the group G and
that the hash function H is a random oracle, then the aforementioned construction satisfies aggregator
oblivious security in the “encrypt-once” model.
We present the proof of Theorem 6.1 in Section 6.3.
Practical Performance. In the proposed cryptographic construction, encryption consists of a
hash operation (e.g., SHA-256), two modular exponentiations, and one multiplication in a DiffieHellman group. The running time is dominated by the two modular exponentiations, as the time
for computing the hash function and group multiplication are much smaller in comparison with
the time for an exponentiation. According to benchmarking numbers reported by the eBACS
project [2], on a modern 64-bit desktop PC, it takes roughly 3ms to compute a modular exponentiation using a classic Diffie-Hellman group modular a 1024-bit prime. Using high-speed elliptic
curves such as “curve25519,” it takes only 0.3ms to compute a modular exponentiation. Therefore,
encryption can be done in roughly 0.6ms on a modern computer. Decryption of the aggregate
statistics requires taking a discrete log, and if one uses the brute-force method, then it takes one
modular exponentiation, that is, 0.3ms, to try each possible plaintext. Therefore, our scheme is
practical in situations where the plaintext space is small. For example, in the application described
by Rieffel et al. [35], each participant’s plaintext is a bit indicating her availability for communication. In this case, with roughly 1,000 participants, decryption can be done in about 0.3s using
the brute-force approach. We can have a further speed-up if we adopt Pollard’s lambda method for
decryption, reducing the running time to about √
nΔ, where n is the number of participants and
assuming each participant’s value comes from {0, 1,..., Δ}.
6.3 Proof of Aggregator Oblivious Security
First, prove that the following intermediate game is difficult to win, given that Decisional DiffieHellman is hard.
ACM Transactions on Algorithms, Vol. 13, No. 4, Article 50. Publication date: December 2017.  
Distributed Private Data Analysis 50:25
Decisional Diffie-Hellman. Let G be a group of prime order p. The input is some tuple
(д, a,b,T ) ∈ G4 for some (secret) integers x and y such that a = дx and b = дy , where T is either
(i) equal to дxy , or (ii) drawn uniformly at random from G. Then, the task is to decide whether T
is equal to дxy .
Lemma 6.2. Assuming that DDH is hard in the group G whose order is a prime p. Let K ⊆ {0,...,n}
denote a compromised set of size at least 2. A challenger picks at random α0, α1,..., αn ∈ Zp such that
n
i=0 αi = 0. The challenger also picks д,h ∈ G at random. Then, except with negligible probability,
no polynomial-time adversary can distinguish a real tuple (д,h, {αi : i ∈ K}, {дαi ,hαi : i ∈ K¯}) and
a random tuple (д,h, {αi : i ∈ K}, {дαi ,h	
i : i ∈ K¯}) where each h	
i is picked independently at random
subject to the condition that 	i ∈K¯ h	
i = 	i ∈K¯ hαi .
Proof. We use a hybrid argument. Define the following sequence of hybrid games. Assume that
the set K¯ is K¯ = {i1,i2,...,im }. For simplicity, we write (β1,..., βm ) := (αi1 ,..., αim ). In Gamed ,
the challenger reveals the following to the adversary in addition to the terms д,h, {αi : i ∈ K}, and
{дαi ,i ∈ K¯} :
R1, R2,..., Rd ,hβd+1 ,...,hβm
Here, each Ri (i ∈ [d]) means an independent fresh random number, and the following condition
holds:


1≤i ≤d
Ri =


1≤i ≤d
hβi .
It is not hard to see that Game0 is equivalent to the real world. Moreover, Gamem is equivalent
to the “random” world.
Due to the hybrid argument, it suffices to show that adjacent games Gamed−1 and Gamed are
computationally indistinguishable. To demonstrate this, we show that if, for some d, there exists
a polynomial-time adversary A who can distinguish between Gamed−1 and Gamed , we can then
construct an algorithm B which can solve the DDH problem.
Suppose B obtains a DDH tuple (д,дx ,дy,T ). Recall that B’s task is to decide whether T = дxy
or whether T is a random element from G. Let j and k denote the dth and the (d + 1)
st values of
the set K¯, respectively.
Now B picks random exponents {αi}ij,ik . B implicitly sets αk = x and αj = −
ij αi . Notice
that B does not know the values of αj and αk ; however, it knows or can compute the values of дαk =
дx and дαj = (
	ij дαi )
−1 = (дx )
−1 ·
	ij,ik д−αi . Then, B gives A the tuple (д,h = дy, {αi : i ∈
K}, {дαi : i ∈ K¯}).
The algorithm B also returns the following tuple to A.
R1, R2,..., Rd−1,
(
	i{i1,...,id+1 }(дy )
αi ·
	d−1 i=1 Ri · T )
−1, T,
(дy )
αid+2 ,..., (дy )
αim .
It is not hard to see that if T = дxy , then the preceding game is equivalent to Gamed−1. Otherwise,
if T ∈R G, then the preceding game is equivalent to Gamed . Therefore, if the adversary A has a
non-negligible advantage in guessing whether it is playing game Gamed−1 or Gamed with B, then
the algorithm B would be able to solve the DDH problem with non-negligible advantage as well.
Proof of Theorem 6.1. By a simple hybrid argument, it is not hard to see that a single-challenge
version of the security game is equivalent to multi-challenge aggregator obliviousness definition
in Definition 5.2. In the single-challenge version of the security game, the vectors x(t) and y(t)
specified by the adversary must be equal in all but one timestep. The one timestep where x(t) and
y(t) differ is called the challenge timestep.
ACM Transactions on Algorithms, Vol. 13, No. 4, Article 50. Publication date: December 2017.         
50:26 E. Shi et al.
Given an adversary A who can break the security game defined in Section 5.2 with nonnegligible probability, we construct an algorithm B who contradicts Lemma 6.2 with nonnegligible probability.
Setup. B obtains from its challenger C the following tuple д,h, {αi : i ∈ K}, {дαi ,Ti : i ∈ K¯}.
B implicitly sets α0 to be the data aggregator’s capability, and α1,..., αn to be the secret
keys of participants 1 through n respectively. The public params is д.
Let qH denote the total number of oracle queries made by the adversary A and by the
algorithm B itself. B guesses at random an index k ∈ [qH ]. Suppose the input to the kth
random oracle query ist ∗. The algorithm B assumes thatt ∗ will be the challenge timestep.
If the guess turns out to be wrong later, B simply aborts.
Hash Function Simulation. B maintains a list L that is initially empty. The adversary submits a hash query for the integert. B first checks the list L to see if t has appeared in any
entry (t, z). If so, B returns дz to the adversary. Otherwise, if this is not the kth query, B
picks a random exponent z and returns дz to the adversary and saves (t, z) to a list L. For
the kth query, B returns h.
Aggregation Queries. In each non-challenge timestep t  t ∗, the adversary A submits a
vector x (t)
. B checks if a hash query has been made on t. If not, B makes a hash oracle
query on t. As a result, B knows the discrete log of H(t). Let H(t) = дz , then B knows z.
Since B also knows дαi , B can compute the ciphertext дx · (дz )
αi as дx · (дαi )
z .
Challenge. The adversary A submits a challenge time t ∗, as well as plaintexts x(t ∗ ) :=
(x1, x2,..., xn ). (We consider the real-or-random version of the security game.) If t ∗ does
not agree with the value submitted in the kth hash query, then B aborts.
The challenger returns the following ciphertexts to the adversary:
∀i ∈ K¯ : дxi · Ti .
Guess. If the adversary A guesses that B has returned a random tuple, then B guesses b	 = 1.
Otherwise, B guesses that b	 = 0.
If the challenger C returns gives B a faithful Diffie-Hellman tuple ∀i ∈ K¯ : Ti = hαi , then the
ciphertext returned to the adversary A is a faithful encryption of the plaintext submitted by the
adversary. Otherwise, if the challenger returns to B a random tuple under the product constraint,
then the ciphertext returned to A is a random tuple under the product constraint.
6.4 Achieving DD-Privacy
We use the diluted geometric perturbation procedure in Lemma 4.7 to achieve DD-privacy with
respect to the function summation. Recall that we assume that at every timestep, each user’s
data comes from {0, 1, 2,..., Δ}, which can be thought of as a subset of D := Zp , for some large
prime p.
The next lemma gives the appropriate value of β in the diluted geometric perturbation (whose
property is proved in Lemma 4.7) procedure when at least γ fraction of users are uncompromised.
Lemma 6.3 (Achieving DD-Privacy). Let ϵ > 0 and 0 < δ < 1, and let 0 < γ ≤ 1 be the fraction of
uncompromised users. Then, the diluted geometric perturbation procedure with β = min{ 1
γ n log 1
δ , 1}
achieves (ϵ, δ )-DD-privacy with respect to sum against any coalition of size at most (1 −γ )n.
Proof. In view of Lemma 4.7, we just need to show that the probability that no user outside
K has generated a copy of Geom(α) is at most δ. This probability is at most (1 − β)
γ n ≤ e−βγn,
where the last inequality follows from 1 − t ≤ e−t .
Observe that by the choice of β, e−βγn ≤ δ, as required.
ACM Transactions on Algorithms, Vol. 13, No. 4, Article 50. Publication date: December 2017.   
Distributed Private Data Analysis 50:27
6.5 Computationally Differentially Private Periodic Stream Aggregation
Using the aggregator oblivious construction in Section 6.2 together with the input perturbation
procedure described in Section 6.4, we can obtain a differentially private aggregation scheme in
the manner described in Lemma 4.3.
We use the following notation when we describe the additive error of estimation. For any two
elements u,v ∈ Zp , we define |u −v| to be the smallest non-negative integer s such that u = v + s
mod p or v = u + s mod p.
Theorem 6.4 (Differentially Private Block Aggregation Scheme). Let ϵ > 0, 0 < δ < 1,
and η : N → R+ be a negligible function used for describing polynomial-time adversaries. Suppose
each user’s data come from {0, 1,..., Δ}, and at leastγ fraction of users are uncompromised, whereγ ≥ 1
n log 1
δ . Then, the BA Scheme described in Section 6.2, when run with the input perturbation procedure
using Geomβ (α) noise with α := e
ϵ
Δ and β := 1
γ n log 1
δ ≤ 1 in Section 6.4, is computationally (ϵ, δ +
η)-differentially private, and for all 0 < τ < 1 such that α log 2
τ ≤ 1
γ log 1
δ , with probability at least
1 − τ , the additive error of the sum is at most
4

1
γ
log 1
δ log 2
τ ·
√
α
α − 1
≤
4Δ
ϵ

1
γ
log 1
δ log 2
τ
.
Proof. The computational differential privacy of the BA Scheme follows from the application
of Lemma 4.3 using computational aggregator obliviousness (proved in Section 6.3) together with
DD-privacy (proved in Section 6.4).
Observe that the additive error is the sum of n independent Geomβ (α) random variables,
which after modulo p, can only have smaller magnitude. Hence, the error bound is given by
Lemma 2.4.
Informally, our mechanism guarantees computational differential privacy, and meanwhile ensures small error of roughly O( Δ
ϵ
 1
γ ) magnitude. As long as a constant fraction γ of users remain
uncompromised, the error term is independent of the number n of users. In fact, our result is nearly
optimal, since an accumulated noise of magnitude Θ( Δ
ϵ ) is necessary to ensure ϵ-differential privacy. Furthermore, consider the extreme case when γ = O( 1
n ), that is, each user believes that all
other users may be compromised or only a constant number of them are uncompromised; this
reduces to the Naïve Scheme and the accumulated noise would be O( Δ
ϵ
 1
γ ) = O( Δ
ϵ
√
n).
According to Theorem 6.4, the accumulated error is bounded byO( Δ
ϵ
 1
γ ) with high probability.
Then, the aggregator simply has to try to decrypt the sum within the range [−O( Δ
ϵ
 1
γ ),nΔ +
O( Δ
ϵ
 1
γ )] mod p, where p is the size of the mathematical group in use. Decryption will succeed
with high probability.
Security in a Stronger Compromise Model. So far, to keep our description clean, we have focused on a static compromise model where the compromised set K is declared before generation
of public parameters. In fact, our scheme can be proven secure in a stronger compromise model,
where nodes are compromised online such that each node remains uncompromised independently
with probability γ . However, the randomness used to decide which node to compromise must be
independent of the randomness of the public parameters and the randomness used in the scheme
to perturb inputs. Our DD-privacy proof readily works in this model of compromise. Our aggregator obliviousness proof can be adapted in a straightforward manner to incorporate dynamic
compromise.
ACM Transactions on Algorithms, Vol. 13, No. 4, Article 50. Publication date: December 2017.      
50:28 E. Shi et al.
7 BINARY PROTOCOL: ACHIEVING FAILURE TOLERANCE
7.1 Overview of the Design Space
We use the BA Scheme described in Section 6 as a building block to build up our final protocol that achieves failure tolerance.. Recall users’ inputs are represented by x := (x1, x2,..., xn ),
where each xi ∈ {0, 1,..., Δ}. The BA Scheme allows an untrusted aggregator to estimate the sum
sum(x) := n
i=1 xi with O(1) error (independent of n). In addition, the BA Scheme guarantees all
users’ differential privacy against polynomial-time adversaries. Note that the O(1) error bound is
only possible in the computational differential privacy model. In fact, the BA Scheme achieves this
by combining cryptography techniques with differential privacy techniques. As shown in Section 3, there is a lower bound of Ω(√
n) error if we adopt the information-theoretic differential
privacy notion. In other words, the Naïve Scheme described in Section 2.3 is the best one can do,
if we are restricted to using information-theoretic techniques alone.
Unfortunately, a severe drawback of the BA Scheme is that it lacks the ability to cope with user
failures. Specifically, each user in the BA Scheme would upload a noisy encryption of its input data,
and only when the aggregator has collected encrypted shares from all users, can it decrypt the sum
of all users’ data. The nature of the all-or-nothing decryption model prevents the BA Scheme from
being failure tolerant.
The Challenge Question. On one hand, we have the Naïive Scheme that achieves O(
√
n) error
and is tolerant of user failures. On the other hand, we have the BA Scheme that achievesO(1) error
but is, unfortunately, not failure tolerant. So a natural question arises: Can we seek middle-ground
between these approaches or combine them in some way, such that we can obtain the best of both
worlds, that is, achieve both fault tolerance and small error?
One idea is to form user groups (henceforth referred to as blocks) and run the BA Scheme for
each block. In this way, the aggregator is able to estimate the sum for each block. If a subset of the
users fail, then we must be able to find a set of disjoint blocks to cover the functioning users. In
this way, the aggregator can estimate the sum of the functioning users. The challenge is how to
achieve this with only a small number of groups.
7.2 Intuition
As depicted in Figure 2, our construction is based on a binary interval tree, hence the name Binary
Protocol. The high level idea is reminiscent of Dwork et al. [18] and Chan et al. [8], where they
use a binary-tree-like construction for a different purpose, that is, to achieve high utility when
releasing statistics continually in a trusted aggregator setting.
For ease of exposition, assume for now that n is a power of 2. Each leaf node is tagged with
a number in [n]. Each internal node in the tree represents a contiguous interval covering all leaf
nodes in its subtree. As a special case, we can think of the leaf nodes as representing intervals of
size 1. For each node in the tree, we also use the term block to refer to the contiguous interval
represented by the node.
Intuitively, the aggregator and users would simultaneously perform the BA Scheme for every
interval (or block) appearing in the binary tree. Hence, the aggregator would obtain an estimated
sum for each of these blocks. Normally, when n is a power of 2, the aggregator could simply
output the block estimate for the entire range [1,n]. However, imagine if a user i fails to respond,
the aggregator would then fail to obtain block estimates for any block containing i, including the
block estimate for the entire range [1,n].
Fortunately, observe that any contiguous interval within [n] can be covered by O(logn) nodes
in the binary interval tree. If κ users have failed, then the numbers 1 through n would be divided
into κ + 1 contiguous intervals, each of which can be covered by O(logn) nodes. This means that
ACM Transactions on Algorithms, Vol. 13, No. 4, Article 50. Publication date: December 2017. 
Distributed Private Data Analysis 50:29
Fig. 2. Intuition for the Binary Protocol.
the aggregator can estimate the sum of the remaining users by summing up O((κ + 1) logn) block
estimates.
Example. For convenience, we use the notation sum[i..j] (where 1 ≤ i ≤ j ≤ n) to denote the
estimated sum for the block xi, xi+1,..., xj of user inputs. Figure 2 depicts a binary tree of size
n = 8. When all users are active, the aggregator can obtain block estimates corresponding to all
nodes in the tree. Therefore, the aggregator can simply output block estimate sum[1..8]. Figure 2
illustrates the case when user 5 has failed. When this happens, the aggregator fails to obtain the
block estimates sum[5..5], sum[5..6], sum[5..8], and sum[1..8], since these blocks contain user 5.
However, the aggregator can still estimate the sum of the remaining users by summing up the
block estimates corresponding to the black nodes in the tree, namely, sum[1..4], sum[6..6], and
sum[7..8].
Privacy–utility Tradeoff. We now give a quick and informal analysis of the privacy–utility tradeoff. It is not hard to see that each user is contained in at most O(logn) blocks. This means that if
a user’s data are changed, then O(logn) blocks would be influenced. Roughly speaking, to satisfy
ϵ-differential privacy, it suffices to add noise proportional to O(
Δ log n
ϵ ) to each block.
If κ users fail, then we would be left with κ + 1 intervals. Each interval can be covered by
O(logn) nodes in the binary tree. Therefore, the final estimate would consist of O((κ + 1) logn)
block estimates. Since each block estimate contains O(
Δ log n
ϵ ) noise, the final estimate would
ACM Transactions on Algorithms, Vol. 13, No. 4, Article 50. Publication date: December 2017.
50:30 E. Shi et al.
contain O((κ + 1) logn) copies of such noises. As some positive and negative noises cancel out,
the final estimate would contain noise of roughly O(
Δ(log n)
1.5
√
κ+1
ϵ ) magnitude.
7.3 Binary Protocol: Construction
The Binary Protocol consists of running the BA Scheme over a collection of blocks simultaneously.
Specifically, if n is a power of 2, then one can build a binary interval tree of the n users such as
in Figure 2(a). Each node in the tree represents a contiguous interval, which we call a block. The
aggregator and users would run the BA Scheme for all blocks depicted in the interval tree. It is not
hard to see that each user i is contained in at most H := 
log2 n

+ 1 blocks, represented by nodes
on the path from the ith leaf node to the root of the tree.
We now state the aforementioned description more formally (also see Figure 3). Given integers
k ≥ 0 and j ≥ 1, the jth block of rank k is the subset Bk
j := {2k (j − 1) + l : 1 ≤ l ≤ 2k } of integers.
If there are n users, then we only need to consider the blocks Bk
j such that Bk
j ⊆ [n]. Define T (n)
to be the set of all relevant blocks when there are n users,
T (n) := {Bk
j |k ≥ 0, j ≥ 1, Bk
j ⊆ [n]}.
Specifically, when n is a power of 2, T (n) basically corresponds to the collection of all nodes in
the binary interval tree with n leaf nodes. It is not hard to see that the total number of blocks is at
most 2n. The following observations will be important in the design of the Binary Protocol.
Observation 1. Each user i ∈ [n] is contained in at most H := 
log2 n

+ 1 blocks. In particular,
each user is in at most one block of rank k.
7.3.1 Setup Phase. Like in the BA Scheme, a one-time trusted setup is performed at system
initialization. At the beginning, the users are randomly permuted and assigned numbers 1 through
n. Since we consider a static compromise model, this random permutation prevents the adversary
from selectively compromising all nodes in the same block.
A trusted dealer distributes O(logn) secret keys to each user. In particular, each user i ∈ [n]
obtains one secret key corresponding to each block containing the user (i.e., the path from the ith
leaf node to the root). We use the notation ski,B to denote user i’s secret key corresponding to the
block B. For each block B ∈ T (n), the trusted dealer issues a capability capB to the aggregator.
The aggregator thus receives O(n) capabilities. The parties also agree on other system parameters
including the privacy parameters (ϵ, δ ).
7.3.2 Periodic Aggregation: User Algorithm. In each timestep t ∈ [n], each user i performs the
following.
For each block B containing the useri, the user generates a fresh random noise r from the diluted
geometric distribution Geomβ (e
ϵ0
Δ ), where the choice of parameters β and ϵ0 will be explained
later. The user adds the noise ri,B to its input data xi , and obtains xi,B := xi + ri,B. The user then
encrypts xi,B using ski,B, that is, its secret key corresponding to the block B. Specifically, user i
computes
ci,B := BA.Encrypt(ski,B,t, xi,B ).
The final ciphertext ci uploaded to the aggregator is the collection of all ciphertexts, one corresponding to each block containing the user i,
ci := {ci,B |B ∈ T (n),i ∈ B}.
As each user is contained in O(logn) blocks, the ciphertext size is also O(logn).
Parameter Choices. Suppose we wish to guarantee computational (ϵ, δ )-differential privacy for
the Binary Protocol, where (ϵ, δ ) are parameters agreed on by all parties in the setup phase. Each
ACM Transactions on Algorithms, Vol. 13, No. 4, Article 50. Publication date: December 2017.   
Distributed Private Data Analysis 50:31
Fig. 3. The Binary Protocol.
ACM Transactions on Algorithms, Vol. 13, No. 4, Article 50. Publication date: December 2017.
50:32 E. Shi et al.
user needs to determine the parameters ϵ0 and β when generating a noise from the diluted geometric distribution Geomβ (e
ϵ0
Δ ). Specifically, each user chooses ϵ0 := ϵ
H , where H := 
log2 n

+ 1.
When selecting noise for a block B of size |B|, the user selects an appropriate β := min{ 1
|B | ln 1
δ0
, 1},
where δ0 = δ
H . As in Lemmas 6.3 and 4.7, each user participating in block B chooses β for the diluted geometric perturbation procedure such that the probability that no independent geometric
noise is generated by an uncompromised user is at most δ0.
Fix a user u in some block B. Observe that there are γ fraction of uncompromised users, and
the block contains a random subset of |B| users due to the random permutation of users in the
setup phase. Hence, the probability of the event Au that no uncompromised geometric noise is
generated by u is at most 1 − βγ . Since the number of uncompromised users is fixed to be γn at the
beginning, the events Au ’s over different users u in block B are negatively correlated. Therefore,
the probability that no geometric noise is generated by an uncompromised user from block B is at
most (1 − βγ )|B |
, which is at most δ0 if we set β := 1
γ |B | ln 1
δ0
.
Notice that due to Theorem 6.4, the preceding choice of ϵ0 and β ensures that each separate
copy of the BA Scheme satisfies computational (ϵ0, δ0)-differential privacy. This fact is used later
to analyze the differential privacy of the entire Binary Protocol.
Computational Differential Privacy. Strictly speaking, because we consider computational differential privacy, we have a negligible function η and achieve guarantees of the form (ϵ, δ + η)-
differential privacy. However, for notational convenience, we omit η throughout this section.
Intuitively, using the diluted geometric distribution, each user effectively adds a geometric noise
with probability β and adds 0 noise with probability 1 − β. Notice that β is smaller if the block size
is bigger, since we wish to guarantee that at least one user added a real geometric noise.
7.3.3 Periodic Aggregation: Aggregator Algorithm. Suppose 0 ≤ κ < n users have failed to respond. Then the entire range [n] would be divided up into κ + 1 contiguous interval. The aggregator will recover the noisy sum for each of these intervals, and the sum of these will be the estimate
of the total sum.
It suffices to describe how to recover the noisy count for each of these contiguous intervals.
An important observation is that each contiguous interval within [n] can be covered uniquely by
O(log2 n) blocks. This is stated more formally in the following observation.
Observation 2 (Uniqe cover for a contiguous interval.). Let [a,b] denote a contiguous
interval of integers within [n], where 1 ≤ a ≤ b ≤ n. We say that [a,b] can be covered uniquely by a
set of blocks B⊆T (n), if every integer in [a,b] appears in exactly one block in B. For any interval
[a,b] ⊆ [n], it is computationally easy to find set of at most 2

log2 n

+ 1 blocks that uniquely cover
[a,b].
Therefore, to recover the noisy count for an interval [a,b] ⊆ [n], the aggregator first finds a set
of blocks B to uniquely cover [a,b]. Then, the aggregator decrypts the noisy count for each block
B ∈ B by calling the decryption algorithm: BA.Decrypt(capB,t, {ci,B }i ∈B ). The sum of all these
block estimates is an estimate of the total count.
One possible optimization for decryption is to leverage the homomorphic property of the BA
Scheme described in Section 6. Instead of decrypting each individual block estimates, the aggregator can rely on the homomorphic property to compute an encryption of the sum of all block
estimates. In this way, only one decryption operation is required to decrypt the estimated total
count. As mentioned in Section 6 decryption takes O(nΔ) time using the brute-force approach,
and O(
√
nΔ) time using Pollard’s Rho method.
ACM Transactions on Algorithms, Vol. 13, No. 4, Article 50. Publication date: December 2017.
Distributed Private Data Analysis 50:33
This concludes the description of our Binary Protocol. Earlier in Section 7.2, we explained the
intuition of the aforementioned Binary Protocol with a small-sized example. In the remainder of
this section, we will focus on the privacy and utility analysis.
7.4 Privacy Analysis
The privacy of the Binary Protocol relies on the privacy of the underlying BA Scheme in the
following way. Suppose each copy of the underlying BA Scheme satisfies (computational) (ϵ0, δ0)-
differential privacy, then our Binary Protocol satisfies (computational) (ϵ0H, δ0H)-differential privacy. This is due to the observation that each user is contained in at most H = 
log2 n

+ 1 blocks,
and hence, each user is involved in at most H copies of the BA Scheme. By the choice of parameters, the probability that no uncompromised geometric noise is generated for a block is at most δ0.
Hence, by union bound, the probability that one of the H blocks has no uncompromised geometric
noise is at most δ0H. More generally, we have the following claim.
Claim 2 (Simple Composition Theorem). Suppose for 0 ≤ k < H, the Block Aggregation Scheme
is (computationally) (ϵk , δk )-differentially private for blocks of rank k, where δk is the probability that
a rank k block does not have an uncompromised geometric noise. Then, the resulting Binary Protocol
is (computationally) (

k ϵk ,

k δk )-differentially private.
Due to Theorem 6.4, and our parameter choices described earlier, each copy of BA Scheme
satisfies computational (ϵ0, δ0)-differential privacy, where parameters ϵ0 := ϵ
H and δ0 := δ
H . Due to
the preceding claim, the Binary Protocol satisfies computational (ϵ, δ )-differential privacy.
7.5 Utility Analysis
Suppose that a subset S ⊆ [n] of users function normally. As mentioned earlier, to obtain the sum
over S, the aggregator finds a collection B of blocks that uniquely covers S. Then, for each block
B ∈ B, the aggregator obtains the estimated sum over B. The sum of these block estimates is an
estimator for the total sum sum(S) :=
i ∈S xi .
Therefore, the additive error made by the aggregator on the total sum sum(S) is the sum of all
the diluted geometric distributions over all users in all blocks in B.
The following lemma states that when a subset S of users can be covered uniquely by L blocks,
then under reasonable technical conditions, the additive error made by the aggregator is bounded
byO( 1
ϵ0
L
γ
ln 1
δ0
) with high probability. Here γ is the fraction of uncompromised users. The lemma
is a consequence of Lemma 2.4.
Lemma 7.1 (General Error Bound). Suppose each Block Aggregation Scheme is (ϵ0, δ0)-
differentially private, and the fraction of uncompromised users is γ . Let α := e
ϵ0
Δ . Consider a subset
of users indexed by S ⊆ [n]. If S can be uniquely covered with L blocks, then for 0 < τ < 1 such that
L
γ
ln 1
δ0 ≥ α ln 2
τ , with probability at least 1 − τ , the aggregator can use the Binary Protocol to estimate
sum(S) with additive error at most 4Δ
ϵ0 ·
L
γ
ln 1
δ0 ·

ln 2
τ .
In particular, recall that when κ users have failed, the range 1 through n is divided up into κ + 1
contiguous intervals, and each interval can be covered uniquely withO(logn) blocks. Therefore, if
we plug in L := O((κ + 1) logn), ϵ0 = ϵ
H , and δ0 = δ
H into Lemma 7.1, then we obtain the following
theorem.
Theorem 7.2 (Error Bound with κ-Failed Users). Let ϵ > 0 and 0 < δ < 1. Suppose the fraction of uncompromised users is γ . Then, the Binary Protocol can be run such that it is computationally
(ϵ, δ )-differentially private. Moreover, when there are κ failed users, for 0 < τ < 1 subject to some
ACM Transactions on Algorithms, Vol. 13, No. 4, Article 50. Publication date: December 2017.      
50:34 E. Shi et al.
technical condition,5 with probability at least 1 − τ , the aggregator can estimate the sum of the participating users’ data with additive error at most O(
Δ(log n)
1.5
ϵ ·
κ+1
γ ·

(log logn + log 1
δ ) log 1
τ ).
Theorem 7.2 states that our Binary Protocol satisfies computational (ϵ, δ )-differential privacy
and achieves an error bound of O˜ (
Δ(log n)
1.5
ϵ
κ+1
γ ) with high probability (hiding a log logn factor
and δ, τ parameters). Here κ is the number of failed users, and γ is the fraction of users that remain
uncompromised.
Alternative Error Bound. We can also apply the advanced composition theorem [20] to obtain
a similar error bound as in Theorem 7.2. We present this alternative theorem for completeness—
here we rely on the following composition theorem from Reference [20, Theorem 3.3],6 instead of
Claim 2.
Fact 4 (Composition Theorem [20]). For any 0 < ϵ0 ≤ 1, δ0, δ 	 > 0 and H ∈ N, the class of
(ϵ0, δ0)-differentially private mechanisms is (ϵ,Hδ0 + δ 	
)-differentially private under L-fold adaptive
composition, where ϵ =

2H ln 1
δ	 · ϵ0 + 2Hϵ2
0 .
Choice of Parameters. In view of Fact 4, we can plug in the following parameters in Lemma 7.1:
ϵ0 := ϵ
2
√H ln 2
δ
, δ0 := δ
2H , and δ 	 := δ
2 . Moreover, we still have L := O((κ + 1) logn). For largeenough n, we will have L
γ
ln 1
δ0 ≥ e
ϵ0
Δ ln 2
τ .
Theorem 7.3 (Improved Error Bound). Let 0 < δ < 1 and 0 < ϵ ≤ ln 1
δ . Suppose the fraction
of uncompromised users is γ . Then, the Binary Protocol can be run such that it is computationally
(ϵ, δ )-differentially private. Moreover, when there are κ failed users, for 0 < τ < 1, with probability
at least 1 − τ , the aggregator can estimate the sum of the participating users’ data with additive error
at most O(
Δ log n
ϵ ·
κ+1
γ · log 1
δ ·

log logn log 1
τ ).
Note that for both Theorem 7.2 and 7.3, if we set the failure probabilities δ and τ to be negligibly
small inn, then both theorems result in an error bound of roughlyO(log2.5 n) (hiding factors related
to κ, γ , Δ, and ϵ).
7.6 Dynamic Joins
First, imagine that the system knows beforehand an upper bound n = 2H on the total number of
users—if n is not a power of 2, then assume we round it up to the nearest power of 2. We will
later discuss the case when more than n users actually join. In this case, when a new user i joins,
it needs to contact the trusted dealer and obtain a secret key ski,B for every block B ∈ T (n) that
contains i. However, existing users need not be notified. In this case, the trusted dealer must be
available to register newly joined users but need not be online for the periodic aggregation phases.
The trusted dealer may permanently erase a user’s secret key (or the aggregator’s capability) after
its issuance.
What happens if more users join than the anticipated number n = 2H ? We propose two strategies in the following text.
Key Updates at Every Power of Two. When the number of users exceeds the budget n = 2H ,
the trusted dealer sets the new budget to be n	 := 2H+1 and issues new keys and capabilities to
5The following condition is satisfied certainly when n is large enough: (κ+1) log2 n
γ ln log2 n
δ ≥ exp( ϵ Δ log2 n ) ln 2
τ .
6We thank a journal reviewer for pointing this out.
ACM Transactions on Algorithms, Vol. 13, No. 4, Article 50. Publication date: December 2017.       
Distributed Private Data Analysis 50:35
the users and aggregator as follows. For every new block B that forms in T (n	
) but is not in
T (n), a new secret key (or capability) needs to be issued to every user contained in B (and the
aggregator). Notice that the secret keys for existing blocks in T (n) need not be updated. In this
way, each existing user obtains one additional secret key, the newly joined user obtains O(logn)
secret keys, and the aggregator obtains O(n) capabilities. Notice that such key updates happen
fairly infrequently, that is, every time the number of users reach the next power of 2.
Allocate a New Tree. When the number of users reach the next power 2H of two, the trusted
dealer allocates a new tree of size 2H . For every block in the new tree, the trusted dealer issues a
capability to the aggregator corresponding to that block. For the next 2H users that join the system,
each user is issued O(H) secret keys corresponding to blocks in the new tree. Hence, the sizes of
the trees are 1, 1, 2, 4, 8,... , and so on.
When the aggregator estimates the total sum, it will simply sum up the estimate corresponding to each tree. Suppose the number of current users is n. Then, there are O(logn) such trees.
A straightforward calculation shows that the additive error made by the aggregator will be
O˜ (
Δ(log n)
3
ϵ ) with high probability.
The advantage of this approach is that only the aggregator needs to be notified when the number
of users changes. The existing users need not be notified. Therefore, this approach is particularly
suited when making push notifications to users may be difficult (e.g., when users are frequently
offline).
7.7 Dynamic Leaves
When a user leaves, that user can be treated as permanently failed. As mentioned in Theorem 7.2,
the estimation error grows only sub-linearly in the number of absent users.
For reduced error and higher utility, sometimes we may consider repeating the setup phase when
too many users have left. The application designer can make this choice to fit the characteristics
and requirements of the specific application.
8 EXTENSIONS AND VARIANTS
Evaluating Distributions. Analysts often would like to study distributions over a population.
Our scheme can be extended to allow the aggregator to periodically evaluate the (approximate)
distribution of n participants’ data. For example, suppose that the distribution is known to be a
Gaussian; then it suffices for each participant to encrypt the original value as well as its square.
It is not hard to see that the aggregator can then recover the distribution through the mean and
the variance (or second moment). For other distributions, the participants may need to encrypt
higher moments as well. In general, the more moments each participant encrypts, the better the
aggregator is able to estimate the distribution.
Public Access to a Statistic. A slight variant on our scheme enables public access to the sum but
not to individual values. In this variant, we simply set the aggregator capability sk0 = 0, essentially
making this capability public. The n participants receive values sk1,...,skn that add up to zero.
Encryption and decryption of aggregate statistics are done as before. To obtain the aggregate sum,
a discrete log must be computed, so again the plaintext space must be small.
Multiple-level Hierarchies. The protocol can be nested to support access control hierarchies, as
described in Rieffel et al. [35], in which entities at higher levels have access only to statistics pooled
over all leaf nodes under them. In the setup phase, an entity at level j > 1 is given the sum of the
secrets of the entities at the level below. (For j = 1, each entity above the leaf nodes is given the
negative of the sum of the secrets of the participants below it, as is done in the basic construction.)
ACM Transactions on Algorithms, Vol. 13, No. 4, Article 50. Publication date: December 2017.
50:36 E. Shi et al.
Product, Rather than Sum. The basic construction can be easily modified to support oblivious
computation of a product instead of a sum. Simply encrypt χ asc ← χ · H(t)
ski . Because the plaintext is no longer in the exponent, this scheme for products does not suffer from the small plaintext
restriction.
9 CONCLUSION
This article considered a scenario of distributed private data analysis, that is, how an untrusted
data aggregator can compute aggregate statistics over ciphertexts from multiple sources, while
preserving each individual’s privacy in a strong sense. We formulate a new problem and appropriate privacy notions. We demonstrate lower bounds for information-theoretic differential privacy
and construct practical protocols that satisfy computational differential privacy.
Part of our work focuses on designing a multi-input functional encryption scheme for summation (where a single trusted setup can be reused for multiple aggregations). An interesting question
is how to extend this to more general types of function evaluations—subsequent works have made
exciting progress along this direction, and multi-input functional encryption schemes for more
general functions have been proposed [5, 6, 24].
